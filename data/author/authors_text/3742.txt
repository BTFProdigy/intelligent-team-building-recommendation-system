Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 74?81,
New York, June 2006. c?2006 Association for Computational Linguistics
Exploiting Domain Structure for Named Entity Recognition
Jing Jiang and ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{jiang4,czhai}@cs.uiuc.edu
Abstract
Named Entity Recognition (NER) is a
fundamental task in text mining and nat-
ural language understanding. Current ap-
proaches to NER (mostly based on super-
vised learning) perform well on domains
similar to the training domain, but they
tend to adapt poorly to slightly different
domains. We present several strategies
for exploiting the domain structure in the
training data to learn a more robust named
entity recognizer that can perform well on
a new domain. First, we propose a sim-
ple yet effective way to automatically rank
features based on their generalizabilities
across domains. We then train a classifier
with strong emphasis on the most general-
izable features. This emphasis is imposed
by putting a rank-based prior on a logis-
tic regression model. We further propose
a domain-aware cross validation strategy
to help choose an appropriate parameter
for the rank-based prior. We evaluated
the proposed method with a task of recog-
nizing named entities (genes) in biology
text involving three species. The exper-
iment results show that the new domain-
aware approach outperforms a state-of-
the-art baseline method in adapting to new
domains, especially when there is a great
difference between the new domain and
the training domain.
1 Introduction
Named Entity Recognition (NER) is the task of
identifying and classifying phrases that denote cer-
tain types of named entities (NEs), such as per-
sons, organizations and locations in news articles,
and genes, proteins and chemicals in biomedical lit-
erature. NER is a fundamental task in many natural
language processing applications, such as question
answering, machine translation, text mining, and in-
formation retrieval (Srihari and Li, 1999; Huang and
Vogel, 2002).
Existing approaches to NER are mostly based on
supervised learning. They can often achieve high
accuracy provided that a large annotated training set
similar to the test data is available (Borthwick, 1999;
Zhou and Su, 2002; Florian et al, 2003; Klein et al,
2003; Finkel et al, 2005). Unfortunately, when the
test data has some difference from the training data,
these approaches tend to not perform well. For ex-
ample, Ciaramita and Altun (2005) reported a per-
formance degradation of a named entity recognizer
trained on CoNLL 2003 Reuters corpus, where the
F1 measure dropped from 0.908 when tested on a
similar Reuters set to 0.643 when tested on a Wall
Street Journal set. The degradation can be expected
to be worse if the training data and the test data are
more different.
The performance degradation indicates that exist-
ing approaches adapt poorly to new domains. We
believe one reason for this poor adaptability is that
these approaches have not considered the fact that,
depending on the genre or domain of the text, the
entities to be recognized may have different mor-
74
phological properties or occur in different contexts.
Indeed, since most existing learning-based NER ap-
proaches explore a large feature space, without regu-
larization, a learned NE recognizer can easily overfit
the training domain.
Domain overfitting is a serious problem in NER
because we often need to tag entities in completely
new domains. Given any new test domain, it is gen-
erally quite expensive to obtain a large amount of
labeled entity examples in that domain. As a result,
in many real applications, we must train on data that
do not fully resemble the test data.
This problem is especially serious in recognizing
entities, in particular gene names, from biomedical
literature. Gene names of one species can be quite
different from those of another species syntactically
due to their different naming conventions. For exam-
ple, some biological species such as yeast use sym-
bolic gene names like tL(CAA)G3, while some other
species such as fly use descriptive gene names like
wingless.
In this paper, we present several strategies for ex-
ploiting the domain structure in the training data to
learn a more robust named entity recognizer that can
perform well on a new domain. Our work is mo-
tivated by the fact that in many real applications,
the training data available to us naturally falls into
several domains that are similar in some aspects but
different in others. For example, in biomedical lit-
erature, the training data can be naturally grouped
by the biological species being discussed, while for
news articles, the training data can be divided by
the genre, the time, or the news agency of the arti-
cles. Our main idea is to exploit such domain struc-
ture in the training data to identify generalizable fea-
tures which, presumably, are more useful for rec-
ognizing named entities in a new domain. Indeed,
named entities across different domains often share
certain common features, and it is these common
features that are suitable for adaptation to new do-
mains; features that only work for a particular do-
main would not be as useful as those working for
multiple domains. In biomedical literature, for ex-
ample, surrounding words such as expression and
encode are strong indicators of gene mentions, re-
gardless of the specific biological species being dis-
cussed, whereas species-specific name characteris-
tics (e.g., prefix = ?-less?) would clearly not gener-
alize well, and may even hurt the performance on a
new domain. Similarly, in news articles, the part-of-
speeches of surrounding words such as ?followed by
a verb? are more generalizable indicators of name
mentions than capitalization, which might be mis-
leading if the genre of the new domain is different;
an extreme case is when every letter in the new do-
main is capitalized.
Based on these intuitions, we regard a feature as
generalizable if it is useful for NER in all training
domains, and propose a generalizability-based fea-
ture ranking method, in which we first rank the fea-
tures within each training domain, and then combine
the rankings to promote the features that are ranked
high in all domains. We further propose a rank-
based prior on logistic regression models, which
puts more emphasis on the more generalizable fea-
tures during the learning stage in a principled way.
Finally, we present a domain-aware validation strat-
egy for setting an appropriate parameter value for
the rank-based prior. We evaluated our method on
a biomedical literature data set with annotated gene
names from three species, fly, mouse, and yeast, by
treating one species as the new domain and the other
two as the training domains. The experiment results
show that the proposed method outperforms a base-
line method that represents the state-of-the-art NER
techniques.
The rest of the paper is organized as follows: In
Section 2, we introduce a feature ranking method
based on the generalizability of features across do-
mains. In Section 3, we briefly introduce the logistic
regression models for NER. We then propose a rank-
based prior on logistic regression models and de-
scribe the domain-aware validation strategy in Sec-
tion 4. The experiment results are presented in Sec-
tion 5. Finally we discuss related work in Section 6
and conclude our work in Section 7.
2 Generalizability-Based Feature Ranking
We take a commonly used approach and treat NER
as a sequential tagging problem (Borthwick, 1999;
Zhou and Su, 2002; Finkel et al, 2005). Each token
is assigned the tag I if it is part of an NE and the tag
O otherwise. Let x denote the feature vector for a
token, and let y denote the tag for x. We first com-
pute the probability p(y|x) for each token, using a
75
learned classifier. We then apply Viterbi algorithm
to assign the most likely tag sequence to a sequence
of tokens, i.e., a sentence. The features we use fol-
low the common practice in NER, including surface
word features, orthographic features, POS tags, sub-
strings, and contextual features in a local window of
size 5 around the target token (Finkel et al, 2005).
As in any learning problem, feature selection
may affect the NER performance significantly. In-
deed, a very likely cause of the domain overfit-
ting problem may be that the learned NE recog-
nizer has picked up some non-generalizable fea-
tures, which are not useful for a new domain. Below,
we present a generalizability-based feature ranking
method, which favors more generalizable features.
Formally, we assume that the training examples
are divided into m subsets T1, T2, . . . , Tm, corre-
sponding to m different domains D1, D2, . . . , Dm.
We further assume that the test set Tm+1 is from
a new domain Dm+1, and this new domain shares
some common features of the m training domains.
Note that these are reasonable assumptions that re-
flect the situation in real problems.
We use generalizability to denote the amount of
contribution a feature can make to the classification
accuracy on any domain. Thus, a feature with high
generalizability should be useful for classification
on any domain. To identify the highly generalizable
features, we must then compare their contributions
to classification among different domains.
Suppose in each individual domain, the features
can be ranked by their contributions to the classifi-
cation accuracy. There are different feature ranking
methods based on different criteria. Without loss of
generality, let us use rT : F ? {1, 2, . . . , |F |} to
denote a ranking function that maps a feature f ? F
to a rank rT (f) based on a set of training examples
T , where F is the set of all features, and the rank de-
notes the position of the feature in the final ranked
list. The smaller the rank rT (f) is, the more impor-
tant the feature f is in the training set T . For the m
training domains, we thus have m ranking functions
rT1 , rT2 , . . . , rTm .
To identify the generalizable features across the m
different domains, we propose to combine the m in-
dividual domain ranking functions in the following
way. The idea is to give high ranks to features that
are useful in all training domains . To achieve this
goal, we first define a scoring function s : F ? R
as follows:
s(f) = mmin
i=1
1
rTi(f) . (1)
We then rank the features in decreasing order of their
scores using the above scoring function. This is es-
sentially to rank features according to their maxi-
mum rank maxi rTi(f) among the m domains. Let
function rgen return the rank of a feature in this com-
bined, generalizability-based ranked list.
The original ranking function rT used for indi-
vidual domain feature ranking can use different cri-
teria such as information gain or ?2 statistic (Yang
and Pedersen, 1997). In our experiments, we used a
ranking function based on the model parameters of
the classifier, which we will explain in Section 5.2.
Next, we need to incorporate this preference for
generalizable features into the classifier. Note that
because this generalizability-based feature ranking
method is independent of the learning algorithm, it
can be applied on top of any classifier. In this work,
we choose the logistic regression classifier. One way
to incorporate the feature ranking into the classifier
is to select the top-k features, where k is chosen by
cross validation. There are two potential problems
with this hard feature selection approach. First, once
k features are selected, they are treated equally dur-
ing the learning stage, resulting in a loss of the pref-
erence among these k features. Second, this incre-
mental feature selection approach does not consider
the correlation among features. We propose an al-
ternative way to incorporate the feature ranking into
the classifier, where the preference for generalizable
features is transformed into a non-uniform prior over
the feature parameters in the model. This can be re-
garded as a soft feature selection approach.
3 Logistic Regression for NER
In binary logistic regression models, the probability
of an observation x being classified as I is
p(I|x,?) = exp(?0 +
?|F |
i=1 ?ixi)
1 + exp(?0 +?|F |i=1 ?ixi)
(2)
= exp(? ? x
?)
1 + exp(? ? x?) , (3)
76
where ?0 is the bias weight, ?i (1 ? i ? |F |)
are the weights for the features, and x? is the aug-
mented feature vector with x0 = 1. The weight vec-
tor ? can be learned from the training examples by
a maximum likelihood estimator. It is worth point-
ing out that logistic regression has a close relation
with maximum entropy models. Indeed, when the
features in a maximum entropy model are defined as
conjunctions of a feature on observations only and
a Kronecker delta of a class label, which is a com-
mon practice in NER, the maximum entropy model
is equivalent to a logistic regression model (Finkel
et al, 2005). Thus the logistic regression method we
use for NER is essentially the same as the maximum
entropy models used for NER in previous work.
To avoid overfitting, a zero mean Gaussian prior
on the weights is usually used (Chen and Rosenfeld,
1999; Bender et al, 2003), and a maximum a poste-
rior (MAP) estimator is used to maximize the poste-
rior probability:
?? = arg max
?
p(?)
N?
j=1
p(yj |xj,?), (4)
where yj is the true class label for xj, N is the num-
ber of training examples, and
p(?) =
|F |?
i=1
1?
2pi?2i
exp(? ?
2
i
2?2i
). (5)
In previous work, ?i are set uniformly to the same
value for all features, because there is in general no
additional prior knowledge about the features.
4 Rank-Based Prior
Instead of using the same ?i for all features, we pro-
pose a rank-based non-uniform Gaussian prior on
the weights of the features so that more general-
izable features get higher prior variances (i.e., low
prior strength) and features on the bottom of the list
get low prior variances (i.e., high prior strength).
Since the prior has a zero mean, such a prior would
force features on the bottom of the ranked list, which
have the least generalizability, to have near-zero
weights, but allow more generalizable features to be
assigned higher weights during the training stage.
4.1 Transformation Function
We need to find a transformation function h :
{1, 2, . . . , |F |} ? R+ so that we can set ?2i =
h(rgen(fi)), where rgen(fi) is the rank of feature
fi in the generalizability-based ranked feature list,
as defined in Section 2. We choose the following
h function because it has the desired properties as
described above:
h(r) = ar1/b , (6)
where a and b (a, b > 0) are parameters that control
the degree of the confidence in the generalizability-
based ranked feature list. Note that a corresponds to
the prior variance assigned to the top-most feature in
the ranked list. When b is small, the prior variance
drops rapidly as the rank r increases, giving only a
small number of top features high prior variances.
When b is larger, there will be less discrimination
among the features. When b approaches infinity, the
prior becomes a uniform prior with the variance set
to a for all features. If we set a small threshold ? on
the variance, then we can derive that at least m =(a
?
)b features have a prior variance greater than ? .
Thus b is proportional to the logarithm of the number
of features that are assigned a variance greater than
the threshold ? when a is fixed. Figure 1 shows the
h function when a is set to 20 and b is set to a set of
different values.
 0
 5
 10
 15
 20
 25
 0  200  400  600  800  1000
h(
r)
r
b = 2
b = 4
b = 6
b = ?
Figure 1: Transformation Function h(r) = 20r1/b
4.2 Parameter Setting using Domain-Aware
Validation
We need to set the appropriate values for the param-
eters a and b. For parameter a, we use the following
77
simple strategy to obtain an estimation. We first train
a logistic regression model on all the training data
using a Gaussian prior with a fixed variance (set to
1 in our experiments). We then find the maximum
weight
?max = |F |maxi=1 |?i| (7)
in this trained model. Finally we set a = ?2max. Our
reasoning is that since a is the variance of the prior
for the best feature, a is related to the ?permissible
range? of ? for the best feature, and ?max gives us a
way for adjusting a according to the empirical range
of ?i?s.
As we pointed out in Section 4.1, when a is fixed,
parameter b controls the number of top features that
are given a relatively high prior variance, and hence
implicitly controls the number of top features to
choose for the classifier to put the most weights on.
To select an appropriate value of b, we can use a
held-out validation set to tune the parameter value
b. Here we present a validation strategy that exploits
the domain structure in the training data to set the
parameter b for a new domain. Note that in regular
validation, both the training set and the validation
set contain examples from all training domains. As
a result, the average performance on the validation
set may be dominated by domains in which the NEs
are easy to classify. Since our goal is to build a clas-
sifier that performs well on new domains, we should
pay more attention to hard domains that have lower
classification accuracy. We should therefore exam-
ine the performance of the classifier on each training
domain individually in the validation stage to gain
an insight into the appropriate value of b for a new
domain, which has an equal chance of being similar
to any of the training domains.
Our domain-aware validation strategy first finds
the optimal value of b for each training domain. For
each subset Ti of the training data belonging to do-
main Di, we divide it into a training set T ti and a val-
idation set T vi . Then for each domain Di, we train a
classifier on the training sets of all domains, that is,
we train on ?mj=1 T tj . We then test the classifier on
T vi . We try a set of different values of b with a fixed
value of a, and choose the optimal b that gives the
best performance on T vi . Let this optimal value of b
for domain Di be bi.
Given bi (1 ? i ? m), we can choose an appropri-
ate value of bm+1 for an unknown test domain Dm+1
based on the assumption that Dm+1 is a mixture of
all the training domains. bm+1 is then chosen to be
a weighted average of bi, (1 ? i ? m):
bm+1 =
m?
i=1
?ibi, (8)
where ?i indicates how similar Dm+1 is to Di. In
many cases, the test domain Dm+1 is completely
unknown. In this case, the best we can do is to set
?i = 1/m for all i, that is, to assume that Dm+1 is
an even mixture of all training domains.
5 Empirical Evaluation
5.1 Experimental Setup
We evaluated our domain-aware approach to NER
on the problem of gene recognition in biomedical
literature. The data we used is from BioCreAtIvE
Task 1B (Hirschman et al, 2005). We chose this
data set because it contains three subsets of MED-
LINE abstracts with gene names from three species
(fly, mouse, and yeast), while no other existing an-
notated NER data set has such explicit domain struc-
ture. The original BioCreAtIvE 1B data was not
provided with every gene annotated, but for each ab-
stract, a list of genes that were mentioned in the ab-
stract was given. A gene synonym list was also given
for each species. We used a simple string matching
method with slight relaxation to tag the gene men-
tions in the abstracts. We took 7500 sentences from
each species for our experiments, where half of the
sentences contain gene mentions. We further split
the 7500 sentences of each species into two sets,
5000 for training and 2500 for testing.
We conducted three sets of experiments, each
combining the 5000-sentence training data of two
species as training data, and the 2500-sentence test
data of the third species as test data. The 2500-
sentence test data of the training species was used
for validation. We call these three sets of experi-
ments F+M?Y, F+Y?M, and M+Y?F.
we use FEX1 for feature extraction and BBR2 for
logistic regression in our experiments.
1http://l2r.cs.uiuc.edu/ cogcomp/asoftware.php?skey=FEX
2http://www.stat.rutgers.edu/ madigan/BBR/
78
5.2 Comparison with Baseline Method
Because the data set was generated by our automatic
tagging procedure using the given gene lists, there is
no previously reported performance on this data set
for us to compare with. Therefore, to see whether
using the domain structure in the training data can
really help the adaptation to new domains, we com-
pared our method with a state-of-the-art baseline
method based on logistic regression. It uses a Gaus-
sian prior with zero mean and uniform variance on
all model parameters. It also employs 5-fold regular
cross validation to pick the optimal variance for the
prior. Regular feature selection is also considered
in the baseline method, where the features are first
ranked according to some criterion, and then cross
validation is used to select the top-k features. We
tested three popular regular feature ranking meth-
ods: feature frequency (F), information gain (IG),
and ?2 statistic (CHI). These methods were dis-
cussed in (Yang and Pedersen, 1997). However, with
any of the three feature ranking criteria, cross valida-
tion showed that selecting all features gave the best
average validation performance. Therefore, the best
baseline method which we compare our method with
uses all features. We call the baseline method BL.
In our method, the generalizability-based feature
ranking requires a first step of feature ranking within
each training domain. While we could also use F,
IG or CHI to rank features in each domain, to make
our method self-contained, we used the following
strategy. We first train a logistic regression model
on each domain using a zero-mean Gaussian prior
with variance set to 1. Then, features are ranked
in decreasing order of the absolute values of their
weights. The rationale is that, in general, features
with higher weights in the logistic regression model
are more important. With this ranking within each
training domain, we then use the generalizability-
based feature ranking method to combine the m
domain-specific rankings. The obtained ranked fea-
ture list is used to construct the rank-based prior,
where the parameters a and b are set in the way as
discussed in Section 4.2. We call our method DOM.
In Table 1, we show the precision, recall, and F1
measures of our domain-aware method (DOM) and
the baseline method (BL) in all three sets of exper-
iments. We see that the domain-aware method out-
performs the baseline method in all three cases when
F1 is used as the primary performance measure. In
F+Y?M and M+Y?F, both precision and recall are
also improved over the baseline method.
Exp Method P R F1
F+M?Y BL 0.557 0.466 0.508
DOM 0.575 0.516 0.544
F+Y?M BL 0.571 0.335 0.422
DOM 0.582 0.381 0.461
M+Y?F BL 0.583 0.097 0.166
DOM 0.591 0.139 0.225
Table 1: Comparison of the domain-aware method
and the baseline method, where in the domain-aware
method, b = 0.5b1 + 0.5b2
Note that the absolute performance shown in Ta-
ble 1 is lower than the state-of-the-art performance
of gene recognition (Finkel et al, 2005).3 One rea-
son is that we explicitly excluded the test domain
from the training data, while most previous work on
gene recognition was conducted on a test set drawn
from the same collection as the training data. An-
other reason is that we used simple string match-
ing to generate the data set, which introduced noise
to the data because gene names often have irregular
lexical variants.
5.3 Comparison with Regular Feature Ranking
Methods
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 1  2  3  4  5  6  7  8  9  10
F1
b
F+M?Y
DOM
F
IG
CHI
BL
Figure 2: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
F+M?Y
3Our baseline method performed comparably to the state-of-
the-art systems on the standard BioCreAtIvE 1A data.
79
 0.3
 0.35
 0.4
 0.45
 0.5
 1  2  3  4  5  6  7  8  9  10
F1
b
F+Y?M
DOM
F
IG
CHI
BL
Figure 3: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
F+Y?M
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 1  2  3  4  5  6  7  8  9  10
F1
b
M+Y?F
DOM
F
IG
CHI
BL
Figure 4: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
M+Y?F
To further understand how our method improved
the performance, we compared the generalizability-
based feature ranking method with the three regular
feature ranking methods, F, IG, and CHI, that were
used in the baseline method. To make fair compar-
ison, for the regular feature ranking methods, we
also used the rank-based prior transformation as de-
scribed in Section 4 to incorporate the preference for
top-ranked features. Figure 2, Figure 3 and Figure 4
show the performance of different feature ranking
methods in the three sets of experiments as the pa-
rameter b for the rank-based prior changes. As we
pointed out in Section 4, b is proportional to the log-
arithm of the number of ?effective features?.
From the figures, we clearly see that the curve for
the generalizability-based ranking method DOM is
always above the curves of the other methods, indi-
cating that when the same amount of top features are
being emphasized by the prior, the features selected
by DOM give better performance on a new domain
than the features selected by the other methods. This
suggests that the top-ranked features in DOM are in-
deed more suitable for adaptation to new domains
than the top features ranked by the other methods.
The figures also show that the ranking method
DOM achieved better performance than the baseline
over a wide range of b values, especially in F+Y?M
and M+Y?F, whereas for methods F, IG and CHI,
the performance quickly converged to the baseline
performance as b increased.
It is interesting to note the comparison between F
and IG (or CHI). In general, when the test data is
similar to the training data, IG (or CHI) is advanta-
geous over F (Yang and Pedersen, 1997). However,
in this case when the test domain is different from
the training domains, F shows advantages for adap-
tation. A possible explanation is that frequent fea-
tures are in general less likely to be domain-specific,
and therefore feature frequency can also be used as a
criterion to select generalizable features and to filter
out domain-specific features, although it is still not
as effective as the method we proposed.
6 Related Work
The NER problem has been extensively studied in
the NLP community. Most existing work has fo-
cused on supervised learning approaches, employ-
ing models such as HMMs (Zhou and Su, 2002),
MEMMs (Bender et al, 2003; Finkel et al, 2005),
and CRFs (McCallum and Li, 2003). Collins and
Singer (1999) proposed an unsupervised method for
named entity classification based on the idea of co-
training. Ando and Zhang (2005) proposed a semi-
supervised learning method to exploit unlabeled data
for building more robust NER systems. In all these
studies, the evaluation is conducted on unlabeled
data similar to the labeled data.
Recently there have been some studies on adapt-
ing NER systems to new domains employing tech-
niques such as active learning and semi-supervised
learning (Shen et al, 2004; Mohit and Hwa, 2005),
80
or incorporating external lexical knowledge (Cia-
ramita and Altun, 2005). However, there has not
been any study on exploiting the domain structure
contained in the training examples themselves to
build generalizable NER systems. We focus on
the domain structure in the training data to build
a classifier that relies more on features generaliz-
able across different domains to avoid overfitting the
training domains. As our method is orthogonal to
most of the aforementioned work, they can be com-
bined to further improve the performance.
7 Conclusion and Future Work
Named entity recognition is an important problem
that can help many text mining and natural lan-
guage processing tasks such as information extrac-
tion and question answering. Currently NER faces
a poor domain adaptability problem when the test
data is not from the same domain as the training
data. We present several strategies to exploit the
domain structure in the training data to improve the
performance of the learned NER classifier on a new
domain. Our results show that the domain-aware
strategies we proposed improved the performance
over a baseline method that represents the state-of-
the-art NER techniques.
Acknowledgments
This work was in part supported by the National
Science Foundation under award numbers 0425852,
0347933, and 0428472. We would like to thank
Bruce Schatz, Xin He, Qiaozhu Mei, Xu Ling, and
some other BeeSpace project members for useful
discussions. We would like to thank Mark Sammons
for his help with FEX. We would also like to thank
the anonymous reviewers for their comments.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for text
chunking. In Proceedings of ACL-2005.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL-2003.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, School of Com-
puter Science, Carnegie Mellon University.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Workshop on Advances
in Structured Learning for Text and Speech Processing
(NIPS-2005).
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP/VLC-1999.
Jenny Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics, 6(Suppl 1):S5.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
Lynette Hirschman, Marc Colosimo, Alexander Morgan,
and Alexander Yeh. 2005. Overview of BioCreAtIvE
task 1B: normailized gene lists. BMC Bioinformatics,
6(Suppl 1):S11.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI-2002.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recogni-
tion with character-level models. In Proceedings of
CoNLL-2003.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Behrang Mohit and Rebecca Hwa. 2005. Syntax-based
semi-supervised named entity tagging. In Proceedings
of ACL-2005.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of ACL-
2004.
Rohini Srihari and Wei Li. 1999. Information extraction
supported question answering. In TREC-8.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML-1997.
Guodong Zhou and Jian Su. 2002. Named entity recog-
nition using an HMM-based chunk tagger. In Proceed-
ings of ACL-2002.
81
Proceedings of NAACL HLT 2007, pages 113?120,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Systematic Exploration of the Feature Space for Relation Extraction
Jing Jiang and ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{jiang4,czhai}@cs.uiuc.edu
Abstract
Relation extraction is the task of find-
ing semantic relations between entities
from text. The state-of-the-art methods
for relation extraction are mostly based
on statistical learning, and thus all have
to deal with feature selection, which can
significantly affect the classification per-
formance. In this paper, we systemat-
ically explore a large space of features
for relation extraction and evaluate the ef-
fectiveness of different feature subspaces.
We present a general definition of fea-
ture spaces based on a graphic represen-
tation of relation instances, and explore
three different representations of relation
instances and features of different com-
plexities within this framework. Our ex-
periments show that using only basic unit
features is generally sufficient to achieve
state-of-the-art performance, while over-
inclusion of complex features may hurt
the performance. A combination of fea-
tures of different levels of complexity and
from different sentence representations,
coupled with task-oriented feature prun-
ing, gives the best performance.
1 Introduction
An important information extraction task is relation
extraction, whose goal is to detect and characterize
semantic relations between entities in text. For ex-
ample, the text fragment ?hundreds of Palestinians
converged on the square? contains the located rela-
tion between the Person entity ?hundreds of Pales-
tinians? and the Bounded-Area entity ?the square?.
Relation extraction has applications in many do-
mains, including finding affiliation relations from
web pages and finding protein-protein interactions
from biomedical literature.
Recent studies on relation extraction have shown
the advantages of discriminative model-based sta-
tistical machine learning approach to this problem.
There are generally two lines of work following this
approach. The first utilizes a set of carefully se-
lected features obtained from different levels of text
analysis, from part-of-speech (POS) tagging to full
parsing and dependency parsing (Kambhatla, 2004;
Zhao and Grishman, 2005; Zhou et al, 2005)1. The
second line of work designs kernel functions on
some structured representation (sequences or trees)
of the relation instances to capture the similarity be-
tween two relation instances (Zelenko et al, 2003;
Culotta and Sorensen, 2004; Bunescu and Mooney,
2005a; Bunescu and Mooney, 2005b; Zhang et al,
2006a; Zhang et al, 2006b). Of particular interest
among the various kernels proposed are the convolu-
tion kernels (Bunescu and Mooney, 2005b; Zhang et
al., 2006a), because they can efficiently compute the
similarity between two instances in a huge feature
space due to their recursive nature. Apart from their
computational efficiency, convolution kernels also
implicitly correspond to some feature space. There-
fore, both lines of work rely on an appropriately de-
1Although Zhao and Grishman (2005) defined a number of
kernels for relation extraction, the method is essentially similar
to feature-based methods.
113
fined set of features. As in any learning problem, the
choice of features can affect the performance signif-
icantly.
Despite the importance of feature selection, there
has not been any systematic exploration of the fea-
ture space for relation extraction, and the choices
of features in existing work are somewhat arbitrary.
In this paper, we conduct a systematic study of the
feature space for relation extraction, and evaluate
the effectiveness of different feature subspaces. Our
motivations are twofold. First, based on previous
studies, we want to identify and characterize the
types of features that are potentially useful for rela-
tion extraction, and define a relatively complete and
structured feature space that can be systematically
explored. Second, we want to compare the effective-
ness of different features. Such a study can guide us
to choose the most effective feature set for relation
extraction, or to design convolution kernels in the
most effective way.
We propose and define a unified graphic repre-
sentation of the feature space, and experiment with
three feature subspaces, corresponding to sequences,
syntactic parse trees and dependency parse trees.
Experiment results show that each subspace is ef-
fective by itself, with the syntactic parse tree sub-
space being the most effective. Combining the three
subspaces does not generate much improvement.
Within each feature subspace, using only the basic
unit features can already give reasonably good per-
formance. Adding more complex features may not
improve the performance much or may even hurt
the performance. Task-oriented heuristics can be
used to prune the feature space, and when appropri-
ately done, can improve the performance. A com-
bination of features of different levels of complex-
ity and from different sentence representations, cou-
pled with task-oriented feature pruning, gives the
best performance.
2 Related Work
Zhao and Grishman (2005) and Zhou et al (2005)
explored a large set of features that are potentially
useful for relation extraction. However, the feature
space was defined and explored in a somewhat ad
hoc manner. We study a broader scope of features
and perform a more systematic study of different
feature subspaces. Zelenko et al (2003) and Culotta
and Sorensen (2004) used tree kernels for relation
extraction. These kernels can achieve high precision
but low recall because of the relatively strict match-
ing criteria. Bunescu and Mooney (2005a) proposed
a dependency path kernel for relation extraction.
This kernel also suffers from low recall for the same
reason. Bunescu and Mooney (2005b) and Zhang
et. al. (2006a; 2006b) applied convolution string ker-
nels and tree kernels, respectively, to relation extrac-
tion. The convolution tree kernels achieved state-
of-the-art performance. Since convolution kernels
correspond to some explicit large feature spaces, the
feature selection problem still remains.
General structural representations of natural lan-
guage data have been studied in (Suzuki et al,
2003; Cumby and Roth, 2003), but these models
were not designed specifically for relation extrac-
tion. Our feature definition is similar to these mod-
els, but more specifically designed for relation ex-
traction and systematic exploration of the feature
space. Compared with (Cumby and Roth, 2003), our
feature space is more compact and provides more
guidance on selecting meaningful subspaces.
3 Task Definition
Given a small piece of text that contains two entity
mentions, the task of relation extraction is to decide
whether the text states some semantic relation be-
tween the two entities, and if so, classify the rela-
tion into one of a set of predefined semantic rela-
tion types. Formally, let r = (s, arg1, arg2) de-
note a relation instance, where s is a sentence, arg1
and arg2 are two entity mentions contained in s, and
arg1 precedes arg2 in the text. Given a set of rela-
tion instances {ri}, each labeled with a type ti ? T ,
where T is the set of predefined relation types plus
the type nil, our goal is to learn a function that maps
a relation instance r to a type t ? T . Note that we
do not specify the representation of s here. Indeed, s
can contain more structured information in addition
to merely the sequence of tokens in the sentence.
4 Feature Space for Relation Extraction
Ideally, we would like to define a feature space with
at least two properties: (1) It should be complete in
the sense that all features potentially useful for the
114
classification problem are included. (2) It should
have a good structure so that a systematic search in
the space is possible. Below we show how a unified
graph-based feature space can be defined to satisfy
these two properties.
4.1 A Unified View of Features for Relation
Extraction
Before we introduce our definition of the feature
space, let us first look at some typical features used
for relation extraction. Consider the relation in-
stance ?hundreds of Palestinians converged on the
square? with arg1 = ?hundreds of Palestinians? and
arg2 = ?the square?. Various types of information
can be useful for classifying this relation instance.
For example, knowing that arg1 is an entity of type
Person can be useful. This feature involves the sin-
gle token ?Palestinians?. Another feature, ?the head
word of arg1 (Palestinians) is followed by a verb
(converged)?, can also be useful. This feature in-
volves two tokens, ?Palestinians? and ?converged?,
with a sequence relation. It also involves the knowl-
edge that ?Palestinians? is part of arg1 and ?con-
verged? is a verb. If we have the syntactic parse tree
of the sentence, we can obtain even more complex
and discriminative features. For example, the syn-
tactic parse tree of the same relation instance con-
tains the following subtree: [VP ? VBD [PP ? [IN
? on] NP] ]. If we know that arg2 is contained in the
NP in this subtree, then this subtree states that arg2
is in a PP that is attached to a VP, and the proposition
is ?on?. This subtree therefore may also a useful
feature. Similarly, if we have the dependency parse
tree of the relation instance, then the dependency
link ?square ? on? states that the token ?square?
is dependent on the token ?on?, which may also be
a useful feature.
Given that useful features are of various forms, in
order to systematically search the feature space, we
need to first have a unified view of features. This
problem is not trivial because it is not immediately
clear how different types of features can be unified.
We observe, however, that in general features fall
into two categories: (1) properties of a single token,
and (2) relations between tokens. Features that in-
volve attributes of a single token, such as bag-of-
word features and entity attribute features, belong
to the first category, while features that involve se-
quence, syntactic or dependency relations between
tokens belong to the second category. Motivated by
this observation, we can represent relation instances
as graphs, with nodes denoting single tokens or syn-
tactic categories such as NPs and VPs, and edges de-
noting various types of relations between the nodes.
4.2 Relation Instance Graphs
We represent a relation instance as a labeled, di-
rected graph G = (V,E,A,B), where V is the set
of nodes in the graph, E is the set of directed edges
in the graph, and A, B are functions that assign la-
bels to the nodes.
First, for each node v ? V , A(v) =
{a1, a2, . . . , a|A(v)|} is a set of attributes associated
with node v, where ai ? ?, and ? is an alphabet that
contains all possible attribute values. The attributes
are introduced to help generalize the node. For ex-
ample, if node v represents a token, then A(v) can
include the token itself, its morphological base form,
its POS, its semantic class (e.g. WordNet synset),
etc. If v also happens to be the head word of arg1 or
arg2, then A(v) can also include the entity type and
other entity attributes. If node v represents a syntac-
tic category such as an NP or VP, A(v) can simply
contain only the syntactic tag.
Next, function B : V ? {0, 1, 2, 3} is introduced
to distinguish argument nodes from non-argument
nodes. For each node v ? V , B(v) indicates how
node v is related to arg1 and arg2. 0 indicates that
v does not cover any argument, 1 or 2 indicates that
v covers arg1 or arg2, respectively, and 3 indicates
that v covers both arguments. We will see shortly
that only nodes that represent syntactic categories in
a syntactic parse tree can possibly be assigned 3. We
refer to B(v) as the argument tag of v.
We now consider three special instantiations of
this general definition of relation instance graphs.
See Figures 1, 2 and 3 for examples of each of the
three representations.
Sequence: Without introducing any additional
structured information, a sequence representation
preserves the order of the tokens as they occur in the
original sentence. Each node in this graph is a token
augmented with its relevant attributes. For example,
head words of arg1 and arg2 are augmented with the
corresponding entity types. A token is assigned the
argument tag 1 or 2 if it is the head word of arg1 or
115
NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area
00 1 0 0 0 2
Person VBD1 0 Bounded-Area2
Figure 1: An example sequence representation. The
subgraph on the left represents a bigram feature. The
subgraph on the right represents a unigram feature
that states the entity type of arg2.
NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area
00 1 0 0 0 2
NPB NPB
PP
NP
1
1
0
1
S
VP
PP
NPB
3
2
2
2
on DT Bounded-Area0 0 2
PP
NPB
2 2
Figure 2: An example syntactic parse tree represen-
tation. The subgraph represents a subtree feature
(grammar production feature).
arg2. Otherwise, it is assigned the argument tag 0.
There is a directed edge from u to v if and only if
the token represented by v immediately follows that
represented by u in the sentence.
Syntactic Parse Tree: The syntactic parse tree
of the relation instance sentence can be augmented
to represent the relation instance. First, we modify
the tree slightly by conflating each leaf node in the
original parse tree with its parent, which is a preter-
minal node labeled with a POS tag. Then, each node
is augmented with relevant attributes if necessary.
Argument tags are assigned to the leaf nodes in the
same way as in the sequence representation. For an
internal node v, argument tag 1 or 2 is assigned if
either arg1 or arg2 is inside the subtree rooted at v,
and 3 is assigned if both arguments are inside the
subtree. Otherwise, 0 is assigned to v.
Dependency Parse Tree: Similarly, the depen-
dency parse tree can also be modified to represent
the relation instance. Assignment of attributes and
argument tags is the same as for the sequence repre-
sentation. To simplify the representation, we ignore
NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area
00 1 0 0 0 2
of Palestinians10
Figure 3: An example dependency parse tree rep-
resentation. The subgraph represents a dependency
relation feature between arg1 ?Palestinians? and
?of?.
the dependency relati types.
4.3 Features
Given the above definition of relation instance
graphs, we are now ready to define features. Intu-
itively, a feature of a relation instance captures part
of the attributive and/or structural properties of the
relation instance graph. Therefore, it is natural to de-
fine a feature as a subgraph of the relation instance
graph. Formally, given a graph G = (V,E,A,B),
which represents a single relation instance, a fea-
ture that exists in this relation instance is a sub-
graph G? = (V ?, E?, A?, B?) that satisfies the fol-
lowing conditions: V ? ? V , E? ? E, and ?v ?
V ?, A?(v) ? A(v), B?(v) = B(v).
We now show that many features that have been
explored in previous work on relation extraction can
be transformed into this graphic representation. See
Figures 1, 2 and 3 for some examples.
Entity Attributes: Previous studies have shown
that entity types and entity mention types of arg1
and arg2 are very useful (Zhao and Grishman, 2005;
Zhou et al, 2005; Zhang et al, 2006b). To represent
a single entity attribute, we can take a subgraph that
contains only the node representing the head word of
the argument, labeled with the entity type or entity
mention type. A particularly useful type of features
are conjunctive entity features, which are conjunc-
tions of two entity attributes, one for each argument.
To represent a conjunctive feature such as ?arg1 is
a Person entity and arg2 is a Bounded-Area entity?,
we can take a subgraph that contains two nodes, one
for each argument, and each labeled with an en-
tity attribute. Note that in this case, the subgraph
contains two disconnected components, which is al-
lowed by our definition.
Bag-of-Words: These features have also been
116
explore by Zhao and Grishman (2005) and Zhou
et. al. (2005). To represent a bag-of-word feature,
we can simply take a subgraph that contains a single
node labeled with the token. Because the node also
has an argument tag, we can distinguish between ar-
gument word and non-argument word.
Bigrams: A bigram feature (Zhao and Grishman,
2005) can be represented by a subgraph consisting
of two connected nodes from the sequence represen-
tation, where each node is labeled with the token.
Grammar Productions: The features in convo-
lution tree kernels for relation extraction (Zhang et
al., 2006a; Zhang et al, 2006b) are sequences of
grammar productions, that is, complete subtrees of
the syntactic parse tree. Therefore, these features
can naturally be represented by subgraphs of the re-
lation instance graphs.
Dependency Relations and Dependency Paths:
These features have been explored by Bunescu and
Mooney (2005a), Zhao and Grishman (2005), and
Zhou et. al. (2005). A dependency relation can be
represented as an edge connecting two nodes from
the dependency tree. The dependency path between
the two arguments can also be easily represented as
a path in the dependency tree connecting the two
nodes that represent the two arguments.
There are some features that are not covered by
our current definition, but can be included if we
modify our relation instance graphs. For example,
gapped subsequence features in subsequence ker-
nels (Bunescu and Mooney, 2005b) can be repre-
sented as subgraphs of the sequence representation
if we add more edges to connect any pair of nodes u
and v provided that the token represented by u oc-
curs somewhere before that represented by v in the
sentence. Since our feature definition is very gen-
eral, our feature space also includes many features
that have not been explored before.
4.4 Searching the Feature Space
Although the feature space we have defined is rel-
atively complete and has a clear structure, it is still
too expensive to exhaustively search the space be-
cause the number of features is exponential in terms
of the size of the relation instance graph. We thus
propose to search the feature space in the follow-
ing bottom-up manner: We start with the conjunc-
tive entity features (defined in Section 4.3), which
have been found effective in previous studies and
are intuitively necessary for relation extraction. We
then systematically add unit features with different
granularities. We first consider the minimum (i.e.
most basic) unit features. We then gradually include
more complex features. The motivations for this
strategy are the following: (1) Using the smallest
features to represent a relation instance graph pre-
sumably covers all unit characteristics of the graph.
(2) Using small subgraphs allows fuzzy matching,
which is good for our task because relation instances
of the same type may vary in their relation instance
graphs, especially with the noise introduced by ad-
jectives, adverbs, or irrelevant propositional phrases.
(3) The number of features of a fixed small size is
polynomial in terms of the size of the relation in-
stance graph. It is therefore feasible to generate all
the small unit features and use any classifier such as
a maximum entropy classifier or an SVM.
In our experiments, we consider three levels of
small unit features in increasing order of their com-
plexity. First, we consider unigram features Guni =
({u}, ?, Auni , B), where Auni(u) = {ai} ? A(u).
In another word, unigram features consist of a sin-
gle node labeled with a single attribute. Examples
of unigram features include bag-of-word features
and non-conjunctive entity attribute features. At the
second level, we consider bigram features Gbi =
({u, v}, {(u, v)}, Auni , B). Bigram features are
therefore single edges connecting two nodes, where
each node is labeled with a single attribute. The
third level of attributes we consider are trigram fea-
tures Gtri = ({u, v, w}, {(u, v), (u,w)}, Auni , B)
or Gtri = ({u, v, w}, {(u, v), (v, w)}, Auni , B).
Thus trigram features consist of two connected
edges and three nodes, where each node is also la-
beled with a single attribute.
We treat the three relation instance graphs (se-
quences, syntactic parse trees, and dependency parse
trees) as three feature subspaces, and search in each
subspace. For each feature subspace, we incremen-
tally add the unigram, bigram and trigram features
to the working feature set. For the syntactic parse
tree representation, we also consider a fourth level of
small unit features, which are single grammar pro-
ductions such as [VP ? VBD PP], because these
are the smallest features in convolution tree kernels.
After we explore each feature subspace, we try to
117
combine the features from the three subspaces to see
whether the performance can be improved, that is,
we test whether the sequence, syntactic and depen-
dency relations can complement each other.
5 Experiments
5.1 Data Set and Experiment Setup
We used the data set from ACE (Automatic Con-
tent Extraction) 2004 evaluation to conduct our ex-
periments. This corpus defines 7 types of relations:
Physical, Personal / Social, Empolyment / Memeber-
ship / Subsidiary, Agent-Artifact, PER / ORG Affili-
ation, GPE Affiliation and Discourse.
We used Collins parser to parse the sentences in
the corpus because Collins parser gives us the head
of each syntactic category, which allows us to trans-
form the syntactic parse trees into dependency trees.
We discarded sentences that could not be parsed
by Collins parser. The candidate relation instances
were generated by considering all pairs of entities
that occur in the same sentence. We obtained 48625
candidate relation instances in total, among which
4296 instances were positive.
As in most existing work, instead of using the en-
tire sentence, we used only the sequence of tokens
that are inside the minimum complete subtree cov-
ering the two arguments. Presumably, tokens out-
side of this subtree are not so relevant to the task. In
our graphic representation of relation instances, the
attribute set for a token node includes the token it-
self, its POS tag, and entity type, entity subtype and
entity mention type when applicable. The attribute
set for a syntactic category node includes only the
syntactic tag. We used both maximum entropy clas-
sifier and SVM for all experiments. We adopted one
vs. others strategy for the multi-class classification
problem. In all experiments, the performance shown
was based on 5-fold cross validation.
5.2 General Search in the Feature Subspaces
Following the general search strategy, we conducted
the following experiments. For each feature sub-
space, we started with the conjunctive entity features
plus the unigram features. We then incrementally
added bigram and trigram features. For the syntac-
tic parse tree feature space, we conducted an addi-
tional experiment: We added basic grammar produc-
tion features on top of the unigram, bigram and tri-
gram features. Adding production features allows us
to study the effect of adding more complex and pre-
sumably more specific and discriminative features.
Table 1 shows the precision (P), recall (R) and F1
measure (F) from the experiments with the maxi-
mum entropy classifier (ME) and the SVM classi-
fier (SVM). We can compare the results in two di-
mensions. First, within each feature subspace, while
bigram features improved the performance signifi-
cantly over unigrams, trigrams did not improve the
performance very much. This trend is observed for
both classifiers. In the case of the syntactic parse tree
subspace, adding production features even hurt the
performance. This suggests that inclusion of com-
plex features is not guaranteed to improve the per-
formance.
Second, if we compare the best performance
achieved in each feature subspace, we can see that
for both classifiers, syntactic parse tree is the most
effective feature space, while sequence and depen-
dency tree are similar. However, the difference in
performance between the syntactic parse tree sub-
space and the other two subspaces is not very large.
This suggests that each feature subspace alone al-
ready captures most of the useful structural informa-
tion between tokens for relation extraction. The rea-
son why the sequence feature subspace gave good
performance although it contained the least struc-
tural information is probably that many relations de-
fined in the ACE corpus are short-range relations,
some within single noun phrases. For such kind of
relations, sequence information may be even more
reliable than syntactic or dependency information,
which may not be accurate due to parsing errors.
Next, we conducted experiments to combine the
features from the three subspaces to see whether
this could further improve the performance. For se-
quence subspace and dependency tree subspace, we
used up to bigram features, and for syntactic parse
tree subspace, we used up to trigram features. In Ta-
ble 2, we show the experiment results. We can see
that for both classifiers, adding features from the se-
quence subspace or from the dependency tree sub-
space to the syntactic parse tree subspace can im-
prove the performance slightly. But combining se-
quence subspace and dependency tree subspace does
not generate any performance improvement. Again,
118
Uni +Bi +Tri +Prod
P 0.647 0.662 0.717
Seq R 0.614 0.701 0.653 N/A
F 0.630 0.681 0.683
P 0.651 0.695 0.726 0.702
ME Syn R 0.645 0.698 0.688 0.691
F 0.648 0.697 0.707 0.696
P 0.647 0.673 0.718
Dep R 0.614 0.676 0.652 N/A
F 0.630 0.674 0.683
P 0.583 0.666 0.684
Seq R 0.586 0.650 0.648 N/A
F 0.585 0.658 0.665
P 0.598 0.645 0.679 0.674
SVM Syn R 0.611 0.663 0.681 0.672
F 0.604 0.654 0.680 0.673
P 0.583 0.644 0.682
Dep R 0.586 0.638 0.645 N/A
F 0.585 0.641 0.663
Table 1: Comparison among the three feature sub-
spaces and the effect of including larger features.
Seq+Syn Seq+Dep Syn+Dep All
P 0.737 0.687 0.695 0.724
ME R 0.694 0.682 0.731 0.702
F 0.715 0.684 0.712 0.713
P 0.689 0.669 0.687 0.691
SVM R 0.686 0.653 0.682 0.686
F 0.688 0.661 0.684 0.688
Table 2: The effect of combining the three feature
subspaces.
this suggests that since many of the ACE relations
are local, there is likely much overlap between se-
quence information and dependency information.
We also tried the convolution tree kernel
method (Zhang et al, 2006a), using an SVM tree
kernel package2. The performance we obtained was
P = 0.705, R = 0.685, and F = 0.6953. This F mea-
sure is higher than the best SVM performance in Ta-
ble 1. The convolution tree kernel uses large subtree
features, but such features are deemphasized with
an exponentially decaying weight. We found that
the performance was sensitive to this decaying fac-
tor, suggesting that complex features can be useful
if they are weighted appropriately, and further study
of how to optimize the weights of such complex fea-
tures is needed.
2http://ai-nlp.info.uniroma2.it/moschitti/Tree-Kernel.htm
3The performance we achieved is lower than that reported
in (Zhang et al, 2006b), due to different data preprocessing,
data partition, and parameter setting.
5.3 Task-Oriented Feature Pruning
Apart from the general bottom-up search strategy we
have proposed, we can also introduce some task-
oriented heuristics based on intuition or domain
knowledge to prune the feature space. In our ex-
periments, we tried the following heuristics.
H1: Zhang et al (2006a) found that using path-
enclosed tree performed better than using minimum
complete tree, when convolution tree kernels were
applied. In path-enclosed trees, tokens before arg1
and after arg2 as well as their links with other nodes
in the tree are removed. Based on this previous
finding, our first heuristic was to change the syntac-
tic parse tree representation of the relation instances
into path-enclosed trees.
H2: We hypothesize that words such as articles,
adjectives and adverbs are not very useful for rela-
tion extraction. We thus removed sequence unigram
features and bigram features that contain an article,
adjective or adverb.
H3: Similar to H2, we can remove bigrams in the
syntactic parse tree subspace if the bigram contains
an article, adjective or adverb.
H4: Similar to H1, we can also remove the to-
kens before arg1 and after arg2 from the sequence
representation of a relation instance.
In Table 3, we show the performance after apply-
ing these heuristics. We started with the best con-
figuration from our previous experiments, that is,
combing up to bigram features in the sequence sub-
space and up to trigram features in the syntactic tree
subspace. We then applied heuristics H1 to H4 in-
crementally unless we saw that a heuristic was not
effective. We found that H1, H2 and H4 slightly
improved the performance, but H3 hurt the perfor-
mance. On the one hand, the improvement suggests
that our original feature configuration included some
irrelevant features, and in turn confirmed that over-
inclusion of features could hurt the performance. On
the other hand, since the improvement brought by
H1, H2 and H4 was rather small, and H3 even hurt
the performance, we could see that it is in general
very hard to find good feature pruning heuristics.
6 Conclusions and Future Work
In this paper, we conducted a systematic study of
the feature space for relation extraction. We pro-
119
ME SVM
P R F P R F
Best 0.737 0.694 0.715 0.689 0.686 0.688
+H1 0.714 0.729 0.721 0.698 0.699 0.699
+H2 0.730 0.723 0.726 0.704 0.704 0.704
+H3 0.739 0.704 0.721 0.701 0.696 0.698
-H3+H4 0.746 0.713 0.729 0.702 0.701 0.702
Table 3: The effect of various heuristic feature prun-
ing methods.
posed and defined a unified graphic representation
of features for relation extraction, which serves as a
general framework for systematically exploring fea-
tures defined on natural language sentences. With
this framework, we explored three different repre-
sentations of sentences?sequences, syntactic parse
trees, and dependency trees?which lead to three
feature subspaces. In each subspace, starting with
the basic unit features, we systematically explored
features of different levels of complexity. The stud-
ied feature space includes not only most of the ef-
fective features explored in previous work, but also
some features that have not been considered before.
Our experiment results showed that using a set of
basic unit features from each feature subspace, we
can achieve reasonably good performance. When
the three subspaces are combined, the performance
can improve only slightly, which suggests that the
sequence, syntactic and dependency relations have
much overlap for the task of relation extraction. We
also found that adding more complex features may
not improve the performance much, and may even
hurt the performance. A combination of features
of different levels of complexity and from different
sentence representations, coupled with task-oriented
feature pruning, gives the best performance.
In our future work, we will study how to auto-
matically conduct task-oriented feature search, fea-
ture pruning and feature weighting using statistical
methods instead of heuristics. In this study, we only
considered features from the local context, i.e. the
sentence that contains the two arguments. Some ex-
isting studies use corpus-based statistics for relation
extraction (Hasegawa et al, 2004). In the future, we
will study the effectiveness of these global features.
Acknowledgments
This work was in part supported by the National Sci-
ence Foundation under award numbers 0425852 and
0428472. We thank Alessandro Moschitti for pro-
viding the SVM tree kernel package. We also thank
Min Zhang for providing the implementation details
of the convolution tree kernel for relation extraction.
References
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kenrel for relation extrac-
tion. In Proceedings of HLT/EMNLP.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of ICML.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku
Maeda. 2003. Hierarchical directed acyclic graph ker-
nel: Methods for structured natural language data. In
Proceedings of ACL.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083?1106.
Min Zhang, Jie Zhang, and Jian Su. 2006a. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In Proceedings of HLT/NAACL.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006b. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of ACL.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In Proceedings of ACL.
120
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264?271,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Instance Weighting for Domain Adaptation in NLP
Jing Jiang and ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{jiang4,czhai}@cs.uiuc.edu
Abstract
Domain adaptation is an important problem
in natural language processing (NLP) due to
the lack of labeled data in novel domains. In
this paper, we study the domain adaptation
problem from the instance weighting per-
spective. We formally analyze and charac-
terize the domain adaptation problem from
a distributional view, and show that there
are two distinct needs for adaptation, cor-
responding to the different distributions of
instances and classification functions in the
source and the target domains. We then
propose a general instance weighting frame-
work for domain adaptation. Our empir-
ical results on three NLP tasks show that
incorporating and exploiting more informa-
tion from the target domain through instance
weighting is effective.
1 Introduction
Many natural language processing (NLP) problems
such as part-of-speech (POS) tagging, named entity
(NE) recognition, relation extraction, and seman-
tic role labeling, are currently solved by supervised
learning from manually labeled data. A bottleneck
problem with this supervised learning approach is
the lack of annotated data. As a special case, we
often face the situation where we have a sufficient
amount of labeled data in one domain, but have little
or no labeled data in another related domain which
we are interested in. We thus face the domain adap-
tation problem. Following (Blitzer et al, 2006), we
call the first the source domain, and the second the
target domain.
The domain adaptation problem is commonly en-
countered in NLP. For example, in POS tagging, the
source domain may be tagged WSJ articles, and the
target domain may be scientific literature that con-
tains scientific terminology. In NE recognition, the
source domain may be annotated news articles, and
the target domain may be personal blogs. Another
example is personalized spam filtering, where we
may have many labeled spam and ham emails from
publicly available sources, but we need to adapt the
learned spam filter to an individual user?s inbox be-
cause the user has her own, and presumably very dif-
ferent, distribution of emails and notion of spams.
Despite the importance of domain adaptation in
NLP, currently there are no standard methods for
solving this problem. An immediate possible solu-
tion is semi-supervised learning, where we simply
treat the target instances as unlabeled data but do
not distinguish the two domains. However, given
that the source data and the target data are from dif-
ferent distributions, we should expect to do better
by exploiting the domain difference. Recently there
have been some studies addressing domain adapta-
tion from different perspectives (Roark and Bacchi-
ani, 2003; Chelba and Acero, 2004; Florian et al,
2004; Daume? III and Marcu, 2006; Blitzer et al,
2006). However, there have not been many studies
that focus on the difference between the instance dis-
tributions in the two domains. A detailed discussion
on related work is given in Section 5.
In this paper, we study the domain adaptation
problem from the instance weighting perspective.
264
In general, the domain adaptation problem arises
when the source instances and the target instances
are from two different, but related distributions.
We formally analyze and characterize the domain
adaptation problem from this distributional view.
Such an analysis reveals that there are two distinct
needs for adaptation, corresponding to the differ-
ent distributions of instances and the different clas-
sification functions in the source and the target do-
mains. Based on this analysis, we propose a gen-
eral instance weighting method for domain adapta-
tion, which can be regarded as a generalization of
an existing approach to semi-supervised learning.
The proposed method implements several adapta-
tion heuristics with a unified objective function: (1)
removing misleading training instances in the source
domain; (2) assigning more weights to labeled tar-
get instances than labeled source instances; (3) aug-
menting training instances with target instances with
predicted labels. We evaluated the proposed method
with three adaptation problems in NLP, including
POS tagging, NE type classification, and spam filter-
ing. The results show that regular semi-supervised
and supervised learning methods do not perform as
well as our new method, which explicitly captures
domain difference. Our results also show that in-
corporating and exploiting more information from
the target domain is much more useful for improv-
ing performance than excluding misleading training
examples from the source domain.
The rest of the paper is organized as follows. In
Section 2, we formally analyze the domain adapta-
tion problem and distinguish two types of adapta-
tion. In Section 3, we then propose a general in-
stance weighting framework for domain adaptation.
In Section 4, we present the experiment results. Fi-
nally, we compare our framework with related work
in Section 5 before we conclude in Section 6.
2 Domain Adaptation
In this section, we define and analyze domain adap-
tation from a theoretical point of view. We show that
the need for domain adaptation arises from two fac-
tors, and the solutions are different for each factor.
We restrict our attention to those NLP tasks that can
be cast into multiclass classification problems, and
we only consider discriminative models for classifi-
cation. Since both are common practice in NLP, our
analysis is applicable to many NLP tasks.
Let X be a feature space we choose to represent
the observed instances, and let Y be the set of class
labels. In the standard supervised learning setting,
we are given a set of labeled instances {(xi, yi)}Ni=1,
where xi ? X , yi ? Y , and (xi, yi) are drawn from
an unknown joint distribution p(x, y). Our goal is to
recover this unknown distribution so that we can pre-
dict unlabeled instances drawn from the same distri-
bution. In discriminative models, we are only con-
cerned with p(y|x). Following the maximum likeli-
hood estimation framework, we start with a parame-
terized model family p(y|x; ?), and then find the best
model parameter ?? that maximizes the expected log
likelihood of the data:
?? = argmax
?
?
X
?
y?Y
p(x, y) log p(y|x; ?)dx.
Since we do not know the distribution p(x, y), we
maximize the empirical log likelihood instead:
?? ? argmax
?
?
X
?
y?Y
p?(x, y) log p(y|x; ?)dx
= argmax
?
1
N
N?
i=1
log p(yi|xi; ?).
Note that since we use the empirical distribution
p?(x, y) to approximate p(x, y), the estimated ?? is
dependent on p?(x, y). In general, as long as we have
sufficient labeled data, this approximation is fine be-
cause the unlabeled instances we want to classify are
from the same p(x, y).
2.1 Two Factors for Domain Adaptation
Let us now turn to the case of domain adaptation
where the unlabeled instances we want to classify
are from a different distribution than the labeled in-
stances. Let ps(x, y) and pt(x, y) be the true un-
derlying distributions for the source and the target
domains, respectively. Our general idea is to use
ps(x, y) to approximate pt(x, y) so that we can ex-
ploit the labeled examples in the source domain.
If we factor p(x, y) into p(x, y) = p(y|x)p(x),
we can see that pt(x, y) can deviate from ps(x, y) in
two different ways, corresponding to two different
kinds of domain adaptation:
265
Case 1 (Labeling Adaptation): pt(y|x) deviates
from ps(y|x) to a certain extent. In this case, it is
clear that our estimation of ps(y|x) from the labeled
source domain instances will not be a good estima-
tion of pt(y|x), and therefore domain adaptation is
needed. We refer to this kind of adaptation as func-
tion/labeling adaptation.
Case 2 (Instance Adaptation): pt(y|x) is mostly
similar to ps(y|x), but pt(x) deviates from ps(x). In
this case, it may appear that our estimated ps(y|x)
can still be used in the target domain. However, as
we have pointed out, the estimation of ps(y|x) de-
pends on the empirical distribution p?s(x, y), which
deviates from pt(x, y) due to the deviation of ps(x)
from pt(x). In general, the estimation of ps(y|x)
would be more influenced by the instances with high
p?s(x, y) (i.e., high p?s(x)). If pt(x) is very differ-
ent from ps(x), then we should expect pt(x, y) to be
very different from ps(x, y), and therefore different
from p?s(x, y). We thus cannot expect the estimated
ps(y|x) to work well on the regions where pt(x, y)
is high, but ps(x, y) is low. Therefore, in this case,
we still need domain adaptation, which we refer to
as instance adaptation.
Because the need for domain adaptation arises
from two different factors, we need different solu-
tions for each factor.
2.2 Solutions for Labeling Adaptation
If pt(y|x) deviates from ps(y|x) to some extent, we
have one of the following choices:
Change of representation:
It may be the case that if we change the rep-
resentation of the instances, i.e., if we choose a
feature space X ? different from X , we can bridge
the gap between the two distributions ps(y|x) and
pt(y|x). For example, consider domain adaptive
NE recognition where the source domain contains
clean newswire data, while the target domain con-
tains broadcast news data that has been transcribed
by automatic speech recognition and lacks capital-
ization. Suppose we use a naive NE tagger that
only looks at the word itself. If we consider capi-
talization, then the instance Bush is represented dif-
ferently from the instance bush. In the source do-
main, ps(y = Person|x = Bush) is high while
ps(y = Person|x = bush) is low, but in the target
domain, pt(y = Person|x = bush) is high. If we
ignore the capitalization information, then in both
domains p(y = Person|x = bush) will be high pro-
vided that the source domain contains much fewer
instances of bush than Bush.
Adaptation through prior:
When we use a parameterized model p(y|x; ?)
to approximate p(y|x) and estimate ? based on the
source domain data, we can place some prior on the
model parameter ? so that the estimated distribution
p(y|x; ??) will be closer to pt(y|x). Consider again
the NE tagging example. If we use capitalization as
a feature, in the source domain where capitalization
information is available, this feature will be given a
large weight in the learned model because it is very
useful. If we place a prior on the weight for this fea-
ture so that a large weight will be penalized, then
we can prevent the learned model from relying too
much on this domain specific feature.
Instance pruning:
If we know the instances x for which pt(y|x) is
different from ps(y|x), we can actively remove these
instances from the training data because they are
?misleading?.
For all the three solutions given above, we need
either some prior knowledge about the target do-
main, or some labeled target domain instances;
from only the unlabeled target domain instances, we
would not know where and why pt(y|x) differs from
ps(y|x).
2.3 Solutions for Instance Adaptation
In the case where pt(y|x) is similar to ps(y|x), but
pt(x) deviates from ps(x), we may use the (unla-
beled) target domain instances to bias the estimate
of ps(x) toward a better approximation of pt(x), and
thus achieve domain adaptation. We explain the idea
below.
Our goal is to obtain a good estimate of ??t that is
optimized according to the target domain distribu-
tion pt(x, y). The exact objective function is thus
??t = argmax
?
?
X
?
y?Y
pt(x, y) log p(y|x; ?)dx
= argmax
?
?
X
pt(x)
?
y?Y
pt(y|x) log p(y|x; ?)dx.
266
Our idea of domain adaptation is to exploit the la-
beled instances in the source domain to help obtain
??t .
Let Ds = {(xsi , ysi )}Nsi=1 denote the set of la-
beled instances we have from the source domain.
Assume that we have a (small) set of labeled and
a (large) set of unlabeled instances from the tar-
get domain, denoted by Dt,l = {(xt,lj , yt,lj )}
Nt,l
j=1 and
Dt,u = {xt,uk }
Nt,u
k=1 , respectively. We now show three
ways to approximate the objective function above,
corresponding to using three different sets of in-
stances to approximate the instance space X .
Using Ds:
Using ps(y|x) to approximate pt(y|x), we obtain
??t ? argmax
?
?
X
pt(x)
ps(x)ps(x)
?
y?Y
ps(y|x) log p(y|x; ?)dx
? argmax
?
?
X
pt(x)
ps(x) p?s(x)
?
y?Y
p?s(y|x) log p(y|x; ?)dx
= argmax
?
1
Ns
Ns?
i=1
pt(xsi )
ps(xsi )
log p(ysi |xsi ; ?).
Here we use only the labeled instances in Ds but
we adjust the weight of each instance by pt(x)ps(x) . The
major difficulty is how to accurately estimate pt(x)ps(x) .
Using Dt,l:
??t ? argmax
?
?
X
p?t,l(x)
?
y?Y
p?t,l(y|x) log p(y|x; ?)dx
= argmax
?
1
Nt,l
Nt,l?
j=1
log p(yt,lj |xt,lj ; ?)
Note that this is the standard supervised learning
method using only the small amount of labeled tar-
get instances. The major weakness of this approxi-
mation is that when Nt,l is very small, the estimation
is not accurate.
Using Dt,u:
??t ? argmax
?
?
X
p?t,u(x)
?
y?Y
pt(y|x) log p(y|x; ?)dx
= argmax
?
1
Nt,u
Nt,u?
k=1
?
y?Y
pt(y|xt,uk ) log p(y|xt,uk ; ?),
The challenge here is that pt(y|xt,uk ; ?) is unknown
to us, thus we need to estimate it. One possibility
is to approximate it with a model ?? learned from
Ds and Dt,l. For example, we can set pt(y|x, ?) =
p(y|x; ??). Alternatively, we can also set pt(y|x, ?)
to 1 if y = argmaxy? p(y?|x; ??) and 0 otherwise.
3 A Framework of Instance Weighting for
Domain Adaptation
The theoretical analysis we give in Section 2 sug-
gests that one way to solve the domain adaptation
problem is through instance weighting. We propose
a framework that incorporates instance pruning in
Section 2.2 and the three approximations in Sec-
tion 2.3. Before we show the formal framework, we
first introduce some weighting parameters and ex-
plain the intuitions behind these parameters.
First, for each (xsi , ysi ) ? Ds, we introduce a pa-
rameter ?i to indicate how likely pt(ysi |xsi ) is close
to ps(ysi |xsi ). Large ?i means the two probabilities
are close, and therefore we can trust the labeled in-
stance (xsi , ysi ) for the purpose of learning a clas-
sifier for the target domain. Small ?i means these
two probabilities are very different, and therefore we
should probably discard the instance (xsi , ysi ) in the
learning process.
Second, again for each (xsi , ysi ) ? Ds, we intro-
duce another parameter ?i that ideally is equal to
pt(xsi )
ps(xsi ) . From the approximation in Section 2.3 thatuses only Ds, it is clear that such a parameter is use-
ful.
Next, for each xt,ui ? Dt,u, and for each possible
label y ? Y , we introduce a parameter ?i(y) that
indicates how likely we would like to assign y as a
tentative label to xt,ui and include (xt,ui , y) as a train-
ing example.
Finally, we introduce three global parameters ?s,
?t,l and ?t,u that are not instance-specific but are as-
sociated with Ds, Dt,l and Dt,u, respectively. These
three parameters allow us to control the contribution
of each of the three approximation methods in Sec-
tion 2.3 when we linearly combine them together.
We now formally define our instance weighting
framework. Given Ds, Dt,l and Dt,u, to learn a clas-
sifier for the target domain, we find a parameter ??
that optimizes the following objective function:
267
?? = argmax
?
[
?s ? 1Cs
Ns?
i=1
?i?i log p(ysi |xsi ; ?)
+?t,l ? 1Ct,l
Nt,l?
j=1
log p(yt,lj |xt,lj ; ?)
+?t,u ? 1Ct,u
Nt,u?
k=1
?
y?Y
?k(y) log p(y|xt,uk ; ?)
+ log p(?)
]
,
where Cs =
?Ns
i=1 ?i?i, Ct,l = Nt,l, Ct,u =?Nt,u
k=1
?
y?Y ?k(y), and ?s + ?t,l + ?t,u = 1. The
last term, log p(?), is the log of a Gaussian prior dis-
tribution of ?, commonly used to regularize the com-
plexity of the model.
In general, we do not know the optimal values of
these parameters for the target domain. Neverthe-
less, the intuitions behind these parameters serve as
guidelines for us to design heuristics to set these pa-
rameters. In the rest of this section, we introduce
several heuristics that we used in our experiments to
set these parameters.
3.1 Setting ?
Following the intuition that if pt(y|x) differs much
from ps(y|x), then (x, y) should be discarded from
the training set, we use the following heuristic to
set ?s. First, with standard supervised learning, we
train a model ??t,l from Dt,l. We consider p(y|x; ??t,l)
to be a crude approximation of pt(y|x). Then, we
classify {xsi}Nsi=1 using ??t,l. The top k instances
that are incorrectly predicted by ??t,l (ranked by their
prediction confidence) are discarded. In another
word, ?si of the top k instances for which ysi 6=
argmaxy p(y|xsi ; ??t,l) are set to 0, and ?i of all the
other source instances are set to 1.
3.2 Setting ?
Accurately setting ? involves accurately estimating
ps(x) and pt(x) from the empirical distributions.
For many NLP classification tasks, we do not have a
good parametric model for p(x). We thus need to re-
sort to non-parametric density estimation methods.
However, for many NLP tasks, x resides in a high
dimensional space, which makes it hard to apply
standard non-parametric density estimation meth-
ods. We have not explored this direction, and in our
experiments, we set ? to 1 for all source instances.
3.3 Setting ?
Setting ? is closely related to some semi-supervised
learning methods. One option is to set ?k(y) =
p(y|xt,uk ; ?). In this case, ? is no longer a constant
but is a function of ?. This way of setting ? corre-
sponds to the entropy minimization semi-supervised
learning method (Grandvalet and Bengio, 2005).
Another way to set ? corresponds to bootstrapping
semi-supervised learning. First, let ??(n) be a model
learned from the previous round of training. We then
select the top k instances from Dt,u that have the
highest prediction confidence. For these instances,
we set ?k(y) = 1 for y = argmaxy? p(y?|xt,uk ; ??(n)),
and ?k(y) = 0 for all other y. In another word, we
select the top k confidently predicted instances, and
include these instances together with their predicted
labels in the training set. All other instances in Dt,u
are not considered. In our experiments, we only con-
sidered this bootstrapping way of setting ?.
3.4 Setting ?
?s, ?t,l and ?t,u control the balance among the three
sets of instances. Using standard supervised learn-
ing, ?s and ?t,l are set proportionally to Cs and Ct,l,
that is, each instance is weighted the same whether
it is in Ds or in Dt,l, and ?t,u is set to 0. Similarly,
using standard bootstrapping, ?t,u is set proportion-
ally to Ct,u, that is, each target instance added to the
training set is also weighted the same as a source
instance. In neither case are the target instances em-
phasize more than source instances. However, for
domain adaptation, we want to focus more on the
target domain instances. So intuitively, we want to
make ?t,l and ?t,u somehow larger relative to ?s. As
we will show in Section 4, this is indeed beneficial.
In general, the framework provides great flexibil-
ity for implementing different adaptation strategies
through these instance weighting parameters.
4 Experiments
4.1 Tasks and Data Sets
We chose three different NLP tasks to evaluate our
instance weighting method for domain adaptation.
The first task is POS tagging, for which we used
268
6166 WSJ sentences from Sections 00 and 01 of
Penn Treebank as the source domain data, and 2730
PubMed sentences from the Oncology section of the
PennBioIE corpus as the target domain data. The
second task is entity type classification. The setup is
very similar to Daume? III and Marcu (2006). We
assume that the entity boundaries have been cor-
rectly identified, and we want to classify the types
of the entities. We used ACE 2005 training data
for this task. For the source domain, we used the
newswire collection, which contains 11256 exam-
ples, and for the target domains, we used the we-
blog (WL) collection (5164 examples) and the con-
versational telephone speech (CTS) collection (4868
examples). The third task is personalized spam fil-
tering. We used the ECML/PKDD 2006 discov-
ery challenge data set. The source domain contains
4000 spam and ham emails from publicly available
sources, and the target domains are three individual
users? inboxes, each containing 2500 emails.
For each task, we consider two experiment set-
tings. In the first setting, we assume there are a small
number of labeled target instances available. For
POS tagging, we used an additional 300 Oncology
sentences as labeled target instances. For NE typ-
ing, we used 500 labeled target instances and 2000
unlabeled target instances for each target domain.
For spam filtering, we used 200 labeled target in-
stances and 1800 unlabeled target instances. In the
second setting, we assume there is no labeled target
instance. We thus used all available target instances
for testing in all three tasks.
We used logistic regression as our model of
p(y|x; ?) because it is a robust learning algorithm
and widely used.
We now describe three sets of experiments, cor-
responding to three heuristic ways of setting ?, ?t,l
and ?t,u.
4.2 Removing ?Misleading? Source Domain
Instances
In the first set of experiments, we gradually remove
?misleading? labeled instances from the source do-
main, using the small number of labeled target in-
stances we have. We follow the heuristic we de-
scribed in Section 3.1, which sets the ? for the top
k misclassified source instances to 0, and the ? for
all the other source instances to 1. We also set ?t,l
and ?t,l to 0 in order to focus only on the effect of
removing ?misleading? instances. We compare with
a baseline method which uses all source instances
with equal weight but no target instances. The re-
sults are shown in Table 1.
From the table, we can see that in most exper-
iments, removing these predicted ?misleading? ex-
amples improved the performance over the baseline.
In some experiments (Oncology, CTS, u00, u01), the
largest improvement was achieved when all misclas-
sified source instances were removed. In the case of
weblog NE type classification, however, removing
the source instances hurt the performance. A pos-
sible reason for this is that the set of labeled target
instances we use is a biased sample from the target
domain, and therefore the model trained on these in-
stances is not always a good predictor of ?mislead-
ing? source instances.
4.3 Adding Labeled Target Domain Instances
with Higher Weights
The second set of experiments is to add the labeled
target domain instances into the training set. This
corresponds to setting ?t,l to some non-zero value,
but still keeping ?t,u as 0. If we ignore the do-
main difference, then each labeled target instance
is weighted the same as a labeled source instance
(?u,l?s =
Cu,l
Cs ), which is what happens in regular su-pervised learning. However, based on our theoret-
ical analysis, we can expect the labeled target in-
stances to be more representative of the target do-
main than the source instances. We can therefore
assign higher weights for the target instances, by ad-
justing the ratio between ?t,l and ?s. In our experi-
ments, we set ?t,l?s = a
Ct,l
Cs , where a ranges from 2 to20. The results are shown in Table 2.
As shown from the table, adding some labeled tar-
get instances can greatly improve the performance
for all tasks. And in almost all cases, weighting the
target instances more than the source instances per-
formed better than weighting them equally.
We also tested another setting where we first
removed the ?misleading? source examples as we
showed in Section 4.2, and then added the labeled
target instances. The results are shown in the last
row of Table 2. However, although both removing
?misleading? source instances and adding labeled
269
POS NE Type Spam
k Oncology k CTS k WL k u00 u01 u02
0 0.8630 0 0.7815 0 0.7045 0 0.6306 0.6950 0.7644
4000 0.8675 800 0.8245 600 0.7070 150 0.6417 0.7078 0.7950
8000 0.8709 1600 0.8640 1200 0.6975 300 0.6611 0.7228 0.8222
12000 0.8713 2400 0.8825 1800 0.6830 450 0.7106 0.7806 0.8239
16000 0.8714 3000 0.8825 2400 0.6795 600 0.7911 0.8322 0.8328
all 0.8720 all 0.8830 all 0.6600 all 0.8106 0.8517 0.8067
Table 1: Accuracy on the target domain after removing ?misleading? source domain instances.
POS NE Type Spam
method Oncology method CTS WL method u00 u01 u02
Ds only 0.8630 Ds only 0.7815 0.7045 Ds only 0.6306 0.6950 0.7644
Ds + Dt,l 0.9349 Ds + Dt,l 0.9340 0.7735 Ds + Dt,l 0.9572 0.9572 0.9461
Ds + 5Dt,l 0.9411 Ds + 2Dt,l 0.9355 0.7810 Ds + 2Dt,l 0.9606 0.9600 0.9533
Ds + 10Dt,l 0.9429 Ds + 5Dt,l 0.9360 0.7820 Ds + 5Dt,l 0.9628 09611 0.9601
Ds + 20Dt,l 0.9443 Ds + 10Dt,l 0.9355 0.7840 Ds + 10Dt,l 0.9639 0.9628 0.9633
D?s + 20Dt,l 0.9422 D?s + 10Dt,l 0.8950 0.6670 D?s + 10Dt,l 0.9717 0.9478 0.9494
Table 2: Accuracy on the unlabeled target instances after adding the labeled target instances.
target instances work well individually, when com-
bined, the performance in most cases is not as good
as when no source instances are removed. We hy-
pothesize that this is because after we added some
labeled target instances with large weights, we al-
ready gained a good balance between the source data
and the target data. Further removing source in-
stances would push the emphasis more on the set
of labeled target instances, which is only a biased
sample of the whole target domain.
The POS data set and the CTS data set have pre-
viously been used for testing other adaptation meth-
ods (Daume? III and Marcu, 2006; Blitzer et al,
2006), though the setup there is different from ours.
Our performance using instance weighting is com-
parable to their best performance (slightly worse for
POS and better for CTS).
4.4 Bootstrapping with Higher Weights
In the third set of experiments, we assume that we
do not have any labeled target instances. We tried
two bootstrapping methods. The first is a standard
bootstrapping method, in which we gradually added
the most confidently predicted unlabeled target in-
stances with their predicted labels to the training
set. Since we believe that the target instances should
in general be given more weight because they bet-
ter represent the target domain than the source in-
stances, in the second method, we gave the added
target instances more weight in the objective func-
tion. In particular, we set ?t,u = ?s such that the
total contribution of the added target instances is
equal to that of all the labeled source instances. We
call this second method the balanced bootstrapping
method. Table 3 shows the results.
As we can see, while bootstrapping can generally
improve the performance over the baseline where
no unlabeled data is used, the balanced bootstrap-
ping method performed slightly better than the stan-
dard bootstrapping method. This again shows that
weighting the target instances more is a right direc-
tion to go for domain adaptation.
5 Related Work
There have been several studies in NLP that address
domain adaptation, and most of them need labeled
data from both the source domain and the target do-
main. Here we highlight a few representative ones.
For generative syntactic parsing, Roark and Bac-
chiani (2003) have used the source domain data
to construct a Dirichlet prior for MAP estimation
of the PCFG for the target domain. Chelba and
Acero (2004) use the parameters of the maximum
entropy model learned from the source domain as
the means of a Gaussian prior when training a new
model on the target data. Florian et al (2004) first
train a NE tagger on the source domain, and then use
the tagger?s predictions as features for training and
testing on the target domain.
The only work we are aware of that directly mod-
270
POS NE Type Spam
method Oncology CTS WL u00 u01 u02
supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068
standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760
balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.9772
Table 3: Accuracy on the target domain without using labeled target instances. In balanced bootstrapping,
more weights are put on the target instances in the objective function than in standard bootstrapping.
els the different distributions in the source and the
target domains is by Daume? III and Marcu (2006).
They assume a ?truly source domain? distribution,
a ?truly target domain? distribution, and a ?general
domain? distribution. The source (target) domain
data is generated from a mixture of the ?truly source
(target) domain? distribution and the ?general do-
main? distribution. In contrast, we do not assume
such a mixture model.
None of the above methods would work if there
were no labeled target instances. Indeed, all the
above methods do not make use of the unlabeled
instances in the target domain. In contrast, our in-
stance weighting framework allows unlabeled target
instances to contribute to the model estimation.
Blitzer et al (2006) propose a domain adaptation
method that uses the unlabeled target instances to
infer a good feature representation, which can be re-
garded as weighting the features. In contrast, we
weight the instances. The idea of using pt(x)ps(x) toweight instances has been studied in statistics (Shi-
modaira, 2000), but has not been applied to NLP
tasks.
6 Conclusions and Future Work
Domain adaptation is a very important problem with
applications to many NLP tasks. In this paper,
we formally analyze the domain adaptation problem
and propose a general instance weighting framework
for domain adaptation. The framework is flexible to
support many different strategies for adaptation. In
particular, it can support adaptation with some target
domain labeled instances as well as that without any
labeled target instances. Experiment results on three
NLP tasks show that while regular semi-supervised
learning methods and supervised learning methods
can be applied to domain adaptation without con-
sidering domain difference, they do not perform as
well as our new method, which explicitly captures
domain difference. Our results also show that incor-
porating and exploiting more information from the
target domain is much more useful than excluding
misleading training examples from the source do-
main. The framework opens up many interesting
future research directions, especially those related to
how to more accurately set/estimate those weighting
parameters.
Acknowledgments
This work was in part supported by the National Sci-
ence Foundation under award numbers 0425852 and
0428472. We thank the anonymous reviewers for
their valuable comments.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120?128.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proc. of EMNLP, pages 285?292.
Hal Daume? III and Daniel Marcu. 2006. Domain adapta-
tion for statistical classifiers. J. Artificial Intelligence
Res., 26:101?126.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL, pages 1?8.
Y. Grandvalet and Y. Bengio. 2005. Semi-supervised
learning by entropy minimization. In NIPS.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptatin to novel domains.
In Proc. of HLT-NAACL, pages 126?133.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
271
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1012?1020,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction
Jing Jiang
School of Information Systems
Singapore Management University
80 Stamford Road, Singapore 178902
jingjiang@smu.edu.sg
Abstract
Creating labeled training data for rela-
tion extraction is expensive. In this pa-
per, we study relation extraction in a spe-
cial weakly-supervised setting when we
have only a few seed instances of the tar-
get relation type we want to extract but
we also have a large amount of labeled
instances of other relation types. Ob-
serving that different relation types can
share certain common structures, we pro-
pose to use a multi-task learning method
coupled with human guidance to address
this weakly-supervised relation extraction
problem. The proposed framework mod-
els the commonality among different re-
lation types through a shared weight vec-
tor, enables knowledge learned from the
auxiliary relation types to be transferred
to the target relation type, and allows easy
control of the tradeoff between precision
and recall. Empirical evaluation on the
ACE 2004 data set shows that the pro-
posed method substantially improves over
two baseline methods.
1 Introduction
Relation extraction is the task of detecting and
characterizing semantic relations between entities
from free text. Recent work on relation extraction
has shown that supervised machine learning cou-
pled with intelligent feature engineering or ker-
nel design provides state-of-the-art solutions to the
problem (Culotta and Sorensen, 2004; Zhou et al,
2005; Bunescu and Mooney, 2005; Qian et al,
2008). However, supervised learning heavily re-
lies on a sufficient amount of labeled data for train-
ing, which is not always available in practice due
to the labor-intensive nature of human annotation.
This problem is especially serious for relation ex-
traction because the types of relations to be ex-
tracted are highly dependent on the application do-
main. For example, when working in the financial
domain we may be interested in the employment
relation, but when moving to the terrorism domain
we now may be interested in the ethnic and ide-
ology affiliation relation, and thus have to create
training data for the new relation type.
However, is the old training data really useless?
Inspired by recent work on transfer learning and
domain adaptation, in this paper, we study how
we can leverage labeled data of some old relation
types to help the extraction of a new relation type
in a weakly-supervised setting, where only a few
seed instances of the new relation type are avail-
able. While transfer learning was proposed more
than a decade ago (Thrun, 1996; Caruana, 1997),
its application in natural language processing is
still a relatively new territory (Blitzer et al, 2006;
Daume III, 2007; Jiang and Zhai, 2007a; Arnold et
al., 2008; Dredze and Crammer, 2008), and its ap-
plication in relation extraction is still unexplored.
Our idea of performing transfer learning is mo-
tivated by the observation that different relation
types share certain common syntactic structures,
which can possibly be transferred from the old
types to the new type. We therefore propose to use
a general multi-task learning framework in which
classification models for a number of related tasks
are forced to share a common model component
and trained together. By treating classification
of different relation types as related tasks, the
learning framework can naturally model the com-
mon syntactic structures among different relation
types in a principled manner. It also allows us
to introduce human guidance in separating the
common model component from the type-specific
components. The framework naturally transfers
the knowledge learned from the old relation types
to the new relation type and helps improve the re-
call of the relation extractor. We also exploit ad-
1012
ditional human knowledge about the entity type
constraints on the relation arguments, which can
usually be derived from the definition of a relation
type. Imposing these constraints further improves
the precision of the final relation extractor. Em-
pirical evaluation on the ACE 2004 data set shows
that our proposed method largely outperforms two
baseline methods, improving the average F1 mea-
sure from 0.1532 to 0.4132 when only 10 seed in-
stances of the new relation type are used.
2 Related work
Recent work on relation extraction has been dom-
inated by feature-based and kernel-based super-
vised learning methods. Zhou et al (2005) and
Zhao and Grishman (2005) studied various fea-
tures and feature combinations for relation extrac-
tion. We systematically explored the feature space
for relation extraction (Jiang and Zhai, 2007b) .
Kernel methods allow a large set of features to be
used without being explicitly extracted. A num-
ber of relation extraction kernels have been pro-
posed, including dependency tree kernels (Culotta
and Sorensen, 2004), shortest dependency path
kernels (Bunescu and Mooney, 2005) and more re-
cently convolution tree kernels (Zhang et al, 2006;
Qian et al, 2008). However, in both feature-based
and kernel-based studies, availability of sufficient
labeled training data is always assumed.
Chen et al (2006) explored semi-supervised
learning for relation extraction using label prop-
agation, which makes use of unlabeled data.
Zhou et al (2008) proposed a hierarchical learning
strategy to address the data sparseness problem in
relation extraction. They also considered the com-
monality among different relation types, but com-
pared with our work, they had a different problem
setting and a different way of modeling the com-
monality. Banko and Etzioni (2008) studied open
domain relation extraction, for which they man-
ually identified several common relation patterns.
In contrast, our method obtains common patterns
through statistical learning. Xu et al (2008) stud-
ied the problem of adapting a rule-based relation
extraction system to new domains, but the types
of relations to be extracted remain the same.
Transfer learning aims at transferring knowl-
edge learned from one or a number of old tasks
to a new task. Domain adaptation is a spe-
cial case of transfer learning where the learn-
ing task remains the same but the distribution
of data changes. There has been an increasing
amount of work on transfer learning and domain
adaptation in natural language processing recently.
Blitzer et al (2006) proposed a structural cor-
respondence learning method for domain adap-
tation and applied it to part-of-speech tagging.
Daume III (2007) proposed a simple feature aug-
mentation method to achieve domain adaptation.
Arnold et al (2008) used a hierarchical prior struc-
ture to help transfer learning and domain adap-
tation for named entity recognition. Dredze and
Crammer (2008) proposed an online method for
multi-domain learning and adaptation.
Multi-task learning is another learning
paradigm in which multiple related tasks are
learned simultaneously in order to achieve better
performance for each individual task (Caruana,
1997; Evgeniou and Pontil, 2004). Although it
was not originally proposed to transfer knowledge
to a particular new task, it can be naturally used to
achieve this goal because it models the common-
ality among tasks, which is the knowledge that
should be transferred to a new task. In our work,
transfer learning is done through a multi-task
learning framework similar to Evgeniou and
Pontil (2004).
3 Task definition
Our study is conducted using data from the Au-
tomatic Content Extraction (ACE) program1. We
focus on extracting binary relation instances be-
tween two relation arguments occurring in the
same sentence. Some example relation instances
and their corresponding relation types as defined
by ACE can be found in Table 1.
We consider the following weakly-supervised
problem setting. We are interested in extracting
instances of a target relation type T , but this re-
lation type is only specified by a small set of seed
instances. We may possibly have some additional
knowledge about the target type not in the form of
labeled instances. For example, we may be given
the entity type restrictions on the two relation ar-
guments. In addition to such limited information
about the target relation type, we also have a large
amount of labeled instances for K auxiliary rela-
tion types A1, . . . ,AK . Our goal is to learn a re-
lation extractor for T , leveraging all the data and
information we have.
1http://projects.ldc.upenn.edu/ace/
1013
Syntactic Pattern Relation Instance Relation Type (Subtype)
arg-2 arg-1 Arab leaders OTHER-AFF (Ethnic)
his father PER-SOC (Family)
South Jakarta Prosecution Office GPE-AFF (Based-In)
arg-1 of arg-2 leader of a minority government EMP-ORG (Employ-Executive)
the youngest son of ex-director Suharto PER-SOC (Family)
the Socialist People?s Party of Montenegro GPE-AFF (Based-In)
arg-1 [verb] arg-2 Yemen [sent] planes to Baghdad ART (User-or-Owner)
his wife [had] three young children PER-SOC (Family)
Jody Scheckter [paced] Ferrari to both victories EMP-ORG (Employ-Staff)
Table 1: Examples of similar syntactic structures across different relation types. The head words of the
first and the second arguments are shown in italic and bold, respectively.
Before introducing our transfer learning solu-
tion, let us first briefly explain our basic classifi-
cation approach and the features we use, as well
as two baseline solutions.
3.1 Feature configuration
We treat relation extraction as a classification
problem. Each pair of entities within a single sen-
tence is considered a candidate relation instance,
and the task becomes predicting whether or not
each candidate is a true instance of T . We use
feature-based logistic regression classifiers. Fol-
lowing our previous work (Jiang and Zhai, 2007b),
we extract features from a sequence representa-
tion and a parse tree representation of each rela-
tion instance. Each node in the sequence or the
parse tree is augmented by an argument tag that
indicates whether the node subsumes arg-1, arg-
2, both or neither. Nodes that represent the argu-
ments are also labeled with the entity type, subtype
and mention type as defined by ACE. Based on the
findings of Qian et al (2008), we trim the parse
tree of a relation instance so that it contains only
the most essential components. We extract uni-
gram features (consisting of a single node) and bi-
gram features (consisting of two connected nodes)
from the graphic representations. An example of
the graphic representation of a relation instance
is shown in Figure 1 and some features extracted
from this instance are shown in Table 2. This
feature configuration gives state-of-the-art perfor-
mance (F1 = 0.7223) on the ACE 2004 data set in
a standard setting with sufficient data for training.
3.2 Baseline solutions
We consider two baseline solutions to the weakly-
supervised relation extraction problem. In the first
NP
NPB
3
PP
1
leader
NN
PER
of
IN
government
NN
ORG
NPB
1 0
2
2
2
Figure 1: The combined sequence and parse tree
representation of the relation instance ?leader of
a minority government.? The non-essential nodes
for ?a? and for ?minority? are removed based on
the algorithm from Qian et al (2008).
Feature Explanation
ORG2 arg-2 is an ORG entity.
of0 government2 arg-2 is ?government? and
follows the word ?of.?
NP3 ? PP2 There is a noun phrase
containing both arguments,
with arg-2 contained in a
prepositional phrase inside
the noun phrase.
Table 2: Examples of unigram and bigram features
extracted from Figure 1.
baseline, we use only the few seed instances of the
target relation type together with labeled negative
relation instances (i.e. pairs of entities within the
same sentence but having no relation) to train a
binary classifier. In the second baseline, we take
the union of the positive instances of both the tar-
get relation type and the auxiliary relation types as
our positive training set, and together with the neg-
ative instances we train a binary classifier. Note
that the second baseline method essentially learns
1014
a classifier for any relation type.
Another existing solution to weakly-supervised
learning problems is semi-supervised learning,
e.g. bootstrapping. However, because our pro-
posed transfer learning method can be combined
with semi-supervised learning, here we do not in-
clude semi-supervised learning as a baseline.
4 A multi-task transfer learning solution
We now present a multi-task transfer learning so-
lution to the weakly-supervised relation extraction
problem, which makes use of the labeled data from
the auxiliary relation types.
4.1 Syntactic similarity between relation
types
To see why the auxiliary relation types may help
the identification of the target relation type, let us
first look at how different relation types may be re-
lated and even similar to each other. Based on our
inspection of a sample of the ACE data, we find
that instances of different relation types can share
certain common syntactic structures. For example,
the syntactic pattern ?arg-1 of arg-2? strongly in-
dicates that there exists some relation between the
two arguments, although the nature of the relation
may be well dependent on the semantic meanings
of the two arguments. More examples are shown
in Table 1. This observation suggests that some
of the syntactic patterns learned from the auxiliary
relation types may be transferable to the target re-
lation type, making it easier to learn the target rela-
tion type and thus alleviating the insufficient train-
ing data problem with the target type.
How can we incorporate this desired knowledge
transfer process into our learning method? While
one can make explicit use of these general syntac-
tic patterns in a rule-based relation extraction sys-
tem, here we restrict our attention to feature-based
linear classifiers. We note that in feature-based lin-
ear classifiers, a useful syntactic pattern is trans-
lated into large weights for features related to the
syntactic pattern. For example, if ?arg-1 of arg-2?
is a useful pattern, in the learned linear classifier
we should have relatively large weights for fea-
tures such as ?the word of occurs before arg-2? or
?a preposition occurs before arg-2,? or even more
complex features such as ?there is a prepositional
phrase containing arg-2 attached to arg-1.? It is
the weights of these generally useful features that
are transferable from the auxiliary relation types
to the target relation type.
4.2 Statistical learning model
As we have discussed, we want to force the linear
classifiers for different relation types to share their
model weights for those features that are related
to the common syntactic patterns. Formally, we
consider the following statistical learning model.
Let ?k denote the weight vector of the linear
classifier that separates positive instances of aux-
iliary type Ak from negative instances, and let ?T
denote a similar weight vector for the target type
T . If different relation types are totally unrelated,
these weight vectors should also be independent of
each other. But because we observe similar syn-
tactic structures across different relation types, we
now assume that these weight vectors are related
through a common component ?:
?T = ?T + ?,
?k = ?k + ? for k = 1, . . . ,K.
If we assume that only weights of certain gen-
eral features can be shared between different rela-
tion types, we can force certain dimensions of ? to
be 0. We express this constraint by introducing a
matrix F and setting F? = 0. Here F is a square
matrix with all entries set to 0 except that Fi,i = 1
if we want to force ?i = 0.
Now we can learn these weight vectors in a
multi-task learning framework. Let x represent
the feature vector of a candidate relation instance,
and y ? {+1,?1} represent a class label. Let
DT = {(xTi , yTi )}NTi=1 denote the set of labeled
instances for the target type T . (Note that the
number of positive instances in DT is very small.)
And let Dk = {(xki , yki )}Nki=1 denote the labeled
instances for the auxiliary type Ak.
We learn the optimal weight vectors {??k}Kk=1,
??T and ?? by optimizing the following objective
function:
(
{??k}Kk=1, ??T , ??
)
= argmin
{?k},?T ,?,F?=0
[
L(DT , ?T + ?)
+
K?
k=1
L(Dk, ?k + ?)
+?T? ??T ?2 +
K?
k=1
?k???k?2 + ?????2
]
. (1)
1015
The objective function follows standard empir-
ical risk minimization with regularization. Here
L(D, ?) is the aggregated loss of labeling x with
y for all (x, y) in D, using weight vector ?. In
logistic regression models, the loss function is the
negative log likelihood, that is,
L(D, ?) = ?
?
(x,y)?D
log p(y|x, ?),
p(y|x, ?) = exp(?y ? x)?
y??{+1,?1} exp(?y? ? x)
.
?T? , ?k? and ?? are regularization parameters.
By adjusting their values, we can control the de-
gree of weight sharing among the relation types.
The larger the ratio ?T? /?? (or ?k?/??) is, the more
we believe that the model for T (or Ak) should
conform to the common model, and the smaller
the type-specific weight vector ?T (or ?k) will be.
The model presented above is based on our pre-
vious work (Jiang and Zhai, 2007c), which bears
the same spirit of some other recent work on multi-
task learning (Ando and Zhang, 2005; Evgeniou
and Pontil, 2004; Daume III, 2007). It is general
for any transfer learning problem with auxiliary la-
beled data from similar tasks. Here we are mostly
interested in the model?s applicability and effec-
tiveness on the relation extraction problem.
4.3 Feature separation
Recall that we impose a constraint F? = 0 when
optimizing the objective function. This constraint
gives us the freedom to force only the weights of a
subset of the features to be shared among different
relation types. A remaining question is how to set
this matrix F , that is, how to determine the set of
general features to use. We propose two ways of
setting this matrix F .
Automatically setting F
One way is to fix the number of non-zero entries
in ? to be a pre-defined number H of general fea-
tures, and allow F to change during the optimiza-
tion process. This can be done by repeating the
following two steps until F converges:
1. Fix F , and optimize the objective function as
in Equation (1).
2. Fix (?T + ?) and (?k + ?), and search for
?T , {?k} and ? that minimizes (?T? ??T ?2 +?K
k=1 ?k???k?2 + ?????2
), subject to the
constraint that at most H entries of ? are non-
zero.
Human guidance
Another way to select the general features is to fol-
low some guidance from human knowledge. Re-
call that in Section 4.1 we find that the common-
ality among different relation types usually lies
in the syntactic structures between the two ar-
guments. This observation gives some intuition
about how to separate general features from type-
specific features. In particular, here we consider
two hypotheses regarding the generality of differ-
ent kinds of features.
Argument word features: We hypothesize that
the head words of the relation arguments are more
likely to be strong indicators of specific relation
types rather than any relation type. For example, if
an argument has the head word ?sister,? it strongly
indicates a family relation. We refer to the set of
features that contain any head word of an argu-
ment as ?arg-word? features.
Entity type features: We hypothesize that the
entity types and subtypes of the relation arguments
are also more likely to be associated with specific
relation types. For example, arguments that are
location entities may be strongly correlated with
physical proximity relations. We refer to the set of
features that contain the entity type or subtype of
an argument as ?arg-NE? features.
We hypothesize that the arg-word and arg-NE
features are type-specific and therefore should be
excluded from the set of general features. We
can force the weights of these hypothesized type-
specific features to be 0 in the shared weight vec-
tor ?, i.e. we can set the matrix F to achieve this
feature separation.
Combined method
We can also combine the automatic way of setting
F with human guidance. Specifically, we still fol-
low the first automatic procedure to choose gen-
eral features, but we then filter out any hypothe-
sized type-specific feature from the set of general
features chosen by the automatic procedure.
4.4 Imposing entity type constraints
Finally, we consider how we can exploit additional
human knowledge about the target relation type T
to further improve the classifier. We note that usu-
ally when a relation type is defined, we often have
strong preferences or even hard constraints on the
types of entities that can possibly be the two rela-
tion arguments. These type constraints can help us
1016
Target Type T BL BL-A TL-auto TL-guide TL-comb TL-NE
P 0.0000 0.1692 0.2920 0.2934 0.3325 0.5056
Physical R 0.0000 0.0848 0.1696 0.1722 0.2383 0.2316
F 0.0000 0.1130 0.2146 0.2170 0.2777 0.3176
Personal P 1.0000 0.0804 0.1005 0.3069 0.3214 0.6412
/Social R 0.0386 0.1708 0.1598 0.7245 0.7686 0.7631
F 0.0743 0.1093 0.1234 0.4311 0.4533 0.6969
Employment P 0.9231 0.3561 0.5230 0.5428 0.5973 0.7145
/Membership R 0.0075 0.1850 0.2617 0.2648 0.3632 0.3601
/Subsidiary F 0.0148 0.2435 0.3488 0.3559 0.4518 0.4789
Agent- P 0.8750 0.0603 0.1813 0.1825 0.1835 0.1967
Artifact R 0.0343 0.2353 0.6471 0.6225 0.6422 0.6373
F 0.0660 0.0960 0.2833 0.2822 0.2854 0.3006
PER/ORG P 0.8889 0.0838 0.1510 0.1592 0.1667 0.1844
Affiliation R 0.0567 0.4965 0.6950 0.8369 0.8794 0.8723
F 0.1067 0.1434 0.2481 0.2676 0.2802 0.3045
GPE P 1.0000 0.2530 0.3904 0.3604 0.3560 0.5824
Affiliation R 0.0077 0.4509 0.6416 0.5992 0.6166 0.6127
F 0.0153 0.3241 0.4854 0.4501 0.4513 0.5972
P 1.0000 0.0298 0.0503 0.0471 0.1370 0.1370
Discourse R 0.0036 0.0789 0.1075 0.1147 0.3477 0.3477
F 0.0071 0.0433 0.0685 0.0668 0.1966 0.1966
P 0.8124 0.1475 0.2412 0.2703 0.2992 0.4231
Average R 0.0212 0.2432 0.3832 0.4764 0.5509 0.5464
F 0.0406 0.1532 0.2532 0.2958 0.3423 0.4132
Table 3: Comparison of different methods on ACE 2004 data set. P, R and F stand for precision, recall
and F1, respectively.
remove some false positive instances. We there-
fore manually identify the entity type constraints
for each target relation type based on the defini-
tion of the relation type given in the ACE annota-
tion guidelines, and impose these type constraints
as a final refinement step on top of the predicted
positive instances.
5 Experiments
5.1 Data set and experiment setup
We used the ACE 2004 data set to evaluate our
proposed methods. There are seven relation types
defined in ACE 2004. After data cleaning, we ob-
tained 4290 positive instances among 48614 can-
didate relation instances. We took each relation
type as the target type and used the remaining
types as auxiliary types. This gave us seven sets
of experiments. In each set of experiments for a
single target relation type, we randomly divided
all the data into five subsets, and used each subset
for testing while using the other four subsets for
training, i.e. each experiment was repeated five
times with different training and test sets. Each
time, we removed most of the positive instances
of the target type from the training set except only
a small number S of seed instances. This gave
us the weakly-supervised setting. We kept all the
positive instances of the target type in the test set.
In order to concentrate on the classification accu-
racy for the target relation type, we removed the
positive instances of the auxiliary relation types
from the test set, although in practice we need
to extract these auxiliary relation instances using
learned classifiers for these relation types.
5.2 Comparison of different methods
We first show the comparison of our proposed
multi-task transfer learning methods with the two
baseline methods described in Section 3.2. The
performance on each target relation type and the
average performance across seven types are shown
in Table 3. BL refers to the first baseline and BL-
A refers to the second baseline which uses auxil-
1017
?T? 100 1000 10000
P 0.6265 0.3162 0.2992
R 0.1170 0.3959 0.5509
F 0.1847 0.2983 0.3423
Table 4: The average performance of TL-comb
with different ?T? . (?k? = 104 and ?? = 1.)
iary relation instances. The four TL methods are
all based on the multi-task transfer learning frame-
work. TL-auto sets F automatically within the
optimization problem itself. TL-guide chooses all
features except arg-word and arg-NE features as
general features and sets F accordingly. TL-comb
combines TL-auto and TL-guide, as described in
Section 4.3. Finally, TL-NE builds on top of TL-
comb and uses the entity type constraints to re-
fine the predictions. In this set of experiments,
the number of seed instances for each target re-
lation type was set to 10. The parameters were
set to their optimal values (?T? = 104, ?k? = 104,
?? = 1, and H = 500).
As we can see from the table, first of all, BL
generally has high precision but very low recall.
BL-A performs better than BL in terms of F1 be-
cause it gives better recall. However, BL-A still
cannot achieve as high recall as the TL methods.
This is probably because the model learned by BL-
A still focuses more on type-specific features for
each relation type rather than on the commonly
useful general features, and therefore does not
help much in classifying the target relation type.
The four TL methods all outperform the two
baseline methods. TL-comb performs better than
both TL-auto and TL-guide, which shows that
while we can either choose general features au-
tomatically by the learning algorithm or manu-
ally with human knowledge, it is more effective
to combine human knowledge with the multi-task
learning framework. Not surprisingly, TL-NE im-
proves the precision over TL-comb without hurt-
ing the recall much. Ideally, TL-NE should not
decrease recall if the type constraints are strictly
observed in the data. We find that it is not always
the case with the ACE data, leading to the small
decrease of recall from TL-comb to TL-NE.
5.3 The effect of ?T?
Let us now take a look at the effect of using dif-
ferent ?T? . As we can see from Table 4, smaller
?T? gives higher precision while larger ?T? gives
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 100  1000  10000
a
vg
 F
1
H
TL-comb
TL-auto
BL-A
Figure 2: Performance of TL-comb and TL-auto
as H changes.
higher recall. These results make sense because
the larger ?T? is, the more we penalize large
weights of ?T . As a result, the model for the tar-
get type is forced to conform to the shared model
? and prevented from overfitting the few seed tar-
get instances. ?T? is therefore a useful parameter
to help us control the tradeoff between precision
and recall for the target type.
While varying ?k? also gives similar effect for
typeAk, we found that setting ?k? to smaller values
would not help T because in this case the auxiliary
relation instances would be used more for train-
ing the type-specific component ?k rather than the
common component ?.
5.4 Sensitivity of H
Another parameter in the multi-task transfer learn-
ing framework is the number of general features
H , i.e. the number of non-zero entries in the
shared weight vector ?. To see how the perfor-
mance may vary as H changes, we plot the per-
formance of TL-comb and TL-auto in terms of the
average F1 across the seven target relation types,
with H ranging from 100 to 50000. As we can see
in Figure 2, the performance is relatively stable,
and always above BL-A. This suggests that the
performance of TL-comb and TL-auto is not very
sensitive to the value of H .
5.5 Hypothesized type-specific features
In Section 4.3, we showed two sets of hypoth-
esized type-specific features, namely, arg-word
features and arg-NE features. We also experi-
mented with each set separately to see whether
both sets are useful. The comparison is shown in
Table 5. As we can see, using either set of type-
specific features in either TL-guide or TL-comb
can improve the performance over BL-A, but the
1018
arg-word arg-NE union
TL-guide 0.2095 0.2983 0.2958
TL-comb 0.2215 0.3331 0.3423
BL-A 0.1532
Table 5: Average F1 using different hypothesized
type-specific features.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 10  100  1000
a
vg
 F
1
S
TL-NE (104)
TL-NE (102)
BL
BL-A
Figure 3: Performance of TL-NE, BL and BL-A
as the number of seed instances S of the target type
increases. (H = 500. ?T? was set to 104 and 102).
arg-NE features are probably more type-specific
than arg-word features because they give better
performance. Using the union of the two sets is
still the best for TL-comb.
5.6 Changing the number of seed instances
Finally, we compare TL-NE with BL and BL-A
when the number of seed instances increases. We
set S from 5 up to 1000. When S is large, the
problem becomes more like traditional supervised
learning, and our setting of ?T? = 104 is no longer
optimal because we are now not afraid of overfit-
ting the large set of seed target instances. There-
fore we also included another TL-NE experiment
with ?T? set to 102. The comparison of the perfor-
mance is shown in Figure 3. We see that as S in-
creases, both BL and BL-A catch up, and BL over-
takes BL-A when S is sufficiently large because
BL uses positive training examples only from the
target type. Overall, TL-NE still outperforms the
two baselines in most of the cases over the wide
range of values of S, but the optimal value for ?T?
decreases as S increases, as we have suspected.
The results show that if ?T? is set appropriately,
our multi-task transfer learning method is robust
and advantageous over the baselines under both
the weakly-supervised setting and the traditional
supervised setting.
6 Conclusions and future work
In this paper, we applied multi-task transfer learn-
ing to solve a weakly-supervised relation extrac-
tion problem, leveraging both labeled instances of
auxiliary relation types and human knowledge in-
cluding hypotheses on feature generality and en-
tity type constraints. In the multi-task learning
framework that we introduced, different relation
types are treated as different but related tasks that
are learned together, with the common structures
among the relation types modeled by a shared
weight vector. The shared weight vector corre-
sponds to the general features across different re-
lation types. We proposed to choose the general
features either automatically inside the learning al-
gorithm or guided by human knowledge. We also
leveraged additional human knowledge about the
target relation type in the form of entity type con-
straints. Experiment results on the ACE 2004 data
show that the multi-task transfer learning method
achieves the best performance when we combine
human guidance with automatic general feature
selection, followed by imposing the entity type
constraints. The final method substantially outper-
forms two baseline methods, improving the aver-
age F1 measure from 0.1532 to 0.4132 when only
10 seed target instances are used.
Our work is the first to explore transfer learning
for relation extraction, and we have achieved very
promising results. Because of the practical impor-
tance of transfer learning and adaptation for rela-
tion extraction due to lack of training data in new
domains, we hope our study and findings will lead
to further investigation into this problem. There
are still many issues that remain unsolved. For ex-
ample, we have not looked at the degrees of re-
latedness between different pairs of relation types.
Presumably, when adapting to a specific target re-
lation type, we want to choose the most similar
auxiliary relation types to use. Our current study
is based on ACE relation types. It would also be
interesting to study similar problems in other do-
mains, for example, the protein-protein interaction
extraction problem in biomedical text mining.
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817?1853, November.
1019
Andrew Arnold, Ramesh Nallapati, and William W.
Cohen. 2008. Exploiting feature hierarchy for
transfer learning in named entity recognition. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, pages 245?
253.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 28?36.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
724?731.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41?75.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and
Zhengyu Niu. 2006. Relation extraction using la-
bel propagation based semi-supervised learning. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 129?136.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics, pages 423?429.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 256?263.
Mark Dredze and Koby Crammer. 2008. Online
methods for multi-domain learning and adaptation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
689?697.
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Proceedings of
the 10th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 109?
117.
Jing Jiang and ChengXiang Zhai. 2007a. Instance
weighting for domain adaptation in nlp. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 264?271.
Jing Jiang and ChengXiang Zhai. 2007b. A systematic
exploration of the feature space for relation extrac-
tion. In Proceedings of the Human Language Tech-
nologies Conference, pages 113?120.
Jing Jiang and ChengXiang Zhai. 2007c. A two-stage
approach to domain adaptation for statistical classi-
fiers. In Proceedings of the 16th ACM Conference
on Information and Knowledge Management, pages
401?410.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaom-
ing Zhu, and Peide Qian. 2008. Exploiting con-
stituent dependencies for tree kernel-based semantic
relation extraction. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics, pages 697?704.
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In Advances in Neural
Information Processing Systems 8, pages 640?646.
Feiyu Xu, Hans Uszkoreit, Hong Li, and Niko Felger.
2008. Adaptation of relation extraction rules to new
domains. In Proceedings of the 6th International
Conference on Language Resources and Evaluation,
pages 2446?2450.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In Proceedings of the Human
Language Technology Conference, pages 288?295.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 419?426.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 427?434.
GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2008. Hierarchical learning strat-
egy in semantic relation extraction. Information
Processing and Management, 44(3):1008?1021.
1020
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 197?200,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Extracting Paraphrases of Technical Terms
from Noisy Parallel Software Corpora
Xiaoyin Wang
1,2
, David Lo
1
, Jing Jiang
1
, Lu Zhang
2
, Hong Mei
2
1
School of Information Systems, Singapore Management University, Singapore, 178902
{xywang, davidlo, jingjiang}@smu.edu.sg
2
Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education
Beijing, 100871, China
{zhanglu, meih}@sei.pku.edu.cn
Abstract
In this paper, we study the problem of ex-
tracting technical paraphrases from a par-
allel software corpus, namely, a collec-
tion of duplicate bug reports. Paraphrase
acquisition is a fundamental task in the
emerging area of text mining for software
engineering. Existing paraphrase extrac-
tion methods are not entirely suitable here
due to the noisy nature of bug reports. We
propose a number of techniques to address
the noisy data problem. The empirical
evaluation shows that our method signifi-
cantly improves an existing method by up
to 58%.
1 Introduction
Using natural language processing (NLP) tech-
niques to mine software corpora such as code com-
ments and bug reports to assist software engineer-
ing (SE) is an emerging and promising research
direction (Wang et al, 2008; Tan et al, 2007).
Paraphrase extraction is one of the fundamental
problems that have not been addressed in this area.
It has many applications including software ontol-
ogy construction and query expansion for retriev-
ing relevant technical documents.
In this paper, we study automatic paraphrase ex-
traction from a large collection of software bug re-
ports. Most large software projects have bug track-
ing systems, e.g., Bugzilla
1
, to help global users to
describe and report the bugs they encounter when
using the software. However, since the same bug
may be seen by many users, many duplicate bug
reports are sent to bug tracking systems. The du-
plicate bug reports are manually tagged and asso-
ciated to the original bug report by either the sys-
tem manager or software developers. These fam-
ilies of duplicate bug reports form a semi-parallel
1
http://www.bugzilla.org/
Parallel bug reports with a pair of true paraphrases
1: connector extend with a straight line in full screen
mode
2: connector show straight line in presentation mode
Non-parallel bug reports referring to the same bug
1: Settle language for part of text and spellchecking
part of text
2: Feature requested to improve the management of a
multi-language document
Context-peculiar paraphrases (shown in italics)
1: status bar appear in the middle of the screen
2: maximizing window create phantom status bar in
middle of document
Table 1: Bug Report Examples
corpus and therefore a good candidate for extrac-
tion of paraphrases of technical terms. Hence, bug
reports interest us because (1) they are abundant
and freely available,(2) they naturally form a semi-
parallel corpus, and (3) they contain many techni-
cal terms.
However, bug reports have characteristics that
raise many new challenges. Different from many
other parallel corpora, bug reports are noisy. We
observe at least three types of noise common in
bug reports. First, many bug reports have many
spelling, grammatical and sentence structure er-
rors. To address this we extend a suitable state-
of-the-art technique that is robust to such cor-
pora, i.e. (Barzilay and McKeown, 2001). Sec-
ond, many duplicate bug report families contain
sentences that are not truly parallel. An exam-
ple is shown in Table 1 (middle). We handle this
by considering lexical similarity between dupli-
cate bug reports. Third, even if the bug reports are
parallel, we find many cases of context-peculiar
paraphrases, i.e., a pair of phrases that have the
same meaning in a very narrow context. An exam-
ple is shown in Table 1 (bottom). To address this,
we introduce two notions of global context-based
score and co-occurrence based score which take
into account all good and bad occurrences of the
phrases in a candidate paraphrase in the corpus.
These scores are then used to identify and remove
197
context-peculiar paraphrases.
The contributions of our work are twofold.
First, we studied the important problem of para-
phrase extraction from a noisy semi-parallel soft-
ware corpus, which has not been studied either in
the NLP or the SE community. Second, taking
into consideration the special characteristics of our
noisy data, we proposed several improvements to
an existing general paraphrase extraction method,
resulting in a significant performance gain ? up to
58% relative improvement in precision.
2 Related Work
In the area of text mining for software engineer-
ing, paraphrases have been used in many tasks,
e.g., (Wang et al, 2008; Tan et al, 2007). How-
ever, most paraphrases used are obtained manu-
ally. A recent study using synonyms from Word-
Net highlights the fact that these are not effective
in software engineering tasks due to domain speci-
ficity (Sridhara et al, 2008). Therefore, an auto-
matic way to derive technical paraphrases specific
to software engineering is desired.
Paraphrases can be extracted from non-parallel
corpora using contextual similarity (Lin, 1998).
They can also be obtained from parallel corpora
if such data is available (Barzilay and McKeown,
2001; Ibrahim et al, 2003). Recently, there are
also a number of studies that extract paraphrases
from multilingual corpora (Bannard and Callison-
Burch, 2005; Zhao et al, 2008).
The approach in (Barzilay and McKeown,
2001) does not use deep linguistic analysis and
therefore is suitable to noisy corpora like ours.
Due to this reason, we build our technique on top
of theirs. The following provides a summary of
their technique.
Two types of paraphrase patterns are defined:
(1) Syntactic patterns which consist of the POS
tags of the phrases. For example, the paraphrases
?a VGA monitor? and ?a monitor? are represented
as ?DT
1
JJ NN
2
? ? ?DT
1
NN
2
?, where the sub-
scripts denote common words. (2) Contextual pat-
terns which consist of the POS tags before and af-
ter the phrases. For example, the contexts ?in the
middle of? and ?in middle of? in Table 1 (bottom)
are represented as ?IN
1
DT NN
2
IN
3
? ? ?IN
1
NN
2
IN
3
?.
During pre-processing, the parallel corpus is
aligned to give a list of parallel sentence pairs.
The sentences are then processed by a POS tag-
ger and a chunker. The authors first used identi-
cal words and phrases as seeds to find and score
contextual patterns. The patterns are scored based
on the following formula: (n+)/n, in which, n+
refers to the number of positively labeled para-
phrases satisfying the patterns and n refers to the
number of all paraphrases satisfying the patterns.
Only patterns with scores above a threshold are
considered. More paraphrases are identified using
these contextual patterns, and more patterns are
then found and scored using the newly-discovered
paraphrases. This co-training algorithm is em-
ployed in an iterative fashion to find more patterns
and positively labeled paraphrases.
3 Methodology
Our paraphrase extraction method consists of
three components: sentence selection, global
context-based scoring and co-occurrence-based
scoring. We marry the three components together
into a holistic solution.
Selection of Parallel Sentences Our corpus con-
sists of short bug report summaries, each contain-
ing one or two sentences only, grouped by the
bugs they report. Each group corresponds to re-
ports pertaining to a single bug and are duplicate
of one another. Therefore, reports belonging to the
same group can be naturally regarded as parallel
sentences.
However, these sentences are only partially par-
allel because two users may describe the same bug
in very different ways. An example is shown in Ta-
ble 1 (middle). This kind of sentence pairs should
not be regarded as parallel. To address this prob-
lem, we take a heuristic approach and only select
sentence pairs that have strong similarities. Our
similarity score is based on the number of com-
mon words, bigrams and trigrams shared between
two parallel sentences. We use a threshold of 5 to
filter out non-parallel sentences.
Global Context-Based Scoring Our context-
based paraphrase scoring method is an extension
of (Barzilay and McKeown, 2001) described in
Sec. 2. Parallel bug reports are usually noisy.
At times, some words might be detected as para-
phrases incidentally due to the noise. In (Barzi-
lay and McKeown, 2001), a paraphrase is reported
as long as there is a single good supporting pair
of sentences. Although this works well for a rel-
atively clean parallel corpus considered in their
work, i.e., novels, this does not work well for bug
reports. Consider the context-peculiar example in
Table 1 (bottom). For a context-peculiar para-
198
phrase, there can be many sentences containing
the pair of phrases but very few support them to
be a paraphrase. We develop a technique to off-
set this noise by computing a global context-based
score for two phrases being a paraphrase over all
their parallel occurrences. This is defined by the
following formula: S
g
=
1
n
?
n
i=1
s
i
, where n is
the number of parallel bug reports with the two
phrases occurring in parallel, and s
i
is the score
for the i?th occurrence. s
i
is computed as follows:
1. We compute the set of patterns with affixed
pattern scores based on (Barzilay and McK-
eown, 2001).
2. For the i?th parallel occurrence of the pair of
phrases we want to score, we try to find a pat-
tern that matches the occurrence and assign
the pattern score to the pair of phrases as s
i
.
If no such pattern exists, we set s
i
to 0.
By taking the average of s
i
as the global score
for a pair of phrases, we do not rely much on a sin-
gle s
i
and can therefore prevent context-peculiar
paraphrases to some degree.
Co-occurrence-Based Scoring We also consider
another global co-occurrence-based score that is
commonly used for finding collocations. A gen-
eral observation is that noise tends to appear in
random but random things do not occur in the
same way often. It is less likely for randomly
paired words or paraphrases to co-occur together
many times. To compute the likelihood of two
phrases occurring together, we use the following
commonly used co-occurrence-based score:
S
c
=
P (w
1
, w
2
)
P (w
1
)P (w
2
)
. (1)
The expression P (w
1
, w
2
) refers to the probability
of a pair of phrases w
1
and w
2
appearing together.
It is estimated based on the proportion of the cor-
pus containing both w
1
and w
2
in parallel. Sim-
ilarly, P (w
1
) and P (w
2
) each corresponds to the
probability of w
1
and w
2
appearing respectively.
We normalize the S
c
score to the range of 0 to 1
by dividing it with the size of the corpus.
Holistic Solution We employ the parallel sen-
tence selection as a pre-processing step, and merge
co-occurrence-based scoring with global context-
based scoring. For each parallel sentence pairs, a
chunker is used to get chunks from each sentence.
All possible pairings of chunks are then formed.
This set of chunk pairs are later fed to the method
in (Barzilay and McKeown, 2001) to produce a
set of patterns with affixed scores. With this we
compute our global-context based scores. The co-
occurrence based scores are computed following
the approach described above.
Two thresholds are used and candidate para-
phrases whose scores are below the respective
thresholds are removed. Alternatively, one of the
score is used as a filter, while the other is used to
rank the candidates. The next section describes
our experimental results.
4 Evaluation
Data Set Our bug report corpus is built from
OpenOffice
2
. OpenOffice is a well-known open
source software which has similar functionalities
as Microsoft Office. We use the bug reports that
are submitted before Jan 1, 2008. Also, we only
use the summary part of the bug reports.
We build our corpus in the following steps. We
collect a total of 13,898 duplicate bug reports from
OpenOffice. Each duplicate bug report is associ-
ated to a master report?there is one master re-
port for each unique bug. From this information,
we create duplicate bug report groups where each
member of a group is a duplicate of all other mem-
bers in the same group. Finally, we extract dupli-
cate bug report pairs by pairing each two members
of each group. We get in total 53,363 duplicate
bug report pairs.
As the first step, we employ parallel sentence
selection, described in Sec. 3, to remove non-
parallel duplicate bug report pairs. After this step,
we find 5,935 parallel duplicate bug report pairs.
Experimental Setup The baseline method we
consider is the one in (Barzilay and McKeown,
2001) without sentence alignment ? as the bug re-
ports are usually of one sentence long. We call it
BL. As described in Sec. 2, BL utilizes a threshold
to control the number of patterns mined. These
patterns are later used to select paraphrases. In the
experiment, we find that running BL using their
default threshold of 0.95 on the 5,935 parallel bug
reports only gives us 18 paraphrases. This num-
ber is too small for practical purposes. Therefore,
we reduce the threshold to get more paraphrases.
For each threshold in the range of 0.45-0.95 (step
size: 0.05), we extract paraphrases and compute
the corresponding precision.
In our approach, we first form chunk pairs from
the 5,935 pairs of parallel sentences and then use
the baseline approach at a low threshold to ob-
2
http://www.openoffice.org/
199
tain patterns. Using these patterns we compute
the global context-based scores S
g
. We also com-
pute the co-occurrence scores S
c
. We rank and
extract top-k paraphrases based on these scores.
We consider 4 different methods: We can use ei-
ther S
g
or S
c
to rank the discovered paraphrases.
We call them Rk-S
g
and Rk-S
c
. We also consider
using one of the scores for ranking and the other
for filtering bad candidate paraphrases. A thresh-
old of 0.05 is used for filtering. We call these two
methods Rk-S
c
+Ft-S
g
and Rk-S
g
+Ft-S
c
. With
ranked lists from these 4 methods, we can com-
pute precision@k for the top-k paraphrases.
Results The comparison among these methods
is plotted in Figure 1. From the figure we can
see that our holistic approach using global-context
score to rank and co-occurrence score to filter
(i.e., Rk-S
g
+Ft-S
c
) has higher precision than the
baseline approach (i.e., BL) in all ks. In general,
the other holistic configuration (i.e., Rk-S
c
+Ft-S
g
)
also works well for most of the ks considered. In-
terestingly, the graph shows that using only one of
the scores alone (i.e., Rk-S
g
and Rk-S
c
) does not
result in a significantly higher precision than the
baseline approach. A holistic approach by merg-
ing global-context score and co-occurrence score
is needed to yield higher precision.
In Table 2, we show some examples of the para-
phrases our algorithm extracted from the bug re-
port corpus. As we can see, most of the para-
phrases are very technical and only make sense in
the software domain. It demonstrates the effec-
tiveness of our method.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 50  100  150  200  250  300  350  400  450
pr
ec
isi
on
 at
 k
k
BL
Rk-SgRk-ScRk-Sc+Ft-SgRk-Sg+Ft-Sc
Figure 1: Precision@k for a range of k.
5 Conclusion
In this paper, we develop a new technique to ex-
tract paraphrases of technical terms from software
bug reports. Paraphrases of technical terms have
been shown to be useful for various software en-
the edit-field ? input line field
presentation mode ? full screen mode
word separator ? a word delimiter
application ? app
freeze ? crash
mru file list ? recent file list
multiple monitor ? extended desktop
xl file ? excel file
Table 2: Examples of paraphrases of technical
terms mined from bug reports.
gineering tasks. These paraphrases could not be
obtained via general purpose thesaurus e.g., Word-
Net. Interestingly, there is a wealth of text data,
in particular bug reports, available for analysis in
open-source software repositories. Despite their
availability, a good technique is needed to extract
paraphrases from these corpora as they are often
noisy. We develop several approaches to address
noisy data via parallel sentence selection, global-
context based scoring and co-occurrence based
scoring. To show the utility of our approach, we
experimented with many parallel bug reports from
a large software project. The preliminary exper-
iment result is promising as it could significantly
improves an existing method by up to 58%.
References
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In ACL: Annual
Meet. of Assoc. of Computational Linguistics.
R. Barzilay and K. R. McKeown. 2001. Extracting
paraphrases from a parallel corpus. In ACL: Annual
Meet. of Assoc. of Computational Linguistics.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Int. Workshop on Paraphrasing.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In ACL: Annual Meet. of Assoc. of
Computational Linguistics.
G. Sridhara, E. Hill, L. Pollock, and K. Vijay-Shanker.
2008. Identifying word relations in software: A
comparative study of semantic similarity tools. In
ICPC: Int. Conf. on Program Comprehension.
L. Tan, D. Yuan, G. Krishna, and Y. Zhou. 2007.
/*icomment: bugs or bad comments?*/. In SOSP:
Symp. on Operating System Principles.
X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. 2008.
An approach to detecting duplicate bug reports us-
ing natural language and execution information. In
ICSE: Int. Conf. on Software Engineering.
S. Zhao, H. Wang, T. Liu, and S. Li. 2008. Pivot ap-
proach for extracting paraphrase patterns from bilin-
gual corpora. In ACL: Annual Meet. of Assoc. of
Computational Linguistics.
200
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1670?1681, Dublin, Ireland, August 23-29 2014.
Generating Supplementary Travel Guides from Social Media
Liu Yang
1,2
, Jing Jiang
2,?
, Lifu Huang
1,2
, Minghui Qiu
2
, Lizi Liao
2,3
1
Peking University / Beijing, China, 100871
2
Singapore Management University / Singapore, Singapore, 178902
3
Beijing Institute of Technology / Beijing, China, 100081
yang.liu@pku.edu.cn, jingjiang@smu.edu.sg
{warrior.fu, minghuiqiu, liaolizi.llz}@gmail.com
Abstract
In this paper we study how to summarize travel-related information in forum threads to gener-
ate supplementary travel guides. Such summaries presumably can provide additional and more
up-to-date information to tourists. Existing multi-document summarization methods have limita-
tions for this task because (1) they do not generate structured summaries but travel guides usually
follow a certain template, and (2) they do not put emphasis on named entities but travel guides
often recommend points of interest to travelers. To overcome these limitations, we propose to
use a latent variable model to align forum threads with the section structure of well-written travel
guides. The model also assigns section labels to named entities in forum threads. We then
propose to modify an ILP-based summarization method to generate section-specific summaries.
Evaluation on threads from Yahoo! Answers shows that our proposed method is able to generate
better summaries compared with a number of baselines based on ROUGE scores and coverage
of named entities.
1 Introduction
Online forums and community question answering (CQA) sites contain much useful information from
ordinary users, such as their personal experience, opinions, suggestions and recommendations. Extract-
ing and summarizing information from these rich information sources has a wide range of applications.
In this work, we study how to tap into user-generated content in forums such as Yahoo! Answers to
generate supplementary city travel guides. Travel guides published by well-known publishers such as
Lonely Planet are written by a small number of authors based on their travel experience. Presumably
if we could summarize the large amount of information given by ordinary users about a city, such a
summary could supplement the official travel guide and cover more up-to-date information.
However, social media content is diverse and noisy because it is contributed by many different au-
thors. Directly applying existing multi-document summarization methods to forum and CQA threads
may not produce good travel guides for the following reasons: (1) Summaries produced by standard
summarization methods are not structured, but travel guides usually follow a template structure. (2)
Travel guides put much emphasis on points of interest, which are usually location entities, but standard
text summarization methods are not entity-oriented.
To illustrate our points, in Table 1 we show (i) the overall structure of a travel guide for Sydney from
Lonely Planet, (ii) an excerpt from a summary generated by a state-of-the-art ILP-based summarization
method (Gillick and Favre, 2009) from a set of threads related to Sydney, and (iii) excerpts of a structured
summary generated by our proposed method. The comparison shows that the summary generated by
the standard ILP method mixes information on different topics together and does not mention many
* Corresponding author.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1670
Travel Guide from Lonely Planet (http://www.lonelyplanet.com/australia/sydney/)
Restaurants:
Sepia:There?s nothing washed out or brown-tinged about Sepia?s food: Martin Benn?s picture-perfect creations are presented in . . .
Icebergs Dining Room: Poised above the famous Icebergs swimming pool, Icebergs views sweep across the Bondi Beach arc to . . .
Shopping:
Strand Arcade: Constructed in 1891, the Strand rivals the QVB in the ornateness stakes. Three floors of designer fashions . . .
Westfield Sydney: The city?s newest shopping mall is a bafflingly large complex gobbling up Sydney Tower and a fair chunk of . . .
Transport:
Sydney Airport: Sydneys Kingsford Smith Airport , 10km south of the city centre, is Australias busiest airport, handling flights . . .
Water Taxis Combined:Fares based on up to four passengers; add $10 per person for additional passengers. Sample fares . . .
Yahoo! Answers Summary Generated by Standard ILP Method Yahoo! Answers Summary Generated by Our Method
It ?s not too far from Sydney . Sydney is the most expensive
place in Australia . They are a little lame ... Then you can go to
Darling Harbour, a beautiful habour which is a 10-minute walk
from town hall station . Make sure , if you are up to it to do the
bridge climb , this is a real treat . There are lots of interesting
things to see and do in and around Sydney . The suburbs-much
cheaper than the CBD. It was in the basement of a big shopping
mall . The only way to do that is to drive . Got to walk on top of
the Sydney harbour bridge and go up centre point tower ! Walk
around the street and see the beach . I would like to stay at a nice
hotel . My friend and I are wanting to take a trip to Sydney for
the summer . But you ?ll need to get there by taxi . Sydney is so
pretty, so you should be able to find stuff to do . And they have
many facilities . Good luck and have fun . Public transport is not
very good . Depending on what you ?re in Sydney to do it ?s hard
to say . . .
Restaurants:
Go to the two major restaurant areas close to the city Dar-
linghurst , along Oxford Street , and Newtown , along King
Street . Chinatown which is off George St. in the city look up
Dixon st. is a great place to get a cheap Chinese meal . . .
Shopping:
Queen Victoria Building and Pitt St Mall , World Square and
the Strand are good ideas to check out . Hair driers you can get
in many places , but the main places would be the department
stores such as Target , Big W , K-Mart , Myer, David Jones . . .
Transport:
The CBD is about 15 minutes by train from the airport and there
is a station at Circular Quay , right on the Harbour with access
to the bridge and the Opera House . You can catch an intercity
train with Cityrail from just about anywhere in Sydney . . .
Table 1: Comparison of different travel guides about Sydney. Top: excerpts from Lonely Planet. Bottom left: excerpt from
a summary generated by standard ILP. Bottom right: excerpts from summary generated by our method. Named entities are
highlighted in bold font.
interesting places to visit. The summary by our proposed method, in contrast, organizes the information
into sections and has a high coverage of places a tourist can visit.
To generate the kind of summaries as shown in the bottom right of Table 1, we propose to first leverage
the section structure of well-written travel guides and use a latent variable model to align forum threads
with the different sections from these travel guides. Moreover, observing that points of interest are orga-
nized by sections in these travel guides, we also identify location names from user-generated content and
try to uncover their underlying section labels. We then treat the remaining problem as a multi-document
summarization task. We modify an Integer Linear Programming (ILP)-based extractive summarization
framework (Gillick and Favre, 2009) to select sentences from forum threads to generate section-specific
summaries, where we specifically emphasize the inclusion of potential points of interest for each sec-
tion. Experiments using threads from Yahoo! Answers show that our proposed method generates better
summaries than a number of baselines in terms of ROUGE scores and coverage of named entities.
Our work makes the following contributions. First, we study a new problem of summarizing multiple
forum threads to generate city travel guides based on known template structure from well-written travel
guides. Second, we propose a principled approach based on latent variable models and Integer Linear
Programming. Third, we evaluate our method using real forum threads and human generated model
summaries, and the results are positive.
2 Overview of Our Method
Our task is to summarize travel-related information from forum threads for potential tourists. In order
to inject some structure into the generated summaries, we assume that we have a set of I well-written
travel guides that correspond to I different cities and have the same structure. We refer to these travel
guides as official travel guides. Each official travel guide consists of a fixed set of S sections such as
restaurants and shopping, and this section structure will be used to organize our generated summaries.
We further assume that each section of an official travel guide consists of a list of points of interest, each
with a name and a short description, as illustrated in Figure 1. We believe that this is a fairly common
structure followed by many if not all travel guides.
Given a target city, we assume that we can collect a set of threads about this city from travel-related
forums. In this paper we use threads from Yahoo! Answers, but our solution does not use any CQA
properties of the threads, so threads from other general forums can also be used. Our goal is to generate
a text summary with S sections from these threads, where each section has a length limit.
1671
As we have mentioned, we treat the problem as a multi-document summarization task. However,
different from standard text summarization, our generated summaries should contain S sections. To
achieve this goal, we first select a set of relevant threads for each section and then perform section-
specific summarization from the selected threads.
Thread selection: To select relevant threads given a section, a naive solution is to rank the threads based
on their relevance to the section, where relevance can be measured by, for example, cosine similarity
between a thread and all the text in the given travel guides belonging to the section. But we observe that
the language used in forum threads could be very different from that in the official travel guides, making
it hard to measure relevance purely based on lexical overlap. For example, in the entertainment section,
forum threads may contain words such as ?djs,? ?Xmas,? ?b?day? and ?anni.?, but these words do not
occur in the official travel guides. To overcome this difficulty, we propose to use a latent variable model
that jointly models official travel guides and forum threads. We treat the S sections as S latent factors
that govern the generation of the forum threads. With the latent factors observed in the official travel
guides, we receive some supervision; and yet by jointly modeling both the official travel guides and the
forum threads, we allow the latent factors to adapt to the lexical variations in user-generated content. In
the end, the learned latent factors can help us align forum threads with the sections and subsequently
select the most relevant ones for each section.
Section-specific summarization: Given the selected relevant threads for a section, we adopt an ILP-
based extractive summarization framework that has been shown to be effective (Gillick and Favre, 2009).
We modify the objective function in this framework to consider two factors: (1) Since not every sentence
in the selected threads is highly relevant to the section, we want to give preference to those more relevant
sentences in the objective function, where relevance can be measured using word distributions learned
by the latent variable model. (2) Since travel guides are expected to recommend points of interest to
readers, we try to maximize the coverage of section-specific location entities in the objective function.
3 Joint City Section Model
3.1 Model
In this section we present our Joint City Section Model (JCSM), which links official travel guides and
forum threads. The model is a typical extension of LDA, where a number of latent topics (i.e. latent
factors) are assumed to have generated the observed text. First of all, for each pre-defined section there
is a latent topic. These explain words such as ?food? and ?menu? for restaurants and ?store? and ?mall?
for shopping. In addition, in both travel guides and forum threads, some words are more related to the
city being discussed than any specific section. For example, when New York City is being discussed,
words such as ?NYC? and ?Manhattan? may frequently show up in any section. We therefore further
assume that for each city there is a city-specific topic. A switch variable is used to determine whether a
word comes from a city-specific or section-specific topic.
A special design of our model that differs from many existing LDA extensions is the treatment of
named entities. We first use a named entity recognizer to identify potential names of locations from
forum threads. We assume that each of these entities belongs to a section, which is indicated by a latent
variable. We then assume that the section labels of the non-entity words in forum threads are dependent
on the section labels of these entities. By doing so, we emphasize the importance of associating potential
points of interest with sections, which will be useful when we generate summaries.
We now formally present JCSM. To simplify the model description, we assume that we work with
I cities, each of which has a given, well-written travel guide and a set of forum threads. Note that in
practice this model can be easily extended such that a target city with forum threads does not need to have
a given travel guide to begin with. Let ?
i
denote the word distribution for the city-specific latent topic
associated with city i. Let ?
s
denote the word distribution for the section-specific latent topic for section
s. Let d
i,s,n
denote the n-th word in the s-th section of the i-th city?s travel guide. Here 1 ? d
i,s,n
? V is
an index into the vocabulary with size V . Let x
i,s,n
be a switch variable associated with d
i,s,n
to indicate
whether this word is city-specific or section-specific. For the j-th forum thread related to the i-th city, we
assume there is a distribution over sections, denoted as ?
i,j
. For the l-th location entity in the k-th post
1672
of this thread, we assume a latent variable c
i,j,k,l
(1 ? c
i,j,k,l
? S) that indicates the section label of this
entity. Then for the m-th word in this post, we first use a switch variable y
i,j,k,m
to determine whether
the word is city-specific or section-specific. If it is section-specific, we then choose one of the entities in
the same post, denoted as z
i,j,k,m
, and its corresponding section label as the section for this word.
All the binary switch variables follow a global Bernoulli distribution parameterized by pi. There are
hyperparameters ?, ?, ?
?
and ? that define the prior distributions. The complete model is depicted in
Figure 1. The generative process of JCSM is also described as follows.
S
N M L
K
J I
 d   w c ?
x y z
?
Figure 1: The plate notation of the Joint City Section Model (JCSM). Dashed variables will be integrated out in Gibbs sampling.
For clarity, the Dirichlet and Beta priors are omitted. The arrow pointing to z indicates that z is drawn from a uniform
distribution over the integers from 1 to L.
? For each city i, (i = 1, 2, ? ? ? , I), draw a city-specific word distribution ?
i
? Dir(?
?
)
? For each section s, (s = 1, 2, ? ? ? , S), draw a section-specific word distribution ?
s
? Dir(?)
? Draw a switch distribution pi ? Beta(?)
? For each city i (i = 1, 2, ? ? ? , I)
? For each section s (s = 1, 2, ? ? ? , S)
? For the n-th word in the given travel guide
- Draw x
i,s,n
? Bernoulli(pi)
- If x
i,s,n
= 1, draw d
i,s,n
? Multi(?
s
); otherwise, draw d
i,s,n
? Multi(?
i
).
? For the j-th thread
? Draw a thread specific section distribution ?
j
? Dir(?)
? For the k-th post
- For the l-th entity, draw c
i,j,k,l
? Multi(?
j
)
- For the m-th word, draw y
i,j,k,m
? Bernoulli(pi). If y
i,j,k,m
= 1, draw z
i,j,k,m
? Uniform(1, ? ? ? , L
i,j,k
)
and then draw w
i,j,k,m
? Multi(?
c
i,j,k,z
i,j,k,m
); otherwise, draw w
i,j,k,m
? Multi(?
i
).
3.2 Inference
We use collapsed Gibbs sampling to estimate the parameters in the model. The problem is to compute
the Gibbs update rules for sampling x
i,s,n
, c
i,j,k,l
, z
i,j,k,m
, y
i,j,k,m
.
Sample entity topic c
i,j,k,l
Let b denote {i, j, k, l} and u denote {i, j, k}. We can derive the Gibbs update rule for sampling entity
topic c
i,j,k,l
as follows:
p(c
b
= s|C
?
b
,W,D,X,Y,Z) =
n
s
i,j,?
b
+ ?
?
S
s
?
=1
n
s
?
i,j,?
b
+ S?
?
?
V
w=1
?
n
w
u,y=1,z=l
i
?
=1
(n
w
y=1,z=l,?
u
+ ? + i
?
? 1)
?
n
w
y=1,z=l,u
j
?
=1
(
?
V
w=1
n
w
y=1,z=l,?
u
+ V ? + j
?
? 1)
,
where n
s
i,j,?
b
denotes the number of entities whose topic assignments are s in thread {i, j} without
consideration of entity {i, j, k, l}. n
w
u,y=1,z=l
denotes the number of times term w occurs in the post
{i, j, k} with the constraint that y = 1 and z = l. n
w
y=1,z=l,?
u
is the number of times term w occurs in
all posts except the post {i, j, k} with the constraint that y = 1 and z = l.
Sample switch label x
i,s,n
We can derive the Gibbs update rule for sampling x
i,s,n
in a similar way. Note that the sampling of
x
i,s,n
is in travel guide word level. Let g denote{i, s, n}, the Gibbs update rule for sampling x
i,s,n
is as
follows:
1673
p(x
g
= 0|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=0
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=0,i,?
g
+ ?
?
?
V
w=1
n
w
x=0,i,?
g
+ V ?
?
p(x
g
= 1|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=1
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=1,s,?
g
+ ?
?
V
w=1
n
w
x=1,s,?
g
+ V ?
Sample post word topic z
i,j,k,m
and switch label y
i,j,k,m
For words in the thread posts, We can derive the Gibbs update rule for sampling post word topic z
i,j,k,m
and switch label y
i,j,k,m
. Note that the sampling of z
i,j,k,m
and y
i,j,k,m
is in post word level. Let f
denote{i, j, k,m}. The Gibbs update rule for sampling z
i,j,k,m
and y
i,j,k,m
is as follows:
p(z
f
= s|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
?
1
L
i,j,k
p(y
f
= 0|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=0
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=0,i,?
f
+ ?
?
?
V
w=1
n
w
y=0,i,?
f
+ V ?
?
p(y
f
= 1|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=1
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
where s
?
= c
i,j,k,l
which is the topic index of the associated entity of this word.
Parameter estimation
After Gibbs Sampling, we can make the following parameter estimation:
?
i,j,s
=
n
s
i,j
+ ?
?
S
s
?
=1
n
s
?
i,j
+ S?
. thread-section distribution.
?
s,w
=
n
w
s,y=1
+ ?
?
V
w
?
=1
n
w
?
s,y=1
+ V ?
. section-word distribution.
?
i,w
=
n
w
i,y=0
+ ?
?
?
V
w
?
=1
n
w
?
i,y=0
+ V ?
?
. city-word distribution.
pi
y
=
n
y
(.)
+ ?
?
1
y
?
=0
n
y
?
(.)
+ 2?
. switch distribution.
4 Generating Section-specific Summaries
With the JCSM model presented in the last section, we can learn a word distribution for each section,
which can help us find more relevant content for the section. For each section, we rank the forum
threads by how likely the words inside a thread is generated from the corresponding section-specific word
distribution. We select the top-K threads for each section to perform section-specific summarization.
Extractive summarization has been well studied and many algorithms have been proposed. We choose
to build our solution on top of an ILP-based framework proposed by Gillick and Favre (2009), partly
because our experiments comparing this ILP framework and other existing methods show its advantage
on our data sets (see Section 5). Below we first briefly review this ILP-based summarization framework
and then present our proposed improvements.
The idea behind the ILP framework by Gillick and Favre (2009) is to maximize the coverage of so-
called ?concepts? from the original corpus in the generated summary. In practice, bigrams are used as
concepts. Specifically, let us use i to index all the concepts from the original corpus. Let w
i
denote
the weight of the i-th concept computed based on its frequency and b
i
? {0, 1} denote the absence or
1674
presence of the concept. The framework aims to maximize
?
i
w
i
b
i
, i.e. the total weighted coverage of
the concepts, subject to the following constraints:
?
j
l
j
s
j
? L, (l
j
is the length of the j-th sentence in terms of words, and L is the length limit of the summary.)
?i, j : s
j
o
i,j
? b
i
, (s
j
? {0, 1} denotes the absence or presence of the j-th sentence.)
?i :
?
j
s
j
o
i,j
? b
i
. (o
i,j
? {0, 1} denotes whether concept i occurs in sentence j.)
Although this framework works well for standard summarization, our task is different. We propose the
following changes to this framework:
Favoring relevant sentences: Recall that although we select presumably the most relevant threads for
each section, we cannot guarantee that each sentence in these threads is related to the section. For
example, we observe that the things-to-do section is often mixed with content from restaurants, sights,
transport and entertainment sections. Also, some sentences are less relevant to the target city than
others. In order to select the more relevant sentences in the summary, we propose to add the second term
in Eqn. 1 below. Here j is used to index all the candidate sentences and u
j
is a weight for sentence j
based on its relevance.
We measure relevance with respect to both the city and the section. Let LL(j, ?) denote the log like-
lihood of generating sentence j from the section-specific topic ? and LL(j, ?) denote the log likelihood
of generating sentence j from the city-specific topic ?. We define u
j
as follows:
u
j
? exp (?LL(j, ?) + (1? ?)LL(j, ?)) .
u
j
are then normalized to be between 0 and 1. Note that here ? is a manually defined parameter used to
control the tradeoff between city-specific relevance and section-specific relevance. As we will show in
Section 5, both relevance factors turn out to be useful.
Covering section-specific points of interest: We hypothesize that a good summary travel guide should
mention potential points of interest to the reader. To this end, the last term in Eqn. 1 is added. Specifically,
k is an index for unique location names we find that have been labeled as belonging to section s according
to the JCSM model. e
k
? {0, 1} denotes whether the k-th entity is present in the selected sentences, and
v
k
denotes the weight for this entity based on its frequency.
Eventually, the summarization task is formulated as the following optimization problem:
Maximize: ?
1
?
i
w
i
b
i
+ ?
2
?
j
u
j
s
j
+ (1? ?
1
? ?
2
)
?
k
v
k
e
k
(1)
Subject to:
?
j
l
j
s
j
? L,
?i :
?
j
s
j
o
i,j
? b
i
, ?i, j : s
j
o
i,j
? b
i
,
?j :
?
k
s
j
p
j,k
? e
k
, ?j, k : s
j
p
j,k
? e
k
.
Here o
i,j
denotes whether concept i occurs in sentence j, and p
j,k
denotes whether entity k occurs in
sentence j. For the weights w
i
and v
k
, we normalize them using the total occurrences of bigrams/entities
to ensure their values are between 0 and 1. We solve the above optimization problem using the IBM
ILOG CPLEX Optimizer
1
.
5 Experiments
5.1 Data and Experimental Setup
We use real data from Yahoo! Answers and Lonely Planet for evaluation. We first crawl the travel guides
for 10 cities from Lonely Planet, where each travel guide has 8 sections. We then crawl the top 60000
Q&A threads ranked by number of posts related to these 10 cities (6000 for each city) from Yahoo!
Answers under the ?travel? category where all questions have been grouped by cities. We filter out
trivial factoid questions using features used by Tomasoni and Huang (2010). We then use the Stanford
1
http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/
1675
Method Singapore Sydney New York City Los Angeles Overall Average
R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4
Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771
Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640
LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894
DivRank 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888
GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553
ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136
Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.1713 0.2086 0.4543 0.1565 0.1945 0.4804
?
0.1715
?
0.2144
?
Table 2: Comparison of the summarization results.
?
means the result is better than others except ILP-BL in the same column
at 5% significance level measured by Wilcoxon signed rank test. Note that only the average scores are tested for statistical
significance based on the 32 summarization tasks in total.
NER tool to recognize named entities in these threads. Since we notice that sometimes entities tagged as
PER are also possible points of interest, we include all entities of LOC, ORG and PER types. In order
to use higher quality threads for evaluation, for each city we pick the top 600 threads that have the most
overlapping points of interest with the Lonely Planet travel guides. On average, each thread contains 5.0
posts and 618.1 words. These 600? 10 threads are used to train the JCSM model.
We need human generated model summaries for evaluation. Since it is too time consuming to ask
human annotators to look through 600 threads and generate structured summaries, we instead opt to
first retrieve the top 30 relevant threads per section per city based on the JCSM results and then ask
human annotators to summarize these 30 threads to generate a section-specific summary. Our summa-
rization method as well as the baselines are also applied to these 30 threads per section per city for fair
comparison. We randomly select 4 cities for human annotation, giving us 8 ? 4 = 32 section-specific
summarization tasks. For each task, we ask four annotators to read all 30 threads and write a summary
as model summaries in our experiments
2
.
We use the following baseline algorithms for comparison: (1) Random, which randomly picks sum-
mary sentences. (2) Centroid (Radev et al., 2004), which selects sentences according to several features
like tfidf, cluster centroid and position. (3) LexRank (Erkan and Radev, 2004b)., which applies a graph-
based algorithm . (4) DivRank (Mei et al., 2010), which employs a time-variant random walk to enhance
diversity. (5) GMDS (Wan, 2008), which incorporates the document-level information and the sentence-
to-document relationship into the ranking process. (6) ILP-BL, which is the method proposed by Gillick
and Favre (2009).
We empirically set Dirichlet hyperparameters ? = 0.5, ? = 0.01, ? = 0.01, ?
?
= 0.1. We run JCSM
with 400 iterations of Gibbs sampling. For the weight parameters in the ILP model, we empirically set
?
1
= 0.7, ?
2
= 0.1, ? = 0.7 after we conduct multiple experiments to determine the best values of them
from 0.1 to 0.9.
5.2 Summarization Results
To compare the summaries generated by our method with those generated by the baselines, we first
compute their ROUGE scores against the human generated model summaries. ROUGE scores have
been widely used for evaluation of summarization systems (Lin and Hovy, 2003). We use the ROUGE
toolkit
3
, which provides multiple kinds of ROUGE metrics including ROUGE-N, ROUGE-L, ROUGE-
W and ROUGE-SU4. In the experiment results we report three ROUGE F-measure scores, namely,
ROUGE-1, ROUGE-2 and ROUGE-SU4. The higher the ROUGE scores, the better a summary is.
In Table 2 we show the summarization results of our method (with the optimal parameter setting) and
the baseline methods. For each city, the scores we show are averaged over the 8 sections. The overall
average scores on the right hand side are averaged over the 4 cities. We have the following findings from
the table: (1) Compared with the other baselines, the ILP-based baseline clearly shows its advantage,
justifying our our design choice of adopting an ILP-based framework as the basis of our method. (2)
Our method performs slightly better than ILP-BL based on the overall scores, but the difference is not
statistically significant.
2
The summary dataset can be found at https://sites.google.com/site/liuyang198908/code-data.
3
http://www.isi.edu/licensed-sw/see/rouge/
1676
section Singapore Sydney New York City Los Angeles
ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method
restaurants 0.3750 0.5417 0.5714 0.7143 0.2500 0.3750 0.1053 0.2105
hotels 0.4091 0.4091 0.0000 0.5000 0.3636 0.5000 0.4500 0.5500
shopping 0.1429 0.5357 0.3750 0.3750 0.1905 0.1905 0.0455 0.1818
sights 0.5000 0.5789 0.3846 0.4615 0.3636 0.6364 0.1143 0.2571
entertainment 0.1304 0.2174 0.2500 0.7500 0.0909 0.2273 0.2500 0.4167
activities 0.4167 0.5833 0.2500 0.2500 0.1250 0.5000 0.2069 0.2759
transport 0.3889 0.5556 0.7500 0.7500 0.6000 0.8000 0.3158 0.7368
things-to-do 0.2105 0.2632 0.2500 0.5500 0.4583 0.5833 0.0000 0.2000
average 0.3217 0.4606 0.3539 0.5439 0.3052 0.4766 0.1860 0.3536
Table 3: Comparison of the recall of named entities of ILP-BL and our method.
Method Our Complete Model ?EC ?SR ?SecRel ?CityRel
R-1 0.4804 0.4520 0.4657 0.4672 0.4796
R-2 0.1715 0.1430 0.1669 0.1652 0.1685
RSU4 0.2144 0.1987 0.2028 0.2039 0.2120
Table 4: Summarization results of the degenerate versions of our method. ??? means removing this component from our
complete method. The table shows the average results over data sets of all cites.
Considering that an importance difference between our method and ILP-BL is our focus on points
of interest, we further compared ILP-BL and our method using a different metric. The objective is to
test the coverage of points of interest in our generated summaries versus the summaries generated by
ILP-BL. To this end, we first identify all the named entities in the model summaries using the Stanford
NER tool. We then check the percentage of these named entities covered in the generated summaries and
report these recall scores in Table 3. We can see that for majority of the 32 section-specific summaries,
our method clearly has a higher recall score than ILP-BL, showing that our method generates summaries
with more potential points of interest.
To further understand whether all the components of our improved ILP method have contributed to
the performance improvement, we compare our overall method with a few degenerate versions of our
method. In each degenerate version, we remove a single component of the objective function. The results
are shown in Table 4, where?EC removes the consideration of entity coverage (i.e. setting ?
1
+?
2
= 1),
?SR removes the consideration of sentence relevance (i.e. setting ?
2
= 0), ?SecRel removes only the
section-specific relevance of the sentences (i.e. setting ? = 0), and ?CityRel removes only the city-
specific relevance of the sentences (i.e. setting ? = 1). We can see that each degenerate version of our
method performs worse than the complete method, which shows that all components of the objective
function are useful. In particular, entity coverage and section-specific relevance seem to be the more
important components.
 0
 0.2
 0.4
 0.6
 0.8
 1 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4
 0.5
 0.6
lambda2
lambda1
 0.445 0.45
 0.455 0.46
 0.465 0.47
 0.475 0.48
 0.485 0.49
 0.495
(a) R-1(?
1
, ?
2
)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
rho
R-1
R-2
RSU4
(b) ?
Figure 2: Summarization performance of our method by varying the value of the parameters ?
1
, ?
2
and ?.
1677
5.3 Analysis of Topic Words
We show some further analysis of our results. To begin with, we analyze the learning results of JCSM.
The top words in city-specific word distributions and section-specific word distributions learnt by JCSM
are presented in Table 5 and Table 6. Generally we observe clean top words for each city and each
section. For each city, city-specific words are those associated with the corresponding city. For example,
for Singapore, we see words such as ?s$? (Singapore dollars), ?sentosa? (an island resort in Singapore),
?orchard? (a boulevard that is the retail and entertainment hub of Singapore) and ?bugis? (a popular
shopping place). For New York City, we see ?square?, ?times? and ?manhattan?. For each section,
section-specific words are those words which frequently appear when people discuss about this section,
such as ?menu?, ?dishes? and ? seafood? for the restaurant section and ?train?, ?bus? and ?station? for
the transport section.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 Topic 10
Singapore SFO Chicago Boston LA NYC Seattle Pairs London Sydney
singapore sf chicago boston beach york downtown paris london sydney
s$ san downtown end hollywood nyc seattle de tube harbour
centre francisco park north los park needle metro underground beach
food gate city downtown angeles central space eiffel central manly
shopping golden neighborhood fenway la square market french centre beaches
sentosa bay north bay downtown times rain la british house
road bart lake harvard drive manhattan place du palace opera
orchard union mile place california broadway pike tower thames quay
chinese wharf loop city miles city center des end australian
mrt muni ave college hills street waterfront rue kensington rocks
bugis square field subway long east area le station bridge
Table 5: Top city specific words discovered by JCSM.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8
restaurants hotels shopping sights entertainment activities transport thingsToDo
food hotel shop museum bar visit train bar
restaurant rooms store city music park bus place
menu free stores park club tour station tour
dishes wi-fi shopping art place fun airport city
place walk shops building night city time food
bar located find built dance walk line art
chicken offers clothes world beer day car day
fish station wear place clubs time walk including
fresh features mall house crowd shopping minutes music
seafood tv place area bars museum hours restaurant
Table 6: Top section specific words discovered by JCSM.
5.4 Parameter Sensitivity Analysis
We further give parameter sensitivity analysis for our proposed method. We show how sensitive our
results are with respect to the parameters ?
1
, ?
2
and ?. We choose the Sydney data set to perform
parameter sensitivity analysis. In Figure 2(a), we show how ROUGE-1 varies with respect to ?
1
and ?
2
.
We can see that the performance fluctuates within a limited range as we vary ?
1
and ?
2
. We find the
trend for ROUGE-2 and ROUGE-SU4 is similar so we leave out the figures for them. In Figure 2(b) we
see that the performance is pretty stable as we vary ?.
5.5 Sample Output and Case Study
Finally, we show a sample travel guide our method generates for Sydney in Table 7. We can see that first
of all the sentences selected by our method have high relevance to the corresponding sections. Second,
through observation we find that humans tend to select sentences containing more points of interest as
summary. Our summary sentences contain many points of interest as highlighted, showing the advantage
of our method.
1678
Sample Summary Sentences Generated from Yahoo! Answers by Our Method for Sydney
Hotel
Sorry can not recommend you a hotels as I have no idea of pricing , but if you want a nice area , check hotels in Bondi and Manly Beaches .
As for the Acer Arena , that is in the Homebush Olympic Park and you can choose to live in either Parramatta or the city .
You need to live in one of the surrounding residential suburbs , close to a train line . Try Alexandria , Newtown , Surry hills for inner suburbs . . . .
Sights
You can walk around the harbor area to the Opera House and you can see the beautiful Harbor Bridge .
All this is apart from the Opera House and the Botanical Gardens . Visit the Custom House Circular Quay and see a model of Sydney. You
must also do a day trip to the Blue Mountains . Harbour Wedding is one of the major attraction in Sydney . . . .
Entertainment
George Street has a number of bars . All the bars around the harbour are really good day and night . If you want to stay in a hotel where there is
entertainment at night , you could look at Woolloomooloo , Darlinghurst , Surry Hills , Kings Cross or Potts Point . Newtown is good for bars .
Get them to see a theatre show or something at the Opera House . . . .
Things-to-do
If you are going out for the day , starting with a walk to the city will be most enjoyable . Take a public ferry from Circular Quay to Darling
Harbour , about 15 minutes across the harbour and under the bridge , when you get to Darling Harbour go and see the Chinese Gardens . There
are lots of interesting things to see and do in and around Sydney . . . .
Activities
They have good markets at the weekend and great views of the Opera House . The Opera House is free to have a look at , if you like art then walk
through the Botanical Gardens and go and see the art gallery . If you ?re feeling brave , you can do a Harbour Bridge walk , though I think it may
be a little pricey . . . .
Table 7: Excerpts from the summary generated from Yahoo! Answers by our method for Sydney. We show summaries for the
5 sections other than the 3 sections shown in Table1. Named entities are highlighted in bold font.
6 Related Work
Multi-document summarization is a process to generate a text summary by reducing documents in size
while retaining the main points of the original documents. It has been extensively studied in the NLP
community, with most efforts on extractive summarization. Our work is also based on extractive sum-
marization. Extractive summarization essentially selects a set of sentences from the original documents
to form a summary.
To select sentences, different features and ranking strategies have been studied. Early work focuses
on finding good features to select summary sentences. Radev et al. (2004) proposed a centroid-based
summarizer which combines several pre-defined features like tfidf, cluster centroid and position to score
sentences. Lin and Hovy (2002) built the NeATS multi-document summarization system using term fre-
quency, sentence position, stigma words and simplified Maximal Marginal Relecvance (MMR). Nenkova
et al. (2006) proved that high-frequency words were significant in reflecting the focus of documents.
Ouyang et al. (2010) studied the influence of different word positions in summarization. Later, graph-
based ranking algorithms have been successfully applied to summarization. LexPageRank (Erkan and
Radev, 2004a) is a representative one based on the PageRank algorithm (Page et al., 1999). Later exten-
sions include ToPageRank (Pei et al., 2012), which incorporates topic information into the propagation
mechanism, the manifold-ranking based method for topic-focused summarization (Wan et al., 2007) and
DivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk to
balance prestige and diversity.
More recently, Integer Linear Programming (ILP) based framework was introduced as a global infer-
ence algorithm for multi-document summarization by McDonald (2007), which considers information
and redundancy at the sentence level. Gillick and Favre (2009) studied information and redundancy at a
sub-sentence, ?concept? level, modeling the value of a summary as a function of the concepts it covers.
In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a re-
gression model to estimate the frequency of bigrams in the reference summary and analyzed the impact
of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed
a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE
gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive
textual overview of subject composed of information drawn from the Internet and applied ILP to opti-
mize both local fit of information into each topic and global coherence across the entire overview. Li
et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend
LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and ILP method
to generate presentation slides for academic papers.
Our work is different from standard ILP-based multi-document summarization. We designed a latent
variable model to first separate the threads to be summarized into sections based on model gravel guides.
1679
We also emphasized the inclusion of potential points of interest in formulating the ILP optimization
problem.
Our work is also closely related to previous work on answer summarization in community-based
QA sites. Previous work on summarizing answers is mainly based on query focused multi-document
summarization techniques to summarize multiple answer documents given a single question. Liu et al.
(2008) proposed a CQA question taxonomy to classify questions in CQA and question-type oriented
answer summarization for better reuse of answers. Tomasoni and Huang (2010) proposed two concept-
scoring functions to combine quality, coverage, relevance and novelty measures for answer summary
in response to a question and showed that their summarized answers constitute a solid complement to
best answers voted by CQA users. Chan et al. (2012) presented an answer summarization method for
complex multi-sentence questions. For our work, we study a new problem of summarizing multiple
threads to automatically generate city travel guides based on known template structure from well-written
travel guides, which is different from the setting of single Q&A thread summarization in the previous
related studies.
7 Conclusion and Future Work
In this paper we proposed a summarization framework to generate well structured supplementary travel
guides from social media based on a latent variable model and integer linear programming. The la-
tent variable model could align forum threads with the section structure of well-written travel guides.
Compared to standard concept based ILP methods, our method additionally tries to cover more named
entities as points of interest and maximizes sentence relevance scores measured by section-specific and
city-specific word distributions learnt by the latent variable model. Extensive experiments with real data
from Yahoo! Answers show that our proposed method is able to generate better summaries compared
with a number of multi-document summarization baselines measured by ROUGE scores.
Currently our generated summaries may have overlap with the well-written model travel guides. In the
future, we plan to improve our method to emphasize the selection of additional information from social
media compared with the model travel guides. We will also look into the problem of how to summarize
information that does not fit into the template structure derived from model travel guides.
Acknowledgments
This work was done during Liu Yang?s visit to Singapore Management University. The authors would
like to thank the reviewers for their valuable comments on this work.
References
Wen Chan, Xiangdong Zhou, Wei Wang, and Tat-Seng Chua. 2012. Community answer summarization for multi-
sentence question with group l1 regularization. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume 1, ACL ?12, pages 582?591, Stroudsburg, PA, USA.
Association for Computational Linguistics.
G?unes Erkan and Dragomir R. Radev. 2004a. Lexpagerank: Prestige in multi-document text summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4 of EMNLP
?04.
G?unes Erkan and Dragomir R. Radev. 2004b. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. J. Artif. Int. Res., 22(1):457?479, December.
Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Langauge Processing, ILP ?09, pages 10?18, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, NAACL ?09, pages 362?370, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1680
Yue Hu and Xiaojun Wan. 2013. Ppsgen: Learning to generate presentation slides for academic papers. In
Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI ?13, pages 2099?2105.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. Generating aspect-oriented multi-document summariza-
tion with event-aspect model. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 1137?1146, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In
Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, ACL ?13, pages
1004?1013, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization: A prototype system and
its evaluation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL
?02, pages 457?464, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, NAACL ?03, pages 71?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on IR Research, ECIR?07, pages 557?564, Berlin, Heidelberg. Springer-
Verlag.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: The interplay of prestige and diversity in informa-
tion networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?10, pages 1009?1018, New York, NY, USA. ACM.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-
document summarizer: Exploring the factors that influence summarization. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ?06, pages
573?580, New York, NY, USA. ACM.
You Ouyang, Wenjie Li, Qin Lu, and Renxian Zhang. 2010. A study on position information in document
summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,
COLING ?10, pages 919?927, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web. Technical Report 1999-66, Stanford InfoLab, November. Previous number = SIDL-WP-
1999-0120.
Yulong Pei, Wenpeng Yin, and Lian?en Huang. 2012. Generic multi-document summarization using topic-oriented
information. In Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelli-
gence, PRICAI?12, pages 435?446, Berlin, Heidelberg. Springer-Verlag.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s, and Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Inf. Process. Manage., 40(6):919?938, November.
Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
208?216, Stroudsburg, PA, USA. Association for Computational Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multi-document
summarization. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI?07,
pages 2903?2908, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Xiaojun Wan. 2008. An exploration of document impact on graph-based multi-document summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages
755?762, Stroudsburg, PA, USA. Association for Computational Linguistics.
1681
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56?65,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid
Wayne Xin Zhao?, Jing Jiang?, Hongfei Yan?, Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University, China
?School of Information Systems, Singapore Management University, Singapore
{zhaoxin,yhf}@net.pku.edu.cn, jingjiang@smu.edu.cn, lxm@pku.edu.cn
Abstract
Discovering and summarizing opinions from
online reviews is an important and challeng-
ing task. A commonly-adopted framework
generates structured review summaries with
aspects and opinions. Recently topic mod-
els have been used to identify meaningful re-
view aspects, but existing topic models do
not identify aspect-specific opinion words. In
this paper, we propose a MaxEnt-LDA hy-
brid model to jointly discover both aspects
and aspect-specific opinion words. We show
that with a relatively small amount of train-
ing data, our model can effectively identify as-
pect and opinion words simultaneously. We
also demonstrate the domain adaptability of
our model.
1 Introduction
With the dramatic growth of opinionated user-
generated content, consumers often turn to online
product reviews to seek advice while companies see
reviews as a valuable source of consumer feedback.
How to automatically understand, extract and sum-
marize the opinions expressed in online reviews has
therefore become an important research topic and
gained much attention in recent years (Pang and Lee,
2008). A wide spectrum of tasks have been studied
under review mining, ranging from coarse-grained
document-level polarity classification (Pang et al,
2002) to fine-grained extraction of opinion expres-
sions and their targets (Wu et al, 2009). In partic-
ular, a general framework of summarizing reviews
of a certain product is to first identify different as-
pects (a.k.a. features) of the given product and then
extract specific opinion expressions for each aspect.
For example, aspects of a restaurant may include
food, staff, ambience and price, and opinion expres-
sions for staff may include friendly, rude, etc. Be-
cause of the practicality of this structured summary
format, it has been adopted in several previous stud-
ies (Hu and Liu, 2004; Popescu and Etzioni, 2005;
Brody and Elhadad, 2010) as well as some commer-
cial systems, e.g. the ?scorecard? feature at Bing
shopping1.
Different approaches have been proposed to iden-
tify aspect words and phrases from reviews. Previ-
ous methods using frequent itemset mining (Hu and
Liu, 2004) or supervised learning (Jin and Ho, 2009;
Jin et al, 2009; Wu et al, 2009) have the limitation
that they do not group semantically related aspect
expressions together. Supervised learning also suf-
fers from its heavy dependence on training data. In
contrast, unsupervised, knowledge-lean topic mod-
eling approach has been shown to be effective in au-
tomatically identifying aspects and their representa-
tive words (Titov and McDonald, 2008; Brody and
Elhadad, 2010). For example, words such as waiter,
waitress, staff and service are grouped into one as-
pect.
We follow this promising direction and extend ex-
isting topic models to jointly identify both aspect
and opinion words, especially aspect-specific opin-
ion words. Current topic models for opinion mining,
which we will review in detail in Section 2, still lack
this ability. But separating aspect and opinion words
can be very useful. Aspect-specific opinion words
can be used to construct a domain-dependent senti-
1http://www.bing.com/shopping
56
ment lexicon and applied to tasks such as sentiment
classification. They can also provide more informa-
tive descriptions of the product or service being re-
viewed. For example, using more specific opinion
words such as cozy and romantic to describe the am-
bience aspect in a review summary is more meaning-
ful than using generic words such as nice and great.
To the best of our knowledge, Brody and Elhadad
(2010) are the first to study aspect-specific opinion
words, but their opinion word detection is performed
outside of topic modeling, and they only consider
adjectives as possible opinion words.
In this paper, we propose a new topic modeling
approach that can automatically separate aspect and
opinion words. A novelty of this model is the inte-
gration of a discriminative maximum entropy (Max-
Ent) component with the standard generative com-
ponent. The MaxEnt component allows us to lever-
age arbitrary features such as POS tags to help sepa-
rate aspect and opinion words. Because the supervi-
sion relies mostly on non-lexical features, although
our model is no longer fully unsupervised, the num-
ber of training sentences needed is relatively small.
Moreover, training data can also come from a differ-
ent domain and yet still remain effective, making our
model highly domain adaptive. Empirical evaluation
on large review data sets shows that our model can
effectively identify both aspects and aspect-specific
opinion words with a small amount of training data.
2 Related Work
Pioneered by the work of Hu and Liu (2004), review
summarization has been an important research topic.
There are usually two major tasks involved, namely,
aspect or feature identification and opinion extrac-
tion. Hu and Liu (2004) applied frequent itemset
mining to identify product features without supervi-
sion, and considered adjectives collocated with fea-
ture words as opinion words. Jin and Ho (2009),
Jin et al (2009) and Wu et al (2009) used super-
vised learning that requires hand-labeled training
sentences to identify both aspects and opinions. A
common limitation of these methods is that they do
not group semantically related aspect expressions to-
gether. Furthermore, supervised learning usually re-
quires a large amount of training data in order to per-
form well and is not easily domain adaptable.
Topic modeling provides an unsupervised and
knowledge-lean approach to opinion mining. Titov
and McDonald (2008) show that global topic models
such as LDA (Blei et al, 2003) may not be suitable
for detecting rateable aspects. They propose multi-
grain topic models for discovering local rateable as-
pects. However, they do not explicitly separate as-
pect and opinion words. Lin and He (2009) propose
a joint topic-sentiment model, but topic words and
sentiment words are still not explicitly separated.
Mei et al (2007) propose to separate topic and sen-
timent words using a positive sentiment model and
a negative sentiment model, but both models cap-
ture general opinion words only. In contrast, we
model aspect-specific opinion words as well as gen-
eral opinion words.
Recently Brody and Elhadad (2010) propose to
detect aspect-specific opinion words in an unsuper-
vised manner. They take a two-step approach by first
detecting aspect words using topic models and then
identifying aspect-specific opinion words using po-
larity propagation. They only consider adjectives as
opinion words, which may potentially miss opinion
words with other POS tags. We try to jointly capture
both aspect and opinion words within topic models,
and we allow non-adjective opinion words.
Another line of related work is about how to in-
corporate useful features into topic models (Zhu and
Xing, 2010; Mimno and McCallum, 2008). Our
MaxEnt-LDA hybrid bears similarity to these recent
models but ours is designed for opinion mining.
3 Model Description
Our model is an extension of LDA (Blei et al, 2003)
but captures both aspect words and opinion words.
To model the aspect words, we use a modified ver-
sion of the multi-grain topic models from (Titov and
McDonald, 2008). Our model is simpler and yet still
produces meaningful aspects. Specifically, we as-
sume that there are T aspects in a given collection of
reviews from the same domain, and each review doc-
ument contains a mixture of aspects. We further as-
sume that each sentence (instead of each word as in
standard LDA) is assigned to a single aspect, which
is often true based on our observation.
To understand how we model the opinion words,
let us first look at two example review sentences
57
from the restaurant domain:
The food was tasty.
The waiter was quite friendly.
We can see that there is a strong association of
tasty with food and similarly of friendly with waiter.
While both tasty and friendly are specific to the
restaurant domain, they are each associated with
only a single aspect, namely food and staff, respec-
tively. Besides these aspect-specific opinion words,
we also see general opinion words such as great
in the sentence ?The food was great!? These gen-
eral opinion words are shared across aspects, as op-
posed to aspect-specific opinion words which are
used most commonly with their corresponding as-
pects. We therefore introduce a general opinion
model and T aspect-specific opinion models to cap-
ture these different opinion words.
3.1 Generative Process
We now describe the generative process of the
model. First, we draw several multinomial word dis-
tributions from a symmetric Dirichlet prior with pa-
rameter ?: a background model ?B, a general aspect
model ?A,g, a general opinion model ?O,g, T as-
pect models {?A,t}Tt=1 and T aspect-specific opin-
ion models {?O,t}Tt=1. All these are multinomial
distributions over the vocabulary, which we assume
has V words. Then for each review document d, we
draw a topic distribution ?d?Dir(?) as in standard
LDA. For each sentence s in document d, we draw
an aspect assignment zd,s?Multi(?d).
Now for each word in sentence s of document d,
we have several choices: The word may describe the
specific aspect (e.g. waiter for the staff aspect), or a
general aspect (e.g. restaurant), or an opinion either
specific to the aspect (e.g. friendly) or generic (e.g.
great), or a commonly used background word (e.g.
know). To distinguish between these choices, we in-
troduce two indicator variable, yd,s,n and ud,s,n, for
the nth word wd,s,n. We draw yd,s,n from a multi-
nomial distribution over {0, 1, 2}, parameterized by
pid,s,n. yd,s,n determines whether wd,s,n is a back-
ground word, aspect word or opinion word. We will
discuss how to set pid,s,n in Section 3.2. We draw
ud,s,n from a Bernoulli distribution over {0, 1} pa-
rameterized by p, which in turn is drawn from a sym-
metric Beta(?). ud,s,n determines whether wd,s,n is
general or aspect-specific. We then draw wd,s,n as
T
?
?
B
?
A,t
?
O,t
?
A,g
?
O,g
D
S
N
d,s
x
d,s,n
pi
d,s,n
y
d,s,n
w
d,s,n
u
d,s,n
z
d,s
?
d
{B,O,A}
? p ?
?
Figure 1: The plate notation of our model.
follows:
wd,s,n ?
?
??????
??????
Multi(?B) if yd,s,n = 0
Multi(?A,zd,s) if yd,s,n = 1, ud,s,n = 0
Multi(?A,g) if yd,s,n = 1, ud,s,n = 1
Multi(?O,zd,s) if yd,s,n = 2, ud,s,n = 0
Multi(?O,g) if yd,s,n = 2, ud,s,n = 1
.
Figure 1 shows our model using the plate notation.
3.2 Setting pi with a Maximum Entropy Model
A simple way to set pid,s,n is to draw it from a
symmetric Dirichlet prior. However, as suggested
in (Mei et al, 2007; Lin and He, 2009), fully un-
supervised topic models are unable to identify opin-
ion words well. An important observation we make
is that aspect words and opinion words usually play
different syntactic roles in a sentence. Aspect words
tend to be nouns while opinion words tend to be ad-
jectives. Their contexts in sentences can also be dif-
ferent. But we do not want to use strict rules to sepa-
rate aspect and opinion words because there are also
exceptions. E.g. verbs such as recommend can also
be opinion words.
In order to use information such as POS tags
to help discriminate between aspect and opinion
words, we propose a novel idea as follows: We set
pid,s,n using a maximum entropy (MaxEnt) model
applied to a feature vector xd,s,n associated with
wd,s,n. xd,s,n can encode any arbitrary features we
think may be discriminative, e.g. previous, current
and next POS tags. Formally, we have
p(yd,s,n = l|xd,s,n) = pid,s,nl =
exp (?l ? xd,s,n
)
?2
l?=0 exp
(?l? ? xd,s,n
) ,
58
where {?l}2l=0 denote the MaxEnt model weights
and can be learned from a set of training sentences
with labeled background, aspect and opinion words.
This MaxEnt-LDA hybrid model is partially in-
spired by (Mimno and McCallum, 2008).
As for the features included in x, currently we
use two types of simple features: (1) lexical features
which include the previous, the current and the next
words {wi?1, wi, wi+1}, and (2) POS tag features
which include the previous, the current and the next
POS tags {POSi?1, POSi, POSi+1}.
3.3 Inference
We use Gibbs sampling to perform model inference.
Due to the space limit, we leave out the derivation
details and only show the sampling formulas. Note
that the MaxEnt component is trained first indepen-
dently of the Gibbs sampling procedure, that is, in
Gibbs sampling, we assume that the ? parameters
are fixed.
We use w to denote all the words we observe in
the collection, x to denote all the feature vectors for
these words, and y, z and u to denote all the hidden
variables. First, given the assignment of all other
hidden variables, to sample a value for zd,s, we use
the following formula:
P (zd,s = t|z?(d,s),y,u,w,x) ?
cd(t) + ?
cd(?) + T?
?
( ?
(
cA,t(?) + V ?
)
?
(
cA,t(?) + nA,t(?) + V ?
) ?
V?
v=1
?
(
cA,t(v) + nA,t(v) + ?
)
?
(
cA,t(v) + ?
)
)
?
( ?
(
cO,t(?) + V ?
)
?
(
cO,t(?) + nO,t(?) + V ?
) ?
V?
v=1
?
(
cO,t(v) + nO,t(v) + ?
)
?
(
cO,t(v) + ?
)
)
.
Here cd(t) is the number of sentences assigned to as-
pect t in document d, and cd(?) is the number of sen-
tences in document d. cA,t(v) is the number of times
word v is assigned as an aspect word to aspect t,
and cO,t(v) is the number of times word v is assigned
as an opinion word to aspect t. cA,t(?) is the total num-
ber of times any word is assigned as an aspect word
to aspect t, and cO,t(?) is the total number of times any
word is assigned as an opinion word to aspect t. All
these counts represented by a c variable exclude sen-
tence s of document d. nA,t(v) is the number of times
word v is assigned as an aspect word to aspect t in
sentence s of document d, and similarly, nO,t(v) is the
number of times word v is assigned as an opinion
word to aspect t in sentence s of document d.
Then, to jointly sample values for yd,s,n and
ud,s,n, we have
P (yd,s,n = 0|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?0 ? xd,s,n)?
l? exp(?l? ? xd,s,n)
?
cB(wd,s,n) + ?
cB(?) + V ?
,
P (yd,s,n = l, ud,s,n = b|z,y?(d,s,n),u?(d,s,n),w,x)
? exp(?l ? xd,s,n)?
l? exp(?l? ? xd,s,n)
? g(wd,s,n, zd,s, l, b),
where the function g(v, t, l, b) (1 ? v ? V, 1 ? t ?
T, l ? {1, 2}, b ? {0, 1}) is defined as follows:
g(v, t, l, b) =
?
??????????
??????????
cA,t(v) +?
cA,t(?) +V ?
? c(0)+?c(?)+2? if l = 1, b = 0
cO,t(v) +?
cO,t(?) +V ?
? c(0)+?c(?)+2? if l = 2, b = 0
cA,g(v) +?
cA,g(?) +V ?
? c(1)+?c(?)+2? if l = 1, b = 1
cO,g(v) +?
cO,g(?) +V ?
? c(1)+?c(?)+2? if l = 2, b = 1.
.
Here the various c variables denote various counts
excluding the nth word in sentence s of document d.
Due to space limit, we do not give full explanation
here.
4 Experiment Setup
To evaluate our MaxEnt-LDA hybrid model for
jointly modeling aspect and opinion words, we used
a restaurant review data set previously used in (Ganu
et al, 2009; Brody and Elhadad, 2010) and a ho-
tel review data set previously used in (Baccianella
et al, 2009). We removed stop words and used the
Stanford POS Tagger2 to tag the two data sets. Only
reviews that have no more than 50 sentences were
used. We also kept another version of the data which
includes the stop words for the purpose of extracting
the contextual features included in x. Some details
of the data sets are given in Table 1.
For our hybrid model, we ran 500 iterations of
Gibbs sampling. Following (Griffiths and Steyvers,
2004), we fixed the Dirichlet priors as follows: ? =
2http://nlp.stanford.edu/software/tagger.shtml
59
data set restaurant hotel
#tokens 1,644,923 1,097,739
#docs 52,574 14,443
Table 1: Some statistics of the data sets.
data set #sentences #tokens
restaurant 46 634
cell phone 125 4414
DVD player 180 3024
Table 2: Some statistics of the labeled training data.
50/T , ? = 0.1 and ? = 0.5. We also experimented
with other settings of these priors and did not notice
any major difference. For MaxEnt training, we tried
three labeled data sets: one that was taken from the
restaurant data set and manually annotated by us3,
and two from the annotated data set used in (Wu et
al., 2009). Note that the latter two were used for test-
ing domain adaptation in Section 6.3. Some details
of the training sets are shown in Table 2.
In our preliminary experiments, we also tried two
variations of our MaxEnt-LDA hybrid model. (1)
The first is a fully unsupervised model where we
used a uniform Dirichlet prior for pi. We found
that this unsupervised model could not separate as-
pect and opinion words well. (2) The second is a
bootstrapping version of the MaxEnt-LDA model
where we used the predicted values of y as pseudo
labels and re-trained the MaxEnt model iteratively.
We found that this bootstrapping procedure did not
boost the overall performance much and even hurt
the performance a little in some cases. Due to the
space limit we do not report these experiments here.
5 Evaluation
In this section we report the evaluation of our
model. We refer to our MaxEnt-LDA hybrid model
as ME-LDA. We also implemented a local version
of the standard LDA method where each sentence
is treated as a document. This is the model used
in (Brody and Elhadad, 2010) to identify aspects,
and we refer to this model as LocLDA.
Food Staff Order Taking Ambience
chocolate service wait room
dessert food waiter dining
cake staff wait tables
cream excellent order bar
ice friendly minutes place
desserts attentive seated decor
coffee extremely waitress scene
tea waiters reservation space
bread slow asked area
cheese outstanding told table
Table 4: Sample aspects of the restaurant domain using
LocLDA. Note that the words in bold are opinion words
which are mixed with aspect words.
5.1 Qualitative Evaluation
For each of the two data sets, we show four sample
aspects identified by ME-LDA in Table 3 and Ta-
ble 5. Because the hotel domain is somehow similar
to the restaurant domain, we used the labeled train-
ing data from the restaurant domain also for the hotel
data set. From the tables we can see that generally
aspect words are quite coherent and meaningful, and
opinion words correspond to aspects very well. For
comparison, we also applied LocLDA to the restau-
rant data set and present the aspects in Table 4. We
can see that ME-LDA and LocLDA give similar as-
pect words. The major difference between these two
models is that ME-LDA can sperate aspect words
and opinion words, which can be very useful. ME-
LDA is also able to separate general opinion words
from aspect-specific ones, giving more informative
opinion expressions for each aspect.
5.2 Evaluation of Aspects Identification
We also quantitatively evaluated the quality of the
automatically identified aspects. Ganu et al (2009)
provide a set of annotated sentences from the restau-
rant data set, in which each sentence has been as-
signed one or more labels from a gold standard label
set S = {Staff, Food, Ambience, Price, Anecdote,
Misc}. To evaluate the quality of our aspect iden-
tification, we chose from the gold standard labels
three major aspects, namely Staff, Food and Ambi-
ence. We did not choose the other aspects because
(1) Price is often mixed with other aspects such as
Food, and (2) Anecdote and Misc do not show clear
3We randomly selected 46 sentences for manual annotation.
60
Food Staff Order Taking Ambience General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
chocolate good service friendly table seated room small good
dessert best staff attentive minutes asked dining nice well
cake great food great wait told tables beautiful nice
cream delicious wait nice waiter waited bar romantic great
ice sweet waiter good reservation waiting place cozy better
desserts hot place excellent order long decor great small
coffee amazing waiters helpful time arrived scene open bad
tea fresh restaurant rude hour rude space warm worth
bread tasted waitress extremely manager sat area feel definitely
cheese excellent waitstaff slow people finally table comfortable special
Table 3: Sample aspects and opinion words of the restaurant domain using ME-LDA.
Service Room Condition Ambience Meal General
Aspect Opinion Aspect Opinion Aspect Opinion Aspect Opinion Opinion
staff helpful room shower room quiet breakfast good great
desk friendly bathroom small floor open coffee fresh good
hotel front bed clean hotel small fruit continental nice
english polite air comfortable noise noisy buffet included well
reception courteous tv hot street nice eggs hot excellent
help pleasant conditioning large view top pastries cold best
service asked water nice night lovely cheese nice small
concierge good rooms safe breakfast hear room great lovely
room excellent beds double room overlooking tea delicious better
restaurant rude bath well terrace beautiful cereal adequate fine
Table 5: Sample aspects and opinion words of the hotel domain using ME-LDA.
patterns in either word usage or writing styles, mak-
ing it even hard for humans to identify them. Brody
and Elhadad (2010) also only used these three as-
pects for quantitative evaluation. To avoid ambigu-
ity, we used only the single-labeled sentences for
evaluation. About 83% of the labeled sentences have
a single label, which confirms our observation that a
sentence usually belongs to a single aspect.
We first ran ME-LDA and LocLDA each to get
an inferred aspect set T . Following (Brody and El-
hadad, 2010), we set the number of aspects to 14
in both models. We then manually mapped each in-
ferred aspect to one of the six gold standard aspects,
i.e., we created a mapping function f(t) : T ? S.
For sentence s of document d, we first assign it to an
inferred aspect as follows:
t? = argmax
t?T
Nd,s?
n=1
logP (wd,s,n|t).
We then assign the gold standard aspect f(t?) to this
Aspect Method Precision Recall F-1
Staff LocLDA 0.804 0.585 0.677
ME-LDA 0.779 0.540 0.638
Food LocLDA 0.898 0.648 0.753
ME-LDA 0.874 0.787 0.828
Ambience LocLDA 0.603 0.677 0.638
ME-LDA 0.773 0.558 0.648
Table 6: Results of aspects identification on restaurant.
sentence. We then calculated the F-1 score of the
three aspects: Staff, Food and Ambience. The re-
sults are shown in Table 6. Generally ME-LDA has
given competitive results compared with LocLDA.
For Food and Ambience ME-LDA outperformed Lo-
cLDA, while for Staff ME-LDA is a little worse
than LocLDA. Note that ME-LDA is not designed
to compete with LocLDA for aspect identification.
61
5.3 Evaluation of Opinion Identification
Since the major advantage of ME-LDA is its abil-
ity to separate aspect and opinion words, we further
quantitatively evaluated the quality of the aspect-
specific opinion words identified by ME-LDA.
Brody and Elhadad (2010) has constructed a gold
standard set of aspect-specific opinion words for the
restaurant data set. In this gold standard set, they
manually judged eight out of the 14 automatically
inferred aspects they had: J = {Ambiance, Staff,
Food-Main Dishes, Atmosphere-Physical, Food-
Baked Goods, Food-General, Drinks, Service}.
Each word is assigned a polarity score ranging from
-2.0 to 2.0 in each aspect. We used their gold stan-
dard words whose polarity scores are not equal to
zero. Because their gold standard only includes
adjectives, we also manually added more opinion
words into the gold standard set. To do so, we took
the top 20 opinion words returned by our method
and two baseline methods, pooled them together,
and manually judged them. We use precision at n
(P@n), a commonly used metric in information re-
trieval, for evaluation. Because top words are more
important in opinion models, we set n to 5, 10 and
20. For both ME-LDA and BL-1 below, we again
manually mapped each automatically inferred aspect
to one of the gold standard aspects.
Since LocLDA does not identify aspect-specific
opinion words, we consider the following two base-
line methods that can identify aspect-specific opin-
ion words:
BL-1: In this baseline, we start with all adjectives
as candidate opinion words, and use mutual infor-
mation (MI) to rank these candidates. Specifically,
given an aspect t, we rank the candidate words ac-
cording to the following scoring function:
ScoreBL-1(w, t) =
?
v?Vt
p(w, v) log p(w, v)p(w)p(v) ,
where Vt is the set of the top-100 frequent aspect
words from ?A,t.
BL-2: In this baseline, we first use LocLDA to learn
a topic distribution for each sentence. We then as-
sign a sentence to the aspect with the largest proba-
bility and hence get sentence clusters. We manually
map these clusters to the eight gold standard aspects.
Finally, for each aspect we rank adjectives by their
Method P@5 P@10 P@20
ME-LDA 0.825?,? 0.700? 0.569?
BL-1 0.400 0.450 0.469
BL-2 0.725 0.650 0.563
Table 7: Average P@n of aspect-specific opinion words
on restaurant. * and ? indicate that the improvement hy-
pothesis is accepted at confidence level 0.9 respectively
for BL-1 and BL-2.
frequencies in the aspect and treat these as aspect-
specific opinion words.
The basic results in terms of the average precision
at n over the eight aspects are shown in Table 7. We
can see that ME-LDA outperformed the two base-
lines consistently. Especially, for P@5, ME-LDA
gave more than 100% relative improvement over
BL-1. The absolute value of 0.825 for P@5 also
indicates that top opinion words discovered by our
model are indeed meaningful.
5.4 Evaluation of the Association between
Opinion Words and Aspects
The evaluation in the previous section shows that our
model returns good opinion words for each aspect.
It does not, however, directly judge how aspect-
specific those opinion words are. This is because the
gold standard created by (Brody and Elhadad, 2010)
also includes general opinion words. E.g. friendly
and good may both be judged to be opinion words
for the staff aspect, but the former is more specific
than the latter. We suspect that BL-2 has comparable
performance with ME-LDA for this reason. So we
further evaluated the association between opinion
words and aspects by directly looking at how easy
it is to infer the corresponding aspect by only look-
ing at an aspect-specific opinion word. We selected
four aspects for evaluation: Ambiance, Staff, Food-
Main Dishes and Atmosphere-Physical . We chose
these four aspects because they are quite different
from each other and thus manual judgments on these
four aspects can be more objective. For each aspect,
similar to the pooling strategy in IR, we pooled the
top 20 opinion words identified by BL-1, BL-2 and
ME-LDA. We then asked two human assessors to
assign an association score to each of these words
as follows: If the word is closely associated with an
aspect, a score of 2 is given; if it is marginally as-
62
Metrics Dataset BL-2 ME-LDA
nDCG@5 Restaurant 0.647 0.764
Hotel 0.782 0.820
nDCG@10 Restaurant 0.781 0.897
Hotel 0.722 0.789
Table 8: Average nDCG performance of BL-2 and ME-
LDA. Because only four aspects were used for evaluation,
we did not perform statistical significance test. We found
that in all cases ME-LDA outperformed BL-2 for either
all aspects or three out of four aspects.
sociated with an aspect, a score of 1 is given; other-
wise, 0 is given. We calculated the Kappa statistics
of agreement, and we got a quite high Kappa value
of 0.8375 and 0.7875 respectively for the restaurant
data set and the hotel data set. Then for each word
in an aspect, we took the average of the scores of
the two assessors. We used an nDCG-like metric to
compare the performance of our model and of BL-2.
The metric is defined as follows:
nDCG@k(t,M) =
?k
i=1
Score(Mt,i)
log2(i+1)
iDCG@k(t) ,
where Mt,i is the ith aspect-specific opinion word
inferred by method M for aspect t, Score(Mt,i) is
the association score of this word, and iDCG@k(t)
is the score of the ideal DCG measure at k for as-
pect t, that is, the maximum DCG score assuming
an ideal ranking. We chose k = 5 and k = 10. The
average nDCG over the four aspects are presented
in Table 8. We can see that ME-LDA outperformed
BL-2 quite a lot for the restaurant data set, which
conforms to our hypothesis that ME-LDA generates
aspect-specific opinion words of stronger associa-
tion with aspects. For the hotel data set, ME-LDA
outperformed a little. This may be due to the fact
that we used the restaurant training data for the ho-
tel data set.
6 Further Analysis of MaxEnt
In this section, we perform some further evaluation
and analysis of the MaxEnt component in our model.
6.1 Feature Selection
Previous studies have shown that simple POS fea-
tures and lexical features can be very effective for
discovering aspect words and opinion words (Hu
Methods Average F-1
LocLDA 0.690
ME-LDA + A 0.631
ME-LDA + B 0.695
ME-LDA + C 0.705
Table 9: Comparison of the average F-1 using different
feature sets for aspect identification on restaurant.
and Liu, 2004; Jin et al, 2009; Wu et al, 2009;
Brody and Elhadad, 2010). for POS features, since
we observe that aspect words tend to be nouns while
opinion words tend to be adjectives but sometimes
also verbs or other part-of-speeches, we can expect
that POS features should be quite useful. As for lexi-
cal features, words from a sentiment lexicon can also
be helpful in discovering opinion words.
However, lexical features are more diverse so pre-
sumably we need more training data in order to de-
tect useful lexical features. Lexical features are also
more domain-dependent. On the other hand, we hy-
pothesize that POS features are more effective when
the amount of training data is small and/or the train-
ing data comes from a different domain. We there-
fore compare the following three sets of features:
? A: wi?1, wi, wi+1
? B: POSi?1, POSi, POSi+1
? C: A+ B
We show the comparison of the performance in Ta-
ble 9 using the average F-1 score defined in Sec-
tion 5.2 for aspect identification, and in Table 10 us-
ing the average P@n measure defined in Section 5.3
for opinion identification. We can see that Set B
plays the most important part, which conforms to
our hypothesis that POS features are very important
in opinion mining. In addition, we can see that Set C
performs a bit better than Set B, which indicates that
some lexical features (e.g., general opinion words)
may also be helpful. Note that here the training data
is from the same domain as the test data, and there-
fore lexical features are likely to be useful.
6.2 Examine the Size of Labeled Data
As we have seen, POS features play the major role
in discriminating between aspect and opinion words.
Because there are much fewer POS features than
word features, we expect that we do not need many
63
Methods P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + A 0.150 0.200 0.231
ME-LDA + B 0.775 0.688 0.569
ME-LDA + C 0.825 0.700 0.569
Table 10: Comparison of the average P@n using different
feature sets for opinion identification on restaurant.
Method F-1
LocalLDA 0.690
ME-LDA + 10 0.629
ME-LDA + 20 0.692
ME-LDA + 30 0.691
ME-LDA + 40 0.726
ME-LDA + 46 0.705
Table 11: Average F-1 with differen sizes of training data
on restaurant.
labeled sentences to learn the POS-based patterns.
We now examine the sensitivity of the performance
with respect to the amount of labeled data. We gen-
erated four smaller training data sets with 10, 20, 30
and 40 sentences each from the whole training data
set we have, which consists of 46 labeled sentences.
The results are shown in Table 11 and Table 12. We
can see that generally the performance stays above
BL when the number of training sentences is 20 or
more. This indicates that our model needs only a
relatively small number of high-quality training sen-
tences to achieve good results.
6.3 Domain Adaption
Since we find that the MaxEnt supervision relies
more on POS features than lexical features, we also
hypothesize that if the training sentences come from
a different domain the performance can still remain
relatively high. To test this hypothesis, we tried two
Method P@5 P@10 P@20
BL-2 0.725 0.650 0.563
ME-LDA + 10 0.700 0.563 0.488
ME-LDA + 20 0.875 0.650 0.600
ME-LDA + 30 0.825 0.700 0.569
ME-LDA + 40 0.825 0.688 0.581
ME-LDA + 46 0.825 0.700 0.569
Table 12: Average P@n of aspect-specific opinion words
with differen sizes of training data on restaurant.
Method Average F-1
restaurant + B 0.695
restaurant + C 0.705
cell phone + B 0.662
cell phone + C 0.629
DVD player + B 0.686
DVD player + C 0.635
Table 13: Average F-1 performance for domain adaption
on restaurant.
Method P@5 P@10 P@20
restaurant + B 0.775 0.688 0.569
restaurant + C 0.825 0.700 0.569
cell phone + B 0.775 0.675 0.588
cell phone + C 0.750 0.688 0.594
DVD player + B 0.775 0.713 0.575
DVD player + C 0.825 0.663 0.588
Table 14: Average P@n of aspect-specific opinion words
for domain adaption on restaurant.
quite different training data sets, one from the cell
phone domain and the other from the DVD player
domain, both used in (Wu et al, 2009).
We consider two feature sets defined in Sec-
tion 6.1 for domain adaption, namely B and C. The
results are shown in Table 13 and Table 14.
For aspect identification, using out-of-domain
training data performed worse than using in-domain
training data, but the absolute performance is still
decent. And interestingly, we can see that using B
is better than using C, indicating that lexical features
may hurt the performance in the cross-domain set-
ting. It suggests that lexical features are not easily
adaptable across domains for aspect identification.
For opinion identification, we can see that there
is no clear difference between using out-of-domain
training data and using in-domain training data,
which may indicate that our opinion identification
component is robust in domain adaption. Also, we
cannot easily tell whetherB has advantage over C for
opinion identification. One possible reason may be
that those general opinion words are useful across
domains, so lexical features may still be useful for
domain adaption.
64
7 Conclusions
In this paper, we presented a topic modeling ap-
proach that can jointly identify aspect and opinion
words, using a MaxEnt-LDA hybrid. We showed
that by incorporating a supervised, discriminative
maximum entropy model into an unsupervised, gen-
erative topic model, we could leverage syntactic fea-
tures to help separate aspect and opinion words.
We evaluated our model on two large review data
sets from the restaurant and the hotel domains. We
found that our model was competitive in identifying
meaningful aspects compared with previous mod-
els. Most importantly, our model was able to iden-
tify meaningful opinion words strongly associated
with different aspects. We also demonstrated that
the model could perform well with a relatively small
amount of training data or with training data from a
different domain.
Our model provides a principled way to jointly
model both aspects and opinions. One of the future
directions we plan to explore is to use this model
to help sentence-level extraction of specific opinions
and their targets, which previously was only tackled
in a fully supervised manner. Another direction is to
extend the model to support polarity classification.
ACKNOWLEDGMENT
The authors Xin Zhao, Hongfei Yan and Xiaom-
ing Li are partially supported by NSFC under the
grant No. 70903008 and 60933004, CNGI grant No.
2008-122, 863 Program No. 2009AA01Z143, and
the Open Fund of the State Key Laboratory of Soft-
ware Development Environment under Grant No.
SKLSDE-2010KF-03, Beihang University.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews. In
Proceedings of the 31st ECIR.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
Proceedings of Human Language Technologies: The
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In Proceedings of the 12th
International Workshop on the Web and Databases.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
HMM-based learning framework for web opinion min-
ing. In Proceedings of the 26th International Confer-
ence on Machine Learning.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
OpinionMiner: A novel machine learning system for
web opinion mining and extraction. In Proceedings of
the 15th ACM SIGKDD.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Proceed-
ing of the Eighteenth ACM Conference on Information
and Knowledge Management.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
Modeling facets and opinions in weblogs. In Proceed-
ings of the 16th International Conference on World
Wide Web.
David Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. In Conference on
Uncertainty in Artificial Intelligence.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of the HLT-EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Jun Zhu and Eric P. Xing. 2010. Conditional topic ran-
dom fields. In Proceedings of the 27th International
Conference on Machine Learning.
65
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804?813,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Linking Entities to a Knowledge Base with Query Expansion
Swapna Gottipati
School of Information Systems
Singapore Management University
Singapore
swapnag.2010@smu.edu.sg
Jing Jiang
School of Information Systems
Singapore Management University
Singapore
jingjiang@smu.edu.sg
Abstract
In this paper we present a novel approach
to entity linking based on a statistical lan-
guage model-based information retrieval with
query expansion. We use both local con-
texts and global world knowledge to expand
query language models. We place a strong
emphasis on named entities in the local con-
texts and explore a positional language model
to weigh them differently based on their dis-
tances to the query. Our experiments on
the TAC-KBP 2010 data show that incor-
porating such contextual information indeed
aids in disambiguating the named entities and
consistently improves the entity linking per-
formance. Compared with the official re-
sults from KBP 2010 participants, our system
shows competitive performance.
1 Introduction
When people read news articles, Web pages and
other documents online, they may encounter named
entities which they are not familiar with and there-
fore would like to look them up in an encyclope-
dia. It would be very useful if these entities could be
automatically linked to their corresponding encyclo-
pedic entries. This task of linking mentions of enti-
ties within specific contexts to their corresponding
entries in an existing knowledge base is called en-
tity linking and has been proposed and studied in the
Knowledge Base Population (KBP) track of the Text
Analysis Conference (TAC) (McNamee and Dang,
2009). Besides improving an online surfer?s brows-
ing experience, entity linking also has potential us-
age in many other applications such as normalizing
entity mentions for information extraction.
The major challenge of entity linking is to resolve
name ambiguities. There are generally two types of
ambiguities: (1) Polysemy: This type of ambigu-
ities refers to the case when more than one entity
shares the same name. E.g. George Bush may re-
fer to the 41st President of the U.S., the 43rd Presi-
dent of the U.S., or any other individual who has the
same name. Clearly polysemous names cause diffi-
culties for entity linking. (2) Synonymy: This type
of ambiguities refers to the case when more than
one name variation refers to the same entity. E.g.
Metro-Goldwyn-Mayer Inc. is often abbreviated as
MGM. Synonymy affects entity linking when the en-
tity mention in the document uses a name variation
not covered in the entity?s knowledge base entry.
Intuitively, to disambiguate a polysemous entity
name, we should make use of the context in which
the name occurs, and to address synonymy, exter-
nal world knowledge is usually needed to expand
acronyms or find other name variations. Indeed
both strategies have been explored in existing litera-
ture (Zhang et al, 2010; Dredze et al, 2010; Zheng
et al, 2010). However, most existing work uses
supervised learning approaches that require careful
feature engineering and a large amount of training
data. In this paper, we take a simpler unsupervised
approach using statistical language model-based in-
formation retrieval. We use the KL-divergence re-
trieval model (Zhai and Lafferty, 2001) and ex-
pand the query language models by considering both
the local contexts within the query documents and
global world knowledge obtained from the Web.
804
Symbol Description
Q Query
DQ Query document
NQ Query name string
E KB entity node
NE KB entity name string
DE KB entity disambiguation text
SQ Set of alternate query name strings
N l,iQ Local alternative name strings
NgQ Global alternative name strings
EQ Candidate KB entries for Q
?Q Query Language Model
?LQ KB entry language model using local context from DQ
?GQ KB entry language model using global knowledge
?L+GQ KB entry language model using local context and global knowledge
?NE KB entry language model with named entities only
?NE+DE KB entry language model with named entities and disambiguation text
Table 1: Notation
We evaluate our retrieval method with query ex-
pansion on the 2010 TAC-KBP data set. We find that
our expanded query language models can indeed
improve the performance significantly, demonstrat-
ing the effectiveness of our principled and yet sim-
ple techniques. Comparison with the official results
from KBP participants also shows that our system is
competitive. In particular, when no disambiguation
text from the knowledge base is used, our system can
achieve an overall 85.2% accuracy and 9.3% relative
improvement over the best performance reported in
KBP 2010.
2 Task Definition and System Overview
Following TAC-KBP (Ji et al, 2010), we define the
entity linking task as follows. First, we assume
the existence of a Knowledge Base (KB) of enti-
ties. Each KB entry E represents a unique entity
and has three fields: (1) a name string NE , which
can be regarded as the official name of the entity,
(2) an entity type TE , which is one of {PER, ORG,
GPE, UNKNOWN}, and (3) some disambiguation
text DE . Given a query Q which consists of a query
name string NQ and a query document DQ where
the name occurs, the task is to return a single KB
entry to which the query name string refers or Nil if
there is no such KB entry.
It is fairly natural to address entity linking by
ranking the KB entries given a query. In this section
we present an overview of our system, which con-
sists of two major stages: a candidate selection stage
to identify a set of candidate KB entries through
name matching, and a ranking stage to link the query
entity to the most likely KB entry. In both stages,
we consider the query?s local context in the query
document and world knowledge obtained from the
Web. It is important to note that the selection stage
is based on string matching where the order of the
word matters. It is different from the ranking stage
where a probabilistic retrieval model based on bag-
of-word representation is used. Our preliminary ex-
periments demonstrate that without the first candi-
date selection stage the linking process results in low
performance.
2.1 Selecting Candidate KB Entries
The first stage of our system aims to filter out irrel-
evant KB entries and select only a set of candidates
that are potentially the correct match to the query.
Intuitively, we determine whether two entities are
the same by comparing their name strings. We there-
fore need to compare the query name stringNQ with
the name string NE of each KB entry. However,
because of the name ambiguity problem, we cannot
expect the correct KB entry to always have exactly
the same name string as the query. To address this
problem, we use a set of alternative name strings ex-
panded from NQ and select KB entries whose name
805
strings match at least one of them. These alterna-
tive name strings come from two sources: the query
document DQ and the Web.
First, we observe that some useful alternative
name strings come from the query document. For
example, a PER query name string may contain only
a person?s last name but the query document con-
tains the person?s full name, which is clearly a less
ambiguous name string to use. Similarly, a GPE
query name string may contain only the name of a
city or town but the query document contains the
state or province, which also helps disambiguate the
query entity. Based on this observation, we do the
following. Given query Q, let SQ denote the set of
alternative query name strings. Initially SQ contains
only NQ. We then use an off-the-shelf NER tagger
to identify named entities from the query document
DQ. For PER and ORG queries, we select named
entities in DQ that contain NQ as a substring. For
GPE queries, we select named entities that are of the
type GPE, and we then combine each of them with
NQ. We denote these alternative name strings as
{N l,iQ }
KQ
i=1, where l indicates that these name strings
come locally fromDQ andKQ is the total number of
such name strings. {N l,iQ } are added to SQ. Figure
1 and Figure 2 show two example queries together
with their SQ.
Sometimes alternative name strings have to come
from external knowledge. For example, one of the
queries we have contains the name string ?AMPAS,?
and the query document also uses only this acronym
to refer to this entity. But the full name of the entity,
?Academy of Motion Pictures Arts and Sciences,? is
needed in order to locate the correct KB entry. To
tackle this problem, we leverage Wikipedia to find
the most likely official name. Given query name
string NQ, we check whether the following link ex-
ists: http://en.wikipedia.org/NQ. If NQ
is an abbreviation, Wikipedia will redirect the link
to the Wikipedia page of the corresponding entity
with its official name. So if the link exists, we use
the title of the Wikipedia page as another alternative
name string for NQ. We refer to this name string as
NgQ to indicate that it is a global name variant. NgQ is
also added to SQ. Figure 2 shows such an example.
For each name stringN in SQ, we find KB entries
whose name strings match N . We take the union of
Query name string (NQ): Mobile
Query document (DQ): The site is near Mount Ver-
non in the Calvert community on the Tombigbee River,
some 25 miles (40 kilometers) north of Mobile. It?s on
a river route to the Gulf of Mexico and near Mobile?s
rails and interstates. Along with tax breaks and $400
million (euro297 million) in financial incentives, Al-
abama offered a site with a route to a Brazil plant that
will provide slabs for processing in Mobile.
Alternative Query Strings (SQ):
from local context: Mobile, Mobile Mount Vernon,
Mobile Calvert, Mobile River, Mobile Mexico, Mobile
Alabama, Mobile Brazil
Figure 1: An example GPE query from TAC 2010.
Query name string (NQ): Coppola
Query document (DQ): I had no idea of all these
semi-obscure connections, felicia! Alex Greenwald
and Claire Oswalt aren?t names I?m at all familiar
with, but Jason Schwartzman I?ve heard of. Isn?t he
Sophia Coppola?s cousin? I think I once saw a pic-
ture of him sometime ago
Alternative Query Strings (SQ):
from local context: Coppola, Sophia Coppola, Sofia
Coppola
from world knowledge(Wikipedia): Sofia Coppola
Figure 2: An example PER query from TAC 2010.
these sets of KB entries and refer to it as EQ. These
are the candidate KB entries for query Q.
2.2 Ranking KB Entries
Given the candidate KB entries EQ, we need to
decide which one of them is the correct match.
We adopt the widely-used KL-divergence retrieval
model, a statistical language model-based retrieval
method proposed by Lafferty and Zhai (2001).
Given a KB entry E and query Q, we score E based
on the KL-divergence defined below:
s(E,Q) = ?Div(?Q??E) = ?
?
w?V
p(w|?Q) log
p(w|?Q)
p(w|?E)
.
(1)
Here ?Q and ?E are the query language model and
the KB entry language model, respectively. A lan-
guage model here is a multinomial distribution over
words (i.e. a unigram language model). V is the
vocabulary and w is a single word.
To estimate ?E , we follow the standard maxi-
mum likelihood estimation with Dirichlet smooth-
806
ing (Zhai and Lafferty, 2004):
p(w|?E) =
c(w,E) + ?p(w|?C)
|E| + ? , (2)
where c(w,E) is the count of w in E, |E| is the
number of words in E, ?C is a background lan-
guage model estimated from the whole KB, and ?
is the Dirichlet prior. Recall that E contains NE , TE
and DE . We consider using either NE only or both
NE and DE to obtain c(w,E) and |E|. We refer
to the former estimated ?E as ?NE and the latter as
?NE+DE .
To estimate ?Q, typically we can use the empirical
query word distribution:
p(w|?Q) =
c(w,NQ)
|NQ|
, (3)
where c(w,NQ) is the count of w in NQ and |NQ|
is the length of NQ. We call this model the original
query language model.
After ranking the candidate KB entries in EQ us-
ing Equation (1), we perform entity linking as fol-
lows. First, using an NER tagger, we determine the
entity type of the query name string NQ. Let TQ de-
note this entity type. We then pick the top-ranked
KB entry whose score is higher than a threshold ?
and whose TE is the same as TQ. The system links
the query entity to this KB entry. If no such entry
exists, the system returns Nil.
3 Query Expansion
We have shown in Section 2.1 that using the origi-
nal query name string NQ itself may not be enough
to obtain the correct KB entry, and additional words
from both the query document and external knowl-
edge can be useful. However, in the KB entry se-
lection stage, these additional words are only used
to enlarge the set of candidate KB entries; they have
not been used to rank KB entries. In this section, we
discuss how to expand the query language model ?Q
with these additional words in a principled way in
order to rank KB entries based on how likely they
match the query entity.
3.1 Using Local Contexts
Let us look at the example from Figure 2 again.
During the KB entry ranking stage, if we use ?Q
estimated from NQ, which contains only the word
?Coppola,? the retrieval function is unlikely to rank
the correct KB entry on the top. But if we include
the contextual word ?Sophia? from the query doc-
ument when estimating the query language model,
KL-divergence retrieval model is likely to rank the
correct KB entry on the top. This idea of using
contextual words to expand the query is very sim-
ilar to (pseudo) relevance feedback in information
retrieval. We can treat the query document DQ as
our only feedback document.
Many different (pseudo) relevance feedback
methods have been proposed. Here we apply the
relevance model (Lavrenko and Croft, 2001), which
has been shown to be effective and robust in a re-
cent comparative study (Lv and Zhai, 2009). We
first briefly review the relevance model. Given a set
of (pseudo) relevant documents Dr, where for each
D ? Dr there is a document language model ?D,
we can estimate a feedback language model ?fbQ as
follows:
p(w|?fbQ) ?
?
D?Dr
p(w|?D)p(?D)p(Q|?D). (4)
For our problem, since we have only a single feed-
back document DQ, the equation above can be sim-
plified. In fact, in this case the feedback language
model is the same as the document language model
of the only feedback document, i.e. ?DQ .
We then linearly interpolate the feedback lan-
guage model with the original query language model
to form an expanded query language model:
p(w|?LQ) = ?p(w|?Q) + (1 ? ?)p(w|?DQ), (5)
where ? is a parameter between 0 and 1, to control
the amount of feedback. The larger ? is, the less we
rely on the local context. L indicates that the query
expansion comes from local context. This ?LQ can
then replace ?Q in Equation (1) to rank KB entries.
Special Treatment of Named Entities
Usually the document language model ?DQ is es-
timated using the entire text from DQ. For entity
linking, we suspect that named entities surrounding
the query name string in DQ are particularly useful
for disambiguation and thus should be emphasized
over other words. This can be done by weighting
807
NE and non-NE words differently. In the extreme
case, we can use only NEs to estimate the document
language model ?DQ as follows:
p(w|?DQ) =
1
KQ
KQ?
i=1
c(w,N l,iQ )
|N l,iQ |
, (6)
where {N l,iQ } are defined in Section 2.
Positional Model
Another observation is that words closer to the
query name string in the query document are likely
to be more important than words farther away. Intu-
itively, we can use the distance between a word and
the query name string to help weigh the word. Here
we apply a recently proposed positional pseudo rel-
evance feedback method (Lv and Zhai, 2010). The
document language model ?DQ now has the follow-
ing form:
p(w|?DQ) =
1
KQ
KQ?
i=1
f(pi, q) ?
c(w,N l,iQ )
|N l,iQ |
, (7)
where pi and q are the absolute positions of N l,iQ
and NQ in DQ. The function f is Gaussian function
defined as follows:
f(p, q) = 1?
2pi?2
exp
(?(p? q)2
2?2
)
. (8)
where variance ? controls the spread of the curve.
3.2 Using Global World Knowledge
Similar to the way we incorporate words from DQ
into the query language model, we can also con-
struct a feedback language model using the most
likely official name of the query entity obtained from
Wikipedia. Specifically, we define
p(w|?NgQ) =
c(w,NgQ)
|NgQ|
. (9)
We can then linearly interpolate ?NgQ with the orig-inal query language model ?Q to form an expanded
query language model ?GQ:
p(w|?GQ) = ?p(w|?Q) + (1 ? ?)p(w|?NgQ). (10)
Here G indicates that the query expansion comes
from global world knowledge.
Entity Type %Nil %non-Nil
GPE 32.8% 67.2 %
ORG 59.5% 40.5 %
PER 71.7% 28.3 %
Table 2: Percentages of Nil and non-Nil queries.
3.3 Combining Local Context and World
Knowledge
We can further combine the two kinds of additional
words into the query language model as follows:
p(w|?L+GQ ) = ?p(w|?Q) + (1 ? ?)
(
?p(w|?DQ)
+(1 ? ?)p(w|?NgQ)
)
. (11)
Note that here we have two parameters ? and ? to
control the amount of contributions from the local
context and from global world knowledge.
4 Experiments
4.1 Experimental Setup
Data Set: We evaluate our system on the TAC-KBP
2010 data set (Ji et al, 2010). The knowledge base
was constructed from Wikipedia with 818,741 en-
tries. The data set contains 2250 queries and query
documents come from news wire and Web pages.
Around 45% of the queries have non-Nil entries in
the KB. Some statistics of the queries are shown in
Table 2.
Tools: In our experiments, to extract named entities
withinDQ and to determine TQ, we use the Stanford
NER tagger1. An example output of the NER tagger
is shown below:
<PERSON>Hugh Jackman<PERSON> is
Jacked!!
This piece of text comes from a query document
where the query name string is ?Jackman.? We can
see that the NER tagger can help locate the full name
of the person.
We use the Lemur/Indri2 search engine for re-
trieval. It implements the KL-divergence retrieval
model as well as many other useful functionalities.
Evaluation Metric: We adopt the Micro-averaged
accuracy metric, which is the mean accuracy over
all queries. It was used in TAC-KBP 2010 (Ji et
1http://nlp.stanford.edu/software/CRF-NER.shtml
2http://www.lemurproject.org/indri.php
808
al., 2010) as the official metric to evaluate the per-
formance of entity linking. This metric is simply
defined as the percentage of queries that have been
correctly linked.
Methods to Compare: Recall that our system con-
sists of a KB entry selection stage and a KB entry
ranking stage. At the selection stage, a set SQ of
alternative name strings are used to select candidate
KB entries. We first define a few settings where dif-
ferent alternative name string sets are used to select
candidate KB entries:
? Q represents the baseline setting which uses
only the original query name string NQ to se-
lect candidate KB entries.
? Q+L represents the setting where alternative
name strings obtained from the query docu-
ment DQ are combined with NQ to select can-
didate KB entries.
? Q+G represents the setting where the alterna-
tive name string obtained from Wikipedia is
combined with NQ to select candidate KB en-
tries.
? Q+L+G represents the setting as we described
in Section 2.1, that is, alternative name strings
from both DQ and Wikipedia are used together
with NQ to select candidate KB entries.
After selecting candidate KB entries, in the KB
entry ranking stage, we have four options for the
query language model and two options for the KB
entry language model. For the query language
model, we have (1) ?Q, the original query language
model, (2) ?LQ, an expanded query language model
using local context from DQ, (3) ?GQ, an expanded
query language model using global world knowl-
edge, and (4) ?L+GQ , an expanded query language
model using both local context and global world
knowledge. For the KB entry language model, we
can choose whether or not to use the KB disam-
biguation text DE and obtain ?NE and ?NE+DE , re-
spectively.
4.2 Results and Discussion
First, we compare the performance of KB entry se-
lection stage for all four settings on non-Nil queries.
The performance measure recall is defined as
recall =
{
1, if E that refers to Q, exists in EQ
0, otherwise
The recall statistics in Table 3 shows that, Q+L+G
has the highest recall of the KB candidate entries.
Method Recall(%)
Q 67.1
Q+L 89.7
Q+G 94.9
Q+L+G 98.2
Table 3: Comparing the effect of candidate entry selec-
tion using different methods - KB entry selection stage
recall.
Before examining the effect of query expansion
in ranking, we now compare the effect of using dif-
ferent sets of alternative query name strings in the
candidate KB entry selection stage. For this set of
experiments, we fix the query language model to ?Q
and the KB entry language model to ?NE in the rank-
ing stage.
Table 4 shows the performance of all the settings
in terms of micro-averaged accuracy. The results
shown in Tables 4, 5 and 6 are based on the opti-
mum parameter settings. We can see that in terms
of the overall performance, both Q+L and Q+G give
better performance than Q with a 7.7% and a 9.9%
relative improvement, respectively. Q+L+G gives
the best performance with a 12.8% relative improve-
ment over Q. If we further zoom into the results, we
see that for ORG and PER queries, when no correct
KB entry exists (i.e. the Nil case), the performance
of Q, Q+L, Q+G and Q+L+G is very close, indicat-
ing that the additional alternative query name strings
do not help. It shows that the alternative query name
strings are most useful for queries that do have their
correct entries in the KB.
We now further analyze the impact of the ex-
panded query language models ?LQ, ?GQ and ?L+GQ .
We first analyze the results without using the KB
disambiguation text, i.e. using ?NE . Table 5 shows
the comparison between ?Q and other expanded
query language models in terms of micro-averaged
accuracy. The results reveal that the expanded query
language models can indeed improve the overall per-
formance (the both Nil and non-Nil case) under all
settings. This shows the effectiveness of using the
principled query expansion technique coupled with
KL-divergence retrieval model to rank KB entries.
809
All Nil Non-Nil
Method ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789
Q+L 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399
Q+G 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291
Q+L+G 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338
Table 4: Comparing the performance of using different sets of query name strings for candidate KB entry selection.
?Q and ?NE are used in KB entry ranking.
All Nil Non-Nil
Method QueryModel ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q+L ?Q 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399?LQ 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493
Q+G ?Q 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291?GQ 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653
Q+L+G ?Q 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338?L+GQ 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357
Table 5: Comparison between the performance of ?Q and expanded query language models in terms of micro average
accuracy. ?NE was used in ranking.
On the other hand, again we observe that the ef-
fects on the Nil and the non-Nil queries are differ-
ent. While in Table 4 the alternative name strings
do not affect the performance much for Nil queries,
now the expanded query language models actually
hurt the performance for Nil queries. It is not sur-
prising to see this result. When we expand the query
language model, we can possibly introduce noise,
especially when we use the external knowledge ob-
tained from Wikipedia, which largely depends on
what Wikipedia considers to be the most popular
official name of a query name string. With noisy
terms in the expanded query language model we in-
crease the chance to link the query to a KB entry
which is not the correct match. The challenge is that
we do not know when additional terms in the ex-
panded query language model are noise and when
they are not, because for non-Nil queries we do ob-
serve a substantial amount of improvement brought
by query expansion, especially with external world
knowledge. We will further investigate this research
question in the future.
We now further study the impact of using the KB
disambiguation text associated with each entry to es-
timate the KB entry language model used in the KL-
divergence ranking function. The results are shown
in Table 6 for all the methods on ?NE vs. ?NE+DE
using the expanded query language models. We can
see that for all methods the impact of using the KB
disambiguation text is very minimal and is observed
only for GPE and ORG queries. Table 7 shows an
example of the KL-divergence scores for a query,
Mobile whose context is previously shown in the
Figure 1. Without the KB disambiguation text both
the KB entry Mobile Alabama and the entry Mobile
River are given the same score, resulting in inaccu-
rate linking in the ?NE case. But with ?NE+DE , Mo-
bile Alabama was scored higher, resulting in an ac-
curate linking. However, we observe that such cases
are very rare in the TAC 2010 query list and thus the
overall improvement observed is minimal.
KB Entry KB Name w/o text w/ text
E0583976 Mobile Alabama -6.28514 -6.3839
E0183287 Mobile River -6.28514 -6.69372
Table 7: The KL-divergence scores of KB entities for the
query Mobile.
Finally, we compare our performance with the
highest scores from TAC-KBP 2010 as shown in the
Table 8. It is important to note that the highest TAC
results shown in the table under each setting are not
necessarily obtained by the same team. We can see
that our overall performance when KB text is used is
competitive compared with the highest TAC score,
and is substantially higher than the TAC score when
KB text is not used. Lehmann et al (2010) achieved
highest TAC scores. They used a variety of evidence
from Wikipedia like disambiguation pages, anchors,
expanded acronyms and redirects to build a rich fea-
ture set. But as we discussed, building a rich fea-
810
All Nil Non-Nil
Method KB Text ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q ?NE 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789?NE+DE 0.6888 0.5607 0.6533 0.8495 0.8618 0.9888 0.9963 0.4135 0.1612 0.4789
Q+L ?NE 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493?NE+DE 0.7707 0.7904 0.6533 0.8682 0.9390 0.9888 0.9944 0.7177 0.1612 0.5493
Q+G ?NE 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653?NE+DE 0.8222 0.7450 0.7827 0.9387 0.8902 0.9372 0.9814 0.6740 0.5559 0.8310
Q+L+G ?NE 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357?NE+DE 0.8524 0.8291 0.7880 0.9401 0.8740 0.9372 0.9814 0.8072 0.5691 0.8357
Table 6: Comparing the performance using KB text and without using KB text for all methods using expanded query
models in terms of micro average accuracy on 2250 queries. ?NE+DE represents method using KB text and ?NE
represents methods without using KB text.
ture set is an expensive task. Their overall accu-
racy is 1.5% higher than our model. Table 8 shows
that the performance of ORG entities is lower when
compared with the TAC results when we used KB
text. In our analysis, we observed that, even though
some entities like AMPAS are linked correctly, the
entities like CCC (Consolidated Contractors Com-
pany) failed due to ambiguity in the title. Here, we
may benefit by leveraging more global knowledge,
i.e, we should expand the NgQ with Wikipedia global
context entities together with the title to fully benefit
from global knowledge. In particular, when KB text
is not used, our system outperforms the highest TAC
results for all three types of queries.
From the analysis by Ji et al (2010), overall the
participating teams generally performed the best on
PER queries and the worst on GPE queries. With our
system, we can achieve good performance on GPE
queries.
KB Text Usage Type Our System TAC Highest
?NE+DE
All 0.8524 0.8680
GPE 0.8291 0.7957
ORG 0.7880 0.8520
PER 0.9401 0.9601
?NE
All 0.8516 0.7791
GPE 0.8278 0.7076
ORG 0.7867 0.7333
PER 0.9401 0.9001
Table 8: Comparison of the best configuration of our sys-
tem (Q+L+Gwith ?L+GQ ) with the TAC-KBP 2010 resultsin terms of micro-averaged accuracy. ?NE+DE represents
the method using KB disambiguation text and ?NE repre-
sents the method without using KB disambiguation text.
4.3 Parameter Sensitivity
In all our experiments, we set the Dirichlet prior ? to
2500 following previous studies. For the threshold ?
we empirically set it to -12.0 in all the experiments
based on preliminary results. Recall that all the ex-
panded query language models also have a control
parameters ?. The local context-based models ?LQ
and ?L+GQ have an additional parameter ? which
controls the proximity weighing. The ?L+GQ model
has another additional parameter ? that controls the
balance between local context and world knowledge.
In this subsection, we study the sensitivity of these
parameters. We plot the sensitivity graphs for all the
methods that involve ? (? set to 0.5) in Figure 3. As
we can see, all the curves appear to be stable and
?=0.4 appears to work well.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8
Mi
cro
 A
ve
rag
ed
 Ac
cu
rac
y
alpha
Q+LQ+GQ+L+G
Figure 3: Sensitivity of ? in regard to micro-averaged
accuracy.
Similarly, we set ?=0.4 and examine how ? af-
fects micro averaged accuracy. We plot the sensi-
tivity curve for ? for the Q+L+G setting with ?L+GQ
in Figure 5. As we can see, the best performance
is achieved when ?=0.5. This implies that the local
811
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 0.85
 0.86
 0  0.2  0.4  0.6  0.8  1
Mi
cro
 A
ve
rag
ed
 Ac
cu
rac
y
beta
Q+L+G
Figure 4: Sensitivity of ? in regard to micro-averaged
accuracy.
context and the global world knowledge are weighed
equally for aiding disambiguation and improving the
entity linking performance.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 40  60  80  100  120
Mi
cro
 A
ve
rag
ed
 Ac
cu
rac
y
sigma
Q+LQ+L+G
Figure 5: Sensitivity of ? with respect to micro-averaged
accuracy.
Furthermore, we systematically test a fixed set of
? values from 25 to 125 with an intervals of 25 and
examine how ? affects micro averaged accuracy. We
set ?=0.4 and ?=0.5, which is the best parameter
setting as discussed above. We plot the sensitivity
curves for the parameter ? for methods that utilize
the local context, i.e. ?LQ and ?L+GQ , in Figure 5. We
observe that all the curves are stable and 75 <= ?
<= 100 appears to work well. We set ?=100 for all
our experiments. Moreover, after 100, the graph be-
comes stable, which indicates that proximity has less
impact on the method from this point on. This im-
plies that an equal weighing scheme actually would
work the same for these experiments. Part of the
reason may be that by using only named entities in
the context rather than all words, we have effectively
picked the most useful contextual terms. Therefore,
positional feedback models do have exhibit much
benefit for our problem.
5 Related Work
Bunescu and Pasca (2006) and Cucerzan (2007) ex-
plored the entity linking task using Vector Space
Models for ranking. They took a classification ap-
proach together with the novel idea of exploiting
Wikipedia knowledge. In their pioneering work,
they used Wikipedia?s category information for en-
tity disambiguation. They show that using differ-
ent background knowledge, we can find efficient ap-
proaches for disambiguation. In their work, they
took an assumption that every entity has a KB en-
try and thus the NIL entries are not handled.
Similar to other researchers, Zhang et al (2010)
took an approach of classification and used a two-
stage approach for entity liking. They proposed a
supervised model with SVM ranking to filter out the
candidates and deal with disambiguation effectively.
For entity diambiguation they used the contextual
comparisons between the Wikipedia article and the
KB article. However, their work ignores the possi-
bilities of acronyms in the entities. Also, the am-
biguous geo-political names are not handled in their
work.
Dredze et al (2010) took the approach that large
number of entities will be unlinkable, as there is
a probability that the relevant KB entry is unavail-
able. Their algorithm for learning NIL has shown
very good results. But their proposal for handling
the alias name or stage name via multiple lists is not
scalable. Unlike their approach, we use the global
knowledge to handle the stage names and thus this
gives an optimized solution to handle alias names.
Similarly, for acronyms we use the global knowl-
edge that aids unabbreviating and thus entity dis-
ambiguation. Similar to other approaches, Zheng
et al (2010) took a learning to rank approach and
compared list-wise rank model to the pair-wise rank
model. They achieved good results on the list-wise
ranking approach. They handled acronyms and dis-
ambiguity through wiki redirect pages and the an-
chor texts which is similar to others ideas.
Challenges in supervised learning includes care-
ful feature selection. The features can be selected in
ad hoc manner - similarity based or semantic based.
Also machine learning approach induces challenges
of handling heterogenous cases. Unlike their ma-
chine learning approach which requires careful fea-
812
ture engineering and heterogenous training data, our
method is simple as we use simple similarity mea-
sures. At the same time, we propose a statistical
language modeling approach to the linking prob-
lem. Many researchers have proposed efficient ideas
in their works. We integrated some of their ideas
like world knowledge with our new techniques to
achieve efficient entity linking accuracy.
6 Conclusions
In this paper we proposed a novel approach to entity
linking based on statistical language model-based
information retrieval with query expansion using the
local context from the query document as well as
world knowledge from the Web. Our model is a sim-
ple unsupervised one that follows principled exist-
ing information retrieval techniques. And yet it per-
forms the entity linking task effectively compared
with the best performance achieved in the TAC-KBP
2010 evaluation.
Currently our model does not exploit world
knowledge from the Web completely. World knowl-
edge, especially obtained from Wikipedia, has
shown to be useful in previous studies. As our future
work, we plan to explore how to further incorporate
such world knowledge into our model in a principled
way.
References
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceesings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9?16, Trento, Italy.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 708?716.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 277?285.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010
knowledge base population track. In Proceedings of
the Third Text Analysis Conference.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 111?119.
Victor Lavrenko and W. Bruce Croft. 2001. Rele-
vance based language models. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 120?127.
John Lehmann, Sean Monahan, Luke Nezda, Arnold
Jung, and Ying Shi. 2010. Lcc approaches to knowl-
edge base population at tac 2010. In Proceedings TAC
2010 Workshop. TAC 2010.
Yuanhua Lv and ChengXiang Zhai. 2009. A compar-
ative study of methods for estimating query language
models with pseudo feedback. In Proceeding of the
18th ACM Conference on Information and Knowledge
Management, pages 1895?1898.
Yuanhua Lv and ChengXiang Zhai. 2010. Positional rel-
evance model for pseudo-relevance feedback. In Pro-
ceeding of the 33rd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 579?586.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference.
ChengXiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proceedings of the 10th Inter-
national Conference on Information and Knowledge
Management, pages 403?410.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Transactions on Information
Systems, 22(2):179?214, April.
Wei Zhang, Jian Su, Chew Lim Tan, andWen TingWang.
2010. Entity linking leveraging automatically gener-
ated annotation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 1290?1298.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483?491.
813
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 814?824,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Information Extraction with Distributional Prior Knowledge
Cane Wing-ki Leung1, Jing Jiang1, Kian Ming A. Chai2, Hai Leong Chieu2, Loo-Nin Teow2
1School of Information Systems, Singapore Management University, Singapore
2DSO National Laboratories, Singapore
{caneleung,jingjiang}@smu.edu.sg, {ckianmin,chaileon,tloonin}@dso.org.sg
Abstract
We address the task of automatic discovery of
information extraction template from a given
text collection. Our approach clusters candi-
date slot fillers to identify meaningful tem-
plate slots. We propose a generative model
that incorporates distributional prior knowl-
edge to help distribute candidates in a docu-
ment into appropriate slots. Empirical results
suggest that the proposed prior can bring sub-
stantial improvements to our task as compared
to a K-means baseline and a Gaussian mixture
model baseline. Specifically, the proposed
prior has shown to be effective when coupled
with discriminative features of the candidates.
1 Introduction
Information extraction (IE) is the task of extract-
ing information from natural language texts to fill a
database record following a structure called a tem-
plate. Such templates are usually defined based
on the domain of interest. For example, the do-
main in the Sixth Message Understanding Confer-
ence (MUC-6, 1995) is management succession, and
the pre-defined template consists of the slots posi-
tion, the person leaving, the person joining, and the
organization.
Previous research on IE often requires the pre-
definition of templates. Template construction is
usually done manually by domain experts, and an-
notated documents are often created to facilitate su-
pervised learning approaches to IE. However, both
manual template construction and data annotation
are labor-intensive. More importantly, templates and
annotated data usually cannot be re-used in new do-
mains due to domain dependency. It is therefore nat-
ural to consider the problem of unsupervised tem-
plate induction and information extraction. This is
the topic of this paper.
There have been a few previous attempts to ad-
dress the unsupervised IE problem (Shinyama and
Sekine, 2006; Sekine, 2006; Rosenfeld and Feld-
man, 2006; Filatova et al, 2006). These approaches
have a commonality: they try to cluster candidate
slot fillers, which are often nouns and noun phrases,
into slots of the template to be constructed. How-
ever, most of them have neglected the following im-
portant observation: a single document or text seg-
ment tends to cover different slots rather than re-
dundantly fill the same slot. In other words, during
clustering, candidates within the same text segment
should be more likely to be distributed into different
clusters.
In this paper, we propose a generative model that
incorporates this distributional prior knowledge. We
define a prior distribution over the possible label
assignments in a document or a text segment such
that a more diversified label assignment is preferred.
This prior is based on the Poisson distribution. We
also compare a number of generative models for
generating slot fillers and find that the Gaussian mix-
ture model is the best. We then combine the Poisson-
based label assignment prior with the Gaussian mix-
ture model to perform slot clustering. We find that
compared with a K-means baseline and a Gaussian
mixture model baseline, our combined model with
the proposed label assignment prior substantially
performs better on two of the three data sets we use
for evaluation. We further analyze the results on the
third data set and find that the proposed prior will
have little effect if there are no good discriminative
features to begin with. In summary, we find that
814
our Poisson-based label assignment prior is effective
when coupled with good discriminative features.
2 Related Work
One common approach to unsupervised IE is based
on automatic IE pattern acquisition on a cluster of
similar documents. For instance, Sudo et al (2003)
and Sekine (2006) proposed different methods for
automatic IE pattern acquisition for a given domain
based on frequent subtree discovery in dependency
parse trees. These methods leveraged heavily on the
entity types of candidates when assigning them to
template slots. As a consequence, potentially dif-
ferent semantic roles of candidates having the same
entity type could become indistinguishable (Sudo et
al., 2003; Sekine, 2006). This problem is alleviated
in our work by exploiting distributional prior knowl-
edge about template slots, which is shown effective
when coupled with discriminative features of can-
didates. Filatova et al (2006) also considered fre-
quent subtrees in dependency parse trees, but their
goal was to build templates around verbs that are
statistically important in a given domain. Our work,
in contrast, is not constrained to verb-centric tem-
plates. We aim to identify salient slots in the given
domain by clustering.
Marx et al (2002) proposed the cross-component
clustering algorithm for unsupervised IE. Their al-
gorithm assigned a candidate from a document to
a cluster based on the candidate?s feature similarity
with candidates from other documents only. In other
words, the algorithm did not consider a candidate?s
relationships with other candidates in the same doc-
ument. Our work is based on a different perspec-
tive: we model label assignments for all candidates
in the same document with a distributional prior that
prefers a document to cover more distinct slots. We
show empirically that this prior improves slot clus-
tering results greatly in some cases.
Also related to our work is open domain IE, which
aims to perform unsupervised relation extraction.
TEXTRUNNER (Banko et al, 2007), for example,
automatically extracts all possible relations between
pairs of noun phrases from a given corpus. The main
difference between open domain IE and our work
is that open domain IE does not aim to induce do-
main templates, whereas we focus on a single do-
main with the goal of inducing a template that de-
scribes salient information structure of that domain.
Furthermore, TEXTRUNNER and related studies on
unsupervised relation extraction often rely on highly
redundant information on the Web or in large cor-
pus (Hasegawa et al, 2004; Rosenfeld and Feldman,
2006; Yan et al, 2009), which is not assumed in our
study.
We propose a generative model with a distribu-
tional prior for the unsupervised IE task, where
slot fillers correspond to observations in the model,
and their labels correspond to hidden variables we
want to learn. In the machine learning literature,
researchers have explored the use of similar prior
knowledge in the form of constraints through model
expectation. For example, Grac?a et al (2007) pro-
posed to place constraints on the posterior proba-
bilities of hidden variables in a generative model,
while Druck et al (2008) studied a similar problem
in a discriminative, semi-supervised setting. These
studies model constraints as features, and enforce
the constraints through expected feature values. In
contrast, we place constraints on label assignments
through a probabilistic prior on the distribution of
slots. The proposed prior is simple and easy to inter-
pret in a generative model. Nevertheless, it will be
interesting to explore how the proposed prior can be
implemented within the posterior constraint frame-
work.
3 Problem Overview
In this section, we first formally define our unsuper-
vised IE problem. We then provide an overview of
our solution, which is based on a generative model.
3.1 Problem Definition
We assume a collection of documents or short text
segments from a certain domain. These documents
or text segments describe different events or enti-
ties, but they are about the same topic or aspect of
the domain. Examples of such collections include
a collection of sentences describing the educational
background of famous scientists and a collection of
aviation incident reports. Our task is to automati-
cally discover an IE template from this collection.
The discovered template should contain a set of slots
that play different semantic roles in the domain.
815
Input text:
Topic: Graduate Student Seminar Lunch
Dates: 13-Apr-95
Time: 12:00 PM - 1:30 PM
PostedBy: Edmund J. Delaney on 5-Apr-95 at 16:24 from andrew.cmu.edu
Abstract:
The last Graduate Student Seminar Series lunch will be held on Thursday, April 13 from noon-1:30 p.m. in room
207, Student Activities Center. Professor Sara Kiesler of SDS will speak on Carving A Successful Research Niche.
Output:
Slot Slot Filler(s)
Slot 1 (start time) 12:00PM
Slot 2 (end time) 1:30PM, 1:30 p.m.
Slot 3 (location) room 207, Student Activities Center
Slot 4 (speaker) Professor Sara Kiesler
Slot 4 (irrelevant information) Edmund J. Delaney
Figure 1: An input text from a seminar announcement collection and the discovered IE template. Note that the slots
are automatically discovered and the slot names are manually assigned.
To construct such a template, we start with identi-
fying candidate slot fillers, hereafter referred to as
candidates, from the input text. Then we cluster
these candidates with the aim that each cluster will
represent a semantically meaningful slot. Figure 1
gives an example of an input text from a collection
of seminar announcements and the resulting tem-
plate discovered from the collection. As we can see,
the template contains some semantically meaningful
slots such as the start time, end time, location and
speaker of a seminar. Moreover, it also contains a
slot that covers an irrelevant candidate. We call such
slots covering irrelevant candidates garbage slots.
We can make two observations on the mapping
from candidates to template slots from real data,
such as the text in Figure 1. Firstly, a template
slot may be filled by more than one candidate from
a single document, although this number has been
observed to be small. For example, the template
slot end time in Figure 1 has two slot fillers: ?1:30
PM? from the semi-structured header and ?1:30
p.m.? from within the abstract. Secondly, a docu-
ment tends to contain candidates that cover different
template slots. We believe that this observation is a
consequence of the fact that a document will tend to
convey as much information as possible. We further
exploit these observations in Section 4.
3.2 A General Solution
Recall that our general solution to the unsupervised
IE problem is to cluster candidate slot fillers in order
to identify meaningful slots. We leave the details of
how to extract the candidates to Section 7.1. In this
section, we assume that we have a set of candidates
x = {xi,j}, where xi,j is the j-th candidate from
the i-th document in the collection. We cluster these
candidates intoK groups for a givenK.
Let yi,j ? {1, . . . ,K} denote the cluster label for
xi,j and y denote the set of all the yi,j?s. Let xi and
yi denote the sets of all the xi,j?s and the yi,j?s in the
i-th document respectively. We assume a generative
model for x and y as follows. For the i-th document
in our collection, we assume that the number of can-
didates is known and we draw a label assignment yi
according to some distribution parameterized by ?.
Then for the j-th candidate, we generate xi,j from
yi,j according to a generative model parameterized
by ?. Since the labels y are hidden, the observed
log-likelihood of the parameters given the observa-
tions x is
L(?,?) = log p(x; ?,?)
=
?
i
log
?
yi
p(xi,yi; ?,?)
=
?
i
log
?
yi
p(yi; ?)
?
j
p(xi,j |yi,j ; ?). (1)
816
D
ni
K
(a) A multinomial prior.
D
ni
KK-1K-1
(b) The proposed Poisson-based
prior.
Figure 2: Generative models with different label assign-
ment priors. D denotes the number of documents in the
given collection, ni denotes the number of candidates in
the i-th document, andK is the number of slots (clusters).
For a given functional form of p(yi; ?) and
p(xi,j |yi,j ; ?), the best model parameters can be es-
timated by maximizing Eq. (1). In the next section,
we detail two designs of the prior p(yi; ?), followed
by different generative models for the distribution
p(xi,j |yi,j ; ?) in Section 5. Then we describe the
estimation of model parameters in Section 6.
4 Label Assignment Prior
The label assignment prior, p(yi; ?), models the
generation of labels for candidates in a document.
In this section, we first describe a commonly used
multinomial prior, and then introduce the proposed
Poisson-based prior for the unsupervised IE task.
4.1 A Multinomial Prior
Usually, one would assume that the labels for
the different candidates in the same document
are generated independently, that is, p(yi; ?) =?
j p(yi,j ; ?). Under this model, we assume that
each yi,j is generated from a multinomial distribu-
tion parameterized by?, where ?y denotes the prob-
ability of generating label y. Our objective function
in Eq. (1) then becomes:
L(?,?) = log p(x; ?,?)
=
?
i,j
log
?
y
?yp(xi,j |y; ?). (2)
Figure 2(a) depicts a generative model with this
multinomial prior in plate notation. Note that the in-
dependence assumption on label assignment in this
model does not capture our observation that candi-
dates in a document are likely to cover different se-
mantic roles.
4.2 The Proposed Poisson-based Prior
We propose a prior distribution that favors more
diverse label assignments. Our proposal takes
into consideration the following three observations.
Firstly, candidates in the same document are likely
to cover different semantic roles. The proposed prior
distribution should therefore assign higher probabil-
ity to a label assignment that covers more distinct
slots. Secondly, the same piece of information is not
likely to be repeated many times in a document. Our
design thus allows a slot to generate multiple fillers
in a document, up to a limited number of times.
Thirdly, there may exist candidates that do not be-
long to slots in the extracted template. Therefore, we
introduce a dummy slot or garbage slot to the label
set to collect such candidates. Yet, we shall not as-
sume any prior/domain knowledge about candidates
generated by the garbage slot as they are essentially
irrelevant in the given domain.
We now detail the prior that exploits the above
observations. First, we fix the K-th slot (or cluster)
in the label set to be the garbage slot. For each of
the non-garbage slot k = 1, . . . ,K ? 1, we also fix
the maximum number of fillers that can be gener-
ated, which we denote by ?k. There is no ?K for the
garbage slot because the number of fillers is not con-
strained for this slot. This allows all candidates in a
document to be generated by the garbage slot. Let
ni be the number of candidates in the i-th document.
Given K, {?k}K?1k=1 and ni, the set of possible labelassignments for the i-th document can be generated.
We illustrate this with an example. Let K = 2 and
?1 = 1. The label set is {1, 2}, where 2 represents
the garbage slot. Let the number of candidates be
ni = 2. The possible label assignments within this
setting are (1, 2), (2, 1) and (2, 2).
The set of possible label assignments for the i-
th document is the sample space on which we place
the prior distribution p(yi; ?). We need a prior that
gives a higher probability to a more diverse label
assignment. For a given yi for the i-th document,
let ni,k be the number of candidates in the docu-
ment that have been assigned to slot k. That is,
ni,k def=
?ni
j=1 1(yi,j = k), where 1(?) is the indica-
tor variable. We propose the following distribution
based on the Poisson distribution:
817
p(yi; ?) def= Z?1i
K?1?
k=1
Poisson(ni,k; ?k), (3)
where Zi is the normalizing constant, and ?k is the
mean parameter of the k-th Poisson distribution,
k = 1, . . .K ? 1. The absence of a factor that
depends on ni,K reflects the lack of prior knowl-
edge on the number of garbage slot fillers. Fig-
ure 2(b) depicts the proposed generative model with
the Poisson-based prior in plate notation.
5 Generating Slot Fillers
Different existing generative models can be used to
model the generation of a slot filler given a label, that
is, p(x|y; ?). We explore four of them for our task,
namely, the naive Bayes model, the Bernoulli mix-
ture model, the Gaussian mixture model, and a lo-
cally normalized logistic regression model proposed
by Berg-Kirkpatrick et al (2010).
5.1 Multinomial Naive Bayes
In the multinomial naive Bayes model, features of
an observation x are assumed to be independent and
each generated from a multinomial distribution. We
first introduce some notations. Let f denote a fea-
ture (e.g. entity type) and Vf denote the set of possi-
ble values for f . Let xf ? Vf be the value of feature
f in x (e.g. person). For a given label y, feature f
follows a multinomial distribution parameterized by
?y,f , where ?y,f,v denotes the probability of feature
f taking the value v ? Vf given label y. The func-
tional form of the conditional probability of x given
a label y is then
p(x|y; ?) =
?
f
p(xf |y; ?) =
?
f
?y,f,xf . (4)
5.2 Bernoulli Mixture Model
In the naive Bayes model our features are defined
to be categorical. For the Bernoulli mixture model,
as well as the Gaussian mixture model and the lo-
cally normalized logistic regression model in the
next subsections, we first convert each observation
x into a binary feature vector x ? {0, 1}F where F
is the number of binary features. An example of a
binary features is ?the entity type is person?.
We assume that, for a given label y, observations
are generated from a multivariate Bernoulli distribu-
tion parameterized by?y,f , where?y,f,v denotes the
probability of feature f taking the value v ? {0, 1}
given label y. The conditional probability of x given
y can then be written as
p(x|y; ?) =
?
f
p(xf = 1|y; ?)xf ? p(xf = 0|y; ?)1?xf
=
?
f
?y,f,xf . (5)
5.3 Gaussian Mixture Model
In the Gaussian mixture model, we assume that a
given label y generates observations with a mul-
tivariate Gaussian distribution N (?y,?y), where
?y ? RF is the mean and ?y ? RF?F is the co-
variance matrix of the Gaussian. If we assume that
the different feature dimensions are independent and
have the same variance, that is, ?y = ?2yI , where I
is the identity matrix, then the conditional density of
x given y is
p(x|y; ?) = 1(2??2y)F/2
exp
(
??x? ?y?
2
2?2y
)
. (6)
5.4 Locally Normalized Logistic Regression
Berg-Kirkpatrick et al (2010) proposed a method
for incorporating features into generative models for
unsupervised learning. Their method models the
generation of x given y as a logistic function param-
eterized by a weight vector wy, defined as follows:
p(x|y; ?) = exp?x,wy??
x? exp?x?,wy?
. (7)
?x,w? denotes the inner product between x and w.
The denominator considers all data points x? in the
data set, thus Eq. (7) gives a probability distribution
over data points for a given y.
818
6 Parameter Estimation
We can apply the Expectation-Maximization (EM)
algorithm (Dempster et al, 1977) to maximize
the log-likelihood functions under both multinomial
prior in Eq. (2) and the proposed Poisson-based prior
in Eq. (1). For the multinomial prior, there are stan-
dard closed form solutions for the naive Bayes, the
Bernoulli mixture and the Gaussian mixture models.
For locally normalized logistic regression, model
parameters can also be learned via EM, but with
a gradient-based M-step (Berg-Kirkpatrick et al,
2010). We leave out the details here and focus on pa-
rameter estimation in the proposed generative model
with the Poisson-based prior.
We assume that in the Poisson-based prior, the
parameters {?k}K?1k=1 and {?k}K?1k=1 are fixed ratherthan learned in this work. For the distribution
p(x|y; ?), let ?(t?1) and ?(t) denote parameter es-
timates from two consecutive EM iterations. At the
t-th iteration, the E-step updates the responsibilities
of each label assignment yi for each document:
?i,yi = p(yi|xi; ?,?(t?1))
= p(yi; ?)p(xi|yi; ?
(t?1))?
y?i p(y
?
i; ?)p(xi|y?i; ?(t?1))
, (8)
where ?i is a distribution over all possible label as-
signments yi?s for the i-th document. The M-step
updates the estimates of ?(t) based on the current
values of ?i?s and ?(t?1). This is done by maximiz-
ing the following objective function:
?
i
?
yi
?i,yi log
(
p(yi; ?)
?
j
p(xi,j |yi,j ; ?(t?1))
)
. (9)
The exact formulas used in the M-step for
updating ? depend on the functional form of
p(xi,j |yi,j ; ?). As an example, we give the formulas
for the Gaussian mixture model, in which? contains
the set of means {?(t)k }Kk=1 and variances {?(t)k }Kk=1.Taking the derivatives of Eq. (9) with respect to ?k
and to ?k, and then setting the derivations to zero,
we can solve for ?k and for ?k to get:
?(t)k =
?
i
?
yi ?i,yi
?
j 1(yi,j = k)xi,j?
i
?
yi ?i,yi
?
j 1(yi,j = k)
, (10)
?(t)k =
?
i
?
yi ?i,yi
?
j 1(yi,j = k)||xi,j ? ?
(t)
k ||2
F?i
?
yi ?i,yi
?
j 1(yi,j = k)
, (11)
where 1(?) is the indicator variable. We skip the
derivations here due to space limit.
Closed form solutions also exist for the naive
Bayes and the Bernoulli mixture models. For lo-
cally normalized logistic regression, parameters can
be learned with a gradient-based M-step as in the
multinomial prior setting. Existing optimization al-
gorithms, such as L-BFGS, can be used for optimiz-
ing model parameters in the M-step as discussed in
(Berg-Kirkpatrick et al, 2010).
7 Experiments
In this section, we first describe the data sets we used
in our experiments, detailing the target slots and can-
didates in each data set, as well as features we ex-
tract for the candidates. We then describe our evalu-
ation metrics, followed by experimental results.
7.1 Data Sets
We use three data sets for evaluating our unsuper-
vised IE task. Note that to speed up computation,
we only include documents or text segments con-
taining no more than 10 candidates in our experi-
ments. The first data set contains a set of seminar an-
nouncements (Freitag and McCallum, 1999), anno-
tated with four slot labels, namely stime (start time),
etime (end time), speaker and location. We used as
candidates all strings labeled in the annotated data
as well as all named entities found by the Stanford
NER tagger for CoNLL (Finkel et al, 2005). There
are 309 seminar announcements with 2262 candi-
dates in this data set.
The second data set is a collection of para-
graphs describing aviation incidents, taken from the
Wikipedia article on ?List of accidents and incidents
involving commercial aircraft? (Wikipedia, 2009).
Each paragraph in the article contains one to a few
sentences describing an incident. In this domain, we
take each paragraph as a separate document, and all
hyperlinked phrases in the original Wikipedia arti-
cle as candidates. For evaluation, we manually an-
notated the paragraphs of incidents from 2006 to
2009 with five slot labels: the flight number (FN),
the airline (AL), the aircraft model (AC), the exact
819
location (LO) of the incident (e.g. airport name),
and the country (CO) where the incident occurred.
The entire data set consists of 564 paragraphs with
2783 candidates. The annotated portion consists of
74 paragraphs with 395 candidates.
The third data set comes from the management
succession domain used in the Sixth Message Un-
derstanding Conference (MUC-6, 1995). We extract
from the original data set al sentences that were
tagged with a management succession event, and use
as candidates all tagged strings in those sentences.
This domain has four target slots, namely PersonIn
(the person moving into a new position), PersonOut
(the person leaving a position), Org (the corpora-
tion?s name) and Post (the position title). Sentences
containing candidates with multiple labels (candi-
dates annotated as both PersonIn and PersonOut) are
discarded. The extracted data set consists of 757
sentences with 2288 candidates.
7.2 Features
To extract features for candidates, we first normal-
ize each word to its lower-case, with digits replaced
by the token digit. We extract the following fea-
tures for every candidate: the candidate phrase it-
self, its head word, the unigram and bigram be-
fore and after the candidate in the sentence where
it appeared, its entity type (person, location, or-
ganization, and date/time), as well as features de-
rived from dependency parse trees. Specifically, we
first apply the Stanford lexical parser to our data
(de Marneffe et al, 2006). Then for each candi-
date, we follow its dependencies in the correspond-
ing dependency parse tree until we find a relation
r ? {nsubj, csubj, dobj, iobj, pobj} in which the
candidate is the dependent. We then construct a fea-
ture (r, v) where v is governor of the relation.
7.3 Evaluation Baseline and Method
We use the standard K-means algorithm (Macqueen,
1967) as a non-generative baseline, since K-means is
commonly used for clustering. To evaluate cluster-
ing results, we match each slot in the labeled data to
the cluster that gives the best F1-measure when eval-
uated for the slot. We report the precision (P), re-
call (R) and F1-measure for individual slot labels, as
well as the macro- and micro- average results across
all labels for each experiment. We conduct 10 trials
of experiment on each model and each data set with
different random initializations. We report the trials
that give the smallest within-cluster sum-of-squares
(WCSS) distance for K-means, and those that give
the highest log-likelihood of data for all other mod-
els. Experimental trials are run until the change in
WCSS/log-likelihood between two EM iterations is
smaller than 1 ? 10?6. All trials converged within
30 minutes.
All models we evaluate involve a parameter K,
which is the number of values that y can take on.
The value of K is manually fixed in this study. As
noted, we use a garbage slot to capture irrelevant
candidates, thus the value of K is set to the number
of target slots plus 1 for each data set. We empir-
ically set the adjustable parameters in the proposed
prior, and the weight of the regularization term in the
locally normalized logistic regression model (Berg-
Kirkpatrick et al, 2010), denoted by ?. Exact set-
tings are given in the next subsection. Note that the
focus of our experiments is on evaluating the effec-
tiveness of the proposed prior. We leave the task of
learning the various parameter values to future work.
7.4 Results
Evaluation on existing generative models
We first evaluate the existing generative models
described in Section 5 with the multinomial prior.
Table 1 summarizes the performance of Naive Bayes
(NB), the Bernoulli mixture model (BMM), the
Gaussian mixture model (GMM), the locally nor-
malized logistic regression (LNLR) model, and K-
means. We only show the F1 measures in the table
due to space limit.
We first observe that NB does not perform well
for our task. LNLR, which is an interesting contri-
bution in its own right, does not seem to be suitable
for our task as well. While NB and LNLR are infe-
rior to K-means for all three data sets, BMM shows
mixed results. Specifically, BMM outperforms K-
means for aviation incidents, but performs poorly
for seminar announcements. GMM and K-means
achieve similar results, which is not surprising be-
cause K-means can be viewed as a special case of
the spherical GMM we used (Duda et al, 2001).
Overall speaking, results show that GMM is the
best among the four generative models for the distri-
820
(a) Results on seminar announcements. No macro- and micro-average result is reported
for NB and BMM as they merged the etime cluster with the stime cluster. Numbers in
brackets are the respective measures of the stime cluster when evaluated for etime.
Model stime etime speaker location Macro-avg Micro-avg Parameter
NB 0.558 (0.342) 0.276 0.172 ? ? Nil
BMM 0.822 (0.440) 0.412 0.402 ? ? Nil
GMM 0.450 0.530 0.417 0.426 0.557 0.455 Nil
LNLR 0.386 0.239 0.200 0.208 0.264 0.266 ? = .0005
K-means 0.560 0.574 0.335 0.426 0.538 0.452 Nil
(b) Results on aviation incidents. Target slots are airline (AL) , flight number (FN), aircraft
model (AC), location (LO) and country (CO).
Model AL FN AC LO CO Macro-avg Micro-avg Parameter
NB 0.896 0.473 0.676 0.504 0.533 0.618 0.628 Nil
BMM 0.862 0.794 0.656 0.695 0.614 0.741 0.724 Nil
GMM 0.859 0.914 0.635 0.576 0.538 0.730 0.692 Nil
LNLR 0.597 0.352 0.314 0.286 0.291 0.379 0.396 ? = .0005
K-means 0.859 0.936 0.661 0.576 0.538 0.729 0.701 Nil
(c) Results on management succession events. Target slots are person joining (PersonIn),
person leaving (PersonOut), organization (Org), and position (Post).
Model PersonIn PersonOut Org Post Macro-avg Micro-avg Parameter
NB 0.545 0.257 0.473 0.455 0.459 0.437 Nil
BMM 0.550 0.437 0.800 0.767 0.650 0.648 Nil
GMM 0.583 0.432 0.813 0.803 0.679 0.676 Nil
LNLR 0.419 0.245 0.319 0.399 0.351 0.346 ? = .0002
K-means 0.372 0.565 0.835 0.814 0.645 0.665 Nil
Table 1: Performance summary of the different generative models and K-means in terms of F1.
Data set Parameter Value
Seminar announcements {?k}4k=1 {2}4k=1
{?k}4k=1 {1}4k=1
Aviation incidents {?k}5k=1 {1}5k=1
{?k}5k=1 {1}5k=1
Management succession {?k}4k=1 {1,2,2,2}
{?k}4k=1 {1,2,2,2}
Table 2: Parameter settings for p(yi; ?).
bution p(x|y; ?). We proceed with incorporating the
proposed prior into GMM for further explorations.
Effectiveness of the proposed prior
We evaluate the effectiveness of the proposed
prior by combining it with GMM. Specifically, the
combined model follows Eq. (1), with p(yi; ?) com-
puted using the Poisson-based formula in Eq. (3) and
p(xi,j |yi,j ; ?) following Eq. (6) as in GMM.
We empirically determine the parameters used in
p(yi; ?) to maximize data?s log-likelihood as noted.
Table 2 reports the values of {?k}K?1k=1 and {?k}K?1k=1for different data sets. Recall that ?k specifies the
maximum number of candidates that the k-th slot can
generate, and its value is observed to be small in real
data. ?k specifies the expected number of candidates
that the k-th slot will generate.
Table 3 reports the performance of the combined
model (?GMM with prior?) on the three data sets,
along with results of GMM and K-means for easy
comparison. The combined model improves over
both GMM and K-means for seminar announce-
ments and aviation incidents, as can be seen from the
models? macro- and micro-average performance.
The advantages brought by the proposed prior are
mainly reflected in slots that are difficult to clus-
ter under GMM and K-means. Taking seminar an-
nouncements as an example, GMM and K-means
achieve high precision but low recall for stime, and
low precision but high recall for etime. When exam-
ining the clusters produced by these two models, we
found one small cluster that contains mostly stime
fillers (thus high precision but low recall), and an-
other much larger cluster that contains mostly etime
fillers together with most of the remaining stime
fillers (thus low precision but high recall for etime).
821
(a) Results on seminar announcements.
Model Metric stime etime speaker location Macro-avg Micro-avg
GMM with Prior P 0.964 0.983 0.232 0.253 0.608 0.416
R 0.680 0.932 0.952 0.481 0.761 0.738
F1 0.798 0.957 0.374 0.331 0.676 0.532
GMM P 1.000 0.362 0.300 0.436 0.524 0.407
R 0.291 0.984 0.686 0.416 0.594 0.518
F1 0.450 0.530 0.417 0.426 0.557 0.455
K-means P 0.890 0.434 0.222 0.436 0.496 0.389
R 0.408 0.847 0.679 0.416 0.588 0.541
F1 0.560 0.574 0.335 0.426 0.538 0.452
(b) Results on aviation incidents.
Model Metric AL FN AC LO CO Macro-avg Micro-avg
GMM with Prior P 1.000 1.000 1.000 0.741 0.833 0.915 0.908
R 0.753 0.877 0.465 0.588 0.727 0.682 0.673
F1 0.859 0.935 0.635 0.656 0.777 0.782 0.773
GMM P 1.000 1.000 1.000 0.563 0.433 0.799 0.724
R 0.753 0.842 0.465 0.588 0.709 0.672 0.664
F1 0.859 0.914 0.635 0.576 0.538 0.730 0.692
K-means P 1.000 0.981 0.830 0.563 0.433 0.761 0.711
R 0.753 0.895 0.549 0.588 0.709 0.699 0.691
F1 0.859 0.936 0.661 0.576 0.538 0.729 0.701
(c) Results on management succession events.
Model Metric PersonIn PersonOut Org Post Macro-avg Micro-avg
GMM with Prior P 0.458 0.610 0.720 0.774 0.640 0.642
R 0.784 0.352 0.969 0.846 0.738 0.731
F1 0.578 0.447 0.826 0.809 0.686 0.683
GMM P 0.464 0.605 0.725 0.792 0.647 0.648
R 0.782 0.336 0.925 0.815 0.715 0.707
F1 0.583 0.432 0.813 0.803 0.679 0.676
K-means P 0.382 0.515 0.733 0.839 0.607 0.639
R 0.363 0.625 0.969 0.791 0.687 0.693
F1 0.372 0.565 0.835 0.814 0.645 0.665
Table 3: Comparison between the combined model (GMM with the proposed prior), GMM and K-means.
This shows that GMM, when used with the multi-
nomial prior, and K-means have difficulties sepa-
rating candidates from these two slots. In contrast,
the combined model improves the recall of stime to
68%, as compared to 29.1% achieved by GMM with
the multinomial prior and 40.8% by K-means, with-
out sacrificing precision. It also improves the preci-
sion of etime from 36.2% to 98.3%.
For aviation incidents, the advantage of the pro-
posed prior is reflected in the location (LO) and
country (CO) slots, which may confuse the various
models as they both belong to the entity type loca-
tion. The proposed prior improves the precision of
these two slots greatly by trying to distribute them
into appropriate slots in the clustering process.
The three models achieve very similar perfor-
mance on management succession events as Ta-
ble 3(c) shows. Surprisingly, incorporating the
Poisson-based prior into GMM does not seem useful
in separating PersonIn and PersonOut slot fillers. To
investigate the possible reasons for this, we exam-
ine feature values in the centriods of the two clusters
learned by the three models.
Tables 4 and 5 respectively list the top-10 features
in the PersonIn cluster and the PersonOut cluster
learned by the combined model1, and their corre-
sponding values in the centriods of the two clusters.
The two clusters share 3 of the top-5 features, some
1We made similar observations from centriods learned in
GMM and K-Means, which are therefore not reported here.
822
Values in the centriod of:
Top-10 features PersonIn PersonOut
type:?person? 0.9985 1
unigram after:, 0.7251 0.3404
unigram before:?s? 0.2705 0
bigram after:, ?digits? 0.2105 0.1879
bigram after:, who 0.1404 0.0567
unigram before:, 0.1067 0.0035
dobj:succeeds 0.0906 0
unigarm before:succeeds 0.0892 0
nsubj:resigned 0.0746 0.0284
unigram before:said 0.0673 0
Table 4: Top-10 features in the PersonIn cluster, as
learned by GMM with the proposed prior.
of them being general context features that might not
help characterizing candidates from different slots
(e.g. the unigram after the candidate is a comma).
Both lists also contain features from dependency
parse trees. Note that the ?dobj:succeeds? feature
in the PersonIn cluster is in fact contributed by Per-
sonOut slot fillers, while the ?nsubj:succeeds? fea-
ture in the PersonOut cluster is contributed by Per-
sonIn slot fillers. Although listed among the top-
10, these features have relatively low values in the
learned centriods (about 0.1). These observations
may suggest that the management succession data
set lacks strong, discriminative features for all mod-
els to effectively distinguish between PersonIn and
PersonOut candidates in an unsupervised manner.
To conclude, the proposed prior is effective in as-
signing different but confusing candidate slot fillers
into appropriate slots, when there exist reasonable
features that can be exploited in the label assign-
ment process. This is evident by the improvements
the proposed prior brings to GMM in the seminar
announcement and aviation incident data sets.
8 Conclusions
We propose a generative model that incorporates
distributional prior knowledge about template slots
in a document for the unsupervised IE task. Specifi-
cally, we propose a Poisson-based prior that prefers
label assignments to cover more distinct slots in the
same document. The proposed prior also allows a
slot to generate multiple fillers in a document, up to
a certain number of times depending on the domain
of interest.
We experimented with four existing generative
Values in the centriod of:
Top-10 features PersonOut PersonIn
type:?person? 1 0.9985
unigram before:mr. 0.9894 0
bigram before:?s? mr. 0.5213 0
unigram after:, 0.3404 0.7251
bigram after:, ?digits? 0.1879 0.2105
unigram after:was 0.1667 0.0556
nsubj:president 0.1667 0.0117
nsubj:succeeds 0.1028 0.0102
bigram before:, mr. 0.0957 0
unigram after:?s 0.0745 0.0073
Table 5: Top-10 features in the PersonOut cluster, as
learned by GMM with the proposed prior.
models for the task of clustering slot fillers with
a multinomial prior, which assumes that labels are
generated independently in a document. We then
evaluate the effectiveness of the proposed prior by
incorporating it into the Gaussian mixture model
(GMM), which is shown to be the best among the
four existing models in our experiments. By incor-
porating the proposed prior into GMM, we can ob-
tain significantly better clustering results on two out
of three data sets.
Further improvements to this work are possible.
Firstly, we assume that some adjustable parameters
in the proposed prior can be manually fixed, such as
the number of template slots in the output and the
maximum numbers of fillers that can be generated
by different slots. We are looking into methods for
automatically learning such parameters. This will
help improve the applicability of our work to differ-
ent domains as an unsupervised model. Secondly,
we currently consider in the prior a probability dis-
tribution over all possible label assignments for ev-
ery document. This can be computationally expen-
sive if input documents are long, or when we aim
to discover large templates with large values of K.
An alternative is to consider an approximate solution
that evaluates, for instance, only the top few label as-
signments that are likely to maximize the likelihood
of our observations. This remains as an interesting
future work of this study.
Acknowledgments
This work is supported by DSO National Laborato-
ries. We thank the anonymous reviewers for their
helpful comments.
823
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In International Joint
Conference on Artificial Intelligence, pages 2670?
2676.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 582?590.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley-Interscience, 2nd
edition.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain
templates. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 207?214, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 363?
370.
Dayne Freitag and Andrew Kachites McCallum. 1999.
Information extraction with HMMs and shrinkage. In
Proceedings of the AAAI-99 Workshop on Machine
Learning for Information Extraction.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Proceedings of the Twenty-First Annual Conference
on Neural Information Processing Systems.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 415, Morristown, NJ, USA. Association
for Computational Linguistics.
J. B. Macqueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, Volume 1, pages 281?297.
Zvika Marx, Ido Dagan, and Eli Shamir. 2002. Cross-
component clustering for template induction. In Pro-
ceedings of the 2002 ICML Workshop on Text Learn-
ing.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference. Morgan Kaufmann, San Fran-
cisco, CA.
Benjamin Rosenfeld and Ronen Feldman. 2006. URES
: An unsupervised Web relation extraction system.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 667?674.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL Main con-
ference poster sessions, pages 731?738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
304?311.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 224?231, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Wikipedia. 2009. List of accidents and
incidents involving commercial aircraft.
http://en.wikipedia.org/wiki/List of accidents and
incidents involving commercial aircraft.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-
lation extraction by mining Wikipedia texts using in-
formation from the web. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of the
AFNLP.
824
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1137?1146,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Aspect-oriented Multi-Document Summarization with
Event-aspect model
Peng Li1 and Yinglin Wang1 and Wei Gao2and Jing Jiang3
1 Department of Computer Science and Engineering, Shanghai Jiao Tong University
2 Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong
3 School of Information Systems, Singapore Management University
{lipeng, ylwang@sjtu.edu.cn} {wgao@se.cuhk.edu.hk} {jingjiang@smu.edu.sg}
Abstract
In this paper, we propose a novel approach to
automatic generation of aspect-oriented sum-
maries from multiple documents. We first de-
velop an event-aspect LDA model to cluster
sentences into aspects. We then use extend-
ed LexRank algorithm to rank the sentences
in each cluster. We use Integer Linear Pro-
gramming for sentence selection. Key features
of our method include automatic grouping of
semantically related sentences and sentence
ranking based on extension of random walk
model. Also, we implement a new sentence
compression algorithm which use dependency
tree instead of parser tree. We compare our
method with four baseline methods. Quantita-
tive evaluation based on Rouge metric demon-
strates the effectiveness and advantages of our
method.
1 Introduction
In recent years, there has been much interest in
the task of multi-document summarization. In this
paper, we study the task of automatically generat-
ing aspect-oriented summaries from multiple docu-
ments. The goal of aspect-oriented summarization
is to present the most important content to the us-
er in a condensed form and a well-organized struc-
ture to satisfy the user?s needs. A summary should
follow a readable structure and cover all the aspect-
s users are interested in. For example, a summary
about natural disasters should include aspects about
what happened, when/where it happened, reasons,
damages, rescue efforts, etc. and these aspects may
be scattered in multiple articles written by different
news agencies. Our goal is to automatically collect
aspects and construct summaries from multiple doc-
uments.
Aspect-oriented summarization can be used in
many scenarios. First of all, it can be used to gener-
ateWikipedia-like summary articles, especially used
to generate introduction sections that summarizes
the subject of articles before the table of contents
and other elaborate sections. Second, opinionat-
ed text often contains multiple viewpoints about an
issue generated by different people. Summarizing
these multiple opinions can help people easily di-
gest them. Furthermore, combined with search en-
gines and question&answering systems, we can bet-
ter organize the summary content based on aspects
to improve user experience.
Despite its usefulness, the problem of modeling
domain specific aspects for multi-document summa-
rization has not been well studied. The most relevant
work is by (Haghighi and Vanderwende, 2009) on
exploring content models for multi-document sum-
marization. They proposed a HIERSUM model for
finding the subtopics or aspects which are combined
by using KL-divergence criterion for selecting rel-
evant sentences. They introduced a general con-
tent distribution and several specific content distri-
butions to discover the topic and aspects for a s-
ingle document collection. However, the aspects
may be shared not only across documents in a sin-
gle collection, but also across documents in different
topic-related collections. Their model is conceptual-
ly inadequate for simultaneously summarizing mul-
tiple topic-related document collections. Further-
more, their sentence selection method based on KL-
divergence cannot prevent redundancy across differ-
ent aspects.
In this paper, we study how to overcome these
1137
limitations. We hypothesize that comparatively
summarizing topics across similar collections can
improve the effectiveness of aspect-oriented multi-
document summarization. We propose a novel
extraction-based approach which consists of four
main steps listed below:
Sentence Clustering: Our goal in this step is to
automatically identify the different aspects and clus-
ter sentences into aspects (See Section 2). We sub-
stantially extend the entity-aspect model in (Li et al,
2010) for generating general sentence clusters.
Sentence Ranking: In this step, we use an exten-
sion of LexRank algorithm proposed by (Paul et al,
2010) to score representative sentences in each clus-
ter (See Section 3).
Sentence Compression: In this step, we aim to
improve the linguistic quality of the summaries by
simplifying the sentence expressions. We prune sen-
tences using grammatical relations defined on de-
pendency trees for recognizing important clauses
and removing redundant subtrees (See Section 4).
Sentence Selection: Finally, we select one com-
pressed version of the sentences from each aspec-
t cluster. We use Integer Linear Programming
(ILP) algorithm, which optimizes a global objective
function, for sentence selection (McDonald, 2007;
Gillick and Favre, 2009; Sauper and Barzilay, 2009)
(See Section 5).
We evaluate our method using TAC2010 Guided
Summarization task data sets1 (Section 6). Our eval-
uation shows that our method obtains better ROUGE
recall score compared with four baseline methods,
and it also achieve reasonably high-quality aspec-
t clusters in terms of purity.
2 Sentence Clustering
In this step, our goal is to discover event aspects con-
tained in a document set and cluster sentences in-
to aspects. Here we substantially extend the entity-
aspect model in Li et al (2010) and refer to it as
event-aspect model. The main difference between
our event-aspect model and entity-aspect model is
that we introduce an additional layer of event topics
and the separation of general and specific aspects.
1http://www.nist.gov/tac/2010/
Summarization/
Our extension is based upon the following ob-
servations. For example, specific events like
?Columbine Massacre? and ?Malaysia Resort Ab-
duction? can be related to the ?Attack? topic. Each
event consists of multiple articles written by dif-
ferent news agencies. Interesting aspects may in-
clude ?what happened, when, where, perpetrators,
reasons, who affected, damages and countermea-
sures,? etc2. We compared the ?Columbine Mas-
sacre? and ?Malaysia Resort Abduction? data set-
s and found 5 different kinds of words in the text:
(1) stop words that occur frequently in any docu-
ment collection; (2) general content words describ-
ing ?damages? or ?countermeasures? aspect of at-
tacks; (3) specific content words describing ?what
happened?, ?who affected? or ?where? aspect of the
concrete event; (4) background words describing the
general topic of ?Attack?; (5) words that are local to
a single document and do not appear across different
documents. Table 1 shows four sentences related to
two major aspects. We found that the entity-aspect
model does not have enough capacity to cluster sen-
tences into aspects (See Section 6). So we introduce
additional layer to improve the effectiveness of sen-
tence clustering. We also found that their one aspect
per sentence assumption is not very strong in this
scenario. Although a sentence may belong to a sin-
gle general aspect, it still contains multiple specific
aspect words like second sentence in Table 1. There-
fore, We assume that each sentence belongs to both
a general aspect and a specific aspect.
2.1 Event-Aspect Model
Stop words can be ignored by LDA model because
they can be easily identified using a standard stop
word list. Suppose that for a given event topic, there
are in total C specific events for which we need to
simultaneously generate summaries. We can assume
four kinds of unigram language models (i.e. multi-
nomial word distributions). For each event topic,
there is a background model ?B that generates words
commonly used in all documents, and there are AG
general aspect models ?ga (1 ? ga ? AG), where
AG is the number of general aspects. For each spe-
cific event in a topic, there are AS specific aspect
2http://www.nist.gov/tac/2010/
Summarization/Guided-Summ.2010.guidelines.
html
1138
countermeasures
Police/GA are/S close/B to/S identifying/GA someone/B responsible/GA
for/S the/S attack/B .
Investigators/GA do/S not/S know/B how/S many/S suspects/SA
they/S are/S looking/B for/S, but/S reported/B progress/B toward/S
identifying/GA one/S of/S the/S bombers/SA .
what happened, when, where
During/S the/S morning/SA rush/D hour/D on/S July/SA 7/SA terrorists/B
exploded/SA bombs/SA on/S three/D London/SA subway/D trains/SA and/S a/S
double-decker/D bus/SA .
Four/D coordinated/B bombings/SA struck/B central/B London/SA on/SA
July/SA 7/SA, three/D in/S subway/D cars/SA and/S one/D on/S a/S bus/SA .
Table 1: Four sentences on ?COUNTERMEASURES? and ?What, When, Where? aspects from the ?Attack? topic. S:
stop word. B: background word. GA: general aspect word. SA: specific aspect word. D: document word.
models ?sa (1 ? sa ? AS), where AS is the num-
ber of specific aspects, and also there are D doc-
ument models ?d (1 ? d ? D), where D is the
number of documents in this collection. We assume
that these word distributions have a uniform Dirich-
let prior with parameter ?.
We introduce a level distribution ? that control-
s whether we choose a word from ?ga or ?sa. ?
is sampled from Beta(?0, ?1) distribution. We also
introduce an aspect distribution ? that controls how
often a general or a specific aspect occurs in the col-
lection, where ? is sampled from another Dirichlet
prior with parameter ?. There is also a multinomi-
al distribution ? that controls in each sentence how
often we encounter a background word, a document
word, or an aspect word. ? has a Dirichlet prior with
parameter ?.
Let Sd denote the number of sentences in docu-
ment d, Nd,s denote the number of words (after stop
word removal) in sentence s of document d, and
wd,s,n denote the n?th word in this sentence. We
introduce hidden variables zgad,s and zsad,s to indicatethat a sentence s of document d belongs to which
general or specific aspects . We introduce hidden
variables yd,s,n for each word to indicate whether a
word is generated from the background model, the
document model, or the aspect model. We also intro-
duce hidden variables ld,s,n to indicate whether the
n?th word in sentence s of document d is generated
from the general aspect model. Figure 1 describes
the process of generating the whole document col-
lection. The plate notation of the model is shown in
Figure 2. Note that the values of ?0, ?1, ?1, ?2, ?
and ? are fixed. The number of general and specific
aspects AG and AS are also empirically set.
Given a document collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assignmen-
t of zgad,s, zsad,s, yd,s,n and ld,s,n that maximizes dis-tribution p(z,y, l|w;?, ?, ?, ?), where z, y, l and w
represent the set of all z, y, l andw variables, respec-
tively. With the assignment, sentences are naturally
clustered into aspects, and words are labeled as ei-
ther a background word, a document word, a general
aspect word or a specific aspect word.
Inference can be done with Gibbs sampling,
which is commonly used in LDA models (Griffiths
and Steyvers, 2004).
In our experiments, we set ?1 = 5, ?2 = 3,
? = 0.01, ? = 20, ?1 = 10 and ?2 = 10. We
run 100 burn-in iterations through all documents in
a collection to stabilize the distribution of z and y
before collecting samples. We take 10 samples with
a gap of 10 iterations between two samples, and av-
erage over these 10 samples to get the estimation for
the parameters.
After estimating all the distributions, we can find
the values of each zgad,s and zsad,s that gives us sen-tences clustered into general and specific aspects.
3 Sentence Ranking
In this step, we want to order the clustered sen-
tences so that the representative sentences can be
ranked higher in each aspect. Inspired by Paul et
al. (2010), we use an extended LexRank algorithm
to obtain top ranked sentences. LexRank (Erkan and
Radev, 2004) algorithm defines a random walk mod-
1139
1. Draw ?1 ? Dir(?1), ?2 ? Dir(?2), ? ? Dir(?)
Draw ? ? Beta(?0, ?1)
2. For each event topic, there is a background model
?B, and there are general aspect ga, where 1 ?
ga ? AG
(a) draw ?B ? Dir(?)
(b) draw ?ga ? Dir(?)
3. For each document collection, there are specific
aspect sa, where 1 ? sa ? AS
(a) draw ?sa ? Dir(?)
4. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zga ? Multi(?1)
ii. draw zsa ? Multi(?2)
iii. for each word n = 1, . . . , Nd,s
A. draw ld,s,n ? Binomial(?)
B. draw yd,s,n ? Multi(?)
C. drawwd,s,n ? Multi(?B) if yd,s,n =
1, wd,s,n ? Multi(?d) if yd,s,n = 2,
wd,s,n ? Multi(?z
sa
d,s) if yd,s,n =
3 and ld,s,n = 1 or wd,s,n ?
Multi(?z
ga
d,s) if yd,s,n = 3 and
ld,s,n = 0
Figure 1: The document generation process.
el on top of a graph that represents sentences to be
summarized as nodes and their similarities as edges.
The LexRank score of a sentence gives the expected
probability that a random walk will visit that sen-
tence in the long run. A variant is called continu-
ous LexRank improved LexRank by making use of
the strength of the similarity links. The continuous
LexRank score can be computed using the following
formula:
L(u) = dN + (1 ? d)
?
v?adj[u]
p(u|v)L(v)
whereL(u) is the LexRank value of sentence u,N is
the total number of nodes in the graph, d is a damp-
ing factor for the convergence of the method, and
p(u|v) is the jumping probability between sentence
u and its neighboring sentence v. p(u|v) is defined
using content similarity function sim(u, v) between
two sentences:
T
yd? d
SD sd ,wSA?
?
C
?
pi ?
?
gaz saz
l
?
GA
B?
1? 2?
1? 2?
Figure 2: The event-aspect model.
p(u|v) = sim(u, v)?
z?adj[v] sim(z, v)
The major extension is to modify this jumping
probability so as to favor visiting representative sen-
tences. More specifically, we scale sim(u, v) by the
likelihood that the two sentences represent the same
general aspect ga or specific aspect sa:
sim?(u, v) = sim(u, v)[
AG?
ga=1
P (ga|u)P (ga|v)
+
AS?
sa=1
P (sa|u)P (sa|v)]
where the value P (ga|u) and P (sa|u) can be
computed by our event-aspect model. We define
sim(u, v) as the tf ? idf weighted cosine similar-
ity between two sentences.
We found that sentence ranking is better con-
ducted before the compression because the pre-
compressed sentences are more informative and the
similarity function in LexRank can be better off with
the complete information.
4 Sentence Compression
It has been shown that sentence compression can
improve linguistic quality of summaries (Zajic et
al., 2007; Gillick et al, 2010). Commonly used
?Syntactic parse and trim? approach may produce
poor compression results. For example, given the
sentence ?We have friends whose children go to
Columbine, the freshman said?, the procedure tries
to remove the clause ?the freshman said? from the
parse tree by using the ?SBAR? label to locate the
1140
clause, and will result in ?whose children go to
Columbine?, which is not adequate. Furthermore,
some important temporal modifier, numeric modifier
and clausal complement need to be retained because
they reflect content aspects of the summary. There-
fore, we propose the ?dependency parse and trim?
approach, which prunes sentences based on depen-
dency tree representations, using English grammati-
cal relations to recognize clauses and remove redun-
dant structures. Table 2 shows two examples by re-
moving redundant auxiliary clauses. Below is the
sentence compression procedure:
1. Select possible subtree root nodes using gram-
matical relations, such as clausal complement,
complementizer, or parataxis 3.
2. Decide which subtree root node can be the root
of clause. If this root contains maximum num-
ber of child nodes and the collection of all child
edges include object or auxiliary relations, it is
selected as the root node.
3. Remove redundant modifiers such as adverbial-
s, relative clause modifiers and abbreviations,
participials and infinitive modifiers.
4. Traverse the subtrees and generate all possible
compression alternatives using the subtree root
node, then keep the top two longest sub sen-
tences.
5. Drop the sub sentences shorter than 5 words.
5 Sentence Selection
After sentence pruning, we prepare for the final
event summary generation process. In this step, we
select one compressed version of the sentence from
each aspect cluster. To avoid redundancy between
aspects, we use Integer Linear Programming to opti-
mize a global objective function for sentence selec-
tion. Inspired by (Sauper and Barzilay, 2009), we
formulate the optimization problem based on sen-
tence ranking information. More specifically, we
3The parataxis relation is a relation between the main verb
of a clause and other sentential elements, such as a sentential
parenthetical, colon, or semicolon
Original Compressed
When rescue workers
arrived, they said, on-
ly one of his limbs was
visible.
When rescue workers
arrived, only one of his
limbs was visible.
Two days earlier, a
massacre by two s-
tudents at Columbine
High, whose teams are
called the Rebels, left
15 people dead and
dozens wounded.
Two days earlier, a
massacre by two stu-
dents at Columbine
High, left 15 peo-
ple dead and dozens
wounded.
Table 2: Example compressed sentences.
would like to select exactly one compressed sen-
tence which receives the highest possible ranking s-
core from each aspect cluster subject to a series of
constraints, such as redundancy and length. We em-
ployed lp solver 4, an efficient mixed integer pro-
gramming solver using the Branch-and-Bound algo-
rithm to select sentences.
Assume that there are in total K aspects in an
event topic. For each aspect j, there are in total R
ranked sentences. The variables Sjl is a binary indi-
cator of the sentence. That is, Sjl= 1 if the sentence
is included in the final summary, and Sjl = 0 other-
wise. l is the ranked position of the sentence in this
aspect cluster.
Objective Function
Top ranked sentences are the most relevant corre-
sponding to the related aspects which we want to in-
clude in the final summary. Thus we try to minimize
the ranks of the sentences to improve the overall re-
sponsiveness.
min(
K?
j=1
Rj?
l=1
l ? Sjl)
Exclusivity Constraints
To prevent redundancy in each aspect, we just
choose one sentence from each general or specific
aspect cluster. The constraint is formulated as fol-
lows:
Rj?
l=1
Sjl = 1 ?j ? {1 . . .K}
4http://lpsolve.sourceforge.net/5.5/
1141
Redundancy Constraints
We also want to prevent redundancy across differ-
ent aspects. If sentence-similarity sim(sjl, sj?l?) be-
tween sentence sjl and sj?l? is above 0.5, then we
drop the pair and choose one sentence ranked higher
from the pair otherwise. This constraint is formulat-
ed as follows:
(Sjl + Sj?l?) ? sim(sjl, sj?l?) ? 0.5
?j, j? ? {1 . . .K}?l ? {1 . . . Rj}?l? ? {1 . . . Rj?}
Length Constraints
We add this constraint to ensure that the length of
the final summary is limited to L words.
K?
j=1
Rj?
l=1
lenjl ? Sjl ? L
where lenjl is the length of Sjl.
6 Evaluation
In order to systematically evaluate our method, we
want to check (1) whether the whole system is effec-
tive, which means to quantitatively evaluate summa-
ry quality, and (2) whether individual components
like clustering and compression algorithms are use-
ful.
6.1 Data
We use TAC2010 Summarization task data set for
the summary content evaluation. This data set pro-
vides 46 events. Each event falls into a predefined
event topic. Each specific event includes an even-
t statement and 20 relevant newswire articles which
have been divided into 2 sets: Document Set A and
Document Set B. Each document set has 10 docu-
ments, and all the documents in Set A chronologi-
cally precede the documents in Set B. We just use
document Set A for our task. Assessors wrote mod-
el summaries for each event, so we can compare
our automatic generated summaries with the model
summaries. We combine topic related data sets to-
gether, then these data sets simultaneously annotated
by our Event-aspect model. After labeling process,
we run sentence ranking, compression and selection
module to get final aspect-oriented summarizations.
6.2 Quality of summary
We use the ROUGE (Lin and Hovy, 2003) metric for
measuring the summarization system performance.
Ideally, a summarization criterion should be more
recall oriented. So the average recall of ROUGE-
1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and
ROUGE-L were computed by running ROUGE-
1.5.5 with stemming but no removal of stop word-
s. We compare our method with the following four
baseline methods.
Baseline 1
In this baseline, we try to compare different sen-
tence clustering algorithms in the multi-document
summarization scenario. First, we use CLUTO 5 to
do K-means clustering. Then we try entity-aspect
model proposed by Li et al (2010) to do sentence
clustering. Entity-aspect model is similar with ?HI-
ERSUM? content model proposed by Haghighi and
Vanderwende (2009). We use the same ranking,
compression, and selection components to generate
aspect-oriented summaries for comparison.
Baseline 2
In this baseline, we compare our method with
traditional ranking and selection summary genera-
tion framework (Erkan and Radev, 2004; Nenkova
and Vanderwende, 2005) to show that our sentence
clustering component is necessary in aspect-oriented
summarization system. Also we want check whether
sentence ranking combined with greedy based sen-
tence selection can prevent redundancy effective-
ly. We follow LexRank based sentence ranking
combined with greedy sentence selection methods.
We implement two greedy algorithms (Zhang et al,
2008; Paul et al, 2010). One is to select the top
ranked sentence simultaneously by removing 10 re-
dundant neighbor sentences from the sentence sim-
ilarity graph if the summary length is less then 100
words. This is repeated until the graph cannot be
partitioned. The similarity graph building threshold
is 0.3, damping factor is 0.2 and error tolerance for
Power Method in LexRank is 0.1. The other is to se-
lect top ranked sentences as long as the redundancy
score (similarity) between a candidate sentence and
5http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/overview
1142
current summary is under 0.5. This is repeated until
the summary reaches a 100 word length limit.
Baseline 3
In this baseline, we compare our ILP based sen-
tence selection with KL-divergence based sentence
selection. The KL-divergence formula we use is be-
low,
KL(PS ||QD) =
?
w
P (w) log P (w)Q(w)
where P (S) is the empirical unigram distribution of
the candidate summary S, and Q(D) is the unigram
distribution of document collection D. We only re-
placed our selection method with the KL-divergence
selection method. Other parts are the same. After
ranking sentences for each aspect, we add the sen-
tence with the highest ranking score from each as-
pect sentence cluster as long as the KL-divergence
between candidate and current summary does not
decrease. This is repeated until the summary reach-
es a 100 word length limit. To our knowledge, this
is the first work to directly compare Integer Lin-
ear Programming based sentence selection with KL-
divergence based sentence selection in summariza-
tion generation framework.
Baseline 4
In this baseline, we directly compare our method
with ?HIERSUM? proposed by (Haghighi and Van-
derwende, 2009). As in Baseline 1, we use entity-
aspect model to approximate ?HIERSUM? mod-
el. We replace unigram distribution of P (w) in
KL-divergence with learned distribution estimated
by ?HIERSUM? model. The KL-divergence based
greedy sentence selection algorithm is similar to
Baseline 3.
For fair comparison, Baselines 1, 2, 3 and 4 use
the same sentence compression algorithm and have
the summary length no more then 100 words. In
Table 3, we show the average ROUGE recall of 46
summaries generated by our method and four base-
line methods. We can see that our method gives
better Rouge recall measures then the four baseline
methods. For BL-1, we can see that LDA-based sen-
tence clustering is better then k-means. For BL-2,
we can see that traditional ranking plus greedy selec-
tion summary generation framework is not suitable
for the aspect-oriented summarization task. More
specifically, greedy-based sentence selection can not
prevent redundancy effectively. BL-3 evaluation re-
sults showed that ILP-based sentence selection is
better then KL-divergence selection in terms of pre-
venting redundancy across different aspects. The
measurement performance between BL-3 and BL-
4 is close. They use the same KL-divergence based
sentence selection, but topic model they use are d-
ifferent, and also BL-3 has a sentence ranking pro-
cess. The Rouge recall of our method is better than
BL-4. It is expected because our event-aspect mod-
el can better find the aspects and also prove that
our LexRank based sentence ranking combined with
ILP-based sentence selection can prevent redundan-
cy.
Due to TAC2010 summarization community just
compute ROUGE-2 and ROUGE-SU4 metrics for
participants, our ROUGE-2 metric ranked 11 out
of 23, ROUGE-SU4 metric ranked 12 out of 23.
They use MEAD6 as their baseline approach. The
ROUGE-2 score of our approach achieve 0.06508
higher than MEAD?s 0.05929. The ROUGE-SU4 s-
core of our approach achieve 0.10146 higher than
MEAD?s 0.09112. Many systems that get high-
er performances leverage domain knowledge bases
like Wikipedia or training data, but we didn?t. The
advantage of our method is that we generate sum-
maries with totally unsupervised framework and this
approach is domain adaptive.
6.3 Quality of aspect-oriented sentence clusters
To judge the quality of the aspect-oriented sentence
clusters, we ask the human judges to group the
ground truth sentences based on the aspect related-
ness in each event topic. We then compute the pu-
rity of the automatically generated clusters against
the human judged clusters. The results are shown
in Table 4. In our experiments, we set the number
of general aspect clusters AG is 5 and specific as-
pect clusters AS is 3. We can see from Table 4 that
our generated aspect clusters can achieve reasonably
good performance.
6http://www.summarization.com/mead/
1143
Rouge Average Recall
Method ROUGE-1 ROUGE-2 ROUGE-SU4 ROUGE-W-1.2 ROUGE-L
BL-1 k-means 0.21895 0.03689 0.06644 0.06683 0.19208
entity-aspect 0.26082 0.05082 0.08286 0.08055 0.22976
BL-2 greedy 1 0.27802 0.04872 0.08302 0.08488 0.24426
greedy 2 0.27898 0.04723 0.08275 0.08500 0.24430
BL-3 KL-Div 0.29286 0.05369 0.09117 0.08827 0.25100
BL-4 HIERSUM 0.28736 0.05502 0.08932 0.08923 0.25285
Without compression 0.30563 0.05983 0.09513 0.09468 0.25487
Our Method 0.32641 0.06508 0.10146 0.09998 0.28610
Table 3: ROUGE evaluation results on TAC2010 Summarization data sets
Category A Purity
Accidents and Natural Disasters 7 0.613
Attacks 8 0.658
Health and Safety 5 0.724
Endangered Resources 4 0.716
Investigations and Trials 6 0.669
Table 4: The true numbers of aspects as judged by the
human annotator (A), and the purity of the clusters.
Category Average Score
Accidents and Natural Disasters 2.4
Attacks 2.3
Health and Safety 2.6
Endangered Resources 2.5
Investigations and Trials 2.4
Table 5: The average score of each event topic.
6.4 Quality of sentence compression
To judge the quality of the dependency tree based
sentence compression algorithm, we ask the human
judges to choose 20 sentences from each event top-
ic then score them. The judges follow 3-point scale
to score each compressed sentence: 1 means poor,
2 means barely acceptable, and 3 means good. We
then compute the average scores. The results are
shown in Table 5. To evaluate the effectiveness of
sentence compression component, we conduct the
system without sentence compression component,
then compare it with our system. In Table 3, we
can see that sentence compression can improve the
system performance.
7 Related Work
Our event-aspect model is related to a number of
previous extensions of LDA models. Chemudugun-
ta et al (2007) proposed to introduce a background
topic and document-specific topics. Our background
and document language models are similar to theirs.
However, they still treat documents as bags of words
rather then sets of sentences as in our models. Titov
and McDonald (2008) exploited the idea that a short
paragraph within a document is likely to be about
the same aspect. The way we separate words in-
to stop words, background words, document word-
s and aspect words bears similarity to that used
in (Daume? III and Marcu, 2006; Haghighi and Van-
derwende, 2009). Paul and Girju (2010) proposed a
topic-aspect model for simultaneously finding topic-
s and aspects. The most related extension is entity-
aspect model proposed by Li et al (2010). The main
difference between event-aspect model and entity-
aspect model is our model further consider aspect
granularity and add a layer to model topic-related
events.
Filippova and Strube (2008) proposed a depen-
dency tree based sentence compression algorithm.
Their approach need a large corpus to build language
model for compression, whereas we prune depen-
dency tree using grammatical rules.
Paul et al (2010) proposed to modify LexRank
algorithm using their topic-aspect model. But their
task is to summarize contrastive viewpoints in opin-
ionated text. Furthermore, they use a simple greedy
approach for constructing summary.
McDonald (2007) proposed to use Integer Linear
Programming framework in multi-document sum-
1144
marization. And Sauper and Barzilay (2009) use in-
teger linear programming framework to automatical-
ly generate Wikipedia articles. There is a fundamen-
tal difference between their method and ours. They
used trained perceptron algorithm for ranking ex-
cerpts, whereas we give an extended LexRank with
integer linear programming to optimize sentence se-
lection for our aspect-oriented multi-document sum-
marization.
8 Conclusions and Future Work
In this paper, we study the task of automatically
generating aspect-oriented summary from multiple
documents. We proposed an event-aspect model
that can automatically cluster sentences into aspect-
s. We then use an extension of the LexRank algo-
rithm to rank sentences. We took advantage of the
output generated by the event-aspect model to mod-
ify jumping probabilities so as to favor visiting rep-
resentative sentence. We also proposed dependen-
cy tree compression algorithm to prune sentence for
improving linguistic quality of the summaries. Fi-
nally we use Integer Linear Programming Frame-
work to select aspect relevant sentences. We con-
ducted quantitative evaluation using standard test
data sets. We found that our method gave overal-
l better ROUGE scores than four baseline methods,
and the new sentence clustering and compression al-
gorithm are robust.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply more linguistic knowl-
edge to improve the quality of sentence compres-
sion. Currently the sentence compression algorith-
m may generate meaningless subtrees. It is rela-
tively hard to decide which clause is redundant in
terms of summarization. Second, we may explore
more domain knowledge to improve the quality of
aspect-oriented summaries. For example, we know
that the ?who-affected? aspect is related to person,
and ?when, where? are related to Time and Location.
we can import Name Entity Recognition to anno-
tate these phrases and then help locate relevant sen-
tences. Third, we want to extend our event-aspect
model to simultaneously find topics and aspects.
Acknowledgments
This work was supported by the National Nat-
ural Science Foundation of China (NSFC No.
60773088), the National High-tech R&D Program
of China (863 Program No. 2009AA04Z106), and
the Key Program of Basic Research of Shanghai
Municipal S&T Commission (No. 08JC1411700).
References
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific aspects
of documents with a probabilistic topic model. In Ad-
vances in Neural Information Processing Systems 19,
pages 241?248.
Hal. Daume? III and Daniel. Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 305?312.
Association for Computational Linguistics.
Gu?nes. Erkan and Dragomir Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22(1):457?479.
K. Filippova and M. Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, pages 25?32. Association for Computa-
tional Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
gauge Processing, pages 10?18.
Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet,
Y. Liu, and S. Xie. 2010. The icsi/utd summarization
system at tac 2009. In Proceedings of the Second Text
Analysis Conference, Gaithersburg, Maryland, USA:
National Institute of Standards and Technology.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National A-
cademy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics on
ZZZ, pages 362?370. Association for Computational
Linguistics.
1145
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the Joint Conference of the 48th Annual Meeting of the
ACL. Association for Computational Linguistics.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71?78. Association for Computation-
al Linguistics.
RyanMcDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances in
Information Retrieval, pages 557?564.
A. Nenkova and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In In AAAI-2010: Twenty-Fourth Con-
ference on Artificial Intelligence.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 66?76, Morristown, NJ, USA.
Association for Computational Linguistics.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 208?216, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web, pages 111?120.
D. Zajic, B.J. Dorr, J. Lin, and R. Schwartz. 2007. Multi-
candidate reduction: Sentence compression as a tool
for document summarization tasks. Information Pro-
cessing & Management, 43(6):1549?1570.
Jin. Zhang, Xueqi. Cheng, and Hongbo. Xu. 2008. GSP-
Summary: a graph-based sub-topic partition algorith-
m for summarization. In Proceedings of the 4th Asi-
a information retrieval conference on Information re-
trieval technology, pages 321?334. Springer-Verlag.
1146
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1245?1254, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Learning for Coreference Resolution with Markov Logic
Yang Song1, Jing Jiang2, Wayne Xin Zhao3, Sujian Li1, Houfeng Wang1
1Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
2School of Information Systems, Singapore Management University, Singapore
3School of Electronics Engineering and Computer Science, Peking University, China
{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.com
Abstract
Pairwise coreference resolution models must
merge pairwise coreference decisions to gen-
erate final outputs. Traditional merging meth-
ods adopt different strategies such as the best-
first method and enforcing the transitivity con-
straint, but most of these methods are used
independently of the pairwise learning meth-
ods as an isolated inference procedure at the
end. We propose a joint learning model which
combines pairwise classification and mention
clustering with Markov logic. Experimen-
tal results show that our joint learning sys-
tem outperforms independent learning sys-
tems. Our system gives a better performance
than all the learning-based systems from the
CoNLL-2011 shared task on the same dataset.
Compared with the best system from CoNLL-
2011, which employs a rule-based method,
our system shows competitive performance.
1 Introduction
The task of noun phrase coreference resolution is to
determine which mentions in a text refer to the same
real-world entity. Many methods have been pro-
posed for this problem. Among them the mention-
pair model (McCarthy and Lehnert, 1995) is one of
the most influential ones and can achieve the state-
of-the-art performance (Bengtson and Roth, 2008).
The mention-pair model splits the task into three
parts: mention detection, pairwise classification and
mention clustering. Mention detection aims to iden-
tify anaphoric noun phrases, including proper nouns,
common noun phrases and pronouns. Pairwise clas-
sification takes a pair of detected anaphoric noun
phrase candidates and determines whether they re-
fer to the same entity. Because these classification
decisions are local, they do not guarantee that can-
didate mentions are partitioned into clusters. There-
fore a mention clustering step is needed to resolve
conflicts and generate the final mention clusters.
Much work has been done following the mention-
pair model (Soon et al 2001; Ng and Cardie, 2002).
In most work, pairwise classification and mention
clustering are done sequentially. A major weak-
ness of this approach is that pairwise classification
considers only local information, which may not be
sufficient to make correct decisions. One way to
address this weakness is to jointly learn the pair-
wise classification model and the mention cluster-
ing model. This idea has been explored to some
extent by McCallum and Wellner (2005) using con-
ditional undirected graphical models and by Finley
and Joachims (2005) using an SVM-based super-
vised clustering method.
In this paper, we study how to use a different
learning framework, Markov logic (Richardson and
Domingos, 2006), to learn a joint model for both
pairwise classification and mention clustering un-
der the mention-pair model. We choose Markov
logic because of its appealing properties. Markov
logic is based on first-order logic, which makes
the learned models readily interpretable by humans.
Moreover, joint learning is natural under the Markov
logic framework, with local pairwise classification
and global mention clustering both formulated as
weighted first-order clauses. In fact, Markov logic
has been previously used by Poon and Domingos
(2008) for coreference resolution and achieved good
1245
results, but it was used for unsupervised coreference
resolution and the method was based on a different
model, the entity-mention model.
More specifically, to combine mention cluster-
ing with pairwise classification, we adopt the com-
monly used strategies (such as best-first clustering
and transitivity constraint), and formulate them as
first-order logic formulas under the Markov logic
framework. Best-first clustering has been previously
studied by Ng and Cardie (2002) and Bengtson and
Roth (2008) and found to be effective. Transitivity
constraint has been applied to coreference resolution
by Klenner (2007) and Finkel and Manning (2008),
and also achieved good performance.
We evaluate Markov logic-based method on the
dataset from CoNLL-2011 shared task. Our ex-
periment results demonstrate the advantage of joint
learning of pairwise classification and mention clus-
tering over independent learning. We examine
best-first clustering and transitivity constraint in our
methods, and find that both are very useful for coref-
erence resolution. Compared with the state of the
art, our method outperforms a baseline that repre-
sents a typical system using the mention-pair model.
Our method is also better than all learning systems
from the CoNLL-2011 shared task based on the re-
ported performance. Even with the top system from
CoNLL-2011, our performance is still competitive.
In the rest of this paper, we first describe a stan-
dard pairwise coreference resolution system in Sec-
tion 2. We then present our Markov logic model for
pairwise coreference resolution in Section 3. Exper-
imental results are given in Section 4. Finally we
discuss related work in Section 5 and conclude in
Section 6.
2 Standard Pairwise Coreference
Resolution
In this section, we describe standard learning-based
framework for pairwise coreference resolution. The
major steps include mention detection, pairwise
classification and mention clustering.
2.1 Mention Detection
For mention detection, traditional methods include
learning-based and rule-based methods. Which kind
of method to choose depends on specific dataset. In
this paper, we first consider all the noun phrases
in the given text as candidate mentions. With-
out gold standard mention boundaries, we use a
well-known preprocessing tool from Stanford?s NLP
group1 to extract noun phrases. After obtaining all
the extracted noun phrases, we also use a rule-based
method to remove some erroneous candidates based
on previous studies (e.g. Lee et al(2011), Uryupina
et al(2011)). Some examples of these erroneous
candidates include stop words (e.g. uh, hmm), web
addresses (e.g. http://www.google.com),
numbers (e.g. $9,000) and pleonastic ?it? pronouns.
2.2 Pairwise Classification
For pairwise classification, traditional learning-
based methods usually adopt a classification model
such as maximum entropy models and support vec-
tor machines. Training instances (i.e. positive and
negative mention pairs) are constructed from known
coreference chains, and features are defined to rep-
resent these instances.
In this paper, we build a baseline system that uses
maximum entropy models as the classification algo-
rithm. For generation of training instances, we fol-
low the method of Bengtson and Roth (2008). For
each predicted mention m, we generate a positive
mention pair between m and its closest preceding
antecedent, and negative mention pairs by pairing m
with each of its preceding predicted mentions which
are not coreferential with m. To avoid having too
many negative instances, we impose a maximum
sentence distance between the two mentions when
constructing mention pairs. This is based on the in-
tuition that for each anaphoric mention, its preced-
ing antecedent should appear quite near it, and most
coreferential mention pairs which have a long sen-
tence distance can be resolved using string match-
ing. During the testing phase, we generate men-
tion pairs for each mention candidate with each of
its preceding mention candidates and use the learned
model to make coreference decisions for these men-
tion pairs. We also impose the sentence distance
constraint and use string matching for mention pairs
with a sentence distance exceeding the threshold.
1http://nlp.stanford.edu/software/corenlp.shtml
1246
2.3 Mention Clustering
After obtaining the coreferential results for all men-
tion pairs, some clustering method should be used to
generate the final output. One strategy is the single-
link method, which links all the mention pairs that
have a prediction probability higher than a threshold
value. Two other alternative methods are the best-
first clustering method and clustering with the tran-
sitivity constraint. Best-first clustering means that
for each candidate mention m, we select the best
one from all its preceding candidate mentions based
on the prediction probabilities. A threshold value
is given to filter out those mention pairs that have a
low probability to be coreferential. Transitivity con-
straint means that if a and b are coreferential and
b and c are coreferential, then a and c must also be
coreferential. Previous work has found that best-first
clustering and transitivity constraint-based cluster-
ing are better than the single-link method. Finally
we remove all the singleton mentions.
3 Markov Logic for Pairwise Coreference
Resolution
In this section, we present our method for joint
learning of pairwise classification and mention clus-
tering using Markov logic. For mention detection,
training instance generation and postprocessing, our
method follows the same procedures as described in
Section 2. In what follows, we will first describe
the basic Markov logic networks (MLN) framework,
and then introduce the first-order logic formulas we
use in our MLN including local formulas and global
formulas which perform pairwise classification and
mention clustering respectively. Through this way,
these two isolated parts are combined together, and
joint learning and inference can be performed in a
single framework. Finally we present inference and
parameter learning methods.
3.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic (Richardson and Domingos,
2006; Riedel, 2008). A Markov logic network con-
sists of a set of first-order clauses (which we will re-
fer to as formulas in the rest of the paper) just like in
first-order logic. However, different from first-order
logic where a formula represents a hard constraint,
in an MLN, these constraints are softened and they
can be violated with some penalty. An MLN M
is therefore a set of weighted formulas {(?i, wi)}i,
where ?i is a first order formula andwi is the penalty
(the formula?s weight). These weighted formulas
define a probability distribution over sets of ground
atoms or so-called possible worlds. Let y denote a
possible world, then we define p(y) as follows:
p(y) = 1
Z
exp
(
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
)
. (1)
Here each c is a binding of free variables in ?i to
constants. Each f?ic represents a binary feature func-
tion that returns 1 if the ground formula we get by
replacing the free variables in ?i with the constants
in c under the given possible world y is true, and 0
otherwise. n?i denotes the number of free variables
of a formula ?i. Cn?i is the set of all bindings for the
free variables in ?i. Z is a normalization constant.
This distribution corresponds to a Markov network
where nodes represent ground atoms and factors rep-
resent ground formulas.
Each formula consists of a set of first-order predi-
cates, logical connectors and variables. Take the fol-
lowing formula as one example:
(?i, wi) : headMatch(a, b)?(a ?= b) ? coref (a, b).
The formula above indicates that if two different
candidate mentions a and b have the same head
word, then they are coreferential. Here a and b are
variables which can represent any candidate men-
tion, headMatch and coref are observed predicate
and hidden predicate respectively. An observed
predicate is one whose value is known from the ob-
servations when its free variables are assigned some
constants. A hidden predicate is one whose value is
not known from the observations. From this exam-
ple, we can see that headMatch is an observed pred-
icate because we can check whether two candidate
mentions have the same head word. coref is a hid-
den predicate because this is something we would
like to predict.
3.2 Formulas
We use two kinds of formulas for pairwise classi-
fication and mention clustering, respectively. For
1247
describing the attributes ofmi
mentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).
entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...
genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.
numberType(i,n) mi has number type SINGULAR, PLURAL or UN.
hasHead(i,h) mi has head word h, here h can represent all possible head words.
firstMention(i) mi is the first mention in its sentence.
reflexive(i) mi is reflexive.
possessive(i) mi is possessive.
definite(i) mi is definite noun phrase.
indefinite(i) mi is indefinite noun phrase.
demonstrative(i) mi is demonstrative.
describing the attributes of relations betweenmj andmi
mentionDistance(j,i,m) Distance between mj and mi in mentions.
sentenceDistance(j,i,s) Distance between mj and mi in sentences.
bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NO
and AGREE UN).
closestMatch(j,i,c) mj is the first agreement in number and gender when looking backward
from mi: CAGREE YES, CAGREE NO and CAGREE UN.
exactStrMatch(j,i) Exact strings match between mj and mi.
pronounStrMatch(j,i) Both are pronouns and their strings match.
nopronounStrMatch(j,i) Both are not pronouns and their strings match.
properStrMatch(j,i) Both are proper names and their strings match.
headMatch(j,i) Head word strings match between mj and mi.
subStrMatch(j,i) Sub-word strings match between mj and mi.
animacyMatch(j,i) Animacy types match between mj and mi.
nested(j,i) mj/i is included in mi/j .
c command(j,i) mj/i C-Commands mi/j .
sameSpeaker(j,i) mj and mi have the same speaker.
entityTypeMatch(j,i) Entity types match between mj and mi.
alias(j,i) mj/i is an alias of mi/j .
srlMatch(j,i) mj and mi have the same semantic role.
verbMatch(j,i) mj and mi have semantic role for the same verb.
Table 1: Observed predicates.
pairwise classification, because the decisions are lo-
cal, we use a set of local formulas. For mention
clustering, we use global formulas to implement
best-first clustering or transitivity constraint. We
naturally combine pairwise classification with men-
tion clustering via local and global formulas in the
Markov logic framework, which is the essence of
?joint learning? in our work.
3.2.1 Local Formulas
A local formula relates any observed predicates to
exactly one hidden predicate. For our problem, we
define a list of observed predicates to describe the
properties of individual candidate mentions and the
relations between two candidate mentions, shown in
Table 1. For our problem, we have only one hidden
predicate, i.e. coref. Most of our local formulas are
from existing work (e.g. Soon et al(2001), Ng and
Cardie (2002), Sapena et al(2011)). They are listed
in Table 2, where the symbol ?+? indicates that for
every value of the variable preceding ?+? there is a
separate weight for the corresponding formula.
3.2.2 Global Formulas
Global formulas are designed to add global con-
straints for hidden predicates. Since in our problem
there is only one hidden predicate, i.e. coref, our
global formulas incorporate correlations among dif-
ferent ground atoms of the coref predicates. Next we
will show the best-first and transitivity global con-
straints. Note that we treat them as hard constraints
so we do not set any weights for these global formu-
las.
1248
Lexical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? exactStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? pronounStrMatch (j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? properStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nopronounStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? headMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? subStrMatch(j,i) ? j ?= i ? coref(j,i)
hasHead(j,h1+) ? hasHead(i,h2+) ? j ?= i ? coref(j,i)
Grammatical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? genderType(j,g1+) ? genderType(i,g2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? numberType(j,n1+) ? numberType(i,n2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? bothMatch(j,i,b+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? closestMatch(j,i,c+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? animacyMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nested(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? c command(j,i) ? j ?= i ? coref(j,i)
(mentionType(j,t1+) ? mentionType(i,t2+)) ? j ?= i ? coref(j,i)
(reflexive(j) ? reflexive(i)) ? j ?= i ? coref(j,i)
(possessive(j) ? possessive(i)) ? j ?= i ? coref(j,i)
(definite(j) ? definite(i)) ? j ?= i ? coref(j,i)
(indefinite(j) ? indefinite(i)) ? j ?= i ? coref(j,i)
(demonstrative(j) ? demonstrative(i)) ? j ?= i ? coref(j,i)
Distance and position Features
mentionType(j,t1+) ? mentionType(i,t2+) ? sentenceDistance(j,i,s+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? mentionDistance (j,i,m+) ? j ?= i ? coref(j,i)
(firstMention(j) ? firstMention(i)) ? j ?= i ? coref(j,i)
Semantic Features
mentionType(j,t1+) ? mentionType(i,t2+) ? alias(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? sameSpeaker(j,i) ? j?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? entityTypeMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? srlMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? verbMatch(j,i) ? j ?= i ? coref(j,i)
(entityType(j,e1+) ? entityType(i,e2+)) ? j ?= i ? coref(j,i)
Table 2: Local Formulas.
Best-First constraint:
coref(j, i) ? ?coref(k, i) ?j, k < i(k ?= j) (2)
Here we assume that coref(j,i) returns true if can-
didate mentions j and i are coreferential and false
otherwise. Therefore for each candidate mention i,
we should only select at most one candidate mention
j to return true for the predicate coref(j,i) from all its
preceding candidate mentions.
Transitivity constraint:
coref(j, k)?coref(k, i)?j < k < i ? coref(j, i) (3)
coref(j, k)?coref(j, i)?j < k < i ? coref(k, i) (4)
coref(j, i)?coref(k, i)?j < k < i ? coref(j, k) (5)
With the transitivity constraint, it means for given
mentions j, k and i, if any two pairs of them are
coreferential, then the third pair of them should be
also coreferential.
We use best-first clustering and transitivity con-
straint in our joint learning model respectively. De-
tailed comparisons between them will be shown in
Section 4.
3.3 Inference
We use MAP inference which is implemented by In-
teger Linear Programming (ILP). Its objective is to
maximize a posteriori probability as follows. Here
we use x to represent all the observed ground atoms
and y to represent the hidden ground atoms. For-
mally, we have
y? = argmax
y
p(y|x) ? argmax
y
s(y, x),
where
s(y, x) =
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y, x). (6)
Each hidden ground atom can only takes a value of
either 0 or 1. And global formulas should be satis-
fied as hard constraints when inferring the best y?. So
1249
the problem can be easily solved using ILP. Detailed
introduction about transforming groundMarkov net-
works in Markov logic into an ILP problem can be
found in (Riedel, 2008).
3.4 Parameter Learning
For parameter learning, we employ the online
learner MIRA (Crammer and Singer, 2003), which
establishes a large margin between the score of the
gold solution and all wrong solutions to learn the
weights. This is achieved by solving the quadratic
program as follows
min ? wt ?wt?1 ? . (7)
s.t. s(yi, xi)? s(y?, xi) ? L(yi, y?)
?y? ?= yi, (yi, xi) ? D
Here D = {(yi, xi)}Ni=1 represents N training in-
stances (each instance represents one single docu-
ment in the dataset) and t represents the number of
iterations. In our problem, we adopt 1-best MIRA,
which means that in each iteration we try to find wt
which can guarantee the difference between the right
solution yi and the best solution y? (i.e. the one with
the highest score s(y?, xi), equivalent to y? in Section
3.3)) is at least as big as the loss L(yi, y?), while
changing wt?1 as little as possible. The number of
false ground atoms of coref predicate is selected as
loss function in our experiments. Hard global con-
straints (i.e. best-first clustering or transitivity con-
straint) must be satisfied when inferring the best y?
in each iteration, which can make learned weights
more effective.
4 Experiments
In this section, we will first describe the dataset and
evaluation metrics we use. We will then present the
effect of our joint learning method, and finally dis-
cuss the comparison with the state of the art.
4.1 Data Set
We use the dataset from the CoNLL-2011 shared
task, ?Modeling Unrestricted Coreference in
OntoNotes? (Pradhan et al 2011)2. It uses the En-
glish portion of the OntoNotes v4.0 corpus. There
are three important differences between OntoNotes
2http://conll.cemantix.org/2011/
and another well-known coreference dataset from
ACE. First, OntoNotes does not label any singleton
entity cluster, which has only one reference in the
text. Second, only identity coreference is tagged in
OntoNotes, but not appositives or predicate nomi-
natives. Third, ACE only considers mentions which
belong to ACE entity types, whereas OntoNotes
considers more entity types. The shared task is to
automatically identify both entity coreference and
event coreference, although we only focus on entity
coreference in this paper. We don?t assume that
gold standard mention boundaries are given. So we
develop a heuristic method for mention detection.
See details in Section 2.1.
The training set consists of 1674 documents from
newswire, magazine articles, broadcast news, broad-
cast conversations and webpages, and the develop-
ment set consists of 202 documents from the same
source. For training set, there are 101264 mentions
from 26612 entities. And for development set, there
are 14291 mentions from 3752 entities (Pradhan et
al., 2011).
4.2 Evaluation Metrics
We use the same evaluation metrics as used in
CoNLL-2011. Specifically, for mention detection,
we use precision, recall and the F-measure. A men-
tion is considered to be correct only if it matches
the exact same span of characters in the annotation
key. For coreference resolution, MUC (Vilain et al
1995), B-CUBED (Bagga and Baldwin, 1998) and
CEAF-E (Luo, 2005) are used for evaluation. The
unweighted average F score of them is used to com-
pare different systems.
4.3 The Effect of Joint Learning
To assess the performance of our method, we set up
several variations of our system to compare with the
joint learning system. The MLN-Local system uses
only the local formulas described in Table 2 with-
out any global constraints under the MLN frame-
work. By default, the MLN-Local system uses the
single-link method to generate clustering results.
The MLN-Local+BF system replaces the single-link
method with best-first clustering to infer mention
clustering results after learning the weights for all
the local formulas. The MLN-Local+Trans sys-
tem replaces the best-first clustering with transitivity
1250
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84
MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85
MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97
MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50
MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57
Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.
constraint. The MLN-Joint system is a joint model
for both pairwise classification and mention cluster-
ing. It can combine either best-first clustering or en-
forcing transitivity constraint with pairwise classifi-
cation, and we denote these two variants of MLN-
Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-
spectively.
To compare the performance of the various sys-
tems above, we use 10-fold cross validation on
the training dataset. We empirically find that our
method has a fast convergence rate, to learn the
MLN model, we set the number of iterations to be
10.
The performance of these compared systems is
shown in Table 3. To provide some context for
the performance of this task, we report the median
average F-score of the official results of CoNLL-
2011, which is 50.12 (Pradhan et al 2011). We can
see that MLN-Local achieves an average F-score of
56.84, which is well above the median score. When
adding best-first or transitivity constraint which
is independent of pairwise classification, MLN-
Local+BF and MLN-Local+Trans achieve better re-
sults of 57.85 and 57.97. Most of all, we can see
that the joint learning model (MLN-Joint(BF) or
MLN-Joint(Trans)) significantly outperforms inde-
pendent learning model (MLN-Local+BF or MLN-
Local+Trans) no matter whether best-first clustering
or transitivity constraint is used (based on a paired 2-
tailed t-test with p < 0.05) with the score of 58.50
or 58.57, which shows the effectiveness of our pro-
posed joint learning method.
Best-first clustering and transitivity constraint
are very useful in Markov logic framework, and
both MLN-Local and MLN-Joint benefit from them.
For MLN-Joint, these two clustering methods re-
sult in similar performance. But actually, transi-
tivity is harder than best-first, because it signifi-
cantly increases the number of formulas for con-
straints and slows down the learning process. In
our experiments, we find that MLN-Joint(Trans)3 is
much slower than MLN-Joint(BF). Overall, MLN-
Joint(BF) has a good trade-off between effectiveness
and efficiency.
4.4 Comparison with the State of the Art
In order to compare our method with the state-of-
the-art systems, we consider the following systems.
We implemented a traditional pairwise coreference
system using Maximum Entropy as the base classi-
fier and best-first clustering to link the results. We
used the same set of local features in MLN-Joint.
We refer to this system as MaxEnt+BF. To replace
best-first clustering with transitivity constraint, we
have another system named as MaxEnt+Trans. We
also consider the best 3 systems from CoNLL-2011
shared task. Chang?s system uses ILP to perform
best-first clustering after training a pairwise corefer-
ence model. Sapena?s system uses a relaxation label-
ing method to iteratively perform function optimiza-
tion for labeling each mention?s entity after learning
the weights for features under a C4.5 learner. Lee?s
system is a purely rule-based one. They use a battery
of sieves by precision (from highest to lowest) to it-
eratively choose antecedent for each mention. They
obtained the highest score in CoNLL-2011.
Table 4 shows the comparisons of our system with
the state-of-the-art systems on the development set
of CoNLL-2011. From the results, we can see that
our joint learning systems are obviously better than
3For MLN-Joint(Trans), not all training instances can be
learnt in a reasonable amount of time, so we set up a time out
threshold of 100 seconds. If the model cannot response in 100
seconds for some training instance, we remove it from the train-
ing set.
1251
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60
MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67
MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19
MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31
Lee?s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60
Sapena?s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61
Chang?s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35
Table 4: Comparisons with state-of-the-art systems on the development dataset.
MaxEnt+BF and MaxEnt+Trans. They also out-
perform the learning-based systems of Sapena et al
(2011) and Chang et al(2011), and perform com-
petitively with Lee?s system (Lee et al 2011). Note
that Lee?s system is purely rule-based, while our
methods are developed in a theoretically sound way,
i.e., Markov logic framework.
5 Related Work
Supervised noun phrase coreference resolution has
been extensively studied. Besides the mention-pair
model, two other commonly used models are the
entity-mention model (Luo et al 2004; Yang et al
2008) and ranking models (Denis and Baldridge,
2008; Rahman and Ng, 2009). Interested readers
can refer to the literature review by Ng (2010).
Under the mention-pair model, Klenner (2007)
and Finkel and Manning (2008) applied Integer Lin-
ear Programming (ILP) to enforce transitivity on the
pairwise classification results. Chang et al(2011)
used the same ILP technique to incorporate best-first
clustering and generate the mention clusters. In all
these studies, however, mention clustering is com-
bined with pairwise classification only at the infer-
ence stage but not at the learning stage.
To perform joint learning of pairwise classifi-
cation and mention clustering, in (McCallum and
Wellner, 2005), each mention pair corresponds to
a binary variable indicating whether the two men-
tions are coreferential, and the dependence between
these variables is modeled by conditional undirected
graphical models. Finley and Joachims (2005) pro-
posed a general SVM-based framework for super-
vised clustering that learns item-pair similarity mea-
sures, and applied the framework to noun phrase
coreference resolution. In our work, we take a differ-
ent approach and apply Markov logic. As we have
shown in Section 3, given the flexibility of Markov
logic, it is straightforward to perform joint learning
of pairwise classification and mention clustering.
In recent years, Markov logic has been widely
used in natural language processing problems (Poon
and Domingos, 2009; Yoshikawa et al 2009; Che
and Liu, 2010). For coreference resolution, the most
notable one is unsupervised coreference resolution
by Poon and Domingos (2008). Poon and Domin-
gos (2008) followed the entity-mention model while
we follow the mention-pair model, which are quite
different approaches. To seek good performance in
an unsupervised way, Poon and Domingos (2008)
highly rely on two important strong indicators:
appositives and predicate nominatives. However,
OntoNotes corpus (state-of-art NLP data collection)
on coreference layer for CoNLL-2011 has excluded
these two conditions of annotations (appositives and
predicate nominatives) from their judging guide-
lines. Compared with it, our methods are more ap-
plicable for real dataset. Huang et al(2009) used
Markov logic to predict coreference probabilities
for mention pairs followed by correlation cluster-
ing to generate the final results. Although they also
perform joint learning, at the inference stage, they
still make pairwise coreference decisions and clus-
ter mentions sequentially. Unlike their method, We
formulate the two steps into a single framework.
Besides combining pairwise classification and
mention clustering, there has also been some work
that jointly performs mention detection and coref-
erence resolution. Daume? and Marcu (2005) de-
veloped such a model based on the Learning as
1252
Search Optimization (LaSO) framework. Rahman
and Ng (2009) proposed to learn a cluster-ranker
for discourse-new mention detection jointly with
coreference resolution. Denis and Baldridge (2007)
adopted an Integer Linear Programming (ILP) for-
mulation for coreference resolution which models
anaphoricity and coreference as a joint task.
6 Conclusion
In this paper we present a joint learning method with
Markov logic which naturally combines pairwise
classification and mention clustering. Experimental
results show that the joint learning method signifi-
cantly outperforms baseline methods. Our method
is also better than all the learning-based systems in
CoNLL-2011 and reaches the same level of perfor-
mance with the best system.
In the future we will try to design more global
constraints and explore deeper relations between
training instances generation and mention cluster-
ing. We will also attempt to introduce more predi-
cates and transform structure learning techniques for
MLN into coreference problems.
Acknowledgments
Part of the work was done when the first author
was a visiting student in the Singapore Manage-
ment University. And this work was partially sup-
ported by the National High Technology Research
and Development Program of China(863 Program)
(No.2012AA011101), the National Natural Science
Foundation of China (No.91024009, No.60973053,
No.90920011), and the Specialized Research Fund
for the Doctoral Program of Higher Education of
China (Grant No. 20090001110047).
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference pro-
tocols for coreference resolution. In CoNLL Shared
Task, pages 40?44, Portland, Oregon, USA. Associa-
tion for Computational Linguistics.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 161?169.
Tsinghua University Press.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
III Hal Daume? and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97?104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
EMNLP, pages 660?669.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
ACL (Short Papers), pages 45?48. The Association for
Computer Linguistics.
T. Finley and T. Joachims. 2005. Supervised clustering
with support vector machines. In International Con-
ference on Machine Learning (ICML), pages 217?224.
Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-
jun Chen. 2009. Coreference resolution using markov
logic networks. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing: 10th Interna-
tional Conference, CICLing 2009.
M. Klenner. 2007. Enforcing consistency on coreference
sets. In RANLP.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-
hatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of the ACL, pages 135?142.
1253
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT/EMNLP, pages 25?
32.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems, pages 905?912. MIT Press.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In ACL, pages 1396?
1411. The Association for Computer Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP, pages 650?659.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of map inference for markov logic. In UAI,
pages 468?475. AUAI Press.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. Amodel-
theoretic coreference scoring scheme. In MUC, pages
45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In ACL, pages 843?851. The Association for
Computer Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In ACL/AFNLP,
pages 405?413. The Association for Computer Lin-
guistics.
1254
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1466?1477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Event-related Bursts via Social Media Activities
Wayne Xin Zhao?, Baihan Shu?, Jing Jiang?, Yang Song?, Hongfei Yan?? and Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,baihan.shu,yhf1029}@gmail.com,ysong@pku.edu.cn
jingjiang@smu.edu.sg, lxm@pku.edu.cn
Abstract
Activities on social media increase at a dra-
matic rate. When an external event happens,
there is a surge in the degree of activities re-
lated to the event. These activities may be
temporally correlated with one another, but
they may also capture different aspects of an
event and therefore exhibit different bursty
patterns. In this paper, we propose to iden-
tify event-related bursts via social media activ-
ities. We study how to correlate multiple types
of activities to derive a global bursty pattern.
To model smoothness of one state sequence,
we propose a novel function which can cap-
ture the state context. The experiments on a
large Twitter dataset shows our methods are
very effective.
1 Introduction
Online social networks (e.g., Twitter, Facebook,
Myspace) significantly influence the way we live.
Activities on social media increase at a dramatic
rate. Millions of users engage in a diverse range
of routine activities on social media such as posting
blog messages, images, videos or status messages,
as well as interacting with items generated by oth-
ers such as forwarding messages. When an event
interesting to a certain group of individuals takes
place, there is usually a surge in the degree of ac-
tivities related to the event (e.g., a sudden explosion
of tweets). Since social media activities may indi-
cate the happenings of external events, can we lever-
age on the rich social media activities to help iden-
tify meaningful external events? This is the research
problem we study in this paper. By external events,
we refer to real-world events that happen external to
the online space.
?Corresponding author.
2 4 6 8 100
2040
6080
100120
140
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(a) Query=?Amazon.?
2 4 6 8 100
50
100
150
Time index 
 all?tweetsretweetsurl?embeddedtweetsNoise
(b) Query=?Eclipse?.
Figure 1: The amount of activities within a 10-hour
window for two queries. Three types of activities
are considered: (1) posting a tweet (upward triangle),
(2) retweet (downward triangle), (3) posting a URL-
embedded tweet (excluding retweet) (filled circle). As
explained in Table 1, both bursts above are noisy.
Mining events from text streams is usually
achieved by detecting bursty patterns (Swan and Al-
lan, 2000; Kleinberg, 2003; Fung et al 2005). How-
ever, previous work has mostly focused on tradi-
tional text streams such as scientific publications and
news articles. There is still a lack of systematic in-
vestigations into the problem of identifying event-
related bursty patterns via social media activities.
There are at least two basic characteristics of social
media that make the problem more interesting and
challenging.
First, social media involve various types of activ-
ities taking place in real time. These activities may
be temporally correlated with one another, but they
may also capture different aspects of an event and
therefore exhibit different bursty patterns. Most of
previous methods (Swan and Allan, 2000; Klein-
berg, 2003; Fung et al 2005) deal with a single type
of textual activities. When applied to social media,
they oversimplify the complex nature of online so-
1466
Bursty Activity Time # in Sr # in Su # in St Noisy?
Sr,St 23:00?23:59, Nov. 23, 2009 108 5 147 Y
See Fig. 1(b) [Query=eclipse] major bursty reason: The tweet from Robert Pattinson ?@twilight: from rob cont .
- i hope you are looking forward to eclipse as much as i am .? has been retweeted many times.
Su,St 07:00?07:59, Jul. 25, 2009 6 122 133 Y
See Fig. 1(a) [Query=Amazon] major bursty reason: Advertisement tweets like ?@fitnessjunkies amazon.com deals
: http://tinyurl.com/lakz3h.? have been posted many times.
St,Su,Sr 09:00?9:59, Oct. 9, 2009 1562 423 2848 N
[Query=Nobel] major bursty reason: The news ?Obama won Nobel Peace Prize? flood Twitter.
Table 1: Examples of bursts. The first two bursts are judged as noise since they do not correspond to any meaningful
external events. In fact, the reasons why a burst appears in social media can be quite diverse. In this paper, we only
focus on event-related bursts. St denotes posting a tweet, Su denotes posting a url-embedded tweet, and Sr denotes
retweet.
cial activities, and therefore they may not be well
suitable to social media. Let us consider a moti-
vating example. Figure 1 shows the change of the
amount of activities of three different types over a
10-hour time window for two queries. If we consider
only the total number of tweets, we can see that for
both queries there is a burst. However, neither of the
two bursts corresponds to a real-world event. The
first burst was caused by the broadcast of an adver-
tisement from several Twitter bots, and the second
burst was caused by numerous retweets of a status
update of a movie star1. The detailed explanations
of why the two bursts are noisy are also shown in
Table 1. On the other hand, interestingly, we can see
that not all the activity streams display noisy bursty
patterns at the same time. It indicates that we may
make use of multiple views of different activities
to detect event-related bursts. The intuition is that
using multiple types of activities may help learn a
better global picture of event-related bursty patterns.
Learning may also be more resistant to noisy bursts.
Second, in social media, burst detection is chal-
lenged by irregular, unpredictable and spurious
noisy bursts. To overcome this challenge, a reason-
able assumption is that a burst corresponding to a
real event should not fluctuate too much within a
relatively short time window. To illustrate it, we
present an example in Figure 2, in which we first
use a simple threshold method to detect bursts and
then analyze the effect of local smoothness. In par-
ticular, if the amount of activities at a certain time
is above a pre-defined threshold, we set its state to
1, which indicates a bursty state. Otherwise, we set
the state to 0. Figure 2(a) shows that for the query
?Eclipse,? with a threshold of 50, the state sequence
for the time window we consider is ?0000100000.?
1The reasons for these bursts were revealed by manually
checking the tweets during the corresponding periods.
1 2 3 4 5 6 7 8 9 100
50
100
150 Correct0000000000Threshold0000100000
(a) Query=?Eclipse?.
1 2 3 4 5 6 7 8 9 100
5001000
15002000
25003000 Correct0000011111Threshold0000011111
(b) Query=?Nobel?.
Figure 2: Analysis of the effect of local smoothness on
threshold method. It shows two examples of threshold
methods for burst detection in a 10-hour window. The
red line denotes the bursty threshold. If the number of
activities is above the threshold in one time interval, the
state of this time interval is judge as bursty. Detailed de-
scriptions of these cases are shown in Table 1.
Although there is a burst in this sequence, its dura-
tion is very short. In fact, this is the first example
shown in Table 1, which is a noisy burst. In con-
trast, in Figure 2(b), the state sequence for the query
?Nobel? is ?0000011111,? in which the longer and
smoother burst corresponds to a true event. A good
function for evaluating the smoothness of a state se-
quence should be able to discriminate these cases
and model the context of state sequences effectively.
With its unique characteristics and challenges,
there is an emergent need to deeply study the prob-
lem of event-related burst detection via social me-
dia activities. In this paper, we conduct a system-
atic investigation on this problem. We formulate
this problem as burst detection from time series of
social media activities. We develop an optimiza-
tion model to learn bursty patterns based on multiple
types of activities. We propose to detect bursts by
considering both local state smoothness and correla-
tion across multiple streams. We define a function to
1467
quantitatively measure local smoothness of one sin-
gle state sequence. We systematically evaluate three
types of activities for burst detection on a large Twit-
ter dataset and analyze different properties of these
three streams for burst detection.
2 Problem Definition
Before formally introducing our problems, we first
define some basic concepts.
Activity: An activity refers to some type of action
that users perform when they are interested in some
topic or event.
Activity Stream: An activity stream of length
N and type m is a sequence of numbers
(nm1 , n
m
2 , ..., n
m
N ), where each n
m
i denotes the
amount of activities of type m that occur during the
ith time interval.
Query: A queryQ is a sequence of terms q1, ..., q|Q|
which can represent the information needs of users.
For example, an example query related to President
Obama is ?barack obama.?
Event-related Burst: Given a query Q, an event-
related burst is defined as a period [ts, te] in which
some event related with Q takes place, where ts and
te are the start timestamp and end timestamp of the
event period respectively. During the event period
the amount of activities is significantly higher than
average.
Based on these definitions, our task is to try to
identify event-related bursts via multiple social me-
dia activity streams.
3 Identifying Event-related Bursts from
Social Media
In this section, we discuss how to identify event-
related bursts via social media activities. For-
mally, given a query Q, we first build M ac-
tivity streams related with Q on T timestamps:
{(nm1 , ..., n
m
T )}
M
m=1. The definition of activity in our
methods is very general; it includes various types of
social media activities, including textual and non-
textual activities, e.g., a click on a shared photo and
a link formation between two users.
Given the input, we try to infer a state sequence
over these T timestamps: z = (z1, ..., zT ), where
zi is 1 or 0. 1 indicates a time point within a burst
while 0 indicates a non-bursty time point.
3.1 Modeling a Single Activity Stream
3.1.1 Generation function
In probability theory and statistics, the Poisson
distribution2 is a discrete probability distribution
that can measure the probability of a given number
of ?activities? occurring in a fixed time interval. We
use the Poisson distribution to study the probability
of observing the number of social media activities,
and we treat one hour as one time interval in this
paper.
Homogeneous Poisson Distribution The genera-
tive probability of the ith number in one activity
stream of type m is defined as f(nmi , i, z
m
i ) =
(?zmi
)n
m
i exp(??zmi
)
nmi !
, where ?0 is the (normal) expec-
tation of the number of activities in one time inter-
val. If one state is bursty, it would emit activities
with a faster rate and result in a larger expectation
?1. We can set ?1 = ?0 ? ?, where ? > 1.
Heterogeneous Poisson Distribution The two-state
machine in (Kleinberg, 2003) used two global refer-
ences for all the time intervals: one for bursty and
the other for non-bursty. In our experiments, we ob-
serve temporal patterns of user behaviors, i.e., activ-
ities in some hours are significantly more than those
in the others. Instead of using fixed global rates ?0
and ?1, we try to model temporal patterns of user
behaviors by parameterizing ?(?) with the time in-
dex. By following (Ihler et al 2006), we use a
set of hour-specific rates {?1,h}24h=1 and {?0,h}
24
h=1.
3
Given a time index h, we set ?0,h to be the expecta-
tion of the number of activities in hth time interval
every day, then we have ?1,h = ?0,h ? ?. In this
paper, ? is empirally set as 1.5.
3.1.2 Smoothness of a State Sequence
For burst detection, the major aim is to identify
steady and meaningful bursts and to discard tran-
sient and spurious bursts. Given a state sequence
z1z2...zT , to quantitatively measure the smoothness
and compactness of it, we introduce some measures.
One simple method is to count the number of
change in the state sequence. Formally, we use the
following formula:
g1(z) = T ?
T?1?
i=1
I(zi 6= zi+1), (1)
2http://en.wikipedia.org/wiki/Poisson distribution
3We can also make the rates both day-specific and hour-
specific, i.e., {?(?),d,h}h?{1,...,24},d?{1,...,7}.
1468
where T is length of the state sequence and I(?)
is an indicator function which returns 1 only if the
statement is true. Let us take the state sequence
?0000100000? (shown in Figure 2(a)) as an example
to see how g1 works. State changes 0pos=4 ? 1pos=5
and 1pos=5 ? 0pos=6 each incur a cost of 1, there-
fore g1(0000100000) = 10 ? 2 = 8. Similarly, we
can get g1(0000000000) = 10. There is a cost dif-
ference between these two sequences, i.e., ?g1 = 2.
Kleinberg (2003) uses state transition probabilities
to model the smoothness of state sequences. With
simple derivations, we can show that Kleinberg?s
model essentially also uses a cost function that is
linear in terms of the number of state changes in a
sequence, and therefore similar to g1.
In social media, very short noisy bursts like
?0000100000? are very frequent. To discard such
noises, we may multiply g1 by a big cost factor to
punish short-term fluctuations. However, it is not
sensitive to the state context4 and may affect the
detection of meaningful bursts. For example, state
change 0pos=4 ? 1pos=5 in ?0000111100? would
receive the same cost as that of 0pos=4 ? 1pos=5
in ?0000100000? although the later is more like a
noise.
To better measure the smoothness of a state se-
quence , we propose a novel context-sensitive func-
tion, which sums the square of the length of the max-
imum subsequences in which all states are the same.
Formally, we have
g2(z1, z2, ..., zT ) =
?
si<ei
(ei ? si + 1)
2, (2)
where si and ei are the start index and end in-
dex of the ith subsequence respectively. To define
?maximum?, we have the constraints zsi = zsi+1 =
... = zei , zsi?1 6= zsi , zei 6= zei+1. For example,
g2(0000000000)= 102 = 100, g2(0000100000)=
42 + 12 + 52 = 42, we can see that ?g2 =
100 ? 42 = 58, which is significantly larger than
?g1(= 2). g2 rewards the continunity of state se-
quences while punish the fluctuating changes, and it
is context-sensitive. State change 0pos=4 ? 1pos=5
in ?0000111100? receives a cost of 4,5 which is
4Here context refers to the window of hidden state se-
quences.
5Indeed, g2 is not designed for a single state change but for
the overall smoothness patterns, so we choose a referring se-
quence generated by making the corresponding state negative to
compute the cost, i.e., |g2(0000011110)?g2(0000001110)| =
4.
much smaller than that of 0pos=4 ? 1pos=5 in
?0000100000?. g2 is also sensitive to the po-
sition of state changes, e.g., g2(0000100000) 6=
g2(0100000000).
3.2 Burst Detection from a Single Activity
Stream
Given an activity stream (nm1 , ..., n
m
T ), we would
like to infer a state sequence over these T times-
tamps, i.e., to find out the most possible state se-
quence z = (zm1 , ..., z
m
T ) based on the data, where
zmi = 1 or 0. We formulate this problem as
an optimization problem. The cost of a state se-
quence includes two parts: generation of activities
and smoothness of the state sequence. The objective
function is to find a state sequence which incurs the
minimum cost. Formally, we define the total cost
function as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i )
? ?? ?
generating cost
+
(
? ?(zm1 , ..., z
m
T ) ? ?1
)
? ?? ?
smoothness cost
,
(3)
where ?1 > 0 is a scaling factor which balance these
two parts. ?(?) function is the smoothness function,
and we can set it as either g1(?) or g2(?).
To seek the optimal state sequence, we can min-
imize Equation 3. However, exact inference is hard
due to the exponential search space. Instead of ex-
amining the smoothness of the whole state sequence,
we propose to measure the smoothness of all the L-
length subsequences, so called ?local smoothness?.
The assumption is that the states in a relatively short
time window should not change too much. The new
objective function is defined as
Cost(z) = ?
T?
i=1
log f(nmi , i, z
m
i ) (4)
?
(
?
i?L
?(zmi , ..., z
m
i+L?1)
)
? ?1.
The objective function in Equation 4 can be
solved efficiently by a dynamic programming algo-
rithm shown in Algorithm 1. The time complexity
of this algorithm is O(T ? 2L). Note that the meth-
ods we present in Equation 4 and Algorithm 1 are
quite general. They are independent of the concrete
forms of f(?) and ?(?), which leaves room for flexi-
ble adaptation or extension in specific tasks. In pre-
vious methods (Kleinberg, 2003), L is often fixed as
1469
2. Indeed, as shown in Figure 2, in some cases, we
may need a longer window to infer the global pat-
terns. In our model, L can be tuned based on real
datasets. We can seek a trade-off between efficiency
and length of context windows.
Algorithm 1: Dynamic Programming for Equation 4.
d[i][s][zi...zi?L+1] denotes the minimum cost of the first1
i timestamps with the state subsequence: zi...zi?L+1 and
zi = s;
set d[0][?][?] = 0;2
set c1[i] = log f(nmi , i, z
m
i );3
set c2[i] = ?(zi, ..., zi?L+1);4
b, b?: previous and current state window are represented as5
L-bit binary numbers;
for i = 1 to T do6
for s = 0 to 1 do7
for b = 0 to 2L ? 1 do8
b? = (b << 1|s)&(1 << L? 1);9
d[i][s][b?]??10
min(d[i][s][b?], d[i?1][s][b]+c1[i]+c2[i]);
end11
end12
end13
3.3 Correlating Multiple Activity Streams
In this section, we discuss how to correlate multi-
ple activity streams to learn a global bursty patterns.
The hidden state sequences corresponding to these
activity streams are not fully independent. An ex-
ternal event may intricate surges in multiple activity
streams simultaneously.
We propose to correlate multiple activity streams
in an optimization model. The idea is that activ-
ity streams related with one query might be depen-
dent, i.e., the states of multiple activity streams on
the same timestamp tend to be the same6; if not,
it would incur a cost. To implement this idea, we
develop an optimization model. For convenience,
we call the states of each activity stream as ?local
states? while the overall states learnt from multiple
activity streams as ?global states?.
The idea is that although various activity streams
are different in the scale of frequencies, they tend to
share similar trend patterns. We incorporate the cor-
relation between local states on the same timestamp.
6In our experiments, we compute the cross correlation be-
tween different streams with a lag factor ?, we find the cross
correlation achieves maximum consistantly when ? = 0.
Formally, we have
Cost(Z) =
M?
m=1
{
?
T?
i=1
log f(nmi , i, z
m
i )
?
?
i?L
?(zmi , ..., z
m
i+L?1) ? ?1
}
+
T?
i=1
?
m1,m2
I(zm1i 6= z
m2
i ) ? ?2, (5)
where I(?) is indicator function, and ?2 is the cost
when a pair of states are different across multiple
streams on the same timestamp.
The objective function in Equation 5 can be
solved by a dynamic programming algorithm pre-
sented in Algorithm 2. The time complexity of this
algorithm is O(T ? 2M ?L+M ). Generally, L can be
set as one small value, e.g., L =2 to 6, and we can
select just a few representative activity streams, i.e.,
M =2 to 6. In this case, the algorithm can be effi-
cient.
Algorithm 2: Dynamic Programming for Equation 5.
d[i][z1i ...z
1
i?L+1; ...; z
M
i ...z
M
i?L+1] denotes the minimum1
cost of the first i timestamps with the local state
subsequence zmi ...z
m
i?L+1 in the mth stream;
set d[0][...] = 0;2
bl, bl
?
: previous and current state windows represented as3
M ? L-bit binary numbers;
c[i, bl, bl
?
] denotes all the cost in the tth timestamp;4
for i = 1 to T do5
for bl = 0 to 2M?L ? 1 do6
deriving current local state sequences bl
?
from bl;7
d[i][b?l]??8
min(d[i][bl
?
], d[i? 1][bl] + c[i, bl, bl
?
]);
end9
end10
Given M types of activity streams, we can get
M (local) state sequences {(zm1 , ..., z
m
T )}
M
m=1. The
next question is how to learn a global state sequence
(zG1 , ..., z
G
T ) based on local state sequences. Here we
give a few options:
CONJUNCT: we set a global state zi as bursty if
all local states are bursty, i.e., zGi = ?
M
m=1z
m
i .
DISJUNCT: we set a global state zi as bursty if
one of the local states is bursty, i.e., zGi = ?
M
m=1z
m
i .
BELIEF: we set a global state zi as the most con-
fident local state, i.e., zGi = argmaxmbelief(z
m
i ).
The belief(?) function can be defined as the ratio be-
tween generating costs from states zmi and 1 ? z
m
i :
belief(zmi ) =
f(nmi ,i,z
m
i )
f(nmi ,i,1?z
m
i )
.
1470
Table 2: Basic statistics of our golden test collection.
# of queries 17
Aver. # of event-related bursts per query 19
Min. bursty interval 3 hours
Max. bursty interval 163 hours
Aver. bursty interval 17.8 hours
L2G: we treat the states of one local stream as the
global states.
4 Experiments
4.1 Construction of Test Collection
We test our algorithms on a large Twitter dataset,
which contains about 200 million tweets and ranges
from July, 2009 to December 2009. We manually
constructed a list of 17 queries that have high vol-
umes of relevant tweets during this period. These
queries have a very broad coverage of topics. Exam-
ple queries are ?Barack Obama?, ?Apple?, ?Earth-
quake?, ?F1? and ?Nobel Prize?. For each query, we
invite two senior graduate students to manually iden-
tify their golden bursty intervals, and each bursty in-
terval is represented as a pair of timestamps in terms
of hours. Specifically, to generate the golden stan-
dard, given a query, the judges first manually gen-
erate a candidate list of external events7; then for
each event, they look into the tweets within the cor-
responding period and check whether there is a surge
on the frequency of tweets. If so, the judges fur-
ther determine the start timepoint and end timepoint
of it. If there is a conflict, a third judge will make
the final decision. We used Cohen?s kappa coeffi-
cient to measure the agreement of between the first
two judges, which turned out to be 0.67, indicating a
good level of agreement8. We present basic statistics
of the test collection in Table 2.
4.2 Evaluation Metrics
Before introducing our evaluation metrics, we first
define the Bursty Interval Overlap Ratio (BIOR)
BIOR(f,X ) =
?
f ??X ?l(f, f
?)
L(f)
,
f is a bursty interval, ?l(f, f ?) is the length of
overlap between f ? and f , L(f) is the length of
7We refer to some gold news resources, e.g., Google News
and Yahoo! News.
8http://en.wikipedia.org/wiki/Cohen?s kappa
Figure 3: Examples to illustrate BIOR. X0, X1
and X2 are three sets of bursty intervals. X0
and X2 consist of one interval, and X1 consists of
two intervals. BIOR(f,X0)=1, BIOR(f,X1)=0.5 and
BIOR(f,X2)=0.5.
bursty period of f . X is a set of bursty intervals,
BIOR measures the proportion of the timestamps in
f which are covered by one of bursty intervals in
X . We use BIOR to measure partial match of inter-
vals, because a system may not return all the exact
bursty intervals9. We show some examples of BIOR
in Figure 3.
We use modified Precision, Recall and F as ba-
sic measures. Given one query, P, R and F can be
defined as follows
R =
?
f?B I
(
1
|Mf |
BIOR(f,M) > 0.5
)
|B|
,
P =
1
|M|
?
f ??M
(BIOR(f ?,B)),
F =
2? P ?R
P + R
,
where M is the set of bursty intervals identified
by one candidate method, B is the set of bursty in-
tervals in golden standards, and Mf is the set of in-
tervals which overlap with f in M. We incorporate
the factor 1|Mf | in Recall to penalize the incontin-
uous coverage of the golden interval, and we also
require that the overlap ratio with penalized factor
is higher than a threshold of 0.5. Given two sets of
bursty intervals which have the same value of BIOR,
we prefer the one with fewer intervals. In Figure 3,
we can easily derive X1 and X2 have the same value
9A simple evaluation method is that we label each one hour
time slot as being part of a burst or not and compare with the
gold standard. However, in our experiments, we find that some
methods tend to break one meaningful burst into small parts and
easier to be affected by small fluctuations although they may
have a good coverage of bursty points. This is why we adopt a
different evaluation approach.
1471
Table 3: Average cross-correlation between different
streams.
St Sr Su
St 1 0.830235 0.851514
Sr 0.830235 1 0.59905
Su 0.851514 0.59905 1
of BIOR, when computing Recall, we prefer X2 to
X1 since X2 consists of only one complete inter-
val whileX1 consists of two inconsecutive intervals.
I(?) is an indicator function which returns 1 only if
the statement if true. In our experiments, we use the
average of R, P and F over all test queries.
4.3 Experiment Setup
Selecting activity streams
We consider three types of activity streams in
Twitter: 1) posting a tweet, denoted as St; 2) for-
warding a tweet (retweet), denoted as Sr; 3) post-
ing a URL-embedded tweet, denoted as Su. It is
natural to test the performance of St in discover-
ing bursty patterns, while Su and Sr measure the
influence of external events on users in Twitter in
two different aspects. Sr: An important convention
in Twitter is the ?retweeting? mechanism, through
which users can actively spread the news or related
information; Su: Another characteristic of Twitter is
that the length of tweets is limited to 140 characters,
which constrains the capacity of information. Users
often embed a URL link in the tweets to help others
know more about the corresponding information.
We compute the average cross correlation be-
tween different activity streams for these 17 queries
in our test collection, and we summarize the results
in Table 3. We can see that both Sr and Su have a
high correlation with St, and Sr has a relatively low
correlation with Su. 10
Methods for comparisons
S(?): using Equation 4 and considers a single ac-
tivity stream, namely St, Su and Sr.
MBurst(?): using Equation 5 and considers mul-
tiple activity streams.
To compare our methods with previous methods,
we adopt the following baselines:
StateMachine: This is the method proposed
in (Kleinberg, 2003). We use heterogeneous Poisson
10We also consider the frequencies of unique users by hours,
however, we find it has a extremely high correlation coefficient
with St, about 0.99, so we do not incorporate it.
function as generating functions instead of binomial
function Cnk because sometimes it is difficult to get
the exact total number n in social media.
Threshold: If we find that the count in one time
interval is higher than a predefined threshold, it is
treated as a burst. The threshold is set as 1.5 times
of the average number.
PeakFinding: This is the method proposed
in (Marcus et al 2011), which aims to automatically
discover peaks from tweets.
Binomial: This is the method proposed in (Fung et
al., 2007a), which uses a cumulative binomial distri-
bution with a base probability estimated by remov-
ing abnormal frequencies.
As for multiple-stream burst detection, to the best
of our knowledge, the only existing work is pro-
posed by (Yao et al 2010), which is supervised and
requires a considerable amount of training time, so
we do not compare our work with it. We compare
our method with the following heuristic baselines:
SimpleConjunct: we first find the optimal state se-
quences for each single activity stream. We then de-
rive a global state sequence by taking the conjunc-
tion of all local states.
SimpleDisjunct: we first find the optimal state se-
quences for each single activity stream, and then we
derive a global state sequence by take the disjunction
of all local states.
Another possible baseline is that we first merge
all the activities, then apply the single-stream algo-
rithm. However, in our data set, we find that the
number of activities in St is significantly larger than
that of the two types. St dominantly determines the
final performance, so we do not incorporate it here
as a comparison.
4.4 Experimental Results
Preliminary results on a single stream
We first examine the performance of our proposed
method on a single stream. Note that, our method
in Equation 4 has two merits: 1) the length of lo-
cal window can be tuned on different datasets; 2) a
novel state smoothness function is adopted.
We set the ? function in Equation 4 respectively
as g1 and g2, and apply our proposed methods to
three streams (St,Sr,Su) mentioned above. Note
that, when L = 2 and ? = g1, our method becomes
the algorithm in (Kleinberg, 2003). We tune the pa-
rameter ?1 in Equation 4 from 2 to 20 with a step of
2. We record the best F performance and compute
1472
the corresponding standard deviation. In Table 5, we
can observe that 1) streams St and Sr perform better
than Su; 2) the length of local window significantly
affects the performance; 3) g2 is much better than g1
in our proposed burst detection algorithm; 4) gen-
erally speaking, a longer window size (L = 3, 4)
performs better than the most common used size 2
in (Kleinberg, 2003).
We can see that our proposed method is more ef-
fective than the other baselines. The major reason is
that none of these methods consider state smooth-
ness in a systematic way. In our preliminary ex-
periments, we find that these baselines usually out-
put a lot of bursts, most of which are broken mean-
ingful bursts. To overcome this, baseline method
StateMachine (g1 + L = 2) requires larger ? and
?1, which may discard relatively small meaningful
bursts; while our proposed single stream method
(g2 + L = 3, 4) tends to identify steady and con-
secutive bursts through the help of longer context
window and context sensitive smoothness function
g2, it is more suitable to be applied to social media
for burst detection.
Compared with the other baselines, (Kleinberg,
2003) is still one good and robust baseline since it
models the state smoothness partially. These prelim-
inary findings indicate that state smoothness is very
important for burst detection, and the length of state
context window will affect the performance signifi-
cantly.
To get a deep analysis of the performance of dif-
ferent streams, we set up three classes, and each
class corresponds to a single stream. Since for each
query, we can obtain multiple results in different ac-
tivity streams, we further categorize the 17 anno-
tated queries to the stream which leads to the opti-
mal performance on that query. Interestingly, we can
see: 1) the url stream gives better performance on
queries about big companies because users in Twit-
ter usually talk about the release of new products
or important evolutionary news via url-embedded
tweets; 2) the retweet stream gives better perfor-
mance on queries which correspond to unexpected
or significant events, e.g., diasters. It is consistent
with our intuitions that users in Twitter do actively
spread such information. Combining previous anal-
ysis of Table 5, overall we find the retweet stream is
more capable to identify bursts which correspond to
significant events.
Table 4: Categorization of 17 queries according to the
optimal performance.
Streams Queries
url Apple,Microsoft,Nokia, climate
retweet bomb,crash,earthquake,typhoon,
F1,Google,Olympics
all tweet Amazon, eclipse, Lakers,
NASA, Nobel Prize, Barack Obama
Table 5: Performance (average F) on a single stream.
???? indicates that the improvement our proposed single-
stream methodg2,L=4 over all the other baselines is ac-
cepted at the confidence level of 0.95, i.e., StateMachine,
PeakingFinding, Binomial and Threshold.
? L St Sr Su
4 0.545/0.015 0.543/0.037 0.451/0.036
g2 3 0.536/0.013 0.549??/0.019 0.464/0.025
2 0.468/0.055 0.542/0.071 0.455/0.045
4 0.513/0.059 0.546/0.058 0.465/0.047
g1 3 0.469/0.055 0.542/0.071 0.455/0.045
2 0.396/0.043 0.489/0.074 0.374/0.035
StateMachine 0.396 0.489 0.374
PeakFinding 0.410 0.356 0.302
Binomial 0.315 0.420 0.341
Threshold 0.195 0.181 0.175
Preliminary results on multiple streams
After examining the basic results on a single
stream, we continue to evaluate the performance of
our proposed models on multiple activity streams.
For MBurst in Equation 5, we have three parame-
ters to set, namely L, ?1 and ?2. We do a grid search
for both ?1 and ?2 from 1 to 12 with a step of 1, and
we also examine the performance when L = 2, 3, 4.
We can see that MBurst has four candidate meth-
ods to derive global states from local states; for L2G,
we use the states of St as the final states, and we em-
pirically find that it performs best compared with the
other two streams in L2G.
Recall that our proposed single-stream method
is better than all the other single-stream baselines,
so here single-best denotes our method in Equa-
tion 4 (? = g2, L = 4) on Sr. For SimpleConjunct
and SimpleDisjunct, we first find the optimal state
sequences for each single activity stream using our
proposed method in Equation 4 (? = g2, L = 4),
and then we derive a global state sequence by take
the conjunction or disjunction of all local states re-
spectively.
Besides the best performance, we further compute
the average of the top 10 results of each method
by tuning parameters to check the average perfor-
1473
Table 6: Performance (average F) on multiple streams.
??? indicates that the improvement our proposed
multiple-stream method over our proposed single-stream
method at the confidence level of 0.9 in terms of average
performance.
Methods best average
single-best (g2 + Sr) 0.549 0.526
SimpleConjunct 0.548 -
SimpleDisjunct 0.465 -
MBurst+CONJUNCTr,t,u 0.555 0.548
MBurst+DISJUNCTr,t,u 0.576 0.570?
MBurst+BELIEFr,t,u 0.568 0.561
MBurst+L2Gr,t,u(t) 0.574 0.567
MBurst+L2Gr,t,u(r) 0.560 0.558
mance. The average performance can show the sta-
bility of models in some degree. If one model out-
puts the maximum in a very limited set of parame-
ters, it may not work well in real data, especially in
social media.
In Table 6, we can seeMBurst+DISJUNCTr,t,u
gives the best performance. MBurst performs
consistently better than single-best which is a very
strong single-stream method, especially for average
performance. MBurst+DISJUNCTr,t,u has an im-
provement of average performance over single-best
by 8.4%. And simply combining three different
streams may hit results (SimpleConjunct and Sim-
pleDisjunct). It indicates that MBurst is more sta-
ble and shows a higher performance.
For different methods to derive global bursty pat-
terns, we can see that MBurst+DISJUNCT per-
forms best while MBurst+CONJUNCT performs
worst. Interestingly, however, SimpleConjunct is
better than SimpleDisjunct, the major reason is that
MBurst performs a local-state correlation of mul-
tiple activity streams to correct possible noisy fluc-
tuations from single streams before the conjunction
or disjunction of local states. After such correlation,
the performance of each activity stream should im-
prove. To see this, we present the optimal results of a
single stream without/with local-state correlation in
Table 7. Local-state correlation significantly boosts
the performance of a single stream. Indeed, we find
that the step of local-state correlation is more impor-
tant for our multiple stream algorithm than the step
of how to derive global states based on local states.
We test our MBurst algorithm with the setting:
T = 4416, L = 4 and M = 3, and for all the test
Table 7: Comparison between the optimal results of a
single stream with/without local-state correlation.
all retweet retweet url
without 0.536 0.549 0.464
with 0.574 0.560 0.547
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?1
Average
 F
 
 MBurst+orsingle?best
(a) ?2 = 4, varying ?1.
2 4 6 8 10 120.54
0.550.56
0.570.58
0.590.6
?2
Average
 F
 
 MBurst+orsingle?best
(b) ?1 = 11, varying ?2.
Figure 4: Parameter sensitivity of MBurst + DIS-
JUNCT.
queries, our algorithm can respond in 2 seconds 11,
which is efficient to be deployed in social media.
Parameter sensitivity
We have shown the performance of different pa-
rameter settings for single stream algorithm in Ta-
ble 5. Next, we check parameter sensitivity in
MBurst. In our experiments, we find a longer lo-
cal window (L = 3, 4) is better than L = 2, so
we first set L = 4, then we select parameter set-
tings of ?2 = 4 and ?1 = 11, which give best per-
formance for MBurst+DISJUNCT. We vary one
with the other fixed to see how one single parame-
ter affects the performance. The results are shown in
Figure 4, and we can see MBurst+DISJUNCT is
consistently better than single-best.
5 Related Work
Our work is related to burst detection from text
streams. Pioneered by the automaton model pro-
posed in (Kleinberg, 2003), many techniques have
been proposed for burst detection such as the ?2-
test based method (Swan and Allan, 2000), the
parameter-free method (Fung et al 2005) and mov-
ing average method (Vlachos et al 2004). Our work
is related to the applications of these burst detection
algorithms for event detection (He et al 2007; Fung
et al 2007b; Shan et al 2012; Zhao et al 2012).
11All experiments are tested in a Mac PC, 2.4GHz Intel Core
2 Duo.
1474
Some recent work try to identify hot trends (Math-
ioudakis and Koudas, 2010; Zubiaga et al 2011;
Budak et al 2011; Naaman et al 2011) or make
use of the burstiness (Sakaki et al 2010; Aramki
et al 2011; Marcus et al 2011) in social media.
However, few of these methods consider modeling
the local smoothness of one state sequence in a sys-
tematic way and often use a fixed window length of
2.
Little work considers making use of different
types of social media activities for burst detection.
(Yao et al 2010; Kotov et al 2011; Wang et al
2007; Wang et al 2009) conducted some prelim-
inary studies of mining correlated bursty patterns
from multiple sources. However, they either highly
relies on high-quality training datasets or require a
considerable amount of training time. Online social
activities are dynamic, with a large number of new
items generated continuously. In such a dynamic
setting, burst detection algorithms should effectively
collect evidence, efficiently adjust prediction models
and respond to the users as social media activities
evolve. Therefore it is not suitable to deploy such
algorithms in social media.
Our work is also similar to studies which aim
to mine and leverage knowledge from social me-
dia (Mathioudakis et al 2010; Ruiz et al 2012;
Morales et al 2012). We share the common point
with these studies that we try to utilize the under-
lying rich knowledge in social media, while our fo-
cus of this work is quite different from theirs, i.e., to
identify event-related bursts.
Another line of related research is Twitter related
studies (Kwak et al 2010; Sakaki et al 2010). Our
proposed methods can provide event-related bursts
for downstream applications.
6 Conclusion
In this paper, we propose to identify event-related
bursts via social media activities. We propose one
optimization model to correlate multiple activity
streams to learn the bursty patterns. To better mea-
sure local smoothness of the state sequence, we pro-
pose a novel state cost function. We test our meth-
ods in a large Twitter dataset. The experiment re-
sults show that our methods are both effective and
efficient. Our work can provide a preliminary un-
derstanding of the correlation between the happen-
ings of events and the degree of online social media
activities.
Finally, we present a few promising directions
which may potentially improve or enrich current
work.
1) Variable-length context. In this paper, L is a
pre-determined parameter which controls the size of
context window. It cannot be modified when the al-
gorithm runs. A large L will significantly increases
the algorithm complexity, and we may not need a
large L for all the states in a Markov chain. This
problem can be addressed by using the variable-
length hidden Markov model (Wang et al 2006),
which is able to learn the ?minimum? context length
for accurately determining each state.
2) Incorporation of more useful features. Our
current model mainly considers temporal variations
of streaming data and searches the surge patterns ex-
isting in it. In some cases, simple frequency infor-
mation may not be capable to identify all the mean-
ingful bursts. It can be potentially useful to leverage
up more features to help filter out noisy bursts, e.g.,
semantic information (Zhao et al 2010).
3) Modeling multi-modality data. We have ex-
amined our multi-stream algorithm by using three
different activity streams. These streams are textual-
based. It will be interesting to check our algorithm in
multi-modality data streams. E.g., in Facebook, we
may collect a stream consisting of the daily frequen-
cies of photo sharing and another stream consisting
of the daily frequencies of text status updates.
4) Evaluation of the identified bursts. In most
of previous work, they seldom construct a gold stan-
dard for quantitative test, instead they qualitatively
evaluate their methods. In our work, we invite hu-
man judges to generate the gold standard. It is time-
consuming, and the bias from human judges cannot
be completely eliminated although more judges can
be invited. A possible evaluation method is to exam-
ine the identified bursts in downstream applications,
e.g., event detection.
Acknowledgement
This work is partially supported by NSFC Grant
61073082, 60933004 and 70903008. Xin Zhao is
supported by Google PhD Fellowship (China). We
thank the insightful comments from Junjie Yao and
the anonymous reviewers.
1475
References
Eiji Aramki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568?1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Ceren Budak, Divyakant Agrawal, and Amr El Abbadi.
2011. Structural trend analysis for online social net-
works. Proc. VLDB Endow., 4, July.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007a. Time-dependent event hierar-
chy construction. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?07.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007b. Time-dependent event hierarchy
construction. In SIGKDD.
Qi He, Kuiyu Chang, and Ee-Peng Lim. 2007. Analyz-
ing feature trajectories for event detection. In SIGIR.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD, pages 207?216, New
York, NY, USA. ACM.
J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.
Alexander Kotov, ChengXiang Zhai, and Richard Sproat.
2011. Mining named entities with temporally corre-
lated bursts from multilingual web news streams. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM, pages
237?246.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a
news media? In WWW ?10: Proceedings of the 19th
international conference on World wide web, pages
591?600.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: aggregating and visualizing
microblogs for event exploration. In Proceedings of
the 2011 annual conference on Human factors in com-
puting systems, CHI ?11.
Michael Mathioudakis and Nick Koudas. 2010. Twit-
termonitor: trend detection over the twitter stream.
In Proceedings of the 2010 international conference
on Management of data, SIGMOD ?10, pages 1155?
1158.
Michael Mathioudakis, Nick Koudas, and Peter Marbach.
2010. Early online identification of attention gather-
ing items in social media. In Proceedings of the third
ACM international conference on Web search and data
mining, WSDM ?10, pages 301?310, New York, NY,
USA. ACM.
Gianmarco De Francisci Morales, Aristides Gionis, and
Claudio Lucchese. 2012. From chatter to headlines:
harnessing the real-time web for personalized news
recommendation. In WSDM, pages 153?162.
Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twitter.
JASIST, 62(5):902?918.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aris-
tides Gionis, and Alejandro Jaimes. 2012. Correlat-
ing financial time series with micro-blogging activity.
pages 513?522.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. WWW, pages 851?860,
New York, NY, USA. ACM.
Dongdong Shan, Wayne Xin Zhao, Rishan Chen, Shu
Baihan, Hongfei Yan, and Xiaoming Li. 2012.
Eventsearch: A system for event discovery and re-
trieval on multi-type historical data. In KDD?12, De-
mostration.
Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.
Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.
Yi Wang, Lizhu Zhou, Jianhua Feng, JianyongWang, and
Zhi-Qiang Liu. 2006. Mining complex time-series
data by learning markovian models. In Proceedings
of the Sixth International Conference on Data Min-
ing, ICDM, pages 1136?1140, Washington, DC, USA.
IEEE Computer Society.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, WSDM, pages 192?201.
Junjie Yao, Bin Cui, Yuxin Huang, and Xin Jin. 2010.
Temporal and social context based burst detection
from folksonomies. In AAAI.
Wayne Xin Zhao, Jing Jiang, Jing He, Dongdong Shan,
Hongfei Yan, and Xiaoming Li. 2010. Context mod-
eling for ranking and tagging bursty features in text
1476
streams. In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?10.
Wayne Xin Zhao, Rishan Chen, Kai Fan, Hongfei Yan,
and Xiaoming Li. 2012. A novel burst-based text
representation model for scalable event detection. In
ACL?12.
Arkaitz Zubiaga, Damiano Spina, V??ctor Fresno, and
Raquel Mart??nez. 2011. Classifying trending topics:
a typology of conversation triggers on twitter. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM.
1477
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858?1868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Topics and Positions from Debatepedia
Swapna Gottipati? Minghui Qiu? Yanchuan Sim? Jing Jiang? Noah A. Smith?
?School of Information Systems, Singapore Management University, Singapore
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?{swapnag.2010,minghui.qiu.2010,jingjiang}@smu.edu.sg
?{ysim,nasmith}@cs.cmu.edu
Abstract
We explore Debatepedia, a community-
authored encyclopedia of sociopolitical de-
bates, as evidence for inferring a low-
dimensional, human-interpretable representa-
tion in the domain of issues and positions. We
introduce a generative model positing latent
topics and cross-cutting positions that gives
special treatment to person mentions and opin-
ion words. We evaluate the resulting repre-
sentation?s usefulness in attaching opinionated
documents to arguments and its consistency
with human judgments about positions.
1 Introduction
The social web has evolved into a forum for large
portions of the population to discuss and debate
complex issues of societal importance. Websites like
Debatepedia,1 an online, community-authored ency-
clopedia of debates (?2), seek to organize some of
this exchange into structured information resources
that summarize arguments and link externally to
texts (editorials, blog posts, etc.) that express and
evoke them. Empirical NLP, we propose, has a
role to play in creating a more compact and easily-
interpretable way to understand the opinion space.
In particular, we envision applications to computa-
tional journalism, where there is high demand for
transformation of and pattern discovery in unman-
ageable, unstructured, evolving data (including text)
to inform the public (Cohen et al, 2011).
In this paper, we develop a generative model for
discovering such a representation (?3), using De-
batepedia as a corpus of evidence. We draw in-
spiration from Lin et al (2008) and Ahmed and
1http://dbp.idebate.org
Xing (2010), who used generative models to infer
topics?distributions over words?and other word-
associated variables representing perspectives or
ideologies. We view topics as lexicons, and propose
that grounding a topic model with evidence beyond
bags of words can lead to more lexicon-like repre-
sentations. Specifically, our generative topic model
grounds topics using the hierarchical organization
of arguments within Debatepedia. Further, we use
named entity recognition as a preprocessing step, an
existing sentiment lexicon to construct an informed
prior, and we incorporate a latent, discrete position
variable that cuts across debates.2
We evaluate the model informally and formally
(?4). Subjectively, the model identifies reasonable
topic and perspective terms, and it associates topics
sensibly with important public figures. In quanti-
tative evaluations, we find the model?s representa-
tion superior to topics from vanilla latent Dirichlet
allocation (Blei et al, 2003) and the joint sentiment
topic model (Lin and He, 2009) in matching external
texts to debates. Further, the position variables can
be used to infer the side of an argument within a de-
bate; our model performs with an accuracy of 86%
on position prediction of the debate argument. The
cross-cutting position variable is not especially con-
sistent with human judgments, suggesting that fur-
ther knowledge sources may be required to improve
interpretability across issues.
2 Data
Debatepedia, like Wikipedia, is constructed by vol-
unteer contributors and has a system of community
2This variable might serve to cluster debate sides according
to ?abstract beliefs commonly shared by a group of people,?
sometimes called ideologies (Van Dijk, 1998). We do not claim
that our model infers ideologies (see ?4).
1858
Debate: Gun control; should laws be passed to limit gun ownership further?
Question: Self-defense ? Is self-defense a good reason for gun ownership?
Side: Yes Side: No
Argument: A citizen has a ?right? to guns as a means
to self-defense: Many groups argue that a citizen
should have the ?right? to defend themselves, and that
a gun is frequently the . . .
Argument: The protection of property is not a good
justification for yielding a lethal weapon. While peo-
ple have a right to their property, this should not justify
wielding a lethal . . .
Argument: Gun restrictions and bans disadvantage cit-
izens against armed criminals. Citizens that are not al-
lowed to carry guns are disadvantaged against lawless
criminals that . . .
Argument: Robert F. Drinan, Former Democratic US
Congressman, ?Gun Control: The Good Outweighs
the Evil?, 1976 ? ?These graphic examples of individ-
ual instances of . . .
Question: Economic benefits ? Is gun control economically beneficial?
Side: Yes Side: No
Argument: Lax gun control laws are economically
costly. The Coalition for Gun Control claims that, ?in
Canada, the costs of firearms death and injury alone
have been estimated at . . .
Argument: Gun sports have economic benefits. Field
sports bring money into poor rural economies and pro-
vide a motivation for landowners to value environmen-
tal protection.
Table 1: An example of a Debatepedia debate on the topic ?Gun control.?
moderation. Many of the debate issues covered are
controversial and salient in current public discourse.
Because it is primarily expressed as text, Debatepe-
dia is a corpus of debate topics, but it is organized
hierarchically, with multiple issues in each debate
topic, questions within each issue, and arguments on
two sides of each question. An important feature of
the corpus is the widespread quotation and linking to
external articles on the web, including news stories,
blog postings, wiki pages, and social media forums;
here we use these external articles in evaluation (?4).
Table 1 shows excerpts from a debate page3 from
Debatepedia. Each debate contains ?questions,?
which reflect the different aspects of a debate. In this
particular debate, there are 13 questions (2 shown),
ranging from economic benefits to enforceability to
social impacts. For each question, there are two dis-
tinct sides, each with its own set of supporting argu-
ments. Many of these arguments also contains links
to online articles where the quotes are extracted from
(not shown in Table 1). For example, in the second
argument on the ?No? side, there is an inline link to
the article written by Congressman Drinan.4
Within a debate topic, the sides cut across differ-
ent questions, aligning arguments together. In gen-
3http://dbp.idebate.org/en/index.php/
Debate:_Gun_control
4http://www.saf.org/LawReviews/Drinan1.
html
Debates 1,303
Arguments 33,556
Articles linked by exactly one argument 3,352
Tokens 1,710,814
Types (excluding NE mentions) 59,601
Person named entity mentions 9,496
Table 2: Debatepedia corpus statistics. Types and tokens
include unigrams, bigrams and person named entities.
eral, the questions are phrased so that a consistent
?pro? and ?con? structure is apparent throughout
each debate, aligned to a high-level question (i.e.,
the ?Yes? sides of all the questions are consistent
with the same side of the larger debate). The ex-
ample of Table 1 deviates from this pattern, with the
self-defense ?Yes? arguing ?no? to the high-level de-
bate question?Should laws be passed to limit gun
ownership further??and the economic ?Yes? argu-
ing ?yes? to the high-level question.
Table 2 presents statistics of our corpus.
2.1 Preprocessing
We scraped the Debatepedia website and extracted
the debate, question, argument, and side structure
of the debate topics. We crawled the external
web articles that were linked from the Debatepe-
dia arguments. For the web articles, we extracted
the main text content (ignoring boilerplate elements
such as navigation and advertisments) using Boil-
1859
erpipe (Kohlschu?tter et al, 2010).5 We tokenized
the text and filtered stopwords.6 We considered both
unigrams and bigrams in our model, keeping all uni-
grams and removing bigram types that appeared less
than 5 times in the corpus. Although our modeling
approach ultimately treats texts as bags of terms (un-
igrams and bigrams), one important preprocessing
step was taken to further improve the interpretabil-
ity of the inferred representation: named entity men-
tions of persons. We identified these mentions of
persons using Stanford NER (Finkel et al, 2005)
and treated each person mention as a single token. In
our qualitative analysis of the model (?4.2), we will
show how this special treatment of person mentions
enables the association of well-known individuals
with debate topics. Though not part of our exper-
imental evaluation in this paper, such associations
are, we believe, an interesting direction for future
applications of the model.
3 Model
Our model defines a probability distribution over
terms7 that are observed in the corpus. Each term
occurs in a context defined by the tuple ?d, q, s, a?
(respectively, a debate, a question within the debate,
a side within the debate, and an argument). At each
level of the hierarchy is a different latent variable:
? Each question q within debate d is associated
with a distribution over topics, denoted ?d,q.8
? Each side s of the debate d is associated with a
position, denoted id,s and we posit a global dis-
tribution ? that cuts across different questions
and arguments. In our experiments, there are
two positions, and the two sides of a debate
are constrained to associate with opposing po-
sitions. As illustrated by Table 1, this assump-
5http://code.google.com/p/boilerpipe
6www.ranks.nl/resources/stopwords.html
7Recall that our model includes bigrams. We treat each un-
igram and bigram token (after filtering discussed in ?2.1) as a
separate term.
8In future work, more sharing across questions within a
debate, or more differentiation among the topic distributions
for arguments under a question, might be explored. Wallach
(2006) describes suitable techniques using hierarchical Dirich-
let draws, and Eisenstein et al (2011) suggests the use of sparse
shocks to log-odds at different levels. Here we work on the
assumption that Debatepedia?s questions are the most topically
coherent level, and work with a single topic mixture at this level.
wz y
Nd,q,s,a
Ad,q,s
?
?
Qd
?
?
?tt?
o
i,t?ii ?et
K TKT
?b
?i ?o ?t ?e
?b
i
?
?
Sd
D
Figure 1: Plate diagram. K is the number of positions,
and T is number of topics. The shaded variables are ob-
served and dashed variables are marginalized. ?,?,?
and all ? are fixed hyperparameters (?3.1).
tion is not always correct, though it tends to
hold most of the time.
? Each term wd,q,s,a,n (n is the position index
of the term within an argument) is associated
with one of five functional term types, denoted
yd,q,s,a,n. This variable is latent, except when it
takes the value ?entity? (e) for terms marked as
named entity mentions. When it is not an en-
tity, it takes one of the other four values: ?gen-
eral position? (i), ?topic-specific position? (o),
?topic? (t), or ?background? (b). Thus, every
term w is drawn from one of these 5 types of
bags, and y acts as a switching variable to se-
lect the type of bag.
? For some term types (the ones where y ?
{o, t}), each term wd,q,s,a,n is associated with
one of T discrete topics, as indexed by
zd,q,s,a,n.
Figure 1 illustrates the plate diagram for the
graphical model underlying our approach. The gen-
erative story is given in Figure 2.
3.1 Priors
Typical probabilistic topic models assume a sym-
metric Dirichlet prior over its term distributions or
1860
1. ? topics t, draw topic-term distribution ?tt ? Dirichlet(?t) and topic-entity distribution ?et ? Dirichlet(?e).
2. ? positions i, draw position-term distribution ?ii ? Dirichlet(?i).
3. ? topics t, ? positions i, draw topic-position term distribution ?oi,t ? Dirichlet(?o).
4. Draw background term distribution ?b ? Dirichlet(?b).
5. Draw functional term type distribution ? ? Dirichlet(?).
6. Draw position distribution ? ? Dirichlet(?).
7. ? debates d:
a. Draw id,1, id,2 ? Multinomial(?), assigning each of the two sides to a position.
b. ? questions q in d:
i. Draw topic mixture proportions ?d,q ? Dirichlet(?).
ii. ? arguments a under question q and term positions n in a:
A. Draw topic label zd,q,s,a ? Multinomial(?d,q).
B. Draw functional term type yd,q,s,a ? Multinomial(?).
C. Draw term wd,q,s,a ? Multinomial (?yd,q,s,a | id,1, id,2, zd,q,s,a).
Figure 2: Generative story for our model of Debatepedia.
apply empirical Bayesian techniques to estimate the
hyperparameters. Motivated by past efforts to ex-
ploit prior knowledge (Zhao et al, 2010; Lin and
He, 2009), we use the OpinionFinder sentiment lex-
icon9 (Wilson et al, 2005) to construct ?i and ?o.
Specifically, terms w in the lexicon were given pa-
rameters ?iw = ?ow = 0.01, and other terms were
given ?iw = ?ow = 0.001, capturing our prior belief
that opinion-expressing terms are likely to be used
in expressing positions. 5,451 types were given a
?boost? through this prior.
Information retrieval has long exploited the ob-
servation that a term?s document frequency (i.e., the
number of documents a term occurs in) is inversely
related its usefulness in retrieval (Jones, 1972). We
encode this in ?b, the prior over the background
term distribution, by setting each value to the log-
arithm of the term?s argument frequency.
The other priors were set to be symmetric: ?e =
0.01 (entity topics), ?t = 0.001 (topics), ? =
50/T = 1.25 (topic mixture coefficients), ? = 0.01
(positions), and ? = 0.01 (functional term types).
Preliminary tests showed that final topics are rela-
tively insensitive to the values of the hyperparame-
ters.
3.2 Inference and Parameter Estimation
Exact inference under this model, like most latent-
variable topic models, is intractable. We apply col-
lapsed Gibbs sampling, a standard approach for such
9http://mpqa.cs.pitt.edu/lexicons/subj_
lexicon/
models (Griffiths and Steyvers, 2004).10 The no-
table deviations from typical uses of collapsed Gibbs
sampling are: (i) we jointly sample id,1 and id,2 to
respect the constraint that they differ; and (ii) we
fix the priors, in some cases to be asymmetric, as
discussed in ?3.1. We perform Gibbs sampling for
2,000 iterations over the dataset, discarding the first
500 iterations for burn-in, and averaging over every
10th iteration thereafter to get estimates for our term
distributions.
3.3 T andK
In all experiments, we use T = 40 topics andK = 2
positions. We did not extensively explore different
values for T and K; preliminary exploration sug-
gested that interpretability, gauged informally by the
authors, degraded for higher values of either.
4 Evaluation
Recall that the aim of this work is to infer a low-
dimensional representation of debate text. We esti-
mated our model on the Debatepedia debates (not in-
cluding hyperlinked articles), and conducted several
evaluations of the model, each considering a differ-
ent aspect of the goal. We exploit external articles
hyperlinked from Debatepedia described in ?2 as
supporting texts for arguments, treating each one?s
association to an argument as variable to be pre-
dicted. Firstly, we evaluate our model on the article
associating task. Secondly, we evaluate our model
on the position prediction task. Then, we compare
10Because this technique is well known in NLP, details are
relegated to supplementary material.
1861
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
N
o 
of
 A
rti
cle
s
JS Divergence
LDA
JST
Our Model
Figure 3: The distribution over Jensen-Shannon diver-
gences between a hyperlinked article and the correspond-
ing Debatepedia argument, n = 3, 352.
our model?s positional assignment of arguments to
human annotated clusterings. Finally, we present
qualitative discussion.
4.1 Quantitative Evaluation
4.1.1 Topics
As described in ?2, our corpus includes 3,352 ar-
ticles hyperlinked by Debatepedia arguments.11 Our
model can be used to infer the posterior over top-
ics associated with such an article, and we compare
that distribution to that of the Debatepedia article
that links to it. Calculating the similarity of these
distributions, we get an estimate of how closely our
model can associate text related to a debate with the
specific argument that linked to it. We compare with
LDA (Blei et al, 2003), which ignores sentiment,
and the joint sentiment topic (JST) model (Lin and
He, 2009), an unsupervised model that jointly cap-
tures sentiment and topic.12 Using Jensen-Shannon
divergence, we find that our approach embeds these
pairs significantly closer than LDA and JST (also
trained with 40 topics), under a Wilcoxon signed
rank test (p < 0.001). Figure 3 shows the histogram
of divergences between our model, JST, and LDA.
Associating external articles. More challenging,
of course, is selecting the argument to which an
external article should be associated. We used the
Jensen-Shannon divergence between topic distribu-
tions of articles and arguments to rank the latter,
for each article. The mean reciprocal rank scores
(Voorhees, 1999) for LDA, JST, and our model were
11We consider only those articles linked by a single Debate-
pedia argument.
12JST multiplies topics out by the set of sentiment labels, as-
signing each token to both a topic and a sentment. We use the
OpinionFinder lexicon in JST?s prior in the same way it is used
in our model.
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
5 10 15 20 25 inf
M
R
R
K
LDA
JST
Our Model
Figure 4: Mean reciprocal ranks for the association task.
0.1272, 0.1421, and 0.1507, respectively; the differ-
ence is significant (Wilcoxon signed rank test, p <
0.001). We found the same pattern for MRR@k,
k ? {5, 10, 15, 20, 25,?}, as shown in Figure 4.
It is likely possible to engineer more accurate
models for attaching articles to arguments, but the
attachment task is our aim only insofar as it con-
tributes to an overall assessment of an inferred rep-
resentation?s quality.
4.1.2 Positions
Positional distance by topic. We next consider
the JS divergences of position term distributions by
topic; for each topic t, we consider the divergence
between inferred values for ?o1,t and ?o2,t. Figure 5
shows these measurements sorted from most to least
different; these might be taken as evidence for which
issue areas? arguments are more lexically distin-
guishable by side, perhaps indicating less common
ground in discourse or (more speculatively) greater
controversy. For example, our model suggests that
debates relating to topics like presidential politics,
foreign policy, teachers, women?s health, religion,
and Israel/Palestine are more heated (within the De-
batepedia community at the time the debates took
place) than those about the minimum wage, Iran as
a nuclear threat, or immigration.
Predicting positions for arguments. We tested
our model?s ability to infer the positions of argu-
ments. In this experiment (only), we held out 3,000
arguments during parameter estimation. The held-
out arguments were selected so that every debate
side maintained at least one argument whose in-
ferred side could serve as the correct answer for the
held-out argument. We then inferred i for each held-
out argument from debate d and side s, given the
parameters, and compared it with the value of id,s
inferred during parameter estimation. The model
achieved 86% accuracy (Table 3 shows the confu-
1862
sion matrix). Note that JST does not provide a base-
line for comparison, since it does not capture debate
sides.
i = 1 i = 2
i? = 1 1,272 216
i? = 2 199 1,313
Table 3: Confusion matrix for position prediction on
held-out arguments.
Predicting positions for external articles. We
can also use the model to predict the position
adopted in an external text. For articles linked from
within Debatepedia, we have a gold standard: from
which side of a debate was it linked? After using
the model to infer a position variable for such a text,
we can check whether the inferred position variable
matches that of the argument that links to it. Table 4
shows that our model does not successfully com-
plete this task, assigning about 60% of both kinds
of articles i = 1.
i = 1 i = 2
i? = 1 1,042 623
i? = 2 1,043 644
Table 4: Confusion matrix for position prediction on hy-
perlinked articles.
Genre. We manually labeled 500 of these articles
into six genre categories. We had two annotators for
this task (Cohen?s ? = 0.856). These categories,
in increasing order of average Jensen-Shannon di-
vergence, are: blogs, editorials, wiki pages, news,
other, and government. Figure 6 shows the results.
While the only difference between the first and last
groups are surprising by chance, we are encouraged
by our model?s suggestion that blogs and editori-
als may be more ?Debatepedia argument-like? than
news and government articles.
Note that our model is learned only from text
within Debatepedia; it does not observe the text of
external linked articles. Future work might incorpo-
rate this text as additional evidence in order to cap-
ture effects on language stemming from the interac-
tion of position and genre.
0.1 0.2 0.3 0.4 0.5comment, minimum, wage, poverty, capitalism
nuclear, weapons, iran, states, threatparty, vote, republican, political, voters
energy, gas, power, fuel, windtax, economic, trade, cost, percent
immigration, cameras, police, immigrants, crimepeople, dont, time, lot, make
food, consumers, products, calorie, informationdeath, crime, punishment, penalty, justice
marijuana, drug, drugs, alcohol, agemarriage, gay, mars, space, moon
rights, law, people, individual, amendmentsouth, kosovo, independence, state, republic
human, rights, animals, life, animalchildren, child, sex, parents, sexual
school, schools, students, education, publicchina, tibet, chinese, people, tibetan
global, emissions, climate, carbon, warminginternational, court, war, crimes, icc
english, language, violence, people, videoorleans, euthanasia, city, suicide, priests
speech, corporations, corporate, public, moneyhealth, care, insurance, public, private
circumcision, men, sexual, circumcised, foreskininformation, torture, science, evidence, wikipedia
companies, market, industry, business, bailoutlaw, workers, union, rights, legal
college, cloning, game, football, incesttimes, york, ban, june, january
countries, eu, european, international, statesoil, water, production, ethanol, environmental
military, war, iraq, forces, marcheconomy, financial, spending, economic, government
government, social, governments, state, programsisrael, gaza, hamas, israeli, palestinian
women, religious, abortion, god, lifeteachers, pay, test, left, merit
peace, state, west, united, actionunited, states, president, administration, foreign
president, washington, obama, american, america
Figure 5: Jensen-Shannon divergences between topic-
specific positional term distributions, for each topic. Top-
ics are labeled by their most frequent terms from ?t.
4.1.3 Comparison to Human Judgments of
Positions
We compared our model?s inferred positions to
human judgments. For each of the 11 topics in Ta-
ble 8, we selected two associated debates with more
arguments than average (24.99). The debates were
provided to each of three human annotators,13 who
13All were native English-speaking American graduate stu-
dents not otherwise involved in this research. Each is known
by the authors to have basic literacy with issues and debates in
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
blog(12) edit(14) wiki(11) news(33) other(18) gov(12)
JS
 D
ive
rge
nc
e S
co
re
Article type(% of articles)
Figure 6: Position prediction on 500 hyperlinked articles
by genre.
1863
?Israel-Palestine? ?Same-sex marriage? ?Drugs? ?Healthcare? ?Death penalty? ?Abortion?
i1
pre emptive same sex hands free single payer anti death pro choice
israeli palestinian long term performance enhancing so called non violent pro life
open and shut second class in depth self sustaining african american non muslim
i2
two state opposite sex long term government run semi automatic would be
long term well intentioned high speed government approved high profile full time
self destructive day time short term high risk hate crime late term
a. Our model: topic-specific position bigrams associated with six selected topics.
?
war large illegal support death power
assault possibility abuse force penalty limit
disproportionate problems high threat murder civil
+
peace civil disease care power care
independence rights nature universal clean suicide
self-determination affirmative potential uninsured waste death
b. JST: sentiments associated with six selected topics manually aligned to our model?s topics.
Table 6: Terms associated with selected topics. The labels and alignments between the two models? topics were
assigned manually. (a.) Our model: topic-specific position bigrams which are ranked by comparing the log odds
conditioned on the position and topic: log ?oi1,t,w? log ?oi2,t,w. We show the top three terms for each position (b.) JST:we show the top three terms for each sentiment (negative and positive).
A1 (11) A2 (5) A3 (16)
Model (2) 3.21 2.58 3.45
A1 (11) 2.15 2.15
A2 (5) 2.63
Table 5: Variation of information scores for each pairing
of annotators and model.
were instructed to group the 44 sides of the debates.
The instructions stated:
Our goal is to see what you think about how
the different sides of different debates can be
lined up. You might find it convenient to
think of these in terms of political philoso-
phies, contemporary political party platforms,
or something else. Any of these is fine; we
want you to tell us the grouping you find most
reasonable.
All three annotators (hereafter denoted A1, A2, and
A3) used fairly involved labeling schemes; the an-
notators used 37, 30, and 16 unique labels, respec-
tively.14 A1 used keyword lists to label items; we
coarsened his labels manually by removing or merg-
ing less common keywords (resulting in: Republi-
can, Democrat, science/environment, nanny, politi-
cal reform, fiscal liberal, fiscal conservative, liber-
tarian, Israel, Palestine, and one unlabeled side).
A2 provided a coarse annotation along with each
American politics.
14In a small number of cases, an annotator declined to label
a side. Each unlabeled item received its own cluster.
fine-grained one (liberal, conservative, ?, and two
unlabeled sides). We used 100 samples from our
Gibbs sampler to estimate posteriors for each id,s;
these were always 99% or more in agreement, so we
mapped each debate side into its single most proba-
ble cluster. Recall that the two sides of each debate
must be in different clusters.
Table 5 shows the variation of information mea-
sure (Meila, 2003) for each pairing among the three
annotators and our model. The model agrees with
A2?s coarse clustering most closely, and in fact is
closer to A2?s clustering than A2 is to A3?s; it also
agrees with A2?s coarse clustering better than A2?s
coarse and fine clusterings agree (3.36, not shown
in the table). This is promising, but we do not
have confidence that the positional dimension is be-
ing captured especially well in this model; for those
debate-sides labeled liberal or conservative by A2,
the best match of our two positions was still only in
agreement only about 60% of the time, and agree-
ment with each human annotator is within the inter-
val of what would be expected if each debate?s sides
were assigned uniformly at random to positions.15
Remarks. Within debates and within topics, the
model uses the position variable to distinguish sides
well. For external text, the model performs well
on articles such as blogs and editorials but on oth-
ers the positional categories do not seem meaning-
15This was determined using a Monte Carlo simulation with
1,000 samples.
1864
Topic i = 1 i = 2
None (?i) vice president, c sections, twenty four, cross pressures,
pre dates, anti ballistic, cost effectiveness, anti land-
mine, court appointed, child poverty
cross examination, under runs, hand outs, half million,
non christians, break down, counter argument, seventy
five, co workers, run up
?Israel-
Palestine?
pre emptive, israeli palestinian, open and shut, first
time, hamas controlled, democratically elected
two state, long term, self destructive, secretary general,
right wing, all out, near daily, short term
?Same-sex
marriage?
same sex, long term, second class, blankenhorn rauch,
wrong headed, self denial, left handed
opposite sex, well intentioned, day time, planet wide,
day night, child rearing, low earth, one way, one third
?Drugs? hands free, performance enhancing, in depth, hand
held, best kept, non pharmaceutical, anti marijuana
long term, high speed, short term, peer reviewed, alco-
hol related, mind altering, inner city, long lasting
?Healthcare? single payer, so called, self sustaining, public private,
for profit, long run, high cost, multi payer
government run, government approved, high risk, two
tier, government appointed, low cost, set up
?Death
penalty?
anti death, non violent, african american, self help, cut
and cover, heavy handed, dp equivalent
semi automatic, high profile, hate crime, assault
weapons, military style, high dollar, self protective
?Abortion? pro choice, pro life, non muslim, well educated, anti
abortion, much needed, church state, birth control
would be, full time, late term, judeo christian, life
style, day to day, non christian, child bearing
Table 7: General position (first row) and topic-specific position bigrams associated with six selected topics.
Topic Terms Person entity mentions
?Israel-
Palestine?
israel, gaza, hamas, israeli, pales-
tinian
Benjamin Netanyahu, Al Jazeera, Mavi Marmara, Nicholas Kristoff,
Steven R. David
?Same-sex
marriage?
marriage, gay, mars, space, moon Buzz Aldrin, Andrew Sullivan, Moon Base, Scott Bidstrup, Ted Olson
?Drugs? marijuana, drug, drugs, alcohol, age Four Loko, Evo Morales, Toni Meyer, Sean Flynn, Robert Hahn
?Healthcare? health, care, insurance, public, pri-
vate
Kent Conrad, Paul Hsieh, Paul Krugman, Ezra Klein, Jacob Hacker
?Death
penalty?
death, crime, punishment, penalty,
justice
Adam Bedau, Thomas R. Eddlem, Jeff Jacoby, John Baer, Peter Bronson
?Abortion? women, religious, abortion, god, life Ronald Reagan, John Paul II, Sara Malkani, Mother Teresa, Marcella
Alsan
Table 8: For 6 selected topics (labels assigned manually), top terms (?t) and person entities (?e). Bigrams were
included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex
marriage with the space program.
ful, perhaps due to the less argumentative nature
of other kinds of articles. Noting the vast litera-
ture focusing on ideological positions expressed in
text, we believe this failure suggests (i) that broad-
based positions that hold across many topics may
require richer textual representations (see, e.g., the
?syntactic priming? of Greene and Resnik, 2009),
or (ii) that an alternative representation of positions,
such as the spatial models favored by political sci-
entists (Poole and Rosenthal, 1991), may be more
discoverable. Aside from those issues, a stronger
theory of positions may be required. Such a the-
ory could be encoded in a more informative prior or
weaker independence assumptions across debates.
Finally, exploiting explicitly ideological texts along-
side the moderated arguments of Debatepedia might
also help to identify textual associations with gen-
eral positions (Sim et al, 2013). We leave these di-
rections to future work.
4.2 Qualitative Analysis
Of the T = 40 topics our model inferred, we subjec-
tively judged 37 to be coherent; a glimpse of each is
given in Figure 5. We manually selected six of the
most interpretable topics for further evaluation.
As a generative modeling approach, our model
was designed for the purpose of reducing the dimen-
sionality of the sociopolitical debate space, as evi-
denced by Debatepedia. It is like other topic models
in this regard, but we believe that some effects of our
design choices are noteworthy. Table 6 compares the
positional bigrams of our model to the sentiments in-
ferred by JST. We observe the benefit of our model
in identifying terms associated with positions on so-
cial issues, while JST selects more general sentiment
terms.
1865
Table 7 shows bigrams most strongly associated
with general position distributions ?i and selected
topic-position distributions ?o.16 We see the poten-
tial benefit of multiword expressions. Although we
have used frequent bigrams as a poor man?s approx-
imation to multiword expression analysis, we find
the topic-specific positions terms to be subjectively
evocative. While somewhat internally coherent, we
do not observe consistent alignment across topics,
and the general distributions ?i are not suggestive.
The separation of personal name mentions into
their own distributions, shown for some topics in
Table 8, gives a distinctive characterization of top-
ics based on relevant personalities. Subjectively, the
top individuals are relevant to the subject matter as-
sociated with each topic (though the topics are not
always pure; same-sex marriage and the space pro-
gram are merged, for example).
5 Related Work
Insofar as debates are subjective, our study is related
to opinion mining. Subjective text classification
(Wiebe and Riloff, 2005) leads to opinion mining
tasks such as opinion extraction (Dave et al, 2003),
positive and negative polarity classification (Pang et
al., 2002), sentiment target detection (Hu and Liu,
2004; Ganapathibhotla and Liu, 2008), and feature-
opinion extraction (Wu et al, 2009). The above
studies are conducted mostly on product reviews, a
domain with a simpler opinion landscape and more
concrete rationales for those opinions, compared to
sociopolitical debates.
Generative topic models have been successfully
implemented in opinion mining tasks such as feature
identification (Titov and McDonald, 2008), entity-
topic extraction (Newman et al, 2006), mining con-
tentious expressions and interactions (Mukherjee
and Liu, 2012) and specific aspect-opinion word ex-
traction from labeled data (Zhao et al, 2010). Most
relevant to this research is work on feature-sentiment
extraction (Lin and He, 2009; Mei et al, 2007). Mei
et al (2007) built on PLSI, which is problematic
for generalizing beyond the training sample. The
JST model of Lin and He (2009) is an LDA-based
topic model in which each word token is assigned
both a sentiment and a topic; they exploited a sen-
16For more topics, please refer to the supplementary notes.
timent lexicon in the prior distribution. Our model
is closely related, but introduces a switching vari-
able that assigns some tokens to positions, some to
topics, and some to both. Unlike Lin and He?s senti-
ments, our model?s positions are associated with the
two sides of a debate, and we incorporate topics at
the level of questions within debates.
Some studies have specifically analyzed con-
trastive viewpoints or stances in general discussion
text.Agrawal et al (2003) used graph mining based
method to classify authors in to opposite camps for
a given topic. Paul et al (2010) developed an unsu-
pervised method for summarizing contrastive opin-
ions from customer reviews. Abu-Jbara et al (2012)
and Dasigi et al (2012) developed techniques to ad-
dress the problem of automatically detecting sub-
groups of people holding similar stances in a dis-
cussion thread.
Several prior studies have considered debates.
Cabrio and Villata (2012) developed a system based
on argumentation theory which recognizes the en-
tailment and contradiction relationships between
two texts. Awadallah et al (2011) used a debate
corpus as a seed for extracting person-opinion-topic
tuples from news and other web documents and in
later work classified the quotations to specific top-
ics and polarity using language models (Awadal-
lah et al, 2012). Somasundaran and Wiebe (2009)
and Anand et al (2011) were interested in ideolog-
ical content in debates, relying on discourse struc-
ture and leveraging sentiment lexicons to recognize
stances.
Closer to the methodology we describe, Lin et
al. (2008) presented a statistical model for politi-
cal discourse that incorporates both topics and ide-
ologies; they used debates on the Israeli-Palestinian
conflict. Fortuna et al (2009) showed that it is pos-
sible to isolate a subset of terms from media content
that are informative of a news organization?s bias to-
wards a particular issue. Ahmed and Xing (2010) in-
troduced multi-level latent Dirichlet alocation, and
Eisenstein et al (2011) introduced sparse additive
generative models, both conceived as extensions to
well-established probabilistic modeling techniques
(Blei et al, 2003); these were applied to debates
and political blog datasets. Our approach builds on
these models (especially the switching variables of
Ahmed and Xing). We go farther in jointly modeling
1866
text across many debates evidenced by the structure
of Debatepedia, thus grounding our models more
solidly in familiar sociopolitical issues, and in mak-
ing extensive use of existing NLP resources.
6 Conclusion
Using text from Debatepedia, we inferred topics and
position term lexicons in the domain of sociopoliti-
cal debates. Our approach brings together tools from
information extraction and sentiment analysis into a
latent-variable topic model and exploits the hierar-
chical structure of the dataset. Our qualitative and
quantitative evaluations show the model?s strengths
and weaknesses.
Acknowledgments
The authors thank several anonymous reviewers,
Justin Gross, David Kaufer, and members of the
ARK group at CMU for helpful feedback on this
work and gratefully acknowledge the assistance of
the annotators. This research is supported by the
Singapore National Research Foundation under its
International Research Centre@Singapore Funding
Initiative and administered by the IDM Programme
Office, by an A?STAR fellowship to Y.S., and by
Google?s support of the Reading is Believing project
at CMU.
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW
?03.
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of EMNLP.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
opinions-people network for politically controversial
topics. In Proceedings of CIKM.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2012. PolariCQ: Polarity classification of
political quotations. In Proceedings of CIKM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings of
ACL.
Sarah Cohen, James T. Hamilton, and Fred Turner. 2011.
Computational journalism. Communications of the
ACM, 54(10):66?71.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: a pilot study of implicit attitude using
latent textual semantics. In Proceedings of ACL.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statis-
tical learning methods. In Ashok N . Srivastava and
Mehran Sahami, editors, Text Mining: Classification,
Clustering, and Applications, pages 27?50. Chapman
& Hall/CRC.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of CIKM.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of documentation, 28(1):11?21.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of WSDM.
1867
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceedings of
CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW.
Marina Meila. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, Learning Theory and Kernel
Machines, volume 2777 of Lecture Notes in Computer
Science, pages 173?187. Springer.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceedings
of KDD.
David Newman, Chaitanya Chemudugunta, and Padhraic
Smyth. 2006. Statistical entity-topic models. In Pro-
ceedings of KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of EMNLP.
Keith Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, pages 118?178.
Yanchuan Sim, Brice Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proceedings of EMNLP.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary
Approach. Sage Publications Limited.
Ellen M. Voorhees. 1999. The trec-8 question answering
track report. In Proceedings of TREC.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of ICML.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT-EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of EMNLP.
1868
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1869?1879,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Unified Model for Topics, Events and Users on Twitter
Qiming Diao
Living Analytics Research Centre
School of Information System
Singapore Management University
qiming.diao.2010@smu.edu.sg
Jing Jiang
Living Analytics Research Centre
School of Information System
Singapore Management University
jingjiang@smu.edu.sg
Abstract
With the rapid growth of social media, Twitter
has become one of the most widely adopted
platforms for people to post short and instant
message. On the one hand, people tweets
about their daily lives, and on the other hand,
when major events happen, people also fol-
low and tweet about them. Moreover, people?s
posting behaviors on events are often closely
tied to their personal interests. In this paper,
we try to model topics, events and users on
Twitter in a unified way. We propose a model
which combines an LDA-like topic model and
the Recurrent Chinese Restaurant Process to
capture topics and events. We further propose
a duration-based regularization component to
find bursty events. We also propose to use
event-topic affinity vectors to model the asso-
ciation between events and topics. Our exper-
iments shows that our model can accurately
identify meaningful events and the event-topic
affinity vectors are effective for event recom-
mendation and grouping events by topics.
1 Introduction
Twitter is arguably the most popular microblog site
where people can post short, instant messages to
share with families, friends and the rest of the
world. For content analysis on Twitter, two impor-
tant concepts have been repeatedly visited: (1) Top-
ics. These are longstanding themes that many per-
sonal tweets revolve around. Example topics range
from music and sports to more serious ones like pol-
itics and religion. Much work has been done to an-
alyze topics on Twitter (Ramage et al, 2010; Hong
and Davison, 2010; Zhao et al, 2011; Lau et al,
2012). (2) Events. These are things that take place
at a certain time and attract many people?s short-
term attention in social media. Example events in-
clude concerts, sports games, scandals and elections.
Event detection on Twitter has been a hot research
topic in recent years (Petrovic? et al, 2010; Weng and
Lee, 2011; Becker et al, 2011; Diao et al, 2012; Li
et al, 2012).
The concepts of topics and events are orthogonal
in that many events fall under certain topics. For
example, concerts fall under the topic about mu-
sic. Furthermore, being social media, Twitter users
play important roles in forming topics and events on
Twitter. Each user has her own topic interests, which
influence the content of her tweets. Whether a user
publishes a tweet related to an event also largely de-
pends on whether her topic interests match the na-
ture of the event. Modeling the interplay between
topics, events and users can deepen our understand-
ing of Twitter content and potentially aid many pred-
ication and recommendation tasks. In this paper, we
aim to construct a unified model of topics, events
and users on Twitter. Although there has been a
number of recent studies on event detection on Twit-
ter, to the best of our knowledge, ours is the first that
links the topic interests of users to their tweeting be-
haviors on events.
Specifically, we propose a probabilistic latent
variable model that identifies both topics and events
on Twitter. To do so, we first separate tweets into
topic tweets and event tweets. The former are related
to a user?s personal life, such as a tweet complain-
ing about the traffic condition or wishing a friend
1869
happy birthday. The latter are about some major
global event interesting to a large group of people,
such as a tweet advertising a concert or comment-
ing on an election result. Although considering only
topic tweets and event tweets is a much simplified
view of the diverse range of tweets, we find it ef-
fective in finding meaningful topics and events. We
further use an LDA-like model (Blei et al, 2003) to
discover topics and the Recurrent Chinese Restau-
rant Process (Ahmed and Xing, ) to discover events.
Details are given in Section 3.1.
Our major contributions lie in two novel modi-
fications to the base model described above. The
first is a duration-based regularization component
that punishes long-term events (Section 3.2). Be-
cause events on Twitter tend to be bursty, this mod-
ification presumably can produce more meaningful
events. More specifically, we borrow the idea of us-
ing pseudo-observed variables to regularize graph-
ical models (Balasubramanyan and Cohen, 2013),
and carefully design the pseudo-observed variable
in our task to capture the burstiness of events. The
second modification is adding event-topic affinity
vectors inspired by PMF-based collaborative filter-
ing (Salakhutdinov and Mnih, 2008) (Section 3.3).
It uses the latent topics to explain users? preferences
of events and subsequently infers the association be-
tween topics and events.
We use a real Twitter data set consisting of 500
users to evaluate our model (Section 4). We find
that the model can discover meaningful topics and
events. Comparison with our base model and with
an existing model for event discovery on Twitter
shows that the two modifications are both effective.
The duration-based regularization helps find more
meaningful events; the event-topic affinity vectors
improve an event recommendation task and helps
produce a meaningful organization of events by top-
ics.
2 Related Work
Study of topics, events and users on Twitter is re-
lated to several branches of work. We review the
most interesting and relevant work below.
Event detection on Twitter: There have been quite
a few studies in this direction in recent years, in-
cluding both online detection (Sakaki et al, 2010;
Petrovic? et al, 2010; Weng and Lee, 2011; Becker et
al., 2011; Li et al, 2012) and offline detection (Diao
et al, 2012). Online detection is mostly concerned
with early detection of major events, so efficiency of
the algorithms is the main focus. These algorithms
do not aim to identify all relevant tweets, nor do
they analyze the association of events with topics. In
comparison, our work focuses on modeling topics,
events and users as well as their relation. Recently,
Petrovic? et al (2013) pointed out that Twitter stream
does not lead news stream for major news events, but
Twitter stream covers a much wider range of events
than news stream. Our work helps better understand
these additional events on Twitter and their relations
with users? topic interests. Our model bears similar-
ity to our earlier work (Diao et al, 2012), but we use
a non-parametric model (RCRP) to discover events
directly inside the probabilistic model.
Temporal topic modeling: A number of models
have been proposed for the temporal aspect of top-
ics (Blei and Lafferty, 2006; Wang and McCallum,
2006; Wang et al, 2007; Hong et al, 2011), but most
of them fix the number of topics. The Recurrent Chi-
nese Restaurant Process (Ahmed and Xing, ) was
proposed to model the life cycles of topics and al-
lows an infinite number of topics. It has later been
combined with LDA to model both topics and events
in news streams and social media streams (Ahmed
et al, 2011; Tang and Yang, 2012). Our work also
jointly models topics and events, but different from
previous work, we do not assume that every docu-
ment (tweet in our case) belongs to an event, which
is important because Twitter contains many personal
posts unrelated to major events.
Collaborative filtering with LDA: Part of our
model is inspired by work on collaborative fil-
tering based on probabilistic matrix factorization
(PMF) (Salakhutdinov and Mnih, 2008). Recently
there has been some work combining LDA with
PMF to recommend items with textual content such
as news articles and advertisements (Wang and Blei,
2011; Agarwal and Chen, 2010). They use topics to
interpret the latent structure of users and items. We
borrow their idea but our items are events, which are
not known and have to be discovered by our model.
1870
Figure 1: Plate notation for the whole model, in which pseudo-observed variables and distributions based on empirical
counts are shown as dotted nodes.
3 Our Model
In this section, we present our model for topics,
events and users on Twitter. We assume that we have
a stream of tweets which are divided into T epoches.
Let t ? {1, 2, . . . , T} be the index of an epoch.
Each epoch contains a set of tweets and each tweet
is a bag of words. We use wt,i,j ? {1, 2, . . . , V }
to denote the j-th word of the i-th tweet in the t-
th epoch, where V is the vocabulary size. The au-
thor of the i-th tweet in the t-th epoch (i.e. the
Twitter user who publishes the tweet) is denoted as
ut,i ? {1, 2, . . . , U}, where U is the total number of
Twitter users we consider.
We first present our base model in Section 3.1.
We then introduce a duration-based regularization
mechanism to ensure the burstiness of events in Sec-
tion 3.2. In Section 3.3 we discuss howwemodel the
relation between topics and events using event-topic
affinity vectors. Finally we discuss model inference
in Section 3.4.
3.1 The Base Model
Recall that our objective is to model topics, events,
users and their relations. As in many topic models,
our topic is a multinomial distribution over words,
denoted as ?a where a is a topic index. Each event is
also a multinomial distribution over words, denoted
as ?k where k is an event index. Because topics are
long-standing and stable, we fix the number of top-
ics to be A, where A can be tuned based on histor-
ical data. In contrast, events emerge and die along
the timeline. We therefore use a non-parametric
model called the Recurrent Chinese Restaurant Pro-
cess (RCRP) (Ahmed and Xing, ) to model the birth
and death of events. To model the relation between
users and topics, we assume each user u has a multi-
nomial distribution over topics, denoted as ?u.
As we have discussed, we separate tweets into two
categories, topic tweets and event tweets. Separa-
tion of these two categories is done through a latent
variable y sampled from a user-specific Bernoulli
distribution ?u. For topic tweets, the topic is sam-
pled from the corresponding user?s topic distribution
?u. For event tweets, the event is sampled accord-
ing to RCRP. We now briefly review RCRP. Gener-
ally speaking, RCRP assumes a Chinese Restaurant
Process (CRP) (Blackwell andMacQueen, 1973) for
items within an epoch and chains up the CRPs in ad-
jacent epochs along the timeline. Specifically, in our
case, the generative process can be described as fol-
lows. Tweets come in according to their timestamps.
In the t-th epoch, for the i-th tweet, we first flip a bi-
ased coin based on probability ?u to decide whether
this tweet is event-related. If it is, then we need to
decide which event it belongs to. It could be an ex-
isting event that has at least one related tweet in the
1871
previous epoch or the current epoch, or it could be a
new event. Let nk,t?1 denote the number of tweets
related to event k at the end of epoch (t ? 1). Let
n(i)k,t denote the number of tweets related to event k
in epoch t before the i-th tweet comes. Let Nt?1
denote the total number of event-related tweets in
epoch (t? 1) and N (i)t denote the number of event-
related tweets in epoch t before the i-th tweet. Then
RCRP assumes that the probability for the i-th tweet
to join event k is nk,t?1+n
(i)
k,t
Nt?1+N(i)t +?
and the probability
to start a new event is ?
Nt?1+N(i)t +?
, where ? is a
parameter. As we can see, RCRP naturally captures
the ?rich-get-richer? phenomenon in social media.
Finally we place Dirichlet and Beta priors on
the various parameters in our model. Formally,
the generative process of our base model is out-
lined in Figure 2, excluding the lines in bold and
blue. We also show the plate notation in Figure 1,
in which the Recurrent Chinese Restaurant Pro-
cess is represented as an infinite dynamic mixture
model (Ahmed and Xing, ) and ?rcrpt means the dis-
tribution on an infinite number of events in epoch t.
Dt is the total number of tweets (both event-related
and topic tweets), while Nt represents the number
event-related tweets in epoch t.
3.2 Regularization on Event Durations
As we have pointed out, events on Twitter tend to
be bursty, i.e. the duration of an event tends to
be short, but this characteristic is not captured by
RCRP. While there can be different ways to incor-
porate this intuition, here we adopt the idea of regu-
larization using pseudo-observed variables proposed
recently by Balasubramanyan and Cohen (2013).
We introduce a pseudo-observed binary variable rt,i
for each tweet, where the value of rt,i is set to 1
for all tweets. We assume that this variable is de-
pendent on the hidden variables y and s. Specif-
ically, if yt,i is 0, i.e. the tweet is topic-related,
then rt,i gets a value of 1 with probability 1. If
yt,i is 1, then we look at all the tweets that belong
to event st,i. Our goal is to make sure that this
tweet is temporally close to these other tweets. So
we assume that rt,i gets a value of 1 with proba-
bility exp(?
?T
t?=1,|t??t|>1 ?|t ? t?|nst,i,t?), where
nst,i,t? is the number of tweets in epoch t? that be-
? For each topic a = 1, . . . , A
- draw ?a ? Dirichlet(?)
? For each user u = 1, . . . , U
- draw ?u ? Dirichlet(?), ?u ? Beta(?)
? For each epoch t and tweet i
- draw yt,i ? Bernoulli(?ut,i)
- If yt,i = 0
* draw zt,i ? Multinomial(?ut,i)
* For each j, draw wt,i,j ? Multinomial(?zt,i)
- If yt,i = 1
* draw st,i from RCRP
* If st,i is a new event
. draw ?st,i ? Dirichlet(?)
. draw ?0st,i ? Gaussian(0, ?
?1)
. draw ?st,i ? Gaussian(0, ?
?1IA)
* draw rt,i ? Bernoulli(?st,i,t), where ?st,i,t =
exp(?
?T
t?=1,|t??t|>1 ?|t
? ? t|nst,i,t?)
* draw ct,i ? Gaussian(?0st,i +?
T
st,i ? z?ut,i , ?
?1)
* For each j, draw wt,i,j ? Multinomial(?st,i)
Figure 2: The generative process of our model, in which
the duration-based regularization (section 3.2) and the
event-topic affinity vector (section 3.3) are in blue and
bold lines.
long to event st,i and ? > 0 is a parameter. We can
see that when we factor in the generation of these
pseud-observed variables r, we penalize long-term
events and favor events whose tweets are concen-
trated along the timeline. Generation of these vari-
ables r is shown in bold and blue in Figure 2.
3.3 Event-Topic Affinity Vectors
So far in our model topics and events are not re-
lated. However, many events are highly related to
certain topics. For example, a concert is related to
music while a football match is related to sports. We
would like to capture these relations between top-
ics and events. One way to do it is to assume that
event tweets also have topical words sampled from
the event?s topic distribution, something similar to
the models by Ahmed et al (2011) and by Tang
and Yang (2012). However, our prelimiary exper-
iments show that this idea does not work well on
Twitter, mainly because tweets are too short. Here
we explore another approach inspired by recommen-
dation methods based on probabilistic matrix factor-
ization (Salakhutdinov and Mnih, 2008). The idea
is that when a user posts a tweet about an event, we
can treat the event as an item and this posting be-
1872
havior as adoption of the item. If we assume that the
adoption behavior is influenced by some latent fac-
tors, i.e. the latent topics, then basically we would
like the topic distribution of this user to be close to
that of the event.
Specifically, we assume that each event k has as-
sociated with it an A-dimensional vector ?k and a
parameter ?0k. The vector ?k represents the event?s
affinity to topics. ?0k is a bias term that represents
the inner popularity of an event regardless of its
affinity to any topic. We further assume that each
tweet has another pseudo-observed variable ct,i that
is set to 1. For topic tweets, ct,i gets a value of 1
with probability 1. For event tweets, ct,i is gener-
ated by a Gaussian distribution with mean equal to
?0st,i + ?st,i ? z?ut,i , where z?u is an A-dimensional
vector denoting the empirical topic distribution of
user u?s tweets. This treatment follows the practice
of fLDA by Agarwal and Chen (2010). Let C?u,a be
the number of tweets by user u assigned to topic a,
based on the values of the latent variables y and z.
Then
z?u,a =
C?u,a
?A
a?=1 C?u,a?
,
ct,i ?Gaussian(?0st,i + ?st,i ? z?ut,i , ?
?1),
where ? is a parameter. We generate ?k and ?0k using
Gaussian priors once event k emerges. The genera-
tion of the variables c is shown in bold and blue in
Figure 2.
3.4 Inference
We train the model using a stochastic EM sampling
scheme. In this scheme, we alternate between Gibbs
sampling and gradient descent. In the Gibbs sam-
pling part, we fix the values of ?0k and ?k for each
event k, and then we sample the latent variables yt,i
,zt,i and st,i for each tweet. In the gradient descent
part, we update the event-topic affinity vectors ?k
and the bias term ?0k of each event k by keeping the
assignment of the variables yt,i ,zt,i and st,i fixed.
For the Gibbs sampling part, we jointly sample
yt,i = 0, zt,i = a (topic tweet) and yt,i = 1, st,i = k
(event tweet) as follows:
Topic tweet:
p(yt,i = 0, zt,i = a|y?t,i, z?t,i,w, r, c, ut,i)
?
n(?)u,0 + ?
n(?)u,(.) + 2?
n(?)u,a + ?
n(?)u,(.) +A?
?V
v=1
?E(v)?1
i=0 (n
(?)
a,v + i+ ?)
?E(.)?1
i=0 (n
(?)
a,(.) + i+ V ?)
?
t?,i??Iu
N (ct?,i? |?0st?,i? + ?st?,i? ? z?
?
u, ??1)
N (ct?,i? |?0st?,i? + ?st?,i? ? z?u, ?
?1)
Event tweet:
p(yt,i = 1, st,i = k|y?t,i, z?t,i,w, r, c, ut,i)
?
n(?)u,1 + ?
n(?)u,(.) + 2?
1
N
(
nRCRPk,t N (ct,i|?0st,i + ?st,i ? z?u, ?
?1)
? exp(?
T
?
t?=1
|t??t|>1
?|t? t?|nk,t?)
)
?V
v=1
?E(v)?1
i=0 (n
(?)
k,v + i+ ?)
?E(.)?1
i=0 (n
(?)
k,(.) + i+ V ?)
in which,
nRCRPk,t =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(nk,t?1 + nk,t)
? nk,t+nk,t+1nk,t if nk,t?1 > 0, nk,t > 0,
nk,t?1 if nk,t?1 > 0, nk,t = 0,
nk,t+1 if nk,t+1 > 0, nk,t = 0,
? if k is a new event,
where we use u to represent ut,i. n(?)u,0 is the number
of topic tweets by user u while n(?)u,1 is the number
of event tweets by user u. They stem from integrat-
ing out the user?s Bernoulli distribution ?u. n(?)u,(.) is
the total number of tweets by user u. Similarly, n(?)u,a
is the number of tweets assigned to topic a for this
user, resulting from integrating out the user?s topic
distribution ?u. n(?)u,(.) is the same as n
(?)
u,0. E(v) is
the number of times word type v appears in the cur-
rent tweet, and E(.) is the total number of words in
the current tweet. n(?)a,v is the number of times word
type v is assigned to topic a, and n(?)a,(.) is the number
of words assigned to topic a. n(?)k,v is the number of
times word type v is assigned to event k, and n(?)k,(.)
is the total number of words assigned to event k.
These word counters stem form integrating out each
event?s word distribution and are set to zero when k
is a new event. Iu = {t?, i?|yt?,i? = 1, ut?,i? = u},
which is the set of event tweets published by user u,
and u represents ut,i for short. z??u is the empirical
1873
counting vector which considers the current tweet?s
topic assignment, while z?u and all other counters do
not consider the current tweet. Finally, N is a lo-
cal normalization factor for event tweets, which in-
cludes the RCRP, event-topic affinity and regulariza-
tion on event duration.
With the previous Gibbs sampling step, we can
get the assignment of variables yt,i ,zt,i and st,i.
Given the assignment, we use gradient descent to
update the values of the bias term ?0k and the event-
topic affinity vectors ?k for each current existing
event k. First, we can get the logarithm of the poste-
rior distribution:
lnP (y, z, s, r, c|w,u, all priors)
= constant ?
?
?
k=1
{ ?2(?
0
k
2 + ?k ? ?k)
+
U
?
u=1
nu,k
?
2 [1 ? (?
0
k + ?k ? z?u)]
2},
where nu,k is the number of event tweets about
event k published by user u. The derivative of the
logarithm of the posterior distribution with respect
to the bias term ?0k and the event-topic affinity vec-
tor ?k are as follows:
? lnP
??0k
= ???0k +
U
?
u=1
?nu,k[1 ? (?0k + ?k ? z?u)],
? lnP
??k
= ???k +
U
?
u=1
?nu,k[1 ? (?0k + ?k ? z?u)]z?u.
4 Experiment
4.1 Dataset and Experiment Setup
We evaluate our model on a Twitter dataset that con-
tains 500 users. These users are randomly selected
from a much larger pool of around 150K users based
in Singapore. Selecting users from the same coun-
try/city ensures that we find coherent and meaning-
ful topics and events. We use tweets published be-
tween April 1 and June 30, 2012 for our experi-
ments. For preprocessing, we use the CMU Twitter
POS Tagger1 to tag these tweets and remove those
non-standard words (i.e. words tagged as punc-
tuation marks, emoticons, urls, at-mentions, pro-
nouns, etc.) and stop words. We also remove tweets
1http://www.ark.cs.cmu.edu/TweetNLP/
with less than three words. After preprocessing, the
dataset contains 655,881 tweets in total.
Recall that our model is designed to identify top-
ics, events and their relations with users. We there-
fore would like to evaluate the quality of the iden-
tified topics and events as well as the usefulness of
the discovered topic distributions of users and event-
topic affinity vectors. Because our topic discovery
mechanism is fairly standard and a quick inspection
shows that the discovered topics are generally mean-
ingful and comparable to those discovered by stan-
dard LDA, here we do not focus on evaluation of
topics. In Section 4.2 we evaluate the quality of the
discovered events. In Section 4.3 we show how the
discovered event-topic affinity vectors can be useful.
For comparison, we consider an existing method
called TimeUserLDA introduced in our previous
work (Diao et al, 2012). TimeUserLDA also mod-
els topics and events by separating topic tweets from
event tweets. However, it groups event tweets into a
fixed number of bursty topics and then uses a two-
state machine in a postprocessing step to identify
events from these bursty topics. Thus, events are
not directly modeled within the generative process
itself. In contrast, events are inherent in our gener-
ative model. We do not compare with other event
detection methods because our objective is not on-
line event detection.
We also compare our final model with two de-
generate versions of it. We refer to the base model
described in Section 3.1 as Base and the model
with the duration-based regularization asBase+Reg.
Comparison with these two degenerate models al-
lows us to assess the effect of the two modifications
we propose. We refer to the final model with both
the duration-based regularization and the event-topic
affinity vectors as Base+Reg+Aff.
For the parameter setting, we empirically set A to
40, ? to 50A , ? to 1, ? to 0.01, ? to 1, ? to 10, ? to 1,
and the duration regularization parameter ? to 0.01.
When a new event k is created, the inner popular-
ity bias term ?0k is set to 1, and the factors in event-
topic affinity vectors ?k are all set to 0. We run the
stochastic EM sampling scheme for 300 iterations.
After Gibbs sampling assigns each variable a value
at the end of each iteration, we update the values of
?0k and ?k for the existing events using gradient de-
scent.
1874
Event Top words Duration Inner popularity (?0k)
debate caused by
Manda Swaggie
singapore, bieber, europe, amanda, justin, trending,
manda, hates, swaggie, hate
17 June - 19 June 0.9457
Indonesia tsunami tsunami, earthquake, indonesia, singapore, hit, warn-
ing, aceh, 8.9, safe, magnitude
10 April - 12 April 0.9439
SJ encore concert #ss4encore, cr, #ss4encoreday2, hyuk, 120526, super,
leader, changmin, fans, teuk
26 May - 28 May 0.8360
Mother?s Day day, happy, mother?s, mothers, love, mom, mum, ev-
eryday, mother, moms
11 May - 14 May 0.9370
April Fools? Day april, fools, day, fool, joke, prank, happy, today, trans,
fool?s
1 April - 3 April 0.9322
Table 1: The top-5 events identified by Base+Reg+Aff. We show the story name which is manually labeled, top ten
ranking words, lasting duration and the inner popularity (?0k) for each event.
4.2 Events
First we quantitatively evaluate the quality of the de-
tected events. Our model finds clusters of tweets
that represent events. We first assess whether these
events are meaningful. We then judge whether the
detected event tweets are indeed related to the corre-
sponding event.
Quality of Top Events
Method P@5 P@10 P@20 P@30
Base+Reg+Aff 1.000 1.000 0.950 0.900
Base+Reg 1.000 1.000 0.950 0.867
Base 0.000 0.200 0.250 0.367
TimeUserLDA 1.000 0.800 0.750 0.600
Table 2: Precision@K for the various methods.
Usually we are interested in the most popular
events on Twitter. We therefore assess whether the
top events are meaningful. For each method, we
rank the detected events based on the number of
tweets assigned to them and then pick the top-30
events for each method. We randomly mix these
events and ask two human judges to label them.
The judges are given 100 randomly selected tweets
for each event (or all tweets if an event contains
less than 100 tweets). The judges can use exter-
nal sources to help them. If an event is meaningful
based on the 100 sample tweets, a score of 1 is given.
Otherwise it is scored 0. The inter-annotator agree-
ment score is 0.744 using Cohen?s kappa, showing
substantial agreement. Finally we treat an event as
meaningful if both judges have scored it 1.
Table 2 shows the performance in terms of
precision@K, and Table 1 shows the top 5 events
of our model (i.e., Base+Reg+Aff). We have the
following findings from the results: (1) Our base
model performs quite poorly for the top events while
Base+Reg and Base+Reg+Aff perform much bet-
ter. This shows that the duration-based regular-
ization is critical in finding meaningful events. A
close examination shows that the base model clus-
ters many general topic tweets as events, such as
tweets about transportation and music and even
foursquare tweets. (2) TimeUserLDA performs well
for the very top events (P@5 and P@10) but its
performance drops for lower-ranked events (P@20
and P@30), similar to what was reported by Diao
et al (2012). A close examination shows that this
method is good at finding major events that do not
have strong topic association and thus attract most
people?s attention, e.g. earthquakes, but not good at
finding topic-oriented events such as some concerts
and sports games. This is because this method mixes
topics and events first and only detects events from
bursty topics in a second stage of postprocessing. In
contrast, our model performs well for topic-oriented
events. (3) The difference between Base+Reg and
Base+Reg+Aff is small, suggesting that the event-
topic affinity vectors are not crucial for event detec-
tion.
Precision of Event Tweets
Next, we evaluate the relevance of the detected
event tweets to each event. To make a fair compar-
ison, we select only the common events identified
by all the methods. We pick 3 out of 5 common
events shared by all methods within top-30 events
1875
Event TimeUserLDA Base Base+Reg Base+Reg+Aff
Father?s Day 0.61 0.63 0.71 0.72
debate caused by Manda Swaggie 0.73 0.74 0.84 0.80
Indonesia tsunami 0.75 0.75 0.82 0.80
Super Junior album release N/A 0.72 0.78 0.81
Table 3: Precision of the event tweets for the 4 common events.
(we pick ?Fathers? day? to represent public festi-
vals, and ignore the similar events ?Mothers? day?
and ?April fools?). We also pick one event shared
by three RCRP based models. We further ask one
of the judges to score the 100 tweets as either 1 or 0
based on their relevance to the event. The precision
of the 100 tweets for each event and each method is
shown in Table 3. We can see that again Base+Ref
and Base+Ref+Aff perform similarly, and both out-
perform the other two methods. We also take a
close look at the tweets and find that the false posi-
tives mislabeled by Base is mainly due to the long-
duration of the discovered events. For example, for
the event ?Super Junior album release,? Base finds
other music-related tweets surrounding the peak pe-
riod of the event itself.
In summary, our evaluation on event quality
shows that (1) Using the non-parametric RCRP
model to identify events within the generative
model itself is advantageous over TimeUserLDA,
which identifies events by postprocessing. (2) The
duration-based regularization is crucial for finding
more meaningful events.
4.3 Event-Topic Association
Besides event identification, our model also finds the
association between events and topics through the
event-topic affinity vectors. The discovered event-
topic association can potentially be used for various
tasks. Here we conduct two experiments to demon-
strate its usefulness.
Event Recommendation
Recall that to discover event-topic association, we
treat an event as an item and a tweet about the event
as indication of the user?s adoption of the item. Fol-
lowing this analogy with item recommendation, we
define an event recommendation task where the goal
is to recommend an event to users who have not
posted any tweet about the event but may potentially
be interested in the event. Intuitively, if a user?s topic
distribution is similar to the event-topic affinity vec-
tor of the event, then the user is likely to be inter-
ested in the event.
Specifically, we use the first two months? data
(April and May 2012) as training data to learn all
the users? topic distributions. We then use a ransom
subset of 250 training users and their tweets in June
to identify events in June as well as the event-topic
affinity vectors of these events. We pick 8 meaning-
ful events that are ranked high by all methods for
testing. For each event, we try to find among the
remaining 250 users those who may be interested
in the event and compare the results with ground
truth obtained by human judgment. Because it is
time consuming to obtain the ground truth for all 250
users, we randomly pick 100 of these 250 users for
testing purpose. For each test user and each event,
we manually inspect the user?s tweets around the
peak days of the event to judge whether she has com-
mented on the event. This is used as ground truth.
With our complete model Base+Reg+Aff, we can
simply rank the 100 test users in decreasing order of
?k ? z?u. For the other methods, because we do not
have any parameter that directly encodes event-topic
association, we cannot rank users based on how sim-
ilar their topic distributions are to the event?s affinity
to topics. We instead adopt a collaborative filtering
strategy and rank the test users by their similarity
with those training users who have tweeted about
the event. Specifically, each of these methods pro-
duces a topic distribution ?u for each user. In addi-
tion, for each test event these methods identify a list
of training users who have tweeted on it. By taking
the average topic distribution of these training users
and compute its cosine similarity with a test user?s
topic distribution, we can rank the 100 test users.
Since we have turned the recommendation task
into a ranking task, we use Average Precision, a
commonly used metric in information retrieval, to
compare the performance. Average Precision is the
1876
Event TimeUserLDA Base Base+Reg Base+Reg+Aff Inner popularity (?0k)
debate caused by Manda Swaggie 0.3533 0.3230 0.3622 0.2956 0.943
Father?s Day 0.3811 0.3525 0.3596 0.4362 0.917
Big Bang album release 0.1406 0.1854 0.1533 0.1902 0.893
City Harvest Church scandal N/A 0.2832 0.1874 0.3347 0.890
Alex Ong pushing an old lady N/A 0.1540 0.1539 0.1113 0.876
final episode of Super Spontan (reality show) N/A 0.0177 0.0331 0.2900 0.862
Super Junior album release N/A 0.0398 0.0330 0.5900 0.792
LionsXII 9-0 Sabah FA (soccer) 0.0711 0.1207 0.2385 0.3220 0.773
MAP N/A 0.1845 0.1901 0.3213
Table 4: For the 8 test events that happened in June 2012, we compute the Average Precision for each event. We also
show the Mean Average Precision (MAP) when applicable.
Topic Top words of the topic Related event Top words of the event
Food eat, food, eating, ice, hungry, din-
ner, cream, lunch, chicken, buy
Ben&Jerry free cone day free, cone, day, ben, jerry?s, today, b&j, zoo,
#freeconeday, singapore
Super Junior encore concert #ss4encore, cr, #ss4encoreday2, hyuk,
120526, super, leader, changmin, fans, teuk
Korean Music
music, big, cr, super, bang, junior,
love, concert, bank, album
Super Junior Shanghai concert #ss4shanghai, cr, 120414, donghae, eun-
hyuk, giraffe, solo, hyuk, ryeowook, shang-
hai
Super Junior Paris concert #ss4paris, cr, paris, super, 120406, ss4, ju-
nior, siwon, show, update
Malay aku, nak, tak, kau, ni, lah, tk, je,mcm, nk
final episode of Super Spontan zizan, johan, friendship, jozan, #superspon-
tan, skips, forever, real, juara, gonna
LionsXII 9-0 Sabah FA sabah, 9-0, #lionsxii, lions, singapore, 7-0,
amet, sucks, sabar, goal
Soccer win, game, man, chelsea, match,city, goal, good, united, team
Man City crowned English champions man, city, united, qpr, fuck, bored, lah, love,
glory, update
Table 5: Example topics and their corresponding correlated events.
average of the precision value obtained for the set
of top items existing after each relevant item is re-
trieved (Manning et al, 2008). We also rank the
8 events in decreasing order of their inner popular-
ity ?0k learned by our complete model. The results
are shown in Table 4. We have the following find-
ings from the table. (1) Our complete method out-
performs the other methods for 6 out of the 8 test
events, suggesting that with the inferred event-topic
affinity vectors we can do better event recommen-
dation. (2) The improvement brought by the event-
topic affinity vectors, as reflected in the difference in
Average Precision between Base+Reg+Aff and Base
(or Base+Reg) is more pronounced for events with
lower inner popularity. Recall that the inner popu-
larity of an event shows the inherent popularity of
an event regardless of its association with any topic,
that is, an event with high inner popularity attracts
attention of many people regardless of their topic
interests, while an event with low inner popularity
tends to attract attention of certain people with simi-
lar topic interests. The finding above suggests that
the event-topic affinity vectors are especially use-
ful for recommending events that attract only certain
people?s attention, such as those related to sports,
music, etc.
One may wonder for the events with low inner
popularity why we could not achieve the same ef-
fect by Base or Base+Reg where we consider the
topic similarity of test users with training users who
have tweeted about the event. Our close examina-
tion shows that for these events although Base and
Base+Reg may identify relevant event tweets with
decent precision, the users they identify who have
1877
tweeted about the event may not share similar topic
interests. As a result, when we average these users?
topic interests, we cannot obtain a clear skewed
topic distribution that explains the event?s affinity
to different topics. In contrast, Base+Reg+Aff ex-
plicitly models the event-topic affinity vector and
prefers to assign a tweet to an event if its author?s
topic distribution is similar to the event?s affinity
vector. Through the training iterations, the users
who have tweeted about an event as identified by
Base+Reg+Aff will gradually converge to share sim-
ilar topic distributions.
Grouping Events by Topics
Finally, we show that the event-topic affinity vec-
tors can also be used to group events by topics. This
can potentially be used to better organize and present
popular events in social media. In Table 5 we show
a few highly related events for a few popular topics
in our Twitter data set. Specifically given a topic a
we rank the meaningful events that contain at least
70 tweets based on ?k,a. We can see from the table
that the events are indeed related to the correspond-
ing topic. The event ?LionsXII 9-0 Sabah FA? is
particularly interesting in that it is highly related to
both the topic on Malay and the topic on soccer. (Li-
onsXII is a soccer team from Singapore and Sabah
FA is a soccer team from Malaysia.)
5 Conclusion
In this paper, we propose a unified model to study
topics, events and users jointly. The base of our
method is a combination of an LDA-like model and
the Recurrent Chinese Restaurant Process, which
aims to model users? longstanding personal topic in-
terests and events over time simultaneously. The Re-
current Chinese Restaurant Process is appealing in
the sense that it provides a principled dynamic non-
parametric model in which the number of events is
not fixed overtime. We further use a time duration-
based regularization to capture the fast emergence
and disappearance of events on Twitter, which is
effective to produce more meaningful events. Fi-
nally, we use an inner popularity bias parameter and
event-topic affinity vectors to interpret an event?s
inherent popularity and its affinity to different top-
ics. Our experiments quantitatively show that our
proposed model can effectively identify meaningful
events and accurately find relevant tweets for these
events. Furthermore, the event-topic association in-
ferred by our model can help an event recommenda-
tion task and organize events by topics.
6 Acknowledgments
This research is supported by the Singapore National
Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office. We
thank the reviewers for their valuable comments.
References
Deepak Agarwal and Bee-Chung Chen. 2010. fLDA:
matrix factorization through latent Dirichlet aloca-
tion. In Proceedings of the third ACM international
conference on Web search and data mining, pages 91?
100.
Amr Ahmed and Eric P. Xing. Dynamic non-parametric
mixture models and the recurrent Chinese restaurant
process: with applications to evolutionary clustering.
In Proceedings of the SIAM International Conference
on Data Mining, SDM 2008.
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J. Smola, and Choon Hui Teo. 2011. Uni-
fied analysis of streaming news. In In Proceedings of
the 20th international conference on World wide web,
pages 267 ? 276.
Ramnath Balasubramanyan and William W. Cohen.
2013. Regularization of latent variable models to ob-
tain sparsity. In Proceedings of SIAM Conference on
Data Mining.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Be-
yond trending topics: Real-world event identification
on twitter. In Fifth International AAAI Conference on
Weblogs and Social Media.
D. Blackwell and J. MacQueen. 1973. Ferguson distribu-
tions via Polya urn schemes. The Annals of Statistics,
pages 353?355.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd international
conference on Machine learning, pages 113 ? 120.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 536 ? 544.
1878
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings of
the First Workshop on Social Media Analytics, pages
80?88.
Liangjie Hong, Byron Dom, Siva Gurumurthy, and
Kostas Tsioutsiouliklis. 2011. A time-dependent topic
model for multiple text streams. In Proceedings of
SIGKDD.
Jey Han Lau, Nigel Collier, and Timothy Baldwin. 2012.
On-line trend analysis with topic models: #twitter
trends detection topic model online. In Proceedings
of the 24th International Conference of on Computa-
tional Linguistics, pages 1519?1534.
Chenliang Li, Aixin Sun, and Anwitaman Datta. 2012.
Twevent: segment-based event detection from tweets.
In In Proceedings of the 21st ACM international con-
ference on Information and knowledge management,
pages 155 ? 164.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze, 2008. Introduction to Information Re-
trieval, chapter Evaluation in information retrieval.
Cambridge University Press.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to Twitter. In The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 181 ? 189.
Sas?a Petrovic?, Miles Osborne, Richard McCreadie,
RichardMacdonald, Richard Ounis, and Luke Shrimp-
ton. 2013. Can twitter replace newswire for breaking
news? In Proceedings of the 7th International Confer-
ence on Weblogs and Social Media.
Daniel Ramage, Susan T. Dumais, and Daniel J. Liebling.
2010. Characterizing microblogs with topic models.
In Proceedings of the 4th International Conference on
Weblogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on World Wide Web,
pages 851?860.
Ruslan Salakhutdinov and Andriy Mnih. 2008. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems, volume 20.
Xuning Tang and Christopher C. Yang. 2012. TUT:
a statistical model for detecting trends, topics and
user interests in social media. In Proceedings of the
21st ACM international conference on Information
and knowledge management, pages 972 ? 981.
Chong Wang and David M. Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 448 ? 456.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining., pages 424 ? 433.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Proceedings of the 13th acm sigkdd
international conference on knowledge discovery and
data mining. In Proceedings of SIGKDD, pages 784 ?
793.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the Fifth International
Conference on Weblogs and Social Media.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing Twitter and traditional media using topic
models. In Proceedings of the 33rd European Confer-
ence on IR Research, pages 338?349.
1879
Proceedings of NAACL-HLT 2013, pages 401?410,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Mining User Relations from Online Discussions using Sentiment Analysis
and Probabilistic Matrix Factorization
Minghui Qiu?, Liu Yang?,?, Jing Jiang?
? School of Information Systems, Singapore Management University, Singapore
? School of Software and Microelectronics, Peking University, China
{minghui.qiu.2010,jingjiang}@smu.edu.sg, yang.liu@pku.edu.cn
Abstract
Advances in sentiment analysis have enabled
extraction of user relations implied in online
textual exchanges such as forum posts. How-
ever, recent studies in this direction only con-
sider direct relation extraction from text. As
user interactions can be sparse in online dis-
cussions, we propose to apply collaborative
filtering through probabilistic matrix factor-
ization to generalize and improve the opinion
matrices extracted from forum posts. Exper-
iments with two tasks show that the learned
latent factor representation can give good per-
formance on a relation polarity prediction task
and improve the performance of a subgroup
detection task.
1 Introduction
The fast growth of the social Web has led to a large
amount of interest in online social network analysis.
Most existing work on social network analysis re-
lies on explicit links among users such as undirected
friendship relations (Liben-Nowell and Kleinberg,
2003), directed following relations (Hopcroft et al,
2011) and trust/distrust relations (Leskovec et al,
2010). However, besides these explicit social rela-
tions, the various kinds of interactions between on-
line users often suggest other implicit relations. In
particular, in online discussion forums, users inter-
act through textual posts and these exchanged texts
often reveal whether two users are friends or foes, or
whether two users share the same viewpoint towards
a given issue.
To uncover such implicit relations requires text
analysis and particularly sentiment analysis. Re-
cently, Hassan et al (2012) studied predicting the
polarity of user interactions in online discussions
based on textual exchanges. They found that the au-
tomatically predicted signed relations had an accu-
racy above 80%. The extracted signed network was
further used to detect ideological subgroups. This is
a piece of pioneering work that extracts online social
relations based on text analysis.
In this paper, we further extend the idea of mining
social relations from online forum posts by incorpo-
rating collaborative filtering. Our work is motivated
by the observation that direct textual exchanges be-
tween users are sparse. For example, in the data set
we use, only around 13% of user-user pairs have di-
rect interactions. Collaborative filtering is a com-
monly used technique in recommender systems to
predict missing ratings. The key assumption is that
if two people have the same opinion on an item A,
they are likely to also have the same opinion on a
different item B. In online discussion forums, users
express their opinions about each other as well as
the various aspects of the topic under discussion, but
not every user comments on every aspect or every
other user. Collaborative filtering allows us to iden-
tify users with the same opinion even if they have not
directly interacted with each other or commented on
any common aspect.
Our method starts with extracting opinions on
users and topic aspects from online posts using sen-
timent analysis. The results are two matrices indi-
cating the sentiment polarity scores between pairs
of users and pairs of a user and an aspect. To in-
corporate collaborative filtering, we choose proba-
bilistic matrix factorization (PMF) (Salakhutdinov
401
and Mnih, 2008), a technique that has been success-
fully applied for collaborative filtering-based recom-
mendation problems. PMF automatically discovers
a low-rank representation for both users and items
based on observed rating data. In our problem, the
predicted sentiment polarity scores are treated as rat-
ing data, and the results of PMF are low-rank vectors
representing each user in online discussions.
We evaluate our method on two tasks. The first
is to predict the polarity of interactions between two
users not from their own textual exchanges but from
their interactions with other users or comments on
topic aspects. The second is to use the latent vectors
to group users based on viewpoints. We find that the
latent factor representation can produce good predic-
tion results for the first task and improve the cluster-
ing results of the second task compared with a num-
ber of baselines, showing the effectiveness of col-
laborative filtering for mining social relations from
online discussions.
2 Related Work
Our work is closely related to recent studies on
detecting subgroups from online discussions (Abu-
Jbara et al, 2012; Dasigi et al, 2012; Hassan et
al., 2012). Abu-Jbara et al (2012) proposed to
build discussant attitude profiles (DAP) from on-
line posts and use these profiles to cluster users into
subgroups. A DAP is a vector that contains the
attitudes of a discussant towards other discussants
and a set of opinion targets. We also extract opin-
ions of users towards other users and opinion tar-
gets from posts, which are similar to DAPs. The
difference is that we further apply probabilistic ma-
trix factorization to derive a low-rank representation
from the raw opinion scores. Our comparison with
DAP-based clustering shows that probabilistic ma-
trix factorization can improve subgroup detection.
Hassan et al (2012) proposed to predict the polar-
ity of interactions between users based on their tex-
tual exchanges. They defined a set of interaction
features using sentiment analysis and applied super-
vised learning for polarity prediction. In compari-
son, our work is unsupervised, that is, we do not use
any ground truth of interaction polarity for training.
Probabilistic matrix factorization was proposed
by Salakhutdinov and Mnih (2008) as a collabo-
rative filtering method for recommender systems.
It has attracted much attention and been extended
by Ma et al (2008) and Wang and Blei (2011).
In particular, Ma et al (2008) proposed a SocRec
model that combines social network information
with rating data using the PMF framework to per-
form social recommendation. Our model bears sim-
ilarity to SocRec in that we also consider two types
of interactions, i.e. user-user interactions and user-
aspect interactions. However, different from Ma et
al. (2008), we predict both the user-user and user-
aspect scores from textual posts using sentiment
analysis, and the user-user opinion polarity scores
are symmetric.
Part of our method uses sentiment analysis to ex-
tract opinions from text. This is built on top of a
large body of existing work on opinion extraction,
e.g. Choi et al (2006) and Wu et al (2009). As the
sentiment analysis component is not our main con-
tribution, we do not review existing work along this
direction in detail here. Interested readers can refer
to Pang and Lee (2008).
The idea of incorporating sentiment analysis into
collaborative filtering algorithms has been explored
by Kawamae (2011), Moshfeghi et al (2011) and
Leung et al (2011). While their work also com-
bines sentiment analysis with collaborative filtering,
the purpose is to improve the accuracy of item rec-
ommendation. In contrast, we borrow the idea and
technique of collaborative filtering to improve user
relation mining from online text.
3 Method Overview
In this section, we provide an overview of our
method. We first introduce some concepts.
User: We use user to refer to a discussant in an on-
line discussion. Each user has an online ID, which
can be used by other users to refer to him/her in a
post. Users are both opinion holders and opinion
targets. For example, User 1 below expresses a neg-
ative opinion towards another user in the following
snippet.
User 1: Actually, I have to disagree with you.
Aspect: We use topic aspect or aspect to refer to an
opinion target that is related to the topic under dis-
cussion. For example, when debating about whether
one should vote for Obama, people may express
402
opinions on targets such as ?President Obama? and
?Republican party,? as shown in the following snip-
pets. These aspects are all related to Obama?s pres-
idential campaign. As we will explain later, the as-
pects we consider are named entities and frequent
noun phrases.
User 2: Americans should vote for President Obama be-
cause he picks good corporations as winners.
User 3: I simply point out how absolutely terrible the Re-
publican party is.
Polarity Score: A sentiment polarity score is a
real number between 0 and 1, where 0 indicates a
completely negative opinion and 1 indicates a com-
pletely positive opinion.
User-User Opinion Matrix: The opinions ex-
tracted from posts between users are represented by
a user-user opinion matrix S, where entry si,j is a
polarity score between the i-th user and the j-th user.
We assume that the polarity scores are symmetric.
User-Aspect Opinion Matrix: The opinions held
by different users on the various topic aspects are
represented by a user-aspect opinion matrix R,
where entry ri,k is a polarity score indicating the i-th
user?s opinion towards the k-th aspect.
Given the matrices S and R, we perform proba-
bilistic matrix factorization to derive a low-rank vec-
tor representation for users and aspects such that if
the polarity score between two users or a user and
an aspect is high, the dot product between the corre-
sponding two vectors is also high.
In Section 4, we will explain in detail how we
identify topic aspects from a discussion thread and
how we obtain polarity scores from posts. In Sec-
tion 5, we will present the details of our probabilistic
matrix factorization model.
4 Construction of Opinion Matrices
The opinion matrices are constructed from a single
forum thread discussing some controversial topic.
4.1 Aspect Identification
As we have pointed out, there are two kinds of opin-
ion targets, namely users and aspects. Users are
clearly defined and can often be identified in posts
by their IDs or second person pronouns. For aspects,
however, there is not a pre-defined set. We observe
that these topic aspects are usually named entities
or noun phrases frequently mentioned. We therefore
use the OpenNLP toolkit1 to perform chunking and
obtain noun phrases and the Standford NER tagger2
to identify named entities from the posts.
Some of the candidate aspect phrases identified
above actually refer to the same actual aspect, e.g.
?Obama voter,? ?Obama voters? and ?the Obama
voter.? We remove stop words from each candidate
phrase and use the WordNet by Miller (1995) to ob-
tain the lemma of each word such that we can nor-
malize the candidate aspect phases to some extent.
Finally, to select salient aspects for a given discus-
sion topic, we count the number of times each candi-
date aspect has been expressed a positive or negative
opinion on by all users, and select those candidate
aspects which have opinion expressions from at least
M users. We set M to 2 in our experiments. Fig-
ure 1 shows the top salient aspects for the thread on
?Will you vote for Obama?? We acknowledge there
are still duplicate aspects in the results like ?Repub-
lican Party? and ?GOP?. To normalize these aspects,
some additional information such as Wikipedia en-
tries and Google snippets may be considered. We
will study this problem in our future work.
4.2 Opinion Expression Identification
Our next step is to identify candidate opinion expres-
sions. This problem has been studied in Hu and Liu
(2004), Popescu and Etzioni (2005), and Hassan
and Radev (2010). Based on previous work, we do
the following. We first combine three popular sen-
timent lexicons to form a single sentiment lexicon:
the lexicon used in Hu and Liu (2004), MPQA Sub-
jectivity Lexicon by Wilson et al (2005) and Senti-
WordNet by Baccianella et al (2010). Our final sen-
timent lexicon contains 15,322 negative expressions
and 10,144 positive expressions. We then identify
candidate opinion expressions by searching for oc-
currences of words in this lexicon in the posts.
4.3 Opinion Relation Extraction
Given a post that contains an aspect and an opin-
ion expression, we still need to determine whether
the opinion expression is used to describe the as-
pect. This is a relation extraction problem. We use a
supervised learning approach based on dependency
1http://opennlp.apache.org/
2http://nlp.stanford.edu/ner/index.shtml
403
0
20
40
60
80
100
OBAM
A BUSHAMER
ICA PALIN
REPU
BLICA
N
CONG
RESSTAX_
CUT
AME
RICA
N
CLIN
TONMCC
AIN
TEA_
PART
Y IRAQ
SARA
H_PA
LIN
PRES
IDEN
T_OB
AMAREAG
AN
RON_
PAUL
ECON
OMIC
_POL
ICY
AFGH
ANIS
TANCART
ER FOX
HEAL
TH_C
ARE
NATI
ONAL
_DEB
T
DEM
OCRA
T GOP
MIDD
LE_C
LASS
OBAM
A_AD
MINIS
TRAT
ION
REPU
BLICA
N_PA
RTY
TAX_
BREA
K
WAS
HING
TON
FEDE
RAL_
GOVE
RNM
ENT
HEAL
TH_C
ARE_
REFO
RMHITLE
R
IRAQ
_WAR
WAL
L_ST
REET
Figure 1: Salient aspects and number of users who express opinions on them in the thread ?Will you vote for Obama??
ID Dependency path rule Example
R1 ADJOP ? amod? NTR I simply point out how terrible REPUBLICAN PARTY is.
R2 ADJOP ? nsubj ? NTR BUSH is even more reasonable for tax hike than Obama.
R3 VOP ? dobj ? NTR I would never support OBAMA.
R4 VOP ? prep ? ? NTR I?ll vote for OBAMA.
R5 VOP ? nsubjpass? NTR DEMOCRATIC PARTY are ultimately corrupted by love of money.
R6 NOP ? dobj ? V ? nsubj ? NTR PAKISTAN is increasing terrorist threat.
R7 ADJOP ? amod? N ? nsubj ? NTR OBAMA was a top scorer for occidental college.
R8 ADVOP ? advmod? V ? nsubj ? NTR OBAMA is smarter than people.
Table 1: Examples of frequent dependency path rules in our training data. OP and TR refer to the opinion and the
target. The opinion words are in italic and the aspect words are in uppercase.
paths. Previous work by Mintz et al (2009), and Qiu
et al (2009) has shown that the shortest path be-
tween a candidate opinion aspect and a candidate
opinion expression in the dependency parse tree can
be effective in extracting opinion relations. We use
the Stanford Parser from Klein and Manning (2003)
to obtain the dependency parse trees for each sen-
tence in the posts and then get the dependency paths
between each pair of candidate aspect and opinion
expression. We use dependency relations and POS
tags of nodes along the path to represent a depen-
dency path. Given a set of training sentences (we
use the one from Wu et al (2009)), we can get a set
of dependency path rules based on their frequencies
in the training data. Table 1 shows the frequent de-
pendency path rules in our training data.
When a pair of aspect and opinion expression is
identified to be related, we use the polarity of the
opinion expression to label the relation. Finally,
given a pair of users, we use the percentage of pos-
itive interactions between them over all subjective
interactions (i.e. interactions with either positive or
negative opinions) as extracted from their exchanged
posts as the sentiment polarity score between the
two users, regardless of the reply-to direction of
the posts. Similarly, given a user and an aspect,
we also use the percentage of positive opinion re-
lations extracted as the sentiment polarity score be-
tween them. Thus the user-user opinion matrix and
the user-aspect opinion matrix are constructed. If
there is no subjective interaction detected between
two users or between a user and an aspect, the cor-
responding entry in the matrix is left empty. We will
see later that empty entries in the matrices are not
used in the probabilistic matrix factorization step.
5 Probabilistic Matrix Factorization
As we have pointed out earlier, a problem with the
matrices extracted as described in Section 4 is that
the matrices are sparse, i.e. many entries are empty.
For the data set we use, we find that around 87% of
entries in the user-user opinion matrix and around
90% of entries in the user-aspect opinion matrix are
empty. In this section, we describe how we use
Probabilistic Matrix Factorization (PMF) to repre-
sent users and aspects in a latent factor space and
thus generalize the user preferences.
Our model is almost a direct application of proba-
404
bilistic matrix factorization from Salakhutdinov and
Mnih (2008), originally proposed for recommender
systems. The main difference is that the user-user
opinion polarity scores are symmetric. Our model is
also similar to the one used by Ma et al (2008). We
describe our model as follows.
We assume that there are K latent factors with
which both users and aspects can be represented. Let
ui ? RK denote the vector in the latent factor space
for the i-th user, and ak the vector for the k-th aspect.
Recall that the opinions extracted from posts be-
tween users are represented by a user-user opinion
matrix S, and the opinions held by different users on
the various topic aspects are represented by a user-
aspect opinion matrix R. We assume that the polar-
ity scores si,j between the i-th and the j-th users and
ri,k between the i-th user and the k-th aspect in the
two matrices S and R are generated in the following
way:
p(si,j |ui, uj , ?
2
1) = N (si,j |g(u
T
i uj), ?
2
1),
p(ri,k|ui, ak, ?
2
2) = N (ri,k|g(u
T
i ak), ?
2
2),
where ?21 and ?
2
2 are variance parameters, g(?) the
logistic function, and N (?|?, ?2) is the normal dis-
tribution with mean ? and variance ?2.
We can see that with this generative assumption,
if two users are similar in terms of their dot product
in the latent factor space, then they are more likely
to have positive interactions as extracted from their
textual exchanges. Similarly, if a user and an aspect
are similar, then the user is more likely to express a
positive opinion on the aspect in his/her posts. The
latent factors can therefore encode user preferences
and similarity between two users in the latent factor
space reflects whether they share similar viewpoints.
We also place the following prior over ui and ak:
p(ui|?
2
U ) = N (ui|~0, ?
2
UI),
p(ak|?
2
A) = N (ak|~0, ?
2
AI),
where ?2U and ?
2
A are two variance parameters for
users and aspects, respectively, and I is the identify
matrix.
Figure 2 shows the plate notation for the genera-
tive model.
Let U be aK?U matrix containing the vectors ui
for allU users, andA be anK?Amatrix containing
Figure 2: Probabilistic matrix factorization model on
opinion matrices.
the vectors ak for all A aspects. To automatically
learn U andA, we minimize the following objective
function:
L(U ,A,S,R)
=
1
2
U?
i=1
A?
k=1
I(ri,k)(ri,k ? g(uTi ak))2
+
?1
2
U?
i=1
U?
j=1
I(si,j)(si,j ? g(uTi uj))2
+
?U
2
||U||2F +
?A
2
||A||2F , (1)
where ? = ?
2
1
?22
, ?U =
?21
?2U
, and ?A =
?21
?2A
, I(s) is
an indicator function which equals 1 when s is not
empty and otherwise 0.
To optimize the objective function above, we can
perform gradient descent on U and A to find a local
optimum point. The derivation is similar to Ma et al
(2008).
Degenerate Versions of the Model
We refer to the complete model described above
as PMF-UOM (PMF model based on User Opinion
Matrices). PMF-UOM has the following two degen-
erate versions by considering either only the user-
user opinion matrix or only the user-aspect opinion
matrix.
PMF-UU: In this degenerate version of the model,
we use only the user-user opinion matrix to learn the
latent factor representation. Specifically, the objec-
tive function is modified such that we drop the sum
405
of the square errors involving R and the regularizer
on A.
PMF-UA: In this degenerate version of the model,
we use only the user-aspect opinion matrix to learn
the latent factor representation. Specifically, the ob-
jective function is modified such that we drop the
sum of the square errors involving S.
6 Experiments
In this section, we present our experiments that eval-
uate our model.
6.1 Data Set and Experiment Settings
The data set we use comes from Abu-Jbara et al
(2012) and Hassan et al (2012). The data set
contains a set of discussion threads collected from
two political forums (Createdebate3 and Politicalfo-
rum4) and one Wikipedia discussion session. We
randomly select 6 threads from the original data set
to evaluate our model. Some details of the data we
use are listed in Table 2.
ID topic #sides #sentences #users
DS1 Vote for Obama 2 12492 197
DS2 Abortion Banned 6 3844 70
DS3 Profile Muslims 4 2167 69
DS4 England and USA 6 2030 62
DS5 Tax Cuts 2 1193 26
DS6 Political Spectrum 7 1130 50
Table 2: Some statistics of the data sets.
In our experiments, for the PMF-based methods,
we set the number of latent factors to be 10 as we
do not observe big difference when vary the latent
factor size from 10 to 50. For the other parame-
ters, we select the optimal setting for each thread
based on the average of 50 runs. ?U is chosen
from {0.1, 0.01}, ?A from {0.01, 0.001} and ? from
{1, 0.1}.
6.2 Relation Polarity Prediction
The first task we use to evaluate our model is to pre-
dict the polarity of interactions between two users.
Different from Hassan et al (2012), however, we
are not using this task to evaluate the accuracy of
sentiment analysis from text. Our experimental set-
ting is completely different in that we do not make
3www.createdebate.com
4www.politicalforum.com
use of the text exchanges between the two users but
instead use their interactions with other users or as-
pects. The purpose is to test the effectiveness of col-
laborative filtering.
Experimental Setting: The experiments are set up
in the following way. Given a pair of users i and j
who have directly exchanged posts, i.e. si,j is not
empty, we first hide the value of si,j in the matrix S.
Let the altered matrix be S?(i,j). We then use S?(i,j)
instead of S in the learning process as described in
Section 5 to learn the latent factor representation.
Let u?i and u?j denote the learned latent vectors for
user i and user j. We predict the polarity of relation
between i and j as follows:
s?i,j =
{
1 if g(u?Ti u?j) > 0.5,
0 otherwise,
where g(?) is the logistic function to convert the dot
product into a value between 0 and 1.
To judge the quality of the predicted polarity s?i,j ,
we could compare it with si,j . But since si,j itself is
predicted from the textual exchanges between i and
j, it is not the ground truth. Instead, we ask two hu-
man annotators to assign the true polarity label for
user i and user j by reading the textual exchanges
between them and judging whether they are friends
or foes in the discussion thread. The annotators are
asked to assign a score of 0 (indicating a negative
relation), 0.5 (indicating a neutral relation) or 1 (in-
dicating a positive relation). The lowest agreement
score based on Cohen?s kappa coefficient among the
6 threads we use is 0.56, showing fair to good agree-
ment. As ground truth, we set the final polarity score
to 1 if the average score of the two annotators is
larger than 0.5 and 0 otherwise.
We compare the PMF-based methods with two
majority baselines: MBL-0 always predicts negative
relations for all the user pairs (assuming most rela-
tions are negative) and MBL-1 always predicts posi-
tive relations (assuming most relations are positive).
We use MAE (mean absolute error) and RMSE
(root mean square error) as defined below as perfor-
mance metrics:
MAE =
?
i,j |s?i,j ? li,j |
N
,
RMSE =
??
i,j(s?i,j ? li,j)
2
N
,
406
0.2
0.4
0.6
0.8
1.0
DS1 DS2 DS3 DS4 DS5 DS6
MA
E
MB-1
MB-0
PMF-UU
PMF-UA
PMF-UOM
Figure 3: Comparing all the methods in terms of MAE.
0.2
0.4
0.6
0.8
1.0
DS1 DS2 DS3 DS4 DS5 DS6
RM
SE
MB-1
MB-0
PMF-UU
PMF-UA
PMF-UOM
Figure 4: Comparing all the methods in terms of RMSE.
where N is the total number of user pairs we test,
and li,j is the ground truth polarity score between
user i and user j.
Results: We show the results of our model and of
PMF-UU and PMF-UA in terms of MAE in Figure 3
and RMSE in Figure 4. The MAE values range be-
tween 0.31 and 0.44 except for DS5, which has a
higher error rate of 0.53. The results show that even
without knowing the textual exchanges between two
users, from their interactions with other users and/or
with topic aspects, we can still infer the polarity of
their relation with decent accuracy most of the time.
The results also show the comparison between our
model and the competing methods. We can see that
overall the complete model (PMF-UOM) performs
better than the two degenerate models (PMF-UU
and PMF-UA). The differences are statistically sig-
nificant at the 5% level without considering DS5, as
indicated by a 2-tailed paired t-test. Comparing to
the majority baselines, our model significantly out-
performs MBL-1 at 1% significance level while out-
performs MBL-0 on all the data sets except DS5. A
close examinations shows DS5 has very unbalanced
relations (around 83% of relations are negative). Ex-
cept for the unbalanced data set, our model has rea-
sonably good performance.
6.3 Subgroup Detection
The second task we study is the problem of detecting
ideological subgroups from discussion threads. The
original data set has been labeled with the ground
truth for this problem, that is, for each thread the
number of viewpoints is known and the viewpoint
held by each user is labeled. A subgroup is defined
as a set of users holding the same viewpoint.
Experimental Setting: Through this second exper-
iment, we would like to verify the hypothesis that
using the learned latent factor representation U for
users, we can better detect subgroups than directly
using the opinion matrices S and R. For all the
methods we compare, we first construct a feature
vector representation for each user. We then apply
K-means clustering to group users. The number of
clusters is set to be the true number of viewpoints
for each thread. The different methods are described
below:
? PMF-based methods: We simply use the
learned latent vectors u?i after optimizing the
objective function as the feature vectors to rep-
resent each user.
? BL-1: This is our own implementation to sim-
ulate the method by Abu-Jbara et al (2012).
Here each user is represented by a (3 ? (U +
A))-dimensional vector, where U is the num-
ber of users and A is the number of aspects,
i.e. (U +A) is the total number of opinion tar-
gets. For each opinion target, there are 3 di-
mensions in the feature vector, corresponding
to the number of positive, neutral and negative
opinion expressions towards the target from the
online posts.
? BL-2: BL-2 is similar to BL-1 except that we
only use a (U+A)-dimensional vector to repre-
407
sent each user. Here for each opinion target, we
directly use the corresponding sentiment polar-
ity score si,j or ri,j from the matrix S orR. For
empty entries in S andR, we use a score of 0.5.
We use Purity (the higher the better), Entropy (the
lower the better) and Rand Index (the higher the bet-
ter) to evaluate the performance of subgroup detec-
tion (Manning et al, 2008). We further use Accuracy
obtained by choosing the best alignment of clusters
with the ground truth class labels and computing the
percentage of users that are ?classified? correctly.
Results: We first give an overview of the perfor-
mance of all the methods on the task. We show the
average performance of the methods on all the data
sets in Figure 5. Overall, our model has a better per-
formance than all the competing methods.
0.4
0.6
0.8
1.0
Purity Entropy Accuracy RandIndex
BL-1
BL-2
PMF-UU
PMF-UA
PMF-UOM
Figure 5: An overview of the average performance of all
the methods on the 6 threads.
We present all the results in Table 3. We per-
form 2-tailed paired t-test on the results. We find
that PMF-UOM outperforms all the other methods
in terms of RandIndex at 5% significance level and
outperforms other methods in terms of Purity and
Entropy at 10% significance level. Furthermore,
the PMF-UOM model outperforms its degenerative
models PMF-UU and PMF-UA at 10% significance
level in terms of all the measures.
We observe that PMF-UOM achieves the best per-
formance in terms of all the measures for almost
all threads. In particular, comparison with BL-1
and BL-2 shows that collaborative filtering can gen-
eralize the user preferences and help better group
the users based on their viewpoints. The fact that
PMF-UOM outperforms both PMF-UU and PMF-
UA shows that it is important to consider both user-
user interactions and user-aspect interactions.
The Effects of Cluster Size: To test the effect of the
number of clusters on the experiment result, we vary
the number of clusters from 2 to 10 in all methods.
We find that all methods tend to achieve better re-
sults when the number of clusters equals the ground
truth cluster size. Overall, our method PMF-UOM
shows a better performance than the other four meth-
ods when the number of clusters changes, which in-
dicates the robustness of our method.
BL-1 BL-2 PMF-UU PMF-UA PMF-UOM
DS1
P 0.61 0.61 0.61 0.61 0.62
E 0.96 0.96 0.94 0.95 0.94
A 0.59 0.59 0.55 0.57 0.60
R 0.51 0.51 0.50 0.51 0.52
DS2
P 0.53 0.63 0.64 0.61 0.68
E 1.17 1.22 1.14 1.09 0.99
A 0.47 0.53 0.48 0.47 0.50
R 0.50 0.50 0.56 0.56 0.58
DS3
P 0.66 0.68 0.62 0.60 0.68
E 1.05 1.01 1.06 1.07 0.94
A 0.61 0.63 0.48 0.47 0.58
R 0.50 0.52 0.53 0.53 0.57
DS4
P 0.64 0.64 0.66 0.65 0.70
E 0.92 0.94 0.90 0.91 0.85
A 0.59 0.64 0.62 0.62 0.68
R 0.49 0.52 0.52 0.51 0.56
DS5
P 0.86 0.86 0.86 0.86 0.86
E 0.56 0.56 0.49 0.48 0.38
A 0.70 0.70 0.57 0.60 0.71
R 0.52 0.52 0.43 0.45 0.56
DS6
P 0.50 0.50 0.60 0.60 0.68
E 1.35 1.35 1.03 1.04 0.79
A 0.40 0.30 0.53 0.54 0.64
R 0.53 0.53 0.68 0.68 0.74
Table 3: Results on subgroup detection on all the 6
threads. P, E, A and R refer to Purity, Entropy, Accuracy
and RandIndex, respectively.
7 Conclusions
In this paper, we studied how to use probabilistic
matrix factorization, a common technique for col-
laborative filtering, to improve relation mining from
online discussion forums. We first applied senti-
ment analysis to extract user-user opinions and user-
aspect opinions from forum posts. The extracted
opinions form two opinion matrices. We then ap-
plied probabilistic matrix factorization using these
408
two matrices to discover a low-rank latent factor
space which aims to better generalize the users? un-
derlying preferences and indicate user similarities
based on their viewpoints. Using a data set with 6
discussion threads, we showed that the learned la-
tent vectors can be used to predict the polarity of
user relations well without using the users? direct
interaction data, demonstrating the effectiveness of
collaborative filtering. We further found that for the
task of subgroup detection, the latent vectors gave
better performance than using the directly extracted
opinion data, again showing that collaborative fil-
tering through probabilistic matrix factorization can
help address the sparseness problem in the extracted
opinion matrices and help improve relation mining.
Our current work mainly focuses on the user opin-
ion matrices. As future work, we would like to ex-
plore how to incorporate textual contents without
opinionated expressions. One possible way is to
consider the combination of matrix factorization and
topic modeling as studied by Wang and Blei (2011)
where we can use topic modeling to study textual
contents.
Acknowledgments
We thank the reviewers for their valuable comments
on this work.
References
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir R. Radev. 2012. Subgroup detection in
ideological discussions. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 399?409.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 431?439, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Pradeep Dasigi, Weiwei Guo, and Mona T. Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude using
textual latent semantics. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 65?69.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 395?403,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by
modeling positive and negative relations among par-
ticipants. In Proceedings of the 2012 EMNLP, pages
59?70.
John Hopcroft, Tiancheng Lou, and Jie Tang. 2011. Who
will follow you back?: reciprocal relationship predic-
tion. In Proceedings of the 20th ACM international
conference on Information and knowledge manage-
ment, pages 1137?1146.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the 10th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177.
Noriaki Kawamae. 2011. Predicting future reviews: sen-
timent analysis models for collaborative filtering. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM ?11,
pages 605?614.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010. Predicting positive and negative links in online
social networks. In Proceedings of the 19th interna-
tional conference on World wide web, pages 641?650.
Cane Wing-Ki Leung, Stephen Chi-Fai Chan, Fu-Lai
Chung, and Grace Ngai. 2011. A probabilistic rat-
ing inference framework for mining user preferences
from reviews. World Wide Web, 14(2):187?215.
David Liben-Nowell and Jon Kleinberg. 2003. The link
prediction problem for social networks. In Proceed-
ings of the twelfth international conference on Infor-
mation and knowledge management.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King.
2008. Sorec: Social recommendation using proba-
bilistic matrix factorization. In Proc. of ACM interna-
tional conference on Information and knowledge man-
agement.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, July.
409
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, Vol. 38, No.
11:39?41.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, ACL
?09, pages 1003?1011, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Yashar Moshfeghi, Benjamin Piwowarski, and Joe-
mon M. Jose. 2011. Handling data sparsity in collabo-
rative filtering using emotion and semantic based fea-
tures. In Proceedings of the 34th international ACM
SIGIR conference on Research and development in In-
formation Retrieval, SIGIR ?11, pages 625?634.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1199?1204, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Ruslan Salakhutdinov and Andriy Mnih. 2008. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems, volume 20.
Chong Wang and David M. Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 448?456.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3
- Volume 3, EMNLP ?09, pages 1533?1541, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
410
Proceedings of NAACL-HLT 2013, pages 1031?1040,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Latent Variable Model for Viewpoint Discovery from
Threaded Forum Posts
Minghui Qiu
School of Information Systems
Singapore Management University
Singapore
minghui.qiu.2010@smu.edu.sg
Jing Jiang
School of Information Systems
Singapore Management University
Singapore
jingjiang@smu.edu.sg
Abstract
Threaded discussion forums provide an im-
portant social media platform. Its rich user
generated content has served as an important
source of public feedback. To automatically
discover the viewpoints or stances on hot is-
sues from forum threads is an important and
useful task. In this paper, we propose a novel
latent variable model for viewpoint discov-
ery from threaded forum posts. Our model is
a principled generative latent variable model
which captures three important factors: view-
point specific topic preference, user identity
and user interactions. Evaluation results show
that our model clearly outperforms a number
of baseline models in terms of both clustering
posts based on viewpoints and clustering users
with different viewpoints.
1 Introduction
Threaded discussion forums provide an important
social media platform that allows netizens to express
their opinions, to ask for advice, and to form on-
line communities. In particular, responses to major
sociopolitical events and issues can often be found
in discussion forums, which serve as an impor-
tant source of public feedback. In such discussion
threads, we often observe heated debates over a con-
troversial issue, with different sides defending their
viewpoints with different arguments. For example,
after the presidential debate between Barack Obama
and Mitt Romney, there were heated discussions in
online forums like CreateDebate1 where some peo-
ple expressed their support for Obama while some
1http://www.createdebate.com/
others have their opposition to him. For a user who
is not closely following an event or issue, instead of
going through all the existing posts in a long thread,
she may want to quickly get an overview of the ma-
jor viewpoints and arguments given by the different
sides. For policy makers who want to obtain pub-
lic feedback on social issues from social media, it is
also desirable to automatically summarize the view-
points on an issue from relevant threads. In this pa-
per, we study the problem of modeling and discov-
ering different viewpoints in forum threads.
Recently there has been some work on finding
contrastive viewpoints from text. The model pro-
posed by Paul et al (2010) assumes viewpoints and
topics are orthogonal dimensions. Another model
proposed by Fang et al (2012) assumes that docu-
ments are already grouped by viewpoints and it fo-
cus on identifying contrastive viewpoint words un-
der the same topic. However, these existing stud-
ies are not based on interdependent documents like
threaded forum posts. As a result, at least two im-
portant characteristics of threaded forum data are not
considered in these models. (1) User identity: The
user or publisher of each forum post is known, and
a user may publish several posts in the same thread.
Since the same user?s opinion on an issue usually re-
mains unchanged, posts published by the same user
are likely to contain the same viewpoint. (2) User
interactions. A thread is like a conversation, where
users not only directly comment on the issue under
discussion but also comment on each other?s posts.
Users having different viewpoints may express their
disagreement or even attack each other while users
having the same viewpoint often support each other.
1031
The interaction expressions in forum posts may help
us infer the relation between two users and subse-
quently infer the viewpoints of the corresponding
posts.
In this paper, we propose a novel latent variable
model for viewpoint discovery from threaded forum
posts. Our model is based on the following obser-
vations: First, posts with different viewpoints tend
to focus on different topics. To illustrate this point,
we first apply the Latent Dirichlet Allocation (LDA)
model (Blei et al, 2003) on a thread about ?will
you vote Obama? and obtain a set of topics. This
thread comes from a data set that has each user?s
viewpoint annotated. Using the ground truth view-
point labels, we group all posts published by users
with viewpoint 1 (or viewpoint 2) and compute the
topic proportions. The two topic distributions are
shown in Figure 1. We can see that indeed the two
viewpoints each have some dominating topics. Our
second observations is that the same user tends to
hold the same viewpoint. In our model, we use a
user-level viewpoint distribution to capture this ob-
servation, and the experiments show that it works
better than assuming a global viewpoint distribution.
Third, users with the same viewpoint are likely to
have positive interactions while users with different
viewpoints tend to have negative interactions. Using
a sentiment lexicon, we can first predict the polarity
of interaction expressions. We then propose a novel
way to incorporate this information into the latent
variable model. In summary, we capture the three
observations above in a principled generative latent
variable model. We present the details of our model
in Section 3.
Figure 1: Topic distributions of two viewpoints for the
thread ?will you vote Obama?? The dotted line is the
average topic probability.
We use two tasks to evaluate our model. In the
first task, we evaluate how well posts with differ-
ent viewpoints are separated. In the second task, we
evaluate how well our model is able to group users
with different viewpoints. For both tasks, we com-
pare our model with an existing model as well as
a few degenerate versions of our model. The re-
sults show that our model can clearly outperform the
baselines in terms of three evaluation metrics. The
experiments are presented in Section 5.
The contributions of our work are threefold: (1)
We identify the importance of using user interac-
tions to help infer viewpoints in forum posts. (2) We
propose a principled latent variable model to jointly
model topics, viewpoints and user interactions. (3)
We empirically verify the validity of the three as-
sumptions in our model using real data sets.
2 Related Work
There are a few different lines of work that are re-
lated to our work. For discovering different view-
points from general text, Paul et al (2010) used the
model proposed by Paul and Girju (2010) to jointly
model topics and viewpoints. They assume these
two concepts are orthogonal and they do not con-
sider user identity. In comparison, our model has
the notion of topics and viewpoints, but we explicitly
model the dependency of topics on viewpoints, i.e.
we assume each viewpoint has a topic distribution.
We also consider author identities as an important
factor of our model. Fang et al (2012) proposed a
model that also combines topics and viewpoints. But
they assume that documents are already grouped by
viewpoints, which is not the case for forum posts.
Therefore, their model cannot be directly applied to
forum posts.
There has also been some work on finding view-
points from social media. Somasundaran and Wiebe
(2010) studied how to identify stances in online de-
bates. They used a supervised approach for classi-
fying stances in ideological debates. In comparison,
our model is an unsupervised method. The same au-
thors proposed an unsupervised method which relies
on associations of aspects with topics indicative of
stances mined from the Web for the task (Somasun-
daran and Wiebe, 2009). In contrast, our model is
also an unsupervised one but we do not rely on any
external knowledge.
Part of our work is related to detecting agree-
ment/disagreement from text. For this task, nor-
1032
mally supervised methods are used (Galley et al,
2004; Abbott et al, 2011), which require sufficient
labeled training data. In our work, since we deal
with different languages, we use a lexicon-based
approach that does not need training data. Re-
cently, Mukherjee and Liu (2012) proposed an un-
supervised model to extract different types of ex-
pressions including agreement/disagreement expres-
sions. However, our focus is not to detect agree-
ment/disagreement expressions but to model the
interplay between agreement/disagreement expres-
sions and viewpoints. The work by Mukherjee and
Liu (2012) can potentially be combined with our
model.
Another line of related work is subgroup detec-
tion, which aims to separate users holding different
viewpoints. This problem has recently been stud-
ied by Abu-Jbara and Radev (2012), Dasigi et al
(2012), Abu-Jbara et al (2012) and Hassan et al
(2012), where a clustering based approach is used.
Lu et al (2012) studied both textual content and
social interactions to find opposing network from
online forums. In our experiments we show that
our model can also be used for subgroup detection,
but meanwhile we also directly identify viewpoints,
which is not the goal of existing work on subgroup
finding or opposing network extraction.
3 Model
3.1 Motivation
Before we formally present our latent variable
model for viewpoint discovery, let us first look at the
assumptions we would like to capture in the model.
Viewpoint-based topic distribution: The first as-
sumption we have is that different viewpoints tend
to touch upon different topics. This is because to
support a viewpoint, users need to provide evidence
and arguments, and for different viewpoints the ar-
guments are likely different. To capture this assump-
tion, in our model, we let each viewpoint have its
own distribution of topics. Given the viewpoint of
a post, the hidden topic of each word in the post is
chosen according to the corresponding topic distri-
bution associated with that viewpoint.
User identify: The second assumption we have
is that the same user tends to talk from the same
viewpoint, although there are also users who do not
clearly have a viewpoint. In our model, we assume
that there is a user-level viewpoint distribution. For
each post by a user, its viewpoint is drawn from the
corresponding viewpoint distribution.
User interaction: An important difference between
threaded forum posts and regular document collec-
tions such as news articles is that posts in the same
thread form a tree structure via the ?reply-to? re-
lations. Many reply posts start with an expression
that comments on a previous post or directly ad-
dresses another user. These interaction expressions
may carry positive or negative sentiment, indicating
an agreement or a disagreement. For example, Ta-
ble 1 shows the interaction expressions from a few
sample posts with words such as ?correct,? ?agree,?
and ?delusional,? implying the polarity of the inter-
action expressions. The polarity of these interaction
expressions can help us infer whether two posts or
two users hold the same viewpoint or not. In our
model, we assume that the polarity of each interac-
tion expression can be detected. Details of how we
perform this detection are in Section 3.4.
Post
+
You are correct. Obama got into office w/ everything ? ? ?
I agree with your post Dan. Obama is so ? ? ?
?
Most of your post is delusional, especially the part ? ? ?
Are you freaking nutz? Palin is a BIMBO!
Table 1: Sample posts with positive (+) and negative(?)
interactions.
While the way to capture the first two assump-
tions discussed above is fairly standard, modeling
user interactions is something new. In our model,
we assume that the polarity of an interaction expres-
sion is generated based on the viewpoint of the cur-
rent post and the viewpoint of post(s) that the current
post replies to. The intuition is that if the viewpoints
are the same, we are more likely to see a positive in-
teraction whereas if the viewpoints are different we
are more likely to see a negative interaction.
3.2 Model description
We use the following notation to represent our data.
We consider a set of forum posts published by U dif-
ferent users on the same event or issue, where user
u (1 ? u ? U ) has published Nu posts. Let wu,n,l
(1 ? l ? Lu,n ) denote the l-th word in the n-th
post by user u, where Lu,n is the number of words
1033
in the n-th post by user u. wu,n,l is represented by
an index between 1 and V where V is the vocabu-
lary size. Furthermore, we assume that some of the
posts have user interaction expressions, where the
polarity of the expression is known. Without loss of
generality, let su,n ? {0, 1} denote the polarity of
the interaction expression of the n-th post by user
u. In addition, for each post that has an interaction
expression, we assume we also know the previous
post(s) it replies to. (In the case when the current
post replies to a user, we assume all that user?s ex-
isting posts are being replied to.) We refer to these
posts as the parent posts of the current post.
We assume that there are T topics where each
topic is essentially a word distribution, denoted as
?t. We also assume that there are Y different view-
points expressed in the collection of posts. For most
controversial issues, Y can be set to 2. Each view-
point y has a topic distribution ?y over the T top-
ics. While these T topics are meant to capture the
topical differences between viewpoints, since these
viewpoints are all about the same issue, there are
also some words commonly used by different view-
points. We therefore introduce a background topic
?B to capture these words. Finally, each user u has
a distribution over the Y viewpoints, denoted as ?u.
Figure 2: Plate notation of the Joint Viewpoint-Topic
Model with User Interaction (JVTM-UI). The dotted cir-
cle for Y means the variables represented by Y are not
new variables but a subset of the y variables.
Figure 2 shows the plate notation of the complete
model. We assume the following generation process
in our model. When user u generates her n-th post,
she first samples a viewpoint from ?u. Let this view-
point be represented by a hidden variable yu,n. For
the l-th word in this post, she first samples an in-
dicator variable xu,n,l from a Bernoulli distribution
parameterized by pi. If xu,n,l = 0, then she draws
wu,n,l from ?B . Otherwise, she first samples a topic,
denoted as zu,n,l, according to ?yu,n , and then draws
wu,n,l from ?zu,n,l .
Furthermore, if this post is a reply to a previous
post or another user, she may first comment on the
parent post(s). The polarity of the interaction ex-
pression in the post is dependent on the viewpoint
yu,n and the viewpoints of the previous post(s). Let
us use Yu,n to denote the set of y variables associ-
ated with the parent posts of the current post. The
user draws su,n according to following distribution:
p(su,n = 1|yu,n,Yu,n, ?) =
?
y??Yu,n
I(yu,n == y?) + ?
|Yu,n|+ 2?
,
p(su,n = 0|yu,n,Yu,n, ?) = 1? p(su,n = 1|yu,n,Yu,n, ?), (1)
where I(?) is 1 if the statement inside is true and 0
otherwise, and ? > 0 is a smoothing parameter.
Finally, we assume that ?B , ?t, ?u, ?y and pi all
have some uniform Dirichlet priors.
3.3 Inference
We use collapsed Gibbs sampling to estimate the
model parameters. In the initialization stage of
Gibbs sampling, for a reply post to a recipient, we
initialize its corresponding reply polarity s accord-
ing to all the labeled polarity of interaction words.
Specifically, if the majority of labeled interaction
words are positive, we set s = 1, otherwise we set
s = 0.
Let Y denote the set of all y variables, and
Y?(u,n) denote Y excluding yu,n. Similar notation
is used for the other variables. We sample yu,n using
the following formula.
p(yu,n = k|Y?(u,n),Z,S,X, ?, ?, ?)
?
p(yu,n = k,Y?(u,n)|?)
p(Y?(u,n)|?)
?
p(Z|yu,n = k,Y?(u,n),X, ?)
p(Z?(u,n)|Y?(u,n),X?(u,n), ?)
?p(S|yu,n = k,Y?(u,n), ?)
=
Cku,?n + ?
C(?)u,?n + Y ?
?
?T
t=1
?Ctu,n?1
a=0 (C
t
k,?(u,n) + ? + a)
?C(?)u,n?1
b=0 (C
(?)
k,?(u,n) + T? + b)
?p(S|yu,n = k,Y?(u,n), ?). (2)
Here all Cs are counters. Cku,?n is the number of
times we observe the viewpoint k from u?s posts,
excluding the n-th post, based on Y?(u,n). C
t
u,n is
1034
the number of times we observe topic t from user
u?s n-th post, based on Zu,n. And Ctk,?(u,n) is the
number of times we observe topic t associated with
viewpoint k, excluding user u?s n-th post. Note that
we need X to know which words are assigned to
the background topic so we can exclude them for
Ctu,n and C
t
k,?(u,n). C
(?)
u,?n is the number of times we
observe any viewpoint from u?s posts, excluding the
n-th post. C(?)u,n and C
(?)
k,?(u,n) are defined similarly.
The last term is further expanded as follows:
p(S|yu,n = k,Y?(u,n), ?) = p(su,n|yu,n = k,Yu,n, ?)
?p(S?(u,n)|yu,n = k,Y?(u,n), ?). (3)
Here p(su,n|yu,n = k,Yu,n, ?) is computed ac-
cording to Eqn. (1). For the latter term, we need to
consider posts which reply to user u?s n-th post be-
cause the value of yu,n affects these posts.
p(S?(u,n)|yu,n = k,Y?(u,n), ?)
?
?
(u?,n?):yu,n?Yu?,n?
p(su?,n? |yu?,n? ,Yu?,n? , ?). (4)
Next, we show how we jointly sample xu,n,l
and zu,n,l. We jointly sample them because when
xu,n,l = 0, zu,n,l does not need a value. We have the
following formulas:
p(xu,n,l = 1, zu,n,l = t|X?(u,n,l),Z?(u,n,l),Y,W, ?, ?, ?, ?
B)
?
C1?(u,n,l) + ?
C(?)
?(u,n,l)
+ 2?
?
Ctyu,n,l,?(u,n,l)
+ ?
C(?)
yu,n,l,?(u,n,l)
+ T?
?
C
wu,n,l
t,?(u,n,l)
+ ?
C(?)
t,?(u,n,l)
+ V ?
, (5)
p(xu,n,l = 0|X?(u,n,l),Z?(u,n,l),Y,W, ?, ?, ?, ?
B)
?
C0?(u,n,l) + ?
C(?)
?(u,n,l)
+ 2?
?
C
wu,n,l
B,?(u,n,l)
+ ?B
C(?)
B,?(u,n,l)
+ V ?B
. (6)
Here again the Cs are counters defined in similar
ways as before. For example, C1?(u,n,l) is the num-
ber of times we observe 1 assigned to an x variable,
excluding xu,n,l.
3.4 Interaction polarity prediction
The problem of detecting agreement and disagree-
ment from forum posts is relatively new. One pos-
sible solution is to use supervised learning, which
requires training data (Galley et al, 2004; Abbott et
al., 2011; Andreas et al, 2012). However, training
data are also likely domain and language dependent,
which makes them hard for re-use. For our task, we
take a simpler approach and use a sentiment lexicon
together with some heuristics to predict the polar-
ity of interaction expressions. Specifically, we first
identify interaction sentences following the strate-
gies from Hassan et al (2012). We assume sentences
containing mentions of the recipient of a post are in-
teraction sentences. Next, we consider words within
a text window of 8 words surrounding these men-
tions. We then use a subjectivity lexicon to label
these words. To form an English lexicon, we com-
bine three popular lexicons: the sentiment lexicon
used by Hu and Liu (2004), Multi-Perspective Ques-
tion Answering Subjectivity Lexicon by Wilson et
al. (2005) and SentiWordNet by Baccianella et al
(2010). Since we also work with a Chinese data set,
to form the Chinese sentiment lexicon, we use opin-
ion words from HowNet2 and NTUSD by Ku et al
(2007). To predict the polarity of an interaction ex-
pression, we simply check whether there are more
positive sentiment words or more negative sentiment
words in the expression, and label the interaction ex-
pression accordingly.
We would like to stress that since this interaction
classification step is independent of the latent vari-
able model, we can always apply a more accurate
method, but this is not the focus of this work.
4 Models for Comparison
In our experiments, we compare our model,
Joint Viewpoint-Topic Model with User Interaction
(JVTM-UI), with the following baseline models.
JVTM: The model is shown in Figure 3(a), a variant
of JVTM-UI that does not consider user interaction.
Through comparison with it, we can check the effect
of modeling user interactions.
JVTM-G: We consider JVTM-G in Figure 3(b), a
variant of JVTM which assumes a global viewpoint
distribution. Comparison with it allows us to check
the usefulness of user identity in the task.
UIM: The third model we consider is a User Interac-
tion Model (UIM) in Figure 3(c), where we rely on
only the users? interactions to infer the viewpoints.
We use it to check how well viewpoints can be dis-
covered from only user interaction expressions.
2http://www.keenage.com/html/e_index.html
1035
Figure 3: (a) JVTM: Joint Viewpoint-Topic Model. (b) JVTM-G: JVTM with a global viewpoint distribution. (c)
UIM: User-Interaction Model.
TAM: The last model we consider is the one by Paul
et al (2010). As TAM is applied at document collec-
tions, we first concatenate all the posts by the same
user into a pseudo document and then apply TAM.
5 Experiments and Analysis
In this section, we evaluate our model with a set of
baseline models using two data sets.
Name Issue #Posts #Users
EDS1 Vote for Obama 2599 197
EDS2 Arizona Immigration Law 738 59
EDS3 Tax Cuts 276 26
CDS1 Tencent and Qihoo dispute 30137 2507
CDS2 Fang Zhouzi questions Han Han 76934 1769
CDS3 Liu Xiang in London Olympics 29486 2774
Table 2: Some statistics of the data set.
5.1 Data Sets and Experimental Settings
We focus our work on finding users? viewpoints on
a controversial issue, where we assume that there
are two contradictory viewpoints. We use two data
sets on controversial issues. The first data set comes
from Abu-Jbara et al (2012) and Hassan et al
(2012). This data set originally was used for finding
subgroups of users, so the annotations were done at
user level, i.e. for each user there is a label indicat-
ing which subgroup he/she belongs to. We use the
top-3 mostly discussed threads with two subgroups
for our study.
In reality, controversial issues are often discussed
across threads. We thus constructed another large
data set which contains more than one thread for
each issue. We chose three hot issues from one of
the most popular Chinese online forums ? TianYa
Club3. The three issues are ?Fang Zhouzi questions
Han Han?4, ?Tencent and Qihoo dispute?5, and ?Liu
Xiang in London Olympics?6. All these issues trig-
gered heated discussions on the forum and we found
that most of the users were divided into two different
groups.
We crawled the data set using the TianYa API7.
The API allows users to issue queries and get threads
most related to the queries. For each issue, we used
entities involved in the event as queries and obtained
750 threads for each query. We then extracted all the
posts in the threads. As there are users who posted
irrelevant posts in the forum, we then filtered out
those users who did not mention the entities or had
fewer than 4 posts.
We refer to the first set of data in English as EDS1,
EDS2 and EDS3, and the second set of data in Chi-
nese as CDS1, CDS2 and CDS3. Some statistics of
the resulting data set are shown in Table 2.
For all the models, we set Y = 2. We set T = 10
for the English data sets and T = 40 for the Chinese
data sets. We run 400 iterations of Gibbs sampling
as burn-in iterations and then take 100 samples with
a gap of 5 to obtain our final results. We empirically
set ? = 0.01, ?B = 0.1, ? = 10 and ? = 0.1 for our
model on all the data sets. ? and ? are set through
grid search where they take values in {0.01, 0.001}.
For each data set, we choose the best setting for each
model and report the corresponding results.
3http://en.wikipedia.org/wiki/Tianya_Club
4http://en.wikipedia.org/wiki/Fang_Zhouzi
5http://en.wikipedia.org/wiki/360_v._Tencent
6http://en.wikipedia.org/wiki/Liu_Xiang
7http://open.tianya.cn/index.php
1036
5.2 Identification of viewpoints
We first evaluate the models on the task of identi-
fying viewpoints. For fair comparison, each model
will output a viewpoint label for each post. For
JVTM-UI, JVTM, JVTM-G and UIM, after we learn
the model, each post will directly have a viewpoint
assignment. For TAM we cannot directly get each
post?s viewpoint as the model assumes a document-
level viewpoint distribution. To estimate each post?s
viewpoint in this model, we use viewpoint assign-
ment at the word level learnt from the model. Then
for each post, we label its viewpoint as the viewpoint
that has the majority count in the post.
Ideally, we would like to manually label all the
posts to obtain the ground truth for evaluation. Since
there are too many posts, we only labeled a sample
of them. For each issue, we randomly selected 150
posts to label their viewpoints. For each post, we
asked two different annotators to label its viewpoint.
We made sure that the annotators understand the is-
sue and the two major viewpoints before they anno-
tated the posts. Specifically, as the Chinese data sets
are about some controversial issues around the enti-
ties involved, we then defined two major viewpoints
as support and not support the entity who initiated
the event. The entities of data set CDS1, CDS2 and
CDS3 are Fang Zhouzi, Tencent and Liu Xiang re-
spectively. For each given post, the annotators were
asked to judge whether the post has expressed view-
points and if so, what is its corresponding view-
point. We measure the agreement score using Co-
hen?s kappa coefficient. The lowest agreement score
for an issue is 0.61 in the data set, showing good
agreement. We then used the set of posts that were
labeled with the same viewpoint by the two annota-
tors as our evaluation data for all the models.
Since our task is essentially a clustering problem,
we use purity and entropy to measure the perfor-
mance (Manning et al, 2008). Furthermore, we also
use accuracy where we choose the better alignment
of clusters with ground truth class labels and com-
pute the percentage of posts that are ?classified? cor-
rectly. For purity and accuracy, the higher the mea-
sure is the better the performance. For entropy, the
lower the measure is the better the performance.
We give an overview of the all the averaged model
results on the data sets in Figure 4. We observed that
0.4
0.6
0.8
1.0
Purity Entropy Accuracy
JVTM-GTAMJVTMUIMJVTM-UI
Figure 4: Averaged results of the models in identification
of viewpoints.
UIM performs relatively better than other methods
except our model. This shows user interactions are
important features to identify post viewpoints. Over-
all, our model has a better performance as it is with
higher purity and accuracy, and lower entropy.
JVTM-UI UIM JVTM TAM JVTM-G
EDS1
P 0.77 0.74 0.64 0.65 0.63
E 0.72 0.76 0.90 0.92 0.94
A 0.77 0.74 0.61 0.60 0.57
EDS2
P 0.82 0.78 0.68 0.65 0.64
E 0.69 0.73 0.79 0.86 0.90
A 0.81 0.78 0.68 0.68 0.65
EDS3
P 0.79 0.73 0.65 0.64 0.62
E 0.67 0.79 0.88 0.89 0.87
A 0.79 0.73 0.65 0.64 0.62
CDS1
P 0.87 0.83 0.83 0.82 0.82
E 0.61 0.64 0.65 0.66 0.64
A 0.60 0.58 0.59 0.58 0.57
CDS2
P 0.71 0.65 0.61 0.63 0.60
E 0.80 0.85 0.92 0.95 0.96
A 0.71 0.65 0.61 0.61 0.59
CDS3
P 0.78 0.78 0.78 0.78 0.78
E 0.73 0.75 0.70 0.72 0.73
A 0.67 0.59 0.67 0.66 0.63
Table 3: Results on viewpoint identification on the all
data sets.
Table 3 shows the detailed results on the data
sets. We perform the 2-tailed paired t-test as used
by Abu-Jbara et al (2012) on the results. All the re-
sult differences are at 10% significance level if not
with further clarification. First, JVTM has a better
performance over JVTM-G, which shows it is im-
portant to consider user identity in the task. Sec-
ond, JVTM and TAM have similar performance on
1037
EDS1 and CDS2, but JVTM has a relatively bet-
ter performance on EDS2, EDS3, CDS1 and CDS3.
This shows it is helpful to consider each viewpoint?s
topic preference. Although as studied by Paul et
al. (2010), by only using unigram features, TAM
may not be able to cluster viewpoints accurately,
our study shows that the results can be improved
when adding each viewpoint?s topic focus. Third,
UIM has relatively better performance than the other
models, which demonstrates that user interactions
alone can do a decent job in inferring viewpoints. Fi-
nally, our proposed model has the best performance
across the board in terms of all three evaluation met-
rics. Note that, our proposed model significantly
outperforms other methods at 5% significance level
except at 10% significance level over JVTM model.
This shows by jointly modeling topics, viewpoints
and user interactions, our model can better identify
posts with different viewpoints.
5.3 Identification of user groups
We also use another task to evaluate our model.
The task here is finding each user?s viewpoint and
subsequently grouping users by their viewpoints.
This task has been studied by Abu-Jbara and Radev
(2012), Dasigi et al (2012), Abu-Jbara et al (2012)
and Hassan et al (2012). For the English data set,
the user-level group labels are provided by the orig-
inal data set. For the Chinese data set, we randomly
selected 150 users for each issue and manually la-
beled them according to their viewpoints as reflected
by their posts. If a user?s posts do not clearly suggest
a viewpoint, we label her as neutral. Again we asked
two human judges to do annotation. The agreement
scores are above 0.70 for all issues, showing sub-
stantial agreement. This score is higher than view-
point identification, which suggests that it is easier
to judge a user?s viewpoint than a single post?s view-
point. We use the set of users who have got the same
labels by the two human judges for our experiments.
Similarly we compute purity, entropy and accuracy
to evaluate the clustering results.
Figure 5 shows the averaged results of all the
models. Similar to previous experiment, our model
has a better performance compared to the competing
models.
The results on the each data set are shown in Ta-
ble 4. The tables show that similar trends can be
0.4
0.6
0.8
1.0
Purity Entropy Accuracy
JVTM-GTAMJVTMUIMJVTM-UI
Figure 5: Averaged results of the models in identification
of user groups.
JVTM-UI UIM JVTM TAM JVTM-G
EDS1
P 0.67 0.67 0.67 0.67 0.67
E 0.85 0.88 0.89 0.89 0.91
A 0.63 0.59 0.58 0.59 0.57
EDS2
P 0.77 0.77 0.77 0.77 0.77
E 0.72 0.76 0.74 0.75 0.76
A 0.62 0.59 0.60 0.58 0.59
EDS3
P 0.68 0.63 0.61 0.61 0.58
E 0.90 0.92 0.95 0.96 0.97
A 0.68 0.63 0.61 0.58 0.57
CDS1
P 0.64 0.60 0.61 0.61 0.60
E 0.91 0.97 0.96 0.96 0.97
A 0.61 0.55 0.55 0.56 0.53
CDS2
P 0.69 0.69 0.69 0.69 0.69
E 0.83 0.89 0.85 0.89 0.89
A 0.62 0.57 0.56 0.58 0.54
CDS3
P 0.67 0.63 0.64 0.60 0.60
E 0.89 0.91 0.92 0.93 0.96
A 0.64 0.62 0.60 0.56 0.54
Table 4: Results on identification of user groups on the
all the data sets.
observed for the task of user group identification.
We also perform the 2-tailed paired t-test on the re-
sults. We find our model significantly outperforms
other models in terms of accuracy at 5% significance
level, and purity and entropy at 10% significance
level. Overall speaking, our joint model performed
the best among all the models for this task for all
three metrics. This shows that it is important to con-
sider the topical preference of individual viewpoint,
user?s identify as well as the interactions between
users.
1038
Figure 6: The user interaction network in a discussion
thread about ?will you vote obama.? Green (left) and
white (right) nodes represent users with two different
viewpoints. Red (thin) and blue(thick) edges represent
negative and positive interactions.
5.4 User interaction network
To gain some direct insight into our results, we show
the user interaction network from one thread in Fig-
ure 6. Here each node denotes a user, and its color
denotes the predicted viewpoint of that user. A link
between a pair of users means these users have in-
teractions and the interaction types have a dominant
polarity. The polarities of these links are predicted
using the interaction expressions and a sentiment
lexicon, whereas the viewpoints of different users
are learned by JVTM-UI, making use of the inter-
action polarities. The figure shows that clearly there
are mostly positive interactions between users with
the same viewpoint and mostly negative interactions
between users with different viewpoints. Note that,
our method to identify user interaction polarity is
rule-based. As this step serves as a preprocessing
step for our latent variable model, we can always
use a more accurate method to improve the perfor-
mances.
6 Conclusion
In this work, we proposed a novel latent variable
model for viewpoint discovery from threaded forum
posts. Our model is based on the three important fac-
tors: viewpoint specific topic preference, user iden-
tity and user interactions. Our proposed model cap-
tures these observations in a principled way. In par-
ticular, to incorporate the user interaction informa-
tion, we proposed a novel generative process. Em-
pirical evaluation on the real forum data sets showed
that our model could cluster both posts and users
with different viewpoints more accurately than the
baseline models we consider. To the best of our
knowledge, our work is the first to incorporate user
interaction polarity into a generative model to dis-
cover viewpoints.
In this work, we only considered unigrams. As
some previous work has shown, more complex lexi-
cal units such as n-grams (Mukherjee and Liu, 2012)
and dependency triplets (Paul et al, 2010) may im-
prove the performance of topic models. We will con-
sider these strategies in our future work. Currently
we use a simple heuristic-based classifier to predict
interaction polarity. In our further work, we plan
to consider more accurate methods using deeper lin-
guistic analysis. We did not study how to summarize
the discovered viewpoints in this work, which is also
something we will look into in our future work.
Acknowledgments
We thank the reviewers for their valuable comments
on this work. We also thank Shuang Xia for his help
on processing and labeling the data sets.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing dis-
agreement in informal political argument. In Proceed-
ings of the Workshop on Language in Social Media
(LSM 2011), pages 2?11.
Amjad Abu-Jbara and Dragomir R. Radev. 2012. Sub-
group detector: A system for detecting subgroups in
online discussions. In ACL (System Demonstrations),
pages 133?138.
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir R. Radev. 2012. Subgroup detection in
ideological discussions. In Proceedings of ACL 2012,
pages 399?409.
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagreement
in threaded discussion. In Proceedings of LREC?12.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Pradeep Dasigi, Weiwei Guo, and Mona T. Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude using
1039
textual latent semantics. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 65?69.
Yi Fang, Luo Si, Naveen Somasundaram, and Zhengtao
Yu. 2012. Mining contrastive opinions on political
texts using cross-perspective topic model. In WSDM,
pages 63?72.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL?04, Main Volume, pages 669?
676.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by
modeling positive and negative relations among par-
ticipants. In Proceedings of the 2012 EMNLP, pages
59?70.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the 10th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177.
Lun-wei Ku, Yong-sheng Lo, and Hsin-hsi Chen. 2007.
Using polarity scores of words for sentence-level opin-
ion extraction. In Proc. of the NTCIR-6 Workshop
Meeting, pages 316?322.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM international conference on
Information and knowledge management, CIKM ?12,
pages 1642?1646, New York, NY, USA. ACM.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, July.
Arjun Mukherjee and Bing Liu. 2012. Modeling review
comments. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
pages 320?329.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In AAAI.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In EMNLP, pages 66?76.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234.
Swapna Somasundaran and Janyce Wiebe. 2010. Recog-
nizing stances in ideological on-line debates. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 116?124.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
1040
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640?649,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Templates of Entity Summaries
with an Entity-Aspect Model and Pattern Mining
Peng Li1 and Jing Jiang2 and Yinglin Wang1
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2School of Information Systems, Singapore Management University
{lipeng,ylwang}@sjtu.edu.cn jingjiang@smu.edu.sg
Abstract
In this paper, we propose a novel approach
to automatic generation of summary tem-
plates from given collections of summary
articles. This kind of summary templates
can be useful in various applications. We
first develop an entity-aspect LDA model
to simultaneously cluster both sentences
and words into aspects. We then apply fre-
quent subtree pattern mining on the depen-
dency parse trees of the clustered and la-
beled sentences to discover sentence pat-
terns that well represent the aspects. Key
features of our method include automatic
grouping of semantically related sentence
patterns and automatic identification of
template slots that need to be filled in. We
apply our method on five Wikipedia entity
categories and compare our method with
two baseline methods. Both quantitative
evaluation based on human judgment and
qualitative comparison demonstrate the ef-
fectiveness and advantages of our method.
1 Introduction
In this paper, we study the task of automatically
generating templates for entity summaries. An en-
tity summary is a short document that gives the
most important facts about an entity. In Wikipedia,
for instance, most articles have an introduction
section that summarizes the subject entity before
the table of contents and other elaborate sections.
These introduction sections are examples of en-
tity summaries we consider. Summaries of enti-
ties from the same category usually share some
common structure. For example, biographies of
physicists usually contain facts about the national-
ity, educational background, affiliation and major
contributions of the physicist, whereas introduc-
tions of companies usually list information such
as the industry, founder and headquarter of the
company. Our goal is to automatically construct
a summary template that outlines the most salient
types of facts for an entity category, given a col-
lection of entity summaries from this category.
Such kind of summary templates can be very
useful in many applications. First of all, they
can uncover the underlying structures of summary
articles and help better organize the information
units, much in the same way as infoboxes do in
Wikipedia. In fact, automatic template genera-
tion provides a solution to induction of infobox
structures, which are still highly incomplete in
Wikipedia (Wu and Weld, 2007). A template
can also serve as a starting point for human edi-
tors to create new summary articles. Furthermore,
with summary templates, we can potentially ap-
ply information retrieval and extraction techniques
to construct summaries for new entities automati-
cally on the fly, improving the user experience for
search engine and question answering systems.
Despite its usefulness, the problem has not been
well studied. The most relevant work is by Fila-
tova et al (2006) on automatic creation of domain
templates, where the defintion of a domain is sim-
ilar to our notion of an entity category. Filatova
et al (2006) first identify the important verbs for
a domain using corpus statistics, and then find fre-
quent parse tree patterns from sentences contain-
ing these verbs to construct a domain template.
There are two major limitations of their approach.
First, the focus on verbs restricts the template pat-
terns that can be found. Second, redundant or
related patterns using different verbs to express
the same or similar facts cannot be grouped to-
gether. For example, ?won X award? and ?re-
ceived X prize? are considered two different pat-
terns by this approach. We propose a method that
can overcome these two limitations. Automatic
template generation is also related to a number of
other problems that have been studied before, in-
640
cluding unsupervised IE pattern discovery (Sudo
et al, 2003; Shinyama and Sekine, 2006; Sekine,
2006; Yan et al, 2009) and automatic generation
of Wikipedia articles (Sauper and Barzilay, 2009).
We discuss the differences of our work from exist-
ing related work in Section 6.
In this paper we propose a novel approach to
the task of automatically generating entity sum-
mary templates. We first develop an entity-aspect
model that extends standard LDA to identify clus-
ters of words that can represent different aspects
of facts that are salient in a given summary col-
lection (Section 3). For example, the words ?re-
ceived,? ?award,? ?won? and ?Nobel? may be
clustered together from biographies of physicists
to represent one aspect, even though they may ap-
pear in different sentences from different biogra-
phies. Simultaneously, the entity-aspect model
separates words in each sentence into background
words, document words and aspect words, and
sentences likely about the same aspect are natu-
rally clustered together. After this aspect identi-
fication step, we mine frequent subtree patterns
from the dependency parse trees of the clustered
sentences (Section 4). Different from previous
work, we leverage the word labels assigned by the
entity-aspect model to prune the patterns and to
locate template slots to be filled in.
We evaluate our method on five entity cate-
gories using Wikipedia articles (Section 5). Be-
cause the task is new and thus there is no stan-
dard evaluation criteria, we conduct both quanti-
tative evaluation using our own human judgment
and qualitative comparison. Our evaluation shows
that our method can obtain better sentence patterns
in terms of f1 measure compared with two baseline
methods, and it can also achieve reasonably good
quality of aspect clusters in terms of purity. Com-
pared with standard LDA and K-means sentence
clustering, the aspects identified by our method are
also more meaningful.
2 The Task
Given a collection of entity summaries from the
same entity category, our task is to automatically
construct a summary template that outlines the
most important information one should include in
a summary for this entity category. For example,
given a collection of biographies of physicists, ide-
ally the summary template should indicate that im-
portant facts about a physicist include his/her ed-
Aspect Pattern
ENT received his phd from ? university
1 ENT studied ? under ?
ENT earned his ? in physics from university of
?
ENT was awarded the medal in ?
2 ENT won the ? award
ENT received the nobel prize in physics in ?
ENT was ? director
3 ENT was the head of ?
ENT worked for ?
ENT made contributions to ?
4 ENT is best known for work on ?
ENT is noted for ?
Table 1: Examples of some good template patterns
and their aspects generated by our method.
ucational background, affiliation, major contribu-
tions, awards received, etc.
However, it is not clear what is the best repre-
sentation of such templates. Should a template
comprise a list of subtopic labels (e.g. ?educa-
tion? and ?affiliation?) or a set of explicit ques-
tions? Here we define a template format based on
the usage of the templates as well as our obser-
vations from Wikipedia entity summaries. First,
since we expect that the templates can be used by
human editors for creating new summaries, we use
sentence patterns that are human readable as basic
units of the templates. For example, we may have
a sentence pattern ?ENT graduated from ? Uni-
versity? for the entity category ?physicist,? where
ENT is a placeholder for the entity that the sum-
mary is about, and ??? is a slot to be filled in. Sec-
ond, we observe that information about entities of
the same category can be grouped into subtopics.
For example, the sentences ?Bohr is a Nobel lau-
reate? and ?Einstein received the Nobel Prize? are
paraphrases of the same type of facts, while the
sentences ?Taub earned his doctorate at Prince-
ton University? and ?he graduated from MIT? are
slightly different but both describe a person?s ed-
ucational background. Therefore, it makes sense
to group sentence patterns based on the subtopics
they pertain to. Here we call these subtopics the
aspects of a summary template.
Formally, we define a summary template to be a
set of sentence patterns grouped into aspects. Each
sentence pattern has a placeholder for the entity to
be summarized and possibly one or more template
slots to be filled in. Table 1 shows some sentence
patterns our method has generated for the ?physi-
cist? category.
641
2.1 Overview of Our Method
Our automatic template generation method con-
sists of two steps:
Aspect Identification: In this step, our goal is
to automatically identify the different aspects or
subtopics of the given summary collection. We si-
multaneously cluster sentences and words into as-
pects, using an entity-aspect model extended from
the standard LDA model that is widely used in
text mining (Blei et al, 2003). The output of this
step are sentences clustered into aspects, with each
word labeled as a stop word, a background word,
a document word or an aspect word.
Sentence Pattern Generation: In this step, we
generate human-readable sentence patterns to rep-
resent each aspect. We use frequent subtree pat-
tern mining to find the most representative sen-
tence structures for each aspect. The fixed struc-
ture of a sentence pattern consists of aspect words,
background words and stop words, while docu-
ment words become template slots whose values
can vary from summary to summary.
3 Aspect Identification
At the aspect identification step, our goal is to dis-
cover the most salient aspects or subtopics con-
tained in a summary collection. Here we propose
a principled method based on a modified LDA
model to simultaneously cluster both sentences
and words to discover aspects.
We first make the following observation. In en-
tity summaries such as the introduction sections
of Wikipedia articles, most sentences are talk-
ing about a single fact of the entity. If we look
closely, there are a few different kinds of words in
these sentences. First of all, there are stop words
that occur frequently in any document collection.
Second, for a given entity category, some words
are generally used in all aspects of the collection.
Third, some words are clearly associated with the
aspects of the sentences they occur in. And finally,
there are also words that are document or entity
specific. For example, in Table 2 we show two
sentences related to the ?affiliation? aspect from
the ?physicist? summary collection. Stop words
such as ?is? and ?the? are labeled with ?S.? The
word ?physics? can be regarded as a background
word for this collection. ?Professor? and ?univer-
sity? are clearly related to the ?affiliation? aspect.
Finally words such as ?Modena? and ?Chicago?
are specifically associated with the subject enti-
ties being discussed, that is, they are specific to
the summary documents.
To capture background words and document-
specific words, Chemudugunta et al (2007)
proposed to introduce a background topic and
document-specific topics. Here we borrow their
idea and also include a background topic as well
as document-specific topics. To discover aspects
that are local to one or a few adjacent sentences but
may occur in many documents, Titov and McDon-
ald (2008) proposed a multi-grain topic model,
which relies on word co-occurrences within short
paragraphs rather than documents in order to dis-
cover aspects. Inspired by their model, we rely
on word co-occurrences within single sentences to
identify aspects.
3.1 Entity-Aspect Model
We now formally present our entity-aspect model.
First, we assume that stop words can be identified
using a standard stop word list. We then assume
that for a given entity category there are three
kinds of unigram language models (i.e. multino-
mial word distributions). There is a background
model ?B that generates words commonly used
in all documents and all aspects. There are D
document models ?d (1 ? d ? D), where D
is the number of documents in the given sum-
mary collection, and there are A aspect models ?a
(1 ? a ? A), where A is the number of aspects.
We assume that these word distributions have a
uniform Dirichlet prior with parameter ?.
Since not all aspects are discussed equally fre-
quently, we assume that there is a global aspect
distribution ? that controls how often each aspect
occurs in the collection. ? is sampled from another
Dirichlet prior with parameter ?. There is also a
multinomial distribution pi that controls in each
sentence how often we encounter a background
word, a document word, or an aspect word. pi has
a Dirichlet prior with parameter ?.
Let Sd denote the number of sentences in doc-
ument d, Nd,s denote the number of words (after
stop word removal) in sentence s of document d,
and wd,s,n denote the n?th word in this sentence.
We introduce hidden variables zd,s for each sen-
tence to indicate the aspect a sentence belongs to.
We also introduce hidden variables yd,s,n for each
word to indicate whether a word is generated from
the background model, the document model, or
the aspect model. Figure 1 shows the process of
642
Venturi/D is/S a/S professor/A of/S physics/B at/S the/S University/A of/S
Modena/D ./S
He/S was/S a/S professor/A of/S physics/B at/S the/S University/A of/S
Chicago/D until/S 1982/D ./S
Table 2: Two sentences on ?affiliation? from the ?physicist? entity category. S: stop word. B: background
word. A: aspect word. D: document word.
1. Draw ? ? Dir(?), ?B ? Dir(?), pi ? Dir(?)
2. For each aspect a = 1, . . . , A,
(a) draw ?a ? Dir(?)
3. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zd,s ? Multi(?)
ii. for each word n = 1, . . . , Nd,s
A. draw yd,s,n ? Multi(pi)
B. draw wd,s,n ? Multi(?B) if yd,s,n = 1,
wd,s,n ? Multi(?d) if yd,s,n = 2, or
wd,s,n ? Multi(?zd,s) if yd,s,n = 3
Figure 1: The document generation process.
y z
?pi
? ?
?
? A
dSD sdN ,
B?
?
w
Figure 2: The entity-aspect model.
generating the whole document collection. The
plate notation of the model is shown in Figure 2.
Note that the values of ?, ? and ? are fixed. The
number of aspects A is also manually set.
3.2 Inference
Given a summary collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assign-
ment of zd,s and yd,s,n, that is, the assignment that
maximizes p(z,y|w;?, ?, ?), where z, y and w rep-
resent the set of all z, y and w variables, respec-
tively. With the assignment, sentences are natu-
rally clustered into aspects, and words are labeled
as either a background word, a document word, or
an aspect word.
We approximate p(y, z|w;?, ?, ?) by
p(y,z|w; ??B, {??d}Dd=1, {??a}Aa=1, ??, p?i), where ??B,
{??d}Dd=1, {??a}Aa=1, ?? and p?i are estimated using
Gibbs sampling, which is commonly used for
inference for LDA models (Griffiths and Steyvers,
2004). Due to space limit, we give the formulas
for the Gibbs sampler below without derivation.
First, given sentence s in document d, we sam-
ple a value for zd,s given the values of all other z
and y variables using the following formula:
p(zd,s = a|z?{d,s},y,w)
? C
A
(a) + ?
CA(?) +A?
?
?V
v=1
?E(v)
i=0 (Ca(v) + i+ ?)?E(?)
i=0 (Ca(?) + i+ V ?)
.
In the formula above, z?{d,s} is the current aspect
assignment of all sentences excluding the current
sentence. CA(a) is the number of sentences assigned
to aspect a, and CA(?) is the total number of sen-
tences. V is the vocabulary size. Ca(v) is the num-
ber of times word v has been assigned to aspect
a. Ca(?) is the total number of words assigned to
aspect a. All the counts above exclude the current
sentence. E(v) is the number of times word v oc-
curs in the current sentence and is assigned to be
an aspect word, as indicated by y, and E(?) is the
total number of words in the current sentence that
are assigned to be an aspect word.
We then sample a value for yd,s,n for each word
in the current sentence using the following formu-
las:
p(yd,s,n = 1|z,y?{d,s,n}) ?
Cpi(1) + ?
Cpi(?) + 3?
?
CB(wd,s,n) + ?
CB(?) + V ?
,
p(yd,s,n = 2|z,y?{d,s,n}) ?
Cpi(2) + ?
Cpi(?) + 3?
?
Cd(wd,s,n) + ?
Cd(?) + V ?
,
p(yd,s,n = 3|z,y?{d,s,n}) ?
Cpi(3) + ?
Cpi(?) + 3?
?
Ca(wd,s,n) + ?
Ca(?) + V ?
.
In the formulas above, y?{d,s,n} is the set of all y
variables excluding yd,s,n. Cpi(1), Cpi(2) and Cpi(3) are
the numbers of words assigned to be a background
word, a document word, or an aspect word, respec-
tively, and Cpi(?) is the total number of words. CB
and Cd are counters similar to Ca but are for the
background model and the document models. In
all these counts, the current word is excluded.
With one Gibbs sample, we can make the fol-
lowing estimation:
643
??Bv =
CB(v) + ?
CB(?) + V ?
, ??dv =
Cd(v) + ?
Cd(?) + V ?
, ??av =
Ca(v) + ?
Ca(?) + V ?
,
??a =
CA(a) + ?
CA(?) +A?
, p?it =
Cpi(t) + ?
Cpi(?) + 3?
(1 ? t ? 3).
Here the counts include all sentences and all
words.
In our experiments, we set ? = 5, ? = 0.01 and
? = 20. We run 100 burn-in iterations through all
documents in a collection to stabilize the distri-
bution of z and y before collecting samples. We
found that empirically 100 burn-in iterations were
sufficient for our data set. We take 10 samples with
a gap of 10 iterations between two samples, and
average over these 10 samples to get the estima-
tion for the parameters.
After estimating ??B, {??d}Dd=1, {??a}Aa=1, ?? and p?i,
we find the values of each zd,s and yd,s,n that max-
imize p(y, z|w; ??B, {??d}Dd=1, {??a}Aa=1, ??, p?i). This as-
signment, together with the standard stop word list
we use, gives us sentences clustered into A as-
pects, where each word is labeled as either a stop
word, a background word, a document word or an
aspect word.
3.3 Comparison with Other Models
A major difference of our entity-aspect model
from standard LDA model is that we assume each
sentence belongs to a single aspect while in LDA
words in the same sentence can be assigned to
different topics. Our one-aspect-per-sentence as-
sumption is important because our goal is to clus-
ter sentences into aspects so that we can mine
common sentence patterns for each aspect.
To cluster sentences, we could have used a
straightforward solution similar to document clus-
tering, where sentences are represented as feature
vectors using the vector space model, and a stan-
dard clustering algorithm such as K-means can
be applied to group sentences together. However,
there are some potential problems with directly ap-
plying this typical document clustering method.
First, unlike documents, sentences are short, and
the number of words in a sentence that imply its
aspect is even smaller. Besides, we do not know
the aspect-related words in advance. As a result,
the cosine similarity between two sentences may
not reflect whether they are about the same aspect.
We can perform heuristic term weighting, but the
method becomes less robust. Second, after sen-
tence clustering, we may still want to identify the
the aspect words in each sentence, which are use-
ful in the next pattern mining step. Directly taking
the most frequent words from each sentence clus-
ter as aspect words may not work well even af-
ter stop word removal, because there can be back-
ground words commonly used in all aspects.
4 Sentence Pattern Generation
At the pattern generation step, we want to iden-
tify human-readable sentence patterns that best
represent each cluster. Following the basic idea
from (Filatova et al, 2006), we start with the parse
trees of sentences in each cluster, and apply a
frequent subtree pattern mining algorithm to find
sentence structures that have occurred at least K
times in the cluster. Here we use dependency parse
trees.
However, different from (Filatova et al, 2006),
the word labels (S, B, D and A) assigned by the
entity-aspect model give us some advantages. In-
tuitively, a representative sentence pattern for an
aspect should contain at least one aspect word. On
the other hand, document words are entity-specific
and therefore should not appear in the generic tem-
plate patterns; instead, they correspond to tem-
plate slots that need to be filled in. Furthermore,
since we work on entity summaries, in each sen-
tence there is usually a word or phrase that refers
to the subject entity, and we should have a place-
holder for the subject entity in each pattern.
Based on the intuitions above, we have the fol-
lowing sentence pattern generation process.
1. Locate subject entities: In each sentence, we
want to locate the word or phrase that refers to the
subject entity. For example, in a biography, usu-
ally a pronoun ?he? or ?she? is used to refer to
the subject person. We use the following heuristic
to locate the subject entities: For each summary
document, we first find the top 3 frequent base
noun phrases that are subjects of sentences. For
example, in a company introduction, the phrase
?the company? is probably used frequently as a
sentence subject. Then for each sentence, we first
look for the title of the Wikipedia article. If it oc-
curs, it is tagged as the subject entity. Otherwise,
we check whether one of the top 3 subject base
noun phrases occurs, and if so, it is tagged as the
subject entity. Otherwise, we tag the subject of the
sentence as the subject entity. Finally, for the iden-
tified subject entity word or phrase, we replace the
label assigned by the entity-aspect model with a
644
professor_A
is_SENT a_S
physics_B university_A
?the_S
nsubj
cop
det
prep_of
det
prep_at
prep_of
Figure 3: An example labeled dependency parse
tree.
new label E.
2. Generate labeled parse trees: We parse each
sentence using the Stanford Parser1. After parsing,
for each sentence we obtain a dependency parse
tree where each node is a single word and each
edge is labeled with a dependency relation. Each
word is also labeled with one of {E, S, B, D,
A}. We replace words labeled with E by a place-
holder ENT, and replace words labeled with D by
a question mark to indicate that these correspond
to template slots. For the other words, we attach
their labels to the tree nodes. Figure 3 shows an
example labeled dependency parse tree.
3. Mine frequent subtree patterns: For the set
of parse trees in each cluster, we use FREQT2, a
software that implements the frequent subtree pat-
tern mining algorithm proposed in (Zaki, 2002), to
find all subtrees with a minimum support of K.
4. Prune patterns: We remove subtree patterns
found by FREQT that do not contain ENT or any
aspect word. We also remove small patterns that
are contained in some other larger pattern in the
same cluster.
5. Covert subtree patterns to sentence patterns:
The remaining patterns are still represented as sub-
trees. To covert them back to human-readable sen-
tence patterns, we map each pattern back to one of
the sentences that contain the pattern to order the
tree nodes according to their original order in the
sentence.
In the end, for each summary collection, we ob-
tain A clusters of sentence patterns, where each
cluster presumably corresponds to a single aspect
or subtopic.
1http://nlp.stanford.edu/software/
lex-parser.shtml
2http://chasen.org/?taku/software/
freqt/
Category D S Sd
min max avg
US Actress 407 1721 1 21 4
Physicist 697 4238 1 49 6
US CEO 179 1040 1 24 5
US Company 375 2477 1 36 6
Restaurant 152 1195 1 37 7
Table 3: The number of documents (D), total
number of sentences (S) and minimum, maximum
and average numbers of sentences per document
(Sd) of the data set.
5 Evaluation
Because we study a non-standard task, there is no
existing annotated data set. We therefore created a
small data set and made our own human judgment
for quantitative evaluation purpose.
5.1 Data
We downloaded five collections of Wikipedia ar-
ticles from different entity categories. We took
only the introduction sections of each article (be-
fore the tables of contents) as entity summaries.
Some statistics of the data set are given in Table 3.
5.2 Quantitative Evaluation
To quantitatively evaluate the summary templates,
we want to check (1) whether our sentence pat-
terns are meaningful and can represent the corre-
sponding entity categories well, and (2) whether
semantically related sentence patterns are grouped
into the same aspect. It is hard to evaluate both
together. We therefore separate these two criteria.
5.2.1 Quality of sentence patterns
To judge the quality of sentence patterns without
looking at aspect clusters, ideally we want to com-
pute the precision and recall of our patterns, that
is, the percentage of our sentence patterns that are
meaningful, and the percentage of true meaningful
sentence patterns of each category that our method
can capture. The former is relatively easy to obtain
because we can ask humans to judge the quality of
our patterns. The latter is much harder to com-
pute because we need human judges to find the set
of true sentence patterns for each entity category,
which can be very subjective.
We adopt the following pooling strategy bor-
rowed from information retrieval. Assume we
want to compare a number of methods that each
can generate a set of sentence patterns from a sum-
mary collection. We take the union of these sets
645
of patterns generated by the different methods and
order them randomly. We then ask a human judge
to decide whether each sentence pattern is mean-
ingful for the given category. We can then treat
the set of meaningful sentence patterns found by
the human judge this way as the ground truth, and
precision and recall of each method can be com-
puted. If our goal is only to compare the different
methods, this pooling strategy should suffice.
We compare our method with the following two
baseline methods.
Baseline 1: In this baseline, we use the same
subtree pattern mining algorithm to find sentence
patterns from each summary collection. We also
locate the subject entities and replace them with
ENT. However, we do not have aspect words or
document words in this case. Therefore we do not
prune any pattern except to merge small patterns
with the large ones that contain them. The pat-
terns generated by this method do not have tem-
plate slots.
Baseline 2: In the second baseline, we apply a
verb-based pruning on the patterns generated by
the first baseline, similar to (Filatova et al, 2006).
We first find the top-20 verbs using the scoring
function below that is taken from (Filatova et al,
2006), and then prune patterns that do not contain
any of the top-20 verbs.
s(vi) = N(vi)?
vj?V N(vj)
? M(vi)D ,
where N(vi) is the frequency of verb vi in the
collection, V is the set of all verbs, D is the total
number of documents in the collection, and M(vi)
is the number of documents in the collection that
contains vi.
In Table 4, we show the precision, recall and f1
of the sentence patterns generated by our method
and the two baseline methods for the five cate-
gories. For our method, we set the support of
the subtree patterns K to 2, that is, each pattern
has occurred in at least two sentences in the cor-
responding aspect cluster. For the two baseline
methods, because sentences are not clustered, we
use a larger support K of 3; otherwise, we find
that there can be too many patterns. We can see
that overall our method gives better f1 measures
than the two baseline methods for most categories.
Our method achieves a good balance between pre-
cision and recall. For BL-1, the precision is high
but recall is low. Intuitively BL-1 should have a
higher recall than our method because our method
Category B Purity
US Actress 4 0.626
Physicist 6 0.714
US CEO 4 0.674
US Company 4 0.614
Restaurant 3 0.587
Table 5: The true numbers of aspects as judged
by the human annotator (B), and the purity of the
clusters.
does more pattern pruning than BL-1 using aspect
words. Here it is not the case mainly because we
used a higher frequency threshold (K = 3) to se-
lect frequent patterns in BL-1, giving overall fewer
patterns than in our method. For BL-2, the preci-
sion is higher than BL-1 but recall is lower. It is
expected because the patterns of BL-2 is a subset
of that of BL-1.
There are some advantages of our method that
are not reflected in Table 4. First, many of our pat-
terns contain template slots, which make the pat-
tern more meaningful. In contrast the baseline pat-
terns do not contain template slots. Because the
human judge did not give preference over patterns
with slots, both ?ENT won the award? and ?ENT
won the ? award? were judged to be meaningful
without any distinction, although the former one
generated by our method is more meaningful. Sec-
ond, compared with BL-2, our method can obtain
patterns that do not contain a non-auxiliary verb,
such as ?ENT was ? director.?
5.2.2 Quality of aspect clusters
We also want to judge the quality of the aspect
clusters. To do so, we ask the human judge to
group the ground truth sentence patterns of each
category based on semantic relatedness. We then
compute the purity of the automatically generated
clusters against the human judged clusters using
purity. The results are shown in Table 5. In our
experiments, we set the number of clusters A used
in the entity-aspect model to be 10. We can see
from Table 5 that our generated aspect clusters can
achieve reasonably good performance.
5.3 Qualitative evaluation
We also conducted qualitative comparison be-
tween our entity-aspect model and standard LDA
model as well as a K-means sentence clustering
method. In Table 6, we show the top 5 fre-
quent words of three sample aspects as found by
our method, standard LDA, and K-means. Note
that although we try to align the aspects, there is
646
Category
Method US Actress Physicist US CEO US Company Restaurant
BL-1 precision 0.714 0.695 0.778 0.622 0.706
recall 0.545 0.300 0.367 0.425 0.361
f1 0.618 0.419 0.499 0.505 0.478
BL-2 precision 0.845 0.767 0.829 0.809 1.000
recall 0.260 0.096 0.127 0.167 0.188
f1 0.397 0.17 0.220 0.276 0.316
Ours precision 0.544 0.607 0.586 0.450 0.560
recall 0.710 0.785 0.712 0.618 0.701
f1 0.616 0.684 0.643 0.520 0.624
Table 4: Quality of sentence patterns in terms of precision, recall and f1.
Method Sample Aspects
1 2 3
Our university prize academy
entity- received nobel sciences
aspect ph.d. physics member
model college awarded national
degree medal society
Standard physics nobel physics
LDA american prize institute
professor physicist research
received awarded member
university john sciences
K-means physics physicist physics
university american academy
institute physics sciences
work university university
research nobel new
Table 6: Comparison of the top 5 words of three
sample aspects using different methods.
no correspondence between clusters numbered the
same but generated by different methods.
We can see that our method gives very mean-
ingful aspect clusters. Standard LDA also gives
meaningful words, but background words such
as ?physics? and ?physicist? are mixed with as-
pect words. Entity-specific words such as ?john?
also appear mixed with aspect words. K-means
clusters are much less meaningful, with too many
background words mixed with aspect words.
6 Related Work
The most related existing work is on domain tem-
plate generation by Filatova et al (2006). There
are several differences between our work and
theirs. First, their template patterns must contain a
non-auxiliary verb whereas ours do not have this
restriction. Second, their verb-centered patterns
are independent of each other, whereas we group
semantically related patterns into aspects, giving
more meaningful templates. Third, in their work,
named entities, numbers and general nouns are
treated as template slots. In our method, we ap-
ply the entity-aspect model to automatically iden-
tify words that are document-specific, and treat
these words as template slots, which can be poten-
tially more robust as we do not rely on the quality
of named entity recognition. Last but not least,
their documents are event-centered while ours are
entity-centered. Therefore we can use heuristics to
anchor our patterns on the subject entities.
Sauper and Barzilay (2009) proposed a frame-
work to learn to automatically generate Wikipedia
articles. There is a fundamental difference be-
tween their task and ours. The articles they gen-
erate are long, comprehensive documents consist-
ing of several sections on different subtopics of
the subject entity, and they focus on learning the
topical structures from complete Wikipedia arti-
cles. We focus on learning sentence patterns of the
short, concise introduction sections of Wikipedia
articles.
Our entity-aspect model is related to a num-
ber of previous extensions of LDA models.
Chemudugunta et al (2007) proposed to intro-
duce a background topic and document-specific
topics. Our background and document language
models are similar to theirs. However, they still
treat documents as bags of words rather than sets
of sentences as in our model. Titov and McDon-
ald (2008) exploited the idea that a short paragraph
within a document is likely to be about the same
aspect. Our one-aspect-per-sentence assumption
is a stricter than theirs, but it is required in our
model for the purpose of mining sentence patterns.
The way we separate words into stop words, back-
ground words, document words and aspect words
bears similarity to that used in (Daume? III and
Marcu, 2006; Haghighi and Vanderwende, 2009),
but their task is multi-document summarization
while ours is to induce summary templates.
647
7 Conclusions and Future Work
In this paper, we studied the task of automati-
cally generating templates for entity summaries.
We proposed an entity-aspect model that can auto-
matically cluster sentences and words into aspects.
The model also labels words in sentences as either
a stop word, a background word, a document word
or an aspect word. We then applied frequent sub-
tree pattern mining to generate sentence patterns
that can represent the aspects. We took advan-
tage of the labels generated by the entity-aspect
model to prune patterns and to locate template
slots. We conducted both quantitative and qualita-
tive evaluation using five collections of Wikipedia
entity summaries. We found that our method gave
overall better template patterns than two baseline
methods, and the aspect clusters generated by our
method are reasonably good.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply linguistic knowledge
to improve the quality of sentence patterns. Cur-
rently the method may generate similar sentence
patterns that differ only slightly, e.g. change of a
preposition. Also, the sentence patterns may not
form complete, meaningful sentences. For exam-
ple, a sentence pattern may contain an adjective
but not the noun it modifies. We plan to study
how to use linguistic knowledge to guide the con-
struction of sentence patterns and make them more
meaningful. Second, we have not quantitatively
evaluated the quality of the template slots, because
our judgment is only at the whole sentence pattern
level. We plan to get more human judges and more
rigorously judge the relevance and usefulness of
both the sentence patterns and the template slots.
It is also possible to introduce certain rules or con-
straints to selectively form template slots rather
than treating all words labeled with D as template
slots.
Acknowledgments
This work was done during Peng Li?s visit to the
Singapore Management University. This work
was partially supported by the National High-tech
Research and Development Project of China (863)
under the grant number 2009AA04Z106 and the
National Science Foundation of China (NSFC) un-
der the grant number 60773088. We thank the
anonymous reviewers for their helpful comments.
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In Advances in Neural Information Processing Sys-
tems 19, pages 241?248.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 305?312.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 207?214.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl. 1):5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 362?370.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 208?216.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 731?738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304?311.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 224?
231.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
648
Proceeding of the 17th International Conference on
World Wide Web, pages 111?120.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In Proceedings of the 16th
ACM Conference on Information and Knowledge
Management, pages 41?50.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining Wikipedia texts using
information from the Web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1021?1029.
Mohammed J. Zaki. 2002. Efficiently mining fre-
quent trees in a forest. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 71?80.
649
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Topical Keyphrase Extraction from Twitter
Wayne Xin Zhao? Jing Jiang? Jing He? Yang Song? Palakorn Achananuparp?
Ee-Peng Lim? Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,peaceful.he,songyangmagic}@gmail.com,
{jingjiang,eplim,palakorna}@smu.edu.sg, lxm@pku.edu.cn
Abstract
Summarizing and analyzing Twitter content is
an important and challenging task. In this pa-
per, we propose to extract topical keyphrases
as one way to summarize Twitter. We propose
a context-sensitive topical PageRank method
for keyword ranking and a probabilistic scor-
ing function that considers both relevance and
interestingness of keyphrases for keyphrase
ranking. We evaluate our proposed methods
on a large Twitter data set. Experiments show
that these methods are very effective for topi-
cal keyphrase extraction.
1 Introduction
Twitter, a new microblogging website, has attracted
hundreds of millions of users who publish short
messages (a.k.a. tweets) on it. They either pub-
lish original tweets or retweet (i.e. forward) oth-
ers? tweets if they find them interesting. Twitter
has been shown to be useful in a number of appli-
cations, including tweets as social sensors of real-
time events (Sakaki et al, 2010), the sentiment pre-
diction power of Twitter (Tumasjan et al, 2010),
etc. However, current explorations are still in an
early stage and our understanding of Twitter content
still remains limited. How to automatically under-
stand, extract and summarize useful Twitter content
has therefore become an important and emergent re-
search topic.
In this paper, we propose to extract keyphrases
as a way to summarize Twitter content. Tradition-
ally, keyphrases are defined as a short list of terms to
summarize the topics of a document (Turney, 2000).
It can be used for various tasks such as document
summarization (Litvak and Last, 2008) and index-
ing (Li et al, 2004). While it appears natural to use
keyphrases to summarize Twitter content, compared
with traditional text collections, keyphrase extrac-
tion from Twitter is more challenging in at least two
aspects: 1) Tweets are much shorter than traditional
articles and not all tweets contain useful informa-
tion; 2) Topics tend to be more diverse in Twitter
than in formal articles such as news reports.
So far there is little work on keyword or keyphrase
extraction from Twitter. Wu et al (2010) proposed
to automatically generate personalized tags for Twit-
ter users. However, user-level tags may not be suit-
able to summarize the overall Twitter content within
a certain period and/or from a certain group of peo-
ple such as people in the same region. Existing work
on keyphrase extraction identifies keyphrases from
either individual documents or an entire text collec-
tion (Turney, 2000; Tomokiyo and Hurst, 2003).
These approaches are not immediately applicable
to Twitter because it does not make sense to ex-
tract keyphrases from a single tweet, and if we ex-
tract keyphrases from a whole tweet collection we
will mix a diverse range of topics together, which
makes it difficult for users to follow the extracted
keyphrases.
Therefore, in this paper, we propose to study the
novel problem of extracting topical keyphrases for
summarizing and analyzing Twitter content. In other
words, we extract and organize keyphrases by top-
ics learnt from Twitter. In our work, we follow the
standard three steps of keyphrase extraction, namely,
keyword ranking, candidate keyphrase generation
379
and keyphrase ranking. For keyword ranking, we
modify the Topical PageRank method proposed by
Liu et al (2010) by introducing topic-sensitive score
propagation. We find that topic-sensitive propaga-
tion can largely help boost the performance. For
keyphrase ranking, we propose a principled proba-
bilistic phrase ranking method, which can be flex-
ibly combined with any keyword ranking method
and candidate keyphrase generation method. Ex-
periments on a large Twitter data set show that
our proposed methods are very effective in topical
keyphrase extraction from Twitter. Interestingly, our
proposed keyphrase ranking method can incorporate
users? interests by modeling the retweet behavior.
We further examine what topics are suitable for in-
corporating users? interests for topical keyphrase ex-
traction.
To the best of our knowledge, our work is the
first to study how to extract keyphrases from mi-
croblogs. We perform a thorough analysis of the
proposed methods, which can be useful for future
work in this direction.
2 Related Work
Our work is related to unsupervised keyphrase ex-
traction. Graph-based ranking methods are the
state of the art in unsupervised keyphrase extrac-
tion. Mihalcea and Tarau (2004) proposed to use
TextRank, a modified PageRank algorithm to ex-
tract keyphrases. Based on the study by Mihalcea
and Tarau (2004), Liu et al (2010) proposed to de-
compose a traditional random walk into multiple
random walks specific to various topics. Language
modeling methods (Tomokiyo and Hurst, 2003) and
natural language processing techniques (Barker and
Cornacchia, 2000) have also been used for unsuper-
vised keyphrase extraction. Our keyword extraction
method is mainly based on the study by Liu et al
(2010). The difference is that we model the score
propagation with topic context, which can lower the
effect of noise, especially in microblogs.
Our work is also related to automatic topic label-
ing (Mei et al, 2007). We focus on extracting topical
keyphrases in microblogs, which has its own chal-
lenges. Our method can also be used to label topics
in other text collections.
Another line of relevant research is Twitter-
related text mining. The most relevant work is
by Wu et al (2010), who directly applied Tex-
tRank (Mihalcea and Tarau, 2004) to extract key-
words from tweets to tag users. Topic discovery
from Twitter is also related to our work (Ramage et
al., 2010), but we further extract keyphrases from
each topic for summarizing and analyzing Twitter
content.
3 Method
3.1 Preliminaries
Let U be a set of Twitter users. Let C =
{{du,m}
Mu
m=1}u?U be a collection of tweets gener-
ated by U , where Mu is the total number of tweets
generated by user u and du,m is the m-th tweet of
u. Let V be the vocabulary. du,m consists of a
sequence of words (wu,m,1, wu,m,2, . . . , wu,m,Nu,m)
where Nu,m is the number of words in du,m and
wu,m,n ? V (1 ? n ? Nu,m). We also assume
that there is a set of topics T over the collection C.
Given T and C, topical keyphrase extraction is to
discover a list of keyphrases for each topic t ? T .
Here each keyphrase is a sequence of words.
To extract keyphrases, we first identify topics
from the Twitter collection using topic models (Sec-
tion 3.2). Next for each topic, we run a topical
PageRank algorithm to rank keywords and then gen-
erate candidate keyphrases using the top ranked key-
words (Section 3.3). Finally, we use a probabilis-
tic model to rank the candidate keyphrases (Sec-
tion 3.4).
3.2 Topic discovery
We first describe how we discover the set of topics
T . Author-topic models have been shown to be ef-
fective for topic modeling of microblogs (Weng et
al., 2010; Hong and Davison, 2010). In Twit-
ter, we observe an important characteristic of tweets:
tweets are short and a single tweet tends to be about
a single topic. So we apply a modified author-topic
model called Twitter-LDA introduced by Zhao et al
(2011), which assumes a single topic assignment for
an entire tweet.
The model is based on the following assumptions.
There is a set of topics T in Twitter, each represented
by a word distribution. Each user has her topic inter-
ests modeled by a distribution over the topics. When
a user wants to write a tweet, she first chooses a topic
based on her topic distribution. Then she chooses a
380
1. Draw ?B ? Dir(?), pi ? Dir(?)
2. For each topic t ? T ,
(a) draw ?t ? Dir(?)
3. For each user u ? U ,
(a) draw ?u ? Dir(?)
(b) for each tweet du,m
i. draw zu,m ? Multi(?u)
ii. for each word wu,m,n
A. draw yu,m,n ? Bernoulli(pi)
B. draw wu,m,n ? Multi(?B) if
yu,m,n = 0 and wu,m,n ?
Multi(?zu,m) if yu,m,n = 1
Figure 1: The generation process of tweets.
bag of words one by one based on the chosen topic.
However, not all words in a tweet are closely re-
lated to the topic of that tweet; some are background
words commonly used in tweets on different topics.
Therefore, for each word in a tweet, the user first
decides whether it is a background word or a topic
word and then chooses the word from its respective
word distribution.
Formally, let ?t denote the word distribution for
topic t and ?B the word distribution for background
words. Let ?u denote the topic distribution of user
u. Let pi denote a Bernoulli distribution that gov-
erns the choice between background words and topic
words. The generation process of tweets is described
in Figure 1. Each multinomial distribution is gov-
erned by some symmetric Dirichlet distribution pa-
rameterized by ?, ? or ?.
3.3 Topical PageRank for Keyword Ranking
Topical PageRank was introduced by Liu et al
(2010) to identify keywords for future keyphrase
extraction. It runs topic-biased PageRank for each
topic separately and boosts those words with high
relevance to the corresponding topic. Formally, the
topic-specific PageRank scores can be defined as
follows:
Rt(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rt(wj)+ (1??)Pt(wi),
(1)
where Rt(w) is the topic-specific PageRank score
of word w in topic t, e(wj , wi) is the weight for the
edge (wj ? wi), O(wj) =
?
w? e(wj , w
?) and ?
is a damping factor ranging from 0 to 1. The topic-
specific preference value Pt(w) for each word w is
its random jumping probability with the constraint
that
?
w?V Pt(w) = 1 given topic t. A large Rt(?)
indicates a word is a good candidate keyword in
topic t. We denote this original version of the Topi-
cal PageRank as TPR.
However, the original TPR ignores the topic con-
text when setting the edge weights; the edge weight
is set by counting the number of co-occurrences of
the two words within a certain window size. Tak-
ing the topic of ?electronic products? as an exam-
ple, the word ?juice? may co-occur frequently with a
good keyword ?apple? for this topic because of Ap-
ple electronic products, so ?juice? may be ranked
high by this context-free co-occurrence edge weight
although it is not related to electronic products. In
other words, context-free propagation may cause the
scores to be off-topic.
So in this paper, we propose to use a topic context
sensitive PageRank method. Formally, we have
Rt(wi) = ?
?
j:wj?wi
et(wj , wi)
Ot(wj)
Rt(wj)+(1??)Pt(wi).
(2)
Here we compute the propagation from wj to wi in
the context of topic t, namely, the edge weight from
wj to wi is parameterized by t. In this paper, we
compute edge weight et(wj , wi) between two words
by counting the number of co-occurrences of these
two words in tweets assigned to topic t. We denote
this context-sensitive topical PageRank as cTPR.
After keyword ranking using cTPR or any other
method, we adopt a common candidate keyphrase
generation method proposed by Mihalcea and Tarau
(2004) as follows. We first select the top S keywords
for each topic, and then look for combinations of
these keywords that occur as frequent phrases in the
text collection. More details are given in Section 4.
3.4 Probabilistic Models for Topical Keyphrase
Ranking
With the candidate keyphrases, our next step is to
rank them. While a standard method is to simply
aggregate the scores of keywords inside a candidate
keyphrase as the score for the keyphrase, here we
propose a different probabilistic scoring function.
Our method is based on the following hypotheses
about good keyphrases given a topic:
381
Figure 2: Assumptions of variable dependencies.
Relevance: A good keyphrase should be closely re-
lated to the given topic and also discriminative. For
example, for the topic ?news,? ?president obama? is
a good keyphrase while ?math class? is not.
Interestingness: A good keyphrase should be inter-
esting and can attract users? attention. For example,
for the topic ?music,? ?justin bieber? is more inter-
esting than ?song player.?
Sometimes, there is a trade-off between these two
properties and a good keyphrase has to balance both.
Let R be a binary variable to denote relevance
where 1 is relevant and 0 is irrelevant. Let I be an-
other binary variable to denote interestingness where
1 is interesting and 0 is non-interesting. Let k denote
a candidate keyphrase. Following the probabilistic
relevance models in information retrieval (Lafferty
and Zhai, 2003), we propose to use P (R = 1, I =
1|t, k) to rank candidate keyphrases for topic t. We
have
P (R = 1, I = 1|t, k)
= P (R = 1|t, k)P (I = 1|t, k, R = 1)
= P (I = 1|t, k, R = 1)P (R = 1|t, k)
= P (I = 1|k)P (R = 1|t, k)
= P (I = 1|k)?
P (R = 1|t, k)
P (R = 1|t, k) + P (R = 0|t, k)
= P (I = 1|k)?
1
1 + P (R=0|t,k)P (R=1|t,k)
= P (I = 1|k)?
1
1 + P (R=0,k|t)P (R=1,k|t)
= P (I = 1|k)?
1
1 + P (R=0|t)P (R=1|t) ?
P (k|t,R=0)
P (k|t,R=1)
= P (I = 1|k)?
1
1 + P (R=0)P (R=1) ?
P (k|t,R=0)
P (k|t,R=1)
.
Here we have assumed that I is independent of t and
R given k, i.e. the interestingness of a keyphrase is
independent of the topic or whether the keyphrase is
relevant to the topic. We have also assumed that R
is independent of t when k is unknown, i.e. without
knowing the keyphrase, the relevance is independent
of the topic. Our assumptions can be depicted by
Figure 2.
We further define ? = P (R=0)P (R=1) . In general we
can assume that P (R = 0)  P (R = 1) because
there are much more non-relevant keyphrases than
relevant ones, that is, ?  1. In this case, we have
logP (R = 1, I = 1|t, k) (3)
= log
(
P (I = 1|k)?
1
1 + ? ? P (k|t,R=0)P (k|t,R=1)
)
? log
(
P (I = 1|k)?
P (k|t, R = 1)
P (k|t, R = 0)
?
1
?
)
= logP (I = 1|k) + log
P (k|t, R = 1)
P (k|t, R = 0)
? log ?.
We can see that the ranking score logP (R = 1, I =
1|t, k) can be decomposed into two components, a
relevance score log P (k|t,R=1)P (k|t,R=0) and an interestingness
score logP (I = 1|k). The last term log ? is a con-
stant and thus not relevant.
Estimating the relevance score
Let a keyphrase candidate k be a sequence of
words (w1, w2, . . . , wN ). Based on an independent
assumption of words given R and t, we have
log
P (k|t, R = 1)
P (k|t, R = 0)
= log
P (w1w2 . . . wN |t, R = 1)
P (w1w2 . . . wN |t, R = 0)
=
N?
n=1
log
P (wn|t, R = 1)
P (wn|t, R = 0)
. (4)
Given the topic model ?t previously learned for
topic t, we can set P (w|t, R = 1) to ?tw, i.e. the
probability of w under ?t. Following Griffiths and
Steyvers (2004), we estimate ?tw as
?tw =
#(Ct, w) + ?
#(Ct, ?) + ?|V|
. (5)
Here Ct denotes the collection of tweets assigned to
topic t, #(Ct, w) is the number of times w appears in
Ct, and #(Ct, ?) is the total number of words in Ct.
P (w|t, R = 0) can be estimated using a smoothed
background model.
P (w|R = 0, t) =
#(C, w) + ?
#(C, ?) + ?|V|
. (6)
382
Here #(C, ?) denotes the number of words in the
whole collection C, and #(C, w) denotes the number
of times w appears in the whole collection.
After plugging Equation (5) and Equation (6) into
Equation (4), we get the following formula for the
relevance score:
log
P (k|t, R = 1)
P (k|t, R = 0)
=
?
w?k
(
log
#(Ct, w) + ?
#(C, w) + ?
+ log
#(C, ?) + ?|V|
#(Ct, ?) + ?|V|
)
=
(?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?, (7)
where ? = #(C,?)+?|V|#(Ct,?)+?|V| and |k| denotes the number
of words in k.
Estimating the interestingness score
To capture the interestingness of keyphrases, we
make use of the retweeting behavior in Twitter. We
use string matching with RT to determine whether
a tweet is an original posting or a retweet. If a
tweet is interesting, it tends to get retweeted mul-
tiple times. Retweeting is therefore a stronger indi-
cator of user interests than tweeting. We use retweet
ratio |ReTweetsk||Tweetsk| to estimate P (I = 1|k). To prevent
zero frequency, we use a modified add-one smooth-
ing method. Finally, we get
logP (I = 1|k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
. (8)
Here |ReTweetsk| and |Tweetsk| denote the num-
bers of retweets and tweets containing the keyphrase
k, respectively, and lavg is the average number of
tweets that a candidate keyphrase appears in.
Finally, we can plug Equation (7) and Equa-
tion (8) into Equation (3) and obtain the following
scoring function for ranking:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(9)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?.
#user #tweet #term #token
13,307 1,300,300 50,506 11,868,910
Table 1: Some statistics of the data set.
Incorporating length preference
Our preliminary experiments with Equation (9)
show that this scoring function usually ranks longer
keyphrases higher than shorter ones. However, be-
cause our candidate keyphrase are extracted without
using any linguistic knowledge such as noun phrase
boundaries, longer candidate keyphrases tend to be
less meaningful as a phrase. Moreover, for our task
of using keyphrases to summarize Twitter, we hy-
pothesize that shorter keyphrases are preferred by
users as they are more compact. We would there-
fore like to incorporate some length preference.
Recall that Equation (9) is derived from P (R =
1, I = 1|t, k), but this probability does not allow
us to directly incorporate any length preference. We
further observe that Equation (9) tends to give longer
keyphrases higher scores mainly due to the term
|k|?. So here we heuristically incorporate our length
preference by removing |k|? from Equation (9), re-
sulting in the following final scoring function:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(10)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
.
4 Experiments
4.1 Data Set and Preprocessing
We use a Twitter data set collected from Singapore
users for evaluation. We used Twitter REST API1
to facilitate the data collection. The majority of the
tweets collected were published in a 20-week period
from December 1, 2009 through April 18, 2010. We
removed common stopwords and words which ap-
peared in fewer than 10 tweets. We also removed all
users who had fewer than 5 tweets. Some statistics
of this data set after cleaning are shown in Table 1.
We ran Twitter-LDA with 500 iterations of Gibbs
sampling. After trying a few different numbers of
1http://apiwiki.twitter.com/w/page/22554663/REST-API-
Documentation
383
topics, we empirically set the number of topics to
30. We set ? to 50.0/|T | as Griffiths and Steyvers
(2004) suggested, but set ? to a smaller value of 0.01
and ? to 20. We chose these parameter settings be-
cause they generally gave coherent and meaningful
topics for our data set. We selected 10 topics that
cover a diverse range of content in Twitter for eval-
uation of topical keyphrase extraction. The top 10
words of these topics are shown in Table 2.
We also tried the standard LDA model and the
author-topic model on our data set and found that
our proposed topic model was better or at least com-
parable in terms of finding meaningful topics. In ad-
dition to generating meaningful topics, Twitter-LDA
is much more convenient in supporting the compu-
tation of tweet-level statistics (e.g. the number of
co-occurrences of two words in a specific topic) than
the standard LDA or the author-topic model because
Twitter-LDA assumes a single topic assignment for
an entire tweet.
4.2 Methods for Comparison
As we have described in Section 3.1, there are three
steps to generate keyphrases, namely, keyword rank-
ing, candidate keyphrase generation, and keyphrase
ranking. We have proposed a context-sensitive top-
ical PageRank method (cTPR) for the first step of
keyword ranking, and a probabilistic scoring func-
tion for the third step of keyphrase ranking. We now
describe the baseline methods we use to compare
with our proposed methods.
Keyword Ranking
We compare our cTPR method with the original
topical PageRank method (Equation (1)), which rep-
resents the state of the art. We refer to this baseline
as TPR.
For both TPR and cTPR, the damping factor is
empirically set to 0.1, which always gives the best
performance based on our preliminary experiments.
We use normalized P (t|w) to set Pt(w) because our
preliminary experiments showed that this was the
best among the three choices discussed by Liu et al
(2010). This finding is also consistent with what Liu
et al (2010) found.
In addition, we also use two other baselines for
comparison: (1) kwBL1: ranking by P (w|t) = ?tw.
(2) kwBL2: ranking by P (t|w) = P (t)?
t
w?
t? P (t
?)?t?w
.
Keyphrase Ranking
We use kpRelInt to denote our relevance and inter-
estingness based keyphrase ranking function P (R =
1, I = 1|t, k), i.e. Equation (10). ? and ? are em-
pirically set to 0.01 and 500. Usually ? can be set to
zero, but in our experiments we find that our rank-
ing method needs a more uniform estimation of the
background model. We use the following ranking
functions for comparison:
? kpBL1: Similar to what is used by Liu et al
(2010), we can rank candidate keyphrases by
?
w?k f(w), where f(w) is the score assigned
to word w by a keyword ranking method.
? kpBL2: We consider another baseline ranking
method by
?
w?k log f(w).
? kpRel: If we consider only relevance but
not interestingness, we can rank candidate
keyphrases by
?
w?k log
#(Ct,w)+?
#(C,w)+? .
4.3 Gold Standard Generation
Since there is no existing test collection for topi-
cal keyphrase extraction from Twitter, we manually
constructed our test collection. For each of the 10
selected topics, we ran all the methods to rank key-
words. For each method we selected the top 3000
keywords and searched all the combinations of these
words as phrases which have a frequency larger than
30. In order to achieve high phraseness, we first
computed the minimum value of pointwise mutual
information for all bigrams in one combination, and
we removed combinations having a value below a
threshold, which was empirically set to 2.135. Then
we merged all these candidate phrases. We did not
consider single-word phrases because we found that
it would include too many frequent words that might
not be useful for summaries.
We asked two judges to judge the quality of the
candidate keyphrases. The judges live in Singapore
and had used Twitter before. For each topic, the
judges were given the top topic words and a short
topic description. Web search was also available.
For each candidate keyphrase, we asked the judges
to score it as follows: 2 (relevant, meaningful and in-
formative), 1 (relevant but either too general or too
specific, or informal) and 0 (irrelevant or meaning-
less). Here in addition to relevance, the other two
criteria, namely, whether a phrase is meaningful and
informative, were studied by Tomokiyo and Hurst
384
T2 T4 T5 T10 T12 T13 T18 T20 T23 T25
eat twitter love singapore singapore hot iphone song study win
food tweet idol road #singapore rain google video school game
dinner blog adam mrt #business weather social youtube time team
lunch facebook watch sgreinfo #news cold media love homework match
eating internet april east health morning ipad songs tomorrow play
ice tweets hot park asia sun twitter bieber maths chelsea
chicken follow lambert room market good free music class world
cream msn awesome sqft world night app justin paper united
tea followers girl price prices raining apple feature math liverpool
hungry time american built bank air marketing twitter finish arsenal
Table 2: Top 10 Words of Sample Topics on our Singapore Twitter Dateset.
(2003). We then averaged the scores of the two
judges as the final scores. The Cohen?s Kappa co-
efficients of the 10 topics range from 0.45 to 0.80,
showing fair to good agreement2. We further dis-
carded all candidates with an average score less than
1. The number of the remaining keyphrases for each
topic ranges from 56 to 282.
4.4 Evaluation Metrics
Traditionally keyphrase extraction is evaluated using
precision and recall on all the extracted keyphrases.
We choose not to use these measures for the fol-
lowing reasons: (1) Traditional keyphrase extraction
works on single documents while we study topical
keyphrase extraction. The gold standard keyphrase
list for a single document is usually short and clean,
while for each Twitter topic there can be many
keyphrases, some are more relevant and interesting
than others. (2) Our extracted topical keyphrases are
meant for summarizing Twitter content, and they are
likely to be directly shown to the users. It is there-
fore more meaningful to focus on the quality of the
top-ranked keyphrases.
Inspired by the popular nDCG metric in informa-
tion retrieval (Ja?rvelin and Keka?la?inen, 2002), we
define the following normalized keyphrase quality
measure (nKQM) for a methodM:
nKQM@K =
1
|T |
?
t?T
?K
j=1
1
log2(j+1)
score(Mt,j)
IdealScore(K,t)
,
where T is the set of topics, Mt,j is the j-
th keyphrase generated by method M for topic
2We find that judgments on topics related to social me-
dia (e.g. T4) and daily life (e.g. T13) tend to have a higher
degree of disagreement.
t, score(?) is the average score from the two hu-
man judges, and IdealScore(K,t) is the normalization
factor?score of the top K keyphrases of topic t un-
der the ideal ranking. Intuitively, ifM returns more
good keyphrases in top ranks, its nKQM value will
be higher.
We also use mean average precision (MAP) to
measure the overall performance of keyphrase rank-
ing:
MAP =
1
|T |
?
t?T
1
NM,t
|Mt|?
j=1
NM,t,j
j
1(score(Mt,j) ? 1),
where 1(S) is an indicator function which returns
1 when S is true and 0 otherwise, NM,t,j denotes
the number of correct keyphrases among the top j
keyphrases returned byM for topic t, and NM,t de-
notes the total number of correct keyphrases of topic
t returned byM.
4.5 Experiment Results
Evaluation of keyword ranking methods
Since keyword ranking is the first step for
keyphrase extraction, we first compare our keyword
ranking method cTPR with other methods. For each
topic, we pooled the top 20 keywords ranked by all
four methods. We manually examined whether a
word is a good keyword or a noisy word based on
topic context. Then we computed the average num-
ber of noisy words in the 10 topics for each method.
As shown in Table 5, we can observe that cTPR per-
formed the best among the four methods.
Since our final goal is to extract topical
keyphrases, we further compare the performance
of cTPR and TPR when they are combined with a
keyphrase ranking algorithm. Here we use the two
385
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
kpBL1 TPR 0.5015 0.54331 0.5611 0.5715 0.5984
kwBL1 0.6026 0.5683 0.5579 0.5254 0.5984
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6279
cTPR 0.6109 0.6218 0.6139 0.6062 0.6608
kpBL2 TPR 0.7294 0.7172 0.6921 0.6433 0.6379
kwBL1 0.7111 0.6614 0.6306 0.5829 0.5416
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6545
cTPR 0.7491 0.7429 0.6930 0.6519 0.6688
Table 3: Comparisons of keyphrase extraction for cTPR and baselines.
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
cTPR+kpBL1 0.61095 0.62182 0.61389 0.60618 0.6608
cTPR+kpBL2 0.74913 0.74294 0.69303 0.65194 0.6688
cTPR+kpRel 0.75361 0.74926 0.69645 0.65065 0.6696
cTPR+kpRelInt 0.81061 0.75184 0.71422 0.66319 0.6694
Table 4: Comparisons of keyphrase extraction for different keyphrase ranking methods.
kwBL1 kwBL2 TPR cTPR
2 3 4.9 1.5
Table 5: Average number of noisy words among the top
20 keywords of the 10 topics.
baseline keyphrase ranking algorithms kpBL1 and
kpBL2. The comparison is shown in Table 3. We
can see that cTPR is consistently better than the three
other methods for both kpBL1 and kpBL2.
Evaluation of keyphrase ranking methods
In this section we compare keypharse ranking
methods. Previously we have shown that cTPR is
better than TPR, kwBL1 and kwBL2 for keyword
ranking. Therefore we use cTPR as the keyword
ranking method and examine the keyphrase rank-
ing method kpRelInt with kpBL1, kpBL2 and kpRel
when they are combined with cTPR. The results are
shown in Table 4. From the results we can see the
following: (1) Keyphrase ranking methods kpRelInt
and kpRel are more effective than kpBL1 and kpBL2,
especially when using the nKQM metric. (2) kpRe-
lInt is better than kpRel, especially for the nKQM
metric. Interestingly, we also see that for the nKQM
metric, kpBL1, which is the most commonly used
keyphrase ranking method, did not perform as well
as kpBL2, a modified version of kpBL1.
We also tested kpRelInt and kpRel on TPR, kwBL1
and kwBL2 and found that kpRelInt and kpRel are
consistently better than kpBL2 and kpBL1. Due to
space limit, we do not report all the results here.
These findings support our assumption that our pro-
posed keyphrase ranking method is effective.
The comparison between kpBL2 with kpBL1
shows that taking the product of keyword scores is
more effective than taking their sum. kpRel and
kpRelInt also use the product of keyword scores.
This may be because there is more noise in Twit-
ter than traditional documents. Common words (e.g.
?good?) and domain background words (e.g. ?Sin-
gapore?) tend to gain higher weights during keyword
ranking due to their high frequency, especially in
graph-based method, but we do not want such words
to contribute too much to keyphrase scores. Taking
the product of keyword scores is therefore more suit-
able here than taking their sum.
Further analysis of interestingness
As shown in Table 4, kpRelInt performs better
in terms of nKQM compared with kpRel. Here we
study why it worked better for keyphrase ranking.
The only difference between kpRel and kpRelInt is
that kpRelInt includes the factor of user interests. By
manually examining the top keyphrases, we find that
the topics ?Movie-TV? (T5), ?News? (T12), ?Music?
(T20) and ?Sports? (T25) particularly benefited from
kpRelInt compared with other topics. We find that
well-known named entities (e.g. celebrities, politi-
cal leaders, football clubs and big companies) and
significant events tend to be ranked higher by kpRe-
lInt than kpRel.
We then counted the numbers of entity and event
keyphrases for these four topics retrieved by differ-
ent methods, shown in Table 6 . We can see that
in these four topics, kpRelInt is consistently better
than kpRel in terms of the number of entity and event
keyphrases retrieved.
386
T2 T5 T10 T12 T20 T25
chicken rice adam lambert north east president obama justin bieber manchester united
ice cream jack neo rent blk magnitude earthquake music video champions league
fried chicken american idol east coast volcanic ash lady gaga football match
curry rice david archuleta east plaza prime minister taylor swift premier league
chicken porridge robert pattinson west coast iceland volcano demi lovato f1 grand prix
curry chicken alexander mcqueen bukit timah chile earthquake youtube channel tiger woods
beef noodles april fools street view goldman sachs miley cyrus grand slam(tennis)
chocolate cake harry potter orchard road coe prices telephone video liverpool fans
cheese fries april fool toa payoh haiti earthquake song lyrics final score
instant noodles andrew garcia marina bay #singapore #business joe jonas manchester derby
Table 7: Top 10 keyphrases of 6 topics from cTPR+kpRelInt.
Methods T5 T12 T20 T25
cTPR+kpRel 8 9 16 11
cTPR+kpRelInt 10 12 17 14
Table 6: Numbers of entity and event keyphrases re-
trieved by different methods within top 20.
On the other hand, we also find that for some
topics interestingness helped little or even hurt the
performance a little, e.g. for the topics ?Food? and
?Traffic.? We find that the keyphrases in these top-
ics are stable and change less over time. This may
suggest that we can modify our formula to handle
different topics different. We will explore this direc-
tion in our future work.
Parameter settings
We also examine how the parameters in our model
affect the performance.
?: We performed a search from 0.1 to 0.9 with a
step size of 0.1. We found ? = 0.1 was the optimal
parameter for cTPR and TPR. However, TPR is more
sensitive to ?. The performance went down quickly
with ? increasing.
?: We checked the overall performance with
? ? {400, 450, 500, 550, 600}. We found that ? =
500 ? 0.01|V| gave the best performance gener-
ally for cTPR. The performance difference is not
very significant between these different values of ?,
which indicates that the our method is robust.
4.6 Qualitative evaluation of cTPR+kpRelInt
We show the top 10 keyphrases discovered by
cTPR+kRelInt in Table 7. We can observe that these
keyphrases are clear, interesting and informative for
summarizing Twitter topics.
We hypothesize that the following applications
can benefit from the extracted keyphrases:
Automatic generation of realtime trendy phrases:
For exampoe, keyphrases in the topic ?Food? (T2)
can be used to help online restaurant reviews.
Event detection and topic tracking: In the topic
?News? top keyphrases can be used as candidate
trendy topics for event detection and topic tracking.
Automatic discovery of important named entities:
As discussed previously, our methods tend to rank
important named entities such as celebrities in high
ranks.
5 Conclusion
In this paper, we studied the novel problem of topical
keyphrase extraction for summarizing and analyzing
Twitter content. We proposed the context-sensitive
topical PageRank (cTPR) method for keyword rank-
ing. Experiments showed that cTPR is consistently
better than the original TPR and other baseline meth-
ods in terms of top keyword and keyphrase extrac-
tion. For keyphrase ranking, we proposed a prob-
abilistic ranking method, which models both rele-
vance and interestingness of keyphrases. In our ex-
periments, this method is shown to be very effec-
tive to boost the performance of keyphrase extrac-
tion for different kinds of keyword ranking methods.
In the future, we may consider how to incorporate
keyword scores into our keyphrase ranking method.
Note that we propose to rank keyphrases by a gen-
eral formula P (R = 1, I = 1|t, k) and we have made
some approximations based on reasonable assump-
tions. There should be other potential ways to esti-
mate P (R = 1, I = 1|t, k).
Acknowledgements
This work was done during Xin Zhao?s visit to the
Singapore Management University. Xin Zhao and
Xiaoming Li are partially supported by NSFC under
387
the grant No. 60933004, 61073082, 61050009 and
HGJ Grant No. 2011ZX01042-001-001.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In Pro-
ceedings of the 13th Biennial Conference of the Cana-
dian Society on Computational Studies of Intelligence:
Advances in Artificial Intelligence, pages 40?52.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in Twitter. In Proceedings of
the First Workshop on Social Media Analytics.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems, 20(4):422?446.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic
relevance models based on document and query gener-
ation. Language Modeling and Information Retrieval,
13.
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen.
2004. Incorporating document keyphrases in search
results. In Proceedings of the 10th Americas Confer-
ence on Information Systems.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong
Sun. 2010. Automatic keyphrase extraction via topic
decomposition. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 366?376.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing or-
der into texts. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing micorblogs with topic models. In Pro-
ceedings of the 4th International Conference on We-
blogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International World Wide Web Conference.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the 4th International
Conference on Weblogs and Social Media.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, (4):303?336.
Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. TwitterRank: finding topic-sensitive influential
twitterers. In Proceedings of the third ACM Interna-
tional Conference on Web Search and Data Mining.
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. Au-
tomatic generation of personalized annotation tags for
twitter users. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 689?692.
Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Lim Ee-
Peng, Hongfei Yan, and Xiaoming Li. 2011. Compar-
ing Twitter and traditional media using topic models.
In Proceedings of the 33rd European Conference on
Information Retrieval.
388
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 536?544,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Finding Bursty Topics from Microblogs
Qiming Diao, Jing Jiang, Feida Zhu, Ee-Peng Lim
Living Analytics Research Centre
School of Information Systems
Singapore Management University
{qiming.diao.2010, jingjiang, fdzhu, eplim}@smu.edu.sg
Abstract
Microblogs such as Twitter reflect the general
public?s reactions to major events. Bursty top-
ics from microblogs reveal what events have
attracted the most online attention. Although
bursty event detection from text streams has
been studied before, previous work may not
be suitable for microblogs because compared
with other text streams such as news articles
and scientific publications, microblog posts
are particularly diverse and noisy. To find top-
ics that have bursty patterns on microblogs,
we propose a topic model that simultaneous-
ly captures two observations: (1) posts pub-
lished around the same time are more like-
ly to have the same topic, and (2) posts pub-
lished by the same user are more likely to have
the same topic. The former helps find event-
driven posts while the latter helps identify and
filter out ?personal? posts. Our experiments
on a large Twitter dataset show that there are
more meaningful and unique bursty topics in
the top-ranked results returned by our mod-
el than an LDA baseline and two degenerate
variations of our model. We also show some
case studies that demonstrate the importance
of considering both the temporal information
and users? personal interests for bursty topic
detection from microblogs.
1 Introduction
With the fast growth of Web 2.0, a vast amount of
user-generated content has accumulated on the so-
cial Web. In particular, microblogging sites such
as Twitter allow users to easily publish short in-
stant posts about any topic to be shared with the
general public. The textual content coupled with
the temporal patterns of these microblog posts pro-
vides important insight into the general public?s in-
terest. A sudden increase of topically similar posts
usually indicates a burst of interest in some event
that has happened offline (such as a product launch
or a natural disaster) or online (such as the spread
of a viral video). Finding bursty topics from mi-
croblogs therefore can help us identify the most pop-
ular events that have drawn the public?s attention. In
this paper, we study the problem of finding bursty
topics from a stream of microblog posts generated
by different users. We focus on retrospective detec-
tion, where the text stream within a certain period is
analyzed in its entirety.
Retrospective bursty event detection from tex-
t streams is not new (Kleinberg, 2002; Fung et al,
2005; Wang et al, 2007), but finding bursty topic-
s from microblog steams has not been well studied.
In his seminal work, Kleinberg (2002) proposed a s-
tate machine to model the arrival times of documents
in a stream in order to identify bursts. This model
has been widely used. However, this model assumes
that documents in the stream are all about a given
topic. In contrast, discovering interesting topics that
have drawn bursts of interest from a stream of top-
ically diverse microblog posts is itself a challenge.
To discover topics, we can certainly apply standard
topic models such as LDA (Blei et al, 2003), but
with standard LDA temporal information is lost dur-
ing topic discovery. For microblogs, where posts are
short and often event-driven, temporal information
can sometimes be critical in determining the topic of
a post. For example, typically a post containing the
536
word ?jobs? is likely to be about employment, but
right after October 5, 2011, a post containing ?jobs?
is more likely to be related to Steve Jobs? death. Es-
sentially, we expect that on microblogs, posts pub-
lished around the same time have a higher probabil-
ity to belong to the same topic.
To capture this intuition, one solution is to assume
that posts published within the same short time win-
dow follow the same topic distribution. Wang et
al. (2007) proposed a PLSA-based topic model that
exploits this idea to find correlated bursty patterns
across multiple text streams. However, their model
is not immediately applicable for our problem. First,
their model assumes multiple text streams where
word distributions for the same topic are different
on different streams. More importantly, their model
was applied to news articles and scientific publica-
tions, where most documents follow the global top-
ical trends. On microblogs, besides talking about
global popular events, users also often talk about
their daily lives and personal interests. In order to
detect global bursty events from microblog posts, it
is important to filter out these ?personal? posts.
In this paper, we propose a topic model designed
for finding bursty topics from microblogs. Our mod-
el is based on the following two assumptions: (1) If
a post is about a global event, it is likely to follow
a global topic distribution that is time-dependent.
(2) If a post is about a personal topic, it is likely
to follow a personal topic distribution that is more
or less stable over time. Separation of ?global? and
?personal? posts is done in an unsupervised manner
through hidden variables. Finally, we apply a state
machine to detect bursts from the discovered topics.
We evaluate our model on a large Twitter dataset.
We find that compared with bursty topics discovered
by standard LDA and by two degenerate variations
of our model, bursty topics discovered by our model
are more accurate and less redundant within the top-
ranked results. We also use some example bursty
topics to explain the advantages of our model.
2 Related Work
To find bursty patterns from data streams, Kleinberg
(2002) proposed a state machine to model the ar-
rival times of documents in a stream. Different states
generate time gaps according to exponential density
functions with different expected values, and bursty
intervals can be discovered from the underlying state
sequence. A similar approach by Ihler et al (2006)
models a sequence of count data using Poisson dis-
tributions. To apply these methods to find bursty
topics, the data stream used must represent a single
topic.
Fung et al (2005) proposed a method that iden-
tifies both topics and bursts from document stream-
s. The method first finds individual words that have
bursty patterns. It then finds groups of words that
tend to share bursty periods and co-occur in the same
documents to form topics. Weng and Lee (2011)
proposed a similar method that first characterizes the
temporal patterns of individual words using wavelet-
s and then groups words into topics. A major prob-
lem with these methods is that the word clustering
step can be expensive when the number of bursty
words is large. We find that the method by Fung
et al (2005) cannot be applied to our dataset be-
cause their word clustering algorithm does not scale
up. Weng and Lee (2011) applied word clustering
to only the top bursty words within a single day, and
subsequently their topics mostly consist of two or
three words. In contrast, our method is scalable and
each detected bursty topic is directly associated with
a word distribution and a set of tweets (see Table 3),
which makes it easier to interpret the topic.
Topic models provide a principled and elegan-
t way to discover hidden topics from large docu-
ment collections. Standard topic models do not con-
sider temporal information. A number of temporal
topic models have been proposed to consider topic
changes over time. Some of these models focus on
the change of topic composition, i.e. word distri-
butions, which is not relevant to bursty topic detec-
tion (Blei and Lafferty, 2006; Nallapati et al, 2007;
Wang et al, 2008). Some other work looks at the
temporal evolution of topics, but the focus is not on
bursty patterns (Wang and McCallum, 2006; Ahmed
and Xing, 2008; Masada et al, 2009; Ahmed and X-
ing, 2010; Hong et al, 2011).
The model proposed by Wang et al (2007) is the
most relevant to ours. But as we have pointed out
in Section 1, they do not need to handle the sep-
aration of ?personal? documents from event-driven
documents. As we will show later in our experi-
ments, for microblogs it is critical to model users?
537
personal interests in addition to global topical trend-
s.
To capture users? interests, Rosen-Zvi et al
(2004) expand topic distributions from document-
level to user-level in order to capture users? specif-
ic interests. But on microblogs, posts are short and
noisy, so Zhao et al (2011) further assume that each
post is assigned a single topic and some words can
be background words. However, these studies do not
aim to detect bursty patterns. Our work is novel in
that it combines users? interests and temporal infor-
mation to detect bursty topics.
3 Method
3.1 Preliminaries
We first introduce the notation used in this paper and
formally formulate our problem. We assume that
we have a stream of D microblog posts, denoted as
d1, d2, . . . , dD. Each post di is generated by a user
ui, where ui is an index between 1 and U , and U is
the total number of users. Each di is also associat-
ed with a discrete timestamp ti, where ti is an index
between 1 and T , and T is the total number of time
points we consider. Each di contains a bag of word-
s, denoted as {wi,1, wi,2, . . . , wi,Ni}, where wi,j is
an index between 1 and V , and V is the vocabulary
size. Ni is the number of words in di.
We define a bursty topic b as a word distri-
bution coupled with a bursty interval, denoted as
(?b, tbs, tbe), where ?b is a multinomial distribution
over the vocabulary, and tbs and tbe (1 ? tbs ? tbe ? T )
are the start and the end timestamps of the bursty in-
terval, respectively. Our task is to find meaningful
bursty topics from the input text stream.
Our method consists of a topic discovery step and
a burst detection step. At the topic discovery step,
we propose a topic model that considers both users?
topical interests and the global topic trends. Burst
detection is done through a standard state machine
method.
3.2 Our Topic Model
We assume that there are C (latent) topics in the text
stream, where each topic c has a word distribution
?c. Note that not every topic has a bursty interval.
On the other hand, a topic may have multiple bursty
intervals and hence leads to multiple bursty topics.
We also assume a background word distribution ?B
that captures common words. All posts are assumed
to be generated from some mixture of these C + 1
underlying topics.
In standard LDA, a document contains a mixture
of topics, represented by a topic distribution, and
each word has a hidden topic label. While this is a
reasonable assumption for long documents, for short
microblog posts, a single post is most likely to be
about a single topic. We therefore associate a single
hidden variable with each post to indicate its topic.
Similar idea of assigning a single topic to a short se-
quence of words has been used before (Gruber et al,
2007; Zhao et al, 2011). As we will see very soon,
this treatment also allows us to model topic distribu-
tions at time window level and user level.
As we have discussed in Section 1, an importan-
t observation we have is that when everything else
is equal, a pair of posts published around the same
time is more likely to be about the same topic than a
random pair of posts. To model this observation, we
assume that there is a global topic distribution ?t for
each time point t. Presumably ?t has a high prob-
ability for a topic that is popular in the microblog-
sphere at time t.
Unlike news articles from traditional media,
which are mostly about current affairs, an important
property of microblog posts is that many posts are
about users? personal encounters and interests rather
than global events. Since our focus is to find popular
global events, we need to separate out these ?person-
al? posts. To do this, an intuitive idea is to compare
a post with its publisher?s general topical interests
observed over a long time. If a post does not match
the user?s long term interests, it is more likely re-
lated to a global event. We therefore introduce a
time-independent topic distribution ?u for each us-
er to capture her long term topical interests.
We assume the following generation process for
all the posts in the stream. When user u publishes
a post at time point t, she first decides whether to
write about a global trendy topic or a personal top-
ic. If she chooses the former, she then selects a topic
according to ?t. Otherwise, she selects a topic ac-
cording to her own topic distribution ?u. With the
chosen topic, words in the post are generated from
the word distribution for that topic or from the back-
ground word distribution that captures white noise.
538
1. Draw ?B ? Dirichlet(?), ? ? Beta(?), ? ?
Beta(?)
2. For each time point t = 1, . . . , T
(a) draw ?t ? Dirichlet(?)
3. For each user u = 1, . . . , U
(a) draw ?u ? Dirichlet(?)
4. For each topic c = 1, . . . , C,
(a) draw ?c ? Dirichlet(?)
5. For each post i = 1, . . . , D,
(a) draw yi ? Bernoulli(?)
(b) draw zi ? Multinomial(?ui) if yi = 0 or
zi ? Multinomial(?ti) if yi = 1
(c) for each word j = 1, . . . , Ni
i. draw xi,j ? Bernoulli(?)
ii. draw wi,j ? Multinomial(?B) if
xi,j = 0 or wi,j ? Multinomial(?zi)
if xi,j = 1
Figure 2: The generation process for all posts.
We use ? to denote the probability of choosing to
talk about a global topic rather than a personal topic.
Formally, the generation process is summarized in
Figure 2. The model is also depicted in Figure 1(a).
There are two degenerate variations of our model
that we also consider in our experiments. The first
one is depicted in Figure 1(b). In this model, we only
consider the time-dependent topic distributions that
capture the global topical trends. This model can be
seen as a direct application of the model by Wang
et al (2007). The second one is depicted in Fig-
ure 1(c). In this model, we only consider the users?
personal interests but not the global topical trends,
and therefore temporal information is not used. We
refer to our complete model as TimeUserLDA, the
model in Figure 1(b) as TimeLDA and the model in
Figure 1(c) asUserLDA. We also consider a standard
LDA model in our experiments, where each word is
associated with a hidden topic.
Learning
We use collapsed Gibbs sampling to obtain sam-
ples of the hidden variable assignment and to esti-
mate the model parameters from these samples. Due
to space limit, we only show the derived Gibbs sam-
pling formulas as follows.
First, for the i-th post, we know its publisher ui
and timestamp ti. We can jointly sample yi and zi
based on the values of all other hidden variables. Let
us use y to denote the set of all hidden variables y
and y?i to denote all y except yi. We use similar
symbols for other variables. We then have
p(yi = p, zi = c|z?i,y?i,x,w) ?
Mpi(p) + ?
Mpi(?) + 2?
?
M l(c) + ?
M l(?) + C?
?
?V
v=1
?E(v)?1
k=0 (M c(v) + k + ?)
?E(?)?1
k=0 (M c(?) + k + V ?)
, (1)
where l = ui when p = 0 and l = ti when p =
1. Here every M is a counter. Mpi(0) is the number
of posts generated by personal interests, while Mpi(1)
is the number of posts coming from global topical
trends. Mpi(?) = M
pi
0 + Mpi1 . M
ui
(c) is the number of
posts by user ui and assigned to topic c, and Mui(?) is
the total number of posts by ui. M ti(c) is the number
of posts assigned to topic c at time point ti, and M ti(?)
is the total number of posts at ti. E(v) is the number
of times word v occurs in the i-th post and is labeled
as a topic word, while E(?) is the total number of
topic words in the i-th post. Here, topic words refer
to words whose latent variable x equals 1. M c(v) is
the number of times word v is assigned to topic c,
and M c(?) is the total number of words assigned to
topic c. All the counters M mentioned above are
calculated with the i-th post excluded.
We sample xi,j for each word wi,j in the i-th post
using
p(xi,j = q|y, z,x?{i,j},w)
?
M?(q) + ?
M?(?) + 2?
?
M l(wi,j) + ?
M l(?) + V ?
, (2)
where l = B when q = 0 and l = zi when q = 1.
M?(0) and M
?
(1) are counters to record the numbers
of words assigned to the background model and any
topic, respectively, andM?(?) = M
?
(0)+M
?
(1). M
B
(wi,j)
is the number of times word wi,j occurs as a back-
ground word. M zi(wi,j) counts the number of times
word wi,j is assigned to topic zi, and M zi(?) is the to-
tal number of words assigned to topic zi. Again, all
counters are calculated with the current word wi,j
excluded.
539
Figure 1: (a) Our topic model for burst detection. (b) A variation of our model where we only consider global topical
trends. (c) A variation of our model where we only consider users? personal topical interests.
3.3 Burst Detection
Just like standard LDA, our topic model itself finds a
set of topics represented by ?c but does not directly
generate bursty topics. To identify bursty topics, we
use the following mechanism, which is based on the
idea by Kleinberg (2002) and Ihler et al (2006). In
our experiments, when we compare different mod-
els, we also use the same burst detection mechanism
for other models.
We assume that after topic modeling, for each dis-
covered topic c, we can obtain a series of counts
(mc1,mc2, . . . ,mcT ) representing the intensity of the
topic at different time points. For LDA, these
are the numbers of words assigned to topic c.
For TimeUserLDA, these are the numbers of posts
which are in topic c and generated by the global top-
ic distribution ?ti , i.e whose hidden variable yi is 1.
For other models, these are the numbers of posts in
topic c.
We assume that these counts are generated by two
Poisson distributions corresponding to a bursty state
and a normal state, respectively. Let ?0 denote the
expected count for the normal state and ?1 for the
bursty state. Let vt denote the state for time point t,
where vt = 0 indicates the normal state and vt = 1
indicates the bursty state. The probability of observ-
ing a count of mct is as follows:
p(mct |vt = l) =
e??l?m
c
t
l
mct !
,
where l is either 0 or 1. The state sequence
(v0, v1, . . . , vT ) is a Markov chain with the follow-
ing transition probabilities:
p(vt = l|vt?1 = l) = ?l,
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.750 N/A
TimeLDA 0.800 0.700 0.600 0.633
UserLDA 0.800 0.700 0.850 0.833
TimeUserLDA 1.000 1.000 0.900 0.800
Table 1: Precision at K for the various models.
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.700 N/A
TimeLDA 0.400 0.500 0.500 0.567
UserLDA 0.800 0.500 0.500 0.600
TimeUserLDA 1.000 0.900 0.850 0.767
Table 2: Precision at K for the various models after we
remove redundant bursty topics.
where l is either 0 or 1.
?0 and ?1 are topic specific. In our experiments,
we set ?0 = 1T
?
t mct , that is, ?0 is the average
count over time. We set ?1 = 3?0. For transition
probabilities, we empirically set ?0 = 0.9 and ?1 =
0.6 for all topics.
We can use dynamic programming to uncover the
underlying state sequence for a series of counts. Fi-
nally, a burst is marked by a consecutive subse-
quence of bursty states.
4 Experiments
4.1 Data Set
We use a Twitter data set to evaluate our models.
The original data set contains 151,055 Twitter users
based in Singapore and their tweets. These Twitter
users were obtained by starting from a set of seed
Singapore users who are active online and tracing
540
Bursty Period Top Words Example Tweets Label
Nov 29 vote, big, awards, (1) why didnt 2ne1 win this time! Mnet Asian
bang, mama, win, (2) 2ne1. you deserved that urgh! Music Awards
2ne1, award, won (3) watching mama. whoohoo (MAMA)
Oct 5 ? Oct 8 steve, jobs, apple, (1) breaking: apple says steve jobs has passed away! Steve Jobs
iphone, rip, world, (2) google founders: steve jobs was an inspiration! death
changed, 4s, siri (3) apple 4 life thankyousteve
Nov 1 ? Nov 3 reservior, bedok, adlyn, (1) this adelyn totally disgust me. slap her mum? girl slapping
slap, found, body, queen of cine? joke please can. mom
mom, singapore, steven (2) she slapped her mum and boasted about it on fb
(3) adelyn lives in woodlands , later she slap me how?
Nov 5 reservior, bedok, adlyn, (1) bedok = bodies either drowned or killed. suicide near
slap, found, body, (2) another body found, in bedok reservoir? bedok reservoir
mom, singapore, steven (3) so many bodies found at bedok reservoir. alamak.
Oct 23 man, arsenal, united, (1) damn you man city! we will get you next time! football game
liverpool, chelsea, city, (2) wtf 90min goal!
goal, game, match (3) 6-1 to city. unbelievable.
Table 3: Top-5 bursty topics ranked by TimeUserLDA. The labels are manually given. The 3rd and the 4th bursty
topics come from the same topic but have different bursty periods.
Rank LDA UserLDA TimeLDA
1 Steve Jobs? death MAMA MAMA
2 MAMA football game MAMA
3 N/A #zamanprimaryschool MAMA
4 girl slapping mom N/A girl slapping mom
5 N/A iphone 4s N/A
Table 4: Top-5 bursty topics ranked by other models. N/A indicates a meaningless burst.
their follower/followee links by two hops. Because
this data set is huge, we randomly sampled 2892
users from this data set and extracted their tweets
between September 1 and November 30, 2011 (91
days in total). We use one day as our time window.
Therefore our timestamps range from 1 to 91. We
then removed stop words and words containing non-
standard characters. Tweets containing less than 3
words were also discarded. After preprocessing, we
obtained the final data set with 3,967,927 tweets and
24,280,638 tokens.
4.2 Ground Truth Generation
To compare our model with other alternative models,
we perform both quantitative and qualitative evalua-
tion. As we have explained in Section 3, each mod-
el gives us time series data for a number of topics,
and by applying a Poisson-based state machine, we
can obtain a set of bursty topics. For each method,
we rank the obtained bursty topics by the number
of tweets (or words in the case of the LDA model)
assigned to the topics and take the top-30 bursty top-
ics from each model. In the case of the LDA mod-
el, only 23 bursty topics were detected. We merged
these topics and asked two human judges to judge
their quality by assigning a score of either 0 or 1.
The judges are graduate students living in Singapore
and not involved in this project. The judges were
given the bursty period and 100 randomly selected
tweets for the given topic within that period for each
bursty topic. They can consult external resources to
help make judgment. A bursty topic was scored 1
if the 100 tweets coherently describe a bursty even-
t based on the human judge?s understanding. The
inter-annotator agreement score is 0.649 using Co-
hen?s kappa, showing substantial agreement. For
ground truth, we consider a bursty topic to be cor-
rect if both human judges have scored it 1. Since
some models gave redundant bursty topics, we al-
so asked one of the judges to identify unique bursty
541
topics from the ground truth bursty topics.
4.3 Evaluation
In this section, we show the quantitative evalua-
tion of the four models we consider, namely, LDA,
TimeLDA, UserLDA and TimeUserLDA. For each
model, we set the number of topics C to 80, ? to 50C
and ? to 0.01 after some preliminary experiments.
Each model was run for 500 iterations of Gibbs sam-
pling. We take 40 samples with a gap of 5 iterations
in the last 200 iterations to help us assign values to
all the hidden variables.
Table 1 shows the comparison between these
models in terms of the precision of the top-K result-
s. As we can see, our model outperforms all other
models for K <= 20. For K = 30, the UserLDA
model performs the best followed by our model.
As we have pointed out, some of the bursty topics
are redundant, i.e. they are about the same bursty
event. We therefore also calculated precision at K
for unique topics, where for redundant topics the one
ranked the highest is scored 1 and the other ones
are scored 0. The comparison of the performance
is shown in Table 2. As we can see, in this case,
our model outperforms other models with all K. We
will further discuss redundant bursty topics in the
next section.
4.4 Sample Results and Discussions
In this section, we show some sample results from
our experiments and discuss some case studies that
illustrate the advantages of our model.
First, we show the top-5 bursty topics discovered
by the TimeUserLDA model in Table 3. As we can
see, all these bursty topics are meaningful. Some of
these events are global major events such as Steve
Jobs? death, while some others are related to online
events such as the scandal of a girl boasting about
slapping her mother on Facebook. For comparison,
we also show the top-5 bursty topics discovered by
other models in Table 4. As we can see, some of
them are not meaningful events while some of them
are redundant.
Next, we show two case studies to demonstrate
the effectiveness of our model.
Effectiveness of Temporal Models: Both
TimeLDA and TimeUserLDA tend to group posts
published on the same day into the same topic. We
find that this can help separate bursty topics from
general ones. An example is the topic on the Circle
Line. The Circle Line is one of the subway lines of
Singapore?s mass transit system. There were a few
incidents of delays or breakdowns during the period
between September and November, 2011. We show
the time series data of the topic related to the Circle
Line of UserLDA, TimeLDA and TimeUserLDA in
Figure 3. As we can see, the UserLDA model de-
tects a much large volume of tweets related to this
topic. A close inspection tells us that the topic under
UserLDA is actually related to the subway systems
in Singapore in general, which include a few other
subway lines, and the Circle Line topic is merged
with this general topic. On the other hand, TimeL-
DA and TimeUserLDA are both able to separate the
Circle Line topic from the general subway topic be-
cause the Circle Line has several bursts. What is
shown in Figure 3 for TimeLDA and TimeUserLDA
is only the topic on the Circle Line, therefore the
volume is much smaller. We can see that TimeLDA
and TimeUserLDA show clearer bursty patterns than
UserLDA for this topic. The bursts around day 20,
day 44 and day 85 are all real events based on our
ground truth.
Effectiveness of User Models: We have stat-
ed that it is important to filter out users? ?person-
al? posts in order to find meaningful global events.
We find that our results also support this hypothesis.
Let us look at the example of the topic on the Mnet
Asian Music Awards, which is a major music award
show that is held by Mnet Media annually. In 2011,
this event took place in Singapore on November 29.
Because Korean pop music is very popular in Singa-
pore, many Twitter users often tweet about Korean
pop music bands and singers in general. All our top-
ic models give multiple topics related to Korean pop
music, and many of them have a burst on Novem-
ber 29, 2011. Under the TimeLDA and UserLDA
models, this leads to several redundant bursty top-
ics for the MAMA event ranked within the top-30.
For TimeUserLDA, however, although the MAMA
event is also ranked the top, there is no redundan-
t one within the top-30 results. We find that this is
because with TimeUserLDA, we can remove tweet-
s that are considered personal and therefore do not
contribute to bursty topic ranking. We show the top-
ic intensity of a topic about a Korean pop singer in
542
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
UserLDA
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
TimeLDA
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
TimeUserLDA
Figure 3: Topic intensity over time for the topic on the Circle Line.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
UserLDA
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
TimeLDA
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
TimeUserLDA
Figure 4: Topic intensity over time for the topic about a Korean pop singer. The dotted curves show the topic on Steve
Jobs? death.
Figure 4. For reference, we also show the intensity
of the topic on Steve Jobs? death under each mod-
el. We can see that because this topic is related to
Korean pop music, it has a burst on day 90 (Novem-
ber 29). But if we consider the relative intensity of
this burst compared with Steve Jobs? death, under
TimeLDA and UserLDA, this topic is still strong but
under TimeUserLDA its intensity can almost be ig-
nored. This is why with TimeLDA and UserLDA
this topic leads to a redundant burst within the top-
30 results but with TimeUserLDA the burst is not
ranked high.
5 Conclusions
In this paper, we studied the problem of finding
bursty topics from the text streams on microblogs.
Because existing work on burst detection from tex-
t streams may not be suitable for microblogs, we
proposed a new topic model that considers both the
temporal information of microblog posts and user-
s? personal interests. We then applied a Poisson-
based state machine to identify bursty periods from
the topics discovered by our model. We compared
our model with standard LDA as well as two de-
generate variations of our model on a real Twitter
dataset. Our quantitative evaluation showed that our
model could more accurately detect unique bursty
topics among the top ranked results. We also used
two case studies to illustrate the effectiveness of the
temporal factor and the user factor of our model.
Our method currently can only detect bursty top-
ics in a retrospective and offline manner. A more in-
teresting and useful task is to detect realtime bursts
in an online fashion. This is one of the directions we
plan to study in the future. Another limitation of the
current method is that the number of topics is pre-
determined. We also plan to look into methods that
allow appearance and disappearance of topics along
the timeline, such as the model by Ahmed and Xing
(2010).
Acknowledgments
This research is supported by the Singapore Nation-
al Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office. We
thank the reviewers for their valuable comments.
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
543
restaurant process: with applications to evolutionary
clustering. In Proceedings of the SIAM International
Conference on Data Mining, pages 219?230.
Amr Ahmed and Eric P. Xing. 2010. Timeline: A dy-
namic hierarchical Dirichlet process model for recov-
ering birth/death and evolution of topics in text stream.
In Proceedings of the 26th Conference on Uncertainty
in Artificial Intelligence, pages 20?29.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd International
Conference on Machine Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In Proceedings of the 31st
International Conference on Very Large Data Bases,
pages 181?192.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov model. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics.
Liangjie Hong, Byron Dom, Siva Gurumurthy, and
Kostas Tsioutsiouliklis. 2011. A time-dependent top-
ic model for multiple text streams. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 832?
840.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 207?216.
Jon Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proceedings of the 8th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 91?101.
Tomonari Masada, Daiji Fukagawa, Atsuhiro Takasu,
Tsuyoshi Hamada, Yuichiro Shibata, and Kiyoshi
Oguri. 2009. Dynamic hyperparameter optimization
for bayesian topical trend analysis. In Proceedings of
the 18th ACM Conference on Information and knowl-
edge management, pages 1831?1834.
Ramesh M. Nallapati, Susan Ditmore, John D. Lafferty,
and Kin Ung. 2007. Multiscale topic tomography. In
Proceedings of the 13th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 520?529.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, pages
487?494.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 424?433.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pattern-
s from coordinated text streams. In Proceedings of
the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 784?
793.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, pages 579?586.
Jianshu Weng and Francis Lee. 2011. Event detection in
Twitter. In Proceedings of the 5th International AAAI
Conference on Weblogs and Social Media.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Proceedings of the 33rd European confer-
ence on Advances in information retrieval, pages 338?
349.
544
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 131?135,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Link Type Based Pre-Cluster Pair Model for Coreference Resolution
Yang Song?, Houfeng Wang? and Jing Jiang?
?Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
?School of Information Systems, Singapore Management University, Singapore
{ysong, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg
Abstract
This paper presents our participation in the
CoNLL-2011 shared task, Modeling Unre-
stricted Coreference in OntoNotes. Corefer-
ence resolution, as a difficult and challenging
problem in NLP, has attracted a lot of atten-
tion in the research community for a long time.
Its objective is to determine whether two men-
tions in a piece of text refer to the same en-
tity. In our system, we implement mention de-
tection and coreference resolution seperately.
For mention detection, a simple classification
based method combined with several effective
features is developed. For coreference resolu-
tion, we propose a link type based pre-cluster
pair model. In this model, pre-clustering of all
the mentions in a single document is first per-
formed. Then for different link types, different
classification models are trained to determine
wheter two pre-clusters refer to the same en-
tity. The final clustering results are generated
by closest-first clustering method. Official test
results for closed track reveal that our method
gives a MUC F-score of 59.95%, a B-cubed
F-score of 63.23%, and a CEAF F-score of
35.96% on development dataset. When using
gold standard mention boundaries, we achieve
MUC F-score of 55.48%, B-cubed F-score of
61.29%, and CEAF F-score of 32.53%.
1 Introduction
The task of coreference resolution is to recognize
all the mentions (also known as noun phrases, in-
cluding names, nominal mentions and pronouns)
in a text and cluster them into equivalence classes
where each quivalence class refers to a real-world
entity or abstract concept. The CoNLL-2011 shared
task1 uses OntoNotes2 as the evaluation corpus. The
coreference layer in OntoNotes constitutes one part
of a multi-layer, integrated annotation of the shal-
low semantic structures in the text with high inter-
annotator agreement. In addition to coreference,
this data set is also tagged with syntactic trees, high
coverage verb and some noun propositions, partial
verb and noun word senses, and 18 named entity
types. The main difference between OntoNotes and
another wellknown coreference dataset ACE is that
the former does not label any singleton entity clus-
ter, which has only one reference in the text. We can
delete all the singleton clusters as a postprocessing
step for the final results. Alternatively, we can also
first train a classifier to separate singleton mentions
from the rest and apply this mention detection step
before coreference resolution. In this work we adopt
the second strategy.
In our paper, we use a traditional learning based
pair-wise model for this task. For mention detec-
tion, we first extract all the noun phrases in the text
and then use a classification model combined with
some effective features to determine whether each
noun phrase is actually a mention. The features in-
clude word features, POS features in the given noun
phrase and its context, string matching feature in
its context, SRL features, and named entity features
among others. More details will be given in Sec-
tion 3. From our in-house experiments, the final F-
scores for coreference resolution can be improved
by this mention detection part. For coreference res-
1http://conll.bbn.com
2http://www.bbn.com/ontonotes/
131
Features describing ci or cj
Words The first and last words of the given NP in ci ( or cj) , also including the words in the
context with a window size 2
POS Tags The part of speech tags corresponding to the words
Pronoun Y if mentions in ci( or cj) are pronouns; else N
Definite Y if mentions in ci( or cj) are definite NP; else N
Demonstrative Y if mentions in ci( or cj) are demonstrative NP; else N
Number Singular or Plural, determined using a data file published by Bergsma and Lin (2006)
Gender Male, Female, Neuter, or Unknown, determined using a data file published by Bergsma
and Lin (2006)
Semantic Class Semantic Classes are given by OntoNotes for named entities
Mentino Type Common Noun Phrases or Pronouns
Table 1: The feature set describing ci or cj .
olution, a traditinal pair-wise model is applied, in
which we first use exact string matching to generate
some pre-clusters. It should be noted that each pro-
noun must be treated as a singleton pre-cluster, be-
cause they are not like names or nominal mentions,
which can be resolved effectively with exact string
matching. We then implement a classification based
pre-cluster pair model combined with several ef-
fective coreference resolution features to determine
whether two pre-clusters refer to the same entity. Fi-
nally, we use closest-first clustering method to link
all the coreferential pre-clusters and generate the fi-
nal cluster results. As mentioned before, mentions
have three types: names, nominal mentions and pro-
nouns. Among them pronouns are very different
from names and nominal mentions, because they can
only supply limited information literally. So we de-
fine three kinds of link types for pre-cluster pairs:
NP-NP link, NP-PRP link and PRP-PRP link. (Here
NP means Noun Phrases and PRP means Pronom-
inal Phrases.) One link represents one pre-cluster
pair. Intuitively, different link types tend to use dif-
ferent features to determine whether this kind of link
is coreferential or not. We implement three kinds
of pre-cluster pair model based on three link types.
Experimental results show that combined with out-
puts from different link type based pre-cluster pair
model can give better results than using an uni-
fied classification model for three different kinds of
link types. For all the classification models, we use
opennlp.maxent3 package.
The rest of this paper is organized as follows. Sec-
tion 2 describes our mention detection method. We
discuss our link type based pre-cluster pair model
for coreference resolution in Section 3, evaluate it in
Section 4, and conclude in Section 5.
2 Mention Detection
We select all the noun phrases tagged by the
OntoNotes corpus as mention candidates and im-
plement a classification-based model combined
with several commonly used features to determine
whether a given noun phrase is a mention. The fea-
tures are given below:
? Word Features - They include the first word and the
last word in each given noun phrase. We also use
words in the context of the noun phrase within a
window size of 2.
? POS Features - We use the part of speech tags of
each word in the word features.
? Position Features - These features indicate where
the given noun phrase appears in its sentence: be-
gining, middle, or end.
? SRL Features - The Semantic Role of the given
noun phrase in its sentence.
? Verb Features - The verb related to the Semantic
Role of the given noun phrase.
3http://incubator.apache.org/opennlp/
132
Features describing the relationship between ci and cj
Distance The minimum distance between mentions in ci and cj
String Match Y if mentions are the same string; else N
Substring Match Y if one mention is a substring of another; else N
Levenshtein Distance Levenshtein Distance between the mentions
Number Agreement Y if the mentions agree in number; else N
Gender Agreement Y if the mentions agree in gender; else N
N & G Agreement Y if mentions agree in both number and gender; else N
Both Pronouns Y if the mentions are both pronouns; else N
Verb Agreement Y if the mentions have the same verb.
SRL Agreement Y if the mentions have the same semantic role
Position Agreement Y if the mentions have the same position (Beginning, Middle or End) in sentences
Table 2: The feature set describing the relationship between ci and cj .
? Entity Type Features - The named entity type for the
given noun phrase.
? String Matching Features - True if there is another
noun phrase wich has the same string as the given
noun phrase in the context.
? Definite NP Features - True if the given noun phrase
is a definite noun phrase.
? Demonstrative NP Features - True if the given noun
phrase is a demonstrative noun phrase.
? Pronoun Features - True if the given noun phrase is
a pronoun.
Intutively, common noun phrases and pronouns
might have different feature preferences. So we train
classification models for them respectively and use
the respective model to predicate for common noun
phrases or pronouns. Our mention detection model
can give 52.9% recall, 80.77% precision and 63.93%
F-score without gold standard mention boundaries
on the development dataset. When gold standard
mention boundaries are used, the results are 53.41%
recall, 80.8% precision and 64.31% F-score. (By us-
ing the gold standard mention boundaries, we mean
we use the gold standard noun phrase boundaries.)
3 Coreference Resolution
After getting the predicated mentions, we use some
heuristic rules to cluster them with the purpose of
generating highly precise pre-clusters. For this task
Metric Recall Precision F-score
MUC 49.64% 67.18% 57.09%
BCUBED 59.42% 70.99% 64.69%
CEAF 45.68% 30.56% 36.63%
AVERAGE 51.58% 56.24% 52.80%
Table 3: Evaluation results on development dataset with-
out gold mention boundaries
Metric Recall Precision F-score
MUC 48.94% 67.72% 56.82%
BCUBED 58.52% 72.61% 64.81%
CEAF 46.49% 30.45% 36.8%
AVERAGE 51.32% 56.93% 52.81%
Table 4: Evaluation results on development dataset with
gold mention boundaries
only identity coreference is considered while attribu-
tive NP and appositive construction are excluded.
That means we cannot use these two important
heuristic rules to generate pre-clusters. In our sys-
tem, we just put all the mentions (names and nomi-
nal mentions, except pronouns) which have the same
string into the identical pre-clusters. With these pre-
clusters and their coreferential results, we imple-
ment a classification based pre-cluster pair model to
determine whether a given pair of pre-clusters re-
fer to the same entity. We follow Rahman and Ng
(2009) to generate most of our features. We also
include some other features which intuitively seem
effective for coreference resolution. These features
133
Metric Recall Precision F-score
MUC 42.66% 53.7% 47.54%
BCUBED 61.05% 74.32% 67.04%
CEAF 40.54% 32.35% 35.99%
AVERAGE 48.08% 53.46% 50.19%
Table 5: Evaluation results on development dataset
with gold mention boundaries using unified classification
model
Metric Recall Precision F-score
MUC 53.73% 67.79% 59.95%
BCUBED 60.65% 66.05% 63.23%
CEAF 43.37% 30.71% 35.96%
AVERAGE 52.58% 54.85% 53.05%
Table 6: Evaluation results on test dataset without gold
mention boundaries
are shown in Table 1 and Table 2. For simplicity, we
use ci and cj to represent pre-clusters i and j. Each
pre-cluster pair can be seen as a link. We have three
kinds of link types: NP-NP link, NP-PRP link and
PRP-PRP link. Different link types may have differ-
ent feature preferences. So we train the classifica-
tion based pre-cluster pair model for each link type
separately and use different models to predicate the
results. With the predicating results for pre-cluster
pairs, we use closest-first clustering to link them and
form the final cluster results.
4 Experimental Results
We present our evaluation results on development
dataset for CoNLL-2011 shared Task in Table 3, Ta-
ble 4 and Table 5. Official test results are given
in Table 6 and Table 7. Three different evaluation
metrics were used: MUC (Vilain et al, 1995), B3
(Bagga and Baldwin, 1998) and CEAF (Luo, 2005).
Finally, the average scores of these three metrics are
used to rank the participating systems. The differ-
ence between Table 3 and Table 4 is whether gold
standard mention boundaries are given. Here ?men-
tion boundaries? means a more broad concept than
the mention definition we gave earlier. We should
also detect real mentions from them. From the ta-
bles, we can see that the scores can be improved litt-
tle by using gold standard mention boundaries. Also
the results from Table 5 tell us that combining differ-
ent link-type based classification models performed
Metric Recall Precision F-score
MUC 46.66% 68.40% 55.48%
BCUBED 54.40% 70.19% 61.29%
CEAF 43.77% 25.88% 32.53%
AVERAGE 48.28% 54.82% 49.77%
Table 7: Evaluation results on test dataset with gold men-
tion boundaries
better than using an unified classification model. For
official test results, our system did not perform as
well as we had expected. Some possible reasons are
as follows. First, verbs that are coreferential with a
noun phrase are also tagged in OntoNotes. For ex-
ample, ?grew ? and ?the strong growth? should be
linked in the following case: ?Sales of passenger
cars grew 22%. The strong growth followed year-
to-year increases.? But we cannot solve this kind
of problem in our system. Second, we should per-
form feature selection to avoid some useless features
harming the scores. Meanwhile, we did not make
full use of the WordNet, PropBank and other back-
ground knowledge sources as features to represent
pre-cluster pairs.
5 Conclusion
In this paper, we present our system for CoNLL-
2011 shared Task, Modeling Unrestricted Corefer-
ence in OntoNotes. First some heuristic rules are
performed to pre-cluster all the mentions. And then
we use a classification based pre-cluster pair model
combined with several cluster level features. We
hypothesize that the main reason why we did not
achieve good results is that we did not carefully ex-
amine the features and dropped the feature selec-
tion procedure. Specially, we did not make full use
of background knowledge like WordNet, PropBank,
etc. In our future work, we will make up for the
weakness and design a more reasonable model to ef-
fectively combine all kinds of features.
Acknowledgments
This research is supported by National Natu-
ral Science Foundation of Chinese (No.60973053,
No.91024009) and Research Fund for the Doc-
toral Program of Higher Education of China
(No.20090001110047).
134
References
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A Model-Theoretic Coreference
Scoring Scheme. In Proceedings of the Sixth Message
Understanding Conference (MUC-6), pages 4552, San
Francisco, CA. Morgan Kaufmann.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the 1st
International Conference on Language Resources and
Evaluation, Granada, Spain, pp. 563566.
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Vancouver, B.C., Canada, pp. 2532.
Vincent Ng. 2008. Unsupervised Models for Corefer-
ence Resolution. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pp. 640?649.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing.
Vincent Ng. 2010. Supervised Noun Phrase Coreference
Research: The First Fifteen Years. In Proceedings of
the 48th Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, pages 1396-1411.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
Path-Based Pronoun Resolution. In COLING?ACL
2006, pages 33?40.
135
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 33?41,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
A Unified Topic-Style Model for Online Discussions
Ying Ding, Jing Jiang, Qiming Diao
School of Information Systems
Singapore Management University
{ying.ding.2011, jingjiang, qiming.diao.2010}@smu.edu.sg
Abstract
Forums have become major places for
online communications for many years,
where people often share and express
opinions. We observe that, when editing
posts, while some people seriously state
their opinions, there are also many peo-
ple playing jokes and writing meaningless
posts on the discussed topics. We design
a unified probabilistic graphical model to
capture both topic-driven words and style-
driven words. The model can help us sepa-
rate serious and unserious posts/users and
identify slang words. An extensive set
of experiments demonstrates the effective-
ness of our model.
1 Introduction
With the fast growth of the popularity of online
social media, people nowadays are very used to
sharing their thoughts and interacting with their
friends on the Internet. Large online social net-
work sites such as Facebook, Twitter and Flickr
have attracted hundreds of millions of users. A-
mong these online social media platforms, forums
have always played an important role with its spe-
cial characteristics. Unlike personal blogs, forum-
s allow many users to engage in online conversa-
tions with a topic focus. Unlike Facebook, forums
are usually open to public and users who post in
forums do not need to reveal too much personal
information. Unlike Wikipedia or Freebase, fo-
rums encourage users to exchange not only factual
information but more importantly subjective opin-
ions. All these characteristics make online forums
a valuable source from which we can retrieve and
summarize the general public?s opinions about a
given topic. This is especially important for busi-
nesses who want to find out how their products
and services have been received and policy mak-
ers who are concerned about people?s opinions on
social issues.
While the freedom with which users can post in
online forums has promoted the popularity of on-
line forums, it has also led to the diversity in post
quality. There are posts which contribute positive-
ly to a discussion by offering relevant, serious and
meaningful opinions, but there are also many posts
which appear irrelevant, disrespectful or meaning-
less. These posts are uninformative, hard to con-
sume and sometimes even destructive. Let us look
at some examples. Table 1 shows two forum posts
in response to a piece of news about GDP bonuses
for senior civil servants in Singapore. We can see
that User A?s post is clearly written. User B?s post,
on the other hand, is hard to comprehend. We see
broken sentences, many punctuation marks such
as ??? and colloquial expressions such as ?ha.?
User B is not seriously contributing to the online
discussion but rather trying to make a joke of the
issue. Generally speaking, User B?s post is less
useful than User A?s post in helping us understand
the public?s response to the news.
Senior civil servants to get bumper
GDP bonuses
User A let us ensure this will be the LAST time
they accord themselves ceiling salary s-
cales and bonuses. i suspect MANY cit-
izens are eagerly looking forward to the
GE.
User B Fever night, fever night, fe..ver..
Fever like to do it
Got it?????? Ha..ha..ha...
Table 1: Two example online posts.
In this work, we opt for a fully unsupervised
approach to modeling this phenomenon in online
discussions. Our solution is based on the observa-
tion that the writing styles of serious posts and un-
serious posts are different, and the writing styles
are often characterized by the words used in the
posts. Moreover, the same user usually exhibits
33
User Post
User A
Re: Creativity, Art in the eyes of beholder. your
take?
The difference is, the human can get tired or
sick, and then it will affect his work, but the
robot can work 24 hours a day 365 days a year
and yet produce the same every time.
Re: Diesel oil spill turns Manila Bay red, poses
risk to health - ST
The question is, will this environmental haz-
ard turn up on the shores of it neighbors? And
maybe even affect Singapore waters?
User B
Re: Will PAP know who i vote in GE?
Hey! Who are you???
You make. ha..ha..ha.. he..he..he..
very angry lah
Re: Gender discrimination must end for Singa-
pore to flourish, says AWARE
Hao nan bu gen nu dou Let you win lah
ha..ha..ha..
Table 2: Sample posts of two example users.
the same writing style in most of his posts. For
example, Table 2 shows two example users, each
with two sample posts. We can see that their writ-
ing styles are consistent in the two posts. If we
treat each writing style as a latent factor associat-
ed with a word distribution, we can associate ob-
served words with the underlying writing styles.
However, not all words in a post are style-driven.
Many words in forum posts are chosen based on
the topic of the corresponding thread. Our model
therefore jointly considers both topics and writing
styles.
We apply our topic-style model to a real on-
line forum dataset from Singapore. By setting the
number of styles to two, we clearly find that one
writing style corresponds to the more serious posts
while the other corresponds to posts that are not so
serious. This topic-style model also automatically
learns a meaningful slang lexicon. Moreover, we
find that topics discovered by our topic-style mod-
el are more distinctive from each other than topics
produced by standard LDA.
Our contributions in this paper can be summa-
rized as follows: 1) We propose a principled topic-
style model to jointly model topics and writing
styles at the same time in online forums. 2) An
extensive set of experiments shows that our mod-
el is effective in separating the more serious posts
and unserious posts and identifying slang words.
2 Related Work
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) has been shown to be useful for many ap-
plications. Many extensions of LDA have been
designed for different tasks, which are not detailed
here. Our model is also an extension of LDA. We
introduce two types of word distributions, one rep-
resenting topics and the other representing writing
styles. We use switch variables to alternate be-
tween these two types of word distributions. We
also assume an author-level distribution over writ-
ing styles. It is worth pointing out that although
our model bears similarity to a number of oth-
er LDA extensions, our objectives are different
from existing work. E.g., the author topic mod-
el (Rosen-Zvi et al., 2004) also assumes an author-
level distribution over topics, but the author-level
distribution is meant to capture an author?s topical
interests. In contrast, our user-level distribution is
over writing styles and is meant to identify serious
versus unserious users. Similar to the models by
Mei et al. (2007) and Paul et al. (2010) , we also
use switch variables to alternate between different
types of word distributions, but our goal is to iden-
tify words associated with writing styles instead of
sentiment words or perspective words.
Another body of related research is around s-
tudying text quality, formality and sarcasm. Pitler
and Nenkova (2008) investigated different fea-
tures for text readability judgement and empirical-
ly demonstrated that discourse relations are high-
ly correlated with perceived readability. Brooke
et al. (2010) applied Latent Semantic Analysis
to determine the formality level of lexical items.
Agichtein et al. (2008) presented a general classifi-
cation framework incorporating community feed-
back to identify high quality content in social me-
dia. Davidov et al. (2010) proposed the first robust
algorithm for recognition of sarcasm. Gonz?alez-
Ib?a?nez et al. (2011) took a closer look at sarcasm
in Twitter messages and found that automatic clas-
sification can be as good as human classification.
All these studies mainly rely on supervised tech-
niques and human annotation needs to be done,
which is very time consuming. Our method is ful-
ly unsupervised, which can automatically uncover
different styles and separate serious posts from un-
serious posts.
Our work is also related to spam/spammer de-
tection in social media, which has been studied
over different platforms for a few years. Jindal
and Liu (2008) first studied opinion spam in on-
line reviews and proposed a classification method
for opinion spam detection. Bhattarai et al. (2009)
34
investigated different content attributes of com-
ment spam in the Blogsphere and built a detection
system with good performance based on these at-
tributes. Ding et al. (2013) proposed to utilize both
content and social features to detect spams in on-
line question answer website. Existing work on
spam detection need annotated data to learn the s-
pam features but our model does not as it is fully
unsupervised.
3 A Topic-Style Model
Writing styles can be reflected in many differen-
t ways. Besides choices of words or expression-
s, many other linguistic features such as sentence
length, sentence complexity and use of punctua-
tion marks may all be associated with one?s writ-
ing style. In this work, however, we try to take
an approach that does not rely on heavy linguistic
analysis or feature engineering. Part of the reason
is that we want our approach to be independent of
language, culture or social norms so that it is ro-
bust and can be easily applied to any online forum.
To this end, we represent a writing style simply
as a distribution over words, much like a topic in
LDA. We assume that there are S latent writing
styles shared by all users contributing to a forum.
Meanwhile, we also assume a different set of T
latent topics. We mix writing styles and topics to
explain the generation of words in forum posts.
A key assumption we have is that the same us-
er tends to maintain a consistent writing style, and
therefore we associate each user with a multinomi-
al distribution over our latent writing styles. This
is similar to associating a document with a distri-
bution over topics in LDA, where the assumption
is that a single document tends to have focused
topics. Another assumption of our model is that
each word in a post is generated from either the
background or a topic or a writing style, as deter-
mined by a binary switch variable.
3.1 Model Description
We now formally describe the topic-style model
we propose. The model is depicted in Figure 1.
We assume that there are T latent topics, where
?
t
is the word distribution for topic t. There are
S latent writing styles, where ?
s
is the word dis-
tribution for writing style s. There are E threads,
where each thread e has a topic distribution ?
e
, and
there are U users, where each user u has a writing
style distribution pi
u
.
Figure 1: Topic-Style Model
Notation Description
?, ?
E
, ?
U
,
?
B
, ?
T
, ?
S
Hyper-parameters of Dirichlet distributions
? A global multinomial distribution over
switching variables x
?
e
, pi
u
Thread-specific topic distributions and user-
specific style distributions
?
B
, ?
t
, ?
s
Word distributions of background, topics
and styles
x
e,p,n
,
y
e,p,n
,
z
e,p,n
Hidden variables: x
e,p,n
for switching,
y
e,p,n
for style of style words, z
e,p,n
for
topic of topic words
e, p, n Indices: e for threads, p for posts, n for
words
E,P
e
, U,
N
e,p
Number of threads, numbers of posts in
threads, number of users and numbers of
words in posts
S,K, V Numbers of styles, topics and word types
Table 3: Notation used in our model.
For each word in a post, first a binary switch
variable x is sampled from a global Bernoulli dis-
tribution parameterized by ?. If x = 0, we draw a
word from the background word distribution. Oth-
erwise, if x = 1, we draw a topic from the corre-
sponding thread?s topic distribution; if x = 2, we
draw a writing style from the corresponding user?s
writing style distribution. We then draw the word
from the corresponding word distribution.
The generative process of our model is de-
scribed as follows. The notation we use in the
model is also summarized in Table 3.
? Draw a global multinomial switching variable distribu-
tion ? ? Dirichlet(?).
? Draw a multinomial background word distribution
?
B
? Dirichlet(?
B
).
? For each topic t = 1, 2, . . . , T , draw a multinomial
topic-word distribution ?
t
? Dirichlet(?
T
).
? For each writing style s = 1, 2, . . . , S, draw a multi-
nomial style-word distribution ?
s
? Dirichlet(?
S
).
? For each user u = 1, 2, . . . , U , draw a multinomial
style distribution pi
u
? Dirichlet(?
u
).
? For each thread e = 1, 2, . . . , E
35
? draw a multinomial topic distribution ?
e
?
Dir(?
E
).
? for each post p = 1, 2, . . . , P
e
in the thread,
where u
e,p
? {1, 2, . . . , U} is the user who has
written the post
? for each word n = 1, 2, . . . , N
e,p
in the
thread, where w
e,p,n
? {1, 2, . . . , V } is the
word type
? draw x
e,p,n
? Multinomial(?).
? If x = 0, draw w
e,p,n
?
Multinomial(?
B
)
? If x = 1, draw y
e,p,n
?
Multinomial(pi
u
e,p
), and then draw
w
e,p,n
? Multinomial(?
y
e,p,n
).
? If x = 2, draw z
e,p,n
?
Multinomial(?
e
), and then draw
w
e,p,n
? Multinomial(?
z
e,p,n
).
3.2 Parameters Estimation
We use Gibbs sampling to estimate the parameters.
The sampling probability that assign the nth word
in post p of thread e to the background topic is as
follows:
P (x
e,p,n
= 0|W ,U ,X?i,Y ?i,Z?i)
?(? + n
0
)?
?
B
+ n
w
e,p,n
B
V ?
B
+ n
0
where n
0
is the number of words assigned as back-
ground words and n
w
e,p,n
B
is the number of times
word type of w
e,p,n
assigned to background. The
probability to assign this word to style s is as fol-
lows:
P (x
e,p,n
= 1, y
e,p,n
= s|W ,U ,X?i,Y ?i,Z?i)
?(? + n
1
)?
?
U
+ n
s
u
e,p
S?
U
+ n
?
u
e,p
?
?
S
+ n
w
e,p,n
s
V ?
S
+ n
?
s
where n
1
is the number of words assigned as style
words, n
?
u
e,p
and n
s
u
e,p
are the number of words
written by user u
e,p
and assigned as style words,
and the number of these words assigned to style
s, respectively. n
?
s
and n
w
e,p,n
s
are the number of
words assigned to style s and the number of times
word type of term w
e,p,n
assigned to style s. The
probability to assign this word topic t is as follows:
P (x
e,p,n
= 2, z
e,p,n
= t|W ,U ,X?i,Y ?i,Z?i)
?(? + n
2
)?
?
E
+ n
t
e
K?
E
+ n
?
e
?
?
T
+ n
w
e,p,n
t
V ?
T
+ n
?
t
where n
2
is the number of words assigned as topic
words, n
?
e
is the number of words in thread e as-
signed as topic words, n
t
e
is the number of words
in thread e assigned to topic t, n
?
t
is the number of
words assigned to topic t, and n
w
e,p,n
t
is the num-
ber of times word type of w
e,p,n
is assigned to top-
ic t.
After running Gibbs sampling for a number of
iterations, we can estimate the parameters based
on the sampled topic assignments. They can be
calculated by the equations below:
?
w
t
=
?
T
+ n
w
t
V ?
T
+ n
?
t
?
w
s
=
?
S
+ n
w
s
V ?
S
+ n
?
s
?
t
e
=
?
E
+ n
t
e
K?
E
+ n
?
e
?
s
u
=
?
U
+ n
s
u
S?
U
+ n
?
u
4 Experiment
4.1 Data Set and Experiment Setup
To evaluate our model, we use forum threads from
AsiaOne
1
, a popular online forum site in Singa-
pore. We crawled all the threads between January
2011 and June 2013 under a category called ?Sin-
gapore,? which is the largest category on AsiaOne.
In the preprocessing stage, we removed the URL-
s, HTML tags and tokenized the text. Emoticons
are kept in our data set as they frequently occur
and indicate users? emotions. All stop words and
words occurring less than 4 times are deleted. We
also removed users who have fewer than 8 post-
s and threads attracting fewer than 21 posts. The
detailed statistics of the processed dataset are giv-
en in Table 4.
#Users #Words #Tokens #Posts/User #Posts/Thread
580 29,619 2,940,886 205.3 69.5
Table 4: Detailed statistics of the dataset.
We fix the hyper-parameters ?, ?
E
, ?
U
, ?
T
and
?
S
to be 10, 1, 1, 0.01 and 0.01 respectively. we
set ?
B,v
to be H ? p
B
(v), where H is set to be 20
and p
B
(v) is the probability of word v as estimated
from the entire corpus. The number of topics K is
set to be 40 empirically.
4.2 Model Development
Before we evaluate the effectiveness of our model,
we first show how we choose the number of styles
to use. Note that although we are interested in sep-
arating serious and unserious posts, our model can
generally handle any arbitrary number of writing
styles. We therefore vary the number of writing
styles to see which number empirically gives the
most meaningful results.
Assuming that different styles are characterized
by words, we expect to see that the discovered
1
http://www.asiaone.com
36
2 3 4 5 6 7 8 912
13
14
15
16
17
18
19
Number of Styles
Aver
age 
Dive
rgen
ce
Figure 2: Average Divergence over different num-
bers of styles.
Style No. Top Words
2 Style 1 singapore, people, years, government
styles Style 2 BIGGRIN, TONGUE, lah, ha
3 Style 1 people, make, WINK, good
styles Style 2 singapore, years, government, mr
Style 3 BIGGRIN, TONGUE, lah, ha
4 Style 1 ha, lah, WINK, dont
styles Style 2 singapore, year, mr, years
Style 3 people, good, make, singapore
Style 4 BIGGRIN, TONGUE, EEK, MAD
Table 5: Sample style words
word distributions for different styles are very d-
ifferent from each other. To measure the distinc-
tion among a set of styles, we define a metric
called Average Divergence (AD) based on KL-
divergence. Average Divergence can be calculated
as follows.
AD(S) =
2
N(N ? 1)
?
i6=j
S
KL
(s
i
||s
j
),
where S is a set of style-word distributions, N is
the size of S and s
i
is the i-th distribution in S.
S
KL
(s
i
||s
j
) is the symmetric KL divergence be-
tween s
i
and s
j
(i.e., D
KL
(s
i
||s
j
) +D
KL
(s
j
||s
i
)).
The higher Average Divergence is, the more dis-
tinctive distributions in S are.
Figure 2 shows the Average Divergence over d-
ifferent numbers of styles. We can clearly see that
the Average Divergence reaches the highest value
when there are only two styles and decreases with
the increase of style number. This means the styles
are mostly distinct from each other when the num-
ber is 2 and their difference decreases when there
are more styles.
To get a better understanding of the differences
of using different numbers of styles, we compare
the top words in each style when the number of
styles is set to be 2, 3 and 4. The results are shown
in Table 5 where all uppercase words represent e-
moticons. From the top words of the first row, we
Serious Unserious
Style Style
singapore lah
people ha
years dont
government stupid
time leh
made ah
year lor
public liao
Table 6: Top words of different styles
can see that Style 1 is dominated by formal words
while Style 2 is dominated by emoticons like BIG-
GRIN and slang words like ?lah? and ?ha.? These
two styles are well distinguished from each other
and humans can easily tell the difference between
them. Also, Style 2 is an unserious style character-
ized by emoticons, slang and urban words. Table 6
shows the top words of these 2 styles excluding e-
moticons. From this table, we can observe that
Style 2 has many slang words with high probabil-
ity while top words in Style 1 are all very formal.
However, styles in the second and third rows of
Table 5 are not easily distinguishable from each
other. In these results, there often exist two styles
very similar to the styles in row 1 while the other
styles look like the combination of these two styles
and humans cannot tell their meanings very clear-
ly. Based on these observations, we fix the number
of styles to 2 in the following experiments.
0 2 4 6 8 10 12 14 16 180
0.05
0.1
0.15
0.2
0.25
Word Length
Prob
abilit
y
 
 Style 1Style 2
Figure 3: Word length distribution
One previous work uses word length as an in-
dicator of formality (Karlgren and Cutting, 1994).
Here, we borrow this idea and compare the word
length of Style 1 and Style 2. We calculate the
distributions of word length and show the results
in Figure 3. It shows that the majority of word-
s in Style 1 are longer compared with those in
Style 2. To have a quantitative view of the differ-
ence between the word lengths of these two styles,
we heuristically extract words labeled with Style 1
37
and Style 2 in our dataset in the final iteration of
Gibbs sampling and apply Mann-Whitney U test
on these two word length populations. The null
hypothesis that the two input populations are the
same is rejected at the 1% significance level. This
verifies the intuition that serious posts tend to use
longer words than unserious posts.
4.3 Post Identification
Our model can also be used to separate serious
posts and unserious posts. We treat this as a re-
trieval problem and use precision/recall for evalu-
ation.
We use a simple scoring function, which is the
proportion of words assigned to the unserious style
when we terminate the Gibbs sampling at the 800-
th iteration, to score each post. When applying this
method to our data, emoticons are all removed.
For comparison, we rank post according to the
number of emoticons inside a post as the baseline.
After getting the result of each method, we ask t-
wo annotators to label the first and last 50 posts
in the ranking list. The first 50 posts are used for
evaluation of unserious post retrieval and the last
50 post are used for evaluation of serious post re-
trieval. This evaluation is based on the assumption
that if a method can separate serious and unserious
posts very well, posts ranked at the top position
should be unserious ones and those ranked near
to the bottom should be serious ones. The results
are shown in Table 7 where our method is denot-
ed as TSM and the baseline method is denoted as
EMO. In serious post retrieval, the baseline have
a perfect performance and our method is compet-
itive. We can see that EMO has a perfect perfor-
mance in identifying serious posts. When posts
are ranked in reverse order according to the num-
ber of emoticons they contain, the last 50 ones do
not contain any emoticons. They can be regarded
as a random sample of posts without emoticons.
Compared with identifying serious posts, identi-
fying unserious posts looks much more difficult.
EMO?s poor performance on this task tells us that
emoticon is not a promising sign to detect unse-
rious posts. However, the word style a post uses
matters more, which also proves the value of our
proposed model.
4.4 User Identification
In this section, we evaluate the performance of
TSM on identifying serious and unserious users.
This identification task is very important as many
P@5 P@15 P@25 P@35
Serious
EMO 1.0 1.0 1.0 1.0
TSM 1.0 1.0 1.0 0.97
Unserious
EMO 0.4 0.67 0.64 0.6
TSM 1.0 0.93 0.96 0.97
Table 7: Precision for Serious and Unserious Post
Retrieval. P@N stands for the precision of the first
N results in ranking list.
P@5 P@15 P@25 P@35
Serious
Baseline 0.6 0.8 0.8 0.83
TSM 1.0 1.0 1.0 0.94
Unserious
Baseline 1.0 0.87 0.92 0.91
TSM 1.0 1.0 1.0 1.0
Table 8: Precision for serious and unserious user
retrieval.
research tasks such as opinion mining and expert
finding are more interested in the serious users.
We treat this task as a retrieval problem as well,
which means we will rank users by a scoring func-
tion and do evaluation on this ranking result.
We rank user according to their style distribu-
tion pi
u
and pick the first 50 and last 50 users for
evaluation. For each user, 10 posts are sampled
to be shown to the annotators. We mix these 100
users and ask two graduate students to do the an-
notations. The evaluation strategy is the same as
that in Section 4.3. We choose a simple base-
line which ranks users by the number of emoticons
they use per post. The evaluation result is shown
in Table 8 for serious and unserious user retrieval
respectively.
In both serious and unserious user retrieval
tasks, our method gets almost perfect perfor-
mance, which is better than the baseline. This
means the user style distributions learned by our
model can help separate serious and unserious
users.
4.5 Perplexity
Perplexity is a widely used criterion in statistical
natural language processing. It measures the pre-
dictive power of a model on unseen data, which
is algebraically equivalent to the inverse of the ge-
ometric mean per-word likelihood. A lower per-
plexity means the test data, which is unseen in the
training phase, can be generated by the model with
a higher probability. So it also indicates that the
model has a better generalization performance.
In this experiment, we leave 10% data for test-
ing and use the remaining 90% data for training.
We choose LDA as a baseline for comparison and
38
treat each thread as a document. The perplexity for
both models is calculated over different numbers
of topics, which ranges from 10 to 100. The result
is show in Figure 4. We can clearly see that our
proposed model has a substantially lower perplexi-
ty than LDA over different numbers of topics. This
proves that our model fits the forum discussion da-
ta better and has a stronger generalization power.
It also indicates that separating topic-driven words
and style-driven words can better fit the generation
of user generated content in forum discussions.
4.6 Topic Distinction
In traditional topic modeling, like LDA, all words
are regarded as topic-driven words, which are gen-
erated by mixture of topics. However, this may not
be true to user-generated content in online forum-
s as not all words are driven by discussed topics.
Take the following post for example:
? Okay lah. Let them be. I mean its their KKB
right? Let it rot lor.
In this post, the words ?lah? and ?lor? are not relat-
ed to the topics under discussion. They appear in
the post because the authors are used to using these
words, which means these words are style driven.
Style-driven words are related to a user?s charac-
teristics and should not be clustered into any top-
ic. Without separating these two types of words,
style-driven words may appear in different topics
and make topics less distinct to each other.
Figure 5 compares the Average Divergence
among discovered topics between TSM (Topic
Style model) and LDA over different numbers of
topics. We can clearly see that the Average Diver-
gence of TSM is substantially larger than that of L-
DA over different numbers of topics. This proves
that in TSM, the learned topics are more distinct
from each other. This is because LDA mixes these
two kinds of words, which introduces noise into
the learned topics and decreases their distinction
between each other. But topic driven words and
style driven words are well separated in TSM. Fig-
ure 5 also plots the Average Divergence between
the learned two styles, which is the curve denot-
ed by DIFF. We can see the AD between differ-
ent styles is even larger than that among topics in
TSM. Different topics may still have some over-
lap in frequently used words but styles may share
few words with each other. So AD of styles can
get higher value. This also proves the effective-
P@5 P@10 P@20 P@30 P@40 P@50
E 0 0.2 0.25 0.23 0.225 0.2
T 0.8 0.9 0.8 0.8 0.675 0.62
Table 9: Slang identification precision. E: Emoti-
con; T:TSM.
#Word/Post #Post
Formal User 34.9 158.3
Informal User 14.5 381
Table 10: Mean Value of average post length and
number of post for different type of users
ness of our model in identifying writing styles and
uncovering more distinct topics.
4.7 Discovering Slang
By looking at Table 5, we notice that the unse-
rious style contains many slang words with high
probability. This indicates that the unserious style
in the dataset we use is also characterized by slang
words. In this section, we will show the useful-
ness of our model in slang discovery. The base-
line method is denoted as Emoticon as it ranks
words according to their probability of occurring
in a post containing emoticons. We ask two Sin-
gaporean annotators to help us identify Singapore-
an slang in the top 50 words. The result is shown
in Table 9. It tells us the unserious style learned
in our model has very good performance in iden-
tifying local slang words. For people preferring
unserious writing style, they would write posts in
a very flexible way and use many informal words,
abbreviations and slang expressions. So our un-
serious style will be characterized by these slang
words and performs very well in identifying these
slang words.
4.8 Analysis of Users
In this subsection, we analyze users in our dataset
based on the result learned by TSM. Figure 8
shows the distribution of the histogram of serious
style probability. The majority of users have a
high serious style probability, which means most
users in our dataset are more eager to give serious
comments and express their opinions. This satis-
fies our observation that most people use forums
mainly to discuss and seek knowledge on differ-
ent topics and they are very eager to express their
thoughts in a serious way.
We heuristically split all users into two sets ac-
cording to user-style probability by setting 0.5 as
threshold. Users with probability of serious style
39
10 20 30 40 50 60 70 80 90 1004000
40504100
41504200
42504300
43504400
44504500
4550
Number of topics
Perple
xity
 
 TSMLDA
Figure 4: Perplexity
10 20 30 40 50 60 70 80 90 1001
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Number of topics
Log(AD
)
 
 
TSMLDADIFF
Figure 5: Average Divergence
Serious User Unserious User0
200400
600800
10001200
14001600
18002000
Numb
er of P
osts
Figure 6: Box plot of post number for serious
and unserious users
Serious User Unserious User0
50
100
150
200
Numb
er of W
ords P
er Pos
t
Figure 7: Box plot of average post length for se-
rious and unserious users
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
20
40
60
80
100
120
140
160
180
200
Seriouness Score
Numb
er of U
ser
Figure 8: Seriousness Score of Users
larger than 0.5 are regarded as serious users and
the remaining are unserious users. Next, we ex-
tract the number of posts each user edit and the
average number of words per post for each user
and compare the difference between these two us-
er sets. Figure 6 and Figure 7 show the box plots
of post number and average post length respective-
ly. We can see that serious users edit fewer posts
but use more words in each post. To see the dif-
ference between serious and unserious users more
clearly, we apply Mann-Whitney U test on the post
number populations and average post length pop-
ulations. The Mann-Whitney U test on both data
set reject the null hypothesis that two input popu-
lations are the same at the 1% significance level.
The mean value for post number and average post
length are also computed and shown in Table 10.
We can find that serious users tend to publish few-
er but longer posts than unserious users. This re-
sult is intuitive as serious users often spend more
effort editing their posts to express their opinions
more clearly. However, for unserious users, they
may just use a few words to play a joke or show
some emotions and they can post many posts with-
out spending too much time.
5 Conclusions
In this paper, we propose a unified probabilistic
graphical model, called Topic-Style Model, which
models topics and styles at the same time. Tra-
ditional topic modeling methods treat a corpus as
a mixture of topics. But user-generated content
in forum discussions contains not only words re-
lated to topics but also words related to different
writing styles. The proposed Topic-Style Model
can perform well in separating topic-driven word-
s and style-driven words. In this model, we as-
sume that writing style is a consistent writing pat-
tern a user will express in her posts across differ-
ent threads and use a latent variable at user lev-
el to capture the user specific preference of writ-
ing styles. Our model can successfully discover
writing styles which are different from each other
both in word distribution and formality. Words be-
longing to different writing styles and user specific
style distribution are captured by our model at the
same time. An extensive set of experiments shows
that our method has good performances in sepa-
rating serious and unserious posts and users. At
the same time, the model can identify slang words
with promising accuracy, which is proven by our
experiments. An analysis based on the learned
parameters in our model reveal the difference be-
tween serious and unserious users in average post
40
length and post number.
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining, pages 183?194, New York, NY,
USA. ACM.
Archana Bhattarai, Vasile Rus, and Dipankar Dasgup-
ta. 2009. Characterizing comment spam in the blo-
gosphere through content analysis. In Computation-
al Intelligence in Cyber Security, 2009. CICS?09.
IEEE Symposium on, pages 37?44. IEEE.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022, March.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010.
Automatic acquisition of lexical formality. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 90?98,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107?116, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhuoye Ding, Yeyun Gong, Yaqian Zhou, Qi Zhang,
and Xuanjing Huang. 2013. Detecting spammer-
s in community question answering. In Proceeding
of International Joint Conference on Natural Lan-
guage Processing, pages 118?126, Nagoya, Japan.
Association for Computational Linguistics.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and Ni-
na Wacholder. 2011. Identifying sarcasm in twitter:
A closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, pages 581?586, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the 2008 International
Conference on Web Search and Data Mining, WSD-
M ?08, pages 219?230, New York, NY, USA. ACM.
Jussi Karlgren and Douglass Cutting. 1994. Recogniz-
ing text genres with simple metrics using discrim-
inant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics - Volume 2,
pages 1071?1075, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of the 16th International Conference on
World Wide Web, WWW ?07, pages 171?180, New
York, NY, USA. ACM.
Michael J. Paul, ChengXiang Zhai, and Roxana Gir-
ju. 2010. Summarizing contrastive viewpoints in
opinionated text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 66?76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 186?195, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th Conference on Uncertainty in Artificial Intelli-
gence, pages 487?494, Arlington, Virginia, United
States. AUAI Press.
41
