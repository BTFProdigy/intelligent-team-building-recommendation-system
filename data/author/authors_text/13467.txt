Machine Translation of Very Close Languages 
Jan HAJI(~ 
Computer Science Dept. 
Johns Hopkins University 
3400 N. Charles St., Baltimore, 
MD 21218, USA 
hajic@cs.jhu.edu 
Jan HRIC 
KTI MFF UK 
Malostransk6 nfim.25 
Praha 1, Czech Republic, 11800 
hric@barbora.m ff.cuni.cz 
Vladislav KUBON 
OFAL MFF UK 
Malostransk6 mim.25 
Praha 1, Czech Republic, 11800 
vk@ufal.mff.cuni.cz 
Abstract 
Using examples of the transfer-based MT 
system between Czech and Russian 
RUSLAN and the word-for-word MT system 
with morphological disambiguation between 
Czech and Slovak (~ESILKO we argue that 
for really close languages it is possible to 
obtain better translation quality by means of 
simpler methods. The problem of translation 
to a group of typologically similar languages 
using a pivot language is also discussed here. 
Introduction 
Although the field of machine translation has a 
very long history, the number of really successful 
systems is not very impressive. Most of the funds 
invested into the development of various MT 
systems have been wasted and have not 
stimulated a development of techniques which 
would allow to translate at least technical texts 
from a certain limited domain. There were, of 
course, exceptions, which demonstrated that 
under certain conditions it is possible to develop 
a system which will save money and efforts 
invested into human translation. The main reason 
why the field of MT has not met the expectations 
of sci-fi literature, but also the expectations of 
scientific community, is the complexity of the 
task itself. A successful automatic translation 
system requires an application of techniques from 
several areas of computational inguistics 
(morphology, syntax, semantics, discourse 
analysis etc.) as a necessary, but not a sufficient 
condition. The general opinion is that it is easier 
to create an MT system for a pair of related 
languages. In our contribution we would like to 
demonstrate hat this assumption holds only for 
really very closely related languages. 
1. Czech-to-Russian MT system RUSLAN 
1.1 History 
The first attempt o verify the hypothesis that 
related languages are easier to translate started in 
mid 80s at Charles University in Prague. The 
project was called RUSLAN and aimed at the 
translation of documentation i the domain of 
operating systems for mainframe computers. It 
was developed in cooperation with the Research 
Institute of Mathematical Machines in Prague. At 
that time in former COMECON countries it was 
obligatory to translate any kind of documentation 
to such systems into Russian. The work on the 
Czech-to-Russian MT system RUSLAN (cf. Oliva 
(1989)) started in 1985. It was terminated in 1990 
(with COMECON gone) for the lack of funding. 
1.2 System description 
The system was rule-based, implemented in 
Colmerauer's Q-systems. It contained a full- 
fledged morphological and syntactic analysis of 
Czech, a transfer and a syntactic and 
morphological generation of Russian. There was 
almost no transfer at the beginning of the project 
due to the assumption that both languages are 
similar to the extent that does not require any 
transfer phase at all. This assumption turned to be 
wrong and several phenomena were covered by 
the transfer in the later stage of the project (for 
example the translation of the Czech verb "b~" 
\[to be\] into one of the three possible Russian 
equivalents: empty form, the form "byt6" in future 
7 
tense and the verb "javljat6sja"; or the translation 
of verbal negation). 
At the time when the work was terminated in 
1990, the system had a main translation 
dictionary of about 8000 words, accompanied by 
so called transducing dictionary covering another 
2000 words. The transducing dictionary was 
based on the original idea described in Kirschner 
(1987). It aimed at the exploitation of the fact 
that technical terms are based (in a majority of 
European languages) on Greek or Latin stems, 
adopted according to the particular derivational 
rules of the given languages. This fact allows for 
the "translation" of technical terms by means of a 
direct transcription of productive ndings and a 
slight (regular) adjustment of the spelling of the 
stem. For example, the English words 
localization and discrimination can be 
transcribed into Czech as "lokalizace" and 
"diskriminace" with a productive nding -ation 
being transcribed to -ace. It was generally 
assumed that for the pair Czech/Russian the 
transducing dictionary would be able to profit 
from a substantially greater number of productive 
rules. This hypothesis proved to be wrong, too 
(see B6mov~, Kubofi (1990)). The set of 
productive ndings for both pairs (English/Czech, 
as developed for an earlier MT system from 
English to Czech, and Czech/Russian) was very 
similar. 
The evaluation of results of RUSLAN showed 
that roughly 40% of input sentences were 
translated correctly, about 40% with minor errors 
correctable by a human post-editor and about 
20% of the input required substantial editing or 
re-translation. There were two main factors that 
caused a deterioration of the translation. The first 
factor was the incompleteness of the main 
dictionary of the system. Even though the system 
contained a set of so-called fail-soft rules, whose 
task was to handle such situations, an unknown 
word typically caused a failure of the module of  
syntactic analysis, because the dictionary entries 
contained - besides the translation equivalents 
and morphological information - very important 
syntactic information. 
The second factor was the module of syntactic 
analysis of Czech. There were several reasons of 
parsing failures. Apart from the common inability 
of most rule-based formal grammars to cover a 
particular natural anguage to the finest detail of 
its syntax there were other problems. One of  them 
was the existence of non-projective constructions, 
which are quite common in Czech even in 
relatively short sentences. Even though they 
account only for 1.7?/'o f syntactic dependencies, 
every third Czech sentence contains at least one, 
and in a news corpus, we discovered as much as 
15 non-projective dependencies; see also Haji6 et 
al. (1998). An example of a non-projective 
construction is "Soubor se nepodafilo otev~it." 
\[lit.: File Refl. was_not._possible to_open. - It was 
not possible to open the file\]. The formalism used 
for the implementation (Q-systems) was not meant 
to handle non-projective constructions. Another 
source of trouble was the use of so-called 
semantic features. These features were based on 
lexical semantics of individual words. Their main 
task was to support a semantically plausible 
analysis and to block the implausible ones. It 
turned out that the question of implausible 
combinations of  semantic features is also more 
complex than it was supposed to be. The practical 
outcome of the use of semantic features was a 
higher atio of parsing failures - semantic features 
often blocked a plausible analysis. For example, 
human lexicographers a signed the verb 'to run' a 
semantic feature stating that only a noun with 
semantic features of a human or other living being 
may be assigned the role of subject of this verb. 
The input text was however full of sentences with 
'programs' or 'systems' running etc. It was of 
course very easy to correct he semantic feature in 
the dictionary, but the problem was that there 
were far too many corrections required. 
On the other hand, the fact that both languages 
allow a high degree of word-order freedom 
accounted for a certain simplification of  the 
translation process. The grammar elied on the 
fact that there are only minor word-order 
differences between Czech and Russian. 
1.3 Lessons learned  f rom RUSLAN 
We have learned several lessons regarding the MT 
of closely related languages: 
? The transfer-based approach provides a 
similar quality of translation both for closely 
related and typologically different languages 
? Two main bottlenecks of full-fledged 
transfer-based systems are: 
8 
- complexity of the syntactic dictionary 
- relative unreliability of the syntactic 
analysis of the source language 
Even a relatively simple component 
(transducing dictionary) was equally complex 
for English-to-Czech and Czech-to-Russian 
translation 
Limited text domains do not exist in real life, 
it is necessary to work with a high coverage 
dictionary at least for the source language. 
2. Translation and localization 
2.1 A pivot language 
Localization of products and their documentation 
is a great problem for any company, which wants 
to strengthen its position on foreign language 
market, especially for companies producing 
various kinds of  software. The amounts of texts 
being localized are huge and the localization 
costs are huge as well. 
It is quite clear that the localization from one 
source language to several target languages, 
which are typologically similar, but different 
from the source language, is a waste of money 
and effort. It is of course much easier to translate 
texts from Czech to Polish or from Russian to 
Bulgarian than from English or German to any of 
these languages. There are several reasons, why 
localization and translation is not being 
performed through some pivot language, 
representing a certain group of closely related 
languages. Apart from political reasons the 
translation through a pivot language has several 
drawbacks. The most important one is the 
problem of the loss of translation quality. Each 
translation may to a certain extent shift the 
meaning of the translated text and thus each 
subsequent translation provides results more and 
more different from the original. The second 
most important reason is the lack of translators 
from the pivot to the target language, while this is 
usually no problem for the translation from the 
source directly to the target language. 
2.2 Translation memory is the key 
The main goal of this paper is to suggest how to 
overcome these obstacles by means of a 
combination of an MT system with commercial 
MAHT (Machine-aided human translation) 
systems. We have chosen the TRADOS 
Translator's Workbench as a representative 
system of a class of these products, which can be 
characterized as an example-based translation 
tools. IBM's Translation Manager and other 
products also belong to this class. Such systems 
uses so-called translation memory, which contains 
pairs of previously translated sentences from a 
source to a target language. When a human 
translator starts translating a new sentence, the 
system tries to match the source with sentences 
already stored in the translation memory. If it is 
successful, it suggests the translation and the 
human translator decides whether to use it, to 
modify it or to reject it. 
The segmentation f a translation memory is a key 
feature for our system. The translation memory 
may be exported into a text file and thus allows 
easy manipulation with its content. Let us suppose 
that we have at our disposal two translation 
memories - one human made for the source/pivot 
language pair and the other created by an MT 
system for the pivot/target language pair. The 
substitution of segments of a pivot language by 
the segments of a target language is then only a 
routine procedure. The human translator 
translating from the source language to the target 
language then gets a translation memory for the 
required pair (source/target). The system of 
penalties applied in TRADOS Translator's 
Workbench (or a similar system) guarantees that if 
there is already a human-made translation present, 
then it gets higher priority than the translation 
obtained as a result of the automatic MT. This 
system solves both problems mentioned above - 
the human translators from the pivot to the target 
language are not needed at all and the machine- 
made translation memory serves only as a 
resource supporting the direct human translation 
from the source to the target language. 
3. Mach ine  t rans lat ion of  (very) closely 
related Slavic languages 
In the group of Slavic languages, there are more 
closely related languages than Czech and Russian. 
Apart from the pair of Serbian and Croatian 
languages, which are almost identical and were 
9 
considered one language just a few years ago, the 
most closely related languages in this group are 
Czech and Slovak. 
This fact has led us to an experiment with 
automatic translation between Czech and Slovak. 
It was clear that application of a similar method 
to that one used in the system RUSLAN would 
lead to similar results. Due to the closeness of 
both languages we have decided to apply a 
simpler method. Our new system, (~ESILKO, 
aims at a maximal exploitation of the similarity 
of both languages. The system uses the method of 
direct word-for-word translation, justified by the 
similarity of syntactic constructions of both 
languages. 
Although the system is currently being tested on 
texts from the domain of documentation to 
corporate information systems, it is not limited to 
any specific domain. Its primary task is, however, 
to provide support for translation and localization 
of various technical texts. 
3.1 System (~ESiLKO 
The greatest problem of the word-for-word 
translation approach (for languages with very 
similar syntax and word order, but different 
morphological system) is the problem of 
morphological ambiguity of individual word 
forms. The type of ambiguity is slightly different 
in languages with a rich inflection (majority of 
Slavic languages) and in languages which do not 
have such a wide variety of forms derived from a 
single lemma. For example, in Czech there are 
only rare cases of part-of-speech ambiguities ( t~t 
\[to stay/the state\], zena \[woman/chasing\] or tri 
\[three/rub(imperative)\]), much more frequent is 
the ambiguity of gender, number and case (for 
example, the form of the adjective jam\[ \[spring\] 
is 27-times ambiguous). The main problem is that 
even though several Slavic languages have the 
same property as Czech, the ambiguity is not 
preserved. It is distributed in a different manner 
and the "form-for-form" translation is not 
applicable. 
Without he analysis of at least nominal groups it 
is often very difficult to solve this problem, 
because for example the actual morphemic 
categories of adjectives are in Czech 
distinguishable only on the basis of gender, 
number and case agreement between an adjective 
and its governing noun. An alternative way to the 
solution of this problem was the application of a 
stochastically based morphological disambiguator 
(morphological tagger) for Czech whose success 
rate is close to 92?/'0. Our system therefore consists 
of the following modules: 
1. Import of the input from so-called 'empty' 
translation memory 
2. Morphological analysis of Czech 
3. Morphological disambiguation 
4. Domain-related bilingual glossaries (incl. 
single- and multiword terminology) 
5. General bilingual dictionary 
6. Morphological synthesis of Slovak 
7. Export of the output o the original translation 
memory 
Letus now look in a more detail at the individual 
modules of the system: 
ad 1. The input text is extracted out of a 
translation memory previously exported into an 
ASCII file. The exported translation memory (of 
TRADOS) has a SGML-Iike notation with a 
relatively simple structure (cf. the following 
example): 
Example 1. - A sample of the exported translation 
memory 
<RTF Preamble>...</RTF Preamble> 
<TrU> 
<CrD>23051999 
<CrU>VK 
<Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete 
rychle a jednoduge vytv~i~et regerge. 
<Seg L=SK_01 >n/a 
</TrU> 
Our system uses only the segments marked by 
<Seg L=CS_01>, which contain one source 
language sentence ach, and <Seg L=SK_01>, 
which is empty and which will later contain the 
same sentence translated into the target language 
by CESiLKO. 
ad 2. The morphological analysis of Czech is 
based on the morphological dictionary developed 
by Jan Haji6 and Hana Skoumalov~i in 1988-99 
(for latest description, see Haji~ (1998)). The 
dictionary contains over 700 000 dictionary 
entries and its typical coverage varies between 
10 
99% (novels) to 95% (technical texts). The 
morphological analysis uses the system of 
positional tags with 15 positions (each 
morphological .category, such as Part-of-speech, 
Number, Gender, Case, etc. has a fixed, single- 
symbol place in the tag). 
Example 2 - tags assigned to the word-form 
"pomoci" (help/by means of) 
pomoci: 
NFP2 .... . .  A .... \]NFS7 ...... A .... I R--2 . . . . . . . . . . .  
where : 
N - noun; R - preposition 
F - feminine gender 
S - singular, P - plural 
7, 2 - case (7 - instrumental, 2 - genitive) 
A - affirmative (non negative) 
ad 3. The module of morphological 
disambiguation is a key to the success of  the 
translation. It gets an average number of 3.58 
tags per token (word form in text) as an input. 
The tagging system is purely statistical, and it 
uses a log-linear model of probability distribution 
- see Haji~, Hladkfi (1998). The learning is based 
on a manually tagged corpus of Czech texts 
(mostly from the general newspaper domain). 
The system learns contextual rules (features) 
automatically and also automatically determines 
feature weights. The average accuracy of tagging 
is between 91 and 93% and remains the same 
even for technical texts (if we disregard the 
unknown names and foreign-language t rms that 
are not ambiguous anyway). 
The lemmatization immediately follows tagging; 
it chooses the first lemma with a possible tag 
corresponding to the tag selected. Despite this 
simple lemmatization method, and also thanks to 
the fact that Czech words are rarely ambiguous in 
their Part-of-speech, it works with an accuracy 
exceeding 98%. 
ad 4. The domain-related bilingual glossaries 
contain pairs of individual words and pairs of 
multiple-word terms. The glossaries are 
organized into a hierarchy specified by the user; 
typically, the glossaries for the most specific 
domain are applied first. There is one general 
matching rule for all levels of glossaries - the 
longest match wins. 
The multiple-word terms are sequences of lemmas 
(not word forms). This structure has several 
advantages, among others it allows to minimize 
the size of the dictionary and also, due to the 
simplicity of the structure, it allows modifications 
of the glossaries by the linguistically naive user. 
The necessary morphological information is 
introduced into the domain-related glossary in an 
off-line preprocessing stage, which does not 
require user intervention. This makes a big 
difference when compared to the RUSLAN 
Czech-to-Russian MT system, when each 
multiword dictionary entry cost about 30 minutes 
of linguistic expert's time on average. 
ad 5. The main bilingual dictionary contains data 
necessary for the translation of  both lemmas and 
tags. The translation of tags (from the Czech into 
the Slovak morphological system) is necessary, 
because due to the morphological differences both 
systems use close, but slightly different tagsets. 
Currently the system handles the 1:1 translation of 
tags (and 2:2, 3:3, etc.). Different ratio of 
translation is very rare between Czech and Siovak, 
but nevertheless an advanced system of dictionary 
items is under construction (for the translation 1:2, 
2:1 etc.). It is quite interesting that the lexically 
homonymous words often preserve their 
homonymy even after the translation, so no 
special treatment of homonyms is deemed 
necessary. 
ad 6. The morphological synthesis of Slovak is 
based on a monolingual dictionary of SIovak, 
developed by J.Hric (1991-99), covering more 
than \]00,000 dictionary entries. The coverage of 
the dictionary is not as high as of  the Czech one, 
but it is still growing. It aims at a similar coverage 
of Slovak as we enjoy for Czech. 
ad 7. The export of  the output of the system 
(~ESILKO into the translation memory (of 
TRADOS Translator's Workbench) amounts 
mainly to cleaning of all irrelevant SGML 
markers. The whole resulting Slovak sentence is 
inserted into the appropriate location in the 
original translation memory file. The following 
example also shows that the marker <CrU> 
contains an information that the target language 
sentence was created by an MT system. 
11 
Example 3. -A  sample of the translation memory 
containing the results of MT 
<RTF Preamble>...</RTF Preamble> 
<TrU> 
<CRD>23051999 
<CrU>MT! 
<Seg L=CS_01>Pomoci v~kazu ad-hoc mfi~ete 
rychle a jednodu~e vytv~i~et re,erie. 
<Seg L=SK_01>Pomoci v~kazov ad-hoc m6~ete 
r~chio a jednoducho vytvhrat' re,erie. 
</TrU> 
3.2 Evaluation of results 
The problem how to evaluate results of automatic 
translation is very difficult. For the evaluation of 
our system we have exploited the close 
connection between our system and the 
TRADOS Translator's Workbench. The method 
is simple - the human translator eceives the 
translation memory created by our system and 
translates the text using this memory. The 
translator is free to make any changes to the text 
proposed by the translation memory. The target 
text created by a human translator is then 
compared with the text created by the mechanical 
application of translation memory to the source 
text. TRADOS then evaluates the percentage of 
matching in the same manner as it normally 
evaluates the percentage of matching of source 
text with sentences in translation memory. Our 
system achieved about 90% match (as defined by 
the TRADOS match module) with the results of 
human translation, based on a relatively large 
(more than 10,000 words) test sample. 
4. Conclusions 
The accuracy of the translation achieved by our 
system justifies the hypothesis that word-for- 
word translation might be a solution for MT of 
really closely related languages. The remaining 
problems to be solved are problems with the one- 
to many or many-to-many translation, where the 
lack of information in glossaries and dictionaries 
sometimes causes an unnecessary translation 
error. 
The success of the system CESILKO has 
encouraged the investigation of the possibility to 
use the same method for other pairs of Slavic 
languages, namely for Czech-to-Polish translation. 
Although these languages are not so similar as 
Czech and Slovak, we hope that an addition of a 
simple partial noun phrase parsing might provide 
results with the quality comparable to the full- 
fledged syntactic analysis based system RUSLAN 
(this is of course true also for the Czechoto-Slovak 
translation). The first results of Czech-to Polish 
translation are quite encouraging in this respect, 
even though we could not perform as rigorous 
testing as we did for Slovak. 
Acknowledgements 
This project was supported by the grant GAt~R 
405/96/K214 and partially by the grant GA(~R 
201/99/0236 and project of the Ministry of 
Education No. VS96151. 
References 
B6movfi, Alevtina and Kubofi, Vladislav (1990). Czech- 
to-Russian Transducing Dictionary; In: Proceedings 
of the Xlllth COLING conference, Helsinki 1990 
Haji~, Jan (1998). Building and Using a Syntactially 
Annotated Coprus: The Prague Dependency 
Treebank. In: Festschrifi for Jarmila Panevov~i, 
Karolinum Press, Charles Universitz, Prague. pp. 
106---132. 
Haji~, Jan and Barbora Hladk~t (1998). Tagging 
Inflective Languages. Prediction of Morphological 
Categories for a Rich, Structured Tagset. ACL- 
Coling'98, Montreal, Canada, August 1998, pp. 483- 
490. 
Haji~, Jan; Brill, Eric; Collins, Michael; Hladk~t 
Barbora; Jones, Douglas; Kuo, Cynthia; Ramshaw, 
Lance; Schwartz, Oren; Tillman, Christoph; and 
Zeman, Daniel: Core Natural Language Processing 
Technology Applicable to Multiple Languages. The 
Workshop'98 Final Report. CLSP JHU. Also at: 
http:llwww.clsp.jhu.edulws981projectslnlplreport. 
Kirschner, Zden~k (1987). APAC3-2: An English-to- 
Czech Machine Translation System; Explizite 
Beschreibung der Sprache und automatische 
Textbearbeitung XII1, MFF UK Prague 
Oliva, Karel (1989). A Parser for Czech Implemented 
in Systems Q; Explizite Beschreibung der Sprache 
und automatische Textbearbeitung XVI, MFF UK 
Prague 
12 
Morphological Tagging: Data vs. Dictionaries 
J an  Haji~.* 
Depar tment  of Computer  Science 
Johns  Hopkins  Univers i ty  
Bal t imore,  MD 21218 
hajic@cs.jhu.edu 
Abst ract  
Part of Speech tagging for English seems to have 
reached the the human levels of error, but full mor- 
phological tagging for inflectionally rich languages, 
such as Romanian, Czech, or Hungarian, is still an 
open problem, and the results are far from being 
satisfactory. This paper presents results obtained 
by using a universalized exponential feature-based 
model for five such languages. It focuses on the data 
sparseness issue, which is especially severe for such 
languages (the more so that there are no extensive 
annotated data for those languages). In conclusion, 
we argue strongly that the use of an independent 
morphological dictionary is the preferred choice to 
more annotated data under such circumstances. 
1 Full Morpho log ica l  Tagg ing  
English Part of Speech (POS) tagging has been 
widely described in the recent past, starting with 
the (Church, 1988) paper, followed by numerous 
others using various methods: neural networks (Ju- 
lian Benello and Anderson, 1989), HMM tagging 
(Merialdo, 1992), decision trees (Schmid, 1994), 
transformation-based error-driven learning (Brill, 
1995), and max imum entropy (Ratnaparkhi, 1996), 
to select just a few. However different the methods 
were, English dominated in these tests. 
Unfortunately, English is a morphologically "im- 
poverished" language: there are no complicated 
agreement relations, word order variation is mini- 
real, and the morphological categories are either ex- 
tremely simple (-s for plural of nouns, for example), 
or (almost) nonexistent (cases expressed by inflec- 
tion, for example) - with not too many exceptions 
and irregularities. Therefore the number of tags se- 
lected for an English tagset is not that large (40-75 
in the typical case). Also, the average ambiguity 
is low (2.32 tags per token on the manually tagged 
* The work described herein has been started and largely 
done within author's home institution, the Institute of For- 
mal and Applied Linguistics, Charles University, Prague, CZ, 
within the project VS96151 of the Ministry of Education 
of the Czech Republic and partially also under the grant 
405/96/K214 ofthe Grant Agency of the Czech Republic. 
Wall Street Journal part in the Penn Treebank, for 
example). 
Highly inflective and agglutinative languages are 
different. Obviously we can limit the number of tags 
to the major part-of-speech classes, plus some (like 
the Xerox Language Tools (Chanod, 1997) for such 
languages do), and in fact achieve similar perfor- 
mance, but that limits the usefulness of the results 
thus obtained for further analysis. These languages, 
obviously, do not use the rich inflection just for the 
amusement (or embarrassment) of their speakers (or 
NLP researchers): the inflectional categories carry 
important information which ought to be known at 
a later time (e.g., during parsing). Thus one wants 
not only to tell apart verbs from nouns, but also 
nominative from genitive, masculine animate from 
inanimate, singular from plural - all of them being 
often ambiguous one way or the other. 
The average tagset, as found even in a moderate 
corpus, contains between 500 and 1,000 distinct ags 
- whereas the size of the set of possible and plausible 
tags can reach 3,000 to 5,000. Obviously, any of the 
statistical methods used for English (even if fully 
supervised) clash with (or, fall through) the data 
sparseness problem (see below Table 1 for details). 
There have been attempts to solve this problem 
for some of the highly inflectional European lan- 
guages ((Daelemans et al, 1996), (Erjavec et al, 
1999), (Tufts, 1999), and also our own in (Haji~ 
and Hladk~, 1997), (Haji~ and Hladk~, 1998), see 
also below), but so far no method nor a tagger has 
been evaluated against a larger number of those lan- 
guages in a similar setting, to allow for a side-by- 
side comparison of the difficulty (or ease) of full 
morphological tagging of those languages. Thanks 
to the Multext-East project (V6ronis, 1996a), there 
are now five annotated corpora available (which are 
manually fully morphologically tagged) to perform 
such experiments. 
2 The  Languages  Used  and  The  
Tra in ing  Data  
We use the Multext-East-annotated version of the 
Orwell's 1984 novel in Czech, Estonian, Hungarian, 
94 
Romanian and Slovene I. The annotation uses a sin- 
gle SGML-based formal scheme, and even common 
guidelines for tagset design and annotation, nev- 
ertheless the tagsets differ substantially since the 
languages differ as well: Romanian is a French-like 
romance language, Hungarian is agglutinative, and 
the other languages are more or less inflectional- 
type languages 2. The annotated data contains about 
100k tokens (including punctuation) for each lan- 
guage; out of those, the first 20k tokens has been 
used for testing, the rest for training. We have also 
extended the tag identifiers by appending a string 
of hyphens ('-') to suit the exponential tagger which 
expects the tags to be of equal length; the mapping 
was 1:1 for all tags in all languages, since the "long" 
tags are in fact the Multext-East standard. 
From the tagging point of view, the language char- 
acteristics displayed in Table 1 are the most rele- 
vant 3 . 
3 The  Methodo logy  
The main tagger used for the comparison experiment 
is the probabilistic exponential-model-based, error- 
driven learner we described in detail in (Haji~ and 
Hladk~, 1998). Modifications had to be made, how- 
ever, to make it more universal across languages. 
3.1 Structure of  the Model  
The model described in (Haji~ and Hladk~, 1998) 
is a general exponential (specifically, a log-linear) 
model (such as the one used for Maximum Entropy- 
based models): 
pAc(ylx ) = exp(~\]in_1AJi(y, x)) 
z(x) (1) 
where fi(y,x) is a binary-valued feature of the 
event value being predicted and its context, A~ is 
a weight of the feature fi, and Z(x) is the natural 
normalization factor. This model is then essentially 
reduced to Naive Bayes by the approximation of the 
IThere are more languages involved in the Multext-East 
project, but only these five languages have been really care- 
fully tagged; English is unfortunately tagged using Eric Brill's 
tagger trained in unsupervised mode, leaving multiple output 
at almost every ambiguous token, and Bulgarian is totally 
unusable since it has been tagged automatically with only 
a baseline tagger. The English results reported below thus 
come from the Penn Treebank data, from which we have used 
roughly 100,000 words to match the training data sizes for the 
remaining languages. For Czech, Hungarian, and Slovene we 
use later versions of the annotated data (than those found on 
the Multext-East CD) which we obtained directly from the 
authors of the annotations after the Multext-CD had been 
published, since the new data contain rather substantial im- 
provements over the originally published data. 
2For detailed account of the lexical characteristics of these 
languages, see (Vdronis, 1996b). 
3We have included English here for comparison purposes, 
since these characteristics are independent of the annotation. 
Ai parameters, which is done because there are mil- 
lions of possible features in the pool and thus the full 
entropy maximization is prohibitively expensive, if
we want to select a small number of features instead 
of keeping them all. 
The tags are predicted separately for each mor- 
phological category (such as POS, NUMBER,  
CASE, DEGREE OF  COMPARISON,  etc.). The 
model makes an extensive use of so-called "ambigu- 
ity classes" (ACs). An ambiguity class is a set of 
values (such as genitive and accusative) of a single 
category (such as CASE) which arises for some word 
forms as a result of morphological analysis. For un- 
ambiguous word forms (unambiguous from the point 
of view of a certain category), the ambiguity class set 
contains only a single value; for ambiguous forms, 
there are 2 or more values in the AC. For example, 
let's suppose we use part-of-speech (POS), number 
and tense as morphological categories for English; 
then the word form "borrowed" is 2-way ambiguous 
in POS ({V, J} for verb and adjective, respectively), 
unambiguous in number (linguistic arguments apart, 
number is typically regarded "not applicable" to ad- 
jectives as well as to almost all forms of verbs in 
English), and 3-way ambiguous in tense ({P,N,-) 
for past tense, past participle, and "not applicable" 
in the adjective form). 
The predictions of the models are always condi- 
tioned on the ambiguity class of the category (POS, 
NUMBER, ...) in question. In other words, there is 
a separate model for each category and an ambigu- 
ity class from that category. Naturally, there is no 
model for unambiguous ACs classes. However, even 
though the ambiguity classes bring very valuable in- 
formation about the word form being tagged and a 
reliable information about the context (since they 
are fixed during tagging), using ACs causes also an 
unwelcome ffect of partitioning the already scarce 
data and also effectively ignores tatistics of the un- 
ambiguous cases. 
The context of features uses the neighboring words 
(original word forms) and ambiguity classes on sub- 
tags, where their relative position in text might be 
either fixed (0, -1, +1) or "variable" using a value of 
the POS subtag as the "stop here" criterion, up to 
4 text positions (words) apart. 
3.2 General Subtag Features 
The original model uses the ambiguity classes not 
only for conditioning on context in features, but also 
for the individual models based on category and an 
AC. 
More general features have been introduced, 
which do not depend on the ambiguity class of the 
subtag being predicted any more. This allows to 
learn also from unambiguous tokens. However, the 
training time is increased ramatically by doing so 
since all events in the training data have to be taken 
95 
Language 
English 4 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
Table 1: Training data in numbers 
Training Size Tagset Size Ambiguous Tokens 
99903 
87071 
81383 
102992 
104583 
94457 
139 
970 
476 
401 
486 
1033 
38.65% 
45.97% 
40.24% 
21.58% 
40.00% 
38.01% 
into consideration, as opposed to the case of training 
the small AC-based model, when only those training 
events which contain the particular AC are used. 
3.3 Var iab le  D is tance  Cond i t ion  
The "stop" criterion for finding the appropriate rel- 
ative position was originally based on hard coded 
choices suitable for the Czech language only, and of 
course it depended on the tagset as well. This depen- 
dency has been removed by selecting the appropriate 
conditions automatically when building the pool of 
possible features at the initialization phase 5 (using 
the relative frequency of the POS ambiguity classes, 
and a threshold to cut off less frequent categories to 
limit the size of the feature pool). 
3.4 Weight  Var ia t ion  
Even though the full computation of the appropriate 
feature weight is still prohibitive (the more so when 
the general features are added), the learner is now 
allowed to vary the weights (in several discrete steps) 
during feature selection, as a (somewhat crude) at- 
tempt to depart from the Naive Bayes simplification 
to the approximation ofthe "correct" Maximum En- 
tropy estimation. 
3.5 Hand l ing  Unknown Words  
In order to compare the effects of (not) using an in- 
dependent dictionary, we have added an unknown 
word handling module to the code. 6 It extracts 
the prefix and suffix frequency information (and the 
combination thereof) from the training data. Then, 
for each of the combinations, it selects the most fre- 
quent set of tags seen in the training data and stores 
it for later use. When tagging, the data is first piped 
through a "guesser" which assigns to the unknown 
words such a set of possible tags which is stored with 
the longest matching prefix/suffix combination. 
5Also, the use of variable-distance context may be switched 
off entirely. 
6Originally, the code relied exclusively on the use of such 
an independent dictionary. Since the coverage of the Czech 
dictionary we have used is extensive, we have been simply 
ignoring the unknown word problem altogether in the past. 
4 The  Resu l ts  
4.1 Repor t ing  Er ror  Rate :  Words  vs .  
Tokens  
Since "best-only" tagging has been carried out, the 
error rate (i.e, 100 - accuracy in %) measure has 
been used throughout as the only evaluation crite- 
rion. However, since some results reported previ- 
ously were apparently obtained using only the "real" 
words as the total for accuracy evaluation, whereas 
in other experiments every token counts (including 
punctuation 7, for example), we have computed both 
and report them separately s. 
4.2 Ava i lab i l i ty  o f  Dic t ionary  In fo rmat ion  
We use two methods to obtain the set of possible 
tags for any given word form (i.e., to analyze it mor- 
phologically). Both methods include handling un- 
known words. First, we use only information which 
may be obtained automatically from the manually 
annotated corpus (we call this method automatic). 
This is the way the Maximum Entropy tagger (Rat- 
naparkhi, 1996) runs if one uses the binary version 
from the website (see the comparison in Section 5). 
However, it is not unreasonable to assume that a 
larger independent dictionary exists which can help 
to obtain a list of possible tags for each word form 
in test data. This is what we have at our disposal 
for the languages in question, since the development 
of such a dictionary was part of the Multext-East 
project. We can thus assume a dictionary info is 
available for unknown words in the test data, i.e., 
even though there is no statistics available for them 
(since they did not appear in the training data), all 
possible tags for (almost 9) every test token are avail- 
able. This method is referred to as independent in
the following text. 
We have also used a third method of obtain- 
ing a dictionary information (called mized), namely, 
by using only the words from the training data, 
rAnd sometimes a separate token for sentence boundary 
STable 1 has been computed using all tokens. In fact, the 
languages differ significantly in the proportion of punctuation: 
from about 18% (English) to 30% (Estonian). 
9Depending on the quality of the independent dictionary. 
Of course, the tagsets must match, which could be a problem 
per se. Here it is simple, since the dictionaries have been 
developed using the same tagsets as the tagged ata. 
96 
but complementing the information about them ob- 
tained from the training data by including all other 
possible tags for such words. Therefore the net result 
is that during testing, we have only training words 
at our disposal, but with a complete dictionary in- 
formation (as if coming from a full morphological 
dictionary) 1?. 
The results on the full training data set are sum- 
marized in Table 2. 
The baseline error rate is computed as follows. 
First of all, we use the independent dictionary for 
obtaining the possible tags for each word. Then we 
extract only the lexical information from the current 
position 11 and counts used for smoothing (which is 
based on the ambiguity classes only and it does not 
use lexical information). The system is then trained 
normally, which means it uses the lexical information 
only if the AC-based smoothing 12 alone does not 
work. This baseline method is thus very close to the 
usual baseline method of using simple conditional 
distribution of tags given words. 
The message of Table 2 seems to be obvious; but 
before we jump to conclusions, let's present another 
set of experiments. 
In view of the recent interest in dealing with 
"small languages", and with regard to the questions 
of cost-effectiveness of using "human" resources (i.e. 
annotation vs. rule-writing vs. tools development 
etc.), we have also performed experiments with re- 
duced training data size (but with an enriched fea- 
ture pool - by lowering thresholds, adding more of 
the "general features" as described above, etc. - as 
allowed by reasonable time/space constraints). 13
These results are summarized in Table 3 (using 
only the dictionary derived from the training data), 
Table 4 (using words from training data with mor- 
phological information complemented from a dictio- 
nary) and Table 5 (using the "independent" dictio- 
nary). In all cases, we again count only true words 
(no punctuation). Accordingly, the major POS er- 
ror rate is reported, too (12 POS tags to be dis- 
tinguished only: Noun, Verb, Adjective . . . .  ; see Ta- 
bles 6, 7, and 8). 
1?This arrangement removes the "closed vocabulary" phe- 
nomenon from the test data, since for the Multext-East data, 
we did not have a truly independent vocabulary available. 
11Words from the training data which are not singletons 
(freq > 1) are used. Surprisingly enough, it would not hurt 
to use them too. We believe it is due to the smoothing method 
used. Even though this is valid only for the baseline xperi- 
ment, we have observed ingeneral that this form of exponen- 
tial model (with error-driven training, that is) is remarkably 
resistant to overtrainlng. 
12Using ACs linearly interpolated with global unigram sub- 
tag distribution and finally the uniform distribution. 
13By reasonable we mean less than a day of CPU for train- 
ing. 
Table 9: Exponential w/feature selection vs. Max- 
imum Entropy tagger (Words-only Error Rate, no 
dictionary) 
Language Tagger 
Exp. 
English 9.18% 
Czech 18.83% 
Estonian 13.95% 
Hungarian 8.16% 
Romanian 7.76% 
Slovene 16.26% 
MaxEnt 
6.38% 
17.77% 
14.92% 
8.55% 
7.66% 
17.44% 
4.3 Tagger  Compar i son  
The work (Erjavec et al, 1999) consistently com- 
pares several taggers (HMM, Brill's Transformation- 
based Tagger, Ratnaparkhi's Maximum Entropy 
tagger, and the Daelemans et al's Memory-based 
Tagger) on Slovene. We have chosen the Maximum 
Entropy tagger (Ratnaparkhi, 1996) for a compari- 
son with our universal tagger, since it achieved (by 
a small margin) the best overall result on Slovene 
as reported there (86.360% on all tokens) of tag- 
gers available to us (MBT, the best overall, was not 
freely available to us at the time of writing). We 
have trained and tested the Maximum Entropy Tag- 
ger on exactly the same data, using the off-the-shelf 
(java binary only) version. 
The results are compared in Table 9. 
Since we want to show how a tagger accuracy is 
influenced by the amount of training data available, 
we have run a series of experiments comparing the 
results of the exponential tagger to the maximum 
entropy tagger when there is only a limited amount 
of data available. The results are summarized in 
Table 10. Since the public version of the MaxEnt 
tagger cannot be modified to take advantage of nei- 
ther the mixed nor the independent dictionary, we 
have compared it only to the automatic dictionary 
version of the exponential tagger. To save space, 
the results are tabulated only for the training data 
sizes of 2000, 5000 and 20000 words. Again, only 
the "true" word error rate is reported. 
As the tables show, for the languages we tested, 
the exponential, feature-based tagger we adapted 
from (Haji~ and Hladk~, 1998) achieves imilar re- 
sults as the Maximum Entropy tagger 14 15. (using 
exactly the same (full) training data; the "score" 
is 3:3, with the MaxEnt tagger being substantially 
better on English; probably the development lan- 
14Otherwise the acknowledged leader in English tagging 
15The only substantial difference we noticed was in tagging 
speed. The runtime speed of the MaxEnt tagger is lower, only 
about 10 words per second vs. almost 500 words per second; 
it should be noted however that we are comparing MaxEnt's 
java bytecode and C. 
97 
Table 2: Results (Error rate, ER) on full training data, only true words counted (no punctuation) 
Dictionary: 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
Automatic 
Baseline Pull 
11.42% 9.18% 
23.02% 18.83% 
16.12% 13.95% 
8.35% 8.16% 
10.87% 7.76% 
20.53% 16.26% 
Mixed 
Baseline Full 
11.40% 7.91% 
22.61% 14.78% 
16.19% 12.98% 
8.31% 8.00% 
10.81% 7.34% 
20.01% 13.29% 
Independent 
Baseline Full 
7.07% 3.58% 
19.40% 9.59% 
9.94% 5.34% 
3.55% 2.58% 
7.49% 3.35% 
17.29% 9.00% 
Table 3: Error rate on reduced training data, dictionary: automatic 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
I000 
36.20% 
48.22% 
48.14% 
39.68% 
40.61% 
45.84% 
2000 
29.36% 
42.95% 
42.10% 
32.21% 
35.02% 
39.58% 
Training 
5000 
23.47% 
36.54% 
32.44% 
23.94% 
25.06% 
33.12% 
data size 
10000 
18.27% 
30.97% 
26.81% 
18.04% 
19.26% 
28.60% 
20000 
14.46% 
27.08% 
21.51% 
13.92% 
15.16% 
24.50% 
Full 
9.18% 
18.83% 
13.95% 
8.16% 
7.76% 
16.26% 
Table 4: Error rate on reduced training data, dictionary: mixed 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
Training data size 
I000 
36.15% 
48.97% 
48.24% 
39.87% 
42.85% 
46.74% 
2000 
29.58% 
41.93% 
42.79% 
32.71% 
35.70% 
39.88% 
5000 
22.93% 
34.37% 
32.98% 
23.63% 
25.46% 
32.00% 
10000 
17.70% 
28.10% 
26.60% 
17.98% 
19.23% 
26.20% 
20000 
14.00% 
23.31% 
21.02% 
13.82% 
14.81% 
21.73% 
Full 
7.91% 
14.78% 
12.98% 
8.00% 
7.34% 
13.29% 
Table 5: Error 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
rate on reduced training data, dictionary: "independent" 
1000 
10.29% 
22.51% 
13.11% 
6.84% 
13.11% 
24.63% 
2000 
7.64% 
18.07% 
11.95% 
5.35% 
9.47% 
19.17% 
Training data size 
5000 10000 
5.53% 4.54% 
17.33% 15.10% 
10.70% 9.29% 
4.29% 4.07% 
7.81% 6.18% 
16.17% 14.12% 
20000 Pull 
3.83% 3.58% 
12.62% 9.59% 
8.10% 5.34% 
3.48% 2.58% 
5.07% 3.35% 
12.62% 9.00% 
guage bias shows herein). However, when the train- 
ing data size goes down, the advantage of predicting 
the single morphological categories separately favors 
the exponential tagger (with the notable and sub- 
stantial exception of English). The less data, the 
larger the difference (Tab 10). 
16On the other hand, the Exponential tagger has been de- 
veloped on Czech originally and it lost on this language. It 
should be noted that the original version of the exponen- 
tial tagger did contain more Czech-specific eatures, and thus 
might in fact do better. 
The resulting accuracy (of both taggers) is still 
unsatisfactory not only from the point of view of 
results obtained on English, but also from the prac- 
tical point of view: approx. 85% accuracy (Czech, 
Slovene) typically means that about five out of six 
10-word sentences contain at least one error in it. 
That is bad news e.g. for parsing projects involving 
tagging as a preliminary step. 
98 
Table 6: POS Error rate on reduced training data, dictionary: automatic 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
1000 
26.77% 
24.32% 
35.81% 
30.54% 
31.33% 
27.16% 
Training data size 
2000 5000 
20.82% 16.11% 
20.20% 13.46% 
30.52% 23.02% 
24.99% 18.09% 
27.59% 19.24% 
23.15% 17.01% 
10000 
11.86% 
9.70% 
18.26% 
13.15% 
14.51% 
12.89% 
20000 Full 
9.48% 5.64% 
7.22% 3.72% 
14.31% 8.46% 
10.29% 5.81% 
11.25% 5.21% 
9.74% 5.61% 
Table 7: POS Error rate on reduced training data, dictionary: mixed 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
1000 
26.69% 
24.32% 
36.48% 
30.28% 
33.56% 
27.58% 
Training data size 
2000 
21.09% 
20.61% 
31.76% 
25.25% 
28.34% 
23.30% 
5OOO 
15.82% 
13.47% 
23.55% 
17.59% 
20.03% 
16.85% 
10000 
11.53% 
10.19% 
18.21% 
12.89% 
14.52% 
12.59% 
20000 
9.08% 
7.37% 
14.32% 
10.15% 
11.03% 
9.88% 
Fu l l  
4.94% 
3.76% 
8.2O% 
5.64% 
5.04% 
5.12% 
Table 8: POS Error rate on reduced training data, dictionary: "independent" 
Language Training data size 
1000 2000 5000 10000 20000 Full 
-English 6.42% 5.36% 3.63% 3.02% 2.53% 2.43% 
Czech 3.21% 2.85% 2.17% 2.01% 1.65% 1.12% 
Estonian 6.71% 6.32% 5.27% 4.31% 3.77% 2.36% 
Hungarian 5.35% 4.42% 3.39% 3.18% 2.75% 2.04% 
Romanian 9.51% 6.54% 5.36% 4.00% 3.18% 1.89% 
Slovene 6.10% 5.19% 4.04% 3.59% 3.25% 2.08% 
5 Conc lus ions  
5.1 The Differences Among Languages 
The following discussion abstracts from the tagset 
design, relying on the fact that the Multext-East 
project has been driven by common tagset guidelines 
to an unprecedented xtent, given the very different 
languages involved. At the same time, we acknowl- 
edge that even so, their design for the individual 
languages might have influenced the results. Also, 
the quality of the annotation is an important factor; 
we believe though that the later data we obtained 
for the experiments described here are within the 
range of usual human error and do not suffer from 
negligence 1~. 
First of all, it is clear that these languages differ 
substantially just by looking at the simple training 
17Specifically, we are sure that the post-release Czech, 
Slovene and Hungarian data we are using are without anno- 
tation defects beyond the usual occasional nnotation error, 
as they have been double checked, and we also believe that 
the other two languages are reasonably clean. Bulgarian, al- 
though present on the CD, is unfortunately unusable since it 
has not been manually annotated; for English, see above. 
data statistics, where the number of unique tags seen 
in a relatively small collection of about 100k tokens is 
high - from 401 (Hungarian) to 1033 (Slovene); com- 
pare that to English with only 139 tags. However, it 
is interesting to see that the average per-token ambi- 
guity is much more narrowly distributed, and in fact 
English ranks 3rd (after Hungarian and Slovene), 
Czech being the last with almost every other token 
ambiguous on average. This ambiguity does not cor- 
respond with the results obtained: Slovene, being 
the second least ambiguous, is the second most dif- 
ficult to tag. Only Czech behaves consistently by 
tailing the pack in both cases. 
5.2 Comparison to Previous Results 
Any comparison is necessarily difficult due to differ- 
ent evaluation methodologies, ven within the "best- 
only", accuracy-based reporting. Nevertheless, we 
will try. 
For Romanian, Tufts in his recent work (Tufts, 
1999) reports 98.5% accuracy (i.e. 1.5% error rate) 
on Romanian, using the classifier combination ap- 
proach advocated by e.g. (Brill and Wu, 1998). His 
99 
Table 10: Error rate comparison on reduced training data, automatic dictionary 
Language 
English 
Czech 
Estonian 
Hungarian 
Romanian 
Slovene 
Training data size 
2000 
ME Exp 
26'03% 29.36% 
50.77% 42.95% 
51.08% 42.10% 
41.12% 32.21% 
42.88% 35.02% 
49.46% 39.58% 
5000 
ME Exp 
17.70% 23.47% 
41.95% 36.54% 
40.09% 32.44% 
30.68% 23.94% 
30.07% 25.06% 
39.34% 33.12% 
20000 
ME Exp 
9.61% 14.46% 
28.16% 27.08% 
25.50% 21.51% 
17.27% 13.92% 
16.67% 15.16% 
27.77% 24.50% 
results are well above the 3.29% error rate achieved 
here (with even a larger tagset of 1391 vs. 486 here), 
but the paper does not say how this number has been 
computed (training data size, the all-token/words- 
only question) thus making any conclusions difficult 
to make. He also argues that his method is language 
independent but no results are mentioned for other 
languages. 
For Czech, previous work achieved similar results 
(6.20% on newspaper text using the all-tokens-based 
error rate computation, on 160,000 training tokens; 
vs. 7.04% here on approx, half that amount of train- 
ing data; same handling of unknown words). This is 
in line with the expectations, since the same method- 
ology (tagging as well as evaluation) has been used, 
except he features used in that work were specifi- 
cally tuned to Czech. 
The most detailed account of Slovene (Erjavec et 
al., 1999) reports various results, which might not 
be directly comparable because it is unclear whether 
they use the all-tokens-based or words-only compu- 
tation of the error rate. They report 6.421% error 
rate on the full tagset on known words, and 13.583% 
on all words (tokens?) including unknown words 
(the exponential tagger we used achieved 13.82% on 
all tokens, 16.26% on words only). They use almost 
the same data (Orwell's 1984, but leaving out the 
Appendices) ls. They also report that the original 
Czech-specific exponential tagger used as a basis for 
the work reported here achieved 7.28% error rate on 
Slovene on full tags on the same data, which means 
that by the changes to the exponential tagger aimed 
at its language independence w  introduced in Sec- 
tion 3, we have not achieved any improvement (on 
Slovene) of the exp. tagger (the error rate stayed at 
7.26% - using all-tokens-based valuation umbers, 
dictionary available; but the data was not exactly 
the same, presumably). 
5.3  D ic t ionary  vs .  T ra in ing  Data  
This is, according to our opinion, the most interest- 
ing result of the experiments described so far. As 
18Their tag count is lower (1021) than here (1033), but 
that's not really relevant. They do not report he average 
ambiguity or a similar measure. 
100 
already Table 2 clearly suggests, even the baseline 
tagging results obtained with the help of an indepen- 
dent dictionary are comparable (if not better) than 
the fully-trained tagger on 100k words, but without 
the dictionary information. The situation is even 
clearer when comparing the POS-only results: here 
the "independent" dictionary results are better by 
far, with almost no training data needed. 
Looking at the characteristics of the languages, it 
is apparent that the inflections cause the problem: 
the coverage of a previously unseen text is inferior to 
the usual coverage of English or another analytical 
language. Therefore, unless we can come up with 
a really clever way of learning rules for dealing with 
previously unseen words, it is clearly strongly prefer- 
able to work on a morphological dictionary 19, rather 
than to try to annotate more data. 
6 Future  Work  
We would like to compare more taggers using still 
other methodologies, especially the MBT tagger, 
which achieved the best results on Slovene but which 
was not available to us at the time of writing this 
paper. Obviously, we would also like to use the clas- 
sifter combination method on them, to confirm the 
really surprisingly good results on Romanian and 
test it on the other languages as well. 
We would also like to enrich the best taggers avail- 
able today (such as the Maximum Entropy tagger) 
by using the dictionary information available and 
compare the results with the exponential feature- 
based tagger we have been using in the experiments 
here. 
For Czech and Slovene, the results are still far be- 
low what one would like to see (in absolute terms). It 
seems that the key lies in the initial feature set defi- 
nition - including statistical tagset clustering, which 
might potentially lead to more reliable estimates of 
certain parameters while using still the same size of 
training data. 
19Not necessarily manually - apparently, even a partially 
supervised method would be of tremendous help. 
7 Acknowledgements  
The author wishes to thank many Multext-East par- 
ticipants for their efforts to improve the original 
data, especially to Niki Petkevi~, Tomaz Erjavec, 
Heiki-Jaan Ka~lep and G?bor Pr6sz6ky, and for pro- 
viding the final versions of the annotated ata for 
the experiments. Any errors and mistakes are solely 
to be blamed on the author, not the annotators, of 
course. 
Re ferences  
Eric Brill and Jun Wu. 1998. Classifier combination 
for improved lexical disambiguation. In Proceed- 
ings of ACL/COLING'g8, pages 191-195, Mon- 
treal, Canada. ACL/ICCL. 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural anguage processing: A case 
study in part-of-speech tagging. Computational 
Linguistics, 21:543-565. 
Jean-Pierre Chanod. 1997. Current developments 
for Central & Eastern European languages. In 
Proceedings of EU Project meeting TELRI I, Ro- 
mania. 
Kenneth W. Church. 1988. A stochastic parts pro- 
gram and noun phrase parser for unrestricted text. 
In Proceedings of the Second Conference on Ap- 
plied Natural Language Processing, pages 136-143, 
Austin, Texas. ACL. 
Walter Daelemans, Jakub Zavrel, Peter Berck, and 
Steven Gillis. 1996. MBT: A memory-based part 
of speech tagger generator. In Proceedings of 
WVLC 4, pages 14-27. ACL. 
Tomaz Erjavec, Saso Dzeroski, and Jakub Zavrd. 
1999. Morphosyntactic Tagging of Slovene: Eval- 
uating PoS Taggers and Tagsets. Technical Re- 
port IJS-DP 8018, Dept. for Intelligent Systems, 
Jozef Stefan Institute, Ljubljana, Slovenia, April 
2nd. 
Jan Haji~ and Barbora Hladk~t. 1997. Tagging of in- 
flective languages: a comparison. In Proceedings 
of ANLP'gT, pages 136-143, Washington, DC. 
ACL. 
Jan Haji~ and Barbora Hladk& 1998. Tagging 
inflective languages: Prediction of morphologi- 
cal categories for a rich, structured tagset. In 
Proceedings of A CL/COLING'98, pages 483-490, 
Montreal, Canada. ACL/ICCL. 
Andrew W. Mackie Julian Benello and James A. An- 
derson. 1989. Syntactic ategory disambiguation 
with neural networks. Computer Speech and Lan- 
guage, 3:203-217. 
Bernard Merialdo. 1992. Tagging text with a 
probabilistic model. Computational Linguistics, 
20(2):155-171. 
Adwait Ratnaparkhi. 1996. A maximum entropy 
model for part-of-speech tagging. In Proceedings 
of EMNLP 1, pages 133-142. ACL. 
Helmut Schmid. 1994. Probabilistic part-of-speech 
tagging using decision trees. In Proceedings of In- 
ternational Con\]erence on New Methods in Lan- 
guage Processing, pages 44-49, Manchester, Eng- 
land. 
Dan Tufts. 1999. Tiered tagging and combined lan- 
guage models classifiers. In Proceedings of Text, 
Speech and Dialogue'99, Mari~nskd LLzn~, Czech 
Republic, Sept. 15-18. 
Jean Vdronis. 1996a. Multext-East 
(Copernicus 106). http://www.lpl.univ- 
aix.fr/projects/multext-east. 
Jean Vdronis. 1996b. Multext-East language- 
specific resources (Copernicus 106). 
http://www.lpl.univ-aix.fr/projects/multext- 
east/MTE2.html. 
101 
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 763?771,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semi-supervised Training for the Averaged Perceptron POS Tagger
Drahom??ra ?johanka? Spoustova? Jan Hajic? Jan Raab Miroslav Spousta
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
{johanka,hajic,raab,spousta}@
ufal.mff.cuni.cz
Abstract
This paper describes POS tagging exper-
iments with semi-supervised training as
an extension to the (supervised) averaged
perceptron algorithm, first introduced for
this task by (Collins, 2002). Experiments
with an iterative training on standard-sized
supervised (manually annotated) dataset
(106 tokens) combined with a relatively
modest (in the order of 108 tokens) un-
supervised (plain) data in a bagging-like
fashion showed significant improvement
of the POS classification task on typo-
logically different languages, yielding bet-
ter than state-of-the-art results for English
and Czech (4.12 % and 4.86 % relative er-
ror reduction, respectively; absolute accu-
racies being 97.44 % and 95.89 %).
1 Introduction
Since 2002, we have seen a renewed interest in
improving POS tagging results for English, and
an inflow of results (initial or improved) for many
other languages. For English, after a relatively big
jump achieved by (Collins, 2002), we have seen
two significant improvements: (Toutanova et al,
2003) and (Shen et al, 2007) pushed the results
by a significant amount each time.1
1In our final comparison, we have also included the re-
sults of (Gime?nez and Ma`rquez, 2004), because it has sur-
passed (Collins, 2002) as well and we have used this tag-
ger in the data preparation phase. See more details below.
Most recently, (Suzuki and Isozaki, 2008) published their
Semi-supervised sequential labelling method, whose results
on POS tagging seem to be optically better than (Shen et al,
2007), but no significance tests were given and the tool is not
available for download, i.e. for repeating the results and sig-
nificance testing. Thus, we compare our results only to the
tools listed above.
Even though an improvement in POS tagging
might be a questionable enterprise (given that its
effects on other tasks, such as parsing or other
NLP problems are less than clear?at least for En-
glish), it is still an interesting problem. Moreover,
the ?ideal?2 situation of having a single algorithm
(and its implementation) for many (if not all) lan-
guages has not been reached yet. We have cho-
sen Collins? perceptron algorithm because of its
simplicity, short training times, and an apparent
room for improvement with (substantially) grow-
ing data sizes (see Figure 1). However, it is clear
that there is usually little chance to get (substan-
tially) more manually annotated data. Thus, we
have been examining the effect of adding a large
monolingual corpus to Collins? perceptron, appro-
priately extended, for two typologically different
languages: English and Czech. It is clear however
that the features (feature templates) that the tag-
gers use are still language-dependent.
One of the goals is also to have a fast im-
plementation for tagging large amounts of data
quickly. We have experimented with various clas-
sifier combination methods, such as those de-
scribed in (Brill and Wu, 1998) or (van Halteren et
al., 2001), and got improved results, as expected.
However, we view this only as a side effect (yet, a
positive one)?our goal was to stay on the turf of
single taggers, which are both the common ground
for competing on tagger accuracy today and also
significantly faster at runtime.3 Nevertheless, we
have found that it is advantageous to use them to
(pre-)tag the large amounts of plain text data dur-
2We mean easy to use for further research on problems
requiring POS tagging, especially multilingual ones.
3And much easier to (re)implement as libraries in proto-
type systems, which is often difficult if not impossible with
other people?s code.
763
Training data size (thousands of tokens)
Accu
racy
 on d
evel
opm
ent d
ata
100 200 300 400 500 600 700 800 900
96.0
96.5
97.0
97.5
98.0
Figure 1: Accuracy of the original averaged per-
ceptron, supervised training on PTB/WSJ (En-
glish)
ing the training phase.
Apart from feeding the perceptron by various
mixtures of manually tagged (?supervised?) and
auto-tagged (?unsupervised?)4 data, we have also
used various feature templates extensively; for ex-
ample, we use lexicalization (with the added twist
of lemmatization, useful especially for Czech, an
inflectionally rich language), ?manual? tag clas-
sification into large classes (again, useful espe-
cially for Czech to avoid the huge, still-to-be-
overcome data sparseness for such a language5),
and sub-lexical features mainly targeted at OOV
words. Inspired i.a. by (Toutanova et al, 2003)
and (Hajic? and Vidova?-Hladka?, 1998), we also use
?lookahead? features (however, we still remain
in the left-to-right HMM world ? in this respect
our solution is closer to the older work of (Hajic?
and Vidova?-Hladka?, 1998) than to (Toutanova et
al., 2003), who uses bidirectional dependencies
to include the right-hand side disambiguated tags,
4For brevity, we will use the terms ?supervised? and ?un-
supervised? data for ?manually annotated? and ?(automat-
ically annotated) plain (raw) text? data, respectively, even
though these adjectives are meant to describe the process of
learning, not the data themselves.
5As (Hajic?, 2004) writes, Czech has 4400 plausible tags,
of which we have observed almost 2000 in the 100M cor-
pus we have used in our experiments. However, only 1100
of them have been found in the manually annotated PDT 2.0
corpus (the corpus on which we have based the supervised
experiments). The situation with word forms (tokens) is even
worse: Czech has about 20M different word forms, and the
OOV rate based on the 1.5M PDT 2.0 data and measured
against the 100M raw corpus is almost 10 %.
which we cannot.)
To summarize, we can describe our system as
follows: it is based on (Votrubec, 2006)?s imple-
mentation of (Collins, 2002), which has been fed
at each iteration by a different dataset consisting
of the supervised and unsupervised part: precisely,
by a concatenation of the manually tagged training
data (WSJ portion of the PTB 3 for English, mor-
phologically disambiguated data from PDT 2.0 for
Czech) and a chunk of automatically tagged unsu-
pervised data. The ?parameters? of the training
process (feature templates, the size of the unsu-
pervised chunks added to the trainer at each itera-
tion, number of iterations, the combination of tag-
gers that should be used in the auto-tagging of the
unsupervised chunk, etc.) have been determined
empirically in a number of experiments on a de-
velopment data set. We should also note that as a
result of these development-data-based optimiza-
tions, no feature pruning has been employed (see
Section 4 for details); adding (even lexical) fea-
tures from the auto-tagged data did not give signif-
icant accuracy improvements (and only made the
training very slow).
The final taggers have surpassed the current
state-of-the-art taggers by significant margins (we
have achieved 4.12 % relative error reduction for
English and 4.86 % for Czech over the best pre-
viously published results, single or combined),
using a single tagger. However, the best En-
glish tagger combining some of the previous state-
of-the-art ones is still ?optically? better (yet not
significantly?see Section 6).
2 The perceptron algorithm
We have used the Morc?e6 tagger (Votrubec, 2006)
as a main component in our experiments. It is a
reimplementation of the averaged perceptron de-
scribed in (Collins, 2002), which uses such fea-
tures that it behaves like an HMM tagger and thus
the standard Viterbi decoding is possible. Collins?
GEN(x) set (a set of possible tags at any given
position) is generated, in our case, using a mor-
phological analyzer for the given language (essen-
6The name ?Morc?e? stands for ?MORfologie C?Es?tiny?
(?Czech morphology?, see (Votrubec, 2006)), since it
has been originally developed for Czech. We keep this
name in this paper as the generic name of the aver-
aged perceptron tagger for the English-language experi-
ments as well. We have used the version available at
http://ufal.mff.cuni.cz/morce/.
764
tially, a dictionary that returns all possible tags7
for an input word form). The transition and out-
put scores for the candidate tags are based on a
large number of binary-valued features and their
weights, which are determined during iterative
training by the averaged perceptron algorithm.
The binary features describe the tag being pre-
dicted and its context. They can be derived from
any information we already have about the text at
the point of decision (respecting the HMM-based
overall setting). Every feature can be true or false
in a given context, so we can consider the true fea-
tures at the current position to be the description
of a tag and its context.
For every feature, the perceptron keeps its
weight coefficient, which is (in its basic version)
an integer number, (possibly) changed at every
training sentence. After its final update, this in-
teger value is stored with the feature to be later
retrieved and used at runtime. Then, the task of
the perceptron algorithm is to sum up all the co-
efficients of true features in a given context. The
result is passed to the Viterbi algorithm as a tran-
sition and output weight for the current state.8 We
can express it as
w(C, T ) =
n?
i=1
?i.?i(C, T ) (1)
where w(C, T ) is the transition weight for tag T
in context C, n is the number of features, ?i is the
weight coefficient of the ith feature and ?i(C, T )
is the evaluation of the ith feature for context C
and tag T . In the averaged perceptron, the val-
ues of every coefficient are added up at each up-
date, which happens (possibly) at each training
sentence, and their arithmetic average is used in-
stead.9 This trick makes the algorithm more re-
sistant to weight oscillations during training (or,
more precisely, at the end of it) and as a result, it
substantially improves its performance.10
7And lemmas, which are then used in some of the fea-
tures. A (high recall, low precision) ?guesser? is used for
OOV words.
8Which identifies unambiguously the corresponding tag.
9Implementation note: care must be taken to avoid inte-
ger overflows, which (at 100 iterations through millions of
sentences) can happen for 32bit integers easily.
10Our experiments have shown that using averaging helps
tremendously, confirming both the theoretical and practical
results of (Collins, 2002). On Czech, using the best feature
set, the difference on the development data set is 95.96 % vs.
95.02 %. Therefore, all the results presented in the following
text use averaging.
The supervised training described in (Collins,
2002) uses manually annotated data for the esti-
mation of the weight coefficients ?. The train-
ing algorithm is very simple?only integer num-
bers (counts and their sums for the averaging) are
updated for each feature at each sentence with
imperfect match(es) found against the gold stan-
dard. Therefore, it can be relatively quickly re-
trained and thus many different feature sets and
other training parameters, such as the number of
iterations, feature thresholds etc. can be con-
sidered and tested. As a result of this tuning,
our (fully supervised) version of the Morc?e tag-
ger gives the best accuracy among all single tag-
gers for Czech and also very good results for En-
glish, being beaten only by the tagger (Shen et al,
2007) (by 0.10 % absolute) and (not significantly)
by (Toutanova et al, 2003).
3 The data
3.1 The ?supervised? data
For English, we use the same data division of Penn
Treebank (PTB) parsed section (Marcus et al,
1994) as all of (Collins, 2002), (Toutanova et al,
2003), (Gime?nez and Ma`rquez, 2004) and (Shen
et al, 2007) do; for details, see Table 1.
data set tokens sentences
train (0-18) 912,344 38,220
dev-test (19-21) 131,768 5,528
eval-test (22-24) 129,654 5,463
Table 1: English supervised data set ? WSJ part
of Penn Treebank 3
For Czech, we use the current standard Prague
Dependency Treebank (PDT 2.0) data sets (Hajic?
et al, 2006); for details, see Table 2.
data set tokens sentences
train 1,539,241 91,049
dev-test 201,651 11,880
eval-test 219,765 13,136
Table 2: Czech supervised data set ? Prague De-
pendency Treebank 2.0
3.2 The ?unsupervised? data
For English, we have processed the North Amer-
ican News Text corpus (Graff, 1995) (without the
765
WSJ section) with the Stanford segmenter and to-
kenizer (Toutanova et al, 2003). For Czech, we
have used the SYN2005 part of Czech National
Corpus (CNC, 2005) (with the original segmenta-
tion and tokenization).
3.3 GEN(x): The morphological analyzers
For English, we perform a very simple morpholog-
ical analysis, which reduces the full PTB tagset to
a small list of tags for each token on input. The re-
sulting list is larger than such a list derived solely
from the PTB/WSJ, but much smaller than a full
list of tags found in the PTB/WSJ.11 The English
morphological analyzer is thus (empirically) opti-
mized for precision while keeping as high recall
as possible (it still overgenerates). It consists of a
small dictionary of exceptions and a small set of
general rules, thus covering also a lot of OOV to-
kens.12
For Czech, the separate morphological analyzer
(Hajic?, 2004) usually precedes the tagger. We use
the version from April 2006 (the same as (Spous-
tova? et al, 2007), who reported the best previous
result on Czech tagging).
4 The perceptron feature sets
The averaged perceptron?s accuracy is determined
(to a large extent) by the set of features used. A
feature set is based on feature templates, i.e. gen-
eral patterns, which are filled in with concrete val-
ues from the training data. Czech and English
are morphosyntactically very different languages,
therefore each of them needs a different set of
feature templates. We have empirically tested
hundreds of feature templates on both languages,
taken over from previous works for direct compar-
ison, inspired by them, or based on a combination
of previous experience, error analysis and linguis-
tic intuition.
In the following sections, we present the best
performing set of feature templates as determined
on the development data set using only the super-
vised training setting; our feature templates have
thus not been influenced nor extended by the un-
supervised data.13
11The full list of tags, as used by (Shen et al, 2007), also
makes the underlying Viterbi algorithm unbearably slow.
12The English morphology tool is also downloadable as a
separate module on the paper?s accompanying website.
13Another set of experiments has shown that there is not,
perhaps surprisingly, a significant gain in doing so.
4.1 English feature templates
The best feature set for English consists of 30 fea-
ture templates. All templates predict the current
tag as a whole. A detailed description of the En-
glish feature templates can be found in Table 3.
Context predicting whole tag
Tags
Previous tag
Previous two tags
First letter of previous tag
Word forms
Current word form
Previous word form
Previous two word forms
Following word form
Following two word forms
Last but one word form
Current word affixes
Prefixes of length 1-9
Suffixes of length 1-9
Current word features
Contains number
Contains dash
Contains upper case letter
Table 3: Feature templates for English
A total of 1,953,463 features has been extracted
from the supervised training data using the tem-
plates from Table 3.
4.2 Czech feature templates
The best feature set for Czech consists of 63 fea-
ture templates. 26 of them predict current tag as
a whole, whereas the rest predicts only some parts
of the current tag separately (e.g., detailed POS,
gender, case) to avoid data sparseness. Such a fea-
ture is true, in an identical context, for several dif-
ferent tags belonging to the same class (e.g., shar-
ing a locative case). The individual grammatical
categories used for such classing have been cho-
sen on both linguistic grounds (POS, detailed fine-
grained POS) and also such categories have been
used which contribute most to the elimination of
the tagger errors (based on an extensive error anal-
ysis of previous results, the detailed description of
which can be found in (Votrubec, 2006)).
Several features can look ahead (to the right
of the current position) - apart from the obvious
word form, which is unambiguous, we have used
(in case of ambiguity) a random tag and lemma of
the first position to the right from the current po-
sition which might be occupied with a verb (based
on dictionary and the associated morphological
guesser restrictions).
A total of 8,440,467 features has been extracted
from the supervised training data set. A detailed
description is included in the distribution down-
loadable from the Morc?e website.
766
5 The (un)supervised training setup
We have extended the averaged perceptron setup
in the following way: the training algorithm is
fed, in each iteration, by a concatenation of the
supervised data (the manually tagged corpus) and
the automatically pre-tagged unsupervised data,
different for each iteration (in this order). In
other words, the training algorithm proper does
not change at all: it is the data and their selection
(including the selection of the way they are auto-
matically tagged) that makes all the difference.
The following ?parameters? of the (unsuper-
vised part of the) data selection had to be deter-
mined experimentally:
? the tagging process for tagging the selected
data
? the selection mechanism (sequential or ran-
dom with/without replacement)
? the size to use for each iteration
? and the use and order of concatenation with
the manually tagged data.
We have experimented with various settings to
arrive at the best performing configuration, de-
scribed below. In each subsection, we compare
the result of our ,,winning? configuration with re-
sults of the experiments which have the selected
attributes omitted or changed; everything is mea-
sured on the development data set.
5.1 Tagging the plain data
In order to simulate the labeled training events,
we have tagged the unsupervised data simply by
a combination of the best available taggers. For
practical reasons (to avoid prohibitive training
times), we have tagged all the data in advance, i.e.
no re-tagging is performed between iterations.
The setup for the combination is as follows (the
idea is simplified from (Spoustova? et al, 2007)
where it has been used in a more complex setting):
1. run N different taggers independently;
2. join the results on each position in the data
from the previous step ? each token thus
ends up with between 1 and N tags, a union
of the tags output by the taggers at that posi-
tion;
3. do final disambiguation (by a single tag-
ger14).
Tagger Accuracy
Morc?e 97.21
Shen 97.33
Combination 97.44
Table 4: Dependence on the tagger(s) used to tag
the additional plain text data (English)16
Table 4 illustrates why it is advantageous to go
through this (still)16 complicated setup against a
single-tagger bootstrapping mechanism, which al-
ways uses the same tagger for tagging the unsu-
pervised data.
For both English and Czech, the selection of
taggers, the best combination and the best over-
all setup has been optimized on the development
data set. A bit surprisingly, the final setup is very
similar for both languages (two taggers to tag the
data in Step 1, and a third one to finish it up).
For English, we use three state-of-the-art tag-
gers: the taggers of (Toutanova et al, 2003) and
(Shen et al, 2007) in Step 1, and the SVM tag-
ger (Gime?nez and Ma`rquez, 2004) in Step 3. We
run the taggers with the parameters which were
shown to be the best in the corresponding papers.
The SVM tagger needed to be adapted to accept
the (reduced) list of possible tags.17
For Czech, we use the Feature-based tagger
(Hajic?, 2004) and the Morc?e tagger (with the new
feature set as described in section 4) in Step 1, and
an HMM tagger (Krbec, 2005) in Step 3. This
combination outperforms the results in (Spoustova?
et al, 2007) by a small margin.
5.2 Selection mechanism for the plain data
We have found that it is better to feed the training
with different chunks of the unsupervised data at
each iteration. We have then experimented with
14This tagger (possibly different from any of theN taggers
from Step 1) runs as usual, but it is given a minimal list of (at
most N ) tags that come from Step 2 only.
15?Accuracy? means accuracy of the semi-supervised
method using this tagger for pre-tagging the unsupervised
data, not the accuracy of the tagger itself.
16In fact, we have experimented with other tagger
combinations and configurations as well?with the TnT
(Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTag-
ger (Schmid, 1994), with or without the Morc?e tagger in the
pack; see below for the winning combination.
17This patch is available on the paper?s website (see Sec-
tion 7).
767
three methods of unsupervised data selection, i.e.
generating the unsupervised data chunks for each
training iteration from the ,,pool? of sentences.
These methods are: simple sequential chopping,
randomized data selection with replacement and
randomized selection without replacement. Ta-
ble 5 demonstrates that there is practically no dif-
ference in the results. Thus, we use the sequential
chopping mechanism, mainly for its simplicity.
Method of data selection English Czech
Sequential chopping 97.44 96.21
Random without replacement 97.44 96.20
Random with replacement 97.44 96.21
Table 5: Unsupervised data selection
5.3 Joining the data
We have experimented with various sizes of the
unsupervised parts (from 500k tokens to 5M) and
also with various numbers of iterations. The best
results (on the development data set) have been
achieved with the unsupervised chunks containing
approx. 4 million tokens for English and 1 million
tokens for Czech. Each training process consists
of (at most) 100 iterations (Czech) or 50 iterations
(English); therefore, for the 50 (100) iterations we
needed only about 200,000,000 (100,000,000) to-
kens of raw texts. The best development data set
results have been (with the current setup) achieved
on the 44th (English) and 33th (Czech) iteration.
The development data set has been also used to
determine the best way to ?merge? the manually
labeled data (the PTB/WSJ and the PDT 2.0 train-
ing data) and the unsupervised parts of the data.
Given the properties of the perceptron algorithm,
it is not too surprising that the best solution is to
put (the full size of) the manually labeled data first,
followed by the (four) million-token chunk of the
automatically tagged data (different data in each
chunk but of the same size for each iteration). It
corresponds to the situation when the trainer is pe-
riodically ?returned to the right track? by giving it
the gold standard data time to time.
Figure 2 (English) and especially Figure 3
(Czech) demonstrate the perceptron behavior in
cases where the supervised data precede the un-
supervised data only in selected iterations. A sub-
set of these development results is also present in
Table 6.
0 10 20 30 40 509
7.20
97.2
5
97.3
0
97.3
5
97.4
0
Iteration
Accu
racy
 on d
evel
opm
ent d
ata
Every iterationEvery 4th iterationEvery 8th iterationEvery 16th iterationOnce at the beginning    No supervised data
Figure 2: Dependence on the inclusion of the su-
pervised training data (English)
English Czech
No supervised data 97.37 95.88
Once at the beginning 97.40 96.00
Every training iteration 97.44 96.21
Table 6: Dependence on the inclusion of the su-
pervised training data
5.4 The morphological analyzers and the
perceptron feature templates
The whole experiment can be performed with
the original perceptron feature set described in
(Collins, 2002) instead of the feature set described
in this article. The results are compared in Table 7
(for English only).
Also, for English it is not necessary to use our
morphological analyzer described in section 3.3
(other variants are to use the list of tags derived
solely from the WSJ training data or to give each
token the full list of tags found in WSJ). It is
practically impossible to perform the unsupervised
training with the full list of tags (it would take sev-
eral years instead of several days with the default
setup), thus we compare only the results with mor-
phological analyzer to the results with the list of
tags derived from the training data, see Table 8.
It can be expected (some approximated exper-
iments were performed) that the results with the
full list of tags would be very similar to the results
with the morphological analyzer, i.e. the morpho-
logical analyzer is used mainly for technical rea-
sons. Our expectations are based mainly (but not
768
0 10 20 30 40 50
95.6
95.7
95.8
95.9
96.0
96.1
96.2
Iteration
Accu
racy
 on d
evel
opm
ent d
ata
Every iterationEvery 4th iterationEvery 8th iterationEvery 16th iterationOnce at the beginning    No supervised data
Figure 3: Dependence on the inclusion of the su-
pervised training data (Czech)
only) on the supervised training results, where the
performance of the taggers using the morpholog-
ical analyzer output and using the full list of tags
are nearly the same, see Table 9.
Feature set Accuracy
Collins? 97.38
Our?s 97.44
Table 7: Dependence on the feature set used by the
perceptron algorithm (English)
GEN(x) Accuracy
List of tags derived from train 97.13
Our morphological analyzer 97.44
Table 8: Dependence on the GEN(x)
6 Results
In Tables 10 and 11, the main results (on the eval-
test data sets) are summarized. The state-of-the
art taggers are using feature sets discribed in the
corresponding articles ((Collins, 2002), (Gime?nez
and Ma`rquez, 2004), (Toutanova et al, 2003) and
(Shen et al, 2007)), Morc?e supervised and Morc?e
semi-supervised are using feature set desribed in
section 4.
For significance tests, we have used the paired
Wilcoxon signed rank test as implemented in the
R package (R Development Core Team, 2008)
GEN(x) Accuracy
List of tags derived from train 95.89
Our morphological analyzer 97.17
Full tagset 97.15
Table 9: Supervised training results: dependence
on the GEN(x)
Tagger accuracy
Collins 97.07 %
SVM 97.16 %
Stanford 97.24 %
Shen 97.33 %
Morc?e supervised 97.23 %
combination 97.48 %
Morc?e semi-supervised 97.44 %
Table 10: Evaluation of the English taggers
Tagger accuracy
Feature-based 94.04 %
HMM 94.82 %
Morc?e supervised 95.67 %
combination 95.70 %
Morc?e semi-supervised 95.89 %
Table 11: Evaluation of the Czech taggers
in wilcox.test(), dividing the data into 100
chunks (data pairs).
6.1 English
The combination of the three existing English tag-
gers seems to be best, but it is not significantly
better than our semi-supervised approach.
The combination is significantly better than
(Shen et al, 2007) at a very high level, but more
importantly, Shen?s results (currently represent-
ing the replicable state-of-the-art in POS tagging)
have been significantly surpassed also by the semi-
supervised Morc?e (at the 99 % confidence level).
In addition, the semi-supervised Morc?e per-
forms (on single CPU and development data set)
77 times faster than the combination and 23 times
faster than (Shen et al, 2007).
6.2 Czech
The best results (Table 11) are statistically signif-
icantly better than the previous results: the semi-
supervised Morc?e is significantly better than both
769
the combination and the supervised (original) vari-
ant at a very high level.
7 Download
We decided to publish our system for wide use un-
der the name COMPOST (Common POS Tagger).
All the programs, patches and data files are avail-
able at the website http://ufal.mff.cuni.cz/compost
under either the original data provider license, or
under the usual GNU General Public License, un-
less they are available from the widely-known and
easily obtainable sources (such as the LDC, in
which case pointers are provided on the download
website).
The Compost website also contains easy-to-run
Linux binaries of the best English and Czech sin-
gle taggers (based on the Morc?e technology) as de-
scribed in Section 6.
8 Conclusion and Future Work
We have shown that the ?right?18 mixture of su-
pervised and unsupervised (auto-tagged) data can
significantly improve tagging accuracy of the av-
eraged perceptron on two typologically different
languages (English and Czech), achieving the best
known accuracy to date.
To determine what is the contribution of the in-
dividual ?dimensions? of the system setting, as
described in Sect. 5, we have performed exper-
iments fixing all but one of the dimensions, and
compared their contribution (or rather, their loss
when compared to the best ?mix? overall). For
English, we found that excluding the state-of-the-
art-tagger (in fact, a carefully selected combina-
tion of taggers yielding significantly higher qual-
ity than any of them has) drops the resulting ac-
curacy the most (0.2 absolute). Significant yet
smaller drop (less than 0.1 percent) appears when
the manually tagged portion of the data is not used
or used only once (or infrequently) in the input
to the perceptron?s learner. The difference in us-
ing various feature templates (yet al largely sim-
ilar to what state-of-the-art taggers currently use)
is not significant. Similarly, the way the unsuper-
vised data is selected plays no role, either; this dif-
fers from the bagging technique (Breiman, 1996)
where it is significant. For Czech, the drop in ac-
curacy appears in all dimensions, except the unsu-
pervised data selection one. We have used novel
features inspired by previous work but not used in
18As empirically determined on the development data set.
the standard perceptron setting yet (linguistically
motivated tag classes in features, lookahead fea-
tures). Interestingly, the resulting tagger is better
than even a combination of the previous state-of-
the-art taggers (for English, this comparison is in-
conclusive).
We are working now on parallelization of the
perceptron training, which seems to be possible
(based i.a. on small-scale preliminary experiments
with only a handful of parallel processes and
specific data sharing arrangements among them).
This would further speed up the training phase, not
just as a nice bonus per se, but it would also allow
for a semi-automated feature template selection,
avoiding the (still manual) feature template prepa-
ration for individual languages. This would in turn
facilitate one of our goals to (publicly) provide
single-implementation, easy-to-maintain state-of-
the-art tagging tools for as many languages as pos-
sible (we are currently preparing Dutch, Slovak
and several other languages).19
Another area of possible future work is more
principled tag classing for languages with large
tagsets (in the order of 103), and/or adding
syntactically-motivated features; it has helped
Czech tagging accuracy even when only the ?in-
trospectively? defined classes have been added. It
is an open question if a similar approach helps
English as well (certain grammatical categories
can be generalized from the current WSJ tagset as
well, such as number, degree of comparison, 3rd
person present tense).
Finally, it would be nice to merge some of the
approaches by (Toutanova et al, 2003) and (Shen
et al, 2007) with the ideas of semi-supervised
learning introduced here, since they seem orthog-
onal in at least some aspects (e.g., to replace the
rudimentary lookahead features with full bidirec-
tionality).
Acknowledgments
The research described here was supported by the
projects MSM0021620838 and LC536 of Ministry
of Education, Youth and Sports of the Czech Re-
public, GA405/09/0278 of the Grant Agency of the
Czech Republic and 1ET101120503 of Academy
of Sciences of the Czech Republic.
19Available soon also on the website.
770
References
Thorsten Brants. 2000. TnT - a Statistical Part-of-
Speech Tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference, pages
224?231, Seattle, WA. ACL.
Leo Breiman. 1996. Bagging predictors. Mach.
Learn., 24(2):123?140.
Eric Brill and Jun Wu. 1998. Classifier Combination
for Improved Lexical Disambiguation. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, pages 191?195, Montreal, Que-
bec, Canada. Association for Computational Lin-
guistics.
CNC, 2005. Czech National Corpus ? SYN2005. In-
stitute of Czech National Corpus, Faculty of Arts,
Charles University, Prague, Czech Republic.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, volume 10,
pages 1?8, Philadelphia, PA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A General POS Tagger Generator Based on Support
Vector Machines. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation, pages 43?46, Lisbon, Portugal.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium, Cat. LDC95T21,
Philadelphia, PA.
Jan Hajic? and Barbora Vidova?-Hladka?. 1998. Tag-
ging Inflective Languages: Prediction of Morpho-
logical Categories for a Rich, Structured Tagset.
In Proceedings of the 17th international conference
on Computational linguistics, pages 483?490. Mon-
treal, Quebec, Canada.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum, Prague.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank
v2.0, CDROM, LDC Cat. No. LDC2006T01. Lin-
guistic Data Consortium, Philadelphia, PA.
Pavel Krbec. 2005. Language Modelling for Speech
Recognition of Czech. Ph.D. thesis, UK MFF,
Prague, Malostranske? na?me?st?? 25, 118 00 Praha 1.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1994. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
R Development Core Team, 2008. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the 1st EMNLP, pages 133?142, New Brunswick,
NJ. ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, page 9pp., Manchester, GB.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided Learning for Bidirectional Sequence Classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan
Votrubec, Pavel Krbec, and Pavel Kve?ton?. 2007.
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech. In Proceedings
of the Workshop on Balto-Slavonic Natural Lan-
guage Processing 2007, pages 67?74, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665?673, Columbus, Ohio, June.
Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In NAACL ?03: Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 173?180, Edmonton,
Canada. Association for Computational Linguistics.
Hans van Halteren, Walter Daelemans, and Jakub Za-
vrel. 2001. Improving accuracy in word class
tagging through the combination of machine learn-
ing systems. Computational Linguistics, 27(2):199?
229.
Jan Votrubec. 2006. Morphological Tagging Based
on Averaged Perceptron. In WDS?06 Proceedings of
Contributed Papers, pages 191?195, Prague, Czech
Republic. Matfyzpress, Charles University.
771
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 523?530, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Non-projective Dependency Parsing using Spanning Tree Algorithms
Ryan McDonald Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
{ryantm,pereira}@cis.upenn.edu
Kiril Ribarov Jan Hajic?
Institute of Formal and Applied Linguistics
Charles University
{ribarov,hajic}@ufal.ms.mff.cuni.cz
Abstract
We formalize weighted dependency pars-
ing as searching for maximum spanning
trees (MSTs) in directed graphs. Using
this representation, the parsing algorithm
of Eisner (1996) is sufficient for search-
ing over all projective trees in O(n3) time.
More surprisingly, the representation is
extended naturally to non-projective pars-
ing using Chu-Liu-Edmonds (Chu and
Liu, 1965; Edmonds, 1967) MST al-
gorithm, yielding an O(n2) parsing al-
gorithm. We evaluate these methods
on the Prague Dependency Treebank us-
ing online large-margin learning tech-
niques (Crammer et al, 2003; McDonald
et al, 2005) and show that MST parsing
increases efficiency and accuracy for lan-
guages with non-projective dependencies.
1 Introduction
Dependency parsing has seen a surge of inter-
est lately for applications such as relation extrac-
tion (Culotta and Sorensen, 2004), machine trans-
lation (Ding and Palmer, 2005), synonym genera-
tion (Shinyama et al, 2002), and lexical resource
augmentation (Snow et al, 2004). The primary
reasons for using dependency structures instead of
more informative lexicalized phrase structures is
that they are more efficient to learn and parse while
still encoding much of the predicate-argument infor-
mation needed in applications.
root John hit the ball with the bat
Figure 1: An example dependency tree.
Dependency representations, which link words to
their arguments, have a long history (Hudson, 1984).
Figure 1 shows a dependency tree for the sentence
John hit the ball with the bat. We restrict ourselves
to dependency tree analyses, in which each word de-
pends on exactly one parent, either another word or a
dummy root symbol as shown in the figure. The tree
in Figure 1 is projective, meaning that if we put the
words in their linear order, preceded by the root, the
edges can be drawn above the words without cross-
ings, or, equivalently, a word and its descendants
form a contiguous substring of the sentence.
In English, projective trees are sufficient to ana-
lyze most sentence types. In fact, the largest source
of English dependency trees is automatically gener-
ated from the Penn Treebank (Marcus et al, 1993)
and is by convention exclusively projective. How-
ever, there are certain examples in which a non-
projective tree is preferable. Consider the sentence
John saw a dog yesterday which was a Yorkshire Ter-
rier. Here the relative clause which was a Yorkshire
Terrier and the object it modifies (the dog) are sep-
arated by an adverb. There is no way to draw the
dependency tree for this sentence in the plane with
no crossing edges, as illustrated in Figure 2. In lan-
guages with more flexible word order than English,
such as German, Dutch and Czech, non-projective
dependencies are more frequent. Rich inflection
systems reduce reliance on word order to express
523
root John saw a dog yesterday which was a Yorkshire Terrier
root O to nove? ve?ts?inou nema? ani za?jem a taky na to ve?ts?inou nema? pen??ze
He is mostly not even interested in the new things and in most cases, he has no money for it either.
Figure 2: Non-projective dependency trees in English and Czech.
grammatical relations, allowing non-projective de-
pendencies that we need to represent and parse ef-
ficiently. A non-projective example from the Czech
Prague Dependency Treebank (Hajic? et al, 2001) is
also shown in Figure 2.
Most previous dependency parsing models have
focused on projective trees, including the work
of Eisner (1996), Collins et al (1999), Yamada and
Matsumoto (2003), Nivre and Scholz (2004), and
McDonald et al (2005). These systems have shown
that accurate projective dependency parsers can be
automatically learned from parsed data. However,
non-projective analyses have recently attracted some
interest, not only for languages with freer word order
but also for English. In particular, Wang and Harper
(2004) describe a broad coverage non-projective
parser for English based on a hand-constructed con-
straint dependency grammar rich in lexical and syn-
tactic information. Nivre and Nilsson (2005) pre-
sented a parsing model that allows for the introduc-
tion of non-projective edges into dependency trees
through learned edge transformations within their
memory-based parser. They test this system on
Czech and show improved accuracy relative to a pro-
jective parser. Our approach differs from those ear-
lier efforts in searching optimally and efficiently the
full space of non-projective trees.
The main idea of our method is that dependency
parsing can be formalized as the search for a maxi-
mum spanning tree in a directed graph. This formal-
ization generalizes standard projective parsing mod-
els based on the Eisner algorithm (Eisner, 1996) to
yield efficient O(n2) exact parsing methods for non-
projective languages like Czech. Using this span-
ning tree representation, we extend the work of Mc-
Donald et al (2005) on online large-margin discrim-
inative training methods to non-projective depen-
dencies.
The present work is related to that of Hirakawa
(2001) who, like us, reduces the problem of depen-
dency parsing to spanning tree search. However, his
parsing method uses a branch and bound algorithm
that is exponential in the worst case, even though
it appears to perform reasonably in limited experi-
ments. Furthermore, his work does not adequately
address learning or measure parsing accuracy on
held-out data.
Section 2 describes an edge-based factorization
of dependency trees and uses it to equate depen-
dency parsing to the problem of finding maximum
spanning trees in directed graphs. Section 3 out-
lines the online large-margin learning framework
used to train our dependency parsers. Finally, in
Section 4 we present parsing results for Czech. The
trees in Figure 1 and Figure 2 are untyped, that
is, edges are not partitioned into types representing
additional syntactic information such as grammati-
cal function. We study untyped dependency trees
mainly, but edge types can be added with simple ex-
tensions to the methods discussed here.
2 Dependency Parsing and Spanning Trees
2.1 Edge Based Factorization
In what follows, x = x1 ? ? ? xn represents a generic
input sentence, and y represents a generic depen-
dency tree for sentence x. Seeing y as the set of tree
edges, we write (i, j) ? y if there is a dependency
in y from word xi to word xj .
In this paper we follow a common method of fac-
toring the score of a dependency tree as the sum of
the scores of all edges in the tree. In particular, we
define the score of an edge to be the dot product be-
524
tween a high dimensional feature representation of
the edge and a weight vector,
s(i, j) = w ? f(i, j)
Thus the score of a dependency tree y for sentence
x is,
s(x,y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? f(i, j)
Assuming an appropriate feature representation as
well as a weight vector w, dependency parsing is the
task of finding the dependency tree y with highest
score for a given sentence x.
For the rest of this section we assume that the
weight vector w is known and thus we know the
score s(i, j) of each possible edge. In Section 3 we
present a method for learning the weight vector.
2.2 Maximum Spanning Trees
We represent the generic directed graph G = (V,E)
by its vertex set V = {v1, . . . , vn} and set E ? [1 :
n]? [1 : n] of pairs (i, j) of directed edges vi ? vj .
Each such edge has a score s(i, j). Since G is di-
rected, s(i, j) does not necessarily equal s(j, i). A
maximum spanning tree (MST) of G is a tree y ? E
that maximizes the value
?
(i,j)?y s(i, j) such that
every vertex in V appears in y. The maximum pro-
jective spanning tree of G is constructed similarly
except that it can only contain projective edges rel-
ative to some total order on the vertices of G. The
MST problem for directed graphs is also known as
the maximum arborescence problem.
For each sentence x we define the directed graph
Gx = (Vx, Ex) given by
Vx = {x0 = root, x1, . . . , xn}
Ex = {(i, j) : i 6= j, (i, j) ? [0 : n] ? [1 : n]}
That is, Gx is a graph with the sentence words and
the dummy root symbol as vertices and a directed
edge between every pair of distinct words and from
the root symbol to every word. It is clear that de-
pendency trees for x and spanning trees for Gx co-
incide, since both kinds of trees are required to be
rooted at the dummy root and reach all the words
in the sentence. Hence, finding a (projective) depen-
dency tree with highest score is equivalent to finding
a maximum (projective) spanning tree in Gx.
Chu-Liu-Edmonds(G, s)
Graph G = (V, E)
Edge weight function s : E ? R
1. Let M = {(x?, x) : x ? V, x? = arg maxx? s(x?, x)}
2. Let GM = (V, M)
3. If GM has no cycles, then it is an MST: return GM
4. Otherwise, find a cycle C in GM
5. Let GC = contract(G, C, s)
6. Let y = Chu-Liu-Edmonds(GC , s)
7. Find a vertex x ? C s. t. (x?, x) ? y, (x??, x) ? C
8. return y ? C ? {(x??, x)}
contract(G = (V, E), C, s)
1. Let GC be the subgraph of G excluding nodes in C
2. Add a node c to GC representing cycle C
3. For x ? V ? C : ?x??C(x?, x) ? E
Add edge (c, x) to GC with
s(c, x) = maxx??C s(x?, x)
4. For x ? V ? C : ?x??C(x, x?) ? E
Add edge (x, c) to GC with
s(x, c) = maxx??C [s(x, x?) ? s(a(x?), x?) + s(C)]
where a(v) is the predecessor of v in C
and s(C) = Pv?C s(a(v), v)
5. return GC
Figure 3: Chu-Liu-Edmonds algorithm for finding
maximum spanning trees in directed graphs.
2.2.1 Non-projective Trees
To find the highest scoring non-projective tree we
simply search the entire space of spanning trees with
no restrictions. Well-known algorithms exist for the
less general case of finding spanning trees in undi-
rected graphs (Cormen et al, 1990).
Efficient algorithms for the directed case are less
well known, but they exist. We will use here the
Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967), sketched in Figure 3 follow-
ing Leonidas (2003). Informally, the algorithm has
each vertex in the graph greedily select the incoming
edge with highest weight. If a tree results, it must be
the maximum spanning tree. If not, there must be a
cycle. The procedure identifies a cycle and contracts
it into a single vertex and recalculates edge weights
going into and out of the cycle. It can be shown that
a maximum spanning tree on the contracted graph is
equivalent to a maximum spanning tree in the orig-
inal graph (Leonidas, 2003). Hence the algorithm
can recursively call itself on the new graph. Naively,
this algorithm runs in O(n3) time since each recur-
sive call takes O(n2) to find the highest incoming
edge for each word and to contract the graph. There
are at most O(n) recursive calls since we cannot
contract the graph more then n times. However,
525
Tarjan (1977) gives an efficient implementation of
the algorithm with O(n2) time complexity for dense
graphs, which is what we need here.
To find the highest scoring non-projective tree for
a sentence, x, we simply construct the graph Gx
and run it through the Chu-Liu-Edmonds algorithm.
The resulting spanning tree is the best non-projective
dependency tree. We illustrate here the application
of the Chu-Liu-Edmonds algorithm to dependency
parsing on the simple example x = John saw Mary,
with directed graph representation Gx,
root
saw
John Mary
10
9
9
30
3020
3
0
11
The first step of the algorithm is to find, for each
word, the highest scoring incoming edge
root
saw
John Mary30
3020
If the result were a tree, it would have to be the
maximum spanning tree. However, in this case we
have a cycle, so we will contract it into a single node
and recalculate edge weights according to Figure 3.
root
saw
John Mary
40
9
30
31
wjs
The new vertex wjs represents the contraction of
vertices John and saw. The edge from wjs to Mary
is 30 since that is the highest scoring edge from any
vertex in wjs. The edge from root into wjs is set to
40 since this represents the score of the best span-
ning tree originating from root and including only
the vertices in wjs. The same leads to the edge
from Mary to wjs. The fundamental property of the
Chu-Liu-Edmonds algorithm is that an MST in this
graph can be transformed into an MST in the orig-
inal graph (Leonidas, 2003). Thus, we recursively
call the algorithm on this graph. Note that we need
to keep track of the real endpoints of the edges into
and out of wjs for reconstruction later. Running the
algorithm, we must find the best incoming edge to
all words
root
saw
John Mary
40
30
wjs
This is a tree and thus the MST of this graph. We
now need to go up a level and reconstruct the graph.
The edge from wjs to Mary originally was from the
word saw, so we include that edge. Furthermore, the
edge from root to wjs represented a tree from root to
saw to John, so we include all those edges to get the
final (and correct) MST,
root
saw
John Mary
10
3030
A possible concern with searching the entire space
of spanning trees is that we have not used any syn-
tactic constraints to guide the search. Many lan-
guages that allow non-projectivity are still primarily
projective. By searching all possible non-projective
trees, we run the risk of finding extremely bad trees.
We address this concern in Section 4.
2.2.2 Projective Trees
It is well known that projective dependency pars-
ing using edge based factorization can be handled
with the Eisner algorithm (Eisner, 1996). This al-
gorithm has a runtime of O(n3) and has been em-
ployed successfully in both generative and discrimi-
native parsing models (Eisner, 1996; McDonald et
al., 2005). Furthermore, it is trivial to show that
the Eisner algorithm solves the maximum projective
spanning tree problem.
The Eisner algorithm differs significantly from
the Chu-Liu-Edmonds algorithm. First of all, it is a
bottom-up dynamic programming algorithm as op-
posed to a greedy recursive one. A bottom-up al-
gorithm is necessary for the projective case since it
must maintain the nested structural constraint, which
is unnecessary for the non-projective case.
2.3 Dependency Trees as MSTs: Summary
In the preceding discussion, we have shown that nat-
ural language dependency parsing can be reduced to
finding maximum spanning trees in directed graphs.
This reduction results from edge-based factoriza-
tion and can be applied to projective languages with
526
the Eisner parsing algorithm and non-projective lan-
guages with the Chu-Liu-Edmonds maximum span-
ning tree algorithm. The only remaining problem is
how to learn the weight vector w.
A major advantage of our approach over other
dependency parsing models is its uniformity and
simplicity. By viewing dependency structures as
spanning trees, we have provided a general frame-
work for parsing trees for both projective and non-
projective languages. Furthermore, the resulting
parsing algorithms are more efficient than lexi-
calized phrase structure approaches to dependency
parsing, allowing us to search the entire space with-
out any pruning. In particular the non-projective
parsing algorithm based on the Chu-Liu-Edmonds
MST algorithm provides true non-projective pars-
ing. This is in contrast to other non-projective meth-
ods, such as that of Nivre and Nilsson (2005), who
implement non-projectivity in a pseudo-projective
parser with edge transformations. This formulation
also dispels the notion that non-projective parsing is
?harder? than projective parsing. In fact, it is eas-
ier since non-projective parsing does not need to en-
force the non-crossing constraint of projective trees.
As a result, non-projective parsing complexity is just
O(n2), against the O(n3) complexity of the Eis-
ner dynamic programming algorithm, which by con-
struction enforces the non-crossing constraint.
3 Online Large Margin Learning
In this section, we review the work of McDonald et
al. (2005) for online large-margin dependency pars-
ing. As usual for supervised learning, we assume a
training set T = {(xt,yt)}Tt=1, consisting of pairs
of a sentence xt and its correct dependency tree yt.
In what follows, dt(x) denotes the set of possible
dependency trees for sentence x.
The basic idea is to extend the Margin Infused
Relaxed Algorithm (MIRA) (Crammer and Singer,
2003; Crammer et al, 2003) to learning with struc-
tured outputs, in the present case dependency trees.
Figure 4 gives pseudo-code for the MIRA algorithm
as presented by McDonald et al (2005). An on-
line learning algorithm considers a single training
instance at each update to w. The auxiliary vector
v accumulates the successive values of w, so that the
final weight vector is the average of the weight vec-
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. min
?
?
?
w(i+1) ? w(i)
?
?
?
s.t. s(xt, yt) ? s(xt, y?) ? L(yt, y?), ?y? ? dt(xt)
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 4: MIRA learning algorithm.
tors after each iteration. This averaging effect has
been shown to help overfitting (Collins, 2002).
On each update, MIRA attempts to keep the new
weight vector as close as possible to the old weight
vector, subject to correctly classifying the instance
under consideration with a margin given by the loss
of the incorrect classifications. For dependency
trees, the loss of a tree is defined to be the number
of words with incorrect parents relative to the correct
tree. This is closely related to the Hamming loss that
is often used for sequences (Taskar et al, 2003).
For arbitrary inputs, there are typically exponen-
tially many possible parses and thus exponentially
many margin constraints in line 4 of Figure 4.
3.1 Single-best MIRA
One solution for the exponential blow-up in number
of trees is to relax the optimization by using only the
single margin constraint for the tree with the highest
score, s(x,y). The resulting online update (to be
inserted in Figure 4, line 4) would then be:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
where y? = arg maxy? s(xt,y?)
McDonald et al (2005) used a similar update with
k constraints for the k highest-scoring trees, and
showed that small values of k are sufficient to
achieve the best accuracy for these methods. How-
ever, here we stay with a single best tree because k-
best extensions to the Chu-Liu-Edmonds algorithm
are too inefficient (Hou, 1996).
This model is related to the averaged perceptron
algorithm of Collins (2002). In that algorithm, the
single highest scoring tree (or structure) is used to
update the weight vector. However, MIRA aggres-
sively updates w to maximize the margin between
527
the correct tree and the highest scoring tree, which
has been shown to lead to increased accuracy.
3.2 Factored MIRA
It is also possible to exploit the structure of the out-
put space and factor the exponential number of mar-
gin constraints into a polynomial number of local
constraints (Taskar et al, 2003; Taskar et al, 2004).
For the directed maximum spanning tree problem,
we can factor the output by edges to obtain the fol-
lowing constraints:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(l, j) ? s(k, j) ? 1
?(l, j) ? yt, (k, j) /? yt
This states that the weight of the correct incoming
edge to the word xj and the weight of all other in-
coming edges must be separated by a margin of 1.
It is easy to show that when all these constraints
are satisfied, the correct spanning tree and all incor-
rect spanning trees are separated by a score at least
as large as the number of incorrect incoming edges.
This is because the scores for all the correct arcs can-
cel out, leaving only the scores for the errors causing
the difference in overall score. Since each single er-
ror results in a score increase of at least 1, the entire
score difference must be at least the number of er-
rors. For sequences, this form of factorization has
been called local lattice preference (Crammer et al,
2004). Let n be the number of nodes in graph Gx.
Then the number of constraints is O(n2), since for
each node we must maintain n ? 1 constraints.
The factored constraints are in general more re-
strictive than the original constraints, so they may
rule out the optimal solution to the original prob-
lem. McDonald et al (2005) examines briefly fac-
tored MIRA for projective English dependency pars-
ing, but for that application, k-best MIRA performs
as well or better, and is much faster to train.
4 Experiments
We performed experiments on the Czech Prague De-
pendency Treebank (PDT) (Hajic?, 1998; Hajic? et al,
2001). We used the predefined training, develop-
ment and testing split of this data set. Furthermore,
we used the automatically generated POS tags that
are provided with the data. Czech POS tags are very
complex, consisting of a series of slots that may or
may not be filled with some value. These slots rep-
resent lexical and grammatical properties such as
standard POS, case, gender, and tense. The result
is that Czech POS tags are rich in information, but
quite sparse when viewed as a whole. To reduce
sparseness, our features rely only on the reduced
POS tag set from Collins et al (1999). The num-
ber of features extracted from the PDT training set
was 13, 450, 672, using the feature set outlined by
McDonald et al (2005).
Czech has more flexible word order than English
and as a result the PDT contains non-projective de-
pendencies. On average, 23% of the sentences in
the training, development and test sets have at least
one non-projective dependency. However, less than
2% of total edges are actually non-projective. There-
fore, handling non-projective edges correctly has a
relatively small effect on overall accuracy. To show
the effect more clearly, we created two Czech data
sets. The first, Czech-A, consists of the entire PDT.
The second, Czech-B, includes only the 23% of sen-
tences with at least one non-projective dependency.
This second set will allow us to analyze the effec-
tiveness of the algorithms on non-projective mate-
rial. We compared the following systems:
1. COLL1999: The projective lexicalized phrase-structure
parser of Collins et al (1999).
2. N&N2005: The pseudo-projective parser of Nivre and
Nilsson (2005).
3. McD2005: The projective parser of McDonald et al
(2005) that uses the Eisner algorithm for both training and
testing. This system uses k-best MIRA with k=5.
4. Single-best MIRA: In this system we use the Chu-Liu-
Edmonds algorithm to find the best dependency tree for
Single-best MIRA training and testing.
5. Factored MIRA: Uses the quadratic set of constraints
based on edge factorization as described in Section 3.2.
We use the Chu-Liu-Edmonds algorithm to find the best
tree for the test data.
4.1 Results
Results are shown in Table 1. There are two main
metrics. The first and most widely recognized is Ac-
curacy, which measures the number of words that
correctly identified their parent in the tree. Complete
measures the number of sentences in which the re-
sulting tree was completely correct.
Clearly, there is an advantage in using the Chu-
Liu-Edmonds algorithm for Czech dependency pars-
528
Czech-A Czech-B
Accuracy Complete Accuracy Complete
COLL1999 82.8 - - -
N&N2005 80.0 31.8 - -
McD2005 83.3 31.3 74.8 0.0
Single-best MIRA 84.1 32.2 81.0 14.9
Factored MIRA 84.4 32.3 81.5 14.3
Table 1: Dependency parsing results for Czech. Czech-B is the subset of Czech-A containing only sentences
with at least one non-projective dependency.
ing. Even though less than 2% of all dependencies
are non-projective, we still see an absolute improve-
ment of up to 1.1% in overall accuracy over the
projective model. Furthermore, when we focus on
the subset of data that only contains sentences with
at least one non-projective dependency, the effect
is amplified. Another major improvement here is
that the Chu-Liu-Edmonds non-projective MST al-
gorithm has a parsing complexity of O(n2), versus
the O(n3) complexity of the projective Eisner algo-
rithm, which in practice leads to improvements in
parsing time. The results also show that in terms
of Accuracy, factored MIRA performs better than
single-best MIRA. However, for the factored model,
we do have O(n2) margin constraints, which re-
sults in a significant increase in training time over
single-best MIRA. Furthermore, we can also see that
the MST parsers perform favorably compared to the
more powerful lexicalized phrase-structure parsers,
such as those presented by Collins et al (1999) and
Zeman (2004) that use expensive O(n5) parsing al-
gorithms. We should note that the results in Collins
et al (1999) are different then reported here due to
different training and testing data sets.
One concern raised in Section 2.2.1 is that search-
ing the entire space of non-projective trees could
cause problems for languages that are primarily pro-
jective. However, as we can see, this is not a prob-
lem. This is because the model sets its weights with
respect to the parsing algorithm and will disfavor
features over unlikely non-projective edges.
Since the space of projective trees is a subset of
the space of non-projective trees, it is natural to won-
der how the Chu-Liu-Edmonds parsing algorithm
performs on projective data since it is asymptotically
better than the Eisner algorithm. Table 2 shows the
results for English projective dependency trees ex-
tracted from the Penn Treebank (Marcus et al, 1993)
using the rules of Yamada and Matsumoto (2003).
English
Accuracy Complete
McD2005 90.9 37.5
Single-best MIRA 90.2 33.2
Factored MIRA 90.2 32.3
Table 2: Dependency parsing results for English us-
ing spanning tree algorithms.
This shows that for projective data sets, training
and testing with the Chu-Liu-Edmonds algorithm is
worse than using the Eisner algorithm. This is not
surprising since the Eisner algorithm uses the a pri-
ori knowledge that all trees are projective.
5 Discussion
We presented a general framework for parsing de-
pendency trees based on an equivalence to maxi-
mum spanning trees in directed graphs. This frame-
work provides natural and efficient mechanisms
for parsing both projective and non-projective lan-
guages through the use of the Eisner and Chu-Liu-
Edmonds algorithms. To learn these structures we
used online large-margin learning (McDonald et al,
2005) that empirically provides state-of-the-art per-
formance for Czech.
A major advantage of our models is the abil-
ity to naturally model non-projective parses. Non-
projective parsing is commonly considered more
difficult than projective parsing. However, under
our framework, we show that the opposite is actually
true that non-projective parsing has a lower asymp-
totic complexity. Using this framework, we pre-
sented results showing that the non-projective model
outperforms the projective model on the Prague De-
pendency Treebank, which contains a small number
of non-projective edges.
Our method requires a tree score that decomposes
according to the edges of the dependency tree. One
might hope that the method would generalize to
529
include features of larger substructures. Unfortu-
nately, that would make the search for the best tree
intractable (Ho?ffgen, 1993).
Acknowledgments
We thank Lillian Lee for bringing an important
missed connection to our attention, and Koby Cram-
mer for his help with learning algorithms. This work
has been supported by NSF ITR grants 0205448 and
0428193.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-
troduction to Algorithms. MIT Press/McGraw-Hill.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Proc.
NIPS.
K. Crammer, R. McDonald, and F. Pereira. 2004. New
large margin algorithms for structured prediction. In
Learning with Structured Outputs Workshop (NIPS).
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, and
B. Vidova Hladka. 2001. The Prague Dependency
Treebank 1.0 CDROM. Linguistics Data Consortium
Cat. No. LDC2001T10.
J. Hajic?. 1998. Building a syntactically annotated cor-
pus: The Prague dependency treebank. Issues of Va-
lency and Meaning, pages 106?132.
H. Hirakawa. 2001. Semantic dependency analysis
method for Japanese based on optimum tree search al-
gorithm. In Proc. of PACLING.
Klaus-U. Ho?ffgen. 1993. Learning and robust learning
of product distributions. In Proceedings of COLT?93,
pages 77?83.
W. Hou. 1996. Algorithm for finding the first k shortest
arborescences of a digraph. Mathematica Applicata,
9(1):1?4.
R. Hudson. 1984. Word Grammar. Blackwell.
G. Leonidas. 2003. Arborescence optimization problems
solvable by Edmonds? algorithm. Theoretical Com-
puter Science, 301:427 ? 437.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
ACL.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proc. HLT.
R. Snow, D. Jurafsky, and A. Y. Ng. 2004. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS 2004.
R.E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25?35.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
W. Wang and M. P. Harper. 2004. A statistical constraint
dependency grammar (CDG) parser. In Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL).
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
D. Zeman. 2004. Parsing with a Statistical Dependency
Model. Ph.D. thesis, Univerzita Karlova, Praha.
530
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Serial Combination of Rules and Statistics: A Case Study in Czech
Tagging
Jan Hajic? Pavel Krbec
IFAL
MFF UK
Prague
Czechia
 
hajic,krbec  @
ufal.mff.cuni.cz
Pavel Kve?ton?
ICNC
FF UK
Prague
Czechia
Pavel.Kveton@
ff.cuni.cz
Karel Oliva
Computational
Linguistics
Univ. of Saarland
Germany
oliva@
coli.uni-sb.de
Vladim??r Petkevic?
ITCL
FF UK
Prague
Czechia
Vladimir.Petkevic@
ff.cuni.cz
Abstract
A hybrid system is described which
combines the strength of manual rule-
writing and statistical learning, obtain-
ing results superior to both methods if
applied separately. The combination of
a rule-based system and a statistical one
is not parallel but serial: the rule-based
system performing partial disambigua-
tion with recall close to 100% is applied
first, and a trigram HMM tagger runs on
its results. An experiment in Czech tag-
ging has been performed with encour-
aging results.
1 Tagging of Inflective Languages
Inflective languages pose a specific problem in
tagging due to two phenomena: highly inflec-
tive nature (causing sparse data problem in any
statistically-based system), and free word order
(causing fixed-context systems, such as n-gram
Hidden Markov Models (HMMs), to be even less
adequate than for English). The average tagset
contains about 1,000 - 2,000 distinct tags; the size
of the set of possible and plausible tags can reach
several thousands.
Apart from agglutinative languages such
as Turkish, Finnish and Hungarian (see e.g.
(Hakkani-Tur et al, 2000)), and Basque (Ezeiza
et al, 1998), which pose quite different and in
the end less severe problems, there have been at-
tempts at solving this problem for some of the
highly inflectional European languages, such as
(Daelemans et al, 1996), (Erjavec et al, 1999)
(Slovenian), (Hajic? and Hladka?, 1997), (Hajic? and
Hladka?, 1998) (Czech) and (Hajic?, 2000) (five
Central and Eastern European languages), but
so far no system has reached - in the absolute
terms - a performance comparable to English tag-
ging (such as (Ratnaparkhi, 1996)), which stands
around or above 97%. For example, (Hajic? and
Hladka?, 1998) report results on Czech slightly
above 93% only. One has to realize that even
though such a performance might be adequate for
some tasks (such as word sense disambiguation),
for many other (such as parsing or translation) the
implied sentence error rate at 50% or more is sim-
ply too much to deal with.
1.1 Statistical Tagging
Statistical tagging of inflective languages
has been based on many techniques, rang-
ing from plain-old HMM taggers (M??rovsky?,
1998), memory-based (Erjavec et al, 1999) to
maximum-entropy and feature-based (Hajic? and
Hladka?, 1998), (Hajic?, 2000). For Czech, the
best result achieved so far on approximately
300 thousand word training data set has been
described in (Hajic? and Hladka?, 1998).
We are using 1.8M manually annotated tokens
from the Prague Dependency Treebank (PDT)
project (Hajic?, 1998). We have decided to work
with an HMM tagger1 in the usual source-channel
setting, with proper smoothing. The HMM tag-
ger uses the Czech morphological processor from
PDT to disambiguate only among those tags
1Mainly because of the ease with which it is trained even
on large data, and also because no other publicly available
tagger was able to cope with the amount and ambiguity of
the data in reasonable time.
which are morphologically plausible for a given
input word form.
1.2 Manual Rule-based Systems
The idea of tagging by means of hand-written
disambiguation rules has been put forward and
implemented for the first time in the form of
Constraint-Based Grammars (Karlsson et al,
1995). From languages we are acquainted with,
the method has been applied on a larger scale only
to English (Karlsson et al, 1995), (Samuelsson
and Voutilainen, 1997), and French (Chanod and
Tapanainen, 1995). Also (Bick, 1996) and (Bick,
2000) use manually written rules for Brazilian
Portuguese, and there are several publications by
Oflazer for Turkish.
Authors of such systems claim that hand-
written systems can perform better than sys-
tems based on machine learning (Samuelsson and
Voutilainen, 1997); however, except for the work
cited, comparison is difficult to impossible due to
the fact that they do not use the standard evalua-
tion techniques (and not even the same data). But
the substantial disadvantage is that the develop-
ment of manual rule-based systems is demanding
and requires a good deal of very subtle linguistic
expertise and skills if full disambiguation also of
?difficult? texts is to be performed.
1.3 System Combination
Combination of (manual) rule-writing and statis-
tical learning has been studied before. E.g., (Ngai
and Yarowsky, 2000) and (Ngai, 2001) provide
a thorough description of many experiments in-
volving rule-based systems and statistical learn-
ers for NP bracketing. For tagging, combination
of purely statistical classifiers has been described
(Hladka?, 2000), with about 3% relative improve-
ment (error reduction from 18.6% to 18%, trained
on small data) over the best original system. We
regard such systems as working in parallel, since
all the original classifiers run independently of
each other.
In the present study, we have chosen a differ-
ent strategy (similar to the one described for other
types of languages in (Tapanainen and Vouti-
lainen, 1994), (Ezeiza et al, 1998) and (Hakkani-
Tur et al, 2000)). At the same time, the rule-
based component is known to perform well in
eliminating the incorrect alternatives2, rather than
picking the correct one under all circumstances.
Moreover, the rule-based system used can exam-
ine the whole sentential context, again a difficult
thing for a statistical system3. That way, the ambi-
guity of the input text4 decreases. This is exactly
what our statistical HMM tagger needs as its in-
put, since it is already capable of using the lexical
information from a dictionary.
However, also in the rule-based approach, there
is the usual tradeoff between precision and recall.
We have decided to go for the ?perfect? solution:
to keep 100% recall, or very close to it, and grad-
ually improve precision by writing rules which
eliminate more and more incorrect tags. This way,
we can be sure (or almost sure) that the perfor-
mance of the HMM tagger performance will not
be hurt by (recall) errors made by the rule compo-
nent.
2 The Rule-based Component
2.1 Formal Means
Taken strictly formally, the rule-based component
has the form of a restarting automaton with dele-
tion (Pla?tek et al, 1995), that is, each rule can
be thought of as a finite-state automaton starting
from the beginning of the sentence and passing to
the right until it finds an input configuration on
which it can operate by deletion of some parts of
the input. Having performed this, the whole sys-
tem is restarted, which means that the next rule
is applied on the changed input (and this input is
again read from the left end). This means that a
single rule has the power of a finite state automa-
ton, but the system as a whole has (even more
than) a context-free power.
2.2 The Rules and Their Implementation
The system of hand-written rules for Czech has a
twofold objective:
 practical: an error-free and at the same time
the most accurate tagging of Czech texts
 theoretical: the description of the syntactic
2Such a ?negative? learning is thought to be difficult for
any statistical system.
3Causing an immediate data sparseness problem.
4As prepared by the morphological analyzer.
system of Czech, its langue, rather than pa-
role.
The rules are to reduce the input ambiguity of
the input text. During disambiguation the whole
rule system combines two methods:
 the oblique one consisting in the elimination
of syntactically wrong tag(s), i.e. in the re-
duction of the input ambiguity by deleting
those tags which are excluded by the context
 the direct choice of the correct tag(s).
The overall strategy of the rule system is to
keep the highest recall possible (i.e. 100%) and
gradually improve precision. Thus, the rules are
(manually) assigned reliabilities which divide the
rules into reliability classes, with the most reli-
able (?bullet-proof?) group of rules applied first
and less reliable groups of rules (threatening to
decrease the 100% recall) being applied in subse-
quent steps. The bullet-proof rules reflect general
syntactic regularities of Czech; for instance, no
word form in the nominative case can follow an
unambiguous preposition. The less reliable rules
can be exemplified by those accounting for some
special intricate relations of grammatical agree-
ment in Czech. Within each reliability group the
rules are applied independently, i.e. in any or-
der in a cyclic way until no ambiguity can be re-
solved.
Besides reliability, the rules can be generally
divided according to the locality/nonlocality of
their scope. Some phenomena (not many) in the
structure of Czech sentence are local in nature:
for instance, for the word ?se? which is two-way
ambiguous between a preposition (with) and a re-
flexive particle/pronoun (himself, as a particle) a
prepositional reading can be available only in lo-
cal contexts requiring the vocalisation of the basic
form of the preposition ?s? (with) resulting in the
form ?se?. However, in the majority of phenom-
ena the correct disambiguation requires a much
wider context. Thus, the rules use as wide con-
text as possible with no context limitations be-
ing imposed in advance. During rules develop-
ment performed so far, sentential context has been
used, but nothing in principle limits the context
to a single sentence. If it is generally appropri-
ate for the disambiguation of the languages of the
world to use unlimited context, it is especially fit
for languages with free word order combined with
rich inflection. There are many syntactic phenom-
ena in Czech displaying the following property: a
word form wf1 can be part-of-speech determined
by means of another word form wf2 whose word-
order distance cannot be determined by a fixed
number of positions between the two word forms.
This is exactly a general phenomenon which is
grasped by the hand-written rules.
Formally, each rule consists of
 the description of the context (descriptive
component), and
 the action to be performed given the context
(executive component): i.e. which tags are
to be discarded or which tag(s) are to be pro-
claimed correct (the rest being discarded as
wrong).
For example,
 Context: unambiguous finite verb, fol-
lowed/preceded by a sequence of tokens
containing neither comma nor coordinating
conjunction, at either side of a word x am-
biguous between a finite verb and another
reading
 Action: delete the finite verb reading(s) at
the word x.
There are two ways of rule development:
 the rules developed by syntactic introspec-
tion: such rules are subsequently verified on
the corpus material, then implemented and
the implemented rules are tested on a testing
corpus
 the rules are derived from the corpus by in-
trospection and subsequently implemented
The rules are formulated as generally as pos-
sible and at the same time as error-free (recall-
wise) as possible. This approach of combining the
requirements of maximum recall and maximum
precision demands sophisticated syntactic knowl-
edge of Czech. This knowledge is primarily based
on the study of types of morphological ambiguity
occurring in Czech. There are two main types of
such ambiguity:
 regular (paradigm-internal)
 casual (lexical)
The regular (paradigm-internal) ambiguities
occur within a paradigm, i.e. they are common
to all lexemes belonging to a particular inflection
class. For example, in Czech (as in many other in-
flective languages), the nominative, the accusative
and the vocative case have the same form (in sin-
gular on the one hand, and in plural on the other).
The casual (lexical, paradigm-external) morpho-
logical ambiguity is lexically specific and hence
cannot be investigated via paradigmatics.
In addition to the general rules, the rule ap-
proach includes a module which accounts for col-
locations and idioms. The problem is that the
majority of collocations can ? besides their most
probable interpretation just as collocations ? have
also their literal meaning.
Currently, the system (as evaluated in Sect. 2.3)
consists of 80 rules.
The rules had been implemented procedurally
in the initial phase; a special feature-oriented, in-
terpreted ?programming language? is now under
development.
2.3 Evaluation of the Rule System Alone
The results are presented in Table 1. We use the
usual equal-weight formula for F-measure:
	
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 945?952,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Leveraging Reusability: Cost-effective Lexical Acquisition 
for Large-scale Ontology Translation 
 
G. Craig Murray 
Bonnie J. Dorr 
Jimmy Lin 
Institute for Advanced Computer Studies 
University of Maryland 
{gcraigm,bdorr,jimmylin}@umd.edu 
Jan Haji? 
Pavel Pecina 
 
Institute for Formal and Applied Linguistics 
Charles University 
{hajic,pecina}@ufal.mff.cuni.cz 
 
  
 
Abstract 
Thesauri and ontologies provide impor-
tant value in facilitating access to digital 
archives by representing underlying prin-
ciples of organization.  Translation of 
such resources into multiple languages is 
an important component for providing 
multilingual access.  However, the speci-
ficity of vocabulary terms in most on-
tologies precludes fully-automated ma-
chine translation using general-domain 
lexical resources.  In this paper, we pre-
sent an efficient process for leveraging 
human translations when constructing 
domain-specific lexical resources.  We 
evaluate the effectiveness of this process 
by producing a probabilistic phrase dic-
tionary and translating a thesaurus of 
56,000 concepts used to catalogue a large 
archive of oral histories.  Our experi-
ments demonstrate a cost-effective tech-
nique for accurate machine translation of 
large ontologies. 
1 Introduction 
Multilingual access to digital collections is an 
important problem in today?s increasingly inter-
connected world.  Although technologies such as 
cross-language information retrieval and ma-
chine translation help humans access information 
they could not otherwise find or understand, they 
are often inadequate for highly specific domains. 
Most digital collections of any significant size 
use a system of organization that facilitates easy 
access to collection contents. Generally, the or-
ganizing principles are captured in the form of a 
controlled vocabulary of keyword phrases (de-
scriptors) representing specific concepts.  These 
descriptors are usually arranged in a hierarchic 
thesaurus or ontology, and are assigned to collec-
tion items as a means of providing access (either 
via searching for keyword phases, browsing the 
hierarchy, or a combination both).  MeSH (Medi-
cal Subject Headings) serves as a good example 
of such an ontology; it is a hierarchically-
arranged collection of controlled vocabulary 
terms manually assigned to medical abstracts in a 
number of databases.  It provides multilingual 
access to the contents of these databases, but 
maintaining translations of such a complex struc-
ture is challenging (Nelson, et al 2004). 
For the most part, research in multilingual in-
formation access focuses on the content of digital 
repositories themselves, often neglecting signifi-
cant knowledge that is explicitly encoded in the 
associated ontologies.  However, information 
systems cannot utilize such ontologies by simply 
applying off-the-shelf machine translation. Gen-
eral-purpose translation resources provide insuf-
ficient coverage of the vocabulary contained 
within these domain-specific ontologies. 
This paper tackles the question of how one 
might efficiently translate a large-scale ontology 
to facilitate multilingual information access.  If 
we need humans to assist in the translation proc-
ess, how can we maximize access while mini-
mizing cost?  Because human translation is asso-
ciated with a certain cost, it is preferable not to 
incur costs of retranslation whenever compo-
nents of translated text are reused. Moreover, 
when exhaustive human translation is not practi-
cal, the most ?useful? components should be 
translated first.  Identifying reusable elements 
and prioritizing their translation based on utility 
is essential to maximizing effectiveness and re-
ducing cost. 
945
We present a process of prioritized translation 
that balances the issues discussed above.  Our 
work is situated in the context of the MALACH 
project, an NSF-funded effort to improve multi-
lingual information access to large archives of 
spoken language (Gustman, et al, 2002).  Our 
process leverages a small set of manually-
acquired English-Czech translations to translate a 
large ontology of keyword phrases, thereby pro-
viding Czech speakers access to 116,000 hours 
of video testimonies in 32 languages. Starting 
from an initial out-of-vocabulary (OOV) rate of 
85%, we show that a small set of prioritized 
translations can be elicited from human infor-
mants, aligned, decomposed and then recom-
bined to cover 90% of the access value in a com-
plex ontology.  Moreover, we demonstrate that 
prioritization based on hierarchical position and 
frequency of use facilitates extremely efficient 
reuse of human input.  Evaluations show that our 
technique is able to boost performance of a sim-
ple translation system by 65%. 
2 The Problem 
The USC Shoah Foundation Institute for Vis-
ual History and Education manages what is pres-
ently the world's largest archive of videotaped 
oral histories (USC, 2006). The archive contains 
116,000 hours of video from the testimonies of 
over 52,000 survivors, liberators, rescuers and 
witnesses of the Holocaust.  If viewed end to 
end, the collection amounts to 13 years of con-
tinuous video.  The Shoah Foundation uses a hi-
erarchically arranged thesaurus of 56,000 key-
word phrases representing domain-specific con-
cepts.  These are assigned to time-points in the 
video testimonies as a means of indexing the 
video content.  Although the testimonies in the 
collection represent 32 different languages, the 
thesaurus used to catalog them is currently avail-
able only in English.  Our task was to translate 
this resource to facilitate multilingual access, 
with Czech as the first target language. 
Our first pass at automating thesaurus transla-
tion revealed that only 15% of the words in the 
vocabulary could be found in an available 
aligned corpus (?mejrek, et al, 2004).  The rest 
of the vocabulary was not available from general 
resources.  Lexical information for translating 
these terms had to be acquired from human in-
put.  Reliable access to digital archives requires 
accuracy. Highly accurate human translations 
incur a cost that is generally proportional to the 
number of words being translated.  However, the 
keyword phrases in the Shoah Foundation?s ar-
chive occur in a Zipfian distribution?a rela-
tively small number of terms provide access to a 
large portion of the video content.  Similarly, a 
great number of highly specific terms describe 
only a small fraction of content.  Therefore, not 
every keyword phrase in the thesaurus carries the 
same value for access to the archive.  The hierar-
chical arrangement of keyword phrases presents 
another issue: some concepts, while not of great 
value for access to segments of video, may be 
important for organizing other concepts and for 
browsing the hierarchy.  These factors must be 
balanced in developing a cost-effective process 
that maximizes utility. 
3 Our Solution 
This paper presents a prioritized human-in-the-
loop approach to translating large-scale ontolo-
gies that is fast, efficient, and cost effective.  Us-
ing this approach, we collected 3,000 manual 
translations of keyword phrases and reused the 
translated terms to generate a lexicon for auto-
mated translation of the rest of the thesaurus.  
The process begins by prioritizing keyword 
phrases for manual translation in terms of their 
value in accessing the collection and the reus-
ability of their component terms.  Translations 
collected from one human informant are then 
checked and aligned to the original English terms 
by a second informant.  From these alignments 
we induce a probabilistic English-Czech phrase 
dictionary.   
To test the effectiveness of this process we 
implemented a simple translation system that 
utilizes the newly generated lexical resources.  
Section 4 reports on two evaluations of the trans-
lation output that quantify the effectiveness of 
our human-in-the-loop approach. 
3.1 Maximizing Value and Reusability 
To quantify their utility, we defined two values 
for each keyword phrase in the thesaurus: a the-
saurus value, representing the importance of the 
keyword phrase for providing access to the col-
lection, and a translation value, representing the 
usefulness of having the keyword phrase trans-
lated.  These values are not identical, but the 
second is related to the first. 
Thesaurus value: Keyword phrases in the 
Shoah Foundation?s thesaurus are arranged into a 
poly-hierarchy in which child nodes may have 
multiple parents.  Internal (non-leaf) nodes of the 
hierarchy are used to organize concepts and sup-
port concept browsing.  Some internal nodes are 
also used to index video content.  Leaf nodes are 
946
very specific and are only used to index video 
content.  Thus, the usefulness of any keyword 
phrase for providing access to the digital collec-
tion is directly related to the concept?s position in 
the thesaurus hierarchy. 
A fragment of the hierarchy is shown in Fig-
ure 1. The keyword phrase ?Auschwitz II-
Birkenau (Poland: Death Camp)?, which de-
scribes a Nazi death camp, is assigned to 17,555 
video segments in the collection.  It has broader 
(parent) terms and narrower (child) terms.  Some 
of the broader and narrower terms are also as-
signed to segments, but not all.  Notably, ?Ger-
man death camps? is not assigned to any video 
segments.  However, ?German death camps? has 
very important narrower terms including 
?Auschwitz II-Birkenau? and others. 
From this example, we can see that an internal 
node is valuable in providing access to its chil-
dren, even if the keyword phrase itself is not as-
signed to any segments.  The value we assign to 
any term must reflect this fact.  If we were to 
reduce cost by translating only the nodes as-
signed to video segments, we would neglect 
nodes that are crucial for browsing.  However, if 
we value a node by the sum value of all its chil-
dren, grandchildren, etc., the resulting calcula-
tion would bias the top of the hierarchy.  Any 
prioritization based on this method would lead to 
translation of the top of the hierarchy first.  
Given limited resources, leaf nodes might never 
be translated.  Support for searching and brows-
ing calls for different approaches to prioritization. 
To strike a balance between these factors, we 
calculate a thesaurus value, which represents the 
importance of each keyword phrase to the the-
saurus as a whole.  This value is computed as: 
( ) ( )kchildren
h
scounth kchildreni ikk
? ?+= )(  
For leaf nodes in our thesaurus, this value is sim-
ply the number of video segments to which the 
concept has been assigned.  For parent nodes, the 
thesaurus value is the number of segments (if 
any) to which the node has been assigned, plus 
the average of the thesaurus value of any child 
nodes. 
This recursive calculation yields a micro-
averaged value that represents the reachability of  
segments via downward edge traversals from a 
given node in the hierarchy.  That is, it gives a 
kind of weighted value for the number of seg-
ments described by a given keyword phrase or its 
narrower-term keyword phrases. 
 
For example, in Figure 2 each of the leaf 
nodes n3, n4, and n5 have values based solely on 
the number of segments to which they are as-
signed. Node n1 has value both as an access point 
to the segments at s2 and as an access point to the 
keyword phrases at nodes n3 and n4.  Other inter-
nal nodes, such as n2 have value only in provid-
ing access to other nodes/keyword phrases. 
Working from the bottom of the hierarchy up to 
the primary node (n0) we can compute the the-
saurus value for each node in the hierarchy.  In 
our example, we start with nodes n3 through n5, 
counting the number of the segments that have 
been assigned each keyword phrase.  Then we 
move up to nodes n1 and n2.  At n1 we count the 
number of segments s2 to which n1 was assigned 
and add that count to the average of the thesau-
rus values for n3, and n4.  At n2 we simply aver-
age the thesaurus values for n4 and n5.  The final 
values quantify how valuable the translation of 
any given keyword phrase would be in providing 
access to video segments. 
Translation value: After obtaining the the-
saurus value for each node, we can compute the 
translation value for each word in the vocabulary 
Figure 2. Bottom-up micro-averaging 
Figure 1. Sample keyword phrase  
with broader and narrower terms 
Auschwitz II-Birkenau (Poland : Death Camp) 
 Assigned to 17555 video segments 
 Has as broader term phrases: 
Cracow (Poland : Voivodship) 
  [ 534 narrower terms] [ 204 segments] 
German death camps 
  [  6 narrower terms] [ 0 segments] 
 Has seven narrower term phrases including: 
Block 25 (Auschwitz II-Birkenau) 
  [leaf node] [ 35 segments]  
Kanada (Auschwitz II-Birkenau) 
  [leaf node] [ 378 segments] 
  ...  
disinfection chamber (Auschwitz II-Birkenau) 
  [leaf node] [ 9 segments]  
primary 
keyword 
segments 
n2 
n4 n3 
n0 
n5 
keyword 
phrases 
s2 
n1 
s1 s3 s4 
947
as the sum of the thesaurus value for every key-
word phrase that contains that word: 
tw= ?
?? wk
kh   where Kw={x | phrase x contains w} 
For example, the word ?Auschwitz? occurs in 35 
concepts.  As a candidate for translation, it car-
ries a large impact, both in terms of the number 
of keyword phrases that contains this word, and 
the potential value of those keyword phrases 
(once they are translated) in providing access to 
segments in the archive.  The end result is a list 
of vocabulary words and the impact that correct 
translation of each word would have on the over-
all value of the translated thesaurus. 
We elicited human translations of entire key-
word phrases rather than individual vocabulary 
terms.  Having humans translate individual 
words without their surrounding context would 
have been less efficient.  Also, the value any 
keyword phrase holds for translation is only indi-
rectly related to its own value as a point of access 
to the collection (i.e., its thesaurus value).  Some 
keyword phrases contain words with high trans-
lation value, but the keyword phrase itself has 
low thesaurus value.  Thus, the value gained by 
translating any given phrase is more accurately 
estimated by the total value of any untranslated 
words it contains. Therefore, we prioritized the 
order of keyword phrase translations based on 
the translation value of the untranslated words in 
each keyword phrase. 
Our next step was to iterate through the the-
saurus keyword phrases, prioritizing their trans-
lation based on the assumption that any words 
contained in a keyword phrase of higher priority 
would already have been translated.  Starting 
from the assumption that the entire thesaurus is 
untranslated, we select the one keyword phrase 
that contains the most valuable un-translated 
words?we simply add up the translation value 
of all the untranslated words in each keyword 
phrase, and select the keyword phrase with the 
highest value.  We add this keyword phrase to a 
prioritized list of items to be manually translated 
and we remove it from the list of untranslated 
phrases.  We update our vocabulary list and, as-
suming translations of all the words in the prior 
keyword phrase to now be translated (neglecting 
issues such as morphology), we again select the 
keyword phrase that contains the most valuable 
untranslated words.  We iterate the process until 
all vocabulary terms have been included at least 
one keyword phrases on the prioritized list.  Ul-
timately we end up with an ordered list of the 
keyword phrases that should be translated to 
cover the entire vocabulary, with the most impor-
tant words being covered first. 
A few words about additional characteristics 
of this approach: note that it is greedy and biased 
toward longer keyword phrases.  As a result, 
some words may be translated more than once 
because they appear in more than one keyword 
phrase with high translation value.  This side 
effect is actually desirable.  To build an accurate 
translation dictionary, it is helpful to have more 
than one translation of frequently occuring words, 
especially for morphologically rich languages 
such as Czech.  Our technique makes the opera-
tional assumption that translations of a word 
gathered in one context can be reused in another 
context.  Obviously this is not always true, but 
contexts of use are relatively stable in controlled 
vocabularies.  Our evaluations address the ac-
ceptability of this operational assumption and 
demonstrate that the technique yields acceptable 
translations. 
Following this process model, the most impor-
tant elements of the thesaurus will be translated 
first, and the most important vocabulary terms 
will quickly become available for automated 
translation of keyword phrases with high thesau-
rus value that do not make it onto the prioritized 
list for manual translation (i.e., low translation 
value).  The overall access value of the thesaurus 
rises very quickly after initial translations.  With 
each subsequent human translation of keyword 
phrases on the prioritized list, we gain tremen-
dous value in terms of providing non-English 
access to the collection of video testimonies.  
Figure 3 shows this rate of gain.  It can be seen 
that prioritization based on translation value 
gives a much higher yield of total access than 
prioritization based on thesaurus value. 
Figure 3. Gain rate of access value based on  
number of human translations 
Gain rate of prioritized translation schemes
0%
20%
40%
60%
80%
100%
0 500 1000 1500 2000
number of translations
pe
rc
en
t o
f t
o
ta
l a
cc
es
s 
v
al
u
e
priority by thesaurus value priority by translation value
948
3.2 Alignment and Decomposition 
Following the prioritization scheme above, we 
obtained professional translations for the top 
3000 English keyword phrases.  We tokenized 
these translations and presented them to another 
bilingual Czech speaker for verification and 
alignment.  This second informant marked each 
Czech word in a translated keyword phrase with 
a link to the equivalent English word(s).  Multi-
ple links were used to convey the relationship 
between a single word in one language and a 
string of words in another.  The output of the 
alignment process was then used to build a prob-
abilistic dictionary of words and phrases. 
 
Figure 4. Sample alignment 
Figure 4 shows an example of an aligned 
tranlsation.  The word ?stills? is recorded as a 
translation for ?statick? sn?mky? and ?kl??tery? 
is recorded as a translation for ?convents and 
monasteries.?  We count the number of occur-
rences of each alignment in all of the translations 
and calculate probabilities for each Czech word 
or phrase given an English word or phrase.  For 
example, in the top 3000 keyword phrases 
?stills? appears 29 times.  It was aligned with 
?statick? sn?mky? 28 times and only once with 
?statick? z?b?ry?, giving us a translation prob-
ability of 28/29=0.9655 for ?statick? sn?mky?. 
Human translation of the 3000 English key-
word phrases into Czech took approximately 70 
hours, and the alignments took 55 hours.  The 
overall cost of human input (translation and 
alignment) was less than 1000 ?.  The projected 
cost of full translation for the entire thesaurus 
would have been close to 20000 ? and would not 
have produced any reusable resources.  Naturally, 
costs for building resources in this manner will 
vary, but in our case the cost savings is approxi-
mately twenty fold. 
3.3 Machine Translation 
To demonstrate the effectiveness of our approach, 
we show that a probabilistic dictionary, induced 
through the process we just described, facilitates 
high quality machine translation of the rest of the 
thesaurus.  We evaluated translation quality us-
ing a relatively simple translation system.  How-
ever, more sophisticated systems can draw equal 
benefit from the same lexical resources. 
Our translation system implemented a greedy 
coverage algorithm with a simple back-off strat-
egy.  It first scans the English input to find the 
longest matching substring in our dictionary, and 
replaces it with the most likely Czech translation.  
Building on the example above, the system looks 
up ?monasteries and convents stills? in the dic-
tionary, finds no translation, and backs off to 
?monasteries and convents?, which is translated 
to ?kl??tery?.  Had this phrase translation not 
been found, the system would have attempted to 
find a match for the individual tokens.  Failing a 
match in our dictionary, the system then backs 
off to the Prague Czech-English Dependency 
Treebank dictionary, a much larger dictionary 
with broader scope.  If no match is found in ei-
ther dictionary for the full token, we stem the 
token and look for matches based on the stem.  
Finally, tokens whose translations can not be 
found are simply passed through untranslated. 
A minimal set of heuristic rules was applied to 
reordering the Czech tokens but the output is 
primarily phrase by phrase/word by word transla-
tion.  Our evaluation scores below will partially 
reflect the simplicity of our system.  Our system 
is simple by design.  Any improvement or degra-
dation to the input of our system has direct influ-
ence on the output.  Thus, measures of transla-
tion accuracy for our system can be directly in-
terpreted as quality measures for the lexical re-
sources used and the process by which they were 
developed. 
4 Evaluation 
We performed two different types of evaluation 
to validate our process.  First, we compared our 
system output to human reference translations 
using Bleu (Papineni, et al, 2002), a widely-
accepted objective metric for evaluation of ma-
chine translations.  Second, we showed corrected 
and uncorrected machine translations to Czech 
speakers and collected subjective judgments of 
fluency and accuracy. 
For evaluation purposes, we selected 418 
keyword phrases to be used as target translations.  
These phrases were selected using a stratified 
sampling technique so that different levels of 
thesaurus value would be represented.  There 
was no overlap between these keyword phrases 
and the 3000 prioritized keyword phrases used to 
build our lexicon.  Prior to machine translation 
we obtained at least two independent human-
generated reference translations for each of the 
418 keyword phrases. 
monasteries convents and stills ( ) 
statick? kl??tery sn?mky ( ) 
949
After collecting the first 2500 prioritized 
translations, we induced a probabilistic diction-
ary and generated machine translations of the 
418 target keyword phrases. These were then 
corrected by native Czech speakers, who ad-
justed word order, word choice, and morphology. 
We use this set of human-corrected machine 
translations as a second reference for evaluation. 
Measuring the difference between our uncor-
rected machine translations (MT) and the human-
generated reference establishes how accurate our 
translations are compared to an independently 
established target.  Measuring the difference be-
tween our MT and the human-corrected machine 
translations (corrected MT) establishes how ac-
ceptable our translations are.  We also measured 
the difference between corrected MT and the 
human-generated translations.  We take this to be 
an upper bound on realistic system performance. 
The results from our objective evaluation are 
shown in Figure 5.  Each set of bars in the graph 
shows performance after adding a different num-
ber of aligned translations into the lexicon (i.e., 
performance after adding 500, 1000, ..., 3000 
aligned translations.)  The zero condition is our 
baseline: translations generated using only the 
dictionary available in the Prague Czech-English 
Dependency Treebank.  Three different reference 
sets are shown: human-generated, corrected MT, 
and a combination of the two. 
There is a notable jump in Bleu score after the 
very first translations are added into our prob-
abilistic dictionary.  Without any elicitation and 
alignment we got a baseline score of 0.46 
(against the human-generated reference transla-
tions).  After the aligned terms from only 500 
translations were added to our dictionary, our 
Bleu score rose to 0.66.  After aligned terms 
from 3000 translations were added, we achieved 
0.69.  Using corrected MT as the reference our 
Bleu scores improve from 0.48 to 0.79.  If hu-
man-generated and human-corrected references 
are both considered to be correct translations, the 
improvement goes from .49 to .80.  Regardless 
of the reference set, there is a consistent per-
formance improvement as more and more trans-
lations are added.  We found the same trend us-
ing the TER metric on a smaller data set 
(Murray, et al, 2006).  The fact that the Bleu 
scores continue to rise indicates that our ap-
proach is successful in quickly expanding the 
lexicon with accurate translations.  It is important 
to point out that Bleu scores are not meaningful 
in an absolute sense; the scores here should be 
interpreted with respect to each other.  The trend 
in scores strongly indicates that our prioritization 
scheme is effective for generating a high-quality 
translation lexicon at relatively low cost.   
To determine an upper bound on machine per-
formance, we compared our corrected MT output 
to the initial human-generated reference transla-
tions, which were collected prior to machine 
translation.  Corrected MT achieved a Bleu score 
of 0.82 when compared to the human-generated 
reference translations.  This upper bound is the 
?limit? indicated in Figure 5. 
To determine the impact of external resources, 
we removed the Prague Czech-English Depend-
ency Treebank dictionary as a back-off resource 
and retranslated keyword phrases using only the 
lexicons induced from our aligned translations.  
The results of this experiment showed only mar-
ginal degradation of the output.  Even when as 
few as 500 aligned translations were used for our 
dictionary, we still achieved a Bleu score of 0.65 
against the human reference translations.  This 
means that even for languages where prior re-
sources are not available our prioritization 
scheme successfully addresses the OOV problem. 
In our subjective evaluation, we presented a 
random sample of our system output to seven 
Distribution of Subjective Judgment Scores
0%
20%
40%
60%
80%
100%
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
fluency accuracy fluency accuracy
MT Corrected MT
Judgment scores
Pe
rc
en
t o
f s
co
re
s
Bleu Scores After Increasing Translations
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 500 1000 1500 2000 2500 3000
Number of Translations
B
le
u
-
4
corrected human reference both limit
Figure 5. Objective evaluation results 
Figure 6. Subjective evaluation results 
950
native Czech speakers and collected judgments 
of accuracy and fluency using a 5-point Likert 
scale (1=good, 3=neutral, 5=bad).  An overview 
of the results is presented in Figure 6.  Scores are 
shown for corrected and uncorrected MT.  In all 
cases, the mode is 1 (i.e., good fluency and good 
accuracy).  59% of the machine translated 
phrases were rated 2 or better for fluency.  66% 
were rated 2 or better for accuracy.  Only a small 
percentage of the translations had meanings that 
were far from the intended meaning.  Disfluen-
cies were primarily due to errors in morphology 
and word order.  
5 Related Work  
Several studies have taken a knowledge-
acquisition approach to collecting multilingual 
word pairs.  For example, Sadat et al (2003) 
automatically extracted bilingual word pairs 
from comparable corpora.  This approach is 
based on the simple assumption that if two words 
are mutual translations, then their most frequent 
collocates are likely to be mutual translations as 
well.  However, the approach requires large com-
parable corpora, the collection of which presents 
non-trivial challenges.  Others have made similar 
mutual-translation assumptions for lexical acqui-
sition (Echizen-ya, et al, 2005; Kaji & Aizono, 
1996; Rapp, 1999; Tanaka & Iwasaki, 1996).  
Most make use of either parallel corpora or a 
bilingual dictionary for the task of bilingual term 
extraction.  Echizen-ya, et al (2005) avoided 
using a bilingual dictionary, but required a paral-
lel corpus to achieve their goal; whereas Fung 
(2000) and others have relied on pre-existing 
bilingual dictionaries.  In either case, large bilin-
gual resources of some kind are required.  In ad-
dition, these approaches focused on the extrac-
tion of single-word pairs, not phrasal units. 
Many recent approaches to dictionary and the-
saurus translation are geared toward providing 
domain-specific thesauri to specialists in a par-
ticular field, e.g., medical terminology (D?jean, 
et al, 2005) and agricultural terminology (Chun 
& Wenlin, 2002).  Researchers on these projects 
are faced with either finding human translators 
who are specialized enough to manage the do-
main-particular translations?or applying auto-
matic techniques to large-scale parallel corpora 
where data sparsity poses a problem for low-
frequency terms.  Data sparsity is also an issue 
for more general state-of-the-art bilingual align-
ment approaches (Brown, et al, 2000; Och & 
Ney, 2003; Wantanabe & Sumita, 2003). 
6 Conclusion 
The task of translating large ontologies can be 
recast as a problem of implementing fast and ef-
ficient processes for acquiring task-specific lexi-
cal resources.  We developed a method for pri-
oritizing keyword phrases from an English the-
saurus of concepts and elicited Czech transla-
tions for a subset of the keyword phrases.  From 
these, we decomposed phrase elements for reuse 
in an English-Czech probabilistic dictionary.  We 
then applied the dictionary in machine translation 
of the rest of the thesaurus.   
Our results show an overall improvement in 
machine translation quality after collecting only 
a few hundred human translations.  Translation 
quality continued to rise as more and more hu-
man translations were added.  The test data used 
in our evaluations are small relative to the overall 
task.  However, we fully expect these results to 
hold across larger samples and for more sophisti-
cated translation systems.   
We leveraged the reusability of translated 
words to translate a thesaurus of 56,000 keyword 
phrases using information gathered from only 
3000 manual translations.  Our probabilistic dic-
tionary was acquired at a fraction of the cost of 
manually translating the entire thesaurus.  By 
prioritizing human translations based on the 
translation value of the words and the thesaurus 
value of the keyword phrases in which they ap-
pear, we optimized the rate of return on invest-
ment. This allowed us to choose a trade-off point 
between cost and utility.  For this project we 
chose to stop human translation at a point where 
less than 0.01% of the value of the thesaurus 
would be gained from each additional human 
translation.  This choice produced a high-quality 
lexicon with significant positive impact on ma-
chine translation systems.  For other applications, 
a different trade-off point will be appropriate, 
depending on the initial OOV rate and the impor-
tance of detailed coverage. 
The value of our work lies in the process 
model we developed for cost-effective elicitation 
of lexical resources.  The metrics we established 
for assessing the impact of each translation item 
are key to our approach.  We use these to opti-
mize the value gained from each human transla-
tion.  In our case the items were keyword phrases 
arranged in a hierarchical thesaurus that de-
scribes an ontology of concepts.  The operational 
value of these keyword phrases was determined 
by the access they provide to video segments in a 
large archive of oral histories.  However, our 
technique is not limited to this application. 
951
We have shown that careful prioritization of 
elicited human translations facilitates cost-
effective thesaurus translation with minimal hu-
man input.  Our use of a prioritization scheme 
addresses the most important deficiencies in the 
vocabulary first.  We induced a framework 
where the utility of lexical resources gained from 
each additional human translation becomes 
smaller and smaller.  Under such a framework, 
choosing the number of human translation to 
elicit becomes merely a function of the financial 
resources available for the task. 
Acknowledgments 
Our thanks to Doug Oard for his contribution to 
this work.  Thanks also to our Czech informants: 
Robert Fischmann, Eliska Kozakova, Alena 
Prunerova and Martin Smok; and to Soumya 
Bhat for her programming efforts. 
This work was supported in part by NSF IIS 
Award 0122466 and NSF CISE RI Award 
EIA0130422.  Additional support also came 
from grants of the MSMT CR #1P05ME786 and 
#MSM0021620838, and the Grant Agency of the 
CR #GA405/06/0589.   
References 
Brown, P. F., Della-Pietra, V. J., Della-Pietra, S. A., 
& Mercer, R. L. (1993). The mathematics of statis-
tical machine translation: Parameter estimation. 
Computational Linguistics, 19(2), 263-311. 
Chun, C., & Wenlin, L. (2002). The translation of 
agricultural multilingual thesaurus. In Proceedings 
of the Third Asian Conference for Information 
Technology in Agriculture. Beijing, China: Chinese 
Academy of Agricultural Sciences (CAAS) and 
Asian Federation for Information Technology in 
Agriculture (AFITA). 
?mejrek, M., Cu??n, J., Havelka, J., Haji?, J., & Ku-
bon, V. (2004). Prague Czech-English dependecy 
treebank: Syntactically annotated resources for ma-
chine translation. In 4th International Conference 
on Language Resources and Evaluation Lisbon, 
Portugal. 
D?jean, H., Gaussier, E., Renders, J.-M., & Sadat, F. 
(2005). Automatic processing of multilingual 
medical terminology: Applications to thesaurus en-
richment and cross-language information retrieval. 
Artificial Intelligence in Medicine, 33(2 ), 111-124. 
Echizen-ya, H., Araki, K., & Momouchi, Y. (2005). 
Automatic acquisition of bilingual rules for extrac-
tion of bilingual word pairs from parallel corpora. 
In Proceedings of the ACL-SIGLEX Workshop on 
Deep Lexical Acquisition (pp. 87-96).  
Fung, P. (2000). A statistical view of bilingual lexicon 
extraction: From parallel corpora to non-parallel 
corpora. In Jean Veronis (ed.), Parallel Text Proc-
essing. Dordrecht: Kluwer Academic Publishers. 
Gustman, Soergel, Oard, Byrne, Picheny, Ramabhad-
ran, & Greenberg. (2002). Supporting access to 
large digital oral history archives.  In Proceedings 
of the Joint Conference on Digital Libraries. Port-
land, Oregon. (pp. 18-27). 
Kaji, H., & Aizono, T. (1996). Extracting word corre-
spondences from bilingual corpora based on word 
co-occurrence information. In Proceedings of 
COLING '96 (pp. 23-28).  
Murray, G. C., Dorr, B., Lin, J., Haji?, J., & Pecina, P. 
(2006).  Leveraging recurrent phrase structure in 
large-scale ontology translation.  In Proceedings of 
the 11th Annual Conference of the European Asso-
ciation for Machine Translation.  Oslo, Norway. 
Nelson, S. J., Schopen, M., Savage, A. G., Schulman, 
J.-L., & Arluk, N. (2004). The MeSH translation 
maintenance system: Structure, interface design, 
and implementation. In Proceedings of the 11th 
World Congress on Medical Informatics. (pp. 67-
69). Amsterdam: IOS Press. 
Och, F. J., & Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29(1), 19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2002). BLEU: A method for automatic evaluation 
of machine translation. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (pp. 331-318). 
Rapp, R. (1999). Automatic identification of word 
translations from unrelated English and German 
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics. (pp. 519-526). 
Sadat, F., Yoshikawa, M., & Uemura, S. (2003). En-
hancing cross-language information retrieval by an 
automatic acquisition of bilingual terminology 
from comparable corpora . In Proceedings of the 
26th Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval (pp. 397-398).  
Tanaka, K., & Iwasaki, H. (1996). Extraction of lexi-
cal translations from non-aligned corpora. In Pro-
ceedings of COLING '96. (pp. 580-585). 
USC. (2006) USC Shoah Foundation Institute for 
Visual History and Education, [online] 
http://www.usc.edu/schools/college/vhi 
Wantanabe, T., & Sumita, E. (2003). Example-based 
decoding for statistical machine translation. In Pro-
ceedings of MT Summit IX (pp. 410-417).  
952
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 67?74,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech
Drahom??ra ?johanka? Spoustova?
Jan Hajic?
Jan Votrubec
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
{johanka,hajic,votrubec}@
ufal.mff.cuni.cz
Pavel Krbec
IBM Czech Republic,
Voice Technologies and Systems,
Prague, Czech Republic,
pavel krbec@cz.ibm.com
Pavel Kve?ton?
Institute of the Czech Language,
Academy of Sciences of the Czech Republic
Pavel.Kveton@seznam.cz
Abstract
Several hybrid disambiguation methods are
described which combine the strength of
hand-written disambiguation rules and sta-
tistical taggers. Three different statistical
(HMM, Maximum-Entropy and Averaged
Perceptron) taggers are used in a tagging
experiment using Prague Dependency Tree-
bank. The results of the hybrid systems are
better than any other method tried for Czech
tagging so far.
1 Introduction
Inflective languages pose a specific problem in tag-
ging due to two phenomena: highly inflective na-
ture (causing sparse data problem in any statistically
based system), and free word order (causing fixed-
context systems, such as n-gram HMMs, to be even
less adequate than for English).
The average tagset contains about 1,000 ? 2,000
distinct tags; the size of the set of possible and plau-
sible tags can reach several thousands. There have
been attempts at solving this problem for some of
the highly inflective European languages, such as
(Daelemans, 1996), (Erjavec, 1999) for Slovenian
and (Hajic?, 2000) for five Central and Eastern Euro-
pean languages.
Several taggers already exist for Czech, e.g.
(Hajic? et al, 2001b), (Smith, 2005), (Hajic? et al,
2006) and (Votrubec, 2006). The last one reaches
the best accuracy for Czech so far (95.12 %). Hence
no system has reached ? in the absolute terms ? a
performance comparable to English tagging (such as
(Ratnaparkhi, 1996)), which stands above 97 %.
We are using the Prague Dependency Treebank
(Hajic? et al, 2006) (PDT) with about 1.8 million
hand annotated tokens of Czech for training and test-
ing. The tagging experiments in this paper all use
the Czech morphological (pre)processor, which in-
cludes a guesser for ?unknown? tokens and which is
available from the PDT website (PDT Guide, 2006)
to disambiguate only among those tags which are
morphologically plausible.
The meaning of the Czech tags (each tag has 15
positions) we are using is explained in Table 1. The
detailed linguistic description of the individual posi-
tions can be found in the documentation to the PDT
(Hajic? et al, 2006).
67
Name Description
1 POS Part of Speech
2 SUBPOS Detailed POS
3 GENDER Gender
4 NUMBER Number
5 CASE Case
6 POSSGENDER Possessor?s Gender
7 POSSNUMBER Possessor?s Number
8 PERSON Person
9 TENSE Tense
10 GRADE Degree of comparison
11 NEGATION Negation
12 VOICE Voice
13 RESERVE1 Unused
14 RESERVE2 Unused
15 VAR Variant
Table 1: Czech Morphology and the Positional Tags
2 Components of the hybrid system
2.1 The HMM tagger
The HMM tagger is based on the well known for-
mula of HMM tagging:
T? = arg max
T
P (T )P (W | T ) (1)
where
P (W |T ) ?
?n
i=1 P (wi | ti, ti?1)
P (T ) ?
?n
i=1 P (ti | ti?1, ti?2).
(2)
The trigram probability P (W | T ) in formula 2
replaces (Hajic? et al, 2001b) the common (and less
accurate) bigram approach. We will use this tagger
as a baseline system for further improvements.
Initially, we change the formula 1 by introduc-
ing a scaling mechanism1: T? = arg maxT (?T ?
logP (T ) + logP (W | T )).
We tag the word sequence from right to left, i.e.
we change the trigram probability P (W | T ) from
formula 2 to P (wi | ti, ti+1).
Both the output probability P (wi | ti, ti+1) and
the transition probability P (T ) suffer a lot due to
the data sparseness problem. We introduce a com-
ponent P (endingi | ti, ti+1), where ending con-
sists of the last three characters of wi. Also, we in-
troduce another component P (t?i | t
?
i+1, t
?
i+2) based
on a reduced tagset T ? that contains positions POS,
GENDER, NUMBER and CASE only (chosen on
linguistic grounds).
1The optimum value of the scaling parameter ?T can be
tuned using held-out data.
We upgrade all trigrams to fourgrams; the
smoothing mechanism for fourgrams is history-
based bucketing (Krbec, 2005).
The final fine-tuned HMM tagger thus uses all
the enhancements and every component contains its
scaling factor which has been computed using held-
out data. The total error rate reduction is 13.98 %
relative on development data, measured against the
baseline HMM tagger.
2.2 Morc?e
TheMorc?e2 tagger assumes some of the HMMprop-
erties at runtime, namely those that allow the Viterbi
algorithm to be used to find the best tag sequence for
a given text. However, the transition weights are not
probabilities. They are estimated by an Averaged
Perceptron described in (Collins, 2002). Averaged
Perceptron works with features which describe the
current tag and its context.
Features can be derived from any information we
already have about the text. Every feature can be
true or false in a given context, so we can regard
current true features as a description of the current
tag context.
For every feature, the Averaged Perceptron stores
its weight coefficient, which is typically an integer
number. The whole task of Averaged Perceptron is
to sum all the coefficients of true features in a given
context. The result is passed to the Viterbi algorithm
as a transition weight for a given tag. Mathemati-
cally, we can rewrite it as:
w(C, T ) =
n?
i=1
?i.?i(C, T ) (3)
where w(C, T ) is the transition weight for tag T in
context C, n is number of features, ?i is the weight
coefficient of ith feature and ?(C, T )i is evaluation
of ith feature for context C and tag T .
Weight coefficients (?) are estimated on training
data, cf. (Votrubec, 2006). The training algorithm
is very simple, therefore it can be quickly retrained
and it gives a possibility to test many different sets of
features (Votrubec, 2005). As a result, Morc?e gives
the best accuracy from the standalone taggers.
2The name Morc?e stands for ?MORfologie C?Es?tiny?
(?Czech morphology?).
68
2.3 The Feature-Based Tagger
The Feature-based tagger, taken also from the PDT
(Hajic? et al, 2006) distribution used in our exper-
iments uses a general log-linear model in its basic
formulation:
pAC(y | x) =
exp(
?n
i=1 ?ifi(y, x))
Z(x)
(4)
where fi(y, x) is a binary-valued feature of the event
value being predicted and its context, ?i is a weight
of the feature fi, and the Z(x) is the natural normal-
ization factor.
The weights ?i are approximated by Maximum
Likelihood (using the feature counts relative to all
feature contexts found), reducing the model essen-
tially to Naive Bayes. The approximation is nec-
essary due to the millions of the possible features
which make the usual entropy maximization infeasi-
ble. The model makes heavy use of single-category
Ambiguity Classes (AC)3, which (being indepen-
dent on the tagger?s intermediate decisions) can be
included in both left and right contexts of the fea-
tures.
2.4 The rule-based component
The approach to tagging (understood as a stand-
alone task) using hand-written disambiguation rules
has been proposed and implemented for the first
time in the form of Constraint-Based Grammars
(Karlsson, 1995). On a larger scale, this aproach was
applied to English, (Karlsson, 1995) and (Samuels-
son, 1997), and French (Chanod, 1995). Also (Bick,
2000) uses manually written disambiguation rules
for tagging Brazilian Portuguese, (Karlsson, 1985)
and (Koskenniemi, 1990) for Finish and (Oflazer,
1997) reports the same for Turkish.
2.4.1 Overview
In the hybrid tagging system presented in this pa-
per, the rule-based component is used to further re-
duce the ambiguity (the number of tags) of tokens
in an input sentence, as output by the morphological
processor (see Sect. 1). The core of the component
is a hand-written grammar (set of rules).
Each rule represents a portion of knowledge of
the language system (in particular, of Czech). The
3If a token can be a N(oun), V(erb) or A(djective), its (major
POS) Ambiguity Class is the value ?ANV?.
knowledge encoded in each rule is formally defined
in two parts: a sequence of tokens that is searched
for in an input sentence and the tags that can be
deleted if the sequence of tokens is found.
The overall strategy of this ?negative? grammar is
to keep the highest recall possible (i.e. 100 %) and
gradually improve precision. In other words, when-
ever a rule deletes a tag, it is (almost) 100% safe that
the deleted tag is ?incorrect? in the sentence, i.e. the
tag cannot be present in any correct tagging of the
sentence.
Such an (virtually) ?error-free? grammar can par-
tially disambiguate any input and prevent the subse-
quent taggers (stochastic, in our case) to choose tags
that are ?safely incorrect?.
2.4.2 The rules
Formally, each rule consists of the description of
the context (sequence of tokens with some special
property), and the action to be performed given the
context (which tags are to be discarded). The length
of context is not limited by any constant; however,
for practical purposes, the context cannot cross over
sentence boundaries.
For example: in Czech, two finite verbs cannot
appear within one clause. This fact can be used to
define the following disambiguation rule:
? context: unambiguous finite verb, fol-
lowed/preceded by a sequence of tokens
containing neither a comma nor a coordinat-
ing conjunction, at either side of a word x
ambiguous between a finite verb and another
reading;
? action: delete the finite verb reading(s) at the
word x.
It is obvious that no rule can contain knowledge
of the whole language system. In particular, each
rule is focused on at most a few special phenomena
of the language. But whenever a rule deletes a tag
from a sentence, the information about the sentence
structure ?increases?. This can help other rules to be
applied and to delete more and more tags.
For example, let?s have an input sentence with two
finite verbs within one clause, both of them ambigu-
ous with some other (non-finite-verbal) tags. In this
situation, the sample rule above cannot be applied.
69
On the other hand, if some other rule exists in the
grammar that can delete non-finite-verbal tags from
one of the tokens, then the way for application of the
sample rule is opened.
The rules operate in a loop in which (theoreti-
cally) all rules are applied again whenever a rule
deletes a tag in the partially disambiguated sentence.
Since deletion is a monotonic operation, the algo-
rithm is guaranteed to terminate; effective imple-
mentation has also been found in (Kve?ton?, 2006).
2.4.3 Grammar used in tests
The grammar is being developed since 2000 as
a standalone module that performs Czech morpho-
logical disambiguation. There are two ways of rule
development:
? the rules developed by syntactic introspection:
such rules are subsequently verified on the cor-
pus material, then implemented and the imple-
mented rules are tested on a testing corpus;
? the rules are derived from the corpus by intro-
spection and subsequently implemented.
In particular, the rules are not based on examina-
tion of errors of stochastic taggers.
The set of rules is (manually) divided into two
(disjoint) reliability classes ? safe rules (100% re-
liable rules) and heuristics (highly reliable rules, but
obscure exceptions can be found). The safe rules re-
flect general syntactic regularities of Czech; for in-
stance, no word form in the nominative case can fol-
low an unambiguous preposition. The less reliable
heuristic rules can be exemplified by those account-
ing for some special intricate relations of grammati-
cal agreement in Czech.
The grammar consists of 1727 safe rules and 504
heuristic rules. The system has been used in two
ways:
? safe rules only: in this mode, safe rules are ex-
ecuted in the loop until some tags are being
deleted. The system terminates as soon as no
rule can delete any tag.
? all rules: safe rules are executed first (see safe
rules only mode). Then heuristic rules start
to operate in the loop (similarly to the safe
rules). Any time a heuristic rule deletes a tag,
the safe rules only mode is entered as a sub-
procedure. When safe rules? execution termi-
nates, the loop of heuristic rules continues. The
disambiguation is finished when no heuristic
rule can delete any tag.
The rules are written in the fast LanGR formalism
(Kve?ton?, 2006) which is a subset of more general
LanGR formalism (Kve?ton?, 2005). The LanGR for-
malism has been developed specially for writing and
implementing disambiguation rules.
3 Methods of combination
3.1 Serial combination
The simplest way of combining a hand-written dis-
ambiguation grammar with a stochastic tagger is to
let the grammar reduce the ambiguity of the tagger?s
input. Formally, an input text is processed as fol-
lows:
1. morphological analysis (every input token gets
all tags that are plausible without looking at
context);
2. rule-based component (partially disambiguates
the input, i.e. deletes some tags);
3. the stochastic tagger (gets partially disam-
biguated text on its input).
This algorithm was already used in (Hajic? et
al., 2001b), only components were changed ? the
ruled-based component was significantly improved
and two different sets of rules were tried, as well
as three different statistical taggers. The best result
was (not surprisingly) achieved with set of safe rules
followed by the Morc?e tagger.
An identical approach was used in (Tapanainen,
1994) for English.
3.2 Serial combination with SUBPOS
pre-processing
Manual inspection of the output of the application of
the hand-written rules on the development data (as
used in the serial combination described in the pre-
vious section) discovered that certain types of dead-
locked (?cross-dependent?) rules prevent successful
disambiguation.
70
Cross-dependence means that a rule A can not
apply because of some remaining ambiguity, which
could be resolved by a ruleB, but the operation ofB
is still dependent on the application of A. In particu-
lar, ambiguity in the Part-of-Speech category is very
problematic. For example, only a few safe rules can
apply to a three-word sentence where all three words
are ambiguous between finite verbs and something
else.
If the Part-of-Speech ambiguity of the input is al-
ready resolved, precision of the rule-based compo-
nent and also of the final result after applying any of
the statistical taggers improves. Full Part-of-Speech
information is represented by the first two categories
of the Czech morphology tagset ? POS and SUB-
POS, which deals with different types of pronouns,
adverbs etc. As POS is uniquely determined by
SUBPOS (Hajic? et al, 2006), it is sufficient to re-
solve the SUBPOS ambiguity only.
All three taggers achieve more than 99% accuracy
in SUBPOS disambiguation. For SUBPOS disam-
biguation, we use the taggers in usual way (i.e. they
determine the whole tag) and then we put back all
tags having the same SUBPOS as the tag chosen by
the tagger.
Thus, the method with SUBPOS pre-processing
operates in four steps:
1. morphological analysis;
2. SUBPOS disambiguation (any tagger);
3. rule-based component;
4. final disambiguation (the same tagger4).
The best results were again achieved with the tag-
ger Morc?e and set of safe rules.
3.3 Combining more taggers in parallel
This method is quite different from previous ones,
because it essentially needs more than one tagger. It
consists of the following steps:
1. morphological analysis;
4This limitation is obviously not necessary, but we treat this
combination primarily as a one-tagger method. Results of em-
ploying two different taggers are only slightly better, but still
much worse than results of other methods presented later be-
low.
2. running N taggers independently;
3. merging the results from the previous step ?
each token ends up with between 1 and N tags,
a union of the taggers? outputs;
4. (optional: the rule-based component;)
5. final disambiguation (single tagger).
The best results were achieved with two taggers
in Step 1 (Feature-based and Morc?e), set of all rules
in Step 3 and the HMM tagger in Step 4.
This method is based on an assumption that dif-
ferent stochastic taggers make complementary mis-
takes, so that the recall of the ?union? of taggers
is almost 100 %. Several existing language mod-
els are based on this assumption ? (Brill, 1998)
for tagging English, (Borin, 2000) for tagging Ger-
man and (Vidova?-Hladka?, 2000) for tagging inflec-
tive languages. All these models perform some kind
of ?voting? ? for every token, one tagger is selected
as the most appropriate to supply the correct tag.
The model presented in this paper, however, entrusts
the selection of the correct tag to another tagger that
already operates on the partially disambiguated in-
put.
4 Results
All the methods presented in this paper have been
trained and tested on the PDT version 2.05. Tag-
gers were trained on PDT 2.0 training data set
(1,539,241 tokens), the results were achieved on
PDT 2.0 evaluation-test data set (219,765 tokens),
except Table 6, where PDT 2.0 development-test
data set (201,651 tokens) was used. The morpholog-
ical analysis processor and all the taggers were used
in versions from April 2006 (Hajic? et al, 2006), the
rule-based component is from September 2006.
For evaluation, we use both precision and recall
(and the corresponding F-measure) and accuracy,
since we also want to evaluate the partial disam-
biguation achieved by the hand-written rules alone.
Let t denote the number of tokens in the test data,
let c denote the number of tags assigned to all to-
kens by a disambiguation process and let h denote
5The results cannot be simply (number-to-number) com-
pared to previous results on Czech tagging, because different
training and testing data (PDT 2.0 instead of PDT 1.0) are used
since 2006.
71
the number of tokens where the manually assigned
tag is present in the output of the process.
? In case of the morphological analysis processor
and the standalone rule-based component, the
output can contain more than one tag for ev-
ery token. Then precision (p), recall (r) and F-
measure (f ) characteristics are defined as fol-
lows:
p = h/c r = h/t f = 2pr/(p + r).
? The output of the stochastic taggers contains al-
ways exactly one tag for every token ? then
p = r = f = h/t holds and this ratio is de-
noted as accuracy.
Table 2 shows the performance of the morpholog-
ical analysis processor and the standalone rule-based
component. Table 3 shows the performance of the
standalone taggers. The improvement of the combi-
nation methods is presented in Table 4.
Table 5 shows the relative error rate reduction.
The best method presented by this paper (parallel
combination of taggers with all rules) reaches the
relative error rate decrease of 11.48 % in compari-
son with the tagger Morc?e (which achieves the best
results for Czech so far).
Table 6 shows error rate (100 % ? accuracy) of
various methods6 on particular positions of the tags
(13 and 14 are omitted). The most problematic posi-
tion is CASE (5), whose error rate was significantly
reduced.
5 Conclusion
We have presented several variations of a novel
method for combining statistical and hand-written
rule-based tagging. In all cases, the rule-based
component brings an improvement ? the smaller
the involvement of the statistical component(s) is,
the bigger. The smallest gain can be observed
in the case of the parallel combination of taggers
(which by itself brings an expected improvement).
The best variation improved the accuracy of the
best-performing standalone statistical tagger by over
6F-b stands for feature-based taggeer, Par for parallel com-
bination without rules and Par+Rul for parallel combination
with rules.
11 % (in terms of relative error rate reduction), and
the inclusion of the rule-component itself improved
the best statistical-only combination by over 3.5 %
relative.
This might actually lead to pessimism regarding
the rule-based component. Most other inflective lan-
guages however have much smaller datasets avail-
able than Czech has today; in those cases, we expect
that the contribution of the rule-based component
(which does not depend on the training data size, ob-
viously) will be much more substantial.
The LanGR formalism, now well-developed,
could be used for relatively fast development for
other languages. We are, of course, unable to give
exact figures of what will take less effort ? whether
to annotate more data or to develop the rule-based
component for a particular language. Our feeling is
that the jury is actually still out on this issue, de-
spite some people saying that annotation is always
cheaper: annotation for morphologically complex
(e.g., inflective) languages is not cheap, and rule-
based development efforts have not been previously
using (unannotated) corpora so extensively (which
is what LanGR supports for ?testing? the developed
rules, leading to more reliable rules and more effec-
tive development cycle).
On the other hand, the rule-based component has
also two obvious and well-known disadvantages: it
is language dependent, and the application of the
rules is slower than even the baseline HMM tagger
despite the ?fast? version of the LanGR implemen-
tation we are using7.
In any case, our experiments produced a software
suite which gives the all-time best results in Czech
tagging, and we have offered to apply it to re-tag the
existing 200 mil. word Czech National Corpus. It
should significantly improve the user experience (for
searching the corpus) and allow for more precise ex-
periments with parsing and other NLP applications
that use that corpus.
7In the tests presented in this paper, the speed of the op-
eration of each stochastic tagger (and the parallel combination
without rules) is several hundreds of tokens processed per sec-
ond (running on a 2.2GHz Opteron processor). The operation of
the standalone rule-based component, however, is cca 10 times
slower ? about 40 tokens per second. The parallel combination
with all rules processes about 60 tokens per second ? the rules
operate faster here because their input in parallel combination
is already partially disambiguated.
72
Method p r f
Morphology 25.72 % 99.39 % 40.87 %
Safe rules 57.90 % 98.83 % 73.02 %
All rules 66.35 % 98.03 % 79.14 %
Table 2: Evaluation of rules alone
Tagger accuracy
Feature-based 94.04 %
HMM 94.82 %
Morc?e 95.12 %
Table 3: Evaluation of the taggers alone
Combination method accuracy
Serial (safe rules+Morc?e) 95.34 %
SUBPOS serial (safe rules+Morc?e) 95.44 %
Parallel without rules 95.52 %
Parallel with all rules 95.68 %
Table 4: Evaluation of the combinations
Method Morc?e Parallel
without
rules
Parallel without rules 8.20 % ?
Parallel with all rules 11.48 % 3.57 %
Table 5: Relative error rate reduction
F-b HMM Morc?e Par Par+Rul
1 0.61 0.70 0.66 0.57 0.57
2 0.69 0.78 0.75 0.64 0.64
3 1.82 1.49 1.66 1.39 1.37
4 1.56 1.30 1.38 1.18 1.15
5 4.03 3.53 3.08 2.85 2.62
6 0.02 0.03 0.03 0.02 0.02
7 0.01 0.01 0.01 0.01 0.01
8 0.06 0.07 0.08 0.06 0.05
9 0.05 0.08 0.07 0.05 0.04
10 0.29 0.28 0.30 0.26 0.27
11 0.29 0.31 0.33 0.28 0.28
12 0.05 0.08 0.06 0.05 0.04
15 0.31 0.31 0.31 0.28 0.29
Table 6: Error rate [%] on particular positions of tags
Acknowledgements
The research described here was supported by the
projects MSM0021620838 and LC536 of Ministry of
Eduation, Youth and Sports of the Czech Republic,
GA405/06/0589 of the Grant Agency of the Czech
Republic and 1ET100610409 Diagnostic and Eval-
uation Tools for Linguistic Software of the Informa-
tion Society Programme of the National Research
Programme of the Czech Republic.
References
Eckhard Bick. 2000. The parsing system ?Palavras?
? automatic grammatical analysis of Portuguese in a
constraint grammar framework. In: Proceedings of the
2nd International Conference on Language Resources
and Evaluation, TELRI. Athens
Lars Borin. 2000. Something borrowed, something blue:
Rule-based combination of POS taggers. In: Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation, Vol. 1, pp. 21?26. Athens
Eric Brill and Jun Wu. 1998. Classifier combination
for improved lexical disambiguation. In: Proceedings
of the 17th international conference on Computational
linguistics, Vol. 1, pp. 191?195. Montreal, Quebec
Jean-Pierre Chanod and Pasi Tapanainen. 1995. Tagging
French ? comparing a statistical and a constraint-
based method. In: Proceedings of EACL-95, pp. 149?
157. Dublin
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In: Proceedings
of EMNLP?02, July 2002, pp. 1?8. Philadelphia
W. Daelemans and Jakub Zavrel and Peter Berck and
Steven Gillis. 1996. MBT: A memory-based part of
speech tagger-generator. In: Proceedings of the 4th
WVLC, pp. 14?27. Copenhagen
Tomaz Erjavec and Saso Dzeroski and Jakub Zavrel.
1999. Morphosyntactic Tagging of Slovene: Evaluat-
ing PoS Taggers and Tagsets. Technical Report, Dept.
for Intelligent Systems, Jozef Stefan Institute. Ljubl-
jana
Jan Hajic? and Barbora Hladka?. 1997. Tagging of in-
flective languages: a comparison. In: Proceedings of
ANLP ?97, pp. 136?143. Washington, DC.
Jan Hajic? 2000. Morphological tagging: Data vs. dic-
tionaries. In: Proceedings of the 6th ANLP / 1st
NAACL?00, pp. 94?101. Seattle, WA
73
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva and
Vladim??r Petkevic?. 2001. Serial Combination of
Rules and Statistics: A Case Study in Czech Tag-
ging. In: Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics. CNRS
? Institut de Recherche en Informatique de Toulouse
and Universite? des Sciences Sociales, pp. 260?267.
Toulouse
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka
and Marie Mikulova?. 2006. Prague De-
pendency Treebank v2.0. CDROM. Linguis-
tic Data Consortium, Cat. LDC2006T01. Philadel-
phia. ISBN 1-58563-370-4. Documentation also at
http://ufal.ms.mff.cuni.cz/pdt2.0.
Fred Karlsson. 1985. Parsing Finnish in terms of a pro-
cess grammar. In: Fred Karlsson (ed.): Computational
Morphosyntax: Report on Research 1981-84, Univer-
sity of Helsinki, Department of General Linguistics
Publications No. 13, pp. 137?176.
Fred Karlsson and Atro Voutilainen and Juha Heikkila?
and Arto Anttila (eds.). 1995. Constraint Grammar: a
language-independent system for parsing unrestricted
text. Natural Language Processing. Vol. 4, Mouton
de Gruyter, Berlin and New York.
Kimmo Koskenniemi. 1990. Finite-State Parsing and
Disambiguation. In: Proceedings of Coling-90, Uni-
versity of Helsinki, 1990, pp. 229?232. Helsinki
Pavel Krbec. 2005. Language Modelling for Speech
Recognition of Czech. PhD Thesis, MFF, Charles Uni-
versity Prague.
Pavel Kve?ton?. 2005. Rule-based Morphological Dis-
ambiguation. PhD Thesis, MFF, Charles University
Prague.
Pavel Kve?ton?. 2006. Rule-based morphological dis-
ambiguation: On computational complexity of the
LanGR formalism. In: The Prague Bulletin of Mathe-
matical Linguistics, Vol. 85, pp. 57?72. Prague
Kemal Oflazer and Go?khan Tu?r. 1997. Morphological
disambiguation by voting constraints. In: Proceedings
of the 8th conference on European chapter of the As-
sociation for Computational Linguistics, pp. 222?229.
Madrid
Karel Oliva, Milena Hna?tkova?, Vladim??r Petkevic? and
Pavel Kve?ton?. 2000. The Linguistic Basis of a Rule-
Based Tagger of Czech. In: Sojka P., Kopec?ek I.,
Pala K. (eds.): Proceedings of the Conference ?Text,
Speech and Dialogue 2000?, Lecture Notes in Artifi-
cial Intelligence, Vol. 1902. Springer-Verlag, pp. 3?8.
Berlin-Heidelberg
PDTGuide. http://ufal.ms.mff.cuni.cz/pdt2.0
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In: Proceedings of the 1st
EMNLP, May 1996, pp. 133?142. Philadelphia
Christer Samuelsson and Atro Voluntainen. 1997. Com-
paring a linguistic and a stochastic tagger. In: Pro-
ceedings of ACL/EACL Joint Converence, pp. 246?
252. Madrid
Noah A. Smith and David A. Smith and Roy W.
Tromble. 2005. Context-Based Morphological Dis-
ambiguation with Random Fields. In: Proceedings of
HLT/EMNLP, pp. 475?482. Vancouver
Drahom??ra ?johanka? Spoustova?. in prep. Kombino-
vane? statisticko-pravidlove? metody znac?kova?n?? c?es?tiny.
(Combining Statistical and Rule-Based Approaches to
Morphological Tagging of Czech Texts). PhD Thesis,
MFF UK, in prep.
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging
accurately: don?t guess if you know. In: Proceedings
of the 4th conference on Applied Natural Language
Processing, pp. 47?52. Stuttgart
Barbora Vidova?-Hladka?. 2000. Czech Language Tag-
ging. PhD thesis, U?FAL MFF UK. Prague
Jan Votrubec. 2005. Volba vhodny?ch rysu? pro morfolog-
icke? znac?kova?n?? c?es?tiny. (Feature Selection for Mor-
phological Tagging of Czech.) Master thesis, MFF,
Charles University, Prague.
Jan Votrubec. 2006. Morphological Tagging Based on
Averaged Perceptron. In: WDS?06 Proceedings of
Contributed Papers, MFF UK, pp. 191?195. Prague
74
Proceedings of the Third Workshop on Statistical Machine Translation, pages 143?146,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Phrase-Based and Deep Syntactic
English-to-Czech Statistical Machine Translation ?
Ondr?ej Bojar and Jan Hajic?
Institute of Formal and Applied Linguistics
?UFAL MFF UK, Malostranske? na?me?st?? 25
CZ-11800 Praha, Czech Republic
{bojar,hajic}@ufal.mff.cuni.cz
Abstract
This paper describes our two contributions to
WMT08 shared task: factored phrase-based
model using Moses and a probabilistic tree-
transfer model at a deep syntactic layer.
1 Introduction
Czech is a Slavic language with very rich morphol-
ogy and relatively free word order. The Czech
morphological system (Hajic?, 2004) defines 4,000
tags in theory and 2,000 were actually seen in a
big tagged corpus while the English Penn Treebank
tagset contains just about 50 tags. In our parallel
corpus (see below), the English vocabulary size is
148k distinct word forms but more than twice as big
in Czech, 343k distinct word forms.
When translating to Czech from an analytic lan-
guage such as English, target word forms have to
be chosen correctly to produce a grammatical sen-
tence and preserve the expressed relations between
elements in the sentence, e.g. verbs and their modi-
fiers.
This year, we have taken two radically different
approaches to English-to-Czech MT. Section 2 de-
scribes our setup of the phrase-based system Moses
(Koehn et al, 2007) and Section 3 focuses on a sys-
tem with probabilistic tree transfer employed at a
deep syntactic layer and the new challenges this ap-
proach brings.
?The work on this project was supported by the grants FP6-
IST-5-034291-STP (EuroMatrix), MSM0021620838, M?SMT
?CR LC536, and GA405/06/0589.
2 Factored Phrase-Based MT to Czech
Bojar (2007) describes various experiments with
factored translation to Czech aimed at improving
target-side morphology. We use essentially the same
setup with some cleanup and significantly larger
target-side training data:
Parallel data from CzEng 0.7 (Bojar et al, 2008),
with original sentence-level alignment and tokeniza-
tion. The parallel corpus was taken as a monolithic
text source disregarding differences between CzEng
data sources. We use only 1-1 aligned sentences.
Word alignment using GIZA++ toolkit (Och and
Ney, 2000), the default configuration as available in
training scripts for Moses. We based the word align-
ment on Czech and English lemmas (base forms
of words) as provided by the combination of tag-
gers and lemmatizers by Hajic? (2004) for Czech and
Brants (2000) followed by Minnen et al (2001) for
English. We symmetrized the two GIZA++ runs us-
ing grow-diag-final heuristic.
Truecasing. We attempted to preserve meaning-
bearing case distinctions. The Czech lemmatizer
produces case-sensitive lemmas and thus makes it
easy to cast the capitalization of the lemma back on
the word form.1 For English we approximate the
same effect by a two-step procedure.2
1We change the capitalization of the form to match the
lemma in cases where the lemma is lowercase, capitalized (uc-
first) or all-caps. For mixed-case lemmas, we keep the form
intact.
2We first collect a lexicon of the most typical ?shapes? for
each word form (ignoring title-like sentences with most words
capitalized and the first word in a sentence). Capitalized and
all-caps words in title-like sentences are then changed to their
143
Decoding steps. We use a simple two-step sce-
nario similar to class-based models (Brown and oth-
ers, 1992): (1) the source English word forms are
translated to Czech word forms and (2) full Czech
morphological tags are generated from the Czech
forms.
Language models. We use the following 6 inde-
pendently weighted language models for the target
(Czech) side:
? 3-grams of word forms based on all CzEng 0.7
data, 15M tokens,
? 3-grams of word forms in Project Syndicate
section of CzEng (in-domain for WMT07 and
WMT08 NC-test set), 1.8M tokens,
? 4-grams of word forms based on Czech Na-
tional Corpus (Kocek et al, 2000), version
SYN2006, 365M tokens,
? three models of 7-grams of morphological tags
from the same sources.
Lexicalized reordering using the mono-
tone/swap/discontinuous bidirectional model based
on both source and target word forms.
MERT. We use the minimum-error rate training
procedure by Och (2003) as implemented in the
Moses toolkit to set the weights of the various trans-
lation and language models, optimizing for BLEU.
Final detokenization is a simple rule-based pro-
cedure based on Czech typographical conventions.
Finally, we capitalize the beginnings of sentences.
See BLEU scores in Table 2 below.
3 MT with a Deep Syntactic Transfer
3.1 Theoretical Background
Czech has a well-established theory of linguistic
analysis called Functional Generative Description
(Sgall et al, 1986) supported by a big treebanking
enterprise (Hajic? and others, 2006) and on-going
adaptations for other languages including English
(Cinkova? and others, 2004). There are two layers
typical shape. In other sentences we change the case only if a
typically lowercase word is capitalized (e.g. at the beginning
of the sentence) or if a typically capitalized word is all-caps.
Unknown words in title-like sentences are lowercased and left
intact in other sentences.
Pred
Sb uvedla , z?e Pred
=
VP
NP said VP
Figure 1: Sample treelet pair, a-layer.
of syntactic analysis, both formally captured as la-
belled ordered dependency trees: the ANALYTICAL
(a-, surface syntax) representation bears a 1-1 corre-
spondence between tokens in the sentence and nodes
in the tree; the TECTOGRAMMATICAL (t-, deep syn-
tax) representation contains nodes only for autose-
mantic words and adds nodes for elements not ex-
pressed on the surface but required by the grammar
(e.g. dropped pronouns).
We use the following tools to automatically anno-
tate plaintext up to the t-layer: (1) TextSeg ( ?Ces?ka,
2006) for tokenization, (2) tagging and lemmatiza-
tion see above, (3) parsing to a-layer: Collins (1996)
followed by head-selection rules for English, Mc-
Donald and others (2005) for Czech, (4) parsing to t-
layer: ?Zabokrtsky? (2008) for English, Klimes? (2006)
for Czech.
3.2 Probabilistic Tree Transfer
The transfer step is based on Synchronous Tree Sub-
stitution Grammars (STSG), see Bojar and ?Cmejrek
(2007) for a detailed explanation. The essence is a
log-linear model to search for the most likely syn-
chronous derivation ?? of the source T1 and target T2
dependency trees:
?? = argmax
? s.t. source is T1
exp
(
M
?
m=1
?mhm(?)
)
(1)
The key feature function hm in STSG represents
the probability of attaching pairs of dependency
treelets ti1:2 such as in Figure 1 into aligned pairs of
frontiers ( ) in another treelet pair tj1:2 given fron-
tier state labels (e.g. Pred- VP in Figure 1):
hSTSG(?) = log
k
?
i=0
p(ti1:2 | frontier states) (2)
Other features include e.g. number of internal
nodes (drawn as in Figure 1) produced, number
of treelets produced, and more importantly the tra-
ditional n-gram language model if the target (a-)tree
144
is linearized right away or a binode model promot-
ing likely combinations of the governor g(e) and the
child c(e) of an edge e ? T2:
hbinode(?) = log
?
e?T2
p(c(e) | g(e)) (3)
The probabilistic dictionary of aligned treelet
pairs is extracted from node-aligned (GIZA++ on
linearized trees) parallel automatic treebank as in
Moses? training: all treelet pairs compatible with the
node alignment.
3.2.1 Factored Treelet Translation
Labels of nodes at the t-layer are not atomic but
consist of more than 20 attributes representing var-
ious linguistic features.3 We can consider the at-
tributes as individual factors (Koehn and Hoang,
2007). This allows us to condition the translation
choice on a subset of source factors only. In order to
generate a value for each target-side factor, we use
a sequence of mapping steps similar to Koehn and
Hoang (2007). For technical reasons, our current
implementation allows to generate factored target-
side only when translating a single node to a single
node, i.e. preserving the tree structure.
In our experiments we used 8 source (English) t-
node attributes and 14 target (Czech) attributes.
3.3 Recent Experimental Results
Table 1 shows BLEU scores for various configura-
tions of our decoder. The abbreviations indicate be-
tween which layers the tree transfer was employed
(e.g. ?eact? means English a-layer to Czech t-layer).
The ?p? layer is an approximation of phrase-based
MT: the surface ?syntactic? analysis is just a left-to-
right linear tree.4 For setups ending in t-layer, we
use a deterministic generation the of Czech sentence
by Pta?c?ek and ?Zabokrtsky? (2006).
For WMT08 shared task, Table 2, we used a vari-
ant of the ?etct factored? setup with the annotation
pipeline as incorporated in TectoMT ( ?Zabokrtsky?,
2008) environment and using TectoMT internal
3Treated as atomic, t-node labels have higher entropy
(11.54) than lowercase plaintext (10.74). The t-layer by itself
does not bring any reduction in vocabulary. The idea is that the
attributes should be more or less independent and should map
easier across languages.
4Unlike Moses, ?epcp? does not permit phrase reordering.
Tree-based Transfer LM Type BLEU
epcp n-gram 10.9?0.6
eaca n-gram 8.8?0.6
epcp none 8.7?0.6
eaca none 6.6?0.5
etca n-gram 6.3?0.6
etct factored, preserving structure binode 5.6?0.5
etct factored, preserving structure none 5.3?0.5
eact, target side atomic binode 3.0?0.3
etct, atomic, all attributes binode 2.6?0.3
etct, atomic, all attributes none 1.6?0.3
etct, atomic, just t-lemmas none 0.7?0.2
Phrase-based (Moses) as reported by Bojar (2007)
Vanilla n-gram 12.9?0.6
Factored to improve target morphology n-gram 14.2?0.7
Table 1: English-to-Czech BLEU scores for syntax-based
MT on WMT07 DevTest.
WMT07 WMT08
DevTest NC Test News Test
Moses 14.9?0.9 16.4?0.6 12.3?0.6
Moses, CzEng data only 13.9?0.9 15.2?0.6 10.0?0.5
etct, TectoMT annotation 4.7?0.5 4.9?0.3 3.3?0.3
Table 2: WMT08 shared task BLEU scores.
rules for t-layer parsing and generation instead of
Klimes? (2006) and (Pta?c?ek and ?Zabokrtsky?, 2006).
3.3.1 Discussion
Our syntax-based approach does not reach scores
of phrase-based MT due to the following reasons:
Cumulation of errors at every step of analysis.
Data loss due to incompatible parses and node
alignment. Unlike e.g. Quirk et al (2005) or Huang
et al (2006) who parse only one side and project the
structure, we parse both languages independently.
Natural divergence and random errors in either of
the parses and/or the alignment prevent us from ex-
tracting many treelet pairs.
Combinatorial explosion in target node at-
tributes. Currently, treelet options are fully built in
advance. Uncertainty in the many t-node attributes
leads to too many insignificant variations while e.g.
different lexical choices are pushed off the stack.
While vital for final sentence generation (see Ta-
ble 1), fine-grained t-node attributes should be pro-
duced only once all key structural, lexical and form
decisions have been made. The same sort of explo-
sion makes complicated factored setups not yet fea-
sible in Moses, either.
145
Lack of n-gram LM in the (deterministic) gen-
eration procedures from a t-tree. While we support
final LM-based rescoring, there is too little variance
in n-best lists due to the explosion mentioned above.
Too many model parameters given our stack
limit. We use identical MERT implementation to
optimize ?ms but in the large space of hypotheses,
MERT does not converge.
3.3.2 Related Research
Our approach should not be confused with the
TectoMT submission by Zdene?k ?Zabokrtsky? with a
deterministic transfer: heuristics fully exploiting the
similarity of English and Czech t-layers.
Ding and Palmer (2005) improve over word-based
MT baseline with a formalism very similar to STSG.
Though not explicitly stated, they seem not to en-
code frontiers in the treelets and allow for adjunction
(adding siblings), like Quirk et al (2005), which sig-
nificantly reduces data sparseness.
Riezler and III (2006) report an improvement in
MT grammaticality on a very restricted test set:
short sentences parsable by an LFG grammar with-
out back-off rules.
4 Conclusion
We have presented our best-performing factored
phrase-based English-to-Czech translation and a
highly experimental complex system with tree-
based transfer at a deep syntactic layer. We have
discussed some of the reasons why the phrase-based
MT currently performs much better.
References
Ondr?ej Bojar and Martin ?Cmejrek. 2007. Mathematical
Model of Tree Transformations. Project EuroMatrix -
Deliverable 3.2, ?UFAL, Charles University, Prague.
Ondr?ej Bojar, Zdene?k ?Zabokrtsky?, Pavel ?Ces?ka, Peter
Ben?a, and Miroslav Jan??c?ek. 2008. CzEng 0.7: Paral-
lel Corpus with Community-Supplied Translations. In
Proc. of LREC 2008. ELRA.
Ondr?ej Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proc. of ACL Workshop on Statistical
Machine Translation, pages 232?239, Prague.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger . In Proc. of ANLP-NAACL.
Peter F. Brown et al 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Pavel ?Ces?ka. 2006. Segmentace textu. Bachelor?s The-
sis, MFF, Charles University in Prague.
Silvie Cinkova? et al 2004. Annotation of English on the
tectogrammatical level. Technical Report TR-2006-
35, ?UFAL/CKL, Prague, Czech Republic.
Michael Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. In Proc. of ACL.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proc. of ACL.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum, Prague.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Extended
Domain of Locality. In Proc. of AMTA, Boston, MA.
Va?clav Klimes?. 2006. Analytical and Tectogrammatical
Analysis of a Natural Language. Ph.D. thesis, ?UFAL,
MFF UK, Prague, Czech Republic.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, et al 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
Proc. of ACL Demo and Poster Sessions.
Ryan McDonald et al 2005. Non-Projective Depen-
dency Parsing using Spanning Tree Algorithms. In
Proc. of HLT/EMNLP 2005.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Franz Josef Och and Hermann Ney. 2000. A Comparison
of Alignment Models for Statistical Machine Transla-
tion. In Proc. of COLING, pages 1086?1090.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL.
Jan Pta?c?ek and Zdene?k ?Zabokrtsky?. 2006. Synthesis
of Czech Sentences from Tectogrammatical Trees. In
Proc. of TSD, pages 221?228.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proc. of ACL, pages 271?279.
Stefan Riezler and John T. Maxwell III. 2006. Grammat-
ical Machine Translation. In Proc. of HLT/NAACL.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia, Prague.
Zdene?k ?Zabokrtsky?. 2008. Tecto MT. Technical report,
?UFAL/CKL, Prague, Czech Republic. In prep.
146
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 58?63,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
An Analysis of Annotation of Verb-Noun Idiomatic Combinations
in a Parallel Dependency Corpus?
Zdenka Uresova and Jana Sindlerova and Eva Fucikova and Jan Hajic
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics?
{uresova,sindlerova,fucikova,hajic}@ufal.mff.cuni.cz
Abstract
While working on valency lexicons for Czech
and English, it was necessary to define treat-
ment of multiword entities (MWEs) with the
verb as the central lexical unit. Morphologi-
cal, syntactic and semantic properties of such
MWEs had to be formally specified in order to
create lexicon entries and use them in treebank
annotation. Such a formal specification has
also been used for automated quality control
of the annotation vs. the lexicon entries. We
present a corpus-based study, concentrating on
multilayer specification of verbal MWEs, their
properties in Czech and English, and a com-
parison between the two languages using the
parallel Czech-English Dependency Treebank
(PCEDT). This comparison revealed interest-
ing differences in the use of verbal MWEs in
translation (discovering that such MWEs are
actually rarely translated as MWEs, at least
between Czech and English) as well as some
inconsistencies in their annotation. Adding
MWE-based checks should thus result in bet-
ter quality control of future treebank/lexicon
annotation. Since Czech and English are typo-
logically different languages, we believe that
our findings will also contribute to a better
understanding of verbal MWEs and possibly
their more unified treatment across languages.
? This work has been supported by the Grant No.
GPP406/13/03351P of the Grant Agency of the Czech Repub-
lic. The data used have been provided by the LINDAT/Clarin
infrastructural project LM2010013 supported by the MSMT CR
(http://lindat.cz).
? Authors? full address: Institute of Formal and Applied
Linguistics, Charles University in Prague, Faculty of Mathe-
matics and Physics, Malostranske nam. 25, 11800 Prague 1,
Czech Republic
1 Introduction: Valency and MWEs
Valency is a linguistic phenomenon which plays a
crucial role in the majority of today?s linguistic the-
ories and may be considered a base for both lexi-
cographical and grammatical work. After valency
was first introduced into linguistics by L. Tesni?re
(1959), the study of valency was taken up by many
scholars, with a wealth of material now available;
cf. (?gel et al, 2006). In the theoretical framework
of Functional Generative Description (Sgall et al,
1986), the following researchers have substantially
contributed to valency research: J. Panevov? (1977;
1998); P. Sgall (1998), M. Lopatkov? (2010), V. Ket-
tnerov? (2012), Z. Ure?ov? (2011a; 2011b).
In general, valency is understood as a specific
ability of certain lexical units - primarily of verbs
- to open ?slots? to be filled in by other lexical units.
By filling up these slots the core of the sentence
structure is built. Valency is mostly approached syn-
tactically, semantically or by combining these two
perspectives. Valency terminology is not consistent
(cf. valency, subcategorization, argument structure,
etc.), however, valency as a verbal feature seems to
be language universal (Goldberg, 1995).
MWEs are expressions which consist of more
than a single word while having non-compositional
meaning. They can be defined (Sag et al, 2002) as
?idiosyncratic interpretations that cross word bound-
aries.? As the MWE Workshop itself attests, MWEs
form a complex issue, both theoretically and practi-
cally in various NLP tasks. Here, we will concen-
trate on certain types of verbal MWEs only.
Verbal MWEs can be divided into several groups
58
(cf. Sect. 1.3.2 in (Baldwin and Kim, 2010)):
? verb-particle constructions (VPCs), such as
take off, play around, or cut short,
? prepositional verbs (PVs), such as refer to, look
for, or come across,
? light-verb constructions (LVCs or verb-
complement pairs or support verb construc-
tions, see e.g. (Calzolari et al, 2002)), such as
give a kiss, have a drink, or make an offer,
? verb-noun idiomatic combinations (VNICs or
VP idioms), such as the (in)famous kick the
bucket, spill the beans, or make a face.
While (Baldwin and Kim, 2010) define VNICs as
being ?composed of a verb and noun in direct object
position,?1 we found that their syntax can be more
diverse and thus we will include also constructions
like be at odds or make a mountain out of a mole-
hill into this class. Our goal is to look mainly at
the surface syntactic representation of MWEs, there-
fore, we will follow the above described typology
even though the exact classification might be more
complex.
2 Verbal Valency and MWEs in
Dependency Treebanks
In the Prague Dependency Treebank family of
projects (PDT(s)) annotated using the Tectogram-
matical Repesentation of deep syntax and seman-
tics (B?hmov? et al, 2005), valency information is
stored in valency lexicons. Each verb token in PDTs
is marked by an ID (i.e., linked to) of the appropri-
ate valency frame in the valency lexicon. For Czech,
both the PDT (Hajic? et al, 2012a) and the Czech part
of the PCEDT 2.0 (Hajic? et al, 2012b)2 use PDT-
Vallex3; for English (the English part of PCEDT,
i.e. the texts from the Wall Street Journal portion of
the Penn Treebank (WSJ/PTB), cf. (Marcus et al,
1993)) we use EngVallex,4 which follows the same
1(Baldwin and Kim, 2010), Sect. 1.3.2.4
2Also available from LDC, Catalog No. LDC2012T08.
3
http://ufal.mff.cuni.cz/lindat/PDT-Vallex
4
http://ufal.mff.cuni.cz/lindat/EngVallex; since it was cre-
ated for the WSJ/PTB annotation, the starting point was Prop-
Bank (Palmer et al, 2005) to which it is also linked.
principles, including entry structure, labeling of ar-
guments etc.
Here is an example of a valency lexicon entry (for
the base sense of to give, simplified):
give ACT(sb) PAT(dobj) ADDR(dobj2)
The verb lemma (give) is associated with its ar-
guments, labeled by functors: ACT for actor (deep
subject), PAT for Patient (deep object), and ADDR
for addressee.5
In the valency lexicon entries, two more argument
labels can be used: effect (EFF) and origin (ORIG).
In addition, if a free modifier (e.g. adverbial, prepo-
sitional phrase, etc.) is so tightly associated to be
deemed obligatory for the given verb sense, it is
also explicitly put into the list of arguments. The
P(CE)DT use about 35 free modifications (such as
LOC, DIR1, TWHEN, TTILL, CAUS, AIM, ...), most
of which can be marked as obligatory with certain
verbs (verb senses).
At each valency slot, requirements on surface syn-
tactic structure and inflectional properties of the ar-
guments may be given. This is much more complex
in inflective languages but it is used in English too,
often as a ?code? assigned to a verb sense, e.g. in
OALDCE (Crowther, 1998).
For details of surface-syntactic structural and
morphological requirements related to Czech va-
lency and subcategorization in Czech, see e.g. Ure-
?ov? (2011a; 2011b).
For the annotation of (general) MWEs (Bejc?ek
and Stran??k, 2010) in the P(CE)DT, the following
principle have been chosen: each MWE is repre-
sented by a single node in the deep dependency
tree. This accords with our principles that ?deep?
representation should abstract from (the peculiari-
ties and idiosyncrasies of) surface syntax and rep-
resent ?meaning.?6 The syntactic (and related mor-
phological) representation of MWEs is annotated at
a ?lower?, purely syntactic dependency layer (here,
each word token is represented by its own node).
5We say that a verb has (zero or more) valency slots; the
verb give as presented here has three.
6Under this assumption, each node in such a dependency
tree should ideally represent a single unit of meaning, and
the ?meaning? of the tree - typically representing a sentence
- should be derived compositionally from the meanings of the
individual nodes and their (labeled, dependency) relations (i.e.
functors, as they are called in the PDT-style treebanks).
59
Subsequently, the two representations are linked.
However, here arises a problem with modifiable
MWEs (such as lose his/my/their/... head): if the
whole MWE is represented as a single node, the
modifier relation to the MWE would be ambiguous
if put simply as the dependent of the MWE (i.e.,
which part of the MWE does it modify?). There-
fore, a rather technical, but unambiguous solution
was adopted: the verb as the head of the verbal
MWE is represented by a node, and the ?rest? of
the MWE gets its own appropriately marked node
(technically dependent on the verb node). Such a re-
lation is labeled with the DPHR functor (?Dependent
part of a PHRase?). The modifier of the MWE can
thus be unambiguously attached as either the depen-
dent node of the verb (if it modifies the whole MWE,
such as a temporal adverbial in hit the books on Sun-
day), or to the DPHR node (if it modifies only that
part of the MWE, such as in hit the history books).7
We believe that this solution which allows the flex-
ibility of considering also modifiable verbal VNICs
to be annotated formally in the same way as fully
fixed VNICs is original in the PDT family of tree-
banks, since we have not seen it neither in the Penn
Treebank nor in other treebanks, including depen-
dency ones.
Since DPHR is technically a dependent node, it
can then be formally included as a slot in the va-
lency dictionary, adding the surface syntactic and/or
morphological representation in the form of an en-
coded surface dependency representation, such as in
the following example of an English VNIC:
make DPHR(mountain.Obj.sg[a],
out[of,molehill.Adv.sg[a])
In Czech, the formal means are extended, e.g. for
the required case (1 - nominative, 6- locative):8
be?hat DPHR(mr?z.S1,po[z?da.P6])
7One can argue that in very complex MWEs, this simple
split into two nodes might not be enough; in the treebanks we
have explored no such multiple dependent modifiers exist.
8The repertoire of possible syntactic and morphological con-
straints, which can be used for the description of possible forms
of the fixed part of the idiomatic expression, covers all aspects
of Czech word formation: case, number, grammatical gender,
possessive gender and number, degree of comparison, nega-
tion, short/long form of certain adjectives, analytical depen-
dency function etc.
make
PRED
complex v
a mountain out of a molehill   
DPHR
dphr n.denot
.
.
making
Pred
a
AuxA
mountain
Obj
out
AuxP
of
AuxP
a
AuxA
molehill
Adv
 
 
  
 
 
 
Figure 1: Verbal MWE: tectogrammatical (left) and syn-
tactic (right) annotation of a VNIC
In Fig. 1, the phrase making a mountain out of a
mole is syntactically annotated in the following way:
? mountain is annotated as the syntactic direct
object of making,
? out of a molehill is annotated as a prepositional
phrase (with the preposition as the head)
On the tectogrammatical layer of annotation, the
verb is the head and the defining part of the MWE
gets a separate node (marked by DPHR).
In the corpus-based analysis of verbal MWEs in
the valency lexicons and the treebanks presented
here, we concentrate mainly on VNICs (see Sect. 1)
and briefly mention LVCs, since the boundary be-
tween them is often a bit grayish. In the P(CE)DT
treebanks, LVCs are always represented as two
nodes: the (light) verb node and the noun com-
plement node. Formally, the representing structure
is the same for both mentioned groups of MWEs,
but it differs in the labels of the verb arguments:
CPHR (Compound PHRase) for LVCs vs. DPHR for
VNICs. Whereas lexical units marked as DPHRs are
mostly limited to a fixed number of words and there-
fore are listed in the lexicon, lexical units marked
as CPHRs are often not limited in their number and
therefore it does not make sense to list them all in
the lexicon.
60
A possible solution to the problem of automatic
identification of (general) MWEs in texts using the
annotation found in the PDT, which is related to the
topic described in this paper but goes beyond its
scope, can be found in (Bejcek et al, 2013).
3 Corpus Analysis
To compare annotation and use of VNICs in Czech
and English, we have used the PCEDT. The PCEDT
contains alignment information, thus it was easy to
extract all cases where a VNIC was annotated (i.e.
where the DPHR functor occurs).9
We found a total of 92890 occurrences of aligned
(non-auxiliary) verbs. Czech VNICs were aligned
with English counterparts not annotated as a VNIC
in 570 cases, and there were 278 occurrences of En-
glish VNICs aligned with Czech non-VNICs, and
only 88 occurrences of VNICs annotated on both
sides were aligned.10 These figures are surpris-
ingly small (less than 1.5% of verbs are marked
as VNICs), however, (a) it is only the VNIC type
(e.g., phrasal verbs would account for far more), and
(b) the annotator guidelines asked for ?conservative-
ness? in creating new VNIC-type verb senses.11
Ideally (for NLP), VNICs would be translated as
VNICs. However, as stated above, this occurred
only in a 88 cases only (a few examples are shown
below).
(1) (wsj0062) toc?it[turn] se[oneself-acc.]
z?dy[back-Noun-sg-instr.]:
thumb(ing) its nose
(2) (wsj0989) podr?ez?vat[saw down]
si[oneself-dat.] pod[under]
sebou[oneself-instr.]
ve?tev[branch-Noun-sg-acc.]:
bit(ing) the hand that feeds them
9The alignment is automatic, the Czech and English tec-
togrammatical annotation (including verb sense/frame assign-
ment) is manual.
10The total number of Czech VNICs in the PCEDT (1300) is
higher than the sum of extracted alignments (570+88=658). The
difference is due to many of the Czech VNICs being aligned to
a node which does not correspond to a verb, or which is not
linked to an English node, or where the alignment is wrong.
11By ?conservative? approach we mean that splitting of verb
senses into new ones has been discouraged in the annotation
guidelines.
Manual inspection of these alignments revealed
(except for a few gray-area cases) no errors. We have
thus concentrated on the asymmetric cases by man-
ually exploring 200 such cases on each side. The
results are summarized in Tab. 1.
Direction / VNIC VNIC
Annotated as in En, in Cz, Examples
(by type) not Cz not En
Correctly annotated (as non-VNIC)
LVC 26 4 l?mat[break]
rekordy:
set records
non-MWE 138 124 pr?eru?it
[interrupt]:
cut short
Annotation Error (should have been VNIC)
LVC 7 17 dr?et[hold]
krok[step]:
keep abreast
non-MWE 28 52 zlomit (mu)
srdce: break
sb?s heart
other error 1 3
Table 1: Breakdown of VNICs linked to non-VNICs
3.1 English VNICs Linked to Non-VNIC Czech
The first column of counts in Tab. 1 refers to cases
where the verb in the English original has been an-
notated as VNIC, but the Czech translation has been
marked as a non-VNIC. We have counted cases,
where we believe that the annotation is correct, even
if it is not annotated as a VNIC (164 in total) and
cases which should have been in fact annotated as a
VNIC (35 cases). Within these two groups, we sep-
arately counted cases where the translation has not
been annotated as a VNIC, but at least as a LVC,
another MWE type (total of 33 such cases). The
proportion of errors (approx. 18%) is higher than
the 5.5% rate reported for semantic relation annota-
tion (?te?p?nek, 2006). Typically, the error would be
corrected by adding a separate idiomatic verb sense
into the valency lexicon and adjusting the annotation
(verb sense and the DPHR label) accordingly.
61
3.2 Czech VNICs Linked to Non-VNIC English
The second column of counts in Tab. 1 shows the
same breakdown as described in the previous sec-
tion, but in the opposite direction: Czech VNICs
which in the English original have been annotated
differently. The first difference is in the number
of erroneously annotated tokens, which is visibly
higher (approx. twice as high) than in the opposite
direction both for LVCs (17) and for constructions
which have not been marked as MWEs at all (52).
This suggests that the authors of the English va-
lency lexicon and the annotators of the English deep
structure have been even more ?conservative? than
their Czech colleagues by not creating many VNIC-
typed verb senses.12 Second, there are only 4 cases
of VNICs translated into and correctly annotated as
LVCs, compared to the English ? Czech direction
(26 cases).
4 Conclusions
We have described the treatment of (an enriched set
of) verb-noun idiomatic combinations (and briefly
other types of MWEs) in the PDT style treebanks
and in the associated valency lexicons. We have
explored the PCEDT to find interesting correspon-
dences between the annotation and lexicon entries
in the English and Czech annotation schemes.
We have found that VNICs, as one of the
types of MWEs, are translated in different ways.
A translation of a VNIC as a VNIC is rare,
even if we take into account the annotation errors
(88+7+17+28+52=192 cases of the 936 extracted).
By far the most common case of translating a VNIC
in both directions is the usage of a completely non-
MWE phrase. There is also a substantial amount
of errors in each direction, higher in cases where
the Czech translation was annotated as a VNIC and
the English original was not. While the low overall
number of VNICs found in the parallel corpus can be
explained by not considering standard phrasal verbs
for this study and by the required conservatism in
marking a phrase as a true VNIC, we can only specu-
late why only a small proportion of VNICs are trans-
lated as VNICs in(to) the other language: manual
12None of the annotators of the English side of the parallel
treebank was a fully native English speaker, which might also
explain this ?conservatism.?
inspection of several cases suggested (but without
a statistically significant conclusions) that this does
not seem to be caused by the specific nature or genre
of the Wall Street Journal texts, but rather by the fact
that the two languages explored, Czech and English,
went generally through different developments un-
der different circumstances and contexts throughout
the years they evolved separately.
While this paper describes only an initial analy-
sis of multiword expressions (of the verb-noun id-
iomatic combination type) in parallel treebanks, we
plan to apply the same classification and checks as
described here to the whole corpus (perhaps auto-
matically to a certain extent), to discover (presum-
ably) even more discrepancies and also more corre-
spondence types. These will again be classified and
corrections in the data will be made. Eventually, we
will be able to get a more reliable material for a thor-
ough study of the use of MWEs in translation, with
the aim of improving identification and analysis of
MWEs (e.g., by enriching the approach taken by and
described in (Bejcek et al, 2013)). We would also
like to improve machine translation results by iden-
tifying relevant features of MWEs (including but not
limited to VNICs) and using the associated informa-
tion stored in the valency lexicons in order to learn
translation correspondences involving MWEs.
Acknowledgments
The authors would like to thank the four reviewers,
especially reviewer #4, for helpful comments which
hopefully have lead to a clearer version of this pa-
per. Also, we would like to thank to all the anno-
tators and technical support staff who made our re-
search possible by creating the treebanks and lexi-
cons which we can now build upon.
References
Vilmos ?gel, Ludwig M. Eichinger, Hans-Werner Eroms,
Peter Hellwig, Hans J?rgen Heringer, Henning Lobin,
and Guta Rau. 2006. Dependenz und Valenz. Walter
de Gruyter, Berlin & New York.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, FL. ISBN 978-1420085921.
62
Eduard Bejc?ek and Pavel Stran??k. 2010. Annotation
of multiword expressions in the prague dependency
treebank. Language Resources and Evaluation, 44(1-
2):7?21.
Eduard Bejcek, Pavel Pecina, and Pavel Stranak. 2013.
Syntactic Identification of Occurrences of Multiword
Expressions in Text using a Lexicon with Dependency
Structures. In Workshop on Multiword Expressions
(NAACL 2013, this volume), New Jersey. Association
for Computational Linguistics.
Alena B?hmov?, Silvie Cinkov?, and Eva Hajic?ov?.
2005. A Manual for Tectogrammatical Layer An-
notation of the Prague Dependency Treebank (En-
glish translation). Technical report, ?FAL MFF UK,
Prague, Czech Republic.
Nicoletta Calzolari, Charles J. Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine Macleod, and
Antonio Zampolli. 2002. Towards best practice for
multiword expressions in computational lexicons. In
LREC.
Jonathan Crowther. 1998. Oxford Advanced Learner?s
Dictionary. Cornelsen & Oxford, 5th edition.
A.E. Goldberg. 1995. Constructions: A Construction
Grammar Approach to Argument Structure. Univer-
sity of Chicago Press.
Jan Hajic?, Eduard Bejc?ek, Jarmila Panevov?, Jir??
M?rovsk?, Johanka Spoustov?, Jan ?te?p?nek, Pavel
Stran??k, Pavel ?id?k, Pavl?na Vimmrov?, Eva
?t?astn?, Magda ?evc??kov?, Lenka Smejkalov?, Petr
Homola, Jan Popelka, Mark?ta Lopatkov?, Lucie
Hrabalov?, Natalia Klyueva, and Zdene?k ?abokrt-
sk?. 2012a. Prague Dependency Treebank 2.5.
https://ufal-point.mff.cuni.cz/xmlui/handle/11858/00-
097C-0000-0006-DB11-8.
Jan Hajic?, Eva Hajic?ov?, Jarmila Panevov?, Petr Sgall,
Ondr?ej Bojar, Silvie Cinkov?, Eva Fuc??kov?, Marie
Mikulov?, Petr Pajas, Jan Popelka, Jir?? Semeck?, Jana
?indlerov?, Jan ?te?p?nek, Josef Toman, Zden?ka Ure-
?ov?, and Zdene?k ?abokrtsk?. 2012b. Announcing
Prague Czech-English Dependency Treebank 2.0. In
Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC 2012),
pages 3153?3160, I?stanbul, Turkey. ELRA, European
Language Resources Association.
V?clava Kettnerov?. 2012. Lexik?lne?-s?mantick? kon-
verze ve valenc?n?m slovn?ku. Ph.D. thesis, Charles
University, Prague, Czech Republic.
Mark?ta Lopatkov?. 2010. Valency Lexicon of Czech
Verbs: Towards Formal Description of Valency and
Its Modeling in an Electronic Language Resource.
Prague.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313?330.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Jarmila Panevov?. 1998. Je?te? k teorii valence. Slovo a
slovesnost, 59(1):1?14.
Jarmila Panevov?. 1977. Verbal Frames Revisited. The
Prague Bulletin of Mathematical Linguistics, (28):55?
72.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proc. of
the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing
2002), pages 1?15.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. Dordrecht, Reidel, and Prague,
Academia.
Petr Sgall. 1998. Teorie valence a jej? form?ln? zpracov-
?n?. Slovo a slovesnost, 59(1):15?29.
Jan ?te?p?nek. 2006. Post-annotation Checking of Prague
Dependency Treebank 2.0 Data. In Lecture Notes
in Artificial Intelligence, Text, Speech and Dialogue.
9th International Conference, TSD 2006, Brno, Czech
Republic, September 11?15, 2006, volume 4188 of
Lecture Notes in Computer Science, pages 277?284,
Berlin / Heidelberg. Springer.
Lucien Tesni?re. 1959. ?l?ments de syntaxe structurale.
Editions Klincksieck, Paris.
Zden?ka Ure?ov?. 2011a. Valence sloves v Pra?sk?m
z?vislostn?m korpusu. Studies in Computational and
Theoretical Linguistics. ?stav form?ln? a aplikovan?
lingvistiky, Prague.
Zden?ka Ure?ov?. 2011b. Valenc?n? slovn?k Pra?sk?ho
z?vislostn?ho korpusu (PDT-Vallex). Studies in Com-
putational and Theoretical Linguistics. ?stav form?ln?
a aplikovan? lingvistiky, Prague.
63
