Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 165?168,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Learning of Acoustic Sub-word Units
Balakrishnan Varadarajan? and Sanjeev Khudanpur?
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{bvarada2,khudanpur}@jhu.edu
Emmanuel Dupoux
Laboratoire de Science Cognitive
et Psycholinguistique
75005, Paris, France
emmanuel.dupoux@gmail.com
Abstract
Accurate unsupervised learning of phonemes
of a language directly from speech is demon-
strated via an algorithm for joint unsupervised
learning of the topology and parameters of
a hidden Markov model (HMM); states and
short state-sequences through this HMM cor-
respond to the learnt sub-word units. The
algorithm, originally proposed for unsuper-
vised learning of allophonic variations within
a given phoneme set, has been adapted to
learn without any knowledge of the phonemes.
An evaluation methodology is also proposed,
whereby the state-sequence that aligns to
a test utterance is transduced in an auto-
matic manner to a phoneme-sequence and
compared to its manual transcription. Over
85% phoneme recognition accuracy is demon-
strated for speaker-dependent learning from
fluent, large-vocabulary speech.
1 Automatic Discovery of Phone(me)s
Statistical models learnt from data are extensively
used in modern automatic speech recognition (ASR)
systems. Transcribed speech is used to estimate con-
ditional models of the acoustics given a phoneme-
sequence. The phonemic pronunciation of words
and the phonemes of the language, however, are
derived almost entirely from linguistic knowledge.
In this paper, we investigate whether the phonemes
may be learnt automatically from the speech signal.
Automatic learning of phoneme-like units has sig-
nificant implications for theories of language ac-
quisition in babies, but our considerations here are
somewhat more technological. We are interested in
developing ASR systems for languages or dialects
? This work was partially supported by National Science
Foundation Grants No
?
IIS-0534359 and OISE-0530118.
for which such linguistic knowledge is scarce or
nonexistent, and in extending ASR techniques to
recognition of signals other than speech, such as ma-
nipulative gestures in endoscopic surgery. Hence an
algorithm for automatically learning an inventory of
intermediate symbolic units?intermediate relative
to the acoustic or kinematic signal on one end and
the word-sequence or surgical act on the other?is
very desirable.
Except for some early work on isolated word/digit
recognition (Paliwal and Kulkarni, 1987; Wilpon
et al, 1987, etc), not much attention has been
paid to automatic derivation of sub-word units from
speech, perhaps because pronunciation lexicons are
now available1 in languages of immediate interest.
What has been investigated is automatically learn-
ing allophonic variations of each phoneme due to
co-articulation or contextual effects (Takami and
Sagayama, 1992; Fukada et al, 1996); the phoneme
inventory is usually assumed to be known.
The general idea in allophone learning is to be-
gin with an inventory of only one allophone per
phoneme, and incrementally refine the inventory to
better fit the speech signal. Typically, each phoneme
is modeled by a separate HMM. In early stages of
refinement, when very few allophones are available,
it is hoped that ?similar? allophones of a phoneme
will be modeled by shared HMM states, and that
subsequent refinement will result in distinct states
for different allophones. The key therefore is to de-
vise a scheme for successive refinement of a model
shared by many allophones. In the HMM setting,
this amounts to simultaneously refining the topol-
ogy and the model parameters. A successive state
splitting (SSS) algorithm to achieve this was pro-
posed by Takami and Sagayama (1992), and en-
1See http://www.ldc.upenn.edu/Catalog/byType.jsp
165
hanced by Singer and Ostendorf (1996). Improve-
ments in phoneme recognition accuracy using these
derived allophonic models over phonemic models
were obtained.
In this paper, we investigate directly learning the
allophone inventory of a language from speech with-
out recourse to its phoneme set. We begin with a
one-state HMM for all speech sounds and modify
the SSS algorithm to successively learn the topol-
ogy and parameters of HMMs with even larger num-
bers of states. States sequences through this HMM
are expected to correspond to allophones. The most
likely state-sequence for a speech segment is inter-
preted as an ?allophonic labeling? of that speech by
the learnt model. Performance is measured by map-
ping the resultant state-sequence to phonemes.
One contribution of this paper is a significant im-
provement in the efficacy of the SSS algorithm as
described in Section 2. It is based on observing
that the improvement in the goodness of fit by up
to two consecutive splits of any of the current HMM
states can be evaluated concurrently and efficiently.
Choosing the best subset of splits from among these
is then cast as a constrained knapsack problem, to
which an efficient solution is devised. Another con-
tribution of this paper is a method to evaluate the
accuracy of the resulting ?allophonic labeling,? as
described in Section 3. It is demonstrated that if
a small amount of phonetically transcribed speech
is used to learn a Markov (bigram) model of state-
sequences that arise from each phone, an evalua-
tion tool results with which we may measure phone
recognition accuracy, even though the HMM labels
the speech signal not with phonemes but merely a
state-sequence. Section 4 presents experimental re-
sults, where the performance accuracies with differ-
ent learning setups are tabulated. We also see how as
little as 5 minutes of speech is adequate for learning
the acoustic units.
2 An Improved and Fast SSS Algorithm
The improvement of the SSS algorithm of Takami
and Sagayama (1992), renamed ML-SSS by Singer
and Ostendorf (1996), proceeds roughly as follows.
1. Model all the speech2 using a 1-state HMM
with a diagonal-covariance Gaussian. (N=1.)
2Note that the original application of SSS was for learning
Figure 1: Modified four-way split of a state s.
2. For each HMM state s, compute the gain in log-
likelihood (LL) of the speech by either a con-
textual or a temporal split of s into two states
s1 and s2. Among the N states, select and and
split the one that yields the most gain in LL.
3. If the gain is above a threshold, retain the split
and set N = N + 1; furthermore, if N is less
than desired, re-estimate all parameters of the
new HMM, and go to Step 2.
Note that the key computational steps are the for-
loop of Step 2 and the re-estimation of Step 3.
Modifications to the ML-SSS Algorithm: We
made the following modifications that are favorable
in terms of greater speed and larger search space,
thereby yielding a gain in likelihood that is poten-
tially greater than the original ML-SSS.
1. Model all the speech using a 1-state HMM with
a full-covariance Gaussian density. Set N = 1.
2. Simultaneously replace each state s of the
HMM with the 4-state topology shown in Fig-
ure 1, yielding a 4N -state HMM. If the state s
had parameters (?s,?s), then means of its 4-
state replacement are ?s1 = ?s? ? = ?s4 and
?s2 = ?s +? = ?s3 , with ? = ?
?v?, where ??
and v? are the principal eigenvalue and eigen-
vector of ?s and 0 <  1 is typically 0.2.
3. Re-estimate all parameters of this (overgrown)
HMM. Gather the Gaussian sufficient statistics
for each of the 4N states from the last pass
of re-estimation: the state occupancy pisi . The
sample mean ?si , and sample covariance ?si .
4. Each quartet of states (see Figure 1) that re-
sulted from the same original state s can be
the allophonic variations of a phoneme; hence the phrase ?all
the speech? meant all the speech corresponding separately to
each phoneme. Here it really means all the speech.
166
merged back in different ways to produce 3, 2
or 1 HMM states. There are 6 ways to end up
with 3 states, and 7 to end up with 2 states. Re-
tain for further consideration the 4 state split of
s, the best merge back to 3 states among the 6
ways, the best merge back to 2 states among the
7 ways, and the merge back to 1 state.
5. Reduce the number of states from 4N toN+?
by optimally3 merging back quartets that cause
the least loss in log-likelihood of the speech.
6. Set N = N + ?. If N is less than the desired
HMM size, retrain the HMM and go to Step 2.
Observe that the 4-state split of Figure 1 permits a
slight look-ahead in our scheme in the sense that the
goodness of a contextual or temporal split of two dif-
ferent states can be compared in the same iteration
with two consecutive splits of a single state. Also,
the split/merge statistics for a state are gathered in
our modified SSS assuming that the other states have
already been split, which facilitates consideration of
concurrent state splitting. If s1, . . . , sm are merged
into s?, the loss of log-likelihood in Step 4 is:
d
2
m?
i=1
pisi log |?s?| ?
d
2
m?
i=1
pisi log |?si | , (1)
where ?s? =
?m
i=1 pisi
(
?si + ?si?
?
si
)
?m
i=1 pisi
? ?s??
?
s?.
Finally, in selecting the best ? states to add to the
HMM, we consider many more ways of splitting the
N original states than SSS does. E.g. going up from
N = 6 toN+? = 9 HMM states could be achieved
by a 4-way split of a single state, a 3-way split of one
state and 2-way of another, or a 2-way split of three
distinct states; all of them are explored in the process
of merging from 4N = 24 down to 9 states. Yet, like
SSS, no original state s is permitted to merge with
another original state s?. This latter restriction leads
to an O(N5) algorithm for finding the best states to
merge down4. Details of the algorithm are ommited
for the sake of brevity.
In summary, our modified ML-SSS algorithm can
leap-frog by ? states at a time, e.g. ? = ?N , com-
pared to the standard algorithm, and it has the benefit
of some lookahead to avoid greediness.
3This entails solving a constrained knapsack problem.
4This is a restricted version of the 0-1 knapsack problem.
3 Evaluating the Goodness of the Labels
The HMM learnt in Section 2 is capable of assign-
ing state-labels to speech via the Viterbi algorithm.
Evaluating whether these labels are linguistically
meaningful requires interpreting the labels in terms
of phonemes. We do so as follows.
Some phonetically transcribed speech is labeled
with the learnt HMM, and the label sequences cor-
responding to each phone segment are extracted.
Since the HMM was learnt from unlabeled speech,
the labels and short label-sequences usually corre-
spond to allophones, not phonemes. Therefore, for
each triphone, i.e. each phone tagged with its left-
and right-phone context, a simple bigram model of
label sequences is estimated. An unweighted ?phone
loop? that accepts all phone sequences is created,
and composed with these bigram models to cre-
ate a label-to-phone transducer capable of mapping
HMM label sequences to phone sequences.
Finally, the test speech (not used for HMM learn-
ing, nor for estimating the bigram model) is treated
as having been ?generated? by a source-channel
model in which the label-to-phone transducer is the
source?generating an HMM state-sequence?and
the Gaussian densities of the learnt HMM states con-
stitute the channel?taking the HMM state-sequence
as the channel input and generating the observed
speech signal as the output. Standard Viterbi decod-
ing determines the most likely phone sequence for
the test speech, and phone accuracy is measured by
comparison with the manual phonetic transcription.
4 Experimental Results
4.1 Impact of the Modified State Splitting
The ML-SSS procedure estimates 2N different
N+1-state HMMs to grow from N to N+1 states.
Our procedure estimates one 4N state HMM to
grow to N+?, making it hugely faster for large N .
Table 1 compares the log-likelihood of the train-
ing speech for ML-SSS and our procedure. The re-
sults validate our modifications, demonstrating that
at least in the regimes feasible for ML-SSS, there is
no loss (in fact a tiny gain) in fitting the speech data,
and a big gain in computational effort5.
5ML-SSS with ?=1 was impractical beyond N=22.
167
# of states SSS (? = 1) ? = 3 ? = N
8 -7.14 -7.13 -7.13
10 -7.08 -7.06 -7.06
22 -6.78 -6.76 N/A
40 N/A -6.23 -6.20
Table 1: Aggressive state splitting does not cause any
degradation in log-likelihood relative to ML-SSS.
4.2 Unsupervised Learning of Sub-word Units
We used about 30 minutes of phonetically tran-
scribed Japanese speech from one speaker6 provided
by Maekawa (2003) for our unsupervised learning
experiments. The speech was segmented via silence
detection into 800 utterances, which were further
partitioned into a 24-minute training set (80%) and
6-minute test set (20%).
Our first experiment was to learn an HMM from
the training speech using our modified ML-SSS pro-
cedure; we tried N = 22, 70 and 376. For each N ,
we then labeled the training speech using the learnt
HMM, used the phonetic transcription of the train-
ing speech to estimate label-bigram models for each
triphone, and built the label-to-phone transducer as
described in Section 3. We also investigated (i) using
only 5 minutes of training speech to learn the HMM,
but still labeling and using all 24 minutes to build
the label-to-phone transducer, and (ii) setting aside
5 minutes of training speech to learn the transducer
and using the rest to learn the HMM. For each learnt
HMM+transducer pair, we phonetically labeled the
test speech.
The results in the first column of Table 2 suggest
that the sub-word units learnt by the HMM are in-
deed interpretable as phones. The second column
suggests that a small amount of speech (5 minutes)
may be adequate to learn these units consistently.
The third column indicates that learning how to map
the learnt (allophonic) units to phones requires rela-
tively more transcribed speech.
4.3 Inspecting the Learnt Sub-word Units
The most frequent 3-, 4- and 5-state sequences in the
automatically labeled speech consistently matched
particular phones in specific articulatory contexts, as
6We heeded advice from the literature indicating that au-
tomatic methods model gross channel- and speaker-differences
before capturing differences between speech sounds.
HMM 24 min 5 min 19 min
label-to-phone 24 min 24 min 5 min
27 states 71.4% 70.9% 60.2%
70 states 84.4% 84.7% 75.8%
376 states 87.2% 86.8% 76.6%
Table 2: Phone recognition accuracy for different HMM
sizes (N), and with different amounts of speech used to
learn the HMM labeler and the label-to-phone transducer.
shown below, i.e. the HMM learns allophones.
HMM labels L-contxt Phone R-contxt
11, 28, 32 vowel t [e|a|o]
15, 17, 2 [g|k] [u|o] [?]
3, 17, 2 [k|t|g|d] a [k|t|g|d]
31, 5, 13, 5 vowel [s|sj|sy] vowel
17, 2, 31, 11 [g|t|k|d] [a|o] [t|k]
3, 30, 22, 34 [?] a silence
6, 24, 8, 15, 22 [?] o silence
4, 3, 17, 2, 21 [k|t] a [k|t]
4, 17, 24, 2, 31 [s|sy|z] o [t|d]
[t|d] o [s|sy|z]
For instance, the label sequence 3, 17, 2, corre-
sponds to an ?a? surrounded by stop consonants
{t, d, k, g}; further restricting the sequence to
4, 3, 17, 2, 21, results in restricting the context to the
unvoiced stops {t, k}. That such clusters are learnt
without knowledge of phones is remarkable.
References
T. Fukada, M. Bacchiani, K. K. Paliwal, and Y. Sagisaka.
1996. Speech recognition based on acoustically de-
rived segment units. In ICSLP, pages 1077?1080.
K. Maekawa. 2003. Corpus of spontaneous japanese:
its design and evaluation. In ISCA/IEEE Workshop on
Spontaneous Speech Processing and Recognition.
K. K. Paliwal and A. M. Kulkarni. 1987. Segmenta-
tion and labeling using vector quantization and its ap-
plication in isolated word recognition. Journal of the
Acoustical Society of India, 15:102?110.
H. Singer and M. Ostendorf. 1996. Maximum likelihood
successive state splitting. In ICASSP, pages 601?604.
J. Takami and S. Sagayama. 1992. A successive state
splitting algorithm for efficient allophone modeling.
In ICASSP, pages 573?576.
J. G. Wilpon, B. H. Juang, and L. R. Rabiner. 1987. An
investigation on the use of acoustic sub-word units for
automatic speech recognition. In ICASSP, pages 821?
824.
168
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 120?123,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
?-extension Hidden Markov Models and Weighted Transducers for
Machine Transliteration
Balakrishnan Vardarajan
Dept. of Electrical and Computer Engineering
Johns Hopkins University
bvarada2@jhu.edu
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
Abstract
We describe in detail a method for translit-
erating an English string to a foreign
language string evaluated on five differ-
ent languages, including Tamil, Hindi,
Russian, Chinese, and Kannada. Our
method involves deriving substring align-
ments from the training data and learning a
weighted finite state transducer from these
alignments. We define an ?-extension Hid-
den Markov Model to derive alignments
between training pairs and a heuristic to
extract the substring alignments. Our
method involves only two tunable parame-
ters that can be optimized on held-out data.
1 Introduction
Transliteration is a letter by letter mapping of one
writing system to another. Apart from the obvi-
ous use in writing systems, transliteration is also
useful in conjunction with translation. For exam-
ple, machine translation BLEU scores are known
to improve when named entities are transliterated.
This engendered several investigations into auto-
matic transliteration of strings, named entities in
particular, from one language to another. See
Knight and Graehl(1997) and later papers on this
topic for an overview.
Hidden Markov Model (HMM) (Rabiner,
1989) is a standard sequence modeling tool used
in various problems in natural language process-
ing like machine translation, speech recognition,
part of speech tagging and information extraction.
There have been earlier attempts in using HMMs
for automatic transliteration. See (Abdul Jaleel
and Larkey, 2003; Zhou et al, 2008) for exam-
ple. In this paper, we define an ?-extension Hid-
den Markov Model that allows us to align source
and target language strings such that the charac-
ters in the source string may be optionally aligned
to the ? symbol. We also introduce a heuristic that
allows us to extract high quality sub-alignments
from the ?-aligned word pairs. This allows us to
define a weighted finite state transducer that pro-
duces transliterations for an English string by min-
imal segmentation.
The overview of this paper is as follows: Sec-
tion 2 introduces ?-extension Hidden Markov
Model and describes our alignment procedure.
Section 3 describes the substring alignment
heuristic and our weighted finite state transducer
to derive the final n-best transliterations. We con-
clude with a result section describing results from
the NEWS 2009 shared task on five different lan-
guages.
2 Learning Alignments
The training data D is given as pairs of strings
(e, f) where e is the English string with the cor-
responding foreign transliteration f . The English
string e consists of a sequence of English letters
(e1, e2, . . . , eN ) while f = (f1, f2, . . . , fM ) .
We represent E as the set of all English symbols
and F as the set of all foreign symbols.1 We also
assume both languages have a special null symbol
?, that is ? ? E and ? ? F .
Our alignment model is a Hidden Markov
Model H(X,Y,S,T,Ps), where
? X is the start state and Y is the end state.
? S is the set of emitting states with S = |S|.
The emitting states are indexed from 1 to S.
The start state X is indexed as state 0 and the
end state Y is indexed as state S + 1.
? T is an (S + 1) ? (S + 1) stochastic matrix
with T = [tij] for i ? {0, 1, . . . , S} and j ?
{1, 2, . . . , S + 1}.
1Alphabets and diacritics are treated as separate symbols.
120
? Ps = [pef ] is an |E| ? |F| matrix of joint
emission probabilities with pef = P (e, f |s)
?s ? S .
We define s? to be an ?-extension of a string of
characters s = (c1, c2, . . . , ck) as the string ob-
tained by pumping an arbitrary number of ? sym-
bols between any two adjacent characters cl and
cl+1. That is, s? = (di1 , . . . , di2 , . . . , dik) where
dij = cj and dl = ? for im < l < im+1 where
1 ? l < k. Observe that there are countably infi-
nite ?-extensions for a given string s since an arbi-
trary number of ? symbols can be inserted between
characters cm and cm+1. Let T (s) denote the set
of all possible ?-extensions for a given string s.
For a given pair of strings (u, v), we define a
joint ?-extension of (u, v) as the pair (u?, v?) s.t. u? ?
T (u) and v? ? T (v) with |u?| = |v?| and ?i s.t.
u?i = v?i = ?. Due to this restriction, there are finite
?-extensions for a pair (u, v) with the length of u?
and v? bounded above by |u| + |v|. 2 Let J(u, v)
denote the set of all joint ?-extensions of (u, v).
Given a pair of strings (e, f) with e =
(e1, e2, . . . , eN ) and f = (f1, f2, . . . , fM ), we
compute the probability ?(e, f, s?) that they are
transliteration pairs ending in state s? as
?(e, f, s?) =
?
(e?,?f)?J(e,f)
?
0=s0,...,s|e?|=s?
t0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
In order to compute the probability Q(e, f) of a
given transliteration pair, the final state has to be
the end state S + 1. Hence
Q(e, f) =
S
?
s=1
?(e, f, s)ts,S+1 (1)
We also write the probability ?(e, f, s?) that they
are transliteration pairs starting in state s? as
?(e, f, s?) =
?
(e?,?f)?J(e,f)
?
s?=s0,...,s|e?|+1=S+1
ts0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
Again noting that the start state of the HMM
H is 0, we have Q(e, f) =
S
?
s=1
?(e, f, s)t0,s. We
2|u?| = |v?| > |u| + |v| would imply ?i s.t. u?i = v?i = ?
which contradicts the definition of joint ?-extension.
denote a subsequence of a string u as umn =
(un, un+1, . . . , um) . Using these definitions, we
can define ?(ei1, f
j
1 , s) as
?
?
?
?
?
?
?
?
?
1 i = j = 0, s = 0
0 i = j = 0, s 6= 0
t0,sP (e1, f1|s) i = j = 1
PS
s?=1 ts?,s?(ei1, f
j?1
1 , s?)P (?, fj |s) i = 1, j > 1
PS
s?=1 ts?,s?(ei?11 , f
j
1 , s?)P (ei, ?|s) i > 1, j = 1
Finally for i > 1 and j > 1,
?(ei1, f j1 , s) =
?
s??S
ts?,s[?(ei1, f j?11 , s?)P (?, fj |s)+
?(ei?11 , f
j
1 , s?)P (ei, ?|s)+
?(ei?11 , f
j?1
1 , s?)P (ei, fj|s)]
Similarly the recurrence for ?(eNi , fMj , s)
?
?
?
?
?
ts,S+1 i = N + 1,
j = M + 1
PS
s?=1 ts,s??(eNi , fMj+1, s?)P (?, fj |s?) i = N, j < M
PS
s?=1 ts,s??(eNi+1, fMj , s?)P (ei, ?|s?) i < N, j = M
For i < N and j < M , ?(eNi , fMj , s) =
?
s??S
ts,s?[?(eNi , fMj+1, s?)P (?, fj |s?)+
?(eNi+1, fMj , s?)P (ei, ?|s?)+
?(eNi+1, fMj+1, s?)P (ei, fj|s?)]
In order to proceed with the E.M. estimation
of the parameters T and Ps , we collect the
soft counts c(e, f |s) for emission probabilities by
looping over the training data D as shown in Fig-
ure 1.
Similarly the soft counts ct(s?, s) for the tran-
sition probabilities are estimated as shown in Fig-
ure 2.
Finally the probabilities P (e, f |s) and tij are re-
estimated as
P? (e, f |s) = c(e, f |s)?
e?E,f?F c(e, f |s)
(2)
t?s?,s =
ct(s?, s)
?
s ct(s?, s)
(3)
We can also compute the most probable align-
ment (e?, f? ) between the two strings e and f as
121
c(e, f |s) =
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei?11 , f
j?1
1 , s?)ts?,sP (ei, fj |s)?(eNi , fMj , s)1(ei = e, fj = f)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei?11 , f
j
1 , s?)ts?,sP (ei, ?|s)?(eNi , fMj , s)1(ei = e, fj = f)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?
s?
?(ei1, f j?11 , s?)ts?,sP (?, fj |s)?(eNi , fMj , s)1(ei = e, fj = f)
Figure 1: EM soft count c(e, f |s) estimation.
ct(s?, s) =
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei?11 , f
j?1
1 , s?)ts?,sP (ei, fj|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei?11 , f
j
1 , s?)ts?,sP (ei, ?|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)
N
?
i=1
M
?
j=1
?(ei1, f j?11 , s?)ts?,sP (?, fj|s)?(eNi , fMj , s)
+
?
(e,f)?D
1
Q(e, f)?(e
N
1 , fM1 , s?)ts?,S+11(s = S + 1)
Figure 2: EM soft count ct(s?, s) estimation.
122
arg max
(e?,?f)?J(e,f)
?
0=s0,...,s|e?|+1=S+1
t0,s1
|e?|
?
i=1
tsi,si+1P (e?i, f?i|si)
The pair (e?, f?) is considered as an alignment be-
tween the training pair (e, f).
3 Transduction of the Transliterated
Output
Given an alignment (e?, f?), we consider all possi-
ble sub-alignments (e?ji , f?
j
i ) as pairs of substrings
obtained from (e?, f? ) such that e?i 6= ?, f?i 6= ?,
e?j+1 6= ? and f?j+1 6= ? . We extract all pos-
sible sub-alignments of all the alignments from
the training data. Let A be the bag of all sub-
alignments obtained from the training data. We
build a weighted finite state transducer that trans-
duces any string in E+ to F+ using these sub-
alignments.
Let (u,v) be an element of A. From the train-
ing data D, observe that A can have multiple re-
alizations of (u,v). Let N(u,v) be the number
of times (u,v) is observed in A. The empirical
probability of transducing string u to v is simply
P (v|u) = N(u,v)?
v:(u,v?)?A N(u,v?)
For every pair (u,v) ? A , we also compute the
probability of transliteration from the HMM H as
Q(u,v) from Equation 1.
We construct a finite state transducer Fu,v that
accepts only u and emits v with a weight wu,v
defined as
wu,v = ? log(P (v|u))?? log(Q(u,v))+? (4)
Finally we construct a global weighted finite
state transducer F by taking the union of all the
Fu,v and taking its closure.
F =
?
?
?
(u,v)?A
Fu,v
?
?
+
(5)
The weight ? is typically sufficiently high so
that a new english string is favored to be broken
into fewest possible sub-strings whose translitera-
tions are available in the training data.
We tune the weights ? and ? by evaluating the
accuracy on the held-out data. The n-best paths
in the weighted finite state transducer F represent
our n-best transliterations.
4 Results
We evaluated our system on the standard track data
provided by the NEWS 2009 shared task orga-
nizers on five different languages ? Tamil, Hindi,
Russian, and Kannada was derived from (Ku-
maran and Kellner, 2007) and Chinese from (Li et
al., 2004). The results of this evaluation on the test
data is shown in Table 1. For a detailed description
Language Top-1 mean MRR
Accuracy F1 score
Tamil 0.327 0.870 0.458
Hindi 0.398 0.855 0.515
Russian 0.506 0.901 0.609
Chinese 0.450 0.755 0.514
Kannada 0.235 0.817 0.353
Table 1: Results on NEWS 2009 test data.
of the evaluation measures used we refer the read-
ers to NEWS 2009 shared task whitepaper (Li et
al., 2009).
5 Conclusion
We described a system for automatic translitera-
tion of pairs of strings from one language to an-
other using ?-extension hidden markov models and
weighted finite state transducers. We evaluated
our system on all the languages for the NEWS
2009 standard track. The system presented is lan-
guage agnostic and can be trained for any language
pair within a few minutes on a single core desktop
computer.
References
Nasreen Abdul Jaleel and Leah Larkey. 2003. Statistical transliteration for english-arabic
cross language information retrieval. In Proceedings of the twelfth international con-
ference on Information and knowledge management, pages 139?146.
Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Computational Lin-
guistics, pages 128?135.
A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration.
In SIGIR ?07: Proceedings of the 30th annual international ACM SIGIR conference
on Research and development in information retrieval, pages 721?722, New York, NY,
USA. ACM.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine
transliteration. In ACL ?04: Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 159, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of
news 2009 machine transliteration shared task. In Proceedings of ACL-IJCNLP 2009
Named Entities Workshop (NEWS 2009).
Lawrence Rabiner. 1989. A tutorial on hidden markov models and selected applications in
speech recognition. In Proceedings of the IEEE, pages 257?286.
Yilu Zhou, Feng Huang, and Hsinchun Chen. 2008. Combining probability models and web
mining models: a framework for jproper name transliteration. Information Technology
and Management, 9(2):91?103.
123
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1?6,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exploring the Relative Role of Bottom-up and Top-down Information in
Phoneme Learning
Abdellah Fourtassi
1
, Thomas Schatz
1,2
, Balakrishnan Varadarajan
3
, Emmanuel Dupoux
1
1
Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris, France
2
SIERRA Project-Team, INRIA/ENS/CNRS, Paris, France
3
Center for Language and Speech Processing, JHU, Baltimore, USA
{abdellah.fourtassi; emmanuel.dupoux; balaji.iitm1}@gmail
thomas.schatz@laposte.net
Abstract
We test both bottom-up and top-down ap-
proaches in learning the phonemic status
of the sounds of English and Japanese. We
used large corpora of spontaneous speech
to provide the learner with an input that
models both the linguistic properties and
statistical regularities of each language.
We found both approaches to help dis-
criminate between allophonic and phone-
mic contrasts with a high degree of accu-
racy, although top-down cues proved to be
effective only on an interesting subset of
the data.
1 Introduction
Developmental studies have shown that, during
their first year, infants tune in on the phonemic cat-
egories (consonants and vowels) of their language,
i.e., they lose the ability to distinguish some
within-category contrasts (Werker and Tees, 1984)
and enhance their ability to distinguish between-
category contrasts (Kuhl et al, 2006). Current
work in early language acquisition has proposed
two competing hypotheses that purport to account
for the acquisition of phonemes. The bottom-up
hypothesis holds that infants converge on the lin-
guistic units of their language through a similarity-
based distributional analysis of their input (Maye
et al, 2002; Vallabha et al, 2007). In contrast,
the top-down hypothesis emphasizes the role of
higher level linguistic structures in order to learn
the lower level units (Feldman et al, 2013; Mar-
tin et al, 2013). The aim of the present work is
to explore how much information can ideally be
derived from both hypotheses.
The paper is organized as follows. First we de-
scribe how we modeled phonetic variation from
audio recordings, second we introduce a bottom-
up cue based on acoustic similarity and top-
down cues based of the properties of the lexicon.
We test their performance in a task that consists
in discriminating within-category contrasts from
between-category contrasts. Finally we discuss
the role and scope of each cue for the acquisition
of phonemes.
2 Modeling phonetic variation
In this section, we describe how we modeled the
representation of speech sounds putatively pro-
cessed by infants, before they learn the relevant
phonemic categories of their language. Following
Peperkamp et al (2006), we make the assumption
that this input is quantized into context-dependent
phone-sized unit we call allophones. Consider the
example of the allophonic rule that applies to the
French /r/:
/r/?
{
[X] / before a voiceless obstruent
[K] elsewhere
Figure 1: Allophonic variation of French /r/
The phoneme /r/ surfaces as voiced ([K]) before
a voiced obstruent like in [kanaK Zon] (?canard
jaune?, yellow duck) and as voiceless ([X]) before
a voiceless obstruent as in [kanaX puXpK] (?ca-
nard pourpre?, purple duck). Assuming speech
sounds are coded as allophones, the challenge fac-
ing the learner is to distinguish the allophonic vari-
ation ([K], [X]) from the phonemic variation (re-
lated to a difference in the meaning) like the con-
trast ([K],[l]).
Previous work has generated allophonic varia-
tion using random contexts (Martin et al, 2013).
This procedure does not take into account the fact
that contexts belong to natural classes. In addition,
it does not enable to compute an acoustic distance.
Here, we generate linguistically and acoustically
controlled allophones using Hidden Markov Mod-
els (HMMs) trained on audio recordings.
1
2.1 Corpora
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al, 2007), which consists of 40
hours of spontaneous conversations with 40 speak-
ers of American English, and the core of the Cor-
pus of Spontaneous Japanese (Maekawa et al,
2000) which also consists of about 40 hours of
recorded spontaneous conversations and public
speeches in different fields. Both corpora are time-
aligned with phonetic labels. Following Boruta
(2012), we relabeled the japanese corpus using 25
phonemes. For English, we used the phonemic
version which consists of 45 phonemes.
2.2 Input generation
2.2.1 HMM-based allophones
In order to generate linguistically and acoustically
plausible allophones, we apply a standard Hidden
Markov Model (HMM) phoneme recognizer with
a three-state per phone architecture to the signal,
as follows.
First, we convert the raw speech waveform of
the corpora into successive vectors of Mel Fre-
quency Cepstrum Coefficients (MFCC), computed
over 25 ms windows, using a period of 10 ms
(the windows overlap). We use 12 MFCC coeffi-
cients, plus the energy, plus the first and second or-
der derivatives, yielding 39 dimensions per frame.
Second, we start HMM training using one three-
state model per phoneme. Third, each phoneme
model is cloned into context-dependent triphone
models, for each context in which the phoneme
actually occurs (for example, the phoneme /A/ oc-
curs in the context [d?A?g] as in the word /dAg/
(?dog?). The triphone models are then retrained on
only the relevant subset of the data, corresponding
to the given triphone context. These detailed mod-
els are clustered back into inventories of various
sizes (from 2 to 20 times the size of the phone-
mic inventory) using a linguistic feature-based de-
cision tree, and the HMM states of linguistically
similar triphones are tied together so as to max-
imize the likelihood of the data. Finally, the tri-
phone models are trained again while the initial
gaussian emission models are replaced by mix-
ture of gaussians with a progressively increasing
number of components, until each HMM state is
modeled by a mixture of 17 diagonal-covariance
gaussians. The HMM were built using the HMM
Toolkit (HTK: Young et al, 2006).
2.2.2 Random allophones
As a control, we also reproduce the random al-
lophones of Martin et al (2013), in which allo-
phonic contexts are determined randomly: for a
given phoneme /p/, the set of all possible con-
texts is randomly partitioned into a fixed number
n of subsets. In the transcription, the phoneme /p/
is converted into one of its allophones (p
1
,p
2
,..,p
n
)
depending on the subset to which the current con-
text belongs.
3 Bottom-up and top-down hypotheses
3.1 Acoustic cue
The bottom-up cue is based on the hypothesis that
instances of the same phoneme are likely to be
acoustically more similar than instances of two
different phonemes (see Cristia and Seidl, in press)
for a similar proposition). In order to provide
a proxy for the perceptual distance between al-
lophones, we measure the information theoretic
distance between the acoustic HMMs of these al-
lophones. The 3-state HMMs of the two allo-
phones were aligned with Dynamic Time Warping
(DTW), using as a distance between pairs of emit-
ting states, a symmetrized version of the Kullback-
Leibler (KL) divergence measure (each state was
approximated by a single non-diagonal Gaussian):
A(x, y) =
?
(i,j)?DTW (x,y)
KL(N
x
i
||N
y
j
) +KL(N
y
j
||N
x
i
)
Where {(i, j) ? DTW (x, y)} is the set of in-
dex pairs over the HMM states that correspond to
the optimal DTW path in the comparison between
phone model x and y, and N
x
i
the full covariance
Gaussian distribution for state i of phone x. For
obvious reasons, the acoustic distance cue cannot
be computed for Random allophones.
3.2 Lexical cues
The top-down information we use in this study, is
based on the insight of Martin et al (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes.
In fact, the latter creates variants (alternants) of the
same lexical item since adjacent sounds condition
the realization of the first and final phoneme. For
example, as shown in figure 1, the phoneme /r/ sur-
faces as [X] or [K] depending on whether or not the
2
next sound is a voiceless obstruent. Therefore, the
lexical item /kanar/ surfaces as [kanaX] or [kanaK].
The lexical cue assumes that a pair of words dif-
fering in the first or last segment (like [kanaX] and
[kanaK]) is more likely to be the result of a phono-
logical process triggered by adjacent sounds, than
a true semantic minimal pair.
However, this strategy clearly gives rise to false
alarms in the (albeit relatively rare) case of true
minimal pairs like [kanaX] (?duck?) and [kanal]
(?canal?), where ([X], [l]) will be mistakenly la-
beled as allophonic.
In order to mitigate the problem of false alarms,
we also use Boruta (2011)?s continuous version,
where each pair of phones is characterized by the
number of lexical minimal pairs it forms.
B(x, y) = |(Ax,Ay) ? L
2
|+ |(xA, yA) ? L
2
|
where {Ax ? L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax,Ay) ?
L
2
} is the set of phonological minimal pairs in
L? L that vary on the final segment.
In addition, we introduce another cue that could
be seen as a normalization of Boruta?s cue:
N (x, y) =
|(Ax,Ay)?L
2
|+|(xA,yA)?L
2
|
|{Ax?L}|+|{Ay?L}|+|{xA?L}|+|{yA?L}|
4 Experiment
4.1 Task
For each corpus we list all the possible pairs of
attested allophones. Some of these pairs are allo-
phones of the same phoneme (allophonic pair) and
others are allophones of different phonemes (non-
allophonic pairs). The task is a same-different
classification, whereby each of these pairs is given
a score from the cue that is being tested. A good
cue gives higher scores to allophonic pairs.
4.2 Evaluation
We use the same evaluation procedure as in Mar-
tin et al (2013). It is carried out by computing
the area under the curve of the Receiver Operat-
ing Characteristic (ROC). A value of 0.5 repre-
sents chance and a value of 1 represents perfect
performance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks (of 20 utterances each). In each run, we
draw randomly with replacement from this set of
blocks a sample of the same size as the original
corpus. This sample is then used to retrain the
acoustic models and generate a phonetic inven-
tory that we use to re-transcribe the corpus and
re-compute the cues. We report scores averaged
over 5 such runs.
4.3 Results
Table 1 shows the classification scores for the lex-
ical cues when we vary the inventory size from
2 allophones per phoneme in average, to 20 al-
lophones per phoneme, using the Random allo-
phones. The top-down scores are very high, repli-
cating Martin et al?s results, and even improving
the performance using Boruta?s cue and our new
Normalized cue.
? English Japanese
Allo./phon. M B N M B N
2 0.784 0.935 0.951 0.580 0.989 1.00
5 0.845 0.974 0.982 0.653 0.978 0.991
10 0.886 0.974 0.981 0.733 0.944 0.971
20 0.918 0.961 0.966 0.785 0.869 0.886
Table 1 : Same-different scores for top-down cues on
Random allophones, as a function of the average number of
allophones per phoneme. M=Martin et al, B=Boruta, N=
Normalized
Table 2 shows the results for HMM-based allo-
phones. The acoustic score is very accurate for
both languages and is quite robust to variation.
Top-down cues, on the other hand, perform, sur-
prisingly, almost at chance level in distinguish-
ing between allophonic and non-allophonic pairs.
A similar discrepancy for the case of Japanese
was actually noted, but not explained, in Boruta
(2012).
? English Japanese
Allo./phon. A M B N A M B N
2 0.916 0.592 0.632 0.643 0.885 0.422 0.524 0.537
5 0.918 0.592 0.607 0.611 0.908 0.507 0.542 0.551
10 0.893 0.569 0.571 0.571 0.827 0.533 0.546 0.548
20 0.879 0.560 0.560 0.559 0.876 0.541 0.543 0.543
Table 2 : Same-different scores for bottom-up and top-down
cues on HMM-based allophones, as a function of the
average number of allophones per phoneme. A=Acoustic,
M=Martin et al, B=Boruta, N= Normalized
5 Analysis
5.1 Why does the performance drop for
realistic allophones?
When we list all possible pairs of allophones in
the inventory, some of them correspond to lexi-
3
cal alternants ([X], [K]) ? ([kanaX] and [kanaK]),
others to true minimal pairs ([K], [l]) ? ([kanaK]
and [kanal]), and yet others will simply not gen-
erate lexical variation at all, we will call those:
invisible pairs. For instance, in English, /h/ and
/N/ occur in different syllable positions and thus
cannot appear in any minimal pair. As defined
above, top-down cues are set to 0 in such pairs
(which means that they are systematically classi-
fied as non-allophonic). This is a correct decision
for /h/ vs. /N/, but not for invisible pairs that also
happen to be allophonic, resulting in false nega-
tives. In tables 3, we show that, indeed, invisible
pairs is a major issue, and could explain to a large
extent the pattern of results found above. In fact,
the proportion of visible allophonic pairs (?allo?
column) is way lower for HMM-based allophones.
This means that the majority of allophonic pairs in
the HMM case are invisible, and therefore, will be
mistakenly classified as non-allophonic.
? Random HMM
? English Japanese English Japanese
Allo./phon. allo ? allo allo ? allo allo ? allo allo ? allo
2 92.9 36.3 100 83.9 48.9 25.3 37.1 53.2
5 97.2 28.4 99.6 69.0 31.1 14.3 25.0 25.9
10 96.8 19.9 96.7 50.1 19.8 4.23 21.0 14.4
20 94.3 10.8 83.4 26.4 14.0 1.89 12.4 4.04
Table 3 : Proportion (in %) of allophonic pairs (allo), and
non-allophonic pairs (? allo) associated with at least one
lexical minimal pair, in Random and HMM allophones.
There are basically two reasons why an allo-
phonic pair would be invisible ( will not generate
lexical alternants). The first one is the absence of
evidence, e.g., if the edges of the word with the
underlying phoneme do not appear in enough con-
texts to generate the corresponding variants. This
happens when the corpus is so small that no word
ending with, say, /r/ appears in both voiced and
voiceless contexts. The second, is when the allo-
phones are triggered on maximally different con-
texts (on the right and the left) as illustrated below:
/p/?
{
[p
1
] / A B
[p
2
] / C D
When A doesn?t overlap with C and B does not
overlap with D, it becomes impossible for the pair
([p
1
], [p
2
]) to generate a lexical minimal pair. This
is simply because a pair of allophones needs to
share at least one context to be able to form vari-
ants of a word (the second or penultimate segment
of this word).
When asked to split the set of contexts in two
distinct categories that trigger [p
1
] and [p
2
] (i.e.,
A B and C D), the random procedure will of-
ten make A overlap with B and C overlap with D
because it is completely oblivious to any acous-
tic or linguistic similarity, thus making it always
possible for the pair of allophones to generate lex-
ical alternants. A more realistic categorization
(like the HMM-based one), will naturally tend to
minimize within-category distance, and maximize
between-category distance. Therefore, we will
have less overlap, making the chances of the pair
to generate a lexical pair smaller. The more al-
lophones we have, the bigger is the chance to end
up with non-overlapping categories (invisible allo-
phonic pairs), and the more mistakes will be made,
as shown in Table 3.
5.2 Restricting the role of top-down cues
The analysis above shows that top-down cues can-
not be used to classify all contrasts. The approxi-
mation that consists in considering all pairs that do
not generate lexical pairs as non-allophonic, does
not scale up to realistic input. A more intuitive,
but less ambitious, assumption is to restrict the
scope of top-down cues to contrasts that do gen-
erate lexical variation (lexical alternants or true
minimal pairs). Thus, they remain completely ag-
nostic to the status of invisible pairs. This restric-
tion makes sense since top-down information boils
down to knowing whether two word forms belong
to the same lexical category (reducing variation to
allophony), or to two different categories (varia-
tion is then considered non-allophonic). Phonetic
variation that does not cause lexical variation is, in
this particular sense, orthogonal to our knowledge
about the lexicon.
We test this hypothesis by applying the cues
only to the subset of pairs that are associated with
at least one lexical minimal pair. We vary the num-
ber of allophones per phoneme on the one hand
(Table 4) and the size of the input on the other
hand (Table 5). We refer to this subset by an aster-
isk (*), by which we also mark the cues that apply
to it. Notice that, in this new framing, the M cue is
completely uninformative since it assigns the same
value to all pairs.
As predicted, the cues perform very well on this
subset, especially the N cue. The combination of
top-down and bottom-up cues shows that the for-
mer is always useful, and that these two sources of
4
? English Japanese
? ? Individual cues Combination ? Individual cues Combination
Allo./phon. * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
2 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
4 14.3 0.918 0.964 0.858 0.951 0.975 0.991 30.88 0.908 0.917 0.850 0.936 0.934 0.976
10 4.24 0.893 0.937 0.813 0.939 0.960 0.968 16.06 0.827 0.839 0.899 0.957 0.904 0.936
20 1.67 0.879 0.907 0.802 0.907 0.942 0.940 5.02 0.876 0.856 0.882 0.959 0.913 0.950
Table 4 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of average
number of allophones per phonemes.
? English Japanese
? ? Individual cues Combination ? Individual cues Combination
Size (hours) * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
1 9.87 0.885 0.907 0.741 0.915 0.927 0.969 34.78 0.890 0.883 0.835 0.915 0.889 0.934
4 18.3 0.918 0.958 0.798 0.917 0.967 0.989 48.00 0.917 0.939 0.860 0.937 0.938 0.973
8 21.3 0.916 0.964 0.837 0.942 0.971 0.992 51.71 0.915 0.940 0.889 0.937 0.954 0.977
20 24.4 0.911 0.960 0.827 0.936 0.969 0.994 58.12 0.921 0.954 0.865 0.912 0.945 0.971
40 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
? 34.82 ? ? ? ? ? ? 72.16 ? ? ? ? ? ?
Table 5 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of corpus size.
* (%) refers to the proportion of the subset of contrasts associated with at least one minimal pair. The cues applied to this
subset are marked with an asterisk (*)
information are not completely redundant. How-
ever, the scope of top-down cues (the proportion of
the subset * ) shrinks as we increase the number of
allophones. Table 5 shows that this problem can,
in principle, be mitigated by increasing the amount
of data available to the learner. As we were limited
to only 40 hours of speech, we generated an artifi-
cial corpus that uses the same lexicon but with all
possible word orders so as to maximize the num-
ber of contexts in which words appear. This artifi-
cial corpus increases the proportion of the subset,
but we are still not at 100 % coverage, which ac-
cording the analysis above, is due (at least in part)
to the irreducible set of non-overlapping pairs.
6 Conclusion
In this study we explored the role of both bottom-
up and top-down hypotheses in learning the
phonemic status of the sounds of two typologically
different languages. We introduced a bottom-up
cue based on acoustic similarity, and we used al-
ready existing top-down cues to which we pro-
vided a new extension. We tested these hypothe-
ses on English and Japanese, providing the learner
with an input that mirrors closely the linguistic
and acoustic properties of each language. We
showed, on the one hand, that the bottom-up cue is
a very reliable source of information, across differ-
ent levels of variation and even with small amount
of data. Top-down cues, on the other hand, were
found to be effective only on a subset of the data,
which corresponds to the interesting contrasts that
cause lexical variation. Their role becomes more
relevant as the learner gets more linguistic experi-
ence, and their combination with bottom-up cues
shows that they can provide non-redundant infor-
mation. Note, finally, that even if this work is
based on a more realistic input compared to previ-
ous studies, it still uses simplifying assumptions,
like ideal word segmentation, and no low-level
acoustic variability. Those assumptions are, how-
ever, useful in quantifying the information that can
ideally be extracted from the input, which is a nec-
essary preliminary step before modeling how this
input is used in a cognitively plausible way. Inter-
ested readers may refer to (Fourtassi and Dupoux,
2014; Fourtassi et al, 2014) for a more learning-
oriented approach, where some of the assumptions
made here about high level representations are re-
laxed.
Acknowledgments
This project is funded in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R?egion Ile de France (DIM cerveau et pens?ee). We
thank Luc Boruta, Sanjeev Khudanpur, Isabelle
Dautriche, Sharon Peperkamp and Benoit Crabb?e
for highly useful discussions and contributions.
5
References
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88?93.
Luc Boruta. 2012. Indicateurs d?allophonie et
de phon?emicit?e. Doctoral dissertation, Universit
?e
Paris-Diderot - Paris VII.
A. Cristia and A. Seidl. In press. The hyperarticula-
tion hypothesis of infant-directed speech. Journal
of Child Language.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. 2013. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review, 120(4):751?778.
Abdellah Fourtassi and Emmanuel Dupoux. 2014. A
rudimentary lexicon and semantics help bootstrap
phoneme acquisition. In Proceedings of the 18th
Conference on Computational Natural Language
Learning (CoNLL).
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th Annual Meeting of the Cognitive Science
Society.
Patricia K. Kuhl, Erica Stevens, Akiko Hayashi,
Toshisada Deguchi, Shigeru Kiritani, and Paul Iver-
son. 2006. Infants show a facilitation effect for na-
tive language phonetic perception between 6 and 12
months. Developmental Science, 9(2):F13?F21.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947?952, Athens, Greece.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103?124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101?B111.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre
Nadal, and Emmanuel Dupoux. 2006. The acqui-
sition of allophonic rules: Statistical learning with
linguistic constraints. Cognition, 101(3):B31?B41.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker,
and S. Amano. 2007. Unsupervised learning
of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 ? 63.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
6
