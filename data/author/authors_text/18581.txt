Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng and Christopher Potts
Stanford University, Stanford, CA 94305, USA
richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu
{jeaneis,manning,cgpotts}@stanford.edu
Abstract
Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics. It pushes the state of the art in single
sentence positive/negative classification from
80% up to 85.4%. The accuracy of predicting
fine-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.
1 Introduction
Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al, 2010;
Zanzotto et al, 2010; Yessenalina and Cardie, 2011;
Socher et al, 2012; Grefenstette et al, 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and
?
0
0This 0film
?
?
?
0does 0n?t
0
+care +0about ++
+
+
+cleverness 0,
0wit
0or
+
0
0any 00other +kind
+
0of ++intelligent + +humor
0.
Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive (? ?, ?, 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.
models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.
The Stanford Sentiment Treebank is the first cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset alows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of
1631
this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.
In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al, 2011b),
matrix-vector RNNs (Socher et al, 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a significant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting fine-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction ?but? dominates.
The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.
2 Related Work
This work is connected to five different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces. The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a
word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pado?, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al, 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al, 2012) but then also be
fine-tuned and trained to specific tasks such as sen-
timent detection (Socher et al, 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces. Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and define composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and find a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models. In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al, 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form. A related field that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned
1632
work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and first models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classifi-
cation (Sutskever et al, 2009; Jenatton et al, 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al, 2012).
Sentiment Analysis. Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al, 2010; Nakagawa et al, 2010).
3 Stanford Sentiment Treebank
Bag of words classifiers can work well in longer
documents by relying on a few words with strong
sentiment like ?awesome? or ?exhilarating.? How-
ever, sentiment accuracies even for binary posi-
tive/negative classification for single sentences has
not exceeded 80% for several years. For the more
difficult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al, 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.
We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-
nerdy ?folks
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
phenomenal ?fantasy ?best ?sellers
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
 ?
 ?
Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.
tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reflects the writer?s over-
all intention for this review. The normalized, lower-
cased text is first used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
influenced by what follows. For more details on the
dataset collection, see supplementary material.
Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the findings from
labeling sentences based on reader?s perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the five po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classification
into these categories captures the main variability
of the labels. We will name this fine-grained senti-
ment classification and our main experiment will be
to recover these five labels for phrases of all lengths.
1633
5 10 15 20 25 30 35 40 45
N-Gram Length
0%
20%
40%
60%
80%
100%
%
 o
f S
en
tim
en
t V
al
ue
s
Neutral
SomeZhat 3ositiYe
3ositiYe
Ver\ 3ositiYe
SomeZhat NegatiYe
NegatiYe
Ver\ NegatiYe
(a)
(a)
(b)
(b)
(c)
(c)
(d)
(d)
Distributions of sentiment values for (a) unigrams, 
(b) 10-grams, (c) 20-grams, and (d) full sentences.
Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.
4 Recursive Neural Models
The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classifier. For ease of exposition,
we will use the tri-gram in this figure to explain all
models.
We first describe the operations that the below re-
cursive neural models have in common: word vector
representations and classification. This is followed
by descriptions of two previous RNN models and
our RNTN.
Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(?r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L ?
Rd?|V |, where |V | is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.
We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classifier. For classification into five
classes, we compute the posterior probability over
    not      very       good ...
        a          b             c 
p1 =g(b,c)
p2 = g(a,p1)
0 0 +
+ +
-
Figure 4: Approach of Recursive Neural Network mod-
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classifier at that node. This func-
tion varies for the different models.
labels given the word vector via:
ya = softmax(Wsa), (1)
where Ws ? R5?d is the sentiment classification
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi ? Rd in a bottom up fashion.
4.1 RNN: Recursive Neural Network
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and Ku?chler, 1996; Socher et al,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two children?s vectors since both
are words. RNNs use the following equations to
compute the parent vectors:
1634
p1 = f
(
W
[
b
c
])
, p2 = f
(
W
[
a
p1
])
,
where f = tanh is a standard element-wise nonlin-
earity, W ? Rd?2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classifier of Eq. 1 to compute its
label probabilities.
This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we fix the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.
4.2 MV-RNN: Matrix-Vector RNN
The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al, 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.
Each word?s matrix is initialized as a d?d identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sification error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:
(p2,P2)
(a,A) (p1,P1)
(b,B) (c,C)
the MV-RNN computes the first parent vector and its
matrix via two equations:
p1 = f
(
W
[
Cb
Bc
])
, P1 = f
(
WM
[
B
C
])
,
where WM ? Rd?2d and the result is again a d ? d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a,A). The vectors are used
for classifying each phrase using the same softmax
classifier as in Eq. 1.
4.3 RNTN:Recursive Neural Tensor Network
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a fixed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specific ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We define the
output of a tensor product h ? Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i] ? Rd?d:
h =
[
b
c
]T
V [1:d]
[
b
c
]
;hi =
[
b
c
]T
V [i]
[
b
c
]
.
where V [1:d] ? R2d?2d?d is the tensor that defines
multiple bilinear forms.
1635
            Slices of       Standard   
                Tensor Layer          Layer
p = f             V[1:2]        +   W
Neural Tensor Layer
b
c
b
c
b
c
T
p = f                             +          
Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of influence a child can have
on its parent.
The RNTN uses this definition for computing p1:
p1 = f
([
b
c
]T
V [1:d]
[
b
c
]
+W
[
b
c
])
,
where W is as defined in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:
p2 = f
([
a
p1
]T
V [1:d]
[
a
p1
]
+W
[
a
p1
])
.
The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of compo-
sition.
An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.
4.4 Tensor Backprop through Structure
We describe in this section how to train the RNTN
model. As mentioned above, each node has a
softmax classifier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.
We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi ? RC?1 at
node i and the target distribution ti ? RC?1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters ? = (V,W,Ws, L) for a sentence is:
E(?) =
?
i
?
j
tij log y
i
j + ????
2 (2)
The derivative for the weights of the softmax clas-
sifier are standard and simply sum up from each
node?s error. We define xi to be the vector at node
i (in the example trigram, the xi ? Rd?1?s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V,W . Let ?i,s ? Rd?1
be the softmax error vector at node i:
?i,s =
(
W Ts (y
i ? ti)
)
? f ?(xi),
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f(xi).
The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We define the complete incoming error
messages for a node i as ?i,com. The top node, in
our case p2, only received errors from the top node?s
softmax. Hence, ?p2,com = ?p2,s which we can
use to obtain the standard backprop derivative for
W (Goller and Ku?chler, 1996; Socher et al, 2010).
For the derivative of each slice k = 1, . . . , d, we get:
?Ep2
?V [k]
= ?p2,comk
[
a
p1
] [
a
p1
]T
,
where ?p2,comk is just the k?th element of this vector.
Now, we can compute the error message for the two
1636
children of p2:
?p2,down =
(
W T ?p2,com + S
)
? f ?
([
a
p1
])
,
where we define
S =
d?
k=1
?p2,comk
(
V [k] +
(
V [k]
)T
)[
a
p1
]
The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete ?. In particular, we have
?p1,com = ?p1,s + ?p2,down[d+ 1 : 2d],
where ?p2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the final word vector derivative for a, it
will be ?p2,down[1 : d].
The full derivative for slice V [k] for this trigram
tree then is the sum at each node:
?E
?V [k]
=
Ep2
?V [k]
+ ?p1,comk
[
b
c
] [
b
c
]T
,
and similarly for W . For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al, 2011) which con-
verges in less than 3 hours to a local optimum.
5 Experiments
We include two types of analyses. The first type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.
For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours. Initial experiments
Model
Fine-grained Positive/Negative
All Root All Root
NB 67.2 41.0 82.6 81.8
SVM 64.3 40.7 84.6 79.4
BiNB 71.0 41.9 82.7 83.1
VecAvg 73.3 32.7 85.1 80.1
RNN 79.0 43.2 86.1 82.4
MV-RNN 78.7 44.4 86.8 82.9
RNTN 80.7 45.7 87.6 85.4
Table 1: Accuracy for fine grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.
showed that the recursive models worked signifi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.
We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).
The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
filters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.
5.1 Fine-grained Sentiment For All Phrases
The main novel experiment and evaluation metric
analyze the accuracy of fine-grained sentiment clas-
sification for all phrases. Fig. 2 showed that a fine
grained classification into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.
Table 1 (left) shows the overall accuracy numbers
for fine grained prediction at all phrase lengths and
full sentences.
1637
    
1*UDP/HQJWK





$
F
F
X
U
D
F
\
    
1*UDP/HQJWK

Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1556?1567,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining Distant and Partial Supervision for Relation Extraction
Gabor Angeli, Julie Tibshirani, Jean Y. Wu, Christopher D. Manning
Stanford University
Stanford, CA 94305
{angeli, jtibs, jeaneis, manning}@stanford.edu
Abstract
Broad-coverage relation extraction either
requires expensive supervised training
data, or suffers from drawbacks inherent
to distant supervision. We present an ap-
proach for providing partial supervision
to a distantly supervised relation extrac-
tor using a small number of carefully se-
lected examples. We compare against es-
tablished active learning criteria and pro-
pose a novel criterion to sample examples
which are both uncertain and representa-
tive. In this way, we combine the ben-
efits of fine-grained supervision for diffi-
cult examples with the coverage of a large
distantly supervised corpus. Our approach
gives a substantial increase of 3.9% end-
to-end F
1
on the 2013 KBP Slot Filling
evaluation, yielding a net F
1
of 37.7%.
1 Introduction
Fully supervised relation extractors are limited to
relatively small training sets. While able to make
use of much more data, distantly supervised ap-
proaches either make dubious assumptions in or-
der to simulate fully supervised data, or make use
of latent-variable methods which get stuck in local
optima easily. We hope to combine the benefits
of supervised and distantly supervised methods by
annotating a small subset of the available data us-
ing selection criteria inspired by active learning.
To illustrate, our training corpus contains
1 208 524 relation mentions; annotating all of
these mentions for a fully supervised classifier, at
an average of $0.13 per annotation, would cost ap-
proximately $160 000. Distant supervision allows
us to make use of this large corpus without requir-
ing costly annotation. The traditional approach is
based on the assumption that every mention of an
entity pair (e.g., Obama and USA) participates in
the known relation between the two (i.e., born in).
However, this introduces noise, as not every men-
tion expresses the relation we are assigning to it.
We show that by providing annotations for only
10 000 informative examples, combined with a
large corpus of distantly labeled data, we can yield
notable improvements in performance over the
distantly supervised data alone. We report results
on three criteria for selecting examples to anno-
tate: a baseline of sampling examples uniformly
at random, an established active learning criterion,
and a new metric incorporating both the uncer-
tainty and the representativeness of an example.
We show that the choice of metric is important
? yielding as much as a 3% F
1
difference ? and
that our new proposed criterion outperforms the
standard method in many cases. Lastly, we train
a supervised classifier on these collected exam-
ples, and report performance comparable to dis-
tantly supervised methods. Furthermore, we no-
tice that initializing the distantly supervised model
using this supervised classifier is critical for ob-
taining performance improvements.
This work makes a number of concrete contri-
butions. We propose a novel application of active
learning techniques to distantly supervised rela-
tion extraction. To the best of the authors knowl-
edge, we are the first to apply active learning to the
class of latent-variable distantly supervised mod-
els presented in this paper. We show that anno-
tating a proportionally small number of examples
yields improvements in end-to-end accuracy. We
compare various selection criteria, and show that
this decision has a notable impact on the gain in
performance. In many ways this reconciles our
results with the negative results of Zhang et al.
(2012), who show limited gains from na??vely an-
notating examples. Lastly, we make our annota-
tions available to the research community.
1
1
http://nlp.stanford.edu/software/
mimlre.shtml
1556
2 Background
2.1 Relation Extraction
We are interested in extracting a set of relations
y
1
. . . y
k
from a fixed set of possible relations R,
given two entities e
1
and e
2
. For example, we
would like to extract that Barack Obama was born
in Hawaii. The task is decomposed into two steps:
First, sentences containing mentions of both e
1
and e
2
are collected. The set of these sentences
x, marked with the entity mentions for e
1
and e
2
,
becomes the input to the relation extractor, which
then produces a set of relations which hold be-
tween the mentions. We are predominantly in-
terested in the second step ? classifying a set of
pairs of entity mentions into the relations they ex-
press. Figure 1 gives the general setting for re-
lation extraction, with entity pairs Barack Obama
and Hawaii, and Barack Obama and president.
Traditionally, relation extraction has fallen into
one of four broad approaches: supervised classi-
fication, as in the ACE task (Doddington et al.,
2004; GuoDong et al., 2005; Surdeanu and Cia-
ramita, 2007), distant supervision (Craven and
Kumlien, 1999; Wu and Weld, 2007; Mintz et
al., 2009; Sun et al., 2011; Roth and Klakow,
2013) deterministic rule-based systems (Soder-
land, 1997; Grishman and Min, 2010; Chen et al.,
2010), and translation from open domain informa-
tion extraction schema (Riedel et al., 2013). We
focus on the first two of these approaches.
2.2 Supervised Relation Extraction
Relation extraction can be naturally cast as a su-
pervised classification problem. A corpus of rela-
tion mentions is collected, and each mention x is
annotated with the relation y, if any, it expresses.
The classifier?s output is then aggregated to decide
the relations between the two entities.
However, annotating supervised training data
is generally expensive to perform at large scale.
Although resources such as Freebase or the TAC
KBP knowledge base have on the order of millions
of training tuples over entities it is not feasible to
manually annotate the corresponding mentions in
the text. This has led to the rise of distantly su-
pervised methods, which make use of this indirect
supervision, but do not necessitate mention-level
supervision.
Barack Obama was born in Hawaii.
Barack Obama visited Hawaii.
The president grew up in Hawaii.
state of birth
state of residence
Barack Obama met former president Clinton.
Obama became president in 2008. title
Figure 1: The relation extraction setup. For a
pair of entities, we collect sentences which men-
tion both entities. These sentences are then used
to predict one or more relations between those
entities. For instance, the sentences containing
both Barack Obama and Hawaii should support
the state of birth and state of residence relation.
2.3 Distant Supervision
Traditional distant supervision makes the assump-
tion that for every triple (e
1
, y, e
2
) in a knowledge
base, every sentence containing mentions for e
1
and e
2
express the relation y. For instance, tak-
ing Figure 1, we would create a datum for each
of the three sentences containing BARACK OBAMA
and HAWAII labeled with state of birth, and like-
wise with state of residence, creating 6 training
examples overall. Similarly, both sentences in-
volving Barack Obama and president would be
marked as expressing the title relation.
While this allows us to leverage a large database
effectively, it nonetheless makes a number of na??ve
assumptions. First ? explicit in the formulation of
the approach ? it assumes that every mention ex-
presses some relation, and furthermore expresses
the known relation(s). For instance, the sen-
tence Obama visited Hawaii would be erroneously
treated as a positive example of the born in rela-
tion. Second, it implicitly assumes that our knowl-
edge base is complete: entity mentions with no
known relation are treated as negative examples.
The first of these assumptions is addressed by
multi-instance multi-label (MIML) learning, de-
scribed in Section 2.4. Min et al. (2013) address
the second assumption by extending the MIML
model with additional latent variables, while Xu
et al. (2013) allow feedback from a coarse relation
extractor to augment labels from the knowledge
base. These latter two approaches are compatible
with but are not implemented in this work.
2.4 Multi-Instance Multi-Label Learning
The multi-instance multi-label (MIML-RE) model
of Surdeanu et al. (2012), which builds upon work
1557
. . .
. . . . . .
. . .
Figure 2: The MIML-RE model, as shown in Sur-
deanu et al. (2012). The outer plate corresponds to
each of the n entity pairs in our knowledge base.
Each entity pair has a set of mention pairs M
i
, and
a corresponding plate in the diagram for each men-
tion pair in M
i
. The variable x represents the in-
put mention pair, whereas y represents the positive
and negative relations for the given pair of entities.
The latent variable z denotes a mention-level pre-
diction for each input. The weight vector for the
multinomial z classifier is given by w
z
, and there
is a weight vector w
j
for each binary y classifier.
by Hoffmann et al. (2011) and Riedel et al. (2010),
addresses the assumptions of distantly supervised
relations extractors in a principled way by positing
a latent mention-level annotation.
The model groups mentions according to their
entity pair ? for instance, every mention pair with
Obama and Hawaii would be grouped together. A
latent variable z
i
is created for every mention i,
where z
i
? R ? {None} takes a single relation
label, or a no relation marker. We create |R| bi-
nary variables y representing the known positive
and negative relations for the entity pair. A set of
binary classifiers (log-linear factors in the graphi-
cal model) links the latent predictions z
1
. . . z
|M
i
|
and each y
j
. These classifiers include two classes
of features: first, a binary feature which fires if at
least one of the mentions expresses a known rela-
tion between the entity pair, and second, a feature
for each co-occurrence of relations for a given en-
tity pair. Figure 2 describes the model.
2.5 Background on Active Learning
We describe preliminaries and prior work on ac-
tive learning; we use this framework to propose
two sampling schemes in Section 3 which we use
to annotate mention-level labels for MIML-RE.
One way of expressing the generalization error
of a hypothesis
?
h is through its mean-squared error
with the true hypothesis h:
E[(h(x)?
?
h(x))
2
]
= E[E[(h(x)?
?
h(x))
2
|x]]
=
?
x
E[(h(x)?
?
h(x))
2
|x]p(x)dx.
The integrand can be further broken into bias
and variance terms:
E[(h(x)?
?
h(x))
2
] = (E[
?
h(x)]? h(x))
2
+ E[(
?
h(x)? E[
?
h(x)])
2
]
where for simplicity we?ve dropped the condition-
ing on x.
Many traditional sampling strategies, such as
query-by-committee (QBC) (Freund et al., 1992;
Freund et al., 1997) and uncertainty sampling
(Lewis and Gale, 1994), work by decreasing the
variance of the learned model. In QBC, we
first create a ?committee? of classifiers by ran-
domly sampling their parameters from a distribu-
tion based on the training data. These classifiers
then make predictions on the unlabeled examples,
and the examples on which there is the most dis-
agreement are selected for labeling. This strat-
egy can be seen as an attempt to decrease the ver-
sion space ? the set of classifiers that are consis-
tent with the labeled data. Decreasing the version
space should lower variance, since variance is in-
versely related to the size of the hypothesis space.
In most scenarios, active learning does not con-
cern itself with the bias term. If a model is fun-
damentally misspecified, then no amount of ad-
ditional training data can lower its bias. How-
ever, our paradigm differs from the traditional set-
ting, in that we are annotating latent variables in
a model with a non-convex objective. These an-
notations may help increase the convexity of our
objective, leading us to a more accurate optimum
and thereby lowering bias.
The other component to consider is
?
x
? ? ? p(x)dx. This suggests that it is impor-
tant to choose examples that are representative
of the underlying distribution p(x), as we want
to label points that will improve the classifier?s
predictions on as many and as high-probability
examples as possible. Incorporating a repre-
sentativeness metric has been shown to provide
a significant improvement over plain QBC or
1558
uncertainty sampling (McCallum and Nigam,
1998; Settles, 2010).
2.6 Active Learning for Relation Extraction
Several papers have explored active learning for
relation extraction. Fu and Grishman (2013) em-
ploy active learning to create a classifier quickly
for new relations, simulated from the ACE corpus.
Finn and Kushmerick (2003) compare a number
of selection criteria ? including QBC ? for a su-
pervised classifier. To the best of our knowledge,
we are the first to apply active learning to distantly
supervised relation extraction. Furthermore, we
evaluate our selection criteria live in a real-world
setting, collecting new sentences and evaluating
on an end-to-end task.
For latent variable models, McCallum and
Nigam (1998) apply active learning to semi-
supervised document classification. We take in-
spiration from their use of QBC and the choice of
metric for classifier disagreement. However their
model assumes a fully Bayesian set-up, whereas
ours does not require strong assumptions about the
parameter distributions.
Settles et al. (2008) use active learning to im-
prove a multiple-instance classifier. Their model
is simpler in that it does not allow for unobserved
variables or multiple labels, and the authors only
evaluate on image retrieval and synthetic text clas-
sification datasets.
3 Example Selection
We describe three criteria for selection examples
to annotate. The first ? sampling uniformly ? is
a baseline for our hypothesis that intelligently se-
lecting examples is important. For this criterion,
we select mentions uniformly at random from the
training set to annotate. This is the approach used
in Zhang et al. (2012). The other two criteria rely
on a metric for disagreement provided by QBC;
we describe our adaptation of QBC for MIML-RE
as a preliminary to introducing these criteria.
3.1 QBC For MIML-RE
We use a version of QBC based on bootstrap-
ping (Saar-Tsechansky and Provost, 2004). To
create the committee of classifiers, we re-sample
the training set with replacement 7 times and train
a model over each sampled dataset. We mea-
sure disagreement on z-labels among the classi-
fiers using a generalized Jensen-Shannon diver-
gence (McCallum and Nigam, 1998), taking the
average KL divergence of all classifier judgments.
We first calculate the mention-level confi-
dences. Note that z
(m)
i
? M
i
denotes the latent
variable in entity pair i with index m; z
(?m)
i
de-
notes the set of all latent variables except z
(m)
i
:
p(z
(m)
i
|y
i
,x
i
) =
p(y
i
, z
(m)
i
|x
i
)
p(y
i
|x
i
)
=
?
z
(?m)
i
p(y
i
, z
i
|x
i
)
?
z
(m)
i
p(y
i
, z
(m)
i
|x
i
)
.
Notice that the denominator just serves to nor-
malize the probability within a sentence group.
We can rewrite the numerator as follows:
?
z
(?m)
i
p(y
i
, z
i
|x
i
)
=
?
z
(?m)
i
p(y
i
|z
i
)p(z
i
|x
i
)
= p(z
(m)
i
|x
i
)
?
z
(?m)
i
p(y
i
|z
i
)p(z
(?m)
i
|x
i
).
For computational efficiency, we approximate
p(z
(?m)
i
|x
i
) with a point mass at its maximum.
Next, we calculate the Jensen-Shannon (JS) diver-
gence from the k bootstrapped classifiers:
1
k
k
?
c=1
KL(p
c
(z
(m)
i
|y
i
,x
i
)||p
mean
(z
(m)
i
|y
i
,x
i
)) (1)
where p
c
is the probability assigned by each of the
k classifiers to the latent z
(m)
i
, and p
mean
is the av-
erage of these probabilities. We use this metric
to capture the disagreement of our model with re-
spect to a particular latent variable. This is then
used to inform our selection criteria.
We note that QBC may be especially useful in
our situation as our objective is highly nonconvex.
If two committee members disagree on a latent
variable, it is likely because they converged to dif-
ferent local optima; annotating that example could
help bring the classifiers into agreement.
The second selection criterion we consider is
the most straightforward application of QBC ? se-
lecting the examples with the highest JS disagree-
ment. This allows us to compare our criterion, de-
scribed next, against an established criterion from
the active learning literature.
1559
3.2 Sample by JS Disagreement
We propose a novel active learning sampling cri-
terion that incorporates not only disagreement but
also representativeness in selecting examples to
annotate. Prior work has taken a weighted combi-
nation of an example?s disagreement and a score
corresponding to whether the example is drawn
from a dense portion of the feature space (e.g.,
McCallum and Nigam (1998)). However, this re-
quires both selecting a criterion for defining den-
sity (e.g., distance metric in feature space), and
tuning a parameter for the relative weight of dis-
agreement versus representativeness.
Instead, we account for choosing representa-
tive examples by sampling without replacement
proportional to the example?s disagreement. For-
mally, we define the probability of selecting an
example z
(m)
i
to be proportional to the Jensen-
Shannon divergence in (1). Since the training set is
an approximation to the prior distribution over ex-
amples, sampling uniformly over the training set is
an approximation to sampling from the prior prob-
ability of seeing an input x. We can view our crite-
rion as an approximation to sampling proportional
to the product of two densities: a prior over exam-
ples x, and the JS divergence mentioned above.
4 Incorporating Sentence-Level
Annotations
Following Surdeanu et al. (2012), MIML-RE is
trained through hard discriminative Expectation
Maximization, inferring the latent z values in the
E-step and updating the weights for both the z and
y classifiers in the M-step. During the E-step, we
constrain the latent z to match our sentence-level
annotations when available.
It is worth noting that even in the hard-EM
regime, we can in principle incorporate annotator
uncertainty elegantly into the model. At each E
step, each z
i
is set according to
z
i
(m)?
? argmax
z?R
[
p(z | x
(m)
i
,w
z
) ?
?
r
p(y
(r)
i
| z
?
i
,w
(r)
y
)
]
where z
?
i
contains the inferred labels from the
previous iteration, but with its mth component re-
placed by z
(m)
i
.
By setting the distribution p(z | x
(m)
i
,w
z
) to re-
flect uncertainty among annotators, we can leave
open the possibility for the model to choose a re-
lation which annotators deemed unlikely, but the
model nonetheless prefers. For simplicity, how-
ever, we treat our annotations as a hard assign-
ment.
In addition to incorporating annotations during
training, we can also use this data to intelligently
initialize the model. Since the MIML-RE objec-
tive is non-convex, the initialization of the classi-
fier weights w
y
and w
z
is important. The y clas-
sifiers are initialized with the ?at-least-once? as-
sumption of Hoffmann et al. (2011); w
z
can be ini-
tialized either using traditional distant supervision
or from a supervised classifier trained on the an-
notated sentences. If initialized with a supervised
classifier, the model can be viewed as augment-
ing this supervised model with a large distantly
labeled corpus, providing both additional entity
pairs to train from, and additional mentions for an
annotated entity pair.
5 Crowdsourced Example Annotation
Most prior work on active learning is done by sim-
ulation on a fully labeled dataset; such a dataset
doesn?t exist for our case. Furthermore, a key aim
of this paper is to practically improve state-of-the-
art performance in relation extraction in addition
to evaluating active learning criteria. Therefore,
we develop and execute an annotation task for col-
lecting labels for our selected examples.
We utilize Amazon Mechanical Turk to crowd-
source annotations. For each task, the annotator
(Turker) is presented with the task description, fol-
lowed by 15 questions, 2 of which are randomly
placed controls. For each question, we present
Turkers with a relation mention and the top 5 re-
lation predictions from our classifier. The Turker
also has an option to freely specify a relation not
presented in the first five options, or mark that
there is no relation. We attempt to heuristically
match common free-form answers to official rela-
tions.
To maintain the quality of the results, we dis-
card all submissions in which both controls were
answered incorrectly, and additionally discard all
submissions from Turkers who failed the controls
on more than
1
3
of their submissions. Rejected
tasks were republished for other workers to com-
plete. We collect 5 annotations for each example,
and use the most commonly agreed answer as the
ground truth. Ties are broken arbitrarily, except in
1560
Figure 3: The task shown to Amazon Mechanical
Turk workers. A sentence along with the top 5 re-
lation predictions from our classifier are shown to
Turkers, as well as an option to specify a custom
relation or manually enter ?no relation.? The cor-
rect response for this example should be either no
relation or a custom relation.
the case of deciding between a relation and no re-
lation, in which case the relation was always cho-
sen.
A total of 23 725 examples were annotated, cov-
ering 10 000 examples for each of the three selec-
tion criteria. Note that there is overlap between
the examples selected for the three criteria. In ad-
dition, 10 023 examples were annotated during de-
velopment; these are included in the set of all an-
notated examples, but excluded from any of the
three criteria. The compensation per task was 23
cents; the total cost of annotating examples was
$3156, in addition to $204 spent on developing the
task. Informally, Turkers achieved an accuracy of
around 75%, as evaluated by a paper author, per-
forming disproportionately well on identifying the
no relation label.
6 Experiments
We evaluate the three high-level research contri-
butions of this work: we show that we improve
the accuracy of MIML-RE, we validate the effec-
tiveness of our selection criteria, and we provide a
corpus of annotated examples, evaluating a super-
vised classifier trained on this corpus. The train-
ing and testing methodology for evaluating these
contributions is given in Sections 6.1 and 6.2; ex-
periments are given in Section 6.3.
6.1 Training Setup
We adopt the setup of Surdeanu et al. (2012) for
training the MIML-RE model, with minor modifi-
cations. We use both the 2010 and 2013 KBP of-
ficial document collections, as well as a July 2013
dump of Wikipedia as our text corpus. We sub-
sample negatives such that
1
3
of our dataset con-
sists of entity pairs with no known relations. In all
experiments, MIML-RE is trained for 7 iterations
of EM; for efficiency, the z classifier is optimized
using stochastic gradient descent;
2
the y classifiers
are optimized using L-BFGS.
Similarly to Surdeanu et al. (2011), we as-
sign negative relations which are either incompat-
ible with the known positive relations (e.g., re-
lations whose co-occurrence would violate type
constraints); or, are actually functional relations
in which another entity already participates. For
example, if we know that Obama was born in the
United States, we could add born in as a negative
relation to the pair Obama and Kenya.
Our dataset consists of 325 891 entity pairs with
at least one positive relation, and 158 091 entity
pairs with no positive relations. Pairs with at least
one known relation have an average of 4.56 men-
tions per group; groups with no known relations
have an average of 1.55 mentions per group. In to-
tal, 1 208 524 distinct mentions are considered; the
annotated examples are selected from this pool.
6.2 Testing Methodology
We compare against the original MIML-RE model
using the same dataset and evaluation methodol-
ogy as Surdeanu et al. (2012). This allows for an
evaluation where the only free variable between
this and prior work is the predictions of the rela-
tion extractor.
Additionally, we evaluate the relation extractors
in the context of Stanford?s end-to-end KBP sys-
tem (Angeli et al., 2014) using the NIST TAC-
KBP 2013 English Slotfilling evaluation. In the
end-to-end framework, the input to the system is a
query entity and a set of articles, and the output is
a set of slot fills ? each slot fill is a candidate triple
in the knowledge base, the first element of which
is the query entity. This amounts to roughly pop-
ulating a data structure like Wikipedia infoboxes
automatically from a large corpus of text.
Importantly, an end-to-end evaluation in a top-
performing full system gives a more accurate idea
of the expected real-world gain from each model.
Both the information retrieval component provid-
ing candidates to the relation extractor, as well as
2
For the sake of consistency, the supervised classifiers and
those in Mintz++ are trained identically to the z classifiers in
MIML-RE.
1561
Method Init
Active Learning Criterion
Not Used Uniform High JS Sample JS All Available
P R F
1
P R F
1
P R F
1
P R F
1
P R F
1
Mintz++ ? 41.3 28.2 33.5 ? ? ? ?
MIML-RE
Dist 38.0 30.5 33.8 39.2 30.4 34.2 41.7 28.9 34.1 36.6 31.1 33.6 37.5 30.6 33.7
Sup 35.1 35.6 35.4 34.4 35.0 34.7 46.2 30.8 37.0 39.4 36.2 37.7 36.0 37.1 36.5
Supervised ? ? 35.5 28.9 31.9 31.3 33.2 32.2 33.5 35.0 34.2 32.9 33.4 33.2
Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The
first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or
a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or
the corresponding supervised classifier (the ?Not Used? column is initialized with the ?All? supervised
classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three
active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE
model; entries in gray perform worse than this model. The bold items denote the best performance
among selection criteria.
the consistency and inference performed on the
classifier output introduce bias in this evaluation?s
sensitivity to particular types of errors. Mistakes
which are easy to filter, or are difficult to retrieve
using IR are less important in this evaluation; in
contrast, factors such as providing good confi-
dence scores for consistency become more impor-
tant.
For the end-to-end evaluation, we use the offi-
cial evaluation script with two changes: First, all
systems are evaluated with provenance ignored, so
as not to penalize any system for finding a new
provenance not validated in the official evaluation
key. Second, each system reports its optimal F
1
along its P/R curve, yielding results which are
optimistic when compared against other systems
entered into the competition. However, this also
yields results which are invariant to threshold tun-
ing, and is therefore more appropriate for compar-
ing between systems in this paper.
Development was done on the KBP 2010?2012
queries, and results are reported using the 2013
queries as a simulated test set. Our best system
achieves an F
1
of 37.7; the top two teams at KBP
2013 (of 18 entered) achieved F
1
scores of 40.2
and 37.1 respectively, ignoring provenance.
6.3 Results
Table 1 summarizes all results for the end-to-end
task; relevant features of the table are copied in
subsequent sections to illustrate key trends. Mod-
els which perform worse than the original MIML-
RE model (MIML-RE, initialized with ?Dist,? un-
der ?Not Used?) are denoted in gray. The best per-
System P R F
1
Mintz++ 41.3 28.2 33.5
MIML + Dist 38.0 30.5 33.8
MIML + Sup 35.1 35.6 35.4
MIML + Dist + SampleJS 36.6 31.1 33.6
MIML + Sup + SampleJS 39.4 36.2 37.7
Table 2: A summary of improvements to MIML-
RE on the end-to-end slotfilling task, copied from
Table 1. Mintz++ is the traditional distantly su-
pervised model. The second row corresponds to
the unmodified MIML-RE model. The third row
corresponds to MIML-RE initialized with a su-
pervised classifier (trained on all examples). The
fourth row is MIML-RE with annotated exam-
ples incorporated during training (but not initial-
ization). The last row shows the best results ob-
tained by our model.
forming model improves on the base model by 3.9
F
1
points on the end-to-end task.
We evaluate each of the individual contribu-
tions of the paper: improving the accuracy of
the MIML-RE relation extractor, evaluating our
example selection criteria, and demonstrating the
annotated examples? effectiveness for a fully-
supervised relation extractor.
Improve MIML-RE Accuracy A key goal of
this work is to improve the accuracy of the MIML-
RE model; we show that we improve the model
both on the end-to-end slotfilling task (Table 2) as
well as on a standard evaluation (Figure 5). Sim-
ilar to our work, recent work by Pershina et al.
1562
System P R F
1
MIML + Sup 35.1 35.6 35.4
MIML + Sup + Uniform 34.4 35.0 34.7
MIML + Sup + HighJS 46.2 30.8 37.0
MIML + Sup + SampleJS 39.4 36.2 37.7
MIML + Sup + All 36.0 37.1 36.5
Table 3: A summary of the performance of each
example selection criterion. In each case, the
model was initialized with a supervised classifier.
The first row corresponds to the MIML-RE model
initialized with a supervised classifier. The middle
three rows show performance for the three selec-
tion criteria, used both for initialization and during
training. The last row shows results if all available
annotations are used, independent of their source.
System P R F
1
Mintz++ 41.3 28.2 33.5
MIML + Dist 38.0 30.5 33.8
Supervised + SampleJS 33.5 35.0 34.2
MIML + Sup 35.1 35.6 35.5
MIML + Sup + SampleJS 39.4 36.2 37.7
Table 4: A comparison of the best performing su-
pervised classifier with other systems. The top
section compares the supervised classifier with
prior work. The lower section highlights the im-
provements gained from initializing MIML-RE
with a supervised classifier.
(2014) incorporates labeled data to guide MIML-
RE during training. They make use of labeled data
to extract training guidelines, which are intended
to generalize across many examples. We show that
we can match or outperform their improvements
with our best criterion.
A few interesting trends emerge from the end-
to-end results in Table 2. Using annotated sen-
tences during training alone did not improve per-
formance consistently, even hurting performance
when the SampleJS criterion was used. This
supports an intuition that the initialization of the
model is important, and that it is relatively difficult
to coax the model out of a local optimum if it is
initialized poorly. This is further supported by the
improvement in performance when the model is
initialized with a supervised classifier, even when
no examples are used during training. Similar
trends are reported in prior work, e.g., Smith and
Eisner (2007) Section 4.4.6.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
MIML-RESurdeanu et al. (2012)Mintz++
Figure 4: MIML-RE and Mintz++ evaluated ac-
cording to Surdeanu et al. (2012). The original
model from the paper is plotted for comparison, as
our training methodology is somewhat different.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
Sample JSPershina et al. (2014)MIML-RE
Figure 5: Our best active learning criterion evalu-
ated against our version of MIML-RE, alongside
the best system of Pershina et al. (2014).
Also interesting is the relatively small gain
MIML-RE provides over traditional distant super-
vision (Mintz++) in this setting. We conjecture
that the mistakes made by Mintz++ are often rel-
atively easily filtered by the downstream consis-
tency component. This is supported by Figure 4;
we evaluate our trained MIML-RE model against
Mintz++ and the results reported in Surdeanu et
al. (2012). We show that our model performs as
well or better than the original implementation,
and consistently outperforms Mintz++.
Evaluate Selection Criteria A key objective of
this work is to evaluate how much of an impact
careful selection of annotated examples has on the
overall performance of the system. We evaluate
the three selection criteria from Section 3.2, show-
ing the results for MIML-RE in Table 3; results
for the supervised classifier are given in Table 1.
In both cases, we show that the sampled JS cri-
1563
terion performs comparably to or better than the
other criteria.
At least two interesting trends can be noted from
these results: First, the uniformly sampled crite-
rion performed worse than MIML-RE initialized
with a supervised classifier. This may be due to
noise in the annotation: a small number of an-
notation errors on entity pairs with only a single
corresponding mention could introduce dangerous
noise into training. These singleton mentions will
rarely have disagreement between the committee
of classifiers, and therefore will generally only be
selected in the uniform criterion.
Second, adding in the full set of examples did
not improve performance ? in fact, performance
generally dropped in this scenario. We conjecture
that this is due to the inclusion of the uniformly
sampled examples, with performance dropping for
the same reasons as above.
Both of these results can be reconciled with
the results of Zhang et al. (2012); like this work,
they annotated examples to analyze the trade-off
between adding more data to a distantly super-
vised system, and adding more direct supervi-
sion. They conclude that annotations provide only
a relatively small improvement in performance.
However, their examples were uniformly selected
from the training corpus, and did not make use
of the structure provided by MIML-RE. Our re-
sults agree in that neither the uniform selection
criterion nor the supervised classifier significantly
outperformed the unmodified MIML-RE model;
nonetheless, we show that if care is taken in se-
lecting these labeled examples we can achieve no-
ticeable improvements in accuracy.
We also evaluate our selection criteria on the
evaluation of Surdeanu et al. (2012), both initial-
ized with Mintz++ (Figure 7) and with the super-
vised classifier (Figure 6). These results mirror
those in the end-to-end evaluation; when initial-
ized with the supervised classifier the high dis-
agreement (High JS) and sampling proportional to
disagreement (Sample JS) criteria clearly outper-
form both the base MIML-RE model as well as
the uniformly sampling criterion. Using the an-
notated examples only during training yielded no
perceivable benefit over the base model (Figure 7).
Supervised Relation Extractor The examples
collected can be used to directly train a supervised
classifier, with results summarized in Table 4. The
most salient insight is that the performance of the
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
Sample JSHigh JSUniformMIML-RE
Figure 6: A comparison of models trained with
various selection criteria on the evaluation of Sur-
deanu et al. (2012), all initialized with the corre-
sponding supervised classifier.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
Sample JSHigh JSUniformMIML-RE
Figure 7: A comparison of models trained with
various selection criteria on the evaluation of Sur-
deanu et al. (2012), all initialized with Mintz++.
best supervised classifier is similar to that of the
MIML-RE model, despite being trained on nearly
two orders of magnitude less training data.
More interestingly, however, the supervised
classifier provides a noticeably better initializa-
tion for MIML-RE than Mintz++, yielding better
results even without enforcing the labels during
EM. These results suggest that the power gained
from the the more sophisticated MIML-RE model
is best used in conjunction with a small amount of
training data. That is, using MIML-RE as a princi-
pled model for combining a large distantly labeled
corpus and a small number of careful annotations
yields significant improvement over using either
of the two data sources alone.
1564
Relation # P R F
1
no relation 3073
employee of 1978 29 32 33 46 31 38
countries of res. 1061 30 42 7 40 11 41
states of residence 427 57 33 14 7 23 12
cities of residence 356 31 52 9 30 14 38
(org:)member of 290 0 0 0 0 0 0
country of hq 280 63 62 65 62 64 62
top members 221 36 26 50 60 42 36
country of birth 205 22 0 40 0 29 0
parents 196 10 26 31 54 15 35
city of hq 194 46 52 57 61 51 56
(org:)alt names 184 52 48 39 39 45 43
founded by 180 100 89 29 38 44 53
city of birth 145 17 50 8 17 11 25
state of hq 132 50 64 30 35 38 45
title 121 20 26 28 35 23 30
subsidiaries 105 33 25 6 3 10 5
founded 90 62 82 62 69 62 75
spouse 88 37 54 85 85 51 66
origin 86 42 43 68 70 51 53
state of birth 83 0 50 0 10 0 17
charges 69 54 54 16 16 24 24
cause of death 69 93 93 39 39 55 55
(per:)alt names 69 9 20 2 3 3 6
country of death 65 100 100 10 10 18 18
members 54 0 0 0 0 0 0
children 52 53 62 14 18 22 27
parents 50 64 64 28 28 39 39
city of death 38 42 75 16 19 23 30
dissolved 38 0 0 0 0 0 0
date of death 33 64 64 44 39 52 48
political affiliation 23 7 25 100 100 13 40
state of death 19 0 0 0 0 0 0
shareholders 19 0 0 0 0 0 0
siblings 16 50 50 33 33 40 40
schools attended 14 80 78 41 48 54 60
date of birth 11 100 100 85 85 92 92
other family 9 0 0 0 0 0 0
age 4 94 97 94 90 94 93
# of employees 3 0 0 0 0 0 0
religion 2 100 100 29 29 44 44
website 0 25 0 3 0 6 0
Table 5: A summary of relations annotated, and
end-to-end slotfilling performance by relation.
The first column gives the relation; the second
shows the number of examples annotated. The
subsequent columns show the performance of the
unmodified MIML-RE model and our best per-
forming model (SampleJS). Changes in values are
bolded; positive changes are shown in green and
negative changes in red. The most frequent 10 re-
lations in the evaluation are likewise bolded.
6.4 Analysis By Relation
In this section, we explore which of the KBP rela-
tions were shown to Turkers, and whether the im-
provements in accuracy correspond to these rela-
tions. We compare only the unmodified MIML-
RE model, and our best model (MIML-RE initial-
ized with the supervised classifier, under the Sam-
pleJS criterion). Results are shown in Table 5.
A few interesting trends emerge from this anal-
ysis. We note that annotating even 80+ examples
for a relation seems to provide a consistent boost
in accuracy, whereas relations with fewer anno-
tated examples tended to show little or no change.
However, the gains of our model are not univer-
sal across relation types, even dropping noticeably
on some ? for instance, F
1
drops on both state of
residence and country of birth. This could suggest
systematic noise from Turker judgments; e.g., for
foreign geography (state of residence) or ambigu-
ous relations (top members).
An additional insight from the table is the mis-
match between examples chosen to be annotated,
and the most popular relations in the KBP evalu-
ation. For instance, by far the most popular KBP
relation (title) had only 121 examples annotated.
7 Conclusion
We have shown that providing a relatively small
number of mention-level annotations can improve
the accuracy of MIML-RE, yielding an end-to-end
improvement of 3.9 F
1
on the KBP task. Further-
more, we have introduced a new active learning
criterion, and shown both that the choice of crite-
rion is important, and that our new criterion per-
forms well. Lastly, we make available a dataset of
mention-level annotations for constructing a tradi-
tional supervised relation extractor.
Acknowledgements
We thank the anonymous reviewers for their
thoughtful comments, and Dan Weld for his feed-
back on additional experiments and analysis. We
gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA)
Deep Exploration and Filtering of Text (DEFT)
Program under Air Force Research Laboratory
(AFRL) contract no. FA8750-13-2-0040. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
1565
References
Gabor Angeli, Arun Chaganty, Angel Chang, Kevin
Reschke, Julie Tibshirani, Jean Y. Wu, Osbert Bas-
tani, Keith Siilats, and Christopher D. Manning.
2014. Stanford?s 2013 KBP system. In TAC-KBP.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang
Li, Wen-Pin Lin, Matthew Snover, Javier Artiles,
Marissa Passantino, and Heng Ji. 2010. CUNY-
BLENDER. In TAC-KBP.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In AAAI.
George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ACE) program?tasks, data, and evalua-
tion. In LREC.
Aidan Finn and Nicolas Kushmerick. 2003. Active
learning selection strategies for information extrac-
tion. In International Workshop on Adaptive Text
Extraction and Mining.
Yoav Freund, H Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1992. Information, prediction, and
query by committee. In NIPS.
Yoav Freund, H Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine learning,
28(2-3):133?168.
Lisheng Fu and Ralph Grishman. 2013. An efficient
active learning framework for new relation types. In
IJCNLP.
Ralph Grishman and Bonan Min. 2010. New York
University KBP 2010 slot-filling system. In Proc.
TAC 2010 Workshop.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL-HLT.
David D Lewis and William A Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing EM and pool-based active learning for text clas-
sification. In ICML.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In NAACL-HLT.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL.
Maria Pershina, Bonan Min, Wei Xu, and Ralph Gr-
ishman. 2014. Infusion of labeled data into distant
supervision for relation extraction. In ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT.
Benjamin Roth and Dietrich Klakow. 2013. Feature-
based models for improving the quality of noisy
training data for relation extraction. In CIKM.
Maytal Saar-Tsechansky and Foster Provost. 2004.
Active sampling for class probability estimation and
ranking. Machine Learning, 54(2):153?178.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In Advances in
neural information processing systems, pages 1289?
1296.
Burr Settles. 2010. Active learning literature survey.
University of Wisconsin Madison Technical Report
1648.
Noah Smith and Jason Eisner. 2007. Novel estimation
methods for unsupervised discovery of latent struc-
ture in natural language text. Ph.D. thesis, Johns
Hopkins.
Stephen G Soderland. 1997. Learning text analysis
rules for domain-specific natural language process-
ing. Ph.D. thesis, University of Massachusetts.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP
slot filling. In Proceedings of the Text Analytics
Conference.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07 Proceedings.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X Chang, Valentin I Spitkovsky, and
Christopher D Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on information and knowl-
edge management. ACM.
1566
Wei Xu, Le Zhao, Raphael Hoffman, and Ralph Grish-
man. 2013. Filling knowledge base gaps for distant
supervision of relation extraction. In ACL.
Ce Zhang, Feng Niu, Christopher R?e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In ACL.
1567
