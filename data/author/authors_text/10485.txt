Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 105?109, Dublin, Ireland, August 23-29 2014.
DKPro Agreement: An Open-Source Java Library for
Measuring Inter-Rater Agreement
Christian M. Meyer,
?
Margot Mieskes,
??
Christian Stab,
?
and Iryna Gurevych
??
?
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Computer Science Department, Technische Universit?at Darmstadt
?
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
?
Information Center for Education
German Institute for Educational Research (DIPF)
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we introduce a novel Java implementation of multiple inter-rater agreement mea-
sures, which we make available as open-source software. Besides assessing the reliability of
coding tasks using S, pi, ?, ?, etc., we particularly support unitizing tasks by measuring ?
U
as
the agreement of the boundaries of the identified annotation units. We provide a unified interface
and data model for both tasks as well as multiple diagnostic devices for analyzing the results.
1 Introduction
Reliability is a necessary precondition for obtaining high-quality datasets and thus for drawing valid con-
clusions from an annotation study. Assessing the reliability by means of inter-rater agreement measures
has been an established scientific practice in psychology (e.g., Cohen, 1960), medicine (Cicchetti et al.,
1978), and content analysis (Krippendorff, 1980) for decades. In the computational linguistics and natu-
ral language processing community, reliability discussions have long been limited or completely ignored.
It was not until Carletta?s (1996) appeal that researchers started measuring the inter-rater agreement at a
larger scale. However, there are still numerous papers published every year that lack a proper discussion
of data quality. The reliability of the utilized datasets is, for example, not discussed at all in six out of the
thirteen task description papers of SemEval-2013, and it remains shallow in another four papers, which
do not provide a suitable inter-rater agreement figure.
1
A major reason for this is the limited availability
of software components, which support the standard measures and are well-integrated with existing sys-
tems. In fact, many researchers currently rely on manual calculations, hasty implementations of single
coefficients, or free online calculators that often lack documentation of the implementation details.
In this work, we present the novel Java-based software library DKPro Agreement for computing mul-
tiple inter-rater agreement measures using a shared interface and data model. For the first time, we
provide a unified model for analyzing coding (i.e., assigning categories to fixed items) and unitizing
studies (i.e., segmenting the data into codable units). By supporting these two fundamental annotation
setups, our software can be used for analyzing many different annotation tasks including syntactic (e.g.,
part-of-speech tagging), semantic (e.g., word sense assignment, keyphrase identification), and discourse
annotation tasks (e.g., dialogue act tagging). We particularly provide diagnostic devices for analyzing
systematic disagreement, which is often overlooked in reliability discussions. DKPro Agreement is avail-
able as open-source software, thoroughly tested on a wide range of examples, and well-documented for
getting started quickly.
2
Our implementation is targeted at scientists and software developers working
in Java, including the large communities around the major Java-based toolkits OpenNLP, DKPro, and
GATE.
3
The software integrates more easily with existing Java applications than statistics software such
as Octave, R, or SPSS, and its usage requires a less pronounced statistics background.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
All SemEval task descriptions (http://www.cs.york.ac.uk/semeval-2013) have been discussed among the authors.
2
DKPro Agreement is part of the DKPro Statistics library. The project is available from https://code.google.com/p/
dkpro-statistics/ and licensed under the Apache License 2.0.
3http://opennlp.apache.org, http://code.google.com/p/dkpro-core-asl, http://gate.ac.uk
105
2 Related Work
Reliability is the subject of an extensive body of literature. Artstein and Poesio (2008) and Krippen-
dorff (1980) give a general introduction to this topic. Implementations of inter-rater agreement mea-
sures exist both as stand-alone and as online tools (e.g., http://uni-leipzig.de/?jenderek/tool/
tool.htm, http://terpconnect.umd.edu/?dchoy/thesis/Kappa). Most of them focus on one
type of agreement measure (e.g., http://vassarstats.net/kappa.html and Randolph (2005) for ? as
well as http://ron.artstein.org/software.html for ?). Others are limited to a few different mea-
sures (e.g., https://mlnl.net/jg/software/ira and http://dfreelon.org/utils/recalfront/recal3).
Frameworks offering a wider range of measures are either commercial with a high price tag (e.g.,
http://www.medcalc.org/manual/kappa.php) or limited to specific platforms (e.g., http://www.
agreestat.com/agreestat.html). The situation is even worse for unitizing studies, for which we are
only aware of Perry and Krippendorff?s (2013) implementation available at http://www.gabriela.
trindade.nom.br/2013/02/calculating-alpha-d-and-alpha-u/. To the best of our knowledge,
there is no software library providing a large variety of agreement measures and diagnostic devices,
which covers both coding and unitizing studies and which integrates well with existing systems.
3 Implementation
The standard workflow for using DKPro Agreement is (1) representing the annotated data using our
unified data model, (2) measuring the agreement among the individual raters, and (3) analyzing the
results using diagnostic devices and visualizations.
Data model. We provide the Java interface IAnnotationStudy as the basic representation of the
annotated data. An annotation study consists of a set of raters, a set of codable units, and a set of
categories, which may be used by the raters to code a unit. The categories do not have a specific type,
such that any Java object (including integers and enums) may be used without any extensions.
Following Krippendorff?s (1980) terminology, we distinguish two basic annotation setups: In cod-
ing studies, the raters receive a set of annotation items with fixed boundaries (e.g., full articles from a
newspaper), which each of them should code (?annotate?) with one of the categories (e.g., politics, eco-
nomics). We consider each rater?s annotation of an item a single annotation unit. In unitizing studies,
the raters are asked to identify the annotation units themselves by marking their boundaries (e.g., high-
lighting key phrases). Depending on the task definition, the identified units may be coded with one of
multiple categories or just distinguish identified segments from so-called gaps between these segments.
Figure 1 illustrates this data model for both setups. The framed boxes show the annotation units and
the categories assigned to them. In coding studies, the units with identical index (i.e., the columns) yield
the annotation items. In unitizing studies, the units may be positioned arbitrarily within the continuum.
We represent missing annotations as empty boxes (coding studies) and as horizontal lines between the
identified units (unitizing studies).
Software developers can either instantiate our default implementation (e.g., by reading data from flat
files or databases) or implement the provided Java interfaces in order to reuse their own data model.
For coding studies, there is an addItem method with a varargs parameter (i.e., a method taking an
arbitrary number of parameters) for specifying the annotation of each rater for a certain item. The line
study.addItem("B", "C", null, "B") indicates that four raters coded an item with the categories B,
C, null, and B. We use null to represent missing annotations. Similarly, unitizing studies provide an
addUnit method, which takes the boundaries and the category assigned to the unit by a certain rater.
The line study.addUnit(10, 4, 2, "A") indicates, for instance, a unit of length 4, which starts at position
10 and which has been annotated as category A by rater 2.
Coding measures. Table 1 shows an overview of the inter-rater agreement measures currently avail-
able in DKPro Agreement. Artstein and Poesio (2008) give an overview of these measures. While the
percentage agreement simply divides the number of agreements by the item count, all other measures
perform a chance correction. A major difference is the assumed probability distribution for the expected
agreement, which is considered different for each study and rater (i.e., rater-specific), the same for all
106
Measure Type Raters Chance correction Weighted
Percentage agreement coding ? 2 ? ?
Bennett et al.?s S (1954) coding 2 uniform ?
Scott?s pi (1955) coding 2 study-specific ?
Cohen?s ? (1960) coding 2 rater-specific ?
Randolph?s ? (2005) [multi-S] coding ? 2 uniform ?
Fleiss?s ? (1971) [multi-pi] coding ? 2 study-specific ?
Hubert?s ? (1977) [multi-?] coding ? 2 rater-specific ?
Krippendorff?s ? (1980) coding ? 2 study-specific X
Cohen?s weighted ?
w
(1968) coding ? 2 rater-specific X
Krippendorff?s ?
u
(1995) unitizing ? 2 study-specific ?
Table 1: Implemented inter-rater agreement measures
Coding studies
items: 1 2 3 4 5 6
...
rater 1 A A B A A B
rater 2 A B A C
.
.
.
Unitizing studies
continuum:
...
rater 1
{
A A A
B B
rater 2
{
A A A
B B
.
.
.
Figure 1: Data model
raters (study-specific), or the same for all studies and raters (uniform). As all of these measures are used
in the literature, we need implementations for each of them to be able to compare different results. This
is particularly important for ? measures, because many different definitions exist. Cohen?s ? (1960) is,
for instance, often compared to Fleiss?s ? (1971), although both measures assume a different probabil-
ity distribution. Since all measures implement a standardized interface, different results can be easily
compared using our software. The line new PercentageAgreement(study).calculateAgreement()
returns, for instance, the percentage agreement of the given study, while the line new FleissKappa-
Agreement(study).calculateAgreement() returns Fleiss?s ? (1971) for the very same study.
Most early measures consider an agreement if, and only if, the categories assigned to an item are
identical. Weighted measures allow for defining a distance function that expresses the similarity of two
categories. We provide distance functions for nominal, ordinal, interval, and ratio scales (Krippendorff,
1980), as well as the MASI distance function for set-valued data (Passonneau, 2006). Set annotations
are an example for a more complex type of category, as they facilitate assigning multiple categories to a
given unit. Additionally, researchers can easily define new study-specific distance functions.
Unitizing measures. Although unitizing has long been identified as a major issue of reliability analy-
sis (cf. Auld and Whitea, 1956), there are so far only few formalizations. The most elaborate measure
is Krippendorff?s ?
U
(1995), which is based on a distance function for comparing units with identical,
overlapping, or disjoint boundaries. As a model for expected disagreement, ?
U
considers all possible
unitizations for the given continuum and raters. Thereby, ?
U
becomes fully compatible with Krippen-
dorff?s ? (1980) for coding tasks. Due to the lack of implementations and the complex statistics, ?
U
is
almost never reported in the literature. While our implementation follows the original definition, it does
not require to explicitly encode the gaps, because we generate them automatically. This simplifies the
usage of this measure substantially, since, for example, UIMA
4
annotations can be directly used to rep-
resent the annotation units. While the original definition focuses on only one category, we additionally
support Krippendorff?s later definition of an aggregating ?
U
over all categories (Krippendorff, 2004).
Analysis. The raw inter-rater agreement scores are useful for comparing multiple annotation studies
with each other, but they are of limited help for diagnosing disagreement and potential systematic issues
with certain categories, items, or raters. This is why we provide interfaces for measuring the agreement of
a specific category, item, or rater. Fleiss (1971) defines, for instance, a category-specific ?
c
that we realize
in our software. The same holds for the unitizing measure ?
U
, which also provides a category-specific
agreement score. Besides measuring a rater-specific agreement score, DKPro Agreement facilitates the
computation of pairwise inter-rater agreement in order to identify the pair of raters with the highest and
lowest agreement. Finally, each measure can return some of its intermediate results, for example, the
expected agreement P
e
of Scott?s pi (1955).
Another means of analysis is to display the annotation units and the disagreement among the raters.
Coding studies can be displayed as a coincidence table or as a reliability matrix (Krippendorff, 1980).
For studies with two raters, our software also allows printing a contingency table. In addition to that,
4
Unstructured Information Management Architecture, http://uima.apache.org
107
we provide a formatter for the weighing matrix of a distance function and a basic visualization of the
continuum of annotation units in unitizing studies ? similar to the representation in Figure 1.
Code example. Consider the coding study described by Krippendorff (1980, p. 139) consisting of nine
annotation items that have been categorized as category 1, 2, 3, or 4 by three raters. We can (re-)analyze
this study with DKPro Agreement using the following Java code:
CodingAnnotationStudy study = new CodingAnnotationStudy(3); ?
study.addItem(1, 1, 1); study.addItem(1, 2, 2); study.addItem(2, 2, 2); ?
study.addItem(4, 4, 4); study.addItem(1, 4, 4); study.addItem(2, 2, 2);
study.addItem(1, 2, 3); study.addItem(3, 3, 3); study.addItem(2, 2, 2);
PercentageAgreement pa = new PercentageAgreement(study); ?
System.out.println(pa.calculateAgreement());
KrippendorffAlphaAgreement alpha = new KrippendorffAlphaAgreement(
study, new NominalDistanceFunction());
System.out.println(alpha.calculateObservedDisagreement());
System.out.println(alpha.calculateExpectedDisagreement());
System.out.println(alpha.calculateAgreement());
System.out.println(alpha.calculateCategoryAgreement(1)); ?
System.out.println(alpha.calculateCategoryAgreement(2));
new CoincidenceMatrixPrinter().print(System.out, study); ?
The first step is ? the instanciation of an IAnnotationStudy for the three human raters. Then, ? we
add all 3 ? 9 = 27 annotation units to the study by providing the category chosen by each rater to code
the nine annotation items. Note that most reliability analyses would read the raters?s decisions from a
file, database, or other data structure rather than typing them in manually like in this example. Once the
data model is complete, ? we calculate the inter-rater agreement. Following the original publication, we
calculate the raw agreement and Krippendorff?s ? (1980) and thus print 0.740 (percentage agreement),
0.259 (observed disagreement D
O
), 0.724 (expected disagreement D
E
), and 0.642 (? coefficient). Fi-
nally, ? we analyze the agreement by calculating the category-specific ? for the categories 1 and 2
yielding a system output of 0.381 and 0.711, and ? we print a coincidence matrix on the system console.
4 Evaluation and Publication
Automatic tests. Even though the agreement measures are clearly defined in the literature, their im-
plementation is error-prone due to the varying notations and the required changes to take efficiency and
numerical stability into account. A single confused index (e.g., p
ij
instead of p
ji
) could easily yield
invalid conclusions for many studies. This is why we provide 61 unit tests to evaluate the correctness of
our implementation. Besides manually devised examples, we use 46 examples from the literature (e.g.,
from Krippendorff, 1980). All examples contain references to their original documentation.
Numerical stability. We especially test the analysis of larger annotation studies, which raise issues of
numerical stability. When scaling the example by Artstein and Poesio (2008, p. 558) with factor 500, an
implementation potentially returns a Cohen?s ? of 0.82, although it is only 0.35. This phenomenon is due
to arithmetic overflows, instable division operations, and rounding errors. Krippendorff?s ?
U
depends,
for instance, on the cubic factor 2l
3
hj
, which can raise such issues even for rather small studies. Where
necessary, we use logarithms or Java?s BigDecimal type to ensure accurate results.
Open-source software. We publish our implementation as open-source software under the Apache
License. By providing access to our source code, researchers can learn how the measures work and how
the software is used. Moreover, they can easily contribute in order to extend the library and evaluate its
correctness in a peer review, which is an essential step to establish the credibility of the software. Finally,
our choice of a free license should ease (re-)using the software in many projects and thus facilitate the
reproduction and comparison of annotation results, which often falls short in our community.
Documentation. DKPro Agreement is fully documented using Javadoc comments. In addition to that,
we provide a general introduction and a getting-started tutorial on the project page, which also points the
users to our numerous usage examples in the form of test cases.
108
5 Conclusion and Future Work
We have presented an open-source Java software for measuring inter-rater agreement of coding and
unitizing studies. In future work, we plan to provide additional diagnostic devices and elaborated visual-
izations (such as Hinton diagrams) and to integrate our measures with existing annotation workbenches.
An early software version has, for example, already been used in the CSniper (Eckart de Castilho et al.,
2012) and WebAnno (Yimam et al., 2013) systems. We plan to extent this to other systems and also make
use of the newly introduced unitizing setup.
Acknowledgments
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806.
References
Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational
Linguistics, 34(4):555?596.
Frank Auld, Jr and Alice M. Whitea. 1956. Rules for Dividing Interviews Into Sentences. The Journal of
Psychology: Interdisciplinary and Applied, 42(2):273?281.
Edward M. Bennett, R. Alpert, and A. C. Goldstein. 1954. Communications Through Limited Response Ques-
tioning. Public Opinion Quarterly, 18(3):303?308.
Jean Carletta. 1996. Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Linguis-
tics, 22(2):249?254.
Domenic V. Cicchetti, Chinyu Lee, Alan F. Fontana, and Barbara Noel Dowds. 1978. A Computer Program for
Assessing Specific Category Rater Agreement for Qualitative Data. Educational and Psychological Measure-
ment, 38(3):805?813.
Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70(4):213?220.
Richard Eckart de Castilho, Sabine Bartsch, and Iryna Gurevych. 2012. CSniper ? Annotation-by-query for
non-canonical constructions in large corpora. In Proceedings of the 50th Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages 85?90, Jeju Island, Korea.
Joseph L. Fleiss. 1971. Measuring Nominal Scale Agreement among many Raters. Psychological Bulletin,
76(5):378?382.
Lawrence Hubert. 1977. Kappa revisited. Psychological Bulletin, 84(2):289?297.
Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Beverly Hills, CA: Sage
Publications.
Klaus Krippendorff. 1995. On the reliability of unitizing contiguous data. Sociological Methodology, 25:47?76.
Published for American Sociological Association.
Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage
Publications, 2nd edition.
Rebecca J. Passonneau. 2006. Measuring agreement on set-valued items (MASI) for semantic and pragmatic
annotation. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages
831?836, Genoa, Italy.
Gabriela Trindade Perry and Klaus Krippendorff. 2013. On the reliability of identifying design moves in protocol
analysis. Design Studies, 34(5):612?635.
Justus J. Randolph. 2005. Free-marginal multirater kappa (multirater ?
free
): An alternative to Fleiss? fixed-
marginal multirater kappa. In Proceedings of the 5th Joensuu University Learning and Instruction Symposium,
Joensuu, Finland.
William A. Scott. 1955. Reliability of Content Analysis: The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Seid Muhie Yimam, Iryna Gurevych, Richard Eckart de Castilho, and Chris Biemann. 2013. WebAnno: A
Flexible, Web-based and Visually Supported System for Distributed Annotations. In Proceedings of the 51st
Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6, Sofia,
Bulgaria.
109
