Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798?1803,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Joint Emotion Analysis via Multi-task Gaussian Processes
Daniel Beck
?
Trevor Cohn
?
Lucia Specia
?
?
Department of Computer Science, University of Sheffield, United Kingdom
{debeck1,l.specia}@sheffield.ac.uk
?
Computing and Information Systems, University of Melbourne, Australia
t.cohn@unimelb.edu.au
Abstract
We propose a model for jointly predicting
multiple emotions in natural language sen-
tences. Our model is based on a low-rank
coregionalisation approach, which com-
bines a vector-valued Gaussian Process
with a rich parameterisation scheme. We
show that our approach is able to learn
correlations and anti-correlations between
emotions on a news headlines dataset. The
proposed model outperforms both single-
task baselines and other multi-task ap-
proaches.
1 Introduction
Multi-task learning (Caruana, 1997) has been
widely used in Natural Language Processing.
Most of these learning methods are aimed for Do-
main Adaptation (Daum?e III, 2007; Finkel and
Manning, 2009), where we hypothesize that we
can learn from multiple domains by assuming sim-
ilarities between them. A more recent use of
multi-task learning is to model annotator bias and
noise for datasets labelled by multiple annotators
(Cohn and Specia, 2013).
The settings mentioned above have one aspect
in common: they assume some degree of posi-
tive correlation between tasks. In Domain Adap-
tation, we assume that some ?general?, domain-
independent knowledge exists in the data. For an-
notator noise modelling, we assume that a ?ground
truth? exists and that annotations are some noisy
deviations from this truth. However, for some set-
tings these assumptions do not necessarily hold
and often tasks can be anti-correlated. For these
cases, we need to employ multi-task methods that
are able to learn these relations from data and
correctly employ them when making predictions,
avoiding negative knowledge transfer.
An example of a problem that shows this be-
haviour is Emotion Analysis, where the goal is to
automatically detect emotions in a text (Strappa-
rava and Mihalcea, 2008; Mihalcea and Strappa-
rava, 2012). This problem is closely related to
Opinion Mining (Pang and Lee, 2008), with sim-
ilar applications, but it is usually done at a more
fine-grained level and involves the prediction of a
set of labels (one for each emotion) instead of a
single label. While we expect some emotions to
have some degree of correlation, this is usually not
the case for all possible emotions. For instance, we
expect sadness and joy to be anti-correlated.
We propose a multi-task setting for Emotion
Analysis based on a vector-valued Gaussian Pro-
cess (GP) approach known as coregionalisation
(
?
Alvarez et al., 2012). The idea is to combine a GP
with a low-rank matrix which encodes task corre-
lations. Our motivation to employ this model is
three-fold:
? Datasets for this task are scarce and small
so we hypothesize that a multi-task approach
will results in better models by allowing a
task to borrow statistical strength from other
tasks;
? The annotation scheme is subjective and very
fine-grained, and is therefore heavily prone to
bias and noise, both which can be modelled
easily using GPs;
? Finally, we also have the goal to learn a
model that shows sound and interpretable
correlations between emotions.
2 Multi-task Gaussian Process
Regression
Gaussian Processes (GPs) (Rasmussen and
Williams, 2006) are a Bayesian kernelised
framework considered the state-of-the-art for
regression. They have been recently used success-
fully for translation quality prediction (Cohn and
Specia, 2013; Beck et al., 2013; Shah et al., 2013)
1798
and modelling text periodicities (Preotiuc-Pietro
and Cohn, 2013). In the following we give a
brief description on how GPs are applied in a
regression setting.
Given an input x, the GP regression assumes
that its output y is a noise corrupted version of a
latent function evaluation, y = f(x) + ?, where
? ? N (0, ?
2
n
) is the added white noise and the
function f is drawn from a GP prior:
f(x) ? GP(?(x), k(x,x
?
)), (1)
where ?(x) is the mean function, which is usually
the 0 constant, and k(x,x
?
) is the kernel or co-
variance function, which describes the covariance
between values of f at locations x and x
?
.
To predict the value for an unseen input x
?
, we
compute the Bayesian posterior, which can be cal-
culated analytically, resulting in a Gaussian distri-
bution over the output y
?
:
1
y
?
? N (k
?
(K + ?
n
I)
?1
y
T
, (2)
k(x
?
,x
?
)? k
T
?
(K + ?
n
I)
?1
k
?
),
where K is the Gram matrix corre-
sponding to the covariance kernel evalu-
ated at every pair of training inputs and
k
?
= [?x
1
,x
?
?, ?x
2
,x
?
?, . . . , ?x
n
,x
?
?] is the
vector of kernel evaluations between the test input
and each training input.
2.1 The Intrinsic Coregionalisation Model
By extending the GP regression framework to
vector-valued outputs we obtain the so-called
coregionalisation models. Specifically, we employ
a separable vector-valued kernel known as Intrin-
sic Coregionalisation Model (ICM) (
?
Alvarez et al.,
2012). Considering a set of D tasks, we define the
corresponding vector-valued kernel as:
k((x, d), (x
?
, d
?
)) = k
data
(x,x
?
)?B
d,d
?
, (3)
where k
data
is a kernel on the input points (here
a Radial Basis Function, RBF), d and d
?
are task
or metadata information for each input and B ?
R
D?D
is the coregionalisation matrix, which en-
codes task covariances and is symmetric and posi-
tive semi-definite.
A key advantage of GP-based modelling is its
ability to learn hyperparameters directly from data
1
We refer the reader to Rasmussen and Williams (2006,
Chap. 2) for an in-depth explanation of GP regression.
by maximising the marginal likelihood:
p(y|X,?) =
?
f
p(y|X,?, f)p(f). (4)
This process is usually performed to learn the
noise variance and kernel hyperparameters, in-
cluding the coregionalisation matrix. In order to
do this, we need to consider how B is parame-
terised.
Cohn and Specia (2013) treat the diagonal val-
ues of B as hyperparameters, and as a conse-
quence are able to leverage the inter-task trans-
fer between each independent task and the global
?pooled? task. They however fix non-diagonal val-
ues to 1, which in practice is equivalent to assum-
ing equal correlation across tasks. This can be lim-
iting, in that this formulation cannot model anti-
correlations between tasks.
In this work we lift this restriction by adopting
a different parameterisation of B that allows the
learning of all task correlations. A straightforward
way to do that would be to consider every corre-
lation as an hyperparameter, but this can result in
a matrix which is not positive semi-definite (and
therefore, not a valid covariance matrix). To en-
sure this property, we follow the method proposed
by Bonilla et al. (2008), which decomposes B us-
ing Probabilistic Principal Component Analysis:
B = U?U
T
+ diag(?), (5)
where U is an D ? R matrix containing the R
principal eigenvectors and ? is a R ? R diago-
nal matrix containing the corresponding eigenval-
ues. The choice of R defines the rank of U?U
T
,
which can be understood as the capacity of the
manifold with which we model the D tasks. The
vector ? allows for each task to behave more or
less independently with respect to the global task.
The final rank of B depends on both terms in
Equation 5.
For numerical stability, we use the incomplete-
Cholesky decomposition over the matrix U?U
T
,
resulting in the following parameterisation for B:
B =
?
L
?
L
T
+ diag(?), (6)
where
?
L is a D ?R matrix. In this setting, we
treat all elements of
?
L as hyperparameters. Set-
ting a larger rank allows more flexibility in mod-
elling task correlations. However, a higher number
of hyperparameters may lead to overfitting prob-
lems or otherwise cause issues in optimisation due
1799
to additional non-convexities in the log likelihood
objective. In our experiments we evaluate this be-
haviour empirically by testing a range of ranks for
each setting.
The low-rank model can subsume the ones pro-
posed by Cohn and Specia (2013) by fixing and
tying some of the hyperparameters:
Independent: fixing
?
L = 0 and ? = 1;
Pooled: fixing
?
L = 1 and ? = 0;
Combined: fixing
?
L = 1 and tying all compo-
nents of ?;
Combined+: fixing
?
L = 1.
These formulations allow us to easily replicate
their modelling approach, which we evaluate as
competitive baselines in our experiments.
3 Experimental Setup
To address the feasibility of our approach, we pro-
pose a set of experiments with three goals in mind:
? To find our whether the ICM is able to learn
sensible emotion correlations;
? To check if these correlations are able to im-
prove predictions for unseen texts;
? To investigate the behaviour of the ICM
model as we increase the training set size.
Dataset We use the dataset provided by the ?Af-
fective Text? shared task in SemEval-2007 (Strap-
parava and Mihalcea, 2007), which is composed
of 1000 news headlines annotated in terms of six
emotions: Anger, Disgust, Fear, Joy, Sadness and
Surprise. For each emotion, a score between 0 and
100 is given, 0 meaning total lack of emotion and
100 maximum emotional load. We use 100 sen-
tences for training and the remaining 900 for test-
ing.
Model For all experiments, we use a Radial Ba-
sis Function (RBF) data kernel over a bag-of-
words feature representation. Words were down-
cased and lemmatized using the WordNet lemma-
tizer in the NLTK
2
toolkit (Bird et al., 2009). We
then use the GPy toolkit
3
to combine this kernel
with a coregionalisation model over the six emo-
tions, comparing a number of low-rank approxi-
mations.
2
http://www.nltk.org
3
http://github.com/SheffieldML/GPy
Baselines and Evaluation We compare predic-
tion results with a set of single-task baselines: a
Support Vector Machine (SVM) using an RBF
kernel with hyperparameters optimised via cross-
validation and a single-task GP, optimised via like-
lihood maximisation. The SVM models were
trained using the Scikit-learn toolkit
4
(Pedregosa
et al., 2011). We also compare our results against
the ones obtained by employing the ?Combined?
and ?Combined+? models proposed by Cohn and
Specia (2013). Following previous work in this
area, we use Pearson?s correlation coefficient as
evaluation metric.
4 Results and Discussion
4.1 Learned Task Correlations
Figure 1 shows the learned coregionalisation ma-
trix setting the initial rank as 1, reordering the
emotions to emphasize the learned structure. We
can see that the matrix follows a block structure,
clustering some of the emotions. This picture
shows two interesting behaviours:
? Sadness and fear are highly correlated. Anger
and disgust also correlate with them, al-
though to a lesser extent, and could be con-
sidered as belonging to the same cluster. We
can also see correlation between surprise and
joy. These are intuitively sound clusters
based on the polarity of these emotions.
? In addition to correlations, the model
learns anti-correlations, especially between
joy/surprise and the other emotions. We also
note that joy has the highest diagonal value,
meaning that it gives preference to indepen-
dent modelling (instead of pooling over the
remaining tasks).
Inspecting the eigenvalues of the learned ma-
trix allows us to empirically determine its result-
ing rank. In this case we find that the model has
learned a matrix of rank 3, which indicates that
our initial assumption of a rank 1 coregionalisa-
tion matrix may be too small in terms of modelling
capacity
5
. This suggests that a higher rank is
justified, although care must be taken due to the
local optima and overfitting issues cited in ?2.1.
4
http://scikit-learn.org
5
The eigenvalues were 592, 62, 86, 4, 3 ? 10
?3
and 9 ?
10
?5
.
1800
Anger Disgust Fear Joy Sadness Surprise All
SVM 0.3084 0.2135 0.3525 0.0905 0.3330 0.1148 0.2603
Single GP 0.1683 0.0035 0.3462 0.2035 0.3011 0.1599 0.3659
ICM GP (Combined) 0.2301 0.1230 0.2913 0.2202 0.2303 0.1744 0.3295
ICM GP (Combined+) 0.1539 0.1240 0.3438 0.2466 0.2850 0.2027 0.3723
ICM GP (Rank 1) 0.2133 0.1075 0.3623 0.2810 0.3137 0.2415 0.3988
ICM GP (Rank 5) 0.2542 0.1799 0.3727 0.2711 0.3157 0.2446 0.3957
Table 1: Prediction results in terms of Pearson?s correlation coefficient (higher is better). Boldface values
show the best performing model for each emotion. The scores for the ?All? column were calculated over
the predictions for all emotions concatenated (instead of just averaging over the scores for each emotion).
Figure 1: Heatmap showing a learned coregional-
isation matrix over the emotions.
4.2 Prediction Results
Table 1 shows the Pearson?s scores obtained in
our experiments. The low-rank models outper-
formed the baselines for the full task (predicting
all emotions) and for fear, joy and surprise sub-
tasks. The rank 5 models were also able to out-
perform all GP baselines for the remaining emo-
tions, but could not beat the SVM baseline. As
expected, the ?Combined? and ?Combined+? per-
formed worse than the low-rank models, probably
due to their inability to model anti-correlations.
4.3 Error analysis
To check why SVM performs better than GPs for
some emotions, we analysed their gold-standard
score distributions. Figure 2 shows the smoothed
distributions for disgust and fear, comparing the
gold-standard scores to predictions from the SVM
and GP models. The distributions for the training
set follow similar shapes.
We can see that GP obtains better matching
score distributions in the case when the gold-
Figure 2: Test score distributions for disgust and
fear. For clarity, only scores between 0 and 50 are
shown. SVM performs better on disgust, while GP
performs better on fear.
standard scores are more spread over the full sup-
port of response values, i.e., [0, 100]. Since our GP
model employs a Gaussian likelihood, it is effec-
tively minimising a squared-error loss. The SVM
model, on the other hand, uses hinge loss, which
is linear beyond the margin envelope constraints.
This affects the treatment of outlier points, which
attract quadratic cf. linear penalties for the GP
and SVM respectively. Therefore, when train-
ing scores are more uniformly distributed (which
is the case for fear), the GP model has to take the
high scores into account, resulting in broader cov-
erage of the full support. For disgust, the scores
are much more peaked near zero, favouring the
1801
more narrow coverage of the SVM.
More importantly, Figure 2 also shows that both
SVM and GP predictions tend to exhibit a Gaus-
sian shape, while the true scores show an expo-
nential behaviour. This suggests that both mod-
els are making wrong prior assumptions about the
underlying score distribution. For SVMs, this is
a non-trivial issue to address, although it is much
easier for GPs, where we can use a different like-
lihood distribution, e.g., a Beta distribution to re-
flect that the outputs are only valid over a bounded
range. Note that non-Gaussian likelihoods mean
that exact inference is no longer tractable, due to
the lack of conjugacy between the prior and likeli-
hood. However a number of approximate infer-
ence methods are appropriate which are already
widely used in the GP literature for use with non-
Gaussian likelihoods, including expectation prop-
agation (Jyl?anki et al., 2011), the Laplace approx-
imation (Williams and Barber, 1998) and Markov
Chain Monte Carlo sampling (Adams et al., 2009).
4.4 Training Set Influence
We expect multi-task models to perform better for
smaller datasets, when compared to single-task
models. This stems from the fact that with small
datasets often there is more uncertainty associated
with each task, a problem which can be alleviated
using statistics from the other tasks. To measure
this behaviour, we performed an additional exper-
iment varying the size of the training sets, while
using 100 sentences for testing.
Figure 3 shows the scores obtained. As ex-
pected, for smaller datasets the single-task mod-
els are outperformed by ICM, but their perfor-
mance become equivalent as the training set size
increases. SVM performance tends to be slightly
worse for most sizes. To study why we obtained
an outlier for the single-task model with 200 sen-
tences, we inspected the prediction values. We
found that, in this case, predictions for joy, sur-
prise and disgust were all around the same value.
6
For larger datasets, this effect disappears and the
single-task models yield good predictions.
5 Conclusions and Future Work
This paper proposed an multi-task approach for
Emotion Analysis that is able to learn correlations
6
Looking at the predictions for smaller datasets, we found
the same behaviour, but because the values found were near
the mean they did not hurt the Pearson?s score as much.
Figure 3: Pearson?s correlation score according to
training set size (in number of sentences).
and anti-correlations between emotions. Our for-
mulation is based on a combination of a Gaussian
Process and a low-rank coregionalisation model,
using a richer parameterisation that allows the
learning of fine-grained task similarities. The pro-
posed model outperformed strong baselines when
applied to a news headline dataset.
As it was discussed in Section 4.3, we plan
to further explore the possibility of using non-
Gaussian likelihoods with the GP models. An-
other research avenue we intend to explore is to
employ multiple layers of metadata, similar to the
model proposed by Cohn and Specia (2013). An
example is to incorporate the dataset provided by
Snow et al. (2008), which provides multiple non-
expert emotion annotations for each sentence, ob-
tained via crowdsourcing. Finally, another possi-
ble extension comes from more advanced vector-
valued GP models, such as the linear model of
coregionalisation (
?
Alvarez et al., 2012) or hierar-
chical kernels (Hensman et al., 2013). These mod-
els can be specially useful when we want to em-
ploy multiple kernels to explain the relation be-
tween the input data and the labels.
Acknowledgements
Daniel Beck was supported by funding from
CNPq/Brazil (No. 237999/2012-9). Dr.
Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
References
Ryan Prescott Adams, Iain Murray, and David J. C.
MacKay. 2009. Tractable Nonparametric Bayesian
1802
Inference in Poisson Processes with Gaussian Pro-
cess Intensities. In Proceedings of ICML, pages 1?8,
New York, New York, USA. ACM Press.
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Rich Caruana. 1997. Multitask Learning. Machine
Learning, 28:41?75.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian Domain Adaptation. In Pro-
ceedings of NAACL.
James Hensman, Neil D Lawrence, and Magnus Rat-
tray. 2013. Hierarchical Bayesian modelling of
gene expression time series across irregularly sam-
pled replicates and clusters. BMC Bioinformatics,
14:252.
Pasi Jyl?anki, Jarno Vanhatalo, and Aki Vehtari. 2011.
Robust Gaussian Process Regression with a Student-
t Likelihood. Journal of Machine Learning Re-
search, 12:3227?3257.
Rada Mihalcea and Carlo Strapparava. 2012. Lyrics,
Music, and Emotions. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 590?599.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of EMNLP.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and Fast - But
is it Good?: Evaluating Non-Expert Annotations
for Natural Language Tasks. In Proceedings of
EMNLP.
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14 : Affective Text. In Pro-
ceedings of SEMEVAL.
Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proceedings of the
2008 ACM Symposium on Applied Computing.
Christopher K. I. Williams and David Barber. 1998.
Bayesian Classification with Gaussian Processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342?1351.
1803
Proceedings of the ACL-HLT 2011 Student Session, pages 36?40,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Syntax-based Statistical Machine Translation using Tree Automata and Tree
Transducers
Daniel Emilio Beck
Computer Science Department
Federal University of Sa?o Carlos
daniel beck@dc.ufscar.br
Abstract
In this paper I present a Master?s thesis
proposal in syntax-based Statistical Machine
Translation. I propose to build discrimina-
tive SMT models using both tree-to-string
and tree-to-tree approaches. Translation and
language models will be represented mainly
through the use of Tree Automata and Tree
Transducers. These formalisms have im-
portant representational properties that makes
them well-suited for syntax modeling. I also
present an experiment plan to evaluate these
models through the use of a parallel corpus
written in English and Brazilian Portuguese.
1 Introduction
Statistical Machine Translation (SMT) has domi-
nated Machine Translation (MT) research in the
last two decades. One of its variants, Phrase-based
SMT (PB-SMT), is currently considered the state
of the art in the area. However, since the advent
of PB-SMT by Koehn et al (2003) and Och and
Ney (2004), purely statistical MT systems have not
achieved considerable improvements. So, new re-
search directions point toward the use of linguistic
resources integrated into SMT systems.
According to Lopez (2008), there are four steps
when building an SMT system: translational equiv-
alence modeling1, parameterization, parameter esti-
mation and decoding. This Master?s thesis proposal
aims to improve SMT systems by including syntac-
tic information in the first and second steps. There-
1For the remainder of this proposal, I will refer to this step
as simply translation model.
fore, I plan to investigate two approaches: the Tree-
to-String (TTS) and the Tree-to-Tree (TTT) models.
In the former, syntactic information is provided only
for the source language while in the latter, it is pro-
vided for both source and target languages.
There are many formal theories to represent
syntax in a language, like Context-free Gram-
mars (CFGs), Tree Substitution Grammars (TSGs),
Tree Adjoining Grammars (TAGs) and all its syn-
chronous counterparts. In this work, I represent each
sentence as a constituent tree and use Tree Automata
(TAs) and Tree Transducers (TTs) in the language
and translation models.
Although this work is mainly language indepen-
dent, proof-of-concept experiments will be executed
on the English and Brazilian Portuguese (en-ptBR)
language pair. Previous research on factored trans-
lation for this pair (using morphological informa-
tion) showed that it improved the results in terms
of BLEU (Papineni et al, 2001) and NIST (Dod-
dington, 2002) scores, as shown in Table 1 (Caseli
and Nunes, 2009). However, even factored transla-
tion models have limitations: many languages (and
Brazilian Portuguese is not an exception) have rela-
tively loose word order constraints and present long-
distance agreements that cannot be efficiently repre-
sented by those models. Such phenomena motivate
the use of more powerful models that take syntactic
information into account.
2 Related work
Syntax-based approaches for SMT have been pro-
posed in many ways. Some apply the TTS model:
Yamada and Knight (2001) uses explicit inser-
36
en-ptBR ptBR-en
BLEU NIST BLEU NIST
PB-SMT 0,3589 7,8312 0,3903 8,3008
FT 0,3713 7,9813 0,3932 8,4421
Table 1: BLEU and NIST scores for PB-SMT and fac-
tored translation experiments for the en-ptBR language
pair
tion, reordering and translation rules, Nguyen et al
(2008) uses synchronous CFGs rules and Liu et al
(2006) uses TTs. Galley et al (2006) also uses
transducer rules but extract them from parse trees in
target language instead (the string-to-tree approach
- STT). Works that apply the TTT model include
Gildea (2003) and Zhang et al (2008). All those
works also include methods and algorithms for ef-
ficient rule extraction since it?s unfeasible to extract
all possible rules from a parsed corpus due to expo-
nential cost.
There have been research efforts to combine
syntax-based systems with phrase-based systems.
These works mainly try to incorporate non-syntatic
phrases into a syntax-based model: while Liu et al
(2006) integrates bilingual phrase tables as separate
TTS templates, Zhang et al (2008) uses an algo-
rithm to convert leaves in a parse tree to phrases be-
fore rule extraction.
Language models that take into account syntac-
tic aspects have also been an active research subject.
While works like Post and Gildea (2009) and Van-
deghinste (2009) focus solely on language modeling
itself, Graham and van Genabith (2010) shows an
experiment that incorporates a syntax-based model
into an PB-SMT system.
3 Tree automata and tree transducers
Tree Automata are similar to Finite-state Automata
(FSA), except they recognize trees instead of strings
(or sequences of words). Formally, FSA can only
represent Regular Languages and thus, cannot ef-
ficiently model several syntactic features, includ-
ing long-distance agreement. TA recognize the so-
called Regular Tree Languages (RTLs), which can
represent Context-free Languages (CFLs) since a set
of all syntactic trees of a CFL is an RTL (Comon
et al, 2007). However, it is important to note that
the reciprocal is not true: there are RTLs that cannot
be modeled by a CFL because those cannot capture
the inner structure of trees. Figure 1 shows such an
RTL, composed of two trees. If we extract an CFG
from this RTL it would have the recursive rule S?
SS, which would generate an infinite set of syntac-
tic trees. In other words, there isn?t an CFG capable
to generate only the syntactic trees contained in the
RTL shown in Figure 1. This feature implies that
RTLs have more representational power than CFLs.
S
S
b
S
a ,
S
S
a
S
b
Figure 1: An RTL that cannot be modeled by a CFL
As a Finite-state Transducer (FST) is an extension
of an FSA that produces strings, a Tree Transducer is
an extension of a TA that produces trees. An FST is
composed by an input RTL, an output RTL and a set
of transformation rules. Restrictions can be added to
the rules, leading to many TT variations, each with
its properties (Graehl et al, 2008). The variations
studied in this work are the xT (extended top-down,
for TTT models) and xTS (extended top-down tree-
to-string, for TTS models).
Top-down (T) transducers processes input trees
starting from its root and descending through its
nodes until it reaches the leaves, in contrast to
bottom-up transducers, which do the opposite. Fig-
ure 2 shows a T rule, where uppercase letters (NP)
represent symbols, lowercase letters (q, r, s) repre-
sent states and x1 and x2 are variables (formal def-
initions can be found in Comon et al (2007)). De-
fault top-down transducers must have only one sym-
bol on the left-hand sides and thus cannot model
some syntactic transformations (like local reorder-
ing, for example) without relying on copy and delete
operations (Maletti et al, 2009). Extended top-
down transducers allow multiple symbols on left-
hand sides, making them more suited for syntax
modeling. This property is shown on Figure 3
(adapted from Maletti et al (2009)). Tree-to-string
transducers simply drop the tree structure on right-
37
hand sides, which makes them adequate for transla-
tion models wihtout syntactic information in one of
the languages. Figure 4 shows an example of a xTS
rule, applied for the en-ptBR pair.
q
NP
x2x1 ??
NP
q
x1
q
x2
Figure 2: Example of a T rule
4 SMT Model
The systems will be implemented using a discrim-
inative, log-linear model (Och and Ney, 2002), us-
ing the language and translation models as feature
functions. Settings that uses more features besides
those two models will also be built. In particu-
lar, I will investigate settings that incorporate non-
syntactic phrases, using methods similar to Liu et al
(2006) and Zhang et al (2008)
The translation models will be weighted TTs
(Graehl et al, 2008), which add probabilities to the
rules. These probabilities will be learned by an EM
algorithm similar to the one described in Graehl et
al. (2008). Rule extraction for TTS will be similar
to the GHKM algorithm described in Galley et al
(2004) but I also plan to investigate the approaches
used by Liu et al (2006) and Nguyen et al (2008).
For TTT rule extraction, I will use a method similar
to the one described in Zhang et al (2008).
I also plan to use language models which takes
into account syntactic properties. Although most
works in syntactic language models uses tree gram-
mars like TSGs and TAGs, these can be simulated by
TAs and TTs (Shieber, 2004; Maletti, 2010). This
property can help the systems implementation be-
cause it?s possible to unite language and translation
modeling in one TT toolkit.
5 Methods
In this section, I present the experiments proposed in
my thesis and the materials required, along with the
metrics used for evaluation. This work is planned to
be done over a year.
q
S
SINV
x3x2
x1
??
S
VP
q
x1
q
x2
q
x3
q
S
x2x1 ??
S
VP
q
x1
s
x2
r
x2
r
SINV
x2x1 ??
q
x2
s
SINV
x2x1 ??
q
x1
Figure 3: Example of a xT rule and its corresponding T
rules
5.1 Materials
To implement and evaluate the techniques described,
a parallel corpus with syntactic annotation is re-
quired. As the focus of this thesis is the English and
Brazilian Portuguese language pair, I will use the
PesquisaFAPESP corpus2 in my experiments. This
corpus is composed of 646 scientific papers, origi-
nally written in Brazilian Portuguese and manually
translated into English, resulting in about 17,000
parallel sentences. As for syntactic annotation, I will
use the Berkeley parser (Petrov and Klein, 2007) for
2http://revistapesquisa.fapesp.br
38
qS
VP
x2V
was
x1
?? x1 foi x2
Figure 4: Example of a xTS rule (for the en-ptBR lan-
guage pair)
English and the PALAVRAS parser (Bick, 2000) for
Brazilian Portuguese.
In addition to the corpora and parsers, the follow-
ing tools will be used:
? GIZA++3 (Och and Ney, 2000) for lexical
alignment
? Tiburon4 (May and Knight, 2006) for trans-
ducer training in both TTS and TTT systems
? Moses5 (Koehn et al, 2007) for decoding
5.2 Experiments and evaluation
Initially the corpus will be parsed using the tools de-
scribed in section 5.1 and divided into a training set
and a test set. For the TTS systems (one for each
translation direction), the training set will be lexi-
cally aligned using GIZA++ and for the TTT system,
its syntactic trees will be aligned using techniques
similar to the ones proposed by Gildea (2003) and
by Zhang et al (2008). Both TTS and TTT systems
will be implemented using Tiburon and Moses. For
evaluation, BLEU and NIST scores on the test set
will be used. The baseline will be the score for fac-
tored translation, shown in Table 1.
6 Contributions
After its conclusion, this thesis will have brought the
following contributions:
3http://www.fjoch.com/GIZA++.html
4http://www.isi.edu/licensed-sw/tiburon
5http://www.statmt.org/moses
? Language-independent SMT models which in-
corporates syntactic information in both lan-
guage and translation models.
? Implementations of these models, using the
tools described in Section 5.
? Experimental results for the en-ptBR language
pair.
Technical reports will be written during this thesis
progress and made publicly available. Paper submis-
sion showing intermediate and final results is also
planned.
Acknowledgments
This research is supported by FAPESP (Project
2010/03807-4).
References
Eckhard Bick. 2000. The Parsing System ?Palavras?:
Automatic Grammatical Analysis of Portuguese in
a Constraint Grammar Framework. Ph.D. thesis,
Aarhus University.
Helena De Medeiros Caseli and Israel Aono Nunes.
2009. Traduc?a?o Automa?tica Estat??stica baseada em
Frases e Fatorada : Experimentos com os idiomas Por-
tugue?s do Brasil e Ingle?s usando o toolkit Moses.
Hubert Comon, Max Dauchet, Remi Gilleron, Florent
Jacquemard, Denis Lugiez, Christof Lo?ding, Sophie
Tison, and Marc Tommasi. 2007. Tree automata tech-
niques and applications, volume 10. Available on:
http://www.grappa.univ-lille3.fr/tata.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 128?132.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. Whats in a translation rule? In
Proceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT/NAACL 2004), pages 273?
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL - ACL ?06, pages 961?968.
39
Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 80?87.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training Tree Transducers. Computational Linguis-
tics, 34:391?427.
Yvette Graham and Josef van Genabith. 2010. Deep
Syntax Language Models and Statistical Machine
Translation. In SSST-4 - 4th Workshop on Syntax and
Structure in Statistical Translation at COLING 2010,
page 118.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology - NAACL ?03,
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th an-
nual meeting of the ACL - ACL ?06, pages 609?616.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-
down tree transducers. SIAM Journal on Computing,
39(2):410?430.
Andreas Maletti. 2010. A Tree Transducer Model for
Synchronous Tree-Adjoining Grammars. Computa-
tional Linguistics, pages 1067?1076.
Jonathan May and Kevin Knight. 2006. Tiburon : A
Weighted Tree Automata Toolkit. Grammars.
Thai Phuong Nguyen, Akira Shimazu, Tu-Bao Ho, Minh
Le Nguyen, and Vinh Van Nguyen. 2008. A tree-
to-string phrase-based model for statistical machine
translation. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning -
CoNLL ?08, pages 143?150.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics - ACL ?02, page 295.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, pages 311?318.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages 404?
411.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. Computing, pages 1?
8.
Stuart M Shieber. 2004. Synchronous Grammars as Tree
Transducers. Applied Sciences, pages 88?95.
Vincent Vandeghinste. 2009. Tree-based target language
modeling. In Proceedings of EAMT, pages 152?159.
Kenji Yamada and Kevin Knight. 2001. A Syntax-based
Statistical Translation Model. In ACL ?01 Proceedings
of the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 523?530.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT, pages 559?567.
40
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543?548,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reducing Annotation Effort for Quality Estimation via Active Learning
Daniel Beck and Lucia Specia and Trevor Cohn
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,l.specia,t.cohn}@sheffield.ac.uk
Abstract
Quality estimation models provide feed-
back on the quality of machine translated
texts. They are usually trained on human-
annotated datasets, which are very costly
due to its task-specific nature. We in-
vestigate active learning techniques to re-
duce the size of these datasets and thus
annotation effort. Experiments on a num-
ber of datasets show that with as little as
25% of the training instances it is possible
to obtain similar or superior performance
compared to that of the complete datasets.
In other words, our active learning query
strategies can not only reduce annotation
effort but can also result in better quality
predictors.
1 Introduction
The purpose of machine translation (MT) qual-
ity estimation (QE) is to provide a quality pre-
diction for new, unseen machine translated texts,
without relying on reference translations (Blatz et
al., 2004; Specia et al, 2009; Callison-Burch et
al., 2012). This task is usually addressed with
machine learning models trained on datasets com-
posed of source sentences, their machine transla-
tions, and a quality label assigned by humans. A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
Since quality scores for the training of QE mod-
els are given by human experts, the annotation pro-
cess is costly and subject to inconsistencies due to
the subjectivity of the task. To avoid inconsisten-
cies because of disagreements among annotators,
it is often recommended that a QE model is trained
for each translator, based on labels given by such
a translator (Specia, 2011). This further increases
the annotation costs because different datasets are
needed for different tasks. Therefore, strategies to
reduce the demand for annotated data are needed.
Such strategies can also bring the possibility of se-
lecting data that is less prone to inconsistent anno-
tations, resulting in more robust and accurate pre-
dictions.
In this paper we investigate Active Learning
(AL) techniques to reduce the size of the dataset
while keeping the performance of the resulting
QE models. AL provides methods to select in-
formative data points from a large pool which,
if labelled, can potentially improve the perfor-
mance of a machine learning algorithm (Settles,
2010). The rationale behind these methods is to
help the learning algorithm achieve satisfactory re-
sults from only on a subset of the available data,
thus incurring less annotation effort.
2 Related Work
Most research work on QE for machine transla-
tion is focused on feature engineering and feature
selection, with some recent work on devising more
reliable and less subjective quality labels. Blatz et
al. (2004) present the first comprehensive study on
QE for MT: 91 features were proposed and used
to train predictors based on an automatic metric
(e.g. NIST (Doddington, 2002)) as the quality la-
bel. Quirk (2004) showed that small datasets man-
ually annotated by humans for quality can result
in models that outperform those trained on much
larger, automatically labelled sets.
Since quality labels are subjective to the anno-
tators? judgements, Specia and Farzindar (2010)
evaluated the performance of QE models using
HTER (Snover et al, 2006) as the quality score,
i.e., the edit distance between the MT output and
its post-edited version. Specia (2011) compared
the performance of models based on labels for
543
post-editing effort, post-editing time, and HTER.
In terms of learning algorithms, by and large
most approaches use Support Vector Machines,
particularly regression-based approaches. For an
overview on various feature sets and machine
learning algorithms, we refer the reader to a re-
cent shared task on the topic (Callison-Burch et
al., 2012).
Previous work use supervised learning methods
(?passive learning? following the AL terminol-
ogy) to train QE models. On the other hand, AL
has been successfully used in a number of natural
language applications such as text classification
(Lewis and Gale, 1994), named entity recognition
(Vlachos, 2006) and parsing (Baldridge and Os-
borne, 2004). See Olsson (2009) for an overview
on AL for natural language processing as well as
a comprehensive list of previous work.
3 Experimental Settings
3.1 Datasets
We perform experiments using four MT datasets
manually annotated for quality:
English-Spanish (en-es): 2, 254 sentences
translated by Moses (Koehn et al, 2007), as pro-
vided by the WMT12 Quality Estimation shared
task (Callison-Burch et al, 2012). Effort scores
range from 1 (too bad to be post-edited) to 5 (no
post-editing needed). Three expert post-editors
evaluated each sentence and the final score was
obtained by a weighted average between the three
scores. We use the default split given in the shared
task: 1, 832 sentences for training and 432 for
test.
French-English (fr-en): 2, 525 sentences trans-
lated by Moses as provided in Specia (2011), an-
notated by a single translator. Human labels in-
dicate post-editing effort ranging from 1 (too bad
to be post-edited) to 4 (little or no post-editing
needed). We use a random split of 90% sentences
for training and 10% for test.
Arabic-English (ar-en): 2, 585 sentences trans-
lated by two state-of-the-art SMT systems (de-
noted ar-en-1 and ar-en-2), as provided in (Specia
et al, 2011). A random split of 90% sentences for
training and 10% for test is used. Human labels in-
dicate the adequacy of the translation ranging from
1 (completely inadequate) to 4 (adequate). These
datasets were annotated by two expert translators.
3.2 Query Methods
The core of an AL setting is how the learner will
gather new instances to add to its training data. In
our setting, we use a pool-based strategy, where
the learner queries an instance pool and selects
the best instance according to an informativeness
measure. The learner then asks an ?oracle? (in this
case, the human expert) for the true label of the in-
stance and adds it to the training data.
Query methods use different criteria to predict
how informative an instance is. We experiment
with two of them: Uncertainty Sampling (US)
(Lewis and Gale, 1994) and Information Density
(ID) (Settles and Craven, 2008). In the following,
we denote M(x) the query score with respect to
method M .
According to the US method, the learner selects
the instance that has the highest labelling variance
according to its model:
US(x) = V ar(y|x)
The ID method considers that more dense regions
of the query space bring more useful information,
leveraging the instance uncertainty and its similar-
ity to all the other instances in the pool:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x, x(u))
)?
The ? parameter controls the relative importance
of the density term. In our experiments, we set it
to 1, giving equal weights to variance and density.
The U term is the number of instances in the query
pool. As similarity measure sim(x, x(u)), we use
the cosine distance between the feature vectors.
With each method, we choose the instance that
maximises its respective equation.
3.3 Experiments
To build our QE models, we extracted the 17 fea-
tures used by the baseline approach in the WMT12
QE shared task.1 These features were used with a
Support Vector Regressor (SVR) with radial basis
function and fixed hyperparameters (C=5, ?=0.01,
=0.5), using the Scikit-learn toolkit (Pedregosa
et al, 2011). For each dataset and each query
method, we performed 20 active learning simu-
lation experiments and averaged the results. We
1We refer the reader to (Callison-Burch et al, 2012) for
a detailed description of the feature set, but this was a very
strong baseline, with only five out of 19 participating systems
outperforming it.
544
started with 50 randomly selected sentences from
the training set and used all the remaining train-
ing sentences as our query pool, adding one new
sentence to the training set at each iteration.
Results were evaluated by measuring Mean Ab-
solute Error (MAE) scores on the test set. We
also performed an ?oracle? experiment: at each it-
eration, it selects the instance that minimises the
MAE on the test set. The oracle results give an
upper bound in performance for each test set.
Since an SVR does not supply variance values
for its predictions, we employ a technique known
as query-by-bagging (Abe and Mamitsuka, 1998).
The idea is to build an ensemble of N SVRs
trained on sub-samples of the training data. When
selecting a new query, the ensemble is able to re-
turnN predictions for each instance, from where a
variance value can be inferred. We used 20 SVRs
as our ensemble and 20 as the size of each training
sub-sample.2 The variance values are then used
as-is in the case of US strategy and combined with
query densities in case of the ID strategy.
4 Results and Discussion
Figure 1 shows the learning curves for all query
methods and all datasets. The ?random? curves
are our baseline since they are equivalent to pas-
sive learning (with various numbers of instances).
We first evaluated our methods in terms of how
many instances they needed to achieve 99% of the
MAE score on the full dataset. For three datasets,
the AL methods significantly outperformed the
random selection baseline, while no improvement
was observed on the ar-en-1 dataset. Results are
summarised in Table 1.
The learning curves in Figure 1 show an inter-
esting behaviour for most AL methods: some of
them were able to yield lower MAE scores than
models trained on the full dataset. This is par-
ticularly interesting in the fr-en case, where both
methods were able to obtain better scores using
only ?25% of the available instances, with the
US method resulting in 0.03 improvement. The
random selection strategy performs surprisingly
well (for some datasets it is better than the AL
strategies with certain number of instances), pro-
viding extra evidence that much smaller annotated
2We also tried sub-samples with the same size of the cur-
rent training data but this had a large impact in the query
methods running time while not yielding significantly better
results.
Figure 1: Learning curves for different query se-
lection strategies in the four datasets. The horizon-
tal axis shows the number of instances in the train-
ing set and the vertical axis shows MAE scores.
545
US ID Random Full dataset#instances MAE #instances MAE #instances MAE
en-es 959 (52%) 0.6818 549 (30%) 0.6816 1079 (59%) 0.6818 0.6750
fr-en 79 (3%) 0.5072 134 (6%) 0.5077 325 (14%) 0.5070 0.5027
ar-en-1 51 (2%) 0.6067 51 (2%) 0.6052 51 (2%) 0.6061 0.6058
ar-en-2 209 (9%) 0.6288 148 (6%) 0.6289 532 (23%) 0.6288 0.6290
Table 1: Number (proportion) of instances needed to achieve 99% of the performance of the full dataset.
Bold-faced values indicate the best performing datasets.
Best MAE US Best MAE ID Full dataset#instances MAE US MAE Random #instances MAE ID MAE Random
en-es 1832 (100%) 0.6750 0.6750 1122 (61%) 0.6722 0.6807 0.6750
fr-en 559 (25%) 0.4708 0.5010 582 (26%) 0.4843 0.5008 0.5027
ar-en-1 610 (26%) 0.5956 0.6042 351 (15%) 0.5987 0.6102 0.6058
ar-en-2 1782 (77%) 0.6212 0.6242 190 (8%) 0.6170 0.6357 0.6227
Table 2: Best MAE scores obtained in the AL experiments. For each method, the first column shows the
number (proportion) of instances used to obtain the best MAE, the second column shows the MAE score
obtained and the third column shows the MAE score for random instance selection at the same number
of instances. The last column shows the MAE obtained using the full dataset. Best scores are shown in
bold and are significantly better (paired t-test, p < 0.05) than both their randomly selected counterparts
and the full dataset MAE.
datasets than those used currently can be sufficient
for machine translation QE.
The best MAE scores achieved for each dataset
are shown in Table 2. The figures were tested for
significance using pairwise t-test with 95% confi-
dence,3 with bold-faced values in the table indicat-
ing significantly better results.
The lower bounds in MAE given by the ora-
cle curves show that AL methods can indeed im-
prove the performance of QE models: an ideal
query method would achieve a very large improve-
ment in MAE using fewer than 200 instances in all
datasets. The fact that different datasets present
similar oracle curves suggests that this is not re-
lated for a specific dataset but actually a common
behaviour in QE. Although some of this gain in
MAE may be due to overfitting to the test set, the
results obtained with the fr-en and ar-en-2 datasets
are very promising, and therefore we believe that
it is possible to use AL to improve QE results in
other cases, as long as more effective query tech-
niques are designed.
5 Further analysis on the oracle
behaviour
By analysing the oracle curves we can observe an-
other interesting phenomenon which is the rapid
increase in error when reaching the last ?200 in-
stances of the training data. A possible explana-
3We took the average of the MAE scores obtained from
the 20 runs with each query method for that.
tion for this behaviour is the existence of erro-
neous, inconsistent or contradictory labels in the
datasets. Quality annotation is a subjective task by
nature, and it is thus subject to noise, e.g., due to
misinterpretations or disagreements. Our hypothe-
sis is that these last sentences are the most difficult
to annotate and therefore more prone to disagree-
ments.
To investigate this phenomenon, we performed
an additional experiment with the en-es dataset,
the only dataset for which multiple annotations
are available (from three judges). We measure the
Kappa agreement index (Cohen, 1960) between all
pairs of judges in the subset containing the first
300 instances (the 50 initial random instances plus
250 instances chosen by the oracle). We then mea-
sured Kappa in windows of 300 instances until the
last instance of the training set is selected by the
oracle method. We also measure variances in sen-
tence length using windows of 300 instances. The
idea of this experiment is to test whether sentences
that are more difficult to annotate (because of their
length or subjectivity, generating more disagree-
ment between the judges) add noise to the dataset.
The resulting Kappa curves are shown in Fig-
ure 2: the agreement between judges is high for
the initial set of sentences selected, tends to de-
crease until it reaches ?1000 instances, and then
starts to increase again. Figure 3 shows the results
for source sentence length, which follow the same
trend (in a reversed manner). Contrary to our hy-
546
Figure 2: Kappa curves for the en-es dataset. The
horizontal axis shows the number of instances and
the vertical axis shows the kappa values. Each
point in the curves shows the kappa index for a
window containing the last 300 sentences chosen
by the oracle.
pothesis, these results suggest that the most diffi-
cult sentences chosen by the oracle are those in the
middle range instead of the last ones. If we com-
pare this trend against the oracle curve in Figure 1,
we can see that those middle instances are the ones
that do not change the performance of the oracle.
The resulting trends are interesting because they
give evidence that sentences that are difficult to an-
notate do not contribute much to QE performance
(although not hurting it either). However, they do
not confirm our hypothesis about the oracle be-
haviour. Another possible source of disagreement
is the feature set: the features may not be discrim-
inative enough to distinguish among different in-
stances, i.e., instances with very similar features
but different labels might be genuinely different,
but the current features are not sufficient to indi-
cate that. In future work we plan to further inves-
tigate this by hypothesis by using other feature sets
and analysing their behaviour.
6 Conclusions and Future Work
We have presented the first known experiments us-
ing active learning for the task of estimating ma-
chine translation quality. The results are promis-
ing: we were able to reduce the number of in-
stances needed to train the models in three of the
four datasets. In addition, in some of the datasets
active learning yielded significantly better models
using only a small subset of the training instances.
Figure 3: Average source and target sentence
lengths for the en-es dataset. The horizontal axis
shows the number of instances and the vertical
axis shows the length values. Each point in the
curves shows the average length for a window con-
taining the last 300 sentences chosen by the oracle.
The oracle results give evidence that it is possi-
ble to go beyond these encouraging results by em-
ploying better selection strategies in active learn-
ing. In future work we will investigate more
advanced query techniques that consider features
other than variance and density of the data points.
We also plan to further investigate the behaviour
of the oracle curves using not only different fea-
ture sets but also different quality scores such as
HTER and post-editing time. We believe that a
better understanding of this behaviour can guide
further developments not only for instance selec-
tion techniques but also for the design of better
quality features and quality annotation schemes.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Lucia Specia).
References
Naoki Abe and Hiroshi Mamitsuka. 1998. Query
learning strategies using boosting and bagging. In
Proceedings of the Fifteenth International Confer-
ence on Machine Learning, pages 1?9.
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Pro-
ceedings of EMNLP, pages 9?16.
547
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, pages 128?132.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
David D. Lewis and Willian A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 1?
10.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical report.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and E?douard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC, pages 825?828.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia and Atefeh Farzindar. 2010. Estimating
machine translation post-editing effort with HTER.
In Proceedings of AMTA Workshop Bringing MT to
the User: MT Research and the Translation Indus-
try.
Lucia Specia, M Turchi, Zhuoran Wang, and J Shawe-
Taylor. 2009. Improving the confidence of machine
translation quality estimates. In Proceedings of MT
Summit XII.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine translation
adequacy. In Proceedings of MT Summit XIII.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
Andreas Vlachos. 2006. Active annotation. In Pro-
ceedings of the Workshop on Adaptive Text Extrac-
tion and Mining at EACL.
548
Proceedings of the ACL 2014 Student Research Workshop, pages 1?9,
Baltimore, Maryland USA, June 22-27 2014.
c?2014 Association for Computational Linguistics
Bayesian Kernel Methods for Natural Language Processing
Daniel Beck
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
debeck1@sheffield.ac.uk
Abstract
Kernel methods are heavily used in Natu-
ral Language Processing (NLP). Frequen-
tist approaches like Support Vector Ma-
chines are the state-of-the-art in many
tasks. However, these approaches lack
efficient procedures for model selection,
which hinders the usage of more advanced
kernels. In this work, we propose the
use of a Bayesian approach for kernel
methods, Gaussian Processes, which allow
easy model fitting even for complex kernel
combinations. Our goal is to employ this
approach to improve results in a number of
regression and classification tasks in NLP.
1 Introduction
In the last years, kernel methods have been suc-
cessfully employed in many Natural Language
Processing tasks. These methods allow the build-
ing of non-parametric models which make less as-
sumptions about the underlying pattern in the data.
Another advantage of kernels is that they can be
defined in arbitrary structures like strings or trees,
which greatly reduce the need for careful feature
engineering in these structures.
The properties cited above make kernel meth-
ods ideal for problems where we do not have
much prior knowledge about how the data be-
haves. This is a common setting in NLP, where
they have been mostly applied in the form of Sup-
port Vector Machines (SVMs). Systems based on
SVMs have been the state-of-the-art in classifica-
tion tasks like Text Categorization (Lodhi et al,
2002), Sentiment Analysis (Johansson and Mos-
chitti, 2013; P?erez-Rosas and Mihalcea, 2013) and
Question Classification (Moschitti, 2006; Croce et
al., 2011). Recently, they were also employed in
regression settings like Machine Translation Qual-
ity Estimation (Specia and Farzindar, 2010; Bojar
et al, 2013) and structured prediction (Chang et
al., 2013).
SVMs are a frequentist method: they aim to find
an approximation to the exact latent function that
explains the data. This is in contrast to Bayesian
settings, which define a prior distribution on this
function and perform inference by marginalizing
over all its possible values. Although there is some
discussion about which approach is better (Mur-
phy, 2012, Sec. 6.6.4), Bayesian methods offer
many useful theoretical properties. In fact, they
have been used before in NLP, especially in gram-
mar induction (Cohn et al, 2010) and word seg-
mentation (Goldwater et al, 2009). However, only
very recently kernel methods have been applied in
NLP using the Bayesian approach.
Gaussian Processes (GPs) are the Bayesian
counterpart of kernel methods and are widely con-
sidered the state-of-the-art for inference on func-
tions (Hensman et al, 2013). They have a number
of advantages which are very useful in NLP:
? Kernels in general can be combined and pa-
rameterized in many ways. This parame-
terization lead to the problem of model se-
lection, which is difficult in frequentist ap-
proaches (mainly based on cross validation).
The Bayesian formulation of GPs let them
deal with model selection in a much more
more efficient and elegant way: by maximiz-
ing the likelihood on the training data. This
opens the door for the use of heavily param-
eterized kernel combinations, like multi-task
kernels for example.
? Being a probabilistic framework, they are
able to naturally encode uncertainty in the
predictions, which can be propagated if the
task is part of a larger system pipeline.
Besides these properties, GPs have also been
applied sucessfully in many Machine Learning
1
tasks. Examples include Robotics (Ko et al,
2007), Bioinformatics (Chu et al, 2005; Polaj-
nar et al, 2011), Geolocation (Schwaighofer et
al., 2004) and Computer Vision (Sinz et al, 2004;
Riihim?aki et al, 2013). In NLP, GPs have been
used only very recently and focused on regression
tasks (Cohn and Specia, 2013; Preotiuc-Pietro and
Cohn, 2013). In this work, we propose to combine
GPs with recent kernel developments to advance
the state-of-the-art in a number of NLP tasks.
2 Gaussian Processes
In this Section, we follow closely the definition of
Rasmussen and Williams (2006). Consider a ma-
chine learning setting, where we have a dataset
X = {(x
1
, y
1
), (x
2
, y
2
), . . . , (x
n
, y
n
)} and our
goal is to infer the underlying function f(x) that
best explains the data. A GP model assumes a
prior stochastic process over this function:
f(x) ? GP(?(x), k(x,x
?
)), (1)
where ?(x) is the mean function, which is usu-
ally the 0 constant, and k(x,x
?
) is the kernel or
covariance function. In this sense, they are analo-
gous to Gaussian distributions, which are also de-
fined in terms of a mean and a variance values, or
in the case of multivariate Gaussians, a mean vec-
tor and a covariance matrix. In fact, a GP can be
interpreted as an infinite-dimensional multivariate
Gaussian distribution.
The full model uses Bayes? rule to define a pos-
terior over f , combining the GP prior with the data
likelihood:
p(f |X,y) =
p(y|X, f)p(f)
p(y|X)
, (2)
where X and y are the training inputs and outputs,
respectively. The posterior is then used to predict
the label for an unseen input x
?
by marginalizing
over all possible latent functions:
p(y
?
|x
?
,X,y) =
?
f
p(y
?
|x
?
,X, f)p(f |X,y)df.
(3)
where y
?
is the predicted output. The choice of
the likelihood distribution depends if the task is re-
gression, classification or other prediction setting.
2.1 GP Regression
In a regression setting, we assume that the output
values are equal to noisy latent function evalua-
tions, i.e., y
i
= f(x
i
) + ?, where ? ? N (0, ?
2
n
) is
the added white noise. We also usually assume a
Gaussian likelihood, because this able us to solve
the integral in Equation 3 analytically. Substitut-
ing the likelihood and the prior in both Equations
2 and 3 and manipulating the result, we compute
the posterior also as a Gaussian distribution:
y
?
? N (k
?
(K + ?
n
I)
?1
y
T
, (4)
k(x
?
,x
?
)? k
T
?
(K + ?
n
I)
?1
k
?
).
where K is the Gram matrix corre-
sponding to the training inputs and
k
?
= [?x
1
,x
?
?, ?x
2
,x
?
?, . . . , ?x
n
,x
?
?] is the
vector of kernel evaluations between the test input
and each training input.
2.2 GP Classification
Consider binary classification using ?1 and +1
as labels
1
. The model in this case use the ac-
tual, noiseless latent function evaluations f and
?squash? them through the [?1,+1] interval to ob-
tain the outputs. The posterior over the outputs is
then defined as:
p(y
?
= +1|x
?
,X,y) =
?
f
?
?(f
?
)p(f
?
|x
?
,X,y)df
?
, (5)
where ?(f
?
) is a squashing function. Two com-
mon choices are the logistic function and the pro-
bit function. The distribution over the latent values
f
?
is obtained by integrating out the latent func-
tion:
p(f
?
|x
?
,X,y) =
?
f
p(f
?
|x
?
,X, f)p(f |X,y)df.
(6)
Because the likelihood is not Gaussian, the re-
sulting posterior integral is not analytically avail-
able anymore. The most common solution to this
problem is to approximate the posterior p(f |X,y)
with a Gaussian q(f |X,y). Two such approxi-
mation algorithms are the Laplace approximation
(Williams and Barber, 1998) and the Expectation
Propagation (Minka, 2001). Another option is to
use Markov Chain Monte Carlo sampling methods
on the true posterior (Neal, 1998).
2.3 Hyperparameter Optimization
The GP prior used in the models described above
usually have a number of hyperparameters. The
1
Extensions to multi-class settings are possible.
2
most important ones are the kernel ones but they
can also include others like the white noise vari-
ance ?
2
n
used in regression. A key property of GPs
is their ability to easily fit these hyperparameters
to the data by maximizing the marginal likelihood:
p(y|X,?) =
?
f
p(y|X,?, f)p(f), (7)
where ? represents the full set of hyperparameters
(which was suppressed from all conditionals until
now for brevity). Optimization involves deriving
the gradients of the marginal log likelihood w.r.t.
the hyperparameters and then employ a gradient
ascent procedure. Gradients can be found ana-
litically for regression and by approximations for
classification, using methods similar to the ones
used for prediction.
2.4 Sparse Approximations for GPs
SVMs are naturally sparse models which use only
a subset of data points to make predictions. This
results in important speed-ups which is one of
the reasons for their success. On the other hand,
canonical GPs are not sparse, making use of all
data points. This results in a training complexity
of O(n
3
) (due to the Gram matrix inversion) and
O(n) for predictions.
Sparse GPs tackle this problem by approximat-
ing the Gram matrix using only a subset of m in-
ducing inputs. Without loss of generalization, con-
sider these m inputs as the first ones in the train-
ing data and (n ? m) the remaining ones. Then
we can partition the Gram matrix in the following
way (Rasmussen and Williams, 2006, Sec. 8.1):
K =
[
K
mm
K
m(n?m)
K
(n?m)m
K
(n?m)(n?m)
]
,
where each block corresponds to a matrix of ker-
nel evaluations between two sets of inputs. For
brevity, we will refer K
m(n?m)
as K
mn
and its
transpose as K
nm
. The block structure of K forms
the base of the so-called Nystr?om approximation:
?
K = K
nm
K
?1
mm
K
mn
. (8)
which result in the following predictive posterior:
y
?
? N (k
T
m?
?
G
?1
K
mn
y, (9)
k(x
?
,x
?
)? k
T
m?
K
?1
mm
k
m?
+
?
2
n
k
T
m?
?
G
?1
k
m?
),
where
?
G = ?
2
n
K
mm
+ K
mn
K
nm
and k
m?
is the
vector of kernel evaluations between test input x
?
and the m inducing inputs. The resulting com-
plexities for training and prediction are O(m
2
n)
and O(m), respectively.
The remaining question is how to choose the in-
ducing inputs. Seeger et al (2003) use an iterative
method that starts with some random data points
and adds new ones based on a greedy procedure,
in an active learning fashion. Snelson and Ghahra-
mani (2006) use a different approach: it defines a
fixed m a priori and use pseudo-inputs which can
be optimized as regular hyperparameters. Later,
Titsias (2009) also used pseudo-inputs but per-
form optimization using a variational method in-
stead. Recently, Hensman et al (2013) modified
this method to allow Stochastic Variational Infer-
ence (Hoffman et al, 2013), which reduces the
training complexity to O(m
3
).
3 Kernels
The core of a GP model is the kernel function. A
kernel k(x,x
?
) is a symmetric and positive semi-
definite function which returns a similarity score
between two inputs in some feature space (Shawe-
Taylor and Cristianini, 2004). Probably the most
used kernel in general is the Radial Basis Func-
tion (RBF) kernel, which is defined over two real-
valued vectors. Our focus in this work is on two
different types of kernels which can be applied for
NLP settings and allow richer parameterizations.
3.1 Kernels for Discrete Structures
In NLP, discrete structures like strings or trees are
common in training data. To apply a vectorial
kernel like the RBF, one can always extract real-
valued features from these structures. However,
kernels can be defined directly on these structures,
potentially reducing the need for feature engineer-
ing. The string and tree kernels we define here
are based on the theory of Convolution kernels
of Haussler (1999), which calculate the similar-
ity between two structures based on the number
of substructures they have in common. Other ap-
proaches include random walk kernels (G?artner et
al., 2003; Vishwanathan et al, 2010) and Fisher
kernels (Jaakkola et al, 2000).
3.1.1 String Kernels
Consider a function ?
s
(x) that counts the number
of times a substring s appears in x. A string kernel
3
is defined as:
k(x, x
?
) =
?
s??
?
w
s
?
s
(x)?
s
(x
?
), (10)
where w
s
is a non-negative weight for substring s
and ?
?
is the set of all possible strings over the
symbol alphabet ?.
Usually in NLP, each word is considered a sym-
bol, although some previous work also considered
characters as symbols (Lodhi et al, 2002). If we
restrict s to be only single words we end up hav-
ing a bag-of-words (BOW) representation. Allow-
ing longer substrings lead us to the Word Sequence
Kernels of Cancedda et al (2003), which also al-
low gaps between words.
One extension of these kernels is to allow soft
matching between substrings. This is done by
defining a similarity matrix S, which encode sym-
bol similarities. This matrix can be defined by ex-
ternal resources, like WordNet, or be inferred from
data using Latent Semantic Analysis (Deerwester
et al, 1990) for example.
3.1.2 Tree Kernels
Collins and Duffy (2001) first introduced Tree
Kernels, which measure the similarity between
two trees by counting the number of fragments
they share, in a very similar way to string kernels.
Consider two trees T
1
and T
2
. We define the set
of nodes in these two trees as N
1
and N
2
respec-
tively. Consider also F the full set of possible tree
fragments (similar to ?
?
in the case of strings). We
define I
i
(n) as an indicator function that returns 1
if fragment f
i
? F has root n and 0 otherwise. A
Tree Kernel can then be defined as:
k(T
1
, T
2
) =
?
n
1
?N
1
?
n
2
?N
2
?(n
1
, n
2
),
where:
?(n
1
, n
2
) =
|F|
?
i=1
?
size(i)
I
i
(n
1
)I
i
(n
2
).
Here, 0 < ? < 1 is a decay factor that penalizes
contributions from larger fragments cf. smaller
ones.
Again, we can put restrictions on the type of
tree fragment considered for comparison. Collins
and Duffy (2001) defined Subtree kernels, which
considered only subtrees as fragments, and Subset
Tree Kernels (SSTK), where fragments can have
non-terminals as leaves. Later, Moschitti (2006)
introduced the Partial Tree Kernels (PTK), by al-
lowing fragments with partial rule expansions.
Tree kernels were used in a variety of tasks, in-
cluding Relation Extraction (Bloehdorn and Mos-
chitti, 2007; Plank and Moschitti, 2013), Ques-
tion Classification (Moschitti, 2006; Croce et al,
2011) and Quality Estimation (Hardmeier, 2011;
Hardmeier et al, 2012). Furthermore, soft match-
ing approaches were also used by Bloehdorn and
Moschitti (2007) and Croce et al (2011).
3.2 Multi-task Kernels
Kernels can also be extended to deal with set-
tings where we want to predict a vector of val-
ues (
?
Alvarez et al, 2012). These settings are use-
ful in multi-task and domain adaptation problems.
Kernels for vector-valued functions are known as
coregionalization kernels in the literature. Here
we are going to refer them as multi-task kernels.
One of the simplest ways to define a kernel for
a multi-task setting is the Intrinsic Coregionaliza-
tion Model (ICM):
K(x,x
?
) = B? k(x,x
?
).
where ? denotes the Kronecker product and B
is the coregionalization matrix, encoding task co-
variances. We also denote the resulting kernel
function as K(x,x
?
) to stress out that its result is
now a matrix instead of a scalar.
Cohn and Specia (2013) used the ICM to model
annotator bias in Quality Estimation datasets.
They parameterize B in a number of differ-
ent ways and get significant improvements over
single-task baselines, especially in post-editing
time prediction. They also point out that the well
known EasyAdapt method (Daum?e III, 2007) for
domain adaptation can be modeled by the ICM us-
ing B = 1+I, i.e., a coregionalization matrix with
its diagonal elements equal to 2 and remaining el-
ements equal to 1.
An extension of the ICM is the Linear Model of
Coregionalization (LMC), which assume a sum of
kernels with different coregionalization matrices:
K(x,x
?
) =
?
k
p
?P
B
p
? k
p
(x,x
?
).
where P is the set of different kernels employed.
?
Alvarez et al (2012) argue that the LMC is much
more flexible than the ICM because the latter as-
sumes that each kernel contributes equally to the
task covariances.
4
4 Planned Work
Our goal in this proposal is to employ GPs and
the kernels introduced in Section 3 to advance
the state-of-the-art in regression and classification
NLP tasks. It would be unfeasible though, at least
for a single thesis, to address all possible tasks so
we are going to focus on three of them where ker-
nel methods were already successfully applied.
4.1 Quality Estimation
The purpose of Machine Translation Quality Esti-
mation is to provide a quality prediction for new,
unseen machine translated texts, without relying
on reference translations (Blatz et al, 2004; Bojar
et al, 2013). A common use of quality predictions
is the decision between post-editing a given ma-
chine translated sentence and translating its source
from scratch.
GP regression models were recently success-
fully employed for post-editing time (Cohn and
Specia, 2013) and HTER
2
prediction (Beck et al,
2013). Both used RBF kernels as the covariance
function so a natural extension is to apply the
structured kernels of Section 3.1. This was already
been done with tree kernels by Hardmeier (2011)
in the context of SVMs.
Multi-task kernels can also be applied for Qual-
ity Estimation in several ways. The model used
by Cohn and Specia (2013) for modelling annota-
tor bias can be further extended for settings with
dozens or even hundreds of annotators. This is a
common setting in crowdsourcing platforms like
Amazon?s Mechanical Turk
3
.
Another plan is to use multi-task kernels to
combine different datasets. Quality annotation is
usually expensive, requiring post-editing or sub-
jective scoring. Possibilities include combining
datasets from different language pairs or different
machine translation systems. Available datasets
include those used in the WMT12 and WMT13
QE shared tasks (Callison-burch et al, 2012; Bo-
jar et al, 2013) and others (Specia et al, 2009;
Specia, 2011; Koponen et al, 2012).
4.2 Question Classification
A Question Classifier is a module that aims to re-
strict the answer hypotheses generated by a Ques-
tion Answering system by applying a label to the
input question (Li and Roth, 2002; Li and Roth,
2
Human Translation Error Rate (Snover et al, 2006).
3
www.mturk.com
2005). This task can be seen as an instance of text
classification, where the inputs are usually com-
posed of only one sentence.
Much of previous work in Question Classifica-
tion largely used SVMs combined with structured
kernels. Zhang and Lee (2003) compares String
Kernels based on BOW and n-gram representa-
tions with the Subset Tree Kernel on constituent
trees. Moschitti (2006) show improved results by
using the Partial Tree Kernel and dependency trees
instead of constituency ones. Bloehdorn and Mos-
chitti (2007) combines a SSTK with different soft
matching approaches to encode lexical similarity
on tree leaves. The same soft matching idea is
used by Croce et al (2011), but applied to PTKs
instead and permitting soft matches between any
nodes in each tree (which is sensible when using
kernels on dependency trees).
Our work proposes to address this task by em-
ploying tree kernels and GPs. Unlike Quality Esti-
mation, this is a classification setting and our pur-
pose is to find if this combination can also improve
the state-of-the-art for tasks of this kind. We will
use the TREC dataset provided by Li and Roth
(2002), which assigns 6000 questions with both a
coarse and a fine-grained label.
4.3 Multi-domain Sentiment Analysis
Sentiment Analysis is defined as ?the computa-
tional treatment of opinion, sentiment and subjec-
tivity in text? (Pang and Lee, 2008). In this pro-
posal, we focus on the specific task of polarity de-
tection, where the goal is to label a text as hav-
ing positive or negative sentiment. State-of-the-art
methods for this task use SVMs as the learning al-
gorithm and vary between the feature sets used.
Polarity predictions can be heavily biased on
the text domain. Consider the example showed
by Turney (2002): the word ?unpredictable? usu-
ally has a positive meaning in a movie review but
a negative one when applied to an automotive re-
view (in a phrase like ?unpredictable steering?, for
instance). One of the first methods to tackle this
issue is the Structural Correspondence Learning
of Blitzer et al (2007). Their method uses pivot
words shared between domains to find correspon-
dencies in words that are not shared.
A previous work that used structured kernels in
Sentiment Analysis is the approach of Wu et al
(2009). Their method uses tree kernels on phrase
dependency trees and outperforms bag-of-words
5
and word dependency approaches. They also show
good results in cross-domain experiments.
We propose to apply GPs with a combination
of structured and multi-task kernels for this task.
The results showed by Wu et al (2009) suggest
that tree kernels on dependency trees are a good
approach but we also plan to employ string ker-
nels on this task. This is because string kernels
have demonstrated promising results for text cate-
gorization in past work. Also, considering model
selection is easily dealt by GPs, we can combine
all those kernels in complex and heavily param-
eterized ways, an unfeasible setting for SVMs.
We will use the Multi-Domain Sentiment Dataset
(Blitzer et al, 2007), composed of Amazon prod-
uct reviews in different categories.
4.4 Research Directions
In Section 2.3 we saw how the Bayesian formu-
lation of GPs let us do model selection by maxi-
mizing the marginal likelihood. In fact, one of our
main research directions in this proposal revolves
around this crucial point: because we can easily fit
hyperparameters to the data we have much more
freedom to use richer kernel parameterizations and
kernel combinations. Multi-task kernels are one
example where we usually have a large number of
hyperparameters because we need to fit all the el-
ements of the coregionalization matrix. This num-
ber can get even larger if we have a LMC model,
with multiple coregionalization matrices. Struc-
tured kernels can also be redefined in a richer way:
tree kernels between constituency trees could have
multiple decay hyperparameters, one for each POS
tag. A more extreme example would be to treat
all weights in a string kernel as hyperparameters.
Thus, we plan to investigate these possibilities in
the context of the three tasks detailed before.
As another research direction we also want to
address the issue of scalability. Although GPs al-
ready showed promising results they can be slow
when compared to other well established meth-
ods like SVM. Fortunately there has been a lot
of advancements in the field of sparse GPs in the
last years and we plan to employ them in our
work. A key question is how to combine sparse
GPs with the structured kernels we presented be-
fore. Although it is perfectly possible to select in-
ducing points using greedy methods, it would be
much more interesting to use the pseudo-inputs
approach. However, it is not clear how to do that
in conjunction with non-vectorial inputs, like the
ones we plan to use in structured kernels, and this
is a key direction that we also plan to investigate.
4.5 GP Toolkits
Available toolkits for GP modelling include
GPML
4
(Rasmussen and Williams, 2006) and GP-
stuff
5
(Vanhatalo et al, 2013), which are written
in Matlab. Our experiments will mainly use GPy
6
,
an open source toolkit written in Python. It imple-
ments models for regression and binary classifi-
cation, including sparse approximations and many
vectorial kernels. We plan to contribute to GPy
by implementing the structured kernels of Section
3.1, effectively extending it to a GP framework for
NLP.
5 Conclusions and Future Work
In this work we showed a proposal for advancing
the state-of-the-art in a number of NLP tasks by
combining Gaussian Process with structured and
multi-task kernels. Our hypothesis is that highly
parameterized kernel combinations allied with the
fitting methods provided by GPs will result in bet-
ter models for these tasks. We also detailed the
future plans for experiments, including available
datasets and toolkits.
Further research directions that can be explored
by this proposal include the use of GPs in different
learning settings. Models for ordinal regression
(Chu and Ghahramani, 2005) and structured pre-
diction (Altun et al, 2004; Brati`eres et al, 2013)
were already proposed in the GP literature and a
natural extension is to apply these models for their
corresponding NLP tasks. Another extension is to
employ other kinds of kernels. The literature on
that subject is quite vast, with many approaches
showing promising results.
Acknowledgements
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9) and from the
EU FP7-ICT QTLaunchPad project (No. 296347).
The author would also like to thank Yahoo for the
financial support and the anonymous reviewers for
their excellent comments.
4
www.gaussianprocess.org/gpml/code/
matlab
5
becs.aalto.fi/en/research/bayes/
gpstuff
6
github.com/SheffieldML/GPy
6
References
Yasemin Altun, Thomas Hofmann, and Alexander J.
Smola. 2004. Gaussian Process Classification for
Segmenting and Annotating Sequences. In Proceed-
ings of ICML, page 8, New York, New York, USA.
ACM Press.
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of ACL.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Exploiting Structure and Semantics for Expressive
Text Kernels. In Proceedings of CIKM.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of WMT13, pages 1?44.
S?ebastien Brati`eres, Novi Quadrianto, and Zoubin
Ghahramani. 2013. Bayesian Structured Prediction
using Gaussian Processes. arXiv:1307.3846, pages
1?17.
Chris Callison-burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean-Michel Renders. 2003. Word-Sequence Ker-
nels. The Journal of Machine Learning Research,
3:1059?1082.
Kai-Wei Chang, Vivek Srikumar, and Dan Roth. 2013.
Multi-core Structural SVM Training. In Proceed-
ings of ECML-PHDD.
Wei Chu and Zoubin Ghahramani. 2005. Gaussian
Processes for Ordinal Regression. Journal of Ma-
chine Learning Research, 6:1019?1041.
Wei Chu, Zoubin Ghahramani, Francesco Falciani, and
David L Wild. 2005. Biomarker discovery in mi-
croarray gene expression data with Gaussian pro-
cesses. Bioinformatics, 21(16):3385?93, August.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning, 11:3053?3096.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Advances in Neu-
ral Information Processing Systems.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured Lexical Similarity via Con-
volution Kernels on Dependency Trees. In Proc. of
EMNLP.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of
the American Society For Information Science, 41.
Thomas G?artner, Peter Flach, and Stefan Wrobel.
2003. On Graph Kernels: Hardness Results and Ef-
ficient Alternatives. LNAI, 2777:129?143.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54, July.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree Kernels for Machine Transla-
tion Quality Estimation. In Proceedings of WMT12,
number 2011, pages 109?113.
Christian Hardmeier. 2011. Improving Machine
Translation Quality Prediction with Syntactic Tree
Kernels. In Proceedings of EAMT, number May.
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. Technical report.
James Hensman, Nicol`o Fusi, and Neil D. Lawrence.
2013. Gaussian Processes for Big Data. In Pro-
ceedings of UAI.
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic Variational Inference. The
Journal of Machine Learning Research.
Tommi Jaakkola, Mark Diekhans, and David Haussler.
2000. A discriminative framework for detecting re-
mote protein homologies. Journal of Computational
Biology, 7:95?114.
Richard Johansson and Alessandro Moschitti. 2013.
Relational Features in Fine-Grained Opinion Analy-
sis. Computational Linguistics, 39(3):473?509.
7
Jonathan Ko, Daniel J. Klein, Dieter Fox, and Dirk
Haehnel. 2007. Gaussian Processes and Reinforce-
ment Learning for Identification and Control of an
Autonomous Blimp. In Proceedings of IEEE Inter-
national Conference on Robotics and Automation,
pages 742?747. Ieee, April.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing time as a measure
of cognitive effort. In Proceedings of WPTP.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of COLING, volume 1, pages
1?7.
Xin Li and Dan Roth. 2005. Learning Question Clas-
sifiers: the Role of Semantic Information. Natural
Language Engineering, 1(1).
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
Classification using String Kernels. The Journal of
Machine Learning Research, 2:419?444.
Thomas P. Minka. 2001. A family of algorithms for
approximate Bayesian inference. Ph.D. thesis.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In Proceedings of ECML.
Kevin P. Murphy. 2012. Machine Learning: a Proba-
bilistic Perspective.
Radford M. Neal. 1998. Regression and Classification
Using Gaussian Process Priors. Bayesian Statistics,
6.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Ver?onica P?erez-Rosas and Rada Mihalcea. 2013. Sen-
timent Analysis of Online Spoken Reviews. In Pro-
ceedings of Interspeech.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adaptation of Relation Extraction. In Pro-
ceedings of ACL, pages 1498?1507.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
Gaussian Processes: a preliminary evaluation. Inter-
national Journal of Data Mining and Bioinformat-
ics, 5(1):52?72, January.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of EMNLP.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Jaakko Riihim?aki, Pasi Jyl?anki, and Aki Vehtari. 2013.
Nested Expectation Propagation for Gaussian Pro-
cess Classification with a Multinomial Probit Like-
lihood. Journal of Machine Learning Research,
14:75?109.
Anton Schwaighofer, Marian Grigoras, Volker Tresp,
and Clemens Hoffmann. 2004. GPPS: A Gaussian
Process Positioning System for Cellular Networks.
In Proceedings of NIPS.
Matthias Seeger, Christopher K. I. Williams, and
Neil D. Lawrence. 2003. Fast Forward Selection
to Speed Up Sparse Gaussian Process Regression.
In Proceedings of AISTATS.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel methods for pattern analysis. Cambridge.
Fabian H. Sinz, Joaquin Qui?nonero Candela,
G?okhan H. Bak?r, Carl E. Rasmussen, and
Matthias O. Franz. 2004. Learning Depth from
Stereo. Pattern Recognition, pages 1?8.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse Gaussian Processes using Pseudo-inputs. In
Proceedings of NIPS.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Lucia Specia and Atefeh Farzindar. 2010. Estimating
machine translation post-editing effort with HTER.
In Proceedings of AMTA Workshop Bringing MT to
the User: MT Research and the Translation Indus-
try.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of EAMT, pages 28?35.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
Michalis K. Titsias. 2009. Variational Learning of In-
ducing Variables in Sparse Gaussian Processes. In
Proceedings of AISTATS, volume 5, pages 567?574.
Peter D. Turney. 2002. Thumbs Up or Thumbs
Down?: Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of
ACL, number July, pages 417?424.
Jarno Vanhatalo, Jaakko Riihim?aki, Jouni Hartikainen,
Pasi Jyl?anki, Ville Tolvanen, and Aki Vehtari. 2013.
GPstuff: Bayesian Modeling with Gaussian Pro-
cesses. The Journal of Machine Learning Research,
14:1175?1179.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi
Kondor, and Karsten M. Borgwardt. 2010. Graph
Kernels. Journal of Machine Learning Research,
11:1201?1242.
8
Christopher K. I. Williams and David Barber. 1998.
Bayesian Classification with Gaussian Processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342?1351.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase Dependency Parsing for Opinion Min-
ing. In Proceedings of EMNLP, pages 1533?1541.
Dell Zhang and Wee Sun Lee. 2003. Question classi-
fication using support vector machines. In Proceed-
ings of SIGIR, New York, New York, USA. ACM
Press.
9
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
SHEF-Lite: When Less is More for Translation Quality Estimation
Daniel Beck and Kashif Shah and Trevor Cohn and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk
Abstract
We describe the results of our submissions
to the WMT13 Shared Task on Quality
Estimation (subtasks 1.1 and 1.3). Our
submissions use the framework of Gaus-
sian Processes to investigate lightweight
approaches for this problem. We focus on
two approaches, one based on feature se-
lection and another based on active learn-
ing. Using only 25 (out of 160) fea-
tures, our model resulting from feature
selection ranked 1st place in the scoring
variant of subtask 1.1 and 3rd place in
the ranking variant of the subtask, while
the active learning model reached 2nd
place in the scoring variant using only
?25% of the available instances for train-
ing. These results give evidence that
Gaussian Processes achieve the state of
the art performance as a modelling ap-
proach for translation quality estimation,
and that carefully selecting features and
instances for the problem can further im-
prove or at least maintain the same per-
formance levels while making the problem
less resource-intensive.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al,
2004; Specia et al, 2009; Callison-burch et al,
2012). A common use of quality predictions is
the decision between post-editing a given machine
translated sentence and translating its source from
scratch, based on whether its post-editing effort is
estimated to be lower than the effort of translating
the source sentence.
The WMT13 QE shared task defined a group
of tasks related to QE. In this paper, we present
the submissions by the University of Sheffield
team. Our models are based on Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2006), a
non-parametric probabilistic framework. We ex-
plore the application of GP models in two con-
texts: 1) improving the prediction performance by
applying a feature selection step based on opti-
mised hyperparameters and 2) reducing the dataset
size (and therefore the annotation effort) by per-
forming Active Learning (AL). We submitted en-
tries for two of the four proposed tasks.
Task 1.1 focused on predicting HTER scores
(Human Translation Error Rate) (Snover et al,
2006) using a dataset composed of 2254 English-
Spanish news sentences translated by Moses
(Koehn et al, 2007) and post-edited by a profes-
sional translator. The evaluation used a blind test
set, measuring MAE (Mean Absolute Error) and
RMSE (Root Mean Square Error), in the case of
the scoring variant, and DeltaAvg and Spearman?s
rank correlation in the case of the ranking vari-
ant. Our submissions reached 1st (feature selec-
tion) and 2nd (active learning) places in the scor-
ing variant, the task the models were optimised
for, and outperformed the baseline by a large mar-
gin in the ranking variant.
The aim of task 1.3 aimed at predicting post-
editing time using a dataset composed of 800
English-Spanish news sentences also translated by
Moses but post-edited by five expert translators.
Evaluation was done based on MAE and RMSE
on a blind test set. For this task our models were
not able to beat the baseline system, showing that
more advanced modelling techniques should have
been used for challenging quality annotation types
and datasets such as this.
2 Features
In our experiments, we used a set of 160 features
which are grouped into black box (BB) and glass
box (GB) features. They were extracted using the
337
open source toolkit QuEst1 (Specia et al, 2013).
We briefly describe them here, for a detailed de-
scription we refer the reader to the lists available
on the QuEst website.
The 112 BB features are based on source and
target segments and attempt to quantify the source
complexity, the target fluency and the source-
target adequacy. Examples of them include:
? Word and n-gram based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
the source side of the MT training cor-
pus;
? Average number of translations per
source word in the segment as given by
IBM 1 model with probabilities thresh-
olded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
? Syntactic features:
? Source and target Probabilistic Context-
free Grammar (PCFG) parse log-
likelihood;
? Source and target PCFG average confi-
dence of all possible parse trees in the
parser?s n-best list;
? Difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP
phrases in the source and target;
? Other features:
? Kullback-Leibler divergence of source
and target topic model distributions;
? Jensen-Shannon divergence of source
and target topic model distributions;
1http://www.quest.dcs.shef.ac.uk
? Source and target sentence intra-lingual
mutual information;
? Source-target sentence inter-lingual mu-
tual information;
? Geometric average of target word prob-
abilities under a global lexicon model.
The 48 GB features are based on information
provided by the Moses decoder, and attempt to in-
dicate the confidence of the system in producing
the translation. They include:
? Features and global score of the SMT model;
? Number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? Average size of the target phrases;
? Relative frequency of the words in the trans-
lation in the n-best list;
? Ratio of SMT model score of the top transla-
tion to the sum of the scores of all hypothesis
in the n-best list;
? Average size of hypotheses in the n-best list;
? N-best list density (vocabulary size / average
sentence length);
? Fertility of the words in the source sentence
compared to the n-best list in terms of words
(vocabulary size / source sentence length);
? Edit distance of the current hypothesis to the
centre hypothesis;
? Proportion of pruned search graph nodes;
? Proportion of recombined graph nodes.
3 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : RF ? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x?))
which is parameterized by a mean function (here,
0) and a covariance kernel function k(x,x?). Each
338
response value is then generated from the function
evaluated at the corresponding input, yi = f(xi)+
?, where ? ? N (0, ?2n) is added white-noise.
Prediction is formulated as a Bayesian inference
under the posterior:
p(y?|x?,D) =
?
f
p(y?|x?, f)p(f |D)
where x? is a test input, y? is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y? ? N (kT? (K + ?2nI)?1y,
k(x?, x?)? kT? (K + ?2nI)?1k?)
where k? = [k(x?,x1)k(x?,x2) . . . k(x?,xd)]T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs.
A nice property of this formulation is that y?
is actually a probability distribution, encoding the
model uncertainty and making it possible to inte-
grate it into subsequent processing. In this work,
we used the variance values given by the model in
an active learning setting, as explained in Section
4.
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work on QE using GP (Cohn and
Specia, 2013; Shah et al, 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x?) = ?2f exp
(
?12
F?
i=1
xi ? x?i
li
)
where F is the number of features, ?2f is the co-
variance magnitude and li > 0 are the feature
length scales.
The resulting model hyperparameters (SE vari-
ance ?2f , noise variance ?2n and SE length scales li)
were learned from data by maximising the model
likelihood. In general, the likelihood function is
non-convex and the optimisation procedure may
lead to local optima. To avoid poor hyperparam-
eter values due to this, we performed a two-step
procedure where we first optimise a model with all
the SE length scales tied to the same value (which
is equivalent to an isotropic model) and we used
the resulting values as starting point for the ARD
optimisation.
All our models were trained using the GPy2
toolkit, an open source implementation of GPs
written in Python.
3.1 Feature Selection
To perform feature selection, we followed the ap-
proach used in Shah et al (2013) and ranked the
features according to their learned length scales
(from the lowest to the highest). The length scales
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
For task 1.1, we performed this feature selection
over all 160 features mentioned in Section 2. For
task 1.3, we used a subset of the 80 most general
BB features as in (Shah et al, 2013), for which we
had all the necessary resources available for the
extraction. We selected the top 25 features for both
models, based on empirical results found by Shah
et al (2013) for a number of datasets, and then
retrained the GP using only the selected features.
4 Active Learning
Active Learning (AL) is a machine learning
paradigm that let the learner decide which data it
wants to learn from (Settles, 2010). The main goal
of AL is to reduce the size of the dataset while
keeping similar model performance (therefore re-
ducing annotation costs). In previous work with
17 baseline features, we have shown that with only
?30% of instances it is possible to achieve 99%
of the full dataset performance in the case of the
WMT12 QE dataset (Beck et al, 2013).
To investigate if a reduced dataset can achieve
competitive performance in a blind evaluation set-
ting, we submitted an entry for both tasks 1.1 and
1.3 composed of models trained on a subset of in-
stances selected using AL, and paired with fea-
ture selection. Our AL procedure starts with a
model trained on a small number of randomly se-
lected instances from the training set and then uses
this model to query the remaining instances in the
training set (our query pool). At every iteration,
the model selects the more ?informative? instance,
asks an oracle for its true label (which in our case
is already given in the dataset, and therefore we
2http://sheffieldml.github.io/GPy/
339
only simulate AL) and then adds it to the training
set. Our procedure started with 50 instances for
task 1.1 and 20 instances for task 1.3, given its re-
duced training set size. We optimised the Gaussian
Process hyperparameters every 20 new instances,
for both tasks.
As a measure of informativeness we used Infor-
mation Density (ID) (Settles and Craven, 2008).
This measure leverages between the variance
among instances and how dense the region (in the
feature space) where the instance is located is:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x,x(u))
)?
The ? parameter controls the relative impor-
tance of the density term. In our experiments, we
set it to 1, giving equal weights to variance and
density. The U term is the number of instances
in the query pool. The variance values V ar(y|x)
are given by the GP prediction while the similar-
ity measure sim(x,x(u)) is defined as the cosine
distance between the feature vectors.
In a real annotation setting, it is important to
decide when to stop adding new instances to the
training set. In this work, we used the confidence
method proposed by Vlachos (2008). This is an
method that measures the model?s confidence on
a held-out non-annotated dataset every time a new
instance is added to the training set and stops the
AL procedure when this confidence starts to drop.
In our experiments, we used the average test set
variance as the confidence measure.
In his work, Vlachos (2008) showed a correla-
tion between the confidence and test error, which
motivates its use as a stop criterion. To check if
this correlation also occurs in our task, we measure
the confidence and test set error for task 1.1 using
the WMT12 split (1832/422 instances). However,
we observed a different behaviour in our experi-
ments: Figure 1 shows that the confidence does
not raise or drop according to the test error but it
stabilises around a fixed value at the same point as
the test error also stabilises. Therefore, instead of
using the confidence drop as a stop criterion, we
use the point where the confidence stabilises. In
Figure 2 we can observe that the confidence curve
for the WMT13 test set stabilises after ?580 in-
stances. We took that point as our stop criterion
and used the first 580 selected instances as the AL
dataset.
Figure 1: Test error and test confidence curves
for HTER prediction (task 1.1) using the WMT12
training and test sets.
Figure 2: Test confidence for HTER prediction
(task 1.1) using the official WMT13 training and
test sets.
We repeated the experiment with task 1.3, mea-
suring the relationship between test confidence
and error using a 700/100 instances split (shown
on Figure 3). For this task, the curves did not fol-
low the same behaviour: the confidence do not
seem to stabilise at any point in the AL proce-
dure. The same occurred when using the official
training and test sets (shown on Figure 4). How-
ever, the MAE curve is quite flat, stabilising after
about 100 sentences. This may simply be a conse-
quence of the fact that our model is too simple for
post-editing time prediction. Nevertheless, in or-
der to analyse the performance of AL for this task
we submitted an entry using the first 100 instances
chosen by the AL procedure for training.
The observed peaks in the confidence curves re-
340
Task 1.1 - Ranking Task 1.1 - Scoring Task 1.3
DeltaAvg ? Spearman ? MAE ? RMSE ? MAE ? RMSE ?
SHEF-Lite-FULL 9.76 0.57 12.42 15.74 55.91 103.11
SHEF-Lite-AL 8.85 0.50 13.02 17.03 64.62 99.09
Baseline 8.52 0.46 14.81 18.22 51.93 93.36
Table 1: Submission results for tasks 1.1 and 1.3. The bold value shows a winning entry in the shared
task.
Figure 3: Test error and test confidence curves
for post-editing time prediction (task 1.3) using a
700/100 split on the WMT13 training set.
Figure 4: Test confidence for post-editing time
prediction (task 1.3) using the official WMT13
training and test sets.
sult from steps where the hyperparameter optimi-
sation got stuck at bad local optima. These de-
generated results set the variances (?2f , ?2n) to very
high values, resulting in a model that considers all
data as pure noise. Since this behaviour tends to
disappear as more instances are added to the train-
ing set, we believe that increasing the dataset size
helps to tackle this problem. We plan to investi-
gate this issue in more depth in future work.
For both AL datasets we repeated the feature se-
lection procedure explained in Section 3.1, retrain-
ing the models on the selected features.
5 Results
Table 1 shows the results for both tasks. SHEF-
Lite-FULL represents GP models trained on the
full dataset (relative to each task) with a feature
selection step. SHEF-Lite-AL corresponds to the
same models trained on datasets obtained from
each active learning procedure and followed by
feature selection.
For task 1.1, our submission SHEF-Lite-FULL
was the winning system in the scoring subtask, and
ranked third in the ranking subtask. These results
show that GP models achieve the state of the art
performance in QE. These are particularly positive
results considering that there is room for improve-
ment in the feature selection procedure to identify
the optimal number of features to be selected. Re-
sults for task 1.3 were below the baseline, once
again evidencing the fact that the noise model used
in our experiments is probably too simple for post-
editing time prediction. Post-editing time is gener-
ally more prone to large variations and noise than
HTER, especially when annotations are produced
by multiple post-editors. Therefore we believe that
kernels that encode more advanced noise models
(such as the multi-task kernel used by Cohn and
Specia (2013)) should be used for better perfor-
mance. Another possible reason for that is the
smaller set of features used for this task (black-
box features only).
Our SHEF-Lite-AL submissions performed bet-
ter than the baseline in both scoring and ranking
in task 1.1, ranking 2nd place in the scoring sub-
task. Considering that the dataset is composed by
only ?25% of the full training set, these are very
encouraging results in terms of reducing data an-
341
notation needs. We note however that these results
are below those obtained with the full training set,
but Figure 1 shows that it is possible to achieve
the same or even better results with an AL dataset.
Since the curves shown in Figure 1 were obtained
using the full feature set, we believe that advanced
feature selection strategies can help AL datasets to
achieve better results.
6 Conclusions
The results obtained by our submissions confirm
the potential of Gaussian Processes to become the
state of the art approach for Quality Estimation.
Our models were able to achieve the best perfor-
mance in predicting HTER. They also offer the ad-
vantage of inferring a probability distribution for
each prediction. These distributions provide richer
information (like variance values) that can be use-
ful, for example, in active learning settings.
In the future, we plan to further investigate these
models by devising more advanced kernels and
feature selection methods. Specifically, we want
to employ our feature set in a multi-task kernel set-
ting, similar to the one proposed by Cohn and Spe-
cia (2013). These kernels have the power to model
inter-annotator variance and noise, which can lead
to better results in the prediction of post-editing
time.
We also plan to pursue better active learning
procedures by investigating query methods specif-
ically tailored for QE, as well as a better stop cri-
teria. Our goal is to be able to reduce the dataset
size significantly without hurting the performance
of the model. This is specially interesting in the
case of QE, since it is a very task-specific problem
that may demand a large annotation effort.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Kashif Shah and Lucia Specia).
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013.
Reducing Annotation Effort for Quality Estimation
via Active Learning. In Proceedings of ACL (to ap-
pear).
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL (to appear).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV (to appear).
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jose? G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session (to appear).
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech & Language,
22(3):295?312, July.
342
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 307?312,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation
Quality Estimation
Daniel Beck and Kashif Shah and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,l.specia}@sheffield.ac.uk
Abstract
We describe our systems for the WMT14
Shared Task on Quality Estimation (sub-
tasks 1.1, 1.2 and 1.3). Our submissions
use the framework of Multi-task Gaus-
sian Processes, where we combine multi-
ple datasets in a multi-task setting. Due to
the large size of our datasets we also ex-
periment with Sparse Gaussian Processes,
which aim to speed up training and predic-
tion by providing sensible sparse approxi-
mations.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al.,
2004; Specia et al., 2009; Bojar et al., 2013). A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
The WMT 2014 QE shared task defined a group
of tasks related to QE. In this paper, we de-
scribe our submissions for subtasks 1.1, 1.2 and
1.3. Our models are based on Gaussian Pro-
cesses (GPs) (Rasmussen and Williams, 2006),
a non-parametric kernelised probabilistic frame-
work. We propose to combine multiple datasets
to improve our QE models by applying GPs in
a multi-task setting. Our hypothesis is that us-
ing sensible multi-task learning settings gives im-
provements over simply pooling all datasets to-
gether.
Task 1.1 focuses on predicting post-editing ef-
fort for four language pairs: English-Spanish
(en-es), Spanish-English (es-en), English-German
(en-de), and German-English (de-en). Each con-
tains a different number of source sentences and
their human translations, as well as 2-3 versions
of machine translations: by a statistical (SMT)
system, a rule-based system (RBMT) system and,
for en-es/de only, a hybrid system. Source sen-
tences were extracted from tests sets of WMT13
and WMT12, and the translations were produced
by top MT systems of each type and a human
translator. Labels range from 1 to 3, with 1 in-
dicating a perfect translation and 3, a low quality
translation.
The purpose of task 1.2 is to predict HTER
scores (Human Translation Error Rate) (Snover
et al., 2006) using a dataset composed of 896
English-Spanish sentences translated by a MT sys-
tem and post-edited by a professional translator.
Finally, task 1.3 aims at predicting post-editing
time, using a subset of 650 sentences from the
Task 1.2 dataset.
For each task, participants can submit two types
of results: scoring and ranking. For scoring, eval-
uation is made in terms of Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). For
ranking, DeltaAvg and Spearman?s rank correla-
tion were used as evaluation metrics.
2 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : R
F
? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x
?
)),
which is parameterised by a mean function (here,
0) and a covariance kernel function k(x,x
?
). Each
response value is then generated from the function
evaluated at the corresponding input, y
i
= f(x
i
)+
?, where ? ? N (0, ?
2
n
) is added white-noise.
307
Prediction is formulated as a Bayesian inference
under the posterior:
p(y
?
|x
?
,D) =
?
f
p(y
?
|x
?
, f)p(f |D),
where x
?
is a test input, y
?
is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y
?
? N (k
T
?
(K + ?
2
n
I)
?1
y,
k(x
?
, x
?
)? k
T
?
(K + ?
2
n
I)
?1
k
?
),
where k
?
= [k(x
?
,x
1
)k(x
?
,x
2
) . . . k(x
?
,x
n
)]
T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs (the Gram matrix).
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work in QE using GP (Cohn and
Specia, 2013; Shah et al., 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x
?
) = ?
2
f
exp
(
?
1
2
F
?
i=1
x
i
? x
?
i
l
i
)
,
where F is the number of features, ?
2
f
is the co-
variance magnitude and l
i
> 0 are the feature
lengthscales.
The resulting model hyperparameters (SE vari-
ance ?
2
f
, noise variance ?
2
n
and SE lengthscales l
i
)
were learned from data by maximising the model
likelihood. All our models were trained using the
GPy
1
toolkit, an open source implementation of
GPs written in Python.
2.1 Multi-task learning
The GP regression framework can be extended to
multiple outputs by assuming f(x) to be a vec-
tor valued function. These models are commonly
referred as coregionalization models in the GP lit-
erature (
?
Alvarez et al., 2012). Here we refer to
them as multi-task kernels, to emphasize our ap-
plication.
In this work, we employ a separable multi-task
kernel, similar to the one used by Bonilla et al.
(2008) and Cohn and Specia (2013). Consider-
ing a set of D tasks, we define the corresponding
multi-task kernel as:
k((x, d), (x
?
, d
?
)) = k
data
(x,x
?
)?B
d,d
?
, (1)
1
http://sheffieldml.github.io/GPy/
where k
data
is a kernel on the input points, d and
d
?
are task or metadata information for each input
and B ? R
D?D
is the multi-task matrix, which
encodes task covariances. For task 1.1, we con-
sider each language pair as a different task, while
for tasks 1.2 and 1.3 we use additional datasets
for the same language pair (en-es), treating each
dataset as a different task.
To perform the learning procedure the multi-
task matrix should be parameterised in a sensible
way. We follow the parameterisations proposed
by Cohn and Specia (2013), which we briefly de-
scribe here:
Independent: B = I. In this setting each task is
modelled independently. This is not strictly
equivalent to independent model training be-
cause the tasks share the same data kernel
(and the same hyperparameters);
Pooled: B = 1. Here the task identity is ignored.
This is equivalent to pooling all datasets in a
single task model;
Combined: B = 1 + ?I. This setting lever-
ages between independent and pooled mod-
els. Here, ? > 0 is treated as an hyperparam-
eter;
Combined+: B = 1 + diag(?). Same as ?com-
bined?, but allowing one different ? value per
task.
2.2 Sparse Gaussian Processes
The performance bottleneck for GP models is the
Gram matrix inversion, which is O(n
3
) for stan-
dard GPs, with n being the number of training in-
stances. For multi-task settings this can be a po-
tential issue because these models replicate the in-
stances for each task and the resulting Gram ma-
trix has dimensionality nd ? nd, where d is the
number of tasks.
Sparse GPs tackle this problem by approximat-
ing the Gram matrix using only a subset of m in-
ducing inputs. Without loss of generalisation, con-
sider these m points as the first instances in the
training data. We can then expand the Gram ma-
trix in the following way:
K =
[
K
mm
K
m(n?m)
K
(n?m)m
K
(n?m)(n?m)
]
.
Following the notation in (Rasmussen and
Williams, 2006), we refer K
m(n?m)
as K
mn
and
308
its transpose as K
nm
. The block structure of K
forms the basis of the so-called Nystr?om approxi-
mation:
?
K = K
nm
K
?1
mm
K
mn
, (2)
which results in the following predictive posterior:
y
?
? N (k
T
m?
?
G
?1
K
mn
y, (3)
k(x
?
,x
?
)? k
T
m?
K
?1
mm
k
m?
+
?
2
n
k
T
m?
?
G
?1
k
m?
),
where
?
G = ?
2
n
K
mm
+ K
mn
K
nm
and k
m?
is the
vector of kernel evaluations between test input x
?
and the m inducing inputs. The resulting training
complexity is O(m
2
n).
The remaining question is how to choose the in-
ducing inputs. We follow the approach of Snelson
and Ghahramani (2006), which note that these in-
ducing inputs do not need to be a subset of the
training data. Their method considers each in-
put as a hyperparameter, which is then optimised
jointly with the kernel hyperparameters.
2.3 Features
For all tasks we used the QuEst framework (Spe-
cia et al., 2013) to extract a set of 80 black-box
features as in Shah et al. (2013), for which we had
all the necessary resources available. Examples of
the features extracted include:
? N-gram-based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
a large corpus of the source language;
? Average number of translations per
source word in the segment as given by
IBM 1 model from a large parallel cor-
pus of the language, with probabilities
thresholded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
For the full set of features we refer readers to
QuEst website.
2
To perform feature selection, we followed the
approach used in Shah et al. (2013) and ranked
the features according to their learned lengthscales
(from the lowest to the highest). The lengthscale
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
3 Preliminary Experiments
Our submissions are based on multi-task settings.
For task 1.1, we consider each language pair as a
different task, training one model for all pairs. For
tasks 1.2 and 1.3, we used additional datasets and
encoded each one as a different task (totalling 3
tasks):
WMT13: these are the datasets provided in last
year?s QE shared task (Bojar et al., 2013).
We combined training and test sets, totalling
2, 754 sentences for HTER prediction and
1, 003 sentences for post-editing time predic-
tion, both for English-Spanish.
EAMT11: this dataset is provided by Specia
(2011) and is composed of 1, 000 English-
Spanish sentences annotated in terms of
HTER and post-editing time.
For each task we prepared two submissions: one
trained on a standard GP with the full 80 features
set and another one trained on a sparse GP with
a subset of 40 features. The features were chosen
by training a smaller model on a subset of 400 in-
stances and following the procedure explained in
Section 2.3 for feature selection, with a pre-define
cutoff point on the number of features (40), based
on previous experiments. The sparse models were
trained using 400 inducing inputs.
To select an appropriate multi-task setting for
our submissions we performed preliminary exper-
iments using a 90%/10% split on the correspond-
ing training set for each task. The resulting MAE
scores are shown in Tables 1 and 2, for standard
and sparse GPs, respectively. The boldface fig-
ures correspond to the settings we choose for the
2
http://www.quest.dcs.shef.ac.uk/
quest_files/features_blackbox
309
Task 1.1 Task 1.2 Task 1.3
en-es es-en en-de de-en en-es en-es
Independent 0.4905 0.5325 0.5962 0.5452 0.2047 0.4486
Pooled 0.4957 0.5171 0.6012 0.5612 0.2036 0.8599
Combined 0.4939 0.5162 0.6007 0.5550 0.2321 0.7489
Combined+ 0.4932 0.5182 0.5990 0.5514 0.2296 0.4472
Table 1: MAE results for preliminary experiments on standard GPs. Post-editing time scores for task 1.3
are shown on log time per word.
Task 1.1 Task 1.2 Task 1.3
en-es es-en en-de de-en en-es en-es
Independent 0.5036 0.5274 0.6002 0.5532 0.3432 0.3906
Pooled 0.4890 0.5131 0.5927 0.5532 0.1597 0.6410
Combined 0.4872 0.5183 0.5871 0.5451 0.2871 0.6449
Combined+ 0.4935 0.5255 0.5864 0.5458 0.1659 0.4040
Table 2: MAE results for preliminary experiments on sparse GPs. Post-editing time scores for task 1.3
are shown on log time per word.
official submissions, after re-training on the corre-
sponding full training sets.
To check the speed-ups obtained from using
sparse GPs, we measured wall clock times for
training and prediction in Task 1.1 using the ?In-
dependent? multi-task setting. Table 3 shows the
resulting times and the corresponding speed-ups
when comparing to the standard GP. For compar-
ison, we also trained a model using 200 inducing
inputs, although we did not use the results of this
model in our submissions.
Time (secs) Speed-up
Standard GP 12122 ?
Sparse GP (m=400) 3376 3.59x
Sparse GP (m=200) 978 12.39x
Table 3: Wall clock times and speed-ups for GPs
training and prediction: full versus sparse GPs.
4 Official Results and Discussion
Table 4 shows the results for Task 1.1. Us-
ing standard GPs we obtained improved results
over the baseline for English-Spanish and English-
German only, with particularly substantial im-
provements for English-Spanish, which also hap-
pens for sparse GPs. This may be related to the
larger size of this dataset when compared to the
others. Our results here are mostly inconclusive
though and we plan to investigate this setting more
in depth in the future. Specifically, due to the
coarse behaviour of the labels, ordinal regression
GP models (like the one proposed in (Chu et al.,
2005)) could be useful for this task.
Results for Task 1.2 are shown in Table 5. The
standard GP model performed unusually poorly
when compared to the baseline or the sparse GP
model. To investigate this, we inspected the re-
sulting model hyperparameters. We found out that
the noise ?
2
n
was optimised to a very low value,
close to zero, which characterises overfitting. The
same behaviour was not observed with the sparse
model, even though it had a much higher number
of hyperparameters to optimise, and was therefore
more prone to overfitting. We plan to investigate
this issue further but a possible cause could be bad
starting values for the hyperparameters.
Table 6 shows results for Task 1.3. In this task,
the standard GP model outperformed the base-
line, with the sparse GP model following very
closely. These figures represent significant im-
provements compared to our submission to the
same task in last year?s shared task (Beck et al.,
2013), where we were not able to beat the baseline.
The main differences between last year?s and this
year?s models are the use of additional datasets
and a higher number of features (25 vs. 40). The
competitive results for the sparse GP models are
very promising because they show we can com-
bine multiple datasets to improve post-editing time
prediction while employing a sparse model to cope
with speed issues.
310
en-es es-en en-de de-en
? ? ? ? ? ? ? ?
Standard GP 0.21 -0.33 0.11 -0.15 0.26 -0.36 0.24 -0.27
Sparse GP 0.17 0.27 0.12 -0.17 0.23 -0.33 0.14 -0.17
Baseline 0.14 -0.22 0.12 -0.21 0.23 -0.34 0.21 -0.25
en-es es-en en-de de-en
MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Standard GP 0.49 0.63 0.62 0.77 0.63 0.74 0.65 0.77
Sparse GP 0.54 0.69 0.54 0.69 0.64 0.75 0.66 0.79
Baseline 0.52 0.66 0.57 0.68 0.64 0.76 0.65 0.78
Table 4: Official results for task 1.1. The top table shows results for the ranking subtask (?: DeltaAvg;
?: Spearman?s correlation). The bottom table shows results for the scoring subtask.
Ranking Scoring
? ? MAE RMSE
Standard GP 0.72 0.09 18.15 23.41
Sparse GP 7.69 0.43 15.04 18.38
Baseline 5.08 0.31 15.23 19.48
Table 5: Official results for task 1.2.
Ranking Scoring
? ? MAE RMSE
Standard GP 16.08 0.64 17.13 27.33
Sparse GP 16.33 0.63 17.42 27.35
Baseline 14.71 0.57 21.49 34.28
Table 6: Official results for task 1.3.
5 Conclusions
We proposed a new setting for training QE mod-
els based on Multi-task Gaussian Processes. Our
settings combined different datasets in a sensible
way, by considering each dataset as a different
task and learning task covariances. We also pro-
posed to speed-up training and prediction times
by employing sparse GPs, which becomes crucial
in multi-task settings. The results obtained are
specially promising in the post-editing time task,
where we obtained the same results as with stan-
dard GPs and improved over our models from the
last evaluation campaign.
In the future, we plan to employ our multi-task
models in large-scale settings, like datasets an-
notated through crowdsourcing platforms. These
datasets are usually labelled by dozens of annota-
tors and multi-task GPs have proved an interest-
ing framework for learning the annotation noise
(Cohn and Specia, 2013). However, multiple tasks
can easily make training and prediction times pro-
hibitive, and thus another direction if work is to
use recent advances in sparse GPs, like the one
proposed by Hensman et al. (2013). We believe
that the combination of these approaches could
further improve the state-of-the-art performance in
these tasks.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from European Union?s Seventh Framework
Programme for research, technological develop-
ment and demonstration under grant agreement
no. 296347 (QTLaunchPad).
References
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Ondej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of WMT13, pages 1?44.
311
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Wei Chu, Zoubin Ghahramani, Francesco Falciani, and
David L Wild. 2005. Biomarker discovery in mi-
croarray gene expression data with Gaussian pro-
cesses. Bioinformatics, 21(16):3385?93, August.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
James Hensman, Nicol`o Fusi, and Neil D. Lawrence.
2013. Gaussian Processes for Big Data. In Pro-
ceedings of UAI.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse Gaussian Processes using Pseudo-inputs. In
Proceedings of NIPS.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jos?e G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
312
