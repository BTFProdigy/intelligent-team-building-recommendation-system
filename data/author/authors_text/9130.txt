More  accurate  tes ts  Ibr the  s ta t i s t i ca l  s ign i f i cance  of resu l t  
d i f ferences  * 
Alexander  Yeh 
Mitre Corp.  
202 Burli l lgl;on Rd.  
Bedford,  MA 01730 
USA 
asy~mit rc .o rg  
Abstract 
Statisti(:a,1 signiticance testing of (litl'erelmeS in 
v;~hl(`-s of metri(:s like recall, i)rccision and bat- 
au(:(~(l F-s(:()rc is a ne(:(`-ssary t)art of eml)irical 
ual;ural language 1)ro(:essing. Unfortunately, we 
lind in a set of (;Xl)erinlc\]d;s (;hal; many (:ore- 
inertly used tesl;s ofte, n underest imate t.he s ignif  
icancc an(l so are less likely to detect differences 
that exist 1)el;ween ditl'ercnt techniques. This 
undel'esi;imation comes from an in(let)endcn('(~ 
a,-;SUlnl)tion that is often violated. \~fe l)oint out 
some useful l;e,%s (;hal; (lo nol; make this assuml)- 
lion, including computationally--intcnsive ran- 
d()mizat,ion 1;cs|;s. 
1 I n t rodu( - t ion  
In Clnl)irical natural  \]al~gUag(~ l)rocessing, on(', 
is ot'tcal |:('~st;ing whether some new technique 
1)ro(lu('es im\])rove(l l'esull;s (as mcasur(xl \])y one 
()1' 111017(` - IIICI;I'\]CS) Oit son Ic  i;esl; (lai;~L set; \V\]l(`-ll 
(;Olll\])aI'e(l l i ( )sol l le (; l lrrel lt  ( l )ascl i lm) l;c(:\]lnique. 
\5/\]lell ,\]le lCsllll;s are better  with the new tcch- 
ni(lUe , a question arises as t() wh(',l;h(;r these l:(`-- 
sult; (litl'eren(:es are due t() the new technique 
a(:t;ually 1)eing l)cl;t('x or just; due 1;o (:han(:e. Un- 
t'ortmmtely, one usually Callll()t) directly answer 
the qnesl;ion "what is the 1)robatfility that 1;11(; 
now l;(x:hni(luC, is t)el;lx~r givell l;he results on the 
t(',sl, dal;a sol;": 
I)(new technique is better \ [ test  set results) 
\]~ul; with statistics, one cml answer the follow- 
ing proxy question: if the new technique was a(> 
tually no ditt'erent han the old t(',('hnique ((;he 
* This paper reports on work l)erfonncd at the MITR1,; 
Corporation under the SUl)porl: of the MITIlJ,; ,qponsored 
Research l)rogrmn. Warren Grcit\[, l ,ynette Il irschlnm b
Christilm l)orall, John llen(lerson, Kelmeth Church, Ted 
l)unning, Wessel Kraaij, Milch Marcus and an anony- 
mous reviewer l)rovided hell)rid suggestions. Copyright 
@2000 The MITRE Corl)oration. All rights r(~s(n'vcd. 
null hyl)othesis), wh~tt is 1:11(; 1)robat)ility that 
the results on the test set would l)e at least this 
skewed in the new technique's favor (Box eta\] . ,  
1978, So(:. 2.3)? Thai; is, what is 
P(test  se, t results at least this skew('A 
in the new techni(lue's favor 
I new technique is no (liffercnt than the old) 
If the i)robtfl)ility is small enough (5% off;on is 
used as the threshold), then one will rqiect the 
mill hyi)otheMs and say that the differences in 
1;he results are :'sta.tisl;ically siglfilicant" aI; that 
thrt,shold level. 
This 1)al)(n" examines some of th(`- 1)ossil)le 
me?hods for trying to detect statistically signif'- 
leant difl'el'enc(`-s in three commonly used met- 
l'i(:s: tel'all, 1)re('ision and balanced F-score. 
Many of these met;Ire(Is arc foun(t to be i)rol)lem- 
a.ti(" ill a, so, t; of eXl)erinw, nts that are performed. 
Thes(~ methods have a, tendency to ullderesti- 
mat(`- th(', signili(:ance, of the results, which tends 
t() 1hake one, 1)elieve thai; some new techni(tuc is 
no 1)el;l;er l;lmn the (:urrent technique even when 
il; is. 
This mtderest imate comes fl'om these lnc|h- 
ells assuming l;hat; the te(:hlfi(tues being con> 
lmrcd produce indepen(lc, nt results when in our 
eXl)eriments , the techniques 1)eing COml)ared 
tend to 1)reduce l)ositively corr(`-lated results. 
To handle this problem, we, point out some 
st~ttistical tests, like the lnatche(t-pair t, sign 
and Wilcoxon tests (Harnett,  1982, See. 8.7 and 
15.5), which do not make this assulnption. One 
Call ITS(', l;llcse tes ts  Oll I;hc recall nlel;r ic, but  l;he 
precision an(l 1)alanced F-score metric have too 
COml)lex a tbrm for these tests. For such com- 
1)lex lne|;ri(;s~ we llSe a colnplll;e-in|;Clisiv(~ ran-  
domization test (Cohen, 1995, Sec. 5.3), which 
also ~tvoids this indet)en(lence assmnption. 
947 
The next section describes many of the stan- 
dard tests used and their problem of assuming 
certain forms of independence. The first subsec- 
rio11 describes tests where this assumption ap- 
pears in estimating the standard deviation of 
the difference between the techniques' results. 
The second subsection describes using contin- 
gency tables and the X 2 test. Following this is a 
section on methods that do not 1hake this inde- 
pendence assumption. Subsections in turn de- 
scribe some analytical tests, how they can apply 
to recall but not precision or the F-score, and 
how to use randomization tests to test preci- 
sion and F-score. We conclude with a discussion 
of dependencies within a test set's instances, a
topic that we have yet to deal with. 
2 Tests  that  assume independence  
between compared  resu l ts  
2.1 F ind ing  and using the variance of a 
result difference 
For each metric, after determining how well a 
new and current technique t)efforms on solne 
test set according to that metric, one takes the 
diflbrence between those results and asks "is 
that difference significant?" 
A way to test this is to expect 11o difference in 
the results (the null hypothesis) and to ask, as- 
suming this expectation, how mmsual are these 
results? One way to answer this question is to 
assulne that the diffb, rence has a normal or t dis- 
tribution (Box et al, 1978, Sec. 2.4). Then one 
calculates the following: 
(d - Z \ [4 ) / s  d = d/,~,~ (1) 
where d = x l -  x2 is the difference found be- 
tween xl and x2, the results for the new and 
current echniques, respectively. E\[d\] is the ex- 
pected difference (which is 0 under the null hy- 
pothesis) and Sd is an estimate of the standard 
deviation of d. Standard eviation is the square 
root of the variance, a measure of how much a 
random variable is expected to vary. The results 
of equation 1are compared to tables (c.f. in Box 
et al (1978, Appendix)) to find out what the 
chances are of equaling or exceeding the equa- 
tion 1 results if the null hypothesis were true. 
The larger the equation 1 results, the more un- 
usual it would be under the null hypothesis. 
A complication of using equation 1 is that 
one usually does not have Sd, but only st and 
s2, where Sl is the estimate for Xl'S standard 
deviation and similarly for s2. Ilow does one 
get the former fi'om the latter? It turns out 
that (Box et al, 1978, Ch. 3) 
o -2 o-12 + a~ d = --  2p12a10-2 
where cri is the true standard eviation (instead 
of the estimate si) and pl'2 is the correlation 
coefficient between xl and :c2. Analogously, it 
turns out that 
2 z S d 82 -t- 82 - -  2 r128182 (2) 
where r12 is an estimate for P12. So not only 
does cr d (and Sd) depend on the properties of 
xl and x2 in isolation, it also depends on how 
Xl and .~'2 interact, as measured by P12 (and 
'rr)). When Xl and x2 are independent, p12 = 
0, and then (Td = ~-+ c7~ and analogously, 
Sd = ~ + s~. When P~2 is positive, ;1; 1 and 
x2 are positively correlated: a rise in xl or x2 
tends to be accompanied by a rise in the other 
result. When P12 is negative, :cl and x2 are 
negatively correlated: a rise in :cl or x9 tends 
to be accompmfied by a decline in the other 
result. -1  < P12 < 1 (Larsen and Marx, 1986, 
Sec. 10.2). 
The assu lnpt ion  of' independence is often used 
in fornlnlas to determine the statistical signifi- 
cance of the difference d = .~:1 - x2. But how 
accurate is this assumption? One nfight expect 
sonic positive correlation from both results com- 
ing from the same test set;. One may also expect 
some positive correlation when either both tech- 
niques are just variations of each other 1 or both 
techniques are trained on the same set of train- 
ing data (and so are missing the same examples 
relative to the test set). 
This assumption was tested during some 
experiments for finding granunatical relations 
(subject, object, various types of nxodifiers, 
etc.). The metric used was the fraction of the 
relations of interest in the test set that were re- 
called (tbund) by some technique. The relations 
of interest were w~rious ubsets of the 748 rela- 
tion instances in that test set. An example sub- 
set is all the modifier elations. Another subset 
is just that of all the time modifier elations. 
1 These  var ia t ions  are often des igned to usual ly  behave  
i l l  the  stone way and  on ly  differ in jus t  a few cases. 
948 
First, two difl'erent e(:hniques, one ltlelllory- 
t)ased and the other tl'ansti)rlnation-rule based, 
wei"e trained on the same training set, and then 
both teste(1 on that ticst set;. l~.e(:all eonlt)a.risons 
we, re made tbr ten subsets of tim relations and 
the r12 was found for each cOral)arisen. From 
Box et al (1978, Ch. 3) 
" '12 = ~( I ,  J lk -- Y l ) (~2k  - -  ~2) / ( 'g l t~2(  71 - -  l . ) )  
k 
where Yil~ = \] if the ith technique recalls the 
tcl;h relation and = 0 if not. 'lz, is the nmnl)cr 
of relations in the subset. !\]i and si are mean 
and stmJ(lard de, vial, ion estimate.s (based on the 
Yik'S), rest)ectively, fl)r the ith technique. 
For the ten subsets, only Clio COlnl)arison had 
a 'r12 (::lose to 0 (It was -0.05). The other nine 
c()ml)arisons had 'r12's 1)etw(x',n 0.29 and 0.53. 
The ten coral)arisen inedian value was 0.38. 
Next;, the transformatiol>rulc t)ased t.cch- 
nique was rUll with difl'erent sets of start ing con- 
ditions and/or  different, but overlapl)ing , sub- 
sets of the training set. Recall comparisons were 
ma(le on the same test (lata. set 1)etween l;he d i f  
fcrent variations. Many of the comparisons were, 
of how well two wu:iations recalled a particular 
subset of the relations. A total of 40 compar- 
isons were made.. The 'r\]2's on all d0 were 1)osi- 
tire. 3 of the 'r,2's w('~re ill the 0.20-0.30 range. 
24 of the rj2's wore in the 0.50--0.79 range. 13 
of the 'r\]2's were in the 0.80-1.0() range. 
So in our ext)erin~ents, we were usually eom- 
t)aring 1)ositivcly correlated results. How much 
error is introdu(:e(t t)y assuming independence? 
An easy--to-analyze case is when the stan- 
dard devial,ions for the results being eoml)ared 
a:t'c the same. ?~ 'J}hen equation 2 reduces to 
s , , -  sV /2 ( l -  r12), where s = sl = ,s'2. If one, 
assumes the re.sults m'e indcpel:dent (~/SSllllle 
r,2 = 0), then sd :-~ .sv/22. Call this wflue sd-i,7,g. 
As flu increases in value, Sd decreases: 
\[().38 d 0.T87(sd_i,,d) 1.27 
\[p.ao I 1.41 
\[O.80J 0.447(Sd.__i.,,.d) 2.24 
'l'he rightmost cohunn above indicates the mag- 
nitude by which erroneously assuming indepen- 
>\[lifts is actually roughly true in the coml)arisons 
nmde, and is assumed to be true in many of the standard 
Wsts for statistical significance. 
(lence (using 8d_in d ill 1)lace of sd) will increase 
the standard eviation estimate. In equation 1, 
sd forms the denominator of the ratio d/.s d. So 
erroneously assmning independence will mean 
that  the mmmrator  d, the difference between the 
two results: will nee(t to increase by that same 
factor in order f()r equation 1 to have the same 
wtlue as without the indel)endence assmnt)tion. 
Since the value of that  equation indicates the 
statistical significance of d, assunfing indepen- 
dence will mean that  e1 will have to be larger 
than without the. assumption to achieve the 
same al)parent level of statistical significance. 
l?roln tile tal)le above, when r12 = 0.50, (1 will 
need to 1)c about 41% larger. Another way to 
look at this is that  assuming indei)en(lenee will 
make the same. v~due, of d appear less statist;i- 
cally signifiealtt. 
The common tests of statistical significance 
use this assumt)tion. The, tesl; klloWlt as the 
1, (Box et; al., 1978, Sec. 4.1) or two-saml)le t
(Harnett,  1982, See. 8.7) test does. This test 
uses equation 1 and then compares the resulting 
va.lue against he t; distr ibution tal)les. This test 
has a (:Oml)licated form for sd l)eeause: 
1. :c! and :c2 can t)e 1)ased on (tiffering num- 
1)ers of saml)les. Call these retail)ors 'n~ and 
'n2 r(;sl)ectivcly. 
2. 111l this t(;st, the z i ' s  are each an ni sam- 
pie average, of altother varial)le ((:all it yi). 
'\['his is important  because the si's in this 
test are standm'd deviation estimates tor 
the yi's, not the xi 's .  The relationship be- 
tween them is that  si for Jli is the same as 
( for :,:,:. 
3. The test itself assumes that !11 and Y2 have 
the same standard eviation (call this com- 
mon value s). The denominator estimates 
,s using a weighte(1 average of 81 and s2. 
The weighting is b~sed on nl and r7,2. 
From Harnett (1982, Scc. 8.7), the denominator 
Sd ~- 
nl  + n2 - 2 
711 -b r~,2 ) 
'i7,177,2 
When 'nl = 'n2 (call this common value 'n), '~1 
and s2 will be given equal weight, and Sd siml)li- 
fie.s to ~ + ,s'~)/n. Making the substitut ion 
described above of si v/57 tbr si leads to an Sd of 
949 
s 2 the fbrm had earlier for the -t-, 2, we us ing  
independence assumption. 
Another test that both makes this assulnt)- 
tion and uses a tbrm of equation 1 is a test tbr 
binonlial data (Harnett, 1982, Sec. 8.1.1) which 
uses the "t'aet" that binomial distributions tend 
to approximate normal distributions. In this 
test, the zi's being compared are the fraction 
of the items of interest that are recovered by 
the ith technique. In this test, the denomina- 
tor sd of equation 1also has a complicated fbrm, 
both due to the reasons mentioned for the t, test 
above and to the fact that with a binomial dis- 
tribution, the standard eviation is a flmction 
of the number of samples and the mean wflue. 
2.2 Using cont ingency tab les  and  X 2 to 
test  precis ion 
A test that does not use equation 1 but still 
makes an assunlption of independence l)etween 
a:l and a:u is that of using contingency tables 
with the chi-squared 0,52) distribution (Box et 
al., 1978, Sec. 5.7). When tile assmnption is 
valid, this test is good for comparing differences 
ill the pr'ecision metric. Precision is the fraction 
of the items "Ibund" 1)y some technique that 
are actually of interest. Precision = l~,/(I~, + S), 
where R is the number of items that are of inter- 
est and m'e Recalled (fbund) by tile technique, 
and S is the munber of items that are found by 
tile technique that turn out to be Spurious (not  
of interest). One can test whether the precision 
results from two techniques are different by us- 
ing a 2 x 2 contingency table to test whether the 
ratio R/S  is different for the two techniques. 
One makes tile latter test, by seeing if tile as- 
sumption that the ratios for the two techniques 
are the same (the null hypothesis) leads to a sta- 
tistically significant result when using a X 2 dis- 
tribution with one degree of freedom. A 2 x 2 ta- 
ble has 4 cells. The top 2 cells are filled with the 
R and S of one technique and the bottom 2 cells 
get the R and S of the other technique. In this 
test, the valuc in each cell is assumed to have a 
Poisson distribution. When the cell values are 
not too small, these Poisson distributions are 
approximately Normal (Gaussiml). As a result, 
when the cell values are independent, smnming 
tlle normalized squares of the difference between 
each cell and its expected value leads to a X 2 
distribution (Box el; al., 1978, Sec. 2.5-2.6). 
How well does this test work in our experi- 
ments? Precision is a non-linear time(ion of two 
random wu'iables R and S, so we did not try to 
estimate the correlation coefficient \]'or precision. 
However, we can easily estimate the correlation 
coefficients for the R's. They are the r12's found 
in section 2.1. As that section mentions, the 
r12's fbund are just about always positive. So 
at least in our experiments, the R's are not ill- 
dependent, but are positively correlated, which 
violates the assumptions of the test. 
An example of how this test behaves is the 
following comparison of the precision of two dif- 
ferent methods at finding the modifier elations 
using tile stone training and test set. The corm- 
lation coefficient estilnate tor R is 0.35 mid the 
data is 
Method 17, 5' t?recision 
1 47 48 4!)% 
2 25 14 64% 
Placing the l~, and S values into a 2 x 2 table 
leads to a X 2 value of 2.38. a At t degree of 
freedom, tile X 2 tables indicate that if the null 
hypothesis were true, there would 1)e a 10% to 
20% chance of producing a X 2 value at least this 
large. So according to this test, this nnlch of an 
observed difference in precision wouht not be 
unusual if no actual differ(,ncc in the precision 
exists between the two nw, thods. 
This test assumes independence b tween the 
/~, wdues. When we use a 22(I (=1048576) trial 
approximate rmldomization test (section 3.3), 
which makes no such assumptions, then we find 
that this latter test indicates that under the 
null hypothesis, there is less than a 4% chance 
of producing a difference in precision results as 
large as the one observed. So this latter test in- 
dicates that this nmch of an observed ifference 
in precision would be mmsual if no actual dif- 
ference ill the precision exists between the two 
methods. 
It should be mentioned that the manner of 
testing here is slightly different han the man- 
ner in the rest of this paper. The X 2 test looks 
at the square of the difference of two results, 
and rejects the mill hylmthesis (the compared 
techniques are the same) when this square is 
a\Ve do not use Yate's adjustment to compensate lbr 
the numbers in the table being integers. 1)oing so would 
lmve made the results even worse. 
950 
large, whel;he, r l;lm largeness is (:aused l)y t;he 
new t;eehni(lue t)l"o(lucing' a, much l)(fl;l;er result; 
titan l;he current, l;e(:hlfique or vice-versa. So 
1,o l)e fair, we eolnl)ared l;he X 2 resull;s with a 
l;wo-sided version of l;hc rmldon~iz~fl;ion t;esl,: es- 
l;inm|;e, l;he likelihood glu~l; l;he obsea'ved magni- 
l;u(le of t, he resull; (lifl'eren(:e would 1)c matched 
or exceeded (regardless of' which l;echnique pro- 
duced l;he betl;er resull;) raider the mill hyl)oth- 
esis. A one-sided version of the test;, which is 
colnt)aral)le t;o what we use in l;he rest of the t)a -
per, esl;inml;es l;he likelihood of a (tifferenl; oul;- 
come under t;he null hyt)oChesis: that of m~l:cll- 
ing or exceeding t;he (lit\['erence of how lllllch 
l)?,l;ter i;he new (possibly 1)ett, er) l;e(:lmi(lue's oh- 
s('a'ved result is than l;he currenl; l;e('hnique's o|)- 
serve,(1 l'esull;, ht t;he ahoy(; scenario, a one-sided 
t(;sl; t)rodu(:es ~ 3(~, tigure insl;ead of s~ d:% figure. 
3 Tests  w i thout  that  independence  
assumpt ion  
a.1 Tests  for  matched pa i rs  
At; l;his point, one may wonder it' all st;al;isl;ical 
t;CSt;S lllil\](e SllC\]l s/,\]l int lepealdenct~ asSllltl\])l;ioll. 
Th(', miswer is no, lml; t;\]l()se lesl:s l;hal; (to nol; 
lltC}~Slll;e, how ll l l lch {;we l;e(;\]lni(llles illl;(',ra(:l; (to 
need  i;o lmtke, some assmnpl ; ioH  al)oul; t;\]m|; ill- 
I;(;r~t('l;ion mid l;yl)it:a.ll E l;\]ml; assuml)l;ioll is in(te- 
t)(~\]ldell(;e. 'Fhose I;esl;s I;ll~H; Hol;i(;c in S()lll(~ \\r;~Br 
how much l;wo tc(:hniqucs hm;ra(:l; (;~1,11 lib(; ~h()se 
ol)servations insl;ead of relying on assumt)l:ions. 
One w',~y t;o measure how 1;we l;e(:lmi(lucs in- 
i;erac(; is 1;o comtm.re \]tow similarly (;he, t;wo t;ecl> 
ni(tues tea.el; 1;o various l)arl;s ()f 1;he l;(;s\[; seA;. 
'_l."his is done in the mal;t:hed-lm.ir 1, I;esl; (Hm'- 
nctl;, 1982, Se(:. 8.7). This l;csI; tin(ls the dith'a'- 
once bet;we, n how t;eclmiques 1 and 2 l)eribrm 
on e~t(::h l;esl; set, Saml)le. The/ ,  dist;ri|)ul;ion and 
a fOI'ln of eqm~l;ion l m:e used. The null }lyl)ol;h- 
esis is st;ill l;\]l~tl; ~he mtmeral;or d \]ms ~t 0 me,m, 
but el is now l;he stun of these difference values 
(divided 1)y t;he number of Smnl)les), instead of 
being :r~ - :re. Similm'ly, the (lenomimd;or .sd is 
now esl;inml;ing l;he si;a.ndm'd (leviation of l;hese 
difl'erenee wdues, instead of being a funcl;ion of 
s:l and su. '.Flfis means for example, (;hal; even if 
t;lm values fl'om l,eclmiques l and 2 vary on (lii- 
ti:rent; test; Smnl)les , Sd will now 1)(' 0 if on each 
tesI; smnl)le, l;echnique \]1)reduces a. value l;lmt is 
the  ssulle C()llS|;allI; tHI1OlllIi; lllOl'e t;han l;he va,\]ue 
fl'om t, echnique 2. 
Two ol;h(',r tests for eomlmring how (;we tech- 
ni(lueS 1)ert'()rm 1) 3, comtmring how well l;hey 
perform on each I;est Smnl)le arc the sign mid 
Wilcoxon tests (Harnel;t;, 1!)82, See. 15.5). Un- 
like, t;\]le nl~tl;ched-tmir t: t;esI;~ neither of t, hese l;wo 
I;CSI;5 slSSllllte t;ln~l; I hc sum of l;he (litl'crences has 
a normal (Gaussian) (listribul;ion. The i;wo tests 
are, so-calh~d nonl)a.rmut%ri(: l;esl;s, which (lo not; 
make assuml)l, ions a.1)out; how l, he rcsull;s axe dis- 
ln'il)ut, ed (thrnel,l,, 1982, Ch. 15). 
il'he sign |;est is I;he simplier of lJm I;wo. It uses 
a 1)inomial dist,rilm|;ion to examine the munber 
of l;esl; smni)les where t;e(:hlfi(lUe \] 1)crforms \])el;- 
l;er t;ha.n l;e(:hnique 2 ve, rsus l;he munl)er where 
1;he Ol)posite occurs. The null hyl)ol;hesis is l;h~d; 
1;he t;wo t;eclmiques 1)ert'orm equally well. 
Unlike the sign t;esl;, t;he Vfilcoxon |;esl; also 
uses inlbl'nlal;ion on how large a difference xisl;s 
1)el;ween t, hc l;wo l;echniques' r(,,sull;s on each of 
l;hc l;csl; smnpl(;s. 
3.2 Us ing  the  tes ts  for matched-pa i rs  
All three of l;hc ma.l,(:he(1-tmir t, sign and 
Wilcoxon t;csl;s can 1)e a.pl)lied t;o t;hc re, call met- 
ric, whicll is the fl'act;ion of |;he il;ems of inl;crcsl; 
in ~,he l:csl; sol; l;lml; a, I;e, ehniquc recalls (finds). 
Each il;em of inl,eresi; in |;he l;esl; (la~;a serves as 
a. l;cst sainlflU. \?e use t;he sign l;esl; b(',causc iI; 
11Htkcs fcwel" assumi)i;ions 1;hart i;he nml;chcd-l)air 
1: I;est and is simplier l;han the Wih'oxon I;esi;. 111 
addit;ion, the fro:i; glml; t~he sign l;e, st ignores l;he 
size of 1;he result; difl'erence on eacll l;esl; Smnl)le 
(tocs llOI; nml;ter here. \?iI:h I;he recall met;rio, 
each sa.mple of int;eresl; is either found or nol; by 
a. t;eehnique. There are no interlnedbtte values. 
While 1;he 1;hree l;esl;s described in sccl;ion 3.1 
can be used on the re(:~dl mctxic, 1;hey CallllO|; bc 
"" ' ' used on ell;lint t;hc precision or slamgh|fforwardly 
1)abmced F-score met;rics. This is because both 
precision and F-score ~tre more coml)licated non- 
linem' flmci;ions of rml(lom varial)lcs than recall. 
In fst(:t bol;h can be l;hought of as non-linem" 
flm(:l;ions involving recall. As described in Sec- 
tion 2.2, precision = 1~./(1~ + S), where I~ is i;he 
nmnl)er of iWms t;lmt; are of inl:eresl; that; are '/'c'- 
called by a W, chnique mid S is l;he mmfl)er of 
it;e, ms (fi)und 1)y s~ technique) that; are nol; of 
interest;. The 1)~dmmed F-score = 2ab/(a + b), 
where a is recall and b is precision. 
951 
3.3 Using randomizat ion fbr precision 
and F -score  
A class of technique that ean handke all ldnds of 
flmetions of random variables without the above 
problenls is the computationally-intellsive ran- 
domization tests (Noreen, 1989, Ch. 2) (Cohen, 
1995, Sec. 5.3). These tests have previously 
used on such flmctions during the "message un- 
derstanding" (MUC) evaluations (Chinchor et 
al., 1993). The randomization test we use is like 
a randomization version of the paired sample 
(matched-1)air) t test (Cohen, 1995, Sec. 5.3.2). 
This is a type of stratified shuffling (Noreen, 
198!), Sec. 2.7). When eomt)aring two tech- 
niques, we gather-u I) all the responses (whether 
actually of interest or not) produced by one 
of the two techniques when examining the test 
data, but not both techniques. Under the 111111 
hyl)othesis , the two techniques are not really 
different, so any resl)onse produced by one of 
the teehniques eonld have just as likely come 
fl'om the other. So we shuffle these responses, 
reassign each response to one of the two tech- 
niques (equally likely to either technique) and 
see how likely such a shuffle 1)roduces a differ- 
ence (new technique lninus old technique) in the 
metric(s) of interest @1 our ease, precision and 
l?-score) that is at least; as large as the difference 
observed when using the two techniques on the 
test data. 
'n responses to shuttle and assign 4 leads to 
2 ~' difl'erent w~\ys to shuffle and assign I;hose re- 
sponses. So when 'n. is small, one can try each 
of the different shuttles once and produce an 
exact randomization. V~;hen n gets large, the 
mmfl)er of different shutttes gets too large to be 
exhaustively evaluated. ~J?hen one performs a.u 
approximate randomization where each shuffle 
is perfornmd with randoln assignments. 
For us, when n < 20 (2'" .<_. 1048576), we use 
an exact randomization. For n > 20, we use an 
approximate randomization with 1048576 shuf  
ties. Because an approximate randomization 
uses random nmnbers, which both lead to oc~ 
casional unusual results and may involve using 
a not-so-good pseudo-random 1111111\])(;I" genera- 
tol "~, we perfbrm the following cheeks: 
4Note that responses produced by both or neither 
techniques do not need to be shulIled and ,~ssigned. 
5One examI)le is the RANDU routine on the IBM360 
(Forsythe t al., 1977, See. 10.1). 
? We run the 1048576 shuttles a seeond time 
and colnpare the two sets of results. 
? We also use tile same shutttes to calcu- 
late the statistical significance for the recall 
metric, and compare this significance value 
with the significance value found for recall 
analytically by the sign test. 
An example of using randomization is to com- 
pare two different methods on finding modifier 
relations ill the same test set,. The results on 
the test; set, are: 
Method ~ ~  Precision F-score t 
i _l_556t ' I 49.5% 47.5% 
Zl: 64.1% 35.2% 
Two questions being tested are whether the ap- 
parent ilnt)rovement in reca.ll and F-score f!rom 
using method I is significant. Also being tested 
is whether the apparent imt)rovenmnt; in pl'eci- 
sion fl'om using method Ii is significant. 
In this example, there are 10"1 relations that 
should be found (are of interest). Of these, 19 
are recalled by both methods, 28 are recalled 
by method I but not; II, and 6 are recalled by 
II but not I. The correlation coeificient estilnate 
between the methods' recalls is 0.35. In addi- 
tion, 5 stmrious (not of interest) relations arc 
found by both methods, with method I find- 
ing an additional 43 Sl)uriolls relationships (not 
found by method II) and me?hod II finding an 
additional 9 relationships. 
There are a total of 28+6+43+9=86 relations 
that are found (whether of interest oi' not) by 
one method, but not the other. This is too 
many to t)erfornl an exact randolnizgtion, so 
a 1048576 trial apt)roximate randomization is 
perfornmd. 
In 96 of these trials, method I's recall 
is greater than method iI's recall by at, 
least (45.6%-24.3%). Similarly, in 14794 
of the trials, the F-score difference is at 
least (47.5%-35.2%). In 25770 of the trials, 
method II's precision is greater than method I's 
precision by at least; (64.1%-49.5%). N:om 
(Noreen, 1989, Sec. aA.a), the significance level 
(probability under the null hypothesis) is at 
most (.,e + 1)/(,~t + 1), where ',,.: is the nul~lt/er 
of trials that meet the criterion alld 1t, t is the 
number of trials. So fbr recall, the significance 
level is at most (96+1)/(1048576+1) =0.00009. 
952 
Similarly, for F-score, the significance level is at 
most 0.()1 d: and for l)re(:ision, the level is at lllOSt 
0.025. A secon(l 1048576 trial t)ro(luces imilar 
results, as does a sign test on recall. 'l'hus, we 
see that all three dit\[ere.n(:es are statistically sig- 
lfiIica.nt. 
4 The  future.: hand l ing  in ter -smnple  
dependenc ies  
An assmnption made by all I;he methods men- 
tioned in this I)~tl)er is ttmt the nlenlbcrs of the 
Lest set are all independent of one anothex. Tlmt 
is, knowing how a method l)('rforms on one test 
sot sanlple should not give any information on 
how that method \ ] )e l ' for l l ls  on  other test set 
samples. This assulnl)tJon is not always true. 
Church and Mercer (1993) give some exaln- 
ples of dependence bctwe.en test set insl;ances 
ill na tura l  la.llguage. One tyt)e of  dci)endence 
is that of a lexeme's part of speech on the 
l)m'l;s of speech of  neighl)oring lexenm,~ (th(,ir 
section 2.1). Sinfilar is the concept of collo- 
ca, t;ion, where the prolml)ility of a lexeme's al> 
l)earance is influenced by the. lexemes ai)pea.rin: ~
i1~ nearby positions (their section 3). A type of 
(tet)en(lence that is less local is that often, a. con-- 
tent word's al)pe.arance in a piece of text gr(;atly 
increases the cha.n('es of th~tt s;ulle wor(1 ~q)l)ear - 
illg b~ter in that 1)iece of texl; (their se(:l;ion 2.;/). 
What ~tr('. the effects when SOllle d{:t)endency 
exists? The expected (average) value of' the in- 
stallC(~ results will stay the, same. However, the 
('lmnees of getting an llllllSllal resl l l t  (;a,lt c\]la.ll~re. 
As an eXmnl)le , take five flips of a Nit coin. 
When no dependen(:ies exist 1)etween the tlil)s , 
the clmnces of the extreme result tha.t all the 
flit)s l:md on :~ particular side is faMy small 
((1/2) 5 -- i\[/32). When the ttil)s are positively 
correlated, these chmices increase. When the 
first flip lands on that side, the chances of the 
other four tlil)s doing the same are now ea.ch 
greater tlmn 1/2. 
Since statistical significance testing involves 
finding the chances of getting an mmsmd 
(skcwe(1) result under some null hyt)othesis, one 
needs to determine those del)endencies in order 
to accurately determine those dmnces, l)eter- 
mining the etk's:t of these dependencies is some- 
thing that is yet to l)e done,. 
5 Conc lus ions  
In elnpirical natural language processing, one 
is often COml)aring differences in values of met- 
rics like recall, precision and balanced F-score. 
Many of the statistics tests commonly used to 
make such comparisons assume the indepen- 
dence between the results being compared. \?e 
ran ~ set of m~tural language processing exper- 
iments and tbund that this assuml)tion is often 
violated in .~uch a way as t,o understate the sta- 
l, istical significance of the difli;rences between 
the results. We point out some analyt;ica.1 statis- 
tics tests like lnatched-l)air t,, sign mid Wilcoxon 
tests, which do not midge this assmnption and 
show that they (;tl,ll \])e l lsed Oll a l l letr ic like 
recall, l?br more complicated 1nettles like pre- 
cision and balanced F-score, wc use a compute-- 
intensive randonfization test, which also avoids 
this assumption. A next topic to address is that 
of possible dependencies l)etween test set  sam-  
ples. 
Re ferences  
G. Box, W. Hunter, and J. Hmlter. 1978. 
,gta, iisl.ics for" <rpc.ri'm.ent, er.~. John Wiley and 
S()llS. 
N. Chinchor, L. Hirschman, and l). Lewis. \] 9!)3. 
\]~vahmtillg message understanding systems: 
an analysis of the, third message understand- 
ing conferc.nce (muc-.3). Co'll~,p'ltt(tt'io'llgl Li ~,- 
gui,stic.s, 1!)(3). 
K. Church mid 171.. Mercer. 1993. Introduction 
1;o the sl)ecial issue on computational linguis- 
tics using large corpora. Cornp'u, tational Lin- 
guistic.s, 1!)(1.):1 24. 
P. Cohen. 1!)95. Empirical Meth, ods for Ar'tifi- 
cial Intelligence.. MIT Press, MA, USA. 
G. Forsythe, M. M~dcolm, and C. Moler. 1977. 
Com, putc'r methods for ~nathcm, atical comp'u- 
l.ati~m,.s. Prentice-lI~dl~ N,J, USA. 
D. Harnett. 1982. Statistical Methods. 
Addison-XYesley Publishing Co., 3rd edi- 
tion. 
R. Larsen and M. Marx. 1986. An Introduc- 
tion to Ma, th, cmatical Statistics and Its Appli- 
cations. Prentice-Hall, N J, USA, 2nd edition. 
E. Noreen. 1989. Computer-intensive met;hods 
.\[br testing h, ypoth, cscs: an int, r'od'ttction. Jolm 
Wiley and Sons, Inc. 
953 
Compar ing  two  t ra inab le  grammat ica l  re la t ions  f inders  
Alexander Yeh 
Mitre Corp. 
202 Burlington Rd. 
Bedford, MA 01730 
USA 
asy~.mitre.org 
Abst rac t  
Grammatical relationships (Glls) form an im- 
portant level of natural language processing, 
but different sets of ORs are useflfl for different 
purposes. Theretbre, one may often only have 
time to obtain a small training corpus with the 
desired GI1. annotations. On su& a small train- 
ing corpus, we compare two systems. They use 
difl'erent learning tedmiques, but we find that 
this difference by itself only has a minor effect. 
A larger factor is that iLL English, a different GI/. 
length measure appears better suited for finding 
simple m:gument GI{s than ~br finding modifier 
GRs. We also find that partitioning the data 
ma W help memory-based learning. 
1 Introduction 
Grmnnmtical relationships (GRs), whidl in- 
clude arguments (e.g., subject and object) and 
modifiers, form an important level of natural 
language processing. Glls in the sentence 
Yesterday, my cat ate th, e food in the bowl. 
include ate having tile subject my cat, the ob- 
ject the food and the time modifier Ycstcr'- 
day, ~md t, hc .food having the location modifier 
in (the bowl). 
However, different sets of GRs are useful for 
dii%rent purposes. For exmnple, Ferro et al 
(1999) is interested in semantic interpretation, 
and needs to differentiate between time, lo- 
cation and other modifiers. The SPARKLE 
project (Carroll et al, 1997), on the other lmnd, 
* This paper reports on work performed at, the MITRE 
Corporation under the support of the MITRE Sponsored 
Research Program. Marc Vilain, Lynette Hirsehman 
and Warren Greiff have helped make this work happen. 
Christine l)oran and John Henderson provided helpflfl 
editing. Copyright @2000 The MITRE Corporation. All 
rights reserved. 
does not differentiate between these types of 
modifiers. As has been mentioned by John Car- 
roll (personal communication), this is fine for 
infbrmation retrieval. Also, having less differ- 
entiation of tile modifiers can make it, easier to 
find them (Ferro et al, 1999). 
Unless the desired set of GRs matches the set 
already annotated in some large training co l  
pus (e.g., the Buchholz el; al. (1999) GR finder 
used the GRs annotated in the Penn 3~'eelmnk 
(Marcus el; al., 1993)), one will have to either 
manually write rules to find tile GI{s or mmo- 
tate a training corpus tbr the desired set. Man- 
ually writing rules is expensive, as is annotating 
a large corpus. We have performed experiments 
on learning to find ORs with just a small an- 
notated training set. Our starting point is the 
work described in l?erro et al (1999), which 
used a faMy smM1 training set. 
This paper reports on a comparison between 
the transforination-based rror-driven learner 
described in Ferro et al (1999) and the 
lnemory-based learner for GRs described in 
Buchholz et M. (1.999) on finding GIls to verbs 1 
by retraining the memory-based learner with 
tile data used in Ferro et al (1999). We find 
that the transformation versus memory-based 
difference only seems to cause a small differ- 
ence in the results. Most of the result differ- 
ences seem to instead be caused by differences 
in tile representations and information used by 
tile learners. An example is that different GR 
length measures are used. In English, one mea- 
sure seems better fbr recovering simple argu- 
ment ORs, while another measure seems better 
ibr modifier GIl.s. We also find that partitioning 
the data sometimes helps melnory-based learn- 
1That is, ORs that  have a verb as the relation target. 
For example, in Cats eat., there is a "subject" relation 
that  has cat as the target and Cats as the source. 
1146 
ing. 
2 D i f fe rences  Between the  Two 
Systems 
Forro  o ta l .  (\].()00) al ld Buchholz et al (1999) 
both describe learning systems to find GRs. 
rl'he former (TI{) uses transformation-based 
error-driven learning (Brill and Resnik, 1994) 
aim the latter (MB) uses lnemory-bascd learn- 
ing (l)aelemans et al, 1999). 
In addition, there are other difl'erences. The 
TR  system includes several types of inibrma- 
tion not used in the MB system (some because 
memory-based systems have a harder time han- 
dling set-wdued attributes): possible syntactic 
(Comlex) and semantic (Wordnet) classes of a 
c\]11111k headword, 1;11(', stem(s) and named-entity 
category (e.g., person, h)cation), if any, of a 
c\]mnk headword, lcxemes in a clmnk besides the 
headword, pp-attachment estimate and cerl;ain 
verb chunk properties (e.g., passive, infinitive). 
Some lexemes (e.g., coordinating COlljllllC- 
tions an(1 lmnctuation) are usually outside of 
any clmnk. The T12, system will store these in 
an attr ibute of the nearest chunk to the left; and 
to the right of such a \]eXellle. r.l'lle MB sys- 
tem represents uch lexemes as if the, y arc Olle 
word chunks. Tim MB system cmmot use 1;11(; 
TI{ syste, m method of storage, l)ecaus('~ melnory- 
based systelns have difficulties with set-v~ducd 
al,l, ribtttes (value is 0 or \]now~ lexemes). 
' \[ ' l ie N,iB systel l t  (all(l llOt the '.\['1{ syStell l) 
a\]so exalnines the areal)or of commas an(t verb 
(:hunks crossed by a potential G12.. 
The si)acc of l)ossible GlTls searched 1)y the 
two systems is slightly different. The TI{, system 
searches fbr Gl~s of length three clmnks or less. 
The MB system set~r(-hes for GRs which cross 
at lllOSt either zero (target to the source's left) 
or one (to the right) verb (:lnulks. 
Also, slightly different are the chunks ex- 
mnined relative to a potential GR. Both sys- 
tems will examine the target and source chunks, 
plus the source's immediate neighboring chunks. 
The MB systeln also examines the source's e('~ 
end neighl)or to the left;. The Tll, system instead 
also exmnines the target's immediate height)ors 
and all the clmnks between the source and tar- 
get. 
The T12, system has more data partitioning 
than the MB system. With the TI{ syst:em, 
possible Gl{s that have a diit'erent source chunk 
tyl)e (e.g., noun versus verl)), o1" a different re- 
lationship type (c o. subiect versus ol)iect ) or \ ",%", , 
direction or length (in chunks) are alwws con- 
sidered separately and will be afl'ected by differ- 
eat rules. The MB system will note such differ- 
ences, but lil W decide to ignore some or all of 
them. 
3 Compar ing  the  Two Systems 
3.1 Exper iment  Set -Up  
One cannot directly coral)are the two systems 
from the descriptions given in Ferro et al 
(1999) and Buchholz et al (1999), as the re- 
suits in the descril)tions we, re based oll different 
(tatt~ sets and on different assumptions of what 
is known and what nee, ds to be fbund. 
Itere we test how well the systems 1)erform 
using the same snm\]l annotated training set, 
the a2.~).0 words of elementary school reading 
comprehension test bodies used in t, brro et al 
(1999). 2 We are mainly interested in compar- 
ing the parts of the system that takes in syn- 
tax (noun, verb, etc.) chunks (also known 
as groups) and tinds the G12.s between those 
chunks. So for the exl)eriment , we used the gen- 
eral 'I'iMBL sysi;eln (l)aelemans et al, 1999) to 
just reconst;ruct the part of the MB systcan that 
takes in (:hmlks an(t finds G12s. Th(', input to 
1)ot\]1 this reconstructed part and the T\]-{ systonl 
is data that has been manually alHlotate(t for 
syntax chunks and ORs, ahmg with automatic 
lexeme and sentence segmentation altd t)art-of - 
sl)eech tagging. In addition, the '\].'12. system 
has nlmltlal nallie(t-e\]ltity allllOt3.tioll~ all(1 alltO- 
matic estimations for verb properties and inel)o- 
sition and sul)ordilmte conjmlction attachments 
(l~k',rro el; al., 1999). Because the MB system 
was originally desigued to handle Gll.s attached 
to verbs (and not noun to 1101111 O12S, etc.), We 
17311 the reconstructed part to only find Glis to 
w;rbs, and ignored other types of GRs when 
eomt)aring the reconstructed part with the T12. 
system. The test set is the 1151 word test set 
used in Ferro et al (t999). Only G12s to verbs 
were examined, so the elt'eetive training set GR 
count fell ti'om 1963 to 1298 and test set C12. 
')Note that if wc had been trying to compare the two 
systems on a large mmotated training set, the, M\] ~ system 
would do better by default just lmcause the TR system 
wotlld take too long to l)roecss a large trailling set. 
1147 
(:ount from 748 to 500. 
3.2 In i t ia l  Resu l ts  
In looking at the test set results, it is useful to 
divide up the Gils into the following sub-tyl)es: 
1. Simple m'guments: ubject, object, indirect 
object, copula subject and object, expletive 
subject (e.g., "It" in "It mined today. "). 
2. Modifiers: time, location and other modi- 
fiers. 
3. Not so simple arguments: arguments that 
syntactically resemble modifiers. These are 
location objects, and also sut)jects, objects 
and indirect objects that are attached via 
a preposition. 
Neither system produces a spurious response 
for tyl)e 3 Gils, but neither sysl;em recalls many 
of the test keys either. The reconstructed MB 
system recalls 6 of the 27 test key instances 
(22%), the TR system recalls 7 (26%). A pos- 
sible ext)lanation tbr these low performances is 
the lack of training data. Only 58 (3%) of the 
training data GR instances are of this type. 
The type 2 GRs are another story. There are 
103 instances of such Glls in the tess set key. 
Tile results are 
Type 2 C4II.s 
System Recall Precision F-s(:ore 
MB 47 (46%) 4!)% 47% 
TR 25 (24%) 64% 35% 
Recall is the number (and percentage) of the 
keys that m'e recalled. Precision is the number 
of correctly recalled keys divided by the munber 
of ORs the system claims to exist. F-score is the 
harmonic mean of recall (r) and precision (1)) 
percentages. It equals 2pr / (p+r) .  Here, the dif- 
ferences in r, p and F-score are all statistically 
significant, a The MB system performs better as 
measured by tile F-score. But a trade-off is in- 
volved. The MB system has both a higher recall 
and a lower precision. 
Tile t)ulk (370 or 74%) of tile 500 Gil, key 
instmmes in tile test set are of type 1 and most 
3When comparing differences in this paper, the sta- 
tistical signiticance of the higher score I)eing better than 
the lower score is tested with a one-sided test. Differ- 
cnccs deemed statistically significant m'e significant at 
the 5% level, l)ifferences deemed non-statistically s ignif  
leant are not significant at the 10% level. 
of these are either subjects or objects. Witll 
type J GRs, the results are 
Type 1 GRs 
System Recall Precision l?-score 
MB 23I. (62%) 66% 64% 
Til. 284 (77%) 82% 79% 
With these GRs, the TR system I)erforms con- 
siderably better both in terms of recall and pre- 
cision. The ditferences in all three scores are 
statistically significant. 
Because 74% of the GI-/. test key instances are 
of tyt)e 1, where the TR system performs better, 
this system peribrlns better when looldng at the 
results for all the test Gl{s coml)ined. Again, 
all three score diffferenees are statistically sig- 
nificant: 
Combined Results 
System Recall l?recision F-score 
MB 284 (57%) 63% 60% 
Til, 31(  (G3%) so% 
Later, we tried some extensions of the re- 
constructed MB systeln to try t;o lint)rove its 
overall result. We eould improve the overall re- 
sult by a combination of using the I\]71 search 
algorithm (instead of IG27~EE) in TiMBL, re- 
stricting the t)otential Gils to those that crossed 
no verb chunks, adding estimates on prepo,si- 
tion and complement attachments (as was done 
in TR) and adding infbrnlat, ion on verb chunks 
about 1)eing passive., an infinitive or an uncon- 
jugated present 1)articit)le. The overall F-score 
rose to 65% (63% recall, 67% precision). This is 
an improvement, but the Til. system is still bet- 
ter. The differences between these scores and 
the other MB and Til, confl)ined scores are sta- 
tistically significant. 
3.3 Explor ing the Result  Differences 
3.3.1 Type 2 GRs: modi f iers  
The reconstructed MB system performs better 
at type 2 Gil,s. How can we account tbr this 
result difl'erence? 
Letting the TR system find longer GRs (be- 
yond 3 chunks in length) does not hell) nmch. 
It only finds one more type 2 Oil, in the test set 
(adds 1% to recall and 1% or less to precision). 
Rerumfing the TR system rule learning with 
an information organization closer to the MB 
system produces the stone 47% F-score as the 
1148 
M\]} sysl;(;nl (rc.ca.ll is \]<)w(~r, Iml; 1)rc(:isi(n, is 
high(;r). S1)c<:ifi(:a.lly, we ~,;()I: 1;his rc.sull; when 
I;\]IC ~I'\]{, sys{;CII I  WaS l 'Cr l l l l  wil;h 1lo informal; ion 
<)n l)p--al;l;a.(:hnl('nl;s, v(;rl) chunl( 1)r<)l>(,rl;i(;,~ (e.g.; 
l)aS,~dv('., ilflinil;ivc), nam(;d-cm;il;y lal)cls <)r hc.a.d- 
r 1 "1 wor(t sLems. Also, l;\]m .I l~. ,%ml;cm How ( ;xmn- 
in(;,q (;h<; <:hmtks cxamin(;(l 1)y l;h<' <)rJj,,ina\] M\ ] I  
s2/sl;('an: garg('.|;, '~ourcc and ,~our(:(;'.~ n('Jj,ihl)<)rs. 
Ill a(l<lil;ion, insl;c, ad of 6 al)s<>lul;e h;ngl,h ca.l;('- 
gories (l;arg;cL i,~; 3 <:\]ranks 1;() l;hc lcf|;, 2 (:\[mnks, 
\] c lmnk, and similar ly for I;11('. righl;), 1;he (\]l{.s 
c, m,~i<l('xc, l now jusl; fall Jill;t);/.11<t a.re \])m'i;it;i(m<'xt 
inL<) 3 re\]at;iv(; cat;(;gorics: i;argcl; is l;h(! tirsl; verb 
chunk 1;o Lhc lolL, ,~dmila.rly /;<) Lh<'. righl; and l;a.r- 
gel; is i;\]m :-~c.(:(m<\[ verb (:hun\]~ 1;<) l:\]~c rig\]~I;. ~l'h('. 
MI\]{ SNS(i('.III (;;~11 (\[isgin?;ui~',h l)('kwe('~n (;\]w+;e sam(; 
r(',\]al;iv(; <:a,1;('gori(',~. 
I h'.<loing l;hi,s 'l.'l{. sy,%<;~ n r<;i'm~ 'wil./m'l+,l, <:hi u ~l~ 
h('.a.(lwor<l ~;ynLa(;l;i<: (>r ,~('.mmg;\](: las~;(',s \])r<>- 
<lu<:cs z~ d(i(/> l:-s(:or<'~. I f  in a<hlii;i(/n, (,h(,: l)l)- 
al;La.(:linmnl;, v(,.rl) <:\]ulnk 1)rol)('.rl;y, nam<;<l--cHl;il;y 
tal)el and \ ]madword ,ql;<'.n~ intbrnml;i<)n are ad<l(:<l 
I)a(;l( in, i;t~('~ i:-s(:or(', aclamlly (h'ol),~ 1;()/J3<~). ' \[.'\]1(; 
<lifl'(;r<',nc<'.,", t)(;1;w<:cn l;hc,~(; d'\[(/), d(i(/> mM d3</) rc.- 
r~m ,~c<)r(;,~ :u:('~ H()I; ,%a,i;isl;i(:alls~ :;ig~fiti(',anl;. 
H<) wit;h I;yt)<; 2 (~1C4, MII  ~y~;l;('.m',~; l)c(;lcr \])('.r- 
\['()rman(:(~ >c('.n~s 1;<) 1)(; l~tainly (lu(: il~'; a l)ilil;y \[;() 
~litl'er(,.nl;ia.I;e lh<; l)<>(;(,nl;ial CI:I,~ I)3~ 1,11('. \['ca.l;ur<~ <)t: 
l;\]t(; ~mml)<;r <>f v<!rt) (:\]mld;,~; <:r(>s>(;<l 1)3: a C,I{. In 
l)a.rl;:i<:uhu', makin~,~ l;hi,~ :/.l~<l a few <>l;hcr <:hm~,;cs 
I;() I;Im ' l ' l{  ,%',~l;<;nl in(:r(,.m:;cs il;:; i:.,~;<:()r<~ l<) \[;11(; 
MB sysl;(;m's \]i'-~4<x>re, a\]l<t 1;\]1('. <)Lh<u' <:}m,ug<;s (r('- 
mov iug  (:cri;ain iniorm al;i<)n) <h)(>~ n()l lmv(' a, ,,~i,"-~ 
nifica,ni; ('Ke.cl;. So u~dng l;h(; rip,hi: lb.a,l;~u('+~ can 
(ll~\[cr(m(:('. make  a, la,rgo ' ' 
For I;hes<~ I;yl)e 2 (~'\]L~ (luoditicr~,) in En- 
glish, il, <lots s('~(un Llm|; l;hc. nmnb(u" ()\[ verb 
cJmnks (:ro,~,s(;<t is a, l)('l;l;(;r was~ i;o {~rou 1) l)OS -- 
si\])\]c, m()<lifi(;r,q Lhan I;11<: al)solul;(~ clmn\]( l<'~ngl;h. 
An cXaml)\](: is comt>aring \] ./ly o'a ~l~ac.sday. mM 
l ./1!/k,o'm,c .fl'o'm, k,(",'c o':~, \[l~u,('.,sd(~?/. in 1)oi;h s(',n- 
l;(;ll(:(',s, on T.,<'sday is a, l;imc m(>(liticr <)f J ly and 
on (:ro,~;s('~s no vca'l),~; 1:<) reach .fly (on al;l;ach('~s 
l;<) l;hc tirsl; verb 1;() ii;s left;). \]htl; in l;h(; fir,~;I; 
s(;nt(;ncc o'~, is ncxl; to  Jly, while in l;h(; s(~(:<)n<l 
SCllt;(~llC(~ 1;lmr(~ arc  l;\]ll"(x ~, chllil\]r~,q ,~c\]);l,r;i.{,ill~ o71~ 
aud .fly. 
a .a .~ Type  1 (-11.{.s: s imple  arguments  
\]~(/r Ly\])e \] ( l \ ]{s ,  l;lm T\]{,  sy,%cm t)( ;r forms 1)('%l;(u'. 
How can w<' a,(:(:<)unl; for t;his'? 
Much <If 1;h(; cxt, ra inf()rnml;ion 1;11(; '.\['\] {.,~;ysl;('.m 
cxamiHc,~ (comt)ar(;d to 1;he MB syslx;m) do(;s 
noI; seem 1;o have much of an ctk, cI;. When th(- 
'.1.'\]-{, sysl;Cll l  Wa,q rerun with no in\['orma.l;ion on 
\]ma(lwol'd synl;acLic or semanLic classes, nmnt'~<l- 
cnl; i ty labels or h(;adword sLems, 1;h(', \].:-scc>r('~ in-- 
c reased  t}om 79% l;o 80%. Anol;lmr r(:;rllll t;lli~|; in 
addiLion had no in format ion (m t)\])-~d,l;a(:hm(ufl; 
c, sl;ima,l;<;.s or any of 1;he non-hca(lwor<t,s in tim 
(:lmnks also ha<l an F-score of 80%. A 1;hir<l 
lC r l l l l  t,hal; fllrl;h(;rlllor(; had no i l~t in :nmLio l l  Oll  
verb ('.hunk 1)ro\])crl;ics (c..g., lmssivc., infinil;ivc) 
had an F-score of 78%. In Lifts set of F-,~t:ore.s, 
only  l;he diffc.rences bc.Lwcen the 8()~) scor('.s and 
I;hc 78(X) s(:orc arc sl;at;isti(:ally signiiicaJfl;. 
Some M\]/  sysl;cni rcrun,q sl iowcd t)~('l;ors thai; 
S(;CIl ICd l;o ltta, l;l;(;r 11101"(;. h i  {;\]lC \[\]1"s|; 17(~1'Ull~ w(; 
l)arl;il;ion(;(l t;h(; (la.La \])y t)ot;(;n/;ial (11{ ,~om'(:<~ 
<:hlud~ i;yt)c (c..g, l loun versus vcrl)) and ra.ll a 
SCl)aral;('. m<mlory-l)a.s(~(l l;railfiUt~ and l;('.~;i; \[or 
ca(:h t)arl;it;ion. '.\['hc. Colnl)ill(~d \]:-s(:<)rc in<:rcav;('.d 
fl'om 64% t;o (i9(fi~. At'Lcrwmds, wc made a r('- 
>m l;haL res<;nfl)l(,d I;h<; TI{. sysl;cm run wil;h l:hc, 
78(/0 \]'Ls(:t>rc, (ex<:('=\])l; l;\]mi; m(unory-l)asc(\] l<'m'n- 
ing was use<l): only C,I./s <)f lcngt~h 3 <:}mnk,~; <n" 
less were (:<)nsid(;rc<l, ;h<; data  was l>arl;il;i<m<;<l 
( in :~<\](lil;ion I;<> s()urcc, <:\]mnk I;y\])c) l)y (111 lengl;\]l 
au<\[ (lir('.<:Li<>n ((, ,,' I;arg(;l; is l;w<> ('.\]mnks I;() l;h<~ 
l(;fL) an(l also t)y r<'\]al;i(>n l;y l)(; (:~c 1)~,ral;c rm Is ti)l 
<;a(:h 1,yl)('.), l;hc c<)111111~/. ~/.\]l(| V(}l'}) <:hunk (:ro,q:;- 
ing (:()unt;s wcr(; n<)l: (:on~d<h'.rc<t, aal(t l,h(; <:lmuk> 
normal ly  cxa.min(~<l 1)y l;h(; '11{ ~ysl;<'m were cx-. 
mni,l(,.(t. 'l'\]fis fm'Lll('~r iJmrcmce<t Lh(; \]:- >;(:<>r('. lx) 
75%. In l;his s<;l; o f  F-scor('~s, all l;h('~ ditl'ercncc> 
are sLa.l;isl;i(:ally ,~ignilica.nl; and al\] I;\]m l -->;(:orc.s 
in Lifts seA; arc. slal;isl;ica.lly signifi(:a.nt;ly diffcrcnl; 
J'l'Olll (;lit', 'i'll. SySi;Cl l \[  l ' l l l lS  wil;h l;h<" 78</) and 7!)</> 
\])'-s(:orcs. 
From l;he sLal;ist;ica.lly ,~;ignitit:ant; scow. <liif<'a- 
(;n(x's, iL s(;<:ms l;\]lal; t)arl;il;ionin/ (\[al;a 1)y l)()- 
l;ent;ia\] CI/. s(>m'(:e <:hunk l;yp<', hell)s (in(:rc.a.q<'~ 
from (id:(/> 1;o (i9%), as does Lh<'. resL <>f l;hc. 
imrl;ii;ioning l)(:rf<>rm(;d mM rim.king some slighl; 
c lmngcs in what  is (',xamincd ( increase 1;o 75%), 
using |;rmlsformaLion-lmscd learning insl;ca(t of  
mcln<)ry-bas(',t learning ( increase to 78(/~) an(l 
using v<',rl) chunk l)rot)(;rty informat, ion (in('r<',ase 
|:o 80%). 
In the. original MB sysl,cm run, Lhc somce 
c lmnk Lyl)('= mid  l;h('= 1)oi;('ag;ial (:,11 \]CltgLh 
an(t (lir('=(:l;Jon wcxe a\ ] rca( ly  deLcrminc( l  l)y I;hc 
m(',m(/ry-bas('d \]carn(;r l;o l)e l;h(', mosL iml)orl;mfl; 
1149 
attributes exanlined. So why would partition- 
ing the data and runs by the values of these 
attributes be of extra help? A possible answer 
is that for different values, the relative order 
of importance of the other attributes (as deter- 
nfined by the menlory-based learner) changes. 
For example, when the som'ce chunk type is a 
noun, the second most inlportant attribute is 
the source dlunk's headword when the target 
is one to the right, but is the source chunk's 
right neighbor's headword when tile target is 
one to the left;. Partitioning the data and runs 
lets these different relative orders be used. Hav- 
ing one combined ata set and rUlL inealLS that 
only one relative order is used. Note that while 
this partitioning may 11ot; be the standard way 
of using memory-based learning, it; is consistent 
with the central idea in memory-based learning 
of storing all the training instances and trying 
to find tile "lmarest" training instance to a test 
case .  
Another question is why using 
transtbrlnatiol>based (rule) learning seems 
to be slightly better than nmmory-1)ased 
learning for these type 1 GFls. Memory-based 
learning keeps all of the training instances and 
does not try to find generalizations such as 
rules (Daelenlans el; al., 1999, Ch. 4). However, 
with type 1 Gl~s, a few simt)le generalizations 
can account for many of the, instances. In the 
nlanner of Stevenson (1998), we wrote a set 
of six simple rules that when run on the test 
set type 1 ORs produces an F-score of 77%. 
This is better than what our reconstructed MB 
system originally achieved and is (:lose to the 
TII. system's original results (close enough not 
to be statistically significantly different). An 
example of these six rules: IF (1) the center 
drank is a verb chunk and (2) is not considered 
as possibly passive and (3) its headword is not 
some fbrm of to be and (4) the right neighbor 
is a noun or ve, rb chunk, THEN consider that 
chunk to the right as 1)eing an object of the 
center ehuuk. 
4 Discuss ion 
ORs are important, but different sets of GR,s 
are useflfl for different imrposes. We have 
been looking at ways of ilni)roving automatic 
Oil, finders when one has only a small amount 
of data with tile desired Oil, mmotations. In 
this paper, we compared a transformation rule- 
based systeln with a menlory-based system oi1 
a small training corpus. We found that oll GIls 
that point to verbs, most of the result ditfer- 
ences can be accounted fbr by ditferences in the 
representations and information used. The type 
of GR determines which information is more im- 
portant. The rule versus memorpbased differ- 
ence itself only seeins to produce a small result 
difference. We also find that partitioning the 
data mw hell) melnory-based learning. 
Re ferences  
E. Brill and P. Resnik. 1994. A rule-based 
approach to prepositional phrase attachlnent 
disambiguation. In lath, lVn, te:r'national Cot@ 
on Computational Linguistics (COLING). 
S. Buchholz, J. Veenstra, and W. Daelelnans. 
1999. Cascaded grmmnatical relation assign- 
ment. In Joi'n,t SIGDAT' Cor@renee on Em- 
pir'ical Methods in NLP and Very Larqe Col  
pora (EMNLP/VLC'99). cs.CL/9906004. 
J. Carroll, T. Briscoe, N. Calzolari, S. Federiei, 
S. Montemagni, V. Pirrelli, G. Grefenstette, 
A. Sanfilippo, G. Carroll, and M. F\[ooth. 
19!)7. Sparkle work package 1, specification 
of phrasal imi'sing, final report. Available 
at; http://www, ilc.pi, cnr . i t / spark le / -  
sparkle, htm, Now;tuber. 
\?. Daelemans, J. Zavrel, K. van der Sloot, and 
A. van den Bosch. 1999. Tilnbl: Tilburg 
memory based learner, version 2.0, refer- 
ence guide. ILK Technical l leport ILK 99-01. 
Available fl'om http : / / i l k .  kub. n l /~ i lk / -  
papers/i lk9901, ps. gz. 
L. Ferro, M. Vilain, and A. Yeh. 1999. Lem'n- 
ing transtbrmation rules to find gralnmatieat 
relations. In Computa, tional natural la'n, guage 
lear'nin9 (CONL?-99), pages 43 52. EACL'99 
workshop, cs. CL/9906015. 
M. Marcus, B. Santorini, and M. Mareinkiewiez. 
1993. Building a large mmotated corpus of 
english: tile penn treebank. ComI)utational 
Linguistics, 19(2). 
M. Stevenson. 1998. Extracting syntactic 
relations using heuristics. In I. Krui j ff  
KorbayovS, editor, Prec. of the 3'rd ESSLLI 
Student Scssion. Chapter 19. 
1150 
Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
Using existing systems to supplement small amounts of
annotated grammatical relations training data
?
Alexander Yeh
Mitre Corp.
202 Burlington Rd.
Bedford, MA 01730
USA
asy@mitre.org
Abstract
Grammatical relationships (GRs)
form an important level of natu-
ral language processing, but dier-
ent sets of GRs are useful for dier-
ent purposes. Therefore, one may of-
ten only have time to obtain a small
training corpus with the desired GR
annotations. To boost the perfor-
mance from using such a small train-
ing corpus on a transformation rule
learner, we use existing systems that
nd related types of annotations.
1 Introduction
Grammatical relationships (GRs), which in-
clude arguments (e.g., subject and object) and
modiers, form an important level of natural
language processing. Examples of GRs in the
sentence
Today, my dog pushed the ball on the oor.
are pushed having the subject my dog, the
object the ball and the time modier To-
day, and the ball having the location modier
on (the oor). The resulting annotation is
my dog ?subj? pushed
on ?mod-loc? the ball
?
This paper reports on work performed at the
MITRE Corporation under the support of the MITRE
Sponsored Research Program. Marc Vilain provided
the motivation to nd GRs. Warren Grei suggested
using randomization-type techniques to determine sta-
tistical signicance. Sabine Buchholz and John Car-
roll ran their GR nding systems over our data for the
experiments. Jun Wu provided some helpful explana-
tions. Christine Doran and John Henderson provided
helpful editing. Three anonymous reviewers provided
helpful suggestions.
etc. GRs are the objects of study in rela-
tional grammar (Perlmutter, 1983). In the
SPARKLE project (Carroll et al, 1997), GRs
form the top layer of a three layer syntax
scheme. Many systems (e.g., the KERNEL
system (Palmer et al, 1993)) use GRs as an
intermediate form when determining the se-
mantics of syntactically parsed text. GRs are
often stored in structures similar to the F-
structures of lexical-functional grammar (Ka-
plan, 1994).
A complication is that dierent sets of GRs
are useful for dierent purposes. For exam-
ple, Ferro et al (1999) is interested in seman-
tic interpretation, and needs to dierentiate
between time, location and other modiers.
The SPARKLE project (Carroll et al, 1997),
on the other hand, does not dierentiate be-
tween these types of modiers. As has been
mentioned by John Carroll (personal commu-
nication), combining modier types together
is ne for information retrieval. Also, having
less dierentiation of the modiers can make
it easier to nd them (Ferro et al, 1999).
Furthermore, unless the desired set of GRs
matches the set aleady annotated in some
large training corpus,
1
one will have to either
manually write rules to nd the GRs, as done
in A?t-Mokhtar and Chanod (1997), or anno-
tate a new training corpus for the desired set.
Manually writing rules is expensive, as is an-
notating a large corpus.
Often, one may only have the resources to
produce a small annotated training set, and
many of the less common features of the set's
1
One example is a memory-based GR nder (Buch-
holz et al, 1999) that uses the GRs annotated in the
Penn Treebank (Marcus et al, 1993).
domain may not appear at all in that set.
In contrast are existing systems that perform
well (probably due to a large annotated train-
ing set or a set of carefully hand-crafted rules)
on related (but dierent) annotation stan-
dards. Such systems will cover many more
domain features, but because the annotation
standards are slightly dierent, some of those
features will be annotated in a dierent way
than in the small training and test set.
A way to try to combine the dierent advan-
tages of these small training data sets and ex-
isting systems which produce related annota-
tions is to use a sequence of two systems. We
rst use an existing annotation system which
can handle many of the less common features,
i.e., those which do not appear in the small
training set. We then train a second system
with that same small training set to take the
output of the rst system and correct for the
dierences in annotations. This approach was
used by Palmer (1997) for word segmentation.
Hwa (1999) describes a somewhat similar ap-
proach for nding parse brackets which com-
bines a fully annotated related training data
set and a large but incompletely annotated -
nal training data set. Both these works deal
with just one (word boundary) or two (start
and end parse bracket) annotation label types
and the same label types are used in both the
existing annotation system/training set and
the nal (small) training set. In compari-
son, our work handles many annotation la-
bel types, and the translation from the types
used in the existing annotation system to the
types in the small training set tends to be both
more complicated and most easily determined
by empirical means. Also, the type of baseline
score being improved upon is dierent. Our
work adds an existing system to improve the
rules learned, while Palmer (1997) adds rules
to improve an existing system's performance.
We use this related system/small training
set combination to improve the performance
of the transformation-based error-driven
learner described in Ferro et al (1999). So
far, this learner has started with a blank
initial labeling of the GRs. This paper
describes experiments where we replace this
blank initial labeling with the output from
an existing GR nder that is good at a
somewhat dierent set of GR annotations.
With each of the two existing GR nders that
we use, we obtained improved results, with
the improvement being more noticeable when
the training set is smaller.
We also nd that the existing GR nders
are quite uneven on how they improve the re-
sults. They each tend to concentrate on im-
proving the recovery of a few kinds of rela-
tions, leaving most of the other kinds alone.
We use this tendency to further boost the
learner's performance by using a merger of
these existing GR nders' output as the initial
labeling.
2 The Experiment
We now improve the performance of the
Ferro et al (1999) transformation rule
learner on a small annotated training set by
using an existing system to provide initial
GR annotations. This experiment is repeated
on two dierent existing systems, which
are reported in Buchholz et al (1999) and
Carroll et al (1999), respectively.
Both of these systems nd a somewhat
dierent set of GR annotations than the
one learned by the Ferro et al (1999) sys-
tem. For example, the Buchholz et al (1999)
system ignores verb complements of verbs
and is designed to look for relationships
to verbs and not GRs that exist between
nouns, etc. This system also handles
relative clauses dierently. For example,
in Miller, who organized ..., this system is
trained to indicate that who is the subject
of organized, while the Ferro et al (1999)
system is trained to indicate that Miller
is the subject of organized. As for the
Carroll et al (1999) system, among other
things, it does not distinguish between sub-
types of modiers such as time, location and
possessive. Also, both systems handle copu-
las (usually using the verb to be) dierently
than in Ferro et al (1999).
2.1 Experiment Set-Up
As described in Ferro et al (1999), the trans-
formation rule learner starts with a p-o-s
tagged corpus that has been chunked into
noun chunks, etc. The starting state also in-
cludes imperfect estimates of pp-attachments
and a blank set of initial GR annotations.
In these experiments, this blank initial set
is changed to be a translated version of the
annotations produced by an existing system.
This is how the existing system transmits
what it found to the rule learner. The set-
up for this experiment is shown in gure 1.
The four components with + signs are taken
out when one wants the transformation rule
learner to start with a blank set of initial GR
annotations.
The two arcs in that gure with a * indicate
where the translations occur. These transla-
tions of the annotations produced by the ex-
isting system are basically just an attempt to
map each type of annotation that it produces
to the most likely type of corresponding an-
notation used in the Ferro et al (1999) sys-
tem. For example, in our experiments, the
Buchholz et al (1999) system uses the anno-
tation np-sbj to indicate a subject, while the
Ferro et al (1999) system uses the annota-
tion subj. We create the mapping by ex-
amining the training set to be given to the
Ferro et al (1999) system. For each type of
relation e
i
output by the existing system when
given the training set text, we look at what
relation types (which t
k
's) co-occur with e
i
in
the training set. We look at the t
k
's with the
highest number of co-occurrences with that
e
i
. If that t
k
is unique (no ties for the highest
number of co-occurrences) and translating e
i
to that t
k
generates at least as many correct
annotations in the training set as false alarms,
then make that translation. Otherwise, trans-
late e
i
to no relation. This latter translation
is not uncommon. For example, in one run of
our experiments, 9% of the relation instances
in the training set were so translated, in an-
other run, 46% of the instances were so trans-
lated.
Some relations in the Carroll et al (1999)
system are between three or four elements.
These relations are each rst translated into
a set of two element sub-relations before the
examination process above is performed.
Even before applying the rules, the trans-
lations nd many of the desired annotations.
However, the rules can considerably improve
what is found. For example, in two of our
early experiments, the translations by them-
selves produced F-scores (explained below)
of about 40% to 50%. After the learned
rules were applied, those F-scores increased
to about 70%.
An alternative to performing translations is
to use the untranslated initial annotations as
an additional type of input to the rule sys-
tem. This alternative, which we have yet
to try, has the advantage of tting into the
transformation-based error-driven paradigm
(Brill and Resnik, 1994) more cleanly than
having a translation stage. However, this ad-
ditional type of input will also further slow-
down an already slow rule-learning module.
2.2 Overall Results
For our experiment, we use the same
1151 word (748 GR) test set used in
Ferro et al (1999), but for a training set, we
use only a subset of the 3299 word training set
used in Ferro et al (1999). This subset con-
tains 1391 (71%) of the 1963 GR instances in
the original training set. The overall results
for the test set are
Smaller Training Set, Overall Results
R P F ER
IaC 478 (63.9%) 77.2% 69.9% 7.7%
IaB 466 (62.3%) 78.1% 69.3% 5.8%
NI 448 (59.9%) 77.1% 67.4%
where row IaB is the result of using the rules
learned when the Buchholz et al (1999) sys-
tem's translated GR annotations are used
as the Initial Annotations, row IaC is the
similar result with the Carroll et al (1999)
system, and row NI is the result of using
the rules learned when No Initial GR an-
notations are used (the rule learner as run
in Ferro et al (1999)). R(ecall) is the num-
ber (and percentage) of the keys that are
recalled. P(recision) is the number of cor-
?
?
?
existing system
+

?
?
?
existing system
+
test set
?
?
?
?
??
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
??
?
?
?
?

?
?
?
??
?
?
?
?
?
?
??
?
small training set
rule learner
key GR annotations for small training set
*
*
rules
+
GR annotations
initial test
+
initial training
GR annotations
nal test
GR annotations
rule interpreter
Figure 1: Set-up to use an existing system to improve performance
rectly recalled keys divided by the num-
ber of GRs the system claims to exist.
F(-score) is the harmonic mean of recall (r)
and precision (p) percentages. It equals
2pr/(p + r). ER stands for Error Reduc-
tion. It indicates how much adding the ini-
tial annotations reduced the missing F-score,
where the missing F-score is 100%?F. ER=
100%?(F
IA
?F
NI
)/(100%?F
NI
), where F
NI
is the F-score for the NI row, and F
IA
is the
F-score for using the Initial Annotations of
interest. Here, the dierences in recall and F-
score between NI and either IaB or IaC (but
not between IaB and IaC) are statistically sig-
nicant. The dierences in precision is not.
2
In these results, most of the modest F-score
gain came from increasing recall.
One may note that the error reductions here
are smaller than Palmer (1997)'s error reduc-
tions. Besides being for dierent tasks (word
segmentation versus GRs), the reductions are
also computed using a dierent type of base-
line. In Palmer (1997), the baseline is how
well an existing system performs before the
rules are run. In this paper, the baseline is
the performance of the rules learned without
2
When comparing dierences in this paper, the
statistical signicance of the higher score being bet-
ter than the lower score is tested with a one-sided
test. Dierences deemed statistically signicant are
signicant at the 5% level. Dierences deemed non-
statistically signicant are not signicant at the 10%
level. For recall, we use a sign test for matched-pairs
(Harnett, 1982, Sec. 15.5). For precision and F-score,
a matched-pairs randomization test (Cohen, 1995,
Sec. 5.3) is used.
rst using an existing system. If we were to
use the same baseline as Palmer (1997), our
baseline would be an F of 37.5% for IaB and
52.6% for IaC. This would result in a much
higher ER of 51% and 36%, respectively.
We now repeat our experiment with the
full 1963 GR instance training set. These re-
sults indicate that as a small training set gets
larger, the overall results get better and the
initial annotations help less in improving the
overall results. So the initial annotations are
more helpful with smaller training sets. The
overall results on the test set are
Full Training Set, Overall Results
R P F ER
IaC 487 (65.1%) 79.7% 71.7% 6.3%
IaB 486 (65.0%) 76.5% 70.3% 1.7%
NI 476 (63.6%) 77.3% 69.8%
The dierences in recall, etc. between IaB and
NI are now small enough to be not statisti-
cally signicant. The dierences between IaC
and NI are statistically signicant,
3
but the
dierence in both the absolute F-score (1.9%
versus 2.5% with the smaller training set) and
ER (6.3% versus 7.7%) has decreased.
2.3 Results by Relation
The overall result of using an existing system
is a modest increase in F-score. However, this
increase is quite unevenly distributed, with a
3
The recall dierence is semi-signicant, being sig-
nicant at the 10% level.
few relation(s) having a large increase, and
most relations not having much of a change.
Dierent existing systems seem to have dier-
ent relations where most of the increase oc-
curs.
As an example, take the results of using
the Buchholz et al (1999) system on the 1391
GR instance training set. Many GRs, like pos-
sessive modier, are not aected by the added
initial annotations. Some GRs, like location
modier, do slightly better (as measured by
the F-score) with the added initial annota-
tions, but some, like subject, do better with-
out. With GRs like subject, some dierences
between the initial and desired annotations
may be too subtle for the Ferro et al (1999)
system to adjust for. Or those dierences may
be just due to chance, as the result dierences
in those GRs are not statistically signicant.
The GRs with statistically signicant result
dierences are the time and other
4
modiers,
where adding the initial annotations helps.
The time modier
5
results are quite dierent:
Smaller Training Set, Time Modiers
R P F ER
IaB 29 (64.4%) 80.6% 71.6% 53%
NI 14 (31.1%) 56.0% 40.0%
The dierence in the number recalled (15) for
this GR accounts for nearly the entire dier-
ence in the overall recall results (18). The re-
call, precision and F-score dierences are all
statistically signicant.
Similarly, when using the
Carroll et al (1999) system on this training
set, most GRs are not aected, while others
do slightly better. The only GR with a sta-
tistically signicant result dierence is object,
where again adding the initial annotations
helps:
Smaller Training Set, Object Relations
R P F ER
IaC 198 (79.5%) 79.5% 79.5% 17%
NI 179 (71.9%) 78.9% 75.2%
The dierence in the number recalled (19) for
this GR again accounts for most of the dif-
4
Modiers that do not fall into any of the subtypes
used, such as time, location, possessive, etc. Examples
of unused subtypes are purpose and modality.
5
There are 45 instances in the test set key.
ference in the overall recall results (30). The
recall and F-score dierences are statistically
signicant. The precision dierence is not.
As one changes from the smaller 1391 GR
instance training set to the larger 1963 GR
instance training set, these F-score improve-
ments become smaller. When using the
Buchholz et al (1999) system, the improve-
ment in the other modier is now no longer
statistically signicant. However, the time
modier F-score improvement stays statisti-
cally signicant:
Full Training Set, Time Modiers
R P F ER
IaB 29 (64.4%) 74.4% 69.0% 46%
NI 15 (33.3%) 57.7% 42.3%
When using the Carroll et al (1999) system,
the object F-score improvement stays statisti-
cally signicant:
Full Training Set, Object Relations
R P F ER
IaC 194 (77.9%) 85.1% 81.3% 16%
NI 188 (75.5%) 80.3% 77.8%
2.4 Combining Sets of Initial
Annotations
So the initial annotations from dierent ex-
isting systems tend to each concentrate on
improving the performance of dierent GR
types. From this observation, one may wonder
about combining the annotations from these
dierent systems in order to increase the per-
formance on all the GR types aected by those
dierent existing systems.
Various works (van Halteren et al, 1998;
Henderson and Brill, 1999; Wilkes and
Stevenson, 1998) on combining dierent sys-
tems exist. These works use one or both of
two types of schemes. One is to have the
dierent systems simply vote. However, this
does not really make use of the fact that dif-
ferent systems are better at handling dier-
ent GR types. The other approach uses a
combiner that takes the systems' output as
input and may perform such actions as de-
termining which system to use under which
circumstance. Unfortunately, this approach
needs extra training data to train such a com-
biner. Such data may be more useful when
used instead as additional training data for
the individual methods that one is consider-
ing to combine, especially when the systems
being combined were originally given a small
amount of training data.
To avoid the disadvantages of these existing
schemes, we came up with a third method.
We combine the existing related systems by
taking a union of their translated annota-
tions as the new initial GR annotation for
our system. We rerun rule learning on the
smaller (1391 GR instance) training set with
a Union of the Buchholz et al (1999) and
Carroll et al (1999) systems' translated GR
annotations. The overall results for the test
set are (shown in row IaU)
Smaller Training Set, Overall Results
R P F ER
IaU 496 (66.3%) 76.4% 71.0% 11%
IaC 478 (63.9%) 77.2% 69.9% 7.7%
IaB 466 (62.3%) 78.1% 69.3% 5.8%
NI 448 (59.9%) 77.1% 67.4%
where the other rows are as shown in Sec-
tion 2.2. Compared to the F-score with
using Carroll et al (1999) (IaC), the IaU
F-score is borderline statistically signi-
cantly better (11% signicance level). The
IaU F-score is statistically signicantly bet-
ter than the F-scores with either using
Buchholz et al (1999) (IaB) or not using any
initial annotations (NI).
As expected, most (42 of 48) of the overall
increase in recall going from NI to IaU comes
from increasing the recall of the object, time
modier and other modier relations, the re-
lations that IaC and IaB concentrate on. The
ER for object is 11% and for time modier is
56%.
When this combining approach is repeated
the full 1963 GR instance training set, the
overall results for the test set are
Full Training Set, Overall Results
R P F ER
IaU 502 (67.1%) 77.7% 72.0% 7.3%
IaC 487 (65.1%) 79.7% 71.7% 6.3%
IaB 486 (65.0%) 76.5% 70.3% 1.7%
NI 476 (63.6%) 77.3% 69.8%
Compared to the smaller training set results,
the dierence between IaU and IaC here is
smaller for both the absolute F-score (0.3%
versus 1.1%) and ER (1.0% versus 3.3%). In
fact, the F-score dierence is small enough to
not be statistically signicant. Given the pre-
vious results for IaC and IaB as a small train-
ing set gets larger, this is not surprising.
3 Discussion
GRs are important, but dierent sets of GRs
are useful for dierent purposes and dierent
systems are better at nding certain types of
GRs. Here, we have been looking at ways of
improving automatic GR nders when one has
only a small amount of data with the desired
GR annotations. In this paper, we improve
the performance of the Ferro et al (1999) GR
transformation rule learner by using existing
systems to nd related sets of GRs. The out-
put of these systems is used to supply ini-
tial sets of annotations for the rule learner.
We achieve modest gains with the existing
systems tried. When one examines the re-
sults, one notices that the gains tend to be
uneven, with a few GR types having large
gains, and the rest not being aected much.
The dierent systems concentrate on improv-
ing dierent GR types. We leverage this ten-
dency to make a further modest improvement
in the overall results by providing the rule
learner with the merged output of these ex-
isting systems. We have yet to try other ways
of combining the output of existing systems
that do not require extra training data. One
possibility is the example-based combiner in
Brill and Wu (1998, Sec. 3.2).
6
Furthermore,
nding additional existing systems to add to
the combination may further improve the re-
sults.
References
S. A?t-Mokhtar and J.-P. Chanod. 1997. Subject
and object dependency extraction using nite-
state transducers. In Proc. ACL workshop on
automatic information extraction and building
6
Based on the paper, we were unsure if extra train-
ing data is needed for this combiner. One of the au-
thors, Wu, has told us that extra data is not needed.
of lexical semantic resources for NLP applica-
tions, Madrid.
E. Brill and P. Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment dis-
ambiguation. In 15th International Conf. on
Computational Linguistics (COLING).
E. Brill and J. Wu. 1998. Classier combina-
tion for improved lexical disambiguation. In
COLING-ACL'98, pages 191195, Montr?al,
Canada.
S. Buchholz, J. Veenstra, and W. Daelemans.
1999. Cascaded grammatical relation assign-
ment. In Joint SIGDAT Conference on Empir-
ical Methods in NLP and Very Large Corpora
(EMNLP/VLC'99). cs.CL/9906004.
J. Carroll, T. Briscoe, N. Calzolari, S. Fed-
erici, S. Montemagni, V. Pirrelli, G. Grefen-
stette, A. Sanlippo, G. Carroll, and M. Rooth.
1997. Sparkle work package 1, spec-
ication of phrasal parsing, nal report.
Available at http://www.ilc.pi.cnr.it/-
sparkle/sparkle.htm, November.
J. Carroll, G. Minnen, and T. Briscoe. 1999.
Corpus annotation for parser evaluation. In
EACL99 workshop on Linguistically Interpreted
Corpora (LINC'99). cs.CL/9907013.
P. Cohen. 1995. Empirical Methods for Articial
Intelligence. MIT Press, Cambridge, MA, USA.
L. Ferro, M. Vilain, and A. Yeh. 1999. Learn-
ing transformation rules to nd grammatical
relations. In Computational natural language
learning (CoNLL-99), pages 4352. EACL'99
workshop, cs.CL/9906015.
D. Harnett. 1982. Statistical Methods. Addison-
Wesley Publishing Co., Reading, MA, USA,
third edition.
J. Henderson and E. Brill. 1999. Exploiting diver-
sity in natural language processing: combining
parsers. In Joint SIGDAT Conference on Em-
pirical Methods in NLP and Very Large Cor-
pora (EMNLP/VLC'99).
R. Hwa. 1999. Supervised grammar induction
using training data with limited constituent in-
formation. In ACL'99. cs.CL/9905001.
R. Kaplan. 1994. The formal architecture of
lexical-functional grammar. In M. Dalrymple,
R. Kaplan, J. Maxwell III, and A. Zaenen, ed-
itors, Formal issues in lexical-functional gram-
mar. Stanford University.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Lin-
guistics, 19(2).
M. Palmer, R. Passonneau, C. Weir, and T. Finin.
1993. The kernel text understanding system.
Articial Intelligence, 63:1768.
D. Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proceedings of
ACL/EACL97.
D. Perlmutter. 1983. Studies in Relational Gram-
mar 1. U. Chicago Press.
H. van Halteren, J. Zavrel, and W. Daelemans.
1998. Improving data driven wordclass tagging
by system combination. In COLING-ACL'98,
pages 491497, Montr?al, Canada.
Y. Wilkes and M. Stevenson. 1998. Word sense
disambiguation using optimized combinations
of knowledge sources. In COLING-ACL'98,
pages 13981402, Montr?al, Canada.
Gene Name Extraction Using FlyBase Resources 
Alex Morgan 
amorgan@mitre.org 
Lynette Hirschman 
lynette@mitre.org 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730-1420 
Alexander Yeh 
asy@mitre.org 
Marc Colosimo 
mcolosim@brandeis.edu  
 
 
Abstract 
Machine-learning based entity extraction re-
quires a large corpus of annotated training to 
achieve acceptable results.  However, the cost 
of expert annotation of relevant data, coupled 
with issues of inter-annotator variability, 
makes it expensive and time-consuming to 
create the necessary corpora. We report here 
on a simple method for the automatic creation 
of large quantities of imperfect training data 
for a biological entity (gene or protein) extrac-
tion system. We used resources available in 
the FlyBase model organism database; these 
resources include a curated lists of genes and 
the articles from which the entries were 
drawn, together a synonym lexicon.  We ap-
plied simple pattern matching to identify gene 
names in the associated abstracts and filtered 
these entities using the list of curated entries 
for the article.  This process created a data set 
that could be used to train a simple Hidden 
Markov Model (HMM) entity tagger. The re-
sults from the HMM tagger were comparable 
to those reported by other groups (F-measure 
of 0.75). This method has the advantage of be-
ing rapidly transferable to new domains that 
have similar existing resources. 
1 
                                                          
Introduction: Biological Databases 
 
There is currently an information explosion in 
biomedical research.  The growth of literature is 
roughly exponential, as can be seen in Figure 1 
which shows the number of literature references in 
FlyBase1 organized by date of publication over a 
hundred year span.2  This growth of literature 
makes it daunting for researchers to keep track of 
the information, even in very small subfields of 
biology. 
1 FlyBase is a database that focuses on research in the genetics 
and molecular biology of the fruit fly (Drosophila melangas-Figure 1: FlyBase References, 1900-2000 
 
Increasingly, biological databases serve to collect 
and organize published experimental results.  A 
wide range of biological databases exist, including 
model organism databases (e.g., for mouse3 and 
yeast4) as well as various protein databases (e.g., 
Protein Information Resource5 (PIR) or SWISS-                                                                                          
tor), a model organism for genetics research: 
http://www.flybase.org. 
PROT6 and   interaction databases such as the 
Biomolecular Interaction Network Database7 
(BIND). These databases are created by a process 
of curation, which is done by Ph.D. biologists who 
read the published literature to cull experimental 
findings and relations. These facts are organized 
into a set of structured fields of a database and 
 
2 Of course most of these early references in FlyBase are not 
in electronic form. The FlyBase database has been in existence 
since 1993. 
3 http://www.informatics.jax.org/ 
4 http://genome-www.stanford.edu/Saccharomyces/ 
5 http://pir.georgetown.edu/pirwww/pirhome3.shtml 
6 http://us.expasy.org/sprot/ 
7 http://www.bind.ca/ 
linked to the source of information (the journal 
article).  As a result, curation is a time-consuming 
and expensive process; database curators are in-
creasingly eager to adopt text mining and natural 
language processing techniques to make curation 
faster and more consistent. As a result, there has 
been growing interest in the application of entity 
extraction and text classification techniques to the 
problem of biological database curation [Hirsch-
man02]. 
2 
                                                          
Entity Extraction Methods 
There are two approaches to entity extraction.  The 
first requires manual or heuristic creation of rules 
to identify the names mentioned in text; the second 
uses machine learning to create the rules that drive 
the entity tagging. Heuristic systems require expert 
developers to create the rules, and these rules must 
be manually changed to handle new domains. Ma-
chine-learning based systems are dependent on 
large quantities of tagged data, consisting of both 
positive and negative examples.8  Figure 2 shows 
results from the IdentiFinder system [Bikel99] il-
lustrating that performance increases roughly with 
the log of quantity of training data. Given the ex-
pense of manual annotation of large quantities of 
data, the challenge for the machine learning ap-
proach is to find ways of creating sufficient quanti-
ties of training data cheaply. 
     Overall, hand-crafted systems seem to outper-
form learning-based systems for biology. How-
ever, it is clear that the quantities of training have 
been small, relative to the results reported for en-
tity extraction in e.g., newswire [Hirschman03]. 
There are several published sets of performance 
results for automatic named biological entity ex-
traction systems.  The system of Collier et al [Col-
lier00] uses a hidden Markov model to achieve an 
F-measure9 of 0.73 when trained on a corpus of 
29,940 words of text from 100 MEDLINE ab-
stracts.   Contrast this with Figure 2, which reports 
results using over 600,000 words of training data, 
and an F-measure of 0.95 for English newswire 
entity extraction (and 0.91 for Spanish).   
                                                          
8 For negative examples, the "closed world" assumption gen-
erally is taken to apply: if an entity is not tagged, it is assumed 
to be a negative example. 
 
Krauthammer et al [Krauthammer00] have taken a 
somewhat different approach which encodes char-
acters as 4-tuples of  DNA bases; they then use 
BLAST together with a lexicon of gene names to 
search for 'gene name homologies'. They report an 
F-measure of 0.75 without the use of a large set of 
rules or annotated training data. 
 
The PASTA system [Gaizauskas03] uses a combi-
nation of heuristic and machine-learned rules to 
achieve a higher F-measure over a larger number 
of classes: F-measure of 0.83 for the task of identi-
fying 12 classes of entities involved in the descrip-
tion of roles of residues in protein molecules. 
Because they used heuristic rules, they were able 
to get these results with a relatively small training 
corpus of 52 MEDLINE abstracts (roughly 12,000 
words). 
Figure 2: Performance of BBN's IdentiFinder named entity 
recognition system relative to the amount of training data, from 
[Bikel99] 
 
These results suggest that machine learning meth-
ods will not be able to compete with heuristic rules 
until there is a way to generate large quantities of 
annotated training data. Biology has the advantage 
that there are rich resources available, such as lexi-
cons, ontologies and hand-curated databases.  
What is missing is a way to convert these into 
training corpora for text mining and natural lan-
guage processing.  Craven and Kumlien [Cra-
ven99] developed an innovative approach that used 
fields in a biological database to locate abstracts 
which mention physiological localization of pro-
teins. Then via a simple pattern matching algo-
9 
Recall) (Precision
Recall)Precision2(
+
??=F   
Manning D, Schutze H. Foundations of Statistical Natural 
Language Processing, 2002: p 269. 
rithm, they identified those sentences where the 
relation was mentioned and matched these with 
entries in the Yeast Protein Database (YPD).  In 
this way, they were able to automatically create an 
annotated gold standard, consisting of sentences 
paired with the curated relations derived from 
those sentences. They then used these for training 
and testing a machine-learning based system.  This 
approach inspired our interest in using existing 
resources to create an annotated corpus automati-
cally.   
3 
r 
3.1 
                                                          
FlyBase: Organization and Resources 
We focused on FlyBase because we had access to 
FlyBase resources from our work in the creation of 
the KDD 2002 Cup Challenge Task 1 [Yeh03].   
Through this work, we had become familiar with 
the multi-stage process of curation.  An early task 
in the curation pipeline is to determine, for a given 
article, whether there are experimental results that 
need to be added to the database. This was the task 
used as the basis for the KDD text data mining 
"challenge evaluation". A later task in the pipeline 
creates a list of the Drosophila genes discussed in 
each curated article. This is the task we focus on in 
this paper.   
 
An example of a FlyBase entry can be seen in Fig-
ure 3 which shows part of the record for the gene 
Toll. Under Molecular Function and Biological 
Process we see that the gene is responsible for en-
coding a transmembrane receptor protein involved 
in antimicrobial humoral response (part of the 
innate immune system of the fly).  We see furthe
that  ?Tl? and ?CG5490? are synonymous for Toll 
(top of the entry next to Symbol), and the link 
Synonyms leads to a long synonym list which in-
cludes: ?Fs(1)Tl?, ?dToll?, ?CT17414?, ?Toll-1?, 
?Fs(3)Tl?, ?mat(3)9?, ?mel(3)10?, and ?mel(3)9?.  
Many of these facts about Toll are linked to a par-
ticular literature reference in the database.  For ex-
ample, following the link for Transcripts will lead 
to a page with links to the abstract of a paper by 
Tauszig et al [Tauszig00] which reports on ex-
periments which measured the lengths of RNA 
transcribed from the Toll gene. 
 
For FlyBase, Drosophila genes are the key bio-
logical entities; each entity (e.g., gene) is associ-
ated with a unique identifier for the underlying 
physical entity. If there were a one-to-one relation-
ship between gene name and unique identifier, the 
gene identification task would be straightforward.  
However, both polysemy and synonymy occur fre-
quently in the naming of biological entities, and 
the gene names of Drosophila are considered to be 
particularly problematic because of creative nam-
ing conventions10.  For example, ?18 wheeler?, 
?batman?, and ?rutabaga? are all Drosophila gene 
names. A single entity (as represented by a unique 
identifier) may have a number of names like Toll 
or even ATP?, which has 38 synonyms listed in 
FlyBase.     
 
Figure 3: FlyBase entry for Toll 
 
Resources 
We obtained a copy of part the FlyBase database,11 
including the lists of genes discussed in each paper 
examined by the curators.  Using the BioPython12 
modules, we were able to obtain MEDLINE ab-
stracts for 15,144 for these papers.  We decided to 
10 At the other end of the spectrum is the yeast nomenclature 
which is strictly controlled ? see <http://genome- 
www.stanford.edu/Saccharomyces/gene_guidelines.shtml> for 
nomenclature conventions. 
11 Special thanks to William Gelbart, David Emmert, Beverly 
Matthews, Leyla Bayraktaroglu, and Don Gilbert. 
12 http://www.biopython.org/ 
set aside the same articles used in the KDD Cup 
Challenge [Yeh03] for evaluation purposes.  This 
left a training set of 14,033 abstracts, consisting of 
a total of 2,664,324 lexemes identified by our 
tokenizer. 
4 
4.1 
                                                          
 
It was only with some reluctance that we decided 
to focus on journal abstracts. From our earlier 
work, we recognized that the majority of the in-
formation entered into FlyBase is missing from the 
abstracts and can be found only in the full text of 
the article [Hirschman03]. However, due to copy-
right restrictions, there is a paucity of freely avail-
able full text for journal articles.  What articles are 
available in electronic form vary in their format-
ting, which can cause considerable difficulty in 
automatic processing. MEDLINE abstracts have a 
uniform format and are readily available. Many 
other experiments have been performed on 
MEDLINE abstracts for similar reasons. 
 
We also created a synonym lexicon from FlyBase.  
We found 35,971 genes with associated ?gene 
symbols? (e.g. Tl is the gene symbol for Toll) and 
48,434 synonyms; therefore, each gene has an av-
erage of 2.3 alternate naming forms, including the 
gene symbol.  The lexicon also allowed us to asso-
ciate each gene with one a unique FlyBase gene 
identifier, providing "term normalization." 
Experiments 
For purposes of evaluation, our task was the identi-
fication of mentions of Drosophila genes in the 
text of abstracts.  We also included mentions of 
protein or transcript where the associated gene 
shared the same name. This occurs when, for ex-
ample, the gene name appears as a pre-nominal 
modifier, as in "the zygotic Toll protein".  We did 
not include mentions of protein complexes because 
these are created out of multiple polypeptide 
chains with multiple genes (e.g., hemoglobin). We 
also did not include families of proteins or genes 
(e.g. lectin), particular alleles of a gene, genes 
which are not part of the natural Drosophila ge-
nome such as reporter genes (e.g. LacZ), and the 
names of genes from other organisms (e.g. sonic 
hedgehog, the mammalian gene homologous to the 
Drosophila hedgehog gene).13 
Background 
Our initial experiment [Hirschman03] had looked 
at creating a gene name finder by simple pattern 
matching, using the extensive FlyBase list of genes 
and their synonyms and identifying each mention 
which occurred in the lexicon with the appropriate 
unique identifier. This yielded spectacularly poor 
results: recall14 on the full papers was quite high 
(84%), but precision was 2%!  For abstracts, the 
recall was predictably lower (31%) and precision 
remained low at 7%.  Our analysis showed that 
polysemy (described in Section 5) and the large 
intersection of gene names with common English 
words caused most of the performance problems. 
In the initial run, where a name was ambiguous, 
we recorded all gene identifiers; this raised recall 
but lowered precision.  After removing all the 
names which were ambiguous for a gene, precision 
climbed to 5% for full papers and 17% in abstracts, 
with a corresponding drop in recall (77% for full 
papers, 28% for abstracts).  We also tried a few 
simple filters, such as ignoring all terms three 
characters or less in length, but the best precision 
we could achieve was 29% in abstracts, certainly 
unacceptable. 
 
We were, however, encouraged by the relatively 
high recall in full papers. Analysis showed that 
many of the missing names were contained only in 
figures or tables that had not been downloaded.  
While these were counted as recall errors when 
compared to the FlyBase curation, there were, in 
fact, no mentions of these genes in the text that had 
been downloaded for this experiment.  Similarly, 
for abstracts, while the recall appeared low com-
pared to the complete set of genes discussed in the 
full paper, these genes were simply not mentioned 
in the abstract.  So from an information extraction 
13 There are no curated lists of complexes or families in Fly-
Base, so we did not train a tagger for these tasks. In our man-
ual curation, we did create separate tags for complexes and 
families, since we believe that these will be important for fu-
ture tasks.  
14 Note that these measures of recall and precision are based 
on the list of unique Drosophila genes curated in a paper. This 
is quite different from recall and precision measuring the men-
tions of gene names in a paper. We used the measure of 
unique genes in a paper because this allowed us to take advan-
tage of the existing FlyBase expert curated resources. 
point of view, the simple pattern matching 
achieved a very high recall for genes mentioned in 
the text being processed. 
4.2 
4.3 
Generating Noisy Training Data 
The initial experiment demonstrated that exact 
match using rich lexical resources was not useful 
on its own. However, we realized that we could 
use the lists of curated genes from FlyBase to con-
strain the possible matches within an abstract ? that 
is, to "license" the tagging of only those genes 
known to occur in the curated full article.  Our 
hope was that this filtered data would provide large 
quantities of cheap but imperfect or noisy training 
data.  
 
Our next experiment focused on generating this 
large but noisy training corpus.  We used our inter-
nal tokenizer, punctoker, originally designed for 
use with newswire data.  There were some errors in 
tokenization, since biological terms have a very 
different morphology from newswire? see 
[Cohen02] for an interesting discussion of tokeni-
zation issues. Among the problems in tokenization 
were uses of "-" instead of white space, or "/" to 
separate recombinant genes.  However, an informal 
examination of errors did not show tokenization 
errors to be a significant contributor to the overall 
performance of the entity extraction system. 
 
To perform the pattern matching, we created a suf-
fix tree of all the synonyms known to FlyBase for 
those genes. This was important, since many bio-
logical entity names are multi-word terms.   We 
then used longest-extent pattern matching to find 
candidate mentions in the abstract of the paper.  
The system tagged only terms licensed by the as-
sociated list of genes for the abstract, assigning the 
appropriate unique gene identifier. Even with the 
FlyBase filtering, this method resulted in some 
errors.  For example, an examination of an abstract 
describing the gene to revealed the unsurprising 
result that all the uses of the word "to" did not refer 
to the gene.  However, the aim was to create data 
of sufficient quantity to lessen the effects of this 
noise. 
Evaluation 
In order to measure performance, we created a 
small doubly annotated test corpus.  We selected a 
sample of 86 abstracts and had two annotators 
mark these abstracts for gene name mentions as 
previously described.  Mentions of families and 
foreign genes were also identified with different 
tags during this process, but not evaluated.   One 
curator was a professional researcher in biology 
with experience as a model organism genome da-
tabase curator (Colosimo).  This set of annotations 
was taken as the "gold-standard". The second an-
notator was the system developer with no particu-
lar annotation experience (Morgan). With two 
annotators, we were able to measure inter-
annotator agreement (F-measure of 0.87). We also 
measured the quality of the automatically created 4.4
     
training data by using the lexical pattern matching 
procedure with filtering to generate annotations for 
86 abstracts in the test set.  The F-measure was 
0.83, when compared against the gold standard, 
shown in Table 1 below. 
F-measure Precision Recall
Training Data
Quality
0.83 0.78 0.88
Inter-
annotator
Agreement
0.87 0.83 0.91
 
 
Ta
We  
tha s 
me
the
[Pa
trai
and
wa
Fig
15 P
http 
ble 1: Training data quality and inter-annotator agreement  
HMM Tagging With Noisy Training Data 
 now had a large quantity of noisy training data
t we could use to train a statistical tagger.   Thi                                                     
thodology is illustrated in Figure 4.  We chose 
 HMM-based trainable entity tagger phrag15 
lmer99] to extract the names in text.  We 
ned phrag on different amounts of training data 
 measured performance.  Our evaluation metric 
s the standard metric used in named entity 
Abstracts
from
PubMed
Lexicon
FlyBase
Large Quantity
of Noisy
Training Data
Plain Text
Genes Tagged
Gene1 Gene2
Other1 Other2
Start End
Text automatically tagged using
FlyBase references and a lexicon is
used to train up a tagger capable of
tagging gene names in new text,
including gene names never observed
before.
Trainable
Tagger
ure 4: Schematic of  Methodology 
hrag is available for download at 
://www.openchannelfoundation.org/projects/Qanda 
Training Data F-measure Precision Recall
531522 0.62 0.73 0.54
529760 0.64 0.75 0.56
1342039 0.72 0.80 0.65
2664324 0.73 0.79 0.67
No Orthographic Correction
  
Table 2: Performance as a function of training data 
 
Training Data F-measure Precision Recall
531522 0.65 0.76 0.56
529760 0.66 0.74 0.59
522825 0.67 0.76 0.59
1322285 0.72 0.77 0.67
1342039 0.75 0.80 0.70
2664324 0.75 0.78 0.71
Orthographic Correction
 
Table 3: Improved performance with orthographical correction 
for Greek letters and case folding for term matching in training 
data  
 
-
f 
-
  
-
", 
p-
m 
n 
.6 
 
entity identification F-measure of 73%.  We then 
made a simple modification of the algorithm to 
correct for variations in orthography due to capi-
talization and representation of Greek letters:  we 
simply expanded the search for letters such as "?" 
to include "Delta" and "delta".  By expanding the 
matching of terms using the orthographical and 
case variants, performance of phrag improved 
slightly, shown in Table 3, improving our best 
performance to an F-measure of 75%.   
5 
 
Figure 5 shows these results in a graphical form.  
Two things are apparent from this graph.  Based on 
the results shown in Figure 2, we might expect the 
performance to be linear with the logarithm of the 
amount of training data, and in this case there is a 
rough fit with a correlation coefficient of .88.  The 
other result which stands out is that there is con-
siderable variation in the performance when train-
ed on different training sets of the same size.  We 
believe that this is due to the very limited amount 
of testing data. 
Error Analysis 
We have identified three types of polysemy in 
Drosophila gene names in FlyBase.  In some cases, 
one name (e.g., ?Clock?) can refer to two distinct 
genes: period or Clock.  The term with the most 
polysemy is ?P450? which is a family of genes and 
is listed as a synonym for 20 different genes in 
FlyBase.  In addition, the same term is often used 
interchangeably to refer to the gene, RNA tran-
script, or the protein. [Hazivassloglou01] presents 
interesting results that demonstrate that experts 
only agree 78% of the time on whether a particular 
mention refers to a gene or a protein.16  The most 
problematic type of polysemy occurs because 
many Drosophila gene names are also regular Eng-
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
100000 1000000 10000000
Training Data (# of Lexemes)
F
-m
ea
su
re
Figure 5: Performance as a function of the amount of train-
ing data.  The line is a least-squares logarithmic fit with an 
R2 value of .8814. 
                                                           
lish words such as "white", ?cycle?, and "bizarre". 
There are some particularly troublesome examples 
that occur because of frequent use of short forms 
(abbreviations) of gene names, e.g., "we", "a", 
"not?, and even ?and? each occur as gene names.  
These short forms are often abbreviations for the 
full gene name.  For example, the gene symbol of 
the gene takeout is "to", and the symbol for the 
16 The entity tagging task for FlyBase was defined to extract 
gene-or-protein names; however, in cases where the article 
talks only about the protein and not about the gene, the protein
name may not appear on the list of curated genes for the arti-
cle, leading to apparent false positives in tagging. evaluation, requiring the matching of a name's ex
tent and tag (except that for our experiment, we 
were only concerned with one tag, Drosophila 
gene).   Extent matching meant exact matching o
gene name boundaries at the level of tokens:   Ex
actly matching boundaries were considered a hit.
Inexact answers are considered a miss.  For exam
ple, a multiword gene name such as "fas receptor
which has been tagged for "fas" but not for "rece
tor" would constitute a miss (recall error) and a 
false alarm (precision error).  
 
Table 2 shows the performance of the basic syste
as a function of the amount of training data.  As 
with Figure 2, we see there is a diminishing retur
as the amount of training data is increased.   At 2
million words or training data, phrag achieved an
gene wee is "we".  It may be that more sophisti-
cated handling of abbreviations can address some 
of these issues. 
An error analysis looking at the results of our sta-
tistical tagger demonstrated some unusual behav-
ior.  Because our gene name tagger phrag uses a 
first order Markov model, it relies on local context 
and occasionally makes errors such as not tagging 
all of the occurrences of the term "rutabaga" in an 
abstract about rutabaga as gene names.  This cer-
tainly opens up the opportunity for some sort of 
post processing step to resolve these problems. 
 
The fact that phrag uses this local context can 
sometimes be a strength, enabling it to identify 
gene names it has never seen.  We estimated the 
ability of the system to identify new terms as gene 
names by substituting strings unknown to phrag in 
place of all the occurrences of gene names in the 
evaluation data.  The performance of the system at 
correctly identifying terms it had never observed 
gave a precision of 68%, a recall of 22% and an F-
measure of 33%.  This result is relatively encour-
aging, compared with the 3.3% precision and 4.4% 
recall for novel gene names reported by Krau-
thammer.  Recognizing novel names is important 
because the nomenclature of biological entities is 
constantly changing and entity tagging systems 
should to be able to rapidly adapt and recognize 
new terms.   
6 Conclusion and Future Directions 
We have demonstrated that we can automatically 
produce large quantities of relatively high quality 
training data; these data were good enough to train 
an HMM-based tagger to identify gene mentions 
with an F-measure of 75% (precision of 78% and 
recall of 71%), evaluated on our small develop-
ment test set of 86 abstracts.    This compares fa-
vorably with other reported results as described in 
Section 2, and as discussed below, we believe that 
we can improve upon these results in various ways.  
These results are still considerably below the re-
sults from [Gaizauskas03] and may be too low to 
be useful as a building block for further automated 
processing, such as relation extraction.  However, 
in the absence of any shared benchmark evaluation 
sets, cross-system performance cannot be evalu-
ated since the task definition and evaluation cor-
pora differ from system to system.   
 
We plan to take this work in several directions.  
First, we believe that we can improve the quality of 
the underlying automatically generated data, and 
with this, the quality of the entity tagging. There 
are several things that could be improved.  
 
A morphological analyzer trained for biological 
text would eliminate some of the tokenization er-
rors and perhaps capture some of the underlying 
regularities, such as addition of Greek letters or 
numbers (with or without preceding hyphen) to 
specify sub-types within a gene family. There can 
also be considerable semantic content in gene 
names and their formatting.  For example, many 
Drosophila genes are differentiated from the genes 
of other organisms by prepending a "d" or "D", 
such as "dToll".  Gene names can also be explicit 
descriptions of their chromosomal location or even 
function (e.g. Dopamine receptor). 
 
The problem of matching abbreviations has been 
tackled by a number of researchers [e.g. Puste-
jovsky02 and Liu03].  As was mentioned above, it 
seems that ambiguity for "short forms" of gene 
names could be partially resolved by detecting lo-
cal definitions for abbreviations.  It should also be 
possible to apply part of speech tagging and corpus 
statistics to avoid mis-tagging of common words, 
such as ?to? or ?and?.  
 
In the longer term, this methodology provides an 
opportunity to go beyond gene name tagging for 
Drosophila. It can be extended to other domains 
that have comparable resources (e.g. other model 
organism genome databases, other biological enti-
ties), and entity tagging itself provides the founda-
tion for more complex tasks, such as relation 
extraction (e.g. using the BIND database) or attrib-
ute extraction (e.g. using FlyBase to identify at-
tributes such as RNA transcript length, associated 
with protein coding genes). 
 
Second, the existence of a synonym lexicon with 
unique identifiers provides data for term normali-
zation, a task of potentially greater utility to biolo-
gists than the tagging of every mention in an 
article.  There are currently few corpora with anno-
tated term normalization; using the methodology 
outlined here makes it possible to produce large 
quantities of normalized data.  The identification 
and characterization of abbreviations and other 
transformations would be particularly important in 
normalization.   
By exploiting the rich set of biological resources 
that already exist, it should be possible to generate 
many kinds of corpora useful for training high-
quality information extraction and text mining 
components. 
References 
 
Bikel D, Schwartz R, Weischedel R. An Algorithm that 
Learns What's in a Name. Machine Learning, Special 
Issue on Natural Language Learning 34 (1999):211-31. 
 
Cohen KB, Dolbey A, Hunter L. ?Contrast and variabil-
ity in gene names.? Proceedings of the workshop on 
natural language processing in the biomedical domain, 
Association for Computational Linguistics, 2002 
 
Collier N, Nobata C, Tsujii J. ?Extracting the Names of 
Genes and Gene Products with a Hidden Markov 
Model.? Proceedings of COLING '2000 (2000): 201-07. 
 
Craven M, Kumlien J. ?Constructing Biological Knowl-
edge Bases by Extracting Information from Text 
Sources.? Proceedings of the Seventh International 
Conference on Intelligent Systems for Molecular Biol-
ogy 1999: 77-86. 
 
Gaizauskas R, Demetriou G, Artymiuk PJ, Willett P. 
?Protein Structures and Information Extraction from 
Biological Texts: The PASTA System.? Bioinformatics. 
19  (2003): 135-43. 
 
Hatzivassiloglou V, Duboue P, Rzhetsky A. ?Disam-
biguating Proteins, Genes, and RNA in Text: A Ma-
chine Learning Approach.? Bioinformatics 2001: 97-
106. 
 
Hirschman L, Park J, Tsujii J, Wong L, Wu C. "Accom-
plishments and Challenges in Literature Data Mining 
for Biology," Bioinformatics 17 (2002):1553-61. 
 
Hirschman L, Morgan A, Yeh A.  ?Rutabaga by Any 
Other Name: Extracting Biological Names." Accepted, 
Journal of Biomedical Informatics, Spring 2003.  
 
Krauthammer M, Rzhetsky A, Morosov P, Friedman C. 
?Using BLAST for Identifying Gene and Protein Names 
in Journal Articles.? Gene 259 (2000): 245-52. 
 
Liu H, Friedman C.  ?Mining Terminological Knowl-
edge in Large Biomedical Corpora.?  Proceedings of the 
Pacific Symposium on Biocomputing.  2003. 
 
Palmer D, Burger J, and Ostendorf M. "Information 
Extraction from Broadcast News Speech Data." Pro-
ceedings of the DARPA Broadcast News and Under-
standing Workshop, 1999. 
 
Pustejovsky J, Casta?o J, Saur? R, Rumshisky A, Zhang 
J, Luo W. ?Medstract: Creating Large-scale Information 
Servers for Biomedical Libraries.? Proceedings of the 
ACL 2002 Workshop on Natural Language Processing 
in the Biomedical Domain. 2002. 
 
Tauszig et al ?Toll-related receptors and the control of 
antimicrobial peptide expression in Drosophila.? Pro-
ceedings of the  National Academy of  Sciences 97 
(2000): 10520-5. 
 
Yeh A., Hirschman L,  Morgan A.  "Evaluation of Text 
Data Mining for Database Curation: Lessons Learned 
from the KDD Challenge Cup." Accepted, Intelligent 
Systems in Molecular Biology, Brisbane, June 2003.  
 
 
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 152?160,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Name Matching Between Chinese and Roman Scripts:                       
Machine Complements Human 
 
Ken Samuel, Alan Rubenstein, Sherri Condon, and Alex Yeh 
The MITRE Corporation; M/S H305; 7515 Colshire Drive; McLean, Virginia 22102-7508 
samuel@mitre.org, rubenstein@mitre.org, scondon@mitre.org, and asy@mitre.org 
 
  
Abstract 
There are generally many ways to translite-
rate a name from one language script into 
another. The resulting ambiguity can make it 
very difficult to ?untransliterate? a name by 
reverse engineering the process. In this paper, 
we present a highly successful cross-script 
name matching system that we developed by 
combining the creativity of human intuition 
with the power of machine learning. Our sys-
tem determines whether a name in Roman 
script and a name in Chinese script match 
each other with an F-score of 96%. In addi-
tion, for name pairs that satisfy a computa-
tional test, the F-score is 98%. 
 
1 Introduction 
There are generally many ways to transliterate a 
person?s name from one language script into 
another. For example, writers have transliterated 
the Arabic name, ??????, into Roman characters 
in at least 13 ways, such as Al Choukri, Ash-
shukri, and al-Schoukri. This ambiguity can 
make it very difficult to ?untransliterate? a name 
by reverse engineering the process. 
We focused on a task that is related to transli-
teration. Cross-script name matching aims to de-
termine whether a given name part in Roman 
script matches a given name part in Chinese 
(Mandarin) script,1 where a name part is a single 
?word? in a person?s name (such as a surname), 
and two names match if one is a transliteration of 
the other.2 Cross-script name matching has many 
                                                 
1 In this paper, we often use the word ?Roman? to refer to 
?Roman script?, and similarly, ?Chinese? usually stands 
for ?Chinese script?. 
2 Sometimes a third script comes between the Roman and 
Chinese versions of the name. For example, a Roman 
name might be transliterated into Arabic, which is then 
transliterated into Chinese, or an Arabic name could be 
transliterated into Roman and Chinese independently. 
applications, such as identity matching, improv-
ing search engines, and aligning parallel corpora. 
We combine a) the creative power of human 
intuition, which can come up with clever ideas 
and b) the computational power of machine 
learning, which can analyze large quantities of 
data. Wan and Verspoor (1998) provided the 
human intuition by designing an algorithm to 
divide names into pieces that are just the right 
size for Roman-Chinese name matching (Section 
2.2.). Armed with Wan and Verspoor?s algo-
rithm, a machine learning approach analyzes 
hundreds of thousands of matched name pairs to 
build a Roman-Chinese name matching system 
(Section 3). 
Our experimental results are in Section 4. The 
system correctly determines whether a Roman 
name and a Chinese name match each other with 
F = 96.5%.3 And F = 97.6% for name pairs that 
satisfy the Perfect Alignment hypothesis condi-
tion, which is defined in Section 2.2. 
 
2 Related Work 
Wan and Verspoor?s (1998) work had a great 
impact on our research, and we explain how we 
use it in Section 2.2. In Section 2.1, we identify 
other related work. 
2.1 Chinese-English Name Matching 
Condon et al (2006) wrote a paper about the 
challenges of matching names across Roman and 
Chinese scripts. In Section 6 of their paper, they 
offered an overview of several papers related to 
Roman-Chinese name matching. (Cohen et al, 
2003; Gao et al, 2004;  Goto et al, 2003; Jung et 
al., 2000; Kang and Choi, 2000; Knight and 
Graehl, 1997; Kondrak, 2000; Kondrak and 
Dorr, 2004; Li et al, 2004; Meng et al, 2001; Oh 
                                                 
3 F stands for F-score, which is a popular evaluation metric. 
(Andrade et al, 2009) 
152
and Choi, 2006; Virga and Khudanpur, 2003; 
Wellner et al, 2005; Winkler, 2002) 
The Levenshtein algorithm is a popular way to 
compute string edit distance. (Levenshtein, 1966) 
It can quantify the similarity between two names. 
However, this algorithm does not work when the 
names are written in different scripts. So Free-
man et al (2006) developed a strategy for Ro-
man-Arabic  string matching that uses equiva-
lence classes of characters to normalize the 
names so that Levenshtein?s method can be ap-
plied.  Later, Mani et al (2006) transformed that 
system from Roman-Arabic to Roman-Chinese 
name matching and extended the Levenshtein 
approach, attaining F = 85.2%. Then when they 
trained a machine learning algorithm on the out-
put, the performance improved to F = 93.1% 
 Mani et al also tried applying a phonological 
alignment system (Kondrak, 2000) to the Ro-
man-Chinese name matching task, and they re-
ported an F-score of 91.2%. However, when they 
trained a machine learning approach on that sys-
tem?s output, the F-score was only 90.6%.  
It is important to recognize that it would be in-
appropriate to present a side-by-side comparison 
between Mani?s work and ours (F = 96.5%), be-
cause there are many differences, such as the 
data that was used for evaluation. 
2.2 Subsyllable Units 
Transliteration is usually based on the way 
names are pronounced.4 However, each character 
in a Roman name generally corresponds to a sin-
gle phoneme, while a Chinese character (CC) 
generally corresponds to a subsyllable unit 
(SSU). A phoneme is the smallest meaningful 
unit of sound, and a subsyllable unit is a se-
quence of one to three phonemes that conform to 
the following three constraints. (Wan and Vers-
poor, 1998) 
                                                 
4  Of course, there are exceptions. For example, when a 
name happens to be a word, sometimes that name is trans-
lated (rather than transliterated) into the other language. 
But our experimental results suggest that the exceptions 
are quite rare. 
(1) There is exactly one vowel phoneme.5 
(2) At most, one consonant phoneme may pre-
cede the vowel phoneme. 
(3) The vowel phoneme may be followed by, at 
most, one nasal phoneme.6 
Consider the example in Table 1. The name 
?Albertson? consists of eight phonemes in three 
syllables.7 The last syllable, SAHN, satisfies the 
definition of SSU, and the other two are split into 
smaller pieces, resulting in a total of five SSUs. 
There are also five CCs in the Chinese version,  
?????. We note that the fourth and sixth rows 
in the table show similarities in their pronuncia-
tions. For example, the first SSU, AE, sounds 
like the first CC, /a/. And, although the sounds 
are not always identical, such as BER and /pei/, 
Wan and Verspoor claimed that these SSU-CC 
correspondences can be generalized in the fol-
lowing way: 
Perfect Alignment (PA) hypothesis 
If a Roman name corresponds to a sequence of n 
SSUs, S1, S2, ..., Sn, and the Chinese form of that 
name is a sequence of n CCs, C1, C2, ..., Cn, then 
Ci matches Si for all 1 ? i ? n. 
In Section 4, we show that the PA hypothesis 
works very well. However, it is not uncommon 
to have more SSUs than CCs in a matching name 
pair, in which case, the PA hypothesis does not 
apply. Often this happens because an SSU is left 
out of the Chinese transliteration, perhaps be-
cause it is a sound that is not common in Chi-
nese. For example, suppose ?Carlberg? (KAA, 
R,L,BER,G) is transliterated as ???? . In 
this example, the SSU, R, does not corres-
pond to any of the CCs. We generalize this 
phenomenon with another hypothesis:  
SSUs Deletion (SSUD) hypothesis 
If a Roman name corresponds to a sequence of 
n+k  SSUs (k>0), S1, S2, ..., Sn+k, and the Chinese 
form of that name is a sequence of n CCs, C1, C2, 
..., Cn, then, for some set of k Si?s, if those SSUs 
are removed from the sequence of SSUs, then the 
PA hypothesis holds. 
And in the case where the number of CCs is 
greater than the number of SSUs, we make the 
                                                 
5 Wan and Verspoor treat the phoneme, /?r/, as in Albertson, 
as a vowel phoneme. 
6 The nasal phonemes are /n/ and /?/, as in ?nothing?. 
7 To represent phonemes, we use two different standards in 
this paper. The symbols between slashes (like /?r/) are in 
the IPA format (International Phonetic Association, 
1999). And the phonemes written in capital letters (like 
ER) are in the ARPABET format (Klatt, 1990). 
Roman Characters: Albertson 
Phonemes: AE,L,B,ER,T,S,AH,N 
Syllables: AEL,BERT,SAHN 
Subsyllable Units: AE,L,BER,T,SAHN 
Chinese: ????? 
Chinese Phonemes: /a/,/?r/,/pei/,/t
h?/,/su?/ 
Table 1. Subsyllable Units 
153
corresponding CCs Deletion (CCD) hypothesis. 
In the next section, we show how we utilize these 
hypotheses. 
 
3 Machine Learning 
We designed a machine learning algorithm to 
establish a mapping between SSUs and CCs. In 
Section 3.1, we show how our system can do 
Roman-Chinese name matching, and then we 
present the training procedure in Section 3.2. 
3.1 Application Mode 
Given a Roman-Chinese name pair, our system 
computes a match score, which is a number be-
tween 0 and 1 that is meant to represent the like-
lihood that two names match.  This is accom-
plished via the process presented in Figure 1. 
Starting in the upper-left node of the diagram 
with a Roman name and a Chinese name, the 
system determines how the Roman name should 
be pronounced by running it through the Festival 
system. (Black et al, 1999) Next, two algorithms 
designed by Wan and Verspoor (1998) join the 
phonemes to form syllables and divide the syl-
lables into SSUs.8 If the number of SSUs is equal 
to the number of characters in the Chinese 
name,9 we apply the PA hypothesis to align each 
SSU with a CC.  
The system computes a match score using a 
data structure called the SSU-CC matrix (subsyl-
lable unit ? Chinese character matrix), which has 
a nonnegative number for each SSU-CC pair, 
and this value should represent the strength of 
the correspondence between the SSU and the 
CC. Table 2 shows an example of an SSU-CC 
matrix. With this matrix, the name pair <Albert, 
????> receives a relatively high match score,  
because the SSUs in Albert are AE, L, BER, and 
T, and the numbers in the SSU-CC matrix for 
<AE,?>, <L,?>, <BER,?> and <T,?> are 2, 2, 
3, and 2, respectively.10 Alternatively, the system 
assigns a very low match score to <Albert,         
????>, because the values of <AE,?>, <L,?>, 
<BER,?>, and <T,?> are all 0. 
3.2 Training Mode 
To generate an SSU-CC matrix, we train our sys-
tem on a corpus of Roman-Chinese name pairs 
                                                 
8  This procedure passes through three separate modules, 
each of which introduces errors, so we would expect the 
system to suffer from compounding errors. However, the 
excellent evaluation results in Section 4 suggest  other-
wise. This may be because the system encounters the 
same kinds of errors during training that it sees in the ap-
plication mode, so perhaps it can learn to compensate for 
them. 
9 Section 3.3 discusses the procedure used when these num-
bers are not equal. 
10 The equation used to derive the match score from these 
values can be found in Section 5. 
 
Figure 2. Training Mode 
 
Figure 1. Application Mode 
 
A 
E 
B 
E 
R 
E 
H G 
K 
A 
A L 
L 
A 
H 
N 
L 
I 
Y 
N 
A 
H R 
S 
A 
H 
N T 
? 0 0 0 0 0 0 1 0 0 0 0 0 
? 0 0 0 0 0 0 0 1 0 0 0 0 
? 0 0 0 0 1 0 0 0 0 0 0 0 
? 0 0 1 0 0 0 0 0 0 0 0 0 
? 0 0 1 0 0 0 0 0 0 0 0 0 
? 0 0 0 0 0 0 0 0 1 0 0 0 
? 0 0 0 0 0 2 0 0 0 1 0 0 
? 0 0 0 0 0 0 0 0 0 0 1 0 
? 0 0 0 0 0 0 0 0 0 0 0 2 
? 0 3 0 0 0 0 0 0 0 0 0 0 
? 0 0 0 0 0 0 1 0 0 0 0 0 
? 0 0 0 1 0 0 0 0 0 0 0 0 
? 2 0 0 0 0 0 0 0 0 0 0 0 
Table 2. SSU-CC Matrix #1 
 
154
that match. Figure 2 shows a diagram of the 
training system. The procedure for transforming 
the Roman name into a sequence of SSUs is 
identical to that presented in Section 3.1. Then, if 
the number of SSUs is the same as the number of 
CCs,9 we apply the PA hypothesis to pair the 
SSUs with the CCs. For example, the third name 
pair in Table 3 has three SSU-CC pairs: <KAA,
?>, <R,?>, and <LIY,?>. So the system mod-
ifies the SSU-CC matrix by adding 1 to each cell 
that corresponds to one of these SSU-CC pairs. 
Training on the five name pairs in Table 3 pro-
duces the SSU-CC matrix in Table 2. 
3.3 Imperfect Alignment 
The system makes two passes through the train-
ing data. In the first pass, whenever the PA hypo-
thesis does not apply to a name pair (because the 
number of SSUs differs from the number of 
CCs), that name pair is skipped.  
Then, in the second pass, the system builds 
another SSU-CC matrix. The procedure for 
processing each name pair that satisfies the PA 
hypothesis?s condition is exactly the same as in 
the first pass (Section 3.2). But the other name 
pairs require the SSUD hypothesis or the CCD 
hypothesis to delete SSUs or CCs. For a given 
Roman-Chinese name pair:  
where D is the set of all deletion sets that make 
the PA hypothesis applicable. Note that the size 
of D grows exponentially as the difference be-
tween the number of SSUs and CCs grows. 
As an example, consider adding the name pair 
<Carlberg, ????> to the data in Table 3. Carl-
berg has five SSUs: KAA,R,L,BER,G, but ???-
? has only four CCs. So the PA hypothesis is not 
applicable, and the system ignores this name pair 
in the first pass. Table 2 shows the values in Ma-
trix #1 when it is completed. 
In the second pass, we must apply the SSUD 
hypothesis to <Carlberg, ????> by deleting 
one of the SSUs. There are five ways to do this, 
as shown in the five rows of Table 4. (For in-
stance, the last row represents the case where G 
is deleted ? the SSU-CC pairs are <KAA,?>, 
<R,?>, <L,?>, <BER,?>, and <G,?>.11) 
Each of the five options are evaluated using 
the values in Matrix #1 (Table 2) to produce the 
scores in the second column of Table 4. Then the 
                                                 
11 The ? represents a deleted SSU. We include a row and 
column named ? in Matrix #2 to record values for the 
cases in which the SSUs and CCs are deleted. 
For every d in D: 
Temporarily make the deletions in d. 
Evaluate the resulting name pair with Matrix #1. 
Scale the evaluation scores of the d?s to sum to 1. 
For every d in D: 
Temporarily make the deletions in d. 
For every SSU-CC pair, ssu-cc, in the result: 
Add d?s scaled score to cell [ssu,cc] in Matrix #2. 
Example # 1 2 3 4 5 
Roman 
Characters 
Albert Albertson Carly Elena Ellenberg 
Subsyllable 
Units 
AE,L,BER,T AE,L,BER,T,SAHN KAA,R,LIY EH,LAHN,NAH EH,LAHN,BER,G 
Chinese 
Characters 
???? ????? ??? ??? ???? 
Table 3. Training Data 
CCs Score Scaled Score 
????? 0.00 0.00 
????? 0.90 0.54 
????? 0.76 0.46 
????? 0.00 0.00 
????? 0.00 0.00 
Table 4. Subsyllable Unit Deletion 
 
 
? 
B 
E 
R G 
K 
A 
A L R ... 
?  0.00 0.00 0.00 0.46 0.54  
? 0.00 0.00 0.00 2.00 0.00 0.00  
? 0.00 0.00 0.00 0.00 2.54 1.46  
? 0.00 4.00 0.00 0.00 0.00 0.00  
? 0.00 0.00 2.00 0.00 0.00 0.00  
...        
Table 5. SSU-CC Matrix #2 
 
155
system scales the scores to sum to 1, as shown in 
the third column, and it uses those values as 
weights to determine how much impact each of 
the five options has on the second matrix. Table 
5 shows part of Matrix #2. 
In application mode, when the system encoun-
ters a name pair that does not satisfy the PA hy-
pothesis?s condition it tries all possible deletion 
sets and selects the one that produces the highest 
match score. 
3.4 Considering Context 
It might be easier to estimate the likelihood that 
an SSU-CC pair is a match by using information 
found in surrounding SSU-CC pairs, such as the 
SSU that follows a given SSU-CC pair. We do 
this by increasing the number of columns in the 
SSU-CC matrix to separate the examples based 
on the surrounding context. 
For example, in Table 2, we cannot determine 
whether LAHN should map to ? or ?. But the 
SSU that follows LAHN clears up the ambiguity, 
because when LAHN immediately precedes 
BER, it maps to  ?, but when it is followed by 
NAH, it corresponds to ?. Table 6 displays a 
portion of the SSU-CC matrix that accounts for 
the contextual information provided by the SSU 
that follows an SSU-CC pair. 
3.5 The Threshold 
Given an SSU-CC name pair, the system produc-
es a number between 0 and 1. But in order to 
evaluate the system in terms of precision, recall, 
and F-score, we need the system to return a yes 
(a match) or no (not a match) response. So we 
use a threshold value to separate those two cases.  
The threshold value can be manually selected 
by a human, but this is often difficult to do effec-
tively. So we developed the following automated 
approach to choose the threshold. After the train-
ing phase finishes developing Matrix #2, the sys-
tem processes the training data12 one more time. 
                                                 
12 We tried selecting the threshold with data that was not 
used in training, and we found no statistically significant 
improvement. 
But this time it runs in application mode (Section 
3.1), computing a match score for each training 
example. Then the system considers all possible 
ways to separate the yes and no responses with a 
threshold, selecting the threshold value that is the 
most effective on the training data. 
Building the SSU-CC matrices does not re-
quire any negative examples (name pairs that do 
not match). However, we do require negative 
examples in order to determine the threshold and 
to evaluate the system. Our technique for gene-
rating negative examples involves randomly 
rearranging the names in the data.13 
 
4 Evaluation of the System 
We ran several experiments to test our system 
under a variety of different conditions. After de-
scribing our data and experimental method, we 
present some of our most interesting experimen-
tal results. 
We used a set of nearly 500,000 Roman-
Chinese person name pairs collected from Xin-
hua News Agency newswire texts. (Huang, 
2005) Table 7 shows the distribution of the data 
based on alignment. Note that the PA hypothesis 
applies to more than 60% of the data. 
We used the popular 10-fold cross validation 
approach 14  to obtain ten different evaluation 
scores. For each experiment we present the aver-
age of these scores. 
Our system?s precision (P), recall (R), and F-
score (F) are: P = 98.19%, R = 94.83%, and F = 
96.48%. These scores are much better than we 
originally expected to see for the challenging 
task of Roman-Chinese name matching.  
Table 8 shows P, R, and F for subsets of the 
test data, organized by the number of SSUs mi-
                                                 
13 Unfortunately, there is no standard way to generate nega-
tive examples. 
14 The data is divided into ten subsets of approximately the 
same size, testing the system on each subset when trained 
on the other nine. 
 
LAHN 
(BER) 
LAHN 
(NAH) 
BER 
(G) 
BER 
(T) 
? 1 0 0 0 
? 0 0 1 2 
? 0 1 0 0 
Table 6. Considering Context 
 
Alignment % of Data 
#SSUs - #CCs ? 3 1.62% 
#SSUs - #CCs = 2 6.66% 
#SSUs - #CCs = 1 20.00% 
#SSUs - #CCs = 0 60.60% 
#SSUs - #CCs = -1 10.48% 
#SSUs - #CCs = -2 0.61% 
#SSUs - #CCs ? -3 0.02% 
Table 7. Statistics of the Data 
156
nus the number of CCs in the name pairs. The 
differences between scores in adjacent rows of 
each column are statistically significant.15  Per-
fectly aligned name pairs proved to be the ea-
siest, with F = 97.55%, but the system was also 
very successful on the examples with the number 
of SSUs and the number of CCs differing by one 
(F = 96.08% and F = 97.37%). These three cases 
account for more than 91% of the positive exam-
ples in our data set. (See Table 7.) 
4.1 Deletion Hypotheses 
We ran tests to determine whether the second 
pass through the training data (in which the 
SSUD and CCD hypotheses are applied) is effec-
tive. Table 9 shows the results on the complete 
set of test data, and all of the differences between 
the scores are statistically significant.  
The first row of Table 9 presents F when the 
system made only one pass through the training 
data. The second row?s experiments utilized the 
CCD hypothesis but ignored examples with more 
SSUs than CCs during training. For the third 
row, we used the SSUD hypothesis, but not the 
CCD hypothesis, and the last row corresponds to 
system runs that used all of the training exam-
ples. From these results, it is clear that both of 
the deletion hypotheses are useful, particularly 
the SSUD hypothesis. 
4.2 Context 
In Section 3.4, we suggested that contextual in-
formation might be useful. So we ran some tests, 
obtaining the results shown in Table 10. For the 
second row, we used no contextual information. 
Row 5 corresponds to the case where we gave 
the system access to the SSU immediately fol-
lowing the SSU-CC pair being analyzed. In row 
                                                 
15 We use the homoscedastic t test (?Student?s t Test?, 2009) 
to decide whether the difference between two results is 
statistically significant. 
6?s experiment, we used the SSU immediately 
preceding the SSU-CC pair under consideration, 
and row 7 corresponds to system runs that ac-
counted for both surrounding SSUs. 
We also tried simplifying the contextual in-
formation to boolean values that specify whether 
an SSU-CC pair is at a boundary of its name or 
not, and rows 1, 3, and 4 of Table 10 show those 
results. ?Left Border? is true if and only if the 
SSU-CC pair is at the beginning of its name, 
?Right Border? is true if and only if the SSU-CC 
pair is at the end of its name, and ?Both Borders? 
is true if and only if the SSU-CC pair is at the 
beginning or end of its name. All differences in 
the table are statistically significant, except for 
those between rows 2, 3, and 4. These results 
suggest that the right border provides no useful 
information, even if the left border is also in-
cluded in the SSU-CC matrix. But when the 
SSU-CC matrix only accounted for the left bor-
der, the F-score was significantly higher than the 
baseline. Providing more specific information in 
the form of SSUs actually made the scores go 
down significantly. 
4.3 Sparse Data 
We were initially surprised to discover that using 
the rich information in the surrounding SSUs 
made the results worse. The explanation for this 
is that adding contextual information increases 
the size of the SSU-CC matrix, and so several of 
the numbers in the matrix become smaller. (For 
example, compare the values in the ?BER? col-
umns in Table 2 and Table 6.) This means that 
the system might have been suffering from a 
sparse data problem, which is a situation where 
there are not enough training examples to distin-
guish correct answers from incorrect answers, 
and so incorrect answers can appear to be correct 
by random chance.  
There are two factors that can contribute to a 
sparse data problem. One is the amount of train-
ing data available ? as the quantity of training 
data increases, the sparse data problem becomes 
less severe. The other factor is the complexity of 
Alignment P R F 
#SSUs - #CCs ? 3 72.38% 94.02% 81.79% 
#SSUs - #CCs = 2 95.26% 92.67% 93.95% 
#SSUs - #CCs = 1 99.07% 93.27% 96.08% 
#SSUs - #CCs = 0 99.87% 95.33% 97.55% 
#SSUs - #CCs = -1 98.33% 96.42% 97.37% 
#SSUs - #CCs = -2 73.80% 94.98% 83.04% 
#SSUs - #CCs ? -3 7.54% 78.04% 13.71% 
Table 8. Varying Alignment of Name Pairs 
# Contextual Information F 
1 Left Border 96.48% 
2 No Context 96.25% 
3 Both Borders 96.24% 
4 Right Border 96.19% 
5 Next SSU 87.53% 
6 Previous SSU 85.89% 
7 Previous SSU and Next SSU 47.89% 
Table 10. Evaluation with Context 
Hypotheses F 
PA 75.25% 
PA & CCD 83.74% 
PA & SSUD 92.86% 
PA & CCD & SSUD 96.48% 
Table 9. Varying the Training Data 
 
157
the learned model ? as the model becomes more 
complex, the sparse data problem worsens. 
Our system?s model is the SSU-CC matrix, 
and a reasonable measure of the its complexity is 
the number of entries in the matrix. The second 
column of Table 11 shows the number of SSU-
CC pairs in training divided by the number of 
cells in the SSU-CC matrix. These ratios are 
quite low, suggesting that there is a sparse data 
problem. Even without using any context, there 
are nearly 8 cells for each SSU-CC pair, on aver-
age.16  
It might be more reasonable to ignore cells 
with extremely low values, since we can assume 
that these values are effectively zero. The third 
column of Table 11 only counts cells that have 
values above 10-7. The numbers in that column 
look better, as the ratio of cells to training pairs 
is better than 1:4 when no context is used. How-
ever, when using the previous SSU, there are still 
more cells than training pairs.  
Another standard way to test for sparse data is 
to compare the system?s results as a function of 
the quantity of training data. As the amount of 
training data increases, we expect the F-score to 
rise, until there is so much training data that the 
F-score is at its optimal value.17 Figure 3 shows 
the results of all of the context experiments that 
we ran, varying the amount of training data. 
(90% of the training data was used to get the F-
scores in Table 10.) The t test tells us that ?No 
Context? is the only curve that does not increase 
significantly on the right end. This suggests that 
all of the other curves might continue increasing 
if we used more training data. So even the ?Both 
SSUs? case could potentially achieve a competi-
tive score, given enough training examples. Also, 
                                                 
16 It is true that a name pair can have multiple SSU-CC 
pairs, but even if the average number of SSU-CC pairs per 
name pair is as high as 8 (and it is not), one training name 
pair per SSU-CC matrix cell is still insufficient. 
17 Note that this value may not be 100%, because there are 
factors that can make perfection difficult to achieve, such 
as errors in the data. 
more training data could produce higher scores 
than 96.48%. 
5 Summary 
We designed a system that achieved an F-score 
of 96.48%, and F = 97.55% on the 60.61% of the 
data that satisfies the PA hypothesis?s condition.  
Due to the paper length restriction, we can on-
ly provide short summaries of the other experi-
ments that that we ran. 
1) We experimentally compared six different 
equations for computing match scores and 
found that the best of them is an arithmetic 
or geometric average of Prob(SSU|CC) and 
Prob(CC|SSU).  
2) We attempted to make use of two simple 
handcrafted rules, but they caused the sys-
tem?s performance to drop significantly. 
3) We compared two approaches for automati-
cally computing the pronunciation of a Ro-
man name and found that using the Festival 
system (Black et al, 1999) alone is just as ef-
fective as using the CMU Pronunciation Dic-
tionary (CMUdict, 1997) supplemented by 
Festival. 
4) We tried computing the threshold value with 
data that was not used in training the system. 
However, this failed to improve the system?s 
performance significantly. 
 
6 Future Work 
There are so many things that we still want to do, 
including: 
1. modifying our system for the task of 
transliteration (Section 6.1),  
2. running fair comparisons between our 
work and related research, 
3. using Levenshtein?s algorithm (Levensh-
tein, 1966) to implement the SSUD and 
Contextual Info. All Cells  Cells > 10-7  
No Context 0.128 4.35 
Right Border 0.071 3.45 
Left Border 0.069 3.45 
Both Borders 0.040 3.13 
Next SSU 0.002 1.12 
Previous SSU 0.001 0.78 
Both SSUs far less far less 
Table 11. Num. SSU-CC Pairs  per Matrix Cell 
 
Figure 3. Testing for Sparse Data 
 
  
40%
50%
60%
70%
80%
90%
100%
10% 20% 30% 40% 50% 60% 70% 80% 90%
F
-S
c
o
re
Training Set Size (% of available data)
Left Border Next SSU
No Context Previous SSU
Right Border Both SSUs
Both Borders
158
CCD hypotheses, instead of exhaustively 
evaluating all possible deletion sets (Sec-
tion 3.3),18  
4. developing a standard methodology for 
creating negative examples,  
5. when using contextual information, split-
ting rows or columns of the SSU-CC 
matrix only when they are ambiguous 
according to a metric such as Informa-
tion Gain (Section 3.4),19 
6. combining our system with other Ro-
man-Chinese name matching systems in 
a voting structure (Van Halteren, Zavrel, 
and Daelemans, 1998), 
7. independently evaluating the modules 
that determine pronunciation, construct 
syllables, and separate subsyllable units 
(Section 3),  
8. converting phonemes into feature vectors 
(Aberdeen, 2006),  
9. modifying our methodology to apply it 
to other similar languages, such as Japa-
nese, Korean, Vietnamese, and Ha-
waiian.  
10. manually creating rules based on infor-
mation in the SSU-CC matrix, and  
11. utilizing graphemic information. 
6.1 Transliteration 
We would like to modify our system to enable 
it to transliterate a given Roman name into Chi-
nese in the following way. First, the system 
computes the SSUs as in Section 3.1. Then it 
produces a match score for every possible se-
quence of CCs that has the same length as the 
sequence of SSUs, returning all of the CC se-
quences with match scores that satisfy a prede-
termined threshold restriction. 
For example, in a preliminary experiment, 
given the Roman name Ellen, the matcher pro-
duced the transliterations below, with the match 
scores in parentheses.20 
 ? ?  (0.32) 
 ? ?  (0.14) 
 ? ?  (0.11)  
 ? ?  (0.05) 
                                                 
18 We thank a reviewer for suggesting this method of im-
proving efficiency. 
19 We thank a reviewer for this clever way to control the 
size of the SSU-CC matrix when context is considered. 
20 A manually-set threshold of 0.05 was used in this experi-
ment. 
Based on our data, the first and fourth results 
are true transliterations of Ellen, and the only 
true transliteration that failed to make the list is 
??. 
 
7 Conclusion 
There was a time when computational linguistics 
research rarely used machine learning. Research-
ers developed programs and then showed how 
they could successfully handle a few examples, 
knowing that their programs were unable to ge-
neralize much further. Then the language com-
munity became aware of the advantages of ma-
chine learning, and statistical systems almost 
completely took over the field. Researchers 
solved all kinds of problems by tapping into the 
computer?s power to process huge corpora of 
data. But eventually, the machine learning sys-
tems reached their limits. 
We believe that, in the future, the most suc-
cessful systems will be those developed by 
people cooperating with machines. Such systems 
can solve problems by combining the computer?s 
ability to process massive quantities of data with 
the human?s ability to intuitively come up with 
new ideas. 
Our system is a success story of human-
computer cooperation. The computer tirelessly 
processes hundreds of thousands of training ex-
amples to generate the SSU-CC matrix. But it 
cannot work at all without the insights of Wan 
and Verspoor. And together, they made a system 
that is successful more than 96% of the time. 
 
References 
Aberdeen, J. (2006) ?geometric-featurechart-jsa-
20060616.xls?. Unpublished. 
Andrade, Miguel. Smith, S. Paul. Cowlisha, Mike F. 
Gantner, Zeno. O?Brien, Philip. Farmbrough, Rich. 
et al ?F1 Score.? (2009) Wikipedia: The Free En-
cyclopedia.  http://en.wikipedia.org/wiki/F-score. 
Black, Alan W. Taylor, Paul. Caley, Richard. (1999) 
The Festival Speech Synthesis System: System Do-
cumentation. Centre for Speech Technology Re-
search (CSTR). The University of Edinburgh. 
http://www.cstr.ed.ac.uk/projects/festival/manual 
CMUdict. (1997) The CMU Pronouncing Dictionary. 
v0.6. The Carnegie Mellon Speech Group. 
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.  
Cohen, W. Ravikumar, P. Fienberg, S. (2003) ?A 
Comparison of String Distance Metrics for Name-
159
Matching Tasks.? Proceedings of the IJCAI-03 
Workshop on Information Integration on the Web. 
Eds. Kambhampati, S. Knoblock, C. 73-78. 
Condon, Sherri. Aberdeen, John. Albin, Matthew. 
Freeman, Andy. Mani, Inderjeet. Rubenstein, Alan. 
Sarver, Keri. Sexton, Mike. Yeh, Alex. (2006) 
?Multilingual Name Matching Mid-Year Status 
Report.? 
Condon, S. Freeman, A. Rubenstein, A. Yeh, A. 
(2006) ?Strategies for Chinese Name Matching.? 
Freeman, A. Condon, S. Ackermann, C. (2006) 
"Cross Linguistic Name Matching in English and 
Arabic: A ?One to Many Mapping? Extension of 
the Levenshtein Edit Distance Algorithm." Pro-
ceedings of NAACL/HLT. 
Gao, W. Wong, K. Lam, W. (2004) ?Phoneme-Based 
Transliteration of Foreign Names for OOV Prob-
lem.? Proceedings of the First International Joint 
Conference on Natural Language Processing. 
Goto, I. Kato, N. Uratani, N. Ehara, T. (2003) ?Trans-
literation Considering Context Information Based 
on the Maximum Entropy Method.? Proceedings 
of MT-Summit IX. 
Huang, Shudong. (2005) ?LDC2005T34: Chinese <-> 
English Named Entity Lists v 1.0.? Linguistics Da-
ta Consortium. Philadelphia, Pennsylvania.  ISBN 
#1-58563-368-2. http://www.ldc.upenn.edu/Cata 
log/CatalogEntry.jsp?catalogId=LDC2005T34. 
International Phonetic Association. (1999) Handbook 
of the International Phonetic Association : A Guide 
to the Use of the International Phonetic Alphabet. 
Cambridge University Press, UK. ISBN 
0521652367. http://www.cambridge.org/uk/cata 
logue/catalogue.asp?isbn=0521652367.  
Jung, S. Hong, S. Paek, E. (2000) ?An English to Ko-
rean Transliteration Model of Extended Markov 
Window.? Proceedings of COLING. 
Kang, B.J. Choi, K.S. (2000) ?Automatic Translitera-
tion and Back-Transliteration by Decision Tree 
Learning.? Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion. 
Klatt, D.H. (1990) ?Review of the ARPA Speech Un-
derstanding Project.? Readings in Speech Recogni-
tion. Morgan Kaufmann Publishers Inc. San Fran-
cisco, CA. ISBN 1-55860-124-4.  554-575. 
Knight, K. Graehl, J. (1997) ?Machine Translitera-
tion.? Proceedings of the Conference of the Asso-
ciation for Computational Linguistics (ACL). 
Kondrak, G. (2000) ?A New Algorithm for the 
Alignment of Phonetic Sequences.? Proceedings of 
the First Meeting of the North American Chapter 
of the Association for Computational Linguistics 
(NAACL). Seattle, Washington. 288-295. 
Kondrak, G. Dorr, B. (2004) ?Identification of Con-
fusable Drug Names: A New Approach and Evalu-
ation Methodology.? Proceedings of the Twentieth 
International Conference on Computational Lin-
guistics (COLING). 952-958. 
 Levenshtein, V.I. (1966) ?Binary Codes Capable of 
Correcting Deletions, Insertions and Reversals.? 
Sov. Phys. Dokl. 6. 707-710. 
Li, H. Zhang, M. Su, J. (2004) ?A Joint Source-
Channel Model for Machine Transliteration.? Pro-
ceedings of ACL 2004. 
Mani, Inderjeet. Yeh, Alexander. Condon, Sherri. 
(2006) "Machine Learning from String Edit Dis-
tance and Phonological Similarity." 
Meng, H. Lo, W. Chen, B. Tang, T. (2001) ?Generat-
ing Phonetic Cognates to Handle Named Entities in 
English-Chinese Cross-Language Spoken Docu-
ment Retrieval.? Proceedings of ASRU. 
Oh, Jong-Hoon. Choi, Key-Sun. (2006) ?An Ensem-
ble of Transliteration Models for Information Re-
trieval.? Information Processing & Management. 
42(4). 980-1002. 
 ?Student?s t Test.? (2009) Wikipedia: The Free En-
cyclopedia. http://en.wikipedia.org/wiki/T_test# 
Equal_sample_sizes.2C_equal_variance. 
Van Halteren, H., Zavrel, J. Daelemans, W. (1998) 
?Improving Data Driven Word-Class Tagging by 
System Combination.? Proceedings of the 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Con-
ference on Computational Linguistics. Montr?al, 
Qu?bec, Canada. 491-497. 
Virga, P. Khudanpur, S. (2003) ?Transliteration of 
Proper Names in Cross-Lingual Information Re-
trieval.? Proceedings of the ACL Workshop on 
Multi-lingual Named Entity Recognition. 
Wan, Stephen. Verspoor, Cornelia Maria. (1998). 
"Automatic English-Chinese Name Transliteration 
for Development of Multilingual Resources." Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics. Montr?al, 
Qu?bec, Canada.                     
Wellner, B. Castano, J. Pustejovsky, J. (2005) ?Adap-
tive String Similarity Metrics for Biomedical Ref-
erence Resolution.? Proceedings of the ACL-ISMB 
Workshop on Linking Biological Literature, Ontol-
ogies, and Databases: Mining Biological Seman-
tics. 9-16. http://www.cs.brandeis.edu/~wellner/ 
pubs/Wellner-StringSim-BioLINK.pdf. 
Winkler, W. ?Methods for Record Linkage and Baye-
sian Networks.? (2002) Proceedings of the Section 
on Survey Research Methods, American Statistical 
Association. http://www.census.gov/srd/www/ 
byyear.html. 
160
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 2?9
Manchester, August 2008
Learning to Match Names Across Languages 
Inderjeet Mani 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730, USA 
imani@mitre.org 
Alex Yeh 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730, USA 
asy@mitre.org 
Sherri Condon 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102, USA 
scondon@mitre.org 
 
Abstract 
We report on research on matching 
names in different scripts across languag-
es. We explore two trainable approaches 
based on comparing pronunciations. The 
first, a cross-lingual approach, uses an 
automatic name-matching program that 
exploits rules based on phonological 
comparisons of the two languages carried 
out by humans. The second, monolingual 
approach, relies only on automatic com-
parison of the phonological representa-
tions of each pair. Alignments produced 
by each approach are fed to a machine 
learning algorithm. Results show that the 
monolingual approach results in ma-
chine-learning based comparison of per-
son-names in English and Chinese at an 
accuracy of over 97.0 F-measure. 
1 Introduction 
The problem of matching pairs of names which 
may have different spellings or segmentation 
arises in a variety of common settings, including 
integration or linking database records, mapping 
from text to structured data (e.g., phonebooks, 
gazetteers, and biological databases), and text to 
text comparison (for information retrieval, 
clustering, summarization, coreference, etc.).  
For named entity recognition, a name from a 
gazetteer or dictionary may be matched against 
text input; even within monolingual applications, 
the forms of these names might differ. In multi-
document summarization, a name may have 
different forms across different sources. Systems 
                                                 
? 2008 The MITRE Corporation.  All rights reserved. Licensed for 
use in the proceedings of the Workshop on Multi-source, Multilin-
gual Information Extraction and Summarization (MIMIES2) at 
COLING?2008. 
that address this problem must be able to handle 
variant spellings, as well as abbreviations, 
missing or additional name parts, and different 
orderings of name parts.  
In multilingual settings, where the names 
being compared can occur in different scripts in 
different languages, the problem becomes 
relevant to additional practical applications, 
including both multilingual information retrieval 
and machine translation. Here special challenges 
are posed by the fact that there usually aren?t 
one-to-one correspondences between sounds 
across languages. Thus the name Stewart, 
pronounced   / s t u w ? r t / in IPA, can be 
mapped to Mandarin ????? ?, which is 
Pinyin ?si tu er te?, pronounced /s i t? u a ? t? e/, 
and the name Elizabeth / I l I z ? b ? ?/ can map 
to ??????, which is Pinyin ?yi li sha bai?, 
pronounced /I l I ? ? p aI/. Further, in a given 
writing system, there may not be a one-to-one 
correspondence between orthography and sound, 
a well-known case in point being English. In 
addition, there may be a variety of variant forms, 
including dialectical variants, (e.g., Bourguiba 
can map to Abu Ruqayba), orthographic 
conventions (e.g., Anglophone Wasim can map 
to Francophone Ouassime), and differences in 
name segmentation (Abd Al Rahman can map to 
Abdurrahman).  Given the high degree of 
variation and noise in the data, approaches based 
on machine learning are needed. 
The considerable differences in possible 
spellings of a name also call for approaches 
which can compare names based on 
pronunciation. Recent work has developed 
pronunciation-based models for name 
comparison, e.g., (Sproat, Tao and Zhai 2006) 
(Tao et al 2006). This paper explores trainable 
pronunciation-based models further.  
2
Table 1: Matching ?Ashburton? and ?????? 
Consider the problem of matching Chinese 
script names against their English (Pinyin) Ro-
manizations. Chinese script has nearly 50,000 
characters in all, with around 5,000 characters in 
use by the well-educated. However, there are 
only about 1,600 Pinyin syllables when tones are 
counted, and as few as 400 when they aren?t. 
This results in multiple Chinese script represen-
tations for a given Roman form name and many 
Chinese characters that map to the same Pinyin 
forms. In addition, one can find multiple Roman 
forms for many names in Chinese script, and 
multiple Pinyin representations for a Chinese 
script representation.  
In developing a multilingual approach that can 
match names from any pair of languages, we 
compare an approach that relies strictly on mo-
nolingual knowledge for each language, specifi-
cally, grapheme-to-phoneme rules for each lan-
guage, with a method that relies on cross-lingual 
rules which in effect map between graphemic 
and/or phonemic representations for the specific 
pair of languages.  
The monolingual approach requires finding 
data on the phonemic representations of a name 
in a given language, which (as we describe in 
Section 4) may be harder than finding more 
graphemic representations. But once the 
phonemic representation is found for names in a 
given language, then as one adds more languages 
to a system, no more work needs to be done in 
that given language. In contrast, with the cross-
lingual approach, whenever a new language is 
added, one needs to  go over all the existing 
languages already in the system and compare 
each of them with the new language to develop 
cross-lingual rules for each such language pair. 
The engineering of such rules requires bilingual 
expertise, and knowledge of differences between 
language pairs. The cross-lingual approach is 
thus more expensive to develop, especially for 
applications which require coverage of a large 
number of languages. 
Our paper investigates whether we can address 
the name-matching problem without requiring 
such a knowledge-rich approach, by carrying out 
a comparison of the performance of the two 
approaches. We present results of large-scale 
machine-learning for matching personal names 
in Chinese and English, along with some 
preliminary results for English and Urdu. 
2 Basic Approaches 
2.1 Cross-Lingual Approach 
Our cross-lingual approach (called MLEV) is 
based on (Freeman et al 2006), who used a 
modified Levenshtein string edit-distance 
algorithm to match Arabic script person names 
against their corresponding English versions. The 
Levenshtein edit-distance algorithm counts the 
minimum number of insertions, deletions or 
substitutions required to make a pair of strings  
match. Freeman et al (2006) used (1) insights 
about phonological differences between the two  
languages to create rules for equivalence classes 
of characters that are treated as identical in the 
computation of edit-distance and (2) the use of 
normalization rules applied to the English and 
transliterated Arabic names based on mappings 
between characters in the respective writing 
systems. For example, characters corresponding 
to low diphthongs in English are normalized as 
?w?, the transliteration for the Arabic 
???character, while high diphthongs are mapped 
to ?y?, the transliteration for the Arabic ??? 
character.   
Table 1 shows the representation and 
comparison of a Roman-Chinese name pair 
(shown in the title) obtained from the Linguistic 
Data Consortium?s LDC Chinese-English name 
pairs corpus (LDC 2005T34). This corpus 
provides name part pairs, the first element in 
English (Roman characters) and the second in 
Chinese characters, created by the LDC from 
Xinhua Newswire's proper name and who's who 
databases. The name part can be a first, middle 
or last name. We compare the English form of 
the name with a Pinyin Romanization of the 
Chinese. (Since the Chinese is being compared 
with English, which is toneless, the tone part of 
Pinyin is being ignored throughout this paper.) 
For this study, the Levenshtein edit-distance 
score (where a perfect match scores zero) is 
 Roman Chinese (Pinyin) Alignment Score 
LEV ashburton ashenbodu |   a   s   h   b   u   r   t   o   n   | 
|   a   s   h   e   n   b  o  d    u   | 
0.67 
MLEV ashburton ashenbodu |  a   s   h   -   -   b   u   r    t   o   n  | 
|  a   s   h   e   n   b   o   -   d   u   -  | 
0.72 
MALINE asVburton aseCnpotu |   a   sV  -   b   <   u   r   t   o   |   n 
|   a   s   eC  n   p   o   -   t   u   |   - 
0.48 
3
normalized to a similarity score as in (Freeman et 
al. 2006), where the score ranges from 0 to 1, 
with 1 being a perfect match. This edit-distance 
score is shown in the LEV row. 
The MLEV row, under the Chinese Name 
column, shows an ?Englishized? normalization 
of the Pinyin for Ashburton. Certain characters or 
character sequences in Pinyin are pronounced 
differently than in English. We therefore apply 
certain transforms to the Pinyin; for example, the 
following substitutions are applied at the start of 
a Pinyin syllable, which makes it easier for an 
English speaker to see how to pronounce it and 
renders the Pinyin more similar to English 
orthography: ?u:? (umlaut ?u?) => ?u?, ?zh? => 
?j?, ?c? => ?ts?, and ?q? => ?ch? (so the Pinyin 
?Qian? is more or less pronounced as if it were 
spelled as ?Chian?, etc.). The MLEV algorithm 
uses equivalence classes that allow ?o? and ?u? 
to match, which results in a higher score than the 
generic score using the LEV method.  
2.2 Monolingual Approach 
Instead of relying on rules that require extensive 
knowledge of differences between a language 
pair2, the monolingual approach first builds pho-
nemic representations for each name, and then 
aligns them. Earlier research by (Kondrak 2000) 
used dynamic programming to align strings of 
phonemes, representing the phonemes as vectors 
of phonological features, which are associated 
with scores to produce similarity values. His 
program ALINE includes a ?skip? function in the 
alignment operations that can be exploited for 
handling epenthetic segments, and in addition to 
1:1 alignments, it also handles 1:2 and 2:1 
alignments. In this research, we made extensive 
modifications to ALINE to add the phonological 
features for languages like Chinese and Arabic 
and to normalize the similarity scores, producing 
a system called MALINE. 
In Table 1, the MALINE row3 shows that the 
English name has a palato-alveolar modification 
                                                 
                                                                         
2As (Freeman et al, 2006) point out, these insights are 
not easy to come by: ?These rules are based on first 
author Dr. Andrew Freeman?s experience with read-
ing and translating Arabic language texts for more 
than 16 years? (Freeman et al, 2006, p. 474). 
3For the MALINE row in Table 1, the ALINE docu-
mentation explains the notation as follows: ?every 
phonetic symbol is represented by a single lowercase 
letter followed by zero or more uppercase letters. The 
initial lowercase letter is the base letter most similar 
to the sound represented by the phonetic symbol. The 
remaining uppercase letters stand for the feature mod-
on the ?s? (expressed as ?sV?), so that we get the 
sound corresponding to ?sh?; the Pinyin name 
inserts a centered ?e? vowel, and devoices the 
bilabial plosive /b/ to /p/. There are actually 
sixteen different Chinese ?pinyinizations? of 
Ashburton, according to our data prepared from 
the LDC corpus.  
3 Experimental Setup 
3.1 Machine Learning Framework 
Neither of the two basic approaches described so 
far use machine learning. Our machine learning 
framework is based on learning from alignments 
produced by either approach. To view the learn-
ing problem as one amenable to a statistical clas-
sifier, we need to generate labeled feature vectors 
so that each feature vector includes an additional 
class feature that can have the value ?true? or 
?false.? Given a set of such labeled feature vec-
tors as training data, the classifier builds a model 
which is then used to classify unlabeled feature 
vectors with the right labels. 
A given set of attested name pairs constitutes a 
set of positive examples. To create negative 
pairs, we have found that randomly selecting 
elements that haven?t been paired will create 
negative examples in which the pairs of elements 
being compared are so different that they can be 
trivially separated from the positive examples. 
The experiments reported here used the MLEV 
score as a threshold to select negatives, so that 
examples below the threshold are excluded. As 
the threshold is raised, the negative examples 
should become harder to discriminate from 
positives (with the harder problems mirroring 
some of the ?confusable name? characteristics of 
the real-world name-matching problems this 
technology is aimed at). Positive examples below 
the threshold are also eliminated. Other criteria, 
including a MALINE score, could be used, but 
the MLEV scores seemed adequate for these 
preliminary experiments.  
Raising the threshold reduces the number of 
negative examples. It is highly desirable to 
balance the number of positive and negative 
examples in training, to avoid the learning being 
 
ifiers which alter the sound defined by the base letter. 
By default, the output contains the alignments togeth-
er with the overall similarity scores. The aligned sub-
sequences are delimited by '|' signs. The '<' sign signi-
fies that the previous phonetic segment has been 
aligned with two segments in the other sequence, a 
case of compression/expansion. The '-' sign denotes a 
?skip?, a case of insertion/deletion.?  
4
biased by a skewed distribution. However, when 
one starts with a balanced distribution of positive 
and negatives, and then excludes a number of 
negative examples below the threshold, a 
corresponding number of positive examples must 
also be removed to preserve the balance. Thus, 
raising the threshold reduces the size of the 
training data. Machine learning algorithms, 
however, can benefit from more training data.  
Therefore, in the experiments below, thresholds 
which provided woefully inadequate training set 
sizes were eliminated.  
One can think of both the machine learning 
method and the basic name comparison methods 
(MLEV and MALINE) as taking each pair of 
names with a known label and returning a 
system-assigned class for that pair. Precision, 
Recall, and F-Measure can be defined in an 
identical manner for both machine learning and 
basic name comparison methods. In such a 
scheme, a threshold on the similarity score is 
used to determine whether the basic comparison 
match is a positive match or not. Learning the 
best threshold for a dataset can be determined by 
searching over different values for the threshold.  
In short, the methodology employed for this 
study involves two types of thresholds: the 
MLEV threshold used to identify negative 
examples and the threshold that is applied to the 
basic comparison methods, MLEV and 
MALINE, to identify matches. To avoid 
confusion, the term negative threshold refers to 
the former, while the term positive threshold is 
used for the latter. 
The basic comparison methods were used as 
baselines in this research. To be able to provide a 
fair basic comparison score at each negative 
threshold, we ?trained? each basic comparison 
matcher at twenty different positive thresholds 
on the same training set used by the learner.  For 
each negative threshold, we picked the positive 
threshold that gave the best performance on the 
training data, and used that to score the matcher 
on the same test data as used by the learner.  
3.2 Feature Extraction 
Consider the MLEV alignment in Table 1. It can 
be seen that the first three characters are matched 
identically across both strings; after that, we get 
an ?e? inserted, an ?n? inserted, a ?b? matched 
identically, a ?u? matched to an ?o?, a ?r? de-
leted, a ?t? matched to a ?d?, an ?o? matched to a 
?u?, and an ?n? deleted. The match unigrams are 
thus ?a:a?, ?s:s?, ?h:h?, ?-:e?, ?-:n?, ?b:b?, ?u:o?, 
?r:-?, ?t:d?, ?o:u?, and ?n:-?. Match bigrams 
were generated by considering any insertion, de-
letion, and (non-identical) substitution unigram, 
and noting the unigram, if any, to its left, pre-
pending that left unigram to it (delimited by a 
comma). Thus, the match bigrams in the above 
example include ?h:h,-:e?, ?-:e,-:n?, ?b:b,u:o?, 
?u:o,r:-?, ?r:-,t:d?, ?t:d,o:u?, ?o:u,n:-?.  
These match unigram and match bigram 
features are generated from just a single MLEV 
match. The composite feature set is the union of 
the complete match unigram and bigram feature 
sets. Given the composite feature set, each match 
pair is turned into a feature vector consisting of 
the following features: string1, string2, the match 
score according to each of the basic comparison 
matchers (MLEV and MALINE), and the 
Boolean value of each feature in the composite 
feature set. 
3.3 Data Set 
Our data is a (roughly 470,000 pair) subset of the 
Chinese-English personal name pairs in LDC 
2005T34. About 150,000 of the pairs had more 
than 1 way to pronounce the English and/or Chi-
nese. For these, to keep the size of the experi-
ments manageable from the point of view of 
training the learners, one pronunciation was ran-
domly chosen as the one to use. (Even with this 
restriction, a minimum negative threshold results 
in over half a million examples). Chinese charac-
ters were mapped into Hanyu Pinyin representa-
tions, which are used for MLEV alignment and 
string comparisons.   Since the input to MALINE 
uses a phonemic representation that encodes 
phonemic features in one or more letters, both 
Pinyin and English forms were mapped into the 
MALINE notation.   
There are a number of slightly varying ways to 
map Pinyin into an international pronunciation 
system like IPA. For example, (Wikipedia 2006) 
and (Salafra 2006) have mappings that differ 
from each other and also each of these two 
sources have changed its mapping over time. We 
used a version of Salafra from 2006 (but we 
ignored the ejectives). For English, the CMU 
pronouncing dictionary (CMU 2008) provided 
phonemic representations that were then mapped 
into the MALINE notation. The dictionary had 
entries for 12% of our data set. For the names not 
in the CMU dictionary, a simple grapheme to 
phoneme script provided an approximate 
phonemic form. We did not use a monolingual 
mapping of Chinese characters (Mandarin 
pronunciation) into IPA because we did not find 
any. 
5
60
65
70
75
80
85
90
95
100
105
0 0.2 0.4 0.6 0.8
M
X
C
MB
XB
CB
Note that we could insist that all pairs in our 
dataset be distinct, requiring that there be exactly 
one match for each Roman name and exactly one 
match for each Pinyin name. This in our view is 
unrealistic, since large corpora will be skewed 
towards names which tend to occur frequently 
(e.g., international figures in news) and occur 
with multiple translations.  We included attested 
match pairs in our test corpora, regardless of the 
number of matches that were associated with a 
member of the pair. 
4 Results 
A variety of machine learning algorithms were 
tested. Results are reported, unless otherwise in-
dicated, using SVM Lite, a Support Vector Ma-
chine (SVM4) classifier5 that scales well to large 
data sets.  
Testing with SVM Lite was done with a 90/10 
train-test split. Further testing was carried out 
with the weka SMO SVM classifier, which used 
built-in cross-validation. Although the latter clas-
sifier didn?t scale to the larger data sets we used, 
it did show that cross-validation didn?t change 
the basic results for the data sets it was tried on.  
4.1 Machine Learning with Different Fea-
ture Sets 
Figure 1:  F-measure with Different Fea-
ture Sets 
Figure 1 shows the F-measure of learning for 
monolingual features (M, based on MALINE), 
cross-lingual features (X, based on MLEV), and 
a combined feature set (C) of both types of fea-
tures6 at different negative thresholds (shown on 
the horizontal axis). Baselines are shown with 
the suffix B, e.g., the basic MALINE without 
learning is MB. When using both monolingual 
and cross-lingual features (C), the baseline (CB) 
                                                 
                                                
4We used a linear kernel function in our SVM expe-
riments; using polynomial or radial basis kernels did 
not improve performance. 
5 From svmlight.joachims.org. 
6In Figure 1, the X curve is more or less under the C 
curve. 
is set to a system response of ?true? only when 
both the MALINE and MLEV baseline systems 
by themselves respond ?true?. Table 2 shows the 
number of examples at each negative threshold 
and the Precision and Recall for these methods, 
along with baselines using the basic methods 
shown in square brackets. 
The results show that the learning method (i) 
outperforms the baselines (basic methods), and 
(ii) the gap between learning and basic compari-
son widens as the problem becomes harder (i.e., 
as the threshold is raised). 
For separate monolingual and cross-lingual 
learning, the increase in accuracy of the learning 
over the baseline (non-learning) results7 was sta-
tistically significant at all negative thresholds 
except 0.6 and 0.7. For learning with combined 
monolingual and cross-lingual features (C), the 
increase over the baseline (non-learning) com-
bined results was statistically significant at all 
negative thresholds except for 0.7. 
In comparing the mono-lingual and cross-
lingual learning approaches, however, the only 
statistically significant differences were that the 
cross-lingual features were more accurate than 
the monolingual features at the 0 to 0.4 negative 
thresholds. This suggests that (iii) the mono-
lingual learning approach is as viable as the 
cross-lingual one as the problem of confusable 
names becomes harder.  
However, using the combined learning ap-
proach (C) is better than using either one. Learn-
ing accuracy with both monolingual and cross-
lingual features is statistically significantly better 
than learning with monolingual features at the 
0.0 to 0.4 negative thresholds, and better than 
learning with cross-lingual features at the 0.0 to 
0.2, and 0.4 negative thresholds. 
 
7Statistical significance between F-measures is not 
directly computable since the overall F-measure is not 
an average of the F-measures of the data samples. 
Instead, we checked the statistical significance of the 
increase in accuracy (accuracy is not shown for rea-
sons of space) due to learning over the baseline. The 
statistical significance test was done by assuming that 
the accuracy scores were binomials that were approx-
imately Gaussian. When the Gaussian approximation 
assumption failed (due to the binomial being too 
skewed), a looser, more general bound was used 
(Chebyshev?s inequality, which applies to all proba-
bility distributions). All statistically significant differ-
ences are at the 1% level (2-sided). 
6
4.2 Feature Set Analyses 
The unigram features reflect common correspon-
dences between Chinese and English pronuncia-
tion.  For example, (Sproat, Tao and Zhai 2006) 
note that Chinese /l/ is often associated with Eng-
lish /r/, and the feature l:r is among the most fre-
quent unigram mappings in both the MLEV and 
MALINE alignments. At a frequency of 103,361, 
it is the most frequent unigram feature in the 
MLEV mappings, and it is the third most fre-
quent unigram feature in the MALINE align-
ments (56,780). 
Systematic correspondences among plosives 
are also captured in the MALINE unigram map-
pings.  The unaspirated voiceless Chinese plo-
sives /p,t,k/ contrast with aspirated plosives 
/p?,t?,k?/, whereas the English voiceless plosives 
(which are aspirated in predictable environments) 
contrast with voiced plosives /b,d,g/.  As a result, 
English /b,d,g/ phonemes are usually translite-
rated using Chinese characters that are pro-
nounced /p,t,k/, while English /p,t,k/ phonemes 
usually correspond to Chinese /p?,t?,k?/. The ex-
amples of Stewart and Elizabeth in Section 1 
illustrate the correspondence of English /t/ and 
Chinese / t?/ and of English /b/ with Chinese /p/ 
respectively. All six of the unigram features that  
result from these correspondences occur among 
the 20 most frequent in the MALINE alignments, 
ranging in frequency from 23,602 to 53,535. 
 
 
Neg-
ative 
Thre-
shold 
Exam-
ples 
Monolingual  (M) Cross-Lingual (X) Combined (C) 
  P R P R P R 
0 538,621 94.69 
[90.6] 
95.73 
[91.0] 
96.5 
[90.0] 
97.15 
[93.4] 
97.13 
[90.8] 
97.65 
[91.0] 
0.1 307,066 95.28 
[87.1] 
96.23 
[83.4] 
98.06 
[89.2] 
98.25 
[89.9] 
98.4 
[87.6] 
98.64 
[84.1] 
0.2 282,214 95.82 
[86.2] 
96.63 
[84.4] 
97.91 
[88.4] 
98.41 
[90.3] 
98.26 
[86.7] 
98.82 
[84.7] 
0.3 183,188 95.79 
[80.6] 
96.92 
[85.3] 
98.18 
[86.3] 
98.8 
[90.7] 
98.24 
[80.6] 
99.27 
[84.8] 
0.4 72,176 96.31 
[77.1] 
98.69 
[82.3] 
97.89 
[91.8] 
99.61 
[86.2] 
98.91 
[77.1] 
99.64 
[80.9] 
0.5 17,914 94.62 
[64.6] 
98.63 
[84.3] 
99.44 
[89.4] 
100.0 
[91.9] 
99.46 
[63.8] 
99.89 
[84.7] 
0.6 2,954 94.94 
[66.1] 
100 
[77.0] 
98.0 
[85.2] 
98.66 
[92.8] 
99.37 
[61.3] 
100.0 
[73.1] 
0.7 362 95.24 
[52.8] 
100 
[100.0] 
94.74 
[78.9] 
100.0 
[78.9] 
100.0 
[47.2] 
94.74 
[100.0] 
Table 2:  Precision and Recall with Different Feature Sets 
(Baseline scores in square brackets) 
 
4.3 Comparison with other Learners 
To compare with other machine learning tools, 
we used the WEKA toolkit (from 
www.weka.net.nz). Table 3 shows the compar-
isons on the MLEV data for a fixed size at one 
threshold. Except for SVM Light, the results 
are based on 10-fold cross validation.  The 
other classifiers appear to perform relatively 
worse at that setting for the MLEV data, but 
the differences in accuracy are not statistically 
significant even at the 5% level. A large con-
tributor to the lack of significance is the small 
test set size of 66 pairs (10% of 660 examples) 
used in the SVM Light test. 
4.4 Other Language Pairs 
Some earlier experiments for Arabic-Roman 
comparisons were carried out using a Condi-
tional Random Field learner (CRF), using the 
Carafe toolkit (from source-
forge.net/projects/carafe). The method com-
putes its own Levenshtein edit-distance scores, 
and learns edit-distance costs from that. The 
scores obtained, on average, had only a .6 cor-
relation with the basic comparison Levenshtein 
scores. However, these experiments did not 
return accuracy results, as ground-truth data 
was not specified for this task. 
7
Several preliminary machine learning expe-
riments were also carried out on Urdu-Roman 
comparisons. The data used were Urdu data 
extracted from a parallel corpus recently pro-
duced by the LDC (LCTL_Urdu.20060408).  
The results are shown in Table 4. Here a .55 
MALINE score and a .85 MLEV score were 
used for selecting positive examples by basic 
comparison, and negative examples were se-
lected at random. Here the MALINE method 
(row 1) using the weka SMO SVM made use 
of a threshold based on a MALINE score. In 
these earlier experiments, machine learning 
does not really improve the system perfor-
mance (F-measure decreases with learning on 
one test and only increases by 0.1% on the 
other test). However, since these earlier expe-
riments did not benefit from the use of differ-
ent negative thresholds, there was no control 
over problem difficulty.  
5 Related Work 
While there is a substantial literature employ-
ing learning techniques for record linkage 
based on the theory developed by Fellegi and 
Sunter (1969), researchers have only recently 
developed applications that focus on name 
strings and that employ methods which do not 
require features to be independent (Cohen and 
Richman 2002). Ristad and Yianilos (1997) 
have developed a generative model for learn-
ing string-edit distance that learns the cost of 
different edit operations during string align-
ment. Bilenko and Mooney (2003) extend Ris-
tad?s approach to include gap penalties (where 
the gaps are contiguous sequences of mis-
matched characters) and compare this genera-
tive approach with a vector similarity approach 
that doesn?t carry out alignment. McCallum et 
al. (2005) use Conditional Random Fields 
(CRFs) to learn edit costs, arguing in favor of 
discriminative training approaches and against 
generative approaches, based in part on the 
fact that the latter approaches ?cannot benefit 
from negative evidence from pairs of strings 
that (while partially overlapping) should be 
considered dissimilar?. Such CRFs model the 
conditional probability of a label sequence (an 
alignment of two strings) given a sequence of 
observations (the strings).  
A related thread of research is work on au-
tomatic transliteration, where training sets are 
typically used to compute probabilities for 
mappings in weighted finite state transducers 
(Al-Onaizan and Knight 2002; Gao et al 2004) 
or source-channel models (Knight and Graehl 
1997; Li et al 2004). (Sproat et al 2006) have 
compared names from comparable and con-
temporaneous English and Chinese texts, scor-
ing matches by training a learning algorithm to 
compare the phonemic representations of the 
names in the pair, in addition to taking into 
account the frequency distribution of the pair 
over time.  (Tao et al 2006) obtain similar re-
sults using frequency and a similarity score 
based on a phonetic cost matrix 
The above approaches have all developed 
special-purpose machine-learning architectures 
to address the matching of string sequences. 
They take pairs of strings that haven?t been 
aligned, and learn costs or mappings from 
them, and once trained, search for the best 
match given the learned representation  
 
Positive  
Threshold
Examples Method P R F Accuracy 
.65 660 SVM Light 90.62 87.88 89.22 89.39   
.65 660 WEKA SMO 80.6 83.3 81.92 81.66 
.65 660 AdaBoost M1 84.9 78.5 81.57 82.27 
Table 3: Comparison of Different Classifiers 
 
Method Positive 
Threshold 
Examples P R F 
WEKA SMO .55 (MALINE) 206 (MALINE) 84.8 [81.5] 86.4 [93.3] 85.6 [87.0] 
WEKA SMO .85 (MLEV) 584 (MLEV) 89.9 [93.2] 94.7 [91.2] 92.3 [92.2] 
Table 4: Urdu-Roman Name Matching Results with Random Negatives 
(Baseline scores in square brackets) 
 
8
Our approach, by contrast, takes pairs of 
strings along with an alignment, and using fea-
tures derived from the alignments, trains a learn-
er to derive the best match given the features. 
This offers the advantage of modularity, in that 
any type of alignment model can be combined 
with SVMs or other classifiers (we have pre-
ferred SVMs since they offer discriminative 
training). Our approach allows leveraging of any 
existing alignments, which can lead to starting 
the learning from a higher baseline and less train-
ing data to get to the same level of performance. 
Since the learner itself doesn?t compute the 
alignments, the disadvantage of our approach is 
the need to engineer features that communicate 
important aspects of the alignment to the learner.  
In addition, our approach, as with McCallum 
et al (2005), allows one to take advantage of 
both positive and negative training examples, 
rather than positive ones alone. Our data genera-
tion strategy has the advantage of generating 
negative examples so as to vary the difficulty of 
the problem, allowing for more fine-grained per-
formance measures. Metrics based on such a 
control are likely to be useful in understanding 
how well a name-matching system will work in 
particular applications, especially those involving 
confusable names. 
6 Conclusion 
The work presented here has established a 
framework for application of machine learning 
techniques to multilingual name matching.  The 
results show that machine learning dramatically 
outperforms basic comparison methods, with F-
measures as high as 97.0 on the most difficult 
problems. This approach is being embedded in a 
larger system that matches full names using a 
vetted database of full-name matches for evalua-
tion.  
So far, we have confined ourselves to minimal 
feature engineering. Future work will investigate 
a more abstract set of phonemic features. We 
also hope to leverage ongoing work on harvest-
ing name pairs from web resources, in addition 
applying them to less commonly taught languag-
es, as and when appropriate resources for them 
become available. 
References 
Al-Onaizan, Y. and K. Knight, K. 2002. Machine 
Transliteration of Names in Arabic Text. Proceed-
ings of the ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Bilenko, M. and Mooney, R.J. 2003. Adaptive dupli-
cate detection using learnable string similarity 
measures. In Proc. of SIGKDD-2003. 
CMU. 2008. The CMU Pronouncing 
nary. ftp://ftp.cs.cmu.edu/project/speech/dict/ 
Cohen, W. W., and Richman, J. 2002. Learning to 
match and cluster large high-dimensional data sets 
for data integration. In Proceedings of The Eighth 
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining (KDD-2002). 
Fellegi, I. and Sunter, A.  1969. A theory for record 
linkage.  Journal of the American Statistical Socie-
ty, 64:1183-1210, 1969. 
Freeman, A., Condon, S. and Ackermann, C. 2006. 
Cross Linguistic Name Matching in English and 
Arabic. Proceedings of HLT. 
Gao, W., Wong, K., and Lam, W. 2004. Phoneme-
based transliteration of foreign names for OOV 
problem.  In Proceedings of First International 
Joint Conference on Natural Language Processing. 
Kondrak, G. 2000. A New Algorithm for the Align-
ment of Phonetic Sequences. Proceedings of the 
First Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(ANLP-NAACL 2000),  288-295.  
Knight, K. and Graehl, J., 1997. Machine Translitera-
tion, In Proceedings of the Conference of the Asso-
ciation for Computation Linguistics (ACL). 
Li, H., Zhang, M., & Su, J. 2004. A joint source-
channel model for machine transliteration.  In Pro-
ceedings of Conference of the Association for 
Computation Linguistics (ACL). 
McCallum, A., Bellare, K. and Pereira, F. 2005. A 
Conditional Random Field for Discriminatively-
trained Finite-state String Edit Distance. Confe-
rence on Uncertainty in AI (UAI). 
Ristad, E. S. and Yianilos, P. N. 1998.  Learning 
string edit distance.  IEEE Transactions on Pattern 
Recognition and Machine Intelligence. 
Salafra. 2006. http://www.safalra.com /science 
/linguistics/pinyin-pronunciation/ 
Sproat, R., Tao, T. and Zhai, C. 2006.  Named Entity 
Transliteration with Comparable Corpora.  In Pro-
ceedings of the Conference of the Association for 
Computational Linguistics.  New York. 
Tao, T., Yoon, S. Fister, A., Sproat, R. and Zhai, C. 
2006.  Unsupervised Named Entity Transliteration 
Using Temporal and Phonetic Correlation.  In Pro-
ceedings of the ACL Empirical Methods in Natural 
Language Processing Workshop. 
Wikipedia. 2006. http://en.wikipedia.org/wiki/Pinyin 
9
