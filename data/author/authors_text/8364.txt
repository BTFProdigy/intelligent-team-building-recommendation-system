Dependency Structure Analysis and Sentence Boundary
Detection in Spontaneous Japanese
Kazuya Shitaoka? Kiyotaka Uchimoto? Tatsuya Kawahara? Hitoshi Isahara?
?School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan,
{shitaoka,kawahara}@ar.media.kyoto-u.ac.jp
?National Institute of Information
and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan,
{uchimoto,isahara}@nict.go.jp
Abstract
This paper describes a project to detect dependen-
cies between Japanese phrasal units called bunsetsus,
and sentence boundaries in a spontaneous speech
corpus. In monologues, the biggest problem with de-
pendency structure analysis is that sentence bound-
aries are ambiguous. In this paper, we propose
two methods for improving the accuracy of sentence
boundary detection in spontaneous Japanese speech:
One is based on statistical machine translation us-
ing dependency information and the other is based
on text chunking using SVM. An F-measure of 84.9
was achieved for the accuracy of sentence bound-
ary detection by using the proposed methods. The
accuracy of dependency structure analysis was also
improved from 75.2% to 77.2% by using automat-
ically detected sentence boundaries. The accuracy
of dependency structure analysis and that of sen-
tence boundary detection were also improved by in-
teractively using both automatically detected depen-
dency structures and sentence boundaries.
1 Introduction
The ?Spontaneous Speech: Corpus and Pro-
cessing Technology? project has been sponsor-
ing the construction of a large spontaneous
Japanese speech corpus, Corpus of Spontaneous
Japanese (CSJ) (Maekawa et al, 2000). The
CSJ is the biggest spontaneous speech corpus in
the world, and it is a collection of monologues
and dialogues, the majority being monologues
such as academic presentations. The CSJ in-
cludes transcriptions of speeches as well as audio
recordings. Approximately one tenth of the CSJ
has been manually annotated with information
about morphemes, sentence boundaries, depen-
dency structures, discourse structures, and so
on. The remaining nine tenths of the CSJ
have been annotated semi-automatically. A fu-
ture goal of the project is to extract sentence
boundaries, dependency structures, and dis-
course structures from the remaining transcrip-
tions. This paper focuses on methods for au-
tomatically detecting sentence boundaries and
dependency structures in Japanese spoken text.
In many cases, Japanese dependency struc-
tures are defined in terms of the dependency
relationships between Japanese phrasal units
called bunsetsus. To define dependency rela-
tionships between all bunsetsus in spontaneous
speech, we need to define not only the depen-
dency structures in all sentences but also the
inter-sentential relationships, or, discourse re-
lationships, between the sentences, as depen-
dency relationships between bunsetsus. How-
ever, it is difficult to define and detect discourse
relationships between sentences because of sig-
nificant inconsistencies in human annotations
of discourse structures, especially with regard
to spontaneous speech. We also need to know
intra-sentential dependency structures in order
to use the results of dependency structure anal-
ysis for sentence compaction in automatic text
summarization or case frame acquisition. Be-
cause it is difficult to define discourse relation-
ships between sentences, depending on the ac-
tual application, it is usually enough to define
and detect the dependency structure of each
sentence. Therefore, the CSJ was annotated
with intra-sentential dependency structures for
sentences in the same way this is usually done
for a written text corpus. However, there is
a big difference between a written text corpus
and a spontaneous speech corpus: In sponta-
neous speech, especially when it is long, sen-
tence boundaries are often ambiguous. In the
CSJ, therefore, sentence boundaries were de-
fined based on clauses whose boundaries were
automatically detected by using surface infor-
mation (Maruyama et al, 2003), and they were
detected manually (Takanashi et al, 2003). Our
definition of sentence boundaries follows the
definition used in the CSJ.
Almost all previous research on Japanese de-
pendency structure analysis dealt with depen-
dency structures in written text (Fujio and Mat-
sumoto, 1998; Haruno et al, 1998; Uchimoto et
al., 1999; Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2000). Although Matsubara and col-
leagues did investigate dependency structures
in spontaneous speech (Matsubara et al, 2002),
the target speech was dialogues where the ut-
terances were short and sentence boundaries
could be easily defined based on turn-taking
data. In contrast, we investigated dependency
structures in spontaneous and long speeches in
the CSJ. The biggest problem in dependency
structure analysis with spontaneous and long
speeches is that sentence boundaries are am-
biguous. Therefore, sentence boundaries should
be detected before or during dependency struc-
ture analysis in order to obtain the dependency
structure of each sentence.
In this paper, we first describe the problems
with dependency structure analysis of sponta-
neous speech. Because the biggest problem is
ambiguous sentence boundaries, we focus on
sentence boundary detection and propose two
methods for improving the accuracy of detec-
tion.
2 Dependency Structure Analysis
and Sentence Boundary Detection
in Spontaneous Japanese
First, let us briefly describe how dependency
structures can be represented in a Japanese sen-
tence. In Japanese sentences, word order is
rather free, and subjects and objects are often
omitted. In languages having such characteris-
tics, the syntactic structure of a sentence is gen-
erally represented by the relationship between
phrasal units, or bunsetsus, based on a depen-
dency grammar. Phrasal units, or bunsetsus,
are minimal linguistic units obtained by seg-
menting a sentence naturally in terms of seman-
tics and phonetics. Each bunsetsu consists of
one or more morphemes. For example, the sen-
tence ???????????? (kare-wa yukkuri
aruite-iru, He is walking slowly)? can be divided
into three bunsetsus, ??? (kare-wa, he)?, ???
?? (yukkuri, slowly)? and ?????? (aruite-
iru, is walking)?. In this sentence, the first and
second bunsetsus depend on the third one.
There are many differences between writ-
ten text and spontaneous speech, and there
are problems peculiar to spontaneous speech
in dependency structure analysis and sentence
boundary detection. The following sections de-
scribe some typical problems and our solutions.
2.1 Problems with Dependency
Structure Analysis
Ambiguous sentence boundaries
As described in Section 1, in this study, we
assumed that ambiguous sentence bound-
aries is the biggest problem in dependency
structure analysis of spontaneous speech.
So in this paper, we mainly focus on this
problem and describe our solution to it.
Independent bunsetsus
In spontaneous speech, we sometimes find
that modifiees are missing because utter-
ance planning changes in the middle of the
speech. Also, we sometimes find bunsetsus
whose dependency relationships are useless
for understanding the utterance. These in-
clude fillers such as ???? (anoh, well)?
and ???? (sonoh, well)?, adverbs that
behave like fillers such as ??? (mou)?,
responses such as ??? (hai, yes)? and ?
?? (un, yes)?, conjunctions such as ??
(de, and)?, and disfluencies. In these cases,
bunsetsus are assumed to be independent,
and as a result, they have no modifiees in
the CSJ. For example, 14,988 bunsetsus in
188 talks in the CSJ are independent.
We cannot ignore fillers, responses, and
disfluencies because they frequently ap-
pear in spontaneous speech. However,
we can easily detect them by using the
method proposed by Asahara and Mat-
sumoto (Asahara and Matsumoto, 2003).
In this paper, fillers, responses, and disflu-
encies were eliminated before dependency
structure analysis and sentence boundary
detection by using morphological informa-
tion and labels. In the CSJ, fillers and re-
sponses are interjections, and almost all of
them are marked with label (F). Disfluen-
cies are marked with label (D).
In this paper, every independent bunsetsu
was assumed to depend on the next one.
However, practically speaking, indepen-
dent bunsetsus should be correctly detected
as ?independent?. This detection is one of
our future goals.
Crossed dependency
In general, dependencies in Japanese writ-
ten text do not cross. In contrast, de-
pendencies in spontaneous speech some-
times do. For example, ???? (kore-ga,
this)? depends on ????? (tadashii-to, is
right)? and ??? (watashi-wa, I)? depends
on ??? (omou, think)? in the sentence ??
??????????????, where ???
denotes a bunsetsu boundary. Therefore,
the two dependencies cross.
However, there are few number of crossed
dependencies in the CSJ: In 188 talks, we
found 689 such dependencies for total of
170,760 bunsetsus. In our experiments,
therefore, we assumed that dependencies
did not cross. Correctly detecting crossed
dependencies is one of our future goals.
Self-correction
We often find self-corrections in sponta-
neous speech. For example, in the 188 talks
in the CSJ there were 2,544 self-corrections.
In the CSJ, self-corrections are represented
as dependency relationships between bun-
setsus, and label D is assigned to them.
Coordination and appositives are also rep-
resented as dependency relationships be-
tween bunsetsus, and labels P and A are
assigned to them, respectively. The defi-
nitions of coordination and appositives fol-
low those of the Kyoto University text cor-
pus (Kurohashi and Nagao, 1997). Both
the labels and the dependencies should
be detected for applications such as au-
tomatic text summarization. However, in
this study, we detected only the dependen-
cies between bunsetsus, and we did it in the
same manner as in previous studies using
written text.
Inversion
Inversion occurs more frequently in spon-
taneous speech than in written text. For
example, in the 188 talks in the CSJ there
were 172 inversions. In the CSJ, inver-
sions are represented as dependency rela-
tionships going in the direction from right
to left. In this study, we thought it impor-
tant to detect dependencies, and we man-
ually changed their direction to that from
left to right. The direction of dependency
has been changed to that from left to right.
2.2 Problems with Sentence Boundary
Detection
In spontaneous Japanese speech, sentence
boundaries are ambiguous. In the CSJ, there-
fore, sentence boundaries were defined based
on clauses whose boundaries were automatically
detected using surface information (Maruyama
et al, 2003), and they were detected manually
(Takanashi et al, 2003). Clause boundaries can
be classified into the following three groups.
Absolute boundaries , or sentence bound-
aries in their usual meaning. Such bound-
aries are often indicated by verbs in their
basic form.
Strong boundaries , or points that can be re-
garded as major breaks in utterances and
that can be used for segmentation. Such
boundaries are often indicated by clauses
whose rightmost words are ?? (ga, but)?,
or ?? (shi, and)?.
Weak boundaries , or points that can
be used for segmentation because they
strongly depend on other clauses. Such
boundaries are often indicated by clauses
whose rightmost words are ??? (node, be-
cause)?, or ??? (tara, if)?.
These three types of boundary differ in the
degree of their syntactic and semantic com-
pleteness and the dependence of their sub-
sequent clauses. Absolute boundaries and
strong boundaries are usually defined as sen-
tence boundaries. However, sentence bound-
aries in the CSJ are different from these two
types of clause boundaries, and the accuracy
of rule-based automatic sentence boundary de-
tection in the 188 talks in the CSJ has an F-
measure of approximately 81, which is the ac-
curacy for a closed test. Therefore, we need a
more accurate sentence boundary detection sys-
tem.
Shitaoka et al (Shitaoka et al, 2002) pro-
posed a method for detecting sentence bound-
aries in spontaneous Japanese speech. Their
definition of sentence boundaries is approxi-
mately the same as that of absolute bound-
aries described above. In this method, sen-
tence boundary candidates are extracted by
character-based pattern matching using pause
duration. However, it is difficult to extract
appropriate candidates by this method be-
cause there is a low correlation between pauses
and the strong and weak boundaries described
above. It is also hard to detect noun-final
clauses by character-based pattern matching.
One method based on machine learning, a
method based on maximum entropy models,
has been proposed by Reynar and Ratnaparkhi
(Reynar and Ratnaparkhi, 2000). However, the
target in their study was written text. This
method cannot readily used for spontaneous
speech because in speech, there are no punc-
tuation marks such as periods. Other features
of utterances should be used to detect sentence
boundaries in spontaneous speech.
3 Approach of Dependency
Structure Analysis and Sentence
Boundary Detection
The outline of the processes is shown in Fig-
ure 1.
0: Morphological
Analysis
1: Sentence Boundary
Detection (Baseline)
3: Dependency Structure
Analysis (Baseline)
2: Sentence Boundary
Detection (SVM)
5: Sentence Boundary
Detection (Language model)
6: Sentence Boundary
Detection (SVM)
7: Dependency Structure
Analysis (Again)
clause
expression
pause
duration
word 3-gram model
pause
duration
clause
expression
word
information
(A)
(B)
word
Information
distance
between 
bunsetsus
(C)
(A) + information of 
dependencies
(B) + information of
dependencies
4: Dependency 
Structure Analysis
Figure 1: Outline of dependency structure anal-
ysis and sentence boundary detection.
3.1 Dependency Structure Analysis
In statistical dependency structure analysis of
Japanese speech, the likelihood of dependency
is represented by a probability estimated by a
dependency probability model.
Given sentence S, let us assume that it is
uniquely divided into n bunsetsus, b1, . . . , bn,
and that it is represented as an ordered set of
bunsetsus, B = {b1, . . . , bn}. Let D be an or-
dered set of dependencies in the sentence and let
D
i
be a dependency whose modifier is bunsetsu
b
i
(i = 1, . . . , n ? 1). Let us also assume that
D = {D1, . . . ,Dn?1}. Statistical dependency
structure analysis finds dependencies that max-
imize probability P (D|S) given sentence S.
The conventional statistical model (Collins,
1996; Fujio and Matsumoto, 1998; Haruno et
al., 1998; Uchimoto et al, 1999) uses only
the relationship between two bunsetsus to es-
timate the probability of dependency, whereas
the model in this study (Uchimoto et al, 2000)
takes into account not only the relationship be-
tween two bunsetsus but also the relationship
between the left bunsetsu and all the bunsetsus
to its right. This model uses more information
than the conventional model.
We implemented this model within a max-
imum entropy modeling framework. The fea-
tures used in the model were basically attributes
of bunsetsus, such as character strings, parts
of speech, and types of inflections, as well as
those that describe the relationships between
bunsetsus, such as the distance between bun-
setsus. Combinations of these features were also
used. To find D
best
, we analyzed the sentences
backwards (from right to left). In the backward
analysis, we can limit the search space effec-
tively by using a beam search. Sentences can
also be analyzed deterministically without great
loss of accuracy (Uchimoto et al, 1999). So we
analyzed a sentence backwards and determinis-
tically.
3.2 Sentence Boundary Detection
Based on Statistical Machine
Translation (Conventional method
(Shitaoka et al, 2002))
The framework for statistical machine trans-
lation is formulated as follows. Given in-
put sequence X, the goal of statistical ma-
chine translation is to find the best output se-
quence, Y , that maximizes conditional proba-
bility P (Y |X):
max
Y
P (Y |X) = max
Y
P (Y )P (X |Y ) (1)
The problem of sentence boundary detection
can be reduced to the problem of translat-
ing a sequence of words, X, that does not in-
clude periods but instead includes pauses into
a sequence of words, Y , that includes peri-
ods. Specifically, in places where a pause
might be converted into a period, which means
P (X|Y ) = 1, the decision whether a period
should be inserted or not is made by comparing
language model scores P (Y ?) and P (Y ??). Here,
the difference between Y ? and Y ?? is in that one
includes a period in a particular place and the
other one does not.
We used a model that uses pause duration
and surface expressions around pauses as trans-
lation model P (X|Y ). We used expressions
around absolute and strong boundaries as de-
scribed in Section 2.2 as surface expressions
around pauses. A pause preceding or follow-
ing surface expressions can be converted into
a period. Specifically, pauses following expres-
sions ?? (to)?, ??? (nai)?, and ?? (ta)?, and
pauses preceding expression ?? (de)?, can be
converted into a period when these pauses are
longer than average. A pause preceding or fol-
lowing other surface expressions can be con-
verted into a period even if its duration is short.
To calculate P (Y ), we used a word 3-gram
model trained with transcriptions in the CSJ.
3.3 Sentence Boundary Detection
Using Dependency Information
(Method 1)
There are three assumptions that should be sat-
isfied by the rightmost bunsetsu in every sen-
tence. In the following, this bunsetsu is referred
to as the target bunsetsu.
(1) One or more bunsetsus depend on the
target bunsetsu. (Figure 2)
Since every bunsetsu depends on another bun-
setsu in the same sentence, the second rightmost
bunsetsu always depends on the rightmost bun-
setsu in any sentence, except in inverted sen-
tences. In inverted sentences in this study, we
changed the direction of all dependencies to that
from left to right.
One or more  
Bunsetsus depend   
Figure 2: One or more bunsetsus depend on
the target bunsetsu. (?|? represents a sentence
boundary.)
(2) There is no bunsetsu that depends
on a bunsetsu beyond the target bunsetsu.
(Figure 3)
Each bunsetsu in a sentence depends on a bun-
setsu in the same sentence.
(3) The probability of the target bun-
setsu is low. (Figure 4)
The target bunsetsu does not depend on any
bunsetsu.
No bunsetsu depend in this way
Figure 3: There is no bunsetsu that depends on
a bunsetsu beyond the target bunsetsu.
This probability should be low
Figure 4: Probability of the target bunsetsu is
low.
Bunsetsus that satisfy assumptions (1)-(3)
are extracted as rightmost bunsetsu candidates
in a sentence. Then, for every point follow-
ing the extracted bunsetsus and for every pause
preceding or following the expressions described
in Section 3.2, a decision is made regarding
whether a period should be inserted or not.
In assumption (2), bunsetsus that depend on a
bunsetsu beyond 50 bunsetsus are ignored be-
cause no such long-distance dependencies were
found in the 188 talks in the CSJ used in our ex-
periments. Bunsetsus whose dependency prob-
ability is very low are also ignored because there
is a high possibility that these bunsetsus? depen-
dencies are incorrect. Let this threshold proba-
bility be p, and let the threshold probability in
assumption (3) be q. The optimal parameters p
and q are determined by using held-out data.
In this approach, about one third of all
bunsetsu boundaries are extracted as sentence
boundary candidates. So, an output sequence
is selected from all possible conversion patterns
generated using two words to the left and two
words to the right of each sentence boundary
candidate. To perform this operation, we used
a beam search with a width of 10 because a
number of conversion patterns can be generated
with such a search.
3.4 Sentence Boundary Detection
Based on Machine Learning
(Method 2)
We use Support Vector Machine (SVM) as a
machine learning model and we approached the
problem of sentence boundary detection as a
text chunking task. We used YamCha (Kudo
and Matsumoto, 2001) as a text chunker, which
is based on SVM and uses polynomial kernel
functions. To determine the appropriate chunk
label for a target word, YamCha uses two words
to the right and two words to the left of the
target word as statistical features, and it uses
chunk labels that are dynamically assigned to
the two preceding or the two following words
as dynamic features, depending on the analysis
direction. To solve the multi-class problem, we
used pairwise classification. This method gen-
erates N ? (N ? 1)/2 classifiers for all pairs of
classes, N , and makes a final decision by their
weighted voting.
The features used in our experiments are the
following:
1. Morphological information of the three words
to the right and three words to the left of the
target word, such as character strings, pronun-
ciation, part of speech, type of inflection, and
inflection form
2. Pause duration normalized in terms of Maha-
lanobis distance
3. Clause boundaries
4. Dependency probability of the target bunsetsu
5. The number of bunsetsus that depend on the
target bunsetsu and their dependency proba-
bilities
We used the IOE labeling scheme for proper
chunking, and the following parameters for
YamCha.
? Degree of polynomial kernel: 3rd
? Analysis direction: Left to right
? Multi-class method: Pairwise
4 Experiments and Discussion
In our experiments, we used the transcriptions
of 188 talks in the CSJ. We used 10 talks for
testing. Dependency structure analysis results
were evaluated for closed- and open-test data in
terms of accuracy, which was defined as the per-
centage of correct dependencies out of all depen-
dencies. In Tables 1 to 3, we use words ?closed?
and ?open? to describe the results obtained for
closed- and open-test data, respectively. Sen-
tence boundary detection results were evaluated
in terms of F-measure.
First, we show the baseline accuracy of depen-
dency structure analysis and sentence boundary
detection. The method described in Section 3.2
was used as a baseline method for sentence
boundary detection (Process 1 in Figure 1). To
train the language model represented by P (Y ),
we used the transcriptions of 178 talks exclud-
ing the test data. The method described in Sec-
tion 3.1 was used as a baseline method for de-
pendency structure analysis. (Process 3 in Fig-
ure 1) As sentence boundaries, we used the re-
sults of the baseline method for sentence bound-
ary detection. We obtained an F-measure of
75.6, a recall of 64.5%, and a precision of 94.2%
for the sentence boundary detection in our ex-
periments. The dependency structure analysis
accuracy was 75.2% for the open data and 80.7%
for the closed data.
The dependency probability of the rightmost
bunsetsus in a given sentence was not calculated
in our model. So, we assumed that the right-
most bunsetsus depended on the next bunsetsu
and that the dependency probability was 0.5
when we used dependency information in the
experiments described in the following sections.
4.1 Sentence Boundary Detection
Results Obtained by Method 1
We evaluated the results obtained by the
method described in Section 3.3. The results
of baseline dependency structure analysis were
used as dependency information (Process 5 in
Figure 1).
First, we investigated the optimal values of
parameters p and q described in Section 3.3 by
using held-out data, which differed from the test
data and consisted of 15 talks. The optimal val-
ues of p and q were, respectively, 0 and 0.9 for
the open-test data, and 0 and 0.8 for the closed-
test data. These values were used in the follow-
ing experiments. The value of p was 0, and these
results show that bunsetsus that depended on a
bunsetsu beyond 50 bunsetsus were ignored as
described in assumption (2) in Section 3.3.
The obtained results are shown in Table 1.
When dependency information was used, the F-
measure increased by approximately 1.4 for the
open-test data and by 2.0 for the closed test
data, respectively. Although the accuracy of de-
pendency structure analysis for closed test data
was about 5.5% higher than that for the open-
test data, the difference between the accuracies
of sentence boundary detection for the closed-
and open-test data was only about 0.6%. These
results indicate that equivalent accuracies can
be obtained for both open- and closed-test data
in detecting dependencies related to sentence
boundaries.
When all the extracted candidates were con-
sidered as sentence boundaries without us-
ing language models, the accuracy of sentence
boundary detection obtained by using the base-
line method was 68.2%(769/1,127) in recall and
81.5%(769/943) in precision, and that obtained
by using Method 1 was 87.2%(983/1,127) in re-
call and 27.7%(983/3,544) in precision. The re-
sults show that additional 214 sentence bound-
ary candidates were correctly extracted by us-
ing dependency information. However, only
108 sentence boundaries were chosen out of
the 214 candidates when language models were
used. We investigated in detail the points
that were not chosen and found errors in noun-
final clauses, clauses where the rightmost con-
stituents were adjectives or verbs such as ??
?? (it to-omou, think)? or ????? (it wa-
muzukashii, difficult)?, and clauses where the
rightmost constituents were ?????? (it to-
Table 1: Sentence boundary detection results
obtained by using dependency information.
recall precision F
With dependency 74.1% 82.5% 78.0
information (open) (835/1,127) (835/1,012)
With dependency 74.2% 83.5% 78.6
information (closed) (836/1,127) (836/1,001)
baseline 64.5% 94.2% 76.6
(727/1,127) (727/772)
iu-no-wa, because)? and ????? (it to-si-te-
wa, as)?, and so on. Some errors, except for
those in noun-final clauses, could have been cor-
rectly detected if we had had more training
data.
We also found that periods were sometimes
erroneously inserted when preceding expres-
sions were ?? (ga, but)?, ???? (mashite,
and)?, and ????? (keredomo, but)?, which
are typically the rightmost constituents of a sen-
tence, as weel as ?? (te, and)?, which is not,
typically, the rightmost constituent of a sen-
tence. The language models were not good at
discriminating between subtle differences.
4.2 Sentence Boundary Detection
Results Obtained by Method 2
We evaluated the results obtained by the
method described in Section 3.4 (Process 6 in
Figure 1). For training, we used 178 talks ex-
cluding test data.
The results are shown in Table 2. The F-
measure was about 6.9 points higher than that
described in Section 4.1. The results show
that the approach based on machine learning
is more effective than that based on statisti-
cal machine translation. The results also show
that the accuracy of sentence boundary detec-
tion can be increased by using dependency in-
formation in Method 2. However, we found that
the amount of accuracy improvement achieved
by using dependency information depended on
the method used. This may be because other
features used in SVM may provide information
similar to dependency information. For exam-
ple, Feature 1 described in Section 3.4 might
provide information similar to that in Features
4 and 5. Although in our experiments we used
only three words to the right and three words
to the left of the target word, the degradation
in accuracy without dependency information
was slight. This may be because long-distance
dependencies may not be related to sentence
boundaries, or because Feature 5 does not con-
tribute to increasing the accuracy because the
accuracy of dependency structure analysis in de-
tecting long-distance dependencies is not high.
Table 2: Sentence boundary detection results
obtained by using SVM.
recall precision F
With dependency 80.0% 90.3% 84.9
information (open) (902/1,127) (902/999)
With dependency 79.7% 90.5% 84.9
information (closed) (900/1,127) (900/994)
Without 79.3% 90.1% 84.4
dependency information (894/1,127) (894/992)
Table 3: Dependency structure analysis results
obtained with automatically detected sentence
boundaries.
open closed
With results in Section 4.1 75.8% 81.2%
With results in Section 4.2 77.2% 82.5%
Baseline 75.2% 80.7%
4.3 Dependency Structure Analysis
Results
We evaluated the results of dependency struc-
ture analysis obtained when sentence bound-
aries detected automatically by the two meth-
ods described above were used as inputs (Pro-
cess 7 in Figure 1). The results are shown in
Table 3. The accuracy of dependency structure
analysis improved by about 2% when the most
accurate and automatically detected sentence
boundaries were used as inputs. This is be-
cause more sentence boundaries were detected
correctly, and the number of bunsetsus that de-
pended on those in other sentences decreased.
We investigated the accuracy of dependency
structure analysis when 100% accurate sentence
boundaries were used as inputs. The accuracy
was 80.1% for the open-test data, and 86.1%
for the closed-test data. Even when the sen-
tence boundary detection was perfect, the er-
ror rate was approximately 14% even for the
closed-test data. The accuracy of dependency
structure analysis for spoken text was about 8%
lower than that for written text (newspapers).
We speculate that this is because spoken text
has no punctuation marks and many bunsetsus
depend on others far from them because of in-
sertion structures. These problems need to be
addressed in future studies.
5 Conclusion
This paper described a project to detect depen-
dencies between bunsetsus and sentence bound-
aries in a spontaneous speech corpus. It is
more difficult to detect dependency structures
in spontaneous spoken speech than in written
text. The biggest problem is that sentence
boundaries are ambiguous. We proposed two
methods for improving the accuracy of sentence
boundary detection in spontaneous Japanese
speech. Using these methods, we obtained an
F-measure of 84.9 for the accuracy of sentence
boundary detection. The accuracy of depen-
dency structure analysis was also improved from
75.2% to 77.2% by using automatically detected
sentence boundaries. The accuracy of depen-
dency structure analysis and that of sentence
boundary detection were improved by interac-
tively using automatically detected dependency
information and sentence boundaries.
There are several future directions. In the fu-
ture, we would like to solve the problems that
we found in our experiments. In particular, we
want to reduce the number of errors due to in-
serted structures and solve other problems de-
scribed in Section 2.1.
References
Masayuki Asahara and Yuji Matsumoto. 2003. Filler and
Disfluency Identification Based on Morphological Analysis
and Chunking. In Proceedings of the ISCA & IEEE Work-
shop on Spontaneous Speech Processing and Recognition,
pages 163?166.
Michael Collins. 1996. A New Statistical Parser Based on
Bigram Lexical Dependencies. In Proceedings of the ACL,
pages 184?191.
Masakazu Fujio and Yuji Matsumoto. 1998. Japanese Depen-
dency Structure Analysis based on Lexicalized Statistics.
In Proceedings of the EMNLP, pages 87?96.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama.
1998. Using Decision Trees to Construct a Practical
Parser. In Proceedings of the COLING-ACL, pages 505?
511.
Taku Kudo and Yuji Matsumoto. 2000. Japanese Depen-
dency Structure Analysis Based on Support Vector Ma-
chines. In Proceedings of the EMLNP, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of the NAACL.
Sadao Kurohashi and Makoto Nagao. 1997. Building a
Japanese Parsed Corpus while Improving the Parsing Sys-
tem. In Proceedings of the NLPRS, pages 451?456.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi
Isahara. 2000. Spontaneous Speech Corpus of Japanese.
In Proceedings of the LREC2000, pages 947?952.
Takehiko Maruyama, Hideki Kashioka, Tadashi Kumano, and
Hideki tanaka. 2003. Rules for Automatic Clause Bound-
ary Detection and Their Evaluation. In Proceedings of
the Nineth Annual Meeting of the Association for Natural
Language proceeding, pages 517?520. (in Japanese).
Shigeki Matsubara, Takahisa Murase, Nobuo Kawaguchi, and
Yasuyoshi Inagaki. 2002. Stochastic Dependency Parsing
of Spontaneous Japanese Spoken Language. In Proceedings
of the COLING2002, pages 640?645.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 2000. A Max-
imum Entropy Approach to Identifying Sentence Bound-
aries. In Proceedings of the ANLP, pages 16?19.
Kazuya Shitaoka, Tatsuya Kawahara, and Hiroshi G. Okuno.
2002. Automatic Transformation of Lecture Transcrip-
tion into Document Style using Statistical Framework. In
IPSJ?WGSLP SLP-41-3, pages 17?24. (in Japanese).
Katsuya Takanashi, Takehiko Maruyama, Kiyotaka Uchi-
moto, and Hitoshi Isahara. 2003. Identification of ?Sen-
tences? in Spontaneous Japanese ? Detection and Mod-
ification of Clause Boundaries ?. In Proceedings of the
ISCA & IEEE Workshop on Spontaneous Speech Process-
ing and Recognition, pages 183?186.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese Dependency Structure Analysis Based on
Maximum Entropy Models. In Proceedings of the EACL,
pages 196?203.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hi-
toshi Isahara. 2000. Dependency Model Using Posterior
Context. In Proceedings of the IWPT, pages 321?322.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 119?122
Manchester, August 2008
Construction of an Infrastructure for Providing Users
with Suitable Language Resources
Hitomi Tohyama? Shunsuke Kozawa? Kiyotaka Uchimoto?
Shigeki Matsubara? and Hitoshi Isahara?
?Nagoya University, Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
{hitomi,kozawa,matubara}@el.itc.nagoya-u.ac.jp
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{uchimioto,isahara}@nict.go.jp
Abstract
Our research organization has been con-
structing a large scale database named
SHACHI by collecting detailed meta in-
formation on language resources (LRs) in
Asia and Western countries. The metadata
database contains more than 2,000 com-
piled LRs such as corpora, dictionaries,
thesauruses and lexicons, forming a large
scale metadata of LRs archive. Its meta-
data, an extended version of OLAC meta-
data set conforming to Dublin Core, have
been collected semi-automatically. This
paper explains the design and the structure
of the metadata database, as well as the re-
alization of the catalogue search tool.
1 Introduction
The construction of LRs such as corpora, dictio-
naries, thesauruses, etc., has boomed for years
throughout the world in its aim of encouraging
research and development in the main media of
spoken and written languages, and its importance
has also been widely recognized. Of the organiza-
tions willing to store and distribute LRs, there ex-
ist some consortia fulfilling their function such as
LDC1, ELRA2, CLARIN3, and OLAC4, in West-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1LDC:Linguistic Data Consortium,
http://www.ldc.upenn.edu/
2ELRA: European LRs Association
3CLARIN: Common Language Resources and Technolo-
gies Infrastructure, http://www.ilsp.gr/clarin eng.html
4OLAC: Open Language Archives Community,
http://www.language-archives.org/
ern countries, and GSK5 which does so mainly in
Japan. However, those released LRs are scarcely
connected with each other because of the dif-
ference between written and spoken language as
well as the difference between languages such
as Japanese, English, and Chinese (OLAC User
Guide, 2008).
This situation makes it difficult for researchers
and users to find LRs which are useful for their re-
searches. In the meantime, by connecting system-
atically existing various LRs with Wrapper Pro-
gram, the attempt to realize multilingual transla-
tion services has already begun (Ishida et al 2008,
Hayashi et al 2008). Moreover, since language in-
formation tags given to those LRs and their data
formats are multifarious, each LR is operated in-
dividually. As LR development generally entails
enormous cost, it is highly desirable that the re-
search efficiency be enhanced by systematically
combining those existing LRs altogether and ex-
tending them, which will encourage an efficient
development of unprecedented LRs.
Our research organization has been constructing
a large scale metadata database named SHACHI6
by collecting detailed meta information on LRs
in Western and Asian countries. This research
project aims to extensively collect metadata such
as tag sets, formats, and usage information about
researches on those LRs. and recorded contents of
LRs existing at home and abroad and store them
systematically. Meanwhile, we have already de-
veloped a search system of LRs by the use of meta
information and are attempting the experiment of
widely providing meta information on our stored
5GSK: Gengo Shigen Kyokai; Language Resource Asso-
ciation, http://www.gsk.or.jp/
6SHACHI: Metadata Database of Language Resources-
SHACHI, Shachi means ?orca? in English.
119
Figure 1: A sample page of SHACHI catalogue (ex. Euro WordNet)
LRs to those from researchers to common users.
This metadata database has been now open to the
public in the Web and allows every Internet user
to access it for the search and read information of
LRs at will.
2 Purpose of Metadata Database
Construction
The purpose of the construction of the database is
the following fivefold.
1. To store language resource metadata:
SHACHI semi-manually collects detailed
metadata of language resources and con-
structs their detailed catalogues. Figure 1
shows a sample page of a LR catalogue stored
in SHACHI (ex. Euro WordNet). The cata-
logue provides more detailed meta informa-
tion than other LR consortia do.
2. To systematize language resource meta-
data: Language resource ontology is tenta-
tively constructed by classifying types of lan-
guage resources (in this paper, it is called ?on-
tology?). Figure 2 shows an example of its
ontology. At the moment, it is under investi-
gation what is the most useful and functional
ontology for users by developing some on-
tologies such as human-made ontology, semi-
automatically produced ontology, and auto-
matically produced ontology.
3. To make each language resource related
to each other: The detailed metadata en-
abled us to describe characteristics of each
language resource and to expectably specify
relationships among language resources. Fig-
ure 3 shows a part of the SHACHI search
screen. It shows language resources found as
a search result, the references to which these
language resources conform as well as other
language resources whose formats are com-
mon to theirs.
4. To statistically investigate language re-
sources: By statistically analyzing the meta-
data, users are able to grasp what kinds of lan-
guage resources exist in different part of the
world and to understand current tendencies of
language resources which have been available
to the public.
5. To promote the distribution of language re-
sources: Since this metadata database en-
ables users to easily gain access to language
resources in accordance with their needs,
owing to fully equipped search functions,
SHACHI will be able to support an effective
use and an efficient development of language
resources.
Some 2,000 resources of metadata have already
been collected in the database so far and they will
be enlarged by a further 3,000. To that end, it is
120
Figure 2: Automatically produced ontology
indispensable for us to work in cooperation with
language resource consortia at home and abroad
and to take the initiative in contributing to Asian
language resources.
3 SHACHI Metadata Set
3.1 Policy for Collecting Metadata
The LRs which our metadata database stores
should satisfy the following conditions:
? Those resources should be stored in a digital
format.
? Those resources should be one of the follow-
ing: corpus, dictionary, thesaurus, or lexicon.
(Numeric data are not considered to be the
subject of collection for SHACHI.)
? Those resources should be collected from En-
glish websites and its data must be open to the
public.
? Those resources should be created by re-
search institutions, researchers, or business
organizations. (Developed tools such as facet
search.)
LRs metadata database SHACHI covers meta
information provided by LR consortia such as
ELRA, LDC, and OLAC whose more detailed
metadata are fed into the database by semi-
automatic means of importing.
3.2 Extensions of Metadata Element
Since users sometimes search for LRs without a
clear objective, it is necessary for language re-
source providers to construct language resource
ontology. This database conforms to the OLAC
metadata set which is based on 15 kinds of fun-
damental elements of Dublin Core7 and consti-
tutes an extended vision of OLAC with 19 newly
added metadata elements which were judged to
be indispensable for describing characteristics of
LRs. SHACHI provides usage information about
how and in which situation language resource re-
searchers utilized each language resource, which
is also important for users. The usage informa-
tion about LRs is automatically retrieved from aca-
demic article databases (Kozawa et al 2008). (See
?Utilization? in Figure1).
3.3 Systematic Storage of LRs
Clear description of the relations among LRs can
be applied to the efficient development of LRs
and search tools for common users of database.
Figure 2 shows ontology generated through auto-
matic means, based on language resource metadata
stored in SHACHI. We first surveyed the frequency
of possible values of metadata element choicesand
generated the ontology by hierarchicalizing meta
elements of our meta categories. While ontology
can be constructed in various ways from different
standpoints, our ontology is particularly designed
for users to enable to find them efficiently by fol-
lowing the hierarchical classes of our ontology.
4 Search Tools for Providing
Users-Oriented Information
Figure 3 shows a screen image of a search re-
sult through SHACHI. This section discusses three
search functions provided in SHACHI.
4.1 Three Types of Search Functions
For the purpose of facilitating users of this meta-
data database to find their intended language re-
source catalogues, SHACHI provides three search
functions:
1. Keyword search function: This tool is suit-
able for users who have clear images to search
7Dublin Core Metadata Initiative, http://dublincore.org/
121
Figure 3: Catalogue search tool
for specified LRs and a technical knowledge
of language processing. It allows them to in-
put keywords as they want and to search all
words stored in SHACHI metadata archive.
2. Facet search function: This tool is suitable
for users who have a vague idea of what kind
of LR they want. It is equipped with a choice
of 15 kinds of metadata elements selected
from the SHACHI metadata set. The users
narrow down the target LRs one by one in
order to find the intended one. For example,
with one click on ?age?, three choices such as
?Childrenfs utterance??, ?Adultsf utterance??
and ?Both are OK?? will be shown.
3. Ontology search function: This tool was de-
veloped by adopting the idea acquired by sys-
tematizing LRs registered in SHACHI. When
using the ontology search function, users find
the intended LRs by following the vertical re-
lationship of the ontology. It was ascertained
that ontology search function tool had the
merit of enabling users to discover LRs that
have not been ever found by keyword search
and facet search functions.
5 Conclusion
In this paper, we reported on the design of
SHACHI, a metadata database of LRs now be-
ing developed, the expansion and construction of
metadata for it, and an actualization of a search
function. At present, it contains approximately
2,000 pieces of meta information on LRs such
as corpora, dictionaries and thesauruses. One of
SHACHIfs characteristic features is that with a
collection of tag sets, format samples, and us-
age information on LRs which is automatically re-
trieved from scholarly papers given to LRs. From
now on the SHACHI project is intended to promote
cooperation among other LRs consortia abroad as
well as in Japan and to take the initiative in con-
tributing to the development of LRs in Asia.
References
Ishida,T., Nadamoto, A., Murakami,Y., Inaba, R. et al
2008. A Non-Profit Operation Model for the Lan-
guage Grid, In proceedings of the 1st International
Conference on Global Interoperability for language
Resources, pp.114-121.
Kozawa, S., Tohyama, H., Uchimoto, K., Matsubara,
S., and Isahara. H. 2008. Automatic Acquisition of
Usage Infor-mation for Language Resources, In pro-
ceedings of the 6th edition of the Language Re-
sources and Evaluation Conference.
OLAC (Open Language Archives Communi-ty), 2008.
Searching of OLAC Metadata: User Guide, http://
www.language-archives.org/tools/search/searchDoc.html
Yoshihito Hayashi, Thierry Declerck, Paul Buitelaar,
Monica Monachini. 2008. Ontologies for a Global
Language Infrastructure, In proceedings of the 1st
International Conference on Global Interoperability
for language Resources, pp.105-112.
122
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570?579,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Improving Dependency Parsing with Subtrees from Auto-Parsed Data
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, kazama, uchimoto, torisawa}@nict.go.jp
Abstract
This paper presents a simple and effective
approach to improve dependency parsing
by using subtrees from auto-parsed data.
First, we use a baseline parser to parse
large-scale unannotated data. Then we ex-
tract subtrees from dependency parse trees
in the auto-parsed data. Finally, we con-
struct new subtree-based features for pars-
ing algorithms. To demonstrate the ef-
fectiveness of our proposed approach, we
present the experimental results on the En-
glish Penn Treebank and the Chinese Penn
Treebank. These results show that our ap-
proach significantly outperforms baseline
systems. And, it achieves the best accu-
racy for the Chinese data and an accuracy
which is competitive with the best known
systems for the English data.
1 Introduction
Dependency parsing, which attempts to build de-
pendency links between words in a sentence, has
experienced a surge of interest in recent times,
owing to its usefulness in such applications as
machine translation (Nakazawa et al, 2006) and
question answering (Cui et al, 2005). To ob-
tain dependency parsers with high accuracy, super-
vised techniques require a large amount of hand-
annotated data. While hand-annotated data are
very expensive, large-scale unannotated data can
be obtained easily. Therefore, the use of large-
scale unannotated data in training is an attractive
idea to improve dependency parsing performance.
In this paper, we present an approach that ex-
tracts subtrees from dependency trees in auto-
parsed data to improve dependency parsing. The
auto-parsed data are generated from large-scale
unannotated data by using a baseline parser. Then,
from dependency trees in the data, we extract dif-
ferent types of subtrees. Finally, we represent
subtree-based features on training data to train de-
pendency parsers.
The use of auto-parsed data is not new. How-
ever, unlike most of the previous studies (Sagae
and Tsujii, 2007; Steedman et al, 2003) that im-
proved the performance by using entire trees from
auto-parsed data, we exploit partial information
(i.e., subtrees) in auto-parsed data. In their ap-
proaches, they used entire auto-parsed trees as
newly labeled data to train the parsing models,
while we use subtree-based features and employ
the original gold-standard data to train the mod-
els. The use of subtrees instead of complete trees
can be justified by the fact that the accuracy of par-
tial dependencies is much higher than that of en-
tire dependency trees. Previous studies (McDon-
ald and Pereira, 2006; Yamada and Matsumoto,
2003; Zhang and Clark, 2008) show that the accu-
racies of complete trees are about 40% for English
and about 35% for Chinese, while the accuracies
of relations between two words are much higher:
about 90% for English and about 85% for Chinese.
From these observations, we may conjecture that
it is possible to conduct a more effective selection
by using subtrees as the unit of information.
The use of word pairs in auto-parsed data was
tried in van Noord (2007) and Chen et al (2008).
However, the information on word pairs is limited.
To provide richer information, we consider more
words besides word pairs. Specifically, we use
subtrees containing two or three words extracted
from dependency trees in the auto-parsed data. To
demonstrate the effectiveness of our proposed ap-
proach, we present experimental results on En-
570
glish and Chinese data. We show that this sim-
ple approach greatly improves the accuracy and
that the use of richer structures (i.e, word triples)
indeed gives additional improvement. We also
demonstrate that our approach and other improve-
ment techniques (Koo et al, 2008; Nivre and Mc-
Donald, 2008) are complementary and that we can
achieve very high accuracies when we combine
our method with other improvement techniques.
Specifically, we achieve the best accuracy for the
Chinese data.
The rest of this paper is as follows: Section 2
introduces the background of dependency parsing.
Section 3 proposes an approach for extracting sub-
trees and represents the subtree-based features for
dependency parsers. Section 4 explains the ex-
perimental results and Section 5 discusses related
work. Finally, in section 6 we draw conclusions.
2 Dependency parsing
Dependency parsing assigns head-dependent rela-
tions between the words in a sentence. A sim-
ple example is shown in Figure 1, where an arc
between two words indicates a dependency rela-
tion between them. For example, the arc between
?ate? and ?fish? indicates that ?ate? is the head of
?fish? and ?fish? is the dependent. The arc be-
tween ?ROOT? and ?ate? indicates that ?ate? is the
ROOT of the sentence.
ROOT    I    ate    the    fish    with    a    fork    .
Figure 1: Example for dependency structure
2.1 Parsing approach
For dependency parsing, there ar two main
types of parsing models (Nivre and McDonald,
2008): graph-based model and transition-based
model, which achieved state-of-the-art accuracy
for a wide range of languages as shown in recent
CoNLL shared tasks (Buchholz et al, 2006; Nivre
et al, 2007). Our subtree-based features can be
applied in both of the two parsing models.
In this paper, as the base parsing system, we
employ the graph-based MST parsing model pro-
posed by McDonald et al (2005) and McDonald
and Pereira (2006), which uses the idea of Max-
imum Spanning Trees of a graph and large mar-
gin structured learning algorithms. The details
of parsing model were presented in McDonald et
al. (2005) and McDonald and Pereira (2006).
2.2 Baseline Parser
In the MST parsing model, there are two well-used
modes: the first-order and the second-order. The
first-order model uses first-order features that are
defined over single graph edges and the second-
order model adds second-order features that are
defined on adjacent edges.
For the parsing of unannotated data, we use the
first-order MST parsing model, because we need
to parse a large number of sentences and the parser
must be fast. We call this parser the Baseline
Parser.
3 Our approach
In this section, we describe our approach of ex-
tracting subtrees from unannotated data. First,
we preprocess unannotated data using the Baseline
Parser and obtain auto-parsed data. Subsequently,
we extract the subtrees from dependency trees in
the auto-parsed data. Finally, we generate subtree-
based features for the parsing models.
3.1 Subtrees extraction
To ease explanation, we transform the dependency
structure into a more tree-like structure as shown
in Figure 2, the sentence is the same as the one in
Figure 1.
ate
I                             fish      with                    .
the                                       fork
ROOT
a
I       ate      the      fish      with      a      fork .
Figure 2: Example for dependency structure in
tree-format
Our task is to extract subtrees from dependency
trees. If a subtree contains two nodes, we call it a
bigram-subtree. If a subtree contains three nodes,
we call it a trigram-subtree.
3.2 List of subtrees
We extract subtrees from dependency trees and
store them in list L
st
. First, we extract bigram-
subtrees that contain two words. If two words have
571
a dependency relation in a tree, we add these two
words as a subtree into list L
st
. Similarly, we can
extract trigram-subtrees. Note that the dependency
direction and the order of the words in the original
sentence are important in the extraction. To enable
this, the subtrees are encoded in the string format
that is expressed as st = w : wid : hid(?w :
wid : hid)+
1
, where w refers to a word in the
subtree, wid refers to the ID (starting from 1) of
a word in the subtree (words are ordered accord-
ing to the positions of the original sentence)
2
, and
hid refers to an ID of the head of the word (hid=0
means that this word is the root of a subtree). For
example, ?ate? and ?fish? have a right dependency
arc in the sentence shown in Figure 2. So the
subtree is encoded as ?ate:1:0-fish:2:1?. Figure 3
shows all the subtrees extracted from the sentence
in Figure 2, where the subtrees in (a) are bigram-
subtrees and the ones in (b) are trigram-subtrees.
ateI I:1:1-ate:2:0atefish ate:1:0-fish:2:1
atefish  with ate:1:0-fish:2:1-with:3:1
atewith ate:1:0-with:2:1
ate
.
ate:1:0-.:2:1
fishthe the:1:1-fish:2:0
with fork with:1:0-fork:2:1forka a:1:1-fork:2:0
ate
with   . ate:1:0-with:2:1-.:3:1(b)
(a)
Figure 3: Examples of subtrees
Note that we only used the trigram-subtrees
containing a head, its dependent d1, and d1?s
leftmost right sibling
3
. We could not consider
the case where two children are on different
sides
4
of the head (for instance, ?I? and ?fish?
for ?ate? in Figure 2). We also do not use the
child-parent-grandparent type (grandparent-type
in short) trigram-subtrees. These are due to the
limitations of the parsing algorithm of (McDonald
and Pereira, 2006), which does not allow the fea-
tures defined on those types of trigram-subtrees.
We extract the subtrees from the auto-parsed
data, then merge the same subtrees into one en-
try, and count their frequency. We eliminate all
subtrees that occur only once in the data.
1
+ refers to matching the preceding element one or more
times and is the same as a regular expression in Perl.
2
So, wid is in fact redundant but we include it for ease of
understanding.
3
Note that the order of the siblings is based on the order
of the words in the original sentence.
4
Here, ?side? means the position of a word relative to the
head in the original sentence.
3.3 Subtree-based features
We represent new features based on the extracted
subtrees and call them subtree-based features. The
features based on bigram-subtrees correspond to
the first-order features in the MST parsing model
and those based on trigram-subtrees features cor-
respond to the second-order features.
We first group the extracted subtrees into dif-
ferent sets based on their frequencies. After ex-
periments with many different threshold settings
on development data sets, we chose the follow-
ing way. We group the subtrees into three sets
corresponding to three levels of frequency: ?high-
frequency (HF)?, ?middle-frequency (MF)?, and
?low-frequency (LF)?. HF, MF, and LF are used
as set IDs for the three sets. The following are the
settings: if a subtree is one of the TOP-10% most
frequent subtrees, it is in set HF; else if a subtree is
one of the TOP-20% subtrees, it is in set MF; else
it is in set LF. Note that we compute these levels
within a set of subtrees with the same number of
nodes. We store the set ID for every subtree in
L
st
. For example, if subtree ?ate:1:0-with:2:1? is
among the TOP-10%, its set ID is HF.
3.3.1 First-order subtree-based features
The first-order features are based on bigram-
subtrees that are related to word pairs. We gener-
ate new features for a head h and a dependent d in
the parsing process. Figure 4-(a)
5
shows the words
and their surrounding words, where h
?1
refers to
the word to the left of the head in the sentence,
h
+1
refers to the word to the right of the head, d
?1
refers to the word to the left of the dependent, and
d
+1
refers to the word to the right of the depen-
dent. Temporary bigram-subtrees are formed by
word pairs that are linked by dashed-lines in the
figure. Then we retrieve these subtrees in L
st
to
get their set IDs (if a subtree is not included in
L
st
, its set ID is ZERO. That is, we have four sets:
HF, MF, LF, and ZERO.).
Then we generate first-order subtree-based fea-
tures, consisting of indicator functions for set IDs
of the retrieved bigram-subtrees. When generating
subtree-based features, each dashed line in Figure
4-(a) triggers a different feature.
To demonstrate how to generate first-order
subtree-based features, we use an example that is
as follows. Suppose that we are going to parse the
sentence ?He ate the cake with a fork.? as shown
5
Please note that d could be before h.
572
? h
-1 h      h+1 ? d-1     d      d+1  ?
(a)
(b)
? h      ? d1 ? d2 ?
Figure 4: Word pairs and triple for feature repre-
sentation
in Figure 5, where h is ?ate? and d is ?with?.
We can generate the features for the pairs linked
by dashed-lines, such as h ? d, h ? d
+1
and so
on. Then we have the temporary bigram-subtrees
?ate:1:0-with:2:1? for h ? d and ?ate:1:0-a:2:1?
for h ? d
+1
, and so on. If we can find subtree
?ate:1:0-with:2:1? for h ? d from L
st
with set ID
HF, we generate the feature ?H-D:HF?, and if we
find subtree ?ate:1:0-a:2:1? for h?d
+1
with set ID
ZERO, we generate the feature ?H-D+1:ZERO?.
The other three features are also generated simi-
larly.
He    ate    the    cake    with    a    fork    .
h
-1 h       h+1 d-1 d      d+1
Figure 5: First-order subtree-based features
3.3.2 Second-order subtree-based features
The second-order features are based on trigram-
subtrees that are related to triples of words. We
generate features for a triple of a head h, its de-
pendent d1, and d1?s right-leftmost sibling d2.
The triple is shown in Figure 4-(b). A temporary
trigram-subtree is formed by the word forms of h,
d1, and d2. Then we retrieve the subtree in L
st
to
get its set ID. In addition, we consider the triples
of ?h-NULL?
6
, d1, and d2, which means that we
only check the words of sibling nodes without
checking the head word.
Then, we generate second-order subtree-based
features, consisting of indicator functions for set
IDs of the retrieved trigram-subtrees.
6
h-NULL is a dummy token
We also generate combined features involving
the set IDs and part-of-speech tags of heads, and
the set IDs and word forms of heads. Specifically,
for any feature related to word form, we remove
this feature if the word is not one of the Top-N
most frequent words in the training data. We used
N=1000 for the experiments in this paper. This
method can reduce the size of the feature sets.
In this paper, we only used bigram-subtrees and
the limited form of trigram-subtrees, though in
theory we can use k-gram-subtrees, which are lim-
ited in the same way as our trigram subtrees, in
(k-1)th-order MST parsing models mentioned in
McDonald and Pereira (2006) or use grandparent-
type trigram-subtrees in parsing models of Car-
reras (2007). Although the higher-order MST
parsing models will be slow with exact inference,
requiring O(n
k
) time (McDonald and Pereira,
2006), it might be possible to use higher-order k-
gram subtrees with approximated parsing model
in the future. Of course, our method can also be
easily extended to the labeled dependency case.
4 Experiments
In order to evaluate the effectiveness of the
subtree-based features, we conducted experiments
on English data and Chinese Data.
For English, we used the Penn Treebank (Mar-
cus et al, 1993) in our experiments and the tool
?Penn2Malt?
7
to convert the data into dependency
structures using a standard set of head rules (Ya-
mada and Matsumoto, 2003). To match previ-
ous work (McDonald et al, 2005; McDonald and
Pereira, 2006; Koo et al, 2008), we split the data
into a training set (sections 2-21), a development
set (Section 22), and a test set (section 23). Fol-
lowing the work of Koo et al (2008), we used
the MXPOST (Ratnaparkhi, 1996) tagger trained
on training data to provide part-of-speech tags for
the development and the test set, and we used 10-
way jackknifing to generate tags for the training
set. For the unannotated data, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ text.
8
We used the MX-
POST tagger trained on training data to assign
part-of-speech tags and used the Basic Parser to
process the sentences of the BLLIP corpus.
For Chinese, we used the Chinese Treebank
7
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
8
We ensured that the text used for extracting subtrees did
not include the sentences of the Penn Treebank.
573
(CTB) version 4.0
9
in the experiments. We also
used the ?Penn2Malt? tool to convert the data and
created a data split: files 1-270 and files 400-931
for training, files 271-300 for testing, and files
301-325 for development. We used gold standard
segmentation and part-of-speech tags in the CTB.
The data partition and part-of-speech settings were
chosen to match previous work (Chen et al, 2008;
Yu et al, 2008). For the unannotated data, we
used the PFR corpus
10
, which has approximately
15 million words whose segmentation and POS
tags are given. We used its original segmentation
though there are differences in segmentation pol-
icy between CTB and this corpus. As for POS
tags, we discarded the original POS tags and as-
signed CTB style POS tags using a TNT-based
tagger (Brants, 2000) trained on the training data.
We used the Basic Parser to process all the sen-
tences of the PFR corpus.
We measured the parser quality by the unla-
beled attachment score (UAS), i.e., the percentage
of tokens (excluding all punctuation tokens) with
the correct HEAD. And we also evaluated on com-
plete dependency analysis.
4.1 Experimental Results
In our experiments, we used MSTParser, a
freely available implementation
11
of the first- and
second-order MST parsing models. For baseline
systems, we used the first- and second-order basic
features, which were the same as the features used
by McDonald and Pereira (2006), and we used
the default settings of MSTParser throughout the
paper: iters=10; training-k=1; decode-type=proj.
We implemented our systems based on the MST-
Parser by incorporating the subtree-based features.
4.1.1 Main results of English data
English
UAS Complete
Ord1 90.95 37.45
Ord1s 91.76(+0.81) 40.68
Ord2 91.71 42.88
Ord2s 92.51(+0.80) 46.19
Ord2b 92.28(+0.57) 45.44
Ord2t 92.06(+0.35) 42.96
Table 1: Dependency parsing results for English
9
http://www.cis.upenn.edu/?chinese/.
10
http://www.icl.pku.edu.
11
http://mstparser.sourceforge.net
The results are shown in Table 1, where
Ord1/Ord2 refers to a first-/second-order
MSTParser with basic features, Ord1s/Ord2s
refers to a first-/second-order MSTParser with
basic+subtree-based features, and the improve-
ments by the subtree-based features over the basic
features are shown in parentheses. Note that
we use both the bigram- and trigram- subtrees
in Ord2s. The parsers using the subtree-based
features consistently outperformed those using
the basic features. For the first-order parser,
we found that there is an absolute improvement
of 0.81 points (UAS) by adding subtree-based
features. For the second-order parser, we got an
absolute improvement of 0.8 points (UAS) by
including subtree-based features. The improve-
ments of parsing with subtree-based features were
significant in McNemar?s Test (p < 10
?6
).
We also checked the sole effect of bigram- and
trigram-subtrees. The results are also shown in
Table 1, where Ord2b/Ord2t refers to a second-
order MSTParser with bigram-/trigram-subtrees
only. The results showed that trigram-subtrees can
provide further improvement, although the effect
of the bigram-subtrees seemed larger.
4.1.2 Comparative results of English data
Table 2 shows the performance of the systems
that were compared, where Y&M2003 refers to
the parser of Yamada and Matsumoto (2003),
CO2006 refers to the parser of Corston-Oliver et
al. (2006), Hall2006 refers to the parser of Hall
et al (2006), Wang2007 refers to the parser of
Wang et al (2007), Z&C 2008 refers to the combi-
nation graph-based and transition-based system of
Zhang and Clark (2008), KOO08-dep1c/KOO08-
dep2c refers to a graph-based system with first-
/second-order cluster-based features by Koo et al
(2008), and Carreras2008 refers to the paper of
Carreras et al (2008). The results showed that
Ord2s performed better than the first five systems.
The second-order system of Koo et al (2008) per-
formed better than our systems. The reason may
be that the MSTParser only uses sibling interac-
tions for second-order, while Koo et al (2008)
uses both sibling and grandparent interactions, and
uses cluster-based features. Carreras et al (2008)
reported a very high accuracy using information of
constituent structure of the TAG grammar formal-
ism. In our systems, we did not use such knowl-
edge.
Our subtree-based features could be combined
574
with the techniques presented in other work,
such as the cluster-based features in Koo et al
(2008), the integrating methods of Zhang and
Clark (2008), and Nivre and McDonald (2008),
and the parsing methods of Carreras et al (2008).
English
UAS Complete
Y&M2003 90.3 38.4
CO2006 90.8 37.6
Hall2006 89.4 36.4
Wang2007 89.2 34.4
Z&C2008 92.1 45.4
KOO08-dep1c 92.23 ?
KOO08-dep2c 93.16 ?
Carreras2008 93.5 ?
Ord1 90.95 37.45
Ord1s 91.76 40.68
Ord1c 91.88 40.71
Ord1i 91.68 41.43
Ord1sc 92.20 42.98
Ord1sci 92.60 44.28
Ord2 91.71 42.88
Ord2s 92.51 46.19
Ord2c 92.40 44.08
Ord2i 92.12 44.37
Ord2sc 92.70 46.56
Ord2sci 93.16 47.15
Table 2: Dependency parsing results for English,
for our parsers and previous work
To demonstrate that our approach and other
work are complementary, we thus implemented
a system using all the techniques we had at hand
that used subtree- and cluster-based features
and applied the integrating method of Nivre and
McDonald (2008). We used the word clustering
tool
12
, which was used by Koo et al (2008), to
produce word clusters on the BLLIP corpus. The
cluster-based features were the same as the fea-
tures used by Koo et al (2008). For the integrating
method, we used the transition MaxEnt-based
parser of Zhao and Kit (2008) because it was
faster than the MaltParser. The results are shown
in the bottom part of Table 2, where Ord1c/Ord2c
refers to a first-/second-order MSTParser with
cluster-based features, Ord1i/Ordli refers to a first-
/second-order MSTParser with integrating-based
features, Ord1sc/Ord2sc refers to a first-/second-
order MSTParser with subtree-based+cluster-
based features, and Ord1sci/Ord2sci refers to
a first-/second-order MSTParser with subtree-
based+cluster-based+integrating-based features.
Ord1c/Ord2c was worse than KOO08-dep1c/-
dep2c, but Ord1sci outperformed KOO08-dep1c
12
http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
and Ord2sci performed similarly to KOO08-dep2c
by using all of the techniques we had. These
results indicated that subtree-based features can
provide different information and work well with
other techniques.
4.1.3 Main results of Chinese data
The results are shown in Table 3 where abbrevia-
tions are the same as in Table 1. As in the English
experiments, parsers with the subtree-based fea-
tures outperformed parsers with the basic features,
and second-order parsers outperformed first-order
parsers. For the first-order parser, the subtree-
based features provided 1.3 absolute points im-
provement. For the second-order parser, the
subtree-based features achieved an absolute im-
provement of 1.25 points. The improvements of
parsing with subtree-based features were signifi-
cant in McNemar?s Test (p < 10
?5
).
Chinese
UAS Complete
Ord1 86.38 40.80
Ord1s 87.68(+1.30) 42.24
Ord2 88.18 47.12
Ord2s 89.43(+1.25) 47.53
Ord2b 89.16(+0.98) 47.12
Ord2t 88.55(+0.37) 47.12
Table 3: Dependency parsing results for Chinese.
4.1.4 Comparative results of Chinese data
Table 4 shows the comparative results, where
Wang2007 refers to the parser of Wang et
al. (2007), Chen2008 refers to the parser of Chen
et al (2008), and Yu2008 refers to the parser of
Yu et al (2008) that is the best reported results
for this data set. And ?all words? refers to all the
sentences in test set and ?? 40 words?
13
refers to
the sentences with the length up to 40. The table
shows that our parsers outperformed previous sys-
tems.
We also implemented integrating systems for
Chinese data as well. When we applied the
cluster-based features, the performance dropped a
little. The reason may be that we are using gold-
POS tags for Chinese data
14
. Thus we did not
13
Wang et al (2007) and Chen et al (2008) reported the
scores on these sentences.
14
We tried to use the cluster-based features for Chinese
with the same setting of POS tags as English data, then the
cluster-based features did provide improvement.
575
use cluster-based features for the integrating sys-
tems. The results are shown in Table 4, where
Ord1si/Ord2si refers to the first-order/second-
order system with subtree-based+intergrating-
based features. We found that the integrating sys-
tems provided better results. Overall, we have
achieved a high accuracy, which is the best known
result for this dataset.
Zhang and Clark (2008) and Duan et al (2007)
reported results on a different data split of Penn
Chinese Treebank. We also ran our systems
(Ord2s) on their data and provided UAS 86.70
(for non-root words)/77.39 (for root words), better
than their results: 86.21/76.26 in Zhang and Clark
(2008) and 84.36/73.70 in Duan et al (2007).
Chinese
all words ? 40 words
UAS Complete UAS Complete
Wang2007 ? ? 86.6 28.4
Chen2008 86.52 ? 88.4 ?
Yu2008 87.26 ? ? ?
Ord1s 87.68 42.24 91.11 54.40
Ord1si 88.24 43.96 91.32 55.93
Ord2s 89.43 47.53 91.67 59.77
Ord2si 89.91 48.56 92.34 62.83
Table 4: Dependency parsing results for Chinese,
for our parsers and for previous work
4.1.5 Effect of different sizes of unannotated
data
Here, we consider the improvement relative to the
sizes of the unannotated data. Figure 6 shows the
results of first-order parsers with different num-
bers of words in the unannotated data. Please note
that the size of full English unannotated data is
43M and the size of full Chinese unannotated data
is 15M. From the figure, we found that the parser
obtained more benefits as we added more unanno-
tated data.
 86
 87
 88
 89
 90
 91
 92
4332168420
U
A
S
Size of unannotated data(M)
EnglishChinese
Figure 6: Results with different sizes of large-
scale unannotated data.
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  1  2  3  4  5  6
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of unknown words
BetterNoChangeWorse
Figure 7: Improvement relative to unknown words
for English
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1  2  3  4  5  6
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of unknown words
BetterNoChangeWorse
Figure 8: Improvement relative to unknown words
for Chinese
4.2 Additional Analysis
In this section, we investigated the results on
sentence level from different views. For Fig-
ures 7-12, we classified each sentence into one of
three classes: ?Better? for those where the pro-
posed parsers provided better results relative to
the parsers with basic features, ?Worse? for those
where the proposed parsers provided worse results
relative to the basic parsers, and ?NoChange? for
those where the accuracies remained the same.
4.2.1 Unknown words
Here, we consider the unknown word
15
problem,
which is an important issue for parsing. We cal-
culated the number of unknown words in one sen-
tence, and listed the changes of the sentences with
unknown words. Here, we compared the Ord1
system and the Ord1s system.
Figures 7 and 8 show the results, where the x
axis refers to the number of unknown words in one
sentence and the y axis shows the percentages of
the three classes. For example, for the sentences
having three unknown words in the Chinese data,
31.58% improved, 23.68% worsened, and 44.74%
were unchanged. We did not show the results of
15
An unknown word is a word that is not included in the
training data.
576
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
43210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of CCs
BetterNoChangeWorse
Figure 9: Improvement relative to number of
conjunctions for English
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
3210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of CCs
BetterNoChangeWorse
Figure 10: Improvement relative to number of
conjunctions for Chinese
the sentences with more than six unknown words
because their numbers were very small. The Bet-
ter and Worse curves showed that our approach al-
ways provided better results. The results indicated
that the improvements apparently became larger
when the sentences had more unknown words for
the Chinese data. And for the English data, the
graph also showed the similar trend, although the
improvements for the sentences have three and
four unknown words were slightly less than the
others.
4.2.2 Coordinating conjunctions
We analyzed our new parsers? behavior for coordi-
nating conjunction structures, which is a very dif-
ficult problem for parsing (Kawahara and Kuro-
hashi, 2008). Here, we compared the Ord2 system
with the Ord2s system.
Figures 9 and 10 show how the subtree-based
features affect accuracy as a function of the num-
ber of conjunctions, where the x axis refers to the
number of conjunctions in one sentence and the
y axis shows the percentages of the three classes.
The figures indicated that the subtree-based fea-
tures improved the coordinating conjunction prob-
lem. In the trigram-subtree list, many subtrees
are related to coordinating conjunctions, such as
?utilities:1:3 and:2:3 businesses:3:0? and ?pull:1:0
and:2:1 protect:3:1?. These subtrees can provide
additional information for parsing models.
4.2.3 PP attachment
We analyzed our new parsers? behavior for
preposition-phrase attachment, which is also a dif-
ficult task for parsing (Ratnaparkhi et al, 1994).
We compared the Ord2 system with the Ord2s sys-
tem. Figures 11 and 12 show how the subtree-
based features affect accuracy as a function of the
number of prepositions, where the x axis refers to
the number of prepositions in one sentence and the
y axis shows the percentages of the three classes.
The figures indicated that the subtree-based fea-
tures improved preposition-phrase attachment.
5 Related work
Our approach is to incorporate unannotated data
into parsing models for dependency parsing. Sev-
eral previous studies relevant to our approach have
been conducted.
Chen et al (2008) previously proposed an ap-
proach that used the information on short de-
pendency relations for Chinese dependency pars-
ing. They only used the word pairs within two
word distances for a transition-based parsing al-
gorithm. The approach in this paper differs in
that we use richer information on trigram-subtrees
besides bigram-subtrees that contain word pairs.
And our work is focused on graph-based parsing
models as opposed to transition-based models. Yu
et al (2008) constructed case structures from auto-
parsed data and utilized them in parsing. Com-
pared with their method, our method is much sim-
pler but has great effects.
Koo et al (2008) used the Brown algorithm to
produce word clusters on large-scale unannotated
data and represented new features based on the
clusters for parsing models. The cluster-based fea-
tures provided very impressive results. In addition,
they used the parsing model by Carreras (2007)
that applied second-order features on both sibling
and grandparent interactions. Note that our ap-
proach and their approach are complementary in
that we can use both subtree- and cluster-based
features for parsing models. The experimental re-
sults showed that we achieved better accuracy for
first-order models when we used both of these two
types of features.
Sagae and Tsujii (2007) presented an co-
training approach for dependency parsing adap-
577
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  1  2  3  4  5  6  7
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of prepositions
BetterNoChangeWorse
Figure 11: Improvement relative to number of
prepositions for English
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
3210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of prepositions
BetterNoChangeWorse
Figure 12: Improvement relative to number of
prepositions for Chinese
tation. They used two parsers to parse the sen-
tences in unannotated data and selected only iden-
tical results produced by the two parsers. Then,
they retrained a parser on newly parsed sentences
and the original labeled data. Our approach repre-
sents subtree-based features on the original gold-
standard data to retrain parsers. McClosky et
al. (2006) presented a self-training approach for
phrase structure parsing and the approach was
shown to be effective in practice. However,
their approach depends on a high-quality reranker,
while we simply augment the features of an ex-
isting parser. Moreover, we could use the output
of our systems for co-training/self-training tech-
niques.
6 Conclusions
We present a simple and effective approach to
improve dependency parsing using subtrees from
auto-parsed data. In our method, first we use a
baseline parser to parse large-scale unannotated
data, and then we extract subtrees from depen-
dency parsing trees in the auto-parsed data. Fi-
nally, we construct new subtree-based features for
parsing models. The results show that our ap-
proach significantly outperforms baseline systems.
We also show that our approach and other tech-
niques are complementary, and then achieve the
best reported accuracy for the Chinese data and an
accuracy that is competitive with the best known
systems for the English data.
References
T. Brants. 2000. TnT?a statistical part-of-speech tag-
ger. Proceedings of ANLP, pages 224?231.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. Proceedings of CoNLL-X.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL 2008, pages 9?16, Manchester, Eng-
land, August. Coling 2008 Organizing Committee.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
E. Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and
M. Johnson. 2000. BLLIP 1987-89 WSJ Corpus
Release 1, LDC2000T43. Linguistic Data Consor-
tium.
WL. Chen, D. Kawahara, K. Uchimoto, YJ. Zhang, and
H. Isahara. 2008. Dependency parsing with short
dependency relations in unlabeled data. In Proceed-
ings of IJCNLP 2008.
S. Corston-Oliver, A. Aue, Kevin. Duh, and Eric Ring-
ger. 2006. Multilingual dependency parsing using
bayes point machines. In HLT-NAACL2006.
H. Cui, RX. Sun, KY. Li, MY. Kan, and TS. Chua.
2005. Question answering passage retrieval us-
ing dependency relations. In Proceedings of SIGIR
2005, pages 400?407, New York, NY, USA. ACM.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, War-
saw, Poland.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative classifiers for deterministic depen-
dency parsing. In In Proceedings of CoLING-ACL.
D. Kawahara and S. Kurohashi. 2008. Coordination
disambiguation without any similarities. In Pro-
ceedings of Coling 2008, pages 425?432, Manch-
ester, UK, August.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
ticss, 19(2):313?330.
578
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL 2005.
T. Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper nlp. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase at-
tachment. In Proceedings of HLT, pages 250?255.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser en-
sembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050.
M. Steedman, M. Osborne, A. Sarkar, S. Clark,
R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and
J. Crim. 2003. Bootstrapping statistical parsers
from small datasets. In Proceedings of EACL 2003,
pages 331?338.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of IWPT-07, June.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of IJCAI2007.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automat-
ically constructed case structures. In Proceedings
of Coling 2008, pages 1049?1056, Manchester, UK,
August.
Y. Zhang and S. Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562?571, Hon-
olulu, Hawaii, October.
H. Zhao and CY. Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceedings of CoNLL
2008, pages 203?207, Manchester, England, Au-
gust.
579
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 658?667,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Can Chinese Phonemes Improve Machine Transliteration?:
A Comparative Study of English-to-Chinese Transliteration Models
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
Inspired by the success of English
grapheme-to-phoneme research in speech
synthesis, many researchers have pro-
posed phoneme-based English-to-Chinese
transliteration models. However, such ap-
proaches have severely suffered from the
errors in Chinese phoneme-to-grapheme
conversion. To address this issue,
we propose a new English-to-Chinese
transliteration model and make system-
atic comparisons with the conventional
models. Our proposed model relies on
the joint use of Chinese phonemes and
their corresponding English graphemes
and phonemes. Experiments showed that
Chinese phonemes in our proposed model
can contribute to the performance im-
provement in English-to-Chinese translit-
eration.
1 Introduction
1.1 Motivation
Transliteration, i.e., phonetic translation, is com-
monly used to translate proper names and techni-
cal terms across languages. A variety of English-
to-Chinese machine transliteration models has
been proposed in the last decade (Meng et al,
2001; Gao et al, 2004; Jiang et al, 2007; Lee
and Chang, 2003; Li et al, 2004; Li et al, 2007;
Wan and Verspoor, 1998; Virga and Khudanpur,
2003). They can be categorized into those based
on Chinese phonemes (Meng et al, 2001; Gao
et al, 2004; Jiang et al, 2007; Lee and Chang,
2003; Wan and Verspoor, 1998; Virga and Khu-
danpur, 2003) and those that don?t rely on Chinese
phonemes (Li et al, 2004; Li et al, 2007).
Inspired by the success of English grapheme-to-
phoneme research in speech synthesis, many re-
searchers have proposed phoneme-based English-
to-Chinese transliteration models. In these ap-
proaches, Chinese phonemes are generated from
English graphemes or phonemes, and then the
Chinese phonemes are converted into Chinese
graphemes (or characters), where Chinese Pinyin
strings1 are used for representing a syllable-level
Chinese phoneme sequence. Despite its high ac-
curacy in generating Chinese phonemes from En-
glish, this approach has severely suffered from er-
rors in Chinese phoneme-to-grapheme conversion,
mainly caused by Chinese homophone confusion
? one Chinese Pinyin string can correspond to sev-
eral Chinese characters (Li et al, 2004). For ex-
ample, the Pinyin string ?LI? corresponds to such
different Chinese characters as,, and. For
this reason, it has been reported that English-to-
Chinese transliteration without Chinese phonemes
outperforms that with Chinese phonemes (Li et al,
2004).
Then ?Can Chinese phonemes improve
English-to-Chinese transliteration, if we can re-
duce the errors in Chinese phoneme-to-grapheme
conversion?? Our research starts from this
question.
1.2 Our Approach
Previous approaches using Chinese phonemes
have relied only on Chinese phonemes in Chi-
nese phoneme-to-grapheme conversion. However,
the simple use of Chinese phonemes doesn?t al-
ways provide a good clue to reduce the ambi-
guity in Chinese phoneme-to-grapheme conver-
sion. Let us explain with an example, the Chinese
transliteration of Greeley in Table 1, where Chi-
nese phonemes are represented in terms of Chi-
nese Pinyin strings and English phonemes are rep-
resented by ARPAbet symbols2.
In Table 1, Chinese Pinyin string ?LI? corre-
sponds to two different Chinese characters, and
1Pinyin, the most commonly used Romanization sys-
tem for Chinese characters, faithfully represents Chinese
658
Table 1: Chinese Pinyin string ?LI? and its corre-
sponding Chinese characters in Chinese transliter-
ation of Greeley
English grapheme g ree ley
English phoneme G R IY L IY
Chinese Pinyin GE LI LI
Chinese character   
. It seems difficult to find evidence for select-
ing the correct Chinese character corresponding to
each Chinese Pinyin string ?LI? by just looking
at the sequence of Chinese Pinyin strings ?GE LI
LI.? However, English graphemes (ree and ley) or
phonemes (?R IY? and ?L IY?) corresponding to
Chinese Pinyin string ?LI?, especially their conso-
nant parts (r and l in the English graphemes and
?R? and ?L? in the English phonemes), provide
strong evidence to resolve the ambiguity. Thus,
we can easily find rules for the conversion from
Chinese Pinyin string ?LI? to and as follows:
? ? ?R IY?, LI ? ?
? ? ?L IY?, LI ? ?
Based on the observation, we propose an
English-to-Chinese transliteration model based on
the joint use of Chinese phonemes and their corre-
sponding English graphemes and phonemes. We
define a set of English-to-Chinese transliteration
models and categorize them into the following
three classes:
? M
I
: Models Independent of Chinese
phonemes
? M
S
: Models based on Simple use of Chinese
phonemes
? M
J
: Models based on Joint use of Chi-
nese phonemes and English graphemes and
phonemes that correspond to our proposed
model.
Our comparison among the three types of translit-
eration models can be summarized as follows.
? The M
I
models relying on either English
graphemes or phonemes could not outper-
form those based on both English graphemes
and phonemes.
phonemes and syllables (Yin and Felley, 1990).
2http://www.cs.cmu.edu/
?
laura/pages/
arpabet.ps
? The M
S
models always showed the worst
performance due to the severe error rate in
Chinese phoneme-to-grapheme conversion.
? The M
J
models significantly reduced er-
rors in Chinese phoneme-to-grapheme con-
version; thus they achieved the best perfor-
mance.
The rest of this paper is organized as follows.
Section 2 introduces the notations used through-
out this paper. Section 3 describes the translitera-
tion models we compared. Section 4 describes our
tests and results. Section 5 concludes the paper
with a summary.
2 Preliminaries
Let E
G
be an English word composed of n English
graphemes, and let E
P
be a sequence of English
phonemes that represents the pronunciation of E
G
.
Let C
G
be a sequence of Chinese graphemes cor-
responding to the Chinese transliteration of E
G
,
and let C
P
be a sequence of Chinese phonemes
that represents the pronunciation of C
G
.
C
P
corresponds to a sequence of the Chinese
Pinyin strings of C
G
. Because a Chinese Pinyin
string represents the pronunciation of a sylla-
ble consisting of consonants and vowels, we di-
vide a Chinese Pinyin string into consonant and
vowel parts like ?L+I?, ?L+I+N?, and ?SH+A.?
In this paper, we define a Chinese phoneme
as the vowel and consonant parts in a Chinese
Pinyin string (e.g., ?L?, ?SH?, and ?I?). A Chi-
nese character usually corresponds to multiple
English graphemes, English phonemes, and Chi-
nese phonemes (i.e.,  corresponds to English
graphemes ree, English phonemes ?R IY?, and
Chinese phonemes ?L I? in Table 1). To repre-
sent these many-to-one correspondences, we use
the well-known BIO labeling scheme to represent
a Chinese character, where B and I represent the
beginning and inside/end of the Chinese charac-
ters, respectively, and O is not used. Each Chi-
nese phoneme corresponds to a Chinese character
with B and I labels. For example, Chinese charac-
ter ?? in Table 1 can be represented as ?:B?
and ?:I?, where ?:B? and ?:I? correspond
to Chinese phonemes ?L? and ?I?, respectively. In
this paper, we define a Chinese grapheme as a Chi-
nese character represented with a BIO label, e.g.,
?:B? and ?:I.?
659
Table 2: eg
i
and its corresponding ep
i
, cp
i
, and cg
i
in Greeley and its corresponding Chinese translit-
eration ??
i 1 2 3 4 5 6 7
E
G
g r e e l e y
E
P
G R IY ? L IY ?
C
P
GE L I ? L I ?
GE LI ? LI ?
C
G
:B :B :I ? :B :I ?
  ?  ?
Then E
P
, C
P
, and C
G
can be segmented into a
series of sub-strings, each of which corresponds to
an English grapheme in E
G
. We can thus write
? E
G
= eg
1
, ? ? ? , eg
n
= eg
n
1
? E
P
= ep
1
, ? ? ? , ep
n
= ep
n
1
? C
P
= cp
1
, ? ? ? , cp
n
= cp
n
1
? C
G
= cg
1
, ? ? ? , cg
n
= cg
n
1
where eg
i
, ep
i
, cp
i
, and cg
i
represent the ith
English grapheme, English phonemes, Chinese
phonemes, and Chinese graphemes corresponding
to eg
i
, respectively.
Based on the definition, we model English-
to-Chinese transliteration so that each English
grapheme is tagged with its corresponding En-
glish phonemes, Chinese phonemes, and Chinese
graphemes. Table 2 illustrates eg
i
, ep
i
, cp
i
, and
cg
i
with the same example listed in Table 1 (En-
glish word Greeley and its corresponding Chinese
transliteration ??)3, where ? represents an
empty string.
3 Transliteration Model
We defined eighteen transliteration models to be
compared. These transliteration models are clas-
sified into three classes, M
I
, M
S
, andM
J
as de-
scribed in Section 1.2; each class has three basic
transliteration models and three hybrid ones. In
this section, we first describe the basic translit-
eration models in each class by focusing on the
main difference among the three classes and then
describe the hybrid transliteration models.
3We performed alignment between E
G
and E
P
and be-
tween E
P
and C
P
in a similar manner presented in Li et al
(2004). Then the two alignment results were merged using
E
P
as a pivot. Finally, we made a correspondence relation
among eg
i
, ep
i
, cp
i
, and cg
i
using the merged alignment re-
sult and the Pinyin table.
3.1 Basic Transliteration Models
The basic transliteration models in each class are
denoted as M(x, y).
? (x, y) ? X ? Y
? x ? X = {E
G
, E
P
, E
GP
}
? y ? Y = {?, C
P
, JC
P
}
x is an English-side parameter representing En-
glish grapheme (E
G
), English phoneme (E
P
), and
the joint use of English grapheme and phoneme
(E
GP
= ?E
G
, E
P
?) that contributes to generat-
ing Chinese phonemes or Chinese graphemes in
a transliteration model. y is a Chinese-phoneme
parameter that represents a way of using Chinese
phonemes to generate Chinese graphemes in a
transliteration model. Since M(x, ?) represents
a transliteration model that does not rely on Chi-
nese phonemes, it falls intoM
I
, while M(x, C
P
)
corresponds to a transliteration model in M
S
that
only uses Chinese phonemes in Chinese phoneme-
to-grapheme conversion. M(x, JC
P
) is a translit-
eration model in theM
J
class that generates Chi-
nese transliterations based on joint use of x and
Chinese phoneme C
P
, where x ? X . Thus,
M(x, JC
P
) can be rewritten as M(x, ?x, C
P
?),
where the joint representation of x and C
P
,
?x, C
P
?, is used in Chinese phoneme-to-grapheme
conversion. The three basic models inM
J
can be
interpreted as follows:
? M(E
G
, JC
P
) = M(E
G
, ?E
G
, C
P
?)
? M(E
P
, JC
P
) = M(E
P
, ?E
P
, C
P
?)
? M(E
GP
, JC
P
) = M(E
GP
, ?E
GP
, C
P
?)
M(E
G
, JC
P
) directly converts English
graphemes into Chinese phonemes without
the help of English phonemes and then gener-
ates Chinese transliterations based on the joint
representation of English graphemes and Chi-
nese phonemes. The main difference between
M(E
P
, JC
P
) and M(E
GP
, JC
P
) lies in the
use of English graphemes to generate Chinese
phonemes and graphemes. English graphemes
are only used in English grapheme-to-phoneme
conversion, and English phonemes play a crucial
role for generating Chinese transliteration in
M(E
P
, JC
P
). Chinese phoneme-to-grapheme
conversion that relies on the joint use of English
graphemes, English phonemes, and Chinese
660
PM(E
G
,JC
P
)
(C
G
|E
G
) =
?
?C
P
P (C
P
|E
G
)? P (C
G
|E
G
, C
P
) (1)
P
M(E
P
,JC
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
P
)? P (C
G
|E
P
, C
P
) (2)
P
M(E
GP
,JC
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
G
, E
P
)? P (C
G
|E
G
, E
P
, C
P
) (3)
P
M(E
G
,C
P
)
(C
G
|E
G
) =
?
?C
P
P (C
P
|E
G
)? P (C
G
|C
P
) (4)
P
M(E
P
,C
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
P
)? P (C
G
|C
P
) (5)
P
M(E
GP
,C
P
)
(C
G
|E
G
) =
?
?C
P
?
?E
P
P (E
P
|E
G
)? P (C
P
|E
G
, E
P
)? P (C
G
|C
P
) (6)
phonemes is the key feature of M(E
GP
, JC
P
).
Because M(x, JC
P
) can be interpreted as
M(x, ?x, C
P
?), English-side parameter x de-
termines the English graphemes and phonemes,
or both jointly used with Chinese phonemes in
Chinese phoneme-to-grapheme conversion. Then
we can represent the three basic transliteration
models as in Eqs. (1)?(3), where P (C
G
|E
G
, C
P
),
P (C
G
|E
P
, C
P
), and P (C
G
|E
G
, E
P
, C
P
) are the
key points in our proposed models,M
J
.
The three basic transliteration models in M
S
? M(E
G
, C
P
), M(E
P
, C
P
), and M(E
GP
, C
P
) ?
are formulated as Eqs. (4)?(6). Chinese phoneme-
based transliteration models in the literature fall
into either M(E
G
, C
P
) or M(E
P
, C
P
) (Meng et
al., 2001; Gao et al, 2004; Jiang et al, 2007; Lee
and Chang, 2003; Wan and Verspoor, 1998; Virga
and Khudanpur, 2003). The three basic transliter-
ation models inM
S
are identical as those inM
J
,
except for the Chinese phoneme-to-grapheme con-
version method. They only depend on Chinese
phonemes in Chinese phoneme-to-grapheme con-
version represented as P (C
G
|C
P
) in Eqs. (4)?(6).
P
M(E
G
,?)
(C
G
|E
G
) = P (C
G
|E
G
) (7)
P
M(E
P
,?)
(C
G
|E
G
) (8)
=
?
?E
P
P (E
P
|E
G
)? P (C
G
|E
P
)
P
M(E
GP
,?)
(C
G
|E
G
) (9)
=
?
?E
P
P (E
P
|E
G
)? P (C
G
|E
G
, E
P
)
The three basic transliteration models in M
I
are
represented in Eqs. (7)?(9). Because theM
I
mod-
els are independent of Chinese phonemes, they are
the same as the transliteration models in the lit-
erature used for machine transliteration from En-
glish to other languages without relying on target-
language phonemes (Karimi et al, 2007; Malik,
2006; Oh et al, 2006; Sherif and Kondrak, 2007;
Yoon et al, 2007). Note that M(E
G
, ?) is the
same transliteration model as the one proposed by
Li et al (2004).
3.2 Hybrid Transliteration Models
The hybrid transliteration models in each class
are defined by discrete mixture between the prob-
ability distribution of the two basic transliter-
ation models, as in Eq. (10) (Al-Onaizan and
Knight, 2002; Oh et al, 2006), where 0 < ? <
1. We denote a hybrid transliteration model be-
tween two basic transliteration models M(x
1
, y)
and M(x
2
, y) as M(x
1
+ x
2
, y, ?), where y ?
Y = {?, C
P
, JC
P
}, x
1
6= x
2
, and x
1
, x
2
?
X = {E
G
, E
P
, E
GP
}. In this paper, we define
three types of hybrid transliteration models in each
class: M(E
G
+ E
P
, y, ?), M(E
G
+ E
GP
, y, ?),
and M(E
P
+ E
GP
, y, ?).
P
M(x
1
+x
2
,y,?)
(C
G
|E
G
) (10)
= ? ? P
M(x
1
,y)
(C
G
|E
G
)
+ (1? ?) ? P
M(x
2
,y)
(C
G
|E
G
)
3.3 Probability Estimation
Because Eqs. (1)?(9) can be estimated in a similar
way, we limit our focus to Eq. (3) in this section.
Assuming that P (E
P
|E
G
), P (C
P
|E
G
, E
P
), and
P (C
G
|E
G
, E
P
, C
P
) in Eq. (3) depend on the size
of the context window, k (k = 3 in this paper),
661
Table 3: Feature functions for P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
) with an example in Table 2, where i = 2
f
1
gram
3
(eg
i
) eg
i+2
i
= ?ree? cg
i
= ?:B?
f
2
pair
11
(cp
i?1
, cg
i?1
) cp
i?1
= ?G?, cg
i?1
= ?:B? cg
i
= ?:B?
f
3
pair
12
(cg
i?1
, cp
i?1
) cp
i
i?1
= ?GE L?, cg
i?1
= ?:B? cg
i
= ?:B?
f
4
pair
22
(cp
i?1
, cg
i?2
) eg
i
i?1
= ?gr?, epi
i?1
= ?G R? cg
i
= ?:B?
f
5
triple
1
(eg
i
, cp
i
, cg
i?1
) eg
i
= ?r?, cp
i?1
= ?GE?, cg
i?1
= ?:B? cg
i
= ?:B?
f
6
triple
2
(eg
i?1
, cg
i?1
, cp
i?1
) eg
i?1
= ?g?, cpi
i?1
= ?GE L?, cg
i?1
= ?:B? cg
i
= ?:B?
they can be simplified into a series of products in
Eqs. (11)?(13).
The maximum entropy model is used to esti-
mate the probabilities in Eqs. (11)?(13) (Berger
et al, 1996). Generally, a conditional maxi-
mum entropy model is an exponential model that
gives the conditional probability, as described in
Eq. (14), where ?
i
is the parameter to be estimated
and f
i
(a, b) is a feature function corresponding to
?
i
(Berger et al, 1996; Ratnaparkhi, 1997):
P (E
P
|E
G
) ?
?
i
P (ep
i
|ep
i?1
i?k
, eg
i+k
i?k
) (11)
P (C
P
|E
G
, E
P
) (12)
?
?
i
P (cp
i
|cp
i?1
i?k
, ?eg, ep?
i+k
i?k
)
P (C
G
|E
G
, E
P
, C
P
) (13)
?
?
i
P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
)
P (b|a) =
exp(
?
i
?
i
f
i
(a, b))
?
b
?
exp(
?
i
?
i
f
i
(a, b
?
))
(14)
f
i
(a, b) is a binary function returning TRUE
or FALSE based on context a and output b.
If f
i
(a, b)=1, its corresponding model parame-
ter ?
i
contributes toward conditional probability
P (b|a) (Berger et al, 1996; Ratnaparkhi, 1997).
The feature functions used here are defined in
terms of context predicates ? a function return-
ing TRUE or FALSE that depends on the presence
of the information in the current context (Ratna-
parkhi, 1997). Context predicates and their de-
scriptions used are given in Table 4.
N-GRAM includes gram
1
(u
j
), gram
2
(u
j
), and
gram
3
(u
j
) corresponding to a unigram, a bigram,
and a trigram, respectively. PAIR includes a pair of
unigrams (pair
11
), unigram and bigram (pair
12
),
and bigrams (pair
22
). TRIPLE includes a triple of
three unigrams (triple
1
) and a triple of two uni-
grams and one bigram (triple
2
). Note that if dif-
ferent context predicates represent the same con-
text, we accept one of them and ignore the others
Table 4: Context predicates and their descriptions
Category Context predicates Description
N-GRAM gram
1
(u
j
) u
j
gram
2
(u
j
) uj+1
j
gram
3
(u
j
) uj+2
j
PAIR pair
11
(u
j
, v
k
) u
j
, v
k
pair
12
(u
j
, v
k
) u
j
, v
k+1
k
pair
22
(u
j
, v
k
) u
j+1
j
, v
k+1
k
TRIPLE triple
1
(u
j
, v
k
, w
l
) u
j
, v
k
, w
l
triple
2
(u
j
, v
k
, w
l
) u
j
, v
k
, w
l+1
l
(e.g., pair
12
(u
j
, u
j+1
) = trigram(u
j
) = u
j+2
j
).
Table 3 represents the examples of feature func-
tions for P (cg
i
|cg
i?1
i?k
, ?eg, ep, cp?
i+k
i?k
).
We used the ?Maximum Entropy Modeling
Toolkit?4 to estimate the probabilities and the
LBFGS algorithm to find ?
i
in Eq. (14). For
each transliteration model, we produced n-best
transliterations using a stack decoder (Schwartz
and Chow, 1990).
3.4 Summary
In this paper, we defined eighteen transliteration
models to be compared. There are six translitera-
tion models, three basic and three hybrid ones, in
each class, M
I
, M
S
, and M
J
. We compared the
transliteration models from the viewpoint of Chi-
nese phonemes or the class of transliteration mod-
els in our experiments.
4 Testing and Results
We used the same test set used in Li et al (2004)
for our testing5. It contains 37,694 pairs of English
words and their official Chinese transliterations
4Available at http://homepages.inf.ed.ac.
uk/s0450736/maxent_toolkit.html
5This test set was also used in ?NEWS09 machine translit-
eration shared task? for English-to-Chinese transliteration (Li
et al, 2009)
662
extracted from the ?Chinese Transliteration of For-
eign Personal Names? (Xinhua News Agency,
1992), which includes names in English, French,
German, and many other foreign languages (Li et
al., 2004). We used the same test data as in Li et
al. (2004). But we randomly selected 90% of the
training data used in Li et al (2004) as our training
data and the remainder as the development data, as
shown in Table 5.
Table 5: Number of English-Chinese translitera-
tion pairs in each data set
Ours Li et al (2004)
Training data 31,299 34,777
Development data 3,478 N/A
Blind test data 2,896 2,896
We used the training data for training the
transliteration models. For each model, we tuned
the parameters including the number of iterations
for training the maximum entropy model and a
Gaussian prior for smoothing the maximum en-
tropy model using the development data. Further,
the development data was used to select param-
eter ? of the hybrid transliteration models. We
varied parameter ? from 0 to 1 in 0.1 intervals
(i.e., ?=0, 0.1, 0.2, ? ? ? ,1) and tested the perfor-
mance of the hybrid models with the development
data. Then we chose ? that showed the best per-
formance in each hybrid model. The blind test
data was used for evaluating the performance of
each transliteration model. The CMU Pronounc-
ing Dictionary6, which contains about 120,000
English words and their pronunciations, was used
for estimating P (E
P
|E
G
).
We conducted two experiments. First, we com-
pared the overall performance of the translitera-
tion models. Second, we investigated the effect
of training data size on the performance of each
transliteration model.
The evaluation was done for word accuracy
in top-1 (ACC), Chinese pronunciation accuracy
(CPA) and a mean reciprocal rank (MRR) met-
ric (Kantor and Voorhees, 2000; Li et al, 2009;
Chang et al, 2009). ACC measures how many
correct transliterations appeared in the top-1 re-
sult of each system. CPA measures the Chinese
pronunciation accuracy in the top-1 of the n-best
Chinese pronunciation. We used CPA for com-
6Available at http://www.speech.cs.cmu.edu/
cgi-bin/cmudict
paring the performance between systems based on
Chinese phonemes. MRR, mean reciprocal ranks
of n-best results of each system over the test en-
tries, is an evaluation measure for n-best translit-
erations. If a transliteration generated by a system
matches a reference transliteration7 at the rth posi-
tion of the n-best results, its reciprocal rank equals
1/r; otherwise its reciprocal rank equals 0, where
1 ? r ? n. We produced 10-best Chinese translit-
erations for each English word in our experiments.
4.1 Comparison of the Overall Performance
Table 6 represents the overall performance of one
system in a previous work (Li et al, 2004) and
eighteen systems based on the transliteration mod-
els defined in this paper. ACC, MRR, and CPA
represent the evaluation results for each model
trained by our training data. To test transliteration
models without the errors introduced by incorrect
Chinese phonemes, we carried out the experiments
with the correct Chinese pronunciation (or the
correct Chinese phoneme sequence) in Chinese
phoneme-to-grapheme conversion. In the exper-
iment, we put the correct Chinese pronunciation
into the top-1 of the n-best Chinese pronunciation
with the highest probability, say P (C
P
|E
G
)=1;
thus CPA was assumed to be 100%. The ACC
of the transliteration models under this condition
is denoted as ACC? in Table 6. TRAIN represents
the evaluation results of the transliteration mod-
els trained by our training data. To compare Li
et al (2004) and transliteration models defined in
this paper under the same condition, we also car-
ried out experiments with the same training data
in Li et al (2004). Since the training data used
in Li et al (2004) is identical as the union of
our training and development data, we denoted it
as TRAIN+DEV in Table 6. In both TRAIN and
TRAIN+DEV, we used the same parameter setting
that was obtained by using the development data.
LI04 represents a system in Li et al (2004),
and its ACC? in TRAIN+DEV is taken from the
literature. The systems based on the translitera-
tion models defined in our paper are represented
from the second row in Table 6. The phoneme-
based transliteration models in the literature cor-
respond to either M(E
G
, C
P
) (Wan and Verspoor,
1998; Lee and Chang, 2003; Jiang et al, 2007) or
M(E
P
, C
P
) (Meng et al, 2001; Gao et al, 2004;
7In our test set, an English word corresponds to one refer-
ence Chinese transliteration.
663
Table 6: Comparison of the overall performance
Class Model TRAIN TRAIN+DEV
ACC MRR CPA ACC? ACC MRR CPA ACC?
LI04 N/A N/A N/A N/A 70.1 N/A N/A N/A
M(E
G
, JC
P
) 71.9 80.4 72.3 88.2 72.3 80.7 73.1 88.9
M(E
P
, JC
P
) 61.1 70.3 62.4 82.8 61.1 70.6 63.1 83.8
M
J
M(E
GP
, JC
P
) 72.3 80.9 73.2 89.6 73.5 81.5 73.9 90.4
M(E
G
+E
P
, JC
P
, 0.7) 72.8 80.7 73.8 89.7 73.2 81.0 74.7 90.5
M(E
G
+E
GP
, JC
P
, 0.6) 73.5 81.7 74.2 90.6 73.7 81.8 74.8 91.2
M(E
P
+E
GP
, JC
P
, 0.1) 71.6 80.3 73.3 89.8 72.5 80.8 73.8 90.1
M(E
G
, ?) 70.0 78.5 N/A N/A 70.6 79.0 N/A N/A
M(E
P
, ?) 58.5 69.3 N/A N/A 59.4 70.1 N/A N/A
M
I
M(E
GP
, ?) 71.2 79.9 N/A N/A 72.3 80.7 N/A N/A
M(E
G
+E
P
, ?, 0.7) 70.7 79.1 N/A N/A 72.0 80.0 N/A N/A
M(E
G
+E
GP
, ?, 0.4) 72.0 80.3 N/A N/A 72.8 80.9 N/A N/A
M(E
P
+E
GP
, ?, 0.1) 71.0 79.6 N/A N/A 72.0 80.4 N/A N/A
M(E
G
, C
P
) 58.9 70.2 72.3 78.4 59.1 70.4 73.1 78.4
M(E
P
, C
P
) 50.2 62.3 62.4 78.4 50.4 62.6 63.1 78.5
M
S
M(E
GP
, C
P
) 59.1 70.4 73.2 78.4 59.3 70.5 73.9 78.5
M(E
G
+E
P
, C
P
, 0.8) 59.7 71.3 73.8 79.0 60.3 71.7 74.7 79.0
M(E
G
+E
GP
, C
P
, 0.6) 59.8 71.7 74.2 78.9 60.6 72.1 74.8 78.9
M(E
P
+E
GP
, C
P
, 0.1) 58.8 70.4 73.3 78.9 59.4 70.7 73.8 78.8
Virga and Khudanpur, 2003).
A comparison between the basic and hybrid
transliteration models showed that the hybrid
ones usually performed better (the exception was
M(E
P
+E
GP
, y, ?) but the performance still com-
parable to the basic ones in each class). Es-
pecially, the hybrid ones based on the best two
basic transliteration models, M(E
G
+E
GP
, y, ?),
showed the best performance.
A comparison among the M
I
, M
S
, and
M
J
models showed that Chinese phonemes did
contribute to the performance improvement of
English-to-Chinese transliteration when Chinese
phonemes were used together with their corre-
sponding English graphemes and phonemes in
Chinese phoneme-to-grapheme conversion. A
one-tail paired t-test between the M
I
and M
J
models showed that the results of the M
J
mod-
els were always significantly better than those
of the M
I
models if the M
I
and M
J
models
shared the same English-side parameter, x ?
{E
G
, E
P
, E
GP
} (level of significance = 0.001).
In the results obtained by the M
S
and M
J
mod-
els, the figures in CPA are the same when theM
S
and our M
J
models share the same English-side
parameter. Moreover, the difference between the
figures in ACC and CPA can be interpreted as
the error rate of Chinese phoneme-to-grapheme
conversion. Our proposed M
J
models gener-
ated Chinese transliterations with a very low er-
ror rate in Chinese phoneme-to-grapheme conver-
sion, while theM
S
models suffered from a signif-
icant error rate in Chinese phoneme-to-grapheme
conversion. ACC? showed that the M
J
models
still outperformed the M
S
models even without
errors in generating Chinese pronunciation from
the English words. These results indicate that the
joint use of Chinese phonemes and their corre-
sponding English graphemes and phonemes sig-
nificantly improved the performance in Chinese
phoneme-to-grapheme conversion and English-to-
Chinese transliteration.
Table 7 shows the Chinese transliterations gen-
erated by M(E
G
, ?), M(E
GP
, ?), M(E
G
, JC
P
),
and M(E
GP
, JC
P
) where English or Chinese
phonemes contributed to the correct translitera-
tion. In this table, the first column show the
English words and their English phonemes, and
the second and third columns represent the Chi-
nese transliterations and their phonemes. Note
that the Chinese phonemes in the second and third
columns of theM
I
models are not used in translit-
eration. They are shown in the table to indicate
the difference in the Chinese phonemes of Chinese
664
Table 7: Top-1 results of M(E
G
, ?), M(E
GP
, ?),
M(E
G
, JC
P
), and M(E
GP
, JC
P
), where * rep-
resents incorrect transliterations
M(EGP,JCP)M(EG,JCP)MJ models
????*
(LAI YIN HA TE)
????*
(LAI YIN HA TE)
Reinhardt
(R AI N HH AA R T)
??
(AI WEI)
??*
(YI WEI)
Ivy
(AY V IY)
???*
(AI MI LI)
???*
(AI MI LI)
Emily
(EH M IH L IY)
????
LAI YIN HA TE
????
LAI YIN HA TE
Reinhardt
(R AI N HH AA R T)
??
AI WEI
??*
YI WEI
Ivy
(AY V IY)
???
AI MI LI
???
AI MI LI
Emily
(EH M IH L IY)
M(EGP,?)M(EG,?)MI models
transliterations between theM
I
andM
J
models.
For Emily and Reinhardt, the M
J
models gen-
erated correct Chinese transliterations, but theM
I
models did not. Figure 1 shows the probabil-
ity distribution when a transliteration model gen-
erates the first Chinese character in the Chinese
transliteration of Reinhardt with and without Chi-
nese phonemes. Two Chinese characters,  and
, were strong candidates and  is the correct
one in this case. Without Chinese phonemes,
M(E
G
, ?), which is based on P(cg|Reinhardt)
in Figure 1(a) preferring  to , generated the
incorrect transliteration as shown in Table 7. How-
ever, Figure 1(b) shows that  can be selected
if the correct Chinese phoneme sequence ?LAI
YIN ...? is given. Three Chinese phoneme se-
quences starting with ?LAI YIN ...?, ?LAI NA
...?, and ?LAI NEI ...? were generated from Rein-
hardt, where ?LAI YIN ...? was the best Chinese
phoneme sequence based on the probability distri-
bution in Figure 1(c). As a result, M(E
G
, JC
P
),
which jointly used Chinese phonemes with En-
glish graphemes, generated the correct Chinese
transliteration of Reinhardt based on two probabil-
ity distribution in Figures 1(b) and 1(c). In the case
of Ivy, English phonemes contributed to generat-
ing the correct transliteration in the M(E
GP
, ?)
and M(E
GP
, JC
P
) models.
Chinese transliterations sometimes reflect the
English word?s pronunciation as well as the Chi-
nese character?s meaning (Li et al, 2007). Li
0
0.2
0.4
0.6
0.8
P(
?
|Reinhardt) P(
?
|Reinhardt)
(a) Probability distribution when Chi-
nese phonemes are not given
0
0.2
0.4
0.6
0.8
1
?
?
P(cg|Reinhardt, "LAI YIN ..") P(cg|Reinhardt, "LAI NA ..")
P(cg|Reinhardt, "LAI NEI ..")
(b) Probability distribution when Chinese phonemes are
given
0
0.2
0.4
0.6
0.8
1
P("LAI YIN .."|Reinhardt) P(?"LAI YIN .."|Reinhardt)
(c) Probability distribution for Chinese phoneme se-
quence ?LAI YIN ...? and others
Figure 1: Probability distribution for the first Chi-
nese character in the Chinese transliteration of
Reinhardt: M(E
G
, ?) vs. M(E
G
, JC
P
)
et al (2007) defined such a Chinese transliter-
ation as a phonetic-semantic transliteration (se-
mantic transliteration) to distinguish it from a
usual phonetic transliteration. One fact that
affects semantic transliteration is gender asso-
ciation (Li et al, 2007). For example, 
(meaining jasmine) is frequently used in Chi-
nese transliterations of female names but sel-
dom in common person names. Because Emily
is often used in female names, the results ob-
tained by the M(E
G
, JC
P
) and M(E
GP
, JC
P
)
models are acceptable. This indicates that Chi-
nese phonemes coupled with English graphemes
or those coupled with English graphemes and
phonemes could provide evidence required for se-
mantic transliteration as well as phonetic translit-
eration. As a result, M(E
GP
, ?), M(E
G
, JC
P
),
665
and M(E
GP
, JC
P
), which used phonemes cou-
pled with English graphemes, achieved higher per-
formance than M(E
G
, ?), which relied only on
English graphemes.
4.2 Effect of Training Data Size
 80
 70
 60
 50
 40
 30
 20
 80 60 40 20
M
R
R
Training Data Size (%)
M(EG,?)
M(EP,?)
M(EGP,?)
M(EG,CP)
M(EP,CP)
M(EGP,CP)
M(EG,JCP)
M(EP,JCP)
M(EGP,JCP)
(a) Basic transliteration models
 80
 70
 60
 50
 40
 30
 80 60 40 20
M
R
R
Training Data Size (%)
M(EG+EP,?,0.7)
M(EG+EGP,?,0.4)
M(EP+EGP,?,0.1)
M(EG+EP,CP,0.8)
M(EG+EGP,CP,0.6)
M(EP+EGP,CP,0.1)
M(EG+EP,JCP,0.7)
M(EG+EGP,JCP,0.6)
M(EP+EGP,JCP,0.1)
(b) Hybrid transliteration models
Figure 2: Performance of each system with differ-
ent training data size
We investigated the effect of training data size
on the performance of each transliteration model.
We randomly selected training data with ratios
from 10 to 90% and compared the performance
of each system trained by different sizes of train-
ing data. The results for the basic translitera-
tion models in Figure 2(a) can be categorized into
three groups. M(E
GP
, ?) and M(E
GP
, JC
P
)
fall into the best group, where they showed the
best performance regardless of training data size.
M(E
G
, ?) and M(E
G
, JC
P
) belong to the mid-
dle group, where they showed lower performance
than the best group if the training data size is
small, but their performance is comparable to the
best group if the size of the training data is large
enough. The others always showed lower perfor-
mance than both the best and middle groups. Fig-
ure 2(b) shows that hybrid transliteration models,
on average, were less sensitive to the training data
size than the basic ones, because the two differ-
ent basic transliteration models used in the hybrid
ones boosted transliteration performance by com-
plementing each other?s weak points.
5 Conclusion
We proposed a new English-to-Chinese transliter-
ation model based on Chinese phonemes and their
corresponding English graphemes and phonemes.
We defined eighteen English-to-Chinese translit-
eration models including our proposed model and
classified them into three classes based on the role
of Chinese phonemes in the transliteration mod-
els. Experiments showed that Chinese phonemes
in our proposed model can contribute to the
performance improvement in English-to-Chinese
transliteration.
Now we can answer Yes to this paper?s key ques-
tion, ?Can Chinese phonemes improve machine
transliteration?? Actually, this is the second time
the same question has been answered. The pre-
vious answer, which was unfortunately reported
as No by Li et al (2004), has been accepted as
true for the last five years; the research issue has
been considered closed. In this paper, we found
a new answer that contradicts the previous an-
swer. We hope that our answer promotes research
on phoneme-based English-to-Chinese translitera-
tion.
Appendix: Illustration of Basic
Transliteration Models inM
J
andM
S
EG
CPEG EP
EG EP
CG
CP
CP
CG
CG:)JC,?(? PG
:)JC,?(?
PP
:)JC,?(?
PGP
(a) M
J
models
EG
CPEG EP
EG EP
CG
CP
CP
CG
CG
:)C,?(?
PGP
:)C,?(?
PP
:)C,?(?
PG
(b) M
S
models
666
References
Y. Al-Onaizan and Kevin Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proc. of ACL ?02, pages 400?408.
A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for translit-
eration discovery. In Proceedings of NAACL HLT?
09.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. of IJCNLP 2004, pages
110?119.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In Proc. of IJCAI ?07, pages
1629?1634.
Paul B. Kantor and Ellen M. Voorhees. 2000. The trec-
5 confusion track: Comparing retrieval methods for
scanned text. Information Retrieval, 2:165?176.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007. Collapsed consonant and vowel models: New
approaches for English-Persian transliteration and
back-transliteration. In Proceedings of ACL ?07,
pages 648?655.
Chun-Jen Lee and Jason S. Chang. 2003. Acqui-
sition of English-Chinese transliterated word pairs
from parallel-aligned texts using a statistical ma-
chine transliteration model. In Proc. of HLT-NAACL
2003 Workshop on Building and Using Parallel
Texts, pages 96?103.
Haizhou Li, Min Zhang, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the 42th Annual Meeting of the As-
sociation of Computational Linguistics, pages 160?
167.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of personal
names. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. of
ACL-IJCNLP 2009 Named Entities Workshop.
M.G. Abbas Malik. 2006. Punjabi machine translit-
eration. In Proceedings of the COLING/ACL 2006,
pages 1137?1144.
H.M. Meng, Wai-Kit Lo, Berlin Chen, and K. Tang.
2001. Generating phonetic cognates to handle
named entities in English-Chinese cross-language
spoken document retrieval. In Proc. of Auto-
matic Speech Recognition and Understanding, 2001.
ASRU ?01, pages 311?314.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119?151.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximal entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 1?
10.
Richard Schwartz and Yen-Lu Chow. 1990. The N-
Best algorithm: an efficient procedure for finding
top N sentence hypotheses. In Proc. of ICASSP ?90,
pages 81?84.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of ACL ?07,
pages 944?951.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. of ACL 2003 Workshop on Multi-
lingual and Mixed-language Named Entity Recogni-
tion, pages 57?64.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. of
COLING ?98, pages 1352?1356.
Xinhua News Agency. 1992. Chinese transliteration
of foreign personal names. The Commercial Press.
Binyong Yin and Mary Felley. 1990. Chinese Roman-
ization: Pronunciation and Orthography. Sinolin-
gua.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using
feature based phonetic method. In Proceedings of
ACL?07, pages 112?119.
667
Building an Annotated Japanese-Chinese Parallel Corpus  
? A Part of NICT Multilingual Corpora  
Yujie Zhang and  Kiyotaka Uchimoto and Qing Ma and Hitoshi Isahara 
 
National Institute of Information and Communications Technology 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 
(yujie, uchimoto,qma, isahara)@nict.go.jp
 
Abstract 
We are constricting a Japanese-Chinese 
parallel corpus, which is a part of the 
NICT Multilingual Corpora. The corpus is 
general domain, of large scale of about 
40,000 sentence pairs, long sentences, 
annotated with detailed information and 
high quality. To the best of our knowledge, 
this will be the first annotated Japanese-
Chinese parallel corpus in the world. We 
created the corpus by selecting Japanese 
sentences from Mainichi Newspaper and 
then manually translating them into 
Chinese. We then annotated the corpus 
with morphological and syntactic 
structures and alignments at word and 
phrase levels. This paper describes the 
specification in human translation and the 
scheme of detailed information annotation, 
and the tools we developed in the corpus 
construction. The experience we obtained 
and points we paid special attentions are 
also introduced for share with other 
researches in corpora construction.     
1 Introduction 
A parallel corpus is a collection of articles, 
paragraphs, or sentences in two different languages. 
Since a parallel corpus contains translation 
correspondences between the source text and its 
translations at different level of constituents, it is a 
critical resource for extracting translation 
knowledge in machine translation (MT). Although 
recently some versions of machine translation 
software have become available in the market, 
translation quality is still a significant problem. 
Therefore, a detailed examination into human 
translation is still required. This will provide a basis 
for radically improving machine translation in the 
near future. In addition, in MT system development, 
the example-based method and the statistics-based 
method are widely researched and applied. So, 
parallel corpora are required by the translation 
studies and practical system development.   
The raw text of a parallel corpus contains 
implicit knowledge. If we annotate some 
information, we can get explicit knowledge from 
the corpus. The more information that is annotated 
on a parallel corpus, the more knowledge we can 
get from the corpus. The parallel corpora of 
European languages are usually raw texts without 
annotation on syntactic structure since their 
syntactic structures are similar and MT does not 
require such annotation information. However, 
when language pairs are different in syntactic 
structures, such as the pair of English and Japanese 
and the pair of Japanese and Chinese, 
transformation between syntactic structures is 
difficult. A parallel corpus annotated with syntactic 
structures would thus be helpful to MT.  Besides 
MT, an annotated parallel corpus can be applied to 
cross-lingual information retrieval, language 
teaching, machine-aided translation, bilingual 
lexicography, and word-sense disambiguation.  
Parallel corpora between European languages 
are well developed and are available through the 
Linguistic Data Consortium (LDC). However, 
parallel corpora between European languages and 
Asian languages are less developed, and parallel 
corpora between two Asian languages are even less 
developed.  
The National Institute of Information and 
Communications Technology therefore started a 
project to build multilingual parallel corpora in 
2002 (Uchimoto et al, 2004). The project focuses 
on Asian language pairs and annotation of detailed 
information, including syntactic structure and 
alignment at word and phrase levels. We call the 
corpus the NICT Multilingual Corpora. The corpus 
will be open to the public in the near future. 
2 Overview of the NICT Multilingual 
Corpora 
At present, a Japanese-English parallel corpus and a 
Japanese-Chinese parallel corpus are under 
construction following systematic specifications. 
The parallel texts in each corpus consist of the 
original text in the source language and its 
translations in the target language. The original data 
is from newspaper articles or journals, such as 
85
Mainichi Newspaper in Japanese. The original 
articles were translated by skilled translators. In 
human translation, the articles of one domain were 
all assigned to the same translators to maintain 
consistent terminology in the target language. 
Different translators then revised the translated 
articles. Each article was translated one sentence to 
one sentence, so the obtained parallel corpora are 
already sentence aligned.  
  The details of the current version of the NICT 
Multilingual Corpora are listed in Table 1. 
Corpora Total Original Translation 
Japanese 
(19,669 
sentences, 
Mainichi 
Newspaper) 
English 
Translation 
Japanese-
English 
Parallel 
Corpus 
37,987 
sentence 
pairs; 
(English 
900,000 
words) English 
(18,318 
Sentences, 
Wall Street 
Journal) 
Japanese 
Translation 
Japanese-
Chinese 
Parallel 
Corpus 
38,383 
sentence 
pairs; 
(Chinese 
1,410,892 
Characters, 
926,838 
words) 
Japanese 
(38,383 
sentences, 
Mainichi 
Newspaper) 
Chinese 
Translation 
Table 1 Details of current version of NICT Multilingual 
Corpora 
 
   The following is an example of English and 
Chinese translations of a Japanese sentence from 
Mainichi Newspaper. 
[Ex. 1] 
J: ????????????????????????
?????? 
E: They were all about nineteen years old and had 
no strength left even to answer questions. 
C: ?????????????????????
?????????????  
 
In addition to the human translation, another big 
task is annotating the information. We finish the 
task by two steps: automatic annotation and human 
revision. In automatic annotation, we applied 
existing analysis techniques and tag sets. In human 
revision, we developed assisting tools that have 
powerful functions to help annotators in revision. 
The annotation task for each language included 
morphological and syntactic structure annotation.  
The annotation task for each language pair included 
alignments at word and phrase level.  
The NICT Multilingual Corpora constructed in 
this way have the following characteristics. 
(1) Since the original data is from newspaper and 
journals, the domain of each corpus is therefore rich.  
(2) Each corpus consists of original sentences and 
their translations, so they are already sentence 
aligned. In translation of each sentence, the context 
of the article is also considered. Thus, the context of 
each original article is also well maintained in its 
translation, which can be exploited in the future. 
(3) The corpora are annotated at high quality with 
morphological and syntactic structures and 
word/phrase alignment.  
 In the following section, we will describe the 
details in the construction of the Japanese-Chinese 
parallel corpus. 
3 Human Translation from Japanese to 
Chinese   
About 40,000 Japanese sentences from issues of 
Mainichi Newspaper were translated by skilled 
translators. The translation guidelines were as 
follows. 
(1) One Japanese sentence is translated into one 
Chinese sentence. 
(2) Among several translation candidates, the one 
that is close to the original sentence in syntactic 
structure is preferred. The aim is to avoid 
translating a sentence too freely, i.e., 
paraphrasing. 
(3) To obtain intelligible Chinese translations, 
information of the proceeding sentences in the 
same article should be added. Especially, a 
subject should be supplemented because a 
subject is usually required in Chinese, while in 
Japanese subjects are often omitted . 
(4)  To obtain natural Chinese translations, 
supplement, deletion, replacement, and 
paraphrase  should be made when necessary. 
When a translation is very long, word order can 
be changed or commons can be inserted. These 
are the restrictions on (2), i.e., the naturalness 
of the Chinese translations is the priority.  
 
  One problem in translation is how to translate 
proper nouns in the newspaper articles. We pay 
special attentions to them in the following way.  
(1) Proper nouns  
When proper nouns did not exist in Japanese-
Chinese dictionaries, new translations were created 
and then confirmed using the Chinese web. For 
kanji in proper nouns, if there was a Chinese 
character having the same orthography as the kanji, 
the Chinese character was used in the Chinese 
translation; if there was a traditional Chinese 
character having the same orthography as the kanji, 
the simplified character of the traditional Chinese 
character was used in the translation; otherwise, a 
Chinese character whose orthography is similar to 
that of the kanji was used in the translation.  
(2) Special things in Japan 
86
 Explanations were added if necessary. For example, 
?????, translated from ????? (grand sumo 
tournament), is well known in China, while ????, 
translated from ???? (spring labor offensive), is 
not known in China. In this case, an explanation 
???????? was added behind the unfamiliar 
term. We attempt to introduce new words about 
Japanese culture into Chinese through the 
construction of the corpus.    
 
   Producing high-quality Chinese translations is 
crucial to this parallel corpus. We controlled the 
quality by the following treatments.  
(1) The first revision of a translated article was 
conducted by a different translator after the first 
translation. The reviewers checked whether the 
meanings of the Chinese translations corresponded 
accurately to the meanings of the original sentences 
and modified the Chinese translations if necessary. 
(2) The second revision was conducted by Chinese 
natives without referring to the original sentences. 
The reviewers checked whether the Chinese 
translations were natural and passed the unnatural 
translations back to translators for modification. 
(3) The third revision was conducted by a Chinese 
native in the annotation process of Chinese 
morphological information. The words that did not 
exist in the dictionary of contemporary Chinese 
were checked to determine whether they were new 
words. If not, the words were designated as 
informal or not written language and were replaced 
with suitable words. The word sequences that 
missed the Chinese language model?s part-of-
speech chain were also adjusted.        
 
 Until now, 38,383 Japanese sentences have 
been translated to Chinese, and of those, 22,000 
Chinese translations have been revised three times, 
and we are still working on the remaining 18,000 
Chinese translations.  
4 Morphological Information Annotation 
Annotation consists of automatic analyses and 
manual revision. 
4.1 Annotation on Japanese Sentences  
Japanese morphological and syntactic analyses 
follow the definitions of part-of-speech categories 
and syntactic labels of the Corpus of Spontaneous 
Japanese (Maekawa, 2000).  
A morphological analyzer developed in that 
project was applied for automatic annotation on the 
Japanese sentences and then the automatically 
tagged sentences were revised manually. An 
annotated senetence is illustrated in Figure 1, which 
is the Japanese sentence in Ex. 1 in Section 2.  
 
 
 
 
 
 
# S-ID:950104141-008 
* 0 2D 
???? ???? * ?? * * * 
* 1 2D 
?? ?????? * ?? ?? * * 
? ?? * ??? ???????? * * 
?? ??? * ??? ???????? * * 
? ? * ?? ???? * * 
* 2 6D 
?? ???? * ?? ???? * * 
? ? ? ??? * ??? ???????? 
? ? * ?? ?? * * 
* 3 4D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 4 5D 
??? ???? ??? ?? * ???? ??? 
* 5 6D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 6 -1D 
??? ???? ?? ?? * ???? ???? 
? ? ?? ??? ?????? ???? ??? 
?? ?? ?? ??? ???? ???? ??? 
? ? * ?? ?? * * 
EOJ 
 
Figure 1. An annotated Japanese sentence 
 
The data of one sentence begins from the line ?# S-
ID... ? and ends with the mark ?EOJ?. The line 
headed by ?*? indicates the beginning of a phrase 
and the following lines are morphemes in that 
phrase. For example, the line ?* 0 2D? indicates the 
phrase whose number is 0. The following line ???
??  ? ? ? ?  * ? ?  * * *? indicates the 
morpheme in the phrase. There are seven fields in 
each morpheme line, token form, phonetic alphabet, 
dictionary form, part-of-speech, sub-part-of-speech, 
verbal category and conjugation form. In the line ?* 
0 2D?, the numeral 2 in ?2D? indicates that the 
phrase 0 ?????? modifies the phrase 2 ???
? ? ?. The syntactic structure analysis adopts 
dependency-structure analysis in which modifier-
modified relations between phrases are determined. 
The dependency-structure of the example in Figure 
1 is demonstrated in Figure 2. 
 
???? ?????? ???? ??? ??? ??? ???????
 
       Figure 2  Example of syntactic structure 
 
87
4.2 Annotation on Chinese Sentences 
For Chinese morphological analysis, we used the 
analyser developed by Peking University, where the 
research on definition of Chinese words and the 
criteria of word segmentation has been conducted 
for over ten years. The achievements include a 
grammatical knowledge base of contemporary 
Chinese, an automatic morphological analyser, and 
an annotated People?s Daily Corpus. Since the 
definition and tagset are widely used in Chinese 
language processing, we also took the criteria as the 
basis of our guidelines.  
A morphological analyzer developed by Peking 
University (Zhou and Yu, 1994) was applied for 
automatic annotation of the Chinese sentences and 
then the automatically tagged sentences were 
revised by humans. An annotated sentence is 
illustrated in Figure 3, which is the Chinese 
sentence in Ex. 1 in Section 2. 
 
S-ID: 950104141-008 
??/r  ??/j  ??/n  ?/d  ?/v   ??/m  ?/q 
??/m  ?/u  ???/n   ?/w  ??/r  ??/d  
?/p  ??/v  ??/n  ?/u  ??/n  ?/d   
??/v  ?/w 
Figure 3  An annotated Chinese sentence  
 
4.3 Tool for Manual Revision 
We developed a tool to assist annotators in revision. 
The tool has both Japanese and Chinese versions. 
Here, we introduce the Chinese version. The input 
of the tool is the automatically segmented and part-
of-speech tagged sentences and the output is revised 
data. The basic functions include separating a 
sequence of characters into two words, combining 
two segmented words into one word, and selecting 
a part-of-speech for a segmented word from a list of 
parts-of-speech. In addition, the tool has the 
following functions. 
(1) Retrieves a word in the grammatical knowledge 
base of contemporary Chinese of Peking University 
(Yu et al, 1997).  
This is convenient when annotators want to 
confirm whether a segmented word is authorized by 
the grammatical knowledge base, and when they 
want to know the parts-of-speech of a word defined 
by the grammatical knowledge base.  
(2) Retrieves a word in other annotated corpora or 
the sentences that have been revised.   
This is convenient when annotators want to see 
how the same word has been annotated before.  
(3) Retrieves a word in the current file.  
It collects all the sentences in the current file 
that contain the same word and then sorts their 
context on the left and right of the word. By 
referring to the sorted contexts, annotators can 
select words with the same syntactic roles and 
change all of the parts-of-speech to a certain one all 
in one operation. This is convenient when 
annotators want to process the same word in 
different sentences, aiming for consistency in 
annotation.     
(4) Adds new words to the grammatical knowledge 
base dynamically.  
The updated grammatical knowledge base can 
be used by the morphological analyser in the next 
analysis. 
(5) Indexes to sentences by an index file.  
The automatically discovered erroneous 
annotations can be stored in one index file, pointing 
to the sentences that are to be revised.  
 
The interface of the tool is shown in Figure 4 
and Figure 5. 
 
Figure 4 Interface of the manual revision tool (Retrieves 
a word in the grammatical knowledge base of 
contemporary Chinese) 
 
Figure 5    Interface of the manual revision tool 
(Retrieves a word in the current file) 
  
 
In Figure 4, the small window in the lower left 
displays the retrieved result of the word ? ??? in 
the grammatical knowledge base; the lower right 
window displays the retrieved result of the same 
word in the annotated People?s Daily Corpus. 
88
In Figure 5, the small window in the lower left is used to 
define retrieval conditions in the current file. In this 
example, the orthography of ???? is defined. The 
lower right window displays the sentences containing the 
word  ???? retrieved from the current file. The left and 
right contexts of one word are shown with the retrieved 
word. The contents of any column can be sorted by 
clicking the top line of the column. 
5 Annotation of word alignment  
Since automatic word alignment techniques cannot 
reach as high a level as the morphological analyses, 
we adopt a practical method of using multiple 
aligners. One aligner is a lexical knowledge-based 
approach, which was implemented by us based on 
the work of Ker (Ker and Chang, 1997). Another 
aligner is the well-known GIZA++ toolkit, which is 
a statistics-based approach. For GIZA++, two 
directions were adopted: the Chinese sentences 
were used as source sentences and the Japanese 
sentences as target sentences, and vice versa.  
The results produced by the lexical knowledge-
based aligner, C? J of GIZA++, and J?C of 
GIZA++ were selected in a majority decision. If an 
alignment result was produced by two or three 
aligners at the same time, the result was accepted. 
Otherwise, was abandoned.  In this way, we aimed 
to utilize the results of each aligner and maintain 
high precision at the same time. Table 2 showed the 
evaluation results of the multi-aligner on 1,127 test 
sentence pairs, which were manually annotated with 
gold standards, totally 17,332 alignments.  
 
 Precision 
(%) 
Recall 
(%) 
F-measure
Multi-aligner 79.3 62.7 70
Table 2 Evaluation results of the multi-aligner 
  
The multi-aligner produced satisfactory results. 
This performance is evidence that the multi-aligner 
is feasible for use in assisting word alignment 
annotation.  
For manual revision, we also developed an 
assisting tool, which consist of a graphical interface 
and internal data management. Annotators can 
correct the output of the automatic aligner and add 
alignments that it has not identified. In addition to 
assisting with word alignment, the tool also 
supports annotation on phrase alignment. Since 
Japanese sentences have been annotated with phrase 
structures, annotators can select each phrase on the 
Japanese side and then align them with words on 
the Chinese side. For idioms in Japanese sentences, 
two or more phrases can be selected. 
The input and output file of the manual 
annotation is in XML format. The data of one 
sentence pair consists of the Chinese sentence 
annotated with morphological information, the 
Japanese sentence annotated with morphological 
and syntactic structure information, word alignment, 
and phrase alignment.  
The alignment annotation at word and phrase is 
ongoing, the former focusing on lexical translations 
and the latter focusing on pattern translations. After 
a certain amount of data is annotated, we plan to 
exploit the annotated data to improve the 
performance of automatic word alignment. We will 
also investigate a method to automatically identify 
phrase alignments from the annotated word 
alignment and a method to automatically discover 
the syntactic structures on the Chinese side from the 
annotated phrase alignments.       
6 Conclusion  
We have described the construction of a Japanese-
Chinese parallel corpus, a part of the NICT 
Multilingual Corpus. The corpus consists of about 
40,000 pairs of Japanese sentences and their 
Chinese translations. The Japanese sentences are 
annotated with morphological and syntactic 
structures and the Chinese sentences are annotated 
with morphological information. In addition, word 
and phrase alignments are annotated. A high quality 
of annotation was obtained through manual 
revisions, which were greatly assisted by the 
revision tools we developed in the project. To the 
best of our knowledge, this will be the first 
annotated Japanese-Chinese parallel corpus in the 
world.  
In the future, we will finish the annotation on the 
remaining data and add syntactic structures to the 
Chinese sentences.  
  
References  
Dice, L.R. 1945. Measures of the amount of 
ecologic association between species. Journal of 
Ecology (26), pages 297?302. 
Ker, S.J., Chang, J.S. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, Vol. 23, Num. 2, pages 313?343. 
Liu Q. 2004.  Research into some aspects of 
Chinese-English machine translation. Doctoral 
Dissertation.  
Maekawa, K., Koiso, H., Furui, F., Isahara, H. 2000. 
Spontaneous Speech Corpus of Japanese. 
Proceedings of LREC2000, pages 947?952. 
LDC. 1992.  Linguistic data Consortium. 
http://www.ldc.upenn.edu/. 
Uchimoto, K. and Zhang,Y., Sudo, K., Murata, M., and 
Sekine, S.,  Isahara,  H. Multilingual Aligned Parallel 
89
Treebank Corpus Reflecting Contextual Information 
and Its Applications. Proceedings of the MLR2004: 
PostCOLING Workshop on Multilingual Linguistic 
Resources, pages 63-70. 
Yamada, K., Knight, K. 2001.A syntax-based Statistical 
Translation Model. In Proceedings of the ACL , pages 
523-530.  
Yu, Shiwen. 1997. Grammatical Knowledge Base of 
Contemporary Chinese. Tsinghua Publishing 
Company. 
Zhang, Y., Ma, Q., Isahara, H. 2005. Automatic 
Construction of Japanese-Chinese Translation 
Dictionary Using English as Intermediary. Journal of 
Natural Language Processing, Vol. 12, No. 2, pages 
63-85. 
Zhou, Q., Yu, S. 1994.  Blending Segmentation with 
Tagging in Chinese Language Corpus 
Processing.  In Proc. of COLING-94, pages 
1274?1278. 
 
 
90
Error Annotation for Corpus of Japanese Learner English 
Emi Izumi                    Kiyotaka Uchimoto                 Hitoshi Isahara 
National Institute of Information and Communications Technology (NICT), 
Computational Linguistics Group 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 
{emi,uchimoto,isahara}@nict.go.jp 
 
 
Abstract 
In this paper, we discuss how error 
annotation for learner corpora should 
be done by explaining the state of the 
art of error tagging schemes in learner 
corpus research. Several learner 
corpora, including the NICT JLE 
(Japanese Learner English) Corpus that 
we have compiled are annotated with 
error tagsets designed by categorizing 
?likely? errors implied from the 
existing canonical grammar rules or 
POS (part-of-speech) system in 
advance. Such error tagging can help to 
successfully assess to what extent 
learners can command the basic 
language system, especially grammar, 
but is insufficient for describing 
learners? communicative competence. 
To overcome this limitation, we re-
examined learner language in the NICT 
JLE Corpus by focusing on 
?intelligibility? and ?naturalness?, and 
determined how the current error tagset 
should be revised. 
1 Introduction 
The growth of corpus research in recent years is 
evidenced not only by the growing number of 
new corpora but also by their wider variety. 
Various ?specialized corpora? have recently 
been created. One of them is the ?learner 
corpus?, which is a collection of the language 
spoken or written by non-native speakers. The 
primary purpose of learner corpora is to offer 
Second Language Acquisition (SLA) 
researchers and language teaching professionals 
resources for their research. In order to develop 
a curriculum or pedagogy of language teaching, 
it would be beneficial to have interlanguage data 
so that researchers can scientifically describe the 
characteristics of each developmental stage of 
their interlanguage. One of the most effective 
ways of doing this is to analyze learner errors. 
Some of the existing learner corpora are 
annotated for errors, and our learner corpus 
called the ?NICT JLE (Japanese Learner 
English) Corpus? is one of them. This is a two- 
million-word speech corpus of Japanese learner 
English. The source of the corpus data is 1,281 
audio-recorded speech samples of an English 
oral proficiency interview test ACTFL-ALC 
Standard Speaking Test (SST). The advantage of 
using the SST data as a source is that each 
speaker?s data includes his or her proficiency 
level based on the SST scoring method, which 
makes it possible to easily analyze and compare 
the characteristics of interlanguage of each 
developmental stage. This is one of the 
advantages of the NICT JLE corpus that is 
rarely found in other learner corpora. 
Although there are a lot of advantages of 
error-annotated learner corpora, we found some 
difficulties in designing an error tagset that 
covers important features of learner errors. The 
current version of our error tagset targets 
morphological, grammatical, and lexical errors, 
and we found it can help to successfully assess 
to what extent learners can command the basic 
language system, especially grammar. However, 
we also found that it is not sufficient to measure 
learners? communicative skills. In order to 
determine how the current error tagset should be 
extended to cover more communicative aspects 
71
of learner language, we re-examined the learner 
data in the NICT JLE Corpus by focusing on 
?intelligibility? and ?naturalness?. 
In this paper, we discuss how error 
annotation for learner corpora should be 
designed and actually performed. The remainder 
of this paper is organized as follows. Section 2 
outlines the influence that Error Analysis (EA) 
in SLA research in the 1970s had on error 
annotation for learner corpora to enable us to 
rethink the concept of error annotation. Section 
3 provides some examples of learner corpus 
projects in which error tagging is performed. 
Section 4 describes the current error tagging 
scheme for the NICT JLE Corpus. Section 5 
examines how we can expand it to make it more 
useful for measuring learners? communicative 
skills. Finally, section 6 draws some general 
conclusions. 
2 Error Tagging and EA 
The idea of trying to tag errors in learner 
corpora might come from the notion of EA in 
SLA research in the 1970s. In order to design an 
error tagset and to actually perform tagging, we 
would like to reconfirm the concept of the 
traditional EA by considering about the 
definition of learner errors, the importance of 
analyzing learner errors for describing learner 
language, the actual EA procedures, and 
problems, and limitations of EA. 
2.1 Definition of Learner Errors 
Errors in a second language (L2) are often 
compared with errors in the first language (L1). 
According to Ellis (1994), before EA was 
introduced, L2 errors were often considered as 
?undesirable forms?. On the other hand, errors 
made by young L1 learners were regarded as the 
?transitional phase? in L1 acquisition, while 
errors made by adult L1 speakers were seen just 
as slips of the tongue. L2 errors were often 
shoved back into the closet as a negative aspect 
of learner language. 
In EA, errors are treated as evidence that 
plays an important role in describing learner 
language. Corder (1981) asserts that talking 
about learner errors only with terms like 
?deviant? or ?ill-formed? is inappropriate 
because it leads to learner errors being treated 
just as superficial deviations. Even if learners 
produce outputs whose surface structures are 
well-formed, this is not enough to prove that 
they have acquired the same language system as 
that L1 speakers have. 
In EA, learner errors are treated as something 
that proves that learners are in the transitional 
phase in L2 acquisition in a similar way to 
treating the language of L1 children. However, it 
is problematic to assume these two are exactly 
the same language. In L1 and L2 acquisition, 
there are certain processes in common, but they 
have not been scientifically confirmed. Many 
differences are found between these two. The 
learner language including errors can be defined 
as ?interlanguage?, which lies between L1 and 
L2 (Selinker, 1972). According to Corder (1981), 
learner errors are evidence of learning strategies 
in which learners are ?investigating? the system 
of the new language (L2) by examining to what 
extent L1 and L2 are similar and how different 
they are. 
2.2 Importance of EA 
Analyzing learner errors is important for 
teachers, researchers, and learners themselves in 
the following way (Corder, 1981). First, for 
teachers, errors can give them hints about the 
extent to which the learners have acquired the 
language system by that time and what they still 
have to learn. For SLA researchers, errors can 
reveal the process by which L2 is acquired and 
the kinds of strategies or methodology the 
learners use in that process. Finally, for learners 
themselves, as stated in 2.1, making errors is one 
of the most important learning strategies for 
testing the interlanguage hypothesis that learners 
have established about L2. In other words, 
knowing what kinds of errors were made by 
themselves or by other learners can be ?negative 
evidence (or feedback)? given directly or 
indirectly to learners that an interlanguage 
hypothesis is incorrect (Ellis, 1997). 
2.3 EA Procedure 
In general, the EA procedure can be divided into 
four stages as shown in Figure 1 (Ellis, 1994). 
 
72
Identifying Errors
Describing Errors
Explaining Errors
Evaluating Errors
 
Figure1. EA Procedure. 
 
In the first stage, identifying errors, it is 
necessary to localize errors by pointing out 
which letters, words, and phrases, or how 
sentence structures or word order, are incorrect. 
In the second stage, identified errors should be 
described by being linguistically categorized 
depending on, for example, their POS (part-of-
speech), linguistic level (morpheme, syntax, 
lexis, or discourse), or how they deviate from 
the correct usage on the surface structure 
(redundancy, omission, or replacement). Thirdly, 
?explaining errors? means identifying why those 
errors occurred. This is a very important task in 
order to figure out the learners? cognitive stage. 
Some causes of learner errors have been 
recognized in common such as errors caused by 
language transfer, learning and communication 
strategy-based errors, and the transfer of training 
and induced errors. Finally, errors are evaluated. 
This can be done by estimating intelligibility or 
near-nativeness of erroneous outputs. In other 
words, ?error gravity? is estimated by examining 
how each error interferes with the intelligibility 
of the entire outputs. 
2.4 Problems and Limitations of 
Traditional EA 
Although it is widely recognized that EA 
contributes to describing learner language and 
the improving second language pedagogy, 
several problems and limitations have been 
pointed out mainly because a concrete 
methodology of EA has not been established yet. 
Most importantly, EA cannot be successful 
without robust error typology, which is often 
very difficult to obtain. Since it used to be 
difficult to collect or access large databases of 
learner language, a robust error typology that 
covers almost all error types was not established 
in traditional EA. 
Another criticism against EA is that errors 
reflect only one side of learner language. A lot 
of people point out that if a researcher analyzes 
only errors and neglects what learners can do 
correctly, he/she will fail to capture the entire 
picture of learner language. It is time-consuming 
to count both correct and incorrect usages in 
learner data, and this must have been quite 
difficult to do in the past before computing 
technology was developed. 
Furthermore, the real significance of EA 
cannot be identified without using diachronic 
data in order to describe learners? developmental 
stages. The types and frequencies of errors 
change with each acquisition phase. Without 
longitudinal data of learner language, it is 
difficult to obtain a reliable result by EA. 
2.5 From EA to Error?coded Learner 
Corpora 
The problems and limitations of traditional EA 
are mainly due to the deficiency of computing 
technology and the lack of large databases in 
early times. However, now that computing 
technology has advanced, and a lot of learner 
data is available, it might be possible to perform 
EA more effectively mainly by annotating errors. 
Although the basic motivations for error 
annotation are the same as those of traditional 
EA, such as describing learner language and 
improving second-language pedagogy, several 
new applications of EA might become possible 
such as the development of a new computer-
aided language learning (CALL) environment 
that can process learners? erroneous input and 
give feedback automatically. 
Degneaux, et al (1998) call EA based on 
learner corpora ?computer-aided error analysis 
(CEA)?, and expect that the rapid progress of 
computing technology and learner corpora will 
be able to solve the problems and overcome the 
limitations of traditional EA. Surely, thanks to 
the quantitative database of learner language, we 
will become able to cover a wider range of 
learner errors. Advances in computing 
technology make it possible to perform 
statistical analysis with quantitative data more 
easily. However, it must be noted that human 
researchers still have a lot of work to do in the 
same manner as in traditional EA, such as 
establishing an error typology for error tagging 
73
or examining results obtained from CEA 
carefully. 
3 Related Work 
There are a few learner corpus projects that 
implement CEA. For example, in the 
International Corpus of Learner English (ICLE) 
project, which was launched by Professor 
Sylviane Granger at the University of Louvain, 
Belgium, and has been a ?pioneer? in learner 
corpus research since the early 1990s, they 
performed error tagging with a custom-designed 
error tagset (Degneaux, et al, 1996). The 
grammatical, lexical and pragmatic errors are 
dealt with in their error tagset and the corrected 
form is also indicated for each error. We guess 
that error categorization has been done mainly 
by translating the basic English grammar or 
lexical rules into an error ontology to try to 
cover as many types of errors as possible. The 
ICLE team currently comprises 17 partners 
internationally, and the corpus encloses 17 
subcorpora of learners of the different mother 
tongues (Bulgarian, Czech, Dutch, Finnish, 
French, German, Italian, Polish, Russian, 
Spanish, Swedish, and so on). A comparison of 
this data will make possible ?contrastive 
interlanguage analysis (CIA)? proposed by 
Granger (2002). CIA involves both NN/NNS 
and NNS/NNS comparisons (NS: native 
speakers; NNS: non-native speakers), as shown 
in Figure 2. NS/NNS comparisons might reveal 
how and why learner language is non-nativelike. 
NNS/NNS comparisons help researchers to 
distinguish features shared by several learner 
populations, which are more likely to be 
developmental from ones peculiar to a certain 
NNS group, which may be L1-dependent. 
CIA
NS NNS NNS NNSvs vs  
Figure 2. Contrastive Interlanguage Analaysis 
(Granger, 2002). 
Another corpus thathas been error tagged is 
the ?Japanese EFL Learner (JEFLL) Corpus?. 
This corpus, which was created by Professor 
Yukio Tono at Meikai University in Japan, has 
three parts: i) the L2 learner corpora which 
include written (composition) and spoken 
(picture description) data of Japanese learner 
English, ii) the L1 corpora consisting of 
Japanese written texts for the same tasks as 
those in the first part and Japanese newspaper 
articles, and iii) the EFL textbook corpus, which 
is the collection of EFL textbooks used officially 
at every junior high school in Japan (Tono, 
2002). Compared with the ICLE, which has 
been annotated with the generic error tagset, the 
error tagging for the JEFFL Corpus focuses on 
specific types of errors, especially major 
grammatical morphemes such as articles, plural 
forms of nouns, and third person singular 
present forms of verbs, and so on. We assume 
that those items were selected due to the corpus 
developer?s research interests. Although 
completeness for covering all errors would be 
decreased by focusing on a limited number of 
error types, annotators will be able to perform 
tagging more stably without being confused 
among various different types of errors. 
The Cambridge Learners? Corpus (CLC), 
which has been compiled by Cambridge 
University Press and Cambridge ESOL (English 
for Speakers of Other Languages), is also an 
error-coded learner corpus. It forms part of the 
Cambridge International Corpus (CIC) and is a 
large collection of essay writing from learners of 
English all over the world. This corpus has been 
utilized for the development of publications by 
authors and writers in Cambridge University 
Press and by members of staff at Cambridge 
ESOL. Over eight million words of the CLC 
have been error-coded with a Learner Error 
Coding System devised by Cambridge 
University Press. In order to make the tagged 
data as consistent as possible, tagging has been 
done by only one annotator since it started in 
1993. Their error tagset covers 80 types of errors. 
The annotator chooses an appropriate tag for 
each error mainly by identifying which POS the 
error involves and how it deviates from the 
correct usage (redundancy, omission, or 
replacement). A corrected form is also indicated 
for each error. 
4 Error Tags in the NICT JLE Corpus 
In this section, we introduce the error annotation 
scheme we used for the NICT JLE Corpus. 
We are aware that it is quite difficult to 
design a consistent error tagset as the learner 
74
errors extend across various linguistic areas, 
including grammar, lexis, and phoneme, and so 
on. We designed the original error tagset only 
for morphological, grammatical, and lexical 
errors, which are relatively easy to categorize 
compared with other error types, such as 
discourse errors and other types of errors related 
to more communicative aspects of learners? 
language. As shown in Figure 3, our error tags 
contain three pieces of information: POS, 
morphological/grammatical/lexical rules, and a 
corrected form. For errors that cannot be 
categorized as they do not belong to any word 
class, such as the misordering of words, we 
prepared special tags. The error tagset currently 
consists of 46 tags (Table 1). 
POS
(i.e. n =noun)
Grammatical system
(i.e. num =number)
Erroneous part
Corrected form
<n_num crr=?X?>?</n_num>
 
example) I belong to two baseball <n_num crr= 
?teams?>team</n_num>. 
Figure 3. Structure of an Error Tag and an 
Example of an Error-tagged Sentence 
The tags are based on XML (extensible 
markup language) syntax. One advantage of 
using XML is that it can clearly identify the 
structure of the text and it is also very beneficial 
when corpus data is utilized for web-based 
pedagogical tools or databases as a hypertext. 
The error tagset was designed based on the 
concept of the ICLE?s error tagging, that is, to 
deal with as many morphological, grammatical, 
and lexical errors as possible to have a generic 
error tagset. However, there are several 
differences between these two tagsets. For 
example, in the ICLE, only replacement-type 
errors are linguistically categorized, and 
redundant- and omission-type errors are not 
categorized any more and just called as ?word 
redundant? or ?word missing?, while in our 
error tagset, all these three types of errors are 
linguistically categorized. 
Although our error tagset covers major 
grammatical and lexical errors, annotators often 
have difficulties to select the most appropriate 
one for each error in actual tagging process. For 
example, one erroneous part can often been 
interpreted as more than one error type, or 
sometimes multiple errors are overlapping in the 
same position. 
To solve these problems, tagging was done 
under a few basic principles as follows. 
1) Because of the limitation of XML syntax 
(i.e. Crossing of different tags is not 
allowed.), each sentence should be 
corrected in a small unit (word or phrase) 
and avoid to change a sentence structure 
unnecessarily. 
2) If one phenomenon can be interpreted as 
more than one error type, select an error 
type with which an erroneous sentence 
can be reconstructed into a correct one 
without changing the sentence structure 
drastically. In this manner, errors should 
be annotated as locally as possible, but 
there is only one exception for 
prefabricated phrases. For example, if a 
sentence ?There are lot of books.? should 
be corrected into ?There are a lot of 
books.?, two ways of tagging are 
possible as shown in a) and b). 
a) There are <at crr= ?a?></at> lot of 
books. 
b) There are <o_lxc crr= ?a lot of?>lot 
of</o_lxc> books. 
In a), just an article ?a? is added before 
?lot of?, while in b), ?lot of? is corrected 
into ?a lot of? as a prefabricated phrase. 
In this case, b) is preferred. 
3) If multiple errors overlap in the same or 
partly-same position, choose error tags 
with which an erroneous sentence can be 
reconstructed into a correct one step by 
step in order to figure out as many errors 
as possible. For example, in the case that 
a sentence ?They are looking monkeys.? 
should be corrected into a sentence 
?They are watching monkeys.?, two ways 
of tagging are possible as shown in c) 
and d). 
c) They are <v_lxc crr= ?watching?> 
looking</v_lxc> monkeys. 
d) They are <v_lxc crr= ?watching?> 
looking<prp_lxc2 crr= ?at?> 
</prp_lxc2></v_lxc> monkeys. 
In c), ?looking? is replaced with 
?watching? in one step, while in d), 
missing of a preposition ?at? is pointed 
out first, then, ?looking at? is replaced 
75
with ?watching?. In our error tagging 
scheme, d) is more preferred. 
Tag Error category
<n_inf>?</n_inf> Noun inflection
<n_num>?</n_num> Noun number
<n_cs>?</n_cs> Noun case
<n_cnt>?</n_cnt> Countability of noun
<n_cmp>?</n_cmp> Complement of noun
<n_lxc>?</n_lxc> Lexis
<v_inf>?</v_inf> Verb inflection
<v_agr>?</v_agr> Subject-verb disagreement
<v_fml>?</v_fml> Verb form
<v_tns>?</v_tns> Verb tense
<v_asp>?</v_asp> Verb aspect
<v_vo>?</v_vo> Verb voice
<v_fin>?</v_fin> Usage of finite/infinite verb
<v_ng>?</v_ng> Verb negation
<v_qst>?</v_qst> Question
<v_cmp>?</v_cmp> Complement of verb
<v_lxc>?</v_lxc> Lexis
<mo_lxc>?</mo_lxc> Lexis
<aj_inf>?</aj_inf> Adjective inflection
<aj_us>?</aj_us> Usage of positive/comparative/superlative of adjective
<aj_num>?</aj_num> Adjective number
<aj_agr>?</aj_agr> Number disagreement of adjective
<aj_qnt>?</aj_qnt> Quantitative adjective
<aj_cmp>?</aj_cmp> Complement of adjective
<aj_lxc>?</aj_lxc> Lexis
<av_inf>?</av_inf> Adverb inflection
<av_us>?</av_us> Usage of positive/comparative/superlative of adverb
<av_pst>? </av_pst> Adverb position
<av_lxc>?</av_lxc> Lexis
<prp_cmp>?</prp_cmp> Complement of preposition
<prp_lxc1>?</prp_lxc1> Normal preposition
<prp_lxc2>?</prp_lxc2> Dependent preposition
<at>?</at> Article
<pn_inf>?</pn_inf> Pronoun inflection
<pn_agr>?</pn_agr> Number/sex disagreement of pronoun
<pn_cs>?</pn_cs> Pronoun case
<pn_lxc>?</pn_lxc> Lexis
<con_lxc>?</con_lxc> Lexis
<rel_cs>?</rel_cs> Case of relative pronoun
<rel_lxc>?</rel_lxc> Lexis
<itr_lxc>?</itr_lxc> Lexis
<o_je>?</o_je> Japanese English
<o_lxc>?</o_lxc> Collocation
<o_odr>?</o_odr> Misordering of words
<o_uk>?</o_uk> Unknown type errors
<o_uit>?</o_uit> Unintelligible utterance
RELATIVE PRONOUN
INTERROGATIVE
OTHERS
PREPOSITION
ARTICLE
PRONOUN
CONJUNCTION
NOUN
VERB
MODAL VERB
ADVERB
ADJECTIVE
 
Table 1. Error Tags for the NICT JLE Corpus. 
4.1 Advantages of Current Error Tagset 
Error tagging for learner corpora including the 
NICT JLE Corpus and the other corpora listed in 
Section 3 is carried out mainly by categorizing 
?likely? errors implied from the existing 
canonical grammar rules or POS system in 
advance. In this sub-section, we examine the 
advantages of this type of error tagging through 
research and development done by using these 
corpora. 
Tono (2002) tried to determine the order in 
which Japanese learners acquire the major 
English grammatical morphemes using the error 
tag information in the JEFFL Corpus. Izumi and 
Isahara (2004) did the same investigation based 
on the NICT JLE Corpus and found that there 
was a significant correlation between their 
sequence and Tono?s except for a few 
differences that we assume arose from the 
difference in the languga e production medium 
(written or spoken). Granger (1999) found that 
French learners of English tended to make verb 
errors in the simple present and past tenses 
based on the French component of the ICLE. 
Izumi et al (2004) also developed a framework 
for automated error detection based on machine 
learning in which the error-tagged data of the 
NICT JLE Corpus was used as training data. In 
the experiment, they obtained 50% recall and 
76% precision. 
Error tagging based on the existing canonical 
grammar rules or POS system can help to 
successfully assess to what extent learners can 
command the basic language system, especially 
grammar. This can assist people such as teachers 
who want to improve their grammar teaching 
method, researchers who want to construct a 
model of learners? grammatical competence, and 
learners who are studying for exams with 
particular emphasis on grammatical accuracy. 
5 Future Improvement 
Finally, let us explain our plans for future 
improving and extending error tagging for the 
NICT JLE Corpus.  
5.1 Problems of Current Error Tagset 
Although the current error tagging scheme is 
beneficial in the ways mentioned in 4.1, it 
cannot be denied that much could be improved 
to make it useful for teachers and researchers 
who want to know learners? communicative 
skills rather than grammatical competence. The 
same can be said for learners themselves. In the 
past, English education in Japan mainly focused 
on developing grammatical competence in the 
past. However, in recent years, because of the 
recognition of English as an important 
communication tool among peoples with 
different languages or cultures, acquiring 
communicative competence, especially 
76
production skills, has become the main goal for 
learners. One of the most important things for 
acquiring communicative skills might be 
producing outputs that can be understood 
properly by others. In other words, for many 
learners, conveying their messages clearly is 
often more important than just producing 
grammatically-correct sentences. 
It is necessary to make the current error 
tagset more useful for measuring learners? 
communicative competence. To do this, firstly 
we need to know what kind of learners? outputs 
can be understood by native speakers and in 
what cases they fail to convey their messages 
properly. By doing this, it should become 
possible to differentiate fatal errors that prevent 
the entire output from being understood from 
small errors that do not interfere with 
understanding. 
Another goal of studying English for learners, 
especially at the advanced level, is to speak like 
a native speaker. Some learners mind whether 
their English sounds natural or not to native 
speakers. In the current error tagging, both 
obvious errors and expressions that are not 
errors but are unnatural are treated at the same 
level. It would be better to differentiate them in 
the new error annotation scheme. 
5.2 Survey for Extending Current Error 
Tagset 
To solve the problems of our current error 
tagging system discussed in 5.1, we decided to 
do a survey to: 
1) Identify fatal errors and small ones by 
examining ?learners? outputs that can 
be understood properly by native 
speakers? and ?those that do not make 
sense to native speakers?. 
2) Identify unnatural and non-nativelike 
expressions and examine why they 
sound unnatural. 
We will do this mainly by examining the 
learner data corrected by a native speaker. 
Correction by NS 
We asked a native speaker of English to correct 
raw learner data (15 interviews, 17,068 words, 
1,657 sentences) from the NICT JLE Corpus 
and add one of the following three comments 
(Table 2) to each part. 
Comment 1 It is obviously an error, but does 
not interfere with understanding. 
Comment 2 The meaning of the utterance does 
not make sense at all. 
Comment 3 It is not an error, and the 
utterance makes sense, but it 
sounds unnatural. 
Table 2. Comments added to each error 
The person who did the corrections is a 
middle-aged British man who has lived in Japan 
for 14 years. He does not have experience as an 
English teacher, but used to teach Japanese 
Linguistics at a British University. Although he 
is familiar with English spoken by Japanese 
people because of his long residence in Japan 
and the knowledge of the Japanese language, we 
asked him to apply the corrections objectively 
with considering whether or not each utterance 
was generally intelligible to native speakers. 
Corrected Parts 
A total of 959 errors were corrected and 724 of 
these were labeled with Comment 1, 57 with 
Comment 2, and 178 with Comment 3, 
respectively (Table 3). 
Comment 1 724 
Comment 2 57 
Comment 3 178 
Total 959 
Table 3. Number of Errors Labeled with Each 
Comment. 
In order to examine what kind of differences 
can be found among errors labeled with these 
comments, we categorized them into four types 
(morpheme, grammar, lexis, and discourse) 
depending on which linguistic level each of 
them belongs to based on corrected forms and 
additional comments made by the labeler (Table 
4). 
 Comment1 Comment2 Comment3 Total 
Morpheme 6 0 0 6 
Grammar 429 0 52 481 
Lexis 286 43 78 407 
Discourse 3 14 48 65 
Total 724 57 178 959 
Table 4. Linguistic Level Involved in Each Error. 
As a whole, the most common type was 
grammar (481), but most of the grammatical 
errors (or cases of unnaturalness) were labeled 
with Comment 1, which implies that in most 
cases, the grammatical errors do not have a fatal 
influence making the entire output unintelligible. 
The second-most common type was lexical 
errors (or cases of unnaturalness) (407). Half of 
them were labeled with Comment 1, but 23 
errors got Comment 2. This means that some 
77
errors can interfere with understanding. 
Discourse errors accounted for a fraction of a 
percent of all errors (65). However, compared 
with other types of errors, the percentage of 
Comment 2 was the highest (14 out of 65), 
which means that discourse errors can greatly 
interfere with the intelligibility of the entire 
output. The main difference between the 
discourse errors labeled with Comment 2 and 
those labeled with Comment 3 was that most of 
the latter related to collocational expressions, 
while the former involved non-collocational 
phrases where learners need to construct a 
phrase or sentence by combining single words. 
In the following sections, we examine the 
characteristics of each type of error (or cases of 
unnaturalness) in detail. 
Comment 1 
Half of the Comment 1 errors were grammatical 
ones. Most of them were local errors such as 
subject-verb disagreement or article errors. 
There were 286 lexical errors, but in most cases, 
they were not very serious, for example lexical 
confusions among semantically similar 
vocabulary items. 
Comment 2 
Most of the Comment 2 errors had something to 
do with lexis or discourse. 
 
1) Too abrupt literary style (discourse error) 
ex) I?ve been to the restaurant is first. I 
took lunch. The curry the restaurant 
serves is very much, so I was surprised 
and I?m now a little sleepy. 
2) Unclear context (discourse error) 
3) Unclear anaphora (pronouns and 
demonstratives) (discourse error) 
4) Mis-selection of vocabulary (lexical 
error) 
5) Omission of an important word (subject, 
predicate or object) (lexical or syntax 
error) 
ex) She didn?t (*) so much about fashion. 
ex) Last year, I enjoyed living alone, but 
nowadays, it?s a little bit troublesome 
because I have to (*) all of the things by 
myself. 
6) Japanese English/Direct translation 
(lexical error) 
ex) bed town (as ?bedroom suburbs?) 
ex) claim (as ?complaint?) 
Comment 3 
There were grammatical, lexical and discourse 
problems with the parts labeled with Comment 
1) Verbose expressions (discourse-level 
unnaturalness) 
ex) T: Can I call you Hanako? 
L: Yes, please call me Hanako. 
better  Yes, please do. 
ex) Three couples are there and they?re having 
dinner. 
better  Three couples are having dinner. 
ex) I told my friends about this, and my friends 
agreed with me. 
better  I told my friends about this, and they 
agreed with me. 
 
2) Socio-linguistically inappropriate expressions 
(discourse/pragmatic-level unnaturalness) 
ex) What? 
better  I beg your pardon? 
ex) Good. 
better  I?m fine. 
 
3) Abrupt expressions (discourse/pragmatic-
level unnaturalness) 
ex) T: Have you been busy lately? 
 L: No. 
better  No, not really. 
 
4) Overstatement (discourse/pragmatic-level 
unnaturalness) 
(In a normal context) 
ex) T: How are you? 
L: I?m very fine. 
better  I?m fine. 
 
5) There are more appropriate words or 
expressions. (discourse/pragmatic-level of 
unnaturalness) 
ex) To go to high school in the mainland, I went 
out of the island. 
better  ... I left the island. 
5.3 Limitation of Current Error 
Annotation Scheme 
It is obvious that discourse and some types of 
lexical errors can often impede the 
understanding of the entire utterance. 
Although our current error tagset does not 
cover discourse errors, it is still possible to 
78
?just? assign any one of error tags to the 
erroneous parts shown in 5.2. There are two 
reasons for this. One is that, in the current error 
tagging principle, it is possible to replace, add or 
delete all POS in order to make it possible to 
?reconstruct? an erroneous sentence into a 
correct one. The other reason is that since 
discourse structure is liked to grammatical and 
lexical selections, it is possible to translate terms 
for describing discourse into terms for 
describing grammar or lexis. 
However, annotating discourse errors with 
tags named with grammatical or lexical terms 
cannot represents the nature of discourse errors. 
Since discourse errors are often related to 
intelligibility of learners? outputs, describing 
those errors with appropriate terms is quite 
important for making the current error tagset 
something helpful for measuring learners? 
communicative competence. We will need to 
know what kind of discourse errors are made by 
learners, and classify them to build in the error 
tagset. Some parts labeled with Comment 3 
were also related to discourse-level problems. It 
would be beneficial to provide learners with 
feedback such as ?Your English sounds 
unnatural because it?s socio-linguistically 
inappropriate?. Therefore, it is also necessary to 
classify discourse-level unnaturalness in learners 
language. 
5.4 Works for Expansion to New Error 
Tagset 
We decided the basic principles for revising the 
current error tagset as following. 
1) Classify second language discourse 
errors and building them into a new 
error tagset. 
2) Differentiate unnatural expressions 
from errors. Information on why it 
sounds unnatural will also be added. 
3) Add information on linguistic level 
(morpheme, grammar, lexis, and 
discourse) to each tag. 
4) Do a further survey on how we can 
differentiate errors that interfere with 
understanding and those that do not, 
and add information on error gravity to 
each tag. 
Classifying discourse errors will be the most 
important task in the tagset revision. In several 
studies, second language discourse has already 
been discussed (James, 1998), but there is no 
commonly recognized discourse error typology. 
Although grammatical and lexical errors can be 
classified based on the existing canonical 
grammar rules or POS system, in order to 
construct the discourse error typology, we will 
need to do more investigation into ?real? 
samples of learners? discourse errors. 
Adding the information on linguistic level 
(morpheme, grammar, lexis and discourse) to 
each tag is also important. From the survey, we 
found that the linguistic level of errors is 
strongly related to the intelligibility of the entire 
output. If linguistic level information is added to 
each error tag, this might help to measure the 
intelligibility of learners? utterances, that is, 
learners? communicative competence. 
6 Conclusion 
In this paper, we discussed how the error 
annotation scheme for learner corpora should be 
designed mainly by explaining the current error 
tagging scheme for the NICT JLE Corpus and 
its future expansion. Through learner data 
corrected by a native speaker, we decided to 
introduce discourse errors into the error 
annotation in order to cover learners? 
communicative competence, which cannot be 
measured with the current error tagging scheme. 
 
References 
Corder, P. (1981). Error Analysis and Interlanguage. 
Oxford: Oxford University Press. 
Degneaux, E., Denness, S., Granger, S., & Meunier, 
F. (1996). Error Tagging Manual Version 1.1. 
Centre for English Corpus Linguistics, Universite 
Catholique de Louvain. 
Degneaux, E., Denness, S., & Ganger, S. (1998). 
Computer-aided error analysis, System, 26, 163-
174. 
Ellis, R. (1994). The Study of Second Language 
Acquisition. Oxford: Oxford University Press. 
Ellis, R. (1997). Second Language Acquisition. 
Oxford: Oxford University Press. pp. 47, 67. 
Granger, S. (1999). Use of tenses by advanced EFL 
learners: evidence from an error-tagged computer 
corpus. In Hasselgard, H., & Oksefjell, S. (Eds). 
Out of Corpora. (pp. 191-202). Amsterdam: 
Rodopi. 
79
Granger, S. (2002) A bird?s-eye view of learner 
corpus research. In Granger, S., Hung, J., and 
Tyson, P.S. (Eds.). (2002). Computer Learner 
Corpora, Second Language Acquisition and 
Foreign Language Teaching, Amsterdam: John 
Benjamins Publishing Company. 
Izumi, E., Uchimoto, K., & Isahara, H. (2004). The 
overview of the SST speech corpus of Japanese 
learner English and evaluation through the 
experiment on automatic detection of learners' 
errors. In Proceedings of Language Resource and 
Evaluation Conference (LREC) 2004, Portugal, 
1435-1438. 
Izumi, E., & Isahara, H. (2004). Investigation into 
language learners' acquisition order based on the 
error analysis of the learner corpus. In 
Proceedings of Pacific-Asia Conference on 
Language, Information and Computation 
(PACLIC) 18 Satellite Workshop on E-Learning. 
Tokyo, Japan. 
James, C. (1998). Errors in Language Learning and 
Use: exploring error analysis. Essex: Longman. 
Selinker, L. (1972). Interlanguage. In Robinett, B. W., 
& Schachter, J. (Eds.). (1983). Second Language 
Learning: Contrastive analysis, error analysis, 
and related aspects. (pp. 173-196). Michigan: The 
University of Michigan Press. 
Tono, Y. (2002). The Role of Learner Corpora in 
SLA Research and Foreign Language Teaching: 
The Multiple Comparison Approach. Unpublished 
Ph.D. Thesis. Lancaster University, UK. 
CLC (Cambridge Learners Corpus)?s Website: 
http://uk.cambridge.org/elt/corpus/clc.htm 
80
Dependency Parsing with Short Dependency Relations in Unlabeled Data
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, dk, uchimoto, yujie, isahara}@nict.go.jp
Abstract
This paper presents an effective dependency
parsing approach of incorporating short de-
pendency information from unlabeled data.
The unlabeled data is automatically parsed
by a deterministic dependency parser, which
can provide relatively high performance for
short dependencies between words. We then
train another parser which uses the informa-
tion on short dependency relations extracted
from the output of the first parser. Our pro-
posed approach achieves an unlabeled at-
tachment score of 86.52, an absolute 1.24%
improvement over the baseline system on
the data set of Chinese Treebank.
1 Introduction
In dependency parsing, we attempt to build the
dependency links between words from a sen-
tence. Given sufficient labeled data, there are sev-
eral supervised learning methods for training high-
performance dependency parsers(Nivre et al, 2007).
However, current statistical dependency parsers pro-
vide worse results if the dependency length be-
comes longer (McDonald and Nivre, 2007). Here
the length of a dependency from word w
i
and word
w
j
is simply equal to |i ? j|. Figure 1 shows the
F
1
score1 provided by a deterministic parser rela-
tive to dependency length on our testing data. From
1precision represents the percentage of predicted arcs of
length d that are correct and recall measures the percentage of
gold standard arcs of length d that are correctly predicted.
F
1
= 2? precision? recall/(precision + recall)
the figure, we find that F
1
score decreases when de-
pendency length increases as (McDonald and Nivre,
2007) found. We also notice that the parser pro-
vides good results for short dependencies (94.57%
for dependency length = 1 and 89.40% for depen-
dency length = 2). In this paper, short dependency
refers to the dependencies whose length is 1 or 2.
 30
 40
 50
 60
 70
 80
 90
 100
 0  5  10  15  20  25  30
F1
Dependency Length
baseline
Figure 1: F-score relative to dependency length
Labeled data is expensive, while unlabeled data
can be obtained easily. In this paper, we present an
approach of incorporating unlabeled data for depen-
dency parsing. First, all the sentences in unlabeled
data are parsed by a dependency parser, which can
provide state-of-the-art performance. We then ex-
tract information on short dependency relations from
the parsed data, because the performance for short
dependencies is relatively higher than others. Fi-
nally, we train another parser by using the informa-
tion as features.
The proposed method can be regarded as a semi-
supervised learning method. Currently, most semi-
88
supervised methods seem to do well with artificially
restricted labeled data, but they are unable to outper-
form the best supervised baseline when more labeled
data is added. In our experiments, we show that our
approach significantly outperforms a state-of-the-art
parser, which is trained on full labeled data.
2 Motivation and previous work
The goal in dependency parsing is to tag dependency
links that show the head-modifier relations between
words. A simple example is in Figure 2, where the
link between a and bird denotes that a is the depen-
dent of the head bird.
I    see    a    beautiful    bird    .
Figure 2: Example dependency graph.
We define that word distance of word w
i
and word
w
j
is equal to |i ? j|. Usually, the two words in a
head-dependent relation in one sentence can be adja-
cent words (word distance = 1) or neighboring words
(word distance = 2) in other sentences. For exam-
ple, ?a? and ?bird? has head-dependent relation in
the sentence at Figure 2. They can also be adjacent
words in the sentence ?I see a bird.?.
Suppose that our task is Chinese dependency
parsing. Here, the string ????JJ(Specialist-
level)/? ?NN(working)/? ?NN(discussion)?
should be tagged as the solution (a) in Figure
3. However, our current parser may choose the
solution (b) in Figure 3 without any additional
information. The point is how to assign the head for
????(Specialist-level)?. Is it ???(working)?
or ???(discussion)??
  
  
  
(b)
(a)
Figure 3: Two solutions for ????(Specialist-
level)/??(working)/??(discussion)?
As Figure 1 suggests, the current dependency
parser is good at tagging the relation between ad-
jacent words. Thus, we expect that dependencies
of adjacent words can provide useful information
for parsing words, whose word distances are longer.
When we search the string ????(Specialist-
level)/??(discussion)? at google.com, many rele-
vant documents can be retrieved. If we have a good
parser, we may assign the relations between the two
words in the retrieved documents as Figure 4 shows.
We can find that ???(discussion)? is the head of
????(Specialist-level)? in many cases.
1)?525	26
///,//?
2)?Learning Reliability of Parses for Domain Adaptation of
Dependency Parsing
Daisuke Kawahara and Kiyotaka Uchimoto
National Institute of Information and Communications Technology,
3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan
{dk, uchimoto}@nict.go.jp
Abstract
The accuracy of parsing has exceeded 90%
recently, but this is not high enough to use
parsing results practically in natural lan-
guage processing (NLP) applications such as
paraphrase acquisition and relation extrac-
tion. We present a method for detecting re-
liable parses out of the outputs of a single
dependency parser. This technique is also
applied to domain adaptation of dependency
parsing. Our goal was to improve the per-
formance of a state-of-the-art dependency
parser on the data set of the domain adap-
tation track of the CoNLL 2007 shared task,
a formidable challenge.
1 Introduction
Dependency parsing has been utilized in a variety
of natural language processing (NLP) applications,
such as paraphrase acquisition, relation extraction
and machine translation. For newspaper articles, the
accuracy of dependency parsers exceeds 90% (for
English), but it is still not sufficient for practical use
in these NLP applications. Moreover, the accuracy
declines significantly for out-of-domain text, such as
weblogs and web pages, which have commonly been
used as corpora. From this point of view, it is impor-
tant to consider the following points to use a parser
practically in applications:
? to select reliable parses, especially for knowl-
edge acquisition,
? and to adapt the parser to new domains.
This paper proposes a method for selecting reli-
able parses from parses output by a single depen-
dency parser. We do not use an ensemble method
based on multiple parsers, but use only a single
parser, because speed and efficiency are important
when processing a massive volume of text. The
resulting highly reliable parses would be useful to
automatically construct dictionaries and knowledge
bases, such as case frames (Kawahara and Kuro-
hashi, 2006). Furthermore, we incorporate the reli-
able parses we obtained into the dependency parser
to achieve domain adaptation.
The CoNLL 2007 shared task tackled domain
adaptation of dependency parsers for the first time
(Nivre et al, 2007). Sagae and Tsujii applied an
ensemble method to the domain adaptation track
and achieved the highest score (Sagae and Tsujii,
2007). They first parsed in-domain unlabeled sen-
tences using two parsers trained on out-of-domain
labeled data. Then, they extracted identical parses
that were produced by the two parsers and added
them to the original (out-of-domain) training set to
train a domain-adapted model.
Dredze et al yielded the second highest score1
in the domain adaptation track (Dredze et al, 2007).
However, their results were obtained without adap-
tation. They concluded that it is very difficult to sub-
stantially improve the target domain performance
over that of a state-of-the-art parser. To confirm
this, we parsed the test set (CHEM) of the domain
adaptation track by using one of the best dependency
parsers, second-order MSTParser (McDonald et al,
1Dredze et al achieved the second highest score on the
CHEM test set for unlabeled dependency accuracy.
709
2006)2. Though this parser was trained on the pro-
vided out-of-domain (Penn Treebank) labeled data,
surprisingly, its accuracy slightly outperformed the
highest score achieved by Sagae and Tsujii (unla-
beled dependency accuracy: 83.58 > 83.42 (Sagae
and Tsujii, 2007)). Our goal is to improve a state-
of-the-art parser on this domain adaptation track.
Dredze et al also indicated that unlabeled de-
pendency parsing is not robust to domain adaptation
(Dredze et al, 2007). This paper therefore focuses
on unlabeled dependency parsing.
2 Related Work
We have already described the domain adaptation
track of the CoNLL 2007 shared task. For the mul-
tilingual dependency parsing track, which was the
other track of the shared task, Nilsson et al achieved
the best performance using an ensemble method
(Hall et al, 2007). They used a method of com-
bining several parsers? outputs in the framework of
MST parsing (Sagae and Lavie, 2006). This method
does not select parses, but considers all the output
parses with weights to decide a final parse of a given
sentence.
Reichart and Rappoport also proposed an ensem-
ble method to select high-quality parses from the
outputs of constituency parsers (Reichart and Rap-
poport, 2007a). They regarded parses as being of
high quality if 20 different parsers agreed. They did
not apply their method to domain adaptation or other
applications.
Reranking methods for parsing have a relation
to parse selection. They rerank the n-best parses
that are output by a generative parser using a lot
of lexical and syntactic features (Collins and Koo,
2005; Charniak and Johnson, 2005). There are
several related methods for 1-best outputs, such
as revision learning (Nakagawa et al, 2002) and
transformation-based learning (Brill, 1995) for part-
of-speech tagging. Attardi and Ciaramita proposed
a method of tree revision learning for dependency
parsing (Attardi and Ciaramita, 2007).
As for the use of unlabeled data, self-training
methods have been successful in recent years. Mc-
Closky et al improved a state-of-the-art con-
stituency parser by 1.1% using self-training (Mc-
2http://sourceforge.net/projects/mstparser/
Table 1: Labeled and unlabeled data provided for
the shared task. The labeled PTB data is used for
training, and the labeled BIO data is used for devel-
opment. The labeled CHEM data is used for the final
test.
name source labeled unlabeled
PTB Penn Treebank 18,577 1,625,606
BIO Penn BioIE 200 369,439
CHEM Penn BioIE 200 396,128
Closky et al, 2006a). They also applied self-training
to domain adaptation of a constituency parser (Mc-
Closky et al, 2006b). Their method simply adds
parsed unlabeled data without selecting it to the
training set. Reichart and Rappoport applied self-
training to domain adaptation using a small set of
in-domain training data (Reichart and Rappoport,
2007b).
Van Noord extracted bilexical preferences from a
Dutch parsed corpus of 500M words without selec-
tion (van Noord, 2007). He added some features into
an HPSG (head-driven phrase structure grammar)
parser to consider the bilexical preferences, and ob-
tained an improvement of 0.5% against a baseline.
Kawahara and Kurohashi extracted reliable de-
pendencies from automatic parses of Japanese sen-
tences on the web to construct large-scale case
frames (Kawahara and Kurohashi, 2006). Then
they incorporated the constructed case frames into a
probabilistic dependency parser, and outperformed
their baseline parser by 0.7%.
3 The Data Set
This paper uses the data set that was used in the
CoNLL 2007 shared task (Nivre et al, 2007). Table
1 lists the data set provided for the domain adapta-
tion track.
We pre-processed all the unlabeled sentences us-
ing a conditional random fields (CRFs)-based part-
of-speech tagger. This tagger is trained on the
PTB training set that consists of 18,577 sentences.
The features are the same as those in (Ratnaparkhi,
1996). As an implementation of CRFs, we used
CRF++3. If a method of domain adaptation is ap-
plied to the tagger, the accuracy of parsing unlabeled
sentences will improve (Yoshida et al, 2007). This
3http://crfpp.sourceforge.net/
710
paper, however, does not deal with domain adapta-
tion of a tagger but focuses on that of a parser.
4 Learning Reliability of Parses
Our approach assesses automatic parses of a single
parser in order to select only reliable parses from
them. We compare automatic parses and their gold-
standard ones, and regard accurate parses as positive
examples and the remainder as negative examples.
Based on these examples, we build a binary classi-
fier that classifies each sentence as reliable or not.
To precisely detect reliable parses, we make use of
several linguistic features inspired by the notion of
controlled language (Mitamura et al, 1991). That is
to say, the reliability of parses is judged based on the
degree of sentence difficulty.
Before describing our base dependency parser and
the algorithm for detecting reliable parses, we first
explain the data sets used for them. We prepared
the following three labeled data sets to train the base
dependency parser and the reliability detector.
PTB base train: training set for the base parser:
14,862 sentences
PTB rel train: training set for reliability detector:
2,500 sentences4
BIO rel dev: development set for reliability detec-
tor: 200 sentences (= labeled BIO data)
PTB base train is used to train the base depen-
dency parser, and PTB rel train is used to train our
reliability detector. BIO rel dev is used for tuning
the parameters of the reliability detector.
4.1 Base Dependency Parser
We used the MSTParser (McDonald et al, 2006),
which achieved top results in the CoNLL 2006
(CoNLL-X) shared task, as a base dependency
parser. To enable second-order features, the param-
eter order was set to 2. The other parameters were
set to default. We used PTB base train (14,862 sen-
tences) to train this parser.
4.2 Algorithm to Detect Reliable Parses
We built a binary classifier for detecting reliable sen-
tences from a set of automatic parses produced by
41,215 labeled PTB sentences are left as another develop-
ment set for the reliability detector, but they are not used in this
paper.
the base dependency parser.
We used support vector machines (SVMs) as a bi-
nary classifier with a third-degree polynomial ker-
nel. We parsed PTB rel train (2,500 sentences) us-
ing the base parser, and evaluated each sentence with
the metric of unlabeled dependency accuracy. We
regarded the sentences whose accuracy is better than
a threshold, ? , as positive examples, and the others
as negative ones. In this experiment, we set the ac-
curacy threshold ? at 100%. As a result, 736 out of
2,500 examples (sentences) were judged to be posi-
tive.
To evaluate the reliability of parses, we take ad-
vantage of the following features that can be related
to the difficulty of sentences.
sentence length: The longer the sentence is, the
poorer the parser performs (McDonald and Nivre,
2007). We determine sentence length by the number
of words.
dependency lengths: Long-distance dependen-
cies exhibit bad performance (McDonald and Nivre,
2007). We calculate the average of the dependency
length of each word.
difficulty of vocabulary: It is hard for super-
vised parsers to learn dependencies that include low-
frequency words. We count word frequencies in the
training data and make a word list in descending or-
der of frequency. For a given sentence, we calculate
the average frequency rank of each word.
number of unknown words: Similarly, depen-
dency accuracy for unknown words is notoriously
poor. We count the number of unknown words in a
given sentence.
number of commas: Sentences with multiple
commas are difficult to parse. We count the num-
ber of commas in a given sentence.
number of conjunctions (and/or): Sentences
with coordinate structures are also difficult to parse
(Kurohashi and Nagao, 1994). We count the num-
ber of coordinate conjunctions (and/or) in a given
sentence.
To apply these features to SVMs in practice, the
numbers are binned at a certain interval for each fea-
ture. For instance, the number of conjunctions is
split into four bins: 0, 1, 2 and more than 2.
711
Table 2: Example BIO sentences judged as reliable. The underlined words have incorrect modifying heads.
dep. accuracy sentences judged as reliable
12/12 (100%) No mutations resulting in truncation of the APC protein were found .
12/13 (92%) Conventional imaging techniques did not show two in 10 of these patients .
6/6 (100%) Pancreatic juice was sampled endoscopically .
11/12 (92%) The specificity of p53 mutation for pancreatic cancer is very high .
9/10 (90%) K-ras mutations are early genetic changes in colon cancer .
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 80  82  84  86  88  90  92  94  96  98  100
Sen
tenc
e co
vera
ge (%
)
Dependency accuracy (%)
Figure 1: Accuracy-coverage curve on BIO rel dev.
4.3 Experiments on Detecting Reliable Parses
We conducted an experiment on detecting the reli-
ability of parses. Our detector was applied to the
automatic parses of BIO rel dev, and only reliable
parses were selected from them. When parsing this
set, the POS tags contained in the set were substi-
tuted with automatic POS tags because it is prefer-
able to have the same environment as when applying
the parser to unlabeled data.
We evaluated unlabeled dependency accuracy of
the extracted parses. The accuracy-coverage curve
shown in Figure 1 was obtained by changing the soft
margin parameter C 5 of SVMs from 0.0001 to 10.
In this figure, the coverage is the ratio of selected
sentences out of all the sentences (200 sentences),
and the accuracy is unlabeled dependency accuracy.
A coverage of 100% indicates that the accuracy of
200 sentences without any selection was 80.85%.
If the soft margin parameter C is set to 0.001,
we can obtain 19 sentences out of 200 at a depen-
dency accuracy of 93.85% (183/195). The average
sentence length was 10.3 words. Out of obtained
19 sentences, 14 sentences achieved a dependency
accuracy of 100%, and thus the precision of the reli-
ability detector itself was 73.7% (14/19). Out of 200
sentences, 36 sentences were correctly parsed by the
5A higher soft margin value allows more classification er-
rors, and thus leads to the increase of recall and the decrease of
precision.
base parser, and thus the recall is 38.9% (14/36).
Table 2 shows some sentences that were evaluated
as reliable using the above setting (C = 0.001). Ma-
jor errors were caused by prepositional phrase (PP)-
attachment. To improve the accuracy of detecting
reliable parses, it would be necessary to consider the
number of PP-attachment ambiguities in a given sen-
tence as a feature.
5 Domain Adaptation of Dependency
Parsing
For domain adaptation, we adopt a self-training
method. We combine in-domain unlabeled (auto-
matically labeled) data with out-of-domain labeled
data to make a training set. There are many possible
methods for combining unlabeled and labeled data
(Daume? III, 2007), but we simply concatenate unla-
beled data with labeled data to see the effectiveness
of the selected reliable parses. The in-domain unla-
beled data to be added are selected by the reliability
detector. We set the soft margin parameter at 0.001
to extract highly reliable parses. As mentioned in
the previous section, the accuracy of selected parses
was approximately 94%.
We parsed the unlabeled sentences of BIO and
CHEM (approximately 400K sentences for each) us-
ing the base dependency parser that is trained on the
entire PTB labeled data. Then, we applied the reli-
ability detector to these parsed sentences to obtain
31,266 sentences for BIO and 31,470 sentences for
CHEM. We call the two sets of obtained sentences
?BIO pool? and ?CHEM pool?.
For each training set of the experiments described
below, a certain number of sentences are randomly
selected from the pool and combined with the entire
out-of-domain (PTB) labeled data.
5.1 Experiment on BIO Development Data
We first conducted an experiment of domain adapta-
tion using the BIO development set.
712
 83
 83.5
 84
 84.5
 85
 0  5000  10000  15000  20000  25000
Acc
ura
cy (%
)
Number of Unlabeled Sentences
reliable parses
randomly selected parses
without addition
Figure 2: Dependency accuracies on BIO when the
number of added unlabeled data is changed.
Figure 2 shows how the accuracy changes when
the number of added reliable parses is changed. The
solid line represents our proposed method, and the
dotted line with points represents a baseline method.
This baseline is a self-training method that simply
adds unlabeled data without selection to the PTB
labeled data. Each experimental result is the aver-
age of five trials done to randomly select a certain
number of parses from the BIO pool. The horizontal
dotted line (84.07%) represents the accuracy of the
parser without adding unlabeled data (trained only
on the PTB labeled data).
From this figure, we can see that the proposed
method always outperforms the baseline by approxi-
mately 0.4%. The best accuracy was achieved when
18,000 unlabeled parses were added. However, if
more than 18,000 sentences are added, the accuracy
declines. This can be attributed to the balance of the
number of labeled data and unlabeled data. Since
the number of added unlabeled data is more than
the number of labeled data, the entire training set
might be unreliable, though the accuracy of added
unlabeled data is relatively high. To address this
problem, it is necessary to weigh labeled data or
to change the way information from acquired unla-
beled data is handled.
5.2 Experiment on CHEM Test Data
The addition of 18,000 sentences showed the high-
est accuracy for the BIO development data. To adapt
the parser to the CHEM test set, we used 18,000 reli-
able unlabeled sentences from the CHEM pool with
the PTB labeled sentences to train the parser. Ta-
ble 3 lists the experimental results. In this table, the
Table 3: Experimental results on CHEM test data.
system accuracy
PTB+unlabel (18,000 sents.) 84.12
only PTB (baseline) 83.58
1st (Sagae and Tsujii, 2007) 83.42
2nd (Dredze et al, 2007) 83.38
3rd (Attardi et al, 2007) 83.08
third row lists the three highest scores of the domain
adaptation track of the CoNLL 2007 shared task.
The baseline parser was trained only on the PTB
labeled data (as described in Section 1). The pro-
posed method (PTB+unlabel (18,000 sents.)) out-
performed the baseline by approximately 0.5%, and
also beat all the systems submitted to the domain
adaptation track. These systems include an en-
semble method (Sagae and Tsujii, 2007) and an
approach of tree revision learning with a selec-
tion method of only using short training sentences
(shorter than 30 words) (Attardi et al, 2007).
6 Discussion and Conclusion
This paper described a method for detecting reliable
parses out of the outputs of a single dependency
parser. This technique was also applied to domain
adaptation of dependency parsing.
To extract reliable parses, we did not adopt an en-
semble method, but used a single-parser approach
because speed and efficiency are important in pro-
cessing a gigantic volume of text to benefit knowl-
edge acquisition. In this paper, we employed the
MSTParser, which can process 3.9 sentences/s on a
XEON 3.0GHz machine in spite of the time com-
plexity of O(n3). If greater efficiency is required,
it is possible to apply a pre-filter that removes long
sentences (e.g., longer than 30 words), which are
seldom selected by the reliability detector. In ad-
dition, our method does not depend on a particu-
lar parser, and can be applied to other state-of-the-
art parsers, such as Malt Parser (Nivre et al, 2006),
which is a feature-rich linear-time parser.
In general, it is very difficult to improve the accu-
racy of the best performing systems by using unla-
beled data. There are only a few successful studies,
such as (Ando and Zhang, 2005) for chunking and
(McClosky et al, 2006a; McClosky et al, 2006b) on
constituency parsing. We succeeded in boosting the
accuracy of the second-order MST parser, which is
713
a state-of-the-art dependency parser, in the CoNLL
2007 domain adaptation task. This was a difficult
challenge as many participants in the task failed to
obtain any meaningful gains from unlabeled data
(Dredze et al, 2007). The key factor in our success
was the extraction of only reliable information from
unlabeled data.
However, that improvement was not satisfactory.
In order to achieve more gains, it is necessary to ex-
ploit a much larger number of unlabeled data. In this
paper, we adopted a simple method to combine un-
labeled data with labeled data. To use this method
more effectively, we need to balance the labeled and
unlabeled data very carefully. However, this method
is not scalable because the training time increases
significantly as the size of a training set expands. We
can consider the information from more unlabeled
data as features of machine learning techniques. An-
other approach is to formalize a probabilistic model
based on unlabeled data.
References
Rie Ando and Tong Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Proceed-
ings of ACL2005, pages 1?9.
Giuseppe Attardi and Massimiliano Ciaramita. 2007. Tree re-
vision learning for dependency parsing. In Proceedings of
NAACL-HLT2007, pages 388?395.
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, Atanas
Chanev, and Massimiliano Ciaramita. 2007. Multilingual
dependency parsing and domain adaptation using DeSR. In
Proceedings of EMNLP-CoNLL2007, pages 1112?1118.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing. Computational Linguis-
tics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of ACL2005, pages 173?180.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Hal Daume? III. 2007. Frustratingly easy domain adaptation. In
Proceedings of ACL2007, pages 256?263.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman
Ganchev, Joa?o V. Grac?a, and Fernando Pereira. 2007. Frus-
tratingly hard domain adaptation for dependency parsing. In
Proceedings of EMNLP-CoNLL2007, pages 1051?1055.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit, Bea?ta
Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single
malt or blended? a study in multilingual parser optimization.
In Proceedings of EMNLP-CoNLL2007, pages 933?939.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for Japanese syntactic and
case structure analysis. In Proceedings of HLT-NAACL2006,
pages 176?183.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long Japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
David McClosky, Eugene Charniak, and Mark Johnson. 2006a.
Effective self-training for parsing. In Proceedings of HLT-
NAACL2006, pages 152?159.
David McClosky, Eugene Charniak, and Mark Johnson. 2006b.
Reranking and self-training for parser adaptation. In Pro-
ceedings of COLING-ACL2006, pages 337?344.
Ryan McDonald and Joakim Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In Pro-
ceedings of EMNLP-CoNLL2007, pages 122?131.
Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006.
Multilingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings of CoNLL-X, pages 216?220.
TerukoMitamura, Eric Nyberg, and Jaime Carbonell. 1991. An
efficient interlingua translation system for multi-lingual doc-
ument production. In Proceedings of MT Summit III, pages
55?61.
Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto. 2002. Re-
vision learning and its application to part-of-speech tagging.
In Proceedings of ACL2002, pages 497?504.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?l sen Eryi git, and
Svetoslav Marinov. 2006. Labeled pseudo-projective de-
pendency parsing with support vector machines. In Proceed-
ings of CoNLL-X, pages 221?225.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The
CoNLL 2007 shared task on dependency parsing. In Pro-
ceedings of EMNLP-CoNLL2007, pages 915?932.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP1996,
pages 133?142.
Roi Reichart and Ari Rappoport. 2007a. An ensemble method
for selection of high quality parses. In Proceedings of
ACL2007, pages 408?415.
Roi Reichart and Ari Rappoport. 2007b. Self-training for
enhancement and domain adaptation of statistical parsers
trained on small datasets. In Proceedings of ACL2007, pages
616?623.
Kenji Sagae and Alon Lavie. 2006. Parser combination by
reparsing. In Proceedings of the Companion Volume to HLT-
NAACL2006, pages 129?132.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser ensem-
bles. In Proceedings of EMNLP-CoNLL2007, pages 1044?
1050.
Gertjan van Noord. 2007. Using self-trained bilexical prefer-
ences to improve disambiguation accuracy. In Proceedings
of IWPT2007, pages 1?10.
Kazuhiro Yoshida, Yoshimasa Tsuruoka, Yusuke Miyao, and
Jun?ichi Tsujii. 2007. Ambiguous part-of-speech tagging
for improving accuracy and domain portability of syntactic
parsers. In Proceedings of IJCAI-07, pages 1783?1788.
714
Proceedings of NAACL HLT 2007, pages 33?40,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic Evaluation of Machine Translation Based on Rate of
Accomplishment of Sub-goals
Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp
Abstract
The quality of a sentence translated by a
machine translation (MT) system is dif-
ficult to evaluate. We propose a method
for automatically evaluating the quality
of each translation. In general, when
translating a given sentence, one or more
conditions should be satisfied to maintain
a high translation quality. In English-
Japanese translation, for example, prepo-
sitions and infinitives must be appropri-
ately translated. We show several proce-
dures that enable evaluating the quality of
a translated sentence more appropriately
than using conventional methods. The
first procedure is constructing a test set
where the conditions are assigned to each
test-set sentence in the form of yes/no
questions. The second procedure is devel-
oping a system that determines an answer
to each question. The third procedure is
combining a measure based on the ques-
tions and conventional measures. We also
present a method for automatically gener-
ating sub-goals in the form of yes/no ques-
tions and estimating the rate of accom-
plishment of the sub-goals. Promising re-
sults are shown.
1 Introduction
In machine translation (MT) research, appropriately
evaluating the quality of MT results is an important
issue. In recent years, many researchers have tried
to automatically evaluate the quality of MT and im-
prove the performance of automatic MT evaluations
(Niessen et al, 2000; Akiba et al, 2001; Papineni et
al., 2002; NIST, 2002; Leusch et al, 2003; Turian et
al., 2003; Babych and Hartley, 2004; Lin and Och,
2004; Banerjee and Lavie, 2005; Gimen?ez et al,
2005) because improving the performance of auto-
matic MT evaluation is expected to enable us to use
and improve MT systems efficiently. For example,
Och reported that the quality of MT results was im-
proved by using automatic MT evaluation measures
for the parameter tuning of an MT system (Och,
2003). This report shows that the quality of MT re-
sults improves as the performance of automatic MT
evaluation improves.
MT systems can be ranked if a set of MT re-
sults for each system and their reference translations
are given. Usually, about 300 or more sentences
are used to automatically rank MT systems (Koehn,
2004). However, the quality of a sentence translated
by an MT system is difficult to evaluate. For exam-
ple, the results of five MTs into Japanese of the sen-
tence ?The percentage of stomach cancer among the
workers appears to be the highest for any asbestos
workers.? are shown in Table 1. A conventional au-
tomatic evaluation method ranks the fifth MT result
first although its human subjective evaluation is the
lowest. This is because conventional methods are
based on the similarity between a translated sentence
and its reference translation, and they give the trans-
lated sentence a high score when the two sentences
are globally similar to each other in terms of lexical
overlap. However, in the case of the above example,
33
Table 1: Examples of conventional automatic evaluations.
Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos work-
ers.
Reference translation
(in Japanese)
roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda .
System MT results BLEU NIST Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata
roudousha no tame ni demo mottomo ookii youdearu .
0.2111 2.1328 2 3
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto
roudousha no tame ni mottomo takai youni omowa re masu .
0.2572 2.1234 2 3
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame
ni mo mottomo takai youni mie masu
0 1.8094 1 2
4 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni wa
mottomo takaku mie masu .
0 1.5902 1 2
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mot-
tomo takai youni mieru .
0.2692 2.2640 1 2
the most important thing to maintain a high trans-
lation quality is to correctly translate ?for? into the
target language, and it would be difficult to detect
the importance just by comparing an MT result and
its reference translations even if the number of ref-
erence translations is increased.
In general, when translating a given sentence, one
or more conditions should be satisfied to maintain a
high translation quality. In this paper, we show that
constructing a test set where the conditions that are
mainly established from a linguistic point of view
are assigned to each test-set sentence in the form
of yes/no questions, developing a system that de-
termines an answer to each question, and combin-
ing a measure based on the questions and conven-
tional measures enable the evaluation of the quality
of a translated sentence more appropriately than us-
ing conventional methods. We also present a method
for automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals.
2 Test Set for Evaluating Machine
Translation Quality
2.1 Test Set
Two main types of data are used for evaluating MT
quality. One type of data is constructed by arbi-
trarily collecting sentence pairs in the source- and
target-languages, and the other is constructed by in-
tensively collecting sentence pairs that include lin-
guistic phenomena that are difficult to automatically
translate. Recently, MT evaluation campaigns such
as the International Workshop on Spoken Language
Translation 1, NISTMachine Translation Evaluation
2
, and HTRDP Evaluation 3 were organized to sup-
port the improvement of MT techniques. The data
used in the evaluation campaigns were arbitrarily
collected from newspaper articles or travel conver-
sation data for fair evaluation. They are classified
as the former type of data mentioned above. On the
other hand, the data provided by NTT (Ikehara et al,
1994) and that constructed by JEIDA (Isahara, 1995)
are classified as the latter type. Almost all the data
mentioned above consist of only parallel translations
in two languages. Data with information for evaluat-
ing MT results, such as JEIDA?s are rarely found. In
this paper, we call data that consist of parallel trans-
lations collected for MT evaluation and that the in-
formation for MT evaluation is assigned to, a test
set.
The most characteristic information assigned to
the JEIDA test set is the yes/no question for assess-
ing the translation results. For example, a yes/no
question such as ?Is ?for? translated into an expres-
sion representing a cause/reason such as ?de??? (in
Japanese) is assigned to a test-set sentence. We can
evaluate MT results objectively by answering the
question. An example of a test-set sample consist-
ing of an ID, a source-language sample sentence, its
reference translation, and a question is as follows.
1http://www.slt.atr.jp/IWSLT2006/
2http://www.nist.gov/speech/tests/mt/index.htm
3http://www.863data.org.cn/
34
ID 1.1.7.1.3-1
Sample sen-
tence
The percentage of stomach can-
cer among the workers appears
to be the highest for any asbestos
workers.
Reference
translation
(in Japanese)
roudousha no igan no wariai wa
, asubesuto roudousha no tame
ni saikou to naru youda .
Question Is ?appear to? translated into an
auxiliary verb such as ?youda??
The questions are classified mainly in terms of
grammar, and the numbers to the left of the hyphen-
ation of each ID such as 1.1.7.1.3 represent the cat-
egories of the questions. For example, the above
question is related to catenative verbs.
The JEIDA test set consists of two parts, one for
the evaluation of English-Japanese MT and the other
for that of Japanese-English MT. We focused on the
part for English-Japanese MT. This part consists of
769 sample sentences, each of which has a yes/no
question.
The 769 sentences were translated by using five
commercial MT systems to investigate the relation-
ship between subjective evaluation based on yes/no
questions and conventional subjective evaluation
based on fluency and adequacy. The instruction for
the subjective evaluation based on fluency and ad-
equacy followed that given in the TIDES specifi-
cation (TIDES, 2002). The subjective evaluation
based on yes/no questions was done by manually
answering each question for each translation. The
subjective evaluation based on the yes/no questions
was stable; namely, it was almost independent of
the human subjects in our preliminary investigation.
There were only two questions for which the an-
swers generated inconsistency in the subjective eval-
uation when 1,500 question-answer pairs were ran-
domly sampled and evaluated by two human sub-
jects.
Then, we investigated the correlation between the
two types of subjective evaluation. The correlation
coefficients mentioned in this paper are statistically
significant at the 1% or less significance level. The
Spearman rank-order correlation coefficient is used
in this paper. In the subjective evaluation based on
yes/no questions, yes and no were numerically trans-
formed into 1 and ?1. For 3,845 translations ob-
tained by using five MT systems, the correlation co-
efficients between the subjective evaluations based
on yes/no questions and based on fluency and ade-
quacy were 0.48 for fluency and 0.63 for adequacy.
These results indicate that the two subjective evalu-
ations have relatively strong correlations. The cor-
relation is especially strong between the subjective
evaluation based on yes/no questions and adequacy.
2.2 Expansion of JEIDA Test Set
Each sample sentence in the JEIDA test set has only
one question. Therefore, in the subjective evalua-
tion using the JEIDA test set, translation errors that
do not involve the pre-assigned question are ignored
even if they are serious. Therefore, translations that
have serious errors that are not related to the ques-
tion tend to be evaluated as being of high quality.
To solve this problem, we expanded the test set by
adding new questions about translations with the se-
rious errors.
Sentences whose average grades were three or
less for fluency and adequacy for the translation re-
sults of the five MT systems were selected for the
expansion. Besides them, sentences whose average
grades were more than three for fluency and ade-
quacy for the translation results of the five MT sys-
tems were selected when a majority of evaluation
results based on yes/no questions about the transla-
tions of the five MT systems were no. The number
of selected sentences was 150. The expansion was
manually performed using the following steps.
1. Serious translation errors are extracted from the
MT results.
2. For each extracted error, questions strongly re-
lated to the error are searched for in the test set.
If related questions are found, the same types
of questions are generated for the selected sen-
tence, and the same ID as that of the related
question is assigned to each generated question.
Otherwise, questions are newly generated, and
a new ID is assigned to each generated ques-
tion.
3. Each MT result is evaluated according to each
added question.
Eventually, one or more questions were assigned to
each selected sentence in the test set. Among the 150
35
Table 2: Expanded test-set samples.
ID 1.1.7.1.3-1
Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any
asbestos workers.
Reference translation
(in Japanese)
roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda
.
Question (Q-0) Is ?appear to? translated into an auxiliary verb such as ?youda??
ID 1.1.6.1.3-5
Expanded Translation error ?For? is not translated appropriately.
Question-1 (Q-1) Is ?for? translated into an expression representing a cause/reason such as ?. . .de??
ID Additional-1
Expanded Translation error Some expressions are not translated.
Question-2 (Q-2) Are all English words translated into Japanese?
Table 3: Examples of subjective evaluations based on yes/no questions.
Answer
System MT results Q-0 Q-1 Q-2 Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata
roudousha no tame ni demo mottomo ookii youdearu .
Yes No Yes 2 3
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto
roudousha no tame ni mottomo takai youni omowa re masu .
Yes Yes Yes 2 3
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no
tame ni mo mottomo takai youni mie masu
Yes No No 1 2
4 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni
wa mottomo takaku mie masu .
Yes No No 1 2
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo
mottomo takai youni mieru .
Yes No No 1 2
selected sentences, questions were newly assigned
to 103 sentences. The number of added questions
was 148. The maximum number of questions added
to a sentence was five. After expanding the test set,
the correlation coefficients between the subjective
evaluations based on yes/no questions and based on
fluency and adequacy increased from 0.48 to 0.51
for fluency and from 0.63 to 0.66 for adequacy. The
differences between the correlation coefficients ob-
tained before and after the expansion are statistically
significant at the 5% or less significance level for
adequacy. These results indicate that the expansion
of the test set significantly improves the correlation
between the subjective evaluations based on yes/no
questions and based on adequacy. When two or
more questions were assigned to a test-set sentence,
the subjective evaluation based on the questions was
decided by the majority answer. The majority an-
swers, yes and no, were numerically transformed
into 1 and ?1. Ties between yes and no were trans-
formed into 0. Examples of added questions and
the subjective evaluations based on the questions are
shown in Tables 2 and 3.
3 Automatic Evaluation of Machine
Translation Based on Rate of
Accomplishment of Sub-goals
3.1 A New Measure for Evaluating Machine
Translation Quality
The JEIDA test set was not designed for auto-
matic evaluation but for human subjective evalua-
tion. However, a measure for automatic MT evalu-
ation that strongly correlates fluency and adequacy
is likely to be established because the subjective
evaluation based on yes/no questions has a rela-
tively strong correlation with the subjective evalua-
tion based on fluency and adequacy, as mentioned in
Section 2. In this section, we describe a method for
automatically evaluating MT quality by predicting
an answer to each yes/no question and using those
answers.
Hereafter, we assume that each yes/no question is
defined as a sub-goal that a given translation should
satisfy and that the sub-goal is accomplished if the
answer to the corresponding yes/no question to the
sub-goal is yes. We also assume that the sub-goal
is unaccomplished if the answer is no. A new eval-
uation score, A, is defined based on a multiple lin-
36
Table 4: Examples of Patterns.
Sample sentence She lived there by herself.
Question Is ?by herself? translated as ?hitori de??
Pattern The answer is yes if the pattern [hitori dake de|hitori kiri de |tandoku de|tanshin de] is included in a
translation. Otherwise, the answer is no.
Sample sentence They speak English in New Zealand.
Question The personal pronoun ?they? is omitted in a translation like ?nyuujiilando de wa eigo wo hanasu??
Pattern The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the
answer is no.
ear regression model as follows using the rate of ac-
complishment of the sub-goals and the similarities
between a given translation and its reference trans-
lation. The best-fitted line for the observed data is
calculated by the method of least-squares (Draper
and Smith, 1981).
A =
m
?
i=1
?
S
i
? S
i
(1)
+
n
?
j=1
(?
Q
j
? Q
j
+ ?
Q
?
j
? Q
?
j
) + ?

Q
j
=
{
1 : if subgoal is accomplished
0 : otherwise (2)
Q
?
j
=
{
1 : if subgoal is unaccomplished
0 : otherwise (3)
Here, the term Q
j
corresponds to the rate of accom-
plishment of the sub-goal having the i-th ID, and
?
Q
j
is a weight for the rate of accomplishment. The
term Q
?
j
corresponds to the rate of unaccomplish-
ment of the sub-goal having the i-th ID, and ?
Q
?
j
is a
weight for the rate of unaccomplishment. The value
n indicates the number of types of sub-goals. The
term ?

is constant.
The term S
i
indicates a similarity between a trans-
lated sentence and its reference translation, and ?
S
i
is a weight for the similarity. Many methods for cal-
culating the similarity have been proposed (Niessen
et al, 2000; Akiba et al, 2001; Papineni et al, 2002;
NIST, 2002; Leusch et al, 2003; Turian et al, 2003;
Babych and Hartley, 2004; Lin and Och, 2004;
Banerjee and Lavie, 2005; Gimen?ez et al, 2005).
In our research, 23 scores, namely BLEU (Papineni
et al, 2002) with maximum n-gram lengths of 1, 2,
3, and 4, NIST (NIST, 2002) with maximum n-gram
lengths of 1, 2, 3, 4, and 5, GTM (Turian et al, 2003)
with exponents of 1.0, 2.0, and 3.0, METEOR (ex-
act) (Banerjee and Lavie, 2005), WER (Niessen et
al., 2000), PER (Leusch et al, 2003), and ROUGE
(Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and
4 variants (LCS, S?, SU?, W-1.2), were used to cal-
culate each similarity S
i
. Therefore, the value of m
in Eq. (1) was 23. Japanese word segmentation was
performed by using JUMAN 4 in our experiments.
As you can see, the definition of our new measure
is based on a combination of an evaluation measure
focusing on local information and that focusing on
global information.
3.2 Automatic Estimation of Rate of
Accomplishment of Sub-goals
The rate of accomplishment of sub-goals is esti-
mated by determining the answer to each question
as yes or no. This section describes a method based
on simple patterns for determining the answers.
An answer to each question is automatically de-
termined by checking whether patterns are included
in a translation or not. The patterns are constructed
for each question. All of the patterns are expressed
in hiragana characters. Before applying the pat-
terns to a given translation, the translation is trans-
formed into hiragana characters, and all punctuation
is eliminated. The transformation to hiragana char-
acters was performed by using JUMAN in our ex-
periments.
Test-set sentences, the questions assigned to
them, and the patterns constructed for the questions
are shown in Table 4. In the patterns, the symbol ?|?
represents ?OR?.
3.3 Automatic Sub-goal Generation and
Automatic Estimation of Rate of
Accomplishment of Sub-goals
We found that expressions important for maintain-
ing a high translation quality were often commonly
4http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html
37
included in the reference translations for each test-
set sentence. We also found that the expression was
also related to the yes/no question assigned to the
test-set sentence. Therefore, we automatically gen-
erate yes/no questions in the following steps.
1. For each test-set sentence, a set of words com-
monly appearing in the reference translations
are extracted.
2. For each combination of n words in the set
of words extracted in the first step, skip word
n-grams commonly appearing in the reference
translations in the same word order are selected
as a set of common skip word n-grams.
3. For each test-set sentence, the sub-goal is de-
fined as the yes/no question ?Are all of the com-
mon skip word n-grams included in the transla-
tion??
If no common skip word n-grams are found, the
yes/no question is not generated. The answer to the
yes/no question is determined to be yes if all of the
common skip word n-grams are included in a trans-
lation. Otherwise, the answer is determined to be
no.
This scheme assigns greater weight to important
phrases that should be included in the translation to
maintain a high translation quality. Our observation
is that those important phrases are often common
between human translations. A similar scheme was
proposed by Babych and Hartley (Babych and Hart-
ley, 2004) for BLEU. In their scheme, greater weight
is assigned to components that are salient through-
out the document. Therefore, their scheme focuses
on global context while our scheme focuses on local
context. We believe that the two schemes are com-
plementary to each other.
4 Experiments and Discussion
In our experiments, the translation results of three
MT systems and their subjective evaluation results
were used as a development set for constructing the
patterns described in Section 3.2 and for tuning the
parameters ?
S
i
, ?
Q
j
, ?
Q
?
j
, and ?

in Eq. (1). The
translations and evaluation results of the remaining
two MT systems were used as an evaluation set for
testing.
In the development set, each test-set sentence has
at least one question, at least one reference transla-
tion, three MT results, and subjective evaluation re-
sults of the three MT results. The patterns for deter-
mining yes/no answers were manually constructed
for the questions assigned to the 769 test-set sen-
tences. There were 917 questions assigned to them.
Among them, the patterns could be constructed for
898 questions assigned to 767 test-set sentences.
The remaining 19 questions were skipped because
making simple patterns as described in Section 3.2
was difficult; for example, one of the questions
was ?Is the whole sentence translated into one sen-
tence??. The yes/no answer determination accura-
cies obtained by using the patterns are shown in Ta-
ble 5.
Table 5: Results of yes/no answer determination.
Test set Accuracy
Development 97.6% (2,629/2,694)
Evaluation 82.8% (1,487/1,796)
We investigated the correlation between the eval-
uation score, A in Eq. (1) and the subjective eval-
uations, fluency and adequacy, for the 769 test-set
sentences. First, to maximize the correlation coeffi-
cients between the evaluation score, A, and the hu-
man subjective evaluations, fluency and adequacy,
the optimal values of ?
S
i
, ?
Q
j
, ?
Q
?
j
, and ?

in
Eq. (1) were investigated using the development
set within a framework of multiple linear regression
modeling (Draper and Smith, 1981). Then, the cor-
relation coefficients were investigated by using the
optimal value set. The results are shown in Table 6,
7, and 8. In these tables, ?Conventional method? in-
dicates the correlation coefficients obtained when A
was calculated by using only similarities S
i
. ?Con-
ventional method (combination)? is a combination
of existing automatic evaluation methods from the
literature. ?Our method (automatic)? indicates the
correlation coefficients obtained when the results of
the automatic determination of yes/no answers were
used to calculate Q
j
and Q?
j
in Eq. (1). For the 19
questions for which the patterns could not be con-
structed, Q
j
was set at 0. ?Our method (full au-
tomatic)? indicates the correlation coefficients ob-
tained when the results of the automatic sub-goal
generation and determination of rate of accomplish-
38
Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy. (A reference transla-
tion is used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.43 0.48 0.42 0.48
Conventional method (combination) 0.52 0.51 0.49 0.47
Our method (automatic) 0.90? 0.59? 0.89? 0.62?
Our method (upper bound) 0.90? 0.62? 0.90? 0.68?
Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy. (Three reference
translations are used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.47 0.51 0.45 0.51
Conventional method (combination) 0.54 0.54 0.51 0.52
Our method (automatic) 0.90? 0.60? 0.90? 0.64?
Our method (full automatic) 0.85? 0.58 0.84? 0.60?
Our method (upper bound) 0.90? 0.62? 0.90? 0.69?
Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy. (Five reference
translations are used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.49 0.53 0.46 0.53
Conventional method (combination) 0.56 0.56 0.52 0.54
Our method (automatic) 0.90? 0.60 0.90? 0.63?
Our method (full automatic) 0.86? 0.59 0.85? 0.60?
Our method (upper bound) 0.91? 0.63? 0.90? 0.69?
In these tables, ? indicates significance at the 5% or less significance level.
ment of sub-goals were used to calculate Q
j
and Q?
j
in Eq. (1). Skip word trigrams, skip word bigrams,
and skip word unigrams were used for generating
the sub-goals according to our preliminary experi-
ments. ?Our method (upper bound)? indicates the
correlation coefficients obtained when human judg-
ments on the questions were used to calculate Q
j
and Q?
j
.
As shown in Table 6, 7, and 8, our methods signif-
icantly outperform the conventional methods from
literature. Note that WER outperformed other indi-
vidual measures like BLEU and NIST in our exper-
iments, and the combination of existing automatic
evaluation methods from the literature outperformed
individual lexical similarity measures by themselves
in almost all cases. The differences between the
correlation coefficients obtained using our method
and the conventional methods are statistically sig-
nificant at the 5% or less significance level for flu-
ency and adequacy, even if the number of reference
translations increases, except in three cases shown
in Table 7 and 8. This indicates that considering
the rate of accomplishment of sub-goals to automat-
ically evaluate the quality of each translation is use-
ful, especially when the number of reference trans-
lations is small.
The differences between the correlation coeffi-
cients obtained using two automatic methods are not
significant. These results indicate that we can reduce
the development cost for constructing sub-goals.
However, there are still significant gaps between the
correlation coefficients obtained using a fully auto-
matic method and upper bounds. These gaps indi-
cate that we need further improvement in automatic
sub-goal generation and automatic estimation of rate
of accomplishment of sub-goals, which is our future
work.
Human judgments of adequacy and fluency are
known to be noisy, with varying levels of intercoder
agreement. Recent work has tended to apply cross-
judge normalization to address this issue (Blatz et
al., 2003). We would like to evaluate against the
normalized data in the future.
39
5 Conclusion and Future Work
We demonstrated that the quality of a translated sen-
tence can be evaluated more appropriately than by
using conventional methods. That was demonstrated
by constructing a test set where the conditions that
should be satisfied to maintain a high translation
quality are assigned to each test-set sentence in the
form of a question, by developing a system that de-
termines an answer to each question, and by com-
bining a measure based on the questions and con-
ventional measures. We also presented a method for
automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals. Promising results were
obtained.
In the near future, we would like to expand the
test set to improve the upper bound obtained by
our method. We are also planning to expand the
method and improve the accuracy of the automatic
sub-goal generation and determination of the rate of
accomplishment of sub-goals. The sub-goals of a
given sentence should be generated by considering
the complexity of the sentence and the alignment in-
formation between the original source-language sen-
tence and its translation. Further advanced genera-
tion and estimation would give us information about
the erroneous parts of MT results and their quality.
We believe that future research would allow us to
develop high-quality MT systems by tuning the sys-
tem parameters based on the automatic MT evalua-
tion measures.
Acknowledgments
The guideline for expanding the test set is based on that con-
structed by the Technical Research Committee of the AAMT
(Asia-Pacific Association for Machine Translation) The authors
would like to thank the committee members, especially, Mr.
Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro,
Mr. Masaru Fuji, and Ms. Yoshiko Matsukawa for their coop-
eration. This research is partially supported by special coordi-
nation funds for promoting science and technology.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001.
Using Multiple Edit Distances to Automatically Rank Ma-
chine Translation Output. In Proceedings of the MT Summit
VIII, pages 15?20.
Bogdan Babych and Anthony Hartley. 2004. Extending the
BLEU MT Evaluation Method with Frequency Weightings.
In Proceedings of the 42nd ACL, pages 622?629.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An au-
tomatic metric for mt evaluation with improved correlation
with human judgments. In Proceedings of Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT and/or
Summarization, pages 65?72.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence Estimation for Machine Trans-
lation. Technical report, Center for Language and Speech
Processing, Johns Hopkins University. Summer Workshop
Final Report.
Norman R. Draper and Harry Smith. 1981. Applied Regression
Analysis. 2nd edition. Wiley.
Jesus? Gimen?ez, Enrique Amigo
?
, and Chiori Hori. 2005. Ma-
chine translation evaluation inside qarla. In Proceedings of
the IWSLT?05.
Satoru Ikehara, Satoshi Shirai, and Kentaro Ogura. 1994. Cri-
teria for Evaluating the Linguistic Quality of Japanese to
English Machine Translations. Transactions of the JSAI,
9(4):569?579. (in Japanese).
Hitoshi Isahara. 1995. JEIDA?s Test-Sets for Quality Evalua-
tion of MT Systems ? Technical Evaluation from the Devel-
oper?s Point of View.
Philipp Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In Proceedings of the 2004
Conference on EMNLP, pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A
Novel String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Proceedings of the
MT Summit IX, pages 240?247.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th COLING,
pages 501?507.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out, pages 74?81.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and Hermann
Ney. 2000. An Evaluation Tool for Machine Translation:
Fast Evaluation for MT Research. In Proceedings of the
LREC 2000, pages 39?45.
NIST. 2002. Automatic Evaluation of Machine Translation
Quality Using N-gram Co-Occurrence Statistics. Technical
report, NIST.
Franz Josef Och. 2003. Minimum Error Training in Statistical
Machine Translation. In Proceedings of the 41st ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu.
2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th ACL, pages
311?318.
TIDES. 2002. Linguistic Data Annotation Specifi-
cation: Assessment of Fluency and Adequacy in
Arabic-English and Chinese-English Translations.
http://www.ldc.upenn.edu/Projects/TIDES/Translation/
TransAssess02.pdf.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of Machine Translation and its Evaluation. In Pro-
ceedings of the MT Summit IX, pages 386?393.
40
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 324?330,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Detection of Quotations and Inserted Clauses and its Application
to Dependency Structure Analysis in Spontaneous Japanese
Ryoji Hamabe  Kiyotaka Uchimoto
 School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
Tatsuya Kawahara  Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
Abstract
Japanese dependency structure is usu-
ally represented by relationships between
phrasal units called bunsetsus. One of the
biggest problems with dependency struc-
ture analysis in spontaneous speech is that
clause boundaries are ambiguous. This
paper describes a method for detecting
the boundaries of quotations and inserted
clauses and that for improving the depen-
dency accuracy by applying the detected
boundaries to dependency structure anal-
ysis. The quotations and inserted clauses
are determined by using an SVM-based
text chunking method that considers in-
formation on morphemes, pauses, fillers,
etc. The information on automatically an-
alyzed dependency structure is also used
to detect the beginning of the clauses.
Our evaluation experiment using Corpus
of Spontaneous Japanese (CSJ) showed
that the automatically estimated bound-
aries of quotations and inserted clauses
helped to improve the accuracy of depen-
dency structure analysis.
1 Introduction
The ?Spontaneous Speech: Corpus and Pro-
cessing Technology? project sponsored the con-
struction of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). The CSJ is the
biggest spontaneous speech corpus in the world,
consisting of roughly 7M words with the total
speech length of 700 hours, and is a collection of
monologues such as academic presentations and
simulated public speeches. The CSJ includes tran-
scriptions of the speeches as well as audio record-
ings of them. Approximately one tenth of the
speeches in the CSJ were manually annotated with
various kinds of information such as morphemes,
sentence boundaries, dependency structures, and
discourse structures.
In Japanese sentences, word order is rather
free, and subjects or objects are often omitted.
In Japanese, therefore, the syntactic structure of
a sentence is generally represented by the re-
lationships between phrasal units, or bunsetsus
in Japanese, based on a dependency grammar,
as represented in the Kyoto University text cor-
pus (Kurohashi and Nagao, 1997). In the same
way, the syntactic structure of a sentence is repre-
sented by dependency relationships between bun-
setsus in the CSJ. For example, the sentence ???
?????????? (He is walking slowly) can
be divided into three bunsetsus, ???, kare-wa?
(he), ?????, yukkuri? (slowly), and ????
??, arui-te-iru? (is walking). In this sentence,
the first and second bunsetsus depend on the third
one. The dependency structure is described as fol-
lows.
??????????? (he)
???????????????????? (slowly)
??????????? (is walking)
In this paper, we first describe the problems
with dependency structure analysis of spontaneous
speech. We focus on ambiguous clause boundaries
as the biggest problem and present a solution.
2 Problems with Dependency Structure
Analysis in Spontaneous Japanese
There are many differences between written text
and spontaneous speech, and consequently, prob-
lems peculiar to spontaneous speech arise in de-
324
pendency structure analysis, such as ambiguous
clause boundaries, independent bunsetsus, crossed
dependencies, self-corrections, and inversions. In
this study, we address the problem of ambiguous
clause boundaries in dependency structure analy-
sis in spontaneous speech. We treated the other
problems in the same way as Shitaoka et al (Shi-
taoka et al, 2004). For example, inversions are
represented as dependency relationships going in
the direction from right to left in the CSJ, and their
direction was changed to that from left to right in
our experiments. In this paper, therefore, all the
dependency relationships were assumed to go in
the direction from left to right (Uchimoto et al,
2006).
There are several types of clause boundaries
such as sentence boundaries, boundaries of quo-
tations and inserted clauses. In the CSJ, clause
boundaries were automatically detected by using
surface information (Maruyama et al, 2003), and
sentence boundaries were manually selected from
them (Takanashi et al, 2003). Boundaries of
quotations and inserted clauses were also defined
and detected manually. Dependency relationships
between bunsetsus were annotated within sen-
tences (Uchimoto et al, 2006). Our definition of
clause boundaries follows the definition used in
the CSJ.
Shitaoka et al worked on automatic sen-
tence boundary detection by using SVM-based
text chunking. However, quotations and inserted
clauses were not considered. In this paper, we fo-
cus on these problems in a context of ambiguous
clause boundaries.
Quotations
In written text, quotations are often bracketed by
??(angle brackets), but no brackets are inserted in
spontaneous speech.
ex) ???????????????? (I want to
go there at any rate) is a quotation. In the CSJ,
quotations were manually annotated as follows.
???????????????? (here)
???????????????? (since early times)
???????????????? (once)
???????????????? (at any rate)
???????????????? (want to go)
???????????????? (is the place I think)
Inserted Clauses
In spontaneous speech, speakers insert clauses in
the middle of other clauses. This occurs when
speakers change their speech plans while produc-
Detection of
Sentence Boundary
Dependency Structure
Analysis (Baseline)
Detection of Quotations
and Inserted Clauses
Dependency Structure
Analysis (Enhanced)
word information
filler existence
pause duration
speaking rate
information of dependencies
word information
distance between bunsetsus
(A) + boundaries of quotations
and inserted clauses
...(A)
Figure 1: Outline of proposed processes
ing utterances, which results in supplements, an-
notations, or paraphrases of main clauses.
ex) ???????????? (where I arrived at
night) is an inserted clause.
????????????????? (hotel)
????????????????? (room)
????????????????? (inside)
????????????????? (without delay)
????????????????? (at night)
????????????????? (arrived)
????????????????? (I checked)
Dependency relationships are closed within a
quotation or an inserted clause. Therefore, de-
pendencies except the rightmost bunsetsu in each
clause do not cross boundaries of the same clause,
meaning no dependency exists between the bun-
setsu inside a clause and that outside the clause.
However, automatically detected dependencies of-
ten cross clause boundaries erroneously because
sentences including quotations or inserted clauses
can have complicated clause structures. This is
one of the reasons dependency structure analysis
of spontaneous speech has more errors than that of
written texts. We propose a method for improving
dependency structure analysis based on automatic
detection of quotations and inserted clauses.
3 Dependency Structure Analysis and
Detection of Quotations and Inserted
Clauses
The outline of the proposed processes is shown in
Figure 1. Here, we use ?clause? to describe a quo-
tation and an inserted clause.
3.1 Dependency Structure Analysis
In this research, we use the method proposed by
Uchimoto et al (Uchimoto et al, 2000) to ana-
325
lyze dependency structures. This method is a two-
step procedure, and the first step is preparation of
a dependency matrix in which each element repre-
sents the likelihood that one bunsetsu depends on
another. The second step of the analysis is find-
ing an optimal set of dependencies for the entire
sentence. The likelihood of dependency is repre-
sented by a probability, using a dependency proba-
bility model. The model in this study (Uchimoto et
al., 2000) takes into account not only the relation-
ship between two bunsetsus but also the relation-
ship between the left bunsetsu and all the bunsetsu
to its right.
We implemented this model within a maximum
entropy modeling framework. The features used
in the model were basically attributes related to
the target two bunsetsus: attributes of a bunsetsu
itself, such as character strings, parts of speech,
and inflection types of a bunsetsu together with at-
tributes between bunsetsus, such as the distance
between bunsetsus, etc. Combinations of these
features were also used. In this work, we added
to the features whether there is a boundary of quo-
tations or inserted clauses between the target bun-
setsus. If there is, the probability that the left bun-
setsu depends on the right bunsetsu is estimated to
be low.
In the CSJ, some bunsetsus are defined to have
no modifiee. In our experiments, we defined their
dependencies as follows.
  The rightmost bunsetsu in a quotation or an
inserted clause depends on the rightmost one
in the sentence.
  If a sentence boundary is included in a quo-
tation or an inserted clause, the bunsetsu to
the immediate left of the boundary depends
on the rightmost bunsetsu in the quotation or
the inserted clause.
  Other bunsetsus that have no modifiee de-
pend on the next one.
3.2 Detection of Quotations and Inserted
Clauses
We regard the problem of clause boundary de-
tection as a text chunking task. We used Yam-
Cha (Kudo and Matsumoto, 2001) as a text chun-
ker, which is based on Support Vector Machine
(SVM). We used the chunk labels consisting of
three tags which correspond to sentence bound-
aries, boundaries of quotations, and boundaries of
inserted clauses, respectively. The tag for sentence
Table 1: Tag categories used for chunking
Tag Explanation of tag
B Beginning of a clause
E End of a clause
I Interior of a clause (except B and E)
O Exterior of a clause
S Clause consisting of one bunsetsu
boundaries can be either E (the rightmost bunsetsu
in a sentence) or I (the others). The tags for the
boundaries of quotations and inserted clauses are
shown in Table 1. An example of chunk labels as-
signed to each bunsetsu in a sentence is as follows.
ex) ???????? (It is because of the budget)
is a quotation, and ??????????????
(which I think is because of the budget) is an in-
serted clause. For a chunk label, for example, the
bunsetsu that the chunk label (I, B, B) is assigned
to means that it is not related to a sentence bound-
ary but is related to the beginning of a quotation
and an inserted clause.
(I,O,O)??????????????? (now)
(I,B,B)?????? ? (budget)
(I,E,I)??????????????? (because of)
(I,O,E)??????????????? (I think)
(I,O,O)??????????????? (in summer)
(I,O,O)??????????????? (three times)
(E,O,O)??????????????? (they do it)
The three tags of each chunk label are simulta-
neously estimated. Therefore, the relationships
between sentence boundaries, quotations, and in-
serted clauses are considered in this model. For in-
stance, quotations and inserted clauses should not
cross the sentence boundaries, and the chunk label
such as (E,I,O) is never estimated because this la-
bel means that a sentence boundary exists within a
quotation.
We used the following parameters for YamCha.
  Degree of polynomial kernel: 3rd
  Analysis direction: Right to left
  Dynamic features: Following three chunk la-
bels
  Multi-class method: Pairwise
The chunk label is estimated for each bunsetsu,
The features used to estimate the chunk labels are
as follows.
(1) word information We used word information
such as character strings, pronunciation, part
of speech, inflection type, and inflection
form. Specific expressions are often used at
the ends of quotations and inserted clauses.
326
B?
E
(1) No bunsetsu to left of B
depends on bunsetsu between B and E
?
B
?
E
(2) Bunsetsu to immediate left of B
depends on bunsetsu to right of E
?
?
Figure 2: Dependency structures of bunsetsus to
left of beginning of quotations or inserted clauses
For instance, ????, to-omou? (think) and
?????, tte-iu? (say) are used at the ends
of quotations. Expressions such as ???
?, desu-ga? and ?????, keredo-mo? are
used at the ends of inserted clauses.
(2) fillers and pauses Fillers and pauses are often
inserted just before or after quotations and in-
serted clauses. Pause duration is normalized
in a talk with its mean and variance.
(3) speaking rate Inside inserted clauses, speak-
ers tend to speak fast. The speaking rate is
also normalized in a talk.
Detecting the ends of clauses appears easy be-
cause specific expressions are frequently used at
the ends of clauses as previously mentioned. How-
ever, determining the beginnings of clauses is dif-
ficult in a single process because all features men-
tioned above are local information. Therefore, the
global information is also used to detect the begin-
ning of the clauses. If the end of a clause is given,
the bunsetsus to the left of the clause should sat-
isfy the two conditions described in Figure 2. Our
method uses the constraint as global information.
They are considered as additional features based
on dependency probabilities estimated for the bun-
setsus to the left of the clause. Thus, our chunk-
ing method has two steps. First, clause boundaries
are detected based on the three types of features
itemized above. Second, the beginnings of clauses
are determined after adding to the features the fol-
lowing probabilities obtained by automatic depen-
dency structure analysis.
(4) probability that bunsetsu to left of target de-
pends on bunsetsu inside clause
(5) probability that bunsetsu to immediate left
of target depends on bunsetsu to right of clause
Figure 2 shows that the target bunsetsu is likely
to be the beginning of the clause if probability (4)
is low and probability (5) is high. For instance,
the following example sentence has an inserted
clause. In the first chunking step, the bunsetsu
????????? (which is a story) is found to
be the end of the inserted clause.
ex) ??????????????? (which is a
story that I heard from my father) is an inserted
clause.
????????????????? (this)
????????????????? (area)
????????????????? (from my father)
????????????????? (heard)
????????????????? (story)
????????????????? (in the old days)
????????????????? (was a rice field)
The three bunsetsus ????, atari-wa?, ???
?, kii-ta?, and ????????, hanashi-na-
ndesu-kedo? are less likely to be the beginning
of the inserted clause because in the three cases
the bunsetsu to the immediate left depends on the
target bunsetsu. On the other hand, the bunsetsu
????, chichi-kara? is the most likely to be the
beginning since the bunsetsu to its immediate left
????, atari-wa? depends on the bunsetsu to the
right of the inserted clause ??????????,
tanbo-datta-ndesu?.
4 Experiments and Discussion
For experimental evaluation, we used the tran-
scriptions of 188 talks in the CSJ, which contain
6,255 quotations and 818 inserted clauses. We
used 20 talks for testing. The test data included
643 quotations and 76 inserted clauses. For train-
ing, we used 168 talks excluding the test data to
conduct the open test and all the 188 talks to con-
duct the closed test.
First, we detected sentence boundaries by using
the method (Shitaoka et al, 2004) and analyzed
the dependency structure of each sentence by the
method described in Section 3.1 without using in-
formation on quotations and inserted clauses. We
obtained an F-measure of 85.9 for the sentence
boundary detection, and the baseline accuracy of
the dependency structure analysis was 77.7% for
the open test and 86.5% for the closed test.
327
(a) Results of clause boundary detection
The results obtained by the method described in
Section 3.2 are shown in Table 2. The table shows
five kinds of results:
  results obtained without dependency struc-
ture (in the first chunking step)
  results obtained with dependency structure
analyzed for the open test (in the second
chunking step)
  results obtained with dependency structure
analyzed for the closed test (in the second
chunking step)
  results obtained with manually annotated de-
pendency structure (in the second chunking
step)
  the rate that the ends of clauses are detected
correctly
These results indicate that around 90% of quo-
tations were detected correctly, and the boundary
detection accuracy of quotations was improved by
using automatically analyzed dependency struc-
ture. We found that features (4) and (5) in Section
3.2 obtained from automatically analyzed depen-
dency structure contributed to the improvement.
In the following example, a part of the quotation
?????????????? (my good virtue)
was erroneously detected as a quotation in the first
chunking step. But, in the second chunking step,
automatically analyzed dependency structure con-
tributed to detection of the correct part ?????
???????????? (this is my good virtue)
as a quotation.
????????????????? (this)
????????????????? (my)
????????????????? (good)
????????????????? (virtue)
????????????????? (I)
????????????????? (think)
We also found that the boundary detection accu-
racy of quotations was significantly improved by
using manually annotated dependency structure.
This indicates that the boundary detection accu-
racy of quotations improves as the accuracy of de-
pendency structure analysis improves.
By contrast, only a few inserted clauses were
detected even if dependency structures were used.
Most of the ends of the inserted clauses were de-
tected incorrectly as sentence boundaries. The
main reason for this is our method could not distin-
guish between the ends of the inserted clauses and
those of the sentences, since the same words often
appeared at the ends of both, and it was difficult
Table 2: Clause boundary detection results (sen-
tence boundaries automatically detected)
Quotations Inserted clauses
recall precision F recall precision F
Without dependency information
41.1% 44.3% 42.6 1.3% 20.0% 2.5
(264/643) (264/596) (1/76) (1/5)
With dependency information (open)
42.1% 45.5% 43.7 2.6% 40.0% 4.9
(271/643) (271/596) (2/76) (2/5)
With dependency information (closed)
50.9% 54.9% 52.8 2.6% 40.0% 4.9
(327/643) (327/596) (2/76) (2/5)
With dependency information (correct)
74.2% 80.0% 77.0 2.6% 33.3% 4.9
(477/643) (477/596) (2/76) (2/6)
Correct end of clauses
89.1% 96.1% 92.5 2.6% 40.0% 4.9
(573/643) (573/596) (2/76) (2/5)
Table 3: Dependency structure analysis results ob-
tained with clause boundaries (sentence bound-
aries automatically detected)
Without boundaries of quotations open 77.7%
and inserted clauses closed 86.5%
With boundaries of quotations and open 78.5%
inserted clauses (automatically detected) closed 86.6%
With boundaries of quotations and open 79.4%
inserted clauses (correct) closed 87.4%
to learn the difference between them even though
our method used features based on acoustic infor-
mation.
(b) Dependency structure analysis results
We investigated the accuracies of dependency
structure analysis obtained when the automatically
or manually detected boundaries of quotations and
inserted clauses were used. The results are shown
in Table 3. Although the accuracy of detecting the
boundaries of quotations and inserted clauses us-
ing automatically analyzed dependency structure
was not high, the accuracy of dependency struc-
ture analysis was improved by 0.7% absolute for
the open test. This shows that the model for depen-
dency structure analysis could robustly learn use-
ful information on clause boundaries even if errors
were included in the results of clause boundary de-
tection. In the following example, for instance,
????????????? (to go out with its
face stuck) was correctly detected as a quotation
in the first chunking step. Then, the initial in-
appropriate modifiee ??????, oboe-te-ki-te?
(learn) of the bunsetsu inside the quotation ???
?, hasan-de? (stick) was correctly modified to the
bunsetsu inside the quotation ?????????,
de-te-shimau-to-iu? (to go) by using the automati-
cally detected boundary of the quotation.
328
??????????????????? (face)
??????????????????? (stick)
??????????????????? (out)
??????????????????? (to go)
??????????????????? (stunt)
??????????????????? (somewhere)
??????????????????? (learn)
(c) Results obtained when correct sentence
boundaries are given
We investigated the clause boundary detection
accuracy of quotations and inserted clauses and
the dependency accuracy when correct sentence
boundaries were given manually. The results are
shown in Tables 4 and 5, respectively.
When correct sentence boundaries were given,
the accuracy of clause detection and dependency
structure analysis was improved significantly. Ta-
ble 4 shows that the boundary detection accuracy
of inserted clauses as well as that of quotations
was significantly improved by using information
of dependencies. Table 5 indicates that when us-
ing automatically detected clause boundaries, the
accuracy of dependency structure analysis was im-
proved by 0.7% for the open test, and it was further
improved by using correct clause boundaries.
These experimental results show that detecting
the boundaries of quotations and inserted clauses
as well as sentence boundaries is sensitive to the
accuracy of dependency structure analysis and the
improvements of the boundary detection of quo-
tations and inserted clauses contribute to improve-
ment of dependency structure analysis. Especially,
the difference between Table3 and 5 shows that
the sentence boundary detection accuracy is more
sensitive to the accuracy of dependency structure
analysis than the boundary detection accuracy of
quotations and inserted clauses. This indicates that
sentence boundaries rather than quotations and in-
serted clauses should be manually examined first
to effectively improve the accuracy of dependency
structure analysis in a semi-automatic way.
5 Conclusion
This paper described the method for detecting the
boundaries of quotations and inserted clauses and
that for applying it to dependency structure analy-
sis. The experiment results showed that the auto-
matically estimated boundaries of quotations and
inserted clauses contributed to improvement of de-
pendency structure analysis. In the future, we plan
to solve the problems found in the experiments and
investigate the robustness of our method when the
Table 4: Clause boundary detection results (sen-
tence boundaries given)
Quotations Inserted clauses
recall precision F recall precision F
Without dependency information
46.0% 50.8% 48.3 22.4% 23.6% 23.0
(296/643) (296/583) (17/76) (17/72)
With dependency information (open)
46.7% 53.3% 49.8 30.3% 38.3% 33.8
(300/643) (300/563) (23/76) (23/60)
With dependency information (closed)
55.1% 62.9% 58.7 30.3% 39.0% 34.1
(354/643) (354/563) (23/76) (23/59)
With dependency information (correct)
75.3% 86.0% 80.3 46.1% 60.3% 52.2
(484/643) (484/563) (35/76) (35/58)
Correct end of clauses
86.5% 95.4% 90.7 64.5% 68.1% 66.2
(556/643) (556/583) (49/76) (49/72)
Table 5: Dependency structure analysis results ob-
tained with clause boundaries (sentence bound-
aries given)
Without boundaries of quotations open 81.0%
and inserted clauses closed 90.3%
With boundaries of quotations and open 81.7%
inserted clauses (automatically detected) closed 90.3%
With boundaries of quotations open 82.8%
and inserted clauses (correct) closed 91.3%
results of automatic speech recognition are given
as the inputs. We will also study use of informa-
tion on quotations and inserted clauses to text for-
matting, such as text summarization.
References
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL.
Sadao Kurohashi and Makoto Nagao. 1997. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. In Proceedings of the NLPRS, pages
451?456.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous Speech Corpus of
Japanese. In Proceedings of the LREC2000, pages
947?952.
Takehiko Maruyama, Hideki Kashioka, Tadashi Ku-
mano, and Hideki Tanaka. 2003. Rules for Auto-
matic Clause Boundary Detection and Their Evalu-
ation. In Proceedings of the Nineth Annual Meeting
of the Association for Natural Language proceeding,
pages 517?520. (in Japanese).
Katsuya Takanashi, Takehiko Maruyama, Kiy-
otaka Uchimoto, and Hitoshi Isahara. 2003.
Identification of ?Sentences? in Spontaneous
329
Japanese ? Detection and Modification of Clause
Boundaries ?. In Proceedings of the ISCA & IEEE
Workshop on Spontaneous Speech Processing and
Recognition, pages 183?186.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency Model Us-
ing Posterior Context. In Proceedings of the IWPT,
pages 321?322.
Kiyotaka Uchimoto, Ryoji Hamabe, Take-
hiko Maruyama, Katsuya Takanashi, Tatsuya Kawa-
hara, and Hitoshi Isahara. 2006. Dependency-
structure Annotation to Corpus of Spontaneous
Japanese. In Proceedings of the LREC2006, pages
635-638.
Kazuya Shitaoka, Kiyotaka Uchimoto, Tatsuya Kawa-
hara, and Hitoshi Isahara. 2004. Dependency Struc-
ture Analysis and Sentence Boundary Detection in
Spontaneous Japanese. In Proceedings of the COL-
ING2004, pages 1107?1113.
330
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 205?208,
Prague, June 2007. c?2007 Association for Computational Linguistics
Minimally Lexicalized Dependency Parsing
Daisuke Kawahara and Kiyotaka Uchimoto
National Institute of Information and Communications Technology,
3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan
{dk, uchimoto}@nict.go.jp
Abstract
Dependency structures do not have the infor-
mation of phrase categories in phrase struc-
ture grammar. Thus, dependency parsing
relies heavily on the lexical information of
words. This paper discusses our investiga-
tion into the effectiveness of lexicalization
in dependency parsing. Specifically, by re-
stricting the degree of lexicalization in the
training phase of a parser, we examine the
change in the accuracy of dependency re-
lations. Experimental results indicate that
minimal or low lexicalization is sufficient
for parsing accuracy.
1 Introduction
In recent years, many accurate phrase-structure
parsers have been developed (e.g., (Collins, 1999;
Charniak, 2000)). Since one of the characteristics of
these parsers is the use of lexical information in the
tagged corpus, they are called ?lexicalized parsers?.
Unlexicalized parsers, on the other hand, achieved
accuracies almost equivalent to those of lexicalized
parsers (Klein and Manning, 2003; Matsuzaki et al,
2005; Petrov et al, 2006). Accordingly, we can
say that the state-of-the-art lexicalized parsers are
mainly based on unlexical (grammatical) informa-
tion due to the sparse data problem. Bikel also in-
dicated that Collins? parser can use bilexical depen-
dencies only 1.49% of the time; the rest of the time,
it backs off to condition one word on just phrasal and
part-of-speech categories (Bikel, 2004).
This paper describes our investigation into the ef-
fectiveness of lexicalization in dependency parsing
instead of phrase-structure parsing. Usual depen-
dency parsing cannot utilize phrase categories, and
thus relies on word information like parts of speech
and lexicalized words. Therefore, we want to know
the performance of dependency parsers that have
minimal or low lexicalization.
Dependency trees have been used in a variety of
NLP applications, such as relation extraction (Cu-
lotta and Sorensen, 2004) and machine translation
(Ding and Palmer, 2005). For such applications, a
fast, efficient and accurate dependency parser is re-
quired to obtain dependency trees from a large cor-
pus. From this point of view, minimally lexicalized
parsers have advantages over fully lexicalized ones
in parsing speed and memory consumption.
We examined the change in performance of de-
pendency parsing by varying the degree of lexical-
ization. The degree of lexicalization is specified by
giving a list of words to be lexicalized, which appear
in a training corpus. For minimal lexicalization, we
used a short list that consists of only high-frequency
words, and for maximal lexicalization, the whole list
was used. Consequently, minimally or low lexical-
ization is sufficient for dependency accuracy.
2 Related Work
Klein and Manning presented an unlexicalized
PCFG parser that eliminated all the lexicalized pa-
rameters (Klein and Manning, 2003). They manu-
ally split category tags from a linguistic view. This
corresponds to determining the degree of lexicaliza-
tion by hand. Their parser achieved an F1 of 85.7%
for section 23 of the Penn Treebank. Matsuzaki et al
and Petrov et al proposed an automatic approach to
205
Dependency accuracy (DA) Proportions of words, except
punctuation marks, that are assigned the correct heads.
Root accuracy (RA) Proportions of root words that are cor-
rectly detected.
Complete rate (CR) Proportions of sentences whose depen-
dency structures are completely correct.
Table 1: Evaluation criteria.
splitting tags (Matsuzaki et al, 2005; Petrov et al,
2006). In particular, Petrov et al reported an F1 of
90.2%, which is equivalent to that of state-of-the-art
lexicalized parsers.
Dependency parsing has been actively studied in
recent years (Yamada and Matsumoto, 2003; Nivre
and Scholz, 2004; Isozaki et al, 2004; McDon-
ald et al, 2005; McDonald and Pereira, 2006;
Corston-Oliver et al, 2006). For instance, Nivre
and Scholz presented a deterministic dependency
parser trained by memory-based learning (Nivre and
Scholz, 2004). McDonald et al proposed an on-
line large-margin method for training dependency
parsers (McDonald et al, 2005). All of them per-
formed experiments using section 23 of the Penn
Treebank. Table 2 summarizes their dependency ac-
curacies based on three evaluation criteria shown in
Table 1. These parsers believed in the generalization
ability of machine learners and did not pay attention
to the issue of lexicalization.
3 Minimally Lexicalized Dependency
Parsing
We present a simple method for changing the de-
gree of lexicalization in dependency parsing. This
method restricts the use of lexicalized words, so it is
the opposite to tag splitting in phrase-structure pars-
ing. In the remainder of this section, we first de-
scribe a base dependency parser and then report ex-
perimental results.
3.1 Base Dependency Parser
We built a parser based on the deterministic algo-
rithm of Nivre and Scholz (Nivre and Scholz, 2004)
as a base dependency parser. We adopted this algo-
rithm because of its linear-time complexity.
In the algorithm, parsing states are represented by
triples ?S, I,A?, where S is the stack that keeps the
words being under consideration, I is the list of re-
DA RA CR
(Yamada and Matsumoto, 2003) 90.3 91.6 38.4
(Nivre and Scholz, 2004) 87.3 84.3 30.4
(Isozaki et al, 2004) 91.2 95.7 40.7
(McDonald et al, 2005) 90.9 94.2 37.5
(McDonald and Pereira, 2006) 91.5 N/A 42.1
(Corston-Oliver et al, 2006) 90.8 93.7 37.6
Our Base Parser 90.9 92.6 39.2
Table 2: Comparison of parser performance.
maining input words, and A is the list of determined
dependencies. Given an input word sequence, W ,
the parser is first initialized to the triple ?nil,W, ??1.
The parser estimates a dependency relation between
two words (the top elements of stacks S and I). The
algorithm iterates until the list I is empty. There are
four possible operations for a parsing state (where t
is the word on top of S, n is the next input word in
I , and w is any word):
Left In a state ?t|S, n|I,A?, if there is no depen-
dency relation (t ? w) in A, add the new de-
pendency relation (t ? n) into A and pop S
(remove t), giving the state ?S, n|I,A ? (t ?
n)?.
Right In a state ?t|S, n|I,A?, if there is no depen-
dency relation (n ? w) in A, add the new de-
pendency relation (n ? t) into A and push n
onto S, giving the state ?n|t|S, I,A?(n ? t)?.
Reduce In a state ?t|S, I,A?, if there is a depen-
dency relation (t ? w) in A, pop S, giving the
state ?S, I,A?.
Shift In a state ?S, n|I,A?, push n onto S, giving
the state ?n|S, I,A?.
In this work, we used Support Vector Machines
(SVMs) to predict the operation given a parsing
state. Since SVMs are binary classifiers, we used the
pair-wise method to extend them in order to classify
our four-class task.
The features of a node are the word?s lemma,
the POS/chunk tag and the information of its child
node(s). The lemma is obtained from the word form
using a lemmatizer, except for numbers, which are
replaced by ??num??. The context features are the
two preceding nodes of node t (and t itself), the two
succeeding nodes of node n (and n itself), and their
1We use ?nil? to denote an empty list and a|A to denote a
list with head a and tail A.
206
 87
 87.2
 87.4
 87.6
 87.8
 88
 88.2
 88.4
 0  1000  2000  3000  4000  5000
Ac
cur
acy
 (%)
Number of Lexicalized Words
Figure 1: Dependency accuracies on the WSJ while
changing the degree of lexicalization.
child nodes (lemmas and POS tags). The distance
between nodes n and t is also used as a feature.
We trained our models on sections 2-21 of the
WSJ portion of the Penn Treebank. We used sec-
tion 23 as the test set. Since the original treebank is
based on phrase structure, we converted the treebank
to dependencies using the head rules provided by
Yamada 2. During the training phase, we used intact
POS and chunk tags3. During the testing phase, we
used automatically assigned POS and chunk tags by
Tsuruoka?s tagger4(Tsuruoka and Tsujii, 2005) and
YamCha chunker5(Kudo and Matsumoto, 2001).
We used an SVMs package, TinySVM6,and trained
the SVMs classifiers using a third-order polynomial
kernel. The other parameters are set to default.
The last row in Table 2 shows the accuracies of
our base dependency parser.
3.2 Degree of Lexicalization vs. Performance
The degree of lexicalization is specified by giving
a list of words to be lexicalized, which appear in
a training corpus. For minimal lexicalization, we
used a short list that consists of only high-frequency
words, and for maximal lexicalization, the whole list
was used.
To conduct the experiments efficiently, we trained
2http://www.jaist.ac.jp/?h-yamada/
3In a preliminary experiment, we tried to use automatically
assigned POS and chunk tags, but we did not detect significant
difference in performance.
4http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/
5http://chasen.org/?taku-ku/software/yamcha/
6http://chasen.org/?taku-ku/software/TinySVM/
 83.6
 83.8
 84
 84.2
 84.4
 84.6
 84.8
 85
 0  1000  2000  3000  4000  5000
Ac
cur
acy
 (%)
Number of Lexicalized Words
Figure 2: Dependency accuracies on the Brown Cor-
pus while changing the degree of lexicalization.
our models using the first 10,000 sentences in sec-
tions 2-21 of the WSJ portion of the Penn Treebank.
We used section 24, which is usually used as the
development set, to measure the change in perfor-
mance based on the degree of lexicalization.
We counted word (lemma) frequencies in the
training corpus and made a word list in descending
order of their frequencies. The resultant list con-
sists of 13,729 words, and the most frequent word is
?the?, which occurs 13,252 times, as shown in Table
3. We define the degree of lexicalization as a thresh-
old of the word list. If, for example, this threshold is
set to 1,000, the top 1,000 most frequently occurring
words are lexicalized.
We evaluated dependency accuracies while
changing the threshold of lexicalization. Figure 1
shows the result. The dotted line (88.23%) repre-
sents the dependency accuracy of the maximal lex-
icalization, that is, using the whole word list. We
can see that the decrease in accuracy is less than
1% at the minimal lexicalization (degree=100) and
the accuracy of more than 3,000 degree slightly ex-
ceeds that of the maximal lexicalization. The best
accuracy (88.34%) was achieved at 4,500 degree and
significantly outperformed the accuracy (88.23%) of
the maximal lexicalization (McNemar?s test; p =
0.017 < 0.05). These results indicate that maximal
lexicalization is not so effective for obtaining accu-
rate dependency relations.
We also applied the same trained models to the
Brown Corpus as an experiment of parser adapta-
tion. We first split the Brown Corpus portion of
207
rank word freq. rank word freq.
1 the 13,252 1,000 watch 29
2 , 12,858
...
...
...
...
...
... 2,000 healthvest 12
100 week 261
...
...
...
...
...
... 3,000 whoop 7
500 estate 64
...
...
...
...
...
...
Table 3: Word list.
the Penn Treebank into training and testing parts in
the same way as (Roark and Bacchiani, 2003). We
further extracted 2,425 sentences at regular intervals
from the training part and used them to measure the
change in performance while varying the degree of
lexicalization. Figure 2 shows the result. The dot-
ted line (84.75%) represents the accuracy of maxi-
mal lexicalization. The resultant curve is similar to
that of the WSJ experiment7. We can say that our
claim is true even if the testing corpus is outside the
domain.
3.3 Discussion
We have presented a minimally or lowly lexical-
ized dependency parser. Its dependency accuracy is
close or almost equivalent to that of fully lexicalized
parsers, despite the lexicalization restriction. Fur-
thermore, the restriction reduces the time and space
complexity. The minimally lexicalized parser (de-
gree=100) took 12m46s to parse the WSJ develop-
ment set and required 111 MB memory. These are
36% of time and 45% of memory reduction, com-
pared to the fully lexicalized one.
The experimental results imply that training cor-
pora are too small to demonstrate the full potential
of lexicalization. We should consider unsupervised
or semi-supervised ways to make lexicalized parsers
more effective and accurate.
Acknowledgment
This research is partially supported by special coor-
dination funds for promoting science and technol-
ogy.
7In the experiment on the Brown Corpus, the difference be-
tween the best accuracy and the baseline was not significant.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL2000, pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and Eric
Ringger. 2006. Multilingual dependency parsing using
bayes point machines. In Proceedings of HLT-NAACL2006,
pages 160?167.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of ACL2004,
pages 423?429.
Yuan Ding and Martha Palmer. 2005. Machine translation
using probabilistic synchronous dependency insertion gram-
mars. In Proceedings of ACL2005, pages 541?548.
Hideki Isozaki, Hideto Kazawa, and Tsutomu Hirao. 2004.
A deterministic word dependency analyzer enhanced with
preference learning. In Proceedings of COLING2004, pages
275?281.
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of ACL2003, pages 423?
430.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of NAACL2001, pages
192?199.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In Proceedings
of ACL2005, pages 75?82.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proceed-
ings of EACL2006, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Online large-margin training of dependency parsers. In Pro-
ceedings of ACL2005, pages 91?98.
Joakim Nivre and Mario Scholz. 2004. Deterministic de-
pendency parsing of English text. In Proceedings of COL-
ING2004, pages 64?70.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL2006, pages 433?
440.
Brian Roark and Michiel Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In Proceed-
ings of HLT-NAACL2003, pages 205?212.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirectional
inference with the easiest-first strategy for tagging sequence
data. In Proceedings of HLT-EMNLP2005, pages 467?474.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings of IWPT2003, pages 195?206.
208
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 217?220,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Hybrid Approach to Word Segmentation and POS Tagging
Tetsuji Nakagawa
Oki Electric Industry Co., Ltd.
2?5?7 Honmachi, Chuo-ku
Osaka 541?0053, Japan
nakagawa378@oki.com
Kiyotaka Uchimoto
National Institute of Information and
Communications Technology
3?5 Hikaridai, Seika-cho, Soraku-gun
Kyoto 619?0289, Japan
uchimoto@nict.go.jp
Abstract
In this paper, we present a hybrid method for
word segmentation and POS tagging. The
target languages are those in which word
boundaries are ambiguous, such as Chinese
and Japanese. In the method, word-based
and character-based processing is combined,
and word segmentation and POS tagging are
conducted simultaneously. Experimental re-
sults on multiple corpora show that the inte-
grated method has high accuracy.
1 Introduction
Part-of-speech (POS) tagging is an important task
in natural language processing, and is often neces-
sary for other processing such as syntactic parsing.
English POS tagging can be handled as a sequential
labeling problem, and has been extensively studied.
However, in Chinese and Japanese, words are not
separated by spaces, and word boundaries must be
identified before or during POS tagging. Therefore,
POS tagging cannot be conducted without word seg-
mentation, and how to combine these two processing
is an important issue. A large problem in word seg-
mentation and POS tagging is the existence of un-
known words. Unknown words are defined as words
that are not in the system?s word dictionary. It is dif-
ficult to determine the word boundaries and the POS
tags of unknown words, and unknown words often
cause errors in these processing.
In this paper, we study a hybrid method for Chi-
nese and Japanese word segmentation and POS tag-
ging, in which word-based and character-based pro-
cessing is combined, and word segmentation and
POS tagging are conducted simultaneously. In the
method, word-based processing is used to handle
known words, and character-based processing is
used to handle unknown words. Furthermore, infor-
mation of word boundaries and POS tags are used
at the same time with this method. The following
sections describe the hybrid method and results of
experiments on Chinese and Japanese corpora.
2 Hybrid Method for Word Segmentation
and POS Tagging
Many methods have been studied for Chinese and
Japanese word segmentation, which include word-
based methods and character-based methods. Nak-
agawa (2004) studied a method which combines a
word-based method and a character-based method.
Given an input sentence in the method, a lattice is
constructed first using a word dictionary, which con-
sists of word-level nodes for all the known words in
the sentence. These nodes have POS tags. Then,
character-level nodes for all the characters in the
sentence are added into the lattice (Figure 1). These
nodes have position-of-character (POC) tags which
indicate word-internal positions of the characters
(Xue, 2003). There are four POC tags, B, I , E
and S, each of which respectively indicates the be-
ginning of a word, the middle of a word, the end
of a word, and a single character word. In the
method, the word-level nodes are used to identify
known words, and the character-level nodes are used
to identify unknown words, because generally word-
level information is precise and appropriate for pro-
cessing known words, and character-level informa-
tion is robust and appropriate for processing un-
known words. Extended hidden Markov models are
used to choose the best path among all the possible
candidates in the lattice, and the correct path is indi-
cated by the thick lines in Figure 1. The POS tags
and the POC tags are treated equally in the method.
Thus, the word-level nodes and the character-level
nodes are processed uniformly, and known words
and unknown words are identified simultaneously.
In the method, POS tags of known words as well as
word boundaries are identified, but POS tags of un-
known words are not identified. Therefore, we ex-
tend the method in order to conduct unknown word
POS tagging too:
Hybrid Method
The method uses subdivided POC-tags in or-
der to identify not only the positions of charac-
ters but also the parts-of-speech of the compos-
ing words (Figure 2, A). In the method, POS
tagging of unknown words is conducted at the
same time as word segmentation and POS tag-
217
Figure 1: Word Segmentation and Known Word POS Tagging using Word and Character-based Processing
ging of known words, and information of parts-
of-speech of unknown words can be used for
word segmentation.
There are also two other methods capable of con-
ducting unknown word POS tagging (Ng and Low,
2004):
Word-based Post-Processing Method
This method receives results of word segmen-
tation and known word POS tagging, and pre-
dicts POS tags of unknown words using words
as units (Figure 2, B). This approach is the
same as the approach widely used in English
POS tagging. In the method, the process of
unknown word POS tagging is separated from
word segmentation and known word POS tag-
ging, and information of parts-of-speech of un-
known words cannot be used for word segmen-
tation. In later experiments, maximum entropy
models were used deterministically to predict
POS tags of unknown words. As features for
predicting the POS tag of an unknown word w,
we used the preceding and the succeeding two
words of w and their POS tags, the prefixes and
the suffixes of up to two characters of w, the
character types contained in w, and the length
of w.
Character-based Post-Processing Method
This method is similar to the word-based post-
processing method, but in this method, POS
tags of unknown words are predicted using
characters as units (Figure 2, C). In the method,
POS tags of unknown words are predicted us-
ing exactly the same probabilistic models as
the hybrid method, but word boundaries and
POS tags of known words are fixed in the post-
processing step.
Ng and Low (2004) studied Chinese word seg-
mentation and POS tagging. They compared sev-
eral approaches, and showed that character-based
approaches had higher accuracy than word-based
approaches, and that conducting word segmentation
and POS tagging all at once performed better than
conducting these processing separately. Our hy-
brid method is similar to their character-based all-at-
once approach. However, in their experiments, only
word-based and character-based methods were ex-
amined. In our experiments, the combined method
of word-based and character-based processing was
examined. Furthermore, although their experiments
were conducted with only Chinese data, we con-
ducted experiments with Chinese and Japanese data,
and confirmed that the hybrid method performed
well on the Japanese data as well as the Chinese
data.
3 Experiments
We used five word-segmented and POS-tagged cor-
pora; the Penn Chinese Treebank corpus 2.0 (CTB),
a part of the PFR corpus (PFR), the EDR cor-
pus (EDR), the Kyoto University corpus version
2 (KUC) and the RWCP corpus (RWC). The first
two were Chinese (C) corpora, and the rest were
Japanese (J) corpora, and they were split into train-
ing and test data. The dictionary distributed with
JUMAN version 3.61 (Kurohashi and Nagao, 1998)
was used as a word dictionary in the experiments
with the KUC corpus, and word dictionaries were
constructed from all the words in the training data in
the experiments with other corpora. Table 1 summa-
rizes statistical information of the corpora: the lan-
guage, the number of POS tags, the sizes of training
and test data, and the splitting methods of them1. We
used the following scoring measures to evaluate per-
formance of word segmentation and POS tagging:
R : Recall (The ratio of the number of correctly
segmented/POS-tagged words in system?s out-
put to the number of words in test data),
P : Precision (The ratio of the number of correctly
segmented/POS-tagged words in system?s out-
put to the number of words in system?s output),
1The unknown word rate for word segmentation is not equal
to the unknown word rate for POS tagging in general, since
the word forms of some words in the test data may exist in the
word dictionary but the POS tags of them may not exist. Such
words are regarded as known words in word segmentation, but
as unknown words in POS tagging.
218
Figure 2: Three Methods for Word Segmentation and POS Tagging
F : F-measure (F = 2?R? P/(R+ P )),
Runknown : Recall for unknown words,
Rknown : Recall for known words.
Table 2 shows the results2. In the table, Word-
based Post-Proc., Char.-based Post-Proc. and Hy-
brid Method respectively indicate results obtained
with the word-based post-processing method, the
character-based post-processing method, and the hy-
brid method. Two types of performance were mea-
sured: performance of word segmentation alone,
and performance of both word segmentation and
POS tagging. We first compare performance of
both word segmentation and POS tagging. The
F-measures of the hybrid method were highest on
all the corpora. This result agrees with the ob-
servation by Ng and Low (2004) that higher accu-
racy was obtained by conducting word segmenta-
tion and POS tagging at the same time than by con-
ducting these processing separately. Comparing the
word-based and the character-based post-processing
methods, the F-measures of the latter were higher
on the Chinese corpora as reported by Ng and
Low (2004), but the F-measures of the former were
slightly higher on the Japanese corpora. The same
tendency existed in the recalls for known words;
the recalls of the character-based post-processing
method were highest on the Chinese corpora, but
2The recalls for known words of the word-based and the
character-based post-processing methods differ, though the
POS tags of known words are identified in the first common
step. This is because known words are sometimes identified as
unknown words in the first step and their POS tags are predicted
in the post-processing step.
those of the word-based method were highest on
the Japanese corpora, except on the EDR corpus.
Thus, the character-based method was not always
better than the word-based method as reported by Ng
and Low (2004) when the methods were used with
the word and character-based combined approach on
Japanese corpora. We next compare performance of
word segmentation alone. The F-measures of the hy-
brid method were again highest in all the corpora,
and the performance of word segmentation was im-
proved by the integrated processing of word seg-
mentation and POS tagging. The precisions of the
hybrid method were highest with statistical signifi-
cance on four of the five corpora. In all the corpora,
the recalls for unknown words of the hybrid method
were highest, but the recalls for known words were
lowest.
Comparing our results with previous work is not
easy since experimental settings are not the same.
It was reported that the original combined method
of word-based and character-based processing had
high overall accuracy (F-measures) in Chinese word
segmentation, compared with the state-of-the-art
methods (Nakagawa, 2004). Kudo et al (2004) stud-
ied Japanese word segmentation and POS tagging
using conditional random fields (CRFs) and rule-
based unknown word processing. They conducted
experiments with the KUC corpus, and achieved F-
measure of 0.9896 in word segmentation, which is
better than ours (0.9847). Some features we did
not used, such as base forms and conjugated forms
of words, and hierarchical POS tags, were used in
219
Corpus Number Number of Words (Unknown Word Rate for Segmentation/Tagging)
(Lang.) of POS [partition in the corpus]
Tags Training Test
CTB 34 84,937 7,980 (0.0764 / 0.0939)
(C) [sec. 1?270] [sec. 271?300]
PFR 41 304,125 370,627 (0.0667 / 0.0749)
(C) [Jan. 1?Jan. 9] [Jan. 10?Jan. 19]
EDR 15 2,550,532 1,280,057 (0.0176 / 0.0189)
(J) [id = 4n+ 0, id = 4n+ 1] [id = 4n+ 2]
KUC 40 198,514 31,302 (0.0440 / 0.0517)
(J) [Jan. 1?Jan. 8] [Jan. 9]
RWC 66 487,333 190,571 (0.0513 / 0.0587)
(J) [1?10,000th sentences] [10,001?14,000th sentences]
Table 1: Statistical Information of Corpora
Corpus Scoring Word Segmentation Word Segmentation & POS Tagging
(Lang.) Measure Word-based Char.-based Hybrid Word-based Char.-based Hybrid
Post-Proc. Post-Proc. Method Post-Proc. Post-Proc. Method
R 0.9625 0.9625 0.9639 0.8922 0.8935 0.8944
CTB P 0.9408 0.9408 0.9519* 0.8721 0.8733 0.8832
(C) F 0.9516 0.9516 0.9578 0.8821 0.8833 0.8887
Runknown 0.6492 0.6492 0.7148 0.4219 0.4312 0.4713
Rknown 0.9885 0.9885 0.9845 0.9409 0.9414 0.9382
R 0.9503 0.9503 0.9516 0.8967 0.8997 0.9024*
PFR P 0.9419 0.9419 0.9485* 0.8888 0.8917 0.8996*
(C) F 0.9461 0.9461 0.9500 0.8928 0.8957 0.9010
Runknown 0.6063 0.6063 0.6674 0.3845 0.3980 0.4487
Rknown 0.9749 0.9749 0.9719 0.9382 0.9403 0.9392
R 0.9525 0.9525 0.9525 0.9358 0.9356 0.9357
EDR P 0.9505 0.9505 0.9513* 0.9337 0.9335 0.9346
(J) F 0.9515 0.9515 0.9519 0.9347 0.9345 0.9351
Runknown 0.4454 0.4454 0.4630 0.4186 0.4103 0.4296
Rknown 0.9616 0.9616 0.9612 0.9457 0.9457 0.9454
R 0.9857 0.9857 0.9850 0.9572 0.9567 0.9574
KUC P 0.9835 0.9835 0.9843 0.9551 0.9546 0.9566
(J) F 0.9846 0.9846 0.9847 0.9562 0.9557 0.9570
Runknown 0.9237 0.9237 0.9302 0.6724 0.6774 0.6879
Rknown 0.9885 0.9885 0.9876 0.9727 0.9719 0.9721
R 0.9574 0.9574 0.9592 0.9225 0.9220 0.9255*
RWC P 0.9533 0.9533 0.9577* 0.9186 0.9181 0.9241*
(J) F 0.9553 0.9553 0.9585 0.9205 0.9201 0.9248
Runknown 0.6650 0.6650 0.7214 0.4941 0.4875 0.5467
Rknown 0.9732 0.9732 0.9720 0.9492 0.9491 0.9491
(Statistical significance tests were performed for R and P , and * indicates significance at p < 0.05)
Table 2: Performance of Word Segmentation and POS Tagging
their study, and it may be a reason of the differ-
ence. Although, in our experiments, extended hid-
den Markov models were used to find the best so-
lution, the performance will be further improved by
using CRFs instead, which can easily incorporate a
wide variety of features.
4 Conclusion
In this paper, we studied a hybrid method in which
word-based and character-based processing is com-
bined, and word segmentation and POS tagging are
conducted simultaneously. We compared its perfor-
mance of word segmentation and POS tagging with
other methods in which POS tagging is conducted
as a separated post-processing. Experimental results
on multiple corpora showed that the hybrid method
had high accuracy in Chinese and Japanese.
References
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings of
EMNLP 2004, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1998. Japanese
Morphological Analysis System JUMAN version 3.61.
Department of Informatics, Kyoto University. (in
Japanese).
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-Level
Information. In Proceedings of COLING 2004, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based? In Proceedings of
EMNLP 2004, pages 277?284.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. International Journal of Compu-
tational Linguistics and Chinese, 8(1):29?48.
220
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 432?440,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
This paper proposes a novel framework
called bilingual co-training for a large-
scale, accurate acquisition method for
monolingual semantic knowledge. In
this framework, we combine the indepen-
dent processes of monolingual semantic-
knowledge acquisition for two languages
using bilingual resources to boost perfor-
mance. We apply this framework to large-
scale hyponymy-relation acquisition from
Wikipedia. Experimental results show
that our approach improved the F-measure
by 3.6?10.3%. We also show that bilin-
gual co-training enables us to build classi-
fiers for two languages in tandem with the
same combined amount of data as required
for training a single classifier in isolation
while achieving superior performance.
1 Motivation
Acquiring and accumulating semantic knowledge
are crucial steps for developing high-level NLP
applications such as question answering, although
it remains difficult to acquire a large amount of
highly accurate semantic knowledge. This pa-
per proposes a novel framework for a large-scale,
accurate acquisition method for monolingual se-
mantic knowledge, especially for semantic rela-
tions between nominals such as hyponymy and
meronymy. We call the framework bilingual co-
training.
The acquisition of semantic relations between
nominals can be seen as a classification task of se-
mantic relations ? to determine whether two nom-
inals hold a particular semantic relation (Girju et
al., 2007). Supervised learning methods, which
have often been applied to this classification task,
have shown promising results. In those methods,
however, a large amount of training data is usually
required to obtain high performance, and the high
costs of preparing training data have always been
a bottleneck.
Our research on bilingual co-training sprang
from a very simple idea: perhaps training data in a
language can be enlarged without much cost if we
translate training data in another language and add
the translation to the training data in the original
language. We also noticed that it may be possi-
ble to further enlarge the training data by trans-
lating the reliable part of the classification results
in another language. Since the learning settings
(feature sets, feature values, training data, corpora,
and so on) are usually different in two languages,
the reliable part in one language may be over-
lapped by an unreliable part in another language.
Adding the translated part of the classification re-
sults to the training data will improve the classifi-
cation results in the unreliable part. This process
can also be repeated by swapping the languages,
as illustrated in Figure 1. Actually, this is nothing
other than a bilingual version of co-training (Blum
and Mitchell, 1998).
Language 1 Language 2
Iteration
Manually Prepared 
Training Data
for Language 1
Classifier Classifier
Training Training
Enlarged 
Training Data
for Language 1
Enlarged 
Training Data
for Language 2
Manually Prepared 
Training Data
for Language 2
ClassifierClassifier
Further Enlarged 
Training Data
for Language 1
Further Enlarged 
Training Data
for Language 2
Translate
reliable parts of
classification 
results
Training
Training Training
Training
?..
?..
Translate
reliable parts of
classification 
results
Figure 1: Concept of bilingual co-training
Let us show an example in our current task:
hyponymy-relation acquisition from Wikipedia.
Our original approach for this task was super-
432
vised learning based on the approach proposed by
Sumida et al (2008), which was only applied for
Japanese and achieved around 80% in F-measure.
In their approach, a common substring in a hyper-
nym and a hyponym is assumed to be one strong
clue for recognizing that the two words constitute
a hyponymy relation. For example, recognizing a
proper hyponymy relation between two Japanese
words, (kouso meaning enzyme) and
 (kasuibunkaikouso meaning hydrolase), is
relatively easy because they share a common suf-
fix: kouso. On the other hand, judging whether
their English translations (enzyme and hydrolase)
have a hyponymy relation is probably more dif-
ficult since they do not share any substrings. A
classifier for Japanese will regard the hyponymy
relation as valid with high confidence, while a
classifier for English may not be so positive. In
this case, we can compensate for the weak part of
the English classifier by adding the English trans-
lation of the Japanese hyponymy relation, which
was recognized with high confidence, to the En-
glish training data.
In addition, if we repeat this process by swap-
ping English and Japanese, further improvement
may be possible. Furthermore, the reliable parts
that are automatically produced by a classifier can
be larger than manually tailored training data. If
this is the case, the effect of adding the transla-
tion to the training data can be quite large, and the
same level of effect may not be achievable by a
reasonable amount of labor for preparing the train-
ing data. This is the whole idea.
Through a series of experiments, this paper
shows that the above idea is valid at least for one
task: large-scale monolingual hyponymy-relation
acquisition from English and Japanese Wikipedia.
Experimental results showed that our method
based on bilingual co-training improved the per-
formance of monolingual hyponymy-relation ac-
quisition about 3.6?10.3% in the F-measure.
Bilingual co-training also enables us to build clas-
sifiers for two languages in tandem with the same
combined amount of data as would be required
for training a single classifier in isolation while
achieving superior performance.
People probably expect that a key factor in the
success of this bilingual co-training is how to
translate the training data. We actually did transla-
tion by a simple look-up procedure in the existing
translation dictionaries without any machine trans-
lation systems or disambiguation processes. De-
spite this simple approach, we obtained consistent
improvement in our task using various translation
dictionaries.
This paper is organized as follows. Section 2
presents bilingual co-training, and Section 3 pre-
cisely describes our system. Section 4 describes
our experiments and presents results. Section 5
discusses related work. Conclusions are drawn
and future work is mentioned in Section 6.
2 Bilingual Co-Training
Let S and T be two different languages, and let
CL be a set of class labels to be obtained as a re-
sult of learning/classification. To simplify the dis-
cussion, we assume that a class label is binary; i.e.,
the classification results are ?yes? or ?no.? Thus,
CL = {yes, no}. Also, we denote the set of all
nonnegative real numbers by R+.
Assume X = XS ? XT is a set of instances in
languages S and T to be classified. In the con-
text of a hyponymy-relation acquisition task, the
instances are pairs of nominals. Then we assume
that classifier c assigns class label cl in CL and
confidence value r for assigning the label, i.e.,
c(x) = (x, cl, r), where x ? X , cl ? CL, and
r ? R+. Note that we used support vector ma-
chines (SVMs) in our experiments and (the abso-
lute value of) the distance between a sample and
the hyperplane determined by the SVMs was used
as confidence value r. The training data are de-
noted by L ? X?CL, and we denote the learning
by function LEARN ; if classifier c is trained by
training data L, then c = LEARN(L). Particu-
larly, we denote the training sets for S and T that
are manually prepared by LS and LT , respectively.
Also, bilingual instance dictionary DBI is defined
as the translation pairs of instances in XS and XT .
Thus, DBI = {(s, t)} ? XS ? XT . In the case
of hyponymy-relation acquisition in English and
Japanese, (s, t) ? DBI could be (s=(enzyme, hy-
drolase), t=( (meaning enzyme),
 (meaning hydrolase))).
Our bilingual co-training is given in Figure 2. In
the initial stage, c0S and c0T are learned with manu-
ally labeled instances LS and LT (lines 2?5). Then
ciS and ciT are applied to classify instances in XS
and XT (lines 6?7). Denote CRiS as a set of the
classification results of ciS on instances XS that is
not in LiS and is registered in DBI . Lines 10?18
describe a way of selecting from CRiS newly la-
433
1: i = 0
2: L0S = LS ; L
0
T = LT
3: repeat
4: ciS := LEARN(L
i
S)
5: ciT := LEARN(L
i
T )
6: CRiS := {c
i
S(xS)|xS ? XS ,
?cl (xS , cl) /? LiS , ?xT (xS , xT ) ? DBI}
7: CRiT := {c
i
T (xT )|xT ? XT ,
?cl (xT , cl) /? LiT , ?xS (xS , xT ) ? DBI}
8: L(i+1)S := L
i
S
9: L(i+1)T := L
i
T
10: for each (xS , clS , rS) ? TopN(CRiS) do
11: for each xT such that (xS , xT ) ? DBI
and (xT , clT , rT ) ? CRiT do
12: if rS > ? then
13: if rT < ? or clS = clT then
14: L(i+1)T := L
(i+1)
T ? {(xT , clS)}
15: end if
16: end if
17: end for
18: end for
19: for each (xT , clT , rT ) ? TopN(CRiT ) do
20: for each xS such that (xS , xT ) ? DBI
and (xS , clS , rS) ? CRiS do
21: if rT > ? then
22: if rS < ? or clS = clT then
23: L(i+1)S := L
(i+1)
S ? {(xS , clT )}
24: end if
25: end if
26: end for
27: end for
28: i = i + 1
29: until a fixed number of iterations is reached
Figure 2: Pseudo-code of bilingual co-training
beled instances to be added to a new training set
in T . TopN(CRiS) is a set of ciS(x), whose rS
is top-N highest in CRiS . (In our experiments,
N = 900.) During the selection, ciS acts as a
teacher and ciT as a student. The teacher instructs
his student in the class label of xT , which is actu-
ally a translation of xS by bilingual instance dic-
tionary DBI , through clS only if he can do it with
a certain level of confidence, say rS > ?, and
if one of two other condition meets (rT < ? or
clS = clT ). clS = clT is a condition to avoid
problems, especially when the student also has a
certain level of confidence in his opinion on a class
label but disagrees with the teacher: rT > ? and
clS 6= clT . In that case, the teacher does nothing
and ignores the instance. Condition rT < ? en-
ables the teacher to instruct his student in the class
label of xT in spite of their disagreement in a class
label. If every condition is satisfied, (xT , clS) is
added to existing labeled instances L(i+1)T . The
roles are reversed in lines 19?27 so that ciT be-
comes a teacher and ciS a student.
Similar to co-training (Blum and Mitchell,
1998), one classifier seeks another?s opinion to se-
lect new labeled instances. One main difference
between co-training and bilingual co-training is
the space of instances: co-training is based on dif-
ferent features of the same instances, and bilin-
gual co-training is based on different spaces of in-
stances divided by languages. Since some of the
instances in different spaces are connected by a
bilingual instance dictionary, they seem to be in
the same space. Another big difference lies in
the role of the two classifiers. The two classifiers
in co-training work on the same task, but those
in bilingual co-training do the same type of task
rather than the same task.
3 Acquisition of Hyponymy Relations
from Wikipedia
Our system, which acquires hyponymy relations
from Wikipedia based on bilingual co-training,
is described in Figure 3. The following three
main parts are described in this section: candidate
extraction, hyponymy-relation classification, and
bilingual instance dictionary construction.
Classifier in E Classifier in J
Labeled 
instances
Labeled 
instances 
Wikipedia
Articles in E
Wikipedia
Articles in J
Candidates
in J
Candidates
in E
Acquisition of 
translation dictionary
Bilingual Co-Training
Unlabeled 
instances in J
Unlabeled 
instances in E
Bilingual instance dictionary
Newly labeled 
instances for E 
Newly labeled 
instances for J
Translation 
dictionary 
Hyponymy-relation 
candidate extraction
Hyponymy-relation 
candidate extraction
Figure 3: System architecture
3.1 Candidate Extraction
We follow Sumida et al (2008) to extract
hyponymy-relation candidates from English and
Japanese Wikipedia. A layout structure is chosen
434
(a) Layout structure
of article TIGER
Range
Siberian tiger
Bengal tiger
Subspecies
Taxonomy
Tiger
Malayan tiger
(b) Tree structure of
Figure 4(a)
Figure 4: Wikipedia article and its layout structure
as a source of hyponymy relations because it can
provide a huge amount of them (Sumida et al,
2008; Sumida and Torisawa, 2008)1, and recog-
nition of the layout structure is easy regardless of
languages. Every English and Japanese Wikipedia
article was transformed into a tree structure like
Figure 4, where layout items title, (sub)section
headings, and list items in an article were used
as nodes in a tree structure. Sumida et al (2008)
found that some pairs consisting of a node and one
of its descendants constituted a proper hyponymy
relation (e.g., (TIGER, SIBERIAN TIGER)), and
this could be a knowledge source of hyponymy
relation acquisition. A hyponymy-relation candi-
date is then extracted from the tree structure by re-
garding a node as a hypernym candidate and all
its subordinate nodes as hyponym candidates of
the hypernym candidate (e.g., (TIGER, TAXON-
OMY) and (TIGER, SIBERIAN TIGER) from Fig-
ure 4). 39 M English hyponymy-relation candi-
dates and 10 M Japanese ones were extracted from
Wikipedia. These candidates are classified into
proper hyponymy relations and others by using the
classifiers described below.
3.2 Hyponymy-Relation Classification
We use SVMs (Vapnik, 1995) as classifiers for
the classification of the hyponymy relations on the
hyponymy-relation candidates. Let hyper be a hy-
pernym candidate, hypo be a hyper?s hyponym
candidate, and (hyper, hypo) be a hyponymy-
relation candidate. The lexical, structure-based,
and infobox-based features of (hyper, hypo) in Ta-
ble 1 are used for building English and Japanese
classifiers. Note that SF
3
?SF
5
and IF were not
1Sumida et al (2008) reported that they obtained 171 K,
420 K, and 1.48 M hyponymy relations from a definition sen-
tence, a category system, and a layout structure in Japanese
Wikipedia, respectively.
used in Sumida et al (2008) but LF
1
?LF
5
and
SF
1
?SF
2
are the same as their feature set.
Let us provide an overview of the feature
sets used in Sumida et al (2008). See Sum-
ida et al (2008) for more details. Lexical fea-
tures LF
1
?LF
5
are used to recognize the lexi-
cal evidence encoded in hyper and hypo for hy-
ponymy relations. For example, (hyper,hypo) is
often a proper hyponymy relation if hyper and
hypo share the same head morpheme or word.
In LF
1
and LF
2
, such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns. TagChunk (Daume? III et
al., 2005) for English and MeCab (MeCab, 2008)
for Japanese were used to provide the lexical fea-
tures. Several simple lexical patterns2 were also
applied to hyponymy-relation candidates. For ex-
ample, ?List of artists? is converted into ?artists?
by lexical pattern ?list of X.? Hyponymy-relation
candidates whose hypernym candidate matches
such a lexical pattern are likely to be valid (e.g.,
(List of artists, Leonardo da Vinci)). We use LF
4
for dealing with these cases. If a typical or fre-
quently used section heading in a Wikipedia arti-
cle, such as ?History? or ?References,? is used as
a hyponym candidate in a hyponymy-relation can-
didate, the hyponymy-relation candidate is usually
not a hyponymy relation. LF
5
is used to recognize
these hyponymy-relation candidates.
Structure-based features are related to the
tree structure of Wikipedia articles from which
hyponymy-relation candidate (hyper,hypo) is ex-
tracted. SF
1
provides the distance between hyper
and hypo in the tree structure. SF
2
represents the
type of layout items from which hyper and hypo
are originated. These are the feature sets used in
Sumida et al (2008).
We also added some new items to the above
feature sets. SF
3
represents the types of tree
nodes including root, leaf, and others. For exam-
ple, (hyper,hypo) is seldom a hyponymy relation
if hyper is from a root node (or title) and hypo
is from a hyper?s child node (or section head-
ings). SF
4
and SF
5
represent the structural con-
texts of hyper and hypo in a tree structure. They
can provide evidence related to similar hyponymy-
relation candidates in the structural contexts.
An infobox-based feature, IF , is based on a
2We used the same Japanese lexical patterns in Sumida et
al. (2008) to build English lexical patterns with them.
435
Type Description Example
LF
1
Morphemes/words hyper: tiger?, hypo: Siberian, hypo: tiger?
LF
2
POS of morphemes/words hyper: NN?, hypo: NP, hypo: NN?
LF
3
hyper and hypo, themselves hyper: Tiger, hypo: Siberian tiger
LF
4
Used lexical patterns hyper: ?List of X?, hypo: ?Notable X?
LF
5
Typical section headings hyper: History, hypo: Reference
SF
1
Distance between hyper and hypo 3
SF
2
Type of layout items hyper: title, hypo: bulleted list
SF
3
Type of tree nodes hyper: root node, hypo: leaf node
SF
4
LF
1
and LF
3
of hypo?s parent node LF
3
:Subspecies
SF
5
LF
1
and LF
3
of hyper?s child node LF
3
: Taxonomy
IF Semantic properties of hyper and hypo hyper: (taxobox,species), hypo: (taxobox,name)
Table 1: Feature type and its value. ? in LF
1
and LF
2
represent the head morpheme/word and its POS.
Except those in LF
4
and LF
5
, examples are derived from (TIGER, SIBERIAN TIGER) in Figure 4.
Wikipedia infobox, a special kind of template, that
describes a tabular summary of an article subject
expressed by attribute-value pairs. An attribute
type coupled with the infobox name to which it
belongs provides the semantic properties of its
value that enable us to easily understand what
the attribute value means (Auer and Lehmann,
2007; Wu and Weld, 2007). For example, in-
fobox template City Japan in Wikipedia article
Kyoto contains several attribute-value pairs such
as ?Mayor=Daisaku Kadokawa? as attribute=its
value. What Daisaku Kadokawa, the attribute
value of mayor in the example, represents is hard
to understand alone if we lack knowledge, but
its attribute type, mayor, gives a clue?Daisaku
Kadokawa is a mayor related to Kyoto. These
semantic properties enable us to discover seman-
tic evidence for hyponymy relations. We ex-
tract triples (infobox name, attribute type, attribute
value) from the Wikipedia infoboxes and encode
such information related to hyper and hypo in our
feature set IF .3
3.3 Bilingual Instance Dictionary
Construction
Multilingual versions of Wikipedia articles are
connected by cross-language links and usually
have titles that are bilinguals of each other (Erd-
mann et al, 2008). English and Japanese articles
connected by a cross-language link are extracted
from Wikipedia, and their titles are regarded as
translation pairs4. The translation pairs between
3We obtained 1.6 M object-attribute-value triples in
Japanese and 5.9 M in English.
4197 K translation pairs were extracted.
English and Japanese terms are used for building
bilingual instance dictionary DBI for hyponymy-
relation acquisition, where DBI is composed of
translation pairs between English and Japanese
hyponymy-relation candidates5.
4 Experiments
We used the MAY 2008 version of English
Wikipedia and the JUNE 2008 version of
Japanese Wikipedia for our experiments. 24,000
hyponymy-relation candidates, randomly selected
in both languages, were manually checked to build
training, development, and test sets6. Around
8,000 hyponymy relations were found in the man-
ually checked data for both languages7. 20,000 of
the manually checked data were used as a train-
ing set for training the initial classifier. The rest
were equally divided into development and test
sets. The development set was used to select the
optimal parameters in bilingual co-training and the
test set was used to evaluate our system.
We used TinySVM (TinySVM, 2002) with a
polynomial kernel of degree 2 as a classifier. The
maximum iteration number in the bilingual co-
training was set as 100. Two parameters, ? and
TopN , were selected through experiments on the
development set. ? = 1 and TopN=900 showed
5We also used redirection links in English and Japanese
Wikipedia for recognizing the variations of terms when we
built a bilingual instance dictionary with Wikipedia cross-
language links.
6It took about two or three months to check them in each
language.
7Regarding a hyponymy relation as a positive sample and
the others as a negative sample for training SVMs, ?positive
sample:negative sample? was about 8,000:16,000=1:2
436
the best performance and were used as the optimal
parameter in the following experiments.
We conducted three experiments to show ef-
fects of bilingual co-training, training data size,
and bilingual instance dictionaries. In the first two
experiments, we experimented with a bilingual in-
stance dictionary derived from Wikipedia cross-
language links. Comparison among systems based
on three different bilingual instance dictionaries is
shown in the third experiment.
Precision (P ), recall (R), and F
1
-measure (F
1
),
as in Eq (1), were used as the evaluation measures,
where Rel represents a set of manually checked
hyponymy relations and HRbyS represents a set
of hyponymy-relation candidates classified as hy-
ponymy relations by the system:
P = |Rel ? HRbyS|/|HRbyS| (1)
R = |Rel ? HRbyS|/|Rel|
F
1
= 2 ? (P ? R)/(P + R)
4.1 Effect of Bilingual Co-Training
ENGLISH JAPANESE
P R F
1
P R F
1
SYT 78.5 63.8 70.4 75.0 77.4 76.1
INIT 77.9 67.4 72.2 74.5 78.5 76.6
TRAN 76.8 70.3 73.4 76.7 79.3 78.0
BICO 78.0 83.7 80.7 78.3 85.2 81.6
Table 2: Performance of different systems (%)
Table 2 shows the comparison results of the four
systems. SYT represents the Sumida et al (2008)
system that we implemented and tested with the
same data as ours. INIT is a system based on ini-
tial classifier c0 in bilingual co-training. We trans-
lated training data in one language by using our
bilingual instance dictionary and added the trans-
lation to the existing training data in the other
language like bilingual co-training did. The size
of the English and Japanese training data reached
20,729 and 20,486. We trained initial classifier c0
with the new training data. TRAN is a system
based on the classifier. BICO is a system based
on bilingual co-training.
For Japanese, SYT showed worse performance
than that reported in Sumida et al (2008), proba-
bly due to the difference in training data size (ours
is 20,000 and Sumida et al (2008) was 29,900).
The size of the test data was also different ? ours
is 2,000 and Sumida et al (2008) was 1,000.
Comparison between INIT and SYT shows the
effect of SF
3
?SF
5
and IF , newly introduced
feature types, in hyponymy-relation classification.
INIT consistently outperformed SYT, although the
difference was merely around 0.5?1.8% in F
1
.
BICO showed significant performance im-
provement (around 3.6?10.3% in F
1
) over SYT,
INIT, and TRAN regardless of the language. Com-
parison between TRAN and BICO showed that
bilingual co-training is useful for enlarging the
training data and that the performance gain by
bilingual co-training cannot be achieved by sim-
ply translating the existing training data.
 81
 79
 77
 75
 73
 60 55 50 45 40 35 30 25 20
F 1
Training Data (103)
English
Japanese
Figure 5: F
1
curves based on the increase of train-
ing data size during bilingual co-training
Figure 5 shows F
1
curves based on the size
of the training data including those manually tai-
lored and automatically obtained through bilin-
gual co-training. The curve starts from 20,000 and
ends around 55,000 in Japanese and 62,000 in En-
glish. As the training data size increases, the F
1
curves tend to go upward in both languages. This
indicates that the two classifiers cooperate well
to boost their performance through bilingual co-
training.
We recognized 5.4 M English and 2.41 M
Japanese hyponymy relations from the classifi-
cation results of BICO on all hyponymy-relation
candidates in both languages.
4.2 Effect of Training Data Size
We performed two tests to investigate the effect of
the training data size on bilingual co-training. The
first test posed the following question: ?If we build
2n training samples by hand and the building cost
is the same in both languages, which is better from
the monolingual aspects: 2n monolingual training
samples or n bilingual training samples?? Table 3
and Figure 6 show the results.
437
In INIT-E and INIT-J, a classifier in each lan-
guage, which was trained with 2n monolingual
training samples, did not learn through bilingual
co-training. In BICO-E and BICO-J, bilingual co-
training was applied to the initial classifiers trained
with n training samples in both languages. As
shown in Table 3, BICO, with half the size of the
training samples used in INIT, always performed
better than INIT in both languages. This indicates
that bilingual co-training enables us to build clas-
sifiers for two languages in tandem with the same
combined amount of data as required for training
a single classifier in isolation while achieving su-
perior performance.
 81
 79
 77
 75
 73
 71
 69
 67
 65
 20000 15000 10000 7500 5000 2500
F 1
Training Data Size
INIT-E
INIT-J
BICO-E
BICO-J
Figure 6: F
1
based on training data size:
with/without bilingual co-training
n
2n n
INIT-E INIT-J BICO-E BICO-J
2500 67.3 72.3 70.5 73.0
5000 69.2 74.3 74.6 76.9
10000 72.2 76.6 76.9 78.6
Table 3: F
1
based on training data size:
with/without bilingual co-training (%)
The second test asked: ?Can we always im-
prove performance through bilingual co-training
with one strong and one weak classifier?? If the
answer is yes, then we can apply our framework
to acquisition of hyponymy-relations in other lan-
guages, i.e., German and French, without much
effort for preparing a large amount of training
data, because our strong classifier in English or
Japanese can boost the performance of a weak
classifier in other languages.
To answer the question, we tested the perfor-
mance of classifiers by using all training data
(20,000) for a strong classifier and by changing the
training data size of the other from 1,000 to 15,000
({1,000, 5,000, 10,000, 15,000}) for a weak clas-
sifier.
INIT-E BICO-E INIT-J BICO-J
1,000 72.2 79.6 64.0 72.7
5,000 72.2 79.6 73.1 75.3
10,000 72.2 79.8 74.3 79.0
15,000 72.2 80.4 77.0 80.1
Table 4: F
1
based on training data size: when En-
glish classifier is strong one
INIT-E BICO-E INIT-J BICO-J
1,000 60.3 69.7 76.6 79.3
5,000 67.3 74.6 76.6 79.6
10,000 69.2 77.7 76.6 80.1
15,000 71.0 79.3 76.6 80.6
Table 5: F
1
based on training data size: when
Japanese classifier is strong one
Tables 4 and 5 show the results, where ?INIT?
represents a system based on the initial classifier
in each language and ?BICO? represents a sys-
tem based on bilingual co-training. The results
were encouraging because the classifiers showed
better performance than their initial ones in every
setting. In other words, a strong classifier always
taught a weak classifier well, and the strong one
also got help from the weak one, regardless of the
size of the training data with which the weaker one
learned. The test showed that bilingual co-training
can work well if we have one strong classifier.
4.3 Effect of Bilingual Instance Dictionaries
We tested our method with different bilingual in-
stance dictionaries to investigate their effect. We
built bilingual instance dictionaries based on dif-
ferent translation dictionaries whose translation
entries came from different domains (i.e., gen-
eral domain, technical domain, and Wikipedia)
and had a different degree of translation ambigu-
ity. In Table 6, D1 and D2 correspond to sys-
tems based on a bilingual instance dictionary de-
rived from two handcrafted translation dictionar-
ies, EDICT (Breen, 2008) (a general-domain dic-
tionary) and ?The Japan Science and Technology
Agency Dictionary,? (a translation dictionary for
technical terms) respectively. D3, which is the
same as BICO in Table 2, is based on a bilingual
438
instance dictionary derived from Wikipedia. EN-
TRY represents the number of translation dictio-
nary entries used for building a bilingual instance
dictionary. E2J (or J2E) represents the average
translation ambiguities of English (or Japanese)
terms in the entries. To show the effect of these
translation ambiguities, we used each dictionary
under two different conditions, ?=5 and ALL. ?=5
represents the condition where only translation en-
tries with less than five translation ambiguities are
used; ALL represents no restriction on translation
ambiguities.
DIC F
1
DIC STATISTICS
TYPE E J ENTRY E2J J2E
D1 ?=5 76.5 78.4 588K 1.80 1.77
D1 ALL 75.0 77.2 990K 7.17 2.52
D2 ?=5 76.9 78.5 667K 1.89 1.55
D2 ALL 77.0 77.9 750K 3.05 1.71
D3 ?=5 80.7 81.6 197K 1.03 1.02
D3 ALL 80.7 81.6 197K 1.03 1.02
Table 6: Effect of different bilingual instance dic-
tionaries
The results showed that D3 was the best and
that the performances of the others were sim-
ilar to each other. The differences in the F
1
scores between ?=5 and ALL were relatively small
within the same system triggered by translation
ambiguities. The performance gap between D3
and the other systems might explain the fact that
both hyponymy-relation candidates and the trans-
lation dictionary used in D3 were extracted from
the same dataset (i.e., Wikipedia), and thus the
bilingual instance dictionary built with the trans-
lation dictionary in D3 had better coverage of
the Wikipedia entries consisting of hyponymy-
relation candidates than the other bilingual in-
stance dictionaries. Although D1 and D2 showed
lower performance than D3, the experimental re-
sults showed that bilingual co-training was always
effective no matter which dictionary was used
(Note that F
1
of INIT in Table 2 was 72.2 in En-
glish and 76.6 in Japanese.)
5 Related Work
Li and Li (2002) proposed bilingual bootstrapping
for word translation disambiguation. Similar to
bilingual co-training, classifiers for two languages
cooperated in learning with bilingual resources in
bilingual bootstrapping. However, the two clas-
sifiers in bilingual bootstrapping were for a bilin-
gual task but did different tasks from the monolin-
gual viewpoint. A classifier in each language is for
word sense disambiguation, where a class label (or
word sense) is different based on the languages.
On the contrary, classifiers in bilingual co-training
cooperate in doing the same type of tasks.
Bilingual resources have been used for mono-
lingual tasks including verb classification and
noun phrase semantic interpolation (Merlo et al,
2002; Girju, 2006). However, unlike ours, their fo-
cus was limited to bilingual features for one mono-
lingual classifier based on supervised learning.
Recently, there has been increased interest in se-
mantic relation acquisition from corpora. Some
regarded Wikipedia as the corpora and applied
hand-crafted or machine-learned rules to acquire
semantic relations (Herbelot and Copestake, 2006;
Kazama and Torisawa, 2007; Ruiz-casado et al,
2005; Nastase and Strube, 2008; Sumida et al,
2008; Suchanek et al, 2007). Several researchers
who participated in SemEval-07 (Girju et al,
2007) proposed methods for the classification of
semantic relations between simple nominals in
English sentences. However, the previous work
seldom considered the bilingual aspect of seman-
tic relations in the acquisition of monolingual se-
mantic relations.
6 Conclusion
We proposed a bilingual co-training approach and
applied it to hyponymy-relation acquisition from
Wikipedia. Experiments showed that bilingual
co-training is effective for improving the perfor-
mance of classifiers in both languages. We fur-
ther showed that bilingual co-training enables us
to build classifiers for two languages in tandem,
outperforming classifiers trained individually for
each language while requiring no more training
data in total than a single classifier trained in iso-
lation.
We showed that bilingual co-training is also
helpful for boosting the performance of a weak
classifier in one language with the help of a strong
classifier in the other language without lowering
the performance of either classifier. This indicates
that the framework can reduce the cost of prepar-
ing training data in new languages with the help of
our English and Japanese strong classifiers. Our
future work focuses on this issue.
439
References
So?ren Auer and Jens Lehmann. 2007. What have
Innsbruck and Leipzig in common? Extracting se-
mantics from wiki content. In Proc. of the 4th
European Semantic Web Conference (ESWC 2007),
pages 503?517. Springer.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT?
98: Proceedings of the eleventh annual conference
on Computational learning theory, pages 92?100.
Jim Breen. 2008. EDICT Japanese/English dictionary
file, The Electronic Dictionary Research and Devel-
opment Group, Monash University.
Hal Daume? III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as classi-
fication. In Proc. of NIPS Workshop on Advances in
Structured Learning for Text and Speech Processing,
Whistler, Canada.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara,
and Shojiro Nishio. 2008. A bilingual dictionary
extracted from the Wikipedia link structure. In Proc.
of DASFAA, pages 686?689.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 13?18.
Roxana Girju. 2006. Out-of-context noun phrase se-
mantic interpretation with cross-linguistic evidence.
In CIKM ?06: Proceedings of the 15th ACM inter-
national conference on Information and knowledge
management, pages 268?276.
Aurelie Herbelot and Ann Copestake. 2006. Acquir-
ing ontological relationships from Wikipedia using
RMRS. In Proc. of the ISWC 2006 Workshop on
Web Content Mining with Human Language Tech-
nologies.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 698?707.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proc. of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 343?351.
MeCab. 2008. MeCab: Yet another part-of-speech
and morphological analyzer. http://mecab.
sourceforge.net/.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 207?214.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition. In
Proc. of AAAI 08, pages 1219?1224.
Maria Ruiz-casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic extraction of semantic
relationships for Wordnet by means of pattern learn-
ing from Wikipedia. In Proc. of NLDB, pages 67?
79. Springer Verlag.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of the 16th international conference
on World Wide Web, pages 697?706.
Asuka Sumida and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition. In
Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883?888, January.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in Wikipedia. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation.
TinySVM. 2002. http://chasen.org/
?
taku/
software/TinySVM.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In CIKM ?07: Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management, pages 41?
50.
440
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513?521,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Error-Driven Word-Character Hybrid Model
for Joint Chinese Word Segmentation and POS Tagging
Canasai Kruengkrai?? and Kiyotaka Uchimoto? and Jun?ichi Kazama?
Yiou Wang? and Kentaro Torisawa? and Hitoshi Isahara??
?Graduate School of Engineering, Kobe University
1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp
Abstract
In this paper, we present a discriminative
word-character hybrid model for joint Chi-
nese word segmentation and POS tagging.
Our word-character hybrid model offers
high performance since it can handle both
known and unknown words. We describe
our strategies that yield good balance for
learning the characteristics of known and
unknown words and propose an error-
driven policy that delivers such balance
by acquiring examples of unknown words
from particular errors in a training cor-
pus. We describe an efficient framework
for training our model based on the Mar-
gin Infused Relaxed Algorithm (MIRA),
evaluate our approach on the Penn Chinese
Treebank, and show that it achieves supe-
rior performance compared to the state-of-
the-art approaches reported in the litera-
ture.
1 Introduction
In Chinese, word segmentation and part-of-speech
(POS) tagging are indispensable steps for higher-
level NLP tasks. Word segmentation and POS tag-
ging results are required as inputs to other NLP
tasks, such as phrase chunking, dependency pars-
ing, and machine translation. Word segmenta-
tion and POS tagging in a joint process have re-
ceived much attention in recent research and have
shown improvements over a pipelined fashion (Ng
and Low, 2004; Nakagawa and Uchimoto, 2007;
Zhang and Clark, 2008; Jiang et al, 2008a; Jiang
et al, 2008b).
In joint word segmentation and the POS tag-
ging process, one serious problem is caused by
unknown words, which are defined as words that
are not found in a training corpus or in a sys-
tem?s word dictionary1. The word boundaries and
the POS tags of unknown words, which are very
difficult to identify, cause numerous errors. The
word-character hybrid model proposed by Naka-
gawa and Uchimoto (Nakagawa, 2004; Nakagawa
and Uchimoto, 2007) shows promising properties
for solving this problem. However, it suffers from
structural complexity. Nakagawa (2004) described
a training method based on a word-based Markov
model and a character-based maximum entropy
model that can be completed in a reasonable time.
However, this training method is limited by the
generatively-trained Markov model in which in-
formative features are hard to exploit.
In this paper, we overcome such limitations
concerning both efficiency and effectiveness. We
propose a new framework for training the word-
character hybrid model based on the Margin
Infused Relaxed Algorithm (MIRA) (Crammer,
2004; Crammer et al, 2005; McDonald, 2006).
We describe k-best decoding for our hybrid model
and design its loss function and the features appro-
priate for our task.
In our word-character hybrid model, allowing
the model to learn the characteristics of both
known and unknown words is crucial to achieve
optimal performance. Here, we describe our
strategies that yield good balance for learning
these two characteristics. We propose an error-
driven policy that delivers this balance by acquir-
ing examples of unknown words from particular
errors in a training corpus. We conducted our ex-
periments on Penn Chinese Treebank (Xia et al,
2000) and compared our approach with the best
previous approaches reported in the literature. Ex-
perimental results indicate that our approach can
achieve state-of-the-art performance.
1A system?s word dictionary usually consists of a word
list, and each word in the list has its own POS category. In
this paper, we constructed the system?s word dictionary from
a training corpus.
513
Figure 1: Lattice used in word-character hybrid model.
Tag Description
B Beginning character in a multi-character word
I Intermediate character in a multi-character word
E End character in a multi-character word
S Single-character word
Table 1: Position-of-character (POC) tags.
The paper proceeds as follows: Section 2 gives
background on the word-character hybrid model,
Section 3 describes our policies for correct path
selection, Section 4 presents our training method
based on MIRA, Section 5 shows our experimen-
tal results, Section 6 discusses related work, and
Section 7 concludes the paper.
2 Background
2.1 Problem formation
In joint word segmentation and the POS tag-
ging process, the task is to predict a path
of word hypotheses y = (y1, . . . , y#y) =
(?w1, p1?, . . . , ?w#y, p#y?) for a given character
sequence x = (c1, . . . , c#x), where w is a word,
p is its POS tag, and a ?#? symbol denotes the
number of elements in each variable. The goal of
our learning algorithm is to learn a mapping from
inputs (unsegmented sentences) x ? X to outputs
(segmented paths) y ? Y based on training sam-
ples of input-output pairs S = {(xt, yt)}Tt=1.
2.2 Search space representation
We represent the search space with a lattice based
on the word-character hybrid model (Nakagawa
and Uchimoto, 2007). In the hybrid model,
given an input sentence, a lattice that consists
of word-level and character-level nodes is con-
structed. Word-level nodes, which correspond to
words found in the system?s word dictionary, have
regular POS tags. Character-level nodes have spe-
cial tags where position-of-character (POC) and
POS tags are combined (Asahara, 2003; Naka-
gawa, 2004). POC tags indicate the word-internal
positions of the characters, as described in Table 1.
Figure 1 shows an example of a lattice for a Chi-
nese sentence: ? ? (Chongming is
China?s third largest island). Note that some nodes
and state transitions are not allowed. For example,
I and E nodes cannot occur at the beginning of the
lattice (marked with dashed boxes), and the transi-
tions from I to B nodes are also forbidden. These
nodes and transitions are ignored during the lattice
construction processing.
In the training phase, since several paths
(marked in bold) can correspond to the correct
analysis in the annotated corpus, we need to se-
lect one correct path yt as a reference for training.2
The next section describes our strategies for deal-
ing with this issue.
With this search space representation, we
can consistently handle unknown words with
character-level nodes. In other words, we use
word-level nodes to identify known words and
character-level nodes to identify unknown words.
In the testing phase, we can use a dynamic pro-
gramming algorithm to search for the most likely
path out of all candidate paths.
2A machine learning problem exists called structured
multi-label classification that allows training from multiple
correct paths. However, in this paper we limit our considera-
tion to structured single-label classification, which is simple
yet provides great performance.
514
3 Policies for correct path selection
In this section, we describe our strategies for se-
lecting the correct path yt in the training phase.
As shown in Figure 1, the paths marked in bold
can represent the correct annotation of the seg-
mented sentence. Ideally, we need to build a word-
character hybrid model that effectively learns the
characteristics of unknown words (with character-
level nodes) as well as those of known words (with
word-level nodes).
We can directly estimate the statistics of known
words from an annotated corpus where a sentence
is already segmented into words and assigned POS
tags. If we select the correct path yt that corre-
sponds to the annotated sentence, it will only con-
sist of word-level nodes that do not allow learning
for unknown words. We therefore need to choose
character-level nodes as correct nodes instead of
word-level nodes for some words. We expect that
those words could reflect unknown words in the
future.
Baayen and Sproat (1996) proposed that the
characteristics of infrequent words in a training
corpus resemble those of unknown words. Their
idea has proven effective for estimating the statis-
tics of unknown words in previous studies (Ratna-
parkhi, 1996; Nagata, 1999; Nakagawa, 2004).
We adopt Baayen and Sproat?s approach as
the baseline policy in our word-character hybrid
model. In the baseline policy, we first count the
frequencies of words3 in the training corpus. We
then collect infrequent words that appear less than
or equal to r times.4 If these infrequent words are
in the correct path, we use character-level nodes
to represent them, and hence the characteristics of
unknown words can be learned. For example, in
Figure 1 we select the character-level nodes of the
word ? ? (Chongming) as the correct nodes. As
a result, the correct path yt can contain both word-
level and character-level nodes (marked with as-
terisks (*)).
To discover more statistics of unknown words,
one might consider just increasing the threshold
value r to obtain more artificial unknown words.
However, our experimental results indicate that
our word-character hybrid model requires an ap-
propriate balance between known and artificial un-
3We consider a word and its POS tag a single entry.
4In our experiments, the optimal threshold value r is se-
lected by evaluating the performance of joint word segmen-
tation and POS tagging on the development set.
known words to achieve optimal performance.
We now describe our new approach to lever-
age additional examples of unknown words. In-
tuition suggests that even though the system can
handle some unknown words, many unidentified
unknown words remain that cannot be recovered
by the system; we wish to learn the characteristics
of such unidentified unknown words. We propose
the following simple scheme:
? Divide the training corpus into ten equal sets
and perform 10-fold cross validation to find
the errors.
? For each trial, train the word-character hybrid
model with the baseline policy (r = 1) us-
ing nine sets and estimate errors using the re-
maining validation set.
? Collect unidentified unknown words from
each validation set.
Several types of errors are produced by the
baseline model, but we only focus on those caused
by unidentified unknown words, which can be eas-
ily collected in the evaluation process. As de-
scribed later in Section 5.2, we measure the recall
on out-of-vocabulary (OOV) words. Here, we de-
fine unidentified unknown words as OOV words
in each validation set that cannot be recovered by
the system. After ten cross validation runs, we
get a list of the unidentified unknown words de-
rived from the whole training corpus. Note that
the unidentified unknown words in the cross val-
idation are not necessary to be infrequent words,
but some overlap may exist. Finally, we obtain the
artificial unknown words that combine the uniden-
tified unknown words in cross validation and in-
frequent words for learning unknown words. We
refer to this approach as the error-driven policy.
4 Training method
4.1 Discriminative online learning
Let Yt = {y1t , . . . , yKt } be a lattice consisting of
candidate paths for a given sentence xt. In the
word-character hybrid model, the lattice Yt can
contain more than 1000 nodes, depending on the
length of the sentence xt and the number of POS
tags in the corpus. Therefore, we require a learn-
ing algorithm that can efficiently handle large and
complex lattice structures.
Online learning is an attractive method for
the hybrid model since it quickly converges
515
Algorithm 1 Generic Online Learning Algorithm
Input: Training set S = {(xt, yt)}Tt=1
Output: Model weight vector w
1: w(0) = 0;v = 0; i = 0
2: for iter = 1 to N do
3: for t = 1 to T do
4: w(i+1) = update w(i) according to (xt, yt)
5: v = v +w(i+1)
6: i = i+ 1
7: end for
8: end for
9: w = v/(N ? T )
within a few iterations (McDonald, 2006). Algo-
rithm 1 outlines the generic online learning algo-
rithm (McDonald, 2006) used in our framework.
4.2 k-best MIRA
We focus on an online learning algorithm called
MIRA (Crammer, 2004), which has the de-
sired accuracy and scalability properties. MIRA
combines the advantages of margin-based and
perceptron-style learning with an optimization
scheme. In particular, we use a generalized ver-
sion of MIRA (Crammer et al, 2005; McDonald,
2006) that can incorporate k-best decoding in the
update procedure. To understand the concept of k-
best MIRA, we begin with a linear score function:
s(x, y;w) = ?w, f(x, y)? , (1)
where w is a weight vector and f is a feature rep-
resentation of an input x and an output y.
Learning a mapping between an input-output
pair corresponds to finding a weight vector w such
that the best scoring path of a given sentence is
the same as (or close to) the correct path. Given
a training example (xt, yt), MIRA tries to estab-
lish a margin between the score of the correct path
s(xt, yt;w) and the score of the best candidate
path s(xt, y?;w) based on the current weight vector
w that is proportional to a loss function L(yt, y?).
In each iteration, MIRA updates the weight vec-
tor w by keeping the norm of the change in the
weight vector as small as possible. With this
framework, we can formulate the optimization
problem as follows (McDonald, 2006):
w(i+1) = argminw?w ?w(i)? (2)
s.t. ?y? ? bestk(xt;w(i)) :
s(xt, yt;w)? s(xt, y?;w) ? L(yt, y?) ,
where bestk(xt;w(i)) ? Yt represents a set of top
k-best paths given the weight vector w(i). The
above quadratic programming (QP) problem can
be solved using Hildreth?s algorithm (Yair Cen-
sor, 1997). Replacing Eq. (2) into line 4 of Al-
gorithm 1, we obtain k-best MIRA.
The next question is how to efficiently gener-
ate bestk(xt;w(i)). In this paper, we apply a dy-
namic programming search (Nagata, 1994) to k-
best MIRA. The algorithm has two main search
steps: forward and backward. For the forward
search, we use Viterbi-style decoding to find the
best partial path and its score up to each node in
the lattice. For the backward search, we use A?-
style decoding to generate the top k-best paths. A
complete path is found when the backward search
reaches the beginning node of the lattice, and the
algorithm terminates when the number of gener-
ated paths equals k.
In summary, we use k-best MIRA to iteratively
update w(i). The final weight vector w is the av-
erage of the weight vectors after each iteration.
As reported in (Collins, 2002; McDonald et al,
2005), parameter averaging can effectively avoid
overfitting. For inference, we can use Viterbi-style
decoding to search for the most likely path y? for
a given sentence x where:
y? = argmax
y?Y
s(x, y;w) . (3)
4.3 Loss function
In conventional sequence labeling where the ob-
servation sequence (word) boundaries are fixed,
one can use the 0/1 loss to measure the errors of
a predicted path with respect to the correct path.
However, in our model, word boundaries vary
based on the considered path, resulting in a dif-
ferent numbers of output tokens. As a result, we
cannot directly use the 0/1 loss.
We instead compute the loss function through
false positives (FP ) and false negatives (FN ).
Here, FP means the number of output nodes that
are not in the correct path, and FN means the
number of nodes in the correct path that cannot
be recognized by the system. We define the loss
function by:
L(yt, y?) = FP + FN . (4)
This loss function can reflect how bad the pre-
dicted path y? is compared to the correct path yt.
A weighted loss function based on FP and FN
can be found in (Ganchev et al, 2007).
516
ID Template Condition
W0 ?w0? for word-level
W1 ?p0? nodes
W2 ?w0, p0?
W3 ?Length(w0), p0?
A0 ?AS(w0)? if w0 is a single-
A1 ?AS(w0), p0? character word
A2 ?AB(w0)? for word-level
A3 ?AB(w0), p0? nodes
A4 ?AE(w0)?
A5 ?AE(w0), p0?
A6 ?AB(w0), AE(w0)?
A7 ?AB(w0), AE(w0), p0?
T0 ?TS(w0)? if w0 is a single-
T1 ?TS(w0), p0? character word
T2 ?TB(w0)? for word-level
T3 ?TB(w0), p0? nodes
T4 ?TE(w0)?
T5 ?TE(w0), p0?
T6 ?TB(w0), TE(w0)?
T7 ?TB(w0), TE(w0), p0?
C0 ?cj?, j ? [?2, 2] ? p0 for character-
C1 ?cj , cj+1?, j ? [?2, 1] ? p0 level nodes
C2 ?c?1, c1? ? p0
C3 ?T (cj)?, j ? [?2, 2] ? p0
C4 ?T (cj), T (cj+1)?, j ? [?2, 1] ? p0
C5 ?T (c?1), T (c1)? ? p0
C6 ?c0, T (c0)? ? p0
Table 2: Unigram features.
4.4 Features
This section discusses the structure of f(x, y). We
broadly classify features into two categories: uni-
gram and bigram features. We design our feature
templates to capture various levels of information
about words and POS tags. Let us introduce some
notation. We write w?1 and w0 for the surface
forms of words, where subscripts ?1 and 0 in-
dicate the previous and current positions, respec-
tively. POS tags p?1 and p0 can be interpreted in
the same way. We denote the characters by cj .
Unigram features: Table 2 shows our unigram
features. Templates W0?W3 are basic word-level
unigram features, where Length(w0) denotes the
length of the word w0. Using just the surface
forms can overfit the training data and lead to poor
predictions on the test data. To alleviate this prob-
lem, we use two generalized features of the sur-
face forms. The first is the beginning and end
characters of the surface (A0?A7). For example,
?AB(w0)? denotes the beginning character of the
current word w0, and ?AB(w0), AE(w0)? denotes
the beginning and end characters in the word. The
second is the types of beginning and end charac-
ters of the surface (T0?T7). We define a set of
general character types, as shown in Table 4.
Templates C0?C6 are basic character-level un-
ID Template Condition
B0 ?w?1, w0? if w?1 and w0
B1 ?p?1, p0? are word-level
B2 ?w?1, p0? nodes
B3 ?p?1, w0?
B4 ?w?1, w0, p0?
B5 ?p?1, w0, p0?
B6 ?w?1, p?1, w0?
B7 ?w?1, p?1, p0?
B8 ?w?1, p?1, w0, p0?
B9 ?Length(w?1), p0?
TB0 ?TE(w?1)?
TB1 ?TE(w?1), p0?
TB2 ?TE(w?1), p?1, p0?
TB3 ?TE(w?1), TB(w0)?
TB4 ?TE(w?1), TB(w0), p0?
TB5 ?TE(w?1), p?1, TB(w0)?
TB6 ?TE(w?1), p?1, TB(w0), p0?
CB0 ?p?1, p0? otherwise
Table 3: Bigram features.
Character type Description
Space Space
Numeral Arabic and Chinese numerals
Symbol Symbols
Alphabet Alphabets
Chinese Chinese characters
Other Others
Table 4: Character types.
igram features taken from (Nakagawa, 2004).
These templates operate over a window of ?2
characters. The features include characters (C0),
pairs of characters (C1?C2), character types (C3),
and pairs of character types (C4?C5). In addi-
tion, we add pairs of characters and character types
(C6).
Bigram features: Table 3 shows our bigram
features. Templates B0-B9 are basic word-
level bigram features. These features aim to
capture all the possible combinations of word
and POS bigrams. Templates TB0-TB6 are the
types of characters for bigrams. For example,
?TE(w?1), TB(w0)? captures the change of char-
acter types from the end character in the previ-
ous word to the beginning character in the current
word.
Note that if one of the adjacent nodes is a
character-level node, we use the template CB0 that
represents POS bigrams. In our preliminary ex-
periments, we found that if we add more features
to non-word-level bigrams, the number of features
grows rapidly due to the dense connections be-
tween non-word-level nodes. However, these fea-
tures only slightly improve performance over us-
ing simple POS bigrams.
517
(a) Experiments on small training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270 3,046 75,169
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 32
OOV (word) 0.0987 (790/8,008)
OOV (word & POS) 0.1140 (913/8,008)
(b) Experiments on large training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270, 18,089 493,939
400-931,
1001-1151
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 35
OOV (word) 0.0347 (278/8,008)
OOV (word & POS) 0.0420 (336/8,008)
Table 5: Training, development, and test data
statistics on CTB 5.0 used in our experiments.
5 Experiments
5.1 Data sets
Previous studies on joint Chinese word segmen-
tation and POS tagging have used Penn Chinese
Treebank (CTB) (Xia et al, 2000) in experiments.
However, versions of CTB and experimental set-
tings vary across different studies.
In this paper, we used CTB 5.0 (LDC2005T01)
as our main corpus, defined the training, develop-
ment and test sets according to (Jiang et al, 2008a;
Jiang et al, 2008b), and designed our experiments
to explore the impact of the training corpus size on
our approach. Table 5 provides the statistics of our
experimental settings on the small and large train-
ing data. The out-of-vocabulary (OOV) is defined
as tokens in the test set that are not in the train-
ing set (Sproat and Emerson, 2003). Note that the
development set was only used for evaluating the
trained model to obtain the optimal values of tun-
able parameters.
5.2 Evaluation
We evaluated both word segmentation (Seg) and
joint word segmentation and POS tagging (Seg
& Tag). We used recall (R), precision (P ), and
F1 as evaluation metrics. Following (Sproat and
Emerson, 2003), we also measured the recall on
OOV (ROOV) tokens and in-vocabulary (RIV) to-
kens. These performance measures can be calcu-
lated as follows:
Recall (R) = # of correct tokens# of tokens in test data
Precision (P ) = # of correct tokens# of tokens in system output
F1 = 2 ?R ? PR+ P
ROOV = # of correct OOV tokens# of OOV tokens in test data
RIV = # of correct IV tokens# of IV tokens in test data
For Seg, a token is considered to be a cor-
rect one if the word boundary is correctly iden-
tified. For Seg & Tag, both the word boundary and
its POS tag have to be correctly identified to be
counted as a correct token.
5.3 Parameter estimation
Our model has three tunable parameters: the num-
ber of training iterations N ; the number of top
k-best paths; and the threshold r for infrequent
words. Since we were interested in finding an
optimal combination of word-level and character-
level nodes for training, we focused on tuning r.
We fixed N = 10 and k = 5 for all experiments.
For the baseline policy, we varied r in the range
of [1, 5] and found that setting r = 3 yielded the
best performance on the development set for both
the small and large training corpus experiments.
For the error-driven policy, we collected unidenti-
fied unknown words using 10-fold cross validation
on the training set, as previously described in Sec-
tion 3.
5.4 Impact of policies for correct path
selection
Table 6 shows the results of our word-character
hybrid model using the error-driven and baseline
policies. The third and fourth columns indicate the
numbers of known and artificial unknown words
in the training phase. The total number of words
is the same, but the different policies yield differ-
ent balances between the known and artificial un-
known words for learning the hybrid model. Op-
timal balances were selected using the develop-
ment set. The error-driven policy provides addi-
tional artificial unknown words in the training set.
The error-driven policy can improve ROOV as well
as maintain good RIV, resulting in overall F1 im-
provements.
518
(a) Experiments on small training corpus
# of words in training (75,169)
Eval type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 63,997 11,172 0.9587 0.9509 0.9548 0.7557 0.9809baseline 64,999 10,170 0.9572 0.9489 0.9530 0.7304 0.9820
Seg & Tag error-driven 63,997 11,172 0.8929 0.8857 0.8892 0.5444 0.9377baseline 64,999 10,170 0.8897 0.8820 0.8859 0.5246 0.9367
(b) Experiments on large training corpus
# of words in training (493,939)
Eval Type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 442,423 51,516 0.9829 0.9746 0.9787 0.7698 0.9906baseline 449,679 44,260 0.9821 0.9736 0.9779 0.7590 0.9902
Seg & Tag error-driven 442,423 51,516 0.9407 0.9328 0.9367 0.5982 0.9557baseline 449,679 44,260 0.9401 0.9319 0.9360 0.5952 0.9552
Table 6: Results of our word-character hybrid model using error-driven and baseline policies.
Method Seg Seg & Tag
Ours (error-driven) 0.9787 0.9367
Ours (baseline) 0.9779 0.9360
Jiang08a 0.9785 0.9341
Jiang08b 0.9774 0.9337
N&U07 0.9783 0.9332
Table 7: Comparison of F1 results with previous
studies on CTB 5.0.
Seg Seg & Tag
N&U07 Z&C08 Ours N&U07 Z&C08 Ours
Trial (base.) (base.)
1 0.9701 0.9721 0.9732 0.9262 0.9346 0.9358
2 0.9738 0.9762 0.9752 0.9318 0.9385 0.9380
3 0.9571 0.9594 0.9578 0.9023 0.9086 0.9067
4 0.9629 0.9592 0.9655 0.9132 0.9160 0.9223
5 0.9597 0.9606 0.9617 0.9132 0.9172 0.9187
6 0.9473 0.9456 0.9460 0.8823 0.8883 0.8885
7 0.9528 0.9500 0.9562 0.9003 0.9051 0.9076
8 0.9519 0.9512 0.9528 0.9002 0.9030 0.9062
9 0.9566 0.9479 0.9575 0.8996 0.9033 0.9052
10 0.9631 0.9645 0.9659 0.9154 0.9196 0.9225
Avg. 0.9595 0.9590 0.9611 0.9085 0.9134 0.9152
Table 8: Comparison of F1 results of our baseline
model with Nakagawa and Uchimoto (2007) and
Zhang and Clark (2008) on CTB 3.0.
Method Seg Seg & Tag
Ours (baseline) 0.9611 0.9152
Z&C08 0.9590 0.9134
N&U07 0.9595 0.9085
N&L04 0.9520 -
Table 9: Comparison of averaged F1 results (by
10-fold cross validation) with previous studies on
CTB 3.0.
5.5 Comparison with best prior approaches
In this section, we attempt to make meaning-
ful comparison with the best prior approaches re-
ported in the literature. Although most previous
studies used CTB, their versions of CTB and ex-
perimental settings are different, which compli-
cates comparison.
Ng and Low (2004) (N&L04) used CTB 3.0.
However, they just showed POS tagging results
on a per character basis, not on a per word basis.
Zhang and Clark (2008) (Z&C08) generated CTB
3.0 from CTB 4.0. Jiang et al (2008a; 2008b)
(Jiang08a, Jiang08b) used CTB 5.0. Shi and
Wang (2007) used CTB that was distributed in the
SIGHAN Bakeoff. Besides CTB, they also used
HowNet (Dong and Dong, 2006) to obtain seman-
tic class features. Zhang and Clark (2008) indi-
cated that their results cannot directly compare to
the results of Shi and Wang (2007) due to different
experimental settings.
We decided to follow the experimental settings
of Jiang et al (2008a; 2008b) on CTB 5.0 and
Zhang and Clark (2008) on CTB 4.0 since they
reported the best performances on joint word seg-
mentation and POS tagging using the training ma-
terials only derived from the corpora. The perfor-
mance scores of previous studies are directly taken
from their papers. We also conducted experiments
using the system implemented by Nakagawa and
Uchimoto (2007) (N&U07) for comparison.
Our experiment on the large training corpus is
identical to that of Jiang et al (Jiang et al, 2008a;
Jiang et al, 2008b). Table 7 compares the F1 re-
sults with previous studies on CTB 5.0. The result
of our error-driven model is superior to previous
reported results for both Seg and Seg & Tag, and
the result of our baseline model compares favor-
ably to the others.
Following Zhang and Clark (2008), we first
generated CTB 3.0 from CTB 4.0 using sentence
IDs 1?10364. We then divided CTB 3.0 into
ten equal sets and conducted 10-fold cross vali-
519
dation. Unfortunately, Zhang and Clark?s exper-
imental setting did not allow us to use our error-
driven policy since performing 10-fold cross val-
idation again on each main cross validation trial
is computationally too expensive. Therefore, we
used our baseline policy in this setting and fixed
r = 3 for all cross validation runs. Table 8 com-
pares the F1 results of our baseline model with
Nakagawa and Uchimoto (2007) and Zhang and
Clark (2008) on CTB 3.0. Table 9 shows a sum-
mary of averaged F1 results on CTB 3.0. Our
baseline model outperforms all prior approaches
for both Seg and Seg & Tag, and we hope that
our error-driven model can further improve perfor-
mance.
6 Related work
In this section, we discuss related approaches
based on several aspects of learning algorithms
and search space representation methods. Max-
imum entropy models are widely used for word
segmentation and POS tagging tasks (Uchimoto
et al, 2001; Ng and Low, 2004; Nakagawa,
2004; Nakagawa and Uchimoto, 2007) since they
only need moderate training times while they pro-
vide reasonable performance. Conditional random
fields (CRFs) (Lafferty et al, 2001) further im-
prove the performance (Kudo et al, 2004; Shi
and Wang, 2007) by performing whole-sequence
normalization to avoid label-bias and length-bias
problems. However, CRF-based algorithms typ-
ically require longer training times, and we ob-
served an infeasible convergence time for our hy-
brid model.
Online learning has recently gained popularity
for many NLP tasks since it performs comparably
or better than batch learning using shorter train-
ing times (McDonald, 2006). For example, a per-
ceptron algorithm is used for joint Chinese word
segmentation and POS tagging (Zhang and Clark,
2008; Jiang et al, 2008a; Jiang et al, 2008b).
Another potential algorithm is MIRA, which in-
tegrates the notion of the large-margin classifier
(Crammer, 2004). In this paper, we first intro-
duce MIRA to joint word segmentation and POS
tagging and show very encouraging results. With
regard to error-driven learning, Brill (1995) pro-
posed a transformation-based approach that ac-
quires a set of error-correcting rules by comparing
the outputs of an initial tagger with the correct an-
notations on a training corpus. Our approach does
not learn the error-correcting rules. We only aim to
capture the characteristics of unknown words and
augment their representatives.
As for search space representation, Ng and
Low (2004) found that for Chinese, the character-
based model yields better results than the word-
based model. Nakagawa and Uchimoto (2007)
provided empirical evidence that the character-
based model is not always better than the word-
based model. They proposed a hybrid approach
that exploits both the word-based and character-
based models. Our approach overcomes the limi-
tation of the original hybrid model by a discrimi-
native online learning algorithm for training.
7 Conclusion
In this paper, we presented a discriminative word-
character hybrid model for joint Chinese word
segmentation and POS tagging. Our approach
has two important advantages. The first is ro-
bust search space representation based on a hy-
brid model in which word-level and character-
level nodes are used to identify known and un-
known words, respectively. We introduced a sim-
ple scheme based on the error-driven concept to
effectively learn the characteristics of known and
unknown words from the training corpus. The sec-
ond is a discriminative online learning algorithm
based on MIRA that enables us to incorporate ar-
bitrary features to our hybrid model. Based on ex-
tensive comparisons, we showed that our approach
is superior to the existing approaches reported in
the literature. In future work, we plan to apply
our framework to other Asian languages, includ-
ing Thai and Japanese.
Acknowledgments
We would like to thank Tetsuji Nakagawa for his
helpful suggestions about the word-character hy-
brid model, Chen Wenliang for his technical assis-
tance with the Chinese processing, and the anony-
mous reviewers for their insightful comments.
References
Masayuki Asahara. 2003. Corpus-based Japanese
morphological analysis. Nara Institute of Science
and Technology, Doctor?s Thesis.
Harald Baayen and Richard Sproat. 1996. Estimat-
ing lexical priors for low-frequency morphologi-
cally ambiguous forms. Computational Linguistics,
22(2):155?166.
520
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Koby Crammer, Ryan McDonald, and Fernando
Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. In NIPS Workshop
on Learning With Structured Outputs.
Koby Crammer. 2004. Online Learning of Com-
plex Categorial Problems. Hebrew Univeristy of
Jerusalem, PhD Thesis.
Zhendong Dong and Qiang Dong. 2006. Hownet and
the Computation of Meaning. World Scientific.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/umass/chop biocreative ii systems. In Pro-
ceedings of the Second BioCreative Challenge Eval-
uation Workshop.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ryan McDonald, Femando Pereira, Kiril Ribarow, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523?530.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
University of Pennsylvania, PhD Thesis.
Masaki Nagata. 1994. A stochastic japanese mor-
phological analyzer using a forward-DP backward-
A* n-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201?207.
Masaki Nagata. 1999. A part of speech estimation
method for japanese unknown words using a statis-
tical model of morphology and context. In Proceed-
ings of ACL, pages 277?284.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sions.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level
information. In Proceedings of COLING, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, pages 277?284.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of IJ-
CAI.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing, pages 133?143.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of japanese using maximum entropy
aided by a dictionary. In Proceedings of EMNLP,
pages 91?99.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu dong Chiou, and
Shizhe Huang. 2000. Developing guidelines and
ensuring consistency for chinese text annotation. In
Proceedings of LREC.
Stavros A. Zenios Yair Censor. 1997. Parallel Op-
timization: Theory, Algorithms, and Applications.
Oxford University Press.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging on a single perceptron. In
Proceedings of ACL.
521
Multilingual Aligned Parallel Treebank Corpus Reflecting
Contextual Information and Its Applications
Kiyotaka Uchimoto? Yujie Zhang? Kiyoshi Sudo?
Masaki Murata? Satoshi Sekine? Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{uchimoto,yujie,murata,isahara}@nict.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
{sudo,sekine}@cs.nyu.edu
Abstract
This paper describes Japanese-English-Chinese
aligned parallel treebank corpora of newspaper
articles. They have been constructed by trans-
lating each sentence in the Penn Treebank and
the Kyoto University text corpus into a cor-
responding natural sentence in a target lan-
guage. Each sentence is translated so as to
reflect its contextual information and is anno-
tated with morphological and syntactic struc-
tures and phrasal alignment. This paper also
describes the possible applications of the par-
allel corpus and proposes a new framework to
aid in translation. In this framework, paral-
lel translations whose source language sentence
is similar to a given sentence can be semi-
automatically generated. In this paper we show
that the framework can be achieved by using
our aligned parallel treebank corpus.
1 Introduction
Recently, accurate machine translation systems
can be constructed by using parallel corpora
(Och and Ney, 2000; Germann et al, 2001).
However, almost all existing machine transla-
tion systems do not consider the problem of
translating a given sentence into a natural sen-
tence reflecting its contextual information in the
target language. One of the main reasons for
this is that we had many problems that had to
be solved by one-sentence to one-sentence ma-
chine translation before we could solve the con-
textual problem. Another reason is that it was
difficult to simply investigate the influence of
the context on the translation because sentence
correspondences of the existing bilingual doc-
uments are rarely one-to-one, and are usually
one-to-many or many-to-many.
On the other hand, high-quality treebanks
such as the Penn Treebank (Marcus et al, 1993)
and the Kyoto University text corpus (Kuro-
hashi and Nagao, 1997) have contributed to
improving the accuracies of fundamental tech-
niques for natural language processing such as
morphological analysis and syntactic structure
analysis. However, almost all of these high-
quality treebanks are based on monolingual cor-
pora and do not have bilingual or multilin-
gual information. There are few high-quality
bilingual or multilingual treebank corpora be-
cause parallel corpora have mainly been actively
used for machine translation between related
languages such as English and French, there-
fore their syntactic structures are not required
so much for aligning words or phrases. How-
ever, syntactic structures are necessary for ma-
chine translation between languages whose syn-
tactic structures are different from each other,
such as in Japanese-English, Japanese-Chinese,
and Chinese-English machine translations, be-
cause it is more difficult to automatically align
words or phrases between two unrelated lan-
guages than between two related languages. Ac-
tually, it has been reported that syntactic struc-
tures contribute to improving the accuracy of
word alignment between Japanese and English
(Yamada and Knight, 2001). Therefore, if we
had a high-quality parallel treebank corpus, the
accuracies of machine translation between lan-
guages whose syntactic structures are differ-
ent from each other would improve. Further-
more, if the parallel treebank corpus had word
or phrase alignment, the accuracy of automatic
word or phrase alignment would increase by
using the parallel treebank corpus as training
data. However, so far, there is no aligned par-
allel treebank corpus whose domain is not re-
stricted. For example, the Japanese Electronics
Industry Development Association?s (JEIDA?s)
bilingual corpus (Isahara and Haruno, 2000)
has sentence, phrase, and proper noun align-
ment. However, it does not have morphologi-
cal and syntactic information, the alignment is
partial, and the target is restricted to a white
paper. The Advance Telecommunications Re-
search dialogue database (ATR, 1992) is a par-
allel treebank corpus between Japanese and En-
glish. However, it does not have word or phrase
alignment, and the target domain is restricted
to travel conversation.
Therefore, we have been constructing aligned
parallel treebank corpora of newspaper articles
between languages whose syntactic structures
are different from each other since 2001; they
meet the following conditions.
1. It is easy to investigate the influence of the con-
text on the translation, which means the sen-
tences that come before and after a particular
sentence, and that help us to understand the
meaning of a particular word such as a pro-
noun.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. They are open to the public.
To construct parallel corpora that satisfy these
conditions, each sentence in the Penn Tree-
bank (Release 2) and the Kyoto University text
corpus (Version 3.0) has been translated into
a corresponding natural sentence reflecting its
contextual information in a target language by
skilled translators, revised by native speakers,
and each parallel translation has been anno-
tated with morphological and syntactic struc-
tures, and phrasal alignment. Henceforth, we
call the parallel corpus that is constructed by
pursuing the above policy an aligned parallel
treebank corpus reflecting contextual informa-
tion. In this paper, we describe an aligned par-
allel treebank corpus of newspaper articles be-
tween Japanese, English, and Chinese, and its
applications.
2 Construction of Aligned Parallel
Treebank Corpus Reflecting
Contextual Information
2.1 Human Translation of Existing
Monolingual Treebank
The Penn Treebank is a tagged corpus of Wall
Street Journal material, and it is divided into 24
sections. The Kyoto University text corpus is a
tagged corpus of the Mainichi newspaper, which
is divided into 16 sections according to the cat-
egories of articles such as the sports section and
the economy section. To maintain the consis-
tency of expressions in translation, a few partic-
ular translators were assigned to translate arti-
cles in a particular section, and the same trans-
lator was assigned to the same section. The
instructions to translators for Japanese-English
translation is basically as follows.
1. One-sentence to one-sentence translation as a
rule
Translate a source sentence into a target sen-
tence. In case the translated sentence becomes
unnatural by pursuing this policy, leave a com-
ment.
2. Natural translation reflecting contextual infor-
mation
Except in the case that the translated sentence
becomes unnatural by pursuing policy 1, trans-
late a source sentence into a target sentence
naturally.
By deletion, replacement, or supplementation,
let the translated sentence be natural in the
context.
In an entire article, the translated sentences
must maintain the same meaning and informa-
tion as those of the original sentences.
3. Translations of proper nouns
Find out the translations of proper nouns by
looking up the nouns in a dictionary or by using
a web search. In case a translation cannot be
found, use a temporary name and report it.
We started the construction of Japanese-
Chinese parallel corpus in 2002. The Japanese
sentences of the Kyoto University text corpus
were also translated into Chinese by human
translators. Then each translated Chinese sen-
tence was revised by a second Chinese native.
The instruction to the translators is the same
as that given in the Japanese-English human
translations.
The breakdown of the parallel corpora is
shown in Table 1. We are planning to trans-
late the remaining 18,714 sentences of the Kyoto
University text corpus and the remaining 30,890
sentences of the Penn Treebank. As for the nat-
uralness of the translated sentences, there are
207 (1%) unnatural English sentences of the
Kyoto University text corpus, and 462 (2.5%)
unnatural Japanese sentences of the Penn Tree-
bank generated by pursuing policy 1.
2.2 Morphological and Syntactic
Annotation
In the following sections, we describe the anno-
tated information of the parallel treebank cor-
pus based on the Kyoto University text corpus.
2.2.1 Morphological and Syntactic
Information of Japanese-English
corpus
Translated English sentences were analyzed by
using the Charniak Parser (Charniak, 1999).
Then, the parsed sentences were manually re-
vised. The definitions of part-of-speech (POS)
categories and syntactic labels follow those of
the Treebank I style (Marcus et al, 1993).
We have finished revising the 10,328 parsed
sentences that appeared from January 1st to
11th. An example of morphological and syn-
tactic structures is shown in Figure 1. In this
figure, ?S-ID? means the sentence ID in the
Kyoto University text corpus. EOJ means the
boundary between a Japanese parsed sentence
and an English parsed sentence. The definition
of Japanese morphological and syntactic infor-
mation follows that of the Kyoto University text
corpus (Version 3.0). The syntactic structure is
represented by dependencies between Japanese
phrasal units called bunsetsus. The phrasal
Table 1: Breakdown of the parallel corpora
Original corpus Languages # of parallel sentences
Kyoto University text corpus Japanese-English 19,669 (from Jan. 1st to 17th in 1995)
Japanese-Chinese 38,383 (all)
Penn Treebank Japanese-English 18,318 (from section 0 to 9)
Total Japanese-English 37,987 (Approximately 900,000 English words)
Japanese-Chinese 38,383 (Approximately 900,000 Chinese words)
# S-ID:950104141-008
* 0 2D
???? ???? * ?? * * *
* 1 2D
?? ?????? * ?? ?? * *
? ?? * ??? ???????? * *
?? ??? * ??? ???????? * *
? ? * ?? ???? * *
* 2 6D
?? ???? * ?? ???? * *
? ? ? ??? * ??? ????????
? ? * ?? ?? * *
* 3 4D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 4 5D
??? ???? ??? ?? * ???? ???
* 5 6D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 6 -1D
??? ???? ?? ?? * ?????? ??????
? ? ?? ??? ?????? ???? ???
?? ?? ?? ??? ????????? ???????? ???
? ? * ?? ?? * *
EOJ
(S1 (S (NP (PRP They))
(VP (VP (VBD were)
(NP (DT all))
(ADJP (NP (QP (RB about)
(CD nineteen))
(NNS years))
(JJ old)))
(CC and)
(VP (VBD had)
(S (NP (DT no)
(NN strength))
(VP (VBN left)
(SBAR (S (VP (ADVP (RB even))
(TO to)
(VP (VB answer)
(NP (NNS questions))))))))))
(. .)))
EOE
Figure 1: Example of morphological and syn-
tactic information.
units or bunsetsus are minimal linguistic units
obtained by segmenting a sentence naturally in
terms of semantics and phonetics, and each of
them consists of one or more morphemes.
2.2.2 Chinese Morphological
Information of Japanese-Chinese
corpus
Chinese sentences are composed of strings of
Hanzi and there are no spaces between words.
The morphological annotation, therefore, in-
cludes providing tags of word boundaries and
POSs of words. We analyzed the Chinese sen-
tences by using the morphological analyzer de-
veloped by Peking University (Zhou and Duan,
1994). There are 39 categories in this POS set.
Then the automatically tagged sentences were
revised by the third native Chinese. In this
pass the Chinese translations were revised again
while the results of word segmentation and POS
tagging were revised. Therefore the Chinese
translations are obtained with a high quality.
We have finished revising the 12,000 tagged sen-
tences. The revision of the remaining sentences
is ongoing. An example of tagged Chinese sen-
tences is shown in Figure 2. The letters shown
Figure 2: Example of morphological informa-
tion of Chinese corpus.
after ?/? indicate POSs. The Chinese sentence is
the translation of the Japanese sentence in Fig-
ure 1. The Chinese sentences are GB encoded.
The 38,383 translated Chinese sentences have
1,410,892 Hanzi and 926,838 words.
2.3 Phrasal Alignment
This section describes the annotated informa-
tion of 19,669 sentences of the Kyoto University
text corpus.
The minimum alignment unit should be as
small as possible, because bigger units can be
constructed from units of the minimum size.
However, we decided to define a bunsetsu as the
minimum alignment unit. One of the main rea-
sons for this is that the smaller the unit is, the
higher the human annotation cost is. Another
reason is that if we define a word or a morpheme
as a minimum alignment unit, expressions such
as post-positional particles in Japanese and arti-
cles in English often do not have alignments. To
effectively absorb those expressions and to align
as many parts as possible, we found that a big-
ger unit than a word or a morpheme is suitable
as the minimum alignment unit. We call the
minimum alignment based on bunsetsu align-
ment units the bunsetsu unit translation pair.
Bigger pairs than the bunsetsu unit translation
pairs can be automatically extracted based on
the bunsetsu unit translation pairs. We call all
of the pairs, including bunsetsu unit transla-
tion pairs, translation pairs. The bunsetsu unit
translation pairs for idiomatic expressions often
become unnatural. In this case, two or more
bunsetsu units are combined and handled as a
minimum alignment unit. The breakdown of
the bunsetsu unit translation pairs is shown in
Table 2.
Table 2: Breakdown of the bunsetsu unit trans-
lation pairs.
(1) total # of translation pairs 172,255
(2) # of different translation pairs 146,397
(3) # of Japanese expressions 110,284
(4) # of English expressions 111,111
(5) average # of English expressions 1.33
corresponding to a Japanese expression ((2)/(3))
(6) average # of Japanese expressions 1.32
corresponding to a English expression ((2)/(4))
(7) # of ambiguous Japanese expressions 15,699
(8) # of ambiguous English expressions 12,442
(9) # of bunsetsu unit translation pairs 17,719
consisting of two or more bunsetsus
An example of phrasal alignment is shown in
Figure 3. A Japanese sentence is shown from
the line after the S-ID to the EOJ. Each line
indicates a bunsetsu. Each rectangular line in-
dicates a dependency between bunsetsus. The
leftmost number in each line indicates the bun-
setsu ID. The corresponding English sentence is
shown in the next line after that of the EOJ
(End of Japanese) until the EOE (End of En-
glish). The English expressions corresponding
to each bunsetsu are tagged with the corre-
sponding bunsetsu ID such as <P id=?bunsetsu
ID?></P>. When there are two or more fig-
ures in the tag id such as id=?1,2?, it means two
or more bunsetsus are combined and handled as
a minimum alignment unit.
For example, we can extract the following
translation pairs from Figure 3.
 (J) ??? (yunyuu-ga) / ????? (kaikin-sa-reta);
(E)that had been under the ban
 (J) ??????? (beikoku-san-ringo-no); (E)of apples
imported from the U.S.
 (J) ???? (dai-ichi-bin-ga); (E)The first cargo
 (J)???????(uridasa-reta); (E)was brought to the
market.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga); (E)The first cargo / of apples im-
ported from the U.S.
# S-ID:950110003-001
1 ????????????????
2 ????????????????
3 ????????????????
4 ????????????????
5 ????????????????
6 ????????????????
7 ????????????????
8 ????????????????
9 ????????????????
10 ???????????????
11 ????????????????
EOJ
<P id="4">The first cargo</P> <P id="3">of apples
imported from the U.S.</P> <P id="1,2">that had been
under the ban</P> <P id="7">completed</P> <P id="6">
quarantine</P> <P id="7">and</P> <P id="11">was brought
to the market</P> <P id="10">for the first time</P>
<P id="5">on the 9th</P> <P id="9">at major supermarket
chain stores</P> <P id="8">in the Tokyo metropolitan
area</P> <P id="11">.</P>
EOE
Figure 3: Example of phrasal alignment.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga) /???????(uridasa-reta); (E)The
first cargo / of apples imported from the U.S. / was
brought to the market.
Here, Japanese and English expressions are
divided by the symbol ?;?, and ?/? means a
bunsetsu boundary.
An overview of the criteria of the alignment
is as follows. Align as many parts as possible,
except if a certain part is redundant. More de-
tailed criteria will be attached with our corpus
when it is open to the public.
1. Alignment of English grammatical elements
that are not expressed in Japanese
English articles, possessive pronouns, infinitive
to, and auxiliary verbs are joined with nouns
and verbs.
2. Alignment between a noun and its substitute
expression
A noun can be aligned with its substitute ex-
pression such as a pronoun.
3. Alignment of Japanese ellipses
An English expression is joined with its related
elements. For example, the English subject is
joined with its related verb.
4. Alignment of supplementary or explanatory ex-
pression in English
Supplementary or explanatory expressions in
English are joined with their related words.
? Ex.?
# S-ID:950104142-003
1 ???????????
2 ???????????
3 ???????????
4 ???????????
5 ???????????
6 ???????????
EOJ
<P id="1">The Chinese character used for "ka"</P>
has such meanings as "beautiful" and "splendid."
EOE
?"?? (ka)??? (niwa)" corresponds to
"The Chinese character used for "ka""
5. Alignment of date and time
When a Japanese noun representing date and
time is adverbial, the English preposition is
joined with the date and time.
6. Alignment of coordinate structures
When English expressions represented by ?X
(A + B)? correspond to Japanese expressions
represented by ?XA + XB?, the alignment of
X overlaps.
? Ex.?
# S-ID:950106149-005
1 ?????????????
2 ?????????????
3 ?????????????
4 ?????????????
5 ?????????????
6 ?????????????
7 ?????????????
8 ?????????????
EOJ
In the Kinki Region, disposal of wastes started
<P id="2"><P id="4"> at offshore sites of</P>
Amagasaki</P> and <P id="4">Izumiotsu</P> from
1989 and 1991 respectively.
EOE
?"??? (Amagasaki-oki) ? (de)" corresponds to
"at offshore sites of Amagasaki"
?"???? (Izumiotsu-oki) ? (de)" corresponds to
"at offshore sites of ? Izumiotsu"
3 Applications of Aligned Parallel
Treebank Corpus
3.1 Use for Evaluation of Conventional
Methods
The corpus as described in Section 2 can be
used for the evaluation of English-Japanese and
Japanese-English machine translation. We can
directly compare various methods of machine
translation by using this corpus. It can be sum-
marized as follows in terms of the characteristics
of the corpus.
One-sentence to one-sentence translation
can be simply used for the evaluation of
various methods of machine translation.
Morphological and syntactic information
can be used for the evaluation of methods
that actively use morphological and syntactic
information, such as methods for example-
based machine translation (Nagao, 1981;
Watanabe et al, 2003), or transfer-based
machine translation (Imamura, 2002).
Phrasal alignment is used for the evaluation of
automatically acquired translation knowledge
(Yamamoto and Matsumoto, 2003).
An actual comparison and evaluation is our
future work.
3.2 Analysis of Translation
One-sentence to one-sentence translation
reflects contextual information. Therefore, it
is suitable to investigate the influence of the
context on the translation. For example, we
can investigate the difference in the use of
demonstratives and pronouns between English
and Japanese. We can also investigate the
difference in the use of anaphora.
Morphological and syntactic information
and phrasal alignment can be used to investi-
gate the appropriate unit and size of transla-
tion rules and the relationship between syntac-
tic structures and phrasal alignment.
3.3 Use in Conventional Systems
One-sentence to one-sentence translation
can be used for training a statistical translation
model such as GIZA++ (Och and Ney, 2000),
which could be a strong baseline system for
machine translation.
Morphological and syntactic information
and phrasal alignment can be used to acquire
translation knowledge for example-based ma-
chine translation and transfer-based machine
translation.
In order to show what kind of units are help-
ful for example-based machine translation, we
investigated whether the Japanese sentences of
newspaper articles appearing on January 17,
1995, which we call test-set sentences, could be
translated into English sentences by using trans-
lation pairs appearing from January 1st to 16th
as a database. First, we found that only one out
of 1,234 test-set sentences agreed with one out
of 18,435 sentences in the database. Therefore,
a simple sentence search will not work well. On
the other hand, 6,659 bunsetsus out of 12,632
bunsetsus in the test-set sentences agreed with
those in the database. If words in bunsetsus are
expanded into their synonyms, the combination
of the expanded bunsetsus sets in the database
may cover the test-set sentences. Next, there-
fore, we investigated whether the Japanese test-
set sentences could be translated into English
sentences by simply combining translation pairs
appearing in the database. Given a Japanese
sentence, words were extracted from it and
translation pairs that include those words or
their synonyms, which were manually evalu-
ated, were extracted from the database. Then,
the English sentence was manually generated by
just combining English expressions in the ex-
tracted translation pairs. One hundred two rel-
atively short sentences (the average number of
bunsetsus is about 9.8) were selected as inputs.
The number of equivalent translations, which
mean that the translated sentence is grammat-
ical and has the same meaning as the source
sentence, was 9. The number of similar transla-
tions, which mean that the translated sentence
is ungrammatical, or different or wrong mean-
ings of words, tenses, and prepositions are used
in the translated sentence, was 83. The num-
ber of other translations, which mean that some
words are missing, or the meaning of the trans-
lated sentence is completely different from that
of the original sentence, was 10. For example,
the original parallel translation is as follows:
Japanese:????????????????????????
????????????????????????
English: New Party Sakigake proposed that towards the or-
dinary session, both parties found a council to dis-
cuss policy and Diet management.
Given the Japanese sentence, the translated
sentence was:
Translation:Sakigake Party suggested to set up an organiza-
tion between the two parties towards the regular
session of the Diet to discuss under the theme of
policies and the management of the Diet.
This result shows that only 9% of input sen-
tences can be translated into sentences equiv-
alent to the original ones. However, we found
that approximately 90% of input sentences can
be translated into English sentences that are
equivalent or similar to the original ones.
3.4 Similar Parallel Translation
Generation
The original aim of constructing an aligned par-
allel treebank corpus as described in Section 2 is
to achieve a new framework for translation aid
as described below.
It would be very convenient if multilingual
sentences could be generated by just writing
sentences in our mother language. Today, it
can be formally achieved by using commercial
machine translation systems. However, the au-
tomatically translated sentences are often in-
comprehensible. Therefore, we have to revise
the original and translated sentences by find-
ing and referring to parallel translation whose
source language sentence is similar to the orig-
inal one. In many cases, however, we cannot
find such similar parallel translations to the in-
put sentence. Therefore, it is difficult for users
who do not have enough knowledge of the target
languages to generate comprehensible sentences
in several languages by just searching similar
parallel translations in this way. Therefore, we
propose to generate similar parallel translations
whose source language sentence is similar to
the input sentence. We call this framework for
translation aid similar parallel translation gen-
eration.
We investigated whether the framework can
be achieved by using our aligned parallel tree-
bank corpus. As the first step of this study,
we investigated whether an appropriate parallel
translation can be generated by simply combin-
ing translation pairs extracted from our aligned
parallel treebank corpus in the following steps.
1. Extract each content word with its adjacent
function word in each bunsetsu in a given sen-
tence
2. The extracted content words and their adjacent
function words are expanded into their syn-
onyms and class words whose major and minor
POS categories are the same
3. Find translation pairs including the expanded
content words with their expanded adjacent
function words in the given sentence
4. For each bunsetsu, select a translation pair that
has similar dependency relationship to those in
the given sentence
5. Generate a parallel translation by combining
the selected translation pairs
The input sentences were randomly selected
from 102 sentences described in Section 3.3.
The above steps, except the third step, were
basically conducted manually. The Examples
of the input sentences and generated parallel
translations are shown in Figure 4.
The basic unit of translation pairs in our
aligned parallel treebank corpus is a bunsetsu,
and the basic unit in the selection of transla-
tion pairs is also a bunsetsu. One of the ad-
vantages of using a bunsetsu as a basic unit is
that a Japanese expression represented as one
of various expressions in English, or omitted in
English, such as Japanese post-positional par-
ticles, is paired with a content word. There-
fore, the translation of such an expression is ap-
propriately selected together with the transla-
tion of a content word when a certain trans-
lation pair is selected. If the translation of
such an expression was selected independently
of the translation of a content word, the com-
bination of each translation would be ungram-
matical or unnatural. Another advantage of the
basic unit, bunsetsu, is that we can easily refer
to dependency information between bunsetsus
when we select an appropriate translation pair
because the original treebank has the depen-
dency information between bunsetsus. These
advantages are utilized in the above generation
steps. For example, in the first step, a content
word ??? (kokkai, Diet session)? in the sec-
ond example in Figure 4 was extracted from the
bunsetsu ????? (tsuujo-kokkai, the ordinary
Diet session) ? (ni, case marker)?, and it was
expanded into its class word ?? (kai, meeting)?
in the second step. Then, a translation pair
?(J)??????????? (kokuren-kodomo-
no-kenri-iinkai)? (ni, case marker); (E)the UN
Committee on the Rights of the Child /(J)
?? (taishi); (E)towards? was extracted as a
translation pair in the third step. Since the
dependency between ????????????
(kokuren-kodomo-no-kenri-iinkai, the UN Com-
mittee on the Rights of the Child)? and ???
(taishi, towards)? is similar to that between ?
???? (tsuujo-kokkai, the ordinary Diet ses-
sion)? (ni, case marker)? and ??? (muke, to-
wards)? in the input sentence, this translation
pair was selected in the fourth step. Finally,
the bunsetsu ???????????? (kokuren-
kodomo-no-kenri-iinkai, the UN Committee on
the Rights of the Child) ? (ni, case marker)?
and its translation ?the UN Committee on the
Rights of the Child? was used for generation of
a parallel translation in the fifth step.
When we use the generated parallel transla-
tion for the exact translation of the input sen-
tence, we should replace ??????????
?? (kokuren-kodomo-no-kenri-iinkai)? and its
translation ?the UN Committee on the Rights
of the Child? with ????? (tsuujo-kokkai, the
ordinary Diet session)? and its translation ?the
ordinary Diet session? by consulting a bilingual
dictionary. In this example, ??? (sono)? and
?them? should also be replaced with ??? (ry-
oto)? and ?both parties?. It is easy to identify
words in the generated translation that should
be replaced with words in the input sentence
because each bunsetsu in translation pairs is al-
ready aligned. In such cases, templates such as
?[?? (kaigi)]? (ni)?? (muke)? and ?towards
[council]? can be automatically generated by
generalizing content words expanded in the sec-
ond step and their translation in the generated
translation. The average number of English ex-
pressions corresponding to a Japanese expres-
sion is 1.3 as shown in Table 2. Even when there
are two or more possible English expressions, an
appropriate English expression can be chosen
by selecting a Japanese expression by referring
to dependencies in extracted translation pairs.
Therefore, in many cases, English sentences can
be generated just by reordering the selected ex-
pressions. The English word order was esti-
mated manually in this experiment. However,
we can automatically estimate English word or-
der by using a language model or an English
surface sentence generator such as FERGUS
(Bangalore and Rambow, 2000). Unnatural or
ungrammatical parallel translations are some-
times generated in the above steps. However,
comprehensible translations can be generated
as shown in Figure 4. The biggest advantage
of this framework is that comprehensible target
sentences can be generated basically by refer-
ring only to source sentences. Although it is
costly to search and select appropriate transla-
tion pairs, we believe that human labor can be
reduced by developing a human interface. For
example, when we use a Japanese text gener-
ation system from keywords (Uchimoto et al,
2002), users should only select appropriate key-
words.
We are investigating whether or not we can
generate similar parallel translations to all of
the Japanese sentences appearing on January
17, 1995. So far, we found that we can gen-
erate similar parallel translations to 691 out of
840 sentences (the average number of bunsetsus
is about 10.3) including the 102 sentences de-
scribed in Section 3.3. We found that we could
not generate similar parallel translations to 149
out of 840 sentences.
In the proposed framework of similar paral-
lel translation generation, the language appear-
ing in a corpus corresponds to a controlled lan-
guage, and users are allowed to use only the
controlled language to write sentences in the
source language. We believe that high-quality
bilingual or multilingual documents can be gen-
erated by letting us adapt ourselves to the con-
trolled environment in this way.
4 Conclusion
This paper described aligned parallel treebank
corpora of newspaper articles between lan-
guages whose syntactic structures are different
from each other; they meet the following condi-
tions.
1. It is easy to investigate the influence of the con-
text on the translation.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. It is open to the public.
To construct parallel corpora that satisfy
these conditions, each sentence in the existing
monolingual high-quality treebanks has been
translated into a corresponding natural sentence
reflecting its contextual information in a target
language by skilled translators, and each par-
allel translation has been annotated with mor-
phological and syntactic structures and phrasal
alignment.
This paper also described the possible ap-
plications of the parallel corpus and proposed
a similar parallel translation generation frame-
work. In this framework, a parallel translation
whose source language sentence is similar to a
given sentence can be semi-automatically gen-
erated. In this paper we demonstrated that
the framework could be achieved by using our
aligned parallel treebank corpus.
In the near future, the aligned parallel tree-
bank corpora will be open to the public, and
expanded. We are planning to use the corpora
actively for machine translation, as a transla-
tion aid, and for second language learning. We
are also planning to develop automatic or semi-
automatic alignment system and an efficient in-
terface for machine translation aid.
Input sentence
(Japanese only)
???????????????????????????????????????????(Prime Minister
Murayama and Finance Minister Takemura met in the presidential office and they exchanged their
opinions, mainly on the issue of the new faction being formed by the New Democratic Union.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????
(E) Finance Minister Takemura held the meeting at the official residence to exchange views about the
formation of the new party of the New Democratic Union.
Input sentence
(Japanese only)
????????????????????????????????????????????????(New
Party Sakigake proposed that towards the ordinary session, both parties found a council to discuss policy
and Diet management.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
???????
(E) Sakigake proposed to set up an organization between them towards the UN Committee on the Rights
of the Child to discuss under the theme of policies and the management of the Diet.
Input sentence
(Japanese only)
?????????????????????????????????????????????(The meeting
was also intended to slow the movement towards the new party by the New Democratic Union, which is
trying to deepen the relationship with the New Frontier Party.)
Generated paral-
lel translation
(J) ?????????????????????????????????????????????
(E) The meeting had meanings to restrict the movement that the new party of New Democratic Union
is progressing to strengthen the coalition with The New Frontier Party.
Input sentence
(Japanese only)
?????????????????????????????????????????????????
???????(Lower House Diet Member Tatsuo Kawabata of the New Frontier Party decided on the
16th that he would hand in notification of his secession to the party on the 17th, in order to form a new
faction with Sadao Yamahana?s group.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
?????????
(E) On 16th Tatsuo Kawabata, a member of the House of Representatives of the New Frontier Party
decided to submit The notice to leave the party to the Shinsei Party on the 17th in order to establish a
new faction with Yuukichi Amano and others.
Input sentence
(Japanese only)
???????????????????????????(As for the faction name in the Upper House,
they will decide after they consider how to form a relationship with Democratic Reform Union.)
Generated paral-
lel translation
(J) ?????????????????????
(E) The name of the faction will be decided after discussing the relationship with the JTUC.
Figure 4: Example of generated similar parallel translations.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data.
References
ATR. 1992. Dialogue Database. http://www.red.atr.co.jp/
database page/taiwa.html.
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
E. Charniak. 1999. A Maximum-Entropy-Inspired Parser.
Technical Report CS-99-12.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada
2001. Fast Decoding and Optimal Decoding for Machine
Translation. In Proceedings of the ACL-EACL, pages 228?
235.
K. Imamura. 2002. Application of translation knowledge ac-
quired by hierarchical phrase alignment for pattern-based
MT. In Proceedings of the TMI, pages 74?84.
H. Isahara and M. Haruno. 2000. Japanese-English aligned
bilingual corpora. In Jean Veronis, editor, Parallel Text
Processing - Alignment and Use of Translation Corpora,
pages 313?334. Kluwer Academic Publishers.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Nagao. 1981. A Framework of a Mechanical Translation
between Japanese and English by Analogy Principle. In
Proceedings of the International NATO Symposium on Ar-
tificial and Human Intelligence.
F. J. Och and H. Ney. 2000. Improved Statistical Alignment
Models. In Proceedings of the ACL, pages 440?447.
K. Uchimoto, S. Sekine, and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of the COLING,
pages 1037?1043.
H. Watanabe, S. Kurohashi, and E. Aramaki. 2003. Finding
Translation Patterns from Paired Source and Target De-
pendency Structures. In Michael Carl and Andy Way, ed-
itors, Recent Advances in Example-Based Machine Trans-
lation, pages 397?420. Kluwer Academic Publishers.
K. Yamada and K. Knight. 2001. A Syntax-based Statistical
Translation Model. In Proceedings of the ACL, pages 523?
530.
K. Yamamoto and Y. Matsumoto. 2003. Extracting Transla-
tion Knowledge from Parallel Corpora. In Michael Carl
and Andy Way, editors, Recent Advances in Example-
Based Machine Translation, pages 365?395. Kluwer Aca-
demic Publishers.
Q. Zhou and H. Duan. 1994. Segmentation and POS Tag-
ging in the Construction of Contemporary Chinese Cor-
pus. Journal of Computer Science of China, Vol.85. (in
Chinese)
Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61?66,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies
Hai Zhao(??)?, Wenliang Chen(???)?,
Jun?ichi Kazama?, Kiyotaka Uchimoto?, and Kentaro Torisawa?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual syntactic and semantic dependency
parsing for our participation in the joint task
of CoNLL-2009 shared tasks. Our system
uses rich features and incorporates various in-
tegration technologies. The system is evalu-
ated on in-domain and out-of-domain evalu-
ation data of closed challenge of joint task.
For in-domain evaluation, our system ranks
the second for the average macro labeled F1 of
all seven languages, 82.52% (only about 0.1%
worse than the best system), and the first for
English with macro labeled F1 87.69%. And
for out-of-domain evaluation, our system also
achieves the second for average score of all
three languages.
1 Introduction
This paper describes the system of National In-
stitute of Information and Communications Tech-
nology (NICT) and City University of Hong Kong
(CityU) for the joint learning task of CoNLL-2009
shared task (Hajic? et al, 2009)1. The system is ba-
sically a pipeline of syntactic parser and semantic
parser. We use a syntactic parser that uses very rich
features and integrates graph- and transition-based
methods. As for the semantic parser, a group of well
selected feature templates are used with n-best syn-
tactic features.
1Our thanks give to the following corpus providers, (Taule?
et al, 2008; Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006) and (Kawahara et al, 2002).
The rest of the paper is organized as follows. The
next section presents the technical details of our syn-
tactic dependency parsing. Section 3 describes the
details of the semantic dependency parsing. Section
4 shows the evaluation results. Section 5 looks into a
few issues concerning our forthcoming work for this
shared task, and Section 6 concludes the paper.
2 Syntactic Dependency Parsing
Basically, we build our syntactic dependency parsers
based on the MSTParser, a freely available imple-
mentation2, whose details are presented in the paper
of McDonald and Pereira (2006). Moreover, we ex-
ploit rich features for the parsers. We represent fea-
tures by following the work of Chen et al (2008) and
Koo et al (2008) and use features based on depen-
dency relations predicted by transition-based parsers
(Nivre and McDonald, 2008). Chen et al (2008) and
Koo et al (2008) proposed the methods to obtain
new features from large-scale unlabeled data. In our
system, we perform their methods on training data
because the closed challenge does not allow to use
unlabeled data. In this paper, we call these new ad-
ditional features rich features.
2.1 Basic Features
Firstly, we use all the features presented by McDon-
ald et al (2006), if they are available in data. Then
we add new features for the languages having FEAT
information (Hajic? et al, 2009). FEAT is a set of
morphological-features, e.g. more detailed part of
speech, number, gender, etc. We try to align differ-
ent types of morphological-features. For example,
2http://mstparser.sourceforge.net
61
we can obtain a sequence of gender tags of all words
from a head h to its dependent d. Then we represent
the features based on the obtained sequences.
Based on the results of development data, we per-
form non-projective parsing for Czech and German
and perform projective parsing for Catalan, Chinese,
English, Japanese, and Spanish.
2.2 Features Based on Dependency Pairs
I    see    a    beautiful    bird    .
Figure 1: Example dependency graph.
Chen et al (2008) presented a method of extract-
ing short dependency pairs from large-scale auto-
parsed data. Here, we extract all dependency pairs
rather than short dependency pairs from training
data because we believe that training data are reli-
able. In a parsed sentence, if two words have de-
pendency relation, we add this word pair into a list
named L and count its frequency. We consider the
direction. For example, in figure 1, a and bird have
dependency relation in the sentence ?I see a beauti-
ful bird.?. Then we add word pair ?a-bird:HEAD?3
into list L and accumulate its frequency.
We remove the pairs which occur only once in
training data. According to frequency, we then
group word pairs into different buckets, with bucket
LOW for frequencies 2-7, bucket MID for frequen-
cies 8-14, and bucket HIGH for frequencies 15+.
We set these threshold values by following the set-
ting of Chen et al (2008). For example, the fre-
quency of pair ?a-bird:HEAD? is 5. Then it is
grouped into bucket ?LOW?. We also add a vir-
tual bucket ?ZERO? to represent the pairs that are
not included in the list. So we have four buckets.
?ZERO?, ?LOW?, ?MID?, and ?HIGH? are used as
bucket IDs.
Based on the buckets, we represent new features
for a head h and its dependent d. We check word
pairs surrounding h and d. Table 1 shows the word
pairs, where h-word refers to the head word, d-word
refers to the dependent word, h-word-1 refers to
3HEAD means that bird is the head of the pair.
the word to the left of the head in the sentence, h-
word+1 refers to the word to the right of the head,
d-word-1 refers to the word to the left of the depen-
dent, and d-word+1 refers the word to the right of
the dependent. Then we obtain the bucket IDs of
these word pairs from L.
We generate new features consisting of indicator
functions for bucket IDs of word pairs. We call these
features word-pair-based features. We also generate
combined features involving bucket IDs and part-of-
speech tags of heads.
h-word, d-word
h-word-1, d-word
h-word+1, d-word
h-word, d-word-1
h-word, d-word+1
Table 1: Word pairs for feature representation
2.3 Features Based on Word Clusters
Koo et al (2008) presented new features based on
word clusters obtained from large-scale unlabeled
data and achieved large improvement for English
and Czech. Here, word clusters are generated only
from the training data for all the languages. We per-
form word clustering by using the clustering tool4,
which also was used by Koo et al (2008). The
cluster-based features are the same as the ones used
by Koo et al (2008).
2.4 Features Based on Predicted Relations
Nivre and McDonald (2008) presented an integrat-
ing method to provide additional information for
graph-based and transition-based parsers. Here, we
represent features based on dependency relations
predicted by transition-based parsers for graph-
based parser. Based on the results on development
data, we choose the MaltParser for Catalan, Czech,
German, and Spanish, and choose another MaxEnt-
based parser for Chinese, English, and Japanese.
2.4.1 A Transition-based Parser: MaltParser
For Catalan, Czech, German, and Spanish, we
use the MaltParser, a freely available implementa-
4http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
62
tion5, whose details are presented in the paper of
Nivre (2003). More information about the parser can
be available in the paper (Nivre, 2003).
Due to computational cost, we do not select new
feature templates for the MaltParser. Following the
features settings of Hall et al (2007), we use their
Czech feature file and Catalan feature file. To sim-
ply, we apply Czech feature file for German too, and
apply Catalan feature file for Spanish.
2.4.2 Another Transition-based Parser:
MaxEnt-based Parser
In three highly projective language, Chinese,
English and Japanese, we use the maximum en-
tropy syntactic dependency parser as in Zhao and
Kit (2008). We still use the similar feature notations
of that work. We use the same greedy feature selec-
tion of Zhao et al (2009) to determine an optimal
feature template set for each language. Full feature
sets for the three languages can be found at website,
http://bcmi.sjtu.edu.cn/?zhaohai.
2.4.3 Feature Representation
For training data, we use 2-way jackknifing to
generate predicted dependency parsing trees by two
transition-based parsers. Following the features of
Nivre and McDonald (2008), we define features for
a head h and its dependent d with label l as shown in
table 2, where GTran refers to dependency parsing
trees generated by the MaltParser or MaxEnt-base
Parser and ? refers to any label. All features are
conjoined with the part-of-speech tags of the words
involved in the dependency.
Is (h, d, ?) in GTran?
Is (h, d, l) in GTran?
Is (h, d, ?) not in GTran?
Is (h, d, l) not in GTran?
Table 2: Features set based on predicted labels
3 n-best Syntactic Features for Semantic
Dependency Parsing
Due to the limited computational resource that we
have, we used the the similar learning framework as
our participant in semantic-only task (Zhao et al,
5http://w3.msi.vxu.se/?nivre/research/MaltParser.html
Normal n-best Matched
Ca 53 54 50
Ch 75 65 55
En 73 70 63
Table 3: Feature template sets:n-best vs. non-n-best
2009). Namely, three languages, a single maximum
entropy model is used for all identification and clas-
sification tasks of predicate senses or argument la-
bels in four languages, Catalan, Czech, Japanese, or
Spanish. For the rest three languages, an individual
sense classifier still using maximum entropy is ad-
ditionally used to output the predicate sense previ-
ously. More details about argument candidate prun-
ing strategies and feature template set selection are
described in Zhao et al (2009).
The same feature template sets as the semantic-
only task are used for three languages, Czech, Ger-
man and Japanese. For the rest four languages, we
further use n-best syntactic features to strengthen
semantic dependency parsing upon those automati-
cally discovered feature template sets. However, we
cannot obtain an obvious performance improvement
in Spanish by using n-best syntactic features. There-
fore, only Catalan, Chinese and English semantic
parsing adopted these types of features at last.
Our work about n-best syntactic features still
starts from the feature template set that is originally
selected for the semantic-only task. The original fea-
ture template set is hereafter referred to ?the normal?
or ?non-n-best?. In practice, only 2nd-best syntactic
outputs are actually adopted by our system for the
joint task.
To generate helpful feature templates from the
2nd-best syntactic tree, we simply let al feature tem-
plates in the normal feature set that are based on
the 1st-best syntactic tree now turn to the 2nd-best
one. Using the same notations for feature template
representation as in Zhao et al (2009), we take an
example to show how the original n-best features
are produced. Assuming a.children.dprel.bag is
one of syntactic feature templates in the normal
set, this feature means that all syntactic children of
the argument candidate (a) are chosen, and their
dependant labels are collected, the duplicated la-
bels are removed and then sorted, finally all these
strings are concatenated as a feature. The cor-
63
Language Features
Catalan p:2.lm.dprel
a.lemma + a:2.h.form
a.lemma + a:2.pphead.form
(a:2:p:2|dpPath.dprel.seq) + p.FEAT1
Chinese a:2.h.pos
a:2.children.pos.seq + p:2.children.pos.seq
a:2:p:2|dpPath.dprel.bag
a:2:p:2|dpPathPred.form.seq
a:2:p:2|dpPath.pos.bag
(a:2:p:2|dpTreeRelation) + p.pos
(a:2:p:2|dpPath.dprel.seq) + a.pos
English a:2:p:2|dpPathPred.lemma.bag
a:2:p:2|dpPathPred.pos.bag
a:2:p:2|dpTreeRelation
a:2:p:2|dpPath.dprel.seq
a:2:p:2|dpPathPred.dprel.seq
a.lemma + a:2.dprel + a:2.h.lemma
(a:2:p:2|dpTreeRelation) + p.pos
Table 4: Features for n-best syntactic tree
responding 2nd-best syntactic feature will be a :
2.children.dprel.bag. As all operations to gener-
ate the feature for a.children.dprel.bag is within
the 1st-best syntactic tree, while those for a :
2.children.dprel.bag is within the 2nd-best one. As
all these 2nd-best syntactic features are generated,
we use the same greedy feature selection procedure
as in Zhao et al (2009) to determine the best fit fea-
ture template set according to the evaluation results
in the development set.
For Catalan, Chinese and English, three opti-
mal n-best feature sets are obtained, respectively.
Though dozens of n-best features are initially gen-
erated for selection, only few of them survive af-
ter the greedy selection. A feature number statis-
tics is in Table 3, and those additionally selected
n-best features for three languages are in Table
4. Full feature lists and their explanation for
all languages will be available at the website,
http://bcmi.sjtu.edu.cn/?zhaohai.
4 Evaluation Results
Two tracks (closed and open challenges) are pro-
vided for joint task of CoNLL2009 shared task.
We participated in the closed challenge and evalu-
ated our system on the in-domain and out-of-domain
evaluation data.
avg. Cz En Gr
Syntactic (LAS) 77.96 75.58 82.38 75.93
Semantic (Labeled F1) 75.01 82.66 74.58 67.78
Joint (Macro F1) 76.51 79.12 78.51 71.89
Table 6: The official results of our submission for out-of-
domain task(%)
Test Dev
Basic ALL Basic ALL
Catalan 82.91 85.88 83.15 85.98
Chinese 74.28 75.67 73.36 75.64
Czech 77.21 79.70 77.91 80.22
English 88.63 89.19 86.35 87.40
German 84.61 86.24 83.99 85.44
Japanese 92.31 92.32 92.01 92.85
Spanish 83.59 86.29 83.73 86.22
Average 83.32 85.04 82.92 84.82
(+1.72) (+1.90)
Table 7: The effect of rich features for syntactic depen-
dency parsing
4.1 Official Results
The official results for the joint task are in Table 5,
and the out-of-domain task in Table 6, where num-
bers in bold stand for the best performances for the
specific language. For out-of-domain (OOD) eval-
uation, we did not perform any domain adaptation.
For both in-domain and out-of-domain evaluation,
our system achieved the second best performance
for the average Macro F1 scores of all the languages.
And our system provided the first best performance
for the average Semantic Labeled F1 score and the
forth for the average Labeled Syntactic Accuracy
score for in-domain evaluation.
4.2 Further results
At first, we check the effect of rich features for syn-
tactic dependency parsing. Table 7 shows the com-
parative results of basic features and all features on
test and development data, where ?Basic? refers to
the system with basic features and ?ALL? refers to
the system with basic features plus rich features. We
found that the additional features provided improve-
ment of 1.72% for test data and 1.90% for develop-
ment data.
Then we investigate the effect of different train-
ing data size for semantic parsing. The learning
64
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 85.04 85.88 75.67 79.70 89.19 86.24 92.32 86.29
Semantic (Labeled F1) 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
Joint (Macro F1) 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
Table 5: The official results of our joint submission (%)
Data Czech Chinese English
normal n-best normal n-best
25% 80.71 75.12 75.24 82.02 82.06
50% 81.52 76.50 76.59 83.52 83.42
75% 81.90 76.92 77.01 84.21 84.30
100% 82.24 77.35 77.34 84.73 84.80
Table 8: The performance in development set (semantic
labeled F1) vs. training corpus size
curves are drawn for Czech, Chinese and English.
We use 25%, 50% and 75% training corpus, respec-
tively. The results in development sets are given in
Table 8. Note that in this table the differences be-
tween normal and n-best feature template sets are
also given for Chinese and English. The results
in the table show that n-best features help improve
Chinese semantic parsing as the training corpus is
smaller, while it works for English as the training
corpus is larger.
5 Discussion
This work shows our further endeavor in syntactic
and semantic dependency parsing, based on our pre-
vious work (Chen et al, 2008; Zhao and Kit, 2008).
Chen et al (Chen et al, 2008) and Koo et al (Koo
et al, 2008) used large-scale unlabeled data to im-
prove syntactic dependency parsing performance.
Here, we just performed their method on training
data. From the results, we found that the new fea-
tures provided better performance. In future work,
we can try these methods on large-scale unlabeled
data for other languages besides Chinese and En-
glish.
In Zhao and Kit (2008), we addressed that seman-
tic parsing should benefit from cross-validated train-
ing corpus and n-best syntactic output. These two
issues have been implemented during this shared
task. Though existing work show that re-ranking for
semantic-only or syntactic-semantic joint tasks may
bring higher performance, the limited computational
resources does not permit us to do this for multiple
languages.
To analyze the advantage and the weakness of our
system, the ranks for every languages of our sys-
tem?s outputs are given in Table 9, and the perfor-
mance differences between our system and the best
one in Table 106. The comparisons in these two ta-
bles indicate that our system is slightly weaker in the
syntactic parsing part, this may be due to the reason
that syntactic parsing in our system does not ben-
efit from semantic parsing as the other joint learn-
ing systems. However, considering that the seman-
tic parsing in our system simply follows the output
of the syntactic parsing and the semantic part of our
system still ranks the first for the average score, the
semantic part of our system does output robust and
stable results. It is worth noting that semantic la-
beled F1 in Czech given by our system is 4.47%
worse than the best one. This forby gap in this lan-
guage further indicates the advantage of our system
in the other six languages and some latent bugs or
learning framework misuse in Czech semantic pars-
ing.
6 Conclusion
We describe the system that uses rich features and
incorporates integrating technology for joint learn-
ing task of syntactic and semantic dependency pars-
ing in multiple languages. The evaluation results
show that our system is good at both syntactic and
semantic parsing, which suggests that a feature-
oriented method is effective in multiple language
processing.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
6The difference for Chinese in the latter table is actually
computed between ours and the second best system.
65
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 4 4 4 4 2 3 3 4
Semantic (Labeled F1) 1 1 3 4 1 2 2 1
Joint (Macro F1) 2 1 3 4 1 3 2 1
Table 9: Our system?s rank within the joint task according to three main measures
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 0.73 1.98 0.84 0.68 0.69 1.24 0.25 1.35
Semantic (Labeled F1) - - 0.38 4.47 - 2.42 0.09 -
Joint (Macro F1) 0.12 - 0.15 2.40 - 1.22 0.37 -
Table 10: The performance differences between our system and the best one within the joint task according to three
main measures
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC-2006,
Genoa, Italy.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP-2008, Hyderabad, In-
dia, January 8-10.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL-
2009, Boulder, Colorado, USA.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech, June.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC-2002, pages 2008?
2013, Las Palmas, Canary Islands.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, USA, June.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X, New York City, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France, April 23-25.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the CoNLL-
2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage maxi-
mum entropy models. In Proceedings of CoNLL-2008,
pages 203?207, Manchester, UK, August 16-17.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of CoNLL-2009, Boul-
der, Colorado, USA.
66
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 1?8,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Enhancing the Japanese WordNet
Francis Bond,? Hitoshi Isahara,? Sanae Fujita,?
Kiyotaka Uchimoto,? Takayuki Kuribayashi? and Kyoko Kanzaki?
? NICT Language Infrastructure Group, ? NICT Language Translation Group
<bond@ieee.org,{isahara,uchimoto,kuribayashi,kanzaki}@nict.go.jp>
? Sanae Fujita, NTT Communications Science Laboratory
<sanae@kecl.cslab.ntt.co.jp>
Abstract
The Japanese WordNet currently has
51,000 synsets with Japanese entries. In
this paper, we discuss three methods of
extending it: increasing the cover, linking
it to examples in corpora and linking it
to other resources (SUMO and GoiTaikei).
In addition, we outline our plans to make
it more useful by adding Japanese defini-
tion sentences to each synset. Finally, we
discuss how releasing the corpus under an
open license has led to the construction
of interfaces in a variety of programming
languages.
1 Introduction
Our goal is to make a semantic lexicon of
Japanese that is both accesible and usable. To
this end we are constructing and releasing the
Japanese WordNet (WN-Ja) (Bond et al, 2008a).
We have almost completed the first stage,
where we automatically translated the English
and Euro WordNets, and are hand correcting it.
We introduce this in Section 2. Currently, we
are extending it in three main areas: the first
is to add more concepts to the Japanese Word-
Net, either by adding Japanese to existing En-
glish synsets or by creating new synsets (? 3).
The second is to link the synsets to text exam-
ples (? 4). Finally, we are linking it to other re-
sources: the Suggested Upper Merged Ontology
(SUMO) (Niles and Pease, 2001), the Japanese
semantic lexicon GoiTaikei (Ikehara et al, 1997),
and a collection of illustrations taken from the
Open ClipArt Library (Phillips, 2005) (? 5).
2 Current State
Currently, the WN-Ja consists of 157,000 senses
(word-synset pairs) 51,000 concepts (synsets) and
81,000 unique Japanese words (version 0.91). The
relational structure (hypernym, meronym, do-
main, . . . ) is based entirely on the English Word-
Net 3.0 (Fellbaum, 1998). We have Japanese
words for 43.0% of the synsets in the English
WordNet. Of these synsets, 45% have been
checked by hand, 8% were automatically cre-
ated by linking through multiple languages and
46% were automatically created by adding non-
ambiguous translations, as described in Bond
et al (2008a). There are some 51,000 synsets with
Japanese candidate words that have not yet been
checked. For up-to-date information on WN-Ja
see: nlpwww.nict.go.jp/wn-ja.
An example of the entry for the synset
02076196-n is shown in Figure 1. Most fields
come from the English WordNet. We have added
the underlined fields (Ja Synonyms, Illustration,
links to GoiTaikei, SUMO) and are currently
adding the translated definition (Def (Ja)). In
the initial automatic construction there were 27
Japanese words associated with the synset,1 in-
cluding many inappropriate translations for other
senses of seal (e.g., ?? hanko ?stamp?). These
were reduced to three after checking: ????,
?? azarashi ?seal? and ?? ? shi-ru ?seal?.
Synsets with? in their names are those for which
there is currently no Japanese entry in the Word-
Net.
The main focus of this year?s work has been
this manual trimming of badly translated words.
The result is a WordNet with a reasonable cov-
erage of common Japanese words. The precision
per sense is just over 90%. We have aimed at high
coverage at the cost of precision for two reasons:
(i) we think that the WordNet must have a rea-
1????, ???, ????, ??, ?, ??, ??, ?
?, ??, ?, ??, ??, ??, ??, ??, ??, ??, ?
?, ???, ?, ???, ??, ??, ??, ??, ?? ?, ?
?, ??, ??, ??, ??, ??, ??,??,?, ??, ??
1
sonable coverage to be useful for NLP tasks and
(ii) we expect to continue refining the accuracy
over the following years. Our strategy is thus dif-
ferent from Euro WordNet (Vossen, 1998), where
initial emphasis was on building a consistent and
complete upper ontology.
3 Increasing Coverage
We are increasing the coverage in two ways. The
first is to continue to manually correct the auto-
matically translated synsets. This is being done
both by hand, as time permits, and also by com-
paring against other resources such as GoiTaikei
and Wikipedia. When we check for poor candi-
dates, we also add in missing words as they occur
to us.
More interestingly, we wish to add synsets for
Japanese concepts that may not be expressed in
the English WordNet. To decide which new con-
cepts to add, we will be guided by the other tasks
we are doing: annotation and linking. We intend
to create new synsets for words found in the cor-
pora we annotate that are not currently covered,
as well as for concepts that we want to link to.
An example for the first is the concept ?? go-
han ?cooked rice?, as opposed to the grain ?
kome ?rice?. An example of the second is???
? ?shinguru ?single: a song usually extracted
from a current or upcoming album to promote
the album?. This is a very common hypernym in
Wikipedia but missing from the English Word-
Net.
As far as possible, we want to coordinate the
creation of new synsets with other projects: for
example KorLex: the Korean WordNet aleady
makes the cooked rice/grain distinction, and the
English WordNet should also have a synset for
this sense of single.
4 Text Annotation
We are in the process of annotating four texts
(Table 1). The first two are translations of Word-
Net annotated English Texts (SemCor and the
WordNet definitions), the third is the Japanese
newspaper text that forms the Kyoto Corpus
and the fourth is an open corpus of bilingual
Japanese-English sentences (Tanaka). In 2009,
we expect to finish translating and annotate all
of SemCor, translate the WordNet definitions and
Name Sentences Words Content Words
SemCor 12,842 224,260 120,000
Definitions 165,977 1,468,347 459,000
Kyoto 38,383 969,558 527,000
Tanaka 147,190 1,151,892 360,000
Table 1: Corpora to be Sense Tagged
start annotation on the Kyoto and Tanaka Cor-
pora.
This annotation is essential for finding missing
senses in the Japanese WordNet, as well as get-
ting the sense distributions that are needed for
supervised word sense disambiguation.
4.1 SemCor
SemCor is a textual corpus in which words have
been both syntactically and semantically tagged.
The texts included in SemCor were extracted
from the Brown corpus (Francis and Kucera,
1979) and then linked to senses in the English
WordNet. The frequencies in this corpus were
used to give the sense frequencies in WordNet
(Fellbaum, 1998). A subset of this corpus (Mul-
tiSemCor) was translated into Italian and used
as a corpus for the Italian WordNet (Bentivogli
et al, 2004). We are translating this subset into
Japanese.
In the same way as Bentivogli et al (2004), we
are exploiting Cross-Language Annotation Trans-
fer to seed the Japanese annotation. For exam-
ple, consider (1)2. The content words answer,
was, simple, honest are tagged in SemCor. They
can be aligned with their translations ?? ko-
tae ?answer?, ?? kantan ?simple?, ?? soc-
choku ?honest? and ??? datta ?was?. This
allows us to tag the Japanese translation with
the same synsets as the English, and thus disam-
biguate them.
(1) His answeri wasj simplek but honestl .
??i ? ??k ???? ??l ? ??
???j ?
However, just because all the English words
have sysnets in WordNet, it is not always the
case for the translations. For example, the En-
glish phrase last night can be translated into ?
? zen?ya ?last-night?. Here the two English
words (and synsets) link to a single Japanese
2Sentence 96 in b13.
2
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Synset 02076196-n
Synonyms
?
?
ja ??, ????, ???
en seal9
fr phoque
?
? Illustration
animal/seal.png
Def (en) ?any of numerous marine mammals that come on shore to breed; chiefly of cold regions?
Def (ja) ???????????????????????????????
Hypernyms ?????/pinniped
Hyponyms ?/crabeater seal ?/eared seal ??/earless seal
GoiTaikei ??537:beast??
SUMO ? Carnivore
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Example Entry for Seal/??
word which has no suitable synset in the English
WordNet. In this case, we need to create a new
synset unique to the Japanese WordNet.3
We chose a translated SemCor as the basis of
annotation for two main reasons: (i) the cor-
pus can be freely redistributed ? we expect
the glosses to be useful as an aligned corpus of
Japanese-English-Italian and (ii) it has other an-
notations associated with it: Brown corpus POS
annotation, Penn Treebank syntactic annotation.
4.2 WordNet Definitions
Our second translated corpus is formed from
the WordNet definitions (and example sentences)
themselves (e.g., the def field shown in Figure 1).
The English definitions have been annotated with
word senses in the Princeton WordNet Gloss Cor-
pus. In the same way that we do for SemCor, we
are translating the definitions and examples, and
using the existing annotation to seed our annota-
tion.
Using the definitions as the base for a sense
annotated corpus is attractive for the following
reasons: (i) the translated corpus can be freely
redistributed ? we expect the definitions to be
useful as an aligned corpus and also to be useful
for many other open lexicons; (ii) the definitions
are useful for Japanese native speakers using the
WordNet, (iii) the definitions are useful for unsu-
pervised sense disambiguation techniques such as
LESK (Baldwin et al, 2008); (iv) other projects
3Arguably, the fact that one says last night (not yester-
day night) for the night proceeding today and tomorrow
night (not next night) for the night following today sug-
gests that these multi-word expressions are lexicalized and
synsets should be created for them in the English Word-
Net. However, in general we expect to create some synsets
that will be unique to the Japanese WordNet.
have also translated synset definitions (e.g. Span-
ish and Korean), so we can hope to create a multi-
lingual corpus here as well and (v) the definitions
can be used as a machine readable dictionary, and
various information extracted from there (Barn-
brook, 2002; Nichols et al, 2006)
4.3 Kyoto Text Corpus
The Kyoto Text Corpus consists of newspaper
text from the Mainichi Newspaper (1995), seg-
mented and annotated with Japanese POS tags
and dependency trees (Kurohashi and Nagao,
2003). The corpus is made up of two parts. The
first consists of 17 full days of articles and the sec-
ond of one year?s editorials. We hope to annotate
at least parts of it during 2009.
Even though the Kyoto Text Corpus is not
freely redistributable, we have chosen to anno-
tate it due to the wealth of annotation associated
with it: dependency trees, predicate-argument re-
lations and co-reference (Iida et al, 2007), trans-
lations into English and Chinese (Uchimoto et al,
2004) and sense annotations from the Hinoki
project (Bond et al, 2006). We also felt it was
important to tag some native Japanese text, not
only translated text.
4.4 Tanaka Corpus
Finally, we will also tag the Tanaka Corpus, an
open corpus of Japanese-English sentence pairs
compiled by Professor Yasuhito Tanaka at Hyogo
University and his students (Tanaka, 2001) and
released into the public domain. The corrected
version we use has around 140,000 sentence pairs.
This corpus is attractive for several reasons.
(i) it is freely redistributable; (ii) it has been in-
dexed to entries in the Japanese-English dictio-
3
nary JMDict (Breen, 2003); (iii) part of it has
also been used in an open HPSG-based treebank
(Bond et al, 2008b); (iv) further, translations in
other languages, most notably French, have been
added by the TATOEBA project.4 Our plan is
to tag this automatically using the tools devel-
oped for the Kyoto corpus annotation, and then
to open the data to the community for refinement.
We give a typical example sentence in (2).
(2) ??????????????????
?Some birds are sitting on the branch of that
tree.? (en)
?Des oiseaux se reposent sur la branche de cet
arbre.? (fr)
5 Linking to other resources
We currently link the Japanese WordNet to three
other resources: the Suggested Upper Merged
Ontology; GoiTaikei, a Japanese Lexicon; and a
collection of pictures from the Open Clip Art Li-
brary (OCAL: Phillips (2005)).
For SUMO we used existing mappings. For the
other resources, we find confident matches auto-
matically and then generalize from them. We find
matches in three ways:
MM Monosemous monolingual matches
e.g. cricket bat or ?? azarashi ?seal?
MB Monosemous bilingual matches
e.g. ????seal?
HH Hypernym/Hyponym pairs
e.g. ?seal ? mammal?
We intend to use the same techniques to link
other resources, such as the concepts from the
EDR lexicon (EDR, 1990) and the automati-
cally extracted hypernym-hyponym links from
Torishiki-kai (Kuroda et al, 2009).
5.1 SUMO
The Suggested Upper Merged Ontology (SUMO)
is a large formal public ontology freely released
by the IEEE (Niles and Pease, 2001).
Because the structure of the Japanese Word-
Net is closely linked to that of the English Word-
Net, we were able to take advantage of the ex-
isting mappings from the English WordNet to
SUMO. There are 102,669 mappings from SUMO
4wwwcyg.utc.fr/tatoeba/
Carnivore Business Competition
Figure 2: SUMO illustrations
to WordNet: 3,593 equivalent, 10,712 where the
WordNet synset subsumes the SUMO concept,
88,065 where the SUMO concept subsumes the
WordNet concept, 293 where the negation of the
SUMO concept subsumes the WordNet synset
and 6 where the negation of the SUMO concept
is equivalent to the WordNet synset. According
to the mapping, synset 02076196-n ?? azarashi
?seal?, shown in Figure 1 is subsumed by the
SUMO concept ??Carnivore??. There is no link
between seal and carnivore in WordNet, which
shows how different ontologies can complement
each other.
Linking to SUMO also allowed us to use the
SUMO illustrations.5 These consist of 12,237
links linking 4,607 concepts to the urls of 10,993
illustrations. These are mainly taken from
from Wikimedia (upload.wikimedia.org), with
around 1,000 from other sources. The pictures
can be linked quite loosely to the concepts. For
example, ??Carnivore?? is illustrated by a lion eat-
ing meat, and ??BusinessCompetition?? by a pic-
ture of Wall Street.
As we wanted our illustrations to be more con-
crete, we only use SUMO illustrations where the
SUMO-WordNet mapping is equivalence. This
gave 4,384 illustrations for 999 synsets.
5.2 GoiTaikei
Linking Goi-Taikei, we used not only the
Japanese dictionary published in Ikehara et al
(1997), but also the Japanese-English dictionary
used in the machine translation system ALT-J/E
(Ikehara et al, 1991). We attempted to match
synsets to semantic categories by matching the
5Available at http://sigmakee.cvs.sourceforge.
net/viewvc/sigmakee/KBs/pictureList.kif, thanks to
Adam Pease for letting us know about them.
4
Japanese, English and English-Japanese pairs to
unambiguous entries in Goi-Taikei. For example,
the synset shown in Figure 1 was automatically
assigned the semantic category ??537:beast??, as
?? appears only once in WN-Ja, with the synset
shown, and once in the Japanese dictionary for
ALT-J/E with a single semantic category.
We are currently evaluating our results against
an earlier attempt to link WordNet and GoiTaikei
that also matched synset entries to words in Goi-
Taikei (Asanoma, 2001), but did not add an extra
constraint (that they must be either monosemous
or match as a hypernym-hyponym pair).
Once we have completed the mapping, we will
use it to check for inconsistencies in the two re-
sources.
5.3 Open ClipArt Library
In order to make the sense distinctions more vis-
ible we have semi-automatically linked synsets
to illustrations from the Open Clip Art Library
(OCAL: Phillips (2005)) using the mappings pro-
duced by Bond et al (2008a).
We manually checked the mappings and added
a goodness score. Illustrations are marked as:
3 the best out of multiple illustrations
2 a good illustration for the synset
1 a suitable illustration, but not perfect
This tag was used for black and white im-
ages, outlines, and so forth.
After the scoring, there were 874 links for 541
synsets (170 scored 1, 642 scored 2 and 62 scored
3). This is only a small subset of illustrations in
OCAL and an even smaller proportion of word-
net. However, because any illustrated synset alo
(in theory) illustrates its hypernyms, we have in-
directly illustrated far more than 541 synsets:
these figures are better than they seem.
There are far fewer OCAL illustrations than
the SUMO linked illustrations. However, they are
in general more representative illustrations (espe-
cially those scored 2 and above), and the source of
the clipart is available as SVG source so it is easy
to manipulate them. We think that this makes
them particularly useful for a variety of tasks.
One is pedagogical ? it is useful to have pic-
tures in learners? dictionaries. Another is in cross-
cultural communication - for example in Pangea,
where children use pictons (small concept repre-
senting pictures) to write messages (Takasaki and
Mori, 2007).
The OCAL illustrations mapped through
WordNet to 541 SUMO concepts. We have given
these links to the SUMO researchers.
6 Interfaces
We released the Japanese WordNet in three for-
mats: tab-delimited text, XML and as an SQLite
database. The license was the same as English
WordNet. This is a permissive license, the data
can be reused within proprietary software on the
condition that the license is distributed with that
software (similar to the MIT X license). The
license is also GPL-compatible, meaning that
the GPL permits combination and redistribution
with software that uses it.
The tab delimited format consists of just a list
of synsets, Japanese words and the type of link
(hand, multi-lingual or monosemous):
02076196-n ?? hand
02076196-n ???? hand
02076196-n ??? hand
We also output in WordNet-LMF (Francopoulo
et al, 2006; Soria et al, 2009), to make the
program easily available for other WordNet re-
searchers. In this case the synset structure was
taken from the English WordNet and the lem-
mas from the Japanese WordNet. Because of the
incomplete coverage, not all synsets contain lem-
mas. This format is used by the Kyoto Project,
and we expect it to become the standard ex-
change format for WordNets (Vossen et al, 2008).
Finally, we also created an SQL database. This
contains information from the English WordNet,
the Japanese WordNet, and links to illustra-
tions. We chose SQLite,6 a self-contained, zero-
configuration, SQL database engine whose source
code is in the public domain. The core structure
is very simple with six tables, as shown in Fig-
ure 3.
As we prepared the release we wrote a perl
module for a basic interface. This was used to
develop a web interface: Figure 4 shows a screen-
shot.
6http://www.sqlite.org
5
word
wordid 
 lang 
 lemma 
 pron 
 pos 
sense
synset 
 wordid 
 lang 
 rank 
 lexid 
 freq 
 src 
1..*1
synset
pos 
 name 
 src 
11..*
synsetDef
synset 
 lang 
 def 
 sid 
11
synlink
synset1 
 synset2 
 link 
 src 
1 1..*
xlink
synset 
 resource 
 xref 
 misc 
 confidence
1 1..*
Figure 3: Database Schema
Figure 4: Web Search Screenshot
6
7 Discussion
In contrast to earlier WordNets, the Japanese
WordNet was released with two known major im-
perfections: (i) the concept hierarchy was en-
tirely based on English with no adaptation to
Japanese and (ii) the data was released with some
unchecked automatically created entries. The re-
sult was a WordNet that did not fully model the
lexical structure of Japanese and was known to
contain an estimated 5% errors. The motivation
behind this was twofold. Firstly, we wanted to try
and take advantage of the open source model. If
the first release was good enough to be useful, we
hoped to (a) let people use it and (b) get feedback
from them which could then be incorporated into
the next release. This is the strategy known as
release early, release often (Raymond, 1999).
Secondly, we anticipated the most common use
of the WordNet to be in checking whether one
word is a hypernym of another. In this case, even
if one word is wrong, it is unlikely that the other
will be, so a small percentage of errors should be
acceptable.
From the practical point of view, the early re-
lease appears to have been a success. The SQL
database proved very popular, and within two
weeks of the first release someone produced a
python API. This was soon followed by inter-
faces in java, ruby, objective C and gauche. We
also received feedback on effective indexing of the
database and some corrections of entries ? these
have been included in the most recent release
(0.91).
The data from the Japanese WordNet has al-
ready been incorporated into other projects. The
first was the Multi-Lingual Semantic Network
(MLSN) (Cook, 2008) a WordNet based net-
work of Arabic, Chinese, English, German and
Japanese. Because both the Japanese WordNet
and MLSN use very open licenses, it is possible
to share entries directly. We have already re-
ceived useful feedback and over a thousand new
entries from MLSN. The second project using our
data is the Asian WordNet (Charoenporn et al,
2008). They have a well developed interface for
collaborative development of linguistic resources,
and we hope to get corrections and additions
from them in the future. Another project us-
ing the Japanese WordNet data is the Language
Grid (Ishida, 2006) which offers the English and
Japanese WordNets as concept dictionaries.
We have also been linked to from other re-
sources. The Japanese-English lexicon project
JMDict (Breen, 2004) now links to the Japanese
WordNet, and members of that project are us-
ing WordNet to suggest new entries. We used
JMDict in the first automatic construction stage,
so it is particularly gratifying to be able to help
JMDict in turn.
Finally, we believe that data about language
should be shared ? language is part of the com-
mon heritage of its speakers. In our case, the
Japanese WordNet was constructed based on the
work that others made available to us and thus we
had a moral obligation to make our results freely
available to others. Further, projects that create
WordNets but do not release them freely hinder
research on lexical semantics in that language ?
people cannot use the unreleased resource, but it
is hard to get funding to duplicate something that
already exists.
In future work, in addition to the planned ex-
tensions listed here, we would like to work on
the following: Explicitly marking lexical variants;
linking to instances in Wikipedia; adding deriva-
tional and antonym links; using the WordNet for
word sense disambiguation.
8 Conclusion
This paper presents the current state of the
Japanese WordNet (157,000 senses, 51,000 con-
cepts and 81,000 unique Japanese words, with
links to SUMO, Goi-Taikei and OCAL) and out-
lined our plans for further work (more words,
links to corpora and other resources). We hope
that WN-Ja will become a useful resource not only
for natural language processing, but also for lan-
guage education/learning and linguistic research.
References
Naoki Asanoma. 2001. Alignment of ontologies:wordnet
and goi-taikei. In NAACL Wokshop on WordNet &
Other Lexical Resources, pages 89?94. Pittsburgh, USA.
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2008. MRD-
based word sense disambiguation: Further extending
Lesk. In Proc. of the 3rd International Joint Conference
on Natural Language Processing (IJCNLP-08), pages
775?780. Hyderabad, India.
Geoff Barnbrook. 2002. Defining Language ? A local
7
grammar of definition sentences. Studies in Corpus Lin-
guistics. John Benjamins.
Luisa Bentivogli, Pamela Forner, and Emanuele Pianta.
2004. Evaluating cross-language annotation transfer in
the MultiSemCor corpus. In 20th International Con-
ference on Computational Linguistics: COLING-2004,
pages 364?370. Geneva.
Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2006. The Hinoki syntactic and semantic treebank of
Japanese. Language Resources and Evaluation, 40(3?
4):253?261. (Special issue on Asian language technol-
ogy).
Francis Bond, Hitoshi Isahara, Kyoko Kanzaki, and Kiy-
otaka Uchimoto. 2008a. Boot-strapping a WordNet
using multiple existing WordNets. In Sixth Interna-
tional conference on Language Resources and Evalua-
tion (LREC 2008). Marrakech.
Francis Bond, Takayuki Kuribayashi, and Chikara
Hashimoto. 2008b. Construction of a free Japanese
treebank based on HPSG. In 14th Annual Meeting of
the Association for Natural Language Processing, pages
241?244. Tokyo. (in Japanese).
James W. Breen. 2003. Word usage examples in an elec-
tronic dictionary. In Papillon (Multi-lingual Dictionary)
Project Workshop. Sapporo.
James W. Breen. 2004. JMDict: a Japanese-multilingual
dictionary. In Coling 2004 Workshop on Multilingual
Linguistic Resources, pages 71?78. Geneva.
Thatsanee Charoenporn, Virach Sornlerlamvanich,
Chumpol Mokarat, and Hitoshi Isahara. 2008. Semi-
automatic compilation of Asian WordNet. In 14th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 1041?1044. Tokyo.
Darren Cook. 2008. MLSN: A multi-lingual semantic net-
work. In 14th Annual Meeting of the Association for
Natural Language Processing, pages 1136?1139. Tokyo.
EDR. 1990. Concept dictionary. Technical report, Japan
Electronic Dictionary Research Institute, Ltd.
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
W. Nelson Francis and Henry Kucera. 1979. BROWN
CORPUS MANUAL. Brown University, Rhode Island,
third edition.
Gil Francopoulo, Monte George, Nicoletta Calzolari, Mon-
ica Monachini, Nuria Bel, Mandy Pet, and Claudia So-
ria. 2006. Lexical markup framework (LMF). In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006). Genoa,
Italy.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference relations.
In ACL Workshop: Linguistic Annotation Workshop,
pages 132?139. Prague.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing ? effects of new methods in ALT-J/E ?. In
Third Machine Translation Summit: MT Summit III,
pages 101?106. Washington DC.
Toru Ishida. 2006. Language grid: An infrastructure for in-
tercultural collaboration. In IEEE/IPSJ Symposium on
Applications and the Internet (SAINT-06), pages 96?
100. (keynote address).
Kow Kuroda, Jae-Ho Lee, Hajime Nozawa, Masaki Mu-
rata, and Kentaro Torisawa. 2009. Manual cleaning of
hypernyms in Torishiki-Kai. In 15th Annual Meeting of
The Association for Natural Language Processing, pages
C1?3. Tottori. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. In Anne Abeille?, editor, Treebanks: Building
and Using Parsed Corpora, chapter 14, pages 249?260.
Kluwer Academic Publishers.
Eric Nichols, Francis Bond, Takaaki Tanaka, Sanae Fu-
jita, and Daniel Flickinger. 2006. Robust ontology ac-
quisition from multiple sources. In Proceedings of the
2nd Workshop on Ontology Learning and Population:
Bridging the Gap between Text and Knowledge, pages
10?17. Sydney.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Chris Welty and Barry Smith, edi-
tors, Proceedings of the 2nd International Conference on
Formal Ontology in Information Systems (FOIS-2001).
Maine.
Jonathan Phillips. 2005. Introduction to the open
clip art library. http://rejon.org/media/writings/
ocalintro/ocal_intro_phillips.html. (accessed
2007-11-01).
Eric S. Raymond. 1999. The Cathedral & the Bazaar.
O?Reilly.
Claudia Soria, Monica Monachini, and Piek Vossen. 2009.
Wordnet-LMF: fleshing out a standardized format for
wordnet interoperability. In Second International Work-
shop on Intercultural Collaboration (IWIC-2009). Stan-
ford.
Toshiyuki Takasaki and Yumiko Mori. 2007. Design and
development of a pictogram communication system for
children around the world. In First International Work-
shop on Intercultural Collaboration (IWIC-2007), pages
144?157. Kyoto.
Yasuhito Tanaka. 2001. Compilation of a multilingual par-
allel corpus. In Proceedings of PACLING 2001, pages
265?268. Kyushu.
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo, Masaki
Murata, Satoshi Sekine, and Hitoshi Isahara. 2004.
Multilingual aligned parallel treebank corpus reflecting
contextual information and its applications. In Gilles
Se?rasset, editor, COLING 2004 Multilingual Linguistic
Resources, pages 57?64. COLING, Geneva, Switzerland.
P Vossen, E. Agirre, N. Calzolari, C. Fellbaum, S. Hsieh,
C. Huang, H. Isahara, K. Kanzaki, A. Marchetti,
M. Monachini, F. Neri, R. Raffaelli, G. Rigau, and
M. Tescon. 2008. KYOTO: A system for mining,
structuring and distributing knowledge across languages
and cultures. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08). Mar-
rakech, Morocco.
Piek Vossen, editor. 1998. Euro WordNet. Kluwer.
8
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 36?39,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Machine Transliteration using Target-Language Grapheme and
Phoneme: Multi-engine Transliteration Approach
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{rovellia,uchimoto,torisawa}@nict.go.jp
Abstract
This paper describes our approach to
?NEWS 2009 Machine Transliteration
Shared Task.? We built multiple translit-
eration engines based on different combi-
nations of two transliteration models and
three machine learning algorithms. Then,
the outputs from these transliteration en-
gines were combined using re-ranking
functions. Our method was applied to all
language pairs in ?NEWS 2009 Machine
Transliteration Shared Task.? The official
results of our standard runs were ranked
the best for four language pairs and the
second best for three language pairs.
1 Outline
This paper describes our approach to ?NEWS
2009 Machine Transliteration Shared Task.?
Our approach was based on two transliteration
models ? TM-G (Transliteration model based
on target-language Graphemes) and TM-GP
(Transliteration model based on target-language
Graphemes and Phonemes). The difference
between the two models lies in whether or
not a machine transliteration process depends
on target-language phonemes. TM-G directly
converts source-language graphemes into target-
language graphemes, while TM-GP first trans-
forms source language graphemes into target-
language phonemes and then target-language
phonemes coupled with their corresponding
source-language graphemes are converted into
target-language graphemes. We used three dif-
ferent machine learning algorithms (conditional
random fields (CRFs), margin infused relaxed al-
gorithm (MIRA), and maximum entropy model
(MEM)) (Berger et al, 1996; Crammer and
Singer, 2003; Lafferty et al, 2001) for build-
ing multiple machine transliteration engines. We
attempted to improve the transliteration quality
by combining the outputs of different machine
transliteration engines operating on the same in-
put. Our approach was applied to all language
pairs in ?NEWS 2009 Machine Transliteration
Shared Task.? The official results of our approach
were ranked as the best for four language pairs and
the second best for three language pairs (Li et al,
2009a).
2 Transliteration Model
Let S be a source-language word and T be a target-
language transliteration of S. T is represented in
two ways ? TG, a sequence of target-language
graphemes, and TP , a sequence of target-language
phonemes. Here, a target-language grapheme is
defined as a target-language character. We regard
consonant and vowel parts in the romanized form
of a target language grapheme as a target-language
phoneme. Then TM-G and TM-GP are formu-
lated as Eq (1) and (2), respectively.
PTM?G(T |S) = P (TG|S) (1)
PTM?GP (T |S) (2)
=
?
?T
P
P (TP |S) ? P (TG|TP , S)
Ja
Ch
En
?:I?:I?:B?:I?:I?:B?:B TG
NUDNILKETP
?:B ?:I?:B ?:B ?:I?:B ?:B TG
NOTNIRKUTP
notnilCS   
Clinton
KELINDUN KURINTON
??? ?????
ClintonClinton
??? ?????
Clinton
TM-G TM-GP
Figure 1: Illustration of the two transliteration
models
36
Figure 1 illustrates the two transliteration mod-
els with examples, Clinton and its Chinese
and Japanese transliterations. Target language
graphemes are represented in terms of the BIO no-
tation. This makes it easier to represent many-
to-one correspondence between target language
phoneme and grapheme.
3 Machine Learning Algorithms
A machine transliteration problem can be con-
verted into a sequential labeling problem, where
each source-language grapheme is tagged with its
corresponding target-language grapheme. This
section briefly describes the machine learning al-
gorithms used for building multiple transliteration
engines.
3.1 Maximum Entropy Model
Machine transliteration based on the maximum
entropy model was described in detail in Oh et al
(2006) along with comprehensive evaluation of its
performance. We used the same way as that pro-
posed by Oh et al (2006), thus its full description
is not presented here.
3.2 Conditional Random Fields (CRFs)
CRFs, a statistical sequence modeling framework,
was first introduced by Lafferty et al (2001).
CRFs has been used for sequential labeling prob-
lems such as text chunking and named entity
recognition (McCallum and Li, 2003). CRF++1
was used in our experiment.
3.3 Margin Infused Relaxed Algorithm
The Margin Infused Relaxed Algorithm (MIRA)
has been introduced by Crammer and Singer
(2003) for large-margin multi-class classification.
Kruengkrai et al (2008) proposed a discriminative
model for joint Chinese segmentation and POS
tagging, where MIRA was used as their machine
learning algorithm. We used the same model for
our machine transliteration, exactly joint syllabi-
cation2 and transliteration.
3.4 Features
We used the following features within the ?3 con-
text window3 for the above mentioned three ma-
1Available at http://crfpp.sourceforge.net/
2A syllable in English is defined as a sequence of English
grapheme corresponding to one target-language grapheme.
3The unit of context window is source-language
grapheme or syllable.
chine learning algorithms.
? Left-three and right-three source-language
graphemes (or syllables)
? Left-three and right-three target-language
phonemes
? Target-language graphemes assigned to the
previous three source-language graphemes
(or syllables)
4 Multi-engine Transliteration
4.1 Individual Transliteration Engine
The main aim of the multi-engine transliteration
approach is to combine the outputs of multiple en-
gines so that the final output is better in quality
than the output of each individual engine. We
designed four transliteration engines using dif-
ferent combinations of source-language translit-
eration units, transliteration models, and machine
learning algorithms as listed in Table 1. We named
four transliteration engines as CRF-G, MEM-G,
MEM-GP, and MIRA-G. Here, the prefixes rep-
resent applied machine learning algorithms (max-
imum entropy model (MEM), CRFs, and MIRA),
while G and GP in the suffix represent the translit-
eration models, TM-G and TM-GP, respectively.
Each individual engine produces 30-best translit-
erations for a given source-language word.
Source-language transliteration unit
Grapheme Syllable
TM-G ME-G, CRF-G MIRA-G
TM-GP ME-GP N/A
Table 1: Design strategy for multiple translitera-
tion engines
4.2 Combining Methodology
We combined the outputs of multiple translitera-
tion engines by means of a re-ranking function,
g(x). Let X be a set of transliterations gener-
ated by multiple transliteration engines for source-
language word s and ref be a reference translit-
eration of s. A re-ranking function is defined as
Eq. (3), where it ranks ref in X higher and the
others lower (Oh and Isahara, 2007).
g(x) : X ? {r : r is ordering of x ? X} (3)
We designed two types of re-ranking functions by
using the rank of each individual engine and ma-
chine learning algorithm.
37
4.2.1 Re-ranking Based on the Rank of
Individual Engines
Two re-ranking functions based on the rank of
each individual engine, grank and gFscore(x),
are used for combining the outputs of multiple
transliteration engines. Let X be a set of outputs
of N transliteration engines for the same input.
grank(x) re-ranks x ? X in the manner shown
in Eq. (4), where Ranki(x) is the position of x in
the n-best list generated by the ith transliteration
engine. grank(x) can be interpreted as the average
rank of x over outputs of each individual engine.
If x is not in the n-best list of the ith transliteration
engine, 1Rank
i
(x) = 0.
grank(x) =
1
N
N
?
i=1
1
Ranki(x)
(4)
gFscore(x) is based on grank(x) and the F-
score measure, which is one of the evaluation met-
rics in the ?NEWS 2009 Machine Transliteration
Shared Task? (Li et al, 2009b). We considered
the top three outputs of each individual engine
as reference transliterations and defined them as
virtual reference transliterations. We calculated
the F-score measure between the virtual reference
transliteration and each output of multiple translit-
eration engines. gFscore(x) is defined by Eq. (5),
where VRef is a set of virtual reference transliter-
ations, and Fscore(vr, x) is a function that restores
the F-score measure between vr and x.
gFscore(x) = grank(x) ?MF (x) (5)
MF (x) =
1
|V Ref |
?
vr?V Ref
Fscore(vr, x)
Since the F-score measure is calculated in terms of
string similarity, x gets a high score from gMF (x)
when it is orthographically similar to virtual refer-
ence transliterations.
4.2.2 Re-ranking based on Machine Learning
Algorithm
We used the maximum entropy model for learn-
ing re-ranking function gME(x). Let ref be a ref-
erence transliteration of source-language word s,
feature(x) be a feature vector of x ? X , and
y ? {ref, wrong} be the training label for x.
gME(x) assigns a probability to x ? X as shown
in Eq. (6).
gME(x) = P (ref |feature(x)) (6)
A feature vector of x is composed of
? ?grank(x), gFscore(x), 1Rank
i
(x) , P (T |S)?
where 1Rank
i
(x) and P (T |S) of each individual en-
gine are used as a feature.
We estimated P (ref |feature(x)) by using the
development data.
5 Our Results
5.1 Individual Engine
CRF-G MEM-G MEM-GP MIRA-G
EnCh 0.628 0.686 0.715 0.684
EnHi 0.455 0.469 0.469 0.412
EnJa 0.514 0.517 0.519 0.490
EnKa 0.386 0.380 0.380 0.338
EnKo 0.460 0.438 0.447 0.367
EnRu 0.600 0.561 0.566 0.568
EnTa 0.453 0.459 0.459 0.412
JnJk N/A 0.532 N/A 0.571
Table 2: ACC of individual engines on the test data
Table 2 presents ACC4 of individual translit-
eration engines, which was applied to all lan-
guage pairs in ?NEWS 2009 Machine Translit-
eration Shared Task? (Li et al, 2004; Kumaran
and Kellner, 2007; The CJK Dictionary Institute,
2009). CRF-G was the best transliteration engine
in EnKa, EnKo, and EnRu. Owing to the high
training costs of CRFs, we trained CRF-G in EnCh
with a very small number of iterations5. Hence,
the performance of CRF-G was poorer than that
of the other engines in EnCh. MEM-GP was the
best transliteration engine in EnCh, EnHi, EnJa,
and EnTa. These results indicate that joint use
of source language graphemes and target language
phonemes were very useful for improving perfor-
mance. MIRA-G was sensitive to the training data
size, because it was based on joint syllabication
and transliteration. Therefore, the performance of
MIRA-G was relatively better in EnCh and EnJa,
whose training data size is bigger than other lan-
guage pairs. CRF-G could not be applied to JnJk,
mainly due to too long training time. Further,
MEM-GP could not be applied to JnJk, because
transliteration in JnJk can be regarded as conver-
sion of target language phonemes to target lan-
guage graphemes. MEM-G and MIRA-G were
4Word accuracy in Top-1 (Li et al, 2009b)
5We applied over 100 iterations to other language pairs
but only 30 iterations to EnCh.
38
applied to JnJk and MIRA-G showed the best per-
formance in JnJK.6
5.2 Combining Multiple Engines
grank gFscore gME I-BEST
EnCh 0.730 0.731 0.731 0.715
EnHi 0.481 0.475 0.483 0.469
EnJa 0.535 0.535 0.537 0.519
EnKa 0.393 0.399 0.398 0.386
EnKo 0.461 0.444 0.473 0.460
EnRu 0.602 0.605 0.600 0.600
EnTa 0.470 0.478 0.474 0.459
JnJk 0.597 0.593 0.590 0.571
Table 3: Multi-engine transliteration results on the
test data: the underlined figures are our official re-
sult
Table 3 presents the ACC of our multi-engine
transliteration approach and that of the best in-
dividual engine (I-BEST) in each language pair.
gME gave the best performance in EnCh, EnHi,
EnJa, and EnKo, while gFscore did in EnCh, EnKa,
EnRu, and EnTa. Comparison between the best
individual transliteration engine and our multi-
engine transliteration showed that grank and gME
consistently showed better performance except in
EnRu, while gFscore showed the poorer perfor-
mance in EnKo. The results to be submitted as
?the standard run? were selected among the re-
sults listed in Table 3 by using cross-validation on
the development data. We submitted the results of
gME as the standard run to ?NEWS 2009 Machine
Transliteration Shared Task? for the six language
pairs in Table 3, while the result of gFscore is sub-
mitted as the standard run for EnRu. The official
results of our standard runs were ranked the best
for EnCh, EnJa, EnKa, and EnTa, and the second
best for EnHi, EnKo, and EnRu (Li et al, 2009a).
6 Conclusion
In conclusion, we have applied multi-engine
transliteration approach to ?NEWS 2009 Machine
Transliteration Shared Task.? We built multiple
transliteration engines based on different com-
binations of transliteration models and machine
learning algorithms. We showed that the translit-
eration model, which is based on target language
6We submitted the results of MEM-G as a standard run for
JnJk because we had only one transliteration engine for JnJK
before the submission deadline of the NEWS 2009 machine
transliteration shared task.
graphemes and phonemes, and our multi-engine
transliteration approach are effective, regardless of
the nature of the language pairs.
References
A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Canasai Kruengkrai, Jun?ichi Kazama, Kiyotaka Uchi-
moto, Kentaro Torisawa, and Hitoshi Isahara. 2008.
A discriminative hybrid model for joint Chinese
word segmentation and pos tagging. In Proc. of The
11th Oriental COCOSDA Workshop.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. of
SIGIR ?07, pages 721?722.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML01, pages 282?289.
Haizhou Li, Min Zhang, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proc. of ACL ?04, pages 160?167.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report on NEWS 2009 machine
transliteration shared task. In Proc. of ACL-IJCNLP
2009 Named Entities Workshop.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. of
ACL-IJCNLP 2009 Named Entities Workshop.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. of CoNLL ?03, pages 188?191.
Jong-Hoon Oh and Hitoshi Isahara. 2007. Machine
transliteration using multiple transliteration engines
and hypothesis re-ranking. In Proc. of the 11th Ma-
chine Translation Summit, pages 353?360.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119?151.
The CJK Dictionary Institute. 2009. http://www.
cjk.org.
39
Hybrid Neuro and Rule-Based Part of Speech Taggers 
Qing Ma, Masaki  Murata,  Kiyotaka Uch imoto ,  H i tosh i  Isahara 
Communic~t ion s l{esea.rch Labora.tory 
Ministry of Posts a.nd Telecommm~ications 
588-2, lwa,oka,, Nishi-ku, Kobe 6511-2/192, 3a, pa,n 
{qma, murata., uchimoto,  isa.hara}{))crl.go.jp 
Abstract  
A hybrid system R)r tagging part of speech is 
descril)ed that consists of a neuro tagger and 
a rule-based correcter. The neuro tagger is 
an initia.1--state a.nnotator tha.t uses difl'ertnt 
h_,,ngths of contexts based on longe, st context l)ri- 
ority. Its inputs a.re weighted 1)y information 
gains tha.t are obtained by information ma.xi- 
mization. The rule-1)ased correcter is construct- 
ed by a. sol; of trm~sfc)rma.tion rules to xna.ke Ul) 
for the shortcomings o\[' the nou17o tagger. Corn- 
puter experiments show that ahnost 20% of the 
errors ma.de by the neuro tagger a.re correct- 
ed by the, st trans\[orma.tion rules, so tha.t the 
hybrid system ca.n reach a.n a,tcura.cy of 95.5% 
counting only the ambiguous words and 99.1% 
counting all words when a. small Thai corpus 
with 22,311 a mbig;uous words is used t))v tra.in- 
ing. This a(;cu racy is far higher than that using 
an IIMM and is also higher tha.n that using a. 
rule-1)ased model. 
1 Introduct ion 
Many pa.rt of speech (POS) tatters  proposed 
so far (e.g., Brill, 1994; Meria.ldo, 1994; l)aele- 
marls, el. al., 1996; and Schmid, 1994) ha.re 
achieved a. high accura.ey partly because a. very 
large amount of dal,~ was used to 1;rain them 
(e.g., on the order of 1,000,000 words for \]'hl- 
glish). For ma.ny other la.nguages (e.g., Thai, 
which we treat in this paper)~ however, it is not 
as easy to cremate \]a.rge corpora from which lm:ge 
amounts of tr~fining data can be extra.cted. It is 
therefore desirable to construct a practic;d tag- 
ger tha.t needs as little training d a.t;a~ as possible. 
A multi-neuro tagger (Ma a.nd ls~hara, 11998) 
and its slimmed-down version called the ela.s- 
tic neuro tagger (Ma, el; al., 1999), which have 
high genera.lizing ability and therefore are good 
at dealing with the problems of data sp~u:se- 
hess, were proposed to satist~y this requh:ement. 
These taggers perform POS tagging using difl'er- 
ent lengths of corltexts I)~.~sed on longest context 
prk)rity, and each element of tile input is weight- 
ed with information gains (Quinla.n, 1993) for 
retlecting that tile elements of the input h~ve 
different rtlevances in t~Gging. They ha.d a tag- 
ging accuracy of 94.4% (counting only the am- 
biguous words in part of speech) in computer ex- 
periments when a. small 'l'ha.i corpus with 22,311 
am biguous words was u se(l for tr~fi n ing. This ~(:- 
curacy is bu" higher thml t\]lat; USillg tile hidden 
Marker model (IIMM), the main approach to 
\])art o\[ speech tagging, ~nd is ~dso higher t,\]lan 
tha.t using a. rule-based mode\]. 
Neuro taggers, however, htwe several crucial 
shortcomings. First, even in the case where the 
POS of a word is uniquely determined by the 
word on its left, for example, a neural net will 
also try to perlbrnl tagging based on tile com- 
plete context. As a result, even for" when the 
word on tile left; is the same, the tagging result~ 
s will be difl'erent if the complete contexts are 
different, rl'ha, t is~ the neuro tagger carl hard- 
ly acquire the rules with single inputs. Fur- 
thermore, although lexica.l in\[brma.tion is very 
ilnport~ult in t~gging, it is difficult for: neural 
nets to use it becmme doing so would make the 
network enorlnous. That is, the neuro tagger 
ca.nnot acquire (;lit rules with lexical informs> 
tion. Additionally, Imca.use of convergence and 
509 
over-training l)roblems, it is impossible and also 
not advisM)le to train neural nets to an a.ccura,- 
cy of 100%. The training should be stopped at 
an appropriate level of a.ceuracy. Consequently, 
neural nets may not acquire some usefnl rules. 
To make up for these shortcomings of the 
neuro tagger, we introduce in this pa.per a rule- 
based corrector as tile post-processor and con- 
struct a hyl)rid system. The rule-based cot- 
rector is constructed by a set of transforma- 
tion rules, which is acqnired by transforma?ion- 
based error-driven learning (Brill, 1.994:) from 
training corpus using a set of templates. The 
templates are designed to SUl)l)ly the rules that 
the neuro tagger can hardly acquire. Actual- 
ly, by examining the transformation rules ac- 
quired in the computer experiments, the 99.9% 
of them are exactly those that the neuro tagger 
can hardly acquire~ even when using a template 
set including those for generating the'rules that 
the neuro tagger can easily acquire. This rein- 
forces onr expectation that the rule-based ap- 
proach is a well-suited method to cope with the 
shortcomings of the neuro t~gger. Computer ex- 
periments hows thai; about  200/0 of errors made 
by the neuro tagger can be corrected by using 
these rules and that the hybrid system ca.n reach 
an accuracy of 95.5% counting only the aml)ign- 
ous words and 99.1% counting all words in l, he 
testing corpus, when tile same corpus described 
above is used for training. 
2 POS Tagg ing  Prob lems 
In this paper, suppose there is a lexicon V, 
where the POSs that can be served by each word 
are list.ed, and tiler(; is a set of POSs, l?. That is, 
unknown words that do not exist in the lexicon 
are not dealt with. The POS tagging problem 
is thus to find a string of POSs T = T172..-% 
(ri C F, i = 1 , - . . , s )  by following procedure 
~o when sentence W = wlw2...w.~ (wi C V, 
i = 1 , . - . , s )  is given. 
#:W ~ -+ rt, (1) 
where t is the index of the target word (the word 
to be tagged), and W t is a word sequence with 
length l + 1 + r (:entered on the target word: 
l i e  t : wt_  1 . . . .  i o  t ? . . Wt+r~ (2) 
where t - 1 > 1, t + r _< s. 'l'agging ca.n thus be 
regarded as a classification problem 1) 3, replacing 
the POS with (;lass and can therefore be handled 
by using neural nets. 
3 Hybr id  System 
Our hybrid system (Fig. J) consists of a neuro 
tagger, which is used as an initial-state an nota- 
i;or, ~nd a rule-based corrector, which corrects 
the outputs of the neuro tagger. When a word 
seque,~ce W t \[see l~q. (2)\] is given, the neuro 
tagger outl)ut a tagging result rN(Wt) for tile 
target word wt at first. The rule-based correc- 
tor then corrects the output of the neuro tagger 
as a fine tuner and gives tile final tagging result 
Ncuro Tagger Rule-Based 
Corrcctor 
Figure 1: Hybrid neuro and rule-based tagger. 
3.1 Neuro  tagger 
As shown in Fig. 2, the neuro tagger consists 
of a three-layer I)erceptron with elastic input. 
This section mainly descril)es the construction 
of inl)ut and output of the nenro tagger, and 
the elasticity by which it; becomes possible to 
use variable length of context for tagging. For 
details of the architecture of l)erceptron see e.g., 
Haykin, 1994 and for details of the features of 
the neuro tagger see Ma and isahara, 1998 and 
Ma, et aJ., 1999. 
lnl)ut I PT  is constructed fi'om word se- 
quence W t \[Eq. (2)\], which is centered on target 
word wt and has length l + 1 + r: 
I PT  = (iptt_ l , . . ' ,  i ph , . . . ,  iph+.,.), (3) 
provided that input length l+ J+r  has elasticity, 
as described a,t tile end of this section. When 
woM wis given in position x (x = t - l , . . . , t+r ) ,  
510 
OPT 
ip t t_  I ...... i l ) \[t_ I ipl t 
11"1" 
il)lt+l ...... i\]Ht+r 
Figure 2: Neuro tagger. 
element ipt ,  of input I PT  is a. weighted pattern, 
defined as 
ipt.,: = :/.,,. (e,,,,('-,, 'e,." ,q,,~), (4) 
where g;,; is the inIbrmation gain which can be 
obtained using information theory (for details 
see Ma and lsahara., 11998) and 7 is the number 
of tyl)es of POSs. l\[' w is a word that apl)ears 
in the tra.ining data, then ea.ch bit e,,,i can be 
obtained: 
= P,,ot,(/ I , , ,) ,  (s) 
where l>'rob(ril'w) is a prior l>robal)ility of r i 
tha.t (;he woM 'w can take,. It is estitnated from 
the t raining (la,ta: 
C _ (<,  "') 
c' ( ,4  ' 
where C'(r ( ,w)  is the number of lfimes both r: 
at++d w al)pea, r , a,nd C(w)  is the number oi' times 
w appears in the training data. 1\[ w is a word 
that does not at)pear ill (,he training data~ then 
each t)it c,,,,i is obtained: 
~ i\['r i is a. candidate 
(7) e.,,,," = (} otherwise, 
where 7,, is the number of P()Ss that the word 
'w Call ta.ke. Output OPT is defined as 
OFT= ,o+), (s) 
provi<led that the output OI)T is decoded as 
S r/ i fO i= l  &O/=0for j? i  
YN(Wt) Unknown otherwise, 
(.9) 
where rN(W,) is the ?a.gging result obtained by 
the neuro tagger. 
There is more inforlnation available for con- 
structing the input for words on the left, be- 
cause they have already been tagged. In the 
tagging phase, instead of using (4:)-(6), the in- 
put can be constructed simply as 
i / , ,_4 = . oPT( - i ) ,  (1()) 
where i = 1, . . . ,1,  and O I )T ( - i )  means the out- 
put of the tagger for the ith word before the 
target word. ltowever, in the training process, 
the out;put of the tagger is not alway.a correct 
a.nd cannot be ted back to the inputs directly. 
Instead, a weighted awerage of the actual output 
a.nd tlm desired output is used: 
iptt_i = 9t- i  ' (wol ,T " 0 PT  ( -  i) + WlOJ,:s " I) l iS) ,  
(1.1) 
where 1)l':,q' is the desired output, 
o: , : ,5 '  : (& , / )2 , . . . ,  
whose bits are defined as 
\] iI' r i is a desired answer 
I)i = 0 otherwise, (la) 
and WOl,'r and w/)l,\],q' are respecLh:ely de\[(ned as 
1'\]013J 
~,:o1 '~ . . . .  (.14) 
1JACT '  
a,nd 
'w>l,; ,s,  = :1 - wopT ,  (15) 
where \]@,uo and \]'JAC'T are  the objective and 
actual errors. Thus, at the beginning of train- 
ing, the weighting of the desired output is largo. 
It decreases to zero during training. 
Elastic inputs are used in the neuro tagger 
so that the length of COlltext is variable in tag- 
ging based on longest context priority. In (te~ 
tail, (l, r) is initially set as large as possible for 
tagging. If rN('Wi) = Unknown,  then (1, r) is 
reduced by some constant interval. This l)ro- 
cess is repeated until rN(W~) 7 k Unknown or 
(1, r) = (0,0). On the other hand, to nmke the 
same set of connection weights of the neu ro tag- 
ger with the largest (1,'r) ava.ilable a.s lnuch as 
511 
possible when using short inputs for tagging, in 
training phase the neuro tagger is regarded as a 
neural network that has gradually grown fi'om 
small one. The training is therefore performed 
step by step from small networks to large ones 
(for details see Ma, et al 1999). 
3.2 Rule-based eorreetor 
Even when the POS of a word can be deter- 
mined with certainty by only the word on the 
left, for example, tile neuro tagger still tries to 
tag based on the complete context. That is, 
in general, what tile neuro tagger can easily 
acquire by learning is the rules whose condi- 
tional parts are constructed by all inpttts ip tx  
(x = t - l , . . . , t  + r) that are .joined with all 
AND logical operator, i.e., ( ip t t - t  & "'" iptt  & 
? .. iptt+~, -+ OPT) .  In other words, it is (lit: 
ticult for tile neuro tagger to learn rules whose 
conditional parts are constructed by only a sin- 
gle input like ( ipt,.  --+ OPT)  ~). Also, although 
lexical information is very important in tagging, 
it is difficult for the neuro tagger to use it, be- 
cause doing so would make the network euof  
mous. Tha.t is, the neuro tagger cannot acquire 
rules whose conditional parts consist of lexical 
information like (w -4 OPT) ,  (w&r  -4  OPT) ,  
and (w~w2 --+ OPT) ,  where w, Wl, and w2 are 
words and 7- is tile POS. Furthermore, because 
of convergence and over-training 1)rol)lems, it is 
iml)ossible and also not advisable to train net> 
ral nets to all accuracy of 100%. The training 
should be stopped at an apl)rol)riate level of a.c- 
curacy. Thus, neural net may not acquire some 
useful rules. 
The transfbrmation rule-based corrector 
makes up for these crucial shortcomings. 
The rules are acquired Dora a training co l  
pus using a set of transformation templates 
by transformation-based rror-driven learning 
(Brill, 1994). Tile templates are constructed 
using only those that supply the rules that tile 
nenro tagger can hardly acquire, i.e., are those 
1)The neuro tagger can also learn this kind of rules 
because it can tag tile word using only ipt, (the input 
of tile target word), ill the case of reducing tile (I, r) to 
(0,0), as described in Sec. a.l. The rules with single 
input described here, however, are a more general case, 
ill which the input call be ipt,~ (~: = t - 1, . . . ,  t + r). 
for acquiring the rules with single input, with 
lexical information, and with AND logica.1 in- 
put of POSs and lexical information. The set of 
templa,tes i  shown in Table 112). 
According to the learning procedure shown 
in Table 2, an ordered list of transformation 
rules are acquired by applying the template set 
to a training corpus, which had ah'eady been 
tagged by the neuro tagger. After tile trans- 
formation rules are acquired, a corl)us is tagged 
as tbllows. It is first tagged by the neuro tag- 
ger. The tagged corpus is then corrected by 
using the ordered list of transformation rules. 
The correction is a repetitive process applying 
the rules ill order to the corptlS, which is then 
updated, until all rules have been applied. 
4 Exper imenta l  Results  
Data :  For our computer experiments, we used 
tile same Thai corpus used by Ma et al (1999). 
Its 10,d52 sentences were randomly divided in- 
to two sets: one with 8,322 sentences for trail> 
ing and the other with 2,1.30 sentences for test- 
int. The training set contained 12d,331 word- 
s, of which 22,311 were ambiguous; the testing 
set contained a4,5~14 words, of which 6,717 were 
ambiguous. For training tile n euro tagger, only 
the ambiguous wor(ls in the. training set were 
used. For training the HMM, all tilt words in 
the training set were used. In both cases, all the 
words in tile training set were used to estimate 
Prob( r i lw) ,  tim probability of "c i that wor(I w 
can be (for details on the HMM, see Ma, et al, 
1999). In the corpus, 4:7 types of POSs are de- 
fined (Charoenporn et al, 1997); i.e., 7 = 47. 
Neuro tagger: The neuro tagger was con- 
structed by a three-layer perceptron whose 
input-middle-outI)ut layers had p z, 2 7 units, 
respectively, where p = 7 ? (1 + I + r). The 
(l + 1 + r) had tile following elasticity. In train- 
ing, tile (I, r) was increased step by step as (71,1) 
-+ (u,2) (a,2) (a,a) a,d gra,dual 
training fl'om a small to a large network was 
pertbrmed. Ill tagging, on the other hand, the 
2)To see whether this set is suitable, a immloer of ad- 
ditional experiments were conducted using various sets 
of templates. The details are described in Sec. 4. 
512 
r ~ I a,1)lc 1: Set o\[' templa.tes for tra.ns\[orln;~tion rules 
Change t;ag v a to t;ag v ? when: 
(single inlm|;) 
( input ('onsists of a POS) 
1. left (right) word is tagged v. 
2. second left (right) word is tagged r. 
3. third left (right) word is ta.gged r. 
(inI)n|; consist;s of a word) 
4. ta.rget word is ~. 
5. left (right) word is w. 
6. second left, (right) word is w. 
(AND logical inpu? ot" words) 
7. l, arget word is 'UO 1 ~tlld left (right) word is wu. 
8. left; (right) word is u,1 and second lcfl, (,'ight) word is w2. 
9. left, word is w~ a.nd right, word is 'wu. 
(AND logical in.lint; of POS and words) 
10. ta.rget word is uq and left (right) word is llaggod r. 
:11. left (righl;) word is .w~ and left. (right) word is tagged r. 
12. ta.rget word is w~, left (right) word is ,w.,, and left (right) word is tagged r. 
Ta,1)le 2: l)roetdure for learning transi'orma,tion rules 
1. Apply neuro taggtr to training corpus, which is then updated. 
2. Compare tagged results with desired ones and find errors.  
3. Ma.teh templates l'or all errors and obtain set of tra.nsformation rules. 
d. Select rule in corpus with the maximum value of' (cn l , _qood-h .  cnl,_bad), where 
cnZ_qood: number that transforms incorrect ags to correct elliS: 
c'nl._bad: number that transforms correct tags to incorrect ones, 
h: weight to control the strict, hess of generating 1;he rule. 
5. Apply selecttd rule to training corpus, which is then updated. 
6. Append selected rule to ordered list o1" trausl'orma.tion rules. 
7. Ih'4)eal; steps :2 through (j until no such rule can I)e selected, i.e., c 'n , t _good-  
h,. cnl,_bad < O. 
(l, 'r) was inversely reduce(l ste l) by step as (3,3) 
-+ (3,2) (2,2) (2,:1) O,:l) (:l,o) 
(0,0) a.s needed, provided tJlat the number of 
units in the middle layer was kept a.t the ma.xi- 
I l l  l l l l l  vahle. 
Ru le -based  cor re t to r :  The parameter h in 
the tw~Juat;ion function (cnl ,_9ood - h, . c'M._bad) 
used in 1;he learning procedure (Table 2) is a 
weight to control the strictness of generating a. 
rule. IF It is large, the weight of cnt_bad is la.rge 
and the possibility of generating incorrect rules 
is reduced. By regarding the neuro tagger as ~d- 
ready having high accuracy and using tile rule-- 
based correcter as a fine tuner, weight h. was set 
to a. large vahm, 100. Applying |;lit templates 
Co the training corptm, which had already been 
tagged 1) 5, the neuro ta.gger, we obta.ined a.n or- 
dered list; of 520 transfbrmation rules. '.l'~d)le 3 
shows the first 15 transfbrmation rules. 
Results :  Table 4 shows the resull;s of I)()S tag- 
ging for the testing data.. In addition to the 
accuracy o\[" the neuro tagger and hybrid sys- 
tem, the ta.ble also shows tile accuracy of a, bast- 
line model, the IIMM, and a rule-based model 
\['or comparison. The baseline model is one that 
performs tagging without using the contextual 
inlorma.tion; instead, it performs ta.gging using 
only f'requency informa.tion: the proba.bility of 
P()S that; tach word can be. The rule-based 
model, to be exact, is also a hybrid system con- 
513 
'l'a.1)le 3: First 15 transfbrmation rules 
No. F rom To  Cond i t i on  
1 PREL 
2 PREL 
3 Unknown 
4 XVHI4 
5 VATT 
6 Unknown 
7 NCI4N 
8 VATT 
O PREL 
i0 VST~ 
ii VfiTT 
12 NCMN 
13 NCHN 
14 Unknown 
15 NCNN 
RPRE le f t  word is punctuation and r ight  tuord is 5~gu 
RPRE le f t  yard is ~ 
RDVN le f t  ~ord Ls tagged XVfiE 
XVBH le f t  word is II~D 
flDVN le f t  word is  ~ 
VRTT le f t  word is  tagged PREL 
RPRE le f t  word is ua 
VSTfi l e f t  word is ~q~ 
RPRE r ight  word is ~gu and second r ight  word is a~q;J 
RDVN target word is ~t~4 
ADVN target  word is  ~4~ 
RPRE target word is n14 and le f t  word is eentluu 
RPRE le f t  word is ~tt and le f t  ward is tagged NCHN 
fiDVN th i rd  le f t  word is tagged WCT 
CNIT taPget ~ord is nn~ 
where PREL: Relat ive Pronoun, RPRE: Preposit ion,  fiDVN: fidverb with normal form . . . .  
Table d: Results of POS ta,gging for testing data* 
model baseline IIMM rule-based lleuro hybrid 
accuracy 0.836 0.891 0.935 0.944 0.955 
*Accurac9 was determiued only for am lfiguous words. 
sisting of an initial-state annotator and a set of 
transformation rules. As the initial-state anno- 
b~tor, however, the baseline model is used in- 
stea.d of' the neuro tagger. And, its rule set. has 
1,177 transformation rules acquired h'om a more 
general teml)late set, which is described at the 
end of this section. The reason for using a gener- 
al template set is that the sol; of tra.nsibrma.tion 
rules in the rule-based model should be the main 
annotator, not a fine post-processing tuner. For 
the same reason, the parameter to control the 
strictness of generating a rule, h, was set to  a 
small value, \], so that a larger number of rules 
were generated. 
As shown in the table, the accuracy of the 
nenro tagger was far higher than that of the 
HMM and higher than that of the rule-based 
model. The accuracy of the rule-based mod- 
el, on the other hand, was also far higher than 
that of the IIMM, ~lthough it was inferior to 
that of the neuro tagger. The accuracy of the 
hybrid system was 1.1% higher than that of the 
neuro tagger. Actually, the rule-based corrector 
corrected 88.4% and 19.7% of the errors made 
by the neuro tagger for the training and testing 
data, respectively. 
Because the template set shown in Table 1 
was designed only to make up for the short- 
comings of the neuro tagger, tile set is smal- 
l compared to that used by Brill (1994). To 
see whether this set is la.rge enough for our sys- 
tem, we perlbrmed two additional experiments 
in which (\]) a sol; constructed 193' adding the 
templates with OR logical input of words to the 
original set and (2) a, set constructed 1)5' fnrther 
adding the templates with AND and OR logi- 
cal inputs of POSs to the set of case (1) were 
used. The set used in case (2) inclnded the set 
used by Brill (\]994) and all the nets nsed in our 
experiments. It was also used for acquiring the 
transformation rules in the rule-based model. 
The experimental results show that compared 
to the original case, the accuracy in case (1) 
was improved very little and the accuracy in 
case (2) was also improved only 0.03%. These 
results show that the original set is nearly la.rge 
enough for our system. 
To see whether tile set is snitable tbr our 
system, we performed ~tn additional experimen- 
t using the original set in which the templa.tes 
with OR logical inputs were used instead of the 
templates with AND logical inputs. The accu- 
racy dropped by 0.1%. Therefore, tile templates 
with AND logical inputs are more suitable than 
514 
those with O11 logical inputs. 
We also performed an experiment using a 
template set without lexical intbrmation. In this 
case, l;he accuracy dropl)ed by 0.9%, indicating 
that lexical informatioll is important in tagging. 
To determine the effect o1' using a. large h, 
for generating rules, we per\['ormed an experi- 
ment with h = 1. In this case, the accuracy 
dropped by only 0.045%, an insignifica.nt differ- 
ence compared to the case of h, = 100. 
By examining the acquired rules that were 
obtained by al)plying the most COml)lete tem- 
plate set, i.e., the set used in case (2) described 
above, we found that 99.9% of them were those 
that can be obtained by a.pl)lying the original 
set of templates, rl'ha.t is, the acquired rules 
were almost those that are dif\[icult \['or the neu- 
re tagger to acquire. '.l'his rein forced our expec- 
tat;ion that the rule-based al)l)roach is a well- 
suited method to cope with the shortcoming of 
the neuro tagger. 
Finally, il, should 1)e noted that ill the liter- 
atures, tile tagging a.ccuracy is usua.lly delined 
by counting a.ll tile words regardless of whether 
they are a.nlbiguous or not. If we used this dell- 
nil:ion, t\]le accura.cy of our hybrid system would 
be 99.1%. 
5 Conc lus ion  
To collstruct a 1)tactical tagger that needs as 
little training data. a.s possible, neuro taggers, 
which have high generalizing al)ility and there- 
fore a.re good at dealing with the problems ofda~ 
ta. sl)a,rseness, have been proposed so fa.r. Neu- 
re tatters,  however, have crucial shortcomings: 
they ca.nnot utilize lexical information; they 
have trouble learning rules with single inputs; 
and they cannot learn training data to an ac~ 
curacy of 100%. To make up for these short- 
comings, we introduced a rule-based correcter, 
which is constructed by a. set of trans\[brma.tion 
rules obtained by error-driven learning, for post 
1)recessing and constructed a hybrid tagging 
system, l{y examining the transtbrma.tion rules 
acquired in the computer experiments, we found 
that 1;he 99.9% of them were those that; the neu- 
re tagger can hardly acquire, even when using a. 
template set including t;hose for generating the 
rules that the neuro tagger can easily acquire. 
This reinlbrced our expecta.tion that the rule- 
based approach is a well-suited method to cope 
with the shortcoming of the neuro tagger. Com- 
puter experiments showed that 19.7% of the er- 
rors made by the neuro tagger were corrected 
by the tra.nslbrmation rules, so the hybrid sys- 
tem rea.ched an accuracy of 95.5% counting only 
the ambiguous words and 99.\]% counting all the 
words in the testing data, when a small corpus 
with only 22,311 ambiguous words was used tbr 
train int. ~l'h is ind icates thai; ou r tagging ,qystem 
can nearly reach a pra.ctica.l level in terms of tag- 
ging accuracy even when a small Thai corpus is 
used tbr tra.ining. This kind of tagging system 
can be used to constructs multilingua.1 corpora 
that include languages in which large corpora 
have not yet been constructed. 
References  
l~rill, E.: Transfornmtion-based rror-driven lca.rn- 
ing and natural language processing: ~ case s- 
tudy ill 1)art-of-sl)eech tagging, Computational 
Li~g'uistics, Vol. 21, No. 4, pp. 543-565, 199~1. 
Cha.roenporll, T., Sornlertlanlva.nich, V., ~md Isa- 
hara, 11.: Building a la.rge Thai text corpus 
parl; of speech tagged corpus: OI{CIlll), Pro< 
Natural Language Processi~fl Pacific lNm ,5'gn~- 
po.du'm \[997, Phuket, Thailand, pp. 509-5\]2, 
1997. 
I)aelemans, W., Z~wrel, a., Berck, P., and C,i/lis, S.: 
MI3'I': A m<mlory-based pm-t of speech tagger- 
genera.tot, P'roc. /tl.h Workshop on Very Large 
Co,'po~zl, Copenhagen, l)em na.rk, pp. 1-1+1, 99(5. 
l\]aykin, S.: Neural Nchvorlcs, Macmillan College 
Publishing Coral)any, Inc., 199/t. 
Ma, Q. and lsahm'a., H.: A multi-neuro tagger us- 
ing variable lengths of contexts, Prec. COLING- 
ACL'g8, Montreal, pp. 802-806, 1998. 
Ma, Q., Uchimoto, K., Mura.ta, M., and 1sahara H.: 
F, lastic neural networks tbr part of speech tag: 
ging, Prec. IJCNN'99, Washington, \])C., pp. 
2991-2996, 1999. 
Meriaklo, B.: Tagging English text with a proba- 
bilistic model, Computational Linguistics, Vo\]. 
20, No. 2, pp. 1.55-171, 19(.)4. 
Quinla.n, 3.: G'~.5: Programs Jot Machine Learning, 
San Mateo, CA: Morgan Kaufinann, 1993. 
Schmid, 1t.: l'art-of-speech tagging with neural net- 
works, Prec. COLING'94, Kyoto, Japan, pp. 
172-176, 1994. 
515 
Bunsetsu  Ident i f i ca t ion  Us ing  Category -Exc lus ive  Ru les  
Masaki Murata Kiyotaka Uchimoto Qing Ma Hitoshi Isahara 
C()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications 
588-2, \]waoka, Nishi-ku, Kobe, 651-2d92, Japan 
? - j  ( ' . tel:-k81- 78-969-2 \]81 tax: +81- 78-369-2189 http://www-karc.crl, go.j p/ips/murata 
{ murata,u(:himoto,qma,isahara}(@crl.go.ji) 
Abstract 
This pal>or describes two new bunsetsu identificatkm 
methods using supervised learning. Sin(:e ,Jat)anese 
syntactic analysis ix usnally done after bunsetsu 
identification, lmnsetsu identiiieation is iml)orl;ant 
for analyzing Japanese sentences. In experiments 
comparing the four previously available machine- 
learning methods (decision tree, maximmn-entropy 
method, example-based apI)roaeh and deeiskm list,) 
an(l two new methods llSing categot'y-exclusive rul s~ 
the new method using l;he category-exclusive rules 
with the highest similarity t)erformetl best. 
1 Introduction 
This paper is about machine learning methods for 
identifying bwnsr'ts'~zs, which correspond to English 
phrasal units such as noun phrases and t)rel)ositional 
phrases. Since .Japanes(.' syntactic analysis ix usu- 
ally done after lmnselisu identitication (Uchimot;o el; 
a\].. 1999), i(lentitlying lmnsetsu is important l'or an- 
alyzing ,J;~p~tnese ltt(}tl(:es. The  conventional stud- 
ies on lmnsetsu  identitieation ~ have used hand-made 
rules (Kameda, \]995; Kurohashi, 3998), })ill; bun- 
sel;su identification is not an easy task. Conventional 
studies used many hand-nmde rules develot)ed at the 
cost of many man-hours. Kurohashi, tbr examl)le, 
made 146 rules for lmnsetsu identification (Kuro- 
hashi, 1998). 
Itl }l.tl a . t te t l lp t  to reduce the mnnber of man- 
hours, we used machine-learning methods for bun- 
setsu identitication. Because it; was not clear which 
machine-learning method would 1)e the one most al)- 
propriate for bunsctsu identification, so we tried a 
variety of them. In this paper we rei)orl; ext)er- 
inlel lts comparing tbur inachine-learning me, thods 
(decision tree, maximmn entropy, example-based, 
and decision list; methods) and our new methods us- 
ing category-exclusive rules. 
l lhmsetsu ideni,illcation is a ln'oblem similar to ohm,king 
(lLamshaw and Marcus, 1995; Sang and \h;ellsl;ra, 1999) in 
other l;mguages. 
2 Bunsetsu identification problem 
We conducted experiments (m the following super- 
vised learning methods tbr idel~tiflying }mnsetsu: 
? \])eeision {;l'ee method 
? Max i lnun l  ent ropy  method  
? Examt)le-based method (use of sinfilarity) 
? Decision list (use of probability and frequency) 
? Method 1 (use of exclusive rules) 
? Method 2 (use of exclusive rules with the high- 
est similarity). 
In general, t)misetsu identification is (tone afl;er 
morl)hological and l)efore syntactic analysis. Mor- 
1)hological analysis correst)onds to part-of-st)ee(:h 
tagging ill English. Japanese syntactic structures are 
usually ref)resented by the. relations between lmn- 
setsus, which correspond to l)hrasal units such as a 
noml l)hrase or a t)repositional 1)hrase in \]r, nglish. 
St), 1)unsetsu identification is imi)ortant in .lnpanese 
sentence mmlysis. 
In this paper, we identit\[y a bunsetsu by using 
intbrmation Dora a morl~hological nalysis. Bun- 
setsu identitication is treated as the task of deciding 
whether to insert a "\[" mark to indicate the partition 
between two hunsetsus as in Figure 1. There, fore, 
bunsetsu identilical;ion is done by judging whether a
partition mark should be inserted between two adja- 
cent nlorphemes or not. (We. do not use l;he inserted 
partition mark in the tbllowing analysis ill this paper 
for the sake of simplicity.) 
Our lmnsetsu identification method uses i;t1(} lilOr- 
phok)gk:al intbrmation of the two preceding and two 
succeeding morphemes ofan analyzed space bel;ween 
two adjacent morphemes. We use the following mor- 
phological information: 
(i) Major part-of  speech (POS) category, 2 
(ii) Minor P()S category or intlection tYl)e, 
(iii) Semantic information (the first three-digit nun> 
bet of a category nmnlmr as used ill "BGIt" 
(NLI{,I, 1964:)), 
2Part-of-spec.ch ~ttegories follow those of 3 \[/MAN (Kuro- 
hashi and N~tgao, 1998). 
565 
bohu .qa 
(I) nominative-case particle 
(I identify bunsetsu.) 
\[ bunsetsu wo 
(bunsetsu) objective-case particle 
I matomeagcru 
(identify) 
Figure 1: Example of identified bunsetsus 
Major POS 
Minor POS 
Semantics 
Word 
bun wo ~ugiru 
(sentence) (obj) (divide) 
((I) divide sentences) 
Noun Particle Verb 
Normal Noun Case-Particle Normal Form 
x None 217 
x wo ku.qiru 
Symbol 
Punctuation 
X 
X 
Figure 2: hfformation used in bunsetsu identification 
(iv) Word (lexical iifformation). 
For simplicity we do not use the "Semmltic infor- 
matioif' and "Word" in either of the two outside 
morphemes. 
Figure 2 shows the information used to judge 
whether or not to insert a partition mark in the space 
between two adjacent morphemes, "wo (obj)" and 
"kugiru (divide)," in the sentence "bun wo kugiru. 
((I) divide sentences)." 
3 Bunsetsu  ident i f i ca t ion  process  fo r  
each  mach ine- learn ing  method 
a.1 Deeision-tree method 
In this work we used the program C4.5 (Quinlan, 
1.995) for the decision-tree l arning method. The 
four types of information, (i) major POS, (ii) mi- 
nor POS, (iii) semmltic information, and (iv) word, 
mentioned in the previous section were also used 
as features with the decision-tree l arning method. 
As shown in Figure 3, the number of features is 12 
(2 + 4 + 4 + 2) because we do not use (iii) semantic 
information and (iv) word information from the two 
outside morphemes. 
In Figure 2, for example, the value of the feature 
'the major POS of the far left morpheme' is 'Noun.' 
a.2 Maximum-entropy method 
The maximum-entropy method is useful with sparse 
data conditions and has been used by many re- 
searchers (Berger et al, 1996; Ratnaparkhi, 1996; 
Ratnaparkhi, 1997; Borthwick el; al., 1998; Uchi- 
moto et al, 1999). In our maximuln-entropy exper- 
iment we used Ristad's system (Ristad, 1998). The 
analysis is performed by calculating the probability 
of inserting or not inserting a partition mark, from 
the output of the system. Whichever probability is 
higher is selected as the desired answer. 
In the maximum-entropy method, we use the same 
four types of morI)hological information, (i) major 
POS, (ii) minor POS, (iii) semantic information, and 
(iv) word, as in the decision-tree method. However, 
it, does not consider a combination of features. Un- 
like the decision-tree method, as a result, we had to 
combine features mmmally. 
First we considered a combination of the bits of 
each morphological information. Because there were 
four types of information, the total number of com- 
binations was 2 ~-  1. Since this number is large 
and intractable, we considered that (i) major POS, 
(ii) minor POS, (iii) semantic information, aim (iv) 
word information gradually becolne inore specific in 
this order, and we coml)ined the four types of infor- 
mation in the following way: 
Information A: (i) major POS 
Intbrmation B: (i) major POS and (ii) minor POS 
hfformat, ion C: (i) major POS, (ii) minor POS and 
(iii) semantic information 
Information D: (i) major POS, (ii) minor POS, 
(iii) semantic informa~aion a d (iv) word 
(~) 
We used only Information A and B for the two out- 
side morphemes because we (lid not use semantic 
and word information in the same way it is used in 
the decision-tree inethod. 
Next, we considered the combinations ofeach type 
of information. As shown in Figure 4, the number 
of combinations was 64 (2 x 4 x 4 x 2). 
For data sparseness, in addition to the above com- 
binations, we considered the cases in which frst, one 
of the two outside morphemes was not used, sec- 
ondly, neither of the two outside ones were used, m~d 
thirdly, only one of the two middle ones is used. The 
nmnber of features used in the maximum-entropy 
method is 152, which is obtained as follows: a
3When we extr~,cted features  f rom all of  the  ar t ic les  on  
566 
Far M't mort)henm Left morl)heme 
( Ma.ior POS '} 
~'Major POS'\[ ,J Minor POS 
I, Minor POgJ +/S,mmntic Information + 
( \?ord , 
2 4 
Right morl)hemc Far right mort)heine 
Minor POS f Ma.ior POS ~ 
Semantic Infi)rmation ' + \[ Minor POS j 
Word 
4 2 
Figure 3: Features used in the decision-tree method 
Far left morl)heme 
Information 
hffbrnmtion A } 
2 
Left morpheme Right morl)hcme 
( In format ion!} (Infbrmation A
J hfformation J Intbrmation B
/hfformation & ~ hfformation C
I, Infbrmation 1, hffbrmation D
4 4 
Far right morpheme 
J" Information 
& \[hlformation B A } 
2 
Figure 4: lJ~,atm'es used in the maximunt-entrol)y met.hod. 
No. of t>atures= 2 x 4 x 4 x 2 
+2 x 4 x 4 
+ 4 x 4 x 2 
+ 4 x 4 
+ 4 
+ 4 
= 152 
In Figure 2, (;lie feature that uses Infornultion 
B in the far left morl)heme, Infbrnmtion D in the 
left mort)heine, Information C in the right mor- 
pheme, and Information A in the fa.r right mop 
l/heme is "Noun: Nornml Noun; Particle: Case- 
Particle: none: wo; Verl): Nornml Form: 217; Sym- 
bol". In tim maximmn-entrol)y method we used for 
each space 152 ligatures uch as this ()tie. 
3.3  Example -based  method (use of  
s imi lar i ty)  
An example-based method was t)rollosed t) 3, Nagao 
(Nagao, 1984) in an attempt to solve I)roblenls in 
machine translation. To resolve a. l)rol)h'm, it; uses 
the most similar (;xami)le. In the i)resent work, the 
examt)le-1)ased method imt)artially used the same 
four types of information (see Eli. (1)) as used in 
the maxinmm-entrotly method, 
To use tills method, we must define the similarity 
of an ini)ut to an example. We use the 152 1)atterns 
fl'om the maximum-entropy method to establish the 
level of similarity. We define the similarity S be- 
tween all input and an exmnl)le according to which 
one of these 152 levels is the lnatching level, as fol- 
lows. (The equation reflects the importance of the 
two middle morphemes.) 
January 1, 1995 of a Kyoto University corpus (l;hc mnnber of 
spaces between mrl)henms was 25,81d) by using this method, 
the nunfl)e,r of types of features was 1,534,701. 
S = s(m_t) x s(m-H) x 10,000 
+ s(m_2) x s(.q.~) (2) 
Here m- l ,  m4-\], m-2, and m+2 refer respectively to 
the left;, rigid;, far M't, ;rod far righl; mortflmnms, and 
s(x) is the mort)hological similarity of a ll lOl't)hell le 
x, which is defined as follows: 
s(x) =1 (when no information of x is matched) 
2 (when Information A of x is matdmd) 
3 (when hfl~)rmal:ion B of x is mate, heal) 
4 (when Information C of x is mat;cited) 
5 (when Information D of x is matched) 
(a) 
Figure 5 shows an exmnple of the levels of sim- 
ilarity. When a pattern matches Information A of 
all four lnort)henies , uch as "Noun; Particle; Verb; 
Symbol", its similarity is 40,004 (2 x 2 x 10,000 + 
2 x 2). When a pattern matches a pattern, such as 
" ; Particle: Case-Particle: none: wo; ; ", its 
similarity is 50,001 (5 x 1 x 10,000 + 1 x 1). 
The exmnl)le-1)ased method extracts the exam- 
ple with the highest level of similm'ity and checks 
whether or not that exami)le is marked. A partition 
marl{ is inserted in tile input data only when the ex~ 
amt)le iv marked. When multit)le exalnl)les have the 
same highest level of similarity, the selection of tile 
best example is ambiguous, hi this case, we count 
tile number of nlarked and mlinarked sl)aces in all 
of the examples and choose the larger. 
a .4  Decis ion- l ist  method (use of p robab i l i ty  
and f l ' equeney)  
T i le  decision-list method was proposed by Rivest 
(Rivest, 1987), in which tile rules are not expressed 
as a tree structure like in the decision-tree method, 
567 
No information 
hffornmtion A
Information B
Information C
Information D
bun wo kugiru 
(senWnce) (obj) (divide) 
S(X) ?~\]'- 2 7D,--1 ?/Zq-1 '11~'+2 
1 . . . .  
2 Noun Particle Verb Symbol 
3 Normal Noun Case-Particle Normal Form Punctuation 
4 x None 217 x 
5 x wo kugiru x 
Figure 5: Exmnple of levels of similarity 
but are expanded by combining all the features, and 
are stored in a one-dimensional list. A priority or- 
der is defined in a certain way and all of the rules 
are arranged in this order. The decision-list method 
searches for rules Dora tile top of the list and an- 
alyzes a particular problem by using only the first 
applicable rule. 
In this study we used ill the decision-list method 
the same 152 types of patterns that were used in/;lie 
ma.ximuln-entropy method. 
To determine the priority order of the rules, we re- 
ferred to Yarowsky's method (Yarowsky, 1994) and 
Nishiokwama's method (Nishiokaymna et al, 1998) 
and used the probability a.nd frequency of each rule 
as measures of this priority order. When nnlltiple 
rifles had the same probability, the rules were ar- 
ranged in order of their frequency. 
Suppose, for example, that Pattern A "Noun: 
Normal Noun; Particle: Case-Particle: none: wo; 
Verb: Normal Form: 217; Symhol: Punctuatioif' 
occurs 13 times in a learlfing set and that tell of 
the occurrences include the inserted partition Inal:k. 
Suppose also thai; Pattern B "Noun; Particle; Verb; 
Symbol" occurs 12a times in a learning set and that 
90 of the occurrences include the mark. 
This exmnple is recognized by the following rules: 
Pattern A ~ Partition 76.9% (10/ 13), Freq. 23 
Pattern B => Partition 73.2% (90/123), Freq. 123 
Many similar rules were made and were then listed 
in order of their probabilities and, for any one prob- 
ability, in order of their frequencies. This list was 
searched from tile top ml(:l the answer was obtained 
by using the first, ai)plicable rule. 
3.5 Method  I (use of eategory-exe lus ive  
rules) 
So far, we have described the four existing machine 
learning methods. In the next two sections we de- 
scribe our methods. 
It is reasoimble to consider tile 152 patterns used 
in three of the previous methods. Now, let us sup- 
pose that the 152 patterns fl'om the learning set yield 
the statistics of Figure 6. 
"Partition" means that the rule determines that a 
partition mark should be inserted in the input data 
and "non-t)arl:ition" ineans that tile rule determines 
that a partition mark should not be inserted. 
Suppose that when we solve a hypothetical prob- 
lem Patterns A to G are apt)licable. If we use the 
decision-list inethod, only Rule A is used, which is 
applied first, and this determines that a partition 
mark should not be inserted. For Rules B, C, and 
D, although the fl'equency of each rule is lower thml 
that of Rule A, tile suln of their frequencies of the 
rules is higher, so we think that it is better to use 
Rules B, C, ml(t D than Rule A. Method 1 follows 
this idea, but we do not simply sum up tile frequen- 
cies. Instead, we count the munber of exalnples used 
ill Rules B~ C, and D and judge the category having 
tile largest number of exmnplcs that satisfy the pat- 
tern with the highest probability to be the desired 
ai1swer. 
For exmnple, suppose that in the above examt)le 
the number of examples atis(ying Rules B, C, and 
D is 65. (Because some exmnples overlq) in multi- 
pie rules, the total nunfl)er of exalnples is actually 
smaller than the total number of tile frequencies of 
the three rules.) In this case, among the examples 
used by the rules having 100% probability, tile nmn- 
ber of examples of partition is 65, m~d the number 
of examt)les of non-t)artitioi~ s 34. So, we deternline 
that tile desired answer is to partition. 
A rule having 100% probability is called a 
category-exclusive rule because all the data satist~y- 
ing it belong to one category, which is either parti- 
tion or noi>partition. Because for any given space 
the number of rules used call be as large as 152, 
category-exclusive rules are applie(t often ~. Method 
1 uses all of these category-exclusive rules, so we call 
it tile method using category-exclusive rules. 
Solving problems by using rules whose prol)abili- 
l;ies are nol; 100% may result ill the wrong solutions. 
Almost all of the traditional machine learning meth- 
ods solve problelns by usiug rules whose i)robabilities 
4'l'he ratio of the spaces analyzed by using category- 
exclusive rules is 99.30% (16864/16983) in Experiinent 1 of 
Section d. This indicates that ahnost all of the spaces are 
analyzed by category-exclusive rules. 
568 
\]{,uIe A: 
l{,uh.' B: 
Rule C: 
Rule D: 
Rule E: 
Rule F: 
Rule G: 
l 'attern A 
Pattern \] 
Pattern C 
Patl;crn \]) 
Patt;ern 1'3 
Pal;l;erll F 
Pat tern  G 
-?- I)rol)ability of non-i)al|;ition 
=> probability of partition 
=> i)rolml)ility of partition 
=> probal)ility of 1)artition 
:~ i)robability of partition 
:~ probability of parl:ition 
=> probability of non-partition 
m0% (a4/34)  
100% (,33/a3) 
\]oo% (25/2s)  
J()0(X, ( 1!)/ 19) 
8J.3% (1?)?}/123) 
76.9% ( 10/ ~a) 
57.4% (31{)/540) 
Figm'e 6: ;Ill (',Xaml)h; of rules used in Method :l 
1,?equency 34 
Frequency 33 
Frequency 25 
Frequency 19 
Frequcncy 123 
Fl'o, qucncy \] 3 
lq'equc, ncy 540 
are not J(}0%. By using such methods, we cannot 
hol)e to improve, a,:curacy. If we want to improve ac- 
(;llra(;y~ we nlllst use catt;gory-excltl,qive l'lllCS. The l 'e  
are some eases, however, tbr which, even if we take 
this at)l)r()ach, eategory-exchlsive rules aye rarely al> 
plied. In such cases, we lilllSl; add new feai;ures t() 
I;he mlalysis to create a situation in which many 
c i~tegory -exeh ls ive  ru les  Call \])(; appli(~d. 
I\]owever, il; is not suflieient to use  t ;~/tt~ory- 
exclusive rules. There arc, many nmaningless rules 
which \]la,1)l)ell to  1)e c~tl;egor3~-ex(;lll,qive o l l ly  ill ~t 
learning set. We lllllSt consider how to (~Iimim/te 
such meaningh;ss rule,q. 
3.6  Method  2 (r ising category -exc lus ive  
ru les  w i th  the  h ighest  sint i lar it ;y) 
Method 2 combines the, exami)h>based method and 
Method 1. That  is, it; combines the. method using 
similarity m~d the method usint'~ category-exchlsive 
rules in order to eliminate the meaningless (:art;gory- 
exclusive rules ment;ion(;(l i l lhe 1)revious t;el;ion. 
Mel;ho(1 2 also uses 152 patl(!rus for i(tentillving 
\])llllS(',|;,qll. q~h(!s(, ~ t)~li;l,(!l'll~q ~/l'(? llF,(RI ;IS rules i~ the 
,q;lllle ~,\;;ty }is ill Met\ ] lo( \ ]  \]. l)esir('d HIISWtH',q ill'(; (h;t(; l-  
mined by using the rule. having the high(>t probabil- 
ity. When mull;iple rules have the same 1)rolmbility, 
M(;thod 2 uses the wdue of the similarity described 
in the section of the examl)h>based m(;thod and }lll- 
alyzes the 1)robk:m with the rule having the highest 
simihu'ity. When multiple rules have th(; stone prob- 
nbilil;y and similm'ity, the method takes the exam- 
pies used by the rules having the highest probabil ity 
and the higlmst si,nilarity, and chooses the (:ategory 
with the larger llllllI))CF Of exami)les as t:hc desired 
answer~ in the same way as in Method 1. 
Itowever, when (:ategory-c.xchlsive rules having 
l l lt iro tha l l  Olte fre(l l lel lCy exist, the a})ove t ) roccdl l r ( ;  
is performed after el iminating all of the category- 
exclusive rules having one frequency, in el;her words, 
category-exclusive rules having more than one fl'e- 
quency are giwm a higher priority than category- 
exclusive rules having only one. flTo.qll(;nc.y })lit hav- 
ing ~ high(w similarity. This is 1)c(:ause eategory- 
(;xclusivc rules having only one fl'equen(:y are not so 
reliabh',. 
4 Experiments and discussion 
In our experiments we used a Kyoto  University text 
eorlms (Knrohashi and Nagao, 1997), which is a 
tagged corpus made Ul) of articles fi'om the Mainichi 
newspaper. All exl)eriments reported in this paper 
we.re performed using art, ielcs dated fi'om ,\]mmary 
\] to 5, 1995. We obtained the correct infi)rnmtion 
()n morphoh)gy and }mnse.t;su identiticathm from the 
tagged corpus. 
The following experiments were conducted to de- 
termine which supervised \]earnillg~ lnethod achieves 
the high<'.st a(:Cllra(:y l~tl;e. 
? Exlmriment \] 
\[,(;arninl,; set: ,Janllary 1, 1.995 
~J?t;Sl; st?t: ,\]atlll~/l'y 3, 1.995 
? \]';xt)eriment 2 
Learning set: 3ammry 4, 1995 
Test set: .\]a.nuary 5, 1995 
ltecm>e we used F, xlmriment \] in maki,lg Method 
I and Method 2, \]i;Xl)erinieut 71 is a ch)sc'd data..~et 
for Mel:l~od \] and Method 2. So, we l)crformed Ex- 
lmriment 2. 
The ,'(;suits arc. listed in '12fl)lt;,q I to d. \Ve used 
KNP2.0b4 (Kurohashi, 11997) mM KNP2.0t/6 (Kuro- 
hashi: 1998), which are bmlsetsu identitication and 
syntael;i(" analysis systems using tmmy hand-made 
rules in addition 1;o the six methods des(:ribed in 
Section 3. Be('mtse KNP is not based on a machine 
learning inethod but :many hand-made rules, in the 
KNP results "Learning selY and '~'.Test et" in the ta- 
llies have nt) meanings. In the eXll(wiment of KNP, 
we also uses morphological information in a corpus. 
~\].~hc ';F': ill l;\]le tables indicates the F-measure~ which 
is the. harmonic mean of a recall and a precision. A 
recall is l;he fl'action of correctly identilied partit ions 
out of all the partitions. A t)reeision is the ffae- 
th)n of correctly identitied partit ions out of all the 
SlmCeS which were judged to have a partit ion mark  
inserted. 
Tables I to -/I show the. following results: 
? In the test set I;he dc.cision-tree method was 
a little better thmt the maximmn-entropy 
569 
Table 1: Results of learning set of Exper iment  1 
Method D ~ 
Decision %-ee 99.58% 
Maximum Entropy 99.20% 
Example-Based 99.98% 
Decision List 99.98% 
Method 1 99.98% 
Method 2 99.98% 
KNP 2.0114 99.23% 
KNP 2.0116 99.73% 
Recall Precision 
99.66% 99.51% 
99.35% 99.06% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
99.78% 98.69% 
99.77% 99.69% 
The number of spaces between two )nor flmmes is 
25,814. The number of lmrtitions is 9,523. 
Table 3: Results of learning set of Exi)er iment 2 
Method 
Decision Tree 
Maximum Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.01)4 
KNP 2.066 
99.07% I 
99.99% I 
99.99% I 
99.99% I 
99.99% I 
98.94% I 
99.47% I 
RecaU Precision 
99.71% 99.69% 
99.23% 98.92% 
100,00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
99.50% 98.39% 
99.47% 99.48% 
The mlmber of spaces between two mor )heines is 
27,665. The number of partitions is 10 143. 
Table 2: Results of test set of Exper iment  1 
Method 
~ ion  Tree 
Maximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.066 
- F Recall 
98.87% 98.67% 99.08% 
98.90% 98.75% 99.06% 
99.02% 198.69% 99.36% 
98.95% i 98.43% ! 99.48% 
98.98% 198.54%! 99.43% 
99.16% I 98.88% ' 99.45% 
99.13% 99.72% ! 98.54% 
99.66% ~_99.68% ! 99.64% 
:)heroes is The lmmber of spaces between two mor 
Precision 
16,983. The mmiber of partitions is 6,166. 
T~ble 4: Results of test set of Exper iment  2 
Method 
Decision Tree 
M aximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.01)6 
P RTcad I Precision 
98.50% 98.51% I 98.49%- 
98.57% 98.55%1 
i 98.82% 98.71%1 
198.75% 98.27%1 
i 98.79% 98.54% I 
i 98.9\[1% 98.65% I 
199.(/7% 99.43%1 
L 99.51% 99.40% ~_ 
98.59% 
98.93% 
99.23% 
99.43% 
99.15% 
98.71% 
99.61% 
The nmnber of" spaces between two morphemes i
32,3o4. The number of partitions is 11,756. 
method.  Al though the maximuln-entropy 
method has a weak point  in that  it, does not 
learn the combinat ions of features, we could 
overcome this weakness by malting almost all of 
the combilmtions of features to produce a higher 
accuracy rate. 
? Tile decision-list n lethod was bet ter  t itan the 
maximum-entropy method in this experinmnt.  
? Tile example-based nlethod obtained the high- 
est accuracy rate among the four exist ing meth- 
ods. 
? Altt lough Method 1, which uses tim category- 
exclusive rule, was worse than the exmnple- 
based method,  it was better  than ti le decision- 
list method.  One reason for this was that  
ti le decision-l ist metl lod chooses rules rmldomly 
when mult iple rules have identical probabi l i t ies 
mid fl'equeneies. 
? Method 2, which uses the category-exchlsive 
rule with the highest similarity, achieved the 
highest accuracy rate among ti le supervised 
learning methods.  
? Tim example-based method,  tim decision-l ist 
inethod, Method 1 and Method 2 obta ined ac- 
curacy rates of about  100% for the leanfing set. 
This indicates that  these methods m:e especial ly 
strong for learning sets. 
? Tile two methods using similarity example-  
based method mid Method 2) were always bet-  
ter than the other methods, indicat ing that  the 
use of s imilar ity is eflective if we can define it 
approl)r iately. 
? We carried out experinmnts by using KNP,  a 
system that  uses ninny ha.nd-made rules. The 
F-measure of KNP was highest in the test set. 
? We used two versions of KNP, KNP 2.0b4 and 
KNP 2.0b6. The lat ter  was mud l  better  t lmn 
tlm former, iudicat ing tha.t the improvements 
made by hand are  effective. But, the mainte-  
nance of rules by hand has a l imit, so the im- 
provements made by hand are not always effec- 
tive. 
Tlle above exper iments indicate that  Method 2 is 
best among the machine learning methods '5. 
In Table 5 we show some cases which were par- 
t i t ioned incorrectly with KNP but correctly with 
51n these experiments, the. differences were very small. 
But, we think that the differences are significant to some ex- 
tent because we performed Experiment 1 and Experiment 2, 
the data we used are a large corplls containing about a few 
ten thousand morphemes and tagged objectively in advance, 
and the difference of about 0.1% is large in the precisions of 
99%. 
570 
Table 5: Cases when KNP was incorrect and Method 
2 wan correct  
ko tsukotsu \[ N H1,,'Tr 9aman.sh i 
(steadily) (lm prurient wit;h) 
L_{"" 1)e patient with ... steadily) 
lyoyuu wo \] motte \]~xrT;~l~ shirizoke 
i(enough Stl'engi;h) obj (have) (1)eat off) 
(... beat off ... having enough sl;rength) 
~aisha wo I gurupu-wake \ [ ~  
,.o.,v,,,,y obj (~r,,,,pi,,~) (do) 
(... 11o grouping companies) 
Method 2. A partition with "NEED" indicates that 
KNP missed inserting the i)artition mark, and a par- 
tition with "WRONG" indicates that KNP inserted 
the partitiol~ mark incorrectly. In the test set of Ex- 
periment 1, the F-measure of KNP2.0b6 was 99.66%. 
The F-measur(. ~ increases to 99.83%, ml(ler the as- 
sumption that when KNP2.0t)6 or Method 2 in cor- 
rect, the answer is correct. Although the accuracy 
rate for KNP2.0b6 was high, there were some cases 
in which KNP t)artitioned incorrectly and Method 
2 partitioned correctly, A combination of Method 
2 with KNP2.0b6 may be able to iml)rove the F- 
lile~lsllrO. 
The only 1)revious research resolving Imnnetsu 
identification by machine learning methods, in the 
work by Zhang (Zhang and Ozeki, 1998). The 
decision-tree, ine, thod was used in this work. But 
this work used only a small mmther of intor- 
l l l;ttion for t)llllsetsll identification" and (lid not 
achieve ll igh accuracy rat;es. (The  recall  rate 
was 97 .6%(=2502/ (2502+62) ) ,  the 1)recision rate 
was 92 .4%(=2502/ (2502+205) ) ,  and F -measure  was 
94.2%.) 
5 Conc lus ion  
To solve tile t)roblem of aecm'ate btmsetsu iden- 
tification, we carried out ext)eriments comt)aring 
tbur existing machine-learning methods (decision- 
tree method, maxilnum-entrol)y method, examI)le- 
based method and decision-list method). We ob- 
tained the following order of acem'acy in bunsetsu 
identification. 
Example-Based > Decision List > 
Maximum Entropy > Decision Tree 
We also described a new method which uses 
category-exclusive rules with the highest similarity. 
This method performed better than the other learn- 
ing methods in ore" exi)eriments. 
(>l'his work used oifly the POS information of the two roof 
phemes of an analyzed space. 
References  
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. \])ella Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Linguistics, 
22(l):ag-rl. 
Andrew Borthwicl% John Sterling, Eugene Agichtein, aim 
Ralph Grishlnan. 1998. Exploiting Diverse Knowledge 
Sources via Maximum I';ntropy in Named Entity ll.ecoglfi- 
tion. In Proceedings of the Sixth Workshop on Very LaT~le 
Corpora, pages 152 -160. 
Masayuki Kameda. 1995. Simple Jalmnese analysis tool q jp.  
The Association for Natural Lan, guage Processing, the Isl 
National Convention, pages 349-352. (ill .)~ti)~tlleSe ). 
Sadao Kurohashi and Makot<) Nagao. 1997. Kyoto University 
text corpus 1)ro./ect. pages 115-118. (in .lapanese). 
Sadao lgurohashi and MM~oto Nagao, 1998. Japanese Mof  
phological Analysis System JUMAN version 3.5. \])ei)art- 
mcnt of Informatics, Kyoto University. (in Japanese). 
Sadao Kurohashi, 1997. Japanese Dependency/Case Struc- 
ture Anahdzer KNI ) version 2.Obj. Department of lnfof  
matics, Kyoto Ulfiversity. (in Japmmse). 
Sadao Kurohashi, 1!)!18. Japanese Dependency/Uase Struc- 
ture Analyzer KNI' version 2.0b6. Del)artment of Infor- 
m~tics, l(yoto University. (in .lapmmse.). 
Makoto Nagao. 1984. A I,'ralneworl( of a Mechanical Transh> 
ti(m between Jai)anese alld English 1)y Analogy lh'incit)le. 
Artificial a\]l(l Iluman hitelligence~ pages 173 q80. 
Shigeyuld Nishiokayama, Takehito Utsuro, and Yu.ii Mat- 
sumoto. 119!18. Extracting preference of dependency be- 
twee/t ,Japanese subordinate clauses from corlms. IE ICE-  
WGNL098- ll, pages 31-38. (in .lapnese). 
NI,I{,\]. 1964. (National Language Resemvh Institute). 
Word List b?l Semantic Principles. Syuei SyuI)pan. (in 
Japanese). 
.\]. 1{.. Quinl;m. 1995. Pro qrams for machine learning. 
Lmme A. I{;mlshaw ;uld Mitchell 1'. Marcus. 1(.)(.15. Text 
clmnking using transformationq)ased l arning. \]11 Proceed- 
ira.IS of th, e Th, ird Workshop on Very La*#e Corpora, l)ages 
82 (.)4. 
Adwait l{.atnat)arklfi. 1996. A Maximum l,;nl;ropy Model tin" 
l'art-OlCSt)eech Tagging. 1)rocecdin9 s of P)mpirieal Method 
for Natural Language l'roeessings, pages 133 1,12. 
Adwait ll,atnaparkhi, 19!17. A l,inear Observed Time Statis- 
ticaI l'm'ser Based ou Maximum \]~ntropy Models. ht t~l'o - 
ceedings of Empirical Method for Natural l, an.quage l'ro- 
cessings. 
Eric Sven Ristad. 1998. Maximum Elltropy Modeling 
Toolkit, Release 1.6 beta. http://www,mnemonic.com/ 
software/metal. 
Ronald L. Rivest. 1987. lmarning l)ecision lasts. Machine 
Learning~ 2:229 -246. 
Erik P. Tjong Kim Sang and .lorn \reenstra. 1999. ll.el)re- 
senting text chunks, in EA CL'99. 
Kiyotaka Uehimoto, Satoshi Sekiim, and IIitoshi Isalmra. 
1999.. Japanese dependency structure analysis based on 
maximmn entrol)y models. In Proceedings of the Ninth 
CoTtference of the 15"ulwpeau Chapter of the Association 
for Computational Linguistics (P)A CL), pages 196-203. 
\])avid Yarowsky. 1!)94. Decision lists fo," lexieal ambiguity 
resolution: Application to accent restoration in Spanish 
and l,Yench. In 22th Alt, Tt~tal Meeting of the Assoeitation 
of the Computational Linguistics, pages 88-95. 
Yujie Zlmng and Kazuhiko Ozeki. 1998. The applica- 
tion of classitieation trees to bunsetsu segmentation of 
Jat)anese sentences. Journal of Natural Language Process- 
ing, 5(4):17-33. 
571 
Backward Beam Search Algorithm 
for Dependency Analysis of Japanese 
Satoshi  Sek ine 
Computer Science Department 
New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekine@cs, nyu. edu 
K iyotaka  Uch imoto  H i tosh i  Isahara 
Communications Research Laboratory 
588-2 Iwaoka, Iwaoka-cho, Nishi-ku, 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto,  i sahara \ ]  @crl. go. j p 
Abst rac t  
Backward beam search tbr dependency analy- 
sis of Japanese is proposed. As dependencies 
normally go fl'om left to right in Japanese, it is 
effective to analyze sentences backwards (from 
right to left). The analysis is based on a statisti- 
cal method and employs a bemn search strategy. 
Based on experiments varying the bemn search 
width, we found that the accuracy is not sen- 
sitivc to the bemn width and even the analysis 
with a beam width of 1 gets ahnost he stone de- 
pendency accuracy as the best accuracy using a 
wider bemn width. This suggested a determin- 
istic algorithm for backwards Japanese depen- 
dency analysis, although still the bemn search 
is eitbctive as the N-best sentence accuracy is 
quite high. The time of analysis is observed to 
be quadratic in the sentence l ngth. 
1 In t roduct ion  
Dependency analysis is regarded as one of the 
standard methods of Japanese syntactic anal- 
ysis. The Japanese dependency structure is 
usually represented by the relationship between 
phrasal units called 'bunsetsu'. A bunsetsu nsu- 
ally contains one or more content words, like a 
noun, verb or adjective, and zero or more func- 
tion words, like a postposition (case marker) 
or verb/noun sul~\[ix. The relation between two 
bunsetsu has a direction front a dependent to 
its head. Figure 1 shows examples of 1)unsetsu 
and dependencies. Each bunsetsu is separated 
by "I"" The  first segment "KARE-HA" consists 
of two words, KARE (He) and HA (subject case 
marker). The  numbers  in the "head" line show 
the head ID of the corresponding bunsetsus. 
Note that the last segment does not have a head, 
and it is the head bunsetsu of the sentence. The 
task of the Japanese dependency analysis is to 
find the head ID for each bunsetsu. 
The analysis proposed in this paper has two 
conceptual steps. In the first step, dependency 
likelihoods are calculated for all possible pairs 
of bunsetsus. In the second step, an optimal de- 
pendency set for the entire sentence is retrieved. 
In this paper, we will mainly discuss the second 
step, a method fbr finding an optimal depen- 
dency set. In practice, the method proposed in 
this paper should be able to be combined with 
any systems which calculate dependency likeli- 
hoods. 
It is said that Japanese dependencies have the 
tbllowing characteristics1: 
(1) Dependencies are directed from left to right 
(2) Dependencies don't cross 
(3) Each seglnent except he rightmost one has 
only one head 
(4) In many cases, the left; context is not nec- 
essary to determine a dependency 
The analysis method proposed in this paper as- 
sumed these characteristics and is designed to 
utilize them. Based on these assumptions, we 
can analyze a sentence backwards (from right 
to left) in an efficient manner. There are two 
merits to this approach. Assume that we are 
analyzing the M-th segment of a sentence of 
length N and analysis has already been done 
for the (M + 1)-th to N-th segments (M < N). 
The first merit is that the head of the depen- 
dency of the M-th segment is one of the seg- 
1Of course, there are several exceptions (S.Shirai, 
1998), but the frequencies of such exceptions are neg- 
ligible compared to the current precision of the system. 
We believe those exceptions have to be treated when the 
problems we are facing at the moment are solved. As- 
sumption (4) has not been discussed very much, but our 
investigation with humans showed that it is true in more 
titan 90?./0 of the cases. 
754 
ID i 2 3 4 5 6 
KARE-HA \[ FUTATABI I PAI-W0 \[ TSUKURI, I KANOJO-NI I 0KUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Head 6 4 4 6 6 - 
Translation: He made a pie again and presented it to her. 
Figure 1: Exmnt)le a JaI)anese sentence, 1)unsetsus and det)endencies 
ments between M + 1 and N (because of as- 
sumption 1), which are already analyzed. Be- 
cause of this, we don't have to kce 1) a huge lnlln- 
1)er of possible analyses, i.e. we can avoid some- 
thing like active edges in a chart parser, or mak- 
ing parallel stacks in GLR parsing, as we can 
make a decision at this time. Also, we can use 
the beam search mechanism, 1)y keet)ing only a 
certain nmnl)er of.analysis candidates at (',ach 
segment. The width of the 1)(;am search can 1)c, 
easily tuned and the memory size of the i)ro- 
(:ess is l)rot)ortional to the 1)roduct of the inl)ut 
sentence length and tile boron search width. 
The other merit is that the possit)le heads 
of tile d(~l)en(lency can t)e narrowed down 1)c- 
cause of the ~ssuml)tion of non-crossing det)en- 
(lencies (assumption 2). For exani1)le , if the 
K-th seglll(;nl; dCl)ends on the L-tll segnient 
(A4 < \]~ <~ L), then the \]~J-th segillent (:~l~n't 
depend on any segments between 1~ and L. 
According to our experilnent, this reduced the 
numl)er of heads to consider to less than 50(X~. 
The te(:hnique of backw~trd analysis of 
,lal)anese sentences has 1)een used in rule-based 
methods, for example (Fujita, 1988). How- 
ever, there are several difficulties with rule- 
based methods. First the rules are created by 
hmnans, so it is difficult to have wide cover- 
age and keel) consistency of the rules. Also, it 
is difficult to incorporate a scoring scheme in 
rule-1)ased methods. Many such met;hods used 
hem'isties to make deterministic decisions (and 
backtracking if it; fails in a sear(:hing) rather 
l;han using a scoring scheme. However, the com- 
1)ination of the backward analysis and the sta- 
tistical method has very strong advantages, one 
of which is the 1)emn search. 
2 Stat i s t i c  f ramework  
We. coin|lined tile backward beam search strat- 
egy with a statistical dependency analysis. 'rile 
det~fil of our statistic framework is described 
ill (Uehimoto et al, 1999). There have been 
a lot of prol)OS~fls for statistical analysis, in 
ninny languages, in particular in English and 
Japanese (Magerman, 1995) (Sekine and Grish- 
man, 1995) (Collins, 1997) (I/atnal)arkhi, 1997) 
(K.Shirai et.al, 1998) (Fujio and Matsnlnoto, 
1998) (Itaruno ct.al, 1997)(Ehara, 1998). One 
of the most advance(t systems in English is l)ro- 
posed 1)y I{atnaparkhi. It, uses the Maximum 
Entropy (ME) model and both of the accuracy 
and the speed of the system arc among the best 
ret)ortcd to date. Our  system uses the ME 
model, too. in the ME model, we define a set 
el! \]2~,atlll'eS which arc thought to l)e uscflfl in 
del)ealden(:y analysis, and it: learns the weights 
of the R~atures fl'om training data. Our t~ntttres 
in(:lude part-of-st)eech, inflections, lexical items, 
the existence of a contain or bra(:ket 1)etween 
the segments, and the distmme between the seg- 
ments. Also, confl)inations of those features are 
used as additional fe, atures. The system eal- 
(:ulates the probabilities of dependencies based 
on the model, which is trained using a training 
corpus. The probability of an entire sentence is 
derived from the 1)roduct of tile probal)ilities of 
all the dependencies in the sentence. We choose 
the analysis with the highest probafl)ility to be 
the analysis of the sentence. Although the ac- 
curacy of the analyzer is not the main issue of 
the t)al)er, as any types of models which use de- 
1)endency 1)rol)al)ilities can be iml)lelnented by 
our method, the 1)ertbrmance r t)orted in (Uchi- 
lnoto et al, 1999) is one of the best results re- 
ported by statistic~flly based systems. 
755 
3 A lgor i thm 
In this section, the analysis algorithm will be de- 
scribed. First the algorithm will be illustrated 
using an example, then the algorithm will be 
formally described. The main characteristics of 
the algorithm are the backward analysis and the 
beam search. 
The sentence "KARE-HA FUTATABI PAI-W\[I 
TSUKURI, KANOJ0-NI 0KUTTA. (He made a pie 
again and presented it to her)" is used as an in- 
put. We assume the POS tagging and segmen- 
tation analysis have been done correctly before 
starting the process. The border of each seg- 
ment is shown by "1". In the figures, the head of 
the dependency for each segment is represented 
by the segment number shown at the top of each 
segment. 
<Initial> 
ID 1 2 3 4 5 6 
RARE-HA \[ FUTATABI \[ PAI-WO \[ TSUKURI, \[ KANOJO-NI I OKUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
................................................................. 
Algorithm 
1. Analyze np to the second segment from the 
end 
The last segment has no dependency, sowe 
don't have to analyze it. The second seg- 
ment fl'om the end always depends on the 
last segment. So the result up to the sec- 
end segment from the end looks like the 
following. 
<Up to the second segment from the end> 
ID 1 2 3 4 5 6 
KARE-HA I FUTATABI \[ PAI-WO I TSUKURI, I KANOJO-NI I OKFITA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Cand 6 
................................................................. 
. The third segment from the end 
This segment ("TSUKURI," ) has two depen- 
dency candidates. One is the 5th segment 
("KANOJ0-NI") and the other is the 6th seg- 
ment ("0KUTTA"). Now, we use the proba- 
bilities calculated using the ME model in 
order to assign probabilities to the two can- 
didates (Candl and Cand2 in the following 
figure). Let's assume the probabilities 0.1 
and 0.9 respectively as an example. At the 
tail of each analysis, the total probability 
(the product of the probabilities of all de- 
pendencies) is shown. The candidates are 
sorted by the total probability. 
. 
<Up to the third segment from the end> 
ID 1 2 3 4 5 6 
KARE-HA I FUTATABI I PAI-WO I TSUKURI, I KANOJO-HI I OKUITA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Candl 6 0 - (0.9) 
Cand2 5 6 - (0.I) 
................................................................. 
The tburth segment from the end 
For each of the two candidates created at 
the previous tage, the dependencies of the 
fburth segment from the end ("PAI-W0") 
will be analyzed. For Candl, the segment 
can't have a dependency to the fifth seg- 
ment ("KANOJ0-1gI"), because of the non- 
crossing assmnption. So the probabili- 
ties of the dependencies only to the fourth 
(Candi-1) and the sixth (Candi-2) seg- 
ments are calculated. In the example, these 
probabilities are assmned to be 0.6 and 0.4. 
A similar analysis is conducted for Cand2 
(here probabilities are assumed to be 0.5, 
0.1 and 0.4) and three candidates are cre- 
ated (Cand2-1, Cand2-2 and Cand2-3). 
<Up to the fourth segment from the end> 
ID 1 2 3 4 5 6 
RARE-HA I FUTATABI I PAI-WO I TSUKURI, I KANOJO-NI I OKUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
C~dt - i  4 6 6 - (0.64) 
Candl-2 6 6 6 - (0.30) 
Cand2-1 4 5 6 - (0.05) 
Cand2-2 6 5 6 - (0.04) 
Caud2-3 5 5 6 - (0.01) 
................................................................. 
As tile analysis proceeds, a large number 
(almost L!) of candidates will he created. 
However, by linfiting the number of candi- 
dates at each stage, the total nmnber of 
candidates can be reduced. This is the 
beam search, one of the characteristics of 
the algorithm. By observing the analyses 
in the example, we can e~sily imagine that 
this beam search may not cause a serious 
problem in performance, because the candi- 
dates with low probabilities may be incor- 
rect anyway. For instance, when we set the 
beam search width = 3, then Canal2-2 and 
Cand2-3 in the figure will be discarded at 
this stage, and hence won't be used in the 
following analyses. The relationship of the 
beam search width and the accuracy oh- 
served in our experiments will be reported 
in the next section. 
756 
. Up to the, first segment 
The analyses are conducted in the, same 
way up to the first segment. For example, 
the result of tile analysis tbr the entire sell- 
tence will be shown below. (Appropriate, 
probabilities are used.) 
4.2 Beam search  w idth  and  accuracy  
In this subsection, the relationship between the 
beam width and the accuracy is discussed. In 
principle, the wider the beam search width, the 
more analyses can be retained and the better 
the accuracy cml be expected. However, the re- 
.................................................................. sultis somewhat different froan tile expectation. 
<Up to the first segment> 
ID 1 2 3 4 5 6 
KARE-IIA \[ FUTATABI \[ PAl-W0 { TSUKURI, \[ KANOJ0-NI \[ 0KUTTA. 
(Ile-subj) (again) (pie-obj) (made ,) (to her) (present) 
Candl 6 4 4 6 0 - (0. ii) 
Cand2 4 4 6 6 6 - (0 .09)  
Cand3 6 4 6 5 6 - (0.05) 
................................................................. 
Now, the formal algorithm is described induc- 
tiveJy in Figure 3. The order of the analysis is 
quadratic ill the length of the sentence. 
4 Exper iments  
In this section, experiments and evaluations will 
be reported. We use the Kyoto University Cor- 
pus (version 2) (Kurohashi el.el, 1{)97), a hand 
created Japanese corpus with POS-tags, bun- 
setsu segments and dependency information. 
The sentences in the articles from January 1, 
1994 to January 8, 1994 (7,960 sentences) a.re 
used t'or tim training of the ME model, and 
the sente, nccs in the artMes of Janum'y 9, 1994: 
(1,246 sentences) are used for the ewduation. 
The seid;ences ill the articles of Ja l luary 10, 1994 
are kept for future evaluations. 
4.1 Bas ic  Resu l t  
The evahlation result of our systenl is shown ill 
Table 1. The experiment uses the correctly seg- 
mente(1 and 1)art-oSsl)eet'h tagger1 sentences of 
the Kyoto University corpus. The bealn search 
width is sol; to 1, in other words, the systeln runs 
deterministically. Here, 'dependency accuracy' 
Table 1: lBvaluation 
Dependency accuracy 
Sentence accuracy 
Average analysis time 
87.14% (9814/11263) 
40.60% 0503/1239) 
0.03 sec 
is the percentage of correctly analyzed depen- 
dencies out of all dependencies. 'Sentence accu- 
racy' is the i)ercentage of the sentences in which 
all the dependencies are analyzed correctly. 
Table 2 shows the dependency accuracy and 
sentence accuracy for bemn widths 1 through 
20. The difference is very small, but the best 
Table 2: Relationship between beam width and 
accuracy 
Bemn width Dependency Sentence 
Accuracy Accuracy 
1 
2 
3 
4 
5 
6 
7 
10 
15 
20 
87.14 
87.16 
87.20 
87.1.5 
87.14 
87.16 
87.20 
87.20 
86.21 
86.21 
40.60 
40.76 
40.76 
40.68 
40.60 
40.60 
40.60 
40.60 
40.60 
40.60 
accuracy is obtained when the beain width is 11 
(fbr the dependency accuracy), and 2 and 3 (tbr 
the sentence accuracy). This proves that there 
are cases where the analysis with the highest 
product of probabilities is not correct, but the 
analysis decide(1 at each stage is correct. This is 
a very interesting result of our experiment, and 
it is related to assulnption 4 regarding Japanese 
dependency, lnentioned earlier. 
This suggests that when we analyze a 
.Japanese sentence backwards, we can do it de- 
terministically without great loss of accuracy. 
Table 3 shows where the mlalysis with bemn 
width 1 appears among the analyses with bealn 
width 200. It shows that most deterministic 
analyses appear as tile best analysis in the non- 
deterministic analyses. Also, mnong the deter- 
aninistic analyses which are correct (503 Sell- 
tences), 498 sentences (99.0%) have the same 
mmlysis at the best rank in the 200-beam-width 
analyses. (Followed by 3 sentences at the see-. 
end, 1 sentence ach at the third and fifth rank.) 
It means that in most of the cases, the mmlysis 
757 
<Variable> 
Length: 
W: 
C\[len\]: 
Length of the input sentence in segments 
The beam search width 
Candidate list; C for each segment keeps 
the top W partial analyses from that segment 
to the last segment. 
<Initial Operation> 
The second segment from the end depends on the last segment. 
This analysis is stored in C\[Length-l\]. 
<Inductive Operation> 
Assume the analysis up to the (M+l)-th segment has been finished. 
For each candidate ~c ' in C\[M+i\], do the following operation. 
Compute the possible dependencies of the M-th segment compatible 
with 'c'. For each dependency, create a new candidate Cd~ by 
adding the dependency to 'c'. Calculate the probability of 'd'. 
If C\[M\] has fewer than W entries, add ~d ~ to C\[M\]; 
else if the probability of Cd~ > the probability of the least 
probable entry of C\[M\], replace this entry by 'd'; 
else ignore 'd ' 
When the operation finishes for all candidates in C\[M+i\], 
proceed to the analysis of the (M-l)-th segment. 
Repeat the operation until the first segment is analyzed. 
The best analysis for the sentence is the best candidate in 
C\[1\]. 
Figure 2: Formal Algorithln 
with the highest probability at each stage also 
has the highest probability as a whole. This is 
related to assumption 4. The best analysis with 
the left context and the best analysis without 
tile left context are the same 95% of the time in 
general, and 99% of the time if the analysis is 
correct. These numbers are much higher than 
our human experinmnt mentioned in the ear- 
lier footnote (note that the number here is the 
percentage in terms of sentences, and the num- 
ber in the footnote is the percentage in terms of 
segnmnts.) It means that we may get good ac- 
curacy even without left contexts in analyz ing 
Japanese dependencies. 
4.3 N-Best  accuracy  
As we can generate N-best results, we measured 
N-best sentence accuracy. Figure 3 shows the 
N-best accuracy. N-best accuracy is the per- 
centage of tile sentences which have the correct 
analysis among its top N analyses. By setting 
a large beam width, we can observe N-best ac- 
curacy. The table shows the N-best accuracy 
when the beam width is set, to 20. When we set 
N = 20, 78.5% of the sentences have the cor- 
rect analysis in the top 20 analyses. If we have 
758 
Rank 1 
\]5"e<luc, n y 1175 
(%) (.{},5.8) 
Rank 11 
Frequen(:y 1 
(%) 
Table 3: The rank of the deterministic analysis 
2 3 4: 5 6 7 8 
20 11 8 4 2 1 2 
(1.6) (0.9) (0.6) (0.3) ( I ) .2 ) (0 .1 ) (0 .2 )  
12 j,5 1(i 17 18 
0 o 1 0 J J 
(0.1) ({}.1) (0.1) (0.1) 
9 10 
0 3 
(02) 
19 20 and more 
0 8 
(0.6) 
80 
70 
60 
50 
40 
30 
Sent;once Accuracy 
.53% 
#, 40.60% 
I I I I I I I I I I I - ~ T E  
0 5 10 11.5 20 
N 
Figure 3: N-best sentenc(~ Accuracy 
an ideal sysl;(ml for finding th(~ COl'lCCi; mmlysis 
a,lnOllg? th(;ln~ which maS, 11.%O SCllltl,lll;ic O1" COll- 
l,(;x{; inforlllt~I;io\]\]~ we can have a v(Ty a(:(;Hr~d;e 
an alyzer. 
\~TC Call llltl,l((; two interesting observations 
trom the result. The ac(:uracy of the 1--best 
mmlysis is about 40%, which is more tlm.n half 
of t, he accura(:y of 20-1)est analysis. This shows 
that although the system is not 1)erfb, ct, the 
computation of the 1)rolml)ilities is t)rol)ably 
good in order l;o find the correct mmlysis at the 
top rank. 
The other point is that the accm'aey is sat- 
urated at m'omM 80%. Iml)rovemel,t over 80% 
seelns very dit\[icult even if we use a very large 
bemn width W. (lf we set; W to the number 
of all possible combinations, which means al- 
most L! for sentence length L, we (21M gC{; 100(~0 
N-best accm'aey, lint this is not worth eonsidel'- 
ing.) This suggests tlmt wc h~we missed some- 
thing important. In part;icular, from our inves- 
tigation of the result, we believe that (:oordinate 
structure is one of the most important factors 
to iml)rove the accuracy. This remains one area 
of fllturc work. 
4.4 Speed of  the  analys is  
Based on the f'(n'nml algorithm, the analysis 
tinle can be estimated as t)rot)orl;ional to the 
square, of the inl)ut sentence length. Figure 4: 
shows the relationshi I) between the analysis 
time and the sentence length when wc set the 
beam width to 1. We use a Sun Ultra10 ma- 
chine and the process size is about 8M byte. 
We can see that the actual analyzing time al- 
Analysis time (see.) 
0.3 
0.2 * / "  
0 ~ ~  , r , , , 
0 10 20 30 40 
Sentence length 
\]?igure 4: \]~.elationshi 1) between sentence length 
and mmlyzing time 
most follows the quadratic urve. The ~verage 
amflysis time is 0.03 second and the ~werage sen- 
tence lengl:h is 10 segments. The analysis time 
for the longest sentence (41 segments) is 0.29 
second. W\; have not ot)l;imized the In'ogram in 
terms of speed aim there is room to shrink /;he 
process ize. 
759 
5 Conc lus ion  
In this paper, we proposed astatistical Jttpanese 
dependency analysis method which processes a
sentence backwards. As dependencies normally 
go from left to right in Japanese, it is eflhctive 
to analyze sentences backwards (from right to 
left). In this paper, we proposed a Japanese de- 
pendency analysis which combines a backward 
analysis and a statistical method. It can nat- 
urally incorporate a beam search strategy, an 
effective way of limiting the search space in the 
backwm'd analysis. We observed that the best 
perfbrmances were achieved when the width is 
very small. Actually, 95% of the analyses ob- 
tained with bemn width=l  were the stone as 
the best analyses with beam width=20. The 
analysis time was proportional to the square of 
the sentence length (nmnber of segments), as 
was predicted from the algorithm. The average 
analysis time was 0.03 second (average sentence 
length was 10.0 bunsetsus) and it took 0.29 sec- 
end to analyze the longest sentence, which has 
41 segments. This method can be ~tpplied to 
various languages which haw~ the stone or simi- 
lar characteristics of dependencies, for example 
Koran, Turkish etc. 
References  
Adam Berger and Harry Printz. 1998 : "A 
Comparison of Criteria for Maximum En- 
tropy / Mininmm Divergence Feature Selec- 
tion". Proceedings of the EMNLP-98 97-106 
Michael Collins. 1997 : "Three Generative, 
Lexicalized Models for Statistical Parsing". 
Proceedings of the ACL-97 16-23 
Terumasa Ehara. 1998 : "CMculation of 
Japanese dependency likelihood based on 
Maximmn Entropy model". Proceedings of 
the ANLP, Japan 382-385 
Masakazn t51jio and Yuuji Matsumoto. 1998 
: "Japanese Dependency Structure Analysis 
based on Lexicalized Statistics". Proceedings 
of the EMNLP-98 87-96 
Katsuhiko Fujita. 1988 : "A Trial of determin- 
istic dependency analysis". Proceedings of the 
Japanese Artificial Intelligence Annual meet- 
in9 399-402 
Masahiko Haruno and Satoshi Shirai and Yoshi- 
fumi Ooyama. 1998 : "Using Decision Trees 
to Construct a Practical Parser". Proceedings 
qf the the COLING/A CL-98 505-511 
Sadao Kurohashi and Makoto Nagao. 1994 : 
"KN Parser : ,J~tpanese Dependency/Case 
Structure Analyzer". Proceedings of The In- 
ternational Workshop on Sharable Natural 
Language Resources 48-55 
Sadao Kurohashi and Makoto Nagao. 1997 : 
"Kyoto University text corpus project". Pro- 
ceedings of the ANLP, Japan 115-118 
David Magerman. 1995 : "Statistical Decision- 
Tree Models for Parsiug". Proceedings of the 
ACL-95 276-283 
Adwait I/,atnaparkhi. 1997 : "A Linear Ob- 
served Time Statistical Parser Based on 
Maximum Entropy Models". Proceedings o.f 
EMNLP-97 
Satoshi Sekine and Ralph Grishman. 1995 : "A 
Corpus-based Probabilistic Grammar with 
Only Two Non-terminals". Proceedings of the 
IWPT-95 216-223 
Satoshi Shirai. 1998 : "Heuristics and its lira- 
itation". Jowrnal o\[ the ANLP, Japan Vol.5 
No.l, 1-2 
Kiyoaki Shirai, Kentaro Inui, Takenobu 'lbku- 
naga and Hozunli Tanaka. 1998 : "An Em- 
pirical Evaluation on Statistical Parsing of 
Japanese Sentences Using Lexical Association 
Statistics". P'roceedings ofEMNLP-98 80-86 
Kiyotaka Uchimoto, Satoshi Sekine, Hitoshi 
Isahara. 1999 : "Jat)anese Dependency 
Structm'e Analysis Based on Maximum En- 
tropy Models". P~vceedings o\[ the EACL-99 
pp196-203 
760 
Word Order  Acqu is i t ion  f rom Corpora  
Kiyotaka Uchimoto I, Masaki Murata t, Qing Ma*, 
Satoshi Sekine*, and Hitoshi Isahara t 
tCommunications Research Laboratory 
Ministry of Posts and Telecommunications 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto ,murata, qma, isahara\] @crl. go. jp 
*New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekYne~cs, nyu. edu 
Abstract 
In this paper we describe a method of acquiring word 
order fl'om corpora. Word order is defined as the or- 
der of modifiers, or the order of phrasal milts called 
'bunsetsu' which depend on the stone modifiee. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to de- 
ciding the word order mid which word order tends to 
be selected when several kinds of information con- 
flict. The contribution rate of each piece of informa- 
tion in deciding word order is eiIiciently learned by a 
model within a maximum entropy framework. The 
performance of this traiimd model can be ewfluated 
by checking how many instances of word order st- 
letted by the model agree with those in the original 
text. In this paper, we show t, hat even a raw cor- 
pits that has not been tagged can be used to train 
the model, if it is first analyzed by a parser. This 
is possible because the word order of the text in the 
corpus is correct. 
1 Introduction 
Although it is said tha~ word order is free in 
Japanese, linguistic research shows that there art 
certain word order tendencies - -  adverbs of time, for 
example, tend to t)recede subjects, mM bunsetsus in 
a sentence that are modified by a long modifier tend 
to precede other bunsetsus in the sentence. Knowl- 
edge of these word order tendencies would be useful 
in analyzing and generating sentences. 
Ii1 this paper we define word order as the order of 
nrodifiers, or the order of bunsetsns wlfich depend on 
the same modifiee. There arc several elements which 
contribute to deciding the word order, and they are 
summarized by Saeki (Saeki, 1.998) as basic condi- 
tions that govern word order. When interpreting 
these conditions according to our definition, we era: 
summarize them ,~ tbllows. 
Component la l  eondit lons 
? A bunsetsu having a deep dependency tends 
to precede a bunsetsu having a shallow depen- 
dency. 
When there is a long distance between amodifier 
and its modifiee, the modifier is defined as a bun- 
setsu having a deep dependency. For example, 
the usual word order of modifiers in Japanese 
is tlm following: a bunsetsu which contains an 
interjection, a bunsetsu which contains an ad- 
verb of time, a bunsetsu which contains a sub- 
ject, and a bunsetsu which contains an object. 
Here, the bunsetsu containing an adverb of time 
is defined as a bunsetsu having deeper depen- 
dency than the one containing a subject. We 
call the concept representing the distance be- 
tween a modifier and its modifiee the depth of 
dependency. 
A bunsetsu having wide dependency tends to 
precede a bunsetsu having narrow dependency. 
A bunsetsu having wide dependency is defined 
as a bunsetsu which does not rigidly restrict its 
modifiee. For example, the bunsetsu "~btqlo_c 
(to Tokyo)" often depends on a bunsetsu whicll 
contains a verb of motion such as "ihu (go)" 
while the bunsetsu "watashi_.qa (I)" can depend 
on a bunsetsu which contains any kind of verb. 
Here, the bunsetsu "watashi_ga (I)" is defined as 
a bunsetsu having wider dependency than 1;11o 
tmnsetsu ':Tok~./o_c (to Tokyo)." We call the 
concept of how rigidly a modifier restricts its 
modifiee the width of dependency. 
Syntact i c  condit ions 
? A bunsetsu modified by a long inodifier ton(Is to 
precede a bunsetsu modified by a short lnodifier. 
A long modifier is a long clause, or a clause that 
contains many bunsetsus. 
? A bunsel, su containing a reference pronoun tends 
to precede other bunsetsus in the sentence. 
? A bunsetsu containing a repetition word tends 
to precede other bunsetsus in the sentence. 
A repetition word is a word referring to a word 
in a preceding sentence. For example, Taro 
mid Hanako in the following text are repetition 
words. "Taro and Hanako love each other. Taro 
is a civil servant and Hanako is a doctor." 
? A bunsetsu containing the case marker "wa" 
tends to precede other bunsetsus in the sentence. 
A mnnber of studies have tried to discover the rela- 
tionship between these conditions and word order in 
871 
Japanese. Tokunaga and Tanalca proposed a model 
for estimating JaI)anese word order based on a dic- 
tionary. They focused on the width of dependency 
(Tokunaga and Tanal~a, 1991). Under their model, 
however, word order is restricted to the order of case 
elements of verbs, and it is pointed out that the 
model can deal with only the obligatory case and 
it cmmot deal with contextual information (Saeki, 
1998). An N-gram model fbr detecting word order 
has also been proposed by Maruyama (Maruyama, 
1994), but under this model word order is defined as 
the order of morpheines in a sentence. The problem 
setting of Maruyama's study thus differed fl'om ours, 
and the conditions listed above were not taken into 
account in that study. As for estimating word or- 
der in English, a statistical model has been proposed 
by Shaw and Hatzivassiloglou (Shaw and Hatzivas- 
siloglou, 1999). Under their model, however, word 
order is restricted to the order of premodifiers or 
modifiers depending on nouns, and the model does 
not simultaneously take into account many elements 
that contribute to determining word order. It would 
be difficult to apply the model to estimating word 
order in Japanese when considering the many condi- 
tions as listed above. 
In this paper, we propose a method for acquiring 
from corpora the relationship between the conditions 
itemized above and word order in Japanese. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to decid- 
ing the word order and which word order tends to be 
selected when several kinds of information conflict. 
The contribution rate of each piece of information in 
deciding word order is efficiently learned by a model 
within a maximum entrot)y (M.E.) framework. The 
performance of the trained model can be evaluated 
according to how many instances of word order se- 
lected by the model agree with those in the original 
text. Because the word order of the text in the corpus 
is correct, the model can be trained using a raw co> 
pus instead of a tagged corpus, if it is first analyzed 
by a parser. In this paper, we show experimental re- 
sults demonstrating that this is indeed possible even 
when the parser is only 90% accurate. 
This work is a part of the corpus based text gen- 
eration. A whole sentence can be generated in the 
natural order by using the trained model, given de- 
pendencies between bunsetsus. It could be helpful 
for several applications uch as refinement support 
and text generation in machine translation. 
2 Word  Order  Acqu is i t ion  and 
Es t imat ion  
2.1 Word  Order  Mode l  
This section describes a model which estimates the 
likelihood of the appropriate word order. We call 
this model a word order model, and we implemented 
it within an M.E. framework. 
Given tokenization of a test corpus, the problem 
of word order estimation in Japanese can be reduced 
to the problem of assigning one of two tags to each 
relationship between two modifiers. A relationship 
could be tagged with "1" to indicate that the order 
of the two modifiers is appropriate, or with "0" to in- 
dicate that it is not. Ordering all modifiers so as to 
assign the tag "1" to all relationshit)s indicates that 
all modifiers art  in the appropriate word order. The 
two tags form the space of "futures" in the M.E. 
formulation of our estimation problem of word or- 
der between two modifiers. The M.E. model, as well 
as other similar models allows the computation of 
P(flh) for any f in the space of possible futures, F, 
and for every h in the space of possible histories, H. 
A "history" in maximum entropy is all of the condi- 
tioning data that enable us to make a decision in the 
space of futures. In the estimation problem of word 
order, we could reformulate this in terms of finding 
the probability of f associated with the relationship 
at index t in the test cortms as: 
P(flht) = P( f l  hfformation derivable 
from the test corpus 
related to relationship t) 
The computation of P(flh) in any M.E. models is 
dependent on a set of "features" which should be 
hdpful in making a prediction about the flmlre. Like 
most current M.E. models in computational linguis- 
tics, our model is restricted to features which are 
binary functions of the history and future. For in- 
stance, one of our features is 
1. : if has(h,x) = true, 
x = "Mdfl'l - Head-  
g(h,f) = POS(Major) : verb" (1) 
&f= l  
0 : otherwise. 
Here "has(h,x)" is a binary flmction which returns 
true if the history h has feature z. We focus on the 
attributes of a bunsetsu itself and on the features 
occurring between bunsetsus. 
Given a set of features and some training data, 
the maximum entropy estimation process produces a
model ill which every feature .qi has associated with it 
a parameter ai. This allows us to compute the con- 
ditional probability as follows (Berger et al, 1996): 
ag~ (h .f) 
P( / Ih ) -  1L ' (2) 
Z (h) 
ct i . (3) 
Y i 
The maximum entropy estimation technique guaran- 
tees that for every feature gi, the expected value of 
gi according to the M.E. model will equal the empir- 
ical expectation of gi in the training corpus. In other 
words: 
P(h,/). Mh, f) 
h,f 
= (4) 
h / 
Here /5 is an empirical probability and l~ Ie  is the 
872 
Table l: Example of estimating the probabilities of word orders. 
? ) I"I~{H (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (Taro) / bLo ( l ) I t~yed. ) "  /I~,,'~<~,~ x llail.-)'nxt, x 1.7:-.x,~j~m ::: 0.6 x 0.8 x 0.3 0.144 ) \["NI!Iii:k (Taro) )  I~1!11 (yesterday) / ?:LX ~ (tennis) / bt:? (played.)" IP:mi~ ~H x I~*H,)-: x,,< x t?r,l~.?::x,: := 0.4 x 0.8 ? 0.7 0.224 
\[")kl'!l~l:~ (Taro) / -Y : :x '~ (tennis) / 'a~H (yesterday) / b?:o (plowed.)" I/)~:,,~ll x tg.=.x'~.,~H x P,k~a*~L-)::?~ =: 0.4 x 0.2 x 0 .7  0.05(i 
1"9:-':;? ~ (tennis) / II~{H (yesl.erd;ty) / >kfl\[~l~: ( l'&l'O) / \[,~:.? (played.)" \[13*lll,2<I,u:l. x l: ':.xt, jall x l!).:.x.~,.j<r~m :: 0.6 x 0.2 x 0.3 0.036 
\["5:=x ~ (tennis) / )kf~lll:t (Taro) / I~I~Ft (yesterday) / blz0 (played.)" ~ ? H  x l~;.:.xr, ~,~ZI? x l?y:.x~l~,~ =: 0.4 x 0.2 x 0.3 0.024 
prol)ability assigned by the M.E. model. 
We detine a word order model as a model which 
learns the at)l)ropriate order of each pair of nlodifiers 
which depend on the same modifiee. 'l'his model is 
derived from Eq. (2) as follows? Assmne that there 
are two bunsetsus 231 and 23~ which depend on the 
buusetsu B and that  It is the information derivable 
from the test corpus. \]?lie probability that "B\] B2" is 
the at)propriate order is given by the following equa- 
tion: 
\]iik: :1 ffi( l  ,b) (~'i ,i 
where .qi(1 < i < k) is a fl,atm'e and "1" indicates 
that the order is at)propriate. The terms cq,i and 
(~0,i are estimated fl'oln a eorl)us which is nlorpho- 
logically and syntactically analyzed. When there are 
three or more b\]msetsus that det)end on tit('. S}tlne 
? moditiee, the probability is estimated as follows: I or 
~'t bunsetsus 231, 232, . . . ,  23n which depend on the 
bmtsetsu B and for the information h derivaMe from 
the test corpus, the prot)ability t;hat "23\] 23~ . . .  23," 
is the at)propriate order, or P( l lh) ,  is represented ass 
the probability that every two bunsetsus "Bi ~i-Fj 
(1 _< i < n - 1,1 < j < 'n - i)" are the appropri~ 
ate. order, or  P({14~i, i_ l . j  = l \ [ l<  i <. n - -  1, l :< j < 
n - - i} lh ) ,  ilere "I4Li+j -- l" represents that ".l~i 
23i-Fj" is the appropriate order. Let us assume that 
every 14Q,~:+j is independent each other. Then 1)(1 Ih,) 
is derived as follows: 
2 ' ( l ib )  = \]1\]. i , . -  \ ] ,  
1 < .i _< , , , -  i}lh) 
n- - |  n - i  
i -- I  j --1 
= l l  1I j), 
i= l  j= l  
where \[Si,i+ j is the information derivable when fb- 
cusing on the bunsetsu 13 m~d its modifiers 13i and 
Bi+j. 
For example, in the sentence "I~ U (kinou, yester- 
day) / :kflll ~ (Taro_wa, Taro) / -P ~ x ~ (tcnnis_wo, 
tennis) / b t:o (sita., l)layed.)," where a "/" repre- 
sents a bunsetsu boundary, there are three bunset- 
sus that depend on the verb "b  ~: (sita)." We train 
a word order inodel under the assmnl)tion that the 
orders of three t)airs of modifiers -"I~l U" and "~ 
f$1.~," "Net\] " and "?  7-:7, ~ ," and ":kl~l*l~" and "5: 
m :7, ~"  .... are al)ttropriate. We use various ldnds of 
intormation in and around the target bunsetsus as 
features. For example, the information or the feature 
that a noun of time i)recedes a t)rot)er noun is deriv- 
able fl'om the order "IP} H (yesterday) / Y;fll~ I~ (Taro) 
/ b 1=o (pl~\yed.)," and the feature that a case fol- 
lowed by a case marker "w?' precedes a case followed 
by a caqe marker "wo" is derivable from the order ":~ 
fll~ It. ( Taro_wa, Taro) / ? ~ 7. ?k (tennis_wo, tennis) / 
b 2C_o (sita., t)layed.)." 
2.2 Word Order  Es t imat ion  
This section describes the algorithm of estimating 
the word order by using a trained word order model. 
The word order estimation is defined as deciding 
the order of ntoditiers or bunsetsus which depend 
on the same modifiee. The input of this task con- 
sists of modifiers and informat, ion necessary to know 
whether or not features are found. The output is 
the order of the inodifiers. We assume that lexical 
selection in each bunsetsu is already done and all 
del)endencies in a sentence are found. The informa- 
tion necessary to know whether or not features are 
found is morphological, syntactic, semmltic, and ('on- 
textual information, and the locations of bunsetsu 
bonndaries. The features used in our ext)eriments 
are described in Section 3. 
Word order is estimated in the following steps. 
Procedures 
1. All possible orders of modifiers are found. 
2. For each, the probability that it is apt)ropriate 
is estimated by a word order model, or Eq. (6). 
3. The order with the highest probability of 1)eing 
approl)riate is selected. 
l)br example, given the sentence "1~ U (kinou, 
yesterday) /:kfilIl~ (Taro_wa, Taro) /? : :x  ~ (tcn- 
nis_wo, temfis) / b t:o (sita., played.)," tim modi- 
tiers of a verb "b  ?:_ (played)" are three tmnsetsus, 
"l~ U (yesterday)," :k~/i ~ (Taro)," "?  = x ~ (ten- 
nis)." Their apt)ropriate order is estimated in the 
following steps. 
1. The probabilities that the orders of the three 
pairs of modifiers "N- LI " and ":is: BII l~ ," "I~ 
U" and "?~:7 ,~,"  and "~fllIl?" and "?  
c .x  ~" are appropriate are estimated. As- 
sume, for example, ~-H ,;k~l~ta, PrI~ It ,? :-~. ~, and 
P;kfzlIla,~ ca  ~ are respectively 0.6, 0.8, and 0.7. 
2. As shown in Table 1, probabilities are estimated 
for all six possible orders. The order "I~ U / :k 
fill IS / -7- ~- y. ~ / b \]Co ," which has the highest 
probability, is selected as the most apt)ropriate 
order. 
2.3 Per fo rmance  Eva luat ion  
The pcrformancc of a word order model can be eval- 
uated in the following way. First, extract from a 
test corpus bunsetsus having two or more modifiers. 
Then, using those 1)unsetsus and their modifiers as 
873 
Data 
~Jtlnsetsll lJtlnsetstl nHIYiber Of babel 
nlllllber modifier 
0 1 P 
1 5 
2 3 
3 4 
4 5 P 
5 
Table 2: Example of modifiers extracted fl'om a corpus. 
Modifiers (Bunsetsu number) 
Strings in a bunsetsu 
>kflli ~ ( Taro_to, Taro and) 
~Y'{a ( lIanako_to, llanako) 
-Y- = x q~ (tennis_no, tennis) 
~lc .  (sial_hi, tournament) 
lll'C, (dete,, participate,) 
~@ b t=, (yusyo_sita., won.) 
Moditiers whose modiliee is the bunsetsu 
in the left column. 
~,~ a (0) 
?=x0~ (2) 
~a  (0) ~-r-~a 0) '~ :  (a) 
~e  (o) ~?-~* (1) If'~ (4) 
input, estimate the orders of the modifiers as de- 
scribed in Section 2.2. The percentage of the modi- 
flees whose modifiers' word order agrees with that in 
the original text then gives what we call the agree- 
ment rate. It is a measure of how close the word 
order estimated by the model is to the actual word 
order in the training corpus. 
We use the following two measurements to calcu- 
late the agreement rate. 
Pa i r  of  modi f ie rs  The first measurement is the 
percentage of the pairs of modifiers whose word 
order agrees with that in the test corpus. For 
exmnple, given the sentence in a test corpus "N 
kl (kinou, yesterday) / ~t I la  (Taro_wa, Taro) 
/ -7- = 2` ~2 (tennis_wo, tennis) / t. ~:o (sita., 
played.)," if the word order estimated by the 
model is "~ H (yesterday) / -7" -- 2. ~ (tennis) / 
~1~ ~:~ (Taro) / b too (played.)," then the or- 
ders of the pairs of modifiers in the original sen- 
tence are "N H / ;k~l~  ," "15 H / -7- =- :7, ~ ," and 
"~lit:~ / ~--2` ~ ," and those in the estimated 
word order are "~H / -~---2`~," "~H / 
1~1~ la~ ," and "Y- = 2` ~ / %:t~ll lak ." The agreement 
rate is 67% (2/3) because two of the three orders 
are the same as those in the original sentence. 
Complete  agreement  The second measurement is 
the percentage of the modifiees whose modifiers' 
word order agrees with that in the test corpus. 
3 Exper iments  and  D iscuss ion  
In our experiment, we used the Kyoto University text 
corpus (Version 2) (Kurohashi mid Nagao, 1997), a 
tagged corpus of the Mainichi newspaper. For train- 
ing, we used 17,562 sentences from newspaper arti- 
cles appearing in 1995, from January 1st to Jmmary 
8th and from Jmmary 10th to June 9th. For testing, 
we used 2,394 sentences fl'om articles appearing on 
January 9th and from June 10th to June 30th. 
3.1 Def in i t ion of  Word  Order  in  a Corpus  
In the Kyoto University corpus, each bunsetsu has 
only one modifiee. When a bunsetsu Bm depends on 
a bunsetsu Bd and there is a bunsetsu /3p that de- 
pends on and is coordinate with \])d, Bp has not only 
the information that its modifiee is \]~d but also a la- 
bel indicating a coordination or the information that 
it is coordinate with B d. This information indirectly 
shows that the bunsetsu Bm can depend on both \]3p 
and Bd. In this case, we consider Bm a modifier of 
both Bv and B d. 
Under this condition, modifiers of a bunsetsu B 
are identified in the following steps. 
1. Bunsetsus that depend on a bunsetsu B are clas- 
sifted as modifiers of B. 
2. When B has a label indicating a coordination, 
bunsetsus that are to tile left of 13 and depend on 
the same modifiee as B are classified as modifiers 
of B. 
3. Bunsetsus that depend on a modifier of B and 
have a label indicating a coordination are clas- 
sifted as modifiers of B. The third step is re- 
peated. 
When the above procedure is completed, all bunset- 
sus that coordinate with each other are identified as 
modifiers which depend oi1 the same nmdifiee. For 
example, from the data listed on the left side of To- 
ble 2, the modifiers listed in the right-hand column 
are identified for each bunsetsu. "Nt~I; ~ (Taro_to, 
Taro and)," "?~g-~ IS (Hanako_to, Hanako)," "ql "(, 
(dete,, participate,)" are all identified as modifiers 
which depend on the same modifiee "~ b 7=? 
(yusyo_sita., won.)." 
3.2 Exper imenta l  Resu l ts  
The features used in our experiment are listed in Ta- 
bles 3 and 4. Each feature consists of a type and 
a value. The features consist basically of some at- 
tributes of the bunsetsu itself, and syntactic and con- 
textual information. We call the features listed in 
Tables 3 'basic features.' We selected them man- 
ually so that they reflect the basic conditions gov- 
erning word order that were sunmmrized by Saeki 
(Saeki, 1998). The features in Table 4 are combina- 
tions of basic features ('combined features') and were 
also selected manually. They are represented by the 
nmne of the target bunsetsu plus the feature type of 
the basic features. The total number of features was 
about 190,000, and 51,590 of them were observed in 
the training cortms three or more times. These were 
the ones we used in our experiment. 
The following terms are used in these tables: 
Mdf r l ,  Mdf r2 ,  Mdfe:  The word order model de- 
scribed in Section 2.1 estimates the probability 
that modifiers are in the appropriate order as 
the product of the probabilities of all pairs of 
modifiers. When estimating the probability tbr 
each pair of modifiers, the model assmnes that 
the two modifiers are in the appropriate order. 
Here we call the left modifier Mdfrl, the right 
modifier Mdfr2, and their modifiee Mdfe. 
Head:  the rightmost word in a bunsetsu other than 
those whose major pro't-of-speech I category is 
1Part-of-speech categories follow those of JUMAN (Kuro- 
hashi and Nagao, 1998). 
874 
Table 3: Basic features. 
Bas ic  features  
Feature values (Number of type) Feature type tegors\] Target 
1)tmsetsus 
1 Mdfrl, Mdfr2, 
Mdfe 
2 Mdfrl, Mdfr2, 
Mdfe 
3 Mdfrl, Mdfr2, 
Mdfe 
4 Mdfrl, Mdh'2, 
Mdfe 
m m  
\[\]ead-POS(Major) 
\[tead-POS(Minor) 
\[lead-hff(Major) 
\[Iead-Inf(Minor) 
\[Iead-SemFeat(110) 
\[1ead-SemFeat(111 ) 
\[Iead-SemFeat(433) 
5 Mdfrl, Mdff2, rype(String) 
Mdfe rype(Major) 
type(Minor) 
6 Mdfrl, Mdfr2, lOSIll l(String) 
Mdfe lOSIIIl(Minor) 
lOSII12(String) 
lOSUI2(Minor) 
7 Mdfrl, Mdfr2, Period 
Mdfe 
8 Mdfrl, Mdfr2 Numl)erOfMdfrs 
Mdfe NumberOfMdfrs 
9 Mdfrl, Mdfr2, 
Mdfe 
10 Mdfl'l, Mdfr2 
Coordination 
(Total : 90) 
Mdfrl-MdfrType-ll )to-Mdfr2-Typc 
Mdfl'2-MdfrTypeql )to-Mdfl-I -q'ype 
Mdfi'l -Md frType-lDto- Md fr2-MdfrType 
11 Mdfrl, Mdfr2, Rel)etition-llead-l,ex 
Mdfe Repetition-Md fr-lleadq,ex 
12 Mdfrl, Mdfr2 ReferencePronoun 
i~efereneePronou  (String) 
',.%066) 
~u (verb), s~u (adjective), ~,'~ (noun) . . . .  (11) 
~'~:~ (common oun), m'~ (quantifier) . . . .  (24) 
~1~ (vowel verb) . . . .  (30) 
'.'~ (stem), t~*~ (fandamental form) . . . .  (60) 
rrue (1) 
true (1) 
true (1) 
:~, :a ,  <-u<, t:u, ~, ~=, t . . . .  (7:3) 
uJ:~ (post-positional particle), . . .  (43) 
?,~JJ'~ (e~use marker), ~*$ (imperative form) . . .  (102) 
'~',5, ~<', a~, ,., l,~. . . . .  (63) 
)ill\], ~$m (ease marker), . . .  (5) 
~, ~, *, ~,*,, ... ((~3) 
~,~;~1 (ease marker) . . . .  (41 
\[nil\], \[exist\] (2) 
A(0), B(1), C(2), 1)(3 or more) (4) 
A(2), B(:3), C(4 or more) (3) 
P(Coordim~te), A(Apposition), I)(otherwise) (3) 
\]'rue, False (2) 
rrue, False (2) 
true, False (2) 
86.65% 73.87% 
\[--0.79%) (--1.54%) 
87.07% 75.03% 
',--O.37%) (--0.38%) 
87.39% 75.20% 
',-0.05%) (-0.m%) 
87.21% 75.20% 
\[--0.23%) (--0.21%) 
84.78% 70.03% 
(-2.66%) (-5.38%) 
87.32% 75.14% 
(-0.12% (--0.27%) 
87.39% 7 ~  
(-0.05%) (+0.13%) 
87.14% 74.86% 
(-0.30%) (-0.55%) 
87.40% 70.30?./o 
(-0.04%) (-0.0~%) 
8~.2~% 73.61% 
(--1.18% (--1.80%) 
87.34% 75.09% 
(--0.10%) (--0.'32%) 
\[nil\],\[exist\] (2) 87.31% 75.id% 
\[nil\], \[exist\] (2) (--0.13%) (--0.27%) 
\[nil\], \[exist\] (2) 87.27% 75.12% 
:~, :a,  :~ .~,~=.~, ,  ~. . . . .  (42) (--0.17%) (--0.29%) 
'%} @ (special marks)," "112 N (1)ost-posi~ioual 
particles)," or "}~N~? (suffixes)." 
Head-Lex :  the fllndalnental forth (unintlected 
forln) of the head word. Only words with a fre- 
quency of tlve or more are used. 
Head- In f :  the inflection type of a head. 
SemFeat :  We use the upper third layers of bunrui 
.qoihyou (NLl/I(National Language Research In- 
stitute), 19641 as semantic features. Bunrui goi- 
hyou is a Japanese thesaurus that has a tree 
structure and consists of seven layers. The tree 
has words in its leaves, and each word has a fig- 
ure indicating its category number. For exam- 
ple, the figure in parenthesis of a feature "Head- 
SemFeat( l l0)" in Table 3 shows the upper three 
digits of the category number of the head word 
or the ancestor node of the head word in the 
third layer in the tree. 
Type:  the rightmost word other than those whose 
major part-of-speech category is "~@ (special 
marks)." If the major category of the word 
is neither "NJN (post-positional particles)" nor 
"}~/~'~ (suffixes)," and the word is inflectable, 2 
then the type is represented by the inflection 
type. 
JOSHI1 ,  JOSHI2 : JOSHI1  is the rightmost post- 
positional particle in the bunsetsu. And if there 
are two or more post-positional particles in the 
bunsetsu, JOSHI2 is the second-rightmost post- 
positiolml particle. 
NmnberOfMdf rs :  number of modifiers. 
2The inflection types follow those of J UMAN. 
Mdfr l -Mdf rType ,  Mdf r2 -Mdf rType:  Types of 
tile modifiers of Mdfi'l and Mdfr2. 
X- IDto -Y :  X is identical to Y. 
Repet i t ion -Head-Lex :  a ret)etition word allpear- 
ing ill a preceding senteuce. 
Re ferencePronour l :  a reference pronoun appear- 
ing in the target bunsetsu or ill its modifiers. 
Categories 1 to 6 ill Table 3 reI)resent attributes 
in a bunsetsu, categories 7 to 10 represent syntac- 
tic information, and categories 11 and 12 represent 
contextual information. 
The results of our experiment are listed in Table 5. 
The first line shows tlle agreement rate when we esti- 
mated word order for 5,278 bunsetsus that have two 
or more modifiers and were extracted from 2,394 sen- 
tences al)pearing on Jmmary 9th and from June 10th 
to June 301tl. \Ve used bunsetsu boundary informa- 
tion and syntactic and contextual information which 
were derivable froln the test corpus and related to 
the input bunsetsus. As syntactic ilffOrlnation we 
used dependency inforlnation, coordinate structure, 
and information on whether the target bunsetsu is at 
the eM of a sentence. As contextual information we 
used the preceding sentence. The values in the row 
labeled Baseline1 in Table 5 are the agreement rates 
obtained when every order of all pairs of modifiers 
was selected randolnly. And values in the B&seline2 
row are the agreement rates obtained when we used 
the following equation instead of Eq. (5): 
freq(w12) 
PMu'(llh) = freq(w12) + frcq(w21)" (7) 
875 
Table 4: Combined features. 
Accuracy without 
the feature 
Pair of Complete 
modifiers :tgreement 
87.23% 74.65% 
(-0.21%) (-0.76%) 
Combined  features  
- -  Twin tbatures 
(Mdfr 1-Type, Mdfr2-Type) ,  
(Mdfr 1-Type,  Mdfe- I Iead-Lex) ,  
(Mdfr  1-Type,  Md fe- t tead-POS) ,  
(Mdfr  1-Type,  Mdfr  1-Coordlnrtt ion),  
(Mdfr  1-Type, Mdf r2 -Mdf rType- IDto -Md fr1-2"ypc), 
(MdfrE-Type,  Mdfe-Head-Lex) ,  
(Mdfrg-Type,  Mdfe-Head-POS) ,  
(Mdfr2-Type,  Mdfr2-Ooord inat ion) ,  
(Mdfr2-Type,  Md fr 1-MdfrType- lDto-Mdfr2-Type) ,  
Mdfr  1-Head-Lex,  Mdfe-Per iod) ,  
Mdfr  1 -nead-POS,  Mdfe-Per lod) ,  
Mdfr  1-1tead-POS, Mdfr  1-Repet l t lon-Head- I ,ex) ,  
Mdfr2- I tead-Lex,  Mdfe-Pet lod) ,  
Mdfr2-Ite,~d-POS, Mdfe-Per lod) ,  
Mdf r2 - I Iead-POS,  Mdfr2- I tepet l t lon- I lead-Lex)  
' t~iplet  tca tures  87.22% i 74.86% 
Mdfr l -Wype,  Mdfr2-Type,  Mdfe- l lead-Lex) ,  (--0.220./0) (--0.55%) 
Mdfr l -Type ,  Mdf r2 -Type,  Mdfe-Head-POS) ,  
Mdf r l -Type ,  Mdf r l -Coord inat lon ,  Mdfe-Type) ,  
MdfrE-Type,  Mdfr2-Coord lnat lon ,  Mdfe-Type) ,  
Mdf r l - JOSHI1 ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-Lex),  
Mdf r l -aOSHl l ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-POS) ,  
Mdf r2 - JOSHI1 ,  Mdfr2- JOSI I I2 ,  Mdfe-Head-Lex),  
Mdf r2-aOSH\[1 ,  Mdfr2-JOSl~12, Mdfo- l lead-POS)  
All of  above  combined  features  85.79% 77.67% 
~--1.65%) (--3.74%) 
Table 5: Results of agreement rates. 
Agreement  ra te  
Pair of modifiers Coml)lete agreement 
Our method 87.44%(72,367/14,137) 75.41% (3,980/5,278) 
Baseline1 48.96% (6,921/14,137) 35.10% (7,747/5,278) 
Baseline2 49.20% (6,956/14,137) 53.84% (1,786/5,278) 
IIere we assume that B1 and \]32 are modifiers, their 
modifiee is B, the word types of B1 and \]32 are re- 
spectively Wl and we. The values frcq(wr2) and 
frcq(w.27 ) then respectively represent the fl'equencies 
with which w7 and w,2 appeared in the order "WT, we, 
mid w" and "w2, WT, and w" in Malnichi newspaper 
articles fl'om 1991 to 1997. a Equation (7) means 
that given the sentence "~t~lt I:t (Taro_wa) / ~- -- ~, 
(tennis_wo) / b ~-:o (sita.)," one of two possibili- 
ties, "1$ (wa) / ~ (wo) / t, ~:o (sita.)" and "#c (wo) 
/ tS (wa) / b ~:o (sita.)," which has the higher fre- 
quency, is selected. 
3.3 Features  and Agreement  Rate  
This section describes how much each feature set con- 
tributes to improving the agreement rate. 
The values listed in the rightmost columns in Ta- 
bles 3 and 4 shows the performance of the word or- 
der estimation without each feature set. The values 
in parentheses are the percentage of improvement or
degradation to the formal experiment. In the exper- 
iments, when a basic feature was deleted, the com- 
bined features that included the basic feature were 
also deleted. The most useful feature is the type of 
3When wl and w2 were the same word, we used the head 
words in Bt  and 132 as Wl and w2. When one offreq(wt2) and 
freq(w21) was zero and the other was five or more, we used 
the f lequencies when they appeared in the order "Wl ws" and 
"w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl). 
When both freq(wl.2) and freq(w27) were zero, we instead 
used random figures between 0 and t. 
bunsetsu, which basically signifies the case marker or 
inflection type. This result is close to our expecta- 
tions. 
We selected features that, according to linguistic 
studies, as mudl  as possible reflect the basic condi- 
tions governing word order. The rightmost column 
in Tables 3 and 4 shows the extent o which each con- 
dition contributes to improving the agreement rate. 
However, each category of features might be rougher 
than that which is linguistically interesting. For ex- 
ample, all case markers uch as "wa" and "wo" were 
classified into the same category, and were deleted 
together in the experiment when single categories 
were removed. An experiment that considers each 
of these markers eparately would help us verify the 
importance of these markers separately. If we find 
new features in future linguistic research on word or- 
der, the experiments lacking each feature separately 
would help us verify their importance in the same 
manner .  
3.4 Tra in ing  Corpus  and Agreement  Rate  
The agreement rates for the training corpus and the 
test corpus are shown in Figure 1 as a function of 
the amount of training data (ntunber of sentences). 
The agreement rates in the "pair of modifiers" and 
'?!19~ . . . . .  %:I:: 'i, ~ . . . . . . . . .  ~':z~-,~',:: : : i 
= 
?I 90 90 
!' 
S5 ~5 i ~ - 
E 
75 ~ 75 
7O 7O 
65 . . . . . . . .  (,5 0 . . . .  
0 2000 400{) 6000 80O0 Io(mO 120{}0 14O00 16000 18000 2000 4000 6,300 ~000 IODO0 12(;00 14000 16O00 la00{1 
\] 11o NtllObor o~ Snnloncos \]o 111o Traif l in9 Data l lm Number ol Sentences in l lm T f 4dlli11U \[)ala 
Figure 1: Relationship between tile amount of training 
data and the agreement rate. 
"Complete agreement" measurements were respec- 
tiw~ly 82.54% and 68.40%. These values were ob- 
tained with very small training sets (250 sentences). 
These rates m'e considerably higher than those of 
the baselines, indicating that word order in Japanese 
can be acquired fl'om newspaper articles even with a 
small training set. 
With 17,562 training sentences, the agreemenl, 
rate in the "Complete agreement" measurement was 
75.41%. We randomly selected and analyzed 100 
modifiees from 1,298 modifiees whose modifiers' word 
order did not agree with those in the original text. 
We found that 48 of them were in a natural order 
and 52 of them were in an unnatural order. The 
former result shows that the word order was rela- 
tively fl'ee and several orders were acceptable. The 
latter result shows that the word order acquisition 
was not sufficient. To complete the acquisition we 
need more training corpora and features which take 
into account different information than that m Ta- 
bles 3 mid 4. We found many idiomatic expres- 
876 
sions in the uimatural word order results, such as "~ 
ffl\[~il~:5~ (houchi-kokka_ga,  country under the rule 
of law) / \ [ l} l~  (kiitc, to listen) /~#t~ (alcireru, 
to disgust), ~rj ~ b ?= a ~ (souan-s~,ta-no_ga, orlgl- 
,ration) / ~ *o ~- *o co (somosomo-no, at all) / ~t~  U 
(hg~'ima~'4 the beginning)," and ""~ l~ (g#-~4 taste) / 
~'~B (seikon, one's heart and soul) /g~ 6 (homcru, 
to trot somethil,g into soinething)." We think that 
the apt)ropriate word order for these idiomatic ex- 
pressions could be acquired if we had more training 
data. We also found several coordinate structures in 
the Ulnlatural word order results, suggesting that we 
should survey linguistic studies on coordinate struc- 
tures and try to find efllcient features for acquiring 
word order from coordinate structures. 
We (lid not use the results of semantic and con- 
textual analyses as input because corpora with se- 
mantic and contextuM tags were not available. If 
such corpora were available, we could more et\[iciently 
use features dealing with seinantic features, reference 
pronouns, and repetition words. We plan to make 
corpora with semantic and contextual tags and use 
these tags as input. 
3.5 Acqu is i t ion  f rom a Raw Corpus  
In this section, we show that a raw cortms instead of 
a tagged corpus can be used to train the lnodel, if it 
is first analyzed by a parser. We used the lnorl)holog- 
ical analyzer JUMAN and a tmrser KNP (Kurohashi, 
11198) which is based on a det)endency grainlnar, 
it, order to extract iuforumtion from a raw corpus 
for detecting whether or not each feature is found. 
'l?tm accuracy of JUMAN for detecting inorphologi- 
cal boundaries and part-of-speech tags is about 98%, 
and the parsecs dependency accuracy is about 90%. 
These results were obtained from analyzing Mainichi 
newspaper articles. 
We used 217,562 sentences for training. When 
these sel~t, ences were all extracted from a raw corlms , 
the agreement rate was 87.64% for "pair of modifiers" 
and was 75.77% for "Colnplete agreement." When 
the 217,562 training sentences were sentences fl'oln 
the tagged cortms (17,562 sentences) used in our for- 
real exl)eriment aInl froln a raw cortms, the agree- 
" e S :~ ment rate for "pair of lno(hfi.r, was 87.66% and 
for "Complete agreement" was 75.88%. These rates 
were about 0.5% higher than those obtained when we 
used only sentences from a tagged corlms. Thus, we 
can acquire word order by adding inforlnation froln 
a rmv corpus even if we do not have a large tagged 
corpus. The results also indicate that the parser ac- 
curacy is not so significant for word order acquisition 
and that an accuracy of about 90% is sufficient. 
4 Conc lus ion 
This paper described a method of acquiring word or- 
der froln corpora. We defined word order as the order 
of lnodifiers which depend on tile same lnodifiee. The 
lnethod uses a model which estimates the likelihood 
of the apt)ropriate word order. The lnodel automat- 
ically discovers what the tendency of the word order 
in Japanese is by nsing various ldnds of information 
in and arouud the target bunsetsus plus syntactic 
and contextual inforlnation. The contribution rate 
of each piece of inforination in deciding word order 
is efficiently learned by a model implemented within 
an ),,I.E. framework. Comparing results of experi- 
ments controlling for each piece of information, we 
found that the type of inforinatiou having the great~ 
est influence was the case marker or inflection type in 
a bunsetsu. Analyzing the relationship between the 
amount of training data and the agreement rate, we 
fimnd that word order could be acquired even with 
a small set of training data. We also folmd that a 
raw cortms as well as a tagged cortms can be used to 
train the model, if it is first, analyzed by a parser. The 
agreement rate was 75.41% for the Kyoto University 
corpus. We analyzed the lnodifiees whose modifiers' 
word order did not agree with that in the original 
text, and folmd that 48% of theln were in a natural 
order. This shows that, in umny cases, word order 
in Japanese is relatively free and several orders are 
acceptable. 
The text we used were lmwspaper articles, which 
tend to have a standard word order, but we think 
that word orders tend to differ between ditferent 
styles of writing. We would therefore like to carry 
out experiments with other types of texts, such as 
novels, having styles different froln that of newspa- 
pers. 
it has been (lift\]cult o evaluate tile reslflts of text 
generation objectively becmlse there have been no 
good stmldards for ewlllmtion. By using the stan- 
(lard we describe in this paper, however, we can evN- 
uate results objectively, at least for word order esti- 
mation in text, generation. 
We expect hat our lnodel can be used for several 
applications as well as linguistic veritication, such as 
text; refinement silt)port and text generation in nla- 
chine translation. 
References  
Adam L. Better, Stephen A. I)ella Pietra, and Vincent J. Della 
Pietra. 199(L A Maximum t'\]ntropy Approach to N~ttural ,~tn- 
gmtge Processing. Computational Linguistics, 22(11:39-71. 
Sadao Kurohashi and Makol.o Nagao. 1997. Kyoto University 
Text Corl)uS Project. In Proceedings of The Third Annual 
Mectin9 of The Association for Natural Language Process- 
ing, pages 115-118. (in Japanese). 
Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morpho- 
logical Analysis System JUMAN Version 3.6. l)epartment of 
Informatics, Kyoto University. 
Sadao Kurohashi, 11198. Japanese Dependency/Case Structure 
Analyzer KNP Version 2.0b6. Department of Inforln~tties, Ky- 
ore University. 
IIiroshl Maruyama. 1994. Experhnents on V~rord-Order Recovery 
Using N-Cram Models. In YTte \]~9th Annual (;onvention IPS 
Japan. (in Japanese). 
NLRl(National Language Research Institute). 1964. Word List 
by Semantic Pri~ciples. Syuei Syuppan. (in Japmlese). 
Tetsuo Saekl. 1998. Yousetsu nihongo no 9ojun (Survey: Word 
Order in Japanese). Kuroshio Syupl)an. (in Japanese). 
James Shaw and Vasileios Ilatzivassiloglou. 1999. Ordering 
Among l'remodifiers. In Proceedings of the 37th Annual Meet- 
ing of the Association for Computational Linguistics (,4 CL), 
pages 135-143. 
Takenobu Tokunaga and IIozumi Tanaka. 1991. On Estimat- 
ing Japanese Word Order B~used on Valency Information. 
Keiryo Kokugogaku (Mathematical Linguistics), 18(21:53-(;5. 
(in Japanese). 
877 
Text Generation from Keywords
Kiyotaka Uchimoto? Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
We describe a method for generating sentences
from ?keywords? or ?headwords?. This method
consists of two main parts, candidate-text con-
struction and evaluation. The construction part
generates text sentences in the form of depen-
dency trees by using complementary informa-
tion to replace information that is missing be-
cause of a ?knowledge gap? and other missing
function words to generate natural text sen-
tences based on a particular monolingual cor-
pus. The evaluation part consists of a model
for generating an appropriate text when given
keywords. This model considers not only word
n-gram information, but also dependency infor-
mation between words. Furthermore, it consid-
ers both string information and morphological
information.
1 Introduction
Text generation is an important technique used
for applications like machine translation, sum-
marization, and human/computer dialogue. In
recent years, many corpora have become avail-
able, and have been used to generate natural
surface sentences. For example, corpora have
been used to generate sentences for language
model estimation in statistical machine trans-
lation. In such translation, given a source lan-
guage text, S, the translated text, T , in the
target language that maximizes the probabil-
ity P (T |S) is selected as the most appropri-
ate translation, T
best
, which is represented as
(Brown et al, 1990)
Tbest = argmaxTP (T |S)
= argmaxT (P (S|T ) ? P (T )) . (1)
In this equation, P (S|T ) represents the model
used to replace words or phrases in a source lan-
guage with those in the target language. It is
called a translation model. P (T ) represents a
language model that is used to reorder trans-
lated words or phrases into a natural order in
the target language. The input of the language
model is a ?bag of words,? and the goal of the
model is basically to reorder the words. At this
point, there is an assumption that natural sen-
tences can be generated by merely reordering
the words given by a translation model. To give
such a complete set of words, however, a trans-
lation model needs a large number of bilingual
corpora. If we could automatically complement
the words needed to generate natural sentences,
we would not have to collect the large number
of bilingual corpora required by a translation
model. In this paper, we assume that the role of
the translation model is not to give a complete
set of words that can be used to generate nat-
ural sentences, but to give a set of headwords
or center words that a speaker might want to
express, and describe a model that can provide
the complementary information needed to gen-
erate natural sentences by using a target lan-
guage corpus when given a set of headwords.
If we denote a set of headwords in a target
language as K, we can express Eq. (1) as
P (T |S) = P (K|S) ? P (T |K). (2)
P (K|S) in this equation represents a model
that gives a set of headwords in the target lan-
guage when given a source-language text sen-
tence. P (T |K) represents a model that gener-
ates text sentence T when given a set of head-
words, K. We call the model represented by
P (T |K) a text-generation model. In this paper,
we describe a text-generation model and a gen-
eration system that uses the model. Given a set
of headwords or keywords, our system outputs
the text sentence that maximizes P (T |K) as an
appropriate text sentence, T
best
:
Tbest = argmaxTP (T |K)
= argmaxT (P (K|T )? P (T )) . (3)
In this equation, we call the model represented
by P (K|T ) a keyword-production model. This
equation is equal to Eq. (1) when a source-
text sentence is replaced with a set of key-
words. Therefore, this model can be regarded
as a model that translates keywords into text
sentences. The model represented by P (T ) in
Eq. (3) is a language model used in statistical
machine translation. The n-gram model is the
most popular one used as a language model.
We assume that there is one extremely proba-
ble ordered set of morphemes and dependencies
between words that produce keywords, and we
express P (K|T ) as
P (K|T ) ? P (K,M,D|T )
= P (K|M,D, T ) ? P (D|M,T )? P (M |T ). (4)
In this equation, M denotes an ordered set of
morphemes and D denotes an ordered set of de-
pendencies in a sentence. P (K|M,D,T ) rep-
resents a keyword-production model. To es-
timate the models represented by P (D|M,T )
and P (M |T ), we use a dependency model and
a morpheme model, respectively, for the depen-
dency analysis and morphological analysis.
Statistical machine translation and example-
based machine translation require numerous
high-quality bilingual corpora. Interlingual ma-
chine translation and transfer-based machine
translation require a parser with high precision.
Therefore, these approaches to translation are
not practical if we do not have enough bilingual
corpora or a good parser. This is especially so if
the source text-sentences are incomplete or have
errors like those often found in OCR and speech-
recognition output. In these cases, however, if
we translate headwords into words in the target
language and generate sentences from the trans-
lated words by using our method, we should be
able to generate natural sentences from which
we can grasp the meaning of the source-text sen-
tences.
The text-generation model represented by
P (T |K) in Eq. (2) can be applied to various
tasks besides machine translation.
? Sentence-generation support system
for people with aphasia: About 300,000
people are reported to suffer from aphasia
in Japan, and 40% of them can select only
a few words to describe a picture. If candi-
date sentences can be generated from these
few words, it would help these people com-
municate with their families and friends.
? Support system for second language
writing: Beginners writing in second lan-
guage usually fined it easy to produce cen-
ter words or headwords, but often have dif-
ficulty generating complete sentences. If
several possible sentences could be gener-
ated from those words, it would help begin-
ners communicate with foreigners or study
second-language writing.
These are just two examples. We believe that
there are many other possible applications.
2 Overview of the Text-Generation
System
In this section, we give an overview of our sys-
tem for generating text sentences from given
keywords. As shown in Fig. 1, this system con-
sists of three parts: generation-rule acquisition,
candidate-text sentence construction, and eval-
uation.
Figure 1: Overview of the text-generation sys-
tem.
Given keywords, text sentences are generated
as follows.
1. During generation-rule acquisition, genera-
tion rules for each keyword are automati-
cally acquired.
2. Candidate-text sentences are constructed
during candidate-text construction by ap-
plying the rules acquired in the first
step. Each candidate-text sentence is rep-
resented by a graph or dependency tree.
3. Candidate-text sentences are ranked ac-
cording to their scores assigned during eval-
uation. The scores are calculated as a
probability estimated by using a keyword-
production model and a language model
that are trained with a corpus.
4. The candidate-text sentence that maxi-
mizes the score or the candidate-text sen-
tences whose scores are over a threshold
are selected as output. The system can
also output candidate-text sentences that
are ranked within the top N sentences.
In this paper, we assume that the target lan-
guage is Japanese. We define a keyword as the
headword of a bunsetsu. A bunsetsu is a phrasal
unit that usually consists of several content and
function words. We define the headword of a
bunsetsu as the rightmost content word in the
bunsetsu, and we define a content word as a
word whose part-of-speech is a verb, adjective,
noun, demonstrative, adverb, conjunction, at-
tribute, interjection, or undefined word. We
define the other words as function words. We
define formal nouns and auxiliary verbs ?SURU
(do)? and ?NARU (become)? as function words,
except when there are no other content words
in the same bunsetsu. Part-of-speech categories
follow those in the Kyoto University text corpus
(Version 3.0) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper.
Figure 2: Example of text generated from key-
words.
For example, given the set of keywords
?kanojo (she),? ?ie (house),? and ?iku (go),? as
shown in Fig. 2, our system retrieves sentences
including each word, and extracts each bunsetsu
that includes each word as a headword of the
bunsetsu. If there is no tagged corpus such
as the Kyoto University text corpus, each bun-
setsu can be extracted by using a morphological-
analysis system and a dependency-analysis sys-
tem such as JUMAN (Kurohashi and Nagao,
1999) and KNP (Kurohashi, 1998). Our system
then acquires generation rules as follows.
? ?kanojo (she)?kanojo (she) no (of)?
? ?kanojo (she)?kanojo (she) ga?
? ?ie (house)?ie (house) ni (to)?
? ?iku (go)?iku (go)?
? ?iku (go)?itta (went)?
The system next generates candidate bunsetsus
for each keyword and candidate-text sentences
in the form of dependency trees, such as ?Can-
didate 1? and ?Candidate 2? in Fig. 2, with
the assumption that there are dependencies be-
tween keywords. Finally, the candidate-text
sentences are ranked by their scores, calculated
by a text-generation model, and transformed
into surface sentences.
In this paper, we focus on the keyword-
production model represented by Eq. (4) and
assume that our system outputs sentences in the
form of dependency trees.
3 Candidate-Text Construction
We automatically acquire generation rules from
a monolingual target corpus at the time of gen-
erating candidate-text sentences. Generation
rules are restricted to those that generate bun-
setsus, and the generated bunsetsus must in-
clude each input keyword as a headword in the
bunsetsu. We then generate candidate-text sen-
tences in the form of dependency trees by simply
combining the bunsetsus generated by the rules.
The simple combination of generated bunsetsus
may produce semantically or grammatically in-
appropriate candidate-text sentences, but our
goal in this work was to generate a variety of
text sentences rather than a few fixed expres-
sions with high precision 1.
3.1 Generation-Rule Acquisition
Let us denote a set of keywords as KS and a
set of rules, each of which generates a bunsetsu
when given keyword k(?KS), as R
k
. We then
restrict r
k
(?R
k
) to those represented as
k ? hkm?. (5)
In this rule, h
k
represents the head morpheme
whose word is equal to keyword k; m? repre-
sents zero, one, or a series of morphemes that
are connected to h
k
in the same bunsetsu. Here,
we define a morpheme as consisting of a word
and its morphological information or grammat-
ical attribute, such as part-of-speech, and we
define a head morpheme as consisting of a head-
word and its grammatical attribute. By apply-
ing these rules, we generate bunsetsus from in-
put keywords.
3.2 Construction of Dependency Trees
Given keywords K = k1k2 . . . kn, candidate bun-
setsus are generated by applying the generation
rules described in Section 3.1. Next, by as-
suming dependency relationships between the
bunsetsus, candidate dependency trees are con-
structed. Dependencies between the bunsetsus
are restricted in that they must have the follow-
ing characteristics of Japanese dependencies:
1Note that 83.33% (3,973/4,768) of the headwords in
the newspaper articles appearing on January 17, 1995
were found in those appearing from January 1st to 16th.
However, only 21.82% (2,295/10,517) of the headword
dependencies in the newspaper articles appearing on
January 17th were found in those appearing from Jan-
uary 1st to 16th.
(i) Dependencies are directed from left to
right.
(ii) Dependencies do not cross.
(iii) All bunsetsus except the rightmost one de-
pend on only one other bunsetsu.
For example, when three keywords are given
and candidate bunsetsus including each keyword
are generated as b1, b2, and b3, the candidate de-
pendency trees are (b1 (b2 b3)) and ((b1 b2) b3)
if we do not reorder keywords, but 16 trees re-
sult if we consider the order of keywords to be
arbitrary.
4 Text-Generation Model
We next describe the model represented by Eq.
(4); that is, a keyword-production model, a
morpheme model that estimates how likely a
string is to be a morpheme, and a dependency
model. The goal of this model is to select
optimal sets of morphemes and dependencies
that can generate natural sentences. We imple-
mented these models within an maximum en-
tropy framework (Berger et al, 1996; Ristad,
1997; Ristad, 1998).
4.1 Keyword-Production Models
This section describes five keyword-production
models which are represented by P (K|M,D,T )
in Eq. (4). In these models, we define the set of
headwords whose frequency in the corpus is over
a certain threshold as a set of keywords, KS,
and we restrict the bunsetsus to those generated
by the generation rules represented in form (5).
We assume that all keywords are independent
and that k
i
corresponds to word w
j
(1 ? j ? m)
when text is given as a series of words w1 . . . wm.
1. trigram model
We assume that k
i
depends only on the two
anterior words w
j?1 and wj?2.
P (K|M,D, T ) =
n
?
i=1
P (ki|wj?1, wj?2).(6)
2. posterior trigram model
We assume that k
i
depends only on the two
posterior words w
j+1 and wj+2.
P (K|M,D, T ) =
n
?
i=1
P (ki|wj+1, wj+2).(7)
3. dependency bigram model
We assume that k
i
depends only on the two
rightmost words w
l
and w
l?1 in the right-
most bunsetsu that modifies the bunsetsu
including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (ki|wl, wl?1). (8)
Figure 3: Relationship between keywords and
words in bunsetsus.
4. posterior dependency bigram model
We assume that k
i
depends only on the
headword, w
s
, and the word on its right,
w
s+1, in the bunsetsu that is modified by
the bunsetsu including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (ki|ws, ws+1). (9)
5. dependency trigram model
We assume that k
i
depends only on the two
rightmost words w
l
and w
l?1 in the right-
most bunsetsu that modifies the bunsetsu,
and on the two rightmost words w
h
and
w
h?1 in the leftmost bunsetsu that modi-
fies the bunsetsu including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (k
i
|w
l
, w
l?1
, w
h
, w
h?1
). (10)
4.2 Morpheme Model
Let us assume that there are l grammatical
attributes assigned to morphemes. We call a
model that estimates the likelihood that a given
string is a morpheme and has the grammatical
attribute j(1 ? j ? l) a morpheme model.
Let us also assume that morphemes in the or-
dered set of morphemes M depend on the pre-
ceding morphemes. We can then represent the
probability of M , given text T ; namely, P (M |T )
in Eq. (4):
P (M |T ) =
n
?
i=1
P (mi|m
i?1
1
, T ), (11)
where m
i
can be one of the grammatical at-
tributes assigned to each morpheme.
4.3 Dependency Model
Let us assume that dependencies d
i
(1 ? i ? n)
in the ordered set of dependencies D are inde-
pendent. We can then represent P (D|M,T ) in
Eq. (4) as
P (D|M,T ) =
n
?
i=1
P (di|M,T ). (12)
5 Evaluation
To evaluate our system we made 30 sets of
keywords, with three keywords in each set, as
shown in Table 1. A human subject selected
the sets from headwords that were found ten
Table 1: Input keywords and examples of sys-
tem output.
Input (Keywords) Ex. of system output
??? ?? ?? (???? (??? ????))
?? ?? ??? ((??? ???) ???)
?? ?? ?? ((??? ???) ??)
??? ??? ?? (???? (??? ???))
?? ?? ???
?? ?? ??? ((??? ???) ???)
??? ?? ?? ((???? ???) ??)
??? ?? ?? (???? (??? ?????))
?? ?? ??
? ?? ?? ((?? ???) ????)
?? ?? ??? ((??? ???) ???)
?? ?? ?? ((??? ?????) ??)
?? ?? ?? (??? (??? ????))
?? ??? ?? ((??? ????) ??)
?? ?? ??? (??? (??? ????))
?? ??? ?? (??? (???? ?????))
?? ?? ??
?? ?? ?? ((??? ???) ??????)
??? ?? ? ((???? ??????) ?)
?? ?? ??? ((??? ???) ??????)
?? ?? ???? ((??? ???) ??????)
?? ?? ??? ((??? ???) ???)
?? ?? ??
??? ?? ?? ((???? ????) ??????)
?? ??? ?? ((??? ????) ????)
?? ?? ??? (??? (??? ???))
?? ??? ???? (??? (???? ???????))
?? ?? ??? ((??? ???) ???)
?? ??? ???
?? ?? ???
times or more in the newspaper articles on Jan-
uary 1st in the Kyoto University text corpus
(Version 3.0) without looking at the articles.
We evaluated each model by the percentage
of outputs that were subjectively judged as ap-
propriate by one of the authors. We used two
evaluation standards.
? Standard 1: If the dependency tree ranked
first is semantically and grammatically ap-
propriate, it is judged as appropriate.
? Standard 2: If there is at least one depen-
dency tree that is ranked within the top
ten and is semantically and grammatically
appropriate, it is judged as appropriate.
We used headwords that were found five times
or more in the newspaper articles appearing
from January 1st to 16th in the Kyoto Univer-
sity text corpus and also found in those appear-
ing on January 1st as the set of headwords, KS.
For headwords that were not in KS, we added
their major part-of-speech categories to the set.
We trained our keyword-production models by
using 1,129 sentences (containing 10,201 head-
words) from newspaper articles appearing on
January 1st. We used a morpheme model and a
dependency model identical to those proposed
by Uchimoto et al (Uchimoto et al, 2001; Uchi-
moto et al, 1999; Uchimoto et al, 2000b). To
train the models, we used 8,835 sentences from
newspaper articles appearing from January 1st
to 9th in 1995. Generation rules were acquired
from newspaper articles appearing from Jan-
uary 1st to 16th. The total number of sentences
was 18,435.
First, we evaluated the outputs generated
when the rightmost two keywords, such as ??
? and??,? on each line of Table 1 were input.
Table 2 shows the results. KM1 through KM5
stand for the five keyword-production models
described in Section 4.1, and MM and DM stand
for the morpheme and the dependency models,
respectively. The symbol + indicates a combi-
nation of models. In the models without MM,
DM, or both, P (M |T ) and P (D|M,T ) were as-
sumed to be 1. We carried out additional ex-
periments with models that considered both the
anterior and posterior words, such as the com-
bination of KM1 and KM2 or KM3 and KM4.
The results were at most 16/30 by standard 1
and 24/30 by standard 1.
Table 2: Results of subjective evaluation.
Model Standard 1 Standard 2
KM1 (trigram) 13/30 28/30
KM1 + MM 21/30 28/30
KM1 + DM 12/30 28/30
KM1 + MM + DM 26/30 28/30
KM2 (posterior trigram) 6/30 15/30
KM2 + MM 8/30 20/30
KM2 + DM 10/30 20/30
KM2 + MM + DM 9/30 25/30
KM3 (dependency bigram) 13/30 29/30
KM3 + MM 26/30 29/30
KM3 + DM 14/30 28/30
KM3 + MM + DM 27/30 29/30
KM4 (posterior dependency bigram) 10/30 18/30
KM4 + MM 9/30 26/30
KM4 + DM 9/30 22/30
KM4 + MM + DM 13/30 27/30
KM5 (dependency trigram) 12/30 26/30
KM5 + MM 17/30 28/30
KM5 + DM 12/30 27/30
KM5 + MM + DM 26/30 28/30
The models KM1+MM+DM,
KM3+MM+DM, and KM5+MM+DM
achieved the best results, as shown in Ta-
ble 2. For models KM1, KM3, and KM5, the
results with MM and DM were significantly
better than those without MM and DM in
the evaluation by standard 1. We believe this
was because cases are more tightly connected
with verbs than with nouns, so models KM1,
KM3, and KM5, which learn the connection
between cases and verbs, can better rank the
candidate-text sentences that have a natural
connection between cases and verbs than other
candidates.
Next, we conducted experiments using the
30 sets of keywords shown in Table 1 as in-
puts. We used two keyword-production mod-
els: model KM3+MM+DM, which achieved
the best results in the first experiment, and
model KM5+MM+DM, which considers the
richest information. We assumed that the in-
put keyword order was appropriate and did not
reorder the keywords. The results for both
models were the same: 19/30 in the evalu-
ation by standard 1 and 24/30 in the eval-
uation by standard 2. The right column of
Table 1 shows examples of the system out-
put. For example, for the input ??? (syourai,
in the future), ??? (shin-shin-tou, the New
Frontier Party), and ???? (umareru, to
be born)?, the dependency tree ?(???
[syourai wa] (???? [shin-shin-tou ga] ?
?????? [umareru darou]))? (?The New
Frontier Party will be born in the future.?)
was generated. This output was automati-
cally complemented by the appropriate modal-
ity ????? (darou, will), which agrees with
the word ???? (syourai, in the future), as
well as by post-positional particles such as ?
?? (wa, case marker) and ??? (ga). For
the input ???? (gaikoku-jin, a foreigner), ?
? (kanyuu, to join), and ?? (zouka, to in-
crease)?, the dependency tree ?(( ????
[gaikokujin no] ???? [kanyuu sya ga]) ?
????? [zouka shite iru] )? (?Foreigner
members are increasing in number.?) was
generated. This output was complemented
not only by the modality expression ???
??? (shite iru, the progressive form) and
post-positional particles such as ??? (no, of)
and ??? (ga), but also by the suffix ???
(sya, person), and a compound noun ?????
(kanyuu sya, member) was generated naturally.
In six cases, though, we did not obtain appro-
priate outputs because the candidate-text sen-
tences were not appropriately ranked. Improv-
ing the back-off ability of the model by using
classified words or synonyms as features should
enable us to rank sentences more appropriately.
6 Related Work
Many statistical generation methods have been
proposed. In this section, we describe the differ-
ences between our method and several previous
methods.
Japanese words are often followed by post-
positional particles, such as ?ga? and ?wo?,
to indicate the subject and object of a sen-
tence. There are no corresponding words in
English. Instead, English words are preceded
by articles, ?the? and ?a,? to distinguish def-
inite and indefinite nouns, and so on, and in
this case there are no corresponding words in
Japanese. Knight et al proposed a way to
compensate for missing information caused by
a lack of language-dependent knowledge, or a
?knowledge gap? (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998a; Langkilde
and Knight, 1998b). They use semantic expres-
sions as input, whereas we use keywords. Also,
they construct candidate-text sentences or word
lattices by applying rules, and apply their lan-
guage model, an n-gram model, to select the
most appropriate surface text. While we can-
not use their rules to generate candidate-text
sentences when given keywords, we can apply
their language model to our system to generate
surface-text sentences from candidate-text sen-
tences in the form of dependency trees. We can
also apply the formalism proposed by Langkilde
(Langkilde, 2000) to express the candidate-text
sentences.
Bangalore and Rambow proposed a method
to generate candidate-text sentences in the form
of trees (Bangalore and Rambow, 2000). They
consider dependency information when deriving
trees by using XTAG grammar, but they as-
sume that the input contains dependency infor-
mation. Our system generates candidate-text
sentences without relying on dependency infor-
mation in the input, and our model estimates
the dependencies between keywords.
Ratnaparkhi proposed models to generate
text from semantic attributes (Ratnaparkhi,
2000). The input of these models is semantic
attributes. His models are similar to ours if the
semantic attributes are replaced with keywords.
However, his models need a training corpus in
which certain words are replaced with seman-
tic attributes. Although our model also needs
a training corpus, the corpus can be automati-
cally created by using a morphological analyzer
and a dependency analyzer, both of which are
readily available.
Humphreys et al proposed using mod-
els developed for sentence-structure analysis to
rank candidate-text sentences (Humphreys et
al., 2001). As well as models developed for
sentence-structure analysis, we also use those
developed for morphological analysis and found
that these models contribute to the generation
of appropriate text.
Berger and Lafferty proposed a language
model for information retrieval (Berger and Laf-
ferty, 1999). Their concept is similar to that of
our model, which can be regarded as a model
that translates keywords into text, while their
model can be regarded as one that translates
query words into documents. However, the pur-
pose of their model is different: their goal is to
retrieve text that already exists while ours is to
generate new text.
7 Conclusion
We have described a method for generating sen-
tences from ?keywords? or ?headwords?. This
method consists of two main parts, candidate-
text construction and evaluation.
1. The construction part generates text sen-
tences in the form of dependency trees by
providing complementary information to
replace that missing due to a ?knowledge
gap? and other missing function words, and
thus generates natural text sentences based
on a particular monolingual corpus.
2. The evaluation part consists of a model
for generating an appropriate text sentence
when given keywords. This model consid-
ers the dependency information between
words as well as word n-gram informa-
tion. Furthermore, the model considers
both string and morphological information.
If a language model, such as a word n-gram
model, is applied to the generated-text sen-
tences in the form of dependency trees, an
appropriate surface-text sentence is generated.
The word-order model proposed by Uchimoto et
al. can also generate surface text in a natural
order (Uchimoto et al, 2000a).
There are several possible directions for our
future research. In particular,
? We would like to expand the generation
rules. We restricted the generation rules
automatically acquired from a corpus to
those that generate a bunsetsu. To gener-
ate a greater variety of candidate-text sen-
tences, we would like to expand the rules
that can generate a dependency tree. Ex-
pansion would lead to complementing with
content words as well as function words.
We also would like to prepare default rules
or to classify words into several classes
when no sentences including the keywords
are found in the target corpus.
? Some of the N-best text sentences gener-
ated by our system are semantically and
grammatically unnatural. To remove such
sentences from among the candidate-text
sentences, we must enhance our model so
that it can consider more information, such
as classified words or those in a thesaurus.
? We restricted keywords to the headwords or
rightmost content words in the bunsetsus.
We would like to expand the definition of
keywords to other content words and to
synonyms of the keywords.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data. We also thank Kimiko
Ohta, Hiroko Inui, Takehito Utsuro, Man-
abu Okumura, Akira Ushioda, Jun?ichi Tsujii,
Kiyosi Yasuda, and Masahisa Ohta for their
beneficial comments during the progress of this
work.
References
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
A. Berger and J. Lafferty. 1999. Information Retrieval as
Statistical Translation. In Proceedings of the ACM SIGIR,
pages 222?229.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A Statistical Approach to Machine Translation.
Computational Linguistics, 16(2):79?85.
K. Humphreys, M. Calcagno, and D. Weise. 2001. Reusing a
Statistical Language Model for Generation. In Proceedings
of the EWNLG.
K. Knight and V. Hatzivassiloglou. 1995. Two-Level, Many-
Paths Generation. In Proceedings of the ACL, pages 252?
260.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
S. Kurohashi, 1998. Japanese Dependency/Case Structure
Analyzer KNP Version 2.0b6. Department of Informatics,
Kyoto University.
I. Langkilde and K. Knight. 1998a. Generation that Exploits
Corpus-Based Statistical Knowledge. In Proceedings of the
COLING-ACL, pages 704?710.
I. Langkilde and K. Knight. 1998b. The Practical Value of
N-grams in Generation. In Proceedings of the INLG.
I. Langkilde. 2000. Forest-Based Statistical Sentence Gener-
ation. In Proceedings of the NAACL, pages 170?177.
A. Ratnaparkhi. 2000. Trainable Methods for Surface Natu-
ral Language Generation. In Proceedings of the NAACL,
pages 194?201.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese De-
pendency Structure Analysis Based on Maximum Entropy
Models. In Proceedings of the EACL, pages 196?203.
K. Uchimoto, M. Murata, Q. Ma, S. Sekine, and H. Isahara.
2000a. Word Order Acquisition from Corpora. In Proceed-
ings of the COLING, pages 871?877.
K. Uchimoto, M. Murata, S. Sekine, and H. Isahara. 2000b.
Dependency Model Using Posterior Context. In Proceed-
ings of the IWPT, pages 321?322.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
Morphological Analysis of The Spontaneous Speech Corpus
Kiyotaka Uchimoto?, Chikashi Nobata?, Atsushi Yamada?,
Satoshi Sekine?, and Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes a project tagging a sponta-
neous speech corpus with morphological infor-
mation such as word segmentation and parts-of-
speech. We use a morphological analysis system
based on a maximum entropy model, which is
independent of the domain of corpora. In this
paper we show the tagging accuracy achieved by
using the model and discuss problems in tagging
the spontaneous speech corpus. We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
1 Introduction
In recent years, systems developed for analyz-
ing written-language texts have become consid-
erably accurate. This accuracy is largely due
to the large amounts of tagged corpora and the
rapid progress in the study of corpus-based nat-
ural language processing. However, the accu-
racy of the systems developed for written lan-
guage is not always high when these same sys-
tems are used to analyze spoken-language texts.
The reason for this remaining inaccuracy is due
to several differences between the two types of
languages. For example, the expressions used
in written language are often quite different
from those in spoken language, and sentence
boundaries are frequently ambiguous in spoken
language. The ?Spontaneous Speech: Corpus
and Processing Technology? project was imple-
mented in 1999 to overcome this problem. Spo-
ken language includes both monologue and dia-
logue texts; the former (e.g. the text of a talk)
was selected as a target of the project because it
was considered to be appropriate to the current
level of study on spoken language.
Tagging the spontaneous speech corpus with
morphological information such as word seg-
mentation and parts-of-speech is one of the
goals of the project. The tagged corpus is help-
ful for us in making a language model in speech
recognition as well as for linguists investigat-
ing distribution of morphemes in spontaneous
speech. For tagging the corpus with morpholog-
ical information, a morphological analysis sys-
tem is needed. Morphological analysis is one of
the basic techniques used in Japanese sentence
analysis. A morpheme is a minimal grammat-
ical unit, such as a word or a suffix, and mor-
phological analysis is the process of segment-
ing a given sentence into a row of morphemes
and assigning to each morpheme grammatical
attributes such as part-of-speech (POS) and in-
flection type. One of the most important prob-
lems in morphological analysis is that posed by
unknown words, which are words found in nei-
ther a dictionary nor a training corpus. Two
statistical approaches have been applied to this
problem. One is to find unknown words from
corpora and put them into a dictionary (e.g.,
(Mori and Nagao, 1996)), and the other is to
estimate a model that can identify unknown
words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both
approaches. They proposed a morphological
analysis method based on a maximum entropy
(M.E.) model (Uchimoto et al, 2001). We used
their method to tag a spontaneous speech cor-
pus. Their method uses a model that can not
only consult a dictionary but can also identify
unknown words by learning certain characteris-
tics. To learn these characteristics, we focused
on such information as whether or not a string
is found in a dictionary and what types of char-
acters are used in a string. The model esti-
mates how likely a string is to be a morpheme.
This model is independent of the domain of cor-
pora; in this paper we demonstrate that this is
true by applying our model to the spontaneous
speech corpus, Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem of
Japanese morphological analysis can be reduced
to the problem of assigning one of two tags to
each string in a sentence. A string is tagged
with a 1 or a 0 to indicate whether or not it is
a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. The 1
tag is thus divided into the number, n, of gram-
matical attributes assigned to morphemes, and
the problem is to assign an attribute (from 0
to n) to every string in a given sentence. The
(n + 1) tags form the space of ?futures? in the
M.E. formulation of our problem of morpholog-
ical analysis. The M.E. model enables the com-
putation of P (f |h) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
The computation of P (f |h) in any M.E. model
is dependent on a set of ?features? which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h, f) =
?
?
?
?
?
1 : if has(h, x) = true,
x = ?POS(?1)(Major) : verb,??
& f = 1
0 : otherwise.
(1)
Here ?has(h,x)? is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and what part-of-speech
the adjacent morpheme is.
Given a set of features and some training
data, the M.E. estimation process produces a
model, which is represented as follows (Berger
et al, 1996; Ristad, 1997; Ristad, 1998):
P (f |h) =
?
i
?
g
i
(h,f)
i
Z
?
(h)
(2)
Z
?
(h) =
?
f
?
i
?
g
i
(h,f)
i
. (3)
We define a model which estimates the like-
lihood that a given string is a morpheme and
has the grammatical attribute i(1 ? i ? n) as a
morpheme model. This model is represented by
Eq. (2), in which f can be one of (n + 1) tags
from 0 to n.
Given a sentence, it is divided into mor-
phemes, and a grammatical attribute is assigned
to each morpheme so as to maximize the sen-
tence probability estimated by our morpheme
model. Sentence probability is defined as the
product of the probabilities estimated for a par-
ticular division of morphemes in a sentence. We
use the Viterbi algorithm to find the optimal set
of morphemes in a sentence.
3 Experiments and Discussion
3.1 Experimental Conditions
We used the spontaneous speech corpus, CSJ,
which is a tagged corpus of transcriptions of
academic presentations and simulated public
speech. Simulated public speech is short speech
spoken specifically for the corpus by paid non-
professional speakers. For training, we used
805,954 morphemes from the corpus, and for
testing, we used 68,315 morphemes from the
corpus. Since there are no boundaries between
sentences in the corpus, we used two types of
boundaries, utterance boundaries, which are au-
tomatically detected at the place where a pause
of 200 ms or longer emerges in the CSJ, and
sentence boundaries assigned by the sentence
boundary identification system, which is based
on hand-crafted rules which use the pauses as
a clue. In the CSJ, fillers and disfluencies are
marked with tags (F) and (D). In the experi-
ments, we did not use those tags. Thus the in-
put sentences for testing are character strings
without any tags. The output is a sequence
of morphemes with grammatical attributes. As
the grammatical attributes, we define the part-
of-speech categories in the CSJ. There are 12
major categories. Therefore, the number of
grammatical attributes is 12, and f in Eq. (2)
can be one of 13 tags from 0 to 12.
Given a sentence, for every string consist-
ing of five or fewer characters and every string
appearing in a dictionary, whether or not the
string is a morpheme was determined and then
the grammatical attribute of each string deter-
mined to be a morpheme was identified and
assigned to that string. We collected all mor-
phemes from the training corpus except dis-
fluencies and used them as dictionary entries.
We denote the entries with a Corpus dictionary.
The maximum length for a morpheme was set
at five because morphemes consisting of six or
more characters are mostly compound words or
words consisting of katakana characters. We as-
sumed that compound words that do not appear
in the dictionary can be divided into strings con-
sisting of five or fewer characters because com-
pound words tend not to appear in dictionar-
ies. Katakana strings that are not found in the
dictionary were assumed to be included in the
dictionary as an entry having the part-of-speech
?Unknown(Major), Katakana(Minor).? An op-
timal set of morphemes in a sentence is searched
for by employing the Viterbi algorithm. The
assigned part-of-speech in the optimal set is se-
lected from all the categories of the M.E. model
except the one in which the string is not a mor-
pheme.
The features used in our experiments are
listed in Table 1. Each feature consists of a
type and a value, which are given in the rows of
the table. The features are basically some at-
tributes of the morpheme itself or attributes of
the morpheme to the left of it. We used the fea-
tures found three or more times in the training
corpus. The notations ?(0)? and ?(-1)? used in
the feature type column in Table 1 respectively
indicate a target string and the morpheme to
the left of it.
The terms used in the table are as follows:
String: Strings appearing as a morpheme three
or more times in the training corpus
Substring: Characters used in a string.
?(Left1)? and ?(Right1)? respectively rep-
resent the leftmost and rightmost charac-
ters of a string. ?(Left2)? and ?(Right2)?
respectively represent the leftmost and
rightmost character bigrams of a string.
Dic: Entries in the Corpus dictionary. As mi-
nor categories we used inflection types such
as a basic form as well as minor part-of-
speech categories. ?Major&Minor? indi-
cates possible combinations between major
and minor part-of-speech categories. When
the target string is in the dictionary, the
part-of-speech attached to the entry corre-
sponding to the string is used as a feature
value. If an entry has two or more parts-
of-speech, the part-of-speech which leads to
the highest probability in a sentence esti-
mated from our model is selected as a fea-
ture value.
Length: Length of a string
TOC: Types of characters used in a string.
?(Beginning)? and ?(End)?, respectively,
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the ?(Begin-
ning)? and ?(End)? are the same character.
?TOC(0)(Transition)? represents the tran-
sition from the leftmost character to the
rightmost character in a string. ?TOC(-
1)(Transition)? represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
character in the target string. For example,
when the adjacent morpheme on the left
is ??? (sensei, teacher)? and the target
string is ?? (ni, case marker),? the feature
value ?Kanji?Hiragana? is selected.
POS: Part-of-speech.
3.2 Results and Discussion
Results of the morphological analysis obtained
by our method are shown in Table 2. Recall
is the percentage of morphemes in the test cor-
pus whose segmentation and major POS tag are
identified correctly. Precision is the percentage
of all morphemes identified by the system that
are identified correctly. The F-measure is de-
fined by the following equation.
F ? measure =
2 ? Recall ? Precision
Recall + Precision
This result shows that there is no significant
difference between accuracies obtained by us-
ing two types of sentence boundaries. However,
we found that the errors that occurred around
utterance boundaries were reduced in the re-
sult obtained with sentence boundaries assigned
by the sentence boundary identification system.
This shows that there is a high possibility that
we can achieve better accuracy if we use bound-
aries assigned by the sentence boundary identi-
fication system as sentence boundaries and if we
use utterance boundaries as features.
In these experiments, we used only the en-
tries with a Corpus dictionary. Next we show
the experimental results with dictionaries de-
veloped for a corpus on a certain domain. We
added to the Corpus dictionary all the approx-
imately 200,000 entries of the JUMAN dictio-
nary (Kurohashi and Nagao, 1999). We also
added the entries of a dictionary developed by
ATR. We call it the ATR dictionary.
Results obtained with each dictionary or each
combination of dictionaries are shown in Ta-
ble 3. In this table, OOV indicates Out-of-
Vocabulary rates. The accuracy obtained with
the JUMAN dictionary or the ATR dictionary
was worse than the accuracy obtained without
those dictionaries. This is because the segmen-
Table 1: Features.
Feature number Feature type Feature value (Number of value)
1 String(0) (223,457)
2 String(-1) (20,769)
3 Substring(0)(Left1) (2,492)
4 Substring(0)(Right1) (2,489)
5 Substring(0)(Left2) (74,046)
6 Substring(0)(Right2) (73,616)
7 Substring(-1)(Left1) (2,237)
8 Substring(-1)(Right1) (2,489)
9 Substring(-1)(Left2) (12,726)
10 Substring(-1)(Right2) (12,241)
11 Dic(0)(Major) Noun, Verb, Adj, . . . Undefined (13)
12 Dic(0)(Minor) Common noun, Topic marker, Basic form. . . (223)
13 Dic(0)(Major&Minor) Noun&Common noun, Verb&Basic form, . . . (239)
14 Length(0) 1, 2, 3, 4, 5, 6 or more (6)
15 Length(-1) 1, 2, 3, 4, 5, 6 or more (6)
16 TOC(0)(Beginning) Kanji, Hiragana, Number, Katakana, Alphabet (5)
17 TOC(0)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
18 TOC(0)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (25)
19 TOC(-1)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
20 TOC(-1)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (18)
21 POS(-1) Verb, Adj, Noun, . . . (12)
22 Comb(1,21) Combinations Feature 1 and 21 (142,546)
23 Comb(1,2,21) Combinations Feature 1, 2 and 21 (216,431)
24 Comb(1,13,21) Combinations Feature 1, 13 and 21 (29,876)
25 Comb(1,2,13,21) Combinations Feature 1, 2, 13 and 21 (158,211)
26 Comb(11,21) Combinations Feature 11 and 21 (156)
27 Comb(12,21) Combinations Feature 12 and 21 (1,366)
28 Comb(13,21) Combinations Feature 13 and 21 (1,518)
Table 2: Results of Experiments (Segmentation and major POS tagging).
Boundary Recall Precision F-measure
utterance 93.97% (64,198/68,315) 93.25% (64,198/68,847) 93.61
sentence 93.97% (64,195/68,315) 93.18% (64,195/68,895) 93.57
tation of morphemes and the definition of part-
of-speech categories in the JUMAN and ATR
dictionaries are different from those in the CSJ.
Given a sentence, for every string consisting
of five or fewer characters as well as every string
appearing in a dictionary, whether or not the
string is a morpheme was determined by our
morpheme model. However, we speculate that
we can ignore strings consisting of two or more
characters when they are not found in the dic-
tionary when OOV is low. Therefore, we carried
out the additional experiments ignoring those
strings. In the experiments, given a sentence,
for every string consisting of one character and
every string appearing in a dictionary, whether
or not the string is a morpheme is determined
by our morpheme model. Results obtained un-
der this condition are shown in Table 4. We
compared the accuracies obtained with dictio-
naries including the Corpus dictionary, whose
OOVs are relatively low. The accuracies ob-
tained with the additional dictionaries increased
while those obtained only with the Corpus dic-
tionary decreased. These results show that a
dictionary whose OOV in the test corpus is low
contributes to increasing the accuracy when ig-
noring the possibility that strings that consist
of two or more characters and are not found in
the dictionary become a morpheme.
These results show that a dictionary devel-
oped for a corpus on a certain domain can be
used to improve accuracy in analyzing a corpus
on another domain.
The accuracy in segmentation and major
POS tagging obtained for spontaneous speech
was worse than the approximately 95% obtained
for newspaper articles. We think the main rea-
son for this is the errors and the inconsistency
of the corpus, and the difficulty in recognizing
characteristic expressions often used in spoken
language such as fillers, mispronounced words,
and disfluencies. The inconsistency of the cor-
pus is due to the way the corpus was made, i.e.,
completely by human beings, and it is also due
Table 3: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.64% (63,288/68,315) 91.83% (63,288/68,917) 92.24 1.84%
Corpus sentence 92.61% (63,265/68,315) 91.79% (63,265/68,923) 92.20 1.84%
JUMAN utterance 90.28% (61,676/68,315) 90.07% (61,676/68,478) 90.17 6.13%
JUMAN sentence 90.33% (61,710/68,315) 90.22% (61,710/68,403) 90.27 6.13%
ATR utterance 89.80% (61,348/68,315) 90.12% (61,348/68,073) 89.96 8.14%
ATR sentence 89.96% (61,453/68,315) 90.30% (61,453/68,057) 90.13 8.14%
Corpus+JUMAN utterance 92.03% (62,872/68,315) 91.77% (62,872/68,507) 91.90 0.52%
Corpus+JUMAN sentence 92.09% (62,913/68,315) 91.80% (62,913/68,534) 91.95 0.52%
Corpus+ATR utterance 92.35% (63,086/68,315) 92.03% (63,086/68,547) 92.19 0.64%
Corpus+ATR sentence 92.30% (63,057/68,315) 91.94% (63,057/68,585) 92.12 0.64%
JUMAN+ATR utterance 91.60% (62,579/68,315) 91.57% (62,579/68,339) 91.59 4.61%
JUMAN+ATR sentence 91.66% (62,618/68,315) 91.67% (62,618/68,311) 91.66 4.61%
Corpus+JUMAN+ATR utterance 91.72% (62,658/68,315) 91.66% (62,658/68,357) 91.69 0.47%
Corpus+JUMAN+ATR sentence 91.72% (62,657/68,315) 91.62% (62,657/68,391) 91.67 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
Table 4: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.80% (63,395/68,315) 90.47% (63,395/70,075) 91.62 1.84%
Corpus sentence 92.71% (63,333/68,315) 90.48% (63,333/70,000) 91.58 1.84%
Corpus+JUMAN utterance 92.45% (63,154/68,315) 91.60% (63,154/68,942) 92.02 0.52%
Corpus+JUMAN sentence 92.48% (63,179/68,315) 91.71% (63,179/68,893) 92.09 0.52%
Corpus+ATR utterance 92.91% (63,474/68,315) 91.81% (63,474/69,137) 92.36 0.64%
Corpus+ATR sentence 92.75% (63,361/68,315) 91.76% (63,361/69,053) 92.25 0.64%
Corpus+JUMAN+ATR utterance 92.30% (63,055/68,315) 91.57% (63,055/68,858) 91.94 0.47%
Corpus+JUMAN+ATR sentence 92.28% (63,039/68,315) 91.55% (63,039/68,860) 91.91 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
to the definition of morphemes. Several incon-
sistencies in the test corpus existed, such as: ?
?? (tokyo, Noun)(Tokyo), ? (to, Other)(the
Metropolis), ? (ritsu, Other)(founded), ?
? (daigaku, Noun)(university),? and ???
(toritsu, Noun)(metropolitan), ?? (daigaku,
Noun)(university).? Both of these are the
names representing the same university. The
???? is partitioned into two in the first one
while it is not partitioned into two in the second
one according to the definition of morphemes.
When such inconsistencies in the corpus exist, it
is difficult for our model to discriminate among
these inconsistencies because we used only bi-
gram information as features. To achieve bet-
ter accuracy, therefore, we need to use trigram
or longer information. To correctly recognize
characteristic expressions often used in spoken
language, we plan to extract typical patterns
used in the expressions, to generalize the pat-
terns manually, and to generate possible expres-
sions using the generalized patterns, and finally,
to add such patterns to the dictionary. We also
plan to expand our model to skip fillers, mispro-
nounced words, and disfluencies because those
expressions are randomly inserted into text and
it is impossible to learn the connectivity be-
tween those randomly inserted expressions and
others.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis without a Dictionary for
Japanese. In Proceedings of the NLPRS, pages 541?544.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proceedings of the
LREC, pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distribu-
tional Analysis. In Proceedings of the COLING, pages
1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method
for Japanese Unknown Words using a Statistical Model
of Morphology and Context. In Proceedings of the ACL,
pages 277?284.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
 
		ffMorphological Analysis of a Large Spontaneous Speech Corpus in Japanese
Kiyotaka Uchimoto? Chikashi Nobata? Atsushi Yamada?
Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
3-5, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes two methods for de-
tecting word segments and their morpho-
logical information in a Japanese sponta-
neous speech corpus, and describes how
to tag a large spontaneous speech corpus
accurately by using the two methods. The
first method is used to detect any type of
word segments. The second method is
used when there are several definitions for
word segments and their POS categories,
and when one type of word segments in-
cludes another type of word segments. In
this paper, we show that by using semi-
automatic analysis we achieve a precision
of better than 99% for detecting and tag-
ging short words and 97% for long words;
the two types of words that comprise the
corpus. We also show that better accuracy
is achieved by using both methods than by
using only the first.
1 Introduction
The ?Spontaneous Speech: Corpus and Process-
ing Technology? project is sponsoring the construc-
tion of a large spontaneous Japanese speech corpus,
Corpus of Spontaneous Japanese (CSJ) (Maekawa
et al, 2000). The CSJ is a collection of mono-
logues and dialogues, the majority being mono-
logues such as academic presentations and simu-
lated public speeches. Simulated public speeches
are short speeches presented specifically for the cor-
pus by paid non-professional speakers. The CSJ in-
cludes transcriptions of the speeches as well as audio
recordings of them. One of the goals of the project
is to detect two types of word segments and cor-
responding morphological information in the tran-
scriptions. The two types of word segments were
defined by the members of The National Institute for
Japanese Language and are called short word and
long word. The term short word approximates a dic-
tionary item found in an ordinary Japanese dictio-
nary, and long word represents various compounds.
The length and part-of-speech (POS) of each are dif-
ferent, and every short word is included in a long
word, which is shorter than a Japanese phrasal unit,
a bunsetsu. If all of the short words in the CSJ
were detected, the number of the words would be
approximately seven million. That would be the
largest spontaneous speech corpus in the world. So
far, approximately one tenth of the words have been
manually detected, and morphological information
such as POS category and inflection type have been
assigned to them. Human annotators tagged every
morpheme in the one tenth of the CSJ that has been
tagged, and other annotators checked them. The hu-
man annotators discussed their disagreements and
resolved them. The accuracies of the manual tagging
of short and long words in the one tenth of the CSJ
were greater than 99.8% and 97%, respectively. The
accuracies were evaluated by random sampling. As
it took over two years to tag one tenth of the CSJ ac-
curately, tagging the remainder with morphological
information would take about twenty years. There-
fore, the remaining nine tenths of the CSJ must be
tagged automatically or semi-automatically.
In this paper, we describe methods for detecting
the two types of word segments and corresponding
morphological information. We also describe how
to tag a large spontaneous speech corpus accurately.
Henceforth, we call the two types of word segments
short word and long word respectively, or merely
morphemes. We use the term morphological anal-
ysis for the process of segmenting a given sentence
into a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a POS cate-
gory.
2 Problems and Their Solutions
As we mentioned in Section 1, tagging the whole of
the CSJ manually would be difficult. Therefore, we
are taking a semi-automatic approach. This section
describes major problems in tagging a large sponta-
neous speech corpus with high precision in a semi-
automatic way, and our solutions to those problems.
One of the most important problems in morpho-
logical analysis is that posed by unknown words,
which are words found in neither a dictionary nor
a training corpus. Two statistical approaches have
been applied to this problem. One is to find un-
known words from corpora and put them into a
dictionary (e.g., (Mori and Nagao, 1996)), and the
other is to estimate a model that can identify un-
known words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both ap-
proaches. They proposed a morphological analysis
method based on a maximum entropy (ME) model
(Uchimoto et al, 2001). Their method uses a model
that estimates how likely a string is to be a mor-
pheme as its probability, and thus it has a potential
to overcome the unknown word problem. Therefore,
we use their method for morphological analysis of
the CSJ. However, Uchimoto et al reported that the
accuracy of automatic word segmentation and POS
tagging was 94 points in F-measure (Uchimoto et
al., 2002). That is much lower than the accuracy ob-
tained by manual tagging. Several problems led to
this inaccuracy. In the following, we describe these
problems and our solutions to them.
? Fillers and disfluencies
Fillers and disfluencies are characteristic ex-
pressions often used in spoken language, but
they are randomly inserted into text, so detect-
ing their segmentation is difficult. In the CSJ,
they are tagged manually. Therefore, we first
delete fillers and disfluencies and then put them
back in their original place after analyzing a
text.
? Accuracy for unknown words
The morpheme model that will be described
in Section 3.1 can detect word segments and
their POS categories even for unknown words.
However, the accuracy for unknown words is
lower than that for known words. One of the
solutions is to use dictionaries developed for a
corpus on another domain to reduce the num-
ber of unknown words, but the improvement
achieved is slight (Uchimoto et al, 2002). We
believe that the reason for this is that defini-
tions of a word segment and its POS category
depend on a particular corpus, and the defi-
nitions from corpus to corpus differ word by
word. Therefore, we need to put only words
extracted from the same corpus into a dictio-
nary. We are manually examining words that
are detected by the morpheme model but that
are not found in a dictionary. We are also
manually examining those words that the mor-
pheme model estimated as having low proba-
bility. During the process of manual exami-
nation, if we find words that are not found in
a dictionary, those words are then put into a
dictionary. Section 4.2.1 will describe the ac-
curacy of detecting unknown words and show
how much those words contribute to improving
the morphological analysis accuracy when they
are detected and put into a dictionary.
? Insufficiency of features
The model currently used for morphological
analysis considers the information of a target
morpheme and that of an adjacent morpheme
on the left. To improve the model, we need to
consider the information of two or more mor-
phemes on the left of the target morpheme.
However, too much information often leads to
overtraining the model. Using all the informa-
tion makes training the model difficult when
there is too much of it. Therefore, the best
way to improve the accuracy of the morpholog-
ical information in the CSJ within the limited
time available to us is to examine and revise
the errors of automatic morphological analysis
and to improve the model. We assume that the
smaller the probability estimated by a model
for an output morpheme is, then the greater
the likelihood is that the output morpheme is
wrong. Therefore, we examine output mor-
phemes in ascending order of their probabili-
ties. The expected improvement of the accu-
racy of the morphological information in the
whole of the CSJ will be described in Sec-
tion 4.2.1
Another problem concerning unknown words
is that the cost of manual examination is high
when there are several definitions for word seg-
ments and their POS categories. Since there
are two types of word definitions in the CSJ,
the cost would double. Therefore, to reduce the
cost, we propose another method for detecting
word segments and their POS categories. The
method will be described in Section 3.2, and
the advantages of the method will be described
in Section 4.2.2
The next problem described here is one that we
have to solve to make a language model for auto-
matic speech recognition.
? Pronunciation
Pronunciation of each word is indispensable for
making a language model for automatic speech
recognition. In the CSJ, pronunciation is tran-
scribed separately from the basic form writ-
ten by using kanji and hiragana characters as
shown in Fig. 1. Text targeted for morpho-
Basic form Pronunciation
0017 00051.425-00052.869 L:
(F??) (F??)
????? ?????????
0018 00053.073-00054.503 L:
???? ????
0019 00054.707-00056.341 L:
???????? ?????????
?Well, I?m going to talk about morphological analysis.?
Figure 1: Example of transcription.
logical analysis is the basic form of the CSJ
and it does not have information on actual pro-
nunciation. The result of morphological anal-
ysis, therefore, is a row of morphemes that
do not have information on actual pronuncia-
tion. To estimate actual pronunciation by using
only the basic form and a dictionary is impossi-
ble. Therefore, actual pronunciation is assigned
to results of morphological analysis by align-
ing the basic form and pronunciation in the
CSJ. First, the results of morphological anal-
ysis, namely, the morphemes, are transliterated
into katakana characters by using a dictionary,
and then they are aligned with pronunciation
in the CSJ by using a dynamic programming
method.
In this paper, we will mainly discuss methods for
detecting word segments and their POS categories in
the whole of the CSJ.
3 Models and Algorithms
This section describes two methods for detecting
word segments and their POS categories. The first
method uses morpheme models and is used to detect
any type of word segment. The second method uses
a chunking model and is only used to detect long
word segments.
3.1 Morpheme Model
Given a tokenized test corpus, namely a set of
strings, the problem of Japanese morphological
analysis can be reduced to the problem of assign-
ing one of two tags to each string in a sentence. A
string is tagged with a 1 or a 0 to indicate whether
it is a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. A tag desig-
nated as a 1 is thus assigned one of a number, n, of
grammatical attributes assigned to morphemes, and
the problem becomes to assign an attribute (from 0
to n) to every string in a given sentence.
We define a model that estimates the likelihood
that a given string is a morpheme and has a gram-
matical attribute i(1 ? i ? n) as a morpheme
model. We implemented this model within an ME
modeling framework (Jaynes, 1957; Jaynes, 1979;
Berger et al, 1996). The model is represented by
Eq. (1):
p
?
(a|b) =
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
Z
?
(b)
(1)
Short word Long word
Word Pronunciation POS Others Word Pronunciation POS Others
?? (form) ????(keitai) Noun ????? (morphological
analysis)
????????
?
(keitaisokaiseki) Noun
? (element) ? (so) Suffix
?? (analysis)????(kaiseki) Noun
? ? (ni) PPP case marker ???? (about) ???? (nitsuite) PPP case marker,
compound
word
?? (relate) ?? (tsui) Verb KA-GYO, ADF, eu-
phonic change
? ? (te) PPP conjunctive
? ? (o) Prefix ??????(talk) ??????? (ohanashiitasi) Verb SA-GYO,
ADF
?? (talk) ??? (hanashi) Verb SA-GYO, ADF
???(do) ??? (itashi) Verb SA-GYO, ADF
?? ?? (masu) AUX ending form ?? ?? (masu) AUX ending form
PPP : post-positional particle , AUX : auxiliary verb , ADF : adverbial form
Figure 2: Example of morphological analysis results.
Z
?
(b) =
?
a
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
, (2)
where a is one of the categories for classification,
and it can be one of (n+1) tags from 0 to n (This is
called a ?future.?), b is the contextual or condition-
ing information that enables us to make a decision
among the space of futures (This is called a ?his-
tory.?), and Z
?
(b) is a normalizing constant deter-
mined by the requirement that
?
a
p
?
(a|b) = 1 for
all b. The computation of p
?
(a|b) in any ME model
is dependent on a set of ?features? which are binary
functions of the history and future. For instance, one
of our features is
g
i,j
(a, b) =
{
1 : if has(b, f
j
) = 1 & a = a
i
f
j
= ?POS(?1)(Major) : verb,??
0 : otherwise.
(3)
Here ?has(b, f
j
)? is a binary function that returns
1 if the history b has feature f
j
. The features used
in our experiments are described in detail in Sec-
tion 4.1.1.
Given a sentence, probabilities of n tags from 1
to n are estimated for each length of string in that
sentence by using the morpheme model. From all
possible division of morphemes in the sentence, an
optimal one is found by using the Viterbi algorithm.
Each division is represented as a particular division
of morphemes with grammatical attributes in a sen-
tence, and the optimal division is defined as a di-
vision that maximizes the product of the probabil-
ities estimated for each morpheme in the division.
For example, the sentence ???????????
??????? in basic form as shown in Fig. 1 is
analyzed as shown in Fig. 2. ??????? is ana-
lyzed as three morphemes, ??? (noun)?, ?? (suf-
fix)?, and ??? (noun)?, for short words, and as one
morpheme, ?????? (noun)? for long words.
In conventional models (e.g., (Mori and Nagao,
1996; Nagata, 1999)), probabilities were estimated
for candidate morphemes that were found in a dic-
tionary or a corpus and for the remaining strings
obtained by eliminating the candidate morphemes
from a given sentence. Therefore, unknown words
were apt to be either concatenated as one word or di-
vided into both a combination of known words and
a single word that consisted of more than one char-
acter. However, this model has the potential to cor-
rectly detect any length of unknown words.
3.2 Chunking Model
The model described in this section can be applied
when several types of words are defined in a cor-
pus and one type of words consists of compounds of
other types of words. In the CSJ, every long word
consists of one or more short words.
Our method uses two models, a morpheme model
for short words and a chunking model for long
words. After detecting short word segments and
their POS categories by using the former model,
long word segments and their POS categories are de-
tected by using the latter model. We define four la-
bels, as explained below, and extract long word seg-
ments by estimating the appropriate labels for each
short word according to an ME model. The four la-
bels are listed below:
Ba: Beginning of a long word, and the POS cat-
egory of the long word agrees with the short
word.
Ia: Middle or end of a long word, and the POS cat-
egory of the long word agrees with the short
word.
B: Beginning of a long word, and the POS category
of the long word does not agree with the short
word.
I: Middle or end of a long word, and the POS cat-
egory of the long word does not agree with the
short word.
A label assigned to the leftmost constituent of a long
word is ?Ba? or ?B?. Labels assigned to other con-
stituents of a long word are ?Ia?, or ?I?. For exam-
ple, the short words shown in Fig. 2 are labeled as
shown in Fig. 3. The labeling is done deterministi-
cally from the beginning of a given sentence to its
end. The label that has the highest probability as es-
timated by an ME model is assigned to each short
word. The model is represented by Eq. (1). In Eq.
(1), a can be one of four labels. The features used in
our experiments are described in Section 4.1.2.
Short word Long word
Word POS Label Word POS
?? Noun Ba ????? Noun
? Suffix I
?? Noun Ia
? PPP Ba ???? PPP
?? Verb I
? PPP Ia
? Prefix B ?????? Verb
?? Verb Ia
??? Verb Ia
?? AUX Ba ?? AUX
PPP : post-positional particle , AUX : auxiliary verb
Figure 3: Example of labeling.
When a long word that does not include a short
word that has been assigned the label ?Ba? or ?Ia?,
this indicates that the word?s POS category differs
from all of the short words that constitute the long
word. Such a word must be estimated individually.
In this case, we estimate the POS category by us-
ing transformation rules. The transformation rules
are automatically acquired from the training corpus
by extracting long words with constituents, namely
short words, that are labeled only ?B? or ?I?. A rule
is constructed by using the extracted long word and
the adjacent short words on its left and right. For
example, the rule shown in Fig. 4 was acquired in
our experiments. The middle division of the con-
sequent part represents a long word ???? (auxil-
iary verb), and it consists of two short words ???
(post-positional particle) and ??? (verb). If several
different rules have the same antecedent part, only
the rule with the highest frequency is chosen. If no
rules can be applied to a long word segment, rules
are generalized in the following steps.
1. Delete posterior context
2. Delete anterior and posterior contexts
3. Delete anterior and posterior contexts and lexi-
cal entries.
If no rules can be applied to a long word segment in
any step, the POS category noun is assigned to the
long word.
4 Experiments and Discussion
4.1 Experimental Conditions
In our experiments, we used 744,204 short words
and 618,538 long words for training, and 63,037
short words and 51,796 long words for testing.
Those words were extracted from one tenth of the
CSJ that already had been manually tagged. The
training corpus consisted of 319 speeches and the
test corpus consisted of 19 speeches.
Transcription consisted of basic form and pronun-
ciation, as shown in Fig. 1. Speech sounds were
faithfully transcribed as pronunciation, and also rep-
resented as basic forms by using kanji and hiragana
characters. Lines beginning with numerical digits
are time stamps and represent the time it took to
produce the lines between that time stamp and the
next time stamp. Each line other than time stamps
represents a bunsetsu. In our experiments, we used
only the basic forms. Basic forms were tagged with
several types of labels such as fillers, as shown in
Table 1. Strings tagged with those labels were han-
dled according to rules as shown in the rightmost
columns in Table 1.
Since there are no boundaries between sentences
in the corpus, we selected the places in the CSJ that
Anterior context Target words Posterior context
Entry ?? (it, go) ? (te)? (mi, try) ?? (tai, want)
POS Verb PPP Verb AUX
Label Ba B I Ba
Antecedent part
?
Anterior context Long word Posterior context
?? (it, go) ?? (temi, try) ?? (tai, want)
Verb AUX AUX
Consequent part
Figure 4: Example of transformation rules.
Table 1: Type of labels and their handling.
Type of Labels Example Rules
Fillers (F??) delete all
Disfluencies (D?)????? (D2?)? delete all
No confidence in
transcription
(? ?????) leave a candidate
Entirely (?) delete all
Several can- (? ???,????) leave the former
didates exist candidate
Citation on sound or
words
(M?)? (M?)??? leave a candidate
Foreign, archaic, or
dialect words
(O???????) leave a candidate
Personal name, dis-
criminating words,
and slander
???? (R??)??? leave a candidate
Letters and their
pronunciation in
katakana strings
(A????;EU) leave the former
candidate
Strings that cannot
be written in kanji
characters
(K? (F??)??;?) leave the latter can-
didate
are automatically detected as pauses of 500 ms or
longer and then designated them as sentence bound-
aries. In addition to these, we also used utterance
boundaries as sentence boundaries. These are au-
tomatically detected at places where short pauses
(shorter than 200 ms but longer than 50 ms) follow
the typical sentence-ending forms of predicates such
as verbs, adjectives, and copula.
4.1.1 Features Used by Morpheme Models
In the CSJ, bunsetsu boundaries, which are phrase
boundaries in Japanese, were manually detected.
Fillers and disfluencies were marked with the labels
(F) and (D). In the experiments, we eliminated fillers
and disfluencies but we did use their positional infor-
mation as features. We also used as features, bun-
setsu boundaries and the labels (M), (O), (R), and
(A), which were assigned to particular morphemes
such as personal names and foreign words. Thus, the
input sentences for training and testing were charac-
ter strings without fillers and disfluencies, and both
boundary information and various labels were at-
tached to them. Given a sentence, for every string
within a bunsetsu and every string appearing in a
dictionary, the probabilities of a in Eq. (1) were es-
timated by using the morpheme model. The output
was a sequence of morphemes with grammatical at-
tributes, as shown in Fig. 2. We used the POS cate-
gories in the CSJ as grammatical attributes. We ob-
tained 14 major POS categories for short words and
15 major POS categories for long words. Therefore,
a in Eq. (1) can be one of 15 tags from 0 to 14 for
short words, and it can be one of 16 tags from 0 to
15 for long words.
Table 2: Features.
Number Feature Type Feature value
(Number of value) (Short:Long)
1 String(0) (113,474:117,002)
2 String(-1) (17,064:32,037)
3 Substring(0)(Left1) (2,351:2,375)
4 Substring(0)(Right1) (2,148:2,171)
5 Substring(0)(Left2) (30,684:31,456)
6 Substring(0)(Right2) (25,442:25,541)
7 Substring(-1)(Left1) (2,160:2,088)
8 Substring(-1)(Right1) (1,820:1,675)
9 Substring(-1)(Left2) (11,025:12,875)
10 Substring(-1)(Right2) (10,439:13,364)
11 Dic(0)(Major) Noun, Verb, Adjective, . . . Unde-
fined (15:16)
12 Dic(0)(Minor) Common noun, Topic marker, Ba-
sic form. . . (75:71)
13 Dic(0)(Major&Minor) Noun&Common noun,
Verb&Basic form, . . . (246:227)
14 Dic(-1)(Minor) Common noun, Topic marker, Ba-
sic form. . . (16:16)
15 POS(-1) Noun, Verb, Adjective, . . . (14:15)
16 Length(0) 1, 2, 3, 4, 5, 6 or more (6:6)
17 Length(-1) 1, 2, 3, 4, 5, 6 or more (6:6)
18 TOC(0)(Beginning) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
19 TOC(0)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
20 TOC(0)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (25:25)
21 TOC(-1)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
22 TOC(-1)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (16:15)
23 Boundary Bunsetsu(Beginning), Bun-
setsu(End), Label(Beginning),
Label(End), (4:4)
24 Comb(1,15) (74,602:59,140)
25 Comb(1,2,15) (141,976:136,334)
26 Comb(1,13,15) (78,821:61,813)
27 Comb(1,2,13,15) (156,187:141,442)
28 Comb(11,15) (209:230)
29 Comb(12,15) (733:682)
30 Comb(13,15) (1,549:1,397)
31 Comb(12,14) (730:675)
The features we used with morpheme models in
our experiments are listed in Table 2. Each feature
consists of a type and a value, which are given in the
rows of the table, and it corresponds to j in the func-
tion g
i,j
(a, b) in Eq. (1). The notations ?(0)? and
?(-1)? used in the feature-type column in Table 2 re-
spectively indicate a target string and the morpheme
to the left of it. The terms used in the table are ba-
sically as same as those that Uchimoto et al used
(Uchimoto et al, 2002). The main difference is the
following one:
Boundary: Bunsetsu boundaries and positional in-
formation of labels such as fillers. ?(Begin-
ning)? and ?(End)? in Table 2 respectively indi-
cate whether the left and right side of the target
strings are boundaries.
We used only those features that were found three or
more times in the training corpus.
4.1.2 Features Used by a Chunking Model
We used the following information as features
on the target word: a word and its POS cate-
gory, and the same information for the four clos-
est words, the two on the left and the two on
the right of the target word. Bigram and tri-
gram words that included a target word plus bigram
and trigram POS categories that included the tar-
get word?s POS category were used as features. In
addition, bunsetsu boundaries as described in Sec-
tion 4.1.1 were used. For example, when a target
word was ??? in Fig. 3, ???, ????, ???, ??
??, ???, ?Suffix?, ?Noun?, ?PPP?, ?Verb?, ?PPP?,
???&??, ??&???, ?? &?? &??, ??
&??&??, ?Noun&PPP?, ?PPP&Verb?, ?Suf-
fix&Noun&PPP?, ?PPP&Verb&PPP?, and ?Bun-
setsu(Beginning)? were used as features.
4.2 Results and Discussion
4.2.1 Experiments Using Morpheme Models
Results of the morphological analysis obtained by
using morpheme models are shown in Table 3 and
4. In these tables, OOV indicates Out-of-Vocabulary
rates. Shown in Table 3, OOV was calculated as the
proportion of words not found in a dictionary to all
words in the test corpus. In Table 4, OOV was cal-
culated as the proportion of word and POS category
pairs that were not found in a dictionary to all pairs
in the test corpus. Recall is the percentage of mor-
phemes in the test corpus for which the segmentation
and major POS category were identified correctly.
Precision is the percentage of all morphemes identi-
fied by the system that were identified correctly. The
F-measure is defined by the following equation.
F ? measure =
2? Recall ? Precision
Recall + Precision
Table 3: Accuracies of word segmentation.
Word Recall Precision F OOV
Short 97.47% (61,444
63,037
) 97.62% (61,444
62,945
) 97.54 1.66%
99.23% (62,553
63,037
) 99.11% (62,553
63,114
) 99.17 0%
Long 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21 5.81%
99.05% (51,306
51,796
) 98.58% (51,306
52,047
) 98.81 0%
Table 4: Accuracies of word segmentation and POS
tagging.
Word Recall Precision F OOV
Short 95.72% (60,341
63,037
) 95.86% (60,341
62,945
) 95.79 2.64%
97.57% (61,505
63,037
) 97.45% (61,505
63,114
) 97.51 0%
Long 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21 6.93%
97.30% (50,396
51,796
) 96.83% (50,396
52,047
) 97.06 0%
Tables 3 and 4 show that accuracies would im-
prove significantly if no words were unknown. This
indicates that all morphemes of the CSJ could be an-
alyzed accurately if there were no unknown words.
The improvements that we can expect by detecting
unknown words and putting them into dictionaries
are about 1.5 in F-measure for detecting word seg-
ments of short words and 2.5 for long words. For de-
tecting the word segments and their POS categories,
for short words we expect an improvement of about
2 in F-measure and for long words 3.
Next, we discuss accuracies obtained when un-
known words existed. The OOV for long words
was 4% higher than that for short words. In gen-
eral, the higher the OOV is, the more difficult de-
tecting word segments and their POS categories
is. However, the difference between accuracies
for short and long words was about 1% in recall
and 2% in precision, which is not significant when
we consider that the difference between OOVs for
short and long words was 4%. This result indi-
cates that our morpheme models could detect both
known and unknown words accurately, especially
long words. Therefore, we investigated the recall
of unknown words in the test corpus, and found
that 55.7% (928/1,667) of short word segments and
74.1% (2,660/3,590) of long word segments were
detected correctly. In addition, regarding unknown
words, we also found that 47.5% (791/1,667) of
short word segments plus their POS categories and
67.3% (2,415/3,590) of long word segments plus
their POS categories were detected correctly. The
recall of unknown words was about 20% higher for
long words than for short words. We believe that
this result mainly depended on the difference be-
tween short words and long words in terms of the
definitions of compound words. A compound word
is defined as one word when it is based on the def-
inition of long words; however it is defined as two
or more words when it is based on the definition of
short words. Furthermore, based on the definition of
short words, a division of compound words depends
on its context. More information is needed to pre-
cisely detect short words than is required for long
words. Next, we extracted words that were detected
by the morpheme model but were not found in a dic-
tionary, and investigated the percentage of unknown
words that were completely or partially matched to
the extracted words by their context. This percent-
age was 77.6% (1,293/1,667) for short words, and
80.6% (2,892/3,590) for long words. Most of the re-
maining unknown words that could not be detected
by this method are compound words. We expect that
these compounds can be detected during the manual
examination of those words for which the morpheme
model estimated a low probability, as will be shown
later.
The recall of unknown words was lower than that
of known words, and the accuracy of automatic mor-
phological analysis was lower than that of manual
morphological analysis. As previously stated, to
improve the accuracy of the whole corpus we take
a semi-automatic approach. We assume that the
smaller the probability is for an output morpheme
estimated by a model, the more likely the output
morpheme is wrong, and we examine output mor-
phemes in ascending order of their probabilities. We
investigated how much the accuracy of the whole
corpus would increase. Fig. 5 shows the relation-
ship between the percentage of output morphemes
whose probabilities exceed a threshold and their
93
94
95
96
97
98
99
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n 
(%
)
Output Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 5: Partial analysis.
precision. In this figure, ?short without UKW?,
?long without UKW??, ?short with UKW?, and
?long with UKW? represent the precision for short
words detected assuming there were no unknown
words, precision for long words detected assuming
there were no unknown words, precision of short
words including unknown words, and precision of
long words including unknown words, respectively.
When the output rate in the horizontal axis in-
creases, the number of low-probability morphemes
increases. In all graphs, precisions monotonously
decrease as output rates increase. This means that
tagging errors can be revised effectively when mor-
phemes are examined in ascending order of their
probabilities.
Next, we investigated the relationship between the
percentage of morphemes examined manually and
the precision obtained after detected errors were re-
vised. The result is shown in Fig. 6. Precision
represents the precision of word segmentation and
POS tagging. If unknown words were detected and
put into a dictionary by the method described in the
fourth paragraph of this section, the graph line for
short words would be drawn between the graph lines
?short without UKW? and ?short with UKW?, and
the graph line for long words would be drawn be-
tween the graph lines ?long without UKW? and
?long with UKW?. Based on test results, we can
expect better than 99% precision for short words
and better than 97% precision for long words in the
whole corpus when we examine 10% of output mor-
93
94
95
96
97
98
99
100
0 20 40 60 80 100 120
Pr
ec
is
io
n 
(%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 6: Relationship between the percentage of
morphemes examined manually and precision ob-
tained after revising detected errors (when mor-
phemes with probabilities under threshold and their
adjacent morphemes are examined).
0
10
20
30
40
50
60
0 5 10 15 20 25 30 35 40 45 50
Er
ro
r R
at
es
 in
 E
xa
m
in
ed
 M
or
ph
em
es
 (%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"short_with_UKW"
"long_without_UKW"
"long_with_UKW"
Figure 7: Relationship between percentage of mor-
phemes examined manually and error rate of exam-
ined morphemes.
phemes in ascending order of their probabilities.
Finally, we investigated the relationship between
percentage of morphemes examined manually and
the error rate for all of the examined morphemes.
The result is shown in Fig. 7. We found that about
50% of examined morphemes would be found as er-
rors at the beginning of the examination and about
20% of examined morphemes would be found as
errors when examination of 10% of the whole cor-
pus was completed. When unknown words were de-
tected and put into a dictionary, the error rate de-
creased; even so, over 10% of examined morphemes
would be found as errors.
4.2.2 Experiments Using Chunking Models
Results of the morphological analysis of long
words obtained by using a chunking model are
shown in Table 5 and 6. The first and second lines
Table 5: Accuracies of long word segmentation.
Model Recall Precision F
Morph 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21
Chunk 97.65% (50,580
51,796
) 97.41% (50,580
51,911
) 97.54
Chunk 98.84% (51,193
51,796
) 98.66% (51,193
51,888
) 98.75
Table 6: Accuracies of long word segmentation and
POS tagging.
Model Recall Precision F
Morph 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21
Chunk 95.59% (49,513
51,796
) 95.38% (49,513
51,911
) 95.49
Chunk 98.56% (51,051
51,796
) 98.39% (51,051
51,888
) 98.47
Chunk w/o TR 92.61% (47,968
51,796
) 92.40% (47,968
51,911
) 92.51
TR : transformation rules
show the respective accuracies obtained when OOVs
were 5.81% and 6.93%. The third lines show the ac-
curacies obtained when we assumed that the OOV
for short words was 0% and there were no errors in
detecting short word segments and their POS cate-
gories. The fourth line in Table 6 shows the accuracy
obtained when a chunking model without transfor-
mation rules was used.
The accuracy obtained by using the chunking
model was one point higher in F-measure than that
obtained by using the morpheme model, and it was
very close to the accuracy achieved for short words.
This result indicates that errors newly produced by
applying a chunking model to the results obtained
for short words were slight, or errors in the results
obtained for short words were amended by apply-
ing the chunking model. This result also shows that
we can achieve good accuracy for long words by ap-
plying a chunking model even if we do not detect
unknown long words and do not put them into a dic-
tionary. If we could improve the accuracy for short
words, the accuracy for long words would be im-
proved also. The third lines in Tables 5 and 6 show
that the accuracy would improve to over 98 points
in F-measure. The fourth line in Tables 6 shows that
transformation rules significantly contributed to im-
proving the accuracy.
Considering the results obtained in this section
and in Section 4.2.1, we are now detecting short and
long word segments and their POS categories in the
whole corpus by using the following steps:
1. Automatically detect and manually examine
unknown words for short words.
2. Improve the accuracy for short words in the
whole corpus by manually examining short
words in ascending order of their probabilities
estimated by a morpheme model.
3. Apply a chunking model to the short words to
detect long word segments and their POS cate-
gories.
As future work, we are planning to use an active
learning method such as that proposed by Argamon-
Engelson and Dagan (Argamon-Engelson and Da-
gan, 1999) to more effectively improve the accuracy
of the whole corpus.
5 Conclusion
This paper described two methods for detecting
word segments and their POS categories in a
Japanese spontaneous speech corpus, and describes
how to tag a large spontaneous speech corpus accu-
rately by using the two methods. The first method is
used to detect any type of word segments. We found
that about 80% of unknown words could be semi-
automatically detected by using this method. The
second method is used when there are several defi-
nitions for word segments and their POS categories,
and when one type of word segments includes an-
other type of word segments. We found that better
accuracy could be achieved by using both methods
than by using only the first method alone.
Two types of word segments, short words and
long words, are found in a large spontaneous speech
corpus, CSJ. We found that the accuracy of auto-
matic morphological analysis for the short words
was 95.79 in F-measure and for long words, 95.49.
Although the OOV for long words was much higher
than that for short words, almost the same accuracy
was achieved for both types of words by using our
proposed methods. We also found that we can ex-
pect more than 99% of precision for short words,
and 97% for long words found in the whole corpus
when we examined 10% of output morphemes in as-
cending order of their probabilities as estimated by
the proposed models.
In our experiments, only the information con-
tained in the corpus was used; however, more appro-
priate linguistic knowledge than that could be used,
such as morphemic and syntactic rules. We would
like to investigate whether such linguistic knowl-
edge contributes to improved accuracy.
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-Based
Sample Selection For Probabilistic Classifiers. Artificial In-
telligence Research, 11:335?360.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
Maximum Entropy Approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39?71.
E. T. Jaynes. 1957. Information Theory and Statistical Me-
chanics. Physical Review, 106:620?630.
E. T. Jaynes. 1979. Where do we Stand on Maximum Entropy?
In R. D. Levine and M. Tribus, editors, The Maximum En-
tropy Formalism, page 15. M. I. T. Press.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis Without a Dictionary for
Japanese. In Proceedings of NLPRS, pages 541?544.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Sponta-
neous Speech Corpus of Japanese. In Proceedings of LREC,
pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distributional
Analysis. In Proceedings of COLING, pages 1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method for
Japanese Unknown Words Using a Statistical Model of Mor-
phology and Context. In Proceedings of ACL, pages 277?
284.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Using
Maximum Entropy Aided by a Dictionary. In Proceedings
of EMNLP, pages 91?99.
K. Uchimoto, C. Nobata, A. Yamada, S. Sekine, and H. Isahara.
2002. Morphological Analysis of The Spontaneous Speech
Corpus. In Proceedings of COLING, pages 1298?1302.
Automatic error detection in the Japanese learners? English spoken data
Emi IZUMI?? 
emi@crl.go.jp 
Kiyotaka UCHIMOTO? 
uchimoto@crl.go.jp 
Toyomi SAIGA? 
hoshi@karl.tis.co.jp
Thepchai Supnithi* 
thepchai@nectec.or.th
Hitoshi ISAHARA?? 
isahara@crl.go.jp 
Abstract 
This paper describes a method of 
detecting grammatical and lexical errors 
made by Japanese learners of English 
and other techniques that improve the 
accuracy of error detection with a limited 
amount of training data. In this paper, we 
demonstrate to what extent the proposed 
methods hold promise by conducting 
experiments using our learner corpus, 
which contains information on learners? 
errors. 
1 Introduction 
One of the most important things in keeping up 
with our current information-driven society is the 
acquisition of foreign languages, especially 
English for international communications. In 
developing a computer-assisted language teaching 
and learning environment, we have compiled a 
large-scale speech corpus of Japanese learner 
English, which provides a great deal of useful 
information on the construction of a model for the 
developmental stages of Japanese learners? 
speaking abilities.  
In the support system for language learning, 
we have assumed that learners must be informed 
of what kind of errors they have made, and in 
which part of their utterances. To do this, we need 
to have a framework that will allow us to detect 
learners? errors automatically.  
In this paper, we introduce a method of detect-
ing learners? errors, and we examine to what ex-
tent this could be accomplished using our learner 
corpus data including error tags that are labeled 
with the learners? errors.  
2 SST Corpus 
The corpus data was based entirely on audio-
recorded data extracted from an interview test, the 
?Standard Speaking Test (SST)?. The SST is a 
face-to-face interview between an examiner and 
the test-taker. In most cases, the examiner is a 
native speaker of Japanese who is officially 
certified to be an SST examiner. All the 
interviews are audio-recorded, and judged by two 
or three raters based on an SST evaluation scheme 
(SST levels 1 to 9). We recorded 300 hours of 
data, totaling one million words, and transcribed 
this. 
2.1 Error tags 
We designed an original error tagset for 
learners? grammatical and lexical errors, which 
were relatively easy to categorize. Our error tags 
contained three pieces of information, i.e., the part 
of speech, the grammatical/lexical system and the 
corrected form. We prepared special tags for some 
errors that cannot be categorized into any word 
class, such as the misordering of words. Our error 
tagset currently consists of 45 tags. The following 
example is a sentence with an error tag. 
*I lived in <at 
crr="">the</at> New Jersey. 
at indicates that it is an article error, and 
crr=?? means that the corrected form does not 
?Computational Linguistics Group, Communications Research Laboratory, 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 
?Graduate School of Science and Technology, Kobe University, 1-1 Rokkodai, Nada-ku, Kobe, Japan 
?TIS Inc., 9-1 Toyotsu, Suita, Osaka, Japan 
*National Electronics and Computer Technology Center, 
112 Pahonyothin Road, Klong 1, Klong Luang, Pathumthani, 12120, Thailand 
need an article. By referring to information on the 
corrected form indicated in an error tag, the sys-
tem can convert erroneous parts into corrected 
equivalents. 
3 Error detection method 
In this section, we would like to describe how 
we proceeded with error detection in the learner 
corpus. 
3.1 Types of errors 
We first divided errors into two groups de-
pending on how their surface structures were dif-
ferent from those of the correct ones. The first was 
an ?omission?-type error, where the necessary 
word was missing, and an error tag was inserted to 
interpolate it. The second was a ?replacement?-
type error, where the erroneous word was en-
closed in an error tag to be replaced by the cor-
rected version. We applied different methods to 
detecting these two kinds of errors. 
3.2 Detection of omission-type errors 
Omission-type errors were detected by estimat-
ing whether or not a necessary word string was 
missing in front of each word, including delimit-
ers. We also estimated to which category the error 
belonged during this process. What we call ?error 
categories? here means the 45 error categories that 
are defined in our error tagset. (e.g. article and 
tense errors) These are different from ?error 
types? (omission or replacement). As we can see 
from Fig. 1, when more than one error category is 
given, we have two ways of choosing the best one. 
Method A allows us to estimate whether there is a 
missing word or not for each error category. This 
can be considered the same as deciding which of 
the two labels (E: ?There is a missing word.? or C: 
?There is no missing word.?) should be inserted in 
front of each word. Here, there is an article miss-
ing in front of ?telephone?, so this can be consid-
ered an omission-type error, which is categorized 
as an article error (?at? is a label that indicates that 
this is an article error.). In Method B, if N error 
categories come up, we need to choose the most 
appropriate error category ?k? from among N+1 
categories, which means we have added one more 
category (+1) of ?There is no missing word.? (la-
beled with ?C?) to the N error categories. This can 
be considered the same as putting one of the N+1 
labels in front of each word. If there is more than 
one error tag inserted at the same location, they 
are combined to form a new error tag. 
As we can see from Fig. 2, we referred to 23 
pieces of information to estimate the error cate-
gory: two preceding and following words, their 
word classes, their root forms, three combinations 
of these (one preceding word and one following 
word/two preceding words and one following 
word/one preceding word and two following 
words), and the first and last letter of the word 
immediately following. (In Fig. 2, ?t? and ?e? in 
?telephone?.) The word classes and root forms 
were acquired with ?TreeTagger?. (Shmid 1994) 
3.3 Detection of replacement-type errors 
Replacement-type errors were detected by es-
timating whether or not each word should be de-
leted or replaced with another word string. The 
error category was also estimated during this 
process. As we did in detecting omission-type er-
rors, if more than one error category was given, 
we use two methods of detection. Method C was 
used to estimate whether or not the word should 
be replaced with another word for each error cate-
gory, and if it was to be replaced, the model esti-
mated whether the word was located at the 
beginning, middle or end of the erroneous part. As 
we can see from Fig. 3, this can be considered the Figure 2. Features used for detecting omission-type errors 
Word   POS    Root form 
there     EX       there 
is      VBZ       be 
telephone     NN       telephone 
and      CC       and 
the  DT       the 
books NNS       books 
.  SENT       . t e
?:feature combination      :single feature 
?Erroneous 
part
Figure 1. Detection of omission-type errors when 
there are more than one (N) error categories. 
Method A 
* there is telephone and the books . 
 
E: There is a missing word 
C: There is no missing word (=correct) 
Mehod B 
* there is telephone and the books . 
 
Ek: There is a missing word and the related error 
category is k (1?k?N) 
C: There is no missing word (=correct) 
? 
C 
? 
C 
? 
Ek 
? 
C 
? 
C 
? 
C 
? 
C 
? 
C 
? 
C 
? 
E 
? 
C 
? 
C 
? 
C 
? 
C 
same as deciding which of the three labels (Eb: 
?The word is at the beginning of the erroneous 
part.?, Ee: ?The word is in the middle or end.? or 
C: ?The word is correct.?) must be applied to each 
word. Method D was used if N error categories 
came up and we chose an appropriate one for the 
word from among 2N+1 categories. ?2N+1 cate-
gories? means that we divided N categories into 
two groups, i.e., where the word was at the begin-
ning of the erroneous part and where the word was 
not at the beginning, and we added one more 
where the word neither needed to be deleted nor 
replaced. This can be considered the same as at-
taching one of the 2N+1 labels to each word. To 
do this, we applied Ramshaw?s IOB scheme 
(Lance 1995). If there was more than one error tag 
attached to the same word, we only referred to the 
tag that covered the highest number of words. 
As Fig. 4 reveals, 32 pieces of information are 
referenced to estimate an error category, i.e., the 
targeted word and the two preceding and follow-
ing words, their word classes, their root forms, 
five combinations of these (the targeted word, the 
one preceding and one following/ the targeted 
word and the one preceding/ the targeted word 
and the one following/ the targeted word and the 
two preceding/ the targeted word and the two fol-
lowing), and the first and last letters of the word. 
3.4 Use of machine learning model 
The Maximum Entropy (ME) model (Jaynes 
1957) is a general technique that is used to esti-
mate the probability distributions of data. The 
over-riding principle in ME is that when nothing 
is known, the distribution should be as uniform as 
possible, i.e., maximum entropy. We calculated 
the distribution of probabilities p(a,b) with this 
method when Eq. 1 was satisfied and Eq. 2 was 
maximized. We then selected the category with 
maximum probability, as calculated from this dis-
tribution of probabilities, to be the correct cate-
gory. 
 
(2)   )),(log(),(             )(             
)1(                                          
(1)          ),(),(~       ),(),(
,
  
, ,
?
? ?
??
?? ??
?=
???
=
BbAa
j
BbAa BbAa
jj
bapbappH
kjffor
bagbapbagbap  
We assumed that the constraint of feature sets 
fi (i?j?k) was defined by Eq. 1. This is where A 
is a set of categories and B is a set of contexts,  
and gj(a,b) is a binary function that returns value 1 
when feature fj exists in context b and the category 
is a. Otherwise, gj(a,b) returns value 0. p~ (a,b) is 
the occurrence rate of the pair (a,b) in the training 
data. 
4 Experiment 
4.1 Targeted error categories 
We selected 13 error categories for detection.  
Table 1. Error categories to be detected 
Noun Number error, Lexical error 
Verb Erroneous subject-verb agreement, Tense error, 
Compliment error 
Adjective Lexical error 
Adverb Lexical error 
Preposition Lexical error on normal and dependent preposition 
Article Lexical error 
Pronoun Lexical error 
Others Collocation error 
 
Figure 4. The features used for detecting replace-
ment-type errors 
?:feature combination      :single feature 
Word     POS         Root form 
there     EX         there 
is      VBZ         be 
telephone     NN         telephone 
and      CC         and 
the      DT         the 
books     NNS         book 
on      IN         on 
the      DT         the 
desk      NN         NN 
.      SENT         . 
t e
?Erroneous
part 
Figure 3. Detection of replacement-type errors 
when there are more than one (N) error categories.
Method C 
* there is telephone and the books on the desk. 
 
 
Eb: The word in the beginning of the part which 
should be replaced. 
Ee: The word in the middle or the end of the part 
which should be replaced. 
C: no need to be replaced (=correct) 
Mehod D 
* there is telephone and the books on the desk. 
 
 
Ebk: The word in the beginning of the part which 
should be replaced and which error category is k. 
Eek: The word in the middle or the end of the part 
which should be replaced and which error category 
is k. (1?k?N) 
C: no need to be replaced (=correct) 
? 
C 
? 
C 
? 
C 
?
Eb
? 
C 
? 
C 
? 
C 
? 
C 
?
C
? 
C 
? 
C 
? 
C 
? 
Ebk 
? 
C 
? 
C 
? 
C 
? 
C 
?
C
4.2 Experiment based on tagged data 
We obtained data from 56 learners? with error 
tags. We used 50 files (5599 sentences) as the 
training data, and 6 files (617 sentences) as the 
test data. 
We tried to detect each error category using the 
methods discussed in Sections 3.2 and 3.3. There 
were some error categories that could not be de-
tected because of the lack of training data, but we 
have obtained the following results for article er-
rors which occurred most frequently. 
Article errors 
Omission- Recall rate 8/71 * 100 = 32.39(%) 
type errors Precision rate 8/11 * 100 = 52.27(%) 
Replacement- Recall rate 0/43 * 100 =  9.30(%) 
type errors Precision rate 0/ 1 * 100 =  22.22(%) 
Results for 13 errors were as follows. 
All errors 
Omission- Recall rate 21/ 93 * 100 = 22.58(%) 
type errors Precision rate 21/ 38 * 100 = 55.26(%) 
Replacement- Recall rate 5/224 * 100 =  2.23(%) 
type errors Precision rate 5/ 56 * 100 =  8.93(%) 
We assumed that the results were inadequate 
because we did not have sufficient training data. 
To overcome this, we added the correct sentences 
to see how this would affect the results. 
4.3 Addition of corrected sentences 
As discussed in Section 2.1, our error tags pro-
vided a corrected form for each error. If the erro-
neous parts were replaced with the corrected 
forms indicated in the error tags one-by-one, ill-
formed sentences could be converted into cor-
rected equivalents. We did this with the 50 items 
of training data to extract the correct sentences 
and then added them to the training data. We also 
added the interviewers? utterances in the entire 
corpus data (totaling 1202 files, excluding 6 that 
were used as the test data) to the training data as 
correct sentences. We added a total of 104925 
correct new sentences. The results we obtained by 
detecting article errors with the new data were as 
follows. 
Article errors 
Omission- Recall rate 8/71 * 100 = 11.27(%) 
type errors Precision rate 8/11 * 100 = 72.73(%) 
Replacement- Recall rate 0/43 * 100 =  0.00(%) 
type errors Precision rate 0/ 1 * 100 =  0.00(%) 
We found that although the recall rate de-
creased, the precision rate went up through adding 
correct sentences to the training data. 
We then determined how we could improve 
the results by adding the artificially made errors to 
the training data. 
4.4 Addition of sentences with artificially 
made errors 
We did this only for article errors. We first ex-
amined what kind of errors had been made with 
articles and found that ?a?, ?an?, ?the? and the 
absence of articles were often confused. We made 
up pseudo-errors just by replacing the correctly 
used articles with one of the others. The results of 
detecting article errors using the new training data, 
including the new corrected sentences described 
in Section 4.2, and 7558 sentences that contained 
artificially made errors were as follows. 
Article errors 
Omission- Recall rate 24/71 * 100 = 33.80(%) 
type errors Precision rate 24/30 * 100 = 80.00(%) 
Replacement- Recall rate 2/43 * 100 =  4.65(%) 
type errors Precision rate 2/ 9 * 100 = 22.22(%) 
We obtained a better recall and precision rate 
for omission-type errors. 
There were no improvements for replacement-
type errors. Since some more detailed context 
might be necessary to decide whether ?a? or ?the? 
must be used, the features we used here might be 
insufficient. 
5 Conclusion 
In this paper, we explained how errors in 
learners? spoken data could be detected and in the 
experiment, using the corpus as it was, the recall 
rate was about 30% and the precision rate was 
about 50%. By adding corrected sentences and 
artificially made errors, the precision rate rose to 
80% while the recall rate remained the same.  
References 
Helmut  Schmid  Probabilistic  part-of-Speech 
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. pp. 44-49, 1994. 
Lance A. Ramshaw and Mitchell P. Marcus. Text 
chunking using transformation-based learning. In 
Proceedings of the Third ACL Workshop on Very 
Large Corpora, pp. 82-94, 1995. 
Jaynes, E. T. ?Information Theory and Statistical Me-
chanics? Physical Review, 106, pp. 620-630, 1957. 
The Unknown Word Problem: a Morphological Analysis of
Japanese Using Maximum Entropy Aided by a Dictionary
Kiyotaka Uchimotoy, Satoshi Sekinez and Hitoshi Isaharay
yCommunications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
[uchimoto, isahara]@crl.go.jp
zNew York University
715 Broadway, 7th oor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
In this paper we describe a morphological analy-
sis method based on a maximum entropy model.
This method uses a model that can not only
consult a dictionary with a large amount of lex-
ical information but can also identify unknown
words by learning certain characteristics. The
model has the potential to overcome the un-
known word problem.
1 Introduction
Morphological analysis is one of the basic tech-
niques used in Japanese sentence analysis. A
morpheme is a minimal grammatical unit, such
as a word or a sux, and morphological analysis
is the process segmenting a given sentence into
a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a part-
of-speech (POS) and an inection type. One of
the most important problems in morphological
analysis is that posed by unknown words, which
are words found in neither a dictionary nor a
training corpus, and there have been two sta-
tistical approaches to this problem. One is to
acquire unknown words from corpora and put
them into a dictionary (e.g., (Mori and Nagao,
1996)), and the other is to estimate a model
that can identify unknown words correctly (e.g.,
(Kashioka et al, 1997; Nagata, 1999)). We
would like to be able to make good use of both
approaches. If words acquired by the former
method could be added to a dictionary and a
model developed by the latter method could
consult the amended dictionary, then the model
could be the best statistical model which has
the potential to overcome the unknown word
problem. Mori and Nagao proposed a statisti-
cal model that can consult a dictionary (Mori
and Nagao, 1998). In their model the proba-
bility that a string of letters or characters is
a morpheme is augmented when the string is
found in a dictionary. The improvement of the
accuracy was slight, however, so we think that
it is dicult to eciently integrate the mecha-
nism for consulting a dictionary into an n-gram
model. In this paper we therefore describe a
morphological analysis method based on a max-
imum entropy (M.E.) model. This method uses
a model that can not only consult a dictionary
but can also identify unknown words by learn-
ing certain characteristics. To learn these char-
acteristics, we focused on such information as
whether or not a string is found in a dictio-
nary and what types of characters are used in a
string. The model estimates how likely a string
is to be a morpheme according to the informa-
tion on hand. When our method was used to
identify morpheme segments in sentences in the
Kyoto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem
of Japanese morphological analysis can be re-
duced to the problem of assigning one of two
tags to each string in a sentence. A string is
tagged with a 1 or a 0 to indicate whether or
not it is a morpheme. When a string is a mor-
pheme, a grammatical attribute is assigned to
it. The 1 tag is thus divided into the num-
ber, n, of grammatical attributes assigned to
morphemes, and the problem is to assign an at-
tribute (from 0 to n) to every string in a given
sentence. The (n+1) tags form the space of \fu-
tures" in the M.E. formulation of our problem
of morphological analysis. The M.E. model, as
well as other similar models, enables the com-
putation of P (f jh) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
A \history" in M.E. is all of the conditioning
data that enable us to make a decision in the
space of futures. In the problem of morphologi-
cal analysis, we can reformulate this in terms of
nding the probability of f associated with the
relationship at index t in the test corpus:
P (f jh
t
) = P (f jInformation derivable
from the test corpus
related to relationship t)
The computation of P (f jh) in any M.E. models
is dependent on a set of \features" which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h; f) =
8
>
<
>
:
1 : if has(h; x) = true;
x = \POS( 1)(Major) : verb;
00
& f = 1
0 : otherwise:
(1)
Here \has(h,x)" is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and the part-of-speech of
the adjacent morpheme.
Given a set of features and some training
data, the M.E. estimation process produces a
model in which every feature g
i
has an associ-
ated parameter 
i
. This enables us to compute
the conditional probability as follows (Berger et
al., 1996):
P (f jh) =
Q
i

g
i
(h;f)
i
Z

(h)
(2)
Z

(h) =
X
f
Y
i

g
i
(h;f)
i
: (3)
The M.E. estimation process guarantees that for
every feature g
i
, the expected value of g
i
accord-
ing to the M.E. model will equal the empirical
expectation of g
i
in the training corpus. In other
words,
X
h;f
~
P (h; f)  g
i
(h; f)
=
X
h
~
P (h) 
X
f
P
M:E:
(f jh)  g
i
(h; f): (4)
Here
~
P is an empirical probability and P
M:E:
is
the probability assigned by the model.
We dene part-of-speech and bunsetsu
boundaries as grammatical attributes. Here a
bunsetsu is a phrasal unit consisting of one or
more morphemes. When there are m types
of parts-of-speech, and the left-hand side of
each morpheme may or may not be a bunsetsu
boundary, the number, n, of grammatical at-
tributes assigned to morphemes is 2m.
1
We
propose a model which estimates the likelihood
that a given string is a morpheme and has the
grammatical attribute i(1  i  n). We call it
a morpheme model. This model is represented
by Eq. (2), in which f can be one of (n + 1)
tags from 0 to n.
A given sentence is divided into morphemes,
and a grammatical attribute is assigned to each
morpheme so as to maximize the sentence prob-
ability estimated by our morpheme model. Sen-
tence probability is dened as the product of the
probabilities estimated for a particular division
of morphemes in a sentence. We use the Viterbi
algorithm to nd the optimal set of morphemes
in a sentence and we use the method proposed
by Nagata (Nagata, 1994) to search for the N-
best sets.
3 Experiments and Discussion
3.1 Experimental Conditions
The part-of-speech categories that we used fol-
low those of JUMAN (Kurohashi and Nagao,
1999). There are 53 categories covering all pos-
sible combinations of major and minor cate-
gories as dened in JUMAN. The number of
grammatical attributes is 106 if we include the
detection of whether or not the left side of a
morpheme is a bunsetsu boundary. We do not
identify inection types probabilistically since
1
Not only morphemes but also bunsetsus can be iden-
tied by considering the information related to their bun-
setsu boundaries.
they can be almost perfectly identied by check-
ing the spelling of the current morpheme after
a part-of-speech has been assigned to it. There-
fore, f in Eq. (2) can be one of 107 tags from 0
to 106.
We used the Kyoto University text corpus
(Version 2) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper. For
training, we used 7,958 sentences from newspa-
per articles appearing from January 1 to Jan-
uary 8, 1995, and for testing, we used 1,246
sentences from articles appearing on January 9,
1995.
Given a sentence, for every string consisting
of ve or less characters and every string ap-
pearing in the JUMAN dictionary (Kurohashi
and Nagao, 1999), whether or not the string is
a morpheme was determined and then the gram-
matical attribute of each string determined to
be a morpheme was identied and assigned to
that string. The maximum length was set at ve
because morphemes consisting of six or more
characters are mostly compound words or words
consisting of katakana characters. The stipula-
tion that strings consisting of six or more char-
acters appear in the JUMAN dictionary was set
because long strings not present in the JUMAN
dictionary were rarely found to be morphemes
in our training corpus. Here we assume that
compound words that do not appear in the JU-
MAN dictionary can be divided into strings con-
sisting of ve or less characters because com-
pound words tend not to appear in dictionar-
ies, and in fact, compound words which con-
sist of six or more characters and do not ap-
pear in the dictionary were not found in our
training corpus. Katakana strings that are not
found in the JUMAN dictionary were assumed
to be included in the dictionary as an entry
having the part-of-speech \Unknown(Major),
Katakana(Minor)." An optimal set of mor-
phemes in a sentence is searched for by em-
ploying the Viterbi algorithm under the con-
dition that connectivity rules dened between
parts-of-speech in JUMAN must be met. The
assigned part-of-speech in the optimal set is
not always selected from the parts-of-speech at-
tached to entries in the JUMAN dictionary, but
may also be selected from the 53 categories of
the M.E. model. It is dicult to select an appro-
priate category from the 53 when there is little
training data, so we assume that every entry in
the JUMAN dictionary has all possible parts-of-
speech, and the part-of-speech assigned to each
morpheme is selected from those attached to the
entry corresponding to the morpheme string.
The features used in our experiments are
listed in Table 1. Each row in Table 1 contains a
feature type, feature values, and an experimen-
tal result that will be explained later. Each fea-
ture consists of a type and a value. The features
are basically some attributes of the morpheme
itself or those of the morpheme to the left of
it. We used the 31,717 features that were found
three or more times in the training corpus. The
notations \(0)" and \(-1)" used in the feature
type column in Table 1 respectively indicate a
target string and the morpheme on the left of
it.
The terms used in the table are the following:
String: Strings which appeared as a morpheme
ve or more times in the training corpus
Length: Length of a string
POS: Part-of-speech. \Major" and \Minor"
respectively indicate major and minor part-
of-speech categories as dened in JUMAN.
Inf: Inection type as dened in JUMAN
Dic: We use the JUMAN dictionary, which has
about 200,000 entries (Kurohashi and Na-
gao, 1999). \Major&Minor" indicates pos-
sible combinations between major and mi-
nor part-of-speech categories. When the
target string is in the dictionary, the part-
of-speech attached to the entry correspond-
ing to the string is used as a feature value.
If an entry has two or more parts-of-speech,
the part-of-speech which leads to the high-
est probability in a sentence estimated from
our model is selected as a feature value.
JUMAN has another type of dictionary,
which is called a phrase dictionary. Each
entry in the phrase dictionary consists of
one or more morphemes such as \? (to,
case marker), ? (wa, topic marker), ??
(ie, say)." JUMAN uses this dictionary to
detect morphemes which need a longer con-
text to be identied correctly. When the
target string corresponds to the string of
the left most morpheme in the phrase dic-
tionary in JUMAN, the part-of-speech at-
Table 1: Features.
Feature Accuracy without
number Feature type Feature value (Number of value) each feature set
Recall Precision F-measure
1 String(0) (4,331) 93.66% 93.81% 93.73
2 String(-1) (4,331) ( 2.14%) ( 1.28%) ( 1.71)
3 Dic(0)(Major) Verb, Verb&Phrase, Adj, Adj&Phrase, 94.64% 92.87% 93.75
: : : (28)
4 Dic(0)(Minor) Common noun, Common noun&Phrase, ( 1.16%) ( 2.22%) ( 1.69)
Topic marker, : : : (90)
5 Dic(0)(Major&Minor) Noun&Common noun,
Noun&Common noun&Phrase, : : : (103)
6 Length(0) 1, 2, 3, 4, 5, 6 or more (6) 95.52% 94.11% 94.81
7 Length(-1) 1, 2, 3, 4, 5, 6 or more (6) ( 0.28%) ( 0.98%) ( 0.63)
8 TOC(0)(Beginning) Kanji, Hiragana, Symbol, Number, 95.17% 93.89% 94.52
Katakana, Alphabet (6)
9 TOC(0)(End) Kanji, Hiragana, Symbol, Number, ( 0.63%) ( 1.20%) ( 0.92)
Katakana, Alphabet (6)
10 TOC(0)(Transition) Kanji!Hiragana, Number!Kanji,
Katakana!Kanji, : : : (30)
11 TOC(-1)(End) Kanji, Hiragana, Symbol, Number,
Katakana, Alphabet (6)
12 TOC(-1)(Transition) Kanji!Hiragana, Number!Kanji,
Katakana!Kanji, : : : (30)
13 POS(-1)(Major) Verb, Adj, Noun, Unknown, : : : (15) 95.60% 95.31% 95.45
14 POS(-1)(Minor) Common noun, Sahen noun, Numeral, ( 0.20%) (+0.22%) (+0.01)
: : : (45)
15 POS(-1)(Major&Minor) [nil], Noun&Common noun,
Noun&Common noun&Phrase, : : : (54)
16 Inf(-1)(Major) Vowel verb, : : : (33) 95.66% 95.00% 95.33
17 Inf(-1)(Minor) Stem, Basic form, Imperative form, : : : (60) ( 0.14%) ( 0.09%) ( 0.11)
18 BB(-1) [nil], [exist] (2) 95.82% 95.25% 95.53
19 BB(-1) & Noun&Common, noun&Bunsetsu boundary, (+0.02%) (+0.16%) (+0.09)
POS(-1)(Major&Minor) Noun&Common, noun&Within a bunsetsu,
: : : (106)
tached to the entry plus the information
that it is in the phrase dictionary (such as
\Verb&Phrase") is used as a feature value.
TOC: Types of characters used in a string.
\(Beginning)" and \(End)" respectively
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the \(Begin-
ning)" and \(End)" are the same charac-
ter. \TOC(0)(Transition)" represents the
transition from the leftmost character to
the rightmost one in a string. \TOC(-
1)(Transition)" represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
one in the target string. For example, when
the adjacent morpheme on the left is \?
? (sensei, teacher)" and the target string
is \? (ni, case marker)," the feature value
\Kanji!Hiragana" is selected.
BB: Indicates whether or not the left side of a
morpheme is a bunsetsu boundary.
3.2 Results and Discussion
Some results of the morphological analysis are
listed in Table 2. Recall is the percentage of
morphemes in the test corpus whose segmen-
tation and major POS tag are identied cor-
rectly. Precision is the percentage of all mor-
phemes identied by the system that are iden-
tied correctly. F represents the F-measure and
is dened by the following equation.
F  measure =
2Recall  Precision
Recall + Precision
Table 2 shows results obtained by using our
method, by using JUMAN, and by using JU-
MAN plus KNP (Kurohashi, 1998). We show
the result obtained using JUMAN plus KNP
because JUMAN alone assigns an \Unknown"
tag to katakana strings when they are not in
the dictionary. All katakana strings not found
Table 2: Results of Experiments (Segmentation and major POS tagging).
Recall Precision F-measure
Our method 95.80% (29,986/31,302) 95.09% (29,986/31,467) 95.44
JUMAN 95.25% (29,814/31,302) 94.90% (29,814/31,417) 95.07
JUMAN+KNP 98.49% (30,830/31,302) 98.13% (30,830/31,417) 98.31
in the dictionary are therefore evaluated as er-
rors. KNP improves on JUMAN by replacing
the \Unknown" tag with a \Noun" tag and dis-
ambiguating part-of-speech ambiguities which
arise during the process of parsing when there is
more than one JUMAN analysis with the same
score.
The accuracy in segmentation and major
POS tagging obtained with our method and
that obtained with JUMAN were about 3%
worse than that obtained with JUMAN plus
KNP. We think the main reason for this was
an insucient amount of training data and fea-
ture sets and the inconsistency of the corpus.
The number of sentences in the training cor-
pus was only about 8,000, and we did not use
as many combined features as were proposed in
Ref. (Uchimoto et al, 1999). We were unable to
use more training data or more feature sets be-
cause every string consisting of ve or less char-
acters in our training corpus was used to train
our model, so the amount of tokenized train-
ing data would have become too large and the
training would not have been completed on the
available machine if we had used more training
data or more feature sets. The inconsistency
of the corpus was due to the way the corpus
was made. The Kyoto University corpus was
made by manually correcting the output of JU-
MAN plus KNP, and it is dicult to manually
correct all of the inconsistencies in the output.
The use of JUMAN plus KNP thus has an ad-
vantage over the use of our method when we
evaluate a system's accuracy by using the Ky-
oto University corpus. For example, the num-
ber of morphemes whose rightmost character is
\?" was 153 in the test corpus, and they were
all the same as those in the output of JUMAN
plus KNP. There were three errors (about 2%)
in the output of our system. There were several
inconsistencies in the test corpus such as \??
(seisan, Noun), ? (sha, Sux)(producer)," and
\??? (shouhi-sha, Noun)(consumer)." They
should have been corrected in the corpus-
making process to \?? (seisan, Noun),? (sha,
Sux)(producer)," and \?? (shouhi, Noun),
? (sha, Sux)(consumer)." It is dicult for
our model to discriminate among these with-
out over-training when there are such incon-
sistencies in the corpus. Other similar incon-
sistencies were, for example, \??? (geijutsu-
ka, Noun)(artist)" and \?? (kougei, Noun),
? (ka, Sux)(craftsman)," \??? (keishi-cho,
Noun)(the Metropolitan Police Board)" and \?
? (kensatsu, Noun), ? (cho, Noun)(the Pub-
lic Prosecutor's Oce)," and \??? (genjitsu-
teki, Adjective)(realistic)" and \?? (risou,
Noun), ? (teki, Sux)(ideal).". If these had
been corrected consistently when making the
corpus, the accuracy obtained by our method
could have been better than that shown in Ta-
ble 2. A study on corpus revision should be un-
dertaken to resolve this issue. We believe it can
be resolved by using our trained model. There
is a high possibility that a morpheme lacks con-
sistency in the training corpus when its proba-
bility, re-estimated by our model, is low. Thus
a method which detects morphemes having a
low probability can identify those lacking con-
sistency in the training corpus. We intend to
try this in the future.
3.3 Features and Accuracy
In our model, dictionary information and cer-
tain characteristics of unknown words are re-
ected as features, as shown in Table 1.
\String" and \Dic" reect the dictionary in-
formation,
2
and \Length" and \TOC"(types
of characters) reect the characteristics of un-
known words. Therefore, our model can not
only consult a dictionary but can also detect un-
known words. Table 1 shows the results of an
2
\String" indicates strings that make up a morpheme
and were found ve or more times in the training corpus.
Using this information as features in our M.E. model
corresponds to consulting a dictionary constructed from
the training corpus.
93
93.5
94
94.5
95
95.5
96
96.5
97
0 1000 2000 3000 4000 5000 6000 7000 8000
F-
m
ea
su
re
Number of Sentences
"training"
"testing"
Figure 1: Relation between accuracy and the number of training sentences.
analysis without the complete feature set. Al-
most all of the feature sets improved accuracy.
The contribution of the dictionary information
was especially signicant.
There were cases, however, in which the use
of dictionary information led to a decrease in
the accuracy. For example, we found these er-
roneous segmentations:
\?? (umi, sea)?? (ni, case marker)???
? (kaketa, bet)????? (romanha, the Ro-
mantic school)?" and \??? (aranami, rag-
ing waves)?? (ni, case marker)??? (make,
lose)???? (naishin, one's inmost heart)
?? (to, case marker)?" (Underlined strings
were errors.) when the correct segmentations
were:
\?? (umi, sea)?? (ni, case marker)???
? (kaketa, bet)???? (roman, romance)??
(wa, topic marker)?" and \??? (aranami,
raging waves)?? (ni, case marker)?????
(makenai, not to lose)?? (kokoro, heart)??
(to, case marker)?" (\?" indicates a morpho-
logical boundary.).
These errors were caused by nonstandard en-
tries in the JUMAN dictionary. The dictio-
nary had not only the usual notation using kanji
characters, \????" and \??," but also the
uncommon notation using hiragana strings, \?
???" and \???". To prevent this type of
error, it is necessary to remove nonstandard en-
tries from the dictionary or to investigate the
frequency of such entries in large corpora and
to use it as a feature.
3.4 Accuracy and the Amount of
Training Data
The accuracies (F-measures) for the training
corpus and the test corpus are shown in Figure 1
plotted against the number of sentences used
for training. The learning curve shows that we
can expect improvement if we use more training
data.
3.5 Unknown Words and Accuracy
The strength of our method is that it can iden-
tify morphemes when they are unknown words
and can assign appropriate parts-of-speech to
them. For example, the nouns \?? (Souseki)"
and \?? (Rohan)" are not found in the JU-
Table 3: Accuracy for unknown words (Recall).
Segmentation and Segmentation and
major POS tagging minor POS tagging
For words not found in the dictionary
nor in our training corpus
Our method 69.90% (432/618) 27.51% (170/618)
JUMAN+KNP 79.29% (490/618) 20.55% (127/618)
For words not found in the dictionary
nor in our features
Our method 76.17% (719/944) 32.20% (304/944)
JUMAN+KNP 85.70% (809/944) 27.22% (257/944)
For words not found in the dictionary
Our method 82.40% (1,138/1,381) 49.24% (680/1,381)
JUMAN+KNP 89.79% (1,240/1,381) 38.60% (533/1,381)
MAN dictionary. JUMAN plus KNP analyzes
them simply as \? (Noun)? (Noun)" and \?
(Adverb)? (Noun)," whereas our system ana-
lyzes both of them correctly. Our system cor-
rectly identied them as names of people even
though they were not in the dictionary and did
not appear as features in our M.E. model. Since
these names, or proper nouns, are newly coined
and can be represented by a variety of expres-
sions, no proper nouns can be included in a dic-
tionary, nor can they appear in a training cor-
pus; this means that proper nouns could easily
be unknown words. We investigated the accu-
racy of our method in identifying morphemes
when they are unknown words, and the re-
sults are listed in Table 3. The rst row in
each section shows the recall for the morphemes
that were unknown words. The second row in
each section shows the percentage of morphemes
whose segmentation and \minor" POS tag were
identied correctly. The dierence between the
rst and second lines, the third and fourth lines,
and fth and sixth lines is the denition of un-
known words. Unknown words were dened re-
spectively as words not found in the dictionary
nor in our training corpus, as words not found
in the dictionary nor in our features, and as
words not found in the dictionary. Our accu-
racy, shown as the second rows in Table 3 was
more than 5% better than that of JUMAN plus
KNP for each denition. These results show
that our model can eciently learn the char-
acteristics of unknown words, especially those
of proper nouns such as the names of people,
organizations, and locations.
4 Related Work
Several methods based on statistical models
have been proposed for the morphological anal-
ysis of Japanese sentences. An F-measure of
about 96% was achieved by a method based
on a hidden Markov model (HMM) (Takeuchi
and Matsumoto, 1997) and by one based on
a variable-memory Markov model (Haruno and
Matsumoto, 1997; Kitauchi et al, 1999). Al-
though the accuracy obtained with these meth-
ods was better than that obtained with ours,
their accuracy cannot be compared directly
with that of our method because their part-
of-speech categories dier from ours. And an
advantage of our model is that it can handle
unknown words, whereas their models do not
handle unknown words well. In their models,
unknown words are divided into a combination
of a word consisting of one character and known
words. Haruno and Matsumoto (Haruno and
Matsumoto, 1997) achieved a recall of about
96% when using trigram or greater information,
but achieved a recall of only 94% when using bi-
gram information. This leads us to believe that
we could obtain better accuracy if we use tri-
gram or greater information. We plan to do so
in future work.
Two approaches have been used to deal with
unknown words: acquiring unknown words from
corpora and putting them into a dictionary
(e.g., (Mori and Nagao, 1996)) and develop-
ing a model that can identify unknown words
correctly (e.g., (Kashioka et al, 1997; Nagata,
1999)). Nagata reported a recall of about 40%
for unknown words (Nagata, 1999). As shown
in Table 3, our method achieved a recall of
69.90% for unknown words. Our accuracy was
about 30% better than his. It is dicult to
compare his method with ours directly because
he used a dierent corpus (the EDR corpus),
but the part-of-speech categories and the def-
inition of morphemes he used were similar to
ours. Thus, this comparison is helpful in evalu-
ating our method. There are no spaces between
morphemes in Japanese. In general, therefore,
detecting whether a given string is an unknown
word or is not a morpheme is dicult when it
is not found in the dictionary, nor in the train-
ing corpus. However, our model learns whether
or not a given string is a morpheme and has a
huge amount of data for learning what in a cor-
pus is not a morpheme. Therefore, we believe
that the characteristics of our model led to its
good results for identifying unknown words.
Mori and Nagao proposed a model that can
consult a dictionary (Mori and Nagao, 1998);
they reported an F-measure of about 92 when
using the EDR corpus and of about 95 when
using the Kyoto University corpus. Their slight
improvement in accuracy by using dictionary in-
formation resulted in an F-measure of about 0.2,
while our improvement was about 1.7. Their
accuracy of 95% when using the Kyoto Univer-
sity corpus is similar to ours, but they added
to their dictionary all of the words appearing
in the training corpus. Therefore, their exper-
iment had to deal with fewer unknown words
than ours did.
With regard to the morphological analy-
sis of English sentences, methods for part-of-
speech tagging based on an HMM (Cutting et
al., 1992), a variable-memory Markov model
(Schutze and Singer, 1994), a decision tree
model (Daelemans et al, 1996), an M.E. model
(Ratnaparkhi, 1996), a neural network model
(Schmid, 1994), and a transformation-based
error-driven learning model (Brill, 1995) have
been proposed, as well as a combined method
(Marquez and Padro, 1997; van Halteren et al,
1998). On available machines, however, these
models cannot handle a large amount of lex-
ical information. We think that our model,
which can not only consult a dictionary with
a large amount of lexical information, but can
also identify unknown words by learning cer-
tain characteristics, has the potential to achieve
good accuracy for part-of-speech tagging in En-
glish. We plan to apply our model to English
sentences.
5 Conclusion
This paper described a method for morpho-
logical analysis based on a maximum entropy
(M.E.) model. This method uses a model
that can not only consult a dictionary but can
also identify unknown words by learning cer-
tain characteristics. To learn these characteris-
tics, we focused on such information as whether
or not a string is found in a dictionary and
what types of characters are used in a string.
The model estimates how likely a string is to
be a morpheme according to the information
on hand. When our method was used to iden-
tify morpheme segments in sentences in the Ky-
oto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%. In our experiments without each fea-
ture set shown in Tables 1, we found that dic-
tionary information signicantly contributes to
improving accuracy. We also found that our
model can eciently learn the characteristics of
unknown words, especially proper nouns such
as the names of people, organizations, and lo-
cations.
References
Adam L. Berger, Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A Max-
imum Entropy Approach to Natural Lan-
guage Processing. Computational Linguis-
tics, 22(1):39{71.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Pro-
cessing: A Case Study in Part-of-Speech Tag-
ging. Computational Linguistics, 21(4):543{
565.
Doung Cutting, Julian Kupiec, Jan Peder-
sen, and Penelope Sibun. 1992. A Practical
Part-of-Speech Tagger. In Proceedings of the
Third Conference on Applied Natural Lan-
guage Processing, pages 133{140.
Walter Daelemans, Jakub Zavrel, Peter Berck,
and Steven Gills. 1996. MBT: A Memory-
Based Part-of-Speech Tagger-Generator. In
Proceedings of the 4th Workshop on Very
Large Corpora, pages 1{14.
Masahiko Haruno and Yuji Matsumoto. 1997.
Mistake-Driven Mixture of Hierarchical-Tag
Context Trees. In Proceedings of the 35th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 230{237.
Hideki Kashioka, Stephen G. Eubank, and
Ezra W. Black. 1997. Decision-Tree Mor-
phological Analysis without a Dictionary for
Japanese. In Proceedings of the Natural Lan-
guage Processing Pacic Rim Symposium,
pages 541{544.
Akira Kitauchi, Takehito Utsuro, and Yuji Mat-
sumoto. 1999. Probabilistic Model Learn-
ing for Japanese Morphological Analysis by
Error-driven Feature Selection. Transactions
of Information Processing Society of Japan,
40(5):2325{2337. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 1997.
Building a Japanese Parsed Corpus while Im-
proving the Parsing System. In Proceedings
of the Natural Language Processing Pacic
Rim Symposium, pages 451{456.
Sadao Kurohashi and Makoto Nagao, 1999.
Japanese Morphological Analysis System JU-
MAN Version 3.61. Department of Informat-
ics, Kyoto University.
Sadao Kurohashi, 1998. Japanese Depen-
dency/Case Structure Analyzer KNP Ver-
sion 2.0b6. Department of Informatics, Ky-
oto University.
Llu

is Marquez and Llu

is Padro. 1997. A Flexi-
ble POS Tagger Using an Automatically Ac-
quired Language Model. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
238{252.
Shinsuke Mori and Makoto Nagao. 1996.
Word Extraction from Corpora and Its Part-
of-Speech Estimation Using Distributional
Analysis. In Proceedings of the 16th Interna-
tional Conference on Computational Linguis-
tics (COLING96), pages 1119{1122.
Shinsuke Mori and Makoto Nagao. 1998. An
Improvement of a Morphological Analysis by
a Morpheme Clustering. Journal of Nat-
ural Language Processing, 5(2):75{103. (in
Japanese).
Masaaki Nagata. 1994. A Stochastic Japanese
Morphological Analyzer Using a Forward-DP
Backward-A

N-Best Search Algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics (COL-
ING94), pages 201{207.
Masaaki Nagata. 1999. A Part of Speech Esti-
mation Method for Japanese UnknownWords
using a Statistical Model of Morphology and
Context. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 277{284.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Model for Part-Of-Speech Tagging. In
Conference on Empirical Methods in Natural
Language Processing, pages 133{142.
Helmut Schmid. 1994. Part-Of-Speech Tagging
with Neural Networks. In Proceedings of the
15th International Conference on Computa-
tional Linguistics (COLING94), pages 172{
176.
Hinrich Schutze and Yoram Singer. 1994. Part-
of-Speech Tagging Using a Variable Memory
Markov Model. In Proceedings of the 32nd
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 181{187.
Koichi Takeuchi and Yuji Matsumoto. 1997.
HMM Parameter Learning for Japanese
Morphological Analyzer. Transactions of
Information Processing Society of Japan,
83(3):500{509. (in Japanese).
Kiyotaka Uchimoto, Satoshi Sekine, and Hi-
toshi Isahara. 1999. Japanese Dependency
Structure Analysis Based on Maximum En-
tropy Models. In Proceedings of the Ninth
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL'99), pages 196{203.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemans. 1998. Improving Data Driven
Wordclass Tagging by System Combination.
In Proceedings of the COLING-ACL '98,
pages 491{497.
 
	ffCombining Outputs of Multiple Japanese Named Entity Chunkers
by Stacking
Takehito Utsuro
Department of Information
and Computer Sciences,
Toyohashi University of Technology
Tenpaku-cho, Toyohashi 441-8580, Japan
utsuro@ics.tut.ac.jp
Manabu Sassano
Fujitsu Laboratories, Ltd.
4-4-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
sassano@jp.fujitsu.com
Kiyotaka Uchimoto
Keihanna Human Info-Communications Research Center,
Communications Research Laboratory
Hikaridai Seika-cho, Kyoto 619-0289, Japan
uchimoto@crl.go.jp
Abstract
In this paper, we propose a method for
learning a classifier which combines out-
puts of more than one Japanese named
entity extractors. The proposed combi-
nation method belongs to the family of
stacked generalizers, which is in principle
a technique of combining outputs of sev-
eral classifiers at the first stage by learn-
ing a second stage classifier to combine
those outputs at the first stage. Individ-
ual models to be combined are based on
maximum entropy models, one of which
always considers surrounding contexts of
a fixed length, while the other consid-
ers those of variable lengths according to
the number of constituent morphemes of
named entities. As an algorithm for learn-
ing the second stage classifier, we employ
a decision list learning method. Experi-
mental evaluation shows that the proposed
method achieves improvement over the
best known results with Japanese named
entity extractors based on maximum en-
tropy models.
1 Introduction
In the recent corpus-based NLP research, sys-
tem combination techniques have been successfully
applied to several tasks such as parts-of-speech
tagging (van Halteren et al, 1998), base noun
phrase chunking (Tjong Kim Sang, 2000), and pars-
ing (Henderson and Brill, 1999; Henderson and
Brill, 2000). The aim of system combination is to
combine portions of the individual systems? outputs
which are partial but can be regarded as highly ac-
curate. The process of system combination can be
decomposed into the following two sub-processes:
1. Collect systems which behave as differently as
possible: it would help a lot if at least the col-
lected systems tend to make errors of differ-
ent types, because simple voting technique can
identify correct outputs.
Previously studied techniques for collecting
such systems include: i) using several exist-
ing real systems (van Halteren et al, 1998;
Brill and Wu, 1998; Henderson and Brill, 1999;
Tjong Kim Sang, 2000), ii) bagging/boosting
techniques (Henderson and Brill, 1999; Hen-
derson and Brill, 2000), and iii) switching the
data expression and obtaining several mod-
els (Tjong Kim Sang, 2000).
2. Combine the outputs of the several systems:
previously studied techniques include: i) vot-
ing techniques (van Halteren et al, 1998;
Tjong Kim Sang, 2000; Henderson and Brill,
1999; Henderson and Brill, 2000), ii) switch-
ing among several systems according to con-
fidence values they provide (Henderson and
Brill, 1999), iii) stacking techniques (Wolpert,
1992) which train a second stage classifier for
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 281-288.
                         Proceedings of the Conference on Empirical Methods in Natural
combining outputs of classifiers at the first
stage (van Halteren et al, 1998; Brill and Wu,
1998; Tjong Kim Sang, 2000).
In this paper, we propose a method for combining
outputs of (Japanese) named entity chunkers, which
belongs to the family of stacking techniques. In
the sub-process 1, we focus on models which dif-
fer in the lengths of preceding/subsequent contexts
to be incorporated in the models. As the base model
for supervised learning of Japanese named entity
chunking, we employ a model based on the maxi-
mum entropy model (Uchimoto et al, 2000), which
performed the best in IREX (Information Retrieval
and Extraction Exercise) Workshop (IREX Commit-
tee, 1999) among those based on machine learning
techniques. Uchimoto et al (2000) reported that the
optimal number of preceding/subsequent contexts to
be incorporated in the model is two morphemes to
both left and right from the current position. In this
paper, we train several maximum entropy models
which differ in the lengths of preceding/subsequent
contexts, and then combine their outputs.
As the sub-process 2, we propose to apply a stack-
ing technique which learns a classifier for com-
bining outputs of several named entity chunkers.
This second stage classifier learns rules for accept-
ing/rejecting outputs of several individual named en-
tity chunkers. The proposed method can be applied
to the cases where the number of constituent systems
is quite small (e.g., two). Actually, in the experimen-
tal evaluation, we show that the results of combining
the best performing model of Uchimoto et al (2000)
with the one which performs poorly but extracts
named entities quite different from those of the
best performing model can help improve the perfor-
mance of the best model.
2 Named Entity Chunking based on
Maximum Entropy Models
2.1 Task of the IREX Workshop
The task of named entity recognition of the IREX
workshop is to recognize eight named entity types
in Table 1 (IREX Committee, 1999). The organizer
of the IREX workshop provided 1,174 newspaper
articles which include 18,677 named entities as the
training data. In the formal run (general domain)
Table 1: Statistics of NE Types of IREX
frequency (%)
NE Type Training Test
ORGANIZATION 3676 (19.7) 361 (23.9)
PERSON 3840 (20.6) 338 (22.4)
LOCATION 5463 (29.2) 413 (27.4)
ARTIFACT 747 (4.0) 48 (3.2)
DATE 3567 (19.1) 260 (17.2)
TIME 502 (2.7) 54 (3.5)
MONEY 390 (2.1) 15 (1.0)
PERCENT 492 (2.6) 21 (1.4)
Total 18677 1510
of the workshop, the participating systems were re-
quested to recognize 1,510 named entities included
in the held-out 71 newspaper articles.
2.2 Named Entity Chunking
We first provide our definition of the task of
Japanese named entity chunking (Sekine et al,
1998; Borthwick et al, 1998; Uchimoto et al,
2000). Suppose that a sequence of morphemes is
given as below:
(
Left
Context ) (Named Entity) (
Right
Context )
? ? ?M
L
?k
? ? ?M
L
?1
M
NE
1
? ? ?M
NE
i
? ? ?M
NE
m
M
R
1
? ? ?M
R
l
? ? ?
?
(Current Position)
Given that the current position is at the morpheme
MNE
i
, the task of named entity chunking is to assign
a chunking state (to be described in section 2.3.1) to
the morpheme MNE
i
at the current position, consid-
ering the patterns of surrounding morphemes. Note
that in the supervised learning phase, we can use the
chunking information on which morphemes consti-
tute a named entity, and which morphemes are in the
left/right contexts of the named entity.
2.3 The Maximum Entropy Model
In the maximum entropy model (Della Pietra et al,
1997), the conditional probability of the output y
given the context x can be estimated as the follow-
ing p
?
(y | x) of the form of the exponential family,
where binary-valued indicator functions called fea-
ture functions f
i
(x, y) are introduced for expressing
a set of ?features?, or ?attributes? of the context x
and the output y. A parameter ?
i
is introduced for
each feature f
i
, and is estimated from a training data.
p
?
(y | x) =
exp
(
?
i
?
i
f
i
(x, y)
)
?
y
exp
(
?
i
?
i
f
i
(x, y)
)
Uchimoto et al (2000) defines the context x as the
patterns of surrounding morphemes as well as that at
the current position, and the output y as the named
entity chunking state to be assigned to the mor-
pheme at the current position.
2.3.1 Named Entity Chunking States
Uchimoto et al (2000) classifies classes of
named entity chunking states into the following 40
tags:
? Each of eight named entity types plus an ?OP-
TIONAL? type are divided into four chunking
states, namely, the beginning/middle/end of an
named entity, or an named entity consisting of
a single morpheme. This amounts to 9?4 = 36
classes.
? Three more classes are distinguished for mor-
phemes immediately preceding/following a
named entity, as well as the one between two
named entities.
? Other morphemes are assigned the class
?OTHER?.
2.3.2 Features
Following Uchimoto et al (2000), feature func-
tions for morphemes at the current position as well
as the surrounding contexts are defined. More
specifically, the following three types of feature
functions are used: 1
1. 2052 lexical items that are observed five times
or more within two morphemes from named
entities in the training corpus.
2. parts-of-speech tags of morphemes2.
3. character types of morphemes (i.e., Japanese
(hiragana or katakana), Chinese (kanji), num-
bers, English alphabets, symbols, and their
combinations).
As for the number of preceding/subsequent mor-
phemes as contextual clues, we consider the follow-
ing models:
1Minor modifications from those of Uchimoto et al (2000)
are: i) we used character types of morphemes because they are
known to be useful in the Japanese named entity chunking, and
ii) the sets of parts-of-speech tags are different.
2As a Japanese morphological analyzer, we used BREAK-
FAST (Sassano et al, 1997) with the set of about 300 part-of-
speech tags. BREAKFAST achieves 99.6% part-of-speech accu-
racy against newspaper articles.
5-gram model
This model considers the preceding two mor-
phemes M
?2
, M
?1
as well as the subsequent two
morphemes M
1
, M
2
as the contextual clue. Both in
(Uchimoto et al, 2000) and in this paper, this is the
model which performs the best among all the indi-
vidual models without system combination.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?2
M
?1
M
0
M
1
M
2
? ? ?
7-gram model
This model considers the preceding three mor-
phemes M
?3
, M
?2
, M
?1
as well as the subsequent
three morphemes M
1
, M
2
, M
3
as the contextual
clue.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?3
M
?2
M
?1
M
0
M
1
M
2
M
3
? ? ?
9-gram model
This model considers the preceding four mor-
phemes M
?4
, M
?3
, M
?2
, M
?1
as well as the subse-
quent four morphemes M
1
, M
2
, M
3
, M
4
as the con-
textual clue.
( LeftContext ) (
Current
Position ) (
Right
Context )
? ? ? M
?4
? ? ?M
?1
M
0
M
1
? ? ?M
4
? ? ?
For both 7-gram and 9-gram models, we consider
the following three modifications to those models:
? with all features
? with lexical items and parts-of-speech
tags (without the character types) of
M
{(?4),?3,3,(4)}
? with only the lexical items of M
{(?4),?3,3,(4)}
In our experiments, the number of features is
13,200 for 5-gram model and 15,071 for 9-gram
model. The number of feature functions is 31,344
for 5-gram model and 35,311 for 9-gram model.
Training a variable length (5?9-gram) model,
testing with 9-gram model
The major disadvantage of the 5/7/9-gram models
is that in the training phase it does not take into ac-
count whether or not the preceding/subsequent mor-
phemes constitute one named entity together with
the morpheme at the current position. Consider-
ing this disadvantage, we examine another model,
namely, variable length model, which incorporates
variable length contextual information. In the train-
ing phase, this model considers which of the preced-
ing/subsequent morphemes constitute one named
entity together with the morpheme at the current po-
sition (Sassano and Utsuro, 2000). It also considers
several morphemes in the left/right contexts of the
named entity. Here we restrict this model to explic-
itly considering the cases of named entities of the
length up to three morphemes and only implicitly
considering those longer than three morphemes. We
also restrict it to considering two morphemes in both
left and right contexts of the named entity.
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
? ? ?MNE
i
? ? ?MNE
m(?3)
MR
1
MR
2
? ? ?
?
(Current Position)
1. In the cases where the current named entity
consists of up to three morphemes, all the con-
stituent morphemes are regarded as within the
current named entity. The following is an ex-
ample of this case, where the current named
entity consists of three morphemes, and the
current position is at the middle of those con-
stituent morphemes as below:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MR
1
MR
2
? ? ?
? (1)
(Current Position)
2. In the cases where the current named entity
consists of more than three morphemes, only
the three constituent morphemes are regarded
as within the current named entity and the rest
are treated as if they were outside the named
entity. For example, suppose that the cur-
rent named entity consists of four morphemes:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MNE
4
MR
1
MR
2
? ? ?
?
(Current Position)
In this case, the fourth constituent morpheme
MNE
4
is treated as if it were in the right context
of the current named entity as below:
( LeftContext ) (Named Entity) (
Right
Context )
? ? ? ML
?2
ML
?1
MNE
1
MNE
2
MNE
3
MNE
4
MR
1
? ? ?
?
(Current Position)
In the testing phase, we apply this model consid-
ering the preceding four morphemes as well as the
subsequent four morphemes at every position, as in
the case of 9-gram model3.
We consider the following three modifications to
this model, where we suppose that the morpheme at
the current position be M
0
:
? with all features
? with lexical items and parts-of-speech tags
(without the character types) of M
{?4,?3,3,4}
? with only the lexical items of M
{?4,?3,3,4}
3 Learning to Combine Outputs of Named
Entity Chunkers
3.1 Data Sets
The following gives the training and test data sets
for our framework of learning to combine outputs of
named entity chunkers.
1. TrI : training data set for learning individual
named entity chunkers.
2. TrC: training data set for learning a classifier
for combining outputs of individual named en-
tity chunkers.
3. Ts: test data set for evaluating the classifier for
combining outputs of individual named entity
chunkers.
3.2 Procedure
The following gives the procedure for learning the
classifier to combine outputs of named entity chun-
kers using TrI and TrC.
1. Train the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) using TrI .
2. Apply the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) to TrC, respectively,
and obtain the list of chunked named entities
NEList
i
(TrC) for each named entity chun-
ker NEchk
i
.
3Note that, as opposed to the training phase, the length of
preceding/subsequent contexts is fixed in the testing phase of
this model. Although this discrepancy between training and
testing damages the performance of this single model (sec-
tion 4.1), it is more important to note that this model tends to
have distribution of correct/over-generated named entities dif-
ferent from that of the 5-gram model. In section 4, we exper-
imentally show that this difference is the key to improving the
named entity chunking performance by system combination.
Table 2: Examples of Event Expressions for Combining Outputs of Multiple Systems
Segment Morpheme(POS) NE Outputs ofIndividual Systems Event Expressions
System 0 System 1
.
.
.
SegEv
i
rainen
(?next year?,
temporal noun)
10gatsu
(?October?,
temporal noun)
rainen
(DATE)
10gatsu
(DATE)
rainen
-10gatsu
(DATE)
{
systems=?0?,mlength=1,
NEtag=DATE,
POS=?temporal noun?, class
NE
=?
}
{
systems=?0?,mlength=1,
NEtag=DATE,
POS=?temporal noun?, class
NE
=?
}
{
systems=?1?,mlength=2,
NEtag=DATE,
POS=?temporal noun, temporal noun?,
class
NE
=+
}
.
.
.
SegEv
i+1
seishoku
(?reproductive?, noun)
iryou
(?medical?, noun)
gijutsu
(?technology?, noun)
seishoku
-iryou
-gijutsu
(ARTIFACT)
{
systems=?0?, class
sys
=?no outputs?
}
{
systems=?1?,mlength=3,
NEtag=ARTIFACT,
POS=?noun,noun,noun?, class
NE
=?
}
nitsuite
(?about?, particle)
.
.
.
3. Align the lists NEList
i
(TrC) (i = 1, . . . , n)
of chunked named entities according to the po-
sitions of the chunked named entities in the text
TrC, and obtain the event expression TrCev
of TrC.
4. Train the classifier NEchk
cmb
for combining
outputs of individual named entity chunkers us-
ing the event expression TrCev.
The following gives the procedure for applying the
learned classifier to Ts.
1. Apply the individual named entity chunkers
NEchk
i
(i = 1, . . . , n) to Ts, respectively,
and obtain the list of chunked named entities
NEList
i
(Ts) for each named entity chunker
NEchk
i
.
2. Align the lists NEList
i
(Ts) (i=1, . . . , n) of
chunked named entities according to the posi-
tions of the chunked named entities in the text
Ts, and obtain the event expression Tsev of
Ts.
3. Apply NEchk
comb
to Tsev and evaluate its
performance.
3.3 Data Expressions
3.3.1 Events
The event expression TrCev of TrC is obtained
by aligning the lists NEList
i
(TrC) (i =1, . . . , n)
of chunked named entities, and is represented as a
sequence of segments, where each segment is a set
of aligned named entities. Chunked named enti-
ties are aligned under the constraint that those which
share at least one constituent morpheme have to be
aligned into the same segment. Examples of seg-
ments, into which named entities chunked by two
systems are aligned, are shown in Table 2. In the
first segment SegEv
i
, given the sequence of the two
morphemes, the system No.0 decided to extract two
named entities, while the system No.1 chunked the
two morphemes into one named entity. In those
event expressions, systems indicates the list of the
indices of the systems which output the named en-
tity, mlength gives the number of the constituent
morphemes, NEtag gives one of the nine named
entity types, POS gives the list of parts-of-speech
of the constituent morphemes, and class
NE
indi-
cates whether the named entity is a correct one com-
pared against the gold standard (?+?), or the one
over-generated by the systems (???).
In the second segment SegEv
i+1
, only the sys-
tem No.1 decided to extract a named entity from
the sequence of the three morphemes. In this case,
the event expression for the system No.0 is the one
which indicates that no named entity is extracted by
the system No.0.
In the training phase, each segment SegEv
j
of
event expression constitutes a minimal unit of an
event, from which features for learning the classi-
fier are extracted. In the testing phase, the classes
of each system?s outputs are predicted against each
segment SegEv
j
.
3.3.2 Features and Classes
In principle, features for learning the classifier for
combining outputs of named entity chunkers are rep-
resented as a set of pairs of the system indices list
?p, . . . , q? and a feature expression F of the named
entity:
f =
{
?systems=?p, . . . , q?, F ?
? ? ?
?systems=?p?, . . . , q??, F ??
}
(2)
In the training phase, any possible feature of this
form is extracted from each segment SegEv
j
of
event expression. The system indices list ?p, . . . , q?
indicates the list of the systems which output the
named entity. A feature expression F of the named
entity can be any possible subset of the full feature
expression {mlength= ? ? ? , NEtag= ? ? ? , POS =
? ? ?}, or the set indicating that the system outputs no
named entity within the segment.
F =
?
?
?
?
?
?
?
any subset of
{
mlength= ? ? ? ,
NEtag= ? ? ? , POS= ? ? ?
}
{
class
sys
=?no outputs?
}
In the training and testing phases, within each
segment SegEv
j
of event expression, a class is as-
signed to each system, where each class classi
sys
for
the i-th system is represented as a list of the classes
of the named entities output by the system:
classi
sys
=
{
+/?, . . . , +/?
?no output? (i = 1, . . . , n)
3.4 Learning Algorithm
We apply a simple decision list learning method
to the task of learning a classifier for combining
outputs of named entity chunkers4. A decision
list (Yarowsky, 1994) is a sorted list of decision
rules, each of which decides the value of class given
some features f of an event. Each decision rule in
a decision list is sorted in descending order with
respect to some preference value, and rules with
higher preference values are applied first when ap-
plying the decision list to some new test data. In
this paper, we simply sort the decision list according
to the conditional probability P (class
i
| f) of the
class
i
of the i-th system?s output given a feature f .
4 Experimental Evaluation
We experimentally evaluate the performance of the
proposed system combination method using the
IREX workshop?s training and test data.
4.1 Comparison of Outputs of Individual
Systems
First, Table 3 shows the performance of the indi-
vidual models described in the section 2.3.2, where
trained with the IREX workshop?s training data, and
tested against the IREX workshop?s test data as Ts.
The 5-gram model performs the best among those
individual models.
Next, assuming that each of the models other
than the 5-gram model is combined with the 5-gram
model, Table 4 compares the named entities of their
outputs. Recall rate of the correct named entities in
the union of their outputs, as well as the overlap rate5
of the over-generated entities against those included
in the output of the 5-gram model are shown.
From the Tables 3 and 4, it is clear that the 7-gram
and 9-gram models are quite similar to the 5-gram
model both in the performance and in the distribu-
tion of correct/over-generated named entities. On
the other hand, variable length models have distri-
bution of correct/over-generated named entities a lit-
4It is quite straightforward to apply any other supervised
learning algorithms to this task.
5For a model X , the overlap rate of the over-generated enti-
ties against those included in the output of the 5-gram model is
defined as: (# of the intersection of the over-generated entities
output by the 5-gram model and those output by the model X)/
(# of the over-generated entities output by the 5-gram model).
Table 3: Performance of Individual Models against
Ts (F-measure (? = 1) (%))
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 80.78 80.81 80.71
9-gram 80.13 80.53 80.53
variable length 45.12 77.02 75.16
5-gram 81.16
Table 4: Difference between 5-gram model and
Other Individual Models (Recall of the Union /
Overlap Rate of Over-generated Entities) (%)
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 79.8/85.2 79.8/85.2 79.7/91.2
9-gram 79.7/84.7 79.7/86.1 79.5/90.7
variable
length 82.6/27.3 81.4/63.4 80.4/72.7
tle different from that of the 5-gram model. Vari-
able length models have lower performance mainly
because of the difference between the training and
testing phases with respect to the modeling of con-
text lengths. Especially, the variable length model
with ?all? features of M
{?4,?3,3,4}
has much lower
performance as well as significantly different dis-
tribution of correct/over-generated named entities.
This is because character types features are so gen-
eral that many (erroneous) named entities are over-
generated, while sometimes they contribute to find-
ing named entities that are never detected by any of
the other models.
4.2 Results of Combining System Outputs
This section reports the results of combining the out-
put of the 5-gram model with that of 7-gram models,
9-gram models, and the variable length models. As
the training data sets TrI and TrC, we evaluate the
following two assignments (a) and (b), where D
CRL
denotes the IREX workshop?s training data:
(a) TrI: D
CRL
? D200
CRL
(200 articles from D
CRL
)
TrC: D200
CRL
(b) TrI = TrC = D
CRL
We use the IREX workshop?s test data for Ts.
In the assignment (a), TrI and TrC are disjoint,
while in the assignment (b), individual named entity
chunkers are applied to their own training data, i.e.,
closed data. The assignment (b) is for the sake of
avoiding data sparseness in learning the classifier for
combining outputs of two named entity chunkers.
Table 5 shows the peformance in F-measure (? =
1) for both assignments (a) and (b). For both (a) and
Table 5: Performance of Combining 5-gram model
and Other Individual Models (against Ts, F-measure
(? = 1) (%))
(a) TrI = D
CRL
? D200
CRL
, TrC = D200
CRL
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 81.54 81.53 80.60
9-gram 81.31 81.26 80.60
variable length 83.43 81.55 81.85
(b) TrI = TrC = D
CRL
Features for M
{(?4),?3,3,(4)}
All Lex+POS Lex
7-gram 81.97 81.83 81.58
9-gram 81.53 81.66 81.52
variable length 84.07 83.07 82.50
(b), ?5-gram + variable length (All)? significantly
outperforms the 5-gram model, which is the best
model among all the individual models without sys-
tem combination. It is remarkable that models which
perform poorly but extract named entities quite dif-
ferent from those of the best performing model can
actually help improve the best model by the pro-
posed method. The performance for the assignment
(b) is better than that for the assignment (a). This re-
sult claims that the training data size should be larger
when learning the classifier for combining outputs of
two named entity chunkers.
In the Table 6, for the best performing result (i.e.,
5-gram + variable length (All)) as well as the con-
stituent individual models (5-gram model and vari-
able length model (All)), we classify the system
output according to the number of constituent mor-
phemes of each named entity. In the Table 7, we
classify the system output according to the named
entity types. The following summarizes several re-
markable points of these results: i) the benefit of the
system combination is more in the improvement of
precision rather than in that of recall. This means
that the proposed system combination technique is
useful for detecting over-generation of named en-
tity chunkers, ii) the combined outputs of the 5-gram
model and the variable length model improve the re-
sults of chunking longer named entities quite well
compared with shorter named entities. This is the
effect of the variable length features of the variable
length model.
Table 6: Evaluation Results of Combining System Outputs, per # of constituent morphemes
(TrI = TrC = D
CRL
, F-measure (? = 1) / Recall / Precision (%))
n Morphemes to 1 Named Entity
n ? 1 n = 1 n = 2 n = 3 n ? 4
5-gram 81.16 83.60 86.94 68.42 50.59
78.87/83.60 84.97/82.28 85.90/88.00 63.64/73.98 35.83/86.00
variable length (All) 45.12 53.77 56.63 33.74 16.78
51.50/40.15 38.69/88.14 71.37/47.93 57.34/23.91 40.00/10.62
5-gram + variable length (All) 84.07 85.06 88.96 75.19 65.96
81.45/86.86 85.12/84.99 87.42/90.56 69.93/81.30 51.67/91.18
Table 7: Evaluation Results of Combining System Outputs, per NE type
(TrI = TrC = D
CRL
, F-measure (? = 1) (Recall, Precision) (%))
ORGANI- PER- LOCA- ARTI- DATE TIME MONEY PER-
ZATION SON TION FACT CENT
67.74 81.82 77.04 30.43 91.49 93.20 92.86 87.18
5-gram (58.45) (79.88) (71.91) (29.17) (88.85) (88.89) (86.67) (80.95)
(80.53) (83.85) (82.96) (31.82) (94.29) (97.96) (100.00) (94.44)
35.48 48.45 38.47 5.80 78.60 56.90 60.61 87.18
variable length (All) (37.40) (48.52) (32.93) (22.92) (81.92) (61.11) (66.67) (80.95)
(33.75) (48.38) (46.26) (3.32) (75.53) (53.23) (55.56) (94.44)
5-gram + 72.18 84.15 79.58 38.71 92.86 93.20 92.86 87.18
variable length (All) (62.88) (81.66) (73.61) (37.50) (90.00) (88.89) (86.67) (80.95)
(84.70) (86.79) (86.61) (40.00) (95.90) (97.96) (100.00) (94.44)
5 Conclusion
This paper proposed a method for learning a classi-
fier to combine outputs of more than one Japanese
named entity chunkers. Experimental evaluation
showed that the proposed method achieved improve-
ment in F-measure over the best known results with
an ME model (Uchimoto et al, 2000), when a com-
plementary model extracted named entities quite dif-
ferently from the best performing model.
References
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Proc.
6th Workshop on VLC, pages 152?160.
E. Brill and J. Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. 17th COLING
and 36th ACL, pages 191?195.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
J. C. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: Combining parsers. In
Proc. 1999 EMNLP and VLC, pages 187?194.
J. C. Henderson and E. Brill. 2000. Bagging and boost-
ing a treebank parser. In Proc. 1st NAACL, pages 34?
41.
IREX Committee, editor. 1999. Proceedings of the IREX
Workshop. (in Japanese).
M. Sassano and T. Utsuro. 2000. Named entity chunking
techniques in supervised learning for Japanese named
entity recognition. In Proceedings of the 18th COL-
ING, pages 705?711.
M. Sassano, Y. Saito, and K. Matsui. 1997. Japanese
morphological analyzer for NLP applications. In Proc.
3rd Annual Meeting of the Association for Natural
Language Processing, pages 441?444. (in Japanese).
S. Sekine, R. Grishman, and H. Shinnou. 1998. A deci-
sion tree method for finding and classifying names in
Japanese texts. In Proc. 6th Workshop on VLC, pages
148?152.
E. Tjong Kim Sang. 2000. Noun phrase recognition by
system combination. In Proc. 1st NAACL, pages 50?
55.
K. Uchimoto, Q. Ma, M. Murata, H. Ozaku, and H. Isa-
hara. 2000. Named entity extraction based on a maxi-
mum entropy model and transformation rules. In Proc.
38th ACL, pages 326?335.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proc. 17th COLING and 36th ACL, pages
491?497.
D. H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in Span-
ish and French. In Proc. 32nd ACL, pages 88?95.
