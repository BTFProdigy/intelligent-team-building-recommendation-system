Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908?1913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Fish transporters and miracle homes:
How compositional distributional semantics can help NP parsing
Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni
Center for Mind/Brain Sciences
University of Trento, Italy
first.last@unitn.it
Abstract
In this work, we argue that measures that have
been shown to quantify the degree of semantic
plausibility of phrases, as obtained from their
compositionally-derived distributional seman-
tic representations, can resolve syntactic am-
biguities. We exploit this idea to choose the
correct parsing of NPs (e.g., (live fish) trans-
porter rather than live (fish transporter)). We
show that our plausibility cues outperform
a strong baseline and significantly improve
performance when used in combination with
state-of-the-art features.
1 Introduction
Live fish transporter: A transporter of live fish
or rather a fish transporter that is not dead?
While our intuition, based on the meaning of this
phrase, prefers the former interpretation, the Stan-
ford parser, which lacks semantic features, incor-
rectly predicts the latter as the correct parse.1 The
correct syntactic parsing of sentences is clearly
steered by semantic information (as formal syn-
tacticians have pointed out at least since Fillmore
(1968)), and consequently the semantic plausibil-
ity of alternative parses can provide crucial evidence
about their validity.
An emerging line of parsing research capitalizes
on the advances of compositional distributional se-
mantics (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Socher et al,
2012). Information related to compositionally-
derived distributional representations of phrases is
1http://nlp.stanford.edu:8080/parser/
index.jsp
integrated at various stages of the parsing process
to improve overall performance.2 We are aware of
two very recent studies exploiting the semantic in-
formation provided by distributional models to re-
solve syntactic ambiguity: Socher et al (2013) and
Le et al (2013).
Socher et al (2013) present a recursive neural net-
work architecture which jointly learns semantic rep-
resentations and syntactic categories of phrases. By
annotating syntactic categories with their distribu-
tional representation, the method emulates lexical-
ized approaches (Collins, 2003) and captures sim-
ilarity more flexibly than solutions based on hard
clustering (Klein and Manning, 2003; Petrov et al,
2006). Thus, their approach mainly aims at improv-
ing parsing by capturing a richer, data-driven cate-
gorial structure.
On the other hand, Le et al (2013) work with the
output of the parser. Their hypothesis is that parses
that lead to less semantically plausible interpreta-
tions will be penalized by a reranker that looks at
the composed semantic representation of the parse.
Their method achieves an improvement of 0.2% in
F-score. However, as the authors also remark, be-
cause of their experimental setup, they cannot con-
clude that the improvement is truly due to the se-
mantic composition component, a crucial issue that
is deferred to further investigation.
This work aims at corroborating the hypothesis
that the semantic plausibility of a phrase can in-
deed determine its correct parsing. We develop a
system based on simple and intuitive measures, ex-
2Distributional representations approximate word and
phrase meaning by vectors that record the contexts in which
they are likely to appear in corpora; for a review see, e.g., Tur-
ney and Pantel (2010).
1908
Type of NP # Example
A (N N) 1296 local phone company
(A N) N 343 crude oil sector
N (N N) 164 miracle home run
(N N) N 424 blood pressure medicine
Total 2227 -
Table 1: NP dataset
tracted from the compositional distributional repre-
sentations of phrases, that have been shown to corre-
late with semantic plausibility (Vecchi et al, 2011).
We develop a controlled experimental setup, fo-
cusing on a single syntactic category, that is, noun
phrases (NP), where our task can be formalized as
(left or right) bracketing. Unlike previous work,
we compare our compositional semantic component
against features based on n-gram statistics, which
can arguably also capture some semantic informa-
tion in terms of frequent occurrences of meaningful
phrases. Inspired by previous literature demonstrat-
ing the power of metrics based on Pointwise Mu-
tual Information (PMI) in NP bracketing (Nakov and
Hearst, 2005; Pitler et al, 2010; Vadas and Curran,
2011), we test an approach exploiting PMI features,
and show that plausibility features relying on com-
posed representations can significantly boost accu-
racy over PMI.
2 Setup
Noun phrase dataset To construct our dataset,
we used the Penn TreeBank (Marcus et al, 1993),
which we enriched with the annotation provided by
Vadas and Curran (2007a), since the original tree-
bank does not distinguish different structures inside
the NPs and always marks them as right bracketed,
e.g., local (phone company) but also blood (pressure
medicine). We focus on NPs formed by three ele-
ments, where the first can be an adjective (A) or a
noun (N), the other two are nouns. Table 1 summa-
rizes the characteristics of the dataset.3
Distributional semantic space As our source cor-
pus we use the concatenation of ukWaC, the English
Wikipedia (2009 dump) and the BNC, with a total of
3The dataset is available from: http://clic.cimec.
unitn.it/composes
about 2.8 billion tokens.4 We collect co-occurrence
statistics for the top 8K Ns and 4K As, plus any
other word from our NP dataset that was below this
rank. Our context elements are composed of the top
10K content words (adjectives, adverbs, nouns and
verbs). We use a standard bag-of-words approach,
counting within-sentence collocates for every target
word. We apply (non-negative) Pointwise Mutual
Information as weighting scheme and dimensional-
ity reduction using Non-negative Matrix Factoriza-
tion, setting the number of reduced-space dimen-
sions to 300.5
Composition functions We experiment with vari-
ous composition functions, chosen among those sen-
sitive to internal structure (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010),
namely dilation (dil), weighted additive (wadd), lex-
ical function (lexfunc) and full additive (fulladd).6
For model implementation and (unsupervised) es-
timation, we rely on the freely available DISSECT
toolkit (Dinu et al, 2013).7 For all methods, vectors
were normalized before composing, both in training
and in generation. Table 2 presents a summary de-
scription of the composition methods we used.
Following previous literature (Mitchell and Lap-
ata, 2010), and the general intuition that adjectival
modification is quite a different process from noun
combination (Gagne? and Spalding, 2009; McNally,
2013), we learn different parameters for noun-noun
(NN) and adjective-noun (AN) phrases. As an ex-
ample of the learned parameters, for the wadd model
the ratio of parameters w1 and w2 is 1:2 for ANs,
whereas for NNs it is almost 1:1, confirming the in-
tuition that a non-head noun plays a stronger role in
composition than an adjective modifier.
4http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
5For tuning the parameters of the semantic space, we com-
puted the correlation of cosines produced with a variety of pa-
rameter settings (SVD/NMF/no reduction, PMI/Local MI/raw
counts/log transform, 150 to 300 dimensions in steps of 50) with
the word pair similarity ratings in the MEN dataset: http:
//clic.cimec.unitn.it/?elia.bruni/MEN
6We do not consider the popular multiplicative model, as it
produces identical representations for NPs irrespective of their
internal structure.
7http://clic.cimec.unitn.it/composes/
toolkit/
1909
Model Composition function Parameters
wadd w1~u+ w2~v w1, w2
dil ||~u||22~v + (?? 1)?~u,~v?~u ?
fulladd W1~u+W2~v W1,W2 ? Rm?m
lexfunc Au~v Au ? Rm?m
Table 2: Composition functions of inputs (u, v).
Recursive composition In this study we also ex-
periment with recursive composition; to the best
of our knowledge, this is the first time that these
composition functions have been explicitly used in
this manner. For example, given the left brack-
eted NP (blood pressure) medicine, we want to
obtain its compositional semantic representation,
???????????????????
blood pressure medicine. First, basic composition
is applied, in which
????
blood and ???????pressure are com-
bined with one of the composition functions. Fol-
lowing that, we apply recursive composition; the
output of basic composition, i.e.,
???????????
blood pressure,
is fed to the function again to be composed with the
representation of
???????
medicine.
The latter step is straightforward for all com-
position functions except lexfunc applied to left-
bracketed NPs, where the first step should return a
matrix representing the left constituent (blood pres-
sure in the running example). To cope with this nui-
sance, we apply the lexfunc method to basic compo-
sition only, while recursive representations are de-
rived by summing (e.g.,
???????????
blood pressure is obtained
by multiplying the blood matrix by the pressure vec-
tor, and it is then summed to
???????
medicine).
3 Experiments
Semantic plausibility measures We use mea-
sures of semantic plausibility computed on com-
posed semantic representations introduced by Vec-
chi et al (2011). The rationale is that the correct
(wrong) bracketing will lead to semantically more
(less) plausible phrases. Thus, a measure able to dis-
criminate semantically plausible from implausible
phrases should also indicate the most likely parse.
Considering, for example, the alternative parses of
miracle home run, we observe that home run is
a more semantically plausible phrase than miracle
home. Furthermore, we might often refer to a base-
ball player?s miracle home run, but we doubt that
even a miracle home can run! Given the com-
posed representation of an AN (or NN), Vecchi et
al. (2011) define the following measures:
? Density, quantified as the average cosine of a
phrase with its (top 10) nearest neighbors, cap-
tures the intuition that a deviant phrase should
be isolated in the semantic space.
? Cosine of phrase and head N aims to capture
the fact that the meaning of a deviant AN (or
NN) will tend to diverge from the meaning of
the head noun.
? Vector length should capture anomalous vec-
tors.
Since length, as already observed by Vecchi et al,
is strongly affected by independent factors such as
input vector normalization and the estimation pro-
cedure, we introduce entropy as a measure of vec-
tor quality. The intuition is that meaningless vec-
tors, whose dimensions contain mostly noise, should
have high entropy.
NP Parsing as Classification Parsing NPs con-
sisting of three elements can be treated as bi-
nary classification; given blood pressure medicine,
we predict whether it is left- ((blood pres-
sure) medicine) or right-bracketed (blood (pressure
medicine)).
We conduct experiments using an SVM with Ra-
dial Basis Function kernel as implemented in the
scikit-learn toolkit.8 Our dataset is split into 10 folds
in which the ratio between the two classes is kept
constant. We tune the SVM complexity parameter
C on the first fold and we report accuracy results on
the remaining nine folds after cross-validation.
Features Given a composition function f , we de-
fine the following feature sets, illustrated with the
usual blood pressure medicine example, which are
used to build different classifiers:
? fbasic consists of the semantic plausibility
measures described above computed for the
two-word phrases resulting from alternative
bracketings, i.e., 3 measures for each bracket-
ing, evaluated on blood pressure and pressure
medicine respectively, for a total of 6 features.
? frec contains 6 features computed on the vec-
tors resulting from the recursive compositions
8http://scikit-learn.org/
1910
Features Accuracy
right 65.6
pos 77.3
lexfuncbasic 74.6
lexfuncrec 74.0
lexfuncplausibility 76.2
waddbasic 75.9
waddrec 78.2
waddplausibility 78.7
pmi 81.2
pmi+lexfuncplausibility 82.9
pmi+waddplausibility 85.6
Table 3: Evaluation of feature sets from Section 3
(blood pressure) medicine and blood (pressure
medicine).
? fplausibility concatenates fbasic and frec.
? pmi contains the PMI scores extracted from
our corpus for blood pressure and pressure
medicine.9
? pmi + fplausibility concatenates pmi and
fplausibility.
Baseline Model Given the skewed bracketing dis-
tribution in our dataset, we implement the following
majority baselines: a) right classifies all phrases
as right-bracketed; b) pos classifies NNN as left-
bracketed (Lauer, 1995), ANN as right-bracketed.
4 Results and Discussion
Table 3 omits results for dil and fulladd since they
were outperformed by the right baseline. That
wadd- and lexfunc-based plausibility features per-
form well above this baseline is encouraging, since
it represents the typical default behaviour of parsers
for NPs, although note that these features perform
comparably to the pos baseline, which would be
quite simple to embed in a parser (for English, at
least). For both models, using both basic and recur-
sive features leads to a boost in performance over
basic features alone. Note that recursive features
(frec) achieve at least equal or better performance
than basic ones (fbasic). We expect indeed that in
many cases the asymmetry in plausibility will be
9Several approaches to computing PMI for these purposes
have been proposed in the literature including the dependency
model (Lauer, 1995) and the adjacency model (Marcus, 1980).
We implement the latter since it has been shown to perform
better (Vadas and Curran, 2007b) on NPs extracted from Penn
TreeBank.
sharper when considering the whole NP rather than
its sub-parts; a pressure medicine is still a conceiv-
able concept, but blood (pressure medicine) makes
no sense whatsoever. Finally, wadd outperforms
both the more informative baseline pos and lexfunc.
The difference between wadd and lexfunc is signif-
icant (p < 0.05)10 only when they are trained with
recursive composition features, probably due to our
suboptimal adaptation of the latter to recursive com-
position (see Section 2).
The pmi approach outperforms the best
plausibility-based feature set waddplausibility.
However, the two make only a small proportion of
common errors (29% of the total waddplausibility
errors, 32% for pmi), suggesting that they are com-
plementary. Indeed the pmi + waddplausibility
combination significantly outperforms pmi alone
(p < 0.001), indicating that plausibility features
can improve NP bracketing on top of the pow-
erful PMI-based approach. The same effect can
also be observed in the combination of pmi +
lexfuncplausibility, which again significantly
outperforms pmi alone (p < 0.05). This behaviour
further suggests that the different types of errors are
not a result of the parameters or type of composi-
tion applied, but rather highlights fundamental dif-
ferences in the kind of information that PMI and
composition models are able to capture.
One hypothesis is that compositional models are
more robust for low-frequency NPs, for which
PMI estimates will be less accurate; results on
those low-frequency trigrams only (20% of the NP
dataset, operationalized as those consisting of bi-
grams with frequency ? 100) revealed indeed that
waddplausibility performed 8.1% better in terms
of accuracy than pmi.
5 Conclusion
Our pilot study showed that semantic plausibility,
as measured on compositional distributional repre-
sentations, can improve syntactic parsing of NPs.
Our results further suggest that state-of-the-art PMI
features and the ones extracted from compositional
representations are complementary, and thus, when
combined, can lead to significantly better results.
Besides paving the way to a more general integration
10Significance values are based on t-tests.
1911
of compositional distributional semantics in syntac-
tic parsing, the proposed methodology provides a
new way to evaluate composition functions.
The relatively simple-minded wadd approach out-
performed more complex models such as lexfunc.
We plan to experiment next with more linguistically
motivated ways to adapt the latter to recursive com-
position, including hybrid methods where ANs and
NNs are treated differently. We would also like
to consider more sophisticated semantic plausibility
measures (e.g., supervised ones), and apply them to
other ambiguous syntactic constructions.
6 Acknowledgments
We thank Georgiana Dinu and German Kruszewski
for helpful discussions and the reviewers for use-
ful feedback. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational linguis-
tics, 29(4):589?637.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Composi-
tion Toolkit. In Proceedings of the System Demonstra-
tions of ACL 2013, Sofia, Bulgaria.
Charles Fillmore. 1968. The case for case. In Emmon
Bach and Robert Harms, editors, Universals in Lin-
guistic Theory, pages 1?89. Holt, Rinehart and Win-
ston, New York.
Christina Gagne? and Thomas Spalding. 2009. Con-
stituent integration during the processing of compound
words: Does it involve the use of relational structures?
Journal of Memory and Language, 60:20?35.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37, Up-
psala, Sweden.
Dan Klein and Christopher D Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430. Association for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, pages 47?54, Cambridge, MA.
Phong Le, Willem Zuidema, and Remko Scha. 2013.
Learning from errors: Using vector-based composi-
tional semantics for parse reranking. In Proceedings of
the ACL 2013 Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):313?330.
Mitchell P Marcus. 1980. Theory of syntactic recogni-
tion for natural languages. MIT press.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17?24, Stroudsburg, PA, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Stroudsburg, PA, USA.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of the
COLING, pages 886?894, Beijing, China.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Korea.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Proceedings of ACL, Sofia, Bul-
garia.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
David Vadas and James Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL, pages 240?247, Prague, Czech Republic.
David Vadas and James R Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In Pro-
ceedings of the PACLING, pages 104?112.
David Vadas and James R. Curran. 2011. Parsing
noun phrases in the penn treebank. Comput. Linguist.,
37(4):753?809.
1912
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1?9, Portland,
OR.
1913
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1517?1526,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Compositional-ly Derived Representations of
Morphologically Complex Words in Distributional Semantics
Angeliki Lazaridou and Marco Marelli and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Speakers of a language can construct an
unlimited number of new words through
morphological derivation. This is a major
cause of data sparseness for corpus-based
approaches to lexical semantics, such as
distributional semantic models of word
meaning. We adapt compositional meth-
ods originally developed for phrases to the
task of deriving the distributional meaning
of morphologically complex words from
their parts. Semantic representations con-
structed in this way beat a strong baseline
and can be of higher quality than represen-
tations directly constructed from corpus
data. Our results constitute a novel evalua-
tion of the proposed composition methods,
in which the full additive model achieves
the best performance, and demonstrate the
usefulness of a compositional morphology
component in distributional semantics.
1 Introduction
Effective ways to represent word meaning are
needed in many branches of natural language pro-
cessing. In the last decades, corpus-based meth-
ods have achieved some degree of success in mod-
eling lexical semantics. Distributional semantic
models (DSMs) in particular represent the mean-
ing of a word by a vector, the dimensions of which
encode corpus-extracted co-occurrence statistics,
under the assumption that words that are semanti-
cally similar will occur in similar contexts (Turney
and Pantel, 2010). Reliable distributional vectors
can only be extracted for words that occur in many
contexts in the corpus. Not surprisingly, there is
a strong correlation between word frequency and
vector quality (Bullinaria and Levy, 2007), and
since most words occur only once even in very
large corpora (Baroni, 2009), DSMs suffer data
sparseness.
While word rarity has many sources, one of the
most common and systematic ones is the high pro-
ductivity of morphological derivation processes,
whereby an unlimited number of new words can
be constructed by adding affixes to existing stems
(Baayen, 2005; Bauer, 2001; Plag, 1999).1 For
example, in the multi-billion-word corpus we in-
troduce below, perfectly reasonable derived forms
such as lexicalizable or affixless never occur. Even
without considering the theoretically infinite num-
ber of possible derived nonce words, and restrict-
ing ourselves instead to words that are already
listed in dictionaries, complex forms cover a high
portion of the lexicon. For example, morphologi-
cally complex forms account for 55% of the lem-
mas in the CELEX English database (see Section
4.1 below). In most of these cases (80% according
to our corpus) the stem is more frequent than the
complex form (e.g., the stem build occurs 15 times
more often than the derived form rebuild, and the
latter is certainly not an unusual derived form).
DSMs ignore derivational morphology alto-
gether. Consequently, they cannot provide mean-
ing representations for new derived forms, nor can
they harness the systematic relation existing be-
tween stems and derivations (any English speaker
can infer that to rebuild is to build again, whether
they are familiar with the prefixed form or not)
in order to mitigate derived-form sparseness prob-
lems. A simple way to handle derivational mor-
1Morphological derivation constructs new words (in
the sense of lemmas) from existing lexical items (re-
source+ful?resourceful). In this work, we do not treat in-
flectional morphology, pertaining to affixes that encode gram-
matical features such as number or tense (dog+s). We use
morpheme for any component of a word (resource and -ful
are both morphemes). We use stem for the lexical item that
constitutes the base of derivation (resource) and affix (pre-
fix or suffix) for the element attached to the stem to derive
the new form (-ful). In English, stems are typically indepen-
dent words, affixes bound morphemes, i.e., they cannot stand
alone. Note that a stem can in turn be morphologically de-
rived, e.g., point+less in pointless+ly. Finally, we use mor-
phologically complex as synonymous with derived.
1517
phology would be to identify the stem of rare de-
rived words and use its distributional vector as a
proxy to derived-form meaning.2 The meaning of
rebuild is not that far from that of build, so the
latter might provide a reasonable surrogate. Still,
something is clearly lost (if the author of a text
felt the need to use the derived form, the stem was
not fully appropriate), and sometimes the jump in
meaning can be quite dramatic (resourceless and
resource mean very different things!).
In the past few years there has been much in-
terest in how DSMs can scale up to represent the
meaning of larger chunks of text such as phrases
or even sentences. Trying to represent the mean-
ing of arbitrarily long constructions by directly
collecting co-occurrence statistics is obviously in-
effective and thus methods have been developed
to derive the meaning of larger constructions as a
function of the meaning of their constituents (Ba-
roni and Zamparelli, 2010; Coecke et al, 2010;
Mitchell and Lapata, 2008; Mitchell and Lapata,
2010; Socher et al, 2012). Compositional distri-
butional semantic models (cDSMs) of word units
aim at handling, compositionally, the high produc-
tivity of phrases and consequent data sparseness.
It is natural to hypothesize that the same methods
can be applied to morphology to derive the mean-
ing of complex words from the meaning of their
parts: For example, instead of harvesting a rebuild
vector directly from the corpus, the latter could be
constructed from the distributional representations
of re- and build. Besides alleviating data sparse-
ness problems, a system of this sort, that automati-
cally induces the semantic contents of morpholog-
ical processes, would also be of tremendous theo-
retical interest, given that the semantics of deriva-
tion is a central and challenging topic in linguistic
morphology (Dowty, 1979; Lieber, 2004).
In this paper, we explore, for the first time (ex-
cept for the proof-of-concept study in Guevara
(2009)), the application of cDSMs to derivational
morphology. We adapt a number of composition
methods from the literature to the morphological
setting, and we show that some of these methods
can provide better distributional representations of
derived forms than either those directly harvested
from a large corpus, or those obtained by using
the stem as a proxy to derived-form meaning. Our
2Of course, spotting and segmenting complex words is a
big research topic unto itself (Beesley and Karttunen, 2000;
Black et al, 1991; Sproat, 1992), and one we completely
sidestep here.
results suggest that exploiting morphology could
improve the quality of DSMs in general, extend
the range of tasks that cDSMs can successfully
model and support the development of new ways
to test their performance.
2 Related work
Morphological induction systems use corpus-
based methods to decide if two words are mor-
phologically related and/or to segment words into
morphemes (Dreyer and Eisner, 2011; Goldsmith,
2001; Goldwater and McClosky, 2005; Goldwater,
2006; Naradowsky and Goldwater, 2009; Wicen-
towski, 2004). Morphological induction has re-
cently received considerable attention since mor-
phological analysis can mitigate data sparseness in
domains such as parsing and machine translation
(Goldberg and Tsarfaty, 2008; Lee, 2004). Among
the cues that have been exploited there is distri-
butional similarity among morphologically related
words (Schone and Jurafsky, 2000; Yarowsky and
Wicentowski, 2000). Our work, however, dif-
fers substantially from this track of research. We
do not aim at segmenting morphological complex
words or identifying paradigms. Our goal is to
automatically construct, given distributional rep-
resentations of stems and affixes, semantic repre-
sentations for the derived words containing those
stems and affixes. A morphological induction sys-
tem, given rebuild, will segment it into re- and
build (possibly using distributional similarity be-
tween the words as a cue). Our system, given
re- and build, predicts the (distributional seman-
tic) meaning of rebuild.
Another emerging line of research uses distribu-
tional semantics to model human intuitions about
the semantic transparency of morphologically de-
rived or compound expressions and how these im-
pact various lexical processing tasks (Kuperman,
2009; Wang et al, 2012). Although these works
exploit vectors representing complex forms, they
do not attempt to generate them compositionally.
The only similar study we are aware of is that
of Guevara (2009). Guevara found a systematic
geometric relation between corpus-based vectors
of derived forms sharing an affix and their stems,
and used this finding to motivate the composition
method we term lexfunc below. However, unlike
us, he did not test alternative models, and he only
presented a qualitative analysis of the trajectories
triggered by composition with various affixes.
1518
3 Composition methods
Distributional semantic models (DSMs), also
known as vector-space models, semantic spaces,
or by the names of famous incarnations such as
Latent Semantic Analysis or Topic Models, ap-
proximate the meaning of words with vectors that
record their patterns of co-occurrence with cor-
pus context features (often, other words). There
is an extensive literature on how to develop such
models and on their evaluation. Recent surveys
include Clark (2012), Erk (2012) and Turney and
Pantel (2010). We focus here on compositional
DSMs (cDSMs). Since the very inception of dis-
tributional semantics, there have been attempts to
compose meanings for sentences and larger pas-
sages (Landauer and Dumais, 1997), but inter-
est in compositional DSMs has skyrocketed in
the last few years, particularly since the influen-
tial work of Mitchell and Lapata (2008; 2009;
2010). For the current study, we have reimple-
mented and adapted to the morphological setting
all cDSMs we are aware of, excluding the tensor-
product-based models that Mitchell and Lapata
(2010) have shown to be empirically disappointing
and the models of Socher and colleagues (Socher
et al, 2011; Socher et al, 2012), that require com-
plex optimization procedures whose adaptation to
morphology we leave to future work.
Mitchell and Lapata proposed a set of simple
and effective models in which the composed vec-
tors are obtained through component-wise opera-
tions on the constituent vectors. Given input vec-
tors u and v, the multiplicative model (mult) re-
turns a composed vector c with: ci = uivi. In the
weighted additive model (wadd), the composed
vector is a weighted sum of the two input vectors:
c = ?u + ?v, where ? and ? are two scalars. In
the dilation model, the output vector is obtained
by first decomposing one of the input vectors, say
v, into a vector parallel to u and an orthogonal
vector. Following this, the parallel vector is dilated
by a factor ? before re-combining. This results in:
c = (?? 1)?u,v?u+ ?u,u?v.
Guevara (2010) and Zanzotto et al (2010) pro-
pose the full additive model (fulladd), where the
two vectors to be added are pre-multiplied by
weight matrices: c = Au+Bv
Since the Mitchell and Lapata and fulladd mod-
els were developed for phrase composition, the
two input vectors were taken to be, very straight-
forwardly, the vectors of the two words to be com-
posed into the phrase of interest. In morphological
derivation, at least one of the items to be composed
(the affix) is a bound morpheme. In our adapta-
tion of these composition models, we build bound
morpheme vectors by accumulating the contexts
in which a set of derived words containing the rel-
evant morphemes occur, e.g., the re- vector aggre-
gates co-occurrences of redo, remake, retry, etc.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application, where the distributional representa-
tion of one element in a composition (the func-
tor) is not a vector but a function. Given that
linear functions can be expressed by matrices and
their application by matrix-by-vector multiplica-
tion, in this lexical function (lexfunc) model, the
functor is represented by a matrix U to be multi-
plied with the argument vector v: c = Uv. In
the case of morphology, it is natural to treat bound
affixes as functions over stems, since affixes en-
code the systematic semantic patterns we intend
to capture. Unlike the other composition meth-
ods, lexfunc does not require the construction of
distributional vectors for affixes. A matrix repre-
sentation for every affix is instead induced directly
from examples of stems and the corresponding de-
rived forms, in line with the intuition that every af-
fix corresponds to a different pattern of change of
the stem meaning.
Finally, as already discussed in the Introduc-
tion, performing no composition at all but using
the stem vector as a surrogate of the derived form
is a reasonable strategy. We saw that morphologi-
cally derived words tend to appear less frequently
than their stems, and in many cases the meanings
are close. Consequently, we expect a stem-only
?composition? method to be a strong baseline in
the morphological setting.
4 Experimental setup
4.1 Morphological data
We obtained a list of stem/derived-form pairs from
the CELEX English Lexical Database, a widely
used 100K-lemma lexicon containing, among
other things, information about the derivational
structure of words (Baayen et al, 1995). For each
derivational affix present in CELEX, we extracted
from the database the full list of stem/derived
pairs matching its most common part-of-speech
signature (e.g., for -er we only considered pairs
1519
Affix Stem/Der. Training HQ/Tot. Avg.
POS Items Test Items SDR
-able verb/adj 177 30/50 5.96
-al noun/adj 245 41/50 5.88
-er verb/noun 824 33/50 5.51
-ful noun/adj 53 42/50 6.11
-ic noun/adj 280 43/50 5.99
-ion verb/noun 637 38/50 6.22
-ist noun/noun 244 38/50 6.16
-ity adj/noun 372 33/50 6.19
-ize noun/verb 105 40/50 5.96
-less noun/adj 122 35/50 3.72
-ly adj/adv 1847 20/50 6.33
-ment verb/noun 165 38/50 6.06
-ness adj/noun 602 33/50 6.29
-ous noun/adj 157 35/50 5.94
-y noun/adj 404 27/50 5.25
in- adj/adj 101 34/50 3.39
re- verb/verb 86 27/50 5.28
un- adj/adj 128 36/50 3.23
tot */* 6549 623/900 5.52
Table 1: Derivational morphology dataset
having a verbal stem and nominal derived form).
Since CELEX was populated by semi-automated
morphological analysis, it includes forms that are
probably not synchronically related to their stems,
such as crypt+ic or re+form. However, we did not
manually intervene on the pairs, since we are in-
terested in training and testing our methods in re-
alistic, noisy conditions. In particular, the need to
pre-process corpora to determine which forms are
?opaque?, and should thus be bypassed by our sys-
tems, would greatly reduce their usefulness. Pairs
in which either word occurred less than 20 times
in our source corpus (described in Section 4.2 be-
low) were filtered out and, in our final dataset, we
only considered the 18 affixes (3 prefixes and 15
suffixes) with at least 100 pairs meeting this con-
dition. We randomly chose 50 stem/derived pairs
(900 in total) as test data. The remaining data were
used as training items to estimate the parameters
of the composition methods. Table 1 summarizes
various characteristics of the dataset3 (the last two
columns of the table are explained in the next para-
graphs).
Annotation of quality of test vectors The qual-
ity of the corpus-based vectors representing de-
rived test items was determined by collecting hu-
man semantic similarity judgments in a crowd-
sourcing survey. In particular, we use the similar-
ity of a vector to its nearest neighbors (NNs) as a
proxy measure of quality. The underlying assump-
3Available from http://clic.cimec.unitn.it/
composes
tion is that a vector, in order to be a good represen-
tation of the meaning of the corresponding word,
should lie in a region of semantic space populated
by intuitively similar meanings, e.g., we are more
likely to have captured the meaning of car if the
NN of its vector is the automobile vector rather
than potato. Therefore, to measure the quality of
a given vector, we can look at the average simi-
larity score provided by humans when comparing
this very vector with its own NNs.
All 900 derived vectors from the test set were
matched with their three closest NNs in our se-
mantic space (see Section 4.2), thus producing a
set of 2, 700 word pairs. These pairs were admin-
istered to CrowdFlower users,4 who were asked
to judge the relatedness of the two meanings on a
7-point scale (higher for more related). In order
to ensure that participants were committed to the
task and exclude non-proficient English speakers,
we used 60 control pairs as gold standard, consist-
ing of either perfect synonyms or completely un-
related words. We obtained 30 judgments for each
derived form (10 judgments for each of 3 neighbor
comparisons), with mean participant agreement of
58%. These ratings were averaged item-wise, re-
sulting in a Gaussian distribution with a mean of
3.79 and a standard deviation of 1.31. Finally,
each test item was marked as high-quality (HQ)
if its derived form received an average score of at
least 3, as low-quality (LQ) otherwise. Table 1 re-
ports the proportion of HQ test items for each af-
fix, and Table 2 reports some examples of HQ and
LQ items with the corresponding NNs. It is worth
observing that the NNs of the LQ items, while not
as relevant as the HQ ones, are hardly random.
Annotation of similarity between stem and de-
rived forms Derived forms differ in terms of
how far their meaning is with respect to that of
their stem. Certain morphological processes have
systematically more impact than others on mean-
ing: For example, the adjectival prefix in- negates
the meaning of the stem, whereas -ly has the sole
function to convert an adjective into an adverb.
But the very same affix can affect different stems
in different ways. For example, remelt means lit-
tle more than to melt again, but rethink has subtler
implications of changing one?s way to look at a
problem, and while one of the senses of cycling is
present in recycle, it takes some effort to see their
relation.
4http://www.crowdflower.com
1520
Affix Type Derived form Neighbors
-ist HQ transcendentalist mythologist, futurist, theosophistLQ florist Harrod, wholesaler, stockist
-ity HQ publicity publicise, press, publicizeLQ sparsity dissimilarity, contiguity, perceptibility
-ment HQ advertisement advert, promotional, advertisingLQ inducement litigant, contractually, voluntarily
in- HQ inaccurate misleading, incorrect, erroneousLQ inoperable metastasis, colorectal, biopsy
re- HQ recapture retake, besiege, captureLQ rename defunct, officially, merge
Table 2: Examples of HQ and LQ derived vectors with their NNs
We conducted a separate crowdsourcing study
where participants were asked to rate the 900
test stem/derived pairs for the strength of their
semantic relationship on a 7-point scale. We
followed a procedure similar to the one de-
scribed for quality measurement; 7 judgments
were collected for each pair. Participants? agree-
ment was at 60%. The last column of Ta-
ble 1 reports the average stem/derived related-
ness (SDR) for the various affixes. Note that
the affixes with systematically lower SDR are
those carrying a negative meaning (in-, un-, -less),
whereas those with highest SDR do little more
than changing the POS of the stem (-ion, -ly, -
ness). Among specific pairs with very low related-
ness we encounter hand/handy, bear/bearable and
active/activist, whereas compulsory/compulsorily,
shameless/shamelessness and chaos/chaotic have
high SDR. Since the distribution of the average
ratings was negatively skewed (mean rating: 5.52,
standard deviation: 1.26),5 we took 5 as the rating
threshold to classify items as having high (HR) or
low (LR) relatedness to their stems.
4.2 Distributional semantic space6
We use as our source corpus the concatenation of
ukWaC, the English Wikipedia (2009 dump) and
the BNC,7 for a total of about 2.8 billion tokens.
We collect co-occurrence statistics for the top 20K
content words (adjectives, adverbs, nouns, verbs)
5The negative skew is not surprising, as derived forms
must have some relation to their stems!
6Most steps of the semantic space construction
and composition pipelines were implemented using
the DISSECT toolkit: https://github.com/
composes-toolkit/dissect.
7http://wacky.sslmit.unibo.it, http:
//en.wikipedia.org, http://www.natcorp.
ox.ac.uk
in lemma format, plus any item from the mor-
phological dataset described above that was below
this rank. The top 20K content words also con-
stitute our context elements. We use a standard
bag-of-words approach, counting collocates in a
narrow 2-word before-and-after window. We ap-
ply (non-negative) Pointwise Mutual Information
as weighting scheme and dimensionality reduc-
tion by Non-negative Matrix Factorization, setting
the number of reduced-space dimensions to 350.
These settings are chosen without tuning, and are
based on previous experiments where they pro-
duced high-quality semantic spaces (Boleda et al,
2013; Bullinaria and Levy, 2007).
4.3 Implementation of composition methods
All composition methods except mult and stem
have weights to be estimated (e.g., the ? parame-
ter of dilation or the affix matrices of lexfunc). We
adopt the estimation strategy proposed by Gue-
vara (2010) and Baroni and Zamparelli (2010),
namely we pick parameter values that optimize
the mapping between stem and derived vectors di-
rectly extracted from the corpus. To learn, say, a
lexfunc matrix representing the prefix re-, we ex-
tract vectors of V/reV pairs that occur with suffi-
cient frequency (visit/revisit, think/rethink. . . ). We
then use least-squares methods to find weights for
the re- matrix that minimize the distance between
each reV vector generated by the model given the
input V and the corresponding corpus-observed
derived vector (e.g., we try to make the model-
predicted re+visit vector as similar as possible
to the corpus-extracted one). This is a general
estimation approach that does not require task-
specific hand-labeled data, and for which simple
analytical solutions of the least-squares error prob-
1521
lem exist for all our composition methods. We use
only the training items from Section 4.1 for esti-
mation. Note that, unlike the test items, these have
not been annotated for quality, so we are adopting
an unsupervised (no manual labeling) but noisy es-
timation method.8
For the lexfunc model, we use the training items
separately to obtain weight matrices represent-
ing each affix, whereas for the other models all
training data are used together to globally de-
rive single sets of affix and stem weights. For
the wadd model, the learning process results in
0.16?affix+0.33? stem, i.e., the affix contributes
only half of its mass to the composition of the
derived form. For dilation, we stretch the stem
(i.e., v of the dilation equation is the stem vector),
since it should provide richer contents than the af-
fix to the derived meaning. We found that, on av-
erage across the training pairs, dilation weighted
the stem 20 times more heavily than the affix
(0.05?affix+1?stem). We then expect that the di-
lation model will have similar performance to the
baseline stem model, as confirmed below.9
For all methods, vectors were normalized be-
fore composing both in training and in generation.
5 Experiment 1: approximating
high-quality corpus-extracted vectors
The first experiment investigates to what extent
composition models can approximate high-quality
(HQ) corpus-extracted vectors representing de-
rived forms. Note that since the test items were
excluded from training, we are simulating a sce-
nario in which composition models must generate
representations for nonce derived forms.
Cosine similarity between model-generated and
corpus-extracted vectors were computed for all
models, including the stem baseline (i.e., co-
sine between stem and derived form). The first
row of Table 3 reports mean similarities. The
stem method sets the level of performance rel-
atively high, confirming its soundness. Indeed,
the parameter-free mult model performs below the
baseline.10 As expected, dilation performs simi-
8More accurately, we relied on semi-manual CELEX in-
formation to identify derived forms. A further step towards a
fully knowledge-free system would be to pre-process the cor-
pus with an unsupervised morphological induction system to
extract stem/derived pairs.
9The other models have thousands of weights to be es-
timated, so we cannot summarize the outcome of parameter
estimation here.
10This result does not necessarily contradict those of
stem mult dil. wadd fulladd lexfunc
All 0.47 0.39 0.48 0.50 0.56 0.54
HR 0.52 0.43 0.53 0.55 0.61 0.58
LR 0.32 0.28 0.33 0.38 0.41 0.42
Table 3: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors, for all as well as high- (HR) and low-
relatedness (LR) test items
larly to the baseline, while wadd outperforms it,
although the effect does not reach significance
(p=.06).11 Both fulladd and lexfunc perform sig-
nificantly better than stem (p < .001). Lexfunc
provides a flexible way to account for affixation,
since it models it directly as a function mapping
from and onto word vectors, without requiring a
vector representation of bound affixes. The rea-
son at the base of its good performance is thus
quite straightforward. On the other hand, it is
surprising that a simple representation of bound
affixes (i.e., as vectors aggregating the contexts
of words containing them) can work so well, at
least when used in conjunction with the granular
dimension-by-dimension weights assigned by the
fulladd method. We hypothesize that these aggre-
gated contexts, by providing information about the
set of stems an affix combines with, capture the
shared semantic features that the affix operates on.
When the meaning of the derived form is far
from that of its stem, the stem baseline should no
longer constitute a suitable surrogate of derived-
form meaning. The LR cases (see Section 4.1
above) are thus crucial to understand how well
composition methods capture not only stem mean-
ing, but also affix-triggered semantics. The HR
and LR rows of Table 3 present the results for
the respective test subsets. As expected, the stem
approach undergoes a strong drop when perfor-
mance is measured on LR items. At the other ex-
treme, fulladd and lexfunc, while also finding the
LR cases more difficult, still clearly outperform
the baseline (p<.001), confirming that they cap-
ture the meaning of derived forms beyond what
their stems contribute to it. The effect of wadd,
again, approaches significance when compared to
the baseline (p= .05). Very encouragingly, both
Mitchell and Lapata and others who found mult to be highly
competitive. Due to differences in co-occurrence weighting
schemes (we use a logarithmically scaled measure, they do
not), their multiplicative model is closer to our additive one.
11Significance assessed by means of Tukey Honestly Sig-
nificant Difference tests (Abdi and Williams, 2010)
1522
stem mult wadd dil. fulladd lexfunc
-less 0.22 0.23 0.30 0.24 0.38 0.44
in- 0.39 0.34 0.45 0.40 0.47 0.45
un- 0.33 0.33 0.41 0.34 0.44 0.46
Table 4: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors with negative affixes
fulladd and lexfunc significantly outperform stem
also in the HR subset (p<.001). That is, the mod-
els provide better approximations of derived forms
even when the stem itself should already be a good
surrogate. The difference between the two models
is not significant.
We noted in Section 4.1 that forms containing
the ?negative? affixes -less, un- and in- received
on average low SDR scores, since negation im-
pacts meaning more drastically than other opera-
tions. Table 4 reports the performance of the mod-
els on these affixes. Indeed, the stem baseline per-
forms quite poorly, whereas fulladd, lexfunc and,
to a lesser extent, wadd are quite effective in this
condition as well, all performing greatly above the
baseline. These results are intriguing in light of
the fact that modeling negation is a challenging
task for DSMs (Mohammad et al, 2013) as well as
cDSMs (Preller and Sadrzadeh, 2011). To the ex-
tent that our best methods have captured the negat-
ing function of a prefix such as in-, they might be
applied to tasks such as recognizing lexical op-
posites, or even simple forms of syntactic nega-
tion (modeling inoperable is just a short step away
from modeling not operable compositionally).
6 Experiment 2: Comparing the quality
of corpus-extracted and
compositionally generated words
The first experiment simulated the scenario in
which derived forms are not in our corpus, so
that directly extracting their representation from
it is not an option. The second experiment tests
if compositionally-derived representations can be
better than those extracted directly from the corpus
when the latter is a possible strategy (i.e., the de-
rived forms are attested in the source corpus). To
this purpose, we focused on those 277 test items
that were judged as low-quality (LQ, see Section
4.1), which are presumably more challenging to
generate, and where the compositional route could
be most useful.
We evaluated the derived forms generated by
corpus stem wadd fulladd lexfunc
All 2.28 3.26 4.12 3.99 3.09
HR 2.29 3.56 4.48 4.31 3.31
LR 2.22 2.48 3.14 3.12 2.52
Table 5: Average quality ratings of derived vectors
Target Model Neighbors
florist
wadd flora, fauna, ecosystem
fulladd flora, fauna, egologist
lexfunc ornithologist, naturalist, botanist
sparsity
wadd sparse, sparsely, dense
fulladd sparse, sparseness, angularity
lexfunc fragility, angularity, smallness
inducement
wadd induce, inhibit, inhibition
fulladd induce, inhibition, mediate
lexfunc impairment, cerebral, ocular
inoperable
wadd operable, palliation, biopsy
fulladd operable, inoperative, ventilator
lexfunc inoperative, unavoidably, flaw
rename
wadd name, later, namesake
fulladd name, namesake, later
lexfunc temporarily, reinstate, thereafter
Table 6: Examples of model-predicted neighbors
for words with LQ corpus-extracted vectors
the models that performed best in the first exper-
iment (fulladd, lexfunc and wadd), as well as the
stem baseline, by means of another crowdsourcing
study. We followed the same procedure used to
assess the quality of corpus-extracted vectors, that
is, we asked judges to rate the relatedness of the
target forms to their NNs (we obtained on average
29 responses per form).
The first line of Table 5 reports the average
quality (on a 7-point scale) of the representations
of the derived forms as produced by the models
and baseline, as well as of the corpus-harvested
ones (corpus column). All compositional models
produce representations that are of significantly
higher quality (p < .001) than the corpus-based
ones. The effect is also evident in qualitative
terms. Table 6 presents the NNs predicted by the
three compositional methods for the same LQ test
items whose corpus-based NNs are presented in
Table 2. These results indicate that morpheme
composition is an effective solution when the qual-
ity of corpus-extracted derived forms is low (and
the previous experiment showed that, when their
quality is high, composition can at least approxi-
mate corpus-based vectors).
With respect to Experiment 1, we obtain a dif-
ferent ranking of the models, with lexfunc being
outperformed by both wadd and fulladd (p<.001),
that are statistically indistinguishable. The wadd
1523
composition is dominated by the stem, and by
looking at the examples in Table 6 we notice that
both this model and fulladd tend to feature the
stem as NN (100% of the cases for wadd, 73%
for fulladd in the complete test set). The question
thus arises as to whether the good performance of
these composition techniques is simply due to the
fact that they produce derived forms that are near
their stems, with no added semantic value from the
affix (a ?stemploitation? strategy).
However, the stemploitation hypothesis is dis-
pelled by the observation that both models signifi-
cantly outperform the stem baseline (p<.001), de-
spite the fact that the latter, again, has good per-
formance, significantly outperforming the corpus-
derived vectors (p < .001). Thus, we confirm
that compositional models provide higher qual-
ity vectors that are capturing the meaning of de-
rived forms beyond the information provided by
the stem.
Indeed, if we focus on the third row of Ta-
ble 5, reporting performance on low stem-derived
relatedness (LR) items (annotated as described in
Section 4.1), fulladd and wadd still significantly
outperform the corpus representations (p<.001),
whereas the quality of the stem representations of
LR items is not significantly different form that of
the corpus-derived ones. Interestingly, lexfunc dis-
plays the smallest drop in performance when re-
stricting evaluation to LR items; however, since it
does not significantly outperform the LQ corpus
representations, this is arguably due to a floor ef-
fect.
7 Conclusion and future work
We investigated to what extent cDSMs can gener-
ate effective meaning representations of complex
words through morpheme composition. Several
state-of-the-art composition models were adapted
and evaluated on this novel task. Our results sug-
gest that morpheme composition can indeed pro-
vide high-quality vectors for complex forms, im-
proving both on vectors directly extracted from the
corpus and on a stem-backoff strategy. This re-
sult is of practical importance for distributional se-
mantics, as it paves the way to address one of the
main causes of data sparseness, and it confirms the
usefulness of the compositional approach in a new
domain. Overall, fulladd emerged as the best per-
forming model, with both lexfunc and the simple
wadd approach constituting strong rivals. The ef-
fectiveness of the best models extended also to the
challenging cases where the meaning of derived
forms is far from that of the stem, including nega-
tive affixes.
The fulladd method requires a vector represen-
tation for bound morphemes. A first direction for
future work will thus be to investigate which as-
pects of the meaning of bound morphemes are
captured by our current simple-minded approach
to populating their vectors, and to explore alterna-
tive ways to construct them, seeing if they further
improve fulladd performance.
A natural extension of our research is to ad-
dress morpheme composition and morphological
induction jointly, trying to model the intuition that
good candidate morphemes should have coherent
semantic representations. Relatedly, in the cur-
rent setting we generate complex forms from their
parts. We want to investigate the inverse route,
namely ?de-composing? complex words to de-
rive representations of their stems, especially for
cases where the complex words are more frequent
(e.g. comfort/comfortable).
We would also like to apply composition to in-
flectional morphology (that currently lies outside
the scope of distributional semantics), to capture
the nuances of meaning that, for example, distin-
guish singular and plural nouns (consider, e.g., the
difference between the mass singular tea and the
plural teas, which coerces the noun into a count
interpretation (Katz and Zamparelli, 2012)).
Finally, in our current setup we focus on a single
composition step, e.g., we derive the meaning of
inoperable by composing the morphemes in- and
operable. But operable is in turn composed of op-
erate and -able. In the future, we will explore re-
cursive morpheme composition, especially since
we would like to apply these methods to more
complex morphological systems (e.g., agglutina-
tive languages) where multiple morphemes are the
norm.
8 Acknowledgments
We thank Georgiana Dinu and Nghia The Pham
for helping out with DISSECT-ion and the review-
ers for helpful feedback. This research was sup-
ported by the ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES).
1524
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database (re-
lease 2). CD-ROM, Linguistic Data Consortium,
Philadelphia, PA.
Harald Baayen. 2005. Morphological productivity. In
Rajmund Piotrowski Reinhard Ko?hler, Gabriel Alt-
mann, editor, Quantitative Linguistics: An Inter-
national Handbook, pages 243?256. Mouton de
Gruyter, Berlin, Germany.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni. 2009. Distributions in text. In Anke
Lu?deling and Merja Kyto?, editors, Corpus Linguis-
tics: An International Handbook, volume 2, pages
803?821. Mouton de Gruyter, Berlin, Germany.
Laurie Bauer. 2001. Morphological Productivity.
Cambridge University Press, Cambridge, UK.
Kenneth Beesley and Lauri Karttunen. 2000. Finite-
State Morphology: Xerox Tools and Techniques.
Cambridge University Press, Cambridge, UK.
Alan Black, Stephen Pulman, Graeme Ritchie, and
Graham Russell. 1991. Computational Morphol-
ogy. MIT Press, Cambrdige, MA.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46,
Potsdam, Germany.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
David Dowty. 1979. Word Meaning and Montague
Grammar. Springer, New York.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of EMNLP, pages 616?627, Edinburgh, UK.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL, pages
371?379, Columbus, OH.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 2(27):153?198.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of EMNLP, pages 676?683,
Vancouver, Canada.
Sharon Goldwater. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Emiliano Guevara. 2009. Compositionality in distribu-
tional semantics: Derivational affixes. In Proceed-
ings of the Words in Action Workshop, Pisa, Italy.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Graham Katz and Roberto Zamparelli. 2012. Quanti-
fying count/mass elasticity. In Proceedings of WC-
CFL, pages 371?379, Tucson, AR.
Victor Kuperman. 2009. Semantic transparency revis-
ited. Presentation at the 6th International Morpho-
logical Processing Conference.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, pages 57?60, Boston, MA.
Rochelle Lieber. 2004. Morphology and Lexical Se-
mantics. Cambridge University Press, Cambridge,
UK.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
1525
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics. In press.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of IJCAI, pages 11?17,
Pasadena, CA.
Ingo Plag. 1999. Morphological Productivity: Struc-
tural Constraints in English Derivation. Mouton de
Gruyter, Berlin, Germany.
Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell
states and negative sentences in the distributed
model of meaning. Electr. Notes Theor. Comput.
Sci., 270(2):141?153.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
ConLL workshop on learning language in logic,
pages 67?72, Lisbon, Portugal.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Sproat. 1992. Morphology and Computation.
MIT Press, Cambrdige, MA.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, and
Marc Pomplun. 2012. Estimating semantic trans-
parency of constituents of English compounds and
two-character Chinese words using Latent Semantic
Analysis. In Proceedings of CogSci, pages 2499?
2504, Sapporo, Japan.
Richard Wicentowski. 2004. Multilingual noise-
robust supervised morphological analysis using the
wordframe model. In Proceedings of SIGPHON,
pages 70?77, Barcelona, Spain.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of ACL,
pages 207?216, Hong Kong.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
1526
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1630?1639,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Bayesian Model for Joint Unsupervised Induction
of Sentiment, Aspect and Discourse Representations
Angeliki Lazaridou
University of Trento
angeliki.lazaridou@unitn.it
Ivan Titov
Saarland University
titov@mmci.uni-saarland.de
Caroline Sporleder
Trier University
csporled@coli.uni-sb.de
Abstract
We propose a joint model for unsuper-
vised induction of sentiment, aspect and
discourse information and show that by in-
corporating a notion of latent discourse re-
lations in the model, we improve the pre-
diction accuracy for aspect and sentiment
polarity on the sub-sentential level. We
deviate from the traditional view of dis-
course, as we induce types of discourse re-
lations and associated discourse cues rel-
evant to the considered opinion analysis
task; consequently, the induced discourse
relations play the role of opinion and as-
pect shifters. The quantitative analysis that
we conducted indicated that the integra-
tion of a discourse model increased the
prediction accuracy results with respect to
the discourse-agnostic approach and the
qualitative analysis suggests that the in-
duced representations encode a meaning-
ful discourse structure.
1 Introduction
With the rapid growth of the Web, it is becoming
increasingly difficult to discern useful from irrel-
evant information, particularly in user-generated
content, such as product reviews. To make it easier
for the reader to separate the wheat from the chaff,
it is necessary to structure the available informa-
tion. In the review domain, this is done in aspect-
based sentiment analysis which aims at identify-
ing text fragments in which opinions are expressed
about ratable aspects of products, such as ?room
quality? or ?service quality?. Such fine-grained
analysis can serve as the first step in aspect-based
sentiment summarization (Hu and Liu, 2004), a
task with many practical applications.
Aspect-based summarization is an active re-
search area for which various techniques have
been developed, both statistical (Mei et al, 2007;
Titov and McDonald, 2008b) and not (Hu and Liu,
2004), and relying on different types of supervi-
sion sources, such as sentiment-annotated texts or
polarity lexica (Turney and Littman, 2002). Most
methods rely on local information (bag-of-words,
short ngrams or elementary syntactic fragments)
and do not attempt to account for more complex
interactions. However, these local lexical repre-
sentations by themselves are often not sufficient to
infer a sentiment or aspect for a fragment of text.
For instance, in the following example taken from
a TripAdvisor1 review:
Example 1. The room was nice but let?s not talk
about the view.
it is difficult to deduce on the basis of local lexical
features alone that the opinion about the view is
negative. The clause let?s not talk about the view
could by itself be neutral or even positive given the
right context (e.g., I?ve never seen such a fancy ho-
tel room, my living room doesn?t look that cool...
and let?s not talk about the view). However, the
contrast relation signaled by the connective but
makes it clear that the second clause has a nega-
tive polarity. The same observations can be made
about transitions between aspects: changes in as-
pect are often clearly marked by discourse connec-
tives. Importantly, some of these cues are not dis-
course connectives in the strict linguistic sense and
are specific to the review domain (e.g., the phrase
I would also in a review indicates that the topic
is likely to be changed). In order to accurately
predict sentiment and topic,2 a model needs to ac-
1http://www.tripadvisor.com/
2In what follows, we use the terms aspect and topic, inter-1630
count for these discourse phenomena and cannot
rely solely on local lexical information.
These issues have not gone unnoticed to the re-
search community. Consequently, there has re-
cently been an increased interest in models that
leverage content and discourse structure in senti-
ment analysis tasks. However, discourse-level in-
formation is typically incorporated in a pipeline
architecture, either in the form of sentiment po-
larity shifters (Polanyi and Zaenen, 2006; Naka-
gawa et al, 2010) that operate on the lexical level
or by using discourse relations (Taboada et al,
2008; Zhou et al, 2011) that comply with dis-
course theories like Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988). Such ap-
proaches have a number of disadvantages. First,
they require additional resources, such as lists of
polarity shifters or discourse connectives which
signal specific relations. These resources are avail-
able only for a handful of languages. Second, re-
lying on a generic discourse analysis step that is
carried out before sentiment analysis may intro-
duce additional noise and lead to error propaga-
tion. Furthermore, these techniques will not nec-
essarily be able to induce discourse relations in-
formative for the sentiment analysis domain (Voll
and Taboada, 2007).
An alternative approach is to define a task-
specific scheme of discourse relations (Somasun-
daran et al, 2009). This previous work showed
that task-specific discourse relations are helpful in
predicting sentiment, however, in doing so they re-
lied on gold-standard discourse annotation at test
time rather than predicting it automatically or in-
ducing it jointly with sentiment polarity.
We take a different approach and induce dis-
course and sentiment information jointly in an un-
supervised (or weakly supervised) manner. This
has the advantage of not having to pre-specify a
mapping from discourse cues to discourse rela-
tions; our model induces this automatically, which
makes it portable to new domains and languages.
Joint induction of discourse and sentiment struc-
ture also has the added benefit that the model is
able to learn exactly those aspects of discourse
structure that are relevant for sentiment analysis.
We start with a relatively standard joint model
of sentiment and topic, which can be regarded as a
cross-breed between the JST model (Lin and He,
2009) and the ASUM model (Jo and Oh, 2011),
changeably as well as sentiment levels and opinion polarity.
both state-of-the-art techniques. This model is
weakly supervised, as it relies solely on document-
level (i.e. not aspect-specific) opinion polarity la-
bels to induce topics and sentiment on the sub-
sentential level. In order to test our hypothesis
that discourse information is beneficial, we add
a discourse modeling component. Note that in
modeling discourse we do not exploit any kind
of supervision. We demonstrate that the resulting
model outperforms the baseline on a product re-
view dataset (see Section 5).
To the best of our knowledge, unsupervised
joint induction of discourse structure, sentiment
and topic information has not been considered
before, particularly not in the context of the
aspect-based sentiment analysis task. Importantly,
our method for discourse modeling is a general
method which can be integrated in virtually any
LDA-style model of aspect and sentiment.
2 Modeling Discourse Structure
Discourse cues typically do not directly indicate
sentiment polarity (or aspect). However, they can
indicate how polarity (or aspect) changes as the
text unfolds. As we have seen in the examples
above, changes in polarity can happen on a sub-
sentential level, i.e., between adjacent clauses or,
from a discourse-theoretic point of view, between
adjacent elementary discourse units (EDUs). To
model these changes we need a strong linguistic
signal, for example, in the form of discourse con-
nectives or other discourse cues. We hypothesize
that these are more likely to occur at the beginning
of an EDU than in the middle or at the end. This is
certainly true for most of the traditional discourse
relation cues (particularly connectives).
Changes in polarity or aspect are often cor-
related with specific discourse relations, such as
?contrast?. However, not all relations are rele-
vant and there is no one-to-one correspondence
between relations and sentiment changes.3 Fur-
thermore, if a discourse relation signals a change,
it is typically ambiguous whether this change oc-
curs with the polarity (example 1) or the aspect
(the room was nice but the breakfast was even bet-
ter) or both (the room was nice but the breakfast
was awful). Therefore, we do not explicitly model
3The ?explanation? relation, for example, can occur with
a polarity change (We were upgraded to a really nice room
because the hotel made a terrible blunder with our booking)
but does not have to (The room was really nice because the
hotel was newly renovated).1631
Name Description
AltSame different polarity, same aspect
SameAlt same polarity, different aspect
AltAlt different polarity and aspect
Table 1: Discourse relations
generic discourse relations; instead, inspired by
the work of Somasundaran et al (2008), we define
three very general relations which encode how po-
larity and aspect change (Table 1). Note that we
do not have a discourse relation SameSame since
we do not expect to have strong linguistic evidence
which states that an EDU contains the same senti-
ment information as the previous one.4 However,
we assume that the sentiment and topic flow is
fairly smooth in general. In other words, for two
adjacent EDUs not connected by any of the above
three relations, the prior probability of staying at
the same topic and sentiment level is higher than
picking a new topic and sentiment level (i.e. we
use ?sticky states? (Fox et al, 2008)).
3 Model
In this section we describe our Bayesian model,
first the discourse-agnostic model and then an ex-
tension needed to encode discourse information.
The formal generative story is presented in Fig-
ure 1: the red fragments correspond to the dis-
course modeling component. In order to obtain the
generative story for the discourse-agnostic model,
they simply need to be ignored.
3.1 Discourse-agnostic model
In our approach we make an assumption that all
the words in an EDU correspond to the same topic
and sentiment level. We also assume that an over-
all sentiment of the document is defined, this is
the only supervision we use in inducing the model.
Unlike some of the previous work (e.g., (Titov and
McDonald, 2008a)), we do not constrain aspect-
specific sentiment to be the same across the docu-
ment. We describe our discourse-agnostic model
by first describing the set of corpus-level and
document-level parameters, and then explain how
the content of each document is generated.
Drawing model parameters On the corpus
level, for every topic z ? {1, . . . ,K} and ev-
ery sentiment polarity level y ? {?1, 0,+1},
we start by drawing a unigram language model
4The typical connective in this situation would be and
which is highly ambiguous and can signal several traditional
discourse relations.
from a Dirichlet prior. For example, the language
model of the aspect service may indicate that the
word friendly is used to express a positive opinion,
whereas the word rude expresses a negative one.
Similarly, for every topic z and every over-
all sentiment polarity y?, we draw a distribution
?y?,z over opinion polarity in this topic z. Intu-
itively, one would expect the sentiment of an as-
pect to more often agree with the overall sentiment
y? than not. This intuition is encoded in an asym-
metric Dirichlet prior Dir(? y?) for ?y?,z : ? y? =
(?y?,1, . . . , ?y?,M ), ?y?,y = ? + ??y,y?, where ?y,y? is
the Kronecker symbol, ? and ? are nonnegative
scalar parameters. Using these ?heavy-diagonal?
priors is crucial, as this is the way to ensure that
the overall sentiment level is tied to the aspect-
specific sentiment level. Otherwise, sentiment lev-
els will be specific to individual aspects (e.g., the
?+1? sentiment for one topic may correspond to
a ?-1? sentiment for another one). Without this
property we would not be able to encode soft con-
straints imposed by the discourse relations.
Drawing documents On the document level, as
in the standard LDA model, we choose the distri-
bution over topics for the document from a sym-
metric Dirichlet prior parametrized by ?, which is
used to control sparsity of topic assignments. Fur-
thermore, we draw the global sentiment y?d from a
uniform distribution.
The generation of a document is done on the
EDU-by-EDU basis. In this work, we assume
that EDU segmentation is provided by the prepro-
cessing step. First, we generate the aspect zd,s
for EDU s according to the distribution of top-
ics ?d. Then, we choose a sentiment level yd,s
for the considered EDU from the categorical dis-
tribution ?y?d,zd,s , conditioned on the aspect zd,s,
as well as on the global sentiment of the document
y?d. Finally, we generate the bag of words for the
EDU by drawing the words from the aspect- and
sentiment-specific language model.
This model can be seen as a variant of a state-of-
the-art model for jointly inducing sentiment and
aspect at the sentence level (Jo and Oh, 2011), or,
more precisely, as its combination with the JST
model (Lin and He, 2009), adapted to the specifics
of our setting. Both these models have been shown
to perform well on sentiment and topic prediction
tasks, outperforming earlier models, such as the
TSM model (Mei et al, 2007). Consequently, it
can be considered as a strong baseline.1632
3.2 Discourse-informed model
In order to integrate discourse information into the
discourse-agnostic model, we need to define a set
of extra parameters and random variables.
Drawing model parameters First, at the corpus
level, we draw a distribution ?? over four discourse
relations: three relations as defined in Table 1 and
an additional dummy relation 4 to indicate that
there is no relation between two adjacent EDUs
(NoRelation). This distribution is drawn from an
asymmetric Dirichlet prior parametrized by a vec-
tor of hyperparameters ?. These parameters en-
code the intuition that most pairs of EDUs do not
exhibit a discourse relation relevant for the task
(i.e. favor NoRelation), that is ?4 has a distinct
and larger value than other parameters ?4?.
Every discourse relation c (including
NoRelation which is treated here as Same-
Same) is associated with two groups of transition
distributions, one governing transitions of sen-
timent (??c) and another one controlling topic
transitions (??c). The parameter ??c,ys , defines a
distribution over sentiment polarity for the EDU
s+ 1 given the sentiment for the sth EDU ys and
the discourse relation c. This distribution encodes
our beliefs about sentiment transitions between
EDUs s and s+ 1 related through c. For example,
the distribution ??SameAlt,+1 would assign higher
probability mass to the positive sentiment polarity
(+1) than to the other 2 sentiment levels (0,
-1). Similarly, the parameter ??c,zs , defines a
distribution over K aspects.
These two families of transition distributions
are each defined in the following way. For the dis-
tribution ??, for relations that favor changing the
aspect (SameAlt and AltAlt), the probability of the
preferred (K-1) transitions is proportional to ??
and for the remaining transitions it is proportional
to 1. On the other hand, for the relations that fa-
vor keeping the same aspect (NoRelation and Alt-
Same), the probability of the preferred transition is
proportional to ???, whereas the probability of the
(K-1) remaining transitions is again proportional
to 1. For the sentiment transitions, the distribution
??c,ys is defined in the analogous way (but depends
on ?? and ???). These scalars are hand-coded and
define soft constraints that discourse relations im-
pose on the local flow of sentiment and aspects.
The parameter ??c is a language model over dis-
course cues w?, which are not restricted to uni-
grams but can generate phrases of arbitrary (and
variable) size. For this reason, we draw them
from a Dirichlet process (DP) (i.e. one for each
discourse relation, except for NoRelation). The
base measure G0 provides the probability of an n-
word sequence calculated with the bigram prob-
ability model estimated from the corpus.5 This
model component bears strong similarities to the
Bayesian model of word segmentation (Goldwa-
ter et al, 2009), though we use the DP process
to generate only the prefix of the EDU, whereas
the rest of the EDU is generated from the bag-of-
words model.
Drawing documents As pointed out above, the
content generation is broken into two steps, where
first we draw the discourse cue w?d,s from ??c and
then we generate the remaining words.
The second difference at the data generation
step (Figure 1) is in the way the aspect and sen-
timent labels are drawn. As the discourse rela-
tion between the EDUs has already been chosen,
we have some expectations about the values of the
sentiment and aspect of the following EDU, which
are encoded by the distributions ?? and ??. These
are only soft constraints that have to be taken into
consideration along with the information provided
by the aspect-sentiment model. This coupling of
information naturally translates into the product-
of-experts (PoE) (Hinton, 1999) approach, where
two sources of information jointly contribute to
the final result. The PoE model seems to be more
appropriate here than a mixture model, as we do
not want the discourse transition to overpower the
sentiment-topic model. In the PoE model, in or-
der for an outcome to be chosen, it needs to have
a non-negligible probability under both models.
4 Inference
Since exact inference of our model is intractable,
we use collapsed Gibbs sampling. The variables
that need to be inferred are the topic assignments
z, the sentiment assignments y, the discourse re-
lations c and the discourse cue w? (or, more pre-
cisely, its length) and are all sampled jointly (for
each EDU) since we expect them to be highly de-
pendent. All other variables (i.e. unknown dis-
tributions) could be marginalized out to obtain a
collapsed Gibbs sampler (Griffiths and Steyvers,
2004).
5This measure is improper but it serves the purpose of
favoring long cues, the behavior arguably desirable for our
application.1633
Global parameters:
?? ? Dir(?) [distrib of disc rel]
for each discourse relation c = 1, .., 4:
??c ? DP(?,Go) [distrib of disc rel specific disc cues]
??c,k - fixed [distrib of rel specific aspect transitions]
??c,y - fixed [distrib of rel specific sent transitions]
for each aspect k = 1, 2...K:
for each sentiment y = ?1, 0,+1:
?k,y ? Dir(?k) [unigram language models]
for each global sentiment y? = ?1, 0,+1:
?y?,k ? Dir(?) [sent distrib given overall sentiment]
Data Generation:
for each document d:
y?d ? Unif(?1, 0,+1) [global sentiment]
?d ? Dir(?) [distr over aspects]
for every EDU s:
cd,s ? ?? [draw disc relation]
if cd,s 6= NoRelation
w?d,s ? ??cd,s [draw disc cue]
zd,s ? ?d ? ??cd,s, zd,s?1 [draw aspect]
yd,s ? ?y?d,zd,s? ??cd,s,yd,s?1 [draw sentiment level]for each word after disc cue:
wd,s ? ?zd,s,yd,s [draw words]
Figure 1: The generative story for the joint model.
The components responsible for modeling dis-
course information are emphasized in red: when
dropped, one is left with the discourse-agnostic
model.
Unfortunately, the use of the PoE model pre-
vents us from marginalizing the parameters ex-
actly. Instead, as in Naseem et al (2009), we re-
sort to an approximation. We assume that zd,s and
yd,s are drawn twice; once from the document spe-
cific distribution and once from the discourse tran-
sition distributions. Under this simplification, we
can easily derive the conditional probabilities for
the collapsed Gibbs sampling.
5 Experiments
To the best of our knowledge, this is the first work
that aims at evaluating directly the joint informa-
tion of the sentiment and aspect assignment at the
sub-sentential level of full reviews; most existing
studies either focus on indirect evaluation of the
produced models (e.g., classifying the overall sen-
timent of sentences (Titov and McDonald, 2008a;
Brody and Elhadad, 2010) or even reviews (Naka-
gawa et al, 2010; Jo and Oh, 2011)) or evaluated
solely at the sentential or even document level.
Consequently, in order to evaluate our methods,
we created a new dataset which will be publicly
released.
Aspects Frequency
service 246
value 55
location 121
rooms 316
sleep quality 56
cleanliness 59
amenities 180
food 81
recommendation 121
rest 306
Total 1541
Table 2: Distribution of aspects in the data.
Dataset and Annotation The dataset we created
consists of 13559 hotel reviews from TripAdvi-
sor.com.6 Since our modeling is performed on the
EDU level, all sentences where segmented using
the SLSEG software package.7 As a result, our
dataset consists of 322,935 EDUs.
For creating the gold standard, 9 annotators an-
notated a random subset of our dataset (65 re-
views, 1541 EDUs). The annotators were pre-
sented with the whole review partitioned in EDUs
and were asked to annotate every EDU with the
aspect and sentiment (i.e. +1, 0 or ?1) it ex-
presses. Table 2 presents the distribution of as-
pects in the dataset. The distribution of the sen-
timents is uniform. The label rest captures cases
where EDUs do not refer to any aspect or to a very
rare aspect. The inter-annotator agreement (IAA),
as measured in terms of Cohen?s kappa score, was
66% for the aspect labeling, 70% for the sentiment
annotation and 61% for the joint task of sentiment
and aspect annotation. Though these scores may
not seem very high, they are similar to the ones re-
ported in related sentiment annotation efforts (see
e.g., Ganu et al (2009)).
Experimental setup In order to quantitatively
evaluate the model predictions, we run two sets of
experiments. In the first, we treat the task as an un-
supervised classification problem and evaluate the
output of the models directly against the gold stan-
dard annotation. This is a very challenging set-up,
as the model has no prior information about the
aspects defined (Table 2). In the second set of
experiments, we show that aspects and sentiments
induced by our model can be used to construct in-
formative features for supervised classification. In
6Downloadable from http://clic.cimec.
unitn.it/?angeliki.lazaridou/datasets/
ACL2013Sentiment.tar.gz
7www.sfu.ca/?mtaboada/research/SLSeg.
html1634
Model Precision Recall F1
Random 3.9 3.8 3.8
SentAsp 15.0 10.2 9.2
Discourse 16.5 13.8 10.8
Table 3: Results in terms of macro-averaged pre-
cision, recall and F1.
Model Unmarked Marked
SentAsp 9.2 5.4
Discourse 9.3 11.5
Table 4: Separate evaluation (F1) of the ?marked?
and the ?unmarked? EDUs.
all the cases, we compare the discourse-agnostic
and the discourse-informed models.
In order to induce the model, we let the sampler
run for 2000 iterations. We use the last sample to
define the labeling. The number of topics K was
set to 10 in order to match the number of aspects
defined in our annotation scheme (see Table 2).
The hyperpriors were chosen in a qualitative ex-
periment over a subset of our dataset by manually
inspecting the produced languages models. The
resulting values are: ? = 10?3, ? = 5 ? 10?4,
? = 5 ? 10?4, ? = 10?3, ?4 = 103, ?4? = 10?4,
?? = 85 and ??? = ?? = ??? = 5.
5.1 Direct clustering evaluation
Our labels encoding aspect and sentiment level can
be regarded as clusters. Consequently we can ap-
ply techniques developed in the context of cluster-
ing evaluation. We use a version of the standard
metrics considered for the word sense induction
task (Agirre and Soroa, 2007) where a clustering
is converted to a classification problem. This is
achieved by splitting the gold standard into two
subsets; the training portion is used to choose one-
to-one correspondence from the gold classes to the
induced clusters and then the chosen mapping is
applied to the testing portion. We perform 10-fold
cross validation and report precision, recall and F1
score. Our dataset is very skewed and the majority
class (rest) is arguably the least important, so we
use macro-averaging over labels and then average
those across folds to arrive to the reported num-
bers. We compare the discourse-informed model
(Discourse) against two baselines; the discourse-
agnostic SentAsp model and Random which as-
signs a random label to an EDU while respecting
the distribution of labels in the training set.
Table 3 presents the first analysis conducted on
the full set of EDUs. We observe that by incor-
porating latent discourse relation we improve per-
Content Aspect Polarity
1 but certainly off its greatness value neg
2 and while small they are nice rooms pos
3 but it is not free for all guests amenities neg
4 and the water was brown clean neg
5 and no tea making facilities rooms neg
6 when i checked out service pos
7 and if you do not service neg
8 when we got home clean neu
Table 5: Examples of EDUs where local informa-
tion is not sufficiently informative.
formance over the discourse-agnostic model Sen-
tAsp (statistically significant according to paired t-
test with p < 0.01). Note that fairly low scores in
this evaluation setting are expected for any unsu-
pervised model of sentiment and topics, as models
are unsupervised both in the aspect-specific senti-
ment and in topic labels and the total number of
labels is 28 (all aspects can be associated with the
3 sentiment levels except for rest which can only
be used with neutral (0) sentiment). Consequently,
induced topics, though informative (as we confirm
in Section 5.3), may not correspond to the topics
defined in the gold standard. For example, one
well-known property of LDA-style topic models
is their tendency to induce topics which account
for similar fraction of words in the dataset (Jagar-
lamudi et al, 2012), thus, over-splitting ?heavy?
topics (e.g. rooms in our case). The same, though
to lesser degree, is true for sentiment levels where
the border between neutral and positive (or nega-
tive) is also vaguely defined.
To gain insight into our model, we conducted
an experiment similar to the one presented in So-
masundaran et al (2009). We divide the dataset in
two subsets; one containing all EDUs starting with
a discourse cue (?marked?) and one containing the
remaining EDUs (?unmarked?). We hypothesize
that the effect of the discourse-aware model should
be stronger on the first subset, since the presence
of the connective indicates the possibility of a dis-
course relation with the previous EDU. The set of
discourse connectives is taken from the Penn Dis-
course Treebank (Prasad et al, 2008), thus creat-
ing a list of 240 potential connectives.
Table 5 presents a subset of ?marked? EDUs for
which trying to assign the sentiment and aspect
out of context (i.e. without the previous EDU) is
a difficult task. In examples 1-3 there is no ex-
plicit mention of the aspect. However, there is
an anaphoric expression (marked in bold) which1635
refers to a mention of the aspect in some previous
EDU. On the other hand, in examples 4 and 5 there
is an ambiguity in the choice of aspect; in example
5, tea making facilities can refer to a breakfast at
the hotel (label food) or to facilities in the room
(label rooms). Finally, examples 6-8 are too short
and not informative at all which indicates that the
segmentation tool does not always predict a de-
sirable segmentation. Consequently, automatic in-
duction of segmentation may be a better option.
Table 4 presents quantitative results of this anal-
ysis. Although the performance over the ?un-
marked? example is the same for the two mod-
els, this is not the case for the ?marked? instances
where the discourse-informed model leverages the
discourse signal and achieves better performance.
This behavior agrees with our initial hypothesis,
and suggests that our discourse representation,
though application-specific, relies in part on the
information encoded in linguistically-defined dis-
course cues. We will confirm this intuition in the
qualitative evaluation section. The increase for the
?marked? EDUs does not translate into greater dif-
ferences for the overall scores (Table 3) as marked
relations are considerably less frequent than un-
marked ones in our gold standard (i.e. 35% of the
EDUs are ?marked?). Nevertheless, this clearly
suggests that the discourse-informed model is in
fact capable of exploiting discourse signal.
5.2 Qualitative analysis
To investigate the quality of the induced discourse
structure, we present the most frequent discourse
cues extracted for every discourse relation. Ta-
ble 6 presents a selection of cues that best explain
the discourse relation they have been associated
with. A general observation is that among the cues
there are not only ?traditional? discourse connec-
tives like even though, although, and, but also cues
that are discriminative for the specific application.
In relation SameAlt we can mostly observe
phrases that tend to introduce a new aspect, since
an explicit mention of it is provided (e.g the loca-
tion is, the room was) and more specific phrases
like in addition are used to introduce a new aspect
with the same sentiment. However, these cues re-
veal important information about the aspect of the
EDU, and since they are associated with the lan-
guage model ??, they are not visible anymore to
the language model of aspects ?.
Cues for the relation AltSame also include
Discourse Discourse Cues
relation
SameAlt the location is , the room was, the hotel
has, and the room, and the bed, breakfast
was, the staff were, in addition, good luck
AltSame but, and, it was, and it was, and they, al-
though, and it, but it, but it was, however,
which was, this is, this was, they were,
the only thing, even though, unfortunately,
needless to say, fortunately
AltAlt the room was, the staff were, the only, the
hotel is, but the, however, also, or, overall
i, unfortunately, we will definitely, on the
plus, the only downside , even though, and
even though, i would definately
Table 6: Induced cues from the discourse relations
phrases that contain some anaphoric expressions,
which might refer to previous mentions of an as-
pect in the discourse (i.e. previous EDU). We ex-
pect that since there is an anaphoric expression,
explicit lexical features for the aspect will be miss-
ing, making thus the decision concerning aspect
assignment ambiguous for any discourse-agnostic
model. Interestingly, we found the expressions un-
fortunately, fortunately, the only thing in the same
relation, since all indicate a change in sentiment.
Finally, AltAlt can be viewed as a mixture of the
other two relations. Furthermore, for this relation
we can find expressions that tend to be used at the
end of a review, since at this point we normally
change the aspect and often even sentiment. Some
examples of these cases are overall, we will defi-
nitely and even the misspelled version of the latter
i would definately.
5.3 Features in supervised learning
As an additional experiment to demonstrate infor-
mative of the output of the two models, we de-
sign a supervised learning task of predicting sen-
timent and topic of EDUs. In this setting, the
feature vector of every EDU consists of its bag-
of-word-representation to which we add two extra
features; the models? predictions of topic and sen-
timent. We train a support vector machine with a
polynomial kernel using the default parameters of
Weka8 and perform 10-fold cross-validation.
Table 7 presents results of this analysis in terms
of accuracy for four classification tasks, i.e. pre-
dicting both sentiment and topic, only sentiment
and only topic for all EDUs, as well as predict-
ing sentiment and topic for the ?marked? dataset.
First, we observe that incorporation of the topic-
8http://www.cs.waikato.ac.nz/ml/weka/1636
Features aspect+sentiment aspect sentiment Marked only
(28 classes) (10 classes) (3 classes) sentiment+aspect (28 classes)
only unigrams 36.3 49.8 57.1 26.2
unigrams + SentAsp 38.0 50.4 59.3 27.8
unigrams + Discourse 39.1 52.4 59.4 29.1
Table 7: Supervised learning at the EDU level (accuracy)
model features on a unigram-only model results
in an improvement in classification performance
across all tasks (predicting sentiment, predicting
aspects, or both); as a matter of fact, our accu-
racy results for predicting sentiment are compa-
rable to the sentence-level results presented by
Ta?ckstro?m and McDonald (2011). We have to
stress that accuracies for the joint task (i.e. pre-
dicting both sentiment and topic) are expected to
be lower since it can also be seen as the product
of the two other tasks (i.e. predicting only senti-
ment and only topic). We also observe that the fea-
tures induced from the Discourse model result in
higher accuracy than the ones from the discourse-
agnostic model SentAsp both in the complete set
of EDUs and the ?marked? subset, results that are
in line with the ones presented in Table 4. Fi-
nally, the fact that the results for the complete set
of EDUs are higher than the ones for the ?marked?
dataset clearly suggests that the latter constitute a
hard case for sentiment analysis, in which exploit-
ing discourse signal proves to be beneficial.
6 Related Work
Recently, there has been significant interest in
leveraging content structure for a number of NLP
tasks (Webber et al, 2011). Sentiment analysis
has not been an exception to this and discourse has
been used in order to enforce constraints on the
assignment of polarity labels at several granular-
ity levels, ranging from the lexical level (Polanyi
and Zaenen, 2006) to the review level (Taboada
et al, 2011). One way to deal with this prob-
lem is to model the interactions by using a pre-
compiled set of polarity shifters (Nakagawa et al,
2010; Polanyi and Zaenen, 2006; Sadamitsu et al,
2008). Socher et al (2011) defined a recurrent
neural network model, which, in essence, learns
those polarity shifters relying on sentence-level
sentiment labels. Though successful, this model is
unlikely to capture intra-sentence non-local phe-
nomena such as effect of discourse connectives,
unless it is provided with syntactic information
as an input. This may be problematic for the
noisy sentiment-analysis domain and especially
for poor-resource languages. Similar to our work,
others have focused on modeling interactions be-
tween phrases and sentences. However, this has
been achieved by either using a subset of relations
that can be found in discourse theories (Zhou et
al., 2011; Asher et al, 2008; Snyder and Barzi-
lay, 2007) or by using directly (Taboada et al,
2008) the output of discourse parsers (Soricut and
Marcu, 2003). Discourse cues as predictive fea-
tures of topic boundaries have also been consid-
ered in Eisenstein and Barzilay (2008). This work
was extended by Trivedi and Eisenstein (2013),
where discourse connectors are used as features
for modeling subjectivity transitions.
Another related line of research was presented
in Somasundaran et al (2009) where a domain-
specific discourse scheme is considered. Simi-
larly to our set-up, discourse relations enforce con-
straints on sentiment polarity of associated sen-
timent expressions. Somasundaran et al (2009)
show that gold-standard discourse information en-
coded in this way provides a useful signal for pre-
diction of sentiment, but they leave automatic dis-
course relation prediction for future work. They
use an integer linear programming framework to
enforce agreement between classifiers and soft
constraints provided by discourse annotations.
This contrasts with our work; we do not rely on
expert discourse annotation, but rather induce both
discourse relations and cues jointly with aspect
and sentiment.
7 Conclusions and Future Work
In this work, we showed that by jointly induc-
ing discourse information in the form of discourse
cues, we can achieve better predictions for aspect-
specific sentiment polarity. Our contribution con-
sists in proposing a general way of how discourse
information can be integrated in any LDA-style
discourse-agnostic model of aspect and sentiment.
In the future, we aim at modeling more flexible
sets of discourse relations and automatically in-
ducing discourse segmentation relevant to the task.
1637
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Se-
mEval, pages 7?12.
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. Proceedings of Coling, pages 5?
8.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
Emily B Fox, Erik B Sudderth, Michael I Jordan, and
Alan S Willsky. 2008. An HDP-HMM for systems
with state persistence. In Proceedings of ICML.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Geoffrey E Hinton. 1999. Products of experts. In Pro-
ceedings of ICANN, volume 1, pages 1?6.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD, pages 168?177.
Jagadeesh Jagarlamudi, Hal Daume? III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. Proceedings of EACL, pages 204?
213.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM, pages 815?824.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceeding of CIKM, pages 375?384.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW, pages 171?180.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings of NAACL, pages 786?794.
Tahira Naseem, Benjamin Snyder, Jacob Eisen-
stein, and Regina Barzilay. 2009. Multilin-
gual part-of-speech tagging: Two unsupervised ap-
proaches. Journal of Artificial Intelligence Re-
search, 36(1):341?385.
Livia Polanyi and Annie Zaenen. 2006. Contextual
valence shifters. Computing attitude and affect in
text: Theory and applications, pages 1?10.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Ya-
mamoto. 2008. Sentiment analysis based on proba-
bilistic models using inter-sentence information. In
Proceedings of ACL, pages 2892?2896.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
Proceedings of HLT-NAACL, pages 300?307.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of Coling, pages 801?808.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP, pages 170?179.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of NAACL, pages 149?156.
Maite Taboada, Kimberly Voll, and Julian Brooke.
2008. Extracting sentiment as a function of dis-
course structure and topicality. Simon Fraser Uni-
versity, Tech. Rep, 20.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of ACL, pages
569?574.
Ivan Titov and Ryan McDonald. 2008a. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL, pages 308?316.1638
Ivan Titov and Ryan McDonald. 2008b. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of WWW, pages 112?120.
Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse
connectors for latent subjectivity in sentiment anal-
ysis. In In Proceedings of NAACL.
Peter D Turney and Michael L Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus.
Kimberly Voll and Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. In Pro-
ceedings of Australian Conf. on AI.
Bonnie Webber, Markus Egg, and Valia Kordoni.
2011. Discourse structure and language technology.
Natural Language Engineering, 1(1):1?54.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-Fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In Proceedings EMNLP, pages
162?171.
1639
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403?1414,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Is this a wampimuk?
Cross-modal mapping between distributional semantics
and the visual world
Angeliki Lazaridou and Elia Bruni and Marco Baroni
Center for Mind/Brain Sciences
University of Trento
{angeliki.lazaridou|elia.bruni|marco.baroni}@unitn.it
Abstract
Following up on recent work on estab-
lishing a mapping between vector-based
semantic embeddings of words and the
visual representations of the correspond-
ing objects from natural images, we first
present a simple approach to cross-modal
vector-based semantics for the task of
zero-shot learning, in which an image
of a previously unseen object is mapped
to a linguistic representation denoting its
word. We then introduce fast mapping, a
challenging and more cognitively plausi-
ble variant of the zero-shot task, in which
the learner is exposed to new objects and
the corresponding words in very limited
linguistic contexts. By combining prior
linguistic and visual knowledge acquired
about words and their objects, as well as
exploiting the limited new evidence avail-
able, the learner must learn to associate
new objects with words. Our results on
this task pave the way to realistic simula-
tions of how children or robots could use
existing knowledge to bootstrap grounded
semantic knowledge about new concepts.
1 Introduction
Computational models of meaning that rely on
corpus-extracted context vectors, such as LSA
(Landauer and Dumais, 1997), HAL (Lund and
Burgess, 1996), Topic Models (Griffiths et al,
2007) and more recent neural-network approaches
(Collobert and Weston, 2008; Mikolov et al,
2013b) have successfully tackled a number of lex-
ical semantics tasks, where context vector sim-
ilarity highly correlates with various indices of
semantic relatedness (Turney and Pantel, 2010).
Given that these models are learned from natu-
rally occurring data using simple associative tech-
niques, various authors have advanced the claim
that they might be also capturing some crucial as-
pects of how humans acquire and use language
(Landauer and Dumais, 1997; Lenci, 2008).
However, the models induce the meaning of
words entirely from their co-occurrence with other
words, without links to the external world. This
constitutes a serious blow to claims of cogni-
tive plausibility in at least two respects. One
is the grounding problem (Harnad, 1990; Searle,
1984). Irrespective of their relatively high per-
formance on various semantic tasks, it is debat-
able whether models that have no access to visual
and perceptual information can capture the holis-
tic, grounded knowledge that humans have about
concepts. However, a possibly even more serious
pitfall of vector models is lack of reference: natu-
ral language is, fundamentally, a means to commu-
nicate, and thus our words must be able to refer to
objects, properties and events in the outside world
(Abbott, 2010). Current vector models are purely
language-internal, solipsistic models of meaning.
Consider the very simple scenario in which visual
information is being provided to an agent about
the current state of the world, and the agent?s task
is to determine the truth of a statement similar to
There is a dog in the room. Although the agent
is equipped with a powerful context vector model,
this will not suffice to successfully complete the
task. The model might suggest that the concepts
of dog and cat are semantically related, but it has
no means to determine the visual appearance of
dogs, and consequently no way to verify the truth
of such a simple statement.
Mapping words to the objects they denote is
such a core function of language that humans are
highly optimized for it, as shown by the so-called
fast mapping phenomenon, whereby children can
learn to associate a word to an object or prop-
erty by a single exposure to it (Bloom, 2000;
Carey, 1978; Carey and Bartlett, 1978; Heibeck
and Markman, 1987). But lack of reference is not
1403
only a theoretical weakness: Without the ability to
refer to the outside world, context vectors are ar-
guably useless for practical goals such as learning
to execute natural language instructions (Brana-
van et al, 2009; Chen and Mooney, 2011), that
could greatly benefit from the rich network of lex-
ical meaning such vectors encode, in order to scale
up to real-life challenges.
Very recently, a number of papers have ex-
ploited advances in automated feature extraction
form images and videos to enrich context vectors
with visual information (Bruni et al, 2014; Feng
and Lapata, 2010; Leong and Mihalcea, 2011;
Regneri et al, 2013; Silberer et al, 2013). This
line of research tackles the grounding problem:
Word representations are no longer limited to their
linguistic contexts but also encode visual informa-
tion present in images associated with the corre-
sponding objects. In this paper, we rely on the
same image analysis techniques but instead focus
on the reference problem: We do not aim at en-
riching word representations with visual informa-
tion, although this might be a side effect of our
approach, but we address the issue of automati-
cally mapping objects, as depicted in images, to
the context vectors representing the correspond-
ing words. This is achieved by means of a simple
neural network trained to project image-extracted
feature vectors to text-based vectors through a hid-
den layer that can be interpreted as a cross-modal
semantic space.
We first test the effectiveness of our cross-
modal semantic space on the so-called zero-shot
learning task (Palatucci et al, 2009), which has re-
cently been explored in the machine learning com-
munity (Frome et al, 2013; Socher et al, 2013). In
this setting, we assume that our system possesses
linguistic and visual information for a set of con-
cepts in the form of text-based representations of
words and image-based vectors of the correspond-
ing objects, used for vision-to-language-mapping
training. The system is then provided with visual
information for a previously unseen object, and the
task is to associate it with a word by cross-modal
mapping. Our approach is competitive with re-
spect to the recently proposed alternatives, while
being overall simpler.
The aforementioned task is very demanding and
interesting from an engineering point of view.
However, from a cognitive angle, it relies on
strong, unrealistic assumptions: The learner is
asked to establish a link between a new object and
a word for which they possess a full-fledged text-
based vector extracted from a billion-word cor-
pus. On the contrary, the first time a learner is
exposed to a new object, the linguistic informa-
tion available is likely also very limited. Thus, in
order to consider vision-to-language mapping un-
der more plausible conditions, similar to the ones
that children or robots in a new environment are
faced with, we next simulate a scenario akin to fast
mapping. We show that the induced cross-modal
semantic space is powerful enough that sensible
guesses about the correct word denoting an object
can be made, even when the linguistic context vec-
tor representing the word has been created from as
little as 1 sentence containing it.
The contributions of this work are three-fold.
First, we conduct experiments with simple image-
and text-based vector representations and compare
alternative methods to perform cross-modal map-
ping. Then, we complement recent work (Frome
et al, 2013) and show that zero-shot learning
scales to a large and noisy dataset. Finally, we pro-
vide preliminary evidence that cross-modal pro-
jections can be used effectively to simulate a fast
mapping scenario, thus strengthening the claims
of this approach as a full-fledged, fully inductive
theory of meaning acquisition.
2 Related Work
The problem of establishing word reference has
been extensively explored in computational sim-
ulations of cross-situational learning (see Fazly et
al. (2010) for a recent proposal and extended re-
view of previous work). This line of research has
traditionally assumed artificial models of the ex-
ternal world, typically a set of linguistic or logi-
cal labels for objects, actions and possibly other
aspects of a scene (Siskind, 1996). Recently,
Yu and Siskind (2013) presented a system that
induces word-object mappings from features ex-
tracted from short videos paired with sentences.
Our work complements theirs in two ways. First,
unlike Yu and Siskind (2013) who considered a
limited lexicon of 15 items with only 4 nouns, we
conduct experiments in a large search space con-
taining a highly ambiguous set of potential target
words for every object (see Section 4.1). Most im-
portantly, by projecting visual representations of
objects into a shared semantic space, we do not
limit ourselves to establishing a link between ob-
1404
jects and words. We induce a rich semantic rep-
resentation of the multimodal concept, that can
lead, among other things, to the discovery of im-
portant properties of an object even when we lack
its linguistic label. Nevertheless, Yu and Siskind?s
system could in principle be used to initialize the
vision-language mapping that we rely upon.
Closer to the spirit of our work are two very
recent studies coming from the machine learning
community. Socher et al (2013) and Frome et al
(2013) focus on zero-shot learning in the vision-
language domain by exploiting a shared visual-
linguistic semantic space. Socher et al (2013)
learn to project unsupervised vector-based image
representations onto a word-based semantic space
using a neural network architecture. Unlike us,
Socher and colleagues train an outlier detector
to decide whether a test image should receive a
known-word label by means of a standard super-
vised object classifier, or be assigned an unseen
label by vision-to-language mapping. In our zero-
shot experiments, we assume no access to an out-
lier detector, and thus, the search for the correct
label is performed in the full concept space. Fur-
thermore, Socher and colleagues present a much
more constrained evaluation setup, where only 10
concepts are considered, compared to our experi-
ments with hundreds or thousands of concepts.
Frome et al (2013) use linear regression to
transform vector-based image representations onto
vectors representing the same concepts in linguis-
tic semantic space. Unlike Socher et al (2013) and
the current study that adopt simple unsupervised
techniques for constructing image representations,
Frome et al (2013) rely on a supervised state-of-
the-art method: They feed low-level features to a
deep neural network trained on a supervised object
recognition task (Krizhevsky et al, 2012). Fur-
thermore, their text-based vectors encode very rich
information, such as
~
king ? ~man + ~woman =
~queen (Mikolov et al, 2013c). A natural ques-
tion we aim to answer is whether the success of
cross-modal mapping is due to the high-quality
embeddings or to the general algorithmic design.
If the latter is the case, then these results could be
extended to traditional distributional vectors bear-
ing other desirable properties, such as high inter-
pretability of dimensions.
(a) (b)
Figure 1: A potential wampimuk (a) together with
its projection onto the linguistic space (b).
3 Zero-shot learning and fast mapping
?We found a cute, hairy wampimuk sleeping be-
hind the tree.? Even though the previous state-
ment is certainly the first time one hears about
wampimuks, the linguistic context already creates
some visual expectations: Wampimuks probably
resemble small animals (Figure 1a). This is the
scenario of zero-shot learning. Moreover, if this is
also the first linguistic encounter of that concept,
then we refer to the task as fast mapping.
Concretely, we assume that concepts, denoted
for convenience by word labels, are represented in
linguistic terms by vectors in a text-based distri-
butional semantic space (see Section 4.3). Objects
corresponding to concepts are represented in vi-
sual terms by vectors in an image-based semantic
space (Section 4.2). For a subset of concepts (e.g.,
a set of animals, a set of vehicles), we possess in-
formation related to both their linguistic and visual
representations. During training, this cross-modal
vocabulary is used to induce a projection func-
tion (Section 4.4), which ? intuitively ? represents
a mapping between visual and linguistic dimen-
sions. Thus, this function, given a visual vector,
returns its corresponding linguistic representation.
At test time, the system is presented with a previ-
ously unseen object (e.g., wampimuk). This object
is projected onto the linguistic space and associ-
ated with the word label of the nearest neighbor in
that space (degus in Figure 1b).
The fast mapping setting can be seen as a spe-
cial case of the zero-shot task. Whereas for the lat-
ter our system assumes that all concepts have rich
linguistic representations (i.e., representations es-
timated from a large corpus), in the case of the for-
mer, new concepts are assumed to be encounted in
a limited linguistic context and therefore lacking
rich linguistic representations. This is operational-
ized by constructing the text-based vector for these
1405
Figure 2: Images of chair as extracted from
CIFAR-100 (left) and ESP (right).
concepts from a context of just a few occurrences.
In this way, we simulate the first encounter of a
learner with a concept that is new in both visual
and linguistic terms.
4 Experimental Setup
4.1 Visual Datasets
CIFAR-100 The CIFAR-100 dataset
(Krizhevsky, 2009) consists of 60,000 32x32
colour images (note the extremely small size)
representing 100 distinct concepts, with 600
images per concept. The dataset covers a wide
range of concrete domains and is organized into
20 broader categories. Table 1 lists the concepts
used in our experiments organized by category.
ESP Our second dataset consists of 100K im-
ages from the ESP-Game data set, labeled through
a ?game with a purpose? (Von Ahn, 2006).
1
The
ESP image tags form a vocabulary of 20,515
unique words. Unlike other datasets used for zero-
shot learning, it covers adjectives and verbs in ad-
dition to nouns. On average, an image has 14
tags and a word appears as a tag for 70 images.
Unlike the CIFAR-100 images, which were cho-
sen specifically for image object recognition tasks
(i.e., each image is clearly depicting a single ob-
ject in the foreground), ESP contains a random se-
lection of images from the Web. Consequently,
objects do not appear in most images in their pro-
totypical display, but rather as elements of com-
plex scenes (see Figure 2). Thus, ESP constitutes
a more realistic, and at the same time more chal-
lenging, simulation of how things are encountered
in real life, testing the potentials of cross-modal
mapping in dealing with the complex scenes that
one would encounter in event recognition and cap-
tion generation tasks.
1
http://www.cs.cmu.edu/
?
biglou/
resources/
4.2 Visual Semantic Spaces
Image-based vectors are extracted using the unsu-
pervised bag-of-visual-words (BoVW) represen-
tational architecture (Sivic and Zisserman, 2003;
Csurka et al, 2004), that has been widely and suc-
cessfully applied to computer vision tasks such as
object recognition and image retrieval (Yang et al,
2007). First, low-level visual features (Szeliski,
2010) are extracted from a large collection of im-
ages and clustered into a set of ?visual words?.
The low-level features of a specific image are then
mapped to the corresponding visual words, and the
image is represented by a count vector recording
the number of occurrences of each visual word in
it. We do not attempt any parameter tuning of the
pipeline.
As low-level features, we use Scale Invariant
Feature Transform (SIFT) features (Lowe, 2004).
SIFT features are tailored to capture object parts
and to be invariant to several image transfor-
mations such as rotation, illumination and scale
change. These features are clustered into vocab-
ularies of 5,000 (ESP) and 4,096 (CIFAR-100) vi-
sual words.
2
To preserve spatial information in the
BoVW representation, we use the spatial pyramid
technique (Lazebnik et al, 2006), which consists
in dividing the image into several regions, comput-
ing BoVW vectors for each region and concatenat-
ing them. In particular, we divide ESP images into
16 regions and the smaller CIFAR-100 images into
4. The vectors resulting from region concatenation
have dimensionality 5000 ? 16 = 80, 000 (ESP)
and 4, 096 ? 4 = 16, 384 (CIFAR-100), respec-
tively. We apply Local Mutual Information (LMI,
(Evert, 2005)) as weighting scheme and reduce the
full co-occurrence space to 300 dimensions using
the Singular Value Decomposition.
For CIFAR-100, we extract distinct visual vec-
tors for single images. For ESP, given the size
and amount of noise in this dataset, we build vec-
tors for visual concepts, by normalizing and sum-
ming the BoVW vectors of all the images that have
the relevant concept as a tag. Note that relevant
literature (Pereira et al, 2010) has emphasized
the importance of learners self-generating multi-
ple views when faced with new objects. Thus, our
multiple-image assumption should not be consid-
ered as problematic in the current setup.
2
For selecting the size of the vocabulary size, we relied on
standard settings found in the relevant literature (Bruni et al,
2014; Chatfield et al, 2011).
1406
Category Seen Concepts Unseen (Test) Concepts
aquatic mammals beaver, otter, seal, whale dolphin
fish ray, trout shark
flowers orchid, poppy, sunflower, tulip rose
food containers bottle, bowl, can ,plate cup
fruit vegetable apple, mushroom, pear orange
household electrical devices keyboard, lamp, telephone, television clock
household furniture chair, couch, table, wardrobe bed
insects bee, beetle, caterpillar, cockroach butterfly
large carnivores bear, leopard, lion, wolf tiger
large man-made outdoor things bridge, castle, house, road skyscraper
large natural outdoor scenes cloud, mountain, plain, sea forest
large omnivores and herbivores camel, cattle, chimpanzee, kangaroo elephant
medium-sized mammals fox, porcupine, possum, skunk raccoon
non-insect invertebrates crab, snail, spider, worm lobster
people baby, girl, man, woman boy
reptiles crocodile, dinosaur, snake, turtle lizard
small mammals hamster, mouse, rabbit, shrew squirrel
vehicles 1 bicycle, motorcycle, train bus
vehicles 2 rocket, tank, tractor streetcar
Table 1: Concepts in our version of the CIFAR-100 data set
We implement the entire visual pipeline with
VSEM, an open library for visual seman-
tics (Bruni et al, 2013).
3
4.3 Linguistic Semantic Spaces
For constructing the text-based vectors, we fol-
low a standard pipeline in distributional semantics
(Turney and Pantel, 2010) without tuning its pa-
rameters and collect co-occurrence statistics from
the concatenation of ukWaC
4
and the Wikipedia,
amounting to 2.7 billion tokens in total. Seman-
tic vectors are constructed for a set of 30K target
words (lemmas), namely the top 20K most fre-
quent nouns, 5K most frequent adjectives and 5K
most frequent verbs, and the same 30K lemmas are
also employed as contextual elements. We collect
co-occurrences in a symmetric context window of
20 elements around a target word. Finally, simi-
larly to the visual semantic space, raw counts are
transformed by applying LMI and then reduced to
300 dimensions with SVD.
5
4.4 Cross-modal Mapping
The process of learning to map objects to the their
word label is implemented by training a projec-
tion function f
proj
v?w
from the visual onto the lin-
guistic semantic space. For the learning, we use
a set of N
s
seen concepts for which we have both
image-based visual representations V
s
? R
N
s
?d
v
3
http://clic.cimec.unitn.it/vsem/
4
http://wacky.sslmit.unibo.it
5
We also experimented with the image- and text-based
vectors of Socher et al (2013), but achieved better perfor-
mance with the reported setup.
and text-based linguistic representations W
s
?
R
N
s
?d
w
. The projection function is subject to
an objective that aims at minimizing some cost
function between the induced text-based represen-
tations
?
W
s
? R
N
s
?d
w
and the gold ones W
s
.
The induced f
proj
v?w
is then applied to the image-
based representations V
u
? R
N
u
?d
v
of N
u
un-
seen objects to transform them into text-based rep-
resentations
?
W
u
? R
N
u
?d
w
. We implement 4
alternative learning algorithms for inducing the
cross-modal projection function f
proj
v?w
.
Linear Regression (lin) Our first model is a very
simple linear mapping between the two modali-
ties estimated by solving a least-squares problem.
This method is similar to the one introduced by
Mikolov et al (2013a) for estimating a translation
matrix, only solved analytically. In our setup, we
can see the two different modalities as if they were
different languages. By using least-squares regres-
sion, the projection function f
proj
v?w
can be de-
rived as
f
proj
v?w
= (V
T
s
V
s
)
?1
V
T
s
W
s
(1)
Canonical Correlation Analysis (CCA)
CCA (Hardoon et al, 2004; Hotelling, 1936)
and variations thereof have been successfully used
in the past for annotation of regions (Socher and
Fei-Fei, 2010) and complete images (Hardoon et
al., 2006; Hodosh et al, 2013). Given two paired
observation matrices, in our case V
s
and W
s
,
CCA aims at capturing the linear relationship
that exists between these variables. This is
achieved by finding a pair of matrices, in our
1407
case C
V
? R
d
v
?d
and C
W
? R
d
w
?d
, such that
the correlation between the projections of the
two multidimensional variables into a common,
lower-rank space is maximized. The resulting
multimodal space has been shown to provide a
good approximation to human concept similarity
judgments (Silberer and Lapata, 2012). In our
setup, after applying CCA on the two spaces V
s
and W
s
, we obtain the two projection mappings
onto the common space and thus our projection
function can be derived as:
f
proj
v?w
= C
V
C
W
?1
(2)
Singular Value Decomposition (SVD) SVD is
the most widely used dimensionality reduction
technique in distributional semantics (Turney and
Pantel, 2010), and it has recently been exploited
to combine visual and linguistic dimensions in
the multimodal distributional semantic model of
Bruni et al (2014). SVD smoothing is also a way
to infer values of unseen dimensions in partially
incomplete matrices, a technique that has been ap-
plied to the task of inferring word tags of unanno-
tated images (Hare et al, 2008). Assuming that the
concept-representing rows of V
s
and W
s
are or-
dered in the same way, we apply the (k-truncated)
SVD to the concatenated matrix [V
s
W
s
], such
that [
?
V
s
?
W
s
] = U
k
?
k
Z
T
k
is a k-rank approxima-
tion of the original matrix.
6
The projection func-
tion is then:
f
proj
v?w
= Z
k
Z
T
k
(3)
where the input is appropriately padded with 0s
([V
u
0
Nu?W
]) and we discard the visual block of
the output matrix [
?
V
u
?
W
u
].
Neural Network (NNet) The last model that we
introduce is a neural network with one hidden
layer. The projection function in this model can
be described as:
f
proj
v?w
= ?
v?w
(4)
where ?
v?w
consists of the model weights ?
(1)
?
R
d
v
?h
and ?
(2)
? R
h?d
w
that map the in-
put image-based vectors V
s
first to the hid-
den layer and then to the output layer in or-
der to obtain text-based vectors, i.e.,
?
W
s
=
?
(2)
(?
(1)
(Vs?
(1)
)?
(2)
), where ?
(1)
and ?
(2)
are
6
We denote the right singular vectors matrix by Z instead
of the customaryV to avoid confusion with the visual matrix.
the non-linear activation functions. We experi-
mented with sigmoid, hyperbolic tangent and lin-
ear; hyperbolic tangent yielded the highest perfor-
mance. The weights are estimated by minimizing
the objective function
J(?
v?w
) =
1
2
(1? sim(W
s
,
?
W
s
)) (5)
where sim is some similarity function. In our ex-
periments we used cosine as similarity function,
so that sim(A,B) =
AB
?A??B?
, thus penalizing pa-
rameter settings leading to a low cosine between
the target linguistic representations W
s
and those
produced by the projection function
?
W
s
. The co-
sine has been widely used in the distributional se-
mantic literature, and it has been shown to out-
perform Euclidean distance (Bullinaria and Levy,
2007).
7
Parameters were estimated with standard
backpropagation and L-BFGS.
5 Results
Our experiments focus on the tasks of zero-shot
learning (Sections 5.1 and 5.2) and fast mapping
(Section 5.3). In both tasks, the projected vector of
the unseen concept is labeled with the word asso-
ciated to its cosine-based nearest neighbor vector
in the corresponding semantic space.
For the zero-shot task we report the accuracy
of retrieving the correct label among the top k
neighbors from a semantic space populated with
the union of seen and unseen concepts. For fast
mapping, we report the mean rank of the correct
concept among fast mapping candidates.
5.1 Zero-shot Learning in CIFAR-100
For this experiment, we use the intersection of
our linguistic space with the concepts present in
CIFAR-100, containing a total of 90 concepts. For
each concept category, we treat all concepts but
one as seen concepts (Table 1). The 71 seen con-
cepts correspond to 42,600 distinct visual vectors
and are used to induce the projection function. Ta-
ble 2 reports results obtained by averaging the per-
formance on the 11,400 distinct vectors of the 19
unseen concepts.
Our 4 models introduced in Section 4.4 are
compared to a theoretically derived baseline
Chance simulating selecting a label at random. For
the neural network NN, we use prior knowledge
7
We also experimented with the same objective func-
tion as Socher et al (2013), however, our objective function
yielded consistently better results in all experimental settings.
1408
PP
P
P
P
P
Model
k
1 2 3 5 10 20
Chance 1.1 2.2 3.3 5.5 11.0 22.0
SVD 1.9 5.0 8.1 14.5 29.0 48.6
CCA 3.0 6.9 10.7 17.9 31.7 51.7
lin 2.4 6.4 10.5 18.7 33.0 55.0
NN 3.9 6.6 10.6 21.9 37.9 58.2
Table 2: Percentage accuracy among top k nearest
neighbors on CIFAR-100.
about the number of concept categories to set the
number of hidden units to 20 in order to avoid
tuning of this parameter. For the SVD model, we
set the number of dimensions to 300, a common
choice in distributional semantics, coherent with
the settings we used for the visual and linguistic
spaces.
First and foremost, all 4 models outperform
Chance by a large margin. Surprisingly, the very
simple lin method outperforms both CCA and SVD.
However, NN, an architecture that can capture
more complex, non-linear relations in features
across modalities, emerges as the best performing
model, confirming on a larger scale the recent find-
ings of Socher et al (2013).
5.1.1 Concept Categorization
In order to gain qualitative insights into the perfor-
mance of the projection process of NN, we attempt
to investigate the role and interpretability of the
hidden layer. We achieve this by looking at which
visual concepts result in the highest hidden unit
activation.
8
This is inspired by analogous quali-
tative analysis conducted in Topic Models (Grif-
fiths et al, 2007), where ?topics? are interpreted
in terms of the words with the highest probability
under each of them.
Table 3 presents both seen and unseen con-
cepts corresponding to visual vectors that trigger
the highest activation for a subset of hidden units.
The table further reports, for each hidden unit, the
?correct? unseen concept for the category of the
top seen concepts, together with its rank in terms
of activation of the unit. The analysis demon-
strates that, although prior knowledge about cat-
egories was not explicitly used to train the net-
work, the latter induced an organization of con-
cepts into superordinate categories in which the
8
For this post-hoc analysis, we include a sparsity param-
eter in the objective function of Equation 5 in order to get
more interpretable results; hidden units are therefore maxi-
mally activated by a only few concepts.
Unseen Concept Nearest Neighbors
tiger cat, microchip, kitten, vet, pet
bike spoke, wheel, brake, tyre, motorcycle
blossom bud, leaf, jasmine, petal, dandelion
bakery quiche, bread, pie, bagel, curry
Table 4: Top 5 neighbors in linguistic space after
visual vector projection of 4 unseen concepts.
hidden layer acts as a cross-modal concept cate-
gorization/organization system. When the induced
projection function maps an object onto the lin-
guistic space, the derived text vector will inherit
a mixture of textual features from the concepts
that activated the same hidden unit as the object.
This suggests a bias towards seen concepts. Fur-
thermore, in many cases of miscategorization, the
concepts are still semantically coherent with the
induced category, confirming that the projection
function is indeed capturing a latent, cross-modal
semantic space. A squirrel, although not a ?large
omnivore?, is still an animal, while butterflies are
not flowers but often feed on their nectar.
5.2 Zero-shot Learning in ESP
For this experiment, we focus on NN, the best per-
forming model in the previous experiment. We
use a set of approximately 9,500 concepts, the in-
tersection of the ESP-based visual semantic space
with the linguistic space. For tuning the number
of hidden units of NN, we use the MEN-concrete
dataset of Bruni et al (2014). Finally, we ran-
domly pick 70% of the concepts to induce the pro-
jection function f
proj
v?w
and report results on the
remaining 30%. Note that the search space for the
correct label in this experiment is approximately
95 times larger than the one used for the experi-
ment presented in Section 5.1.
Although our experimental setup differs from
the one of Frome et al (2013), thus preventing a
direct comparison, the results reported in Table 5
are on a comparable scale to theirs. We note that
previous work on zero-shot learning has used stan-
dard object recognition benchmarks. To the best
of our knowledge, this is the first time this task has
been performed on a dataset as noisy as ESP. Over-
all, the results suggest that cross-modal mapping
could be applied in tasks where images exhibit a
more complex structure, e.g., caption generation
and event recognition.
1409
Seen Concepts Unseen Concept Rank of Correct CIFAR-100 Category
Unseen Concept
Unit 1 sunflower, tulip, pear butterfly 2 (rose) flowers
Unit 2 cattle, camel, bear squirrel 2 (elephant) large omnivores and herbivores
Unit 3 castle, bridge, house bus 4 (skyscraper) large man-made outdoor things
Unit 4 man, girl, baby boy 1 people
Unit 5 motorcycle, bicycle, tractor streetcar 2 (bus) vehicles 1
Unit 6 sea, plain, cloud forest 1 large natural outdoor scenes
Unit 7 chair, couch, table bed 1 household furniture
Unit 8 plate, bowl, can clock 3 (cup) food containers
Unit 9 apple, pear, mushroom orange 1 fruit and vegetables
Table 3: Categorization induced by the hidden layer of the NN; concepts belonging in the same CIFAR-
100 categories, reported in the last column, are marked in bold. Example: Unit 1 receives the highest
activation during training by the category flowers and at test time by butterfly, belonging to insects. The
same unit receives the second highest activation by the ?correct? test concept, the flower rose.
P
P
P
P
P
P
Model
k
1 2 5 10 50
Chance 0.01 0.02 0.05 0.10 0.5
NN 0.8 1.9 5.6 9.7 30.9
Table 5: Percentage accuracy among top k nearest
neighbors on ESP.
5.3 Fast Mapping in ESP
In this section, we aim at simulating a fast map-
ping scenario in which the learner has been just
exposed to a new concept, and thus has limited lin-
guistic evidence for that concept. We operational-
ize this by considering the 34 concrete concepts
introduced by Frassinelli and Keller (2012), and
deriving their text-based representations from just
a few sentences randomly picked from the corpus.
Concretely, we implement 5 models: context 1, con-
text 5, context 10, context 20 and context full, where
the name of the model denotes the number of sen-
tences used to construct the text-based representa-
tions. The derived vectors were reduced with the
same SVD projection induced from the complete
corpus. Cross-modal mapping is done via NN.
The zero-shot framework leads us to frame fast
mapping as the task of projecting visual represen-
tations of new objects onto language space for re-
trieving their word labels (v? w). This mapping
from visual to textual representations is arguably
a more plausible task than vice versa. If we think
about how linguistic reference is acquired, a sce-
nario in which a learner first encounters a new ob-
ject and then seeks its reference in the language of
the surrounding environment (e.g., adults having a
conversation, the text of a book with an illustration
of an unknown object) is very natural. Further-
more, since not all new concepts in the linguistic
environment refer to new objects (they might de-
note abstract concepts or out-of-scene objects), it
seems more reasonable for the learner to be more
alerted to linguistic cues about a recently-spotted
new object than vice versa. Moreover, once the
learner observes a new object, she can easily con-
struct a full visual representation for it (and the
acquisition literature has shown that humans are
wired for good object segmentation and recogni-
tion (Spelke, 1994)) ? the more challenging task is
to scan the ongoing and very ambiguous linguistic
communication for contexts that might be relevant
and informative about the new object. However,
fast mapping is often described in the psycholog-
ical literature as the opposite task: The learner
is exposed to a new word in context and has to
search for the right object referring to it. We im-
plement this second setup (w? v) by training the
projection function f
proj
w?v
which maps linguis-
tic vectors to visual ones. The adaptation of NN is
straightforward; the new objective function is de-
rived as
J(?
w?v
) =
1
2
(1? sim(V
s
,
?
V
s
)) (6)
where
?
V
s
= ?
(2)
(?
(1)
(Ws?
(1)
)?
(2)
), ?
(1)
?
R
d
w
?h
and ?
(2)
? R
h?d
v
.
Table 7 presents the results. Not surprisingly,
performance increases with the number of sen-
tences that are used to construct the textual repre-
sentations. Furthermore, all models perform bet-
ter than Chance, including those that are based on
just 1 or 5 sentences. This suggests that the system
can make reasonable inferences about object-word
connections even when linguistic evidence is very
scarce.
Regarding the sources of error, a qualitative
analysis of predicted word labels and objects as
1410
v?w w?v
cooker?potato dishwasher? corkscrew
clarinet? drum potato? corn
gorilla? elephant guitar? violin
scooter? car scarf? trouser
Table 6: Top-ranked concepts in cases where the
gold concepts received numerically high ranks.
X
X
X
X
X
X
X
X
Context
Mapping
v? w w? v
Chance 17 17
context 1 12.6 14.5
context 5 8.08 13.29
context 10 7.29 13.44
context 20 6.02 12.17
context full 5.52 5.88
Table 7: Mean rank results averaged across 34
concepts when mapping an image-based vector
and retrieving its linguistic neighbors (v? w) as
well as when mapping a text-based vector and
retrieving its visual neighbors (w? v). Lower
numbers cue better performance.
presented in Table 6 suggests that both textual
and visual representations, although capturing rel-
evant ?topical? or ?domain? information, are not
enough to single out the properties of the target
concept. As an example, the textual vector of dish-
washer contains kitchen-related dimensions such
as ?fridge, oven, gas, hob, ..., sink?. After projecting
onto the visual space, its nearest visual neighbours
are the visual ones of the same-domain concepts
corkscrew and kettle. The latter is shown in Figure
3a, with a gas hob well in evidence. As a further
example, the visual vector for cooker is extracted
from pictures such as the one in Figure 3b. Not
surprisingly, when projecting it onto the linguis-
tic space, the nearest neighbours are other kitchen-
related terms, i.e., potato and dishwasher.
6 Conclusion
At the outset of this work, we considered the
problem of linking purely language-based distri-
(a) A kettle
(b) A cooker
Figure 3: Two images from ESP.
butional semantic spaces with objects in the vi-
sual world by means of cross-modal mapping. We
compared recent models for this task both on a
benchmark object recognition dataset and on a
more realistic and noisier dataset covering a wide
range of concepts. The neural network architec-
ture emerged as the best performing approach, and
our qualitative analysis revealed that it induced a
categorical organization of concepts. Most impor-
tantly, our results suggest the viability of cross-
modal mapping for grounded word-meaning ac-
quisition in a simulation of fast mapping.
Given the success of NN, we plan to experi-
ment in the future with more sophisticated neural
network architectures inspired by recent work in
machine translation (Gao et al, 2013) and mul-
timodal deep learning (Srivastava and Salakhut-
dinov, 2012). Furthermore, we intend to adopt
visual attributes (Farhadi et al, 2009; Silberer
et al, 2013) as visual representations, since they
should allow a better understanding of how cross-
modal mapping works, thanks to their linguistic
interpretability. The error analysis in Section 5.3
suggests that automated localization techniques
(van de Sande et al, 2011), distinguishing an ob-
ject from its surroundings, might drastically im-
prove mapping accuracy. Similarly, in the textual
domain, models that extract collocates of a word
that are more likely to denote conceptual proper-
ties (Kelly et al, 2012) might lead to more infor-
mative and discriminative linguistic vectors. Fi-
nally, the lack of large child-directed speech cor-
pora constrained the experimental design of fast
mapping simulations; we plan to run more realis-
tic experiments with true nonce words and using
source corpora (e.g., the Simple Wikipedia, child
stories, portions of CHILDES) that contain sen-
tences more akin to those a child might effectively
hear or read in her word-learning years.
Acknowledgments
We thank Adam Li?ska for helpful discussions and
the 3 anonymous reviewers for useful comments.
This work was supported by ERC 2011 Starting
Independent Research Grant n. 283554 (COM-
POSES).
References
Barbara Abbott. 2010. Reference. Oxford University
Press, Oxford, UK.
1411
Paul Bloom. 2000. How Children Learn the Meanings
of Words. MIT Press, Cambridge, MA.
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL/IJCNLP, pages 82?90.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open
library for visual semantics representation. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word. Papers and Reports on Child Lan-
guage Development, 15:17?29.
Susan Carey. 1978. The child as a word learner. In
M. Halle, J. Bresnan, and G. Miller, editors, Linguis-
tics Theory and Psychological Reality. MIT Press,
Cambridge, MA.
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and
Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding meth-
ods. In Proceedings of BMVC, Dundee, UK.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of AAAI, pages
859?865, San Francisco, CA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167, Helsinki, Fin-
land.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and C?edric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop
on Statistical Learning in Computer Vision, ECCV,
pages 1?22, Prague, Czech Republic.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778?
1785, Miami Beach, FL.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science,
34:1017?1063.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of HLT-NAACL, pages 91?99, Los Angeles, CA.
Diego Frassinelli and Frank Keller. 2012. The plausi-
bility of semantic properties generated by a distribu-
tional model: Evidence from a visual world experi-
ment. In Proceedings of CogSci, pages 1560?1565.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639?2664.
David R Hardoon, Craig Saunders, Sandor Szedmak,
and John Shawe-Taylor. 2006. A correlation ap-
proach for automatic image annotation. In Ad-
vanced Data Mining and Applications, pages 681?
692. Springer.
Jonathon Hare, Sina Samangooei, Paul Lewis, and
Mark Nixon. 2008. Semantic spaces revisited: In-
vestigating the performance of auto-annotation and
semantic retrieval using semantic spaces. In Pro-
ceedings of CIVR, pages 359?368, Niagara Falls,
Canada.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335?
346.
Tracy Heibeck and Ellen Markman. 1987. Word learn-
ing in children: an examination of fast mapping.
Child Development, 58:1021?1024.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853?899.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11?20, Montreal, Canada.
1412
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Proceedings of NIPS,
pages 1106?1114.
Alex Krizhevsky. 2009. Learning multiple layers of
features from tiny images. Master?s thesis.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In Proceedings of CVPR, pages 2169?2178, Wash-
ington, DC.
Alessandro Lenci. 2008. Distributional approaches in
linguistic and cognitive research. Italian Journal of
Linguistics, 20(1):1?31.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?
208.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111?3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746?751, Atlanta, Georgia.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Proceedings of NIPS,
pages 1410?1418, Vancouver, Canada.
Alfredo F Pereira, Karin H James, Susan S Jones,
and Linda B Smith. 2010. Early biases and de-
velopmental changes in self-generated object views.
Journal of vision, 10(11).
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics, 1:25?36.
John Searle. 1984. Minds, Brains and Science. Har-
vard University Press, Cambridge, MA.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423?1433, Jeju, Korea.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of ACL, pages 572?582,
Sofia, Bulgaria.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61:39?91.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470?
1477, Nice, France.
Richard Socher and Li Fei-Fei. 2010. Connecting
modalities: Semi-supervised segmentation and an-
notation of images using unaligned text corpora. In
Proceedings of CVPR, pages 966?973.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Elizabeth Spelke. 1994. Initial knowledge: Six sug-
gestions. Cognition, 50:431?445.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In Proceedings of NIPS, pages 2231?2239.
Richard Szeliski. 2010. Computer Vision : Algorithms
and Applications. Springer, Berlin.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Koen van de Sande, Jasper Uijlings, Theo Gevers, and
Arnold Smeulders. 2011. Segmentation as selec-
tive search for object recognition. In Proceedings of
ICCV, pages 1879?1886, Barcelona, Spain.
Luis Von Ahn. 2006. Games with a purpose. Com-
puter, 29(6):92?94.
Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, and
Chong-Wah Ngo. 2007. Evaluating bag-of-visual-
words representations in scene classification. In
James Ze Wang, Nozha Boujemaa, Alberto Del
Bimbo, and Jia Li, editors, Multimedia Information
Retrieval, pages 197?206. ACM.
1413
Haonan Yu and Jeffrey Siskind. 2013. Grounded lan-
guage learning from video described with sentences.
In Proceedings of ACL, pages 53?63, Sofia, Bul-
garia.
1414
Proceedings of the 25th International Conference on Computational Linguistics, pages 112?114,
Dublin, Ireland, August 23-29 2014.
Coloring Objects: Adjective-Noun Visual Semantic Compositionality
Dat Tien Nguyen
(1,2)
Angeliki Lazaridou
(2)
Raffaella Bernardi
(2)
(1)
EM LCT,
(2)
University of Trento/ Italy
name.surname@unitn.it
Abstract
This paper reports preliminary experiments aiming at verifying the conjecture that semantic com-
positionality is a general process irrespective of the underlying modality. In particular, we model
compositionality of an attribute with an object in the visual modality as done in the case of an ad-
jective with a noun in the linguistic modality. Our experiments show that the concept topologies
in the two modalities share similarities, results that strengthen our conjecture.
1 Language and Vision
Recently, fields like computational linguistics and computer vision have converged to a common way of
capturing and representing the linguistic and visual information of atomic concepts, through vector space
models. At the same time, advances in computational semantics have lead to effective and linguistically
inspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g.
phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010).
Compositionality is not to be considered only an important component from a linguistic perspective,
but also from a cognitive perspective and there has been efforts to validate it as a general cognitive
process. However, in computer vision so far compositionality has received limited attention. Thus, in
this work, we study the phenomenon of visual compositionality and we complement limited previous
literature that has focused on event compositionality (St?ottinger et al., 2012) or general image struc-
ture (Socher et al., 2011), by studying models of attribute-object semantic composition.
In a nutshell, our work consists of learning vector representations of attribute-object (e.g., ?red car?,
?cute dog? etc.) and objects (e.g., ?car?, ?dog?, ?truck?, ?cat? etc.) and by using those compute the
representation of new objects having similar attributes (?red truck?, ?cute cat? etc.). This question has
both theoretical and applied impact. The possibility of developing a visual compositional model of
attribute-object, on the one hand, could shed light on the acquisition of such ability in humans; how we
learn attribute representation and compose them with different objects is still an open question within the
cognitive science community (Mintz and Gleitman, 2002). On the other hand, computer vision systems
could become generative and be able to recognize unseen attribute-object combinations, a component
especially useful for object recognition and image retrieval.
2 Visual Compositional Model
As our source of inspiration regarding the type of compositionality, we use the Lexical Functional model
(LF) (Baroni and Zamparelli, 2010), under which adjectives, in linguistic compositionality, are repre-
sented as linear functions (i.e., matrix of weights). Concretely, each adjective function f
W
adj
is induced
from corpus-observed vectors of adjective-noun phrases w
i
? W
phrase
and noun w
j
? W
noun
, e.g.,
?(w
red car
, w
car
), (w
red flag
, w
flag
), . . .?, by solving the least-squares regression problem:
arg min
f
W
adj
?R
d?d
||W
phrase
? f
W
adj
W
noun
||
This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:
http://creativecommons.org/licenses/by/4.0/
112
In this work, we propose to import the LF method in the visual modality, aiming at develop-
ing a Visual Compositional Model. Similarly to the case of linguistic compositionality, each at-
tribute function f
V
attr
is induced from image-harvested vector representations of attribute-object v
i
?
V
phrase
and object v
j
? V
object
, e.g. for training the function f
V
red
the following data can be used
?(v
red car
, v
car
), (v
red flag
, v
flag
), . . .?.
3 Experiments
The visual representations of attribute-objects and objects are created with the PHOW-color fea-
tures (Bosch et al., 2007) and SIFT color-agnostic features (Lowe, 2004) respectively. The linguistic
representations for the adjective-noun W
phrase
and noun W
noun
are built with the word2vec toolkit
1
using a corpus of 3 billion tokens.
2
Both visual and linguistic representations consist of 300 dimensions.
In this work, we focus on attributes related to 10 colors (Russakovsky and Fei-Fei, 2012) for a
total number of 9699 images depicting 202 unique objects/nouns and 886 unique phrases (attribute-
object/adjective-noun). Our experiments are conducted with aggregated attribute-object representations
obtained by summing the visual vectors extracted from images representing the same attribute-object,
The same pipeline is followed for the objects to obtain aggregated object vectors.
This work aims at comparing the behavior of the semantically-driven compositionality process across
the two modalities. For this reason, we report results on the intersection of V
phrase
and W
phrase
, a
process that results in 266 attribute-object/adjective-noun items. Furthermore, although the training data
for the two modalities are different, the size of the training data is identical, i.e., the f
V
attr
is trained using
the remaining 620 attribute-object items, whereas for the f
W
adj
, we randomly sample 620 adjective-noun
items from the language space.
3.1 Analysis of Language and Visual Semantic Spaces
This experiment aims at assessing the degree to which language and vision share commonalities. To this
end, we compute the cosine similarities between all possible combination of objects (resp., nouns) and
perform a correlation analysis of the similarity of the corresponding pairs in the two lists resulting in 0.45
Spearman correlation ? e.g., we correlate the similarity between v
cat
and v
dog
with that between w
cat
and
w
dog
. For instance, ?goat? and ?sheep? are highly similar in both spaces, whereas ?whale? and ?bird?
are similar only linguistically, whereas ?blackboard? and ?chair? are similar only visually. The same
experiment is performed between all possible combinations of attribute-object/adjective-noun items, e.g.
we correlate the similarity between v
white cat
and v
black dog
with that between w
white cat
and w
black dog
,
resulting in 0.33 Spearman correlation (see Table 1).
Overall, our results suggest that the topologies of the semantic spaces are similar in the two modalities.
Furthermore, since this phenomenon is also apparent in the cases of attribute-object and adjective-noun
pairs, this alludes to the possibility of transferring approaches of semantic compositionality from the
linguistic to the visual modality.
High Visual Low Visual
High Linguistic goat-sheep, jaguar- lion baboon-transporter, bird-whale
black bag - brown bag, brown bear - yellow dog blue grass - blue van, gray whale - white deer
Low Linguistic ball-horse, blackboard-chair baboon-sofa, backboard-panda
red strawberry - white ball, white bear - yellow dog black bag - green bridge, green table - yellow stick
Table 1: Similar and dissimilar concepts in the language and vision space.
3.2 Semantically-driven composition for attribute-object representations
The findings of the previous experiment suggest a high correlation between the visual attribute-attribute
representations and the corpus-harvested adjective-noun representations. An interesting question that
arises is whether we could approximate such visual representations of complex visual units, similarly to
1
https://code.google.com/p/word2vec/
2
http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk
113
how is done in Computational Linguistics for approximating the text-based representations of adjective-
noun phrases. Thus, this experiment is designed in order to assess the validity of the semantically-driven
compositionality approach in the visual domain. Results are reported in Table 2. Since we expect that
the quality of the aggregated vectors depends on the numbers of available images, we report results for
subsets of the original data set that differ on the number of images per phrase.
By means of the LF composition method sketched in Section 2, we obtain the compositional represen-
tations of attribute-object (V
comp
phrase
) and adjective-noun (W
comp
phrase
) items. We then perform the correlation
analyses between the similarities obtained in the composed visual space V
comp
phrase
with: 1) the equiva-
lent image-harvested representations V
phrase
, 2) the equivalent corpus-derived linguistic representations
W
phrase
, 3) the equivalent compositionally-derived linguistic representations W
comp
phrase
.
Overall, the correlation between V
comp
space
and V
space
suggests that the visual compositionality of
attribute-object can account, to some extend, for the visual semantics of the respective image, and it
further improves with the number of images we consider for obtaining the aggregated vectors of the vi-
sual phrases. Finally, as expected, the correlations between V
comp
space
although lower than the ones reported
in Section 3.1, i.e., 0.22 vs 0.32, are still non negligible.
all phrases > 10 images > 20 images > 30 images
V
comp
phrase
- V
phrase
0.24 0.40 0.53 0.58
V
comp
phrase
- W
phrase
0.10 0.22 0.19 0.23
V
comp
phrase
- W
comp
phrase
0.04 0.05 0.18 0.10
Table 2: Spearman correlations between the similarities in the V
comp
phrase
and other semantic spaces.
4 Conclusions
In this work, we have experimented with semantically-driven compositionality of attributes with objects
in the visual modality, by adopting an out-of-the-box composition method from the computational se-
mantics literature. Our preliminary results have shown that the visual representations of attribute-objects
when obtained compositionally reflect properties similar not only to the ones found in representations
harvested automatically from images, but also from those extracted from text corpora. These results
show that semantic compositionality might be a general process irrespective of the underlying modality.
We have just scratched the surface on this topic and in the future we plan to experiment with a larger
variety of attributes and use and design alternative visual compositional models.
Acknowledgements
The second and third author acknowledge ERC 2011 Starting Independent Research Grant n. 283554
(COMPOSES). We thank the 3 anonymous reviewers for their comments, Marco Baroni and Elia Bruni
for their constant and useful feedback.
References
[Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are
matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, 1183?1193.
[Bosch et al.2007] Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random
forests and ferns. In Proceedings of ICCV, 1?8.
[Lowe2004] David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60:91?110.
[Mintz and Gleitman2002] Toben H. Mintz and Lila R. Gleitman. 2002. Adjectives really do modify nouns: the
incremental and restricted nature of early adjective acquisition. Cognition, 84:267?293.
[Mitchell and Lapata2010] Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of
semantics. Cognitive Science, 34(8):1388?1429.
[Russakovsky and Fei-Fei2012] Olga Russakovsky and Li Fei-Fei. 2012. Attribute learning in large-scale datasets.
In Trends and Topics in Computer Vision, 1?14. Springer.
[Socher et al.2011] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes
and natural language with recursive neural networks. In Proceedings of ICML, 129?136.
[St?ottinger et al.2012] J. St?ottinger, J.R.R. Uijlings, A.K. Pandey, N. Sebe, and F. Giunchiglia. 2012. (unseen)
event recognition via semantic compositionality. In CVPR.
114
