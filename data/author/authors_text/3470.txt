Chinese Generation in a Spoken Dialogue Translation System 
Hua Wu, Taiyi Huang, Chengqing Zong and Bo Xu 
National Laboratory of Pattern Recognition, Institute of Automation 
Chinese Academy of Sciences, Beijing 100080, China 
E-mail: { wh, huang, cqzong, xubo } @nlpr.ia.ac.cn 
Abstract:  
A Chinese generation module in a speech to 
speech dialogue translation system is presented 
he:re. The input of the generation module is the 
underspecified semantic representation. Its design 
is strongly influenced by the underspecification f 
the inlmtS and the necessity of real-time and 
robust processing. We design an efficient 
generation system comprising a task-oriented 
microplanner and a general surface realization 
module for Chinese. The microplanner performs 
the lexical and syntactic choice and makes 
inferences fiOln the input and domain knowledge. 
The output of the microplanner is fully 
instantiated. This enables the surface realizer to 
traverse ltle input in a top-down, depth-first 
fashion, which in turn speeds the whole 
generation procedure. The surface realizer also 
combines the template method and deep 
generation technology in the same formalism. 
Preliminary results are also presented in this 
paper. 
1,, In t roduct ion  
In this paper, we will present the core aspects 
of the generation component of our speech to 
speech dialogue translation system, the domain of 
which is hotel reservation. The whole system 
consists of five modules: speech recognizen 
translator, dialogue manageh generator and speech 
synthesizer. And the system takes the interlingua 
method in order to achieve multilinguality. Here 
the interlingua is an underspecified selnantic 
representation (USR). And the target language is 
Chinese in this paper. 
Reiter (Reiter 1995) made a clear distinction 
between templates and deep generation. The 
template method is rated as efficient but inflexible, 
while deep generation method is considered as 
flexible but inefficient. So the hybrid method to 
combine both the methods has been adopted in the 
last few years. Busemann (Busemann 1996) used 
hybrid method to allow template, canned texts and 
general rules appearing in one formalism and to 
tackle the problem of the inefficiency of the 
grammar-based surface generation system. Pianta 
(Pianta 1999) used the mixed representation 
approach to allow the system to choose between 
deep generation technology and template method. 
Our system keeps the surface generation 
module general for Chinese. At the same time, we 
can also deal with templates in tile input without 
changing tile whole generation process. If tile 
attribute in the feature structure is "template", 
then the value must be taken as a word string, 
which will appear in the output without 
modification. The surface generation module 
assumes  the input as a predicate-argument 
structure, which is called intermediate 
representation here. And any input of it must be 
first converted into an intermediate r presentation. 
The whole generation process can be 
modularized fimher into two separate components: 
microplanner and syntactic realizer. The 
microplanner is task-oriented. The input is an 
USR and the function of it is to plan an utterance 
on a phrase- or sentence-level. It maps concepts 
defined in the domain to a functional 
representation which is used by the syntactic 
generation components to realize an appropriate 
surface string for it. The functional description is 
made of feature structures, the attribute-value 
1141 
pairs. And the functional representation serves as 
the intermediate representation between the 
microplanner and the syntactic generator. The 
intermediate representation is fully instantiated. 
This enables the surface realizer to traverse the 
input in a top-down, depth-first fashion to work 
out a grammatically correct word string for the 
input, which in turn speeds the whole generatiou 
procedure. So our system use a task-oriented 
microplanner and a general surface realizer. The 
main advantage is that it is easy to adapt the 
system to other domains and maintain the 
flexibility of the system. 
In this paper, section 2 gives a brief 
description of our semantic representation. 
Section 3 presents our method on the 
microplanning procedure. Section 4 describes the 
syntactic generation module. Section 5 presents 
the preliminary results of our generation system. 
Section 6 presents discussions and future work. 
2. Semantic Representation 
The most obvious characteristics of the 
selnantic representation are its independence of 
peculiarities of any language and its 
underspecification. But it lnUSt capture the 
speaker's intent. The whole semantic 
representation has up to four components as 
shown iu figure l: speaker tag, speech act, topic 
and arguments. 
The speaker tag is either "a" lbr agent or "c" 
for customer to indicate who is speaking. The 
speech act indicates the speaker's intent. The topic 
expresses the current focus. The arguments 
indicate other inforlnatiou which is necessary to 
express the entire meaning of the source sentence. 
USR::= speaker: speech act: topic: m'gument 
Speaker::= alc 
Speech_act ::= give-information I request- 
information\[... 
Topic ::= (concept = attribute) ^
Argument ::= (concept=attribute)l* 
Figure 1 Underspecified Semantic Representation 
Both the topic and arguments are made up of 
attribute-value pairs in functional formalisms. The 
attribute can be any concept defined in the dolnain 
of hotel reservation. The value can be an atomic 
symbol or recursively an attribute-value pair. The 
symbol "^" in the topic expression indicate that 
the expression can appears zero to one time, while 
The symbol "*" iu the argument expression shows 
that the expression can appears zero to any times. 
And the attribute-value pairs are order free. Both 
topic and arguments are optional parts in the USR. 
Let us consider a complex semantic 
expression extracted from our corpus. It is shown 
in Example I: 
a: give-information: (available -- (room = 
(room- type = double ))) : (price = (quantity 
=200&240,currency=dollor)) I (1) 
In Example 1, the speech act is give- 
information, which means that the agent is 
offering information to the customer. The topic 
indicates there are double rooms. The arguments 
list the prices of double rooms, which shows that 
there are two kinds of double rooms available. So 
the meaning of this representation is " We have 
two kinds of double rooms which cost 200 mad 
240 dollars respectively". From the USR, the 
kinds of rooms are not expressed explicitly in the 
format. Only from the composite value of the 
concept "price " can we judge there are two kinds 
of rooms because the price is different. This is 
only one example of underspecification, which 
needs inferences from the input and the domain 
knowledge. 
3. The Microplanner 
The input to our microplanner is the 
underspecified semantic representation. From the 
above semantic representation, we can see that it 
is underspecified because it lacks infornlation 
such as predicate-argument structure, cognitive 
status of referents, or restrictive/attribute fimction 
of semantic properties. Some of the non-specified 
pieces of ilfformation such as predicate/argument 
structure are essential to generate a correct 
translation of the source sentence. Fortmmtely, 
much of the information which is not explicitly 
represented can be inferred fiom default 
knowledge about the specific domain and the 
general world knowledge. 
The lnicroplanner includes two parts: 
sentence-level planning and phrase-level planning. 
1142 
The sentence planner maps the semantic 
representation into predicate argument structure. 
And the phrase planner maps the concepts defined 
in the domain into Chinese phrases. 
In order to express rules, we design a format 
t'or them. The rules are represented as pattern- 
constraints-action triples. A pattern is to be 
matched with part of the input on the sentence 
level and with the concepts on the phrase level. 
The constraints describe additional context- 
dependent requirements to be fulfilled by the 
input. And the action part describes the predicate 
argument structure and other information such as 
mood and sentence type. An example describiug a 
sentence-level rule is shown in Figure 2. 
((speaker= a ) ( speech_act =give-information )( topic 
= available ) ( topic_value = room )); 
//pattern 
(exist(concept, 'price' )); //constraint 
( (cat = clause) ( mood = declarative) 
( tense = present) (voice = active) 
(sentence type = possessive) 
(predicate ='4f') 
(args = (((case - pos) 
(lex :- #get(attribute, 'room' ))) 
((case = bel) 
(cat = de) 
(modifier-(#gct(altributc, 'price' ))) 
(auxiliary = 'l'l{j')))) 
(!optiolml: pre_mod = ( time = #get ( attribute, 
'lime')))); //action 
Figure 2 Example Microplanning l>,ule 
First, we match the pattern part with the input 
USI>,. If matched, the constraint is tested. In the 
example, the concept price lnust exist in the input. 
The action part describes the whole sentence 
structure such as predicate argument structure, 
sentence type, voice, mood. The symbol "#get" in 
the action part indicates thai the value can be 
obtained by accessing the phrase rules or the 
dictionary to colnplete the structure recursively. 
The "#get" expression has two parameters. The 
first parameter can be "concept" or "attribute" to 
indicate to access the dictionary and phrase rues 
respectively. The second parameter is a concept 
defined in the domain. In the example, the "#get" 
expression is used to get the value of the domain 
concepts room and price respectively. The symbol 
"optionah" indicates that the attribute-value pair 
behind it is optional. If the input has the concept, 
we fill it. 
After the sentence- and phrase-level phmning, 
we must access the Chinese dictionary to get the 
part-of-speech of the lexicon and other syntactic 
information. If the input is the representation i  
Example I, the result of the microplanning is 
shown in Figure 3. 
(cat  = clat, se) 
( sentence_type =possessive) 
(mood = declarative) 
( tense = present) (voice = active) 
(predicate =((cat=vcm) (lex ='4f'))) 
(args=(((case = pos)(cat = nct)(lex ='~J, Vl'iq')) 
((case = bel) 
(cat =de) 
(modi fier=((cat=mp) 
(cardinal =((cat=nc) 
(n l=((cat=num) (lex='200')) 
(n2=((cat=num)(lex="240')) 
(qtf= ((cat=ncl) ( lex ='0~)t\]')))) 
(at, x il iary =(lex =' I'1I'.1')))) 
lVigure 3 Microplanning Result for Example 1 
In the above example, "cat" indicates the 
category of the sentence, plnases or words. "h'x" 
denotes the Chinese words. "case" describes the 
semantic roles of the arguments. 
Target language generation in dialogue 
translation systems imposes strong constraints on 
the whole generation. A prominent pmblena is the 
non-welformedness of the input. It forces the 
generation module to be robust to cope with the 
erroneous and incomplete input data. In this level, 
we design some general rules. The input is first to 
be matched with the specific rules. If there is no 
rules matched, we access the general rules to 
match with the input. In this way, although the 
input is somehow ill-formed, the output still 
includes the main information of the input. An 
example is shown in (2). The utterance is 
supposed for the custom to accept he single room 
offered by the agent. But the speech act is wrong 
because the speech act "ok" is only used to 
1143 
indicate' that the custol-u and tile agenl has agreed 
on one Iopic. 
c: ok: ( room = ( room-type := single, 
quantity= 1 )): (2) 
Although example (2) is ill formed, it 
includes most information of the source sentence. 
Our robust generator can produce the sentence 
shown in (3). 
Cl'i-)kf/iJ~J: ( yes, a single roont ) (3) 
4. Syntactic realizatim  
The syntactic realizer proceeds from the 
microplannir~g result as shown in t"igure 3. The 
realizer is based on a flmctional uuificati,,m 
fornmlism. 
lit tMs module, we also introduce the 
template nlethod. If lhe input includes an 
attribute~wflue pair which uses "template" as file 
attribute, then rite wflue is taken as canned lexts or 
word strhws wilh slots. It will appear in the output 
without any modificati(m. So we can embed tile 
template into the surface realization without 
modifying tlw whoh: generation l)rocedure. When 
the hybrid method is used, the input is first 
matched with the templates defined. If matched, 
the inputs will go lo llle surface realizer directly, 
skiplfing tl,c microplanning process. 
The task of the Chinese realizer i:; as tollows: 
, Define the sentence struclure 
? Provide ordering constraints among the 
syntactic onstituents of the sentence 
? Select the functional words 
4.1 \]Intermediate Representation 
The intermediate representation(IR) is made 
up of feature structures. It corresponds to the 
predicate argument structure. The aim is to 
normalize the input of tile surface realizer. It is of 
considerable practical benefit to keep the rule 
basis as independent as possible front external 
conditions (such as the domain and output of tile 
preceding system). 
The intermediate representation includes 
three parts: predicate int"ormation, obligatory 
arguments and optional arguments. The predicate 
inR)rmation describes the top-level information in 
a clause includiug the main verb, lhe mood, the 
voice, and so on. The obligatory arguments are 
slots of roles that must be filled in a clause for it 
to be contplete. And the optional arguments 
specify the location, the time, the purpose of the 
event etc. They arc optional because they do not 
affect rite contpleteness of a clause. An example is 
shown in Figure 4. The input is for the sentence 
"{~J~ l'f\] ~\]l~ 1{ 1'1 @) \  \[)iJ li!.~ ?" (Do you have single 
rooms now?). "agrs" antt '?opt" in Figure 4 
represent obligatory arguntents and optional 
arguments respectively. 
((cat = clause) 
( sentence )ype =possessive) 
(mood: yes-no) 
( lense = present) (wfice -: active) 
(predicate =((cat=veto) (lex ="(J"))) 
(args=(((case :-: pos)(ca! -pron)(lex ='{?j<{f\]')) 
((case "- bel) (cai ~:nct)(lex=' "l%)v. \['(iJ ')))) 
(opt=(d me=((cat=:adv) tie x=' J:l)lu (I i'))))) 
Fip, urc 4 F, xample Intermediate l),el)rescnlalion 
4.2 Chinese Reallizalion 
In tile synlaclic generation module, we use 
ihe \[unclional unification fommlism. At tile same 
lime, we make use of dlc systclnic viewpoirl/ of 
lhe systcrnic function grammar. The rule system is 
made up of many sub-.sysienls such as transitivily 
system, mood system, tense system and voice 
systcllt. The input 111ust depend on all of these 
systems to make difR:rent level decisions. 
In a spoken dialogue Iranslalion system, real= 
lime generation is tile basic requiremenl. As we 
see froln the input as shown in Figure 3, the inlmt 
to the syntaclic generation provides enough 
iuformation about sentence and phrase structme. 
Most of the informatiou in tile input ix instautiatcd, 
such as the verb, the subcategorization frame and 
the phrase members. So the generation engine can 
traverse the input in a top-down, depth-first 
fashion using tmification algorithm (Elhadad 
1992). The whole syntactic generation process is 
described in Figure 5. 
The input is an intermediate representation 
and the output is Chinese texts. The sentence 
unification phase defines the sentence structure 
and orders the components anloDg, tile sentence. 
1144 
The phrase unification phase dcl'ines the phrase 
structure, orders the co~nponenls inside the 
phrases and adds the function words. Unlike 
English, Chinese has no morphological markers 
for tenses and moods. They arc expressed with 
fmlclional words. Selecting functiolml words 
correctly is crilical for Chillesc generation. 
,qelltellCC\[ ~'t "~''~'-t ~unifica|ion ~lst? -- -- !'-;~7~II1 I - tlni fcatiOll \]~CXt 
Figure 5 Sleps of the Synlacfic generator 
The whole unification procedure is: 
,, Unify the input with the grammar at the 
sentence l vel. 
? identify the conslitules inside the inptll 
? Unify the constituents with tile grammar a! the 
phrase level recursively in a top-down, depth- 
first fashion. 
5. Results 
The current version of the system has been 
tested on our hotel reservation corpus (Chengqing 
Zong, 1999). The whole corpus includes about 90 
dialogues, annotated by hand with underspecificd 
semantic representation. I1 contains about 3000 
USRs. Now we have 23 speech acls and about 60 
concepts in lhe corpus. 
The generation lnodulc is tested on all 
sentences in the corpus. And 90% of the generated 
sentences arc rated as grammatically and 
semantically correct. The other 10% are rated as 
wrong because the mood of the sentences i not 
conect. This is mainly caused by the lack of the 
dialogue context. 
6. Discussion and Future Work 
In spoken language translation systems, one 
problem is the ill-formed input. How to tackle this 
problem robustly is very important. At the 
microplanning level, we design some general 
rules. The input is first to be matched with the 
sl~e<:ific roles. If there is no rules matched, we 
access the gene.ral roles to Inalch with the input. In 
this way, although the inl)U! is somehow ill- 
formed, the output includes the main information 
of the input. And at the surface realization level, 
we make some relaxation on tests to improve the 
robuslness, l;,.g, oMigatory arguments may be 
missing in the utterance. This can be caused by 
ellipsis in sentences such as the utterances "{:\]{: ~ 
J~." (stay for three days). We have to accept it as a 
sentence without the subject because they are 
acceptable in spoken Chinese and often appear in 
daily dialogues. 
We arc planning to l:tuther increase the 
robustness of the system. And if possible, we also 
hope to adapt our generation system to other 
(lolnaills. 
Acknowledgements 
The research work described in lhis paper is 
Sulsportcd by the National Natural Science 
t;oundation of China under grant number 
69835030 and by the National '863' Hi-Tcch 
Program under grant nunlber 863-306-ZT03-02-2. 
Thanks also go to several allonyl/lOtlS l'eVieWel'S 
for their valuable comments. 
Reference ;  
Stephan I} uscmann. (1996) Best- first surl'ace 
realization. In t i le Eighth lntcrnatiolml Natural 
l.anguagc Generation Workshop, Sussex, pages 101- 
I10 
Michael Elhadad and Jacques Robin. (1992) 
Controlling Content P.calization with Functional 
Unification Grammars. Aspects of Automated Natural 
Language Generation. t51)89 - 104 
E.Pianta, M.Tovcna. (1999) XIG: Generating from 
Interchange Format Using Mixed Representation. 
AAAI'99 
Ehud Reiter. (1995) NLG vs. Templates. In lhe Fiflh 
Et, ropcan Workshop on Natural Language Generation, 
Leiden, 
Chengqing Zong, Hua Wu, Taiyi Huang, Be Xu. (1999) 
Analysis on Characteristics of Chinese Spoken 
Language. In the Fiflh Natural Language Processing 
Pacific Rim Symposium, 151)358-362 
1145 
Bridging the Gap
Between Dialogue Management and Dialogue Models
Weiqun Xu and Bo Xu and Taiyi Huang and Hairong Xia
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Beijing, 100080, P. R. China
 wqxu, xubo, huang, hrxia@nlpr.ia.ac.cn
Abstract
Why do few working spoken dialogue sys-
tems make use of dialogue models in their
dialogue management? We find out the
causes and propose a generic dialogue
model. It promises to bridge the gap be-
tween practical dialogue management and
(pattern-based) dialogue model through
integrating interaction patterns with the
underling tasks and modeling interaction
patterns via utterance groups using a high
level construct different from dialogue act.
1 Introduction
Due to the rapid progress of speech and language
processing technologies (Cole et al, 1998; Juang
and Furui, 2000), ever-increasing computing power,
and vast quantity of social requirements, spoken di-
alogue systems (SDSs), which promise to provide
natural and ubiquitous access to online information
and service, have become the focus of many research
groups (both academic and industrial) with many
projects sponsored by EU, US (D)ARPA and others
in the past few years (Zue and Glass, 2000; McTear,
2002; Xu, 2001). The last decade saw the emergence
of a great deal of SDSs.
Despite so much progress, some problems still
remain, prominent among which are usability and
reusability (or portability across domains and lan-
guages). Through a survey of typical working spo-
ken (or natural language) dialogue systems in the
nineties (Xu, 2001), we find their central control-
ling component ? dialogue management ? is rela-
tively less well-established than other components.
In most working SDSs, the design of dialogue
management is usually guided by some principles
(den Os et al, 1999), strategies (Souvignier et al,
2000), or objectives (Lamel et al, 2000). In some
even these guidelines are implicit. The problem is
more outstanding in those SDSs developed by the
speech recognition community, in which most work-
ing SDSs come into being. Among many causes,
we think, the most important is that dialogue man-
agement is short of solid theoretical support from
dialogue models (the distinction between dialogue
management and dialogue model will be explicated
in section 2), in addition to the design of SDSs being
a real world problem.
The approach we adopt in building dialogue man-
agement model for SDSs is to study human-human
dialogues solving the same or similar problem.
Though human-computer dialogues may be differ-
ent in some aspects from human-human dialogues,
the design of human-computer dialogue will bene-
fit a lot from the study of human-human dialogues.
It will not be clear whether those that characterize
human-human dialogues are applicable to human-
computer dialogues until they are well studied. Ap-
plicable or not, they are sure to contribute some in-
sights to the design of dialogue management.
In what follows we first inspect main approaches
to dialogue modeling and dialogue management and
find two deep causes behind the gap between them
(section 2). Against the causes we propose a generic
dialogue model which distinguishes five ranks of
discourse units and three levels of dialogue dynam-
     Philadelphia, July 2002, pp. 201-210.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
ics (section 3). Then we apply it to information-
seeking (one of the most common tasks adopted in
the study of SDSs) dialogues and elaborate interac-
tion patterns as utterance groups, which are classi-
fied along two dimensions (initiative and direction
of information flow) into four basic types with some
variations (section 4). We also experiment on seg-
menting utterance groups in our corpus with a sub-
ject and three algorithms.
2 The Gap
Why do most working SDSs make little use of di-
alogue models in their dialogue management? Or,
why is there a gap between dialogue management
and dialogue models?
To make it clear, we first distinguish between di-
alogue models and dialogue management models1,
or equivalently, between dialogue modeling and di-
alogue management modeling.The goal of dialogue
modeling is to develop general theories of (coopera-
tive task-oriented) dialogues and to uncover the uni-
versals in dialogues and, if appropriate, to provide
dialogue management with theoretical support. It
takes an analyzer?s point of view. While the goal
of dialogue management modeling is to integrate di-
alogue model with task model in some specific do-
main to ?develop algorithms and procedures to sup-
port a computer?s participation in a cooperative dia-
logue? (Cohen, 1998, p.204). It takes the viewpoint
of a dialogue system designer.
Next, we briefly overview main approaches to di-
alogue modeling and dialogue management, then
point out the causes behind the gap.
2.1 Dialogue Models
There are mainly two approaches to dialogue mod-
eling: pattern-based and plan-based.2
1The distinction between dialogue models and dialogue
management models is close to what Cohen (1998) makes in
dialogue modeling. He distinguishes ?two related, but at times
conflicting, research goals ... often adopted by researchers of
dialogue?. Roughly speaking, one is theoretical and the other is
practical.
2Cohen (1998) gives a more detailed discussion on dialogue
modeling. Below we draw a lot from there. He mentions ?three
approaches to modeling dialogue ? dialogue grammars, plan-
based models of dialogue, and joint action theories of dialogue?.
We treat joint action theories as further development of original
plan-based approach. So his latter two correspond to our plan-
based approach in general.
Patten-based approach models recurrent interac-
tion patterns or regularities in dialogues at the illo-
cutionary force level of speech acts (Austin, 1962;
Searle, 1969) in terms of dialogue grammar (Sin-
clair and Coulthard, 1975), dialogue/conversational
game (Carlson, 1983; Kowtko et al, 1992; Mann,
2001), or adjacency pairs (Sacks et al, 1974). It
benefits a lot from the insights of discourse analy-
sis (Sinclair and Coulthard, 1975; Coulthard, 1992;
Brown and Yule, 1983) and conversation analysis
(Levinson, 1983).
Plan-based approach relates speech acts per-
formed in utterances to plans and complex mental
states (Cohen and Perrault, 1979; Allen and Perrault,
1980; Lochbaum et al, 2000) and uses AI plan-
ning techniques (Fikes and Nilsson, 1971). Later de-
velopments of plan-based dialogue models include
multilevel plan extension (Litman and Allen, 1987;
Litman and Allen, 1990; Carberry, 1990; Lambert
and Carberry, 1991), theories of joint action (Co-
hen and Levesque, 1991) and SharedPlan (Grosz and
Sidner, 1990; Grosz and Kraus, 1996).
Pattern-based dialogue model describes what hap-
pens in dialogues at the speech act level and cares
little about why. Plan-based dialogue model ex-
plains why agents act in dialogues, but at the ex-
pense of complex representation and reasoning. In
other words, the former is shallow and descrip-
tive and the latter is deep and explanatory. Hul-
stijn (2000) argues for the complementary aspects of
the two approaches and claims that ?dialogue games
are recipes for joint action?.
Since, on the one hand, our target tasks belong to
the class of simple service, like information-seeking
and simple transactions, which are relatively well-
structured and well-defined and not too complex for
pattern-based dialogue models, on the other hand,
there are some significant problems in using plan-
based models in practical SDSs ? those of ?knowl-
edge representation, knowledge engineering, com-
putational complexity, and noisy input? (Allen et
al., 2000), we will choose pattern-based instead of
plan-based dialogue model as our theoretical basis
for practical dialogue management at present.
2.2 Dialogue Management Models
We view dialogue management as an organic com-
bination of dialogue model with task model in some
specific domain. Its basic functionalities include in-
terpretation in context, generation in context, task
management, interaction management, choice of di-
alogue strategies, and context management. All
of them require contextual (linguistic and/or world)
knowledge.
According to how task model and dialogue model
are used, approaches to dialogue management can
be classified into four categories3 in Table 1.
Table 1: Classifying dialogue management models
Task Model
implicit explicit
Dialogue implicit DITI DITE
Model explicit DETI DETE
DITI or graph-based, both dialogue model and task
model are implicit. Dialogue is controlled via
finite state transitions (McTear, 1998). Topic
flow is predetermined. It is neither flexible nor
natural, but simple and efficient. It?s suitable
for simple and well-structured tasks similar to
automated services over ATMs or telephones
with DTMF input.
DITE or frame-based, with no explicit dialogue
model, but task is explicitly represented as a
frame or a form (Goddeau et al, 1996), a task
description table (Lin et al, 1998), a topic for-
est (Wu et al, 2000), or an agenda (Xu and
Rudnicky, 2000), etc. Both system and user
may take the initiative. Topic flow is not prede-
termined. It?s more flexible than that of DITI,
but still far from naturalness and friendliness,
since it makes no explicit use of dialogue mod-
els. Most working SDSs adopt this way of dia-
logue management.
DETI there is no practical dialogue management
3For a more comprehensive discussion on dialogue man-
agement (and SDSs), see (McTear, 2002). He identifies two
aspects of dialogue control (i.e., dialogue management) ? ini-
tiative and flow of dialogue, and three strategies for dialogue
control ? finite-state-based, frame-based, and agent-based. The
first two are similar to DITI and DITE respectively and the third
is a collection of some other approaches which are now hardly
applicable for practical dialogue management, among which is
plan-based. Our classification below is more clear.
using such a combination of task model and di-
alogue model.
DETE both dialogue model and task model are
explicit. This type of dialogue management
shares the advantages of frame-based one. At
the same time it is potential to allow of more
natural interactions according to the dialogue
model used. This is what we are after here.
2.3 The Causes behind the Gap
From the analysis above we can see the surface
gap between (DITE) dialogue management in most
working SDSs and (pattern-based) dialogue models
is mainly due to a deep one, i.e., the one between
dialogue models and the underlying tasks.
There is another important cause ? the interaction
patterns are described at the level of speech act or
dialogue act.4 To link dialogue acts to utterances,
three problems5 must be addressed at the same time:
  Dialogue act classification scheme and its reli-
ability in coding corpus, (Carletta et al, 1997;
Allen and Core, 1997; Traum, 1999);
  Choice of features/cues that can support auto-
matic dialogue act identification, including lex-
ical, syntactic, prosodic, collocational, and dis-
course cues;
  A model that correlates dialogue acts with
those features.
Some of the problems are discussed in (Jurafsky et
al., 1998; Stolcke et al, 2000; Jurafsky and Martin,
2000; Jurafsky, 2002). The empirical work on dia-
logue act classification and recognition did not begin
until some dialogue corpora (like Map Task, Verb-
mobil, TRAINS, and our NLPR-TI) were available.
But how could dialogue act recognition be suc-
cessfully applied to practical dialogue management
remains to be seen. So we choose a higher level
4Following Jurafsky (2002), we will adopt the term dia-
logue act, which captures the illocutionary force or commuca-
tive function of speech act. Though there are some arguments
in (Levinson, 1983) and others against using dialogue act to
model dialogues, and there are indeed some unresolved prob-
lems in linking dialogue acts to utterances, it will be our choice
for the time being.
5We extend Webber?s (2001) idea by splitting feature choice
out.
construct (UT-3, see section 3.1.3) to describe inter-
action patterns instead. We are by no means deny-
ing the important role dialogue act plays in dialogue
modeling, but try to incorporate high level knowl-
edge into dialogue modeling.
3 The Bridge ? GDM
Against the above gap and its causes we propose a
generic dialogue model (GDM) for task-oriented di-
alogues, which consists of five ranks of discourse
units and three levels of dialogue dynamics. It cap-
tures two important aspects of task-oriented dia-
logue ? interaction patterns at the low level and un-
derlying task at the high level.
3.1 Discourse Units
We distinguish five ranks of discourse units in de-
scribing task-oriented dialogues: dialogue, phase,
transaction, utterance group, and utterance.
3.1.1 Dialogue, Phase, and Transaction
The overall organization of a typical task-oriented
dialogue can be divided into three phases, namely,
an opening phase, a closing phase, and between
them a problem-solving phase, which can be subdi-
vided into transactions depending on how the under-
lying task is divided into subtasks. Each subtask cor-
responds to a transaction. If a task is atomic, there
will be only one transaction in the problem-solving
phase, just like the task of tourism information-
seeking.
3.1.2 Utterance Group
In performing a subtask (or task, if atomic), some
interaction patterns will recur. We name the interac-
tion patterns utterance groups (or groups, for short).
It?s also called exchanges or conversational games
(see section 2.1). The unit at this level involves com-
plex grounding process towards common ground or
mutual knowledge (Clark and Schaefer, 1989; Clark,
1996; Traum, 1994).
3.1.3 Utterance
The elementary unit in our model is utterance.
Every utterance either initiates a new group, contin-
ues, or ends an old one. Usually it is what a speaker
utters in his/her turn (for simplification, overlaps
will not be considered here). But there are some
turns with two or more utterances. These multi-
utterance turns usually end an old group with their
first utterance and begin a new one with their last ut-
terance. Similar observations are found in Verbmo-
bil corpus (Alexandersson and Reithinger, 1997).
Each utterance can be analyzed at three levels
and assigned a type correspondingly (utterance type,
UT):
UT-1 sentence type or mood, i.e., declarative, im-
perative, and interrogative (including yes-no
question (ynq), wh-question (whq), alterna-
tive question (atq), disjunctive question (djq),
which can be identified using surface lexical
and prosodic features).
UT-2 dialogue act, see section 2.3.
UT-3 a more general communicative function, rel-
ative to a group, of a small number, including
initiative (I), response/reply (R), feedback (F),
acknowledgement (A) (typical in information-
seeking dialogues), and others. It can be iden-
tified using UT-1 and semantic content (or ut-
terance topic) and preceding UT-3s, It is at this
level that interaction patterns are more obvi-
ous. What?s more, it can be recognized without
UT-2 (dialogue act) but contribute to dialogue
act recognition.
3.2 Dialogue Dynamics
By dialogue dynamics, we mean the dynamic pro-
cess within dialogues, i.e., how dialogues flow from
one partner?s utterance to another?s all the way till
the closing. The dynamic process includes that of
intra-utterance (micro-dynamics) and that of inter-
utterance. Inter-utterance dynamics is further di-
vided into intra-group dynamics (meso-dynamics)
and inter-group dynamics (macro-dynamics).
3.2.1 Micro-dynamics
Micro-dynamics deals with how discourse phe-
nomena (like anaphora, ellipsis, etc.) within one
utterance are decoded (interpretation) or encoded
(generation) in discourse context and how utterance
level intention (dialogue act) is recognized using
lexical, prosodic, and other cues and discourse struc-
ture (see section 2.3). Discourse phenomena contain
much discourse-level context information. It is those
that contribute partly to the naturalness and coher-
ence in human-human dialogues. But it?s very dif-
ficult for computers to make full use of them, either
in interpretation or in generation. They are imple-
mented in few of present SDSs, though much effort
has been put on the study of computational models
of discourse phenomena (see (Webber, 2001) for an
overview and references therein for further details).
3.2.2 Meso-dynamics
Meso-dynamics explains utterance-to-utterance
moves within one group which present recurrent
interaction patterns. Our corpus study shows that
those patterns in information-seeking dialogues are
closely related to two factors ? initiative and direc-
tion of information flow between user and server
(see section 4.1).
3.2.3 Macro-dynamics
Macro-dynamics describes inter-group moves,
which may take place intra-transactionally within
one subtask or inter-transactionally between sub-
tasks. Inter-group moves are subject to the under-
lying task. It?s difficult to give an account like intra-
group moves, because they reflect the process how
a problem is solved.The account depends on how
tasks are represented and reasoned. We may gain
some hints from the study of general problem solv-
ing in AI (Bonet and Geffner, 2001).
3.3 Discussion
GDM as we propose above is a DETE dialogue man-
agement framework with fine-grained patterns. We
discuss related work and its implication for dialogue
management below.
3.3.1 Discourse Unit
Different discourse units are used by different re-
searchers in studying discourse. In (Sinclair and
Coulthard, 1975), five ranks of units are used to ana-
lyze classroom interactions: lesson, transaction, ex-
change, move, and act. The first four roughly cor-
respond to our dialogue, transaction, group, utter-
ance. We add the unit phase and omit act, which
is a sub-utterance unit. In (Alexandersson and Re-
ithinger, 1997), four ranks of units are used to ana-
lyze meeting scheduling dialogues: dialogue, phase,
turn, and dialogue act. Turn is a natural unit that
appears in dialogues, but is it an basic unit? Four
units with conversation acts (Traum and Hinkelman,
1992; Traum, 1994), are used to analyze TRAINS
(freight scheduling) dialogues: multiple discourse
unit (argumentation act), discourse unit (core speech
act), utterance unit (grounding act), sub-utterance
unit (turn-taking act). Theirs differ a lot from ours
partly because they pay more attention to grounding.
3.3.2 Discourse Structure
In GDM the structure of discourse6 is accounted
for from two aspects: local structure is reflected
in utterance groups and shaped by meso-dynamics;
global structure is determined by the underlying task
and shaped by macro-dynamics. This is obvious to
task-oriented dialogues in view of GDM.
3.3.3 Dialogue Strategies
In most working SDSs dialogue strategies are
handcrafted by system developers. Recently there
are some efforts in applying machine learning ap-
proaches to the acquisition of dialogue strategies
(Walker, 2000; Levin et al, 2000). We hope to
find out what strategies are used in human-human
dialogue and how they could be applied to human-
computer dialogue. We first refine the concept of
dialogue strategies. From the view of GDM, the
strategies a dialogue agent may choose can also be
classified into three levels, i.e.,
Micro-level strategies how to realize information
structure, anaphora, ellipsis, and others, in ut-
terances,
Meso-level strategies what to say regarding current
group status, so as to complete ongoing group
more friendly,
Macro-level strategies how to choose discourse
topic regarding current task status, so as to
complete the underlying task more efficiently.
6Grosz and Sidner (1986) proposed a tripartite discourse
model consisting of attentional state, intentional structure, and
linguistic structure. It is influential and covers both dialogue
and text. But their intentional structure fails to capture the dis-
tinction between global level and local level structure. Their
discourse unit ? discourse segment ? is used without noticing
that there are different ranks of discourse unit in dialogues. This
is partly due to that they looked more at the similarities between
dialogue and text and less at the differences between them. Di-
alogue and text, as two types of discourse, share something in
common, but there is also something that makes them different.
3.3.4 The Complexity of Dialogue Management
Since dialogue management is closely related to
dialogue model and underlying task and domain,
the complexity of dialogue management can be de-
composed into three parts, i.e., the complexity of
dialogue model, the complexity of task, and the
complexity of domain. The complexity of dialogue
model is affected by what kind of initiative and dia-
logue phenomena are allowed. The task complexity
is affected by the number of its possible actions and
by whether it is well-structured and well-defined.
The domain complexity is affected by domain en-
tities and their relations and by the volume of in-
formation. The three are not independent but inter-
twined.
4 Utterance Groups in GDM-IS
We now apply GDM to information-seeking dia-
logues (GDM-IS) and search for interaction patterns
in the NLPR-TI corpus. We first try to classify and
segment utterance groups. This is a preliminary step
toward group pattern recognition. Details of the
recognition process and results will be given in (Xu,
2002).
4.1 Group Classification
Group patterns are recurrent, but how many? Or,
is there a limited number? In our NLPR-TI corpus
information-seeking dialogues (see section 4.2.1),
we find four basic groups with some variations.
4.1.1 Basic Groups
The recurrent patterns, according to our observa-
tion, can be classified into one of the four types in
Table 2 along two dimensions ? initiative and the di-
rection of information flow (determined using world
knowledge in the domain).
Table 2: Basic utterance groups
Information Flow
S  U U  S
Group User UISU UIUS
Initiative Server SISU SIUS
Direction of information flow In the dialogues
of information-seeking, there are two directions of
information flow: one from user to server (U  S)
and the other from server to user (S  U). In the
tourism domain, the former includes intended route
(or sight-spot, or a rough area, obligatory), intended
start time, number of tourists (optional); the latter in-
cludes start time, duration, vehicle, price, accommo-
dation, meal, schedule, and more. Server must know
the information about user?s intended route before
providing user with other information.
Initiative7 In GDM, initiative always starts a new
utterance group. It is one of utterance?s general com-
municative functions relative to a group, together
with reply, feedback, acknowledgement, as we men-
tion in section 3.1.3. Regarding one group topic
there are user initiatives (UI) and there are server ini-
tiatives (SI). Group patterns depend heavily on who
initiates the group regarding some specific topic.
This is due to the role asymmetry of the dialogue
partners.
4.1.2 Complex Groups
Though most groups can be covered by the above
basic patterns, there are some exceptions which are
more complex. They are usually embedded ones.
When one partner signals non-understanding or non-
hearing, or a normal group is suspended, one or two
more utterances will be inserted, either to repeat pre-
vious utterance or resume suspended group. The
embedded groups may also be precondition groups.
Precondition groups occur when some obligatory in-
formation is missing before the salient issue could
be addressed. Once the missing is provided, the
outer group will continue. Complex groups can also
occur when one partner lists more than one items or
does some repairing.
4.2 Group Segmentation
Given the above group classification, how to rec-
ognize them? We have to segment and classify
groups, and determine UT-3 of every utterance
within groups. This is a big problem. Only the ex-
periment on group segmentation is reported in this
paper.
7We note that there are task initiative and dialogue initia-
tive (Chu-Carroll and Brown, 1998) and there are local initiative
and global initiative (Rich and Sidner, 1998). Our initiative-in-
group is more task-related and global. For a comprehensive dis-
cussion on mixed initiative interaction, see (Haller and McRoy,
1998, 1999).
To segment a dialogue into groups is first to deter-
mine the beginning of a group, i.e., to determine if
an utterance is an initiative or not. (Multi-utterance
turns are manually segmented beforehand for sim-
plification.)
4.2.1 NLPR-TI Corpus
We use NLPR-TI corpus (Xu et al, 1999) in the
experiment. It consists of 60 spontaneous human-
human dialogues (about 5.5 hours) over telephones
on tourism information-seeking. There are total
2716 turns (1346 by the user and 1370 by the
server). The average length of user?s turns is about
7 Chinese characters and server?s about 9. The first
20 dialogues (transcript) are used for current group
segmentation.
4.2.2 Manual Segmentation
A subject was given the basic ideas about GDM
and utterance groups in GDM-IS and segmented two
dialogues with an expert?s guide before starting the
work.
To test the reliability of group segmentation
within GDM-IS, we calculate the kappa coefficient
()8 (Carletta, 1996; Carletta et al, 1997; Flam-
mia, 1998) to measure pairwise agreement between
the subject and the expert. Two coders segmented
the first 20 dialogues (totally 845 utterances). They
reached       , which shows a high reliabil-
ity. Using the expert?s segmentation as reference, we
also measure the subject?s segmentation using infor-
mation retrieval metrics ? precision (P), recall (R),
and F-measure9 (see Table 3 for the result).
4.2.3 Automated Segmentation
Three simple algorithms in Figure 1 are used to
perform the same task on the 20 dialogues. The in-
put is a semantic tag sequence produced by a statis-
tical parser (Deng et al, 2000)10.
8
             , where   is the
proportion of times that the coders agree and   is the pro-
portion of times that one would expect them to agree by chance.
? From (Carletta, 1996)
9Combined metric          , from
(Jurafsky and Martin, 2000, p.578),    .
10That we adopt such deep features in discourse segmenta-
tion is mainly due to our target application ? dialogue manage-
ment. This makes it different from others using surface features
like (Passonneau and Litman, 1997).
I. Using topic only for segmentation
if topic is new
then UT-3 = initiative
else UT-3 = non-initiative
II. Using UT-1 only for segmentation
if UT-1  interrogatives
then UT-3 = initiative
else UT-3 = non-initiative
III. Using both for segmentation
if topic is new  UT-1  interrogatives
then UT-3 = initiative
else UT-3 = non-initiative
Figure 1: Group segmentation algorithms
Given the semantic tag sequence of an utterance,
we determine its topic11 and UT-1 (what we are most
interested in is interrogatives (ynq, whq, atq, and
djq)). Since the parser performs with an error rate
of , there will be some wrong semantic tags,
which lead to errors in assigning UT-1 and topic.
Then we use the three simple algorithms to seg-
ment groups in the 20 dialogues. Their performance
(also using the expert?s segmentation as reference)
is given in Table 3.
Table 3: Group segmentation results
subject I II III
Precision .88 .59 .67 .83
Recall .92 .82 .62 .56
F-measure .90 .69 .64 .67
4.3 Discussion
Table 3 shows the results of group segmentation,
both manual and automated. Though none of the
three algorithms outperforms the subject, they all
show that topic change and UT-1 as interrogative
are acceptable and also good indicators of utterance
group beginning, esp. when topic and UT-1 are the
11We presume that the topic of an utterance is the last one in
the candidate tags. This seems problematic but is true to most of
the utterances according to our observation. How to determine
the topic of an utterance needs further study.
only information sources and when discourse mark-
ers (Schiffrin, 1987) in spontaneous speech are un-
available in current deep analysis.
There is no obvious performance difference in
segmenting dialogue into groups with the three al-
gorithms. The performance of algorithm I may be
improved if the noises brought by the parser and
our simple topic identification algorithm are cleared.
This implies that topic change is a potentially bet-
ter indicator of the beginning of new groups. The
result using UT-1 only is the worst. This is partly
because not all groups begin with interrogatives and
that interrogatives do not always occur at the begin-
ning of a group. When using both topic and UT-1,
the performance changes little, though seemly more
constraints are used. This possibly is because topic
change and UT-1 as interrogative overlap a lot.
5 Conclusions
After a survey of the main approaches to dialogue
modeling and dialogue management in working
SDSs, we find the causes behind the gap between
practical dialogue management and dialogue models
and propose GDM, which consists of five ranks of
discourse units and three levels of dialogue dynam-
ics. It promises to bridge the gap through integrat-
ing meso-dynamics at the group level with macro-
dynamics at the task level, and modeling interaction
patterns via utterance groups using UT-3.
Then we apply it to information-seeking dia-
logues and elaborate utterance groups (or interaction
patterns) in the model. We also classify and seg-
ment utterance groups in our information-seeking
corpus, which takes a preliminary step toward bet-
ter dialogue modeling for practical dialogue man-
agement with empirical justification. A more chal-
lenging task ? group pattern recognition ? is under
way (Xu, 2002). After that we will investigate how
local discourse structure in terms of utterance group
structure could contribute to the recognition of dia-
logue act (UT-2).
GDM takes a step further toward better dialogue
modeling for practical dialogue management with
empirical justification. It is expected to be used in
practical dialogue management in the near future for
better usability and portability.
Acknowledgments
The work described in this paper was partly sup-
ported by the National Key Fundamental Research
Program (the 973 Program) of China under the grant
G19980300504 and the National Natural Science
Foundation of China under the grant 69835003.
References
Jan Alexandersson and Norbert Reithinger. 1997. Learn-
ing dialogue structures from a corpus. In Proceedings
of the 5th European Conference on Speech Communi-
cation and Technology, volume 4, pages 2231?2234.
James Allen and Mark Core. 1997. Draft of damsl:
Dialog act markup in several layers. Available from
http://www.cs.rochester.edu/research/
cisd/resources/damsl/.
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15(3):143?178.
James Allen, George Ferguson, Bradford W. Miller,
Eric K. Ringger, and Teresa Sikorski Zollo, 2000. Di-
alogue Systems: From Theory to Practice in TRAINS-
96, chapter 14, pages 347?376. In Dale et al (Dale et
al., 2000).
J. L. Austin. 1962. How to do Things with Words.
Clarendon Press, Oxford.
Blai Bonet and He?ctor Geffner. 2001. Planning and Con-
trol in Artificial Intelligence: A Unifying Perspective.
Applied Intelligence, 14(3):237?252.
Gillian Brown and George Yule. 1983. Discourse Anal-
ysis. Cambridge University Press.
Sandra Carberry. 1990. Plan Recognition in Natural
Language Dialogue. ACL-MIT Press Series in Nat-
ural Language Processing. A Bradford book, MIT
Press, Cambridge, Massachusetts.
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-
Sneddon, and A. H. Anderson. 1997. The reliability
of a dialogue structure coding scheme. Computational
Linguistics, 23(1):13?31.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: The Kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Lari Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel, Dordrecht, Holland.
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User-
Adapted Interaction, 8(3-4):215?253.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?294.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Philip Cohen, Jerry Morgan, and Martha Pollack , editors.
1990. Intentions in Communication. MIT Press.
P. R. Cohen and H. J. Levesque. 1991. Teamwork. Nou?s,
25(4):487?512.
P R. Cohen and C. R. Perrault 1979. Elements of a
plan-based theory of speech acts. Cognitive Science,
3(3):177?212.
Phil Cohen, 1998. Dialogue Modeling, chapter 6.3. In
Cole et al (Cole et al, 1998).
Ronald Cole, Joseph Mariani, Hans Uszkoreit, Giovanni
Varile, Annie Zaenen, Antonio Zampolli, and Victor
Zue, editors. 1998. Survey of the State of the Art in
Human Language Technology. Cambridge University
Press, Cambridge.
Malcolm Coulthard, editor. 1992. Advances in Spoken
Discourse Analysis. Routledge. London.
Robert Dale, Hermann Moisl, and Harold Somers, edi-
tors. 2000. Handbook of Natural Language Process-
ing. Marcel Dekker. New York.
Yunbin Deng, Bo Xu, and Taiyi Huang. 2000. Chi-
nese spoken language understanding across domain.
In Proceedings of the 6th International Conference on
Spoken Language Processing, volume 1, pages 230?
233.
Richard Fikes and Nils J. Nilsson. 1971. STRIPS: A
new approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2(3-4):189?
208.
Giovanni Flammia. 1998. Discourse segmentation of
spoken dialogue: an empirical approach. Ph.D. the-
sis, MIT.
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and
S. Busayapongchai. 1996. A form-based dialogue
manager for spoken language applications. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing, volume 2, pages 701?704.
Barbara J. Grosz and Sarit Kraus. 1996. Collaborative
plans for complex group action. Artificial Intelligence,
86(2):269?357.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention, and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Barbara J. Grosz and Candace L. Sidner. 1990. Plans for
Discourse. In Cohen et al (Cohen et al, 1990).
Susan Haller and Susan McRoy, editors. 1998, 1999.
User Modeling and User-Adapted Interaction, Special
Issue on Computational Models for Mixed Initiative
Interaction, 8(3-4),9(1-2).
Joris Hulstijn. 2000. Dialogue games are recipes for joint
action. In Proceedings of the Forth Workshop on the
Semantics and Pragmatics of Dialogue (Gotalog?00).
Biing-Hwang Juang and Sadaoki Furui, editors. 2000.
Proceedings of the IEEE , Special Issue on Spoken
Language Processing, 88(8).
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Switchboard discourse language
modeling project report. Technical Report Research
Note No. 30, Center for Speech and Language Pro-
cessing, Johns Hopkins University, Baltimore, MD.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics. Prentice-Hall.
Daniel Jurafsky, 2002. Pragmatics and Computational
Linguistics. To appear in Laurence R. Horn and Gre-
gory Ward, editors. Handbook of Pragmatics. Black-
well, Oxford.
J. Kowtko, S. Isard, and G. M. Doherty. 1992. Conver-
sational games within dialogue. Research Paper 31,
Human Communication Research Centre, Edinburgh
University, Edinburgh.
Lynn Lambert and Sandra Carberry. 1991. A tripartite
plan-based model of dialogue. In Proceedings of the
29th Annual Meeting of the Association for Computa-
tional Linguistics, pages 47?54, Berkeley, CA.
L. Lamel, S. Rosset, J.L. Gauvain, S. Bennacef,
M. Garnier-Rizet, and B. Prouts. 2000. The LIMSI
ARISE system. Speech Communication, 31(4):339?
354.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine interac-
tion for learning dialog strategies. IEEE Transactions
on Speech and Audio Processing, 8(1):11?24.
Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press.
Y. Lin, T. Chiang, H. Wang, C. Peng, and C. Chang.
1998. The design of a multi-domain mandarin Chinese
spoken dialogue system. In Proceedings of the 5th In-
ternational Conference on Spoken Language Process-
ing, volume 1, pages 230?233.
Diane J. Litman and James F. Allen. 1987. A plan recog-
nition model for subdialogues in conversation. Cogni-
tive Science, 11(2):163?200.
Diane J. Litman and James F. Allen. 1990. Discourse
Processing and Commonsense Plans. In Cohen et al
(Cohen et al, 1990).
Karen E. Lochbaum, Barbara J. Grosz, and Candace L.
Sidner, 2000. Discourse Structure and Intention
Recognition, chapter 6, pages 123?146. In Dale et al
(Dale et al, 2000).
William C. Mann. 2001. The genre diversity of dia-
logue game theory. Available from http://www-
rcf.usc.edu/ billmann/memos.htm.
Michael F McTear. 1998. Modelling spoken dialogues
with state transition diagrams: experiences with the
CSLU toolkit. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 1223?1226.
Michael F. McTear. 2002. Spoken dialogue technology:
Enabling the conversational user interface. ACM Com-
puting Surveys,34(1):90?169.
Els den Os, Lou Boves, Lori Lamel, and Paolo Baggia.
1999. Overview of the ARISE project. In Proceedings
of the 6th European Conference on Speech Communi-
cation and Technology, volume 4, pages 1527?1530.
Rebecca Passonneau and Diane Litman. 1997. Discourse
segmentation by human and automated means. Com-
putational Linguistics, 23(1):103?140.
Charles Rich and Candace L. Sidner. 1998. Colla-
gen: A collaboration manager for software interface
agents. User Modeling and User-Adapted Interaction,
8(3-4):315?350.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press.
J. R. Searle. 1969. Speech Acts. Cambridge University
Press.
John M. Sinclair and Malcolm Coulthard. 1975. Towards
an Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press.
Bernd Souvignier, Andreas Kellner, Bernhard Rueber,
Hauke Schramm, and Frank Seide. 2000. The
thoughtful elephant: Strategies for spoken dialog sys-
tems. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):51?62.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?371.
David R. Traum and Elizabeth A. Hinkelman. 1992.
Conversation acts in task-oriented spoken dialogue.
Computational Intelligence, 8(3):575?599.
David R. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversation. Ph.D.
thesis, University of Rochester.
David R. Traum. 1999. 20 questions for dialogue act tax-
onomies. In Proceedings of the Third Workshop on the
Semantics and Pragmatics of Dialogue (Amstelog?99).
Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Artificial
Intelligence Research, 12:387?416.
Bonnie Webber. 2001. Computational Perspectives on
Discourse and Dialogue. In Deborah Schiffrin, Deb-
orah Tannen, and Heidi Hamilton, editors. Handbook
of Discourse Analysis. Blackwell, Oxford.
Xiaojun Wu, Fang Zheng, and Mingxing Xu. 2000.
Topic forest: A plan-based dialog management struc-
ture. In Proceedings of ICASSP, volume 1, pages 617?
620.
Wei Xu and Alexander I. Rudnicky. 2000. Task-based di-
alog management using an agenda. In Proceedings of
ANLP/NAACL 2000 Workshop on Conversational Sys-
tems, pages 42?47.
B. Xu, T.Y. Huang, X. Zhang, and C. Huang. 1999. A
Chinese spoken dialogue database and its application
for travel routine information retrieval. In Proceed-
ings of the Second International Workshop on East-
Asia Language Resources and Evaluation, Taipei.
Weiqun Xu. 2001. Survey of the state of the art in spoken
dialogue systems. Manuscript.
Weiqun Xu. 2002. Grouping utterances in information-
seeking dialogues. In preparation.
Victor Zue and Jim Glass. 2000. Conversational inter-
faces: Advances and challenges. Proceedings of the
IEEE, 88(8):1166?1180.
Interactive Chinese-to-English Speech Translation Based on  
Dialogue Management 
 Chengqing Zong, Bo Xu, and Taiyi Huang 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
P. O. Box 2728, Beijing 100080, China 
{cqzong, xubo, huang}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In this paper, we propose a novel 
paradigm for the Chinese-to-English 
speech-to-speech (S2S) translation, which 
is interactive under the guidance of 
dialogue management. In this approach, 
the input utterance is first pre-processed 
and then serially translated by the 
template-based translator and the inter-
lingua based translator. The dialogue 
management mechanism (DMM) is 
employed to supervise the interactive 
analysis for disambiguation of the input. 
The interaction is led by the system, so 
the system always acts on its own 
initiative in the interactive procedure. In 
this approach, the complicated semantic 
analysis is not involved. 
1 Introduction 
2 
Over the past decade, many approaches to S2S 
translation have been proposed. Unfortunately, the 
S2S translation systems still suffer from the poor 
performance, even though the application domains 
are restricted. The common questions are: what 
translation strategies are necessary? What do the 
problems exist in the current S2S systems? And 
what performance of a system is acceptable? 
Based on the questions, we have analyzed the 
current approaches to machine translation (MT) 
and investigated some experimental systems and 
the user?s requirements. A novel paradigm for the 
Chinese-to-English S2S translation has been 
proposed, which is interactive under the guidance 
of DMM. In this approach, the input utterance is 
first pre-processed and serially translated by the 
template-based translator and the inter-lingua 
based translator. If the two translators are failed to 
translate the input, the dialogue management 
mechanism is brought into play to supervise the 
interactive analysis for disambiguation of the input. 
The interaction is led by the system, so the system 
always acts on its own initiative in the interactive 
procedure. In this approach, the complicated 
semantic analysis is not involved.  
Remainder of the paper presents our 
motivations and the proposal scheme in detail. 
Section 2 gives analysis on the current MT 
approaches and the user?s requirements. Section 3 
describes in detail our approach to Chinese-to-
English S2S translation. Section 4 draws 
conclusions and presents the future work. 
Analysis on MT approaches and S2S 
translation systems 
2.1 Analysis on MT approaches 
In the past decades, many MT approaches have 
been proposed. We roughly divided the current 
approaches into two types, which are respectively 
named as the mainstream approaches and the non-
mainstream approaches. The mainstream 
approaches include four basic methods: the 
analysis-based method, the example-based method, 
the template-based method and also the statistical 
method as well. The analysis-based method here 
includes the rule-based method, the inter-lingual 
method, or even the knowledge-based method. In 
the recent years, the approach based on multi-
engine has been practiced in many systems (Lavie, 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 61-68.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
1999; Wahlster, 2000; Zong, 2000a). However, the 
engines employed in these experimental systems 
are mainly based on the four mainstream methods. 
The strong points and the weak points of the four 
methods have been analyzed in many works (Zong, 
1999; Ren, 1999; Zhao, 2000). 
The non-mainstream approach here refers to 
any other methods exclusive of the four methods 
mentioned above. To improve the performance of 
MT systems, especially to cope with the specific 
problems in S2S translation, many schemes have 
been proposed. Ren (1999) proposed a super-
function based MT method, which tries to address 
the MT users? requests and translates the input 
without thorough syntactic and semantic analysis. 
The super-function based MT system is fast, 
inexpensive, easy to control and easy to update. 
However, the fluency and the correctness of the 
translation results are usual not high. Moreover, to 
extract the practical super-functions from the 
corpus is also a hard work. Yamamoto et al (2001) 
proposed a paradigm named Sandglass. In the 
sandglass system, the input utterances from a 
speech recognizer are paraphrased firstly, and the 
paraphrased text is passed to the transfer controller. 
The task of the paraphrasing module for the source 
language is to deal with noisy inputs from the 
speech recognizer and provides different 
expressions of the input. An obvious question 
about the Sandglass is why the system would 
rather rewrite the input than to translate it directly?  
Zong et al (2000b) proposed an MT method based 
on the simple expression. In the method the 
keywords in an input utterances are spotted out 
firstly and the dependence relation among the 
keywords are analyzed. Then, the translation 
module searches the examples in the knowledge 
base according to the keywords and their 
dependence relation. If an example is matched with 
the conditions, the target language expression of 
the example is sent out as the translation result of 
the input. When the input is not very long, and the 
domain and the type of the input are restricted, the 
method is very practical. However, to develop the 
knowledge base with dependence relation of 
keywords and to match an input with all examples 
in the knowledge base are sometimes difficult. 
Wakita et al (1997) proposed a robust translation 
method which locally extracts only reliable parts, 
i.e., those within the semantic distance threshold 
and over some word length. This technique, 
however, does not split input into units globally, or 
sometimes does not output any translation result 
(Furuse et al 1998). In addition, the method 
closely lies on the semantic computation, and 
sometimes it is hard to compute the semantic 
distance for the spoken utterances. 
In summary, both mainstream MT methods and 
non-mainstream methods have been practiced in 
many experimental S2S translation systems. 
However, all methods mentioned above are 
unilateral and based on user's own wishful thinking. 
The system is passive and blind in some extent. 
The task that machine translates is imposed by 
human, and some problems are also brought by the 
speaker, e.g., the topics are changed casually, or 
the ill-formed expressions are uttered. In these 
cases, it is unreasonable to expect the system to get 
the correct translation results, but not to give the 
system any rights to ask the speaker about his or 
her intention or some ambiguous words. In fact, if 
we examine the procedures that human interpreters 
use, we can see that the translation is usually 
interactive. When an interpreter is unable to 
directly translate an utterance due to an ill-formed 
expression or something even worse, the 
interpreter may have to ask the speaker to repeat or 
explain his / her words. Based on the ideas, the 
interactive paradigms for S2S translation have 
been proposed (Blanchon, 1996; Waibel, 1996; 
Seligman, 1997; Seligman, 2000; Ren, 2000). 
Seligman (2000) proposed a ? quick and dirty? or 
?low road? scheme, in which he suggested that, by 
stressing interactive disambiguation, practically 
usable speech translation systems may be 
constructable in the near term. In addition, two 
interactive MT demos were shown respectively in 
1997 and 1998 (Seligman, 2000). However, all the 
proposed interactive schemes and the demos put 
the emphasis on the interface between speech 
recognition (SR) and analysis. The interface can be 
supplied entirely by the user, who can correct SR 
results before passing them to translation 
components. That means the translation system is 
still passive. Actually, as we know that the parsing 
results and the translation results are not certainly 
correct even though the input is completely correct, 
but some noisy words usually have not any 
influence whether they are correct or not. In this 
sense, the user should know what the system needs? 
And what brought the system ambiguity? This 
means, the system has rights and obligations to tell 
the user what the system want to know. In another 
words, the system necessitates a DMM to guide the 
interaction between the system and user, and 
sometimes the system should play the leading role.  
2.2 Analysis on user?s requirements 
Although much progress in SR and spoken 
language parsing has been made, there is still a 
long way to reach the final and ideal goal that the 
translation results are complete correct. In this 
situation, let?s think does a user always need the 
complete correct translation results? Please see the 
following three examples: 
(1) Input: ?????? ????????
?????????????? (Oh, 
that ? well, please reserve a single room 
for me, sure, a single room.) 
In the input, there are many redundant words, 
such as, ?(Oh)???(that), ???(well) and so 
on. If all words in the input are translated, the 
translation result is verbose and wordy. In fact, in 
the input only three keywords are useful, which are: 
??(reserve), ??(one), and ???(single room) 
as well. The preposition phrase ???(for me)? is 
not  obligatory. Even the word ???? is also not 
obligatory.  
(2) Input: ? ? ? ? ? ?? ? ?(Is this ? 
Xiang Ge Li La? Hotel?) 
In the example, the four characters with 
underline are originally a hotel name ????
??(Shangri-la), but they are wrong transliterated 
and separated due to the absence of the word in the 
SR dictionary. In this case, it is impossible to 
correctly parse the input without user?s help.  
(3) Input: ??? ? ?? ? ?? ?? ???(Is 
there any ? ask ... have? route to 
Huangshan mountain?) 
The input is a result of the SR component. 
Obviously, in the input two characters with 
stressing dots are wrong recognized from the 
original word ???  (tour)?. In this case, if all 
words are translated, the results will be 
inconceivable. On the contrary, the result is quite 
understandable if the two characters with stressing 
dots are omitted or ignored.  
The example (1) shows that if the input is 
recognized completely correct, the parsing result is 
still probably wrong due to the ill-formed 
expression of the input. The example (2) means 
that it is impossible to correctly parse the input due 
to the unknown word and its incorrect recognition. 
The example (3) shows that even though the 
expression is formal and there is not any unknown 
word in the input, the result of SR is still probably 
wrong. The parser is impossible to correctly 
analyze the wrong SR result.  
From the three examples we can easily get the 
following standpoints: a) the user expects his or 
her intentions to be translated rather than his (her) 
all words. The keywords and their dependence 
relations are the main objects to hold the user?s 
intentions. b) For the translation component, it is 
not indispensable to correct all mistakes in the 
input from the SR component. c) If the parser is 
failed to parse the input, and the system only 
translates the keywords, the translation results may 
be still understandable and acceptable. 
3 Interactive translation based on 
dialogue management 
3.1 Overview of the paradigm 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Utterances
SR N-best Speaker 
Pre-processor M
achine learning
Interactive
interface BP identifier
Uttr. segment.DMM 
n partsInputF
Template-based 
translator FParser &Evaluation 
ResultsS Inter-lingua S
Language 
generator 
TTS 
Target speech Slot-based translator
 
 
Figure 1. The paradigm of interactive translation 
Based on the analysis on MT approaches and the 
user?s requirements, we propose an interactive 
paradigm for the S2S translation, which is based 
on the template-based translation, inter-lingual 
translation and the DMM based translation as well. 
The paradigm is shown as Figure 1. 
Where, the letter S beside the line with arrow 
means that the results of the former module are 
successful, and the letter F means the results are 
failure. 
According to the paradigm, an input from the SR 
component is probably processed and translated by 
the following four steps. First, the input is pre-
processed. Some noisy words are recognized, some 
repeated words are deleted, and the numbers are 
processed (Zong, 2000a). Then the base phrases 
(BP) in the input are identified, which include 
noun phrase (NP) and verb phrase (VP) mainly. 
And also, if the input is a long utterance containing 
several simple sentences or some fixed expressions, 
the input is possibly segmented into n parts. n is an 
integer, and n ? 1. Second, each part of the input is 
passed to the template-based translator. If the input 
part is matched with a translation template, the 
translation result is sent to the text-to-speech (TTS) 
synthesizer directly. Otherwise, the input part will 
be passed to the inter-lingual translator. Third, in 
the inter-lingual translator, the input is parsed and 
the parsing results are evaluated. If the evaluation 
score is bigger than the given threshold value, the 
parsing results will be mapped into the inter-lingua, 
and the translation result will be generated by the 
inter-lingua based target language generator. 
Otherwise, the system performs the fourth step. 
Fourth, DMM works to supervise the interaction 
for disambiguation of the input. In the interaction, 
the user is asked to answer some questions 
regarding to the input part. The system will fill the 
slots according to the question-answers. The slots 
are designed to express the user?s intentions in the 
input. The system directly generates the translation 
result according to the slots. So, the translation in 
the fourth step is named as slot-based translation. 
Where, the template-based translator employs 
the forward maximum match algorithm (Zong, 
2000c). The inter-lingua uses the interchangeable 
format (IF) developed by C-STAR (Consortium for 
Speech Translation Advanced Research). The 
parser oriented to IF is realized on the basis of 
HMM spoken language understanding model. In 
the experimental system we use the tri-gram to 
compute the probability of the sequence of 
semantic units (Xie, 2002). The IF-based language 
generator employs a task-oriented micro-planner 
and a general surface realizer. The target language 
is generated by the combination of template 
method and generation technology (Wu, 2000). 
The generic DMM has been proposed by (Xu, 
2001), which combines both interaction patterns 
and task structure. The machine learning module is 
taking charge of recording the dialogue patterns, 
topics and modifying the dialogue history, and so 
on. This module is still under construction. 
3.2 Utterance segmentation 
In an S2S translation system, how to split the long 
input utterances is one of the key problems, 
because an input is often uttered by the 
spontaneous speech, and there is not any special 
mark to indicate which word is the beginning or 
the end of each simple sentence inside the 
utterance. In our system an input Chinese utterance 
is first split by the SR component according to the 
acoustic features, including the prosodic cues and 
pause etc. Suppose an input utterance has been 
transcribed by SR and separated into k parts P1, 
P2, ? Pk (k is an integer, and k ? 1.). Each part Pi 
(i?[1 .. k]) is possibly further segmented into m 
(m is an integer and m?1) units U1, U2, ?, Um by 
the segmentation module based on the linguistic 
analysis (SBLA). Where, all Pi (i?[1 .. k]) and Uj 
(j?[1 .. m]) are called as the split units in our 
system. A split unit is one of the following 
expressions: 
z A single word. 
z A fixed expression, such as a greeting 
phrase in Chinese. 
z A simple sentence. 
z A clause indicated by some special 
conjunctions. For example, an input similar 
with the pattern ???(because) ? , ??
(therefore) ? ? will be separated into two 
parts ? ? ? (because)?? and ? ? ? 
(therefore) ? ?. 
Each Pi (i?[1 .. n]) is analyzed and segmented 
by SBLA through the following three steps: 
splitting on the shallow level, splitting on the 
middle level, and splitting on the deep level. This 
means if a string S is separated into n parts by 
using the method on the shallow level, each part 
will possibly be further segmented by the method 
on the middle level, and so on.  
3.3 Slot-based translation with DMM 
The slot-based translation with DMM is built on 
the following viewpoints and hypothesis: 1) there 
are some noisy words or ambiguous words in the 
results from SR component, but the keywords are 
recognized correctly; 2) the user?s intentions lie on 
the keywords and their dependence relations; and 3) 
the translation results based on the keywords are 
understandable and reflect the main intentions of 
the user. The slot-based translation under the 
guidance of DMM is performed as the following 
steps:  
i) Re-analyze the original input string, spot out 
the keywords, and also do the analysis on the 
dependence relation of the keywords.  
ii) Interact with the user, make decision about 
the keywords and their dependence relation, 
and fill the slots for the translation.  
iii) Generate the translation results according to 
the slots.  
iv) DMM writes down the keywords and their 
dependence relations and modifies the 
dialogue history. 
3.3.1 Keywords spotting and dependence 
analysis 
According to the evaluation score, if the parsing 
result of an input part is too worse, the parsing is 
treated as failure, and all analysis results, including 
base phrases, are ignored. The system will spot out 
the keywords from the original input and analyze 
the dependence relation among the keywords. 
Please note that the dependence relation of the 
keywords in this component is used for seizing the 
user?s intentions and generating the translation 
results. It is different with the function in the 
simple expression based translation (Zong, 2000b). 
In a specific domain, it is easy to define some 
keywords according to the statistical results of the 
collected corpus. In our system, a word is treated 
as the keyword if the following two conditions are 
met: 
? The part-of-speech (POS) of the word is 
one of the following three POSs: noun (N), 
verb (V), and adjective (A), and the word 
occurs with high probability in the specific 
domain. 
? The word is a number or a time word. 
In our method, the verb keyword is always treated 
as the center when the dependence relations are 
analyzed. The dependence relations between the 
verb keyword and the noun keywords are defined 
as four types: (1) agent, (2) direct object, (3) 
indirect object, and (4) the pivot word as well. The 
agent is usually located at the left of the verb 
keyword. In general, the direct object, indirect 
object, and the pivot word all occur at the right of 
the verb keyword. The pronoun is treated as the 
noun. Other content words are treated as the 
modification words of the keywords. The search 
direction and the position relation may be shown as 
the following Figure 2. Where, Wi means a 
common word, and KWi means a keyword.. 
 
W1  ?  KW1(verb) ?  Wi  ?  KW2(noun)?  
 modifications 
 agent object / pivot word 
 
Figure 2. Keywor lations 
According to the chara
verbs, there are five case
? There is no object af
? There is one object o
? There are two objec
and another one is t
? The object is a claus
? After the verb keyw
object (pivot word
agent of another fol
In the keyword dictionar
its all possible POSs a
DMM asks the user q
features of a specific ver
3.3.2 Interaction and s
In the DMM module, 
express the user?s inten
series of slots as follows
 ds and their recteristics of the Chinese 
s respectively: 
ter the verb; 
nly; 
ts. One is the direct object 
he indirect object. 
e. 
ord, the first noun is the 
) and acts as the role of 
lowed verb. 
y, each verb is tagged with 
nd relative features. The 
uestions according to the 
b, its context, and the slots. 
lot filling 
a frame is designed to 
tions, which consists of a 
.  
Frame: ACTION: Keywords (verb)  
 TENSE: {Present/Past/?} 
EXP. TYPE: {Interrogative/?} 
AGENT: noun; 
OBJECT1: noun; 
OBJECT2: noun; 
QUANTITY1: numeral; 
UNIT1: classifier; 
QUANTITY2: numeral; 
UNIT2: classifier; 
TIME: numeral & classifier; 
 HOW: adjective; 
 
 Figure 3. Frame of slots 
 
Where, QUANTITY1 and UNIT1 modify the 
agent, QUANTITY2 and UNIT2 modify the 
OBJECT1 or OBJECT2. Because the keywords 
have been spotted out and their dependence 
relations have been analyzed, the DMM asks the 
user according to the analysis results and the 
concrete context. Please see the following example. 
Input: ? ? ? ? ? ?? ? ?(Is this ? 
Xiang Ge Li La ? Hotel?) 
Two keywords, ??(be)? and ???(hotel)?, are 
spotted, and the word ???? is recognized as the 
object of the verb ?? ?. i.e.,  ACTION=? ; 
TENSE=Present; EXP. TYPE= Interrogative; 
OBJECT1=?? ; and other slots are empty. 
However, there are four noisy characters between 
the two keywords. The DMM will ask the user by 
using the question pattern: ???(what) X ??. The 
variable X is just replaced with the keyword ????. 
The user needs to answer the hotel name. Because 
the SR module still does not recognize the speech 
of the word ?????(Shangri-la)?, the DMM is 
unable to parse the user?s answer. The following 
dialogue will be done: 
System: ?????????????????
(Is the word ?????? an adjective or a noun?) 
User:   (?)??(It is a noun.) 
System: ??????? ? (Is it the hotel 
name?) 
User:  ??(yes). 
System: ??????????? (Please 
input the English name of the word ??????). 
The DMM will append the word ?????? 
both into the SR dictionary and translation 
dictionary and treat it as the attribute of the 
keyword ????. The input is finally translated by 
using the template ?Is this the X ??.  
3.3.3 Generation based on slots 
After the interaction, the translation result will be 
generated based on the templates that are consisted 
of the slots. For example, if AGENT and ACTION 
are filled, the EXP. TYPE = Statement, and other 
slots are empty. The generation template 
is: !AGENT !ACTION. Where, !AGENT means 
the English word corresponding to the Chinese 
word in the AGENT slot. !ACTION is the English 
word corresponding to the Chinese word in the 
ACTION slot. However, the morphology of the 
verb will be changed according to the agent. 
From the frame of slots we can see that the 
frame can only express the analysis results of 
simple sentence. So, the translation result is always 
expressed by the simple sentence. If the subject or 
the object of a Chinese input is a clause, the input 
will be translated into two or more simple English 
sentences. For instance,  
Input: ??????????????
(How much does it cost if I reserve two single 
rooms?) 
The input will be mapped into two frames. In 
the first frame, AGENT=?; ACTION=??; EXP. 
TYPE=Statement; QUANTITY2=?; UNIT2=?; 
OBJECT1= ? ? ? . In the second frame, 
ACTION= ? ? ; EXP. TYPE= Interrogative; 
QUANTITY1=?? ; OBJECT1=? . Therefore, 
the input is separately translated into two simple 
English sentences: ?I reserve two single rooms.?, 
and ?How much does it cost??. Obviously, in the 
specific context, the results are completely 
understandable and acceptable. 
4 Conclusion 
This paper describes a new paradigm for S2S 
translation system, which is based on DMM. 
According to the description we can see that the 
paradigm is of the following features: 
(1) The S2S translation is realized in the 
combination of direct translation 
engines and the interaction led by DMM. 
The interaction is not always brought 
into the role, and it only works when the 
former translation engines work failed.  
(2) The interaction is impersonative, target- 
oriented, and led by the system, not 
blind. The user does not need to correct 
all of the errors in the results of SR. He 
or she only needs to concern what the 
system asks. 
(3) The system can always give the results 
for an input speech despite of the ill-
formed expressions and the worse 
recognition results. 
Although the whole experimental system is under 
construction, some preliminary results have been 
gained. Zong (2000c) reported the performance of 
the template-based translator; Xie (2002) reported 
the results of the robust parser for the Chinese 
spoken language; Xu (2001) presented the results 
of dialogue model; and so on. The results have 
made us confident to develop the practical S2S 
translation system based on the dialogue 
management. However, we are facing much hard 
work that involve the following aspects at least: 
? Develop the reasonable strategies and 
standards to evaluate the parsing results; 
? Design the effective templates to ask the 
user questions according the keywords and 
the concrete context; 
? Define the practical templates to generate 
the translation results; 
? Build the machine learning mechanism to 
enrich the knowledge base of the system. 
References 
Blanchon, H. 1996. A Customizable Interactive 
Disambiguation Methodology and Two 
Implementations to Disambiguate French and 
English Input. In Proceedings of MIDDIM-96 
(International Seminar on Multimodal Interactive 
Disambiguation), Col de Porte, Fance. 
Furuse, O., Satsuo Yamada and Kazuhide Yamamoto. 
1998. Splitting Long or Ill-formed Input for Robust 
Spoken-language Translation. In Proceeding of 
COLING-ACL, Canada. Vol. I, pp. 421-427. 
Lavie, A., Lori Levin et al 1999. The JANUS-III 
Translation System: Speech-to- Speech Translation 
in Multiple Domains. In Proceedings of C-STAR II 
Workshop, Schwetzingen of Germany, 24 Sept., 
1999. 
Ren, F., Shigang Li. 2000. Dialogue Machine 
Translation Based upon Parallel Translation Engines 
and Face Image Processing. In Journal of 
INFORMATION?Vol.3, No.4, pp.521-531. 
Ren, F. 1999. Super-function Based Machine 
Translation, in Communications of COLIPS, 9(1): 
83-100. 
Seligman, M. 1997. Interactive Real-time Translation 
via the Internet. In Working Notes, Natural 
Language Processing for the World Wide Web. 
AAAI-97 Spring Symposium, Stanford University. 
March 24-26, 1997. 
Seligman, M. 2000. Nine Issues in Speech Translation. 
In Machine Translation. 15: 149-185. 
Waibel, A. 1996. Interactive Translation of 
Conversational Speech. In Proceedings of ATR 
International Workshop on Speech Translation. pp. 
1~17. 
Wahlster, W. 2000. Mobile Speech-to-Speech 
Translation of Spontaneous Dialogs: An Overview 
of the Final Verbmobil System. In Verbmobil: 
Foundations of Speech-to-Speech Translation. 
Springer Press. pp. 3-21. 
Wakita, Y., Jun Kawai, Hitoshi Iida. 1997. Correct Parts 
Extraction from Speech Recognition Results Using 
Semantic Distance Calculation, and Its Application 
to Speech Translation. In Proceedings of a 
Workshop Sponsored by the ACL and by the 
European Network in Language and Speech 
(ELSNET). pp. 24-29. 
Wu, H., Taiyi Huang, Chengqing Zong, and Bo Xu. 
2000. Chinese Generation in a Spoken Dialogue 
Translation System. In Proceedings of COLING. pp. 
1141-1145. 
Xie, G., Chengqing Zong, and Bo, Xu. 2002. Chinese 
Spoken Language Analyzing Based on Combination 
of Statistical and Rule Methods. Submitted to the 
International Conference on Spoken Language 
Processing (ICSLP-2002). 
Xu, W., Taiyi Huang, and Bo Xu. Towards a Generic 
Dialogue Model for Information-seeking Dialogues. 
In Proceedings of the National Conference on Man-
Machine Speech Communications (NCMMSC6). 
Shenzhen,  China. pp. 125-130. 
Yamamoto, K., Satoshi Shirai, Masashi Sakamoto, and 
Yujie Zhang. 2001. Sandglass: Twin Paraphrasing 
Spoken Language Translation. In Proceedings of the 
19th International Conference on Computer 
Processing of Oriental Languages (ICCPOL- 2001). 
pp. 154-159. 
Zhao, T. et al 2000. The Principle of Machine 
Translation (in Chinese). Press of Harbin Institute of 
Technology.  
Zong, C., Taiyi Huang and Bo XU. 1999. Technical 
Analysis on Automatic Spoken Language 
Translation Systems (in Chinese). In Journal of 
Chinese Information Processing, 13(2):55-65. 
Zong, C., Taiyi Huang and Bo Xu. 2000a. Design and 
Implementation of a Chinese-to-English Spoken 
Language Translation System. In Proceedings of the 
International Symposium of Chinese Spoken 
Language Processing (ISCSLP-2000), Beijing, 
China. pp. 367-370. 
Zong, C., Yumi Wakita, Bo Xu, Kenji Matsui and 
Zhenbiao Chen. 2000b. Japanese-to-Chinese Spoken 
Language Translation Based on the Simple 
Expression. In Proceedings of International 
Conference on Spoken Language Processing 
(ICSLP-2000). Beijing, China. pp. 418-421. 
Zong, C., Taiyi Huang and Bo Xu. 2000c. An Improved 
Template-based Approach to Spoken Language 
Translation. In Proceedings of International 
Conference on Spoken Language Processing 
(ICSLP-2000). Beijing, China. pp. 440-443. 
 
