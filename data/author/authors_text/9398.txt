Chinese Generation in a Spoken Dialogue Translation System 
Hua Wu, Taiyi Huang, Chengqing Zong and Bo Xu 
National Laboratory of Pattern Recognition, Institute of Automation 
Chinese Academy of Sciences, Beijing 100080, China 
E-mail: { wh, huang, cqzong, xubo } @nlpr.ia.ac.cn 
Abstract:  
A Chinese generation module in a speech to 
speech dialogue translation system is presented 
he:re. The input of the generation module is the 
underspecified semantic representation. Its design 
is strongly influenced by the underspecification f 
the inlmtS and the necessity of real-time and 
robust processing. We design an efficient 
generation system comprising a task-oriented 
microplanner and a general surface realization 
module for Chinese. The microplanner performs 
the lexical and syntactic choice and makes 
inferences fiOln the input and domain knowledge. 
The output of the microplanner is fully 
instantiated. This enables the surface realizer to 
traverse ltle input in a top-down, depth-first 
fashion, which in turn speeds the whole 
generation procedure. The surface realizer also 
combines the template method and deep 
generation technology in the same formalism. 
Preliminary results are also presented in this 
paper. 
1,, In t roduct ion  
In this paper, we will present the core aspects 
of the generation component of our speech to 
speech dialogue translation system, the domain of 
which is hotel reservation. The whole system 
consists of five modules: speech recognizen 
translator, dialogue manageh generator and speech 
synthesizer. And the system takes the interlingua 
method in order to achieve multilinguality. Here 
the interlingua is an underspecified selnantic 
representation (USR). And the target language is 
Chinese in this paper. 
Reiter (Reiter 1995) made a clear distinction 
between templates and deep generation. The 
template method is rated as efficient but inflexible, 
while deep generation method is considered as 
flexible but inefficient. So the hybrid method to 
combine both the methods has been adopted in the 
last few years. Busemann (Busemann 1996) used 
hybrid method to allow template, canned texts and 
general rules appearing in one formalism and to 
tackle the problem of the inefficiency of the 
grammar-based surface generation system. Pianta 
(Pianta 1999) used the mixed representation 
approach to allow the system to choose between 
deep generation technology and template method. 
Our system keeps the surface generation 
module general for Chinese. At the same time, we 
can also deal with templates in tile input without 
changing tile whole generation process. If tile 
attribute in the feature structure is "template", 
then the value must be taken as a word string, 
which will appear in the output without 
modification. The surface generation module 
assumes  the input as a predicate-argument 
structure, which is called intermediate 
representation here. And any input of it must be 
first converted into an intermediate r presentation. 
The whole generation process can be 
modularized fimher into two separate components: 
microplanner and syntactic realizer. The 
microplanner is task-oriented. The input is an 
USR and the function of it is to plan an utterance 
on a phrase- or sentence-level. It maps concepts 
defined in the domain to a functional 
representation which is used by the syntactic 
generation components to realize an appropriate 
surface string for it. The functional description is 
made of feature structures, the attribute-value 
1141 
pairs. And the functional representation serves as 
the intermediate representation between the 
microplanner and the syntactic generator. The 
intermediate representation is fully instantiated. 
This enables the surface realizer to traverse the 
input in a top-down, depth-first fashion to work 
out a grammatically correct word string for the 
input, which in turn speeds the whole generatiou 
procedure. So our system use a task-oriented 
microplanner and a general surface realizer. The 
main advantage is that it is easy to adapt the 
system to other domains and maintain the 
flexibility of the system. 
In this paper, section 2 gives a brief 
description of our semantic representation. 
Section 3 presents our method on the 
microplanning procedure. Section 4 describes the 
syntactic generation module. Section 5 presents 
the preliminary results of our generation system. 
Section 6 presents discussions and future work. 
2. Semantic Representation 
The most obvious characteristics of the 
selnantic representation are its independence of 
peculiarities of any language and its 
underspecification. But it lnUSt capture the 
speaker's intent. The whole semantic 
representation has up to four components as 
shown iu figure l: speaker tag, speech act, topic 
and arguments. 
The speaker tag is either "a" lbr agent or "c" 
for customer to indicate who is speaking. The 
speech act indicates the speaker's intent. The topic 
expresses the current focus. The arguments 
indicate other inforlnatiou which is necessary to 
express the entire meaning of the source sentence. 
USR::= speaker: speech act: topic: m'gument 
Speaker::= alc 
Speech_act ::= give-information I request- 
information\[... 
Topic ::= (concept = attribute) ^
Argument ::= (concept=attribute)l* 
Figure 1 Underspecified Semantic Representation 
Both the topic and arguments are made up of 
attribute-value pairs in functional formalisms. The 
attribute can be any concept defined in the dolnain 
of hotel reservation. The value can be an atomic 
symbol or recursively an attribute-value pair. The 
symbol "^" in the topic expression indicate that 
the expression can appears zero to one time, while 
The symbol "*" iu the argument expression shows 
that the expression can appears zero to any times. 
And the attribute-value pairs are order free. Both 
topic and arguments are optional parts in the USR. 
Let us consider a complex semantic 
expression extracted from our corpus. It is shown 
in Example I: 
a: give-information: (available -- (room = 
(room- type = double ))) : (price = (quantity 
=200&240,currency=dollor)) I (1) 
In Example 1, the speech act is give- 
information, which means that the agent is 
offering information to the customer. The topic 
indicates there are double rooms. The arguments 
list the prices of double rooms, which shows that 
there are two kinds of double rooms available. So 
the meaning of this representation is " We have 
two kinds of double rooms which cost 200 mad 
240 dollars respectively". From the USR, the 
kinds of rooms are not expressed explicitly in the 
format. Only from the composite value of the 
concept "price " can we judge there are two kinds 
of rooms because the price is different. This is 
only one example of underspecification, which 
needs inferences from the input and the domain 
knowledge. 
3. The Microplanner 
The input to our microplanner is the 
underspecified semantic representation. From the 
above semantic representation, we can see that it 
is underspecified because it lacks infornlation 
such as predicate-argument structure, cognitive 
status of referents, or restrictive/attribute fimction 
of semantic properties. Some of the non-specified 
pieces of ilfformation such as predicate/argument 
structure are essential to generate a correct 
translation of the source sentence. Fortmmtely, 
much of the information which is not explicitly 
represented can be inferred fiom default 
knowledge about the specific domain and the 
general world knowledge. 
The lnicroplanner includes two parts: 
sentence-level planning and phrase-level planning. 
1142 
The sentence planner maps the semantic 
representation into predicate argument structure. 
And the phrase planner maps the concepts defined 
in the domain into Chinese phrases. 
In order to express rules, we design a format 
t'or them. The rules are represented as pattern- 
constraints-action triples. A pattern is to be 
matched with part of the input on the sentence 
level and with the concepts on the phrase level. 
The constraints describe additional context- 
dependent requirements to be fulfilled by the 
input. And the action part describes the predicate 
argument structure and other information such as 
mood and sentence type. An example describiug a 
sentence-level rule is shown in Figure 2. 
((speaker= a ) ( speech_act =give-information )( topic 
= available ) ( topic_value = room )); 
//pattern 
(exist(concept, 'price' )); //constraint 
( (cat = clause) ( mood = declarative) 
( tense = present) (voice = active) 
(sentence type = possessive) 
(predicate ='4f') 
(args = (((case - pos) 
(lex :- #get(attribute, 'room' ))) 
((case = bel) 
(cat = de) 
(modifier-(#gct(altributc, 'price' ))) 
(auxiliary = 'l'l{j')))) 
(!optiolml: pre_mod = ( time = #get ( attribute, 
'lime')))); //action 
Figure 2 Example Microplanning l>,ule 
First, we match the pattern part with the input 
USI>,. If matched, the constraint is tested. In the 
example, the concept price lnust exist in the input. 
The action part describes the whole sentence 
structure such as predicate argument structure, 
sentence type, voice, mood. The symbol "#get" in 
the action part indicates thai the value can be 
obtained by accessing the phrase rules or the 
dictionary to colnplete the structure recursively. 
The "#get" expression has two parameters. The 
first parameter can be "concept" or "attribute" to 
indicate to access the dictionary and phrase rues 
respectively. The second parameter is a concept 
defined in the domain. In the example, the "#get" 
expression is used to get the value of the domain 
concepts room and price respectively. The symbol 
"optionah" indicates that the attribute-value pair 
behind it is optional. If the input has the concept, 
we fill it. 
After the sentence- and phrase-level phmning, 
we must access the Chinese dictionary to get the 
part-of-speech of the lexicon and other syntactic 
information. If the input is the representation i  
Example I, the result of the microplanning is 
shown in Figure 3. 
(cat  = clat, se) 
( sentence_type =possessive) 
(mood = declarative) 
( tense = present) (voice = active) 
(predicate =((cat=vcm) (lex ='4f'))) 
(args=(((case = pos)(cat = nct)(lex ='~J, Vl'iq')) 
((case = bel) 
(cat =de) 
(modi fier=((cat=mp) 
(cardinal =((cat=nc) 
(n l=((cat=num) (lex='200')) 
(n2=((cat=num)(lex="240')) 
(qtf= ((cat=ncl) ( lex ='0~)t\]')))) 
(at, x il iary =(lex =' I'1I'.1')))) 
lVigure 3 Microplanning Result for Example 1 
In the above example, "cat" indicates the 
category of the sentence, plnases or words. "h'x" 
denotes the Chinese words. "case" describes the 
semantic roles of the arguments. 
Target language generation in dialogue 
translation systems imposes strong constraints on 
the whole generation. A prominent pmblena is the 
non-welformedness of the input. It forces the 
generation module to be robust to cope with the 
erroneous and incomplete input data. In this level, 
we design some general rules. The input is first to 
be matched with the specific rules. If there is no 
rules matched, we access the general rules to 
match with the input. In this way, although the 
input is somehow ill-formed, the output still 
includes the main information of the input. An 
example is shown in (2). The utterance is 
supposed for the custom to accept he single room 
offered by the agent. But the speech act is wrong 
because the speech act "ok" is only used to 
1143 
indicate' that the custol-u and tile agenl has agreed 
on one Iopic. 
c: ok: ( room = ( room-type := single, 
quantity= 1 )): (2) 
Although example (2) is ill formed, it 
includes most information of the source sentence. 
Our robust generator can produce the sentence 
shown in (3). 
Cl'i-)kf/iJ~J: ( yes, a single roont ) (3) 
4. Syntactic realizatim  
The syntactic realizer proceeds from the 
microplannir~g result as shown in t"igure 3. The 
realizer is based on a flmctional uuificati,,m 
fornmlism. 
lit tMs module, we also introduce the 
template nlethod. If lhe input includes an 
attribute~wflue pair which uses "template" as file 
attribute, then rite wflue is taken as canned lexts or 
word strhws wilh slots. It will appear in the output 
without any modificati(m. So we can embed tile 
template into the surface realization without 
modifying tlw whoh: generation l)rocedure. When 
the hybrid method is used, the input is first 
matched with the templates defined. If matched, 
the inputs will go lo llle surface realizer directly, 
skiplfing tl,c microplanning process. 
The task of the Chinese realizer i:; as tollows: 
, Define the sentence struclure 
? Provide ordering constraints among the 
syntactic onstituents of the sentence 
? Select the functional words 
4.1 \]Intermediate Representation 
The intermediate representation(IR) is made 
up of feature structures. It corresponds to the 
predicate argument structure. The aim is to 
normalize the input of tile surface realizer. It is of 
considerable practical benefit to keep the rule 
basis as independent as possible front external 
conditions (such as the domain and output of tile 
preceding system). 
The intermediate representation includes 
three parts: predicate int"ormation, obligatory 
arguments and optional arguments. The predicate 
inR)rmation describes the top-level information in 
a clause includiug the main verb, lhe mood, the 
voice, and so on. The obligatory arguments are 
slots of roles that must be filled in a clause for it 
to be contplete. And the optional arguments 
specify the location, the time, the purpose of the 
event etc. They arc optional because they do not 
affect rite contpleteness of a clause. An example is 
shown in Figure 4. The input is for the sentence 
"{~J~ l'f\] ~\]l~ 1{ 1'1 @) \  \[)iJ li!.~ ?" (Do you have single 
rooms now?). "agrs" antt '?opt" in Figure 4 
represent obligatory arguntents and optional 
arguments respectively. 
((cat = clause) 
( sentence )ype =possessive) 
(mood: yes-no) 
( lense = present) (wfice -: active) 
(predicate =((cat=veto) (lex ="(J"))) 
(args=(((case :-: pos)(ca! -pron)(lex ='{?j<{f\]')) 
((case "- bel) (cai ~:nct)(lex=' "l%)v. \['(iJ ')))) 
(opt=(d me=((cat=:adv) tie x=' J:l)lu (I i'))))) 
Fip, urc 4 F, xample Intermediate l),el)rescnlalion 
4.2 Chinese Reallizalion 
In tile synlaclic generation module, we use 
ihe \[unclional unification fommlism. At tile same 
lime, we make use of dlc systclnic viewpoirl/ of 
lhe systcrnic function grammar. The rule system is 
made up of many sub-.sysienls such as transitivily 
system, mood system, tense system and voice 
systcllt. The input 111ust depend on all of these 
systems to make difR:rent level decisions. 
In a spoken dialogue Iranslalion system, real= 
lime generation is tile basic requiremenl. As we 
see froln the input as shown in Figure 3, the inlmt 
to the syntaclic generation provides enough 
iuformation about sentence and phrase structme. 
Most of the informatiou in tile input ix instautiatcd, 
such as the verb, the subcategorization frame and 
the phrase members. So the generation engine can 
traverse the input in a top-down, depth-first 
fashion using tmification algorithm (Elhadad 
1992). The whole syntactic generation process is 
described in Figure 5. 
The input is an intermediate representation 
and the output is Chinese texts. The sentence 
unification phase defines the sentence structure 
and orders the components anloDg, tile sentence. 
1144 
The phrase unification phase dcl'ines the phrase 
structure, orders the co~nponenls inside the 
phrases and adds the function words. Unlike 
English, Chinese has no morphological markers 
for tenses and moods. They arc expressed with 
fmlclional words. Selecting functiolml words 
correctly is crilical for Chillesc generation. 
,qelltellCC\[ ~'t "~''~'-t ~unifica|ion ~lst? -- -- !'-;~7~II1 I - tlni fcatiOll \]~CXt 
Figure 5 Sleps of the Synlacfic generator 
The whole unification procedure is: 
,, Unify the input with the grammar at the 
sentence l vel. 
? identify the conslitules inside the inptll 
? Unify the constituents with tile grammar a! the 
phrase level recursively in a top-down, depth- 
first fashion. 
5. Results 
The current version of the system has been 
tested on our hotel reservation corpus (Chengqing 
Zong, 1999). The whole corpus includes about 90 
dialogues, annotated by hand with underspecificd 
semantic representation. I1 contains about 3000 
USRs. Now we have 23 speech acls and about 60 
concepts in lhe corpus. 
The generation lnodulc is tested on all 
sentences in the corpus. And 90% of the generated 
sentences arc rated as grammatically and 
semantically correct. The other 10% are rated as 
wrong because the mood of the sentences i not 
conect. This is mainly caused by the lack of the 
dialogue context. 
6. Discussion and Future Work 
In spoken language translation systems, one 
problem is the ill-formed input. How to tackle this 
problem robustly is very important. At the 
microplanning level, we design some general 
rules. The input is first to be matched with the 
sl~e<:ific roles. If there is no rules matched, we 
access the gene.ral roles to Inalch with the input. In 
this way, although the inl)U! is somehow ill- 
formed, the output includes the main information 
of the input. And at the surface realization level, 
we make some relaxation on tests to improve the 
robuslness, l;,.g, oMigatory arguments may be 
missing in the utterance. This can be caused by 
ellipsis in sentences such as the utterances "{:\]{: ~ 
J~." (stay for three days). We have to accept it as a 
sentence without the subject because they are 
acceptable in spoken Chinese and often appear in 
daily dialogues. 
We arc planning to l:tuther increase the 
robustness of the system. And if possible, we also 
hope to adapt our generation system to other 
(lolnaills. 
Acknowledgements 
The research work described in lhis paper is 
Sulsportcd by the National Natural Science 
t;oundation of China under grant number 
69835030 and by the National '863' Hi-Tcch 
Program under grant nunlber 863-306-ZT03-02-2. 
Thanks also go to several allonyl/lOtlS l'eVieWel'S 
for their valuable comments. 
Reference ;  
Stephan I} uscmann. (1996) Best- first surl'ace 
realization. In t i le Eighth lntcrnatiolml Natural 
l.anguagc Generation Workshop, Sussex, pages 101- 
I10 
Michael Elhadad and Jacques Robin. (1992) 
Controlling Content P.calization with Functional 
Unification Grammars. Aspects of Automated Natural 
Language Generation. t51)89 - 104 
E.Pianta, M.Tovcna. (1999) XIG: Generating from 
Interchange Format Using Mixed Representation. 
AAAI'99 
Ehud Reiter. (1995) NLG vs. Templates. In lhe Fiflh 
Et, ropcan Workshop on Natural Language Generation, 
Leiden, 
Chengqing Zong, Hua Wu, Taiyi Huang, Be Xu. (1999) 
Analysis on Characteristics of Chinese Spoken 
Language. In the Fiflh Natural Language Processing 
Pacific Rim Symposium, 151)358-362 
1145 
Chinese Syntactic Parsing Based on Extended GLR Parsing 
Algorithm with PCFG* 
 
Yan Zhang, Bo Xu and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation 
Chinese Academy of sciences, Beijing 100080, P. R. China 
E-mail: {yzhang, xubo, cqzong}@nlpr.ia.ac.cn 
 
Abstract  
This paper presents an extended GLR 
parsing algorithm with grammar PCFG* that 
is based on Tomita?s GLR parsing algorithm 
and extends it further. We also define a new 
grammar?PCFG* that is based on PCFG 
and assigns not only probability but also 
frequency associated with each rule. So our 
syntactic parsing system is implemented 
based on rule-based approach and statistics 
approach. Furthermore our experiments are 
executed in two fields: Chinese base noun 
phrase identification and full syntactic 
parsing. And the results of these two fields 
are compared from three ways. The 
experiments prove that the extended GLR 
parsing algorithm with PCFG* is an 
efficient parsing method and a 
straightforward way to combine statistical 
property with rules. The experiment results 
of these two fields are presented in this 
paper. 
1. Introduction 
Recently the syntactic parsing system is one of 
significant components in natural language 
processing. Many parsing methods have been 
developed as the development of corpus 
linguistics and applications of linguistics. 
Tomita? GLR parsing (Tomita M., 1986, 1987) 
is the most general shift-reduce method of 
bottom-up parsing and widely used in syntactic 
parsing. Several methods are based on it. Lavie 
(Lavie A., 1996) used the GLR* parsing 
algorithm for spoken language system. It uses a 
finite-state probabilistic model to compute the 
action probabilities. Inui (Inui K. et al, 1997, 
1998) presented a formalization of probabilistic 
GLR (PGLR) parsing model which assigns a 
probability to each LR parsing action. To 
shallow parsing, many researchers have made 
experiments with identification of noun phrases. 
Abney (Abney S., 1991) used two level 
grammar rules to implement the noun phrase 
parsing through pure LR parsing algorithm.  
Some new methods based on GLR algorithm 
aim to capture action probabilities by statistics 
distribution and context relations. This paper 
combines rule approach and statistics approach 
simultaneously. Furthermore, based on GLR and 
PCFG, we present an extended GLR parsing and 
a new grammar PCFG* that provides the action 
probabilities to prune the meaningless branches 
in the parsing table. Our experiments are also 
made in two parts: Chinese base noun phrase 
parsing and Chinese full parsing. The former is a 
simplified formalization of full parsing and is 
relatively simpler than the latter. 
This paper includes four sections. Section 2 
presents a brief description of rule structure 
system-PCFG*. Section 3 gives our extended 
GLR parsing algorithm and the parsing 
processing. Section 4 shows the experiment 
results of our parser including Chinese base 
noun phrases (baseNP) identification and 
Chinese full syntactic parser. The conclusions 
are drawn in section 5. 
2. A New Grammar (PCFG*) and the 
Rule Structure 
Grammar system is one of the important pars of 
a parsing system. We explain it in detail in the 
following section.  
2.1 Structure of Rules 
The definition of symbols in our system inherits 
the classifications of Penn Chinese tree-bank 
(Xia F., 2000). There are totally 33 
part-of-speech tags, 23 syntactic tags and 26 
functional tags in the Chinese tree-bank tag set. 
The POS tags belong to terminal symbols, while 
others belong to non-terminal symbols.  
In the final rule base there are about 2000 rules 
and 400 rules learned from corpus for full 
parsing and base noun phrases identification 
respectively. The rules have the following 
format showed in table 1. 
num rule probability frequency
1 VCD?VV  
+VV 
0.754491 126 
2 VCP?
VV+VC 
0.545455 6 
3 VCP?
VV+VV 
0.454545 5 
    Table 1: the format of grammar rules 
In order to denote each rule explicitly, the mark 
?+? is used as the junction mark. In above 
examples, symbols VP, VCD and VCP are verb 
phrase and verb compounds. Symbols VV and 
VC stand for common verbs and copula ??? 
respectively. 
2.2  A New Grammar (PCFG*) 
Context-free grammars (CFGs) are widely used 
to describe the grammar structures in natural 
language processing. And probabilistic 
context-free grammars (PCFGs) directly add the 
probabilities to the rules. But it is sometimes not 
sufficient to only associate probability with each 
rule. So we define a new grammar 
system-PCFG*: each rule is assigned probability 
distribution and frequency distribution 
simultaneously. The probability number is the 
relative value since it is the percentage value in 
the rule group that have the same left sides. 
While the frequency number is the absolute 
value because it is the total numbers occurred in 
whole corpus. The probability property is the 
key value to full parsing. The probability 
attribute is superior to frequency attribute. 
A sample is presented to show how to use 
probability and frequency of a rule.  
Suppose there are three rules showed in table 2 
and the relations is displayed in figure 1. 
Rule F(r) P(r) 
X?A+C f1 p1=f1/(f1+f2)
X?A+B+C f2 >f1 p2=f2/(f1+f2)
Y?A+C f3 <f1 p3 =1>p1 
Table 2: the examples of rule 
A B
X Y
C  
Figure 1: structure of rules 
Suppose the input symbols contain A, B and C. 
When rule 1 and rule 3 simultaneously satisfy 
the reduce condition, rule 3 is executed and the 
left side item ?Y? is pushed to the stack because 
p3 is bigger than p1. To complete parsing, 
probability always has the priority to frequency. 
But to baseNP parsing, frequency is superior to 
probability attribution. Since f1>f3, rule1 is 
executed first. If f1 is equal to f3, then go on to 
compare probability.  
3. Parsing Algorithm 
The parsing algorithm is very significant as well 
as the grammar rules to the parsing system. We 
produce an extended GLR parsing algorithm 
based on the Tomita?s GLR parsing algorithm in 
our system.  
3.1 the Extended GLR Parsing Algorithm 
The GLR method augments the LR parser and 
overcomes the drawback of the LR parser. In 
fact, from the point of parsing algorithm, there 
are no clear differences between LR and GLR 
algorithm. In parsing processing, there are also 
four actions in GLR algorithm that are similar to 
the LR parsing. But GLR parsing algorithm 
admits multiple entries in the parsing table. Our 
extended GLR algorithm also permits that 
several shift and reduce actions exist in one 
branch in the parsing table simultaneously. So 
there are mainly two types of conflicts: 
shift-reduce conflict and reduce-reduce conflict. 
These conflicts are the most difficult problems 
of GLR algorithm. In the parsing process, when 
the conflicts between shift and reduce occur, the 
principle of our parsing method is that the 
reduce action is superior to the shift action.  
If only grammar rules are used to describe the 
context relations, they may produce many 
conflicts when several rules satisfy the 
conditions. So we use the grammar 
system--PCFG* to add statistical information. 
The probabilities distributions are associated 
with the rules to each parsing action and decide 
which step to continue.  
Therefore the extended GLR algorithm handles 
the conflicts with two steps: (1). The reduce 
action is always executed first, then the shift 
action. (2). When more than one reduce actions 
satisfy the conditions, probability and frequency 
decide the order of these reduce actions.  
 
3.2 Parsing Actions and Parsing Process 
3.2.1 Parsing Table and Actions 
The parsing table consists of two sub-tables: 
ACTION table and GOTO table that are 
constructed by the grammar rules. The GOTO 
table is not different from GLR table. Just 
ACTION table is modified a little. Figure 2 
shows the structure of the parsing table. 
ACTION GOTO State 
X1, X2, ?, Xi ,       # Y1, ?, 
Yj  
S0 Sh1  
S1  Re1  
?  Re-Sh  
Sn  Accept 
 
Figure 2: the parsing table 
 
The ACTION table contains four action 
sub-tables: Sh1, Re1, Re-Sh and Accept. They 
stand for shift part, reduce part, reduce-shift part 
and accept part respectively. Because the error 
action is similar to accept action, it is not 
explained here. The Re-Sh part is the key part in 
the table. It contains multiple entries while the 
others have no conflicts. In the Re-Sh part, the 
rules are firstly arranged according to the 
probabilities and then compared based on the 
frequencies. The maximum probability is put on 
the top. This sequence continues until the last 
rule with minimum probability. According to the 
order of Re-Sh sub-table, the parsing program is 
transformed to the corresponding state of the 
stack. This order suits for the full parsing. But to 
the base noun phrases identification, frequency 
is firstly compared. 
Since the ambiguities and conflicts existed in the 
Re-Sh sub-table, we give a limit that no more 
than 20 entries in the Re-Sh part. From the 
experiment results, it is better to select 20 rules 
as the branch limit in the parsing process 
because it not only permits the multiple entries, 
but also fits for the performance efficiency of 
our program.  
Since the parser uses PCFG*, it has strong 
control to handle action conflicts and rule 
ambiguities. The parsing process need to prune 
the meaningless parsing branches. Excessive 
pruning may cause the loss of some grammar 
rules and add the error opportunities. Reasonable 
pruning can improve efficiency. 
 
3.2.2 the Parsing Process 
We give the following the symbols definition 
and interpretation to explain the parsing process.  
Let ?#? denotes the start and the end of the input 
Chinese sentence. The system contains a list of 
stacks simultaneously. The parsing table 
contains two elements: state nodes and symbol 
nodes. The parsing stack includes state stack 
(StateStack, name in the program), symbol state 
(SymbolStack) and input stack (InputStack) 
whose pointers are ps, pb and pi respectively.  
Following algorithm is established for the 
shift-reduce parsing process. 
Input:  
An input Chinese words sequence W in which 
each word has its part-of-speech and a parsing 
table produced by grammar rules; 
Output:  
If the input word sequence W satisfies the 
grammar rules and is accepted according to the 
parsing table, then output the parsing result of W, 
otherwise give error result; 
 
Main Loop:  
It mainly consists of four parts: shift, reduce, 
accept and error in the parsing process.  
Repeat  
Begin 
s := *ps++;  //s is current state 
b := *pb++; //to the next symbol 
c := *pi++; //to the next input word 
if Action[reduce rule 
VtVnVnAA ?????? ,, ] = reduce(), 
then begin 
1) Pop |?| symbols from top of the symbol 
stack, and push the left side symbol A to 
the symbol state; 
2) Pop |?| symbols from top of the state 
stack, and push s* 
3) ps -= |?|; *ps := s*;  
 end reduce(); //reduce part 
  
else if Action[] = shift(input s*),  
then begin 
 pi++; *pi := s*; pb++; *pb := s*; 
end shift(); //shift part 
 
else if Action[] = accept() 
then Success and Output; //the parsing 
succeeds 
else 
     error(); // parsing is error here 
End 
Until: The input symbol is the end of the 
sentence. Or accept function occurs or error 
function occurs. 
 
(1) Reduce Action 
When the reduce action is performed, the rule 
candidates are selected in the list from the first 
rule to the last one that are arranged according to 
the probabilities and frequencies. If one of these 
rules satisfies the condition, then the flag of this 
rule is changed from FALSE to TRUE and stop 
here, and continue to read input word. Otherwise 
trace back.  
(2) Shift Action 
Shift action is executed under two conditions. 
One is based on the action table. The other is 
that when error action occurs, the base noun 
phrase identification continues to perform shift 
action while the full parsing enters trace part. 
(3) Error Action 
When error action occurs, trace back to the 
previous branch and perform another rule 
candidate listed in the entry. If there is no path 
can be searched in the current branch point or all 
routes are not passed through, the parsing fails 
and output the final error symbol. This situation 
is only used to the full parsing.   
3.2.3 the Comparison with GLR 
In order to explain explicitly our extended GLR 
parsing algorithm, we compare it with GLR 
algorithm. Table 3 gives the comparison results. 
    methods 
aspects 
GLR algorithm Our 
algorithm 
Grammar 
System 
CFG PCFG* 
Statistical 
Information 
no Probability, 
Frequency 
Data Structure Graph-Structured 
Stack 
Stack List 
Parsing 
Process 
Not simplified Pruning  
Other 
Attributes 
Augmentation to 
each rule 
no 
Table 3: Comparison with GLR  
4. Experiment and Results 
Our experiments include two parts: Chinese base 
noun phrase parsing and Chinese full syntactic 
parsing. 
The obvious difference of Chinese baseNP 
parsing and full parsing is that the former must 
give the parsing results while the latter 
sometimes need to trace back and output the 
error symbols. Because baseNP identification 
belongs to the shallow parsing, it only need to 
gives the recognized noun phrase structures. If 
there are no phrases found, then output the 
original sentence. Obviously Chinese baseNP 
parsing is much simpler and more efficient than 
the full parsing from the point of the method and 
the runtime.  
Our experiments are performed based on 
Chinese tree-bank corpus. There are totally 
10,000 Chinese sentences whose grammar 
structures are described by brackets. Table 4 
shows the characteristic of the corpus in the 
parsing process. 
Corpus 
 
Style 
Of Parsing 
Number of the 
Sentences. 
Average 
length of 
each 
sentence 
Training: 97% 22 words BaseNP 
Identification Testing: 3% 15 words 
Training: 98% 22 words Full Parsing 
Test: 2% 15 words 
Table 4: characteristic of corpus 
 
To two styles of parsing presented above, we 
give two types of results respectively.  
(1). Chinese BaseNP identification 
In our system, base noun phrases are defined to 
include not only pure noun phrase (NP) but also 
quantifier phrase (QP), such as QP ( ???/CD 
?/M ). 
To each Chinese sentence, baseNP identification 
always gives the final parsing results in which 
the base noun phrases are distinguished by 
brackets. Some samples are listed. 
1. ??/VV ?/AS  NP (??/NR ??/NN) 
?/DEG  NP(??/JJ ??/NN)  
2. (?/DT ?/M ??/NN ) ??/VV ?/AS 
(???/NN ???/NN) ?/DEG (?/JJ 
?/NN) 
There are two and three base noun phrases in 
sentence 1 and sentence 2 respectively.  
 
(2). Chinese full parsing 
Following sentences are the results of Chinese 
full parsing.  
1. VP (VP (??/VV ?/AS)  NP ( NP ( ??
/NR ??/NN )  ?/DEG  NP ( ??/JJ ?
?/NN ) ) ) 
2.IP ( NP (?/DT ?/M ??/NN )  VP( ??
/VV ?/AS)   NP ( NP (???/NN ???
/NN) ?/DEG  NP (?/JJ ?/NN)))) 
In order to display the parsing result clearly, 
sentence 2 is showed in the tree bank format. 
IP (NP ( DT   ? 
M   ? 
NN  ??) 
VP (VV ?? 
       AS  ?? 
NP ( NP ( NN ??? 
NN ???) 
        DEG ? 
NP ( JJ ? 
NN ?))) 
Type Precision 
(%) 
Recall  
(%) 
Num 
of 
Rules 
BaseNP 87.42 81.4 400 
Full 
parsing 
70.56 67.77 2000 
Table 5 is the results of these types of 
parsing. 
The experimental results show that our parsing 
algorithm, extended GLR parsing algorithm, is 
efficient to both Chinese baseNP parsing and 
full parsing. 
5. Conclusions 
In our system, we present the extended GLR 
parsing algorithm that is based on the Tomita?s 
GLR algorithm. A new grammar system PCFG* 
based on PCFG is proposed to describe the 
grammatical rules that are added probability and 
frequency attributes. So our parsing system 
combines Chinese grammar phenomena with 
statistics distribution. This is feasible and 
efficient to implement Chinese shallow parsing 
and full parsing. In the future task, we further 
improve the efficiency and robust of our parsing 
algorithm and expand Chinese grammatical rules 
with both statistical attributions and language 
information. It is important to utilize the results 
of base noun phrases identification and to 
improve the precision of Chinese full parsing.  
Acknowledgements 
The research work described in this paper is 
supported by the National Nature Science 
Foundation of China under grant number 
9835003 and the National Science Foundation of 
China under grand number 60175012 and the 
National Key Basic Research Program of China 
under grand number G1998030504. 
References  
Masaru Tomita, Efficient Parsing for Natural 
Language ? A Fast Algorithm for Practical 
Systems, Kluwer Academic Publishers, 1986 
Tomita M., an Efficient Augmented-Context-Free 
Algorithm, Computational Linguistics, Volume 13, 
Numbers 1-2, 1987 
Inui K., Sornlertlamvanich V., Tanaka H. and 
Tokunaga T., Probabilistic GLR Parsing: a New 
Formalization and Its Impact on parsing 
Performance, Journal of Natural Language 
Processing, Vol.5, No.3, pp.33-52, 1998 
Sornlertlamvanich V., Inui K., Tanaka H. and 
Tokunaga, T., A New Probabilistic LR Parsing, 
Proceedings of Annual Meeting of the Japan 
Association for Natural Language Processing,  
1997 
Lavie A., GLR*: A Robust Grammar-Focus Parser 
for Spontaneously Spoken Language, Ph.D. thesis, 
Carnegie Mellon University, USA, 1996 
Abney S., Parsing by Chunks, Kluwer Academic 
Publishers, 1991 
Xia F., the Segmentation Guidelines for the Penn 
Chinese Treebank (3.0), 2000 
 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993?1000
Manchester, August 2008
Domain Adaptation for Statistical Machine Translation with Domain 
Dictionary and Monolingual Corpora 
Hua Wu,  Haifeng Wang 
Toshiba (China) R&D Center 
Beijing, 100738, China 
 wuhua@rdc.toshiba.com.cn 
wanghaifeng@rdc.toshiba.com.cn 
Chengqing Zong 
NLPR, Institute of Automation 
Chinese Academy of Sciences 
 Beijing 100080, China 
cqzong@nlpr.ia.ac.cn 
 
Abstract tra
Statistical machine translation systems 
are usually trained on large amounts of 
bilingual text and monolingual text. In 
this paper, we propose a method to per-
form domain adaptation for statistical 
machine translation, where in-domain bi-
lingual corpora do not exist. This method 
first uses out-of-domain corpora to train a 
baseline system and then uses in-domain 
translation dictionaries and in-domain 
monolingual corpora to improve the in-
domain performance. We propose an al-
gorithm to combine these different re-
sources in a unified framework. Experi-
mental results indicate that our method 
achieves absolute improvements of 8.16 
and 3.36 BLEU scores on Chinese to 
English translation and English to French 
translation respectively, as compared 
with the baselines using only out-of-
domain corpora. 
1 Introduction 
In statistical machine translation (SMT), the 
translation process is modeled to obtain the 
translation  of the source sentence f  by 
maximizing the following posterior probability 
(Brown et al, 1993). 
beste
)()(maxarg
)(maxarg
|
|
ee
fee
fe
e
LM
best
pp
p
=
=
 (1)
State-of-the-art SMT systems are trained on 
large collections of bilingual corpora for the 
                                                 
?C 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
nslation model )( | efp  and monolingual tar-
get language corpora for the language model 
(LM) )(eLMp . The trained SMT systems are 
suitable for translating texts in the same domain 
as the training corpus. However, for some spe-
cific domains, it is difficult to obtain a bilingual 
corpus. In this case, the performance of SMT 
systems will be degraded. 
Generally, it is easier to obtain in-domain 
monolingual corpora in either source or target 
language. Moreover, in some specific domains, 
although in-domain bilingual corpora do not ex-
ist, in-domain translation dictionaries, which 
usually contain domain-specific terms and their 
translations, are available. And even if such dic-
tionaries are not available, it is easier to compile 
one than to build a bilingual corpus. Thus, in this 
paper, we address the problem of domain-
specific SMT, where only domain-specific dic-
tionaries and/or monolingual corpora exist. In a 
specific domain, there are two kinds of words: 
common words, which also frequently occur in 
out-of-domain corpora, and domain-specific 
words, which only occur in the specific domain. 
Thus, we can combine the out-of-domain bilin-
gual corpus, the in-domain translation dictionary, 
and monolingual corpora for in-domain transla-
tion. 
If an in-domain translation dictionary is avail-
able, we combine it with the out-of-domain 
translation model to improve translation quality. 
If an in-domain target language corpus (TLC) is 
available, we use it to build an in-domain lan-
guage model, which can be combined with the 
out-of-domain language model to further im-
prove translation quality. Moreover, if an in-
domain source language corpus (SLC) is avail-
able, we automatically translate it and obtain a 
synthetic in-domain bilingual corpus. By adding 
this synthetic bilingual corpus to the training data, 
we rebuild the translation model to improve 
993
translation quality. We can repeatedly translate 
the in-domain source language corpus with the 
improved model until no more improvement can 
be made. This is similar to transductive learning 
described in (Ueffing et al, 2007). 
We perform domain adaptation experiments 
on two tasks: one is the Chinese to English trans-
lation, using the test set released by the Interna-
tio
inese to English translation 
an
MT sys-
tem
lation model and language model adapta-
in domain adaptation for 
del adaptation has been 
 
shared task focused on do-
main adaptation for machine translation among 
Eu
rpora. Adding the extracted bilin-
gu
the performance of a SMT system 
tra
Moses (Koehn et al, 2007). In 
babilities, reorder-
e model probabili-
nal Workshop on Spoken Language Transla-
tion 2006 (IWSLT 2006), and the other is the 
English to French translation, using the data re-
leased by the Second Workshop on Statistical 
Machine Translation (WMT 2007) (Callison-
Burch et al, 2007). 
Experimental results indicate that our method 
achieves absolute improvements of 8.16 and 3.36 
BLEU scores on Ch
d English to French translation respectively, as 
compared with the baselines only using the out-
of-domain corpora. The results on both transla-
tion tasks also show that the translation quality 
achieved by our methods is comparable to that of 
the method using both in-domain and out-of-
domain bilingual corpora. Moreover, even if in-
domain and out-of-domain bilingual corpora are 
available, adding an in-domain dictionary also 
helps to improve the translation quality. 
The remainder of the paper is organized as fol-
lows. In section 2, we describe the related work. 
Section 3 briefly introduces the baseline 
 used in our experiments. Section 4 describes 
our domain adaptation method of using in-
domain dictionary and monolingual corpora. And 
then we present the experimental results in sec-
tions 5. In the last section, we conclude this pa-
per. 
2 Related Work 
Trans
tion are usually used 
SMT. Language mo
widely used in speech recognition (Bacchiani 
and Roark, 2003). In recent years, language 
model adaptation has also been studied for SMT 
(Bulyko et al, 2007). They explored discrimina-
tive estimation of language model weights by 
directly optimizing machine translation perform-
ances such as BLEU score (Papineni et al, 2002).
Their experiments indicated about 0.4 BLEU 
score improvement. 
A shared task is organized as part of the Sec-
ond Workshop on Statistical Machine Transla-
tion. A part of this 
ropean languages. Several studies investigated 
mixture model adaptation for both translation 
model and language model in SMT (Civera and 
Juan, 2007; Foster and Kuhn, 2007). Koehn and 
Schroeder (2007) investigated different adapta-
tion methods for SMT. Their experiments indi-
cate an absolute improvement of more than 1 
BLEU score. 
To enlarge the in-domain bilingual corpus, 
Munteanu and Marcu (2005) automatically ex-
tracted in-domain bilingual sentence pairs from 
comparable co
al corpus to the training data improved the per-
formance of the MT system. In addition, Ueffing 
et al (2007) explored transductive learning for 
SMT, where source language corpora are used to 
train the models. They repeatedly translated 
source sentences from the development set and 
test set. Then the generated translations are used 
to improve the performance of the SMT system. 
This kind of transductive learning can be seen as 
a means to adapt the SMT system to a new type 
of texts. 
In this paper, we use an in-domain translation 
dictionary and/or in-domain monolingual corpora 
(in both source language and target language) to 
improve 
ined on the out-of-domain corpora. Thus, our 
method uses these resources, instead of an in-
domain bilingual corpus, to adapt a baseline sys-
tem trained on the out-of-domain corpora to in-
domain texts. 
3 Baseline MT System 
The phrase-based SMT system used in our ex-
periments is 
Moses, phrase translation pro
ing probabilities, and languag
ties are combined in the log-linear model to ob-
tain the best translation beste  of the source sen-
tence f : 
?
=
?
=
M
p | )(maxarg fee ebest
 (2)
m
mmh
1
,(maxarg f)ee ?
The weights are set by a discriminative train-
ing method using a held-out data set as describ
in (Och, 2003). The models or features which are 
employed by the decoder are (a) one or several 
ph
ed 
rases tables, (b) one or more language models 
trained with SRILM toolkit (Stolcke, 2002), (c) 
distance-based and lexicalized distortion models, 
(d) word penalty, (e) phrase penalty. 
994
Input  Out-of-domain training data OL  
ary D  In-domain translation diction  I
In-domain target language corpus IT  (optional) 
           In-d
, where  represents the general model. 
e 
f 
If  
         
e
ing step: Translate  with  to get a synthetic bilingual corpus 
Un
E
End 
Outpu l  for in-domain translation 
omain source language corpus I  (optional) S
Begin Assign translation probabilities to ID  
If IT  is available 
Training step: (L Estimate= ),, IIO TD?
Els
?
Training step: ),( IO DL Estimate=?  
End i
 S  is availableI
    ? =)0( ?,  0=i
R peat 
 
1+= ii  
Label IS
)1( ?i? IL  
Training step: ),, IIO
til no more improvement can be achieved 
()(i LDL estimate-Re=?  
)(i?  ?=
nd if 
t  Mode  ?
Figure 1.The domain adaptation algorithm 
4 The Framework 
n about the algorithm is 
 our algorithm, a phrase 
available, we train an in-domain LM, which is 
co
ilable, we use the built -linear 
m
). With the 
 order to 
 to assign prob-
nary.  
m
4.1 The Algorithm 
The detailed informatio
shown in Figure 1. In
table and a language model are first constructed 
based on the out-of-domain corpus OL . Then 
probabilities are automatically assigned to the 
entries in the in-domain translation dictionary 
ID , from which another phrase table is con-
structed. At last, the two phrase tables are com-
d. This is the procedure of the training step 
),( IO DL Estimate=? . 
If an in-domain target language corpus is 
bine
mbined with the out-of-domain LM. The built 
phrase tables and LMs are integrated in the log-
linear model as described in section 3. This is the 
procedure of ),,( IIO TDL Estimate=? . 
Moreover, if an in-domain source language 
corpus is ava log
odel to translate the in-domain source texts and 
obtain a synthetic bilingual corpus. And then we 
add the synthetic bilingual corpus into the train-
ing data to improve the current log-linear model 
improved model, we repeatedly translate the in-
domain source texts until no more improvement 
on a development set can be achieved.  
4.2 Dictionary Probabilities 
In general, there is no translation probability in a 
manually-made translation dictionary. In
( estima-Re=? ),,()( IIOi LDL te
construct a phrase table, we have
abilities to the entries in the dictio
Uniform Translation Probability: Since we 
have no parallel corpus to estimate the translation 
probabilities, we simply assign uniform prob-
abilities to the entries in the dictionary. With this 
ethod, if a source word has n  translations, then 
we assign n1  to each translation of this phrase 
for all the four scores of the phrase pair. 
Constant Translation Probability: For each 
entry in the dictionary, we as ign a fixed score. 
In this case e sum of the translation probability 
is not necessarily equal to 1. 
s
, th
anslation probabili-
ties for the entries in the dictionary. And for the 
Corpus Translation Probability: If an in-
domain monolingual source corpus exists, we 
translate it with the method as described in Fig-
ure 1, and then estimate the tr
995
en
s.  
mmonly-used 
tries whose translation probabilities are not 
estimated, we assign average probabilities that 
are calculated from the entries that have obtained 
probabilities. 
4.3 Combining Phrase Tables  
In the algorithm, there are two kinds of phrase 
tables. We need to combine them to translate the 
in-domain text
Mixture Model: The most co
method is linear interpolation.  
)()1()()( ||| fefefe oI ppp ?? ?+=  (3)
W
ion probabili-
ties. 
here )( | feIp  and )( | feop  are the in-
domain and out-of-domain translat
?  is the interpolation weight. 
Discriminative Model: An alternative is to 
two tables in the log-linea  
ranslation oses, for 
en uses them 
fo
he out-of-domain lan-
tigate two 
ation and 
two different tasks: one 
is the Chinese to English translation in IWSLT 
 the other is the English to 
n adaptation translation in the 
red task. 
airs, with about 3 mil-
lio
                                                
combine the r model.
During t with M each phrase in 
the sentence, the decoder obtains all of its trans-
lations in both phrase tables, and th
r translation expansion. 
4.4 Combining Language Models 
If an in-domain target language corpus exists, we 
use it to construct an in-domain language model, 
which is combined with t
guage model. In this paper, we inves
combination methods: linear interpol
log-linear interpolation. 
5 Experiments  
5.1 Setting 
We ran experiments on 
2006 evaluation, and
French domai
WMT 2007 sha
For the Chinese to English translation task, we 
use the Chinese-English bilingual corpus pro-
vided by the Chinese Linguistic Data Consortium 
(CLDC)2 as the out-of-domain corpus. It con-
tains 156,840 sentence p
n English words and about 5 million Chinese 
characters.  In addition, we use the Basic Travel-
ing Expression Corpus (BTEC) released by 
IWSLT 2006 (Paul, 2006) to construct an in-
domain phrase table, as a comparison with that 
one constructed with the in-domain dictionary. 
 
2  http://www.chineseldc.org/EN/index.htm. The catalog 
number is CLDC-LAC-2003-004. It is a balanced corpus 
containing sentence pairs in multiple domains. 
Corpora Sentences OOV 
CLDC 156,840 89 (6.31%) 
BTEC 39,953 179 (12.69%) 
IWSLT06-dev4 489 NA 
IWSLT06-test 500 NA 
Table 1. Ch sh cor
aries Entries 
inese-Engli pora 
Diction OOV 
LDC 82,090 228 (16.16%) 
in-domain 32  .57%) ,821 121 (8
T s 
ces 
able 2. Chinese-English dictionarie
Corpora Senten OOV 
Europarl 949,410 412 (5.90%) 
NC 43,060 599 (8.58%) 
WMT07 dev 1,057 NA 
WMT07 test 2,007 NA 
Table 3. E ch cor
I art and rt a sed 
a in m ual  ex-
p  ta rts of  CLDC and 
BTEC are used for language m  construction 
(see Tab uation, 
nglish-Fren pora 
ts source p  target p
g
a re separately u
ra in ours the in-doma
e
onolin corpo
eriments. Th rget pa  both
odel
le 1). From the IWSLT 2006 eval
we choose the devset4 as our development data. 
Evaluation was performed on IWSLT 2006 test 
set. The references for the test set contain lower-
case words and punctuations. The detailed in-
formation is shown in Table 1. 
We use two kinds of manually-made diction-
aries for comparison: one is the LDC Chinese-
English Translation Lexicon Version 3.0 
(LDC2002L27), and the other is the in-domain 
spoken language dictionary made by ourselves, 
which contains in-domain Chinese words and 
their English translations. The dictionary is ma-
nually constructed. Some entries of the diction-
ary are collected from phrase books. Some of 
them are collected from the general-domain dic-
tionaries. And then, the entries are filtered and 
modified by a Chinese native speaker specialized 
in English. The detailed information is shown in 
Table 2. If a source word has two translations, it 
is counted as two entries. The OOV rates of the 
test set uncovered by the LDC dictionary and the 
in-domain dictionary are 16.16% and 8.57%, 
respectively. 
For the English to French translation task, the 
out-of-domain corpus is the Europarl corpus dis-
tributed for the shared task of WMT 2007 (Calli-
son-Burch et al, 2007)3. We filter the sentence 
pairs whose lengths are above 40 words. For the 
                                                 
3 http://www.statmt.org/wmt07/shared-task.html 
996
in-domain corpus, we use the News Commentary 
(NC) corpus distributed in WMT 2007. We also 
use the same development set and test set in the 
domain adaptation shared task (see Table 3). We 
manually built an in-domain English-French dic-
tionary according to the in-domain bilingual cor-
pus, which includes 26,821 entries. It contains 
in-domain English words and their French trans-
lations. The OOV rate of the test set uncovered 
by this dictionary is 22.34%. 
5.2 Evaluation Measures 
To perform phrase-based SMT, we use the 
rt training scripts. 
efault settings and 
ary 
rase table. With the in-
domain translation dictionary, we construct in-
do
lts, log-linear 
tra
tionary into the training corpus. In 
th
omain 
corpus to train a phrase table. Then we use both  
Moses decoder and its suppo
We run the decoder with its d
then use Moses' implementation of minimum 
error rate training (Och, 2003) to tune the feature 
weights on the development set. Translation 
quality was evaluated using BLEU score (Pap-
ineni et al, 2002).  
5.3 Results on Chinese-English Translation 
Translation Diction
With the out-of-domain bilingual corpus, we 
train an out-of-domain ph
main phrase tables by assigning different 
translation probabilities with two different meth-
ods: uniform and constant. For the constant 
translation probability, we set the score using the 
development set. In our experiments, we set it to 
1. We use the target part of the out-of-domain 
corpus to train a language model4. 
With two phrase tables, we combine them in a 
linear or log-linear method as described in sec-
tion 4.3. In our experimental resu
nslation models outperform the linear models 
(16.38 vs. 15.12), where the entries of the dic-
tionary are assigned with the constant translation 
probabilities. Thus, we will use log-linear models 
for phrase table combination in the following 
experiments. 
Another method to combine the out-of-domain 
corpus and the translation dictionary is to add the 
in-domain dic
is case, only one phrase table is trained. 
Table 4 describes the results using the out-of-
domain corpus and the in-domain dictionary. The 
baseline method only uses the out-of-d
                                                 
4 We also used LDC English Gigaword to train a large lan-
guage model. However, this language model did not im-
prove the translation quality. 
Methods Resources Used BLEU(%)
baseline out-of-domain corpus 13.59 
+dictionary as corpus 15.52 
+uniform prob. 16.00 
+constant prob. 16.38 
baseline + 
dictionary
+corpus prob. 16.72 
Table 4. Translation results of using out-of-
d ictionary
the out-of-dom  the in-dom c-
5 . The 
 also 
im
bine it with the 
 linear inter-
polation and log-linear interpolation. The ex-
pe
oves 
th
ifference be-
tw
 
                                                
omain corpus and in-domain d  
ain corpus and ain di
tionary. The results indicate that adding an in- 
domain dictionary significantly improves the 
translation quality by 2.79 BLEU score
methods using the dictionary as a phrase table 
outperform the method adding it to the training 
corpus. And the method using constant transla-
tion probabilities significantly outperforms that 
using the uniform translation probabilities. 
For comparison, we also assign corpus prob-
abilities to the entries in the dictionary by trans-
lating the source part of the BTEC corpus with 
the method described in Section 4.2. This
proves the translation quality. 
In-Domain Monolingual Corpora 
We use the target part of the BTEC corpus to 
train an in-domain LM. We com
out-of-domain LM in two methods:
rimental results indicate that linear interpola-
tion outperforms log-linear interpolation (17.16 
vs. 16.20). Thus, we will use linear interpolation 
for LMs in all of the following experiments. 
Table 5 describes the results of using the in-
terpolated language model. As compared with 
the results in Table 4, it can be seen that adding 
the in-domain language model greatly impr
e translation quality. It achieves an absolute 
improvement of 3.57 BLEU score as compared 
with the baseline model. If the in-domain transla-
tion dictionary is used, the translation quality is 
further improved by 4 BLEU score. 
If the in-domain source language data is avail-
able, we translate it and obtain a synthetic bilin-
gual corpus. Then we perform transductive learn-
ing as described in Figure 1. The d
een our method and that in (Ueffing et al, 
2007) is that we translate a larger in-domain 
source corpus, and we use 1-best translation 
 
5 We use the method described in (Koehn and Monz, 2006) 
for significance test. In this paper, significant improvement 
means method A outperforms method B on a significance 
level of no less than 95%. 
997
Methods Models  Resources used BLEU(%)
baseline Model 1 out-of-domain corpus 13.59 
baseline + TLC  Model 2 + in-domain TLC 17.16 
Model 3 + in-domain TLC + dictionary (uniform prob.) 20.83 baseline + TLC 
 dictionary (constant prob.) + dictionary Model 4 + in-domain TLC + 21.16 
Model 5 + in-domain SLC 15.98 
Model 6 + in-domain SLC and TLC 18.19 
transductive 
learning 
nd TLC + dictionary (corpus prob.) Model 7 + in-domain SLC a 21.75 
Table 5. 
Diction  BLEU(%)
Translation results of using in-domain resources 
ary types Entries OOV
general domain LDC 228 (1 %) 82,090 6.16 19.11 
manual 121 ) 32,821 (8.57% 21.16 
in-domain 
extracted 11,765 330 (23.39%) 19.88 
LD al C + manu 106,572 45 (3.19%) 21.34 combined 
LDC + extracted 95,660 202 (14.31%) 20.49 
Table 6. Comparison of ictionar
result with full re-training.
that transductive learning 
ed, the transla-
tio
tionaries 
with concern to the translation quality. Besides 
ld as described in (Wu and 
Wang, 2007). 
 phrase table, extract the 
 their translations. 
 
us
 when an in-domain bi-
m
d  different ies 
 The results indicate 
improves translation 
? From the filtered
Chinese words and
quality in all cases. For example, Model 5 
achieves an absolute improvement of 2.39 BLEU 
score over Model 1, and Model 6 achieves 1.03 
BLEU score improvement over Model 2. Model 
7 uses the in-domain dictionary with corpus 
translation probabilities, which are obtained from 
the phrase table trained with the synthetic bilin-
gual corpus. The results indicate that Model 7 
outperforms Model 4, with a significant im-
provement of 0.59 BLEU score.  
The results also indicate that when only the in-
domain monolingual corpus is us
n quality is improved by 4.6 BLEU score 
(Model 6 vs. Model 1). By adding the in-domain 
dictionary, the translation quality is further im-
proved, achieving an absolute improvement of 
8.16 BLEU score (Model 7 vs. Model 1). 
Comparison of Different Dictionaries 
We compare the effects of different dic
the manually-made in-domain dictionary, we use 
other two dictionaries: the LDC dictionary and 
an automatically built dictionary, which is ex-
tracted from the BTEC corpus. This extracted 
dictionary only contains Chinese words and their 
translations. The extraction method is as follows:  
? Build a phrase table with the in-domain bi-
lingual corpus. 
? Filter those phrase pairs whose values are 
below a thresho
? Assign constant translation probabilities to 
the entries of the extracted dictionary. 
Table 6 shows the translation results. All of 
the methods use the out-of-domain corpus, the 
in-domain target language corpus, and the corre-
sponding translation dictionaries with constant 
translation probabilities. The results indicate that
ing the general-domain dictionary also im-
proves translation quality, achieving an im-
provement of about 2 BLEU score as compared 
with Model 2 in Table 5. It can also be seen that 
the in-domain dictionaries significantly outper-
form the LDC dictionary although the extracted 
dictionary has a higher OOV rate than the LDC 
dictionary. Further analysis shows that the LDC 
dictionary does not contain the in-domain trans-
lations of some words. Results also indicate that 
combining the two kinds of dictionaries helps to 
slightly improve translation quality since the 
OOV rates are reduced. 
Comparison with In-domain Bilingual Corpus 
The aim of this section is to investigate 
whether the in-domain dictionary helps to im-
prove translation quality
lingual corpus is available. And we will also 
compare the translation results with those of the 
ethods only using in-domain dictionaries and 
monolingual corpora. 
To train the in-domain translation model, we 
use the BTEC corpus. The translation results are 
998
13
14
15
16
17
18
19
20
21
22
23
100k 200k 300k all
In-domain sentence pairs
BL
EU
 (
%)
CLDC
BTEC
CLDC+BTEC
CLDC+BTEC+Dic
CLDC+Mono+Dic
Figure 2. Comparison of different methods using 
different resources. 
shown in Figure 2. CLDC and BTEC represent 
s linear interpolation of the 
Interpolated LM" means that 
the methods that only use the out-of-domain and 
the in-domain corpus, respectively. The method 
"CLDC+BTEC" use
phrase tables and LMs trained with CLDC and 
BTEC.   "Dic" means using the in-domain dic-
tionary, and "Mono" means using in-domain 
source and target language corpora. 
From the results, it can be seen that (a) even if 
an in-domain bilingual corpus exists, the in-
domain dictionary also helps to improve the 
translation quality, as "CLDC+BTEC+Dic" 
achieves an improvement of about 1 BLEU score 
in comparison with "CLDC+BTEC"; (b) the 
method "CLDC+Mono+Dic", which uses both 
the in-domain monolingual corpora and the in-
domain dictionary, achieves high translation 
quality. It achieves slightly higher translation 
quality than "CLDC+BTEC" that uses the in-
domain bilingual corpus (21.75 vs.  21.62)  and 
achieves slightly lower translation quality than 
"CLDC+BTEC+Dic" (21.75 vs. 22.05). But the 
differences are not significant. This indicates that 
our method using an in-domain dictionary and 
in-domain monolingual corpora is effective for 
domain adaptation. 
5.4 Results on English-French Translation 
We perform the same experiments for English to 
French translation. Table 7 describes the domain 
adaptation results. "
we use the target part of the NC corpus to train 
an in-domain LM, and then linearly interpolate it 
with the out-of-domain LM trained with the Eu-
roparl corpus. The results indicate that using an 
in-domain target corpus significantly improves 
the translation quality, achieving an improve-
ment of 2.19 BLEU score (from 25.44 to 27.63).  
Methods Out-of-domain LM 
Interpolated 
LM 
Europarl 25.44 27.63 
Europarl+Dic 26.24 28.22 
transductive 
learning - 2  8.80
Europarl+NC - 29.19 
Europarl+NC+Dic - 29.41 
T ation result f using i in 
d onolingual corpora 
Using the in-domain translation dictionary im-
used (from 
29.19 to 29.41). 
n Table 7. The results indicate 
th
28.80). Although the translation quality 
is 
This 
 an out-of-domain corpus to 
ystem, and then used an in-
ion dictionary, to improve the transla-
able 7. Transl
ictionary and m
s o n-doma
proves translation quality in all cases, even when 
the in-domain bilingual corpus is 
We also perform transductive learning with 
the source part of the NC corpus. The model 
used to translate the corpus is that one created by 
"Europarl+Dic" i
at transductive learning significantly improves 
translation quality, achieving an absolute im-
provement of 0.58 BLEU score (from 28.22 to 
28.80).  
In summary, using an in-domain dictionary 
and in-domain monolingual corpora improves the 
translation quality by 3.36 BLEU score (from 
25.44 to 
slightly lower than that method of using both 
in-domain and out-of-domain bilingual corpora, 
the difference is not statistically significant. 
6 Conclusion 
This paper proposed a domain adaptation ap-
proach for statistical machine translation. 
approach first used
build a baseline s
domain translation dictionary and in-domain 
monolingual corpora to adapt it to the in-domain 
texts. The contribution of this paper lies in the 
following points: 
? We proposed a method to integrate a do-
main-specific translation dictionary into a 
phrase-based SMT system for domain adap-
tation. 
? We investigated the way of using in-domain 
monolingual corpora in either source or tar-
get language, together with the in-domain 
translat
tion quality of a baseline system. 
999
We performed experiments on both Chinese to 
English and English to French translation. Ex-
perimental results on Chinese to English transla-
tio
-
vised Language Model Adaptation. In Proc. of the 
ational Conference on Acoustics, 
 Signal Processing (ICASSP-2003), 
Br
n. Computational Linguistics, 
Bu
f the 32nd International Confer-
Ca
 Statistical Ma-
Ci
 
, pages 177-180. 
Fo
Ko
ation of Machine Translation be-
Ko
erico, Nicola Bertoldi, 
Ko
istical Ma-
M
Oc
ranslation. In Proc. of 
Pa
2. BLEU: a Method for Auto-
Pa
ation Campaign. In Proc. of the International 
St
 Proc. of International 
Ue
rning for Statistical 
W
ics and Phrase-
n indicate that all of the in-domain resources 
are useful to improve in-domain translation qual-
ity, with an overall improvement of 8.16 BLEU 
score as compared with the baseline trained with 
out-of-domain corpora. Results on English to 
French translation also show that using in-
domain translation dictionaries and in-domain 
monolingual corpora is effective for domain ad-
aptation, achieving an absolute improvement of 
3.36 BLEU score. And the results on both trans-
lation tasks indicate that the translation quality 
achieved by our methods is comparable with that 
of the method using both in-domain and out-of-
domain bilingual corpora. Moreover, even if in-
domain and out-of-domain bilingual corpora are 
available, adding an in-domain dictionary also 
helps to improve the translation quality. 
In the future work, we will investigate to as-
sign translation probabilities to the dictionaries 
using comparable in-domain corpora and exam-
ine its effect on the MT performance. And we 
will also examine the effect of an in-domain dic-
tionary on transductive learning in more details. 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper
28th Intern
Speech, and
pages 224-227. 
own, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimatio
19(2): 263-311. 
lyko, Ivan, Spyros Matsoukas, Richard Schwartz, 
Long Nguyen, and John Makhoul. 2007. Language 
Model Adaptation in Machine Translation from 
Speech. In Proc. o
ence on Acoustics, Speech, and Signal Processing 
(ICASSP-2007), pages 117-120. 
llison-Burch, Chris, Cameron Fordyce, Philipp 
Koehn, Christof Monz, and Josh Schroeder. 2007. 
(Meta-) Evaluation of Machine Translation. In 
Proc. of the Second Workshop on
chine Translation, pages 136-158. 
vera, Jorge and Alfons Juan. 2007. Domain Adapta-
tion in Statistical Machine Translation with Mix-
ture Modelling. In Proc. of the Second Workshop
on Statistical Machine Translation
ster, George and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second 
Workshop on Statistical Machine Translation, 
pages 128-135. 
ehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evalu
tween European Languages. In Proc. of the HLT-
NAACL 2006 Workshop on Statistical Machine 
Translation, pages 102-121. 
ehn, Philipp, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Fed
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proc. of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2007), 
demonstration session, pages 177-180. 
ehn, Philipp and Josh Schroeder. 2007. Experi-
ments in Domain Adaptation for Stat
chine Translation. In Proc. of the Second Workshop 
on Statistical Machine Translation, pages 224-227. 
unteanu, Dragos Stefan, and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
h, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine T
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), pages 160-
167. 
pineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 200
matic Evaluation of Machine Translation. In Proc. 
of the 40th Annual Meeting of the Association of 
Computational Linguistics (ACL-2002), pages 311-
318. 
ul, Michael. 2006. Overview of the IWSLT 2006 
Evalu
Workshop on Spoken Language Translation 
(IWSLT-2006), pages 1-15. 
olcke, Andrea. 2002. SRILM -- an Extensible Lan-
guage Modeling Toolkit. In
Conference on Spoken Language Processing 
(ICSLP-2002), pages 901-904. 
ffing, Nicola, Gholamreza Haffari, and Anoop 
Sarkar. 2007. Transductive Lea
Machine Translation. In Proc. of 45th Annual 
Meeting of the Association of Computational Lin-
guistics (ACL-2007), pages 25-32. 
u, Hua and Haifeng Wang. 2007. Comparative 
Study of Word Alignment Heurist
Based SMT. In Proc. of Machine Translation 
Summit XI, pages 507-514. 
1000
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1089?1096
Manchester, August 2008
    Sentence Type Based Reordering Model for Statistical Machine 
Translation 
Jiajun Zhang, Chengqing Zong, Shoushan Li 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{jjzhang, cqzong, sshanli}@nlpr.ia.ac.cn 
 
 
Abstract 
Many reordering approaches have been 
proposed for the statistical machine 
translation (SMT) system. However, the 
information about the type of source 
sentence is ignored in the previous 
works. In this paper, we propose a group 
of novel reordering models based on the 
source sentence type for Chinese-to-
English translation. In our approach, an 
SVM-based classifier is employed to 
classify the given Chinese sentences into 
three types: special interrogative sen-
tences, other interrogative sentences, and 
non-question sentences. The different 
reordering models are developed ori-
ented to the different sentence types. 
Our experiments show that the novel re-
ordering models have obtained an im-
provement of more than 2.65% in BLEU 
for a phrase-based spoken language 
translation system.  
1 Introduction 
The phrase-based translation approach has been 
the popular and widely used strategy to the sta-
tistical machine translation (SMT) since Och, et 
al. (2002) proposed the log-linear model. How-
ever, reordering is always a key issue in the de-
coding process. A number of models have been 
developed to deal with the problem of reorder-
ing. The existing reordering approaches could 
be divided into two categories: one is integrated 
into the decoder and the other is employed as a 
preprocessing module.   
                                                 
  ? 2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
Many reordering methods belong to the for-
mer category. Distortion model was first em-
ployed by Koehn et al (2003); a lexicalized re-
ordering model was proposed by Och et al 
(2004) and Koehn et al (2005); and the formal 
syntax-based reordering models were proposed 
by Chiang (2005) and Xiong et al (2006). It is 
worthy to note that little syntactic knowledge is 
used in the models mentioned above. 
Compared to the reordering models that are 
integrated into the decoder, the reordering at the 
source side can utilize more syntactic knowl-
edge, with the goal of  adjusting the source lan-
guage sentence to make its word order closer to 
that of the target language. The most notable 
models are given by Xia and McCord (2004), 
Collins et al (2005), Li et al (2007) and Wang 
et al (2007). Xia and McCord (2004) parsed the 
source and target sides of the training data and 
then automatically extracted the rewriting pat-
terns. The rewriting patterns are employed on 
the input source sentence to make the word or-
der more accordant to target language. Collins et 
al. (2005) described an approach to reorder Ger-
man in German-to-English translation. The 
method concentrates on the German clauses and 
six types of transforming rules are applied to the 
parsed source sentence. However, all the rules 
are manually built. Li et al (2007) used a parser 
to get the syntactic tree of the source language 
sentence. In this method, a maximum entropy 
model is developed to determine how probable 
the children of a node are to be reordered. Obvi-
ously, there is also disadvantage in this method 
because the parsing tree is obtained by a full 
parser and contains too many nodes that are not 
involved in desired reorderings. Wang et al 
(2007) discussed three categories which are con-
sidered to be the most prominent candidates for 
reordering in Chinese-to-English translation, 
including verb phrases (VPs), noun phrases 
(NPs), and localizer phrases (LCPs). The 
1089
method deals with some special modifiers of 
VPs and NPs because they have the property 
that some specific modifiers appear before VPs 
or NPs in Chinese but occur after VPs or NPs in 
its English translation. We observe that all the 
transformation rules in this method are hard 
crafted. Furthermore, there are some other re-
lated works, such as Costa-jussa and Fonollosa?s 
work (2006) and Zhang et al?s work (2007). 
Costa-jussa and Fonollosa (2006) considered the 
source reordering as a translation task which 
translates the source sentence into reordered 
source sentence. A chunk-level reordering 
model was first proposed by Zhang et al (2007). 
However, all the existing models make no 
distinction between the different types of the 
source sentence. Intuitively, we have different 
reordering information in different sentence type. 
Taking Chinese special interrogative sentence as 
an example, there is a fixed phrase that usually 
occurs at the end of Chinese sentence but ap-
pears at the beginning part of its English transla-
tion. See the following Chinese to English trans-
lation: 
Chinese: ? ? ? ??? ? ?? ? 
English: What kind of seats do you like ? 
Obviously, the Chinese question phrase ??
?? ? ?? (What kind of seats)? should be 
put at the beginning of its English translation. 
However, many phrase-based systems fail to do 
this. 
In this paper, we are interested in investigat-
ing the value of Chinese sentence types in reor-
dering for Chinese-to-English spoken language 
translation. Due to the syntactic difference be-
tween Chinese and English, different sentence 
type provides different reordering information. 
A phrase-ahead model is developed to exploit 
and utilize the reordering information of special 
interrogative sentences. A phrase-back model is 
employed to catch and make use of the reorder-
ing information of other sentence types. How-
ever, the sentence type should be first identified 
by an SVM-based classifier before reordering 
the source sentence. The method overall is used 
as a preprocessing module for translation. We 
will introduce our method in detail later. 
 The remainder of this paper is organized as 
follows: Section 2 introduces our motivations; 
Section 3 gives the details on the implementa-
tion of our approach; the experiments are shown 
in Section 4; and the final concluding remarks 
are given in Section 5. 
2 Our Motivations 
In this section, before we analyze the Chinese-
to-English spoken language translation corpus,  
some definitions are given first. 
2.1 Definitions 
z Special interrogative sentence / other inter-
rogative sentence / non-question sentence 
Chinese sentence can be divided into question 
sentence and non-question sentence. If a Chi-
nese question sentence is translated into the 
English sentence of wh-questions, the sentence 
is named as a Chinese special interrogative sen-
tence; otherwise, it is called the Chinese other 
interrogative sentence. Figure 1-3 show some 
examples for the three sentence types respec-
tively. 
z SQP / TP / SP 
In Chinese special interrogative sentence, the 
question phrase is always moved ahead while it 
is translated into English. Correspondingly, the 
question phrase is named as the special question 
phrase (SQP). For example, the question  phrase  
???? ? ?? (What kind of seats)? in the 
example mentioned above is an SQP.  
A few quantifier phrases (QPs) like ?? ? 
(many times)?, ??? ? (many years)? in Chi-
nese and some LCPs like ??? ?? ? (after 
the accident happened)?, ??? ?? ? (before 
the meeting ends)? together with some NPs like 
temporal phrases are named temporal phrase 
(TP) in our model. Some LCPs like ??? ? (at 
the front of the hotel)?, ??? ? (near the ta-
ble)? and a few NPs like spatial phrases are 
called spatial phrase (SP) in our model. As PPs1, 
TPs and SPs are the most prominent candidates 
for reordering in Chinese other interrogative 
sentences and non-question sentences, they will 
be handled in the phrase-back reordering model.  
 
Figure 1.  An example of Chinese special inter-
rogative sentence with its English translation. 
 
Figure 2.  An example of Chinese other inter-
rogative sentence with its English translation. 
                                                 
1 PPs here mean prepositional phrases 
?  ?  ?  ??  ?  ? 
Can you speak Japanese ? 
?  ?  ?  ???  ?  ??  ? 
What kind of seats do you like ? 
1090
My wallet was stolen in the subway . 
 
Figure 3.  An example of Chinese non-question 
sentence with its English translation. 
2.2 Analysis of Corpus and  Our Motivations 
In order to have an overview of the distribution 
of the Chinese sentence types, we have made a 
survey based on our training set for translation, 
which contains about 277k Chinese and English 
sentence pairs. We found that about 17.2% of 
the sentences are special interrogative sentences, 
about 25.5% of sentences are other interrogative 
sentences and the remainders are all non-
question sentences. 
Each sentence type has its own reordering 
strategy, as demonstrated in Figures 1-3. There 
is a settled phrase (SQP) in Chinese special in-
terrogative sentence which usually appears at 
the end but will be translated first in English, 
just as Figure 1 illustrates. For other interroga-
tive sentences, some specific Chinese words like 
???????? will just be translated into 
?Can? or ?Do?  and come first in English. At 
present, this information is not used in our ap-
proach. Figure 2 gives an example. For non-
questions, the reordering candidates usually 
need to be moved back during translation. An 
example is shown in Figure 3. 
According to the analysis above, it is mean-
ingful to develop reordering models based on 
the source sentence types. 
2.3 Framework 
As we mentioned above, our framework is illus-
trated as follows: 
 
Figure 4.  Architecture of the framework, where 
C1 means the special interrogative sentence, C2 
is other interrogative sentence and C3 is non-
question sentence. 
 
Conventional preprocessing approaches di-
vide the translation into two phases: 
                                       (1) 'S S T? ?
'
'
'cS S S T? ? ?
c
cS
'S
                                                
Reordering is first done in the source side 
which changes the source sentence S into reor-
dered one S , and then a standard phrase-based 
translation engine is used to translate the reor-
dered source sentence S  into target language 
sentence T. 
? ??  ? ?? ? ?? ? ? 
In our method, to utilize the information of 
sentence types, a new approach is proposed to 
improve the translation performance by devel-
oping a hybrid model as follows: 
                      (2) 
Before the source sentence is reordered, an 
SVM-based classifier is first employed to de-
termine its sentence type S , then, different re-
ordering model is used to reorder the source 
sentence with the specific sentence type . Af-
ter getting the reordered source sentence , we 
use our phrase-based SMT to obtain the optimal 
target language sentence.  
The contribution of this paper is embodied in 
the first two steps of our method. 
In the first step, an SVM classifier is used to 
identify the type of source sentence2.  
In the second step, two reordering models are 
built according to the different sentence types. A 
phrase-ahead reordering model is developed for 
the special interrogative sentences which uses 
shallow parsing technology to recognize the 
most prominent candidates for reordering (spe-
cial question phrase) and extracts reordering 
templates from bilingual corpus. For other sen-
tence types, we build a phrase-back reordering 
model which uses shallow parsing technology to 
identify the phrases that are almost always 
moved back during translation and applies 
maximum entropy algorithm to determine 
whether we should reorder them. 
Source text 
sentence 3 Models and Algorithms 
In this section, we first introduce the sentence 
type classifier model, and then we describe in 
detail the two reordering models, phrase-ahead 
reordering model and phrase-back reordering 
model. 
3.1 Sentence Type Identification 
Many models are used for classification such as 
Na?ve Bayes, decision tree and maximum en-
tropy. In our approach, we use an SVM-based 
classifier to classify the sentence types. SVM 
 
2 There are three sentence types: special interrogative sen-
tence, other interrogative sentence and non-question sen-
tence, which are defined in sub-section 2.1. 
 
Target 
sentence 
C1 
C3 
C2 
Phrase-ahead 
model 
Phrase-back 
model 
Phrase-
based 
decoder 
SVM 
classifier 
Phrase-back 
model 
1091
has been shown to be highly effective at tradi-
tional text categorization. For our problem, we 
regard a sentence as a text. The decision bound-
ary in SVM is a hyperplane, represented by vec-
tor , which separates the two classes, leaving 
the largest margin between the vectors of the 
two classes (Vapnik, 1998). The search of mar-
gin corresponds to a constrained optimization 
problem. Suppose 
w
G
{1, 1}jc ? ? (positive and 
negative) be the correct class of sentence js , the 
solution can be formalized as: 
: j j j
j
w c?=?G Gs 0j   ? ?              (3) 
Where the js
G
 is feature vector of our sen-
tence js .  We get j? s through solving a dual 
optimization problem. Identifying the type of a 
sentence is just to determine which side of w
G
?s 
hyperplane it will fall in. 
Feature selection is an important issue. We 
directly use all the words occurring in the sen-
tence as features. 
Some readers may argue that the features to 
distinguish the sentence types are very obvious 
in Chinese. For example, ??? can easily sepa-
rate the interrogative sentences from non-
question sentences. In this case, a simple classi-
fier like decision tree will work. It is true when 
the punctuation always appears in the sentence. 
However, sometimes there is no punctuation in 
the spoken language text. Under this situation, 
the decision tree will lose the most powerful 
features, but the performance of SVM is not af-
fected by the punctuations. The experimental 
results verifying this will be given in Section4. 
3.2 Phrase-ahead Reordering Model 
As we mentioned above, about 17.2% of the 
spoken language sentences are special interroga-
tive sentences. Furthermore, we note that each 
Chinese special interrogative sentence has one 
or more special question phrases (SQP) that we 
defined in section 2.1. Due to the difference be-
tween Chinese and English word order, the SQP 
needs to be moved ahead3 when it is translated 
into English. 
    Let S be a Chinese special interrogative sen-
tence, our first problem is to recognize the SQPs 
in S. If we have known the SQP, namely S be-
comes  (  is the left part of  the 0    S SQP S
                                                
1 0S
 
1S
0S
3 There is a specific situation that the SQP don?t have to be 
moved. In this case, we suppose it needs to be moved, but 
the distance is 0. 
sentence before SQP, and  is the right part of 
the sentence after SQP), our second problem is 
to find the correct position in where SQP will 
be moved to. 
 For the first problem, because each syntactic 
component is possible a SQP, for example, ??
?? ? ??? in Figure 1 is NP, ?? ??
(Where)? in Chinese sentence ?? ? ?? ? 
? ? ? ?(Where can I buy the ticket?)? is 
PP (also a VP modifier), ???  ?  (How to 
go)? in ?? ?? ?? ? ?(How to go to the 
beach?)? is VP, it is very difficult to find the 
SQP by syntax. In our model, we first find out 
all the key words, which we list below, in the 
special interrogative sentences through mutual 
information. Then, we define the syntactic com-
ponent containing the key word as an SQP. In-
stead of full syntactic parser, we utilize a CRF 
toolkit named FlexCrfs4 to train, test and predict 
the SQPs chunking. 
 
?? What 
? (?? / ???) Where 
? (?? / ???) How much/many/old?
? (??/??? ?) What about/How 
? (?? / ???) Who/whose/whom 
? (?? / ???) How many/old When?
??? Why 
?(?? / ???) When/where 
Table 1.  The special key words set 
 
For the second problem, we note that there 
are only three positions where the SQP will be 
moved to:  (1) the beginning of the sentence; (2) 
just after the rightmost punctuation (?,?, ?;? or 
?:?) before the SQP; (3) or after a regular phrase 
such as ???  (May I ask)? and ???  ? 
(Please tell me)?. Therefore, we can learn the 
reordering templates from bilingual corpus 5 . 
The simple algorithm is illustrated in Figure 5, 
and some reordering templates are shown in Ta-
bl
                                                
e 2.  
On the whole, When we reorder the special 
interrogative sentence, we first identify the SQP, 
then we find out whether there are punctuations 
(?,? , ?;? or ?:?) before SQP; if any, we keep the 
rightmost punctuation index, otherwise we keep 
the index 0 (beginning of sentence). In the third 
 
4 See http://flexCRF.sourceforge.net 
5 The bilingual corpus is the corpus combined by the train-
ing corpus for chunking SQPs and its corresponding Eng-
lish translation. 
1092
step, if we find that a reordering template like 
some one given in Table 2 can match the sen-
tence, we just apply the template, otherwise we 
just move the SQP after the index that we kept 
efore (0 or punctuation index). 
 
 empirical value N is 10 in our ex-
eriment. 
 
b
 
Figure 5. Reordering template extraction algo-
rithm. The
p
X1?? X2 SQP X1 ?? SQP X2 
X1 ?? ? X2 SQP X1 ?? ? SQP X2 
X1 P X1 ? ? ?? X2 SQ ? ? ?? SQP X2
X1 ? SQP X1 ? P X2? X2 ? SQ  
?? ?? 
 Table 2.  Some reordering templates 
3.3 Phrase-back Reordering Model 
In this paper, we employ the phrase-back reor-
dering model for Chinese other interrogative 
 posi-
tio
makes our model suitable for 
m e
 
??? (sign your name)? 
 identified as a NP. 
 
 
z The form of phrase-back reordering rules: 
sentences and non-question sentences. 
   Inspired by the work of Wang et al (2007), 
we only consider the most prominent candidates 
for reordering. The VP modifiers like PP, TP, 
and SP which we defined in sub-section 2.1 are 
typically in pre-verb position in Chinese but al-
most always appear after the verb in its corre-
sponding English translation. Wang et al (2007) 
concentrate on VP, NP, then determine whether 
their modifiers should be moved back. Instead, 
our interests are focused on the modifiers: PP, 
TP and SP; namely, we consider the modifiers 
PP, TP and SP as triggers, and the first VP oc-
curring after triggers will be the candidate
n where the triggers may be moved to. 
Changing the focus gives us the ability to 
handle a specific situation that there is no VP 
after the triggers for recognition error or other 
reasons. As the example in Figure 6, there is no 
VP after PP (?? ???) because the phrase ??
?? next to PP is wrongly recognized to be a NP. 
To deal with the case, we will further define a 
fake verb phrase (FVP): the phrase after PP (TP 
or SP) until the punctuation (?,?, ?;? or ?.?). The 
phrase ??? (sign your name)? in Figure 6 is 
an FVP. Here, FVP is given the same function 
with VP, thus it 
or  situations. 
 
Figure 6.  An example of FVP. In our model the 
whole sentence is recognized as a VP, ?? ?? 
(here)? is a PP, and 
is
Unlike hard reordering rules of Wang et al 
(2007), we develop a probabilistic reordering 
model to alleviate the impact of the errors 
caused by the parser when recognizing PPs, TPs, 
SPs and VPs. We believe that no reordering is 
better than bad reordering. The rule forms and 
1:  Input: special interrogative sentence pair (s, t) in which 
se which aligns to 
ndex-1] 
NONE then 
; 
_Phrase if Count(C_Phrase)<N 
SQP is labeled and their alignment M is given 
2:  R={} 
3: Find the rightmost punctuation index c_punc_index before 
SQP and English index e_punc_index aligned to 
c_punc_index 
4: Find the smallest index e_smallest_index of English which 
align to the SQP  
 C_Phra5: Get the Chinese phrase
[e_punc_index+1, e_smallest_i
6:  if C_Phrase is 
7:       Continue ; 
8:  end if 
Phrase in R then 8:  if C_
9:       Count(C_Phrase)++; 
10: else 
11:     Insert C_Phrase into R
12:     Count(C_Phrase)=1; 
13: end if 
14: remove C
? ? ??   ?? ? 
the probabilistic model will be given as follows:
A : 1 22
2 1
A XA straight
A XA1 XA A inver
?? ??
 
Where, 1 { , , }A PP TP SP
ted
?   { , }VP FVP?   2A
1 2{ }X phrases between A  and A?  
z We use the Maximum Entropy Model  
which is implemented by Zhang6.  The model is 
trained from bilingual spoken language corpus 
determine whether 1A  should be moved after 
2A . The features that we investigated include 
the leftmost, rightmost, and their POSs 
to 
of 1A  
and 2A . It leads to the following formula: 
exp( ( , ))
( | )
exp( ( , ))
i ii
i iO i
h O A
P O A
h O A
?
?=
?
? ?           (4) 
sWhere, { , }O traignt inverted? , ( , )ih O A  is a 
feature, and i? is the weight of the feature. 
When app  the rules, we first identify 
pairs like ( 1 2A XA ) in the sentence, and then 
m beginning t  
1A  behind 2A  if ( | ) ( | )P inverted A P straight A> . 
After all the pairs are pr
lying
fro o end of the sentence, we move
ocessed, we will get the 
reordered source result. 
                                                 
6http://homepages.inf.ed.ac.uk/s0450736/maxent_too
lkit.html 
15: output R 
1093
4 Experiments 
We have conducted several experiments to 
evaluate the models.  In this section, we first 
introduce the corpora, and then we discuss the 
performance of the SVM-based classifier, 
chunking and reordering models respectively. 
4.1 Corpora 
We perform our experiments on Chinese-to-
English speech translation task. The statistics of 
the corpus is given in Table 3 where CE_train 
means the Chinese-to-English training data re-
leased by IWSLT 2007; CE_sent_filtered means 
the bilingual sentence pairs filtered from the 
open resources of the bilingual sentences on the 
website; CE_dict_filtered means the bilingual 
dictionary filtered from the open resources of 
the bilingual dictionaries on the website; 
CE_dev123 denotes the bilingual sentence pairs 
obtained by the combination of the development 
data IWSLT07_CE_devset1, IWSLT07_CE_devset2 
and IWSLT07_CE_devset3 which are released 
by the IWSLT 2007; CE_dev4 and CE_dev5 are 
the remainder of development data released by 
IWSLT 2007; CE_test means the final test set 
released by IWSLT 2007. 
We combine the data from the top four rows 
as our training set. We use CE_dev4 as our de-
velopment set. CE_dev5 and CE_test are our 
two test data. The test data released by IWSLT 
2007 is based on the clean text with punctuation 
information, so we add the punctuation informa-
tion on the Chinese sentences of CE_dev4 and 
CE_dev5 by our SVM sentence type classifier to 
form the final development set. The detailed 
statistics are given in Table 4. 
4.2 Classification Result 
To evaluate the performance of SVM-based 
classifier on classifying the sentence types, we 
first use a simple decision tree to divide the 
Chinese sentences of our training data for trans-
lation into three sentence types. Then we clean 
them by hand in order to remove the errors. At 
last, 10k sentences for each sentence type are 
randomly selected as the experiment data. For 
each sentence type, 80% of the data are used as 
training data, 20% as test data. Table 5 gives the 
classification results. 
Punctuation in Table 5 means the punctuation 
which occurs at the end of the sentence such as  
??? and ???. We can see from the table that 
SVM classifier performs very well even if we 
remove the punctuations at the end of every sen-
tence. Therefore, almost no errors will be passed 
to the reordering stage. 
 
Data Chinese English 
CE_train 39,953 39,953 
CE_sent_filtered 188,282 188,282 
CE_dict_filtered 31,132 31,132 
CE_dev123 24,192 24,192 
CE_dev4 489 3,423 
CE_dev5 500 3,500 
CE_test 489 2,934 
Table 3.  Statistics of training data, development 
data and test data 
 
 Chinese English 
sentences 276,633 
Train set
words 1,665,073 1,198,984
sentences 489 489*7 Dev set  
CE_dev4 words 6241 47609 
sentences 500 500*7 Test set  
CE_dev5 words 6596 52567 
sentences 489 489*6 Test set   
CE_test words 3166 22574 
Table 4.  Detailed statistics of training data on 
development set 
 
 Accuracy (%)
With punctuation 99.80 
Without punctuation 98.00 
Table 5.  The accuracy of SVM classifier 
4.3 Chunking Results 
In our experiment, except that VPs are obtained 
by a syntactic parser (Klein and Manning, 2003),  
SQPs, PPs, TPs, SPs are all chunked by the 
FlexCrfs. 
The chunking data used for training and test 
in Table 6 are annotated by ourselves. Every 
chunk is  annotated according to the definition 
that we define in sub-section 2.1. The raw train-
ing and test data are all extracted from our train-
ing set for translation. TPs, SPs are annotated 
together; SQPs, PPs are annotated respectively. 
The statistics of the training and test data are 
shown in Table 6. Table 7 gives the chunking 
results. 
The precision, recall and F-Measure are met-
rics for the chunking results. F-Measure follows 
the criteria of CoNLL-20007.  
2*( * )precision recall
F Measure  
precision recall
? = +
                                                 
7 See  http://www.cnts.ua.ac.be/conll2000/chunking/ 
1094
Because the SQPs have the regularity that 
each one contains a key word listed in Table 1, 
the result of SQPs chunking is quite good. 
Moreover, the chunking of PPs, TPs and SPs 
also performs well. 
 
 Train Test 
sentences 10,000 500 SQP 
chunks 10030 501 
sentences 10,000 500 PP 
chunks 10106 512 
sentences 11,000 500 SP and TP 
chunks 10342 523 
Table 6.  Statistics of train and test data 
 
 Precision (%) 
Recall 
(%) 
F-Measure 
(%) 
SQP 95.52 95.52 95.52 
PP 94.65 93.31 93.98 
SP and TP 93.92 92.68 93.25 
Table 7.  Chunking results on test set 
4.4 Translation Results 
For the translation experiments, BLEU-4 and 
NIST are used as the evaluation metric. The 
baseline SMT uses the standard phrase-based 
decoder that applies the log-linear model (Och 
and Ney, 2002).  
  In the preprocessing module, all the Chinese 
words are segmented by the free software toolkit 
ICTCLAS3.08, and the POS tags are obtained 
by using the Stanford parser with its POS pars-
ing function. For the decoder, the phrase table is 
obtained as described in (Koehn et al, 2005), 
and our 4-gram language model is trained by the 
open SRILM9 toolkit. It should be noted that we 
use monotone decoding in translation. 
We have done three groups of experiments 
for translation. The first one is to test the effect 
of phrase-ahead reordering model, the result of 
which is shown in Table 8. Compared to the 
baseline system, phrase-ahead reordering model 
improves the results of the two test sets by 
0.41% and 1.87% in BLEU respectively. The 
difference in the performance gains can be at-
tributed to the fact that there are 100 Chinese 
special interrogative sentences in Test 2, while 
only 30 are found in Test 1. Accordingly, the 
reordering candidates of Test 1 are much fewer 
than that of Test 2. Thus, we can conclude that 
the more special interrogative sentences the bet-
ter performance of the translation. Furthermore, 
                                                 
8 See http://www.nlp.org.cn 
9 See http://www.speech.sri.com/projects/srilm 
the results show that the reordering on special 
interrogative sentences is a good try. 
The second experiment is conducted to test 
the effect of phrase-back reordering model. Ta-
ble 8 gives the results. For the two test sets, the 
model brings an improvement to the baseline by 
2.24% and 0.93% in BLEU respectively. How-
ever, the difference between them is still very 
big. We think there are two reasons: firstly, 
there are much more special interrogative sen-
tences in Test 2 than in Test 1, so the sentences 
of other sentence types in Test 2 are much fewer 
than that in Test 1. Thus, fewer candidates are 
found in Test 2 than in Test 1. Secondly, the 
average sentence length of Test 2 (6.5 words) is 
much shorter than that of Test 1 (13.2 words). 
We know that if the sentence is very short, the 
PP, TP, and SP will seldom occur. Naturally, 
only 89 candidates are found in Test 2 but 366 
in Test 1. Regardless of the difference, the 
phrase-back reordering model indeed improves 
the translation quality significantly. 
The last experiment merges the two reorder-
ing model together. The results in Table 8 show 
that the overall reordering model has done very 
well in both test sets: it improves the two test 
sets by 2.65% and 2.78% in BLEU score respec-
tively. It demonstrates that every reordering 
model has a positive effect on translation. 
Therefore, our reordering model based on the 
sentence type is quite successful. 
5 Conclusions and Future Work 
In this paper, we have investigated the effect of 
the Chinese sentence types on reordering prob-
lem for Chinese-to-English statistical machine 
translation. We have succeeded in applying a 
phrase-ahead reordering model to process the 
special interrogative sentences and a phrase-
back reordering model to deal with other sen-
tence types. Experiments show that our reorder-
ing model obtains a significant improvement in 
BLEU score on the IWSLT-07 task. 
With the encouraging experimental results, 
we believe that we can mine more reordering 
information from the Chinese sentence types. In 
this paper, we only apply a phrase-back model 
to reorder Chinese other interrogative sentences. 
In the next step, we will try to develop a special 
reordering model for this sentence type. Fur-
thermore, we plan to integrate the phrase-back 
model into phrase-ahead model for special inter-
rogative sentences and investigate the value of 
this integration. 
1095
 Table 8.  Statistics of translation results 
Notes: candidates here mean how many candidate reordering phrases are recognized for each model. Sentences 
mean the number of sentences belonging to the specific sentence type, i.e. for phrase-ahead reordering in Test 1, 
31 special question phrases (SQP) are recognized in 30 Chinese special interrogative sentences. 
 
Acknowledgments 
The research work described in this paper has 
been partially supported by the Natural Science 
Foundation of China under Grant No. 60575043 
and 60736014, the National High-Tech Research 
and Development Program (863 Program) of 
China under Grant No. 2006AA01Z194 and 
2006AA010108, the National Key Technologies 
R&D Program of China under Grant No. 
2006BAH03B02, and Nokia (China) Co. Ltd as 
well. 
 
References 
Cao Wang, Michael Collins and Philipp Koehn. 2007. 
Chinese syntactic reordering for statistical machine 
translation. In Proceedings of joint Conference on 
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, 2007.  
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou 
Minghui Li and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical 
machine translation. In Proceedings of 45th Meet-
ing of the Association for Computational Linguis-
tics . 
Dan Klein and Christopher D. manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of  41st  
Meeting of the Association for Computational Lin-
guistics.  
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Pro-
ceedings of 43rd Meeting of Association for Com-
putational Linguistics.  
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the joint conference of the International Committee 
on Computational Linguistics and the Association 
for Computational Linguistics 2006.  
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of 20th International 
Conference on Computational Linguistics. 
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for 
statistical machine translation. In Proceedings of 
40th Meeting of Association for Computational 
Linguistics.  
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine 
translation. Computational Linguistics, 30:417-449 
Marta R. Costa-jussa and Jose A.R. Fonollosa. 2006. 
Statistical machine reordering. In proceedings of 
Conference on Empirical Methods in Natural Lan-
guage Processing 2006.  
Michael Collins, Philipp Koehn, and Ivona Kucerova. 
2005. Clause restructuring for statistical machine 
translation. In proceedings of 43rd Meeting of the 
Association for Computational Linguistics.  
Philipp Koehn, Franz J. Och. and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In proceed-
ings of HLT-NAACL 2003. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgn System Description 
for the 2005 IWSLT Speech Translation Evalua-
tion. In International Workshp on Spoken Lan-
guage Translation. 
Yuqi Zhang, Richard Zens and Hermann Ney. 2007. 
Chunk-level reordering of source language sen-
tence with automatically learned rules for statistical 
machine translation. In Proceedings of SSST, 
NAACL-HLT 2007/AMTA Workshop on Syntax and 
Structure in Statistical Translation. 
Vladimir Naumovich Vapnik. 1998. Statistical Learn-
ing Theory. John Wiley and Sons, Inc.  
 BLEU (%) NIST Sentences Candidates
Baseline 32.16 6.4844 500  
Phrase-ahead reordering 32.57 6.5579 30 31 
Phrase-back reordering 34.40 6.6857 470 366 
Test 1 
CE_dev5 
Phrase-ahead+phrase-back 34.81 6.7584 500 397 
Baseline 34.04 5.8340 489  
Phrase-ahead reordering 35.91 6.0693 100 97 
Phrase-back reordering 34.97 5.9172 389 89 
Test 2 
CE_test 
Phrase-ahead+phrase-back 36.82 6.1535 489 186 
1096
A Hierarchical Parsing Approach with Punctuation Processing for 
Long Chinese Sentences 
Xing Li  
Institute of Automation,  
Chinese Academy of 
Sciences, Beijing 100080 
xli@nlpr.ia.ac.cn 
Chengqing Zong  
Institute of Automation,  
Chinese Academy of 
Sciences, Beijing 100080 
 cqzong@nlpr.ia.ac.cn
Rile Hu 
Institute of Automation,  
CAS, Beijing 100080 
Nokia (China) Research 
Center, Hepingli Dongjie 
11,Beijing 100013 
rlhu@nlpr.ia.ac.cn 
 
Abstract 
In this paper, the usage and function of 
Chinese punctuations are studied in 
syntactic parsing and a new 
hierarchical approach is proposed for 
parsing long Chinese sentences. It 
differentiates from most of the previous 
approaches mainly in two aspects. 
Firstly, Chinese punctuations are 
classified as ?divide? punctuations and 
?ordinary? ones. Long sentences which 
include ?divide? punctuations are 
broken into suitable units, so the 
parsing will be carried out in two stages. 
This ?divide-and-rule? strategy greatly 
reduces the difficulty of acquiring the 
boundaries of sub-sentences and 
syntactic structures of sub-sentences or 
phrases simultaneously in once-level 
parsing strategy of previous approaches. 
Secondly, a grammar rules system 
including all punctuations and 
probability distribution is built to be 
used in parsing and disambiguating. 
Experiments show that our approach 
can significantly reduce the time 
consumption and numbers of 
ambiguous edges of traditional 
methods, and also improve the 
accuracy and recall when parsing long 
Chinese sentences. 
1 Introduction 
Until recently, although punctuations are clearly 
important parts of the written Chinese, many 
Chinese parsing systems developed to date have 
simply ignored them. Some researches have 
been done on English punctuations in parsing [1, 
2, 3, 4, 5], their researches have used plenty of 
theoretical and experimental facts to prove that 
it is effective to incorporate punctuation 
information into parsing of long complex 
sentences. But as far as we know, little work has 
been done in Chinese syntactic parsing. 
Because the derivation of Chinese punctuations 
was referring to western language [3], they have 
many similarities in usage. Researches on 
Chinese punctuations in parsing will be valuable. 
However, our study shows, there are still 
differences between them, special research on 
Chinese punctuations is necessary. 
In this paper, differences in English and Chinese 
punctuations are compared and the special 
difficulty and corresponding cause in parsing 
Chinese long sentences are analyzed. Then a 
new hierarchical parsing (HP) approach is 
proposed instead of traditional parsing (TP) 
method. This ?divide-and-rule? strategy greatly 
reduces the time consumption. Open test shows, 
parsing accuracy and recall of HP method are 
both about 7% higher than those of TP. 
The remainder of this paper is organized as 
follows: Section 2 is related work. Section 3 
mainly discusses the special difficulties and 
solution in parsing long Chinese sentences. Then 
HP method is discussed in detail in Section 4. 
Section 5 gives the final experiment results and 
corresponding analyses. Finally, the further 
work is expected. 
2 Related Work  
Nunberg?s The Linguistics of Punctuation [2] is 
the foundation for most of the latter researches 
7
in syntactic account of punctuation. In his 
important study, he advocates two separate 
grammars, operating at different levels. A 
lexical grammar accounts for the text-categories 
(text-clauses, text-phrases) occurring between 
the punctuation marks, and a text grammar deals 
with the structure of punctuation, and the 
relation of those marks to the lexical expressions 
they separate.  
Based on above theory, Jones proposes his 
method which uses an integrated grammar. He 
divided punctuations into conjoining and 
adjoining punctuation. Conjoining punctuations 
can be used to indicate coordinate relationship 
between components. Adjoining punctuations, 
otherwise, only can be attached to their adjacent 
sentence components. In Jones? theory, in a 
sense, conjoining punctuation could also be 
treated under the adjunctive principle [3]. So, 
punctuations in his theory are still attached to 
adjacent lexical expressions. An integrated 
syntactic punctuation grammar is given. 
Jones? method shows good modularity. 
However, the grammars he designed can only 
cover a subset of all punctuation phenomena. 
His experiment shows that when parsing a set of 
ten previously unseen punctuationally complex 
sentences, seven of the ten are unparsable! 
In Chinese, Zhou Qiang[6] has used 
punctuations to do automatic acquisition of 
coordinate phrases. In machine translation, 
Chengqing Zong[7] and Huang He-yan[8] have 
used punctuations associating with relative 
pronouns to segment complex sentences into 
several independent simple sentences. Above all, 
none of previous work has carried out a 
thorough study on punctuations from the 
syntactic point of view. 
3 Motivations 
3.1 Differences between Chinese and 
English Punctuations 
In Chinese, there are some punctuations which 
don?t exist in English. The first one is a pair of 
Chinese book-name mark ??? and ???, which 
are obvious marks that the content between them 
must be name of a book. The second one is 
pause mark ???, which replaces comma as the 
separating mark between coordinate components. 
For instance, sentence ?I like to walk, skip, and 
run.? can be translated into Chinese one as ??
??????????. Chinese pause mark is 
the evident mark with the exclusive usage is to 
separate coordinate words or simple phrases, so 
it is easier to get coordinate words or simple 
phrases in Chinese sentences. 
3.2 Special Difficulty in Parsing Long 
Chinese Sentences  
In essence, English is a kind of hypotaxis 
language, so an intact syntax structure denotes a 
sentence. When several simple sentences are 
connected to form a compound sentence, there 
should be obvious conjunctions between them. 
Differently, Chinese is a kind of parataxis 
language, and the language unit which expresses 
a complete thought is an intact Chinese sentence. 
Therefore, several sentences with associative 
meanings can be connected by come 
punctuations to form a compound one without 
any conjunctions. This type of sentence is called 
?run-on sentence?, and which is prevalent in 
Chinese. For example, we randomly selected 
4431 sentences whose lengths are over 30 
characters from a Chinese corpus named TCT 
973.1 There are 1830 run-on sentences, covering 
41.3%. Chinese sentence ?????????
?????????????? is this kind of 
sentence. The corresponding English meaning is 
?Now, I am not young and I still have to take 
bus to work everyday,  which make me very 
tired?. So, in above Chinese sentences, commas 
are used not only as separating marks of sub-
sentences but also as separating marks of 
components in one sub-sentence. However, lack 
of connections makes methods [7, 8] of 
segmenting complex sentences invalid. In this 
situation, acquisition of the boundaries of sub-
sentences and syntactic structure of sub-
sentences or phrases should be done 
simultaneously in once-level parsing strategy, 
which will undoubtedly increase the difficulty of 
parsing long sentences. 
3.3 Corresponding Solution   
In order to solve this problem, a hierarchical 
parsing (HP) approach is proposed by us. 
Nunberg?s theory of two categories grammars 
provides us the theoretical base of HP approach. 
                                                        
1 Please refer to http://www.chineseldc.org/ 
8
According to his definition of two categories of 
grammars described in section 2, the two 
grammars can operate at different levels 
independently. Punctuations which can occur as 
elements of text grammar are defined by us as 
?divide? punctuations. Then punctuations which 
can occur as elements of lexical grammar are 
?ordinary? ones. The ?divide? punctuations can 
be used to divide the whole sentence into several 
parts. Then the parsing will be carried out in two 
steps. Thus, acquisition of syntactic structure of 
sub-sentences or phrases is done in the first level 
parsing, and acquisition of the boundaries of 
sub-sentences and relationship of sub-sentences 
or phrases can be done in second level parsing. 
This is the main idea of HP approach, which can 
reduce the difficulty of parsing run-on sentences 
and other types of compound sentences. The 
framework of HP approach is shown as 
following Figure 1:  
 Original
sentence
Sub-sen.1
Sub-sen.3
Sub-sen.2
Sub-sen.n
??
Sentence
Division
Sub-tree 1
Sub-tree 3
Sub-tree 2
Sub-tree n?
Sub-tree 1'
Sub-tree 2'
Sub-tree n
Parsing tree
of whole sen
First level
parsing
Second level
parsing
Detection of
Improper Division
and Combination
? ?
? ?
?
 
Figure 1. Framework of HP Approach 
4  Hierarchical Parsing Approach 
4.1 Classification of Chinese Punctuations 
In this paper, the ?divide? punctuations are 
defined as follows: If lexical sentences or 
phrases which are separated by certain 
punctuations must be correlative to each other 
wholly not partly, these punctuations are in level 
of text grammar, which are classified as ?divide? 
punctuations. Punctuations in a and b of Figure 
2 are examples of two categories of punctuations 
( P stands for punctuations). 
In Chinese, the semicolon is used to separate 
coordinate sub-sentences. The colon is used as 
separation mark of interpretative phrases or sub-
sentences from former sub-sentences. So, 
according to above definition, they can be 
classified as ?divide? punctuations. The comma, 
specially, can occur as a mark of coordinate 
phrases element. So, using of it as ?divide? 
punctuation may cause improper division 
problems and a compensatory solution is 
introduced in, which will be discussed in detail 
in Section 4.3.3.  
 
 
Figure  2. ?Divide? punctuations (first) and 
?ordinary? punctuations (second) 
4.2 Grammar Rules 
The automatic extraction of grammar rules 
which include punctuations depends on large 
scales of parsed Chinese corpus which has 
ample syntactic phenomena and standard usage 
of punctuations. Fortunately, Chinese tree-bank 
named TCT 973 is such a corpus. It includes 
1,000, 000 words and covers all kinds of text 
after 1990th. The average length of each sentence 
is 23.3 words. Long sentences of over 20 words 
length account for half of it. 
Firstly, original grammar rules are extracted. 
Then generalizations are done about the use of 
the various punctuation marks from the rules set. 
For example, as mentioned before, Chinese 
book-name mark ?? ? and ?? ?are obvious 
marks that the content between ? ? ? and 
???must be name of a book by any syntactic 
category. Therefore, a generalized rule can be 
deduced as below: 
        :{ , , ,  ......}?NP X X NP VP S PP? ?  (1) 
In above generalized rule, X can be any POS of 
phrases or single word, so possible rules that 
have not been deduced from tree-bank are added 
into the grammar rules set with probabilities 1. 
Except for above special situations, 
corresponding probabilities of all grammar rules 
are computed by Maximum Likelihood Estimate 
9
(MLE) method.At last, all rules are combined to 
form an intact grammar system. 
4.3  Parsing Strategy 
4.3.1  Sentence Division 
Depending on above classification, commas, 
semicolons and colons are used to divide 
sentences into a series of sub-sentences. Notice 
that quotation marks and parenthesis are treated 
as transparent and syntactically non-functional. 
4.3.2  First Level Parsing  
All sub-sentences and phrases gotten from the 
division processing are inputs of the first level 
parsing. A chart parsing algorithm is used here. 
The grammar rules and corresponding 
probabilities are used to do parsing and 
disambiguating. Then for all sub-sentences and 
phrases, their parsing trees are the highest 
probabilities ones of all possible trees.  
4.3.3 Detection of Improper Division and 
Combination  
Because of the specialty of comma, using of it 
as the division mark may cause improper 
divisions. The main causation is improper 
division between coordinate phrases which have 
been same component of the sentence. For 
example, Chinese sentence ?????????
???????????????????
????????????????? is a 
typical coordinate structure similar to ?I like to 
do ..., to do ..., to do..., but I like better to...? in 
English. So, the first three ????are coordinate 
predicates of the sentences.  Then the improper 
division will break up this relationship. In this 
section, a detection and combination method is 
proposed by us to solve this problem in parsing 
Chinese sentences.  
Because the lexical expressions surrounding 
punctuations are parsed in first level parsing, it 
is easy to get their internal syntactic structures 
information we need. Just a simple analysis 
procedure is needed to judge if there exists such 
a coordinate relationship between lexical 
expressions surrounding commas. 
A description of the analysis strategy is given 
according to this example. 
Just like Figure 3 shows, the components after 
the first comma are parsed as verb phrase (VP) 
marked as B. Obviously B is composed of a 
preposition phrase (PP) and a verb phrase. If 
there exists a minimal length of phrase 
immediately before the first comma and this 
phrase has totally the same structure to phrase B, 
then they are coordinate phrases. In Figure 3, A2 
is such a phrase. The components after other 
commas are analyzed similarly. Finally, A2, B 
and C are coordinate phrases. Since the verb 
phrase D immediately after the second comma 
has obviously different structure from A2, B and 
C, so they aren?t coordinate components. The 
part-of-speech tags throughout this paper follow 
the standard of TCT973.  
 
Figure 3. Syntactic structure of example 
sentence  
Through the above analysis, we can see that the 
first and second commas are actually in level of 
lexical grammar, using them as ?divide? 
punctuations will cause the improper division as 
shown in Figure 2 of b. Therefore, we present a 
method to use sub-tree adjoining operation, 
firstly combine the sub-tree A2 with tree B and C, 
then use the new tree A2? to replace original A2  
without changing original structure of A . Figure 
4 shows the adjoining procedure.  
 
Figure 4. Sub-tree adjoining operation 
Then the execution conditions and results of 
such adjoining operation are summarized as 
following rules:  
 
[ ] [ ... ] [ [[ ] ] ... ]+ +?S X Y Y S X X X Y YX? ? ? ?
 
X = {NP, VP, AP, DP} ,  S stands for sentence,  
Y = * ( any legal POS)  
(2) 
[ ... ][ ] [ ... [ [ ] ]]+ +?S Y Y X S Y Y X X XX? ? ? ?
 
X = {NP, VP, AP, DP} ,  S stands for sentence,  
Y = * ( any legal POS)  
(3) 
10
 The execution conditions of both Rule (2) and (3) 
are defined as follows:  all X should be 
coordinate phrases with the same syntactic 
categories.  
4.3.4  Second Level Parsing 
The parsing algorithm of this module is totally 
the same to the first level parsing; with the 
difference is the input string. At the first parsing 
stage, inputs are POS sequence of words, but at 
the second parsing stage, inputs are POS 
sequence of all sub-tree root nodes. After this 
stage of parsing, the best parsing trees of whole 
sentences will be constructed.  
5 Performance Evaluation 
5.1 Test Sentences 
The primary aim of the HP strategy is to take 
use of the punctuation information to help to 
conquer the difficulty of parsing long sentences. 
Chinese sentences with over 20 words are 
generally regarded as long sentences. Therefore, 
we conduct experiments on the sentences with 
the length over 20 words. 
Firstly, 8,059 sentences were chosen randomly 
from TCT 973 as train set. The 3,795 PCFG 
rules used in our system are extracted from the 
train set after generalizing. Then, for other 847 
sentences, whose lengths are less than 20 words 
are filtered and 420 sentences are finally 
conserved as our open test data set. Distribution 
of these sentences is shown in Table 1 below:  
Text Type Num
ber of 
Sen 
Length of 
Sen 
(Words) 
Average 
Length of 
Sen (Words) 
Literature 116 21?123 36.06 
News 123 22?100 37.73 
Science 114 21?131 39.47 
Practical writing 67 20?98 38.36 
Total 420 20?131 37.84 
Table 1. Distribution of test sentences 
 
5.2  Efficiency Evaluation  
In order to compare our HP approach with TP 
method of once-parsing algorithm, we do 
compared experiments using same data set in 
Table 1 and same grammar rules set.  
5.2.1  Time Consumption Evaluation  
Running two systems on a PC (Pentium 4, 
1.20GHz, 256M of RAM), their time 
consumptions are shown in Figure 5. 
Time Consumed Comparison
0
20
40
60
80
100
120
140
160
20 40 60 80 100 120 140
Length of Sentence(Words)
T
i
m
e
 
C
o
n
s
u
m
e
d
(
s
e
c
)
HP method
TP method
 
Figure 5. Time consumption 
In our experiment system, we set the upper limit 
execution time as 120 seconds per sentence, 
judging at the end of every algorithm cycle. 
When parsing time of the sentence is overtime, 
the system will exit without getting final result. 
Experiment results shown in Fig.5 prove that 
time efficiency of HP method is greatly superior 
to TP, especially when the sentence has more 
than 40 words. With the increasing of sentence 
length, it is more difficult for TP method to 
parse successfully. 
5.2.2  Accuracy and Recall Evaluation 
Firstly, Table 2 shows numbers of sentences 
failed to be parsed in two methods with the time 
limitation of 120 seconds. 
 
 
Methods 
Numbers of 
Test  Sen 
Numbers of 
Failed Sen Ratio 
TP 420 97 23.1% 
HP 420 16 3.8% 
Table 2. Ratio of failed sentences 
It is evident that HP method can largely reduce 
failed sentences in given time limitation. 
Then, except for failed sentences, only 
considering the successfully parsed sentences, 
the parsing accuracy and recall of the two 
methods should be compared. The standard 
PARSEVAL measures [9] are used to evaluate 
two methods. Results are shown in Table 3.  
From Table 3, we can see that the total parsing 
accuracy and recall of HP method are both 
almost 7% higher than those of TP method. 
Amounts of average crossing brackets are also 
reduced greatly. 
11
Analyzing data in Table 3, to different text types, 
the accuracy and improvement effect of TP 
method are slightly different. Sentences of 
literature text have the highest parsing accuracy 
and recall. Studied show that there are 97 ?run-
on sentences? in the 116 literature text sentences, 
covering 84%. The comparatively higher 
accuracy and recall of these sentences prove that 
our HP approach is effective. 
Text 
type 
Meth
od 
LP% LR% CBs 0CB
% 
?2CB
s% 
TP 67.31 66.76 6.97 19.77 48.84 Literatu
re HP 73.57 73.77 3.24 40.74 62.09 
TP 61.05 61.69 5.80 10.47 34.88 News HP 70.66 70.58 3.52 28.33 61.83 
TP 61.20 60.89 5.63 12.66 37.97 Science HP 68.74 68.98 4.14 23.37 59.10 
TP 64.10 64.61 6.17 6.25 27.08 Practical 
writing HP 66.55 67.81 4.68 21.54 50.77 
TP 63.38 63.41 6.14 13.04 38.46 Total HP 70.06 70.03 3.80 30.24 61.01 
Table 3. Results using standard PARSEVAL 
measures 
Sentences of application have lowest parsing 
accuracy and smallest improvement. Because 
comparing to other three types, sentences of this 
type have more long nested noun phrases or 
coordinate components, such as long 
organization names and commodity names, 
which will cause noun phrase combination 
disambiguation. 
6  Conclusion and Future Work 
This paper studies the usage and function of 
Chinese punctuations in syntactic parsing. A 
new hierarchical parsing approach is proposed. 
Besides, a grammar rules system including all 
punctuations and probability distribution is built 
to be used in parsing and disambiguation. 
Compared experiments prove that HP method is 
effective in long Chinese sentences parsing. 
In future work, theories of punctuations should 
be studied further to get a more formal point of 
view. 
Acknowledges 
The research work is supported by the national 
natural science foundation of China under grant 
No.60375018 and 60121302, and also supported 
by the outstanding oversea scholar foundation of 
CAS. 
References 
1. Benard Jones, Towards a Syntactic Account of 
Punctuation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark, 
August . (1996b) 
2. Geoffrey Nunberg. The Linguistics of Punctuation. 
CSLI Lecture Notes, No. 18, Stanford CA, (1990) 
3. Benard Jones,What?s the Point? A (Computational ) 
Theory of Punctuations. PhD thesis, Centre for 
Cognitive Science, Universito of Edinburgh, 
Edinburgh, UK, (1997) 
4. Edward Briscoe. The Syntax and Semantics of 
Punctuation and its Use in Interpretation. In 
Proceedings of the ACL/SIGPARSE International 
Meeting on Punctuation in Computational 
Linguistics, Santa Cruz, California. (1996) 1?7. 
5. Charles Meyer. A Linguistic Study of American 
Punctuation. Peter Lang: New York. 1987. 
6. Zhou Qiang. The Chunk Parsing Algorithm for 
Chinese Language. In Proceedings of JSCL'99, 
(1999) 242-247 
7. Chengqing Zong, Yujie Zhang, Kazuhide 
Yamamoto, Masashi Sakamoto,etc. Chinese 
Utterance Paraphrasing for Spoken Language 
Translation, In Journal of Chinese Language 
Computing, Singapore, 2002,12 (1): 63-77.  
8. Huang He-yan, Chen Zhao-xiong, The Hybrid 
Strategy Processing Approach of Complex Long 
Sentence, In Journal of Chinese Information 
processing, 2002,16(3):1-7.  
9. E.Charniak, ?Statistical parsing with a context-free 
grammar and word statistics?. In Proc of AAAI?97, 
1997. 
 
12
A New Approach to Automatic Document Summarization 
Xiaofeng Wu 
National Laboratory of Pattern Recognition,  
Institute of Automation, 
Chinese Academy of Sciences 
Beijing, China 
xfwu@nlpr.ia.ac.cn 
Chengqing Zong 
National Laboratory of Pattern Recognition, 
Institute of Automation, 
Chinese Academy of Sciences 
Beijing, China 
cqzong@nlpr.ia.ac.cn 
 
 
Abstract 
In this paper we propose a new approach 
based on Sequence Segmentation Models 
(SSM) to the extractive document summa-
rization, in which summarizing is regarded 
as a segment labeling problem. Comparing 
with the previous work, the difference of 
our approach is that the employed features 
are obtained not only from the sentence 
level, but also from the segment level. In 
our approach, the semi-Markov CRF model 
is employed for segment labeling. The pre-
liminary experiments have shown that the 
approach does outperform all other tradi-
tional supervised and unsupervised ap-
proaches to document summarization. 
1 Introduction 
Document summarization has been a rapidly 
evolving subfield of Information Retrieval (IR) 
since (Luhn, 1958). A summary can be loosely 
defined as a text that is produced from one or more 
texts and conveys important information of the 
original text(s). Usually it is no longer than half of 
the original text(s) or, significantly less (Radev et 
al., 2002). Recently, many evaluation competitions 
(like the Document Understanding Conference 
DUC ?http://duc.nist.gov?, in the style of NIST?s 
TREC), provided some sets of training corpus. It is 
obvious that, in the age of information explosion, 
document summarization will be greatly helpful to 
the internet users; besides, the techniques it uses 
can also find their applications in speech tech-
niques and multimedia document retrieval, etc. 
The approach to summarizing can be catego-
rized in many ways. Some of them are: 1) indica-
tive, informative and evaluative, according to func-
tionality; 2) single-document and multi-document, 
according to the amount of input documents; 3) 
generic and query-oriented, according to applica-
tions. Yet the taxonomy currently widely em-
ployed is to categorize summarization into abstrac-
tive and extractive. 
According to (Radev et al, 2002), all methods 
that are not explicitly extractive are categorized as 
abstractive. These approaches include ontological 
information, information fusion, and compression. 
Abstract-based summarization never goes beyond 
conceptual stage, though ever since the dawn of 
summarization it has been argued as an alternative 
for its extract-based counterpart. On the other hand, 
extractive summarization is still attracting a lot of 
researchers (Yeh et al, 2005) (Daum?e III and 
Marcu, 2006) and many practical systems, say, 
MEAD ?http://www.summarization.com/mead/?, 
have been produced. Using supervised or unsuper-
vised machine learning algorithms to extract sen-
tences is currently the mainstream of the extractive 
summarization. However, all pervious methods 
focus on obtaining features from the sentence gra-
nularity. 
In this paper we focus on generating summariza-
tion by using a supervised extractive approach in 
which the features are obtained from a larger gra-
nularity, namely segment. The remainder of the 
paper is organized as follows: Section 2 introduces 
the related work concerning the extract-based 
summarization. Section 3 describes our motiva-
tions. Our experiments and results are given in 
Section 4, and Section 5 draws the conclusion and 
mentions the future work. 
126
2 Related Work 
Early researchers approached the summarization 
problem by scoring each sentence with a combina-
tion of the features like word frequency and distri-
bution, some proper names (Luhn, 1958), sentence 
positions in a paragraph (Baxendale, 1958), and 
sentence similarity (Gong, 2001) etc. The results 
were comparatively good. Most supervised extrac-
tive methods nowadays focus on finding powerful 
machine learning algorithms that can properly 
combine these features. 
Bayesian classifier was first applied to summari-
zation by (Pedersen and Chen, 1995), the authors 
claimed that the corpus-trained feature weights 
were in agreement with (Edmundson, 1969), which 
employed a subjective combination of weighted 
features. Another usage of the na?ve Bayesian 
model in summarization can be found in (Aone et 
al., 1997). Bayesian model treats each sentence 
individually, and misses the intrinsic connection 
between the sentences. (Yeh et al, 2005) employed 
genetic algorithm to calculate the belief or score of 
each sentence belonging to the summary, but it 
also bears this shortcoming. 
To overcome this independence defect, (Conroy 
and O?leary, 2001) pioneered in deeming this prob-
lem as a sequence labeling problem. The authors 
used HMM, which has fewer independent assump-
tions. However, HMM can not handle the rich lin-
guistic features among the sentences either. Re-
cently, as CRF (Lafferty and McCallum, 2001) has 
been proved to be successful in part-of-speech tag-
ging and other sequence labeling problems, (Shen 
et al, 2007) attempted to employ this model in 
document summarization. CRF can leverage all 
those features despite their dependencies, and ab-
sorb other summary system?s outcome. By intro-
ducing proper features and making a comparison 
with SVM, HMM, etc., (Shen et al, 2007) claimed 
that CRF could achieve the best performance. 
All these approaches above share the same 
viewpoint that features should be obtained at sen-
tence level. Nevertheless, it can be easily seen that 
the non-summary or summary sentences tend to 
appear in a consecutive manner, namely, in seg-
ments. These rich features of segments can surely 
not be managed by those traditional methods.  
Recently, Sequence Segmentation Model (SSM) 
has attracted more and more attention in some 
traditional sequence learning tasks. SSM builds a 
direct path to encapsulate the rich segmental 
features (e.g., entity length and the similarity with 
other entities, etc., in entity recognition). Semi-
CRF (Sarawagi and Cohen, 2004) is one of the 
SSMs, and generally outperforms CRF. 
3 Motivations 
According to the analysis in Section 2, our basic 
idea is clear that we regard the supervised summa-
rizing as a problem of sequence segmentation. 
However, in our approach, the features are not only 
obtained on the sentence level but also on the seg-
ment level.  
Here a segment means one or more sentences 
sharing the same label (namely, non-summary or 
summary), and a text is regarded as a sequence of 
segments. Semi-CRF is a qualified model to ac-
complish the task of segment labeling, besides it 
shares all the virtues of CRF. Using semi-CRF, we 
can easily leverage the features both in traditional 
sentence level and in the segment level. Some fea-
tures, like Log Likelihood or Similarity, if obtained 
from each sentence, are inclined to give unex-
pected results due to the small granularity. Fur-
thermore, semi-CRF is a generalized version of 
CRF. The features designed for CRF can be used 
in semi-CRF directly, and it has been proved that 
semi-CRF outperforms CRF in some Natural Lan-
guage Processing (NLP) problems (Sarawagi and 
Cohen, 2004).  
In the subsections below, we first introduce 
semi-CRF then describe the features we used in 
our approach. 
3.1 Semi-CRF 
CRF was first introduced in (Lafferty and 
McCallum, 2001). It is a conditional model P(Y|X), 
and here both X and Y may have complex structure. 
The most prominent merits of CRF are that it 
offers relaxation of the strong independence 
assumptions made in HMM or Maximum Entropy 
Markov Models (MEMM) (McCallum, 2000) and 
it is no victim of the label bias problem. Semi-CRF 
is a generalization version of sequential CRF. It 
extends CRF by allowing each state to persist for a 
non-unit length of time. After this time has elapsed, 
the system might transmit to a new state, which 
only depends on its previous one. When the system 
is in the ?segment of time?, it is allowed to behave 
non-Markovianly. 
127
3.1.1 CRF vs. Semi-CRF 
Given an observed sentence sequence 
X=(x1,x2,?,xM). The corresponding output labels 
are Y=(y1,y2,?,yM), where yi gets its value from a 
fixed set ?. For document summarization, 
?={0,1}. Here 1 for summary and 0 for non-
summary. The goal of CRF is to find a sequence of 
Y, that maximize the probability: 
      
1
( | , ) exp( ( , ))
( )
P Y X W W F X Y
Z X
= ?          (1) 
Here?  is a vertical vector of 
size T. The vertical vector 
1
( , ) f ( , , )
M
i
F X Y i X Y==?
1 2
'f ( , , ..., )Tf f f= means 
there are T feature functions, and each of them can 
be written as ft(i,X,Y)?R,t?(1,?,T),i?(1,?,M). 
For example, in our experiment the 10th feature 
function is expressed as: [if the length of current 
sentence is bigger than the predefined threshold 
value]&[if the current sentence is a summary]. 
When this feature function is acting upon the third 
sentence in text_1 with label_sequence_1, the fol-
lowing feature equation f10(3,text_1, la-
bel_sequence_1) means: in text_1 with la-
bel_sequence_1, [if the length of the third sentence 
is bigger than the predefined threshold value]&[if 
the third sentence is a summary]. W is a horizontal 
vector of size T that represents the weights of these 
features respectively. Equation (2) gives the defini-
tion of Z(X), which is a normalization constant that 
makes the probabilities of all state sequences sum 
to 1. 
'( ) exp( ( , '))YZ X W F X= ?? Y
|
     (2) 
If we change the sequence vector X to 
S=<s1,s2,?,sN>, which means one way to split X 
into N segments, we have the semi-CRF. Each 
element in S is a triple: Sj=<tj,uj,yj>, which de-
notes the jth segment in this way of segmentation. 
In the triple, tj denotes the start point of the jth seg-
ment, uj denotes its end position, and yj is the out-
put label of the segment (recall the example at the 
beginning of this subsection that there is only one 
output for a segment). Under this definition, seg-
ments should have no overlapping, and satisfy the 
following conditions: 
1
| | |
N
j
js X=
=?                                    (3) 
    (4) 1 11, | |,1 | |, 1N j j jt u X t u X t u+= = ? ? ? = +
Here, |?| denotes the length of?. 
 
Figure 1  A 10-sentences text with label sequence 
 
For example, one way to segment a text of 10 sen-
tences in Figure 1 is S=<(1,1,1),(2,4,0),(5,5,1), 
(6,9,0),(10,10,1)> . The circles in the second row 
represent sentences, and actually are only some 
properties of the corresponding sentences. 
Consequently, the feature function f in CRF 
converts to the segmental feature function 
g=(g1,g2,?,g T?). Like f, gt(i,x,s) ?R also maps a 
triple (i,x,s) to a real number. Similarly, we may 
define . Now we give the 
final equation used to estimate the probability of S.           
Given a sequence X and feature weight W, we have 
1
( , ) g( , , )
N
i
G X S i X S== ?
1
( | , ) exp( ( , ))
( )
P S X W W G X S
Z X
= ?          (5) 
Here,  
'
( ) exp( ( , '))
S
Z X W G X
??
= ? S?                 (6) 
Where, { }all segmentations allowed? = ? ? . 
3.1.2 Inference 
The inference or the testing problem of semi-CRF 
is to find the best S that maximizes Equation (5). 
We use the following Viterbi-like algorithm to cal-
culate the optimum path. 
Suppose the longest segment in corpus is K, let 
S1:i,y represent all possible segmentations starting 
from 1 to i , and the output of the last segment is y. 
V(i,y) denotes the biggest value of P(S?|X,W). Note 
that it?s also the largest value of W?G(X,S?), 
S??S1:i,y. 
Compared with the traditional Viterbi algorithm 
used in CRF, the inference for semi-CRF is more 
time-consuming. But by studying Algorithm 1, we 
can easily find out that the cost is only linear in K. 
j
128
      
3.1.3 Parameter Estimation 
Define the following function 
log ( | , )
( ( , ) log (    
lW l l
l l l l
L P S X W
W G X S Z X
= ?
= ? ?? ))   (8) 
In this approach, the problem of parameter estima-
tion is to find the best weight W that maximizes LW. 
According to (Bishop, 2006), the Equation (8) is 
convex. So it can be optimized by gradient ascent. 
Various methods can be used to do this work (Pie-
tra et al 1997). In our system, we use L-BFGS, a 
quasi-Newton algorithm (Liu and Nocedal. 1989), 
because it has the fast converging speed and effi-
cient memory usage. APIs we used for estimation 
and inference can be found in website 
?http:\\crf.sourcefourge.net?. 
3.2 Features 
(Shen et al 2007) has made a thorough investiga-
tion of the performances of CRF, HMM, and SVM. 
So, in order to simplify our work and make it com-
parable to the previous work, we shape our desig-
nation of features mainly under their framework.  
The mid column in Table 1 lists all of the fea-
tures we used in our semi-CRF approach. For the 
convenience of comparison, we also list the name 
of the features used in (Shen et al 2007) in the 
right column, and name them Regular Features. 
The features in bold-face in the mid column are the 
corresponding features tuned to fit for the usage of 
semi-CRF. We name them Extended Features. 
There are some features that are not in bold-face in 
the mid column. These features are the same as the 
Regular Features in the right column. We also 
used them in our approach. The mark star denotes 
that there is no counterpart. We number these fea-
tures in the left column.  Algorithm 1: 
Step1. Initialization: 
 Let V i  ( , ) 0,  0
 
No. semi-CRF CRF 
1 Ex_Position        Position 
2 Ex_Length         Length 
3 Ex_Log_Likelihood  Log Likeli-hood 
 
4 
Ex_Similarity_to_ 
Neighboring_   
Segments           
Similarity to 
Neighboring 
Sentences 
5 Ex_Segment_ 
Length     * 
6 Thematic           Thematic  
7 Indicator           Indicator  
8 Upper Case         Upper Case  
y for i= =
Step2. Induction: 
 0for i >   
', 1,...,( , ) max ( , ')
           g( , ', , 1, )
y k KV i y V i k y
W y y x i d i
== ?
+ ? ? +         (7)  
Step3. Termination and path readout: 
       max (| |, )ybestSegment V X y=
               Table 1. Features List 
The details of the features we used in semi-
CRF are explained as follow. 
Extended Features: 
Ex_Position: is an extended version of the Po-
sition feature. It gives the description of the po-
sition of a segment in the current segmentation. 
If the sentences in the current segment contain 
the beginning sentence of a paragraph, the value 
of this feature will be 1, 2 if it contains the end 
of a paragraph; and 3 otherwise; 
Ex_Length: the number of words in the cur-
rent segment after removing some stop-words. 
Ex_Log_Likelihood: the log likelihood of the 
current segment being generated by the docu-
ment. We use Equation (9) below to calculate 
this feature. N(wj,si) denotes the number of oc-
currences of the word wj in the segment si, and 
we use ( , ) / ( , )
k
j w k
N w D N w D?  to estimate the 
probability of a word being generated by a doc-
ument. 
log ( | ) ( , ) log ( | )
j
i j iw j
P s D N w s p w D=?      (9) 
Ex_Similarity_to_Neighboring_Segments: 
we define the cosine similarity based on the 
TF*IDF (Frakes &Baeza-Yates, 1992) between 
a segment and its neighbors. But unlike (Shen et 
al. 2007), in our work only the adjacent neighbors 
of the segment in our work are considered. 
EX_Segment_Length: this feature describes 
the number of sentences contained in a segment. 
129
All these features above are actually an ex-
tended version used in the regular CRF (or in 
other supervised model). It is easy to see that, if 
the segment length is equal to 1, then the fea-
tures will degrade to their normal forms.  
There are some features that are also used in 
semi-CRF but we don?t extend them like those 
features above. Because the extended version of 
these features leads to no improvement of our 
result. These features are: 
Regular features we used: 
Thematic: with removing of stop words, we 
define the words with the highest frequency in 
the document to be the thematic words. And this 
feature gives the count of these words in each 
sentence. 
Indicator: indicative words such as ?conclu-
sion? and ?briefly speaking? are very likely to be 
included in summary sentences, so we define 
this feature to signal if there are such words in a 
sentence. 
Upper Case: some words with upper case are 
of high probability to be a name, and sentences 
with such words together with other words 
which the author might want to emphasize are 
likely to be appeared in a summary sentence. So 
we use this feature to indicate whether there are 
such words in a sentence. 
It should be noted that theoretically the num-
ber of extended features obtained from the cor-
pus goes linearly with K in Equation (7). 
 
4 Experiments 
4.1 Corpus & Evaluation Criteria 
To evaluate our approach, we applied the widely 
used test corpus of (DUC2001), which is spon-
sored by ARDA and run by NIST 
?http://www.nist.gov?. The basic aim of DUC 
2001 is to further progress of summarization and 
enable researchers to participate into large-scale 
experiments. The corpus DUC2001 we used con-
tains 147 news texts, each of which has been la-
beled manually whether a sentence belongs to a 
summary or not. Because in (Shen et al 2007) all 
the experiments were conducted upon DUC2001, 
we may make a comparison between the sequence 
labeling models and the sequence segmentation 
modes we used. The only preprocessing we did is 
to remove some stop words according to a stop 
word list.  
We use F1 score as the evaluation criteria which is 
defined as: 
2*Precesion*Recall
1
Precesion+Recall
F =                 (10) 
We used 10-fold cross validation in order to reduce 
the uncertainty of the model we trained. The final 
F1 score reported is the average of all these 10 ex-
periments. 
All those steps above are strictly identical to the 
work in (Shen et al 2007), and its result is taken as 
our baseline. 
4.2 Results & Analysis 
As we mentioned in Sub-Section 3.2, those ex-
tended version of features only work when seg-
ment length is bigger than one. So, each of these 
extended version of features or their combination 
can be used together with all the other regular fea-
tures listed in the right column in Table 1. In order 
to give a complete test of the capacity of all these 
extended features and their combinations, we do 
the experiments according to the power set of {1, 2, 
3, 4, 5} (the numbers are the IDs of these extended 
features as listed in Table 1), that is we need to do 
the test 25-1 times with different combinations of 
the extended features. The results are given in Ta-
ble 2. The rows with italic fonts (1, 3, 5, 7, 9, 11, 
13), in Table 2 denote the extended features used. 
For example, ?1+2? means that the features 
Ex_Positon and the Ex_Length are together used 
with all other regular features are used.  
Table 2. Experiment results. 
 1 2 3 4 5 
F1 0.395 0.391 0.398 0.394 0.392 
 1+2 1+3 1+4 1+5 2+3 
F1 0.395 0.396 0.396 0.395 0.382 
 2+4 2+5 3+4 3+5 4+5 
F1 0.389 0.384 0.398 0.399 0.380 
 1+2+3 1+2+4 1+2+5 1+3+4 1+3+5
F1 0.398 0.397 0.393 0.403 0.402 
 1+4+5 2+3+4 2+3+5 2+4+5 3+4+5
F1 0.402 0.403 0.401 0.403 0.404 
 1+2 +3+4 
1+2 
+3+5 
1+2 
+4+5 
1+3 
+4+5 
2+3 
+4+5 
F1 0.407 0.404 0.406 0.402 0.404 
 All CRF 
F1 0.406 0.389 
130
Other rows (2, 4, 6, 8, 10, 12, 14) give F1 scores 
corresponding to the features used. 
     In Table 3 we compare our approach with some 
of the most popular unsupervised methods, includ-
ing LSA (Frakes & Baeza-Yates, 1992) and HITS 
(Mihalcea 2005). The experiments were conducted 
by (Shen et al 2007). 
Table 3 Comparison with unsupervised methods 
 
From the results in Table 2 we can see that indi-
vidually applying these extended features can im-
prove the performance somewhat. The best one of 
these extended features is feature 3, as listed in the 
2nd row, the 5th column. The highest improvement, 
1.8%, is obtained by combining the features 1, 2, 3 
and 4. Although a few of the combinations hurt the 
performance, most of them are helpful. This veri-
fies our hypothesis that the extended features under 
SSM have greater power than the regular features. 
The results in Table 3 demonstrate that our ap-
proach significantly outperforms the traditional 
unsupervised methods. 8.3% and 4.9% improve-
ments are respectively gained comparing to LSA 
and HITS models 
Currently, the main problem of our method is 
that the searching space goes large by using the 
extended features and semi-CRF, so the training 
procedure is time-consuming. However, it is not so 
unbearable, as it has been proved in (Sarawagi and 
Cohen, 2004). 
5 Conclusion and Future Work 
In this paper, we exploit the capacity of semi-CRF , 
we also make a test of most of the common fea-
tures and their extended version designed for doc-
ument summarization. We have compared our ap-
proach with that of the regular CRF and some of 
the traditional unsupervised methods. The com-
parison proves that, because summary sentences 
and non-summary sentences are very likely to 
show in a consecutive manner, it is more nature to 
obtain features from a lager granularity than sen-
tence.  
In our future work, we will test this approach on 
some other well known corpus, try the complex 
features used in (Shen et al 2007), and reduce the 
time for training. 
 
Acknowledgements 
The research work described in this paper has been 
funded by the Natural Science Foundation of Chi-
na under Grant No. 60375018 and 60121302. 
     
References 
C.Aone, N. Charocopos, J. Gorlinsky. 1997. An 
Intelligent Multilingual Information Browsing and 
Retrieval System Using Information Extraction. In 
ANLP, 332-339. 
P.B. Baxendale. 1958. Man-made Index for Tech-
nical Literature -An Experiment. IBM Journal of 
Research and Development, 2(4):354-361.  
C.M. Bishop. 2006. Linear Models for Classifica-
tion, Pattern Recognition and Machine Learning, 
chapter 4, Springer. 
J. M. Conroy and D. P. O?leary. 2001. Text Sum-
marization via Hidden Markov Models. In SIGIR, 
406-407. 
Hal Daum?e III, and D. Marcu.  2006. Bayesian 
Query- Focused Summarization, In ACL 
H. P. Edmundson. 1969. New Methods in Auto-
matic Extracting. Journal of the Association for 
Computing Machinery, 16(2):264-285. 
W. B. Frakes, R. Baeza-Yates, 1992, Information 
Retrieval Data Structures & Algorithms. Prentice 
Hall PTR, New Jersey  
Y. H. Gong and X. Liu. 2001. Generic text summa-
rization using relevance measure and latent seman-
tic analysis. In SIGIR, 19-25 
J. Kupiec, J. Pedersen, and F. Chen. 1995. A 
Trainable Document Summarizer. Research and 
Development in Information Retrieval, 68-73 
J. D. Lafferty, A. McCallum and F. C. N. Pereira. 
2001. Conditional random fields: probabilistic 
models for segmenting and labeling sequence data. 
ICML, 282-289. 
D. C. Liu and J. Nocedal. 1989. On the limited 
memory BFGS method for large-scale optimiza-
tion. Mathematic Programming, 45:503-528. 
H. P. Luhn. 1958. The Automatic Creation of Lit-
erature Abstracts. IBM Journal of Research and 
Development, 2(2): 159 -165. 
 LSA HITS Seim-CRF 
F1 0.324 0.368 0.407 
131
A. McCallum, D. Freitag, and F. Pereira. 2000. 
Maximum entropy Markov models for information 
extraction and segmentation. In ICML, 591-598 
Mihalcea R. Mihalcea. 2005. Language independ-
ent extractive summarization. In AAAI, 1688-1689 
S. D. Pietra, V. D. Pietra, and J. D. Lafferty. 1997. 
Inducing features of random fields. IEEE Tran. on 
Pattern Analysis and Machine Intelligence, 
19(:)380?393. 
D. R. Radev, E. Hovy and K. McKeown. 2002. 
Introduction to the Special Issue on Summarization. 
Computational Linguistics, 28(4): 399-408.  
S. Sarawagi and W.W. Cohen. 2004. Semi-markov 
conditional random fields for information extrac-
tion.In NIPS 
D. Shen, J. T. Sun, H. Li, Q. Yang, Z. Chen. 2007. 
Document Summarization using Conditional Ran-
dom Fields? In IJCAI, 1805-1813 
J. Y. Yeh, H. R. Ke, W. P. Yang and I. H. Meng. 
2005. Text summarization using trainable summar-
izer and latent semantic analysis. IPM, 41(1): 75?
95  
 
132
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 257?260,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multi-domain Sentiment Classification 
 
Shoushan Li and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{sshanli,cqzong}@nlpr.ia.ac.cn 
 
 
 
 
 
Abstract 
This paper addresses a new task in sentiment 
classification, called multi-domain sentiment 
classification, that aims to improve perform-
ance through fusing training data from multi-
ple domains. To achieve this, we propose two 
approaches of fusion, feature-level and classi-
fier-level, to use training data from multiple 
domains simultaneously. Experimental stud-
ies show that multi-domain sentiment classi-
fication using the classifier-level approach 
performs much better than single domain 
classification (using the training data indi-
vidually). 
1 Introduction 
Sentiment classification is a special task of text 
categorization that aims to classify documents 
according to their opinion of, or sentiment toward 
a given subject (e.g., if an opinion is supported or 
not) (Pang et al, 2002). This task has created a 
considerable interest due to its wide applications. 
Sentiment classification is a very domain-
specific problem; training a classifier using the 
data from one domain may fail when testing 
against data from another. As a result, real 
application systems usually require some labeled 
data from multiple domains, guaranteeing an 
acceptable performance for different domains. 
However, each domain has a very limited amount 
of training data due to the fact that creating large-
scale high-quality labeled corpora is difficult and 
time-consuming.  Given the limited multi-domain 
training data, an interesting task arises, how to 
best make full use of all training data to improve 
sentiment classification performance. We name 
this new task, ?multi-domain sentiment 
classification?. 
In this paper, we propose two approaches to 
multi-domain sentiment classification. In the first, 
called feature-level fusion, we combine the feature 
sets from all the domains into one feature set. 
Using the unified feature set, we train a classifier 
using all the training data regardless of domain. In 
the second approach, classifier-level fusion, we 
train a base classifier using the training data from 
each domain and then apply combination methods 
to combine the base classifiers. 
2 Related Work 
Sentiment classification has become a hot topic 
since the publication work that discusses classifi-
cation of movie reviews by Pang et al (2002). 
This was followed by a great many studies into 
sentiment classification focusing on many do-
mains besides that of movie. 
Research into sentiment classification over 
multiple domains remains sparse. It is worth not-
ing that Blitzer et al (2007) deal with the domain 
adaptation problem for sentiment classification 
where labeled data from one domain is used to 
train a classifier for classifying data from a differ-
ent domain. Our work focuses on the problem of 
how to make multiple domains ?help each other? 
when all contain some labeled samples. These two 
problems are both important for real applications 
of sentiment classification. 
3 Our Approaches 
3.1 Problem Statement 
In a standard supervised classification problem, 
we seek a predictor f (also called a classifier) that 
257
maps an input vector x to the corresponding class 
label y. The predictor is trained on a finite set of 
labeled examples { ( , )i iX Y } (i=1,?,n) and its 
objective is to minimize expected error, i.e., 
l arg min ( ( ), )
n
i i
f i
f L f X Y
?
= ?
?
 
Where L is a prescribed loss function and H is a 
set of functions called the hypothesis space, which 
consists of functions from x to y. In sentiment 
classification, the input vector of one document is 
constructed from weights of terms. The terms 
1( ,..., )Nt t  are possibly words, word n-grams, or 
even phrases extracted from the training data, with 
N being the number of terms. The output label y 
has a value of 1 or -1 representing a positive or 
negative sentiment classification. 
  In multi-domain classification, m different 
domains are indexed by k={1,?,m}, each with 
kn training samples ( , )k ki iX Y {1,..., }k ki n= . A 
straightforward approach is to train a predictor kf  
for the k-th domain only using the training 
data {( , )}
k ki i
X Y . We call this approach single 
domain classification and show its architecture in 
Figure 1. 
 
Figure 1: The architecture of single domain classifica-
tion. 
3.2 Feature-level Fusion Approach 
Although terms are extracted from multiple do-
mains, some occur in all domains and convey the 
same sentiment (this can be called global senti-
ment information). For example, some terms like 
?excellent? and ?perfect? express positive senti-
ment information independent of domain. To learn 
the global sentiment information more correctly, 
we can pool the training data from all domains for 
training. Our first approach is using a common set 
of terms 1( ' ,..., ' )allNt t  to construct a uniform fea-
ture vector 'x  and then train a predictor using all 
training data: 
m
1 1
arg min ( ( ' ), )
k
k k
all all k
nm
all i i
f k i
f L f X Y
? = =
= ??
?
 
We call this approach feature-level fusion and 
show its architecture in Figure 2. The common set 
of terms is the union of the term sets from 
multiple domains.  
 
Figure 2: The architecture of the feature-level fusion 
approach  
 
Feature-level fusion approach is simple to 
implement and needs no extra labeled data. Note 
that training data from different domains 
contribute differently to the learning process for a 
specific domain. For example, given data from 
three domains, books, DVDs and kitchen, we 
decide to train a classifier for classifying reviews 
from books. As the training data from DVDs is 
much more similar to books than that from 
kitchen (Blitzer et al, 2007), we should give the 
data from DVDs a higher weight. Unfortunately, 
the feature-level fusion approach lacks the 
capacity to do this. A more qualified approach is 
required to deal with the differences among the 
classification abilities of training data from 
different domains. 
3.3 Classifier-level Fusion Approach 
As mentioned in sub-Section 2.1, single domain 
classification is used to train a single classifier for 
each domain using the training data in the corre-
sponding domain. As all these single classifiers 
aim to determine the sentiment orientation of a 
document, a single classifier can certainly be used 
to classify documents from other domains. Given 
multiple single classifiers, our second approach is 
to combine them to be a multiple classifier system 
for sentiment classification. We call this approach 
classifier-level fusion and show its architecture in 
Figure 3. This approach consists of two main steps: 
Training Data 
from Domain 1 
Training Data 
from Domain 2 
Training Data 
from Domain m
Classifier  
1 
Classifier 
2 
Classifier 
m 
Testing Data 
from Domain 1 
Testing Data 
from Domain 2 
Testing Data 
from Domain m
   . . . 
   . . . 
   . . . 
Training Data 
from Domain 1
Training Data 
from Domain 2 
Training Data 
from Domain m
Classifier 
Testing Data 
from Domain 1
Testing Data 
from Domain 2
Testing Data 
from Domain m
   . . . 
   . . . 
Training Data from all Domains 
using a Uniform Feature Vector 
258
(1) train multiple base classifiers (2) combine the 
base classifiers. In the first step, the base classifi-
ers are multiple single classifiers kf  (k=1,?,m) 
from all domains. In the second step, many com-
bination methods can be applied to combine the 
base classifiers. A well-known method called 
meta-learning (ML) has been shown to be very 
effective (Vilalta and Drissi, 2002). The key idea 
behind this method is to train a meta-classifier 
with input attributes that are the output of the base 
classifiers. 
 
Figure 3: The architecture of the classifier-level fusion 
approach 
 
Formally, let 'kX denote a feature vector of a 
sample from the development data of the 
'-thk domain ( ' 1,..., )k m= . The output of the 
-thk base classifier kf on this sample is the 
probability distribution over the set of classes 
1 2{ , ,..., }nc c c , i.e., 
' 1 ' '( )  ( | ),..., ( | )k k k k k n kp X p c X p c X= < >  
For the '-thk domain, we train a meta-classifier 
'  ( ' 1,..., )kf k m= using the development data from 
the '-thk domain with the meta-level feature 
vector '
meta m n
kX R
??  
' 1 ' ' ' ( ),..., ( ),..., ( )
meta
k k k k m kX p X p X p X= < >  
Each meta-classifier is then used to test the testing 
data from the same domain.  
Different from the feature-level approach, the 
classifier-level approach treats the training data 
from different domains individually and thus has 
the ability to take the differences in classification 
abilities into account. 
4 Experiments 
Data Set:  We carry out our experiments on the 
labeled product reviews from four domains: books, 
DVDs, electronics, and kitchen appliances1. Each 
domain contains 1,000 positive and 1,000 
negative reviews.  
Experiment Implementation: We apply SVM 
algorithm to construct our classifiers which has 
been shown to perform better than many other 
classification algorithms (Pang et al, 2002). Here, 
we use LIBSVM2 with a linear kernel function for 
training and testing. In our experiments, the data 
in each domain are partitioned randomly into 
training data, development data and testing data 
with the proportion of 70%, 20% and 10% 
respectively. The development data are used to 
train the meta-classifier. 
Baseline: The baseline uses the single domain 
classification approach mentioned in sub-Section 
2.1. We test four different feature sets to construct 
our feature vector. First, we use unigrams (e.g., 
?happy?) as features and perform the standard fea-
ture selection process to find the optimal feature 
set of unigrams (1Gram). The selection method is 
Bi-Normal Separation (BNS) that is reported to be 
excellent in many text categorization tasks (For-
man, 2003). The criterion of the optimization is to 
find the set of unigrams with the best performance 
on the development data through selecting the 
features with high BNS scores.  Then, we get the 
optimal word bi-gram (e.g., ?very happy?) (2Gram) 
and mixed feature set (1+2Gram) in the same way. 
The fourth feature set (1Gram+2Gram) also con-
sists of unigrams and bi-grams just like the third 
one. The difference between them lies in their se-
lection strategy. The third feature set is obtained 
through selecting the unigrams and bi-grams with 
high BNS scores while the fourth one is obtained 
through simply uniting the two optimal sets of 
1Gram and 2Gram.  
From Table 1, we see that 1Gram+2Gram fea-
tures perform much better than other types of fea-
tures, which implies that we need to select good 
unigram and bi-gram features separately before 
combine them. Although the size of our training 
data are smaller than that reported in Blitzer et al 
                                                          
1 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
2 LIBSVM is an integrated software for SVM: 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
Training Data 
from Domain 1 
Training Data 
from Domain 2 
Training Data 
from Domain m
Multiple Classifier 
System 1 
Testing Data 
from Domain 1 
Testing Data 
from Domain 2 
Testing Data 
from Domain m
   . . . 
   . . . 
Base Classifier 
1 
Base Classifier 
2 
Base Classifier
m 
   . . . 
Multiple Classifier 
System 2 
Multiple Classifier 
System m 
Development Data 
from Domain 1 
Development Data 
from Domain 2 
Development Data 
from Domain m 
   . . . 
   . . . 
259
(2007) (70% vs. 80%), the classification perform-
ance is comparative to theirs. 
 
We implement the fusion using 1+2Gram and 
1Gram+2Gram respectively. From Figure 4, we 
see that both the two fusion approaches generally 
outperform single domain classification when us-
ing 1+2Gram features. They increase the average 
accuracy from 0.8 to 0.82375 and 0.83875, a sig-
nificant relative error reduction of 11.87% and 
19.38% over baseline.  
1+2Gram Features
76.5
81 80
8382.5 82.5 82.581
83 84
86
83
72
74
76
78
80
82
84
86
88
Books DVDs Electronics Kitchen
Ac
cu
ra
cy
(%
)
  
1Gram+2Gram Features
79
84.584
82
84.5 85 838283.5
86
8889
74
76
78
80
82
84
86
88
90
Books DVDs Electronics Kitchen
Ac
cu
ra
cy
(%
)
Single domain classification
Feature-level fusion
Classifier-level fusion with ML
 Figure 4: Accuracy results on the testing data using 
multi-domain classification with different approaches. 
 
However, when the performance of baseline in-
creases, the feature level approach fails to help the 
performance improvement in three domains. This 
is mainly because the base classifiers perform ex-
tremely unbalanced on the testing data of these 
domains. For example, the four base classifiers 
from Books, DVDs, Electronics, and Kitchen 
achieve the accuracies of 0.675, 0.62, 0.85, and 
0.79 on the testing data from Electronics respec-
tively. Dealing with such an unbalanced perform-
ance, we definitely need to put enough high 
weight on the training data from Electronics. 
However, the feature-level fusion approach sim-
ply pools all training data from different domains 
and treats them equally. Thus it can not capture 
the unbalanced information. In contrast, meta-
learning is able to learn the unbalance automati-
cally through training the meta-classifier using the 
development data. Therefore, it can still increase 
the average accuracy from 0.8325 to 0.8625, an 
impressive relative error reduction of 17.91% over 
baseline. 
5 Conclusion 
In this paper, we propose two approaches to multi-
domain classification task on sentiment classifica-
tion. Empirical studies show that the classifier-
level approach generally outperforms the feature 
approach.  Compared to single domain classifica-
tion, multi-domain classification with the classi-
fier-level approach can consistently achieve much 
better results. 
Acknowledgments 
The research work described in this paper has 
been partially supported by the Natural Science 
Foundation of China under Grant No. 60575043, 
and 60121302, National High-Tech Research and 
Development Program of China under Grant No. 
2006AA01Z194, National Key Technologies 
R&D Program of China under Grant No. 
2006BAH03B02, and Nokia (China) Co. Ltd as 
well. 
References 
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, 
Bollywood, Boom-boxes and Blenders: Domain ad-
aptation for sentiment classification. In Proceedings 
of ACL.  
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Journal 
of Machine Learning Research, 3: 1533-7928. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP. 
R. Vilalta and Y. Drissi. 2002. A perspective view and 
survey of meta-learning. Artificial Intelligence Re-
view, 18(2): 77?95. 
Features Books DVDs Elec-
tronic 
Kitchen
1Gram 0.75 0.84 0.8 0.825 
2Gram 0.75 0.73 0.815 0.785 
1+2Gram 0.765 0.81 0.825 0.80 
1Gram+2Gram 0.79 0.845 0.85 0.845 
 
Table 1: Accuracy results on the testing data of single 
domain classification using different feature sets. 
260
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692?700,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Framework of Feature Selection Methods for Text Categorization 
 
 
Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1 
 
1
 Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang} 
@gmail.com 
 
2
 National Laboratory of Pattern 
Recognition 
 Institute of Automation 
 Chinese Academy of Sciences  
{rxia,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In text categorization, feature selection (FS) is 
a strategy that aims at making text classifiers 
more efficient and accurate. However, when 
dealing with a new task, it is still difficult to 
quickly select a suitable one from various FS 
methods provided by many previous studies. 
In this paper, we propose a theoretic 
framework of FS methods based on two basic 
measurements: frequency measurement and 
ratio measurement. Then six popular FS 
methods are in detail discussed under this 
framework. Moreover, with the guidance of 
our theoretical analysis, we propose a novel 
method called weighed frequency and odds 
(WFO) that combines the two measurements 
with trained weights. The experimental results 
on data sets from both topic-based and 
sentiment classification tasks show that this 
new method is robust across different tasks 
and numbers of selected features.  
1 Introduction 
With the rapid growth of online information, text 
classification, the task of assigning text 
documents to one or more predefined categories, 
has become one of the key tools for 
automatically handling and organizing text 
information. 
The problems of text classification normally 
involve the difficulty of extremely high 
dimensional feature space which sometimes 
makes learning algorithms intractable. A 
standard procedure to reduce the feature 
dimensionality is called feature selection (FS). 
Various FS methods, such as document 
frequency (DF), information gain (IG), mutual 
information (MI), 2? -test (CHI), Bi-Normal 
Separation (BNS), and weighted log-likelihood 
ratio (WLLR), have been proposed for the tasks 
(Yang and Pedersen, 1997; Nigam et al, 2000; 
Forman, 2003) and make text classification more 
efficient and accurate. 
However, comparing these FS methods 
appears to be difficult because they are usually 
based on different theories or measurements. For 
example, MI and IG are based on information 
theory, while CHI is mainly based on the 
measurements of statistic independence. 
Previous comparisons of these methods have 
mainly depended on empirical studies that are 
heavily affected by the experimental sets. As a 
result, conclusions from those studies are 
sometimes inconsistent. In order to better 
understand the relationship between these 
methods, building a general theoretical 
framework provides a fascinating perspective. 
Furthermore, in real applications, selecting an 
appropriate FS method remains hard for a new 
task because too many FS methods are available 
due to the long history of FS studies. For 
example, merely in an early survey paper 
(Sebastiani, 2002), eight methods are mentioned. 
These methods are provided by previous work 
for dealing with different text classification tasks 
but none of them is shown to be robust across 
different classification applications. 
In this paper, we propose a framework with 
two basic measurements for theoretical 
comparison of six FS methods which are widely 
used in text classification. Moreover, a novel 
method is set forth that combines the two 
measurements and tunes their influences 
considering different application domains and 
numbers of selected features. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work on 
692
feature selection for text classification. Section 3 
theoretically analyzes six FS methods and 
proposes a new FS approach. Experimental 
results are presented and analyzed in Section 4. 
Finally, Section 5 draws our conclusions and 
outlines the future work. 
2 Related Work 
FS is a basic problem in pattern recognition and 
has been a fertile field of research and 
development since the 1970s. It has been proven 
to be effective on removing irrelevant and 
redundant features, increasing efficiency in 
learning tasks, and improving learning 
performance. 
FS methods fall into two broad categories, the 
filter model and the wrapper model (John et al, 
1994). The wrapper model requires one 
predetermined learning algorithm in feature 
selection and uses its performance to evaluate 
and determine which features are selected. And 
the filter model relies on general characteristics 
of the training data to select some features 
without involving any specific learning 
algorithm. There is evidence that wrapper 
methods often perform better on small scale 
problems (John et al 1994), but on large scale 
problems, such as text classification, wrapper 
methods are shown to be impractical because of 
its high computational cost. Therefore, in text 
classification, filter methods using feature 
scoring metrics are popularly used. Below we 
review some recent studies of feature selection 
on both topic-based and sentiment classification. 
In the past decade, FS studies mainly focus on 
topic-based classification where the classification 
categories are related to the subject content, e.g., 
sport or education. Yang and Pedersen (1997) 
investigate five FS metrics and report that good 
FS methods improve the categorization accuracy 
with an aggressive feature removal using DF, IG, 
and CHI. More recently, Forman (2003) 
empirically compares twelve FS methods on 229 
text classification problem instances and 
proposes a new method called 'Bi-Normal 
Separation' (BNS). Their experimental results 
show that BNS can perform very well in the 
evaluation metrics of recall rate and F-measure. 
But for the metric of precision, it often loses to 
IG. Besides these two comparison studies, many 
others contribute to this topic (Yang and Liu, 
1999; Brank et al, 2002; Gabrilovich and 
Markovitch, 2004) and more and more new FS 
methods are generated, such as, Gini index 
(Shang et al, 2007), Distance to Transition Point 
(DTP) (Moyotl-Hernandez and Jimenez-Salazar, 
2005), Strong Class Information Words (SCIW) 
(Li and Zong, 2005) and parameter tuning based 
FS for Rocchio classifier (Moschitti, 2003). 
Recently, sentiment classification has become 
popular because of its wide applications (Pang et 
al., 2002). Its criterion of classification is the 
attitude expressed in the text (e.g., recommended 
or not recommended, positive or negative) rather 
than some facts (e.g., sport or education). To our 
best knowledge, yet no related work has focused 
on comparison studies of FS methods on this 
special task. There are only some scattered 
reports in their experimental studies. Riloff et al 
(2006) report that the traditional FS method 
(only using IG method) performs worse than the 
baseline in some cases. However, Cui et al 
(2006) present the experiments on the sentiment 
classification for large-scale online product 
reviews to show that using the FS method of CHI 
does not degrade the performance but can 
significantly reduce the dimension of the feature 
vector. 
Moreover, Ng et al (2006) examine the FS of 
the weighted log-likelihood ratio (WLLR) on the 
movie review dataset and achieves an accuracy 
of 87.1%, which is higher than the result reported 
by Pang and Lee (2004) with the same dataset. 
From the analysis above, we believe that the 
performance of the sentiment classification 
system is also dramatically affected by FS. 
3 Our Framework 
In the selection process, each feature (term, or 
single word) is assigned with a score according 
to a score-computing function. Then those with 
higher scores are selected. These mathematical 
definitions of the score-computing functions are 
often defined by some probabilities which are 
estimated by some statistic information in the 
documents across different categories. For the 
convenience of description, we give some 
notations of these probabilities below. 
( )P t : the probability that a document x  contains 
term t ; 
( )iP c : the probability that a document x  does 
not belong to category ic ; 
( , )iP t c : the joint probability that a document x  
contains term t  and also belongs to category ic ; 
( | )iP c t : the probability that a document x belongs 
to category ic ?under the condition that it contains  
term t. 
693
( | )iP t c : the probability that, a document x does 
not contain term t with the condition that x belongs to 
category ic ; 
Some other probabilities, such as ( )P t , ( )iP c , 
( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , are 
similarly defined. 
In order to estimate these probabilities, 
statistical information from the training data is 
needed, and notations about the training data are 
given as follows: 
1{ }mi ic = : the set of categories; 
iA : the number of the documents that contain the 
term t  and also belong to category ic ; 
iB : the number of the documents that contain the 
term t  but do not belong to category ic ; 
iN : the total number of the documents that belong 
to category ic ; 
allN : the total number of all documents from the 
training data. 
iC : the number of the documents that do not 
contain the term t  but belong to category ic , i.e., 
i iN A?  
iD : the number of the documents that neither 
contain the term t  nor belong to category ic , i.e., 
all i iN N B? ? ; 
In this section, we would analyze theoretically 
six popular methods, namely DF, MI, IG, CHI, 
BNS, and WLLR. Although these six FS 
methods are defined differently with different 
scoring measurements, we believe that they are 
strongly related. In order to connect them, we 
define two basic measurements which are 
discussed as follows. 
The first measurement is to compute the 
document frequency in one category, i.e., iA .  
The second measurement is the ratio between 
the document frequencies in one category and 
the other categories, i.e., /i iA B . The terms with 
a high ratio are often referred to as the terms with 
high category information. 
These two measurements form the basis for all 
the measurements that are used by the FS 
methods throughout this paper. In particular, we 
show that DF and MI are using the first and 
second measurement respectively. Other 
complicated FS methods are combinations of 
these two measurements. Thus, we regard the 
two measurements as basic, which are referred to 
as the frequency measurement and ratio 
measurement. 
3.1 Document Frequency (DF) 
DF is the number of documents in which a term 
occurs. It is defined as 
1
( )m iiDF A==?  
The terms with low or high document 
frequency are often referred to as rare or 
common terms, respectively. It is easy to see that 
this FS method is based on the first basic 
measurement. It assumes that the terms with 
higher document frequency are more informative 
for classification. But sometimes this assumption 
does not make any sense, for example, the stop 
words (e.g., the, a, an) hold very high DF scores, 
but they seldom contribute to classification. In 
general, this simple method performs very well 
in some topic-based classification tasks (Yang 
and Pedersen, 1997). 
3.2 Mutual Information (MI) 
The mutual information between term t  and 
class ic  is defined as 
( | )( , ) log ( )
i
i
P t cI t c
P t
=  
And it is estimated as 
log ( )( )
i all
i i i i
A NMI
A C A B
?
=
+ +
 
Let us consider the following formula (using 
Bayes theorem) 
( | ) ( | )( , ) log log( ) ( )
i i
i
i
P t c P c tI t c
P t P c
= =  
Therefore, 
( , )= log ( | ) log ( )i i iI t c P c t P c?  
And it is estimated as 
log log
      log log
1
      log(1 ) log
/
i i
i i all
i i i
i all
i
i i all
A NMI
A B N
A B N
A N
N
A B N
= ?
+
+
= ? ?
= ? + ?
 
From this formula, we can see that the MI score 
is based on the second basic measurement. This 
method assumes that the term with higher 
category ratio is more effective for classification. 
It is reported that this method is biased 
towards low frequency terms and the bias 
becomes extreme when ( )P t  is near zero. It can 
be seen in the following formula (Yang and 
Pedersen, 1997)  
( , ) log( ( | )) log( ( ))i iI t c P t c P t= ?  
694
Therefore, this method might perform badly 
when common terms are informative for 
classification. 
Taking into account mutual information of all 
categories, two types of MI score are commonly 
used: the maximum score ( )
max
I t  and the 
average score ( )avgI t , i.e.,  
1( ) max { ( , )}mmax i iI t I t c== , 
1
( ) ( ) ( , )mavg i iiI t P c I t c== ?? .  
We choose the maximum score since it performs 
better than the average score (Yang and Pedersen, 
1997). It is worth noting that the same choice is 
made for other methods, including CHI, BNS, 
and WLLR in this paper. 
3.3 Information Gain (IG) 
IG measures the number of bits of information 
obtained for category prediction by recognizing 
the presence or absence of a term in a document 
(Yang and Pedersen, 1997). The function is 
1
1
1
( ) { ( ) log ( )}
            +{ ( )[ ( | ) log ( | )]
           ( )[ ( | ) log ( | )]}
m
i ii
m
i ii
m
i ii
G t P c P c
P t P c t P c t
P t P c t P c t
=
=
=
= ?
+
?
?
?
 
And it is estimated as 
1
1 1
1 1
{ log }
    +( / )[ log ]
  ( / )[ log ]
m i i
i
all all
m m i i
i alli i
i i i i
m m i i
i alli i
i i i i
N NIG
N N
A AA N
A B A B
C CC N
C D C D
=
= =
= =
= ?
+ +
+
+ +
?
? ?
? ?
From the definition, we know that the 
information gain is the weighted average of the 
mutual information ( , )iI t c and ( , )iI t c  where 
the weights are the joint probabilities ( , )iP t c and 
( , )iP t c : 
1 1
( ) ( , ) ( , ) ( , ) ( , )m mi i i ii iG t P t c I t c P t c I t c= == +? ?  
Since ( , )iP t c is closely related to the 
document frequency iA  and the mutual 
information ( , )iI t c  is shown to be based on the 
second measurement, we can say that the IG 
score is influenced by the two basic 
measurements. 
3.4 2?  Statistic (CHI) 
The CHI measurement (Yang and Pedersen, 
1997) is defined as 
2( )
( ) ( ) ( ) ( )
all i i i i
i i i i i i i i
N A D C BCHI
A C B D A B C D
? ?
=
+ ? + ? + ? +
 
In order to get the relationship between CHI 
and the two measurements, the above formula is 
rewritten as follows 
2[ ( ) ( ) ]
( ) ( ) [ ( )]
all i all i i i i i
i all i i i all i i
N A N N B N A BCHI
N N N A B N A B
? ? ? ? ?
=
? ? ? + ? ? +
  
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same 
( 2
all iN N= ). The CHI score then can be written 
as
 
2
2
2 ( )
( ) [2 ( )]
2 ( / 1)
      2( / 1) [ / ( / 1)]
i i i
i i i i i
i i i
i
i i i i i i
i
N A BCHI
A B N A B
N A B
NA B A B A B
A
?
=
+ ? ? +
?
=
+ ? ? ? +
 
From the above formula, we see that the CHI 
score is related to both the frequency 
measurement iA
 
and ratio measurement 
/i iA B . Also, when keeping the same ratio value, 
the terms with higher document frequencies will 
yield higher CHI scores. 
3.5 Bi-Normal Separation (BNS) 
BNS method is originally proposed by Forman 
(2003) and it is defined as 
1 1( , ) ( ( | )) ( ( | )i i iBNS t c F P t c F P t c? ?= ?  
It is calculated using the following formula 
1 1( ) ( )i i
i all i
A B
BNS F F
N N N
? ?
= ?
?
 
where ( )F x  is the cumulative probability 
function of standard normal distribution. 
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same, 
i.e., 2
all iN N=  and we also assume that i iA B> . 
It should be noted that this assumption is only to 
allow easier analysis but will not be applied in 
our experiment implementation. In addition, we 
only consider the case when / 0.5i iA N ? . In 
fact, most terms take the document frequency 
iA which is less than half of iN .  
Under these conditions, the BNS score can be 
shown in Figure 1 where the area of the shadow 
part represents ( / / )i i i iA N B N?  and the length 
of the projection to the x  axis represents the 
BNS score. 
695
From Figure 1, we can easily draw the two 
following conclusions: 
1) Given the same value of iA , the BNS score 
increases with the increase of i iA B? . 
2) Given the same value of i iA B? , BNS score 
increase with the decrease of iA . 
 
Figure 1. View of BNS using the normal probability 
distribution. Both the left and right graphs have 
shadowed areas of the same size. 
 
And the value of i iA B?  can be rewritten as 
the following 
1(1 )
/
i i
i i i i
i i i
A BA B A A
A A B
?
? = ? = ? ?  
The above analysis gives the following 
conclusions regarding the relationship between 
BNS and the two basic measurements: 
1) Given the same iA , the BNS score increases 
with the increase of /i iA B . 
2) Given the same /i iA B , when iA  increases, 
i iA B?  also increase. It seems that the BNS 
score does not show a clear relationship with 
iA . 
In summary, the BNS FS method is biased 
towards the terms with the high category ratio 
but cannot be said to be sensitive to document 
frequency. 
3.6 Weighted Log Likelihood Ratio 
(WLLR) 
WLLR method (Nigam et al, 2000) is defined as 
( | )( , ) ( | ) log ( | )
i
i i
i
P t cWLLR t c P t c
P t c
=  
And it is estimated as 
( )logi i all i
i i i
A A N NWLLR
N B N
? ?
=
?
 
The formula shows WLLR is proportional to 
the frequency measurement and the logarithm of 
the ratio measurement. Clearly, WLLR is biased 
towards the terms with both high category ratio 
and high document frequency and the frequency 
measurement seems to take a more important 
place than the ratio measurement. 
3.7 Weighed Frequency and Odds (WFO)  
So far in this section, we have shown that the 
two basic measurements constitute the six FS 
methods. The class prior probabilities, 
( ),  1,2,...,iP c i m= , are also related to the 
selection methods except for the two basic 
measurements. Since they are often estimated 
according to the distribution of the documents in 
the training data and are identical for all the 
terms in a class, we ignore the discussion of their 
influence on the selection measurements. In the 
experiment, we consider the case when training 
data have equal class prior probabilities. When 
training data are unbalanced, we need to change 
the forms of the two basic measurements to 
/i iA N  and ( ) / ( )i all i i iA N N B N? ? ? . 
Because some methods are expressed in 
complex forms, it is difficult to explain their 
relationship with the two basic measurements, 
for example, which one prefers the category ratio 
most. Instead, we will give the preference 
analysis in the experiment by analyzing the 
features in real applications. But the following 
two conclusions are drawn without doubt 
according to the theoretical analysis given above. 
1) Good features are features with high 
document frequency; 
2) Good features are features with high 
category ratio. 
These two conclusions are consistent with the 
original intuition. However, using any single one 
does not provide competence in selecting the 
best set of features. For example, stop words, 
such as ?a?, ?the? and ?as?, have very high 
document frequency but are useless for the 
classification. In real applications, we need to 
mix these two measurements to select good 
features. Because of different distribution of 
features in different domains, the importance of 
each measurement may differ a lot in different 
applications. Moreover, even in a given domain, 
when different numbers of features are to be 
selected, different combinations of the two 
measurements are required to provide the best 
performance. 
Although a great number of FS methods is 
available, none of them can appropriately change 
the preference of the two measurements. A better 
way is to tune the importance according to the 
application rather than to use a predetermined 
combination. Therefore, we propose a new FS 
method called Weighed Frequency and Odds 
(WFO), which is defined as 
696
 ( | ) / ( | ) 1i iwhen P t c P t c >  
1( | )( , ) ( | ) [log ]( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
                 ( , ) 0i
else
WFO t c =
 
And it is estimated as 
1( )( ) (log )i i all i
i i i
A A N NWFO
N B N
? ??? ?
=
?
 
where ?
 
is the parameter for tuning the weight 
between frequency and odds. The value of ?
 
varies from 0 to 1. By assigning different value 
to ?  we can adjust the preference of each 
measurement. Specially, when 0? = , the 
algorithm prefers the category ratio that is 
equivalent to the MI method; when 1? = , the 
algorithm is similar to DF; when 0.5? = , the 
algorithm is exactly the WLLR method. In real 
applications, a suitable parameter ?  needs to be 
learned by using training data. 
4 Experimental Studies  
4.1 Experimental Setup 
Data Set:  The experiments are carried out on 
both topic-based and sentiment text classification 
datasets. In topic-based text classification, we 
use two popular data sets: one subset of 
Reuters-21578 referred to as R2 and the 20 
Newsgroup dataset referred to as 20NG. In detail, 
R2 consist of about 2,000 2-category documents 
from standard corpus of Reuters-21578. And 
20NG is a collection of approximately 20,000 
20-category documents 1 . In sentiment text 
classification, we also use two data sets: one is 
the widely used Cornell movie-review dataset2 
(Pang and Lee, 2004) and one dataset from 
product reviews of domain DVD3 (Blitzer et al, 
2007). Both of them are 2-category tasks and 
each consists of 2,000 reviews. In our 
experiments, the document numbers of all data 
sets are (nearly) equally distributed cross all 
categories. 
Classification Algorithm: Many 
classification algorithms are available for text 
classification, such as Na?ve Bayes, Maximum 
Entropy, k-NN, and SVM. Among these methods, 
SVM is shown to perform better than other 
methods (Yang and Pedersen, 1997; Pang et al, 
                                                      
1
 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 
2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 
3
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
 
2002). Hence we apply SVM algorithm with the 
help of the LIBSVM 4  tool. Almost all 
parameters are set to their default values except 
the kernel function which is changed from a 
polynomial kernel function to a linear one 
because the linear one usually performs better for 
text classification tasks. 
Experiment Implementation: In the 
experiments, each dataset is randomly and 
evenly split into two subsets: 90% documents as 
the training data and the remaining 10% as 
testing data. The training data are used for 
training SVM classifiers, learning parameters in 
WFO method and selecting "good" features for 
each FS method. The features are single words 
with a bool weight (0 or 1), representing the 
presence or absence of a feature. In addition to 
the ?principled? FS methods, terms occurring in 
less than three documents ( 3DF ? ) in the 
training set are removed. 
4.2 Relationship between FS Methods and 
the Two Basic Measurements 
To help understand the relationship between FS 
methods and the two basic measurements, the 
empirical study is presented as follows. 
Since the methods of DF and MI only utilize 
the document frequency and category 
information respectively, we use the DF scores 
and MI scores to represent the information of the 
two basic measurements. Thus we would select 
the top-2% terms with each method and then 
investigate the distribution of their DF and MI 
scores.  
First of all, for clear comparison, we 
normalize the scores coming from all the 
methods using Min-Max normalization method 
which is designed to map a score s  to 's  in 
the range [0, 1] by computing 
'
s Min
s
Max Min
?
=
?
 
where
 
Min
 
and Max
 
denote the minimum 
and maximum values respectively in all terms? 
scores using one FS method. 
Table 1 shows the mean values of all top-2% 
terms? MI scores and DF scores of all the six FS 
methods in each domain. From this table, we can 
apparently see the relationship between each 
method and the two basic measurements. For 
instance, BNS most distinctly prefers the terms 
with high MI scores and low DF scores. 
According to the degree of this preference, we 
                                                      
4
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
697
FS 
Methods 
Domain 
20NG R2 Movie DVD 
DF score MI score DF score MI score DF score MI score DF score MI score 
MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 
BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 
CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 
IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 
WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 
DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111 
 
Table 1. The mean values of all top-2% terms? MI and DF scores using six FS methods in each domain 
 
can rank these six methods as 
MI, BNS IG, CHI, WLLR DFf f , where x yf
 
means method x
 
prefers the terms with  
higher MI scores (higher category information) 
and lower DF scores (lower document frequency) 
than method y. This empirical discovery is in 
agreement with the finding that WLLR is biased 
towards the high frequency terms and also with 
the finding that BNS is biased towards high 
category information (cf. Section 3 theoretical 
analysis). Also, we can find that CHI and IG 
share a similar preference of these two 
measurements in 2-category domains, i.e., R2, 
movie, and DVD. This gives a good explanation 
that CHI and IG are two similar-performed 
methods for 2-category tasks, which have been 
found by Forman (2003) in their experimental 
studies. 
According to the preference, we roughly 
cluster FS methods into three groups. The first 
group includes the methods which dramatically 
prefer the category information, e.g., MI and 
BNS; the second one includes those which prefer 
both kinds of information, e.g., CHI, IG, and 
WLLR; and the third one includes those which 
strongly prefer frequency information, e.g., DF. 
4.3 Performances of Different FS Methods 
It is worth noting that learning parameters in 
WFO is very important for its good performance. 
We use 9-fold cross validation to help learning 
the parameter ?  so as to avoid over-fitting. 
Specifically, we run nine times by using every 8 
fold documents as a new training data set and the 
remaining one fold documents as a development 
data set. In each running with one fixed feature 
number m, we get the best 
,i m best? ? (i=1,..., 9) 
value through varying 
,i m?  from 0 to 1 with the 
step of 0.1 to get the best performance in the 
development data set. The average value 
m best? ? , 
i.e., 
9
,1
( ) / 9m best i m besti? ?? ?== ?  
is used for further testing. 
Figure 2 shows the experimental results when 
using all FS methods with different selected 
feature numbers. The red line with star tags 
represents the results of WFO. At the first glance, 
in R2 domain, the differences of performances 
across all are very noisy when the feature 
number is larger than 1,000, which makes the 
comparison meaningless. We think that this is 
because the performances themselves in this task 
are very high (nearly 98%) and the differences 
between two FS methods cannot be very large 
(less than one percent). Even this, WFO method 
do never get the worst performance and can also 
achieve the top performance in about half times, 
e.g., when feature numbers are 20, 50, 100, 500, 
3000. 
Let us pay more attention to the other three 
domains and discuss the results in the following 
two cases. 
In the first case when the feature number is 
low (about less than 1,000), the FS methods in 
the second group including IG, CHI, WLLR,  
always perform better than those in the other two 
groups. WFO can also perform well because its 
parameters 
m best? ?  are successfully learned to be 
around 0.5, which makes it consistently belong 
to the second group. Take 500 feature number 
for instance, the parameters 500 best? ?  are 0.42, 
0.50, and 0.34 in these three domains 
respectively. 
In the second case when the feature number is 
large, among the six traditional methods, MI and 
BNS take the leads in the domains of 20NG and 
Movie while IG and CHI seem to be better and 
more stable than others in the domain of DVD. 
As for WFO, its performances are excellent cross 
all these three domains and different feature 
numbers. In each domain, it performs similarly 
as or better than the top methods due to its 
well-learned parameters. For example, in 20NG, 
the parameters 
m best? ?  are 0.28, 0.20, 0.08, and 
0.01 when feature numbers are 10,000, 15,000, 
20,000, and 30,000. These values are close to 0 
698
(WFO equals MI when 0? = ) while MI is the 
top one in this domain. 
10 20 50 100 200 500 1000 2000 3000 4227
0.88
0.9
0.92
0.94
0.96
0.98
1
feature number
a
cc
u
ra
cy
Topic - R2
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
200 500 1000 2000 5000 10000 15000 20000 30000 32091
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
feature number
ac
cu
ra
cy
Topic - 20NG
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
50 200 500 1000 4000 7000 10000 13000 15176
0.55
0.6
0.65
0.7
0.75
0.8
0.85
feature number
ac
cu
ra
cy
Sentiment - Movie
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
20 50 100 500 1000 1500 2000 3000 4000 5824
0.5
0.55
0.6
0.65
0.7
0.75
0.8
feature number
ac
cu
ra
cy
Sentiment - DVD
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
Figure 2. The classification accuracies of the four domains 
using seven different FS methods while increasing the 
number of selected features. 
 
From Figure 2, we can also find that FS does 
help sentiment classification. At least, it can 
dramatically decrease the feature numbers 
without losing classification accuracies (see 
Movie domain, using only 500-4000 features is 
as good as using all 15176 features). 
5 Conclusion and Future Work 
In this paper, we propose a framework with two 
basic measurements and use it to theoretically 
analyze six FS methods. The differences among 
them mainly lie in how they use these two 
measurements. Moreover, with the guidance of 
the analysis, a novel method called WFO is 
proposed, which combine these two 
measurements with trained weights. The 
experimental results show that our framework 
helps us to better understand and compare 
different FS methods. Furthermore, the novel 
method WFO generated from the framework, can 
perform robustly across different domains and 
feature numbers. 
In our study, we use four data sets to test our 
new method. There are much more data sets on 
text categorization which can be used. In 
additional, we only focus on using balanced 
samples in each category to do the experiments. 
It is also necessary to compare the FS methods 
on some unbalanced data sets, which are 
common in real-life applications (Forman, 2003; 
Mladeni and Marko, 1999). These matters will 
be dealt with in the future work. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in the 
Hong Kong Polytechnic University. 
References  
J. Blitzer, M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain adaptation for sentiment 
classification. In Proceedings of ACL-07, the 45th 
Meeting of the Association for Computational 
Linguistics. 
J. Brank, M. Grobelnik, N. Milic-Frayling, and D. 
Mladenic. 2002. Interaction of feature selection 
methods and linear classification models. In 
Workshop on Text Learning held at ICML. 
H. Cui, V. Mittal, and M. Datar. 2006. Comparative 
experiments on sentiment classification for online 
product reviews. In Proceedings of AAAI-06, the 
21st National Conference on Artificial Intelligence. 
G. Forman. 2003. An extensive empirical study of 
feature selection metrics for text classification. The 
Journal of Machine Learning Research, 3(1): 
1289-1305. 
699
E. Gabrilovich and S. Markovitch. 2004. Text 
categorization with many redundant features: using 
aggressive feature selection to make SVMs 
competitive with C4.5. In Proceedings of the ICML, 
the 21st International Conference on Machine 
Learning. 
G. John, K. Ron, and K. Pfleger. 1994. Irrelevant 
features and the subset selection problem. In 
Proceedings of ICML-94, the 11st International 
Conference on Machine Learning.  
S. Li and C. Zong. 2005. A new approach to feature 
selection for text categorization. In Proceedings of 
the IEEE International Conference on Natural 
Language Processing and Knowledge Engineering 
(NLP-KE). 
D. Mladeni and G. Marko. 1999. Feature selection for 
unbalanced class distribution and naive bayes. In 
Proceedings of ICML-99, the 16th International 
Conference on Machine Learning. 
A. Moschitti. 2003. A study on optimal parameter 
tuning for Rocchio text classifier. In Proceedings 
of ECIR, Lecture Notes in Computer Science, 
vol. 2633, pp. 420-435. 
E. Moyotl-Hernandez and H. Jimenez-Salazar. 2005. 
Enhancement of DTP feature selection method for 
text categorization. In Proceedings of CICLing, 
Lecture Notes in Computer Science, vol.3406, 
pp.719-722.  
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. 
Examining the role of linguistic knowledge sources 
in the automatic identification and classification of 
reviews. In Proceedings of the COLING/ACL Main 
Conference Poster Sessions. 
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 
2000. Text classification from labeled and 
unlabeled documents using EM. Machine Learning, 
39(2/3): 103-134. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP-02, 
the Conference on Empirical Methods in Natural 
Language Processing. 
B. Pang and L. Lee. 2004. A sentimental education: 
Sentiment analysis using subjectivity 
summarization based on minimum cuts. In 
Proceedings of ACL-04, the 42nd Meeting of the 
Association for Computational Linguistics. 
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature 
subsumption for opinion analysis. In Proceedings 
of EMNLP-06, the Conference on Empirical 
Methods in Natural Language Processing,. 
F. Sebastiani. 2002. Machine learning in automated 
text categorization. ACM Computing Surveys, 
34(1): 1-47. 
W. Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z. 
Wang. 2007. A novel feature selection algorithm 
for text categorization. The Journal of Expert 
System with Applications, 33:1-5. 
Y. Yang and J. Pedersen. 1997. A comparative study 
on feature selection in text categorization. In 
Proceedings of ICML-97, the 14th International 
Conference on Machine Learning. 
Y. Yang and X. Liu. 1999. A re-examination of text 
categorization methods. In Proceedings of 
SIGIR-99, the 22nd annual international ACM 
Conference on Research and Development in 
Information Retrieval.  
700
Interactive Chinese-to-English Speech Translation Based on  
Dialogue Management 
 Chengqing Zong, Bo Xu, and Taiyi Huang 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
P. O. Box 2728, Beijing 100080, China 
{cqzong, xubo, huang}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In this paper, we propose a novel 
paradigm for the Chinese-to-English 
speech-to-speech (S2S) translation, which 
is interactive under the guidance of 
dialogue management. In this approach, 
the input utterance is first pre-processed 
and then serially translated by the 
template-based translator and the inter-
lingua based translator. The dialogue 
management mechanism (DMM) is 
employed to supervise the interactive 
analysis for disambiguation of the input. 
The interaction is led by the system, so 
the system always acts on its own 
initiative in the interactive procedure. In 
this approach, the complicated semantic 
analysis is not involved. 
1 Introduction 
2 
Over the past decade, many approaches to S2S 
translation have been proposed. Unfortunately, the 
S2S translation systems still suffer from the poor 
performance, even though the application domains 
are restricted. The common questions are: what 
translation strategies are necessary? What do the 
problems exist in the current S2S systems? And 
what performance of a system is acceptable? 
Based on the questions, we have analyzed the 
current approaches to machine translation (MT) 
and investigated some experimental systems and 
the user?s requirements. A novel paradigm for the 
Chinese-to-English S2S translation has been 
proposed, which is interactive under the guidance 
of DMM. In this approach, the input utterance is 
first pre-processed and serially translated by the 
template-based translator and the inter-lingua 
based translator. If the two translators are failed to 
translate the input, the dialogue management 
mechanism is brought into play to supervise the 
interactive analysis for disambiguation of the input. 
The interaction is led by the system, so the system 
always acts on its own initiative in the interactive 
procedure. In this approach, the complicated 
semantic analysis is not involved.  
Remainder of the paper presents our 
motivations and the proposal scheme in detail. 
Section 2 gives analysis on the current MT 
approaches and the user?s requirements. Section 3 
describes in detail our approach to Chinese-to-
English S2S translation. Section 4 draws 
conclusions and presents the future work. 
Analysis on MT approaches and S2S 
translation systems 
2.1 Analysis on MT approaches 
In the past decades, many MT approaches have 
been proposed. We roughly divided the current 
approaches into two types, which are respectively 
named as the mainstream approaches and the non-
mainstream approaches. The mainstream 
approaches include four basic methods: the 
analysis-based method, the example-based method, 
the template-based method and also the statistical 
method as well. The analysis-based method here 
includes the rule-based method, the inter-lingual 
method, or even the knowledge-based method. In 
the recent years, the approach based on multi-
engine has been practiced in many systems (Lavie, 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 61-68.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
1999; Wahlster, 2000; Zong, 2000a). However, the 
engines employed in these experimental systems 
are mainly based on the four mainstream methods. 
The strong points and the weak points of the four 
methods have been analyzed in many works (Zong, 
1999; Ren, 1999; Zhao, 2000). 
The non-mainstream approach here refers to 
any other methods exclusive of the four methods 
mentioned above. To improve the performance of 
MT systems, especially to cope with the specific 
problems in S2S translation, many schemes have 
been proposed. Ren (1999) proposed a super-
function based MT method, which tries to address 
the MT users? requests and translates the input 
without thorough syntactic and semantic analysis. 
The super-function based MT system is fast, 
inexpensive, easy to control and easy to update. 
However, the fluency and the correctness of the 
translation results are usual not high. Moreover, to 
extract the practical super-functions from the 
corpus is also a hard work. Yamamoto et al (2001) 
proposed a paradigm named Sandglass. In the 
sandglass system, the input utterances from a 
speech recognizer are paraphrased firstly, and the 
paraphrased text is passed to the transfer controller. 
The task of the paraphrasing module for the source 
language is to deal with noisy inputs from the 
speech recognizer and provides different 
expressions of the input. An obvious question 
about the Sandglass is why the system would 
rather rewrite the input than to translate it directly?  
Zong et al (2000b) proposed an MT method based 
on the simple expression. In the method the 
keywords in an input utterances are spotted out 
firstly and the dependence relation among the 
keywords are analyzed. Then, the translation 
module searches the examples in the knowledge 
base according to the keywords and their 
dependence relation. If an example is matched with 
the conditions, the target language expression of 
the example is sent out as the translation result of 
the input. When the input is not very long, and the 
domain and the type of the input are restricted, the 
method is very practical. However, to develop the 
knowledge base with dependence relation of 
keywords and to match an input with all examples 
in the knowledge base are sometimes difficult. 
Wakita et al (1997) proposed a robust translation 
method which locally extracts only reliable parts, 
i.e., those within the semantic distance threshold 
and over some word length. This technique, 
however, does not split input into units globally, or 
sometimes does not output any translation result 
(Furuse et al 1998). In addition, the method 
closely lies on the semantic computation, and 
sometimes it is hard to compute the semantic 
distance for the spoken utterances. 
In summary, both mainstream MT methods and 
non-mainstream methods have been practiced in 
many experimental S2S translation systems. 
However, all methods mentioned above are 
unilateral and based on user's own wishful thinking. 
The system is passive and blind in some extent. 
The task that machine translates is imposed by 
human, and some problems are also brought by the 
speaker, e.g., the topics are changed casually, or 
the ill-formed expressions are uttered. In these 
cases, it is unreasonable to expect the system to get 
the correct translation results, but not to give the 
system any rights to ask the speaker about his or 
her intention or some ambiguous words. In fact, if 
we examine the procedures that human interpreters 
use, we can see that the translation is usually 
interactive. When an interpreter is unable to 
directly translate an utterance due to an ill-formed 
expression or something even worse, the 
interpreter may have to ask the speaker to repeat or 
explain his / her words. Based on the ideas, the 
interactive paradigms for S2S translation have 
been proposed (Blanchon, 1996; Waibel, 1996; 
Seligman, 1997; Seligman, 2000; Ren, 2000). 
Seligman (2000) proposed a ? quick and dirty? or 
?low road? scheme, in which he suggested that, by 
stressing interactive disambiguation, practically 
usable speech translation systems may be 
constructable in the near term. In addition, two 
interactive MT demos were shown respectively in 
1997 and 1998 (Seligman, 2000). However, all the 
proposed interactive schemes and the demos put 
the emphasis on the interface between speech 
recognition (SR) and analysis. The interface can be 
supplied entirely by the user, who can correct SR 
results before passing them to translation 
components. That means the translation system is 
still passive. Actually, as we know that the parsing 
results and the translation results are not certainly 
correct even though the input is completely correct, 
but some noisy words usually have not any 
influence whether they are correct or not. In this 
sense, the user should know what the system needs? 
And what brought the system ambiguity? This 
means, the system has rights and obligations to tell 
the user what the system want to know. In another 
words, the system necessitates a DMM to guide the 
interaction between the system and user, and 
sometimes the system should play the leading role.  
2.2 Analysis on user?s requirements 
Although much progress in SR and spoken 
language parsing has been made, there is still a 
long way to reach the final and ideal goal that the 
translation results are complete correct. In this 
situation, let?s think does a user always need the 
complete correct translation results? Please see the 
following three examples: 
(1) Input: ?????? ????????
?????????????? (Oh, 
that ? well, please reserve a single room 
for me, sure, a single room.) 
In the input, there are many redundant words, 
such as, ?(Oh)???(that), ???(well) and so 
on. If all words in the input are translated, the 
translation result is verbose and wordy. In fact, in 
the input only three keywords are useful, which are: 
??(reserve), ??(one), and ???(single room) 
as well. The preposition phrase ???(for me)? is 
not  obligatory. Even the word ???? is also not 
obligatory.  
(2) Input: ? ? ? ? ? ?? ? ?(Is this ? 
Xiang Ge Li La? Hotel?) 
In the example, the four characters with 
underline are originally a hotel name ????
??(Shangri-la), but they are wrong transliterated 
and separated due to the absence of the word in the 
SR dictionary. In this case, it is impossible to 
correctly parse the input without user?s help.  
(3) Input: ??? ? ?? ? ?? ?? ???(Is 
there any ? ask ... have? route to 
Huangshan mountain?) 
The input is a result of the SR component. 
Obviously, in the input two characters with 
stressing dots are wrong recognized from the 
original word ???  (tour)?. In this case, if all 
words are translated, the results will be 
inconceivable. On the contrary, the result is quite 
understandable if the two characters with stressing 
dots are omitted or ignored.  
The example (1) shows that if the input is 
recognized completely correct, the parsing result is 
still probably wrong due to the ill-formed 
expression of the input. The example (2) means 
that it is impossible to correctly parse the input due 
to the unknown word and its incorrect recognition. 
The example (3) shows that even though the 
expression is formal and there is not any unknown 
word in the input, the result of SR is still probably 
wrong. The parser is impossible to correctly 
analyze the wrong SR result.  
From the three examples we can easily get the 
following standpoints: a) the user expects his or 
her intentions to be translated rather than his (her) 
all words. The keywords and their dependence 
relations are the main objects to hold the user?s 
intentions. b) For the translation component, it is 
not indispensable to correct all mistakes in the 
input from the SR component. c) If the parser is 
failed to parse the input, and the system only 
translates the keywords, the translation results may 
be still understandable and acceptable. 
3 Interactive translation based on 
dialogue management 
3.1 Overview of the paradigm 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Utterances
SR N-best Speaker 
Pre-processor M
achine learning
Interactive
interface BP identifier
Uttr. segment.DMM 
n partsInputF
Template-based 
translator FParser &Evaluation 
ResultsS Inter-lingua S
Language 
generator 
TTS 
Target speech Slot-based translator
 
 
Figure 1. The paradigm of interactive translation 
Based on the analysis on MT approaches and the 
user?s requirements, we propose an interactive 
paradigm for the S2S translation, which is based 
on the template-based translation, inter-lingual 
translation and the DMM based translation as well. 
The paradigm is shown as Figure 1. 
Where, the letter S beside the line with arrow 
means that the results of the former module are 
successful, and the letter F means the results are 
failure. 
According to the paradigm, an input from the SR 
component is probably processed and translated by 
the following four steps. First, the input is pre-
processed. Some noisy words are recognized, some 
repeated words are deleted, and the numbers are 
processed (Zong, 2000a). Then the base phrases 
(BP) in the input are identified, which include 
noun phrase (NP) and verb phrase (VP) mainly. 
And also, if the input is a long utterance containing 
several simple sentences or some fixed expressions, 
the input is possibly segmented into n parts. n is an 
integer, and n ? 1. Second, each part of the input is 
passed to the template-based translator. If the input 
part is matched with a translation template, the 
translation result is sent to the text-to-speech (TTS) 
synthesizer directly. Otherwise, the input part will 
be passed to the inter-lingual translator. Third, in 
the inter-lingual translator, the input is parsed and 
the parsing results are evaluated. If the evaluation 
score is bigger than the given threshold value, the 
parsing results will be mapped into the inter-lingua, 
and the translation result will be generated by the 
inter-lingua based target language generator. 
Otherwise, the system performs the fourth step. 
Fourth, DMM works to supervise the interaction 
for disambiguation of the input. In the interaction, 
the user is asked to answer some questions 
regarding to the input part. The system will fill the 
slots according to the question-answers. The slots 
are designed to express the user?s intentions in the 
input. The system directly generates the translation 
result according to the slots. So, the translation in 
the fourth step is named as slot-based translation. 
Where, the template-based translator employs 
the forward maximum match algorithm (Zong, 
2000c). The inter-lingua uses the interchangeable 
format (IF) developed by C-STAR (Consortium for 
Speech Translation Advanced Research). The 
parser oriented to IF is realized on the basis of 
HMM spoken language understanding model. In 
the experimental system we use the tri-gram to 
compute the probability of the sequence of 
semantic units (Xie, 2002). The IF-based language 
generator employs a task-oriented micro-planner 
and a general surface realizer. The target language 
is generated by the combination of template 
method and generation technology (Wu, 2000). 
The generic DMM has been proposed by (Xu, 
2001), which combines both interaction patterns 
and task structure. The machine learning module is 
taking charge of recording the dialogue patterns, 
topics and modifying the dialogue history, and so 
on. This module is still under construction. 
3.2 Utterance segmentation 
In an S2S translation system, how to split the long 
input utterances is one of the key problems, 
because an input is often uttered by the 
spontaneous speech, and there is not any special 
mark to indicate which word is the beginning or 
the end of each simple sentence inside the 
utterance. In our system an input Chinese utterance 
is first split by the SR component according to the 
acoustic features, including the prosodic cues and 
pause etc. Suppose an input utterance has been 
transcribed by SR and separated into k parts P1, 
P2, ? Pk (k is an integer, and k ? 1.). Each part Pi 
(i?[1 .. k]) is possibly further segmented into m 
(m is an integer and m?1) units U1, U2, ?, Um by 
the segmentation module based on the linguistic 
analysis (SBLA). Where, all Pi (i?[1 .. k]) and Uj 
(j?[1 .. m]) are called as the split units in our 
system. A split unit is one of the following 
expressions: 
z A single word. 
z A fixed expression, such as a greeting 
phrase in Chinese. 
z A simple sentence. 
z A clause indicated by some special 
conjunctions. For example, an input similar 
with the pattern ???(because) ? , ??
(therefore) ? ? will be separated into two 
parts ? ? ? (because)?? and ? ? ? 
(therefore) ? ?. 
Each Pi (i?[1 .. n]) is analyzed and segmented 
by SBLA through the following three steps: 
splitting on the shallow level, splitting on the 
middle level, and splitting on the deep level. This 
means if a string S is separated into n parts by 
using the method on the shallow level, each part 
will possibly be further segmented by the method 
on the middle level, and so on.  
3.3 Slot-based translation with DMM 
The slot-based translation with DMM is built on 
the following viewpoints and hypothesis: 1) there 
are some noisy words or ambiguous words in the 
results from SR component, but the keywords are 
recognized correctly; 2) the user?s intentions lie on 
the keywords and their dependence relations; and 3) 
the translation results based on the keywords are 
understandable and reflect the main intentions of 
the user. The slot-based translation under the 
guidance of DMM is performed as the following 
steps:  
i) Re-analyze the original input string, spot out 
the keywords, and also do the analysis on the 
dependence relation of the keywords.  
ii) Interact with the user, make decision about 
the keywords and their dependence relation, 
and fill the slots for the translation.  
iii) Generate the translation results according to 
the slots.  
iv) DMM writes down the keywords and their 
dependence relations and modifies the 
dialogue history. 
3.3.1 Keywords spotting and dependence 
analysis 
According to the evaluation score, if the parsing 
result of an input part is too worse, the parsing is 
treated as failure, and all analysis results, including 
base phrases, are ignored. The system will spot out 
the keywords from the original input and analyze 
the dependence relation among the keywords. 
Please note that the dependence relation of the 
keywords in this component is used for seizing the 
user?s intentions and generating the translation 
results. It is different with the function in the 
simple expression based translation (Zong, 2000b). 
In a specific domain, it is easy to define some 
keywords according to the statistical results of the 
collected corpus. In our system, a word is treated 
as the keyword if the following two conditions are 
met: 
? The part-of-speech (POS) of the word is 
one of the following three POSs: noun (N), 
verb (V), and adjective (A), and the word 
occurs with high probability in the specific 
domain. 
? The word is a number or a time word. 
In our method, the verb keyword is always treated 
as the center when the dependence relations are 
analyzed. The dependence relations between the 
verb keyword and the noun keywords are defined 
as four types: (1) agent, (2) direct object, (3) 
indirect object, and (4) the pivot word as well. The 
agent is usually located at the left of the verb 
keyword. In general, the direct object, indirect 
object, and the pivot word all occur at the right of 
the verb keyword. The pronoun is treated as the 
noun. Other content words are treated as the 
modification words of the keywords. The search 
direction and the position relation may be shown as 
the following Figure 2. Where, Wi means a 
common word, and KWi means a keyword.. 
 
W1  ?  KW1(verb) ?  Wi  ?  KW2(noun)?  
 modifications 
 agent object / pivot word 
 
Figure 2. Keywor lations 
According to the chara
verbs, there are five case
? There is no object af
? There is one object o
? There are two objec
and another one is t
? The object is a claus
? After the verb keyw
object (pivot word
agent of another fol
In the keyword dictionar
its all possible POSs a
DMM asks the user q
features of a specific ver
3.3.2 Interaction and s
In the DMM module, 
express the user?s inten
series of slots as follows
 ds and their recteristics of the Chinese 
s respectively: 
ter the verb; 
nly; 
ts. One is the direct object 
he indirect object. 
e. 
ord, the first noun is the 
) and acts as the role of 
lowed verb. 
y, each verb is tagged with 
nd relative features. The 
uestions according to the 
b, its context, and the slots. 
lot filling 
a frame is designed to 
tions, which consists of a 
.  
Frame: ACTION: Keywords (verb)  
 TENSE: {Present/Past/?} 
EXP. TYPE: {Interrogative/?} 
AGENT: noun; 
OBJECT1: noun; 
OBJECT2: noun; 
QUANTITY1: numeral; 
UNIT1: classifier; 
QUANTITY2: numeral; 
UNIT2: classifier; 
TIME: numeral & classifier; 
 HOW: adjective; 
 
 Figure 3. Frame of slots 
 
Where, QUANTITY1 and UNIT1 modify the 
agent, QUANTITY2 and UNIT2 modify the 
OBJECT1 or OBJECT2. Because the keywords 
have been spotted out and their dependence 
relations have been analyzed, the DMM asks the 
user according to the analysis results and the 
concrete context. Please see the following example. 
Input: ? ? ? ? ? ?? ? ?(Is this ? 
Xiang Ge Li La ? Hotel?) 
Two keywords, ??(be)? and ???(hotel)?, are 
spotted, and the word ???? is recognized as the 
object of the verb ?? ?. i.e.,  ACTION=? ; 
TENSE=Present; EXP. TYPE= Interrogative; 
OBJECT1=?? ; and other slots are empty. 
However, there are four noisy characters between 
the two keywords. The DMM will ask the user by 
using the question pattern: ???(what) X ??. The 
variable X is just replaced with the keyword ????. 
The user needs to answer the hotel name. Because 
the SR module still does not recognize the speech 
of the word ?????(Shangri-la)?, the DMM is 
unable to parse the user?s answer. The following 
dialogue will be done: 
System: ?????????????????
(Is the word ?????? an adjective or a noun?) 
User:   (?)??(It is a noun.) 
System: ??????? ? (Is it the hotel 
name?) 
User:  ??(yes). 
System: ??????????? (Please 
input the English name of the word ??????). 
The DMM will append the word ?????? 
both into the SR dictionary and translation 
dictionary and treat it as the attribute of the 
keyword ????. The input is finally translated by 
using the template ?Is this the X ??.  
3.3.3 Generation based on slots 
After the interaction, the translation result will be 
generated based on the templates that are consisted 
of the slots. For example, if AGENT and ACTION 
are filled, the EXP. TYPE = Statement, and other 
slots are empty. The generation template 
is: !AGENT !ACTION. Where, !AGENT means 
the English word corresponding to the Chinese 
word in the AGENT slot. !ACTION is the English 
word corresponding to the Chinese word in the 
ACTION slot. However, the morphology of the 
verb will be changed according to the agent. 
From the frame of slots we can see that the 
frame can only express the analysis results of 
simple sentence. So, the translation result is always 
expressed by the simple sentence. If the subject or 
the object of a Chinese input is a clause, the input 
will be translated into two or more simple English 
sentences. For instance,  
Input: ??????????????
(How much does it cost if I reserve two single 
rooms?) 
The input will be mapped into two frames. In 
the first frame, AGENT=?; ACTION=??; EXP. 
TYPE=Statement; QUANTITY2=?; UNIT2=?; 
OBJECT1= ? ? ? . In the second frame, 
ACTION= ? ? ; EXP. TYPE= Interrogative; 
QUANTITY1=?? ; OBJECT1=? . Therefore, 
the input is separately translated into two simple 
English sentences: ?I reserve two single rooms.?, 
and ?How much does it cost??. Obviously, in the 
specific context, the results are completely 
understandable and acceptable. 
4 Conclusion 
This paper describes a new paradigm for S2S 
translation system, which is based on DMM. 
According to the description we can see that the 
paradigm is of the following features: 
(1) The S2S translation is realized in the 
combination of direct translation 
engines and the interaction led by DMM. 
The interaction is not always brought 
into the role, and it only works when the 
former translation engines work failed.  
(2) The interaction is impersonative, target- 
oriented, and led by the system, not 
blind. The user does not need to correct 
all of the errors in the results of SR. He 
or she only needs to concern what the 
system asks. 
(3) The system can always give the results 
for an input speech despite of the ill-
formed expressions and the worse 
recognition results. 
Although the whole experimental system is under 
construction, some preliminary results have been 
gained. Zong (2000c) reported the performance of 
the template-based translator; Xie (2002) reported 
the results of the robust parser for the Chinese 
spoken language; Xu (2001) presented the results 
of dialogue model; and so on. The results have 
made us confident to develop the practical S2S 
translation system based on the dialogue 
management. However, we are facing much hard 
work that involve the following aspects at least: 
? Develop the reasonable strategies and 
standards to evaluate the parsing results; 
? Design the effective templates to ask the 
user questions according the keywords and 
the concrete context; 
? Define the practical templates to generate 
the translation results; 
? Build the machine learning mechanism to 
enrich the knowledge base of the system. 
References 
Blanchon, H. 1996. A Customizable Interactive 
Disambiguation Methodology and Two 
Implementations to Disambiguate French and 
English Input. In Proceedings of MIDDIM-96 
(International Seminar on Multimodal Interactive 
Disambiguation), Col de Porte, Fance. 
Furuse, O., Satsuo Yamada and Kazuhide Yamamoto. 
1998. Splitting Long or Ill-formed Input for Robust 
Spoken-language Translation. In Proceeding of 
COLING-ACL, Canada. Vol. I, pp. 421-427. 
Lavie, A., Lori Levin et al 1999. The JANUS-III 
Translation System: Speech-to- Speech Translation 
in Multiple Domains. In Proceedings of C-STAR II 
Workshop, Schwetzingen of Germany, 24 Sept., 
1999. 
Ren, F., Shigang Li. 2000. Dialogue Machine 
Translation Based upon Parallel Translation Engines 
and Face Image Processing. In Journal of 
INFORMATION?Vol.3, No.4, pp.521-531. 
Ren, F. 1999. Super-function Based Machine 
Translation, in Communications of COLIPS, 9(1): 
83-100. 
Seligman, M. 1997. Interactive Real-time Translation 
via the Internet. In Working Notes, Natural 
Language Processing for the World Wide Web. 
AAAI-97 Spring Symposium, Stanford University. 
March 24-26, 1997. 
Seligman, M. 2000. Nine Issues in Speech Translation. 
In Machine Translation. 15: 149-185. 
Waibel, A. 1996. Interactive Translation of 
Conversational Speech. In Proceedings of ATR 
International Workshop on Speech Translation. pp. 
1~17. 
Wahlster, W. 2000. Mobile Speech-to-Speech 
Translation of Spontaneous Dialogs: An Overview 
of the Final Verbmobil System. In Verbmobil: 
Foundations of Speech-to-Speech Translation. 
Springer Press. pp. 3-21. 
Wakita, Y., Jun Kawai, Hitoshi Iida. 1997. Correct Parts 
Extraction from Speech Recognition Results Using 
Semantic Distance Calculation, and Its Application 
to Speech Translation. In Proceedings of a 
Workshop Sponsored by the ACL and by the 
European Network in Language and Speech 
(ELSNET). pp. 24-29. 
Wu, H., Taiyi Huang, Chengqing Zong, and Bo Xu. 
2000. Chinese Generation in a Spoken Dialogue 
Translation System. In Proceedings of COLING. pp. 
1141-1145. 
Xie, G., Chengqing Zong, and Bo, Xu. 2002. Chinese 
Spoken Language Analyzing Based on Combination 
of Statistical and Rule Methods. Submitted to the 
International Conference on Spoken Language 
Processing (ICSLP-2002). 
Xu, W., Taiyi Huang, and Bo Xu. Towards a Generic 
Dialogue Model for Information-seeking Dialogues. 
In Proceedings of the National Conference on Man-
Machine Speech Communications (NCMMSC6). 
Shenzhen,  China. pp. 125-130. 
Yamamoto, K., Satoshi Shirai, Masashi Sakamoto, and 
Yujie Zhang. 2001. Sandglass: Twin Paraphrasing 
Spoken Language Translation. In Proceedings of the 
19th International Conference on Computer 
Processing of Oriental Languages (ICCPOL- 2001). 
pp. 154-159. 
Zhao, T. et al 2000. The Principle of Machine 
Translation (in Chinese). Press of Harbin Institute of 
Technology.  
Zong, C., Taiyi Huang and Bo XU. 1999. Technical 
Analysis on Automatic Spoken Language 
Translation Systems (in Chinese). In Journal of 
Chinese Information Processing, 13(2):55-65. 
Zong, C., Taiyi Huang and Bo Xu. 2000a. Design and 
Implementation of a Chinese-to-English Spoken 
Language Translation System. In Proceedings of the 
International Symposium of Chinese Spoken 
Language Processing (ISCSLP-2000), Beijing, 
China. pp. 367-370. 
Zong, C., Yumi Wakita, Bo Xu, Kenji Matsui and 
Zhenbiao Chen. 2000b. Japanese-to-Chinese Spoken 
Language Translation Based on the Simple 
Expression. In Proceedings of International 
Conference on Spoken Language Processing 
(ICSLP-2000). Beijing, China. pp. 418-421. 
Zong, C., Taiyi Huang and Bo Xu. 2000c. An Improved 
Template-based Approach to Spoken Language 
Translation. In Proceedings of International 
Conference on Spoken Language Processing 
(ICSLP-2000). Beijing, China. pp. 440-443. 
 
Utterance Segmentation Using Combined Approach  
Based on Bi-directional N-gram and Maximum Entropy 
 
Ding Liu 
National Laboratory of Pattern Recognition
Institute of Automation 
Chinese Academy of Sciences 
Beijing 100080, China. 
dliu@nlpr.ia.ac.cn 
Chengqing Zong 
National Laboratory of Pattern Recognition
Institute of Automation 
Chinese Academy of Sciences 
Beijing 100080, China. 
cqzong@nlpr.ia.ac.cn  
 
 
 
Abstract 
This paper proposes a new approach to 
segmentation of utterances into sentences 
using a new linguistic model based upon 
Maximum-entropy-weighted Bi-
directional N-grams. The usual N-gram 
algorithm searches for sentence bounda-
ries in a text from left to right only. Thus 
a candidate sentence boundary in the text 
is evaluated mainly with respect to its left 
context, without fully considering its right 
context. Using this approach, utterances 
are often divided into incomplete sen-
tences or fragments. In order to make use 
of both the right and left contexts of can-
didate sentence boundaries, we propose a 
new linguistic modeling approach based 
on Maximum-entropy-weighted Bi-
directional N-grams. Experimental results 
indicate that the new approach signifi-
cantly outperforms the usual N-gram al-
gorithm for segmenting both Chinese and 
English utterances. 
1 Introduction 
Due to the improvement of speech recognition 
technology, spoken language user interfaces, spo-
ken dialogue systems, and speech translation sys-
tems are no longer only laboratory dreams. 
Roughly speaking, such systems have the structure 
shown in Figure 1. 
 
 
Figure 1. System with speech input. 
 
In these systems, the language analysis module 
takes the output of speech recognition as its input, 
representing the current utterance exactly as pro-
nounced, without any punctuation symbols mark-
ing the boundaries of sentences. Here is an 
example: ???????? 9 ???????
???????? 913??? . (this way please 
please take this elevator to the ninth floor the floor 
attendant will meet you at your elevator entrance 
there and show you to room 913.) As the example 
shows, it will be difficult for a text analysis module 
to parse the input if the utterance is not segmented. 
Further, the output utterance from the speech rec-
ognizer usually contains wrongly recognized 
words or noise words. Thus it is crucial to segment 
the utterance before further language processing. 
We believe that accurate segmentation can greatly 
improve the performance of language analysis 
modules. 
Stevenson et al have demonstrated the difficul-
ties of text segmentation through an experiment in 
which six people, educated to at least the Bache-
lor?s degree level, were required to segment into 
sentences broadcast transcripts from which all 
punctuation symbols had been removed. The ex-
perimental results show that humans do not always 
agree on the insertion of punctuation symbols, and 
that their segmentation performance is not very 
good (Stevenson and Gaizauskas, 2000). Thus it is 
a great challenge for computers to perform the task 
Output (text 
or speech) Language 
analysis and 
generation 
Speech  
 
recognition
Input speech
automatically. To solve this problem, many meth-
ods have been proposed, which can be roughly 
classified into two categories. One approach is 
based on simple acoustic criteria, such as non-
speech intervals (e.g. pauses), pitch and energy. 
We can call this approach acoustic segmentation. 
The other approach, which can be called linguistic 
segmentation, is based on linguistic clues, includ-
ing lexical knowledge, syntactic structure, seman-
tic information etc. Acoustic segmentation can not 
always work well, because utterance boundaries do 
not always correspond to acoustic criteria. For ex-
ample: ??<pause>??<pause>??????
???<pause>??<pause>?????. Since 
the simple acoustic criteria are inadequate, linguis-
tic clues play an indispensable role in utterance 
segmentation, and many methods relying on them 
have been proposed. 
This paper proposes a new approach to linguis-
tic segmentation using a Maximum-entropy-
weighted Bi-directional N-gram-based algorithm 
(MEBN). To evaluate the performance of MEBN, 
we conducted experiments in both Chinese and 
English. All the results show that MEBN outper-
forms the normal N-gram algorithm. The remain-
der of this paper will focus on description of our 
new approach for linguistic segmentation. In Sec-
tion 2, some related work on utterance segmenta-
tion is briefly reviewed, and our motivations are 
described. Section 3 describes MEBN in detail. 
The experimental results are presented in Section 4. 
Finally, Section 5 gives our conclusion. 
2 Related Work and Our Motivations  
2.1 Related Work 
Stolcke et al (1998, 1996) proposed an approach 
to detection of sentence boundaries and disfluency 
locations in speech transcribed by an automatic 
recognizer, based on a combination of prosodic 
cues modeled by decision trees and N-gram lan-
guage models. Their N-gram language model is 
mainly based on part of speech, and retains some 
words which are particularly relevant to segmenta-
tion. Of course, most part-of-speech taggers re-
quire sentence boundaries to be pre-determined; so 
to require the use of part-of-speech information in 
utterance segmentation would risk circularity. Cet-
tolo et al?s (1998) approach to sentence boundary 
detection is somewhat similar to Stolcke et al?s. 
They applied word-based N-gram language models 
to utterance segmentation, and then combined 
them with prosodic models. Compared with N-
gram language models, their combined models 
achieved an improvement of 0.5% and 2.3% in 
precision and recall respectively. 
Beeferman et al (1998) used the CYBERPUNC 
system to add intra-sentence punctuation (espe-
cially commas) to the output of an automatic 
speech recognition (ASR) system. They claim that, 
since commas are the most frequently used punc-
tuation symbols, their correct insertion is by far the 
most helpful addition for making texts legible. 
CYBERPUNC augmented a standard trigram 
speech recognition model with lexical information 
concerning commas, and achieved a precision of 
75.6% and a recall of 65.6% when testing on 2,317 
sentences from the Wall Street Journal. 
Gotoh et al (1998) applied a simple non-speech 
interval model to detect sentence boundaries in 
English broadcast speech transcripts. They com-
pared their results with those of N-gram language 
models and found theirs far superior. However, 
broadcast speech transcripts are not really spoken 
language, but something more like spoken written 
language. Further, radio broadcasters speak for-
mally, so that their reading pauses match sentence 
boundaries quite well. It is thus understandable that 
the simple non-speech interval model outperforms 
the N-gram language model under these conditions; 
but segmentation of natural utterances is quite dif-
ferent. 
Zong et al (2003) proposed an approach to ut-
terance segmentation aiming at improving the per-
formance of spoken language translation (SLT) 
systems. Their method is based on rules which are 
oriented toward key word detection, template 
matching, and syntactic analysis. Since this ap-
proach is intended to facilitate translation of Chi-
nese-to-English SLT systems, it rewrites long 
sentences as several simple units. Once again, 
these results cannot be regarded as general-purpose 
utterance segmentation. Furuse et al (1998) simi-
larly propose an input-splitting method for translat-
ing spoken language which includes many long or 
ill-formed expressions. The method splits an input 
into well-balanced translation units, using a seman-
tic dictionary. 
Ramaswamy et al (1998) applied a maximum 
entropy approach to the detection of command 
boundaries in a conversational natural language 
user interface. They considered as their features 
words and their distances to potential boundaries. 
They posited 400 feature functions, and trained 
their weights using 3000 commands. The system 
then achieved a precision of 98.2% in a test set of 
1900 commands. However, command sentences 
for conversational natural language user interfaces 
contain much smaller vocabularies and simpler 
structures than the sentences of natural spoken lan-
guage. In any case, this method has been very 
helpful to us in designing our own approach to ut-
terance segmentation. 
There are several additional approaches which are 
not designed for utterance segmentation but which 
can nevertheless provide useful ideas. For example, 
Reynar et al (1997) proposed an approach to the 
disambiguation of punctuation marks. They con-
sidered only the first word to the left and right of 
any potential sentence boundary, and claimed that 
examining wider context was not beneficial. The 
features they considered included the candidate?s 
prefix and suffix; the presence of particular charac-
ters in the prefix or suffix; whether the candidate 
was honorific (e.g. Mr., Dr.); and whether the can-
didate was a corporate designator (e.g. Corp.). The 
system was tested on the Brown Corpus, and 
achieved a precision of 98.8%. Elsewhere, Nakano 
et al (1999) proposed a method for incrementally 
understanding user utterances whose semantic 
boundaries were unknown. The method operated 
by incrementally finding plausible sequences of 
utterances that play crucial roles in the task execu-
tion of dialogues, and by utilizing beam search to 
deal with the ambiguity of boundaries and with 
syntactic and semantic ambiguities. Though the 
method does not require utterance segmentation 
before discourse processing, it employs special 
rule tables for discontinuation of significant utter-
ance boundaries. Such rule tables are not easy to 
maintain, and experimental results have demon-
strated only that the method outperformed the 
method assuming pauses to be semantic boundaries.  
2.2 Our motivations 
Though numerous methods for utterance segmen-
tation have been proposed, many problems remain 
unsolved.  
One remaining problem relates to the language 
model. The N-gram model evaluates candidate 
sentence boundaries mainly according to their left 
context, and has achieved reasonably good results, 
but it can?t take into account the distant right con-
text to the candidate. This is the reason that N-
gram methods often wrongly divide some long 
sentences into halves or multiple segments. For 
example:????????. The N-gram method 
is likely to insert a boundary mark between ??? 
and ???, which corresponds to our everyday im-
pression that, if reading from the left and not 
considering several more words to the right of the 
current word, we will probably consider ????
?? as a whole sentence. However, we find that, if 
we search the sentence boundaries from right to 
left, such errors can be effectively avoided. In the 
present example, we won?t consider ?????? 
as a whole sentence, and the search will be contin-
ued until the word ??? is encountered. Accord-
ingly, in order to avoid segmentation errors made 
by the normal N-gram method, we propose a re-
verse N-gram segmentation method (RN) which 
does seek sentence boundaries from right to left. 
Further, we simply integrate the two N-gram 
methods and propose a bi-directional N-gram 
method (BN), which takes into account both the 
left and the right context of a candidate segmenta-
tion site. Since the relative usefulness or signifi-
cance of the two N-gram methods varies 
depending on the context, we propose a method of 
weighting them appropriately, using parameters 
generated by a maximum entropy method which 
takes as its features information about words in the 
context. This is our Maximum-Entropy-Weighted 
Bi-directional N-gram-based segmentation method. 
We hope MEBN can retain the correct segments 
discovered by the usual N-gram algorithm, yet ef-
fectively skip the wrong segments. 
3 Maximum-Entropy-Weighted Bi-
directional N-gram-based Segmentation 
Method 
3.1 Normal N-gram Algorithm (NN) for Ut-
terance Segmentation 
Assuming that mWWW ...21 (where m is a natural 
number) is a word sequence, we consider it as an n 
order Markov chain, in which the word 
)1( miWi ??  is predicted by the n-1 words to its 
left. Here is the corresponding formula: 
)...|()...|( 11121 ?+?? = iniiii WWWPWWWWP  
From this conditional probability formula for a 
word, we can derive the probability of a word se-
quence iWWW ...21 :  
)...|()...()...( 12112121 ?? ?= iiii WWWWPWWWPWWWP  
Integrating the two formulas above, we get: 
)...|()...()...( 1112121 ?+?? ?= iniiii WWWPWWWPWWWP  
Let us use SB to indicate a sentence boundary 
and add it to the word sequence. The value of 
)...( 121 +ii SBWWWWP  and )...( 121 +iiWWWWP will 
determine whether a specific word 
)1( miWi ?? is the final word of a sentence. We 
say iW  is the final word of a sentence if and only 
if )...( 121 +ii SBWWWWP > )...( 121 +iiWWWWP .  
Taking the trigram as our example and consid-
ering the two cases where Wi-1 is and is not the 
final word of a sentence, )...( 121 +ii SBWWWWP  
and )...( 121 +iiWWWWP  is computed respectively 
by the following two formulas: 
)|()...(
)|()...()...(
)|()|()...(
)|()|()...()...(
11121
121121
11121
121121
iiiii
iiiii
iiiiii
iiiiii
WWWPWWWWP
SBWWPSBWWWPWWWWP
SBWWPWWSBPWWWWP
SBWWPSBWSBPSBWWWPSBWWWWP
?+?
++
+??
++
?+
?=
??+
??=
 
In the normal N-gram method, the above iterative 
formulas are computed to search the sentence 
boundaries from 1W  to mW . 
3.2 Reverse N-gram Algorithm (RN) for Ut-
terance Segmentation 
In the reverse N-gram segmentation method, we 
take the word sequence mWWW ...21  as a reverse 
Markov chain in which )1( miWi ??  is predicted 
by the n-1 words to its right. That is: 
 )...|()...|( 1111 +?++? = iniiimmi WWWPWWWWP  
As in the N-gram algorithm, we compute the 
occurring probability of word sequence 
mWWW ...21  using the formula: 
)...|()...()...( 11111 +?+?? ?= immiimmimm WWWWPWWWPWWWP  
Then the iterative computation formula is: 
)...|()...()...( 11111 +?++?? ?= iniiimmimm WWWPWWWPWWWP  
By adding SB to the word sequence, we say iW  
is the final word of a sentence if and only if 
)...( 11 iimm SBWWWWP +? > )...( 11 iimm WWWWP +? . 
Similar to NN, )...( 11 iimm SBWWWWP +?  and 
)...( 11 iimm WWWWP +?  are computed as follows in 
the trigram: 
)|()...(
)|()...()...(
)|()|()...(
)|()|()...()...(
12121
11111
112121
111111
++++?
++?+?
+++++?
+++?+?
?+
?=
??+
??=
iiiiimm
iiimmiimm
iiiiiimm
iiiimmiimm
WWWPWWWWP
SBWWPSBWWWPWWWWP
SBWWPWWSBPWWWWP
SBWWPSBWSBPSBWWWPSBWWWWP
  
In contrast to the normal N-gram segmentation 
method, we compute the above iterative formulas 
to seek sentence boundaries from mW  to 1W . 
3.3 Bi-directional N-gram Algorithm for Ut-
terance Segmentation 
From the iterative formulas of the normal N-gram 
algorithm and the reverse N-gram algorithm, we 
can see that the normal N-gram method recognizes 
a candidate sentence boundary location mainly 
according to its left context, while the reverse N-
gram method mainly depends on its right context. 
Theoretically at least, it is reasonable to suppose 
that, if we synthetically consider both the left and 
the right context by integrating the NN and the RN, 
the overall segmentation accuracy will be im-
proved. 
Considering the word sequence mWWW ...21 , the 
candidate sites for sentence boundaries may be 
found between 1W  and 2W , between 2W  and 
3W , ?, or between 1?mW and mW . The number of 
candidate sites is thus m-1. We number those m-1 
candidate sites 1, 2 ? m-1 in succession, and we 
use )(iPis )11( ??? mi  and 
)(iPno )11( ??? mi  respectively to indicate the 
probability that the current site i really is, or is not, 
a sentence boundary. Thus, to compute the word 
sequence segmentation, we must compute )(iPis  
and )(iPno  for each of the m-1 candidate sites. In 
the bi-directional BN, we compute )(iPis  and 
)(iPno  by combining the NN results and RN re-
sults. The combination is described by the follow-
ing formulas: 
)()()(
)()()(
___
___
iPiPiP
iPiPiP
RNnoNNnoBNno
RNisNNisBNis
?=
?=
 
where )(_ iP NNis , )(_ iP NNno  denote the probabili-
ties calculated by NN which correspond to 
)...( 121 +ii SBWWWWP  and )...( 121 +iiWWWWP in 
section 3.1 respectively and )(_ iP RNis , )(_ iP RNno  
denote the probabilities calculated by RN which 
correspond to )...( 11 iimm SBWWWWP +?  and 
)...( 11 iimm WWWWP +?  in section 3.2 respectively. 
We say there exits a sentence boundary at site i 
)11( ??? mi if and only if )()( __ iPiP BNnoBNis > . 
3.4 Maximum Entropy Approach for Utter-
ance Segmentation 
In this section, we explain our maximum-entropy-
based model for utterance segmentation. That is, 
we estimate the joint probability distribution of the 
candidate sites and their surrounding words. Since 
we consider information concerning the lexical 
context to be useful, we define the feature func-
tions for our maximum method as follows: 
??
? ===
else
bScefixincludeif
cbf jj 0
)0&&)),((Pr(1
),(10
 
??
? ===
else
bScefixincludeif
cbf jj 0
)1&&)),((Pr(1
),(11
 
??
? ===
else
bScSuffixincludeif
cbf jj 0
)0&&)),(((1
),(20
 
??
? ===
else
bScSuffixincludeif
cbf jj 0
)1&&)),(((1
),(21
 
Sj denotes a sequence of one or more words 
which we can call the Matching String. (Note that 
Sj may contain the sentence boundary mark ?SB?.) 
The candidate c?s state is denoted by b, where b=1 
indicates that c is a sentence boundary and b=0 
indicates that it is not a boundary. Prefix(c) de-
notes all the word sequences ending with c (that is, 
c's left context plus c) and Suffix(c) denotes all the 
word sequences beginning with c (in other words, 
c plus its right context). For example: in the utter-
ance: ?<c1>?<c2>?<c3>?<c4>?<c5>?, 
???, ????,  and ????? are c3?s Prefix, while 
??? , ????and ????? are c3?s Suffix. The 
value of function )),((Pr jScefixinclude  is true 
when word sequence Sj is one of c?s Prefixes, and 
the value of function )),(( jScSuffixinclude  is 
true when Sj is one of c?s Suffixes. 
Corresponding to the four feature functions 
),(10 cbf j , ),(11 cbf j , ),(20 cbf j , ),(21 cbf j  are the 
four parameters 10j? , 11j? , 20j? , 21j? . Thus the 
joint probability distribution of the candidate sites 
and their surrounding contexts is given by: 
)(),(
1
),(
21
),(
20
),(
11
),(
10
21201110? = ???= kj cbfjcbfjcbfjcbfj jjjjbcP ?????
where k is the total number of the Matching Strings 
and ? is a parameter set to make P(c,1) and P(c,0) 
sum to 1. The unknown parameters 
10j? , 11j? , 20j? , 21j?  are chosen to maximize the 
likelihood of the training data using the General-
ized Iterative Scaling (Darroch and Ratcliff, 1972) 
algorithm. In the maximum entropy approach, we 
say that a candidate site is a sentence boundary if 
and only if P(c, 1) > P(c, 0). (At this point, we can 
anticipate a technical problem with the maximum 
approach to utterance segmentation. When a 
Matching String contains SB, we cannot know 
whether it belongs to the Prefixes or Suffixes of 
the candidate site until the left and right contexts of 
the candidate site have been segmented. Thus if the 
segmentation proceeds from left to right, the lexi-
cal information in the right context of the current 
candidate site will always remain uncertain. Like-
wise, if it proceeds from right to left, the informa-
tion in the left context of the current candidate site 
remains uncertain. The next subsection will de-
scribe a pragmatic solution to this problem.) 
3.5 Maximum-Entropy-Weighted Bi-
directional N-gram Algorithm for Utter-
ance Segmentation 
In the bi-directional N-gram based algorithm, we 
have considered the left-to-right N-gram algorithm 
and the right-to-left algorithm as having the same 
significance. Actually, however, they should be 
assigned differing weights, depending on the lexi-
cal contexts. The combination formulas are as fol-
lows: 
)()()()()(
)()()()()(
____
____
iPCWiPCWiP
iPCWiPCWiP
RNnoinorNNnoinonno
RNisiisrNNisiisnis
???=
???=  
)(_ iisn CW , )(_ inon CW , )(_ iisr CW , )(_ inor CW  
are the functions of the context surrounding candi-
date site i which denotes the weights of 
)(_ iP NNis , )(_ iP NNno , )(_ iP RNis  and )(_ iP RNno  re-
spectively. Assuming that the weights of )(_ iP NNis  
and )(_ iP NNno  depend upon the context to the left 
of the candidate site, and that the weights of 
)(_ iP RNis  and )(_ iP RNno  depend on the context to 
the right of the candidate site, the weight functions 
can be rewritten as: 
)(_ iisn LeftCW , )(_ inon LeftCW , )(_ iisr RightCW ,
)(_ inor RightCW . It is reasonable to assume that as 
the joint probability ),( SBiLeftCP i =  rises, 
)(_ iP NNis  will increase in significance. (The joint 
probability in question is the probability of the cur-
rent candidate?s left context, taken together with 
the probability that the candidate is a sentence 
boundary.) Therefore the value of )(_ iisn LeftCW  
is given by ),()(_ SBiLeftCPLeftCW iiisn == . 
Similarly we can give the formulas for comput-
ing )(_ inon LeftCW , )(_ iisr RightCW , and 
)(_ inor RightCW  as follows: 
)!,()(_ SBiLeftCPLeftCW iinon ==  
),()(_ SBiRightCPRightCW iiisr ==  
)!,()(_ SBiRightCPRightCW iinor ==  
We can easily get the values of 
),( SBiLeftCP i = , )!,( SBiLeftCP i = ,
),( SBiRightCP i = , and )!,( SBiRightCP i =  
using the method described in the maximum en-
tropy approach section. For example: 
? === kj ifji jSBiLeftCP 1 ),1(1111),( ??  
? === kj ifji jSBiLeftCP 1 ),0(1010)!,( ??  
As mentioned in last subsection, we need seg-
mented contexts for maximum entropy approach. 
Since the maximum entropy parameters for MEBN 
algorithm are used as modifying NN and RN, we 
just estimate the joint probability of the candidate 
and its surrounding contexts based upon the seg-
ments by NN and RN. Using NLeftCi indicate the 
left context to the candidate i which has been seg-
mented by NN algorithm and RRightCi indicate the 
right context to i which has been segmented by RN, 
the combination probability computing formulas 
for MEBN are as follows: 
)()!,(
)()!,()(
)(),(
)(),()(
_
__
_
__
iPSBiRRightCP
iPSBiNLeftCPiP
iPSBiRRightCP
iPSBiNLeftCPiP
RNnoi
NNnoiMEBNno
RNisi
NNisiMEBNis
?=?
?==
?=?
?==
 
We evaluate site i as a sentence boundary if and 
only if )()( __ iPiP MEBNnoMEBNis > . 
4 Experiment 
4.1 Model Training 
Our models are trained on both Chinese and Eng-
lish corpora, which cover the domains of hotel res-
ervation, flight booking, traffic information, 
sightseeing, daily life and so on. We replaced the 
full stops with ?SB? and removed all other punc-
tuation marks in the training corpora. Since in most 
actual systems part of speech information cannot 
be accessed before determining the sentence 
boundaries, we use Chinese characters and English 
words without POS tags as the units of our N-gram 
models. Trigram and reverse trigram probabilities 
are estimated based on the processed training cor-
pus by using Modified Kneser-Ney Smoothing 
(Chen and Goodman, 1998). As to the maximum 
entropy model, the Matching Strings are chosen as 
all the word sequences occurring in the training 
corpus whose length is no more than 3 words. The 
unknown parameters corresponding to the feature 
functions are generated based on the training cor-
pus using the Generalized Iterative Scaling algo-
rithm. Table 1 gives an overview of the training 
corpus. 
Corpus SIZE SB Num-
ber 
Average Length 
of Sentence 
Chinese 4.02MB 148967 8 Chinese charac-
ters 
English 4.49MB 149311 6 words 
Table 1. Overview of the Training Corpus. 
4.2 Testing Results 
We test our methods using open corpora which are 
also limited to the domains mentioned above. All 
punctuation marks are removed from the test cor-
pora. An overview of the test corpus appears in 
table 2. 
Corpus SIZE SB 
Number 
Average Length 
of Sentence 
Chinese 412KB 12032 10 Chinese char-
acters 
English 391KB 10518 7 words 
Table 2. Overview of the Testing Corpus. 
We have implemented four segmentation algo-
rithms using NN, RN, BN and MEBN respectively. 
If we use ?RightNum? to denote the number of 
right segmentations, ?WrongNum? denote the 
number of wrong segmentations, and ?TotalNum? 
to denote the number of segmentations in the 
original testing corpus, the precision (P) can be 
computed using the formula 
P=RightNum/(RightNum+WrongNum), the recall 
(R) is computed as R=RightNum/TotalNum, and 
the F-Score is computed as F-Score = 
RP
RP
+
??2
. 
The testing results are described in Table 3 and 
Table 4. 
Methods Total Num 
Right 
Num 
Wrong 
Num 
Preci-
sion  Recall F-Score 
NN 12032 10167 2638 79.4% 84.5% 81.9% 
RN 12032 10396 2615 79.9% 86.4% 83.0% 
BN 12032 10528 2249 82.4% 87.5% 84.9% 
MEBN 12032 10348 1587 86.7% 86.0% 86.3% 
Table 3. Experimental Results for Chinese Utter-
ance Segmentation. 
Methods Total Num 
Right 
Num 
Wrong
Num 
Preci-
sion  Recall F-Score 
NN 10518 8730 3164 73.4% 83.0% 77.9% 
RN 10518 9014 3351 72.9% 85.7% 78.8% 
BN 10518 9056 3019 75.0% 86.1% 80.2% 
MEBN 10518 8929 2403 78.8% 84.9% 81.7% 
Table 4. Experimental Results for English Utter-
ance Segmentation. 
From the result tables it is clear that RN, BN, and 
MEBN all outperforms the normal N-gram algo-
rithm in the F-score for both Chinese and English 
utterance segmentation. MEBN achieved the best 
performance which improves the precision by 
7.3% and the recall by 1.5% in the Chinese ex-
periment, and improves the precision by 5.4% and 
the recall by 1.9% in the English experiment. 
4.3 Result analysis 
MEBN was proposed in order to maintain the cor-
rect segments of the normal N-gram algorithm 
while skipping the wrong segments. In order to see 
whether our original intention has been realized, 
we compared the segments as determined by RN 
with those determined by NN, compare the seg-
ments found by BN with those of NN and then 
compare the segments found by MEBN with those 
of NN. For RN, BN and MEBN, suppose TN de-
notes the number of total segmentations, CON de-
notes the number of correct segmentations 
overlapping with those found by NN; SWN de-
notes the number of wrong NN segmentations 
which were skipped; WNON denotes the number 
of wrong segmentations not overlapping with those 
of NN; and CNON denotes the number of segmen-
tations which were correct but did not overlap with 
those of NN. The statistical results are listed in 
Table 5 and Table 6. 
Methods TN CON SWN WNON CNON
RN 13011 9525 1098 1077 870 
BN 12777 9906 753 355 622 
MEBN 11935 9646 1274 223 678 
Table 5. Chinese Utterance Segmentation Results 
Comparison. 
Methods TN CON SWN WNON CNON 
RN 12365 8223 1077 1271 792 
BN 12075 8565 640 488 491 
MEBN 11332 8370 1247 486 559 
Table 6. English Utterance Segmentation Results 
Comparison. 
Focusing upon the Chinese results, we can see that 
RN skips 1098 incorrect segments found by NN, 
and has 9525 correct segments in common with 
those of NN. It verifies our supposition that RN 
can effectively avoid some errors made by NN. 
But because at the same time RN brings in 1077 
new errors, RN doesn?t improve much in precision. 
BN skips 753 incorrect segments and brings in 355 
new segmentation errors; has 9906 correct seg-
ments in common with those of NN and brings in 
622 new correct segments. So by equally integrat-
ing NN and RN, BN on one hand finds more cor-
rect segments, on the other hand brings in less 
wrong segments than NN. But in skipping incor-
rect segments by NN, BN still performs worse than 
RN, showing that it only exerts the error skipping 
ability of RN to some extent. As for MEBN, it 
skips 1274 incorrect segments and at the same time 
brings in only 223 new incorrect segments. Addi-
tionally it maintains 9646 correct segments in com-
mon with those of NN and brings in 678 new 
correct segments. In recall MEBN performs a little 
worse than BN, but in precision it achieves a much 
better performance than BN, showing that modi-
fied by the maximum entropy weights, MEBN 
makes use of the error skipping ability of RN more 
effectively. Further, in skipping wrong segments 
by NN, MEBN even outperforms RN, which indi-
cates the weights we set on NN and RN not only 
act as modifying parameters, but also have direct 
beneficial affection on utterance segmentation. 
5 Conclusion 
This paper proposes a reverse N-gram algorithm, a 
bi-directional N-gram algorithm and a Maximum-
entropy-weighted Bi-directional N-gram algorithm 
for utterance segmentation. The experimental re-
sults for both Chinese and English utterance seg-
mentation show that MEBN significantly 
outperforms the usual N-gram algorithm. This is 
because MEBN takes into account both the left and 
right contexts of candidate sites: it integrates the 
left-to-right N-gram algorithm and the right-to-left 
N-gram algorithm with appropriate weights, using 
clues on the sites? lexical context, as modeled by 
maximum entropy. 
Acknowledgements 
This work is sponsored by the Natural Sciences 
Foundation of China under grant No.60175012, as 
well as supported by the National Key Fundamen-
tal Research Program (the 973 Program) of China 
under the grant G1998030504. 
The authors are very grateful to Dr. Mark Selig-
man for his very useful suggestions and his very 
careful proofreading.  
References 
Beeferman D., A. Berger, and J. Lafferty. 1998. 
CYBERPUNC: A lightweight punctuation annotation 
system for speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal 
Processing, Seattle, WA. pp. 689-692. 
Beeferman D., A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning 
34, pp 177-210.  
Berger A., S. Della Pietra, and V. Della Pietra. 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, 22(1), pp. 39-
71.  
Cettolo M. and D. Falavigna. 1998. Automatic 
Detection of Semantic Boundaries Based on Acoustic 
and Lexical Knowledge. ICSLP 1998, pp. 1551-1554. 
Chen S. F. and J. Goodman. 1998. An empirical study 
of smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Center for Research in Com-
puting Technology, Harvard University. pp.243-255. 
Darroch J. N. and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Linear Models. The Annals of 
Mathematical Statistics, 43(5), pp. 1470-1480. 
Furuse O., S. Yamada, and K. Yamamoto. 1998. Split-
ting Long or Ill-formed Input for Robust Spoken-
language Translation. COLING-ACL 1998, pp. 421-
427. 
Gotoh Y. and S. Renals. 2000. Sentence Boundary De-
tection in Broadcast Speech Transcripts. In Proc. In-
ternational Workshop on Automatic Speech 
Recognition, pp. 228-235. 
Nakano M., N. Miyazaki, J. Hirasawa, K. Dohsaka, and 
T. Kawabata. 1999. Understanding Unsegmented User 
Utterances in Real-Time Spoken Dialogue Systems. 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-99), Col-
lege Park, MD, USA, pp. 200-207. 
Ramaswamy N. G. and J. Kleindienst. 1998. Automatic 
Identification of Command Boundaries in a Conversa-
tional Natural Language User Interface. ICSLP 1998. 
pp. 401-404. 
Reynar J. and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In 
Proceedings of the 5th Conference on Applications of 
Natural Language Processing (ANLP), Washington 
DC, pp. 16-19.  
Seligman M. 2000. Nine Issues in Speech Translation. 
In Machine Translation, 15, pp. 149-185. 
Stevenson M. and R. Gaizauskas. 2000. Experiments on 
sentence boundary detection. In Proceedings of the 
Sixth Conference on Applied Natural Language Proc-
essing and the First Conference of the North American 
Chapter of the Association for Computational Linguis-
tics, pp. 24-30. 
Stolcke A. and E. Shriberg. 1996. Automatic linguistic 
segmentation of conversational speech. Proc. Intl. 
Conf. on Spoken Language Processing, Philadelphia, 
PA, vol. 2, pp. 1005-1008.  
Stolcke A., E. Shriberg, R. Bates, M. Ostendorf, D. 
Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Auto-
matic Detection of Sentence Boundaries and Disfluen-
cies based on Recognized Words. Proc. Intl. Conf. on 
Spoken Language Processing, Sydney, Australia, vol. 
5, pp. 2247-2250.  
Zong, C. and F. Ren. 2003. Chinese Utterance Segmen-
tation in Spoken Language translation. In Proceedings 
of the 4th international conference on intelligent text 
processing and Computational Linguistics (CICLing), 
Mexico, Feb 16-22. pp. 516-525. 
Zhou Y. 2001. Utterance Segmentation Based on Deci-
sion Tree. Proceedings of the 6th National joint Con-
ference on Computational Linguistics, Taiyuan, China, 
pp. 246-252.  
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 87?93,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Approach to Chinese Base Noun Phrase Chunking 
 
Fang Xu Chengqing Zong Jun Zhao 
National Laboratory of Pattern Recognition 
Institute of Automation 
Chinese Academy of Sciences, Beijing 100080,China 
{fxu, cqzong, jzhao}@nlpr.ia.ac.cn 
 
 
 
 
Abstract 
In this paper, we propose a hybrid ap-
proach to chunking Chinese base noun 
phrases (base NPs), which combines 
SVM (Support Vector Machine) model 
and CRF (Conditional Random Field) 
model. In order to compare the result 
respectively from two chunkers, we use 
the discriminative post-processing 
method, whose measure criterion is the 
conditional probability generated from 
the CRF chunker. With respect to the 
special structures of Chinese base NP 
and complete analyses of the first two 
results, we also customize some appro-
priate grammar rules to avoid ambigui-
ties and prune errors. According to our 
overall experiments, the method 
achieves a higher accuracy in the final 
results. 
1 Introduction  
Chunking means extracting the non-overlapping 
segments from a stream of data. These segments 
are called chunks (Dirk and Satoshi, 2003). The 
definition of base noun phrase (base NP) is sim-
ple and non-recursive noun phrase which does 
not contain other noun phrase descendants. Base 
NP chunking could be used as a precursor for 
many elaborate natural language processing tasks, 
such as information retrieval, name entity extrac-
tion and text summarization and so on. Many 
other problems similar to text processing can also 
benefit from base NP chunking, for example, 
finding genes in DNA and phoneme information 
extraction. 
The initial work on base NP chunking is fo-
cused on the grammar-based method. Ramshaw 
and Marcus (1995) introduced a transformation-
based learning method which considered chunk-
ing as a kind of tagging problem. Their work in-
spired many others to study the applications of 
learning methods to noun phrase chunking. 
(Cardie and Pierce, 1998, 1999) applied a scoring 
method to select new rules and a naive heuristic 
for matching rules to evaluate the results' accu-
racy. 
CoNLL-2000 proposed a shared task (Tjong 
and Buchholz, 2000), which aimed at dividing a 
text in syntactically correlated parts of words. 
The eleven systems for the CoNLL-2000 shared 
task used a wide variety of machine learning 
methods. The best system in this workshop is on 
the basis of Support Vector Machines used by 
(Kudo and Matsumoto, 2000). 
Recently, some new statistical techniques, 
such as CRF (Lafferty et al 2001) and structural 
learning methods (Ando and Zhang, 2005) have 
been applied on the base NP chunking. (Fei and 
Fernando, 2003) considered chunking as a se-
quence labeling task and achieved good perform-
ance by an improved training methods of CRF. 
(Ando and Zhang, 2005) presented a novel semi-
supervised learning method on chunking and 
produced performances higher than the previous 
best results. 
The research on Chinese Base NP Chunking is, 
however, still at its developing stage. Research-
ers apply similar methods of English Base NP 
chunking to Chinese. Zhao and Huang (1998) 
made a strict definition of Chinese base NP and 
put forward a quasi-dependency model to analy-
sis the structure of Chinese base NPs. There are 
some other methods to deal with Chinese phrase 
(no only base NP) chunking, such as HMM 
(Heng Li et al, 2003), Maximum Entropy (Zhou 
Yaqian et al, 2003), Memory-Based Learning 
(Zhang and Zhou, 2002) etc. 
87
However, according to our experiments over 
30,000 Chinese words, the best results of Chi-
nese base NP chunking are about 5% less than 
that of English chunking (Although we should 
admit the chunking outcomes vary among differ-
ent sizes of corpus and rely on the details of ex-
periments). The differences between Chinese 
NPs and English NPs are summarized as follow-
ing points: First, the flexible structure of Chinese 
noun phrase often results in the ambiguities dur-
ing the recognition procedure. For example, 
many English base NPs begin with the determi-
native, while the margin of Chinese base NPs is 
more uncertain. Second, the base NPs begins 
with more than two noun-modifiers, such as ??
(high)/JJ  ?(new)/JJ ??(technology)/NN?, the 
noun-modifiers ??/JJ ? can not be completely 
recognized. Third, the usage of Chinese word is 
flexible, as a Chinese word may serve with multi 
POS (Part-of-Speech) tags. For example, a noun 
is used as a verbal or an adjective component in 
the sentence. In this way the chunker is puzzled 
by those multi-used words. Finally, there are no 
standard datasets and elevation systems for Chi-
nese base NP chunking as the CoNLL-2000 
shared task, which makes it difficult to compare 
and evaluate different Chinese base NP chunking 
systems. 
In this paper, we propose a hybrid approach to 
extract the Chinese base NPs with the help of the 
conditional probabilities derived from the CRF 
algorithm and some appropriate grammar rules. 
According to our preliminary experiments on 
SVM and CRF, our approach outperforms both 
of them.  
The remainder of the paper is organized as fol-
lows. Section 2 gives a brief introduction of the 
data representations and methods. We explain 
our motivations of the hybrid approach in section 
3. The experimental results and conclusions are 
introduced in section 4 and section 5 respectively. 
2 Task Description 
2.1 Data Representation 
Ramshaw and Marcus (1995) gave mainly two 
kinds of base NPs representation ?  the 
open/close bracketing and IOB tagging. For ex-
ample, a bracketed Chinese sentence, 
[ ??(foreign businessmen) ??(investment)] 
?? (become) [ ??  (Chinese) ?? (foreign 
trade)] [ ??(important) ???(growth)] ?  
The IOB tags are used to indicate the bounda-
ries for each base NP where letter ?B? means the 
current word starts a base NP, ?I? for a word in-
side a base NP and ?O? for a word outside a NP 
chunk. In this case the tokens for the former sen-
tence would be labeled as follows:    
??/B ??/I ??/V ??/B ??/I ??/B      
???/O  ?/O   
Currently, most of the work on base NP identi-
fication employs the trainable, corpus-based al-
gorithm, which makes full use of the tokens and 
corresponding POS tags to recognize the chunk 
segmentation of the test data. The SVM and CRF 
are two representative effective models widely 
used. 
2.2 Chunking with SVMs  
SVM is a machine learning algorithm for a linear 
binary classifier in order to maximize the margin 
of confidence of the classification on the training 
data set. According to the different requirements, 
distinctive kernel functions are employed to 
transfer non-linear problems into linear problems 
by mapping it to a higher dimension space.  
By transforming the training data into the form 
with IOB tags, we can view the base NP chunk-
ing problem as a multi-class classification prob-
lem. As SVMs are binary classifiers, we use the 
pairwise method to convert the multi-class prob-
lem into a set of binary class problem, thus the 
I/O/B classifier is reduced into 3 kinds of binary 
classifier ? I/O classifier, O/B classifier, B/I 
classifier. 
In our experiments, we choose TinySVM 1  to-
gether with YamCha 2  (Kudo and Matsumoto, 
2001) as the one of the baseline systems for our 
chunker. In order to construct the feature sets for 
training SVMs, all information available in the 
surrounding contexts, including tokens, POS tags 
and IOB tags. The tool YamCha makes it possi-
ble to add new features on your own. Therefore, 
in the training stage, we also add two new fea-
tures according to the words. First, we give spe-
cial tags to the noun words, especially the proper 
noun, as we find in the experiment the proper 
nouns sometimes bring on errors, such as base 
                                                          
1 http://chasen.org/~taku/software/TinySVM/ 
2 http://chasen.org/~taku/software/yamcha 
88
NP ???(Sichuan)/NR ??(basin)/NN?, con-
taining the proper noun ??? /NR?, could be 
mistaken for a single base NP ???/NN?; Sec-
ond, some punctuations such as separating marks, 
contribute to the wrong chunking, because many 
Chinese compound noun phrases are connected 
by separating mark, and the ingredients in the 
sentence are a mixture of simple nouns and noun 
phrases, for example, 
??? (National)/NN ??? ( Statistics Of-
fice)/NN???(Chinese)/NR ??(Social Sci-
ences)/NN ??? (Academy)/NN ? (and)/CC 
??? (Chinese Academy of Sciences)/NN-
SHORT? 
The part of base NP ? ???/B ??/I ???
/I? can be recognized as three independent base 
NPs --???/B ??/B ???/B?. The kind of 
errors comes from the conjunction ??(and)? and 
the successive sequences of nouns, which con-
tribute little to the chunker. More information 
d analyses will be provided in Section 4. an  
2.3 Conditional Random Fields 
Lafferty et al( 2001) present the Conditional 
Random Fields for building probabilistic models 
to segment and label sequence data, which was 
used effectively for base NP chunking (Sha & 
Pereira, 2003). Lafferty et al (2001) point out 
that each of the random variable label sequences 
Y conditioned on the random observation se-
quence X. The joint distribution over the label 
sequence Y given X has the form 
1
1
1
( | , ) exp( ( , ))
( )
( , ) ( , , , )
j j
j
n
i i
i
p y x F y x
Z x
F y x f y y x i
? ?
?
=
=
=
?
?
 
where 1( , , ,j i i )f y y x i?  is either a transition fea-
ture function ( 1, , ,i is y y x i? ) or a state feature 
function 1( , , , )i it y y x i?  ; 1,iy y? i are labels, x is 
an input sequence, i  is an input position, ( )Z x is 
a normalization factor; k? is the parameter to be 
estimated from training data.
Then we use the maximum likelihood training, 
such as the log-likelihood to train CRF given 
training data ( ){ },k kT x y= , 
1
( ) log ( , )
( ) k kk k
L F y x
Z x
? ??= + ?? ?? ??
( )L ? is minimized by finding unique zero of 
the gradient 
( | , )( ) [ ( , ) ( , )]kk k p Y x k
k
L F y x E F Y x??? = ??  
( | , ) ( , )kp Y x kE F Y x?  can be computed using a vari-
ant of the forward-backward algorithm. We de-
fine a transition matrix as following: 
' '( , | ) exp( ( , , , ))i j j
j
M y y x f y y x i?= ?  
Then, 
1
1
1
1
( | , ) ( , | )
( )
n
i i i
i
p y x M y y x
Z x
? + ?
=
= ?  
and let * denote component-wise matrix product,   
( | , )
1
( , ) ( | , ) ( , )
( )
( )
( ) 1
kp Y x k k k
y
T
i i i i
i
T
n
E F Y x p Y y x F y x
f M
Z x
Z x a
? ?
? ??
= =
?                           =
 = ?
?
?
 
Where i i? ?,  as the forward and backward 
state-cost vectors defined by 
1 1 1 1,
1 0 1
T
i i T i i
i i
M i n M i n
i i n
? ?? ?? + +    0 < ? ?  ? <?=  =? ?              =              =? ?
    Sha & Pereira (2003) provided a thorough dis-
cussion of CRF training methods including pre-
conditioned Conjugate Gradient, limited-
Memory Quasi-Newton and voted perceptron. 
They also present a novel approach to model 
construction and feature selection in shallow 
parsing.  
We use the software CRF++3 as our Chinese 
base NP chunker baseline software. The results 
of CRF are better than that of SVM, which is the 
same as the outcome of the English base NP 
chunking in (Sha & Pereira, 2003). However, we 
find CRF products some errors on identifying 
long-range base NP, while SVM performs well 
in this aspect and the errors of SVM and CRF are 
of different types. In this case, we develop a 
combination approach to improve the results. 
 
3 Our Approach 
?
 
(Tjong et al, 2000) pointed out that the perform-
ance of machine learning can be improved by 
combining the output of different systems, so 
they combined the results of different classifiers 
                                                          
3 http://www.chasen.org/~taku/software/CRF++/ 
89
and obtained good performance. Their combina-
tion system generated different classifiers by us-
ing different data labels and applied respective 
voting weights accordingly. (Kudo and Matsu-
moto 2001) designed a voting arrangement by 
applying cross validation and VC-bound and 
Leave-One-Out bound for the voting weights.  
The voting systems improve the accuracy, the 
choices of weights and the balance between dif-
ferent weights is based on experiences, which 
does not concern the inside features of the classi-
fication, without the guarantee of persuasive 
theoretical supports. Therefore, we developed a 
hybrid approach to combine the results of the 
SVM and CRF and utilize their advantages. 
(Simon, 2003) pointed out that the SVM guaran-
tees a high generalization using very rich features 
from the sentences, even with a large and high-
dimension training data. CRF can build efficient 
and robust structure model of the labels, when 
one doesn?t have prior knowledge about data. 
Figure 1 shows the preliminary chunking and 
pos-processing procedure in our experiments 
 First of all, we use YamCha and CRF++ re-
spectively to treat with the testing data. We got 
two original results from those chunkers, which 
use the exactly same data format; in this case we 
can compare the performance between CRF and 
SVM. After comparisons, we can figure out the 
same words with different IOB tags from the two 
former chunkers. Afterward, there exist two 
problems: how to pick out the IOB tags identi-
fied improperly and how to modify those wrong 
IOB tags.  
To solve the first question, we use the condi-
tional probability from the CRF to help deter-
mine the wrong IOB tags. For each word of the 
testing data, the CRF chunker works out a condi-
tional probability for each IOB tag and chooses 
the most probable tag for the output. We bring 
out the differences between the SVM and CRF, 
such as ??? (Sichuan)? in a base noun phrase 
is recognized as ?I? and ?O? respectively, and 
the distance between P(I| ????) and P(O| ??
??) is tiny. According to our experiment, about 
80% of the differences between SVM and CRF 
share the same statistical characters, which indi-
cate the correct answers are inundated by the 
noisy features in the classifier.  
  
CRF SVM 
Testing data 
Comparison 
Error pruning with 
rules and P (Y|X) 
Final result  
Figure 1 the Experiments? Procedure 
 
Using the comparison between SVM and CRF 
we can check most of those errors. Then we 
could build some simple grammar rules to figure 
out the correct tags for the ambiguous words cor-
responding to the surrounding contexts. Then At 
the error pruning step, judging from the sur-
rounding texts and the grammar rules, the base 
NP is corrected to the right form. We give 5 
mainly representative grammar rules to explain 
how they work in the experiments.  
The first simple sample of grammar rules is 
just like ?BNP ? NR NN?, used to solve the 
proper noun problems. Take the ???  (Si-
chuan)/NR/B ?? (basin)/NN/I? for example, 
the comparison finds out the base NP recognized 
as ??? (Sichuan)/NR/I ?? (basin)/NN/B?. 
Second, with respect to the base NP connecting 
with separating mark and conjunction words, two 
rules ?BNP ? BNP CC (BNP | Noun), BNP 
?BNP PU (BNP | Noun)? is used to figure out 
those errors; Third, with analyzing our experi-
ment results, the CRF and SVM chunker recog-
nize differently on the determinative, therefore 
the rule ?BNP ? JJ BNP?, our combination 
methods figure out new BNP tags from the pre-
liminary results according to this rule. Finally, 
the most complex situation is the determination 
of the Base NPs composed of series of nouns, 
especially the proper nouns. With figuring out 
the maximum length of this kind of noun phrase, 
we highlight the proper nouns and then separate 
the complex noun phrase to base noun phrases, 
and according to the our experiments, this 
90
method could solve close to 75% of the ambigu-
ity in the errors from complex noun phrases. To-
tally, the rules could solve about 63% of the 
found errors.  
 
4 Experiments 
The CoNLL 2000 provided the software4 to con-
vert Penn English Treebank II into the IOB tags 
form. We use the Penn Chinese Treebank 5.05, 
which is improved and involved with more POS 
tags, segmentation and syntactic bracketing. As 
the sentences in the Treebank are longer and re-
lated to more complicated structures, we modify 
the software with robust heuristics to cope with 
those new features of the Chinese Treebank and 
generate the training and testing data sets from 
the Treebank. Afterward we also make some 
manual adjustments to the final data.  
 In our experiments, the SVM chunker uses a 
polynomial kernel with degree 2; the cost per 
unit violation of the margin, C=1; and tolerance 
of the termination criterion, 0.01? = . 
In the base NPs chunking task, the evaluation 
metrics for base NP chunking include precision P, 
recall R and the F? . Usually we refer to the F?  
as the creditable metric.  
2
2
#
100%
#
#
100%
#
( 1)
( 1)
of correct proposed baseNP
P
of proposed baseNP
of correct proposed baseNP
R
of corect baseNP
RF
F
R F?
? ??
     =    
     =    
+=      =+
?
?
                                                          
 
All the experiments were performed on a 
Linux system with 3.2 GHz Pentium 4 and 2G 
memory. The total size of the Penn Chinese 
Treebank words is 13 MB, including about 
500,000 Chinese words. The quantity of training 
corpus amounts to 300,000 Chinese words. Each 
word contains two Chinese characters in average. 
We mainly use five kinds of corpus, whose sizes 
include 30000, 40000, 50000, 60000 and 70,000 
words. The corpus with an even larger size is 
improper according to the training corpus 
amount.  
4 http://ilk.kub.nl/~sabine/chunklink/ 
5 http://www.cis.upenn.edu/~chinese/  
 
From Figure 2, we can see that the results 
from CRF are better than that from SVM and the 
error-pruning performs the best. Our hybrid er-
ror-pruning method achieves an obvious im-
provement F-scores by combining the outcome 
from SVM and CRF classifiers. The test F-scores 
are decreasing when the sizes of corpus increase. 
The best performance with F-score of 89.27% is 
achieved by using a test corpus of 30k words. 
We get about 1.0% increase of F-score after us-
ing the hybrid approach. The F-score is higher 
than F-score 87.75% of Chinese base NP chunk-
ing systems using the Maximum Entropy method 
in (Zhou et al, 2003),. Which used the smaller 3 
MB Penn Chinese Treebank II as the corpus.  
The Chinese Base NP chunkers are not supe-
rior to those for English. Zhang and Ando (2005) 
produce the best English base NP accuracy with 
F-score of 94.39+ (0.79), which is superior to our 
best results. The previous work mostly consid-
ered base NP chunking as the classification prob-
lem without special attention to the lexical 
information and syntactic dependence of words. 
On the other hand, we add some grammar rules 
to strength the syntactic dependence between the 
words. However, the syntactic structure derived 
from Chinese is much more flexible and complex 
than that from English. First, some Chinese 
words contain abundant meanings or play differ-
ent syntactic roles. For example, "?? (among 
which)/NN ? ? (Chongqing)/NR ? ?
(district)/NN" is recognized as a base NP. Actu-
91
ally the Chinese word ???/NN (among)? refers 
to the content in the previous sentence and ??? 
(thereinto)? sometimes used as an adverb. Sec-
ond, how to deal with the conjunctions is a major 
problem, especially the words ?? (and)? can 
appear in the preposition structure ?? ?? ?
? (relate to)?, which makes it difficult to judge 
those types of differences. Third, the chunkers 
can not handle with compact sequence data of 
chunks with name entities and new words (espe-
cially the transliterated words) satisfactorily, 
such as  
??? ( China ) /NR ????( Red Cross ) 
/NR?? ( Honorary ) /NN ?? (Chairman ) 
/NN ???( Jiang Ze-min ) /NR?  
As it points above, the English name entities 
sequences are connected with the conjunction 
such as ?of, and, in?. While in Chinese there are 
no such connection words for name entities se-
quences. Therefore when we use the statistical 
methods, those kinds of sequential chunks con-
tribute slightly to the feature selection and classi-
fier training, and are treated as the useless noise 
in the training data. In the testing section, it is 
close the separating margin and hardly deter-
mined to be in the right category. What?s more, 
some other factors such as Idiomatic and special-
ized expressions also account for the errors. By 
highlighting those kinds of words and using 
some rules which emphasize on those proper 
words, we use our error-pruning methods and 
useful grammar rules to correct about 60% errors.  
5 Conclusions  
This paper presented a new hybrid approach for 
identifying the Chinese base NPs. Our hybrid 
approach uses the SVM and CRF algorithm to 
design the preliminary classifiers for chunking. 
Furthermore with the direct comparison between 
the results from the former chunkers, we figure 
out that those two statistical methods are myopic 
about the compact chunking data of sequential 
noun. With the intention of capturing the syntac-
tic dependence within those sequential chunking 
data, we make use of the conditional probabili-
ties of the chunking tags given the corresponding 
tokens derived from CRF and some simple 
grammar rules to modify the preliminary results.  
The overall results achieve 89.27% precision 
on the base NP chunking. We attempt to explain 
some existing semantic problems and solve a 
certain part of problems, which have been dis-
covered and explained in the paper. Future work 
will concentrate on working out some adaptive 
machine learning methods to make grammar 
rules automatically, select better features and 
employ suitable classifiers for Chinese base NP 
chunking. Finally, the particular Chinese base 
phrase grammars need a complete study, and the 
approach provides a primary solution and 
framework to process an analyses and compari-
sons between Chinese and English parallel base 
NP chunkers. 
Acknowledgments 
This work was partially supported by the Natural 
Science Foundation of China under Grant No. 
60575043, and 60121302, the China-France PRA 
project under Grant No. PRA SI02-05, the Out-
standing Overseas Chinese Scholars Fund of the 
Chinese Academy of Sciences under Grant 
No.2003-1-1, and Nokia (China) Co. Ltd, as well. 
References  
Claire Cardie and David Pierce. 1998. Error-Driven 
Pruning of Treebank Grammars for Base Noun 
Phrase Identification. Proceedings of the 36th ACL 
and COLING-98, 218-224. 
Claire Cardie and David Pierce. 1999. The role of 
lexicalization and pruning for base noun phrase 
grammars. Proceedings of the 16th AAAI, 423-430. 
Dirk Ludtke and Satoshi Sato. 2003. Fast Base NP 
Chunking with Decision Trees ?? Experiments on 
Different POS tag Settings. CICLing 2003, 136-
147. LNC S2588, Springer-Verlag Berlin Heidel-
berg. 
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. 
Introduction to the CoNLL-2000 Shared Task: 
Chunking. Proceedings of CoNLL and LLL-2000, 
127-132. 
Erik F. Tjong Kim Sang, Walter Daelemans, Herv?   
D?ean, Rob Koeling, Yuval Krymolowski, Vasin 
Punyakanok, and Dan Roth. 2000. Applying system 
combination to base noun phrase identification.  
Proceedings of COLING 2000, 857-863. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. Proceedings of 
HLT-NAACL 2003, 134-141.  
Heng Li, Jonathan J. Webster, Chunyu Kit, and 
Tianshun Yao. 2003. Transductive HMM based 
Chinese Text Chunking. IEEE NLP-KE 2003, Bei-
jing, 257-262. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text Chunking using Transformation-Based Learn-
ing.    Proceedings of the Third ACL Workshop on 
Very Large Corpora, 82?94. 
92
Lafferty A. McCallum and F. Pereira. 2001. Condi-
tional random Fields. Proceedings of ICML 2001, 
282-289. 
Rie Kubota Ando and Tong Zhang. 2004. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. RC23462. Technical 
report, IBM. 
Rie Kubota Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Method 
for Text Chunking. Proceedings of the 43rd Annual 
Meeting of ACL, 1-9. 
Simon Lacoste-Julien. 2003. Combining SVM with 
graphical models for supervised classification: an 
introduction to Max-Margin Markov Network. 
CS281A Project Report, UC Berkeley. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with support vector machine. Proceeding of the 
NAACL, 192-199. 
Zhang Yuqi and Zhou Qiang. 2002. Chinese Base-
Phrases Chunking. First SigHAN Workshop on 
Chinese Language Processing, COLING-02. 
Zhao Jun and Huang Changling. 1998. A Quasi-
Dependency Model for Structural Analysis of Chi-
nese BaseNPs. 36th Annual Meeting of the ACL 
and 17th International Conference on Computa-
tional Linguistics. 
Zhou Yaqian, Guo YiKun, Huang XuanLing and Wu 
Lide. 2003. Chinese and English Base NP Recogni-
tion on a Maximum Entropy Model. Vol140, No13. 
Journal of Computer Research and Development. 
(In Chinese) 
93
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447?455,
Beijing, August 2010
A Novel Reordering Model Based on Multi-layer Phrase for Sta-
tistical Machine Translation 
Yanqing He1,     Yu Zhou2,     Chengqing Zong2,     Huilin Wang1
1Institute of Scientific and Technical 
Information of China 
{heyq,wanghl}@istic.ac.cn
2Institute of Automation, Chinese  
Academy of Sciences 
{yzhou,cqzong}@nlpr.ia.ac.cn
Abstract
Phrase reordering is of great importance 
for statistical machine translation. Ac-
cording to the movement of phrase trans-
lation, the pattern of phrase reordering 
can be divided into three classes: mono-
tone, BTG (Bracket Transduction 
Grammar) and hierarchy. It is a good 
way to use different styles of reordering 
models to reorder different phrases ac-
cording to the characteristics of both the 
reordering models and phrases itself.  In 
this paper a novel reordering model 
based on multi-layer phrase (PRML) is 
proposed, where the source sentence is 
segmented into different layers of phras-
es on which different reordering models 
are applied to get the final translation.  
This model has some advantages: differ-
ent styles of phrase reordering models 
are easily incorporated together; when a 
complicated reordering model is em-
ployed, it can be limited in a smaller 
scope and replaced with an easier reor-
dering model in larger scope. So this 
model better trade-offs the translation 
speed and performance simultaneously.  
1 Introduction 
In statistical machine translation (SMT), phrase 
reordering is a complicated problem. According 
to the type of phrases, the existing phrase reor-
dering models are divided into two categories: 
contiguous phrase-based reordering models and 
non-contiguous phrase-based reordering models.  
Contiguous phrase-based reordering models 
are designed to reorder contiguous phrases. In 
such type of reordering models, a contiguous 
phrase is reordered as a unit and the movements 
of phrase don?t involve insertions inside the oth-
er phrases. Some of these models are content-
independent, such as distortion models (Och and 
Ney, 2004; Koehn et al, 2003) which penalize 
translation according to jump distance of phrases, 
and flat reordering model (Wu, 1995; Zens et al, 
2004)which assigns constant probabilities for 
monotone order and non-monotone order. These 
reordering models are simple and the contents of 
phrases have not been considered. So it?s hard to 
obtain a satisfactory translation performance. 
Some lexicalized reordering models (Och et al, 
2004; Tillmann 2004, Kumar and Byrne, 2005, 
Koehn et al, 2005) learn local orientations (mo-
notone or non-monotone) with probabilities for 
each bilingual phrase from training data. These 
models are phrase-dependent, so improvements 
over content-independent reordering models are 
obtained. However, many parameters need to be 
estimated.  
Non-contiguous phrase-based reordering 
models are proposed to process non-contiguous 
phrases and the movements of phrase involve 
insertion operations. This type of reordering 
models mainly includes all kinds of syntax-
based models where more structural information 
is employed to obtain a more flexible phrase 
movement. Linguistically syntactic approaches 
(Yamada and Knight, 2001; Galley et al, 2004, 
2006; Marcu et al, 2006; Liu et al, 2006; Shie-
ber et al, 1990; Eisner, 2003; Quirk et al, 2005; 
Ding and Palmer, 2005) employ linguistically 
syntactic information to enhance their reordering 
capability and use non-contiguous phrases to 
447
obtain some generalization. The formally syn-
tax-based models use synchronous context-free 
grammar (SCFG) but induce a grammar from a 
parallel text without relying on any linguistic 
annotations or assumptions (Chiang, 2005; 
Xiong et al, 2006). A hierarchical phrase-based 
translation model (HPTM) reorganizes phrases 
into hierarchical ones by reducing sub-phrases to 
variables (Chiang 2005). Xiong et al (2006) is 
an enhanced bracket transduction grammar with 
a maximum entropy-based reordering model 
(MEBTG). Compared with contiguous phrase-
based reordering model, Syntax-based models 
need to shoulder a great deal of rules and have 
high computational cost of time and space. The 
type of reordering models has a weaker ability of 
processing long sentences and large-scale data, 
which heavily restrict their application. 
The above methods have provided various 
phrases reordering strategies. According to the 
movement of phrase translation, the pattern of 
phrase reordering can be divided into three 
classes: monotone, BTG (Bracket Transduction 
Grammar) (Wu, 1995) and hierarchy.  In fact for 
most sentences, there may be some phrases 
which have simple reordering patterns, such as 
monotone or BTG style. It is not necessary to 
reorder them with a complicated mechanism, e.g. 
hierarchy. It is a good idea that different reorder-
ing models are employed to reorder different 
phrases according to the characteristics of both 
the reordering models and the phrases itself. 
This paper thus gives a novel reordering model 
based on multi-layer phrase (PRML), where the 
source sentence is segmented into different lay-
ers of phrases on which different reordering 
models are applied to get the final translation. 
Our model has the advantages as follow: (1) 
PRML segments source sentence into multiple-
layer phrases by using punctuation and syntactic 
information and the design of segmentation al-
gorithm corresponds to each reordering model. 
Different reordering models are chosen for each 
layer of phrases. (2) In our model different reor-
dering models can be easily integrated together 
to obtain a combination of multiple phrase reor-
dering models.  (3) Our model can incorporate 
some complicated reordering models. We limit 
them in relatively smaller scopes and replace 
them with easier reordering models in larger 
scopes. In such way our model better trade-offs 
the translation speed and performance simulta-
neously. (4) Our segmentation strategy doesn?t 
impair translation quality by controlling phrase 
translation tables to determine the scope of each 
reordering model in each source sentence.  The 
poor phrase translations generated by the former 
reordering model, still have chances of being 
revised by the latter reordering model.  
Our work is similar to the phrase-level system 
combination (Mellebeek et al, 2006). We share 
one important characteristic: we decompose in-
put sentence into chunks and recompose the 
translated chunks in output. The differences are 
that, we segment the input sentence into multi-
layer phrases and we reorder their translations 
with a multi-layer decoder.  
The remainder of the paper is organized as 
follows: Section 2 gives our reordering model 
PRML. Section 3 presents the details of the sen-
tence segmentation algorithm and the decoding 
algorithm. Section 4 shows the experimental re-
sults. Finally, the concluding remarks are given 
in Section 5. 
2 The Model 
We use an example to demonstrate our motiva-
tion. Figure 1 shows a Chinese and English sen-
tence pair with word alignment. Each solid line 
denotes the corresponding relation between a 
Chinese word and an English word. Figure 2 
shows our reordering mechanism. For the source 
sentence, the phrases in rectangle with round 
corner in row 2 obviously have a monotone 
translation order. For such kinds of phrase a mo-
notone reordering model is enough to arrange 
their translations.  Any two neighbor consecutive 
phrases in the ellipses in row 3 have a straight 
orders or inverted order. So BTG reordering 
model is appropriate to predict the order of this 
type of phrases. Inside the phrases in the ellipses 
in row 3 there are possibly more complicated 
hierarchical structures. For the phrase ??? ??
? ??, a rule ? 1 1X towards the road to X&o ?? ??? ?
has the ascendancy over the monotone and BTG 
style of reordering model.  Hierarchy style of 
reordering models, such as HPTM reordering 
model, can translate non-contiguous phrases and 
has the advantage of capturing the translation of 
such kind of phrases. 
The whole frame of our model PRML is 
shown in Figure 3. PRML is composed of a 
448
segmentation sentence module and a decoder 
which consists of three different styles of phrase 
reordering models. The source sentence is seg-
mented into 3 layers of phrases: the original 
whole sentence, sub-sentences and chunks. The 
original whole sentence is considered as the 
first-layer phrase and is segmented into sub-
sentences to get the second-layer phrase. By fur-
ther segmenting these sub-sentences, the chunks 
are obtained as the third-layer phrase. The whole 
translation process includes three steps: 1) In 
order to capture the most complicated structure 
of phrases inside chunks, HPTM reordering 
model are chosen to translate the chunks. So the 
translations of chunks are obtained. 2) Combine 
the bilingual chunks generated by step 1 with 
those bilingual phases generated by the MEBTG 
training model as the final phrase table and 
translate the sub-sentences with MEBTG reor-
dering model, the translations of sub-sentences 
are obtained. 3) Combine the bilingual sub-
sentences generated by step 2 with those bilin-
gual phases generated by the Monotone training 
model as the final phrase table and translate the 
original whole sentences with monotone reorder- 
Figure 1.  An example of Chinese-English sentence pair with their word alignment 
Figure 2.  Diagram of Translation Using PRML.  
Figure 3. Frame of PRML 
449
Figure 4. General frame of our model 
ing model, the translations of  the original whole 
sentences are obtained. 
We also give a general frame of our model in 
Figure 4. In the segmentation module, an input 
source sentence is segmented into G layers of 
contiguous source strings, Layer 1, Layer 2, ?, 
Layer G. The phrases of lower-order layer are 
re-segmented into the phrases of higher-order 
layer. The phrases of the same layer can be 
combined into the whole source sentence. The 
decoding process starts from the phrases of the 
highest-order layer. For each layer of phrases a 
reordering model is chosen to generate the trans-
lations of phrases according to their characteris-
tics. The generated translations of phrases in the 
higher-order layer are fed as a new added trans-
lation source into the next lower-order reorder-
ing model. After the translations of the phrase in 
Layer 2 are obtained, they are fed into the Reor-
dering model 1 as well as the source sentence 
(the phrase in Layer 1) to get the target transla-
tion.  
Due to the complexity of the language, there 
may be some sentences whose structures don?t 
conform to the pattern of the reordering models 
we choose. So in our segmentation module, if 
the sentence doesn?t satisfy the segmentation 
conditions of current layer, it will be fed into the 
segmentation algorithm of the next layer. Even 
in the worst condition when the sentence isn?t 
segmented into any phrase by segmentation 
module, it will be translated as the whole sen-
tence to get the final translation by the highest-
order reordering model.  
Our model tries to grasp firstly the simple 
reordering modes in source sentence by the low-
er layer of phrase segmentations and controls 
more complicated reordering modes inside the 
higher layers of phrases. Then we choose some 
complicated reordering models to translate those 
phrases. Thus search space and computational 
complexity are both reduced. After obtaining the 
translation of higher layer?s phrases, it is enough 
for simple reordering models to reorder them.  
Due to phrase segmentation some phrases may 
be translated poorly by the higher layer of reor-
dering models, but they still have chances of be-
ing revised by the lower layer of reordering 
model because in lower layer of reordering mod-
el the input phrases have not these hard segmen-
tation boundary and our model uses phrase trans-
lation tables to determine the scope of each reor-
dering model.  
 There are two key issues in our model. The 
first one is how to segment the source sentence 
into different layers of phrases. The second one 
is how to choose a reordering model for different 
layer of phrases. In any case the design of seg-
menting sentence module should consider the 
characteristic of the reordering model of phrases. 
3 Implementation 
The segmentation module consists of the sub-
sentence segmentation and chunk segmentation. 
The decoder combines three reordering models, 
HPTM, MEBTG, and a monotone reordering 
model. 
3.1 Segmentation module
We define the sub-sentence as the word se-
quence which can be translated in monotone or-
der. The following six punctua-
tions: ? ? ?  ?  ?  ? in Chinese, 
and . ! ? , : ; in English are chosen as the seg-
mentation anchor candidates.   Except Chinese 
comma, all the other five punctuations can ex-
450
press one semantic end and another semantic 
beginning.  In most of the time, it has high error 
risk to segment the source sentence by commas. 
So we get help from syntactic information of 
Chinese dependency tree to guarantee the mono-
tone order of Chinese sub-sentences.  
The whole process of sub-sentence 
segmentation includes training and segmenting. 
Training: 1) The word alignment of training 
parallel corpus is obtained; 2) The parallel 
sentence pairs in training corpus are segmented 
into sub-sentences candidates. For a Chinese-
English sentence pair with their word alignment 
in training data, all bilingual punctuations are 
found firstly, six punctuations respectively 
???????? in Chinese and ?? ! . , : ;? in 
English. The punctuation identification number 
(id) sets in Chinese and English are respectively 
extracted.  For a correct punctuation id pair (id_c,
id_e), the phrase before id_e in English sentence 
should be the translation of the phrase before 
id_c in Chinese sentence, namely the number of 
the links 1 between the two phrases should be 
equal. In order to guarantees the property we 
calculate a bilingual alignment ratio for each 
Chinese-English punctuation id pair according to 
the following equation. For the punctuation id 
pair (id_c, id_e), bilingual alignment ratio 
consists of two value, Chinese-English 
alignment ratio (CER) and English-Chinese 
alignment ratio (ECR).
1 _
1
1 _
1
( )
( )
ij
i id c
j J
ij
j id e
i I
A
CER
A
G
G
d d
d d
d d
d d
 
?
?
1 _
1
1 _
1
( )
( )
ij
j id e
i I
ij
i id c
j J
A
ECR
A
G
G
d d
d d
d d
d d
 
?
?
where ( )ijAG is an indicator function whose value 
is 1 when the word id pair ( , )i j is in the word 
alignment and is 0 otherwise.  I and J are the 
length of the Chinese English sentence pair. 
CER of a correct punctuation id pair will be 
equal to 1.0. So does ECR.  In view of the error 
rate of word alignment, the punctuation id pairs 
will be looked as the segmentation anchor if 
both CER and ECR are falling into the threshold 
range (minvalue, maxvalue). Then all the 
punctuation id pairs are judged according to the 
same method and those punctuation id pairs 
1 Here a link between a Chinese word and an English word 
means the word alignment between them.
satisfying the requirement segment the sentence 
pair into sub-sentence pairs. 3) The first word of 
Chinese sub-sentence in each bilingual sub-
sentence pair is collected.  We filter these words 
whose frequency is larger than predefined 
threshold to get segmentation anchor word set 
(SAWS).
Segmenting: 1) The test sentence in Chinese is 
segmented into segments by the six Chinese 
punctuation ???????? in the sentence. 2)
If the first word of a segment is in SAWS the 
punctuation at the end of the segment is chosen 
as the segmentation punctuation. 3) If a segment 
satisfies the property of ?dependency integrity? 
the punctuation at the end of the segment is also 
chosen as the segmentation punctuation. Here 
?dependency integrity? is defined in a 
dependency tree. Figure 5 gives the part output  
Figure 5. The part dependency parser output 
of a Chinese sentence. 
of ?lexical dependency parser?2  for a Chinese 
sentence. There are five columns of data for each 
word which are respectively the word id, the 
word itself, its speech of part, the id of its head 
word and their dependency type. In the sentence 
the Chinese word sequence ??? ?? ?? ?
? (US congressional representatives say that)? 
has such a property: Each word in the sequence 
has a dependency relation with the word which 
is still in the sequence except one word which 
has a dependency relation with the root, e.g. id 4. 
We define the property as ?dependency integri-
ty?. Our reason is: a sub-sentence with the prop-
erty of ?dependency integrity? has relatively in-
dependent semantic meaning and a large possi-
bility of monotone translation order. 4) The un-
ion of the segmentation punctuations in step 2) 
and 3) are the final sub-sentence segmentation 
tags.
2 http://www.seas.upenn.edu/
~strctlrn/MSTParser/MSTParser.html
ID              word          POS        head id  dependency type 
1 ?? NR 3 NMOD 
2 ?? NN 3 NMOD 
3 ?? NN 4 SUB 
4 ?? VV 0 ROOT 
5 ? PU 4 P 
6 ??? NN 7 VMOD 
7 ?? VV 9 VMOD 
8 ? PU 9 P 
? ?            ? ?            ? ?            ? ?                  ? ? 
451
After sub-sentence segmentation, chunks 
segmentation is carried out in each sub-sentence. 
We define the chunks as the word sequence 
which can be translated in monotone order or 
inverted order. Here the knowledge of the 
?phrase structure parser? 3  and the ?lexicalized 
dependency parser? are integrated to segment 
the sub-sentence into chunks. In a Chinese 
phrase structure parser tree the nouns phrase (NP) 
and preposition phrase (PP) are relatively inde-
pendent in semantic expressing and relatively 
flexible in translation. So in the chunk segmenta-
tion, only the NP structure and PP structure in 
the Chinese structure parsing tree are found as 
phrase structure chunk. The process of chunk 
segmentation is described as follows: 1) the test 
sub-sentence is parsed to get the phrase structure 
tree and dependency parsing tree; 2) We traverse 
the phrase structure tree to extract sub-tree of 
?NP? and ?PP? to obtain the phrase structure 
chunks. 3) We mark off the word sequences with 
?dependency integrity? in the dependency tree. 4)
Both the two kinds of chunks are recombined to 
obtain the final result of chunk segmentation. 
3.2 Decoding
Our decoder is composed of three styles of reor-
dering models: HPTM, MEBTG and a monotone 
reordering model. 
According to Chiang (2005), given the 
chunk chunkc , a CKY parser finds ch u n ke

, the Eng-
lish yield of the best derivation hptmD

that has 
Chinese yield chunkc :
( )
( )
( argmax Pr( ))
hptm chunk
chunk chunk hptm
chunk hptm
C D C
e e D
e D
 
 
 

Here the chunks not the whole source sentence 
are fed into HPTM decoder to get the L-best 
translations and feature scores of the chunks. We 
combine all the chunks, their L-best translations 
and the feature scores into a phrase table, namely 
chunk phrase table. We only choose 4 translation 
scores (two translation probability based on fre-
quency and two lexical weights based on word 
alignment) because the language model score, 
phrase penalty score and word penalty score will 
be re-calculated in the lower layer of reordering 
3 http://nlp.stanford.edu/software/lex-parser.shtml
model and need not be kept here. Meantime we 
change the log values of the scores into probabil-
ity value. In the chunk phrase table each phrase 
pair has a Chinese phrase, an English phrase and 
four translations feature scores. In each phrase 
pair the Chinese phrase is one of our chunks, the 
English phrase is one translation of L-best of the 
chunk. 
 In MEBTG (Xiong et al, 2006), three rules 
are used to derive the translation of each sub-
sentence: lexical rule, straight rule and inverted 
rule. Given a source sub-sentence sub sentC  , it 
finds the final sub-sentence translation sub sentE 

from the best derivation m eb tgD

:
( )
( )
( arg max Pr( ))
mebtg sub sent
sub sent sub sent mebtg
mebtg
C D C
E E D
E D

 
 
 
 
 
Generally chunk segmentation will make some 
HPTM rules useless and reduce the translation 
performance. So in MEBTG we also use base 
phrase pair table which contains the contiguous 
phrase translation pairs consistent with word 
alignment.  We merge the chunk phrase table 
and base phrase table together and feed them 
into MEBTG to translate each sub-sentence. 
Thus the K-Best translation and feature scores of 
each sub-sentence are obtained and then are re-
combined into a new phrase table, namely sub-
sentence phrase table, by using the same method 
with chunk phrase table. 
 Having obtained the translation of each sub-
sentence we generate the final translation of the 
whole source sentence by a monotone reordering 
model. Our monotone reordering model employs 
a log-linear direct translation model. Three 
phrase tables: chunk phrase table, sub-sentence 
phrase table and base phrase table are merged 
together and fed into the monotone decoder. 
Thus the decoder will automatically choose 
those phrases it need. In each phrase table each 
source phrase only has four translation probabili-
ties for its candidate translation. So it?s easy to 
merge them together. In such way all kinds of 
phrase pairs will automatically compete accord-
ing to their translation probabilities. So our 
PRML model can automatically decide which 
reordering model is employed in each phrase 
scope of the whole source sentence. It?s worth 
noting that the inputs of the three reordering 
452
model have no segmentation tag. Because any 
segmentation for the input before decoding will 
influence the use of some rules or phrase pairs 
and may cause some rules or phrase pairs losses. 
It would be better to employ different phrase 
table to limit reordering models and let each de-
coder automatically decide reordering model for 
each segments of the input. Thus by controlling 
the phrase tables we apply different reordering 
models on different phrases. For each reordering 
model we perform the maximum BLEU training 
(Venugopal et al 2005) on a development set. 
For HPTM the training is same as Chiang 2007. 
For MEBTG we use chunk phrase table and base 
table to obtain translation parameters. For mono-
tone reordering model all the three phrase tables 
are merged to get translation weights. 
4  Experiments 
This section gives the experiments with Chinese-
to-English translation task in news domain. Our 
evaluation metric is case-insensitive BLEU-4 
(Papineni et al 2002). We use NIST MT 2005, 
NIST MT 2006 and NIST MT 2008 as our test 
data. Our training data is filtered from the LDC 
corpus4. Table 1 gives the statistics of our data.  
4.1 Evaluating translation Performance  
We compare our PRML against two baselines: 
MEBTG system developed in house according 
to Xiong (2006, 2008) and HPTM system5 in 
PYTHON based on HPTM reordering model 
(Chiang 2007). In MEBTG phrases of up to 10 
words in length on the Chinese side are extracted 
and reordering examples are obtained without 
limiting the length of each example.  Only the 
last word of each reordering example is used as 
lexical feature in training the reordering model 
by the maximum entropy based classifier6. We 
also set a swapping window size as 8 and the 
beam threshold as 10.  It is worth noting that our 
MEBTG system uses cube-pruning algorithm 
(Chiang 2005) from bottom to up to generate the  
4 LDC corpus lists: LDC2000T46,  LDC2000T50, 
LDC2002E18, LDC2002E27, LDC2002L27, LDC2002T01, 
LDC2003E07, LDC2003E14, LDC2003T17, LDC2004E12, 
LDC2004T07, LDC2004T08, LDC2005T01, LDC2005T06, 
LDC2005T10, LDC2005T34, LDC2006T04, LDC2007T09 
5 We are extremely thankful to David Chiang who original-
ly implement the PYTHON decoder and share with us. 
6 http://maxent.sourceforge.net/
Set Language Sentence Vocabulary A. S. L
Train
data
Chinese 297,069 6,263 11.9
English 297,069 8,069 13.6
NIST
05 
Chinese 1,082 5669 28.2
English 4,328 7575 32.7
NIST
06 
Chinese 1,664 6686 23.5
English 6,656 9388 28.9
NIST
08 
Chinese 1,357 6,628 24.5
English 5,428 9,594 30.8
Table 1. The statistics of training data and test 
data, A. S. L is average sentence length. 
N-best list not the lazy algorithm of (Huang and 
Chiang, 2005). We also limit the length of the 
HPTM initial rules no more than 10 words and 
the number of non-terminals within two. In the 
decoding for the rules the beam pruning parame-
ter is 30 and threshold pruning parameter is 1.0. 
For hypotheses the two pruning parameters are 
respectively 30 and 10. In our PRML minva-
lue=0.8, maxvalue=1.25, which are obtained by 
minimum error rate training on the development 
set. The predefined value for filtering SAWS is
set as 100.
The translation performance of the three reor-
dering model is shown in Table 2. We can find 
that PRML has a better performance than 
MEBTG with a relatively 2.09% BLEU score in 
NIST05, 5.60% BLEU score in NIST06 and  
5.0% BLEU score in NIST08. This indicates that 
the chunk phrase table increases the reordering 
ability of MEBTG. Compared with HPTM, 
PRML has a comparable translation performance 
in NIST08. In NIST05 and NIST06 our model 
has a slightly better performance than HPTM. 
Because PRML limit hierarchical structure reor-
dering model in chunks while HPTM use them 
in the whole sentence scope (or in a length 
scope), HPTM has a more complicated reorder-
ing mechanism than PRML. The experiment re-
sult shows even though we use easier reordering 
moels in larger scope, e.g. MEBTG and monoto- 
Model Nist05 Nist06 Nist08 
HPTM 0.3183 0.1956 0.1525 
MEBTG 0.3049 0.1890 0.1419 
PRML 0.3205 0.1996 0.1495 
Table 2. The translation performance  
453
ne reordering model, we have a comparatively 
translation performance as HPTM.  
4.2 Evaluating translation speed  
Table 3 shows the average decoding time on test 
data for the three phrase reordering models on a 
double processor of a dual 2.0 Xeon machine. 
Time denotes mean time of per-sentence, in 
seconds. It is seen that PRML is the slower than 
MEBTG but reduce decoding time with a rela-
tively 54.85% seconds in NIST05, 75.67% 
seconds in NIST06 and 65.28% seconds in 
NIST08. For PRML, 93.65% average decoding 
time in NIST05 is spent in HPTM, 4.89% time 
in MEBTG and 1.46% time in monotone reor-
dering decoder.  
Model Nist05 Nist06 Nist08 
HPTM 932.96 1235.21 675 
MEBTG 43.46 27.16 10.24 
PRML 421.20 300.52 234.33 
Table 3. The average decoding time 
4.3 Evaluating the performance of each 
layer of phrase table
In order to evaluate the performance of each 
reordering model, we run the monotone decoder 
with different phrase table in NIST05. Table 4 
list the size of each phrase table. From the re-
sults in Table 5 it is seen that the performance of 
using three phrase tables is the best.  Compared 
with the base phrase table, the   translation per-
formances are improved with relatively 10.86% 
BLEU score by adding chunk phrase table and 
11% BLEU score by adding sub-sentence table. 
The result of row 4 has a comparable to the one 
in row 5. It indicates the sub-sentence phrase 
table has contained the information of HPTM 
reordering model. The case of row 4 to row 2 is 
the same. 
Phrase table Phrase pair 
Base 732732 
Chunk 86401 
Sub-sentence 24710 
Table 4.  The size of each phrase table. 
Phrase table Reordering model BLEU
Base Monotone 0.2871
Base +chunk monotone+HPTM 0.3180
Base +sub-
sentence table
monotone+HPTM 
+MEBTG
0.3187
Base +chunk 
+subsentence
monotone+HPTM 
+MEBTG
0.3205
Table 5.  The performance of phrase table 
5  Conclusions 
In this paper, we propose a novel reordering 
model based on multi-layer phrases (PRML), 
where the source sentence is segmented into dif-
ferent layers of phrases and different reordering 
models are applied to get the final translation. 
Our model easily incorporates different styles of 
phrase reordering models together, including 
monotone, BTG, and hierarchy or other more 
complicated reordering models. When a compli-
cated reordering model is used, our model can 
limit it in a smaller scope and replace it with an 
easier reordering model in larger scope. In such 
way our model better trade-offs the translation 
speed and performance simultaneously.  
In the next step, we will use more features to 
segment the sentences such as syntactical fea-
tures or adding a dictionary to supervise the 
segmentation. And also we will try to incorpo-
rate other systems into our model to improve the 
translation performance. 
6 Acknowledgements 
The research work has been partially funded by 
the Natural Science Foundation of China under 
Grant No. 6097 5053, and 60736014, the Na-
tional Key Technology R&D Program under 
Grant No. 2006BAH03B02, the Hi-Tech Re-
search and Development Program (?863? Pro-
gram) of China under Grant No. 
2006AA010108-4, and also supported by the 
China-Singapore Institute of Digital Media 
(CSIDM) project under grant No. CSIDM-
200804, and Research Project ?Language and 
Knowledge Technology? of Institute of Scientif-
ic and Technical Information of China 
(2009DP01-6). 
454
References 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Pro-
ceedings of ACL 2005, pages 263?270. 
David Chiang, 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics,33(2):201-
228. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In proceeding of 43th Meet-
ing of the Association for Computational Linguis-
tics, 541-548 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In proceedings 
of the 41th Meeting of the Association for Compu-
tational Linguistics (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and 
Daniel Marcu. 2004. What?s in a translation rule?
In proceedings of HLTNAACL- 2004. 
Michel Galley, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang, Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In 
Proceedings of the joint conference of the Interna-
tional Committee on Computational Linguistics 
and the Association for Computational Linguistics. 
Sydney, Australia. 
Liang Huang and David Chiang. 2005. Better k-best 
parsing. In Proceedings of the Ninth International 
Workshop on Parsing Technology, Vancouver, 
October, pages 53?64. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu, 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the ACL. 
page 311-318, Philadelphia, PA. 
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003. 
Statistical phrase-based translation. In proceed-
ings of HLT-NAACL-03, 127-133 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Eval-
uation. In International Workshop on Spoken Lan-
guage Translation. 
Shankar Kumar and William Byrne. 2005. Local 
phrase reordering models for statistical machine 
translation. In Proceedings of HLT-EMNLP. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In proceedings of ACL-06, 609-616. 
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical ma-
chine translation. In proceedings of EMNLP-02, 
133-139. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, 
and Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Lan-
guage Phrases. In Proceedings of EMNLP-2006, 
44-52, Sydney, Australia 
Bart Mellebeek, Karolina Owczarzak, Josef Van Ge-
nabith, Andy Way. 2006. Multi-Engine Machine 
Translation By Recursive Sentence Decomposition.
In Proceedings of AMTA 2006 
Franz J. Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417-449 
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Ke-
vin Knight, Dragos Stefan Munteanu, Quamrul Ti-
pu, Michel Galley, andMark Hopkins. 2004. Arab-
ic and Chinese MT at USC/ISI. Presentation given 
at NIST Machine Translation Evaluation Work-
shop.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In proceedings of the 43th 
Meeting of the Association for Computational 
Linguistics, 271-279 
S. Shieber and Y. Schabes. 1990. Synchronous tree 
adjoining grammars. In proceedings of COLING-
90. 
Christoph Tillmann. 2004. A block orientation model 
for statistical machine translation. In HLT-
NAACL, Boston, MA, USA. 
Ashish Venugopal, Stephan Vogel and Alex Waibel. 
2003. Effective Phrase Translation Extraction 
from Alignment Models, in Proceedings of the 41st 
ACL,  319-326. 
Dekai Wu. 1995. Stochastic inversion transduction 
grammars, with application to segmentation, 
bracketing, and alignment of parallel corpora. In 
proceeding of IJCAL 1995, 1328-1334,Montreal, 
August.  
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum Entropy Based phrase reordering model for 
statistical machine translation. In proceedings of 
COLING-ACL, Sydney, Australia. 
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun 
Liu and Shouxun Lin. Refinements in BTG-based 
Statistical Machine Translation. In Proceedings of 
IJCNLP 2008. 
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In proceedings 
of the 39th Meeting of the ACL, 523-530. 
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. 
Reordering Constraints for Phrase-Based Statis-
tical MachineTranslation. In Proceedings of CoL-
ing 2004, Geneva, Switzerland, pp. 205-211. 
455
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173?1181,
Beijing, August 2010
 A Character-Based Joint Model for Chinese Word Segmentation
Kun Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation
 
Kysu@bdc.com.tw 
 
Abstract 
The character-based tagging approach 
is a dominant technique for Chinese 
word segmentation, and both discrimi-
native and generative models can be 
adopted in that framework. However, 
generative and discriminative charac-
ter-based approaches are significantly 
different and complement each other. 
A simple joint model combining the 
character-based generative model and 
the discriminative one is thus proposed 
in this paper to take advantage of both 
approaches. Experiments on the Sec-
ond SIGHAN Bakeoff show that this 
joint approach achieves 21% relative 
error reduction over the discriminative 
model and 14% over the generative one. 
In addition, closed tests also show that 
the proposed joint model outperforms 
all the existing approaches reported in 
the literature and achieves the best F-
score in four out of five corpora. 
1 Introduction 
Chinese word segmentation (CWS) plays an 
important role in most Chinese NLP applica-
tions such as machine translation, information 
retrieval and question answering. Many statis-
tical methods for CWS have been proposed in 
the last two decades, which can be classified as 
either word-based or character-based. The 
word-based approach regards the word as the 
basic unit, and the desired segmentation result 
is the best word sequence found by the search 
process. On the other hand, the character-based 
approach treats the word segmentation task as 
a character tagging problem. The final segmen-
tation result is thus indirectly generated ac-
cording to the tag assigned to each associated 
character. Since the vocabulary size of possible 
character-tag-pairs is limited, the character-
based models can tolerate out-of-vocabulary 
(OOV) words and have become the dominant 
technique for CWS in recent years. 
On the other hand, statistical approaches can 
also be classified as either adopting a genera-
tive model or adopting a discriminative model. 
The generative model learns the joint probabil-
ity of the given input and its associated label 
sequence, while the discriminative model 
learns the posterior probability directly. Gen-
erative models often do not perform well be-
cause they make strong independence assump-
tions between features and labels. However, 
(Toutanova, 2006) shows that generative mod-
els can also achieve very similar or better per-
formance than the corresponding discrimina-
tive models if they have a structure that avoids 
unrealistic independence assumptions.  
In terms of the above dimensions, methods 
for CWS can be classified as:  
1) The word-based generative model (Gao et 
al., 2003; Zhang et al, 2003), which is a well-
known approach and has been used in many 
successful applications;  
2) The word-based discriminative model 
(Zhang and Clark, 2007), which generates 
word candidates with both word and character 
features and is the only word-based model that 
adopts the discriminative approach? 
3) The character-based discriminative model 
(Xue, 2003; Peng et al, 2004; Tseng et al, 
2005; Jiang et al, 2008), which has become 
the dominant method as it is robust on OOV 
words and is capable of handling a range of 
different features, and it has been adopted in 
many previous works;  
1173
4) The character-based generative model 
(Wang et al, 2009), which adopts a character-
tag-pair-based n-gram model and achieves 
comparable results with the popular character-
based discriminative model. 
In general, character-based models are much 
more robust on OOV words than word-based 
approaches do, as the vocabulary size of char-
acters is a closed set (versus the open set of 
that of words). Furthermore, among those 
character-based approaches, the generative 
model and the discriminative one complement 
each other in handling in-vocabulary (IV) 
words and OOV words. Therefore, a character-
based joint model is proposed to combine them. 
This proposed joint approach has achieved 
good balance between IV word recognition 
and OOV word identification. The experiments 
of closed tests on the second SIGHAN Bakeoff 
(Emerson, 2005) show that the joint model 
significantly outperforms the baseline models 
of both generative and discriminative ap-
proaches. Moreover, statistical significance 
tests also show that the joint model is signifi-
cantly better than all those state-of-the-art sys-
tems reported in the literature and achieves the 
best F-score in four of the five corpora tested. 
2 Character-Based Models for CWS 
The goal of CWS is to find the corresponding 
word sequence for a given character sequence. 
Character-based model is to find out the corre-
sponding tags for given character sequence. 
2.1 Character-Based Discriminative Model 
The character-based discriminative model 
(Xue, 2003) treats segmentation as a tagging 
problem, which assigns a corresponding tag to 
each character. The model is formulated as: 
1
1 1 1 1 2
1 1
( ) ( , ) (
n n
n n k n k
k k
k k
P t c P t t c P t c? ?
= =
= ?? ? 2 )k +        (1) 
Where tk is a member of {Begin, Middle, End, 
Single} (abbreviated as B, M, E and S from 
now on) to indicate the corresponding position 
of character ck in its associated word. For ex-
ample, the word ???? (Beijing City)? will 
be assigned with the corresponding tags as: ??
/B (North) ?/M (Capital) ?/E (City)?.  
Since this tagging approach treats characters 
as basic units, the vocabulary size of those 
possible character-tag-pairs is limited. There-
fore, this method is robust to OOV words and 
could possess a high recall of OOV words 
(ROOV). Although the dependency between ad-
jacent tags/labels can be addressed, the de-
pendency between adjacent characters within a 
word cannot be directly modeled under this 
framework. Lower recall of IV words (RIV) is 
thus usually accompanied (Wang et al, 2009).  
In this work, the character-based discrimina-
tive model is implemented by adopting the fea-
ture templates given by (Ng and Low, 2004), 
but excluding those ones that are forbidden by 
the closed test regulation of SIGHAN (e.g., 
Pu(C0): whether C0 is a punctuation). Those 
feature templates adopted are listed below: 
1
1 1
( ) ( 2, 1,0,1, 2);
( ) ( 2, 1,0,1);
( )
n
n n
a C n
b C C n
c C C
+
?
= ? ?
= ? ?  
For example, when we consider the third 
character ??? in the sequence ???????, 
template (a) results in the features as following: 
C-2=?, C-1=?, C0=?, C1=?, C2=?, and tem-
plate (b) generates the features as: C-2C-1=??, 
C-1C0=??, C0C1=??, C1C2=??, and tem-
plate (c) gives the feature C-1C1=??. 
2.2 Character-Based Generative Model 
To incorporate the dependency between adja-
cent characters in the character-based approach, 
(Wang et al, 2009) proposes a character-based 
generative model. In this approach, word wi is 
first replaced with its corresponding sequence 
of [character, tag] (denoted as [c, t]), where tag 
is the same as that adopted in the above char-
acter-based discriminative model. With this 
representation, this model can be expressed as:  
 
1 1 1 1
1 1 1 1
( ) ([ , ] )
( [ , ] ) ([ , ] ) ( )
m n n n
n n n n
P w c P c t c
P c c t P c t P c
?
= ?                    (2) 
Since 1 1( [ , ] ) 1
n nP c c t ?  and  is the same for 
various candidates, only should be 
considered. It can be further simplified with 
Markov Chain assumption as: 
1( )
nP c
([ ,P c 1] )
nt
 11
1
([ , ] ) ([ , ] [ , ] ).
n
n
i i k
i
P c t P c t c t ??
=
?? i                     (3) 
Compared with the character-based dis-
criminative model, this generative model keeps 
the capability to handle OOV words because it 
also regards the character as basic unit. In ad-
dition, the dependency between adjacent 
1174
? Gold and Discriminative Tag: M Generative Trigram Tag: E 
Tag probability:  B/0.0333 E/0.2236 M/0.7401 S/0.0030 
Feature 
Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1
B -1.4375 0.1572 0.0800 0.2282 0.7709 0.2741 0.0000 0.0000 -0.6718 0.0000
E 1.3558 0.1910 0.7229 -1.2696 -0.5970 0.0049 0.0921 0.0000 0.8049 0.0000
M 1.1071 -0.5527 -0.3174 2.9422 0.4636 -0.1708 0.0000 0.0000 -0.9700 0.0000
S -1.0254 0.2046 -0.4856 -1.9008 -0.6375 0.0000 0.0000 0.0000 0.8368 0.0000
? Gold and Discriminative Tag: E Generative Trigram Tag: S 
Tag probability:  B/0.0009 E/0.8138 M/0.0012 S/0.1841 
Feature 
Tag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1
B 0.3586 0.4175 0.0000 -0.7207 0.4626 0.0085 0.0000 0.0000 0.0000 0.0000
E 0.3666 0.0687 4.5381 2.8300 -0.0846 0.0000 0.0000 -1.0279 0.6127 0.0000
M -0.5657 -0.4330 1.8847 0.0000 -0.0918 0.0000 0.0000 0.0000 0.0000 0.0000
S -0.1595 -0.0532 2.7360 1.8223 -0.2862 -0.0024 0.0000 1.0494 0.7113 0.0000
Table 1: The corresponding lambda weight of features for ????? in the sentence ?[?] [?] [?] [???] 
[?] [?] [?] [?]?. In the Feature column and Tag row, the value is the corresponding lambda weight for 
the feature and tag under ME framework. The meanings of those features are explained in Section 2.1. 
 
characters is now directly modeled. This will 
give sharper preference when the history of 
assignment is given. Therefore, this approach 
not only holds robust IV performance but also 
achieves comparable results with the discrimi-
native model. However, the OOV performance 
of this approach is still lower than that of the 
discriminative model (see in Table 5), which 
would be discussed in the next section. 
3 Problems with the Character-Based 
Generative Model 
The character-based generative model can 
handle the dependency between adjacent char-
acters and thus performs well on IV words. 
However, this generative trigram model is de-
rived under the second order Markov Chain 
assumption. Future character context (i.e., C1 
and C2) is thus not utilized in the model when 
the tag of the current character (i.e., t0) is de-
termined. Nevertheless, the future context 
would help to select the correct tag when the 
associated trigram has not been observed in the 
training-set, which is just the case for those 
OOV words. In contrast, the discriminative 
one could get help from the future context in 
this case. The example given in the next para-
graph clearly shows the above situation. 
At the sentence ??(that) ?(place) ?(of) ?
??(street sleeper) ?(only) ?(have) ?(some) 
?(person) (There are only some street sleepers 
in that place)? in the CITYU corpus, ??/B?
/M?/E(street sleeper)? is observed to be an 
OOV word, while ?? /B? /E(sleep on the 
street)? is an IV word, where the associated tag 
of each character is given after the slash sym-
bol. The character-based generative model 
wrongly splits ????? into two words ??/B
?/E? and ??/S (person)?, as the associated 
trigram for ????? is not seen in the training 
set. However, the discriminative model gives 
the correct result for ??/M? and the dominant 
features come from its future context ??? and 
???. Similarly, the future context ??? helps 
to give the correct tag to ??/E?. Table 1 gives 
the corresponding lambda feature weights (un-
der the Maximum Entropy (ME) (Ratnaparkhi, 
1998) framework) for ????? in the dis-
criminative model. It shows that in the column 
of ?C1? below ???, the lambda value associ-
ated with the correct tag ?M? is 2.9422, which 
is the highest value in that column and is far 
greater than that of the wrong tag ?E? (i.e., -
1.2696) assigned by the generative model. 
Which indicates that the future feature ?C1? is 
the most useful feature for tagging ???. 
The above example shows the character-
based generative model fails to handle some 
OOV words such as ????? because this ap-
proach cannot utilize future context when it is 
indeed required. However, the future context 
for the generative model scanning from left to 
right is just its past context when it scans from 
right to left. It is thus expected that this kind of 
1175
errors will be fixed if we let the model scans 
from both directions, and then combine their 
results. Unfortunately, it is observed that these 
two scanning modes share over 90% of their 
errors. For example, in CITYU corpus, the 
left-to-right scan generates 1,958 wrong words 
and the right-to-left scan results 1,947 ones, 
while 1,795 of them are the same. Similar be-
havior can also be observed on other corpora. 
To find out what are the problems, 10 errors 
that are similar to ????? are selected to ex-
amine. Among those errors, only one of them 
is fixed, and ????? still cannot be correctly 
segmented. Having analyzed the scores of the 
model scanning from both directions, we found 
that the original scores (from left-to-right scan) 
at the stages ??? and ??? indeed get better if 
the model scans from right-to-left. However, 
the score at the stage ??? deteriorates because 
the useful feature ??? (a past non-adjacent 
character for ??? when scans form right-to-
left) still cannot be utilized when the past con-
text ???? as a whole is unseen, when the re-
lated probabilities are estimated via modified 
Kneser-Ney smoothing (Chen and Goodman, 
1998) technique. 
Two scanning modes seem not complement-
ing each other, which is out of our original ex-
pectation. However, we found that the charac-
ter-based generative model and the discrimina-
tive one complement each other much more 
than the two scanning modes do. It is observed 
that these two approaches share less than 50% 
of their errors. For example, in CITYU corpus, 
the generative approach generates 1,958 wrong 
words and the discriminative one results 2,338 
ones, while only 835 of them are the same. 
The statistics of the remaining errors re-
sulted from the generative model and the dis-
criminative model is shown in Table 2. As 
shown in the table, it can be seen that the gen-
erative model and the discriminative model 
complement each other on handling IV words 
and OOV words (In the ?IV Errors? column, 
the number of ?G+D-? is much more than the 
?G-D+?, while the behavior is reversed in the 
?OOV Errors? column). 
4 Proposed Joint Model 
Since the performance of both IV words and 
OOV words are important for real applications, 
IV Errors OOV Errors 
G+D- G-D+ G-D- G+D- G-D+ G-D-
12,027 4,723 7,481 2,384 6,139 3,975
Table 2: Statistics for remaining errors of the char-
acter-based generative model and the discriminative 
one on the second SIGHAN Bakeoff (?G+D-? in 
the ?IV Errors? column means that the generative 
model segments the IV words correctly but the dis-
criminative one gives wrong results. The meanings 
of other abbreviations are similar with this one.). 
we need to combine the strength from both 
models. Among various combining methods, 
log-linear interpolation combination is a sim-
ple but effective one (Bishop, 2006). Therefore, 
the following character-based joint model is 
proposed, and a parameter ?  is used to weight 
the generative model in a cross-validation set. 
 
1
2
2
2
( ) log( ([ , ] [ , ] ))
(1 ) log( ( ))
k
k k
k
k k
Score t P c t c t
P t c
?
?
?
?
+
?
= ?
+ ? ?
k
           (4) 
Where tk indicates the corresponding position 
of character ck, and (0.0 1.0)? ?? ?  is the 
weight for the generative model. Score(tk) will 
be used during searching the best sequence. It 
can be seen that these two models are inte-
grated naturally as both are character-based. 
Generally speaking, if the ?G(or D)+? has a 
strong preference on the desired candidate, but 
the ?D(or G)-? has a weak preference on its 
top-1 incorrect candidate, then this combining 
method would correct most ?G+D- (also  G-
D+)? errors. On the other hand, the advantage 
of combining two models would vanish if the 
?G(or D)+? has a weak preference while the 
?D(or G)-? has a strong preference over their 
top-1 candidates. In our observation, these two 
models meet this requirement quite well. 
5 Weigh Various Features Differently 
For a given observation, intuitively each 
feature should be trained only once under the 
ME framework and its associated weight will 
be automatically learned from the training cor-
pus. However, when we repeat the work of 
(Jiang et al, 2008), which reports to achieve 
the state-of-art performance in the data-sets 
that we adopt, it has been found that some fea-
tures (e.g., C0) are unnoticeably trained several 
times in their model (which are implicitly gen-
erated from different feature templates used in 
the paper). For example, the feature C0 actually 
1176
Corpus Abbrev. Encoding Training Size(Words/Type)
Test Size 
(Words/Type) OOV Rate
Academia Sinica (Taipei) AS Unicode/Big5 5.45M/141K 122K/19K 0.046 
City University of Hong Kong CITYU Unicode/Big5 1.46M/69K 41K/9K 0.074 
Microsoft Research (Beijing) MSR Unicode/CP936 2.37M/88K 107K/13K 0.026 
PKU(ucvt.) Unicode/CP936 1.1M/55K 104K/13K 0.058 Peking University 
PKU(cvt.) Unicode/CP936 1.1M/55K 104K/13K 0.035 
Table 3: Corpus statistics for the second SIGHAN Bakeoff 
 
appears twice, which is generated from two 
different templates Cn (with n=0, generates C0) 
and [C0Cn] (used in (Jiang et al, 2008), with 
n=0, generates [C0C0]). The meanings of fea-
tures are illustrated in Section 2.1. Those re-
petitive features also include [C-1C0] and 
[C0C1], which implicitly appear thrice. And it 
is surprising to discover that its better perform-
ance is mainly due to this implicit feature repe-
tition but the authors do not point out this fact. 
As all the features adopted in (Jiang et al, 
2008) possess binary values, if a binary feature 
is repeated n times, then it should behave like a 
real-valued feature with its value to be ?n?, at 
least in principle. Inspired by the above dis-
covery, accordingly, we convert all the binary-
value features into their corresponding real-
valued features. After having transformed bi-
nary features into their corresponding real-
valued ones, the original discriminative model 
is re-trained under the ME framework. 
This new implementation, which would be 
named as the character-based discriminative-
plus model, just weights various features dif-
ferently before conducting ME training. Af-
terwards, it is further combined with the gen-
erative trigram model, and is called the charac-
ter-based joint-plus model. 
6 Experiments 
The corpora provided by the second SIGHAN 
Bakeoff (Emerson, 2005) were used in our ex-
periments. The statistics of those corpora are 
shown in Table 3. 
Note that the PKU corpus is a little different 
from others. In the training set, Arabic num-
bers and English characters are in full-width 
form occupying two bytes. However, in the 
testing set, these characters are in half-width 
form occupying only one byte. Most research-
ers in the SIGHAN Bakeoff competition per-
formed a conversion before segmentation 
(Xiong et al, 2009). In this work, we conduct 
the tests on both unconverted (ucvt.) case and 
converted (cvt.) case. After the conversion, the 
OOV rate of converted corpus is obviously 
lower than that of unconverted corpus. 
To fairly compare the proposed approach 
with previous works, we only conduct closed 
tests1. The metrics Precision (P), Recall (R), 
F-score (F) (F=2PR/(P+R)), Recall of OOV 
(ROOV) and Recall of IV (RIV) are used to 
evaluate the results. 
6.1 Character-Based Generative Model 
and Discriminative Model 
As shown in (Wang et al, 2009), the character-
based generative trigram model significantly 
exceeds its related bigram model and performs 
the same as its 4-gram model. Therefore,  SRI 
Language Modeling  Toolkit2 (Stolcke, 2002) 
is used to train the trigram model with modi-
fied Kneser-Ney smoothing (Chen and Good-
man, 1998). Afterwards, a beam search de-
coder is applied to find out the best sequence. 
For the character-based discriminative 
model, the ME Package3 given by Zhang Le is 
used to conduct the experiments. Training was 
done with Gaussian prior 1.0 and 300, 150 it-
erations for AS and other corpora respectively.  
Ta
                                                
ble 5 gives the segmentation results of both 
the character-based generative model and the 
discriminative model. From the results, it can 
be seen that the generative model achieves 
comparable results with the discriminative one 
and they outperform each other on different 
corpus. However, the generative model ex-
ceeds the discriminative one on RIV (0.973 vs. 
0.956) but loses on ROOV (0.511 vs. 0.680). It 
illustrates that they complement each other. 
 
1 According to the second Sighan Bakeoff regulation, the 
closed test could only use the training data directly pro-
vided. Any other data or information is forbidden, includ-
ing the knowledge of characters set, punctuation set, etc. 
2 http://www.speech.sri.com/projects/srilm/ 
3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
1177
Joint model performance on Development sets
0.9300
0.9400
0.9500
0.9600
0.9700
0.9800
0.9900
0.0
0
0.1
0
0.2
0
0.3
0
0.4
0
0.5
0
0.6
0
0.7
0
0.8
0
0.9
0
1.0
0
alpha
F-
sc
or
e
AS
CITYU
MSR
PKU
 
Figure 1: Development sets performance of Charac-
ter-based joint model. 
Corpus Set Words  OOV Num OOV Rate
Development 17,243 445 0.026  AS 
Testing 122,610 5,308/5,311 0.043/0.043
Development 17,324 355 0.020 MSR 
Testing 106,873 2,829/2,833 0.026/0.027
Development 12,075 537 0.044 CITYU 
Testing 40,936 3,028/3,034 0.074/0.074
Development 13,576 532 0.039 
Testing (ucvt.) 104,372 6,006/6,054 0.058/0.058PKU 
Testing (cvt.) 104,372 3,611/3,661 0.035/0.035
Table 4: Corpus statistics for Development sets and 
Testing sets. A ?/? separates the OOV number (or 
OOV rate) with respect to the original training sets 
and the new training sets. 
6.2 Character-Based Joint Model 
For the character-based joint model, a devel-
opment set is required to obtain the weight ?  
for its associated generative model. A small 
portion of each original training corpus is thus 
extracted as the development set and the re-
maining data is regarded as the new training-
set, which is used to train two new parameter-
sets for both generative and discriminative 
models associated.  
The last 2,000, 600, 400, and 300 sentences 
for AS, MSR, CITYU, and PKU are extracted 
from the original training corpora as their cor-
responding development sets. The statistics for 
new data sets are shown in Table 4. It can be 
seen that the variation of the OOV rate could 
be hardly noticed. The F-scores of the joint 
model, versus different ? , evaluated on four 
development sets are shown in Figure 1. It can 
be seen that the curves are not sharp but flat 
near the top, which indicates that the character-
based joint model is not sensitive to the ?  
value selected. From those curves, the best 
suitable ?  for AS, CITYU, MSR and PKU are 
found to be 0.30, 0.60, 0.60 and 0.60, respec-
Corpus Model R P F ROOV RIV
tively. Those alpha values will then be adopted 
to conduct the experiments on the testing sets. 
G 0.958 0.938 0.948 0.518 0.978
D 0 0.946 0  0.967.955 .951 0.707 
D-Plus 0.960 0.948 0.954 0.680 0.973
J 0.962 0.950 0.956 0.679 0.975
AS 
J-Plus 0.963 0.949 0.956 0.652 0.977
G 0.951 0.937 0.944 0.609 0.978
D 0.941 0.944 0.942 0.708 0.959
D-Plus 0.951 0.952 0.952 0.720 0.970
J 0.957 0.951 0.954 0.691 0.979
CITYU
J-Plus 0.959 0.952 0.956 0.700 0.980
G 0.974 0.967 0.970 0.561 0.985
D 0.957 0.962 0.960 0.719 0.964
D-Plus 0.965 0.967 0.966 0.675 0.973
J 0.974 0.971 0.972 0.659 0.983
MSR 
J-Plus 0.975 0.970 0.972 0.632 0.984
G 0.929 0.933 0.931 0.435 0.959
D 0.922 0.941 0.932 0.620 0.941
D-Plus 0.934 0.949 0.941 0.649 0.951
J 0.935 0.946 0.941 0.561 0.958
PKU 
(ucvt.) 
J-Plus 0.937 0.947 0.942 0.556 0.960
G 0.952 0.951 0.952 0.503 0.968
D 0.940 0.951 0.946 0.685 0.949
D-Plus 0.949 0.958 0.953 0.674 0.958
J 0.954 0.958 0.956 0.616 0.966
PKU 
(cvt.) 
J-Plus 0.955 0.958 0.957 0.610 0.967
G 0.953 0.946 0.950 0.511 0.973
D 0.944 0.950 0.947 0.680 0.956
D-Plus 0.952 0.955 0.953 0.676 0.965
J 0.957 0.955 0.956 0.633 0.971
Overall
J-Plus 0.958 0.955 0.957 0.621 0.973
Table 5: ent e
based m n t G  
ificantly outperforms both the character-
ba
 Segm
odels o
ation r sults of various character-
he second SI HAN Bakeoff, the
generative trigram model (G), the discriminative 
model (D), the discriminative-plus model (D-Plus), 
the joint model (J) and the joint-plus model (J-Plus). 
 
As shown in Table 5, the joint model sig-
n
sed generative model and the discriminative 
one in F-score on all the testing corpora. Com-
pared with the generative approach, the joint 
model increases the overall ROOV from 0.510 to 
0.633, with the cost of slightly degrading the 
overall RIV from 0.973 to 0.971. This shows 
that the joint model holds the advantage of the 
generative model on IV words. Compared with 
the discriminative model, the proposed joint 
model improves the overall RIV from 0.956 to 
0.971, with the cost of degrading the overall 
ROOV from 0.680 to 0.633. It clearly shows that 
the joint model achieves a good balance be-
tween IV words and OOV words and achieves 
the best F-scores obtained so far (21% relative 
error reduction over the discriminative model 
and 14% over the generative model). 
1178
6.3 Weigh Various Features Differently 
Inspired by (Jiang et al, 2008), we set the real-
d 
 
Although Table 5 has shown that the proposed 
all the 
value of C0 to be 2.0, the value of C-1C0 an
C0C1 to be 3.0, and the values of all other fea-
tures to be 1.0 for the character-based dis-
criminative-plus model. Although it seems rea-
sonable to weight those closely relevant fea-
tures more (C0 should be the most relevant fea-
ture for assigning tag t0), both implementations 
seem to be equal if their corresponding 
lambda-values are also updated accordingly. 
However, Table 5 shows that this new dis-
criminative-plus implementation (D-Plus) sig-
nificantly outperforms the original one (overall 
F-score is raised from 0.947 to 0.953) when 
both of them adopt real-valued features. It is 
not clear how this change makes the difference. 
Similar improvements can be observed with 
two other ME packages. One anonymous re-
viewer pointed out that the duplicated features 
should not make difference if there is no regu-
larization. However, we found that the dupli-
cated features would improve the performance 
whether we give Gaussian penalty or not. 
Afterwards, this new implementation and 
the generative trigram model are further com-
bined (named as the joint-plus model). Table 5 
shows that this joint-plus model also achieves 
better results compared with the discrimina-
tive-plus model, which illustrates that our joint 
approach is an effective and robust method for 
CWS. However, compared with the original 
joint model, the new joint-plus approach does 
not show much improvement, regardless of the 
significant improvement made by the discrimi-
native-plus model, as the additional benefit 
generated by the discriminative-plus model has 
already covered by the generative approach 
(Among the 6,965 error words corrected by the 
discriminative-plus model, 6,292 (90%) of 
them are covered by the generative model). 
7 Statistical Significance Tests 
joint (joint-plus) model outperforms 
baselines mentioned above, we want to know 
if the difference is statistically significant 
enough to make such a claim. Since there is 
only one testing set for each training corpus, 
the bootstrapping technique (Zhang et al, 2004) 
is adopted to conduct the tests: Giving an  
Models  
A B 
AS CITYU MSR PKU (ucvt.) 
PKU
(cvt.)
G D <  ~ >  ~ >  
D-Plus G >  >  <  >  >  
D-Plus D >  >  >  >  >  
J G >  >  >  >  >  
J D >  >  >  >  >  
J-Plus G >  >  >  >  >  
J-Plus D-Plus >  >  >  ~ >  
J-Plus J ~ >  ~ >  >  
Table 6 atistic sign anc est F- e 
 v er-b d m ls. 
f T0) will 
be generated by repeatedly re-sampling data 
eas-
 the dis-
he confi-
 
the pro-
po
e-
ng 
d. 
tegory 
includes (Asahara et al, 2005) (denoted as 
: St al ific e t of scor
among arious charact ase ode
testing-set T0, additional M-1 new testing-sets 
T0,?,TM-1 (each with the same size o
from T0. Then, we will have a total of M 
testing-sets (M=2000 in our experiments). 
7.1 Comparisons with Baselines 
We then follow (Zhang et al, 2004) to m
ure the 95% confidence interval for
crepancy between two models. If t
dence interval does not include the origin point,
we then claim that system A is significantly 
different from system B. Table 6 gives the re-
sults of significant tests among various models 
mentioned above. In this table, ?>? means that 
system A is significantly better than B, where 
as ?<? denotes that system A is significantly 
worse than B, and ?~? indicates that these two 
systems are not significantly different. 
As shown in Table 6, the proposed joint 
model is significantly better than the two base-
line models on all corpora. Similarly, 
sed joint-plus model also significantly out-
performs the generative model and the dis-
criminative-plus model on all corpora except 
on the PKU(ucvt.). The comparison shows that 
the proposed joint (also joint-plus) model in-
deed exceeds each of its component models. 
7.2 Comparisons with Previous Works 
The above comparison mainly shows the sup
riority of the proposed joint model amo
those approaches that have been implemente
However, it would be interesting to know if the 
joint (and joint-plus) model also outperforms 
those previous state-of-the-art systems.  
The systems that performed best for at least 
one corpus in the second SIGHAN Bakeoff are 
first selected for comparison. This ca
1179
A-sets. In-
st
th
                                                
sahara05) and (Tseng et al, 2005) 4  
(Tseng05). (Asahara et al, 2005) achieves the 
best result in the AS corpus, and (Tseng et al, 
2005) performs best in the remaining three 
corpora. Besides, those systems that are re-
ported to exceed the above two systems are 
also selected. This category includes (Zhang et 
al., 2006) (Zhang06), (Zhang and Clark, 2007) 
(Z&C07) and (Jiang et al, 2008) (Jiang08). 
They are briefly summarized as follows. 
(Zhang et al, 2006) is based on sub-word tag-
ging and uses a confidence measure method to 
combine the sub-word CRF (Lafferty et al, 
2001) and rule-based models. (Zhang and 
Clark, 2007) uses perceptron (Collins, 2002) to 
generate word candidates with both word and 
character features. Last, (Jiang et al, 2008)5  
adds repeated features implicitly based on (Ng 
and Low, 2004). All of the above models, ex-
cept (Zhang and Clark, 2007), adopt the char-
acter-based discriminative approach. 
All the results of the systems mentioned 
above are shown in Table 7. Since the systems 
are not re-implemented, we cannot generate 
paired samples from those M testing
ead, we calculate the 95% confidence inter-
val of the joint (also joint-plus) model. After-
wards, those systems can be compared with 
our proposed models. If the F-score of system 
B does not fall within the 95% confidence in-
terval of system A (joint or joint-plus), then 
they are statistically significantly different. 
Table 8 gives the results of significant tests 
for those systems mentioned in this section. It 
shows that both our joint-plus model and joint 
model exceed (or are comparable to) almost all 
e state-of-the-art systems across all corpora, 
except (Zhang and Clark, 2007) at PKU(ucvt.). 
In that special case, (Zhang and Clark, 2007) 
 
4 We are not sure whether (Asahara et al, 2005) and 
(Tseng et al, 2005) performed a conversion before seg-
mentation in PKU corpus. In this paper, we followed 
previous works, which cited and compared with them. 
5 The data for (Jiang et al, 2008) given at Table 7 are 
different from what were reported at their paper. In the 
communication with the authors, it is found that the script 
for evaluating performance, provided by the SIGHAN 
Bakeoff, does not work correctly in their platform. After 
the problem is fixed, the re-evaluated real performances 
reported here deteriorate from their original version. 
Please see the announcement in Jiang?s homepage 
(http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_corre
ction.pdf). 
Corpus
Participants AS CITYU MSR 
PKU 
(ucvt.) 
PKU
(cvt.)
Asahara05 0.952 0.941 0.958 N/A 0.941
Tseng05 0.947 0.943 0.964 N/A 0.950
Zhang06 0.951 0.951 0.971 N/A 0.951
Z&C07 0.946 0.951 0.972 0.945 N/A
Jiang08 0.953 0.948 0.966 0.937 N/A
Our Joint 0.956 0.954 0.972 0.941 0.956
Our Joint-Plus 0.956 0.956 0.972 0.942 0.957
Table 7: Compari r  p u
the-art sy
sons of F-sco e with revio s 
state-of- stems. 
Systems 
A B 
AS CITYU MSR (ucvt.)
PKU 
 (cvt.)
PKU
Asahara05 > > > N/A > 
Tseng05 > > > N/A > 
Zhang06 > ~ ~ N/A > 
Z&C07 > > ~ < N/A
J 
Jiang08 > > > > N/A
Asahara05 > > > N/A > 
Tseng05 > > > N/A > 
Zhang06 > > ~ N/A > 
Z&C07 > > ~ < N/A
J-Plus
Jiang08 ~ > > > N/A
Table al s ific e te of r 
f-the  syst s. 
outpe he jo -plu model by .3%  
 and 0.5%, re-
ne, 
e two models complement 
dling IV words and OOV 
e-
nomenon.  
8: Statistic ign anc st  F-score fo
previous state-o -art em
rforms t int s  0  on
F- score (0.4% for the joint model). However, 
our joint-plus model exceeds it more over AS 
and CITYU corpora by 1.0%
spectively (1.0% and 0.3% for the joint model). 
Thus, it is fair to say that both our joint model 
and joint-plus model are superior to the state-
of-the-art systems reported in the literature. 
8 Conclusion 
From the error analysis of the character-based 
generative model and the discriminative o
we found that thes
each other on han
words. To take advantage of these two ap-
proaches, a joint model is thus proposed to 
combine them. Experiments on the Second 
SIGHAN Bakeoff show that the joint model 
achieves 21% error reduction over the dis-
criminative model (14% over the generative 
model). Moreover, closed tests on the second 
SIGHAN Bakeoff corpora show that this joint 
model significantly outperforms all the state-
of-the-art systems reported in the literature. 
Last, it is found that weighting various fea-
tures differently would give better result. How-
ever, further study is required to find out the 
true reason for this strange but interesting ph
1180
A Generic-Beam-Search code and 
o Ms. Nanyan Kuo for 
eric-Beam-Search code.  
m
optimum 
entation. In Proceedings of 
GHAN Workshop on Chinese Lan-
St
Th
Jia
W
Jo
Hw
Fu
ction using conditional random fields. 
Ad
MNLP, pages 
Hu
ld Word Segmenter 
Ku d Keh-Yih Su, 2009. 
Yi
scriminative 
Ni
ssing, 8 (1). pages 
Hu
Second 
Ru
2006. Subword-based Tagging for Con-
Yi  
 scores: How much im-
Yu
f ACL, pages 840-847, 
cknowledgement 
The authors extend sincere thanks to Wenbing 
Jiang for his helps with our experiments. Also, 
we thank Behavior Design Corporation for 
using their
show special thanks t
her helps with the Gen
The research work has been partially funded 
by the Natural Science Foundation of China 
under Grant No. 60975053, 90820303 and 
60736014, the National Key Technology R&D 
Program under Grant No. 2006BAH03B02, 
and also the Hi-Tech Research and Develop-
ent Program (?863? Program) of China under 
Grant No. 2006AA010108-4 as well. 
References 
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, 
Chooi-Ling Goh, Yotaro Watanabe, Yuji Ma-
tsumoto and Takashi Tsuzuki, 2005. Combina-
tion of machine learning methods for 
Chinese word segm
the Fourth SI
guage Processing, pages 134?137, Jeju, Korea. 
Christopher M. Bishop, 2006. Pattern recognition 
and machine learning. New York: Springer  
anley F. Chen and Joshua Goodman, 1998. An 
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, 
Harvard University Center for Research in 
Computing Technology. 
Michael Collins, 2002. Discriminative training 
methods for hidden markov models: theory and 
experiments with perceptron algorithms. In Pro-
ceedings of EMNLP, pages 1-8, Philadelphia. 
omas Emerson, 2005. The second international 
Chinese word segmentation bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chi-
nese Language Processing, pages 123-133. 
nfeng Gao, Mu Li and Chang-Ning Huang, 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of ACL, 
pages 272-279. 
enbin Jiang, Liang Huang, Qun Liu and Yajuan 
Lu, 2008. A Cascaded Linear Model for Joint 
Chinese Word Segmentation and Part-of-Speech 
Tagging. In Proceedings of ACL, pages 897-904. 
hn Lafferty, Andrew McCallum and Fernando 
Pereira, 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling 
Sequence Data. In Proceedings of ICML, pages 
282-289. 
ee Tou Ng and Jin Kiat Low, 2004. Chinese 
part-of-speech tagging: one-at-a-time or all-at-
once? word-based or character-based. In Pro-
ceedings of EMNLP, pages 277-284. 
chun Peng, Fangfang Feng and Andrew 
McCallum, 2004. Chinese segmentation and new 
word dete
In Proceedings of COLING, pages 562?568. 
wait Ratnaparkhi, 1998. Maximum entropy 
models for natural language ambiguity resolu-
tion. University of Pennsylvania. 
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the 
International Conference on Spoken Language 
Processing, pages 311-318. 
Kristina Toutanova, 2006. Competitive generative 
models with structure learning for NLP classifi-
cation tasks. In Proceedings of E
576-584, Sydney, Australia. 
ihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. 
A Conditional Random Fie
for Sighan Bakeoff 2005. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 168-171. 
n Wang, Chengqing Zong an
Which is more suitable for Chinese word seg-
mentation, the generative model or the discrimi-
native one? In Proceedings of PACLIC, pages 
827-834, Hong Kong, China. 
ng Xiong, Jie Zhu, Hao Huang and Haihua Xu, 
2009. Minimum tag error for di
training of conditional random fields. Informa-
tion Sciences, 179 (1-2). pages 169-179. 
anwen Xue, 2003. Chinese Word Segmentation 
as Character Tagging. Computational Linguistics 
and Chinese Language Proce
29-48. 
aping Zhang, Hongkui Yu, Deyi Xiong and Qun 
Liu, 2003. HHMM-based Chinese lexical ana-
lyzer ICTCLAS. In Proceedings of the 
SIGHAN Workshop on Chinese Language Proc-
essing, pages 184?187. 
iqiang Zhang, Genichiro Kikui and Eiichiro 
Sumita, 
fidence-dependent Chinese Word Segmentation. 
In Proceedings of the COLING/ACL, pages 961-
968, Sydney, Australia. 
ng Zhang, Stephan Vogel and Alex Waibel, 2004.
Interpreting BLEU/NIST
provement do we need to have a better system. 
In Proceedings of LREC, pages 2051?2054. 
e Zhang and Stephen Clark, 2007. Chinese Seg-
mentation with a Word-Based Perceptron Algo-
rithm. In Proceedings o
Prague, Czech Republic. 
1181
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1362?1370,
Beijing, August 2010
A Minimum Error Weighting Combination Strategy for Chinese
Semantic Role Labeling
Tao Zhuang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{tzhuang, cqzong}@nlpr.ia.ac.cn
Abstract
Many Semantic Role Labeling (SRL)
combination strategies have been pro-
posed and tested on English SRL task.
But little is known about how much Chi-
nese SRL can benefit from system combi-
nation. And existing combination strate-
gies trust each individual system?s output
with the same confidence when merging
them into a pool of candidates. In our ap-
proach, we assign different weights to dif-
ferent system outputs, and add a weighted
merging stage to the conventional SRL
combination architecture. We also pro-
pose a method to obtain an appropriate
weight for each system?s output by min-
imizing some error function on the devel-
opment set. We have evaluated our strat-
egy on Chinese Proposition Bank data set.
With our minimum error weighting strat-
egy, the F1 score of the combined result
achieves 80.45%, which is 1.12% higher
than baseline combination method?s re-
sult, and 4.90% higher than the best in-
dividual system?s result.
1 Introduction
In recent years, Chinese Semantic Role Labeling
has received much research effort (Sun and Juraf-
sky, 2004; Xue, 2008; Che et al, 2008; Ding and
Chang, 2008; Sun et al, 2009; Li et al, 2009).
And Chinese SRL is also included in CoNLL-
2009 shared task (Hajic? et al, 2009). On the data
set used in (Xue, 2008), the F1 score of the SRL
results using automatic syntactic analysis is still
in low 70s (Xue, 2008; Che et al, 2008; Sun et
al., 2009). As pointed out by Xue (Xue, 2008),
the SRL errors are mainly caused by the errors
in automatic syntactic analysis. In fact, Chinese
SRL suffers from parsing errors even more than
English SRL, because the state-of-the-art parser
for Chinese is still not as good as that for En-
glish. And previous research on English SRL
shows that combination is a robust and effective
method to alleviate SRL?s dependency on pars-
ing results (Ma`rquez et al, 2005; Koomen et
al., 2005; Pradhan et al, 2005; Surdeanu et al,
2007; Toutanova et al, 2008). However, the ef-
fect of combination for Chinese SRL task is still
unknown. This raises two questions at least: (1)
How much can Chinese SRL benefit from combi-
nation? (2) Can existing combination strategies
be improved? All existing combination strate-
gies trust each individual system?s output with the
same confidence when putting them into a pool
of candidates. But according to our intuition, dif-
ferent systems have different performance. And
the system that have better performance should
be trusted with more confidence. We can use our
prior knowledge about the combined systems to
do a better combination.
The observations above motivated the work in
this paper. Instead of directly merging outputs
with equal weights, different outputs are assigned
different weights in our approach. An output?s
weight stands for the confidence we have in that
output. We acquire these weights by minimizing
an error function on the development set. And
we use these weights to merge the outputs. In
this paper, outputs are generated by a full parsing
based Chinese SRL system and a shallow parsing
based SRL system. The full parsing based system
1362
use multiple parse trees to generate multiple SRL
outputs. Whereas the shallow parsing based sys-
tem only produce one SRL output. After merging
all SRL outputs, we use greedy and integer lin-
ear programming combination methods to com-
bine the merged outputs.
We have evaluated our combination strategy on
Chinese Propbank data set used in (Xue, 2008)
and get encouraging results. With our minimum
error weighting (MEW) strategy, the F1 score
of the combined result achieves 80.45%. This
is a significant improvement over the best re-
ported SRL performance on this data set, which
is 74.12% in the literature (Sun et al, 2009).
2 Related work
A lot of research has been done on SRL combina-
tion. Most of them focused on English SRL task.
But the combination methods are general. And
they are closely related to the work in this paper.
Punyakanok et al (2004) formulated an Integer
Linear Programming (ILP) model for SRL. Based
on that work, Koomen et al (2005) combined sev-
eral SRL outputs using ILP method. Ma`rquez et
al. (2005) proposed a combination strategy that
does not require the individual system to give a
score for each argument. They used a binary clas-
sifier to filter different systems? outputs. Then
they used a greedy method to combine the can-
didates that pass the filtering process. Pradhan
et al (2005) combined systems that are based on
phrase-structure parsing, dependency parsing, and
shallow parsing. They also used greedy method
when combining different outputs. Surdeanu et
al. (2007) did a complete research on a variety of
combination strategies. All these research shows
that combination can improve English SRL per-
formance by 2?5 points on F1 score. However,
little is known about how much Chinese SRL can
benefit from combination. And, as we will show,
existing combination strategies can still be im-
proved.
3 Individual SRL Systems
3.1 Full Parsing Based System
The full parsing based system utilize full syn-
tactic analysis to perform semantic role labeling.
We implemented a Chinese semantic role label-
ing system similar to the one described in (Xue,
2008). Our system consists of an argument identi-
fication stage and an argument classification stage.
In the argument identification stage, a number of
argument locations are identified in a sentence.
In the argument classification stage, each location
identified in the first stage is assigned a semantic
role label. The features used in this paper are the
same with those used in (Xue, 2008).
Maximum entropy classifier is employed for
both the argument identification and classification
tasks. And Zhang Le?s MaxEnt toolkit1 is used for
implementation.
3.2 Shallow Parsing Based System
The shallow parsing based system utilize shal-
low syntactic information at the level of phrase
chunks to perform semantic role labeling. Sun
et al (2009) proposed such a system on Chinese
SRL and reported encouraging results. The sys-
tem used in this paper is based on their approach.
For Chinese chunking, we adopted the method
used in (Chen et al, 2006), in which chunking is
regarded as a sequence labeling task with IBO2
representation. The features used for chunking
are the uni-gram and bi-gram word/POS tags with
a window of size 2. The SRL task is also re-
garded as a sequence labeling problem. For an
argument with label ARG*, we assign the label
B-ARG* to its first chunk, and the label I-ARG*
to its rest chunks. The chunks outside of any argu-
ment are assigned the label O. The features used
for SRL are the same with those used in the one-
stage method in (Sun et al, 2009).
In this paper, we employ Tiny SVM along with
Yamcha (Kudo and Matsumoto, 2001) for Chi-
nese chunking, and CRF++2 for SRL.
3.3 Individual systems? outputs
The maximum entropy classifier used in full pars-
ing based system and the CRF model used in shal-
low paring based system can both output classi-
fication probabilities. For the full parsing based
system, the classification probability of the ar-
1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit
.html
2http://crfpp.sourceforge.net/
1363
gument classification stage is used as the argu-
ment?s probability. Whereas for the shallow pars-
ing based system, an argument is usually com-
prised of multiple chunks. For example, an argu-
ment with label ARG0 may contain three chunks
labeled as: B-ARG0, I-ARG0, I-ARG0. And each
chunk has a label probability. Thus we have three
probabilities p1, p2, p3 for one argument. In this
case, we use the geometric mean of individual
chunks? probabilities (p1 ? p2 ? p3)1/3 as the ar-
gument?s probability.
As illustrated in Figure 1, in an individual sys-
tem?s output, each argument has three attributes:
its location in sentence loc, represented by the
number of its first word and last word; its semantic
role label l; and its probability p.
Sent: ?????????????????
Args: [ ARG0 ] [pred] [ ARG1 ]
loc: (0, 2) (4, 7)
l: ARG0 ARG1
p: 0.94 0.92
Figure 1: Three attributes of an output argument:
location loc, label l, and probability p.
So each argument outputted by a system is a
triple (loc, l, p). For example, the ARG0 in Fig-
ure 1 is ((0, 2),ARG0, 0.94). Because the outputs
of baseline systems are to be combined, we call
such triple a candidate for combination.
4 Approach Overview
As illustrated in Figure 2, the architecture of our
system consists of a candidates generation stage, a
weighted merging stage, and a combination stage.
In the candidates generation stage, the baseline
systems are run individually and their outputs are
collected. We use 2-best parse trees of Berkeley
parser (Petrov and Klein, 2007) and 1-best parse
tree of Bikel parser (Bikel, 2004) and Stanford
parser (Klein and Manning, 2003) as inputs to the
full parsing based system. The second best parse
tree of Berkeley parser is used here for its good
quality. So together we have four different out-
puts from the full parsing based system. From the
shallow parsing based system, we have only one
output.
Sentence
Weighted
merging
Full parsing based SRL system Shallow parsing based SRL system
Berkeley
parser
Bikel
parser
Stanford
parser Chunker
Output1 Output4Output3Output2 Output5
Candidates pool
Combination
Final results
Candidates
Generation
Stage
Weigthed
Merging
Stage
Combination
Stage
Figure 2: The overall architecture of our system.
In the weighted merging stage, each system
output is assigned a weight according to our prior
knowledge obtained on the development set. De-
tails about how to obtain appropriate weights will
be explained in Section 6. Then all candidates
with the same loc and l are merged to one by
weighted summing their probabilities. Specifi-
cally, suppose that there are n system outputs to
be combined, with the i-th output?s weight to be
wi. And the candidate in the i-th output with loc
and l is (loc, l, pi) (If there is no candidate with loc
and l in the i-th output, pi is 0.). Then the merged
candidate is (loc, l, p), where p = ?ni=1 wipi.
After the merging stage, a pool of merged can-
didates is obtained. In the combination stage,
candidates in the pool are combined to form a
consistent SRL result. Greedy and integer lin-
ear programming combination methods are exper-
imented in this paper.
1364
5 Combination Methods
5.1 Global constraints
When combining the outputs, two global con-
straints are enforced to resolve the conflict be-
tween outputs. These two constraints are:
1. No duplication: There is no duplication for
key arguments: ARG0 ? ARG5.
2. No overlapping: Arguments cannot overlap
with each other.
We say two argument candidates conflict with
each other if they do not satisfy the two constraints
above.
5.2 Two combination methods
Under these constraints, two methods are explored
to combine the outputs. The first one is a greedy
method. In this method, candidates with probabil-
ity below a threshold are deleted at first. Then the
remaining candidates are inspected in descending
order according to their probabilities. And each
candidate will be put into a solution set if it does
not conflict with candidates already in the set.
This greedy combination method is very simple
and has been adopted in previous research (Prad-
han et al, 2005; Ma`rquez et al, 2005).
The second combination method is integer lin-
ear programming (ILP) method. ILP method was
first applied to SRL in (Punyakanok et al, 2004).
Here we formulate an ILP model whose form is
different from the model in (Punyakanok et al,
2004; Koomen et al, 2005). For convenience, we
denote the whole label set as {l1, l2, . . . , ln}. And
let l1 ? l6 stand for the key argument labels ARG0
? ARG5 respectively. Suppose there are m differ-
ent locations, denoted as loc1, . . . , locm, among
all candidates in the pool. And the probability of
assigning lj to loci is pij . A binary variable xij is
defined as:
xij =
{
1 if loci is assigned label lj ,
0 otherwise.
The objective of the ILP model is to maximize the
sum of arguments? probabilities:
max
m?
i=1
n?
j=1
(pij ? T )xij (1)
where T is a threshold to prevent including too
many candidates in solution. T is similar to the
threshold in greedy combination method. In this
paper, both thresholds are empirically tuned on
development data, and both are set to be 0.2.
The inequalities in equation (2) make sure that
each loc is assigned at most one label.
?1 ? i ? m :
n?
j=1
xij ? 1 (2)
The inequalities in equation (3) satisfy the No
duplication constraint.
?1 ? j ? 6 :
m?
i=1
xij ? 1 (3)
For any location loci, let Ci denote the index
set of the locations that overlap with it. Then
the No overlapping constraint means that if loci
is assigned a label, i.e., ?nj=1 xij = 1, then for
any k ? Ci, lock cannot be assigned any label,
i.e., ?nj=1 xkj = 0. A common technique in ILP
modeling to form such a constraint is to use a suf-
ficiently large auxiliary constant M . And the con-
straint is formulated as:
?1 ? i ? m :
?
k?Ci
n?
j=1
xkj ? (1?
n?
j=1
xij)M
(4)
In this case, M only needs to be larger than the
number of candidates to be combined. In this pa-
per, M = 500 is large enough. And we employ
lpsolve3 to solve the ILP model.
Note that the form of the ILP model in this
paper is different from that in (Punyakanok et
al., 2004; Koomen et al, 2005) in three as-
pects: (1) A special label class null, which means
no label is assigned, was added to the label set
in (Punyakanok et al, 2004; Koomen et al, 2005).
Whereas no such special class is needed in our
model, because if no label is assigned to loci,?n
j=1 xij = 0 would simply indicate this case.
This makes our model contain fewer variables.
(2) Without null class in our model, we need to
use a different technique to formulate the No-
overlapping constraint. (3) In order to compare
3http://lpsolve.sourceforge.net/
1365
with the greedy combination method, the ILP
model in this paper conforms to exactly the same
constraints as the greedy method. Whereas many
more global constraints were taken into account
in (Punyakanok et al, 2004; Koomen et al, 2005).
6 Train Minimum Error Weights
The idea of minimum error weighting is straight-
forward. Individual outputs O1, O2, . . . , On
are assigned weights w1, w2, . . . , wn respectively.
These weights are normalized, i.e., ?ni=1 wi = 1.
An output?s weight can be seen as the confidence
we have in that output. It is a kind of prior knowl-
edge we have about that output. We can gain this
prior knowledge on the development set. As long
as the data of the development set and the test set
are similar, this prior knowledge should be able
to help to guide SRL combination on test set. In
this section, we discuss how to obtain appropriate
weights.
6.1 Training model
Suppose the golden answer and SRL result on de-
velopment set are d and r respectively. An error
function Er(r, d) is a function that measures the
error contained in r in reference to d. An error
function can be defined as the number of wrong
arguments in r. It can also be defined using preci-
sion, recall, or F1 score. For example, Er(r, d) =
1? Precision(r, d), or Er(r, d) = 1? F1(r, d).
Smaller value of error function means less error in
r.
The combination process can also be seen as
a function, which maps the outputs and weights
to the combined result r: r = Comb(On1 , wn1 ).
Therefore, the error function of our system on de-
velopment set is:
Er(r, d) = Er(Comb(On1 , wn1 ), d) (5)
From equation (5), it can be seen that: Given de-
velopment set d, if the outputs to be combined On1
and the combination method Comb are fixed, the
error function is just a function of the weights. So
we can obtain appropriate weights by minimizing
the error function:
w?n1 = argminwn1
Er(Comb(On1 , wn1 ), d) (6)
6.2 Training algorithm
Algorithm 1 Powell Training Algorithm.
1: Input : Error function Er(w).
2: Initialize n directions d1, . . . ,dn, and
a start point w in Rn.
3: Set termination threshold ?.
4: do:
5: w1 ? w
6: for i ? 1, . . . , n:
7: ?i ? argmin? f(wi + ?di)
8: wi+1 ? wi + ?idi
9: dn+1 ? wn+1 ?w
10: ?? ? argmin
?
f(w + ?dn+1)
11: w? ? w + ??dn+1
12: ?Er ? Er(w)? Er(w?)
13: i ? arg max
1?j?n
Er(wj)? Er(wj+1)
14: if (??)2 ? ?ErEr(wi)? Er(wi+1) :
15: for j ? i, . . . , n:
16: dj ? dj+1
17: w ? w?
18: while ?Er > ?
19: Output: The minimum error weights w.
There are two difficulties to solve the optimiza-
tion problem in equation 6. The first one is that
the error function cannot be written to an analyt-
ical form. This is because the Comb function,
which stands for the combination process, cannot
be written as an analytical formula. So the prob-
lem cannot be solved using canonical gradient-
based optimization algorithms, because the gradi-
ent function cannot be derived. The second diffi-
culty is that, according to our experience, the er-
ror function has many local optima, which makes
it difficult to find a global optima.
To resolve the first difficulty, Modified Powell?s
method (Yuan, 1993) is employed to solve the op-
timization problem. Powell?s method is a heuris-
tic search method that does not require the objec-
tive function to have an explicit analytical form.
The training algorithm is presented in Algorithm
1. In Algorithm 1, the line search problem in steps
7 and 10 is solved using Brent?s method (Yuan,
1993). And the temination threshold ? is empiri-
cally set to be 0.001 in this paper.
1366
To resolve the second difficulty, we perform
multiple searches using different start points, and
then choose the best solution found.
7 Experiments
7.1 Experimental setup
We use Chinese Proposition Bank (CPB) 1.0 and
Chinese Tree Bank (CTB) 5.0 of Linguistic Data
Consortium corpus in our experiments. The train-
ing set is comprised of 648 files(chtb 081.fid to
chtb 885.fid). The development set is comprised
of 40 files(chtb 041.fid to chtb 080.fid). The
test set is comprised of 72 files(chtb 001.fid to
chtb 040.fid and chtb 900.fid to chtb 931.fid).
The same data setting has been used in (Xue,
2008; Ding and Chang, 2008; Sun et al, 2009).
Sun et al (2009) used sentences with golden seg-
mentation and POS tags as input to their SRL
system. However, we use sentences with only
golden segmentation as input. Then we perform
automatic POS tagging using Stanford POS tag-
ger (Toutanova et al, 2003). In (Xue, 2008), the
parser used by the SRL system is trained on the
training and development set plus 275K words of
broadcast news. In this paper, all parsers used
by the full parsing based system are trained on
the training set plus the broadcast news portion
of CTB6.0. And the chunker used in the shallow
parsing based system is trained just on the training
set.
7.2 Individual outputs? performance
In this paper the four outputs of the full parsing
based system are represented by FO1 ? FO4 re-
spectively. Among them, FO1 and FO2 are the
outputs using the first and second best parse trees
of Berkeley parser, FO3 and FO4 are the outputs
using the best parse trees of Stanford parser and
Bikel parser respectively. The output of the shal-
low parsing based system is represented by SO.
The individual outputs? performance on develop-
ment and test set are listed in Table 1.
From Table 1 we can see that the performance
of individual outputs are similar on development
set and test set. On both sets, the F1 scores of
individual outputs are in the same order: FO1 >
FO2 > SO > FO3 > FO4.
Data set Outputs P (%) R(%) F1
FO1 79.17 72.09 75.47
FO2 77.89 70.56 74.04
development FO3 72.57 67.02 69.68
FO4 75.60 63.45 69.00
SO 73.72 67.35 70.39
FO1 80.75 70.98 75.55
FO2 79.44 69.37 74.06
test FO3 73.95 66.37 70.00
FO4 75.89 63.26 69.00
SO 75.69 67.90 71.59
Table 1: The results of individual systems on de-
velopment and test set.
7.3 Combining outputs of full parsing based
system
In order to investigate the benefit that the full
parsing based system can get from using multi-
ple parsers, we combine the four outputs FO1 ?
FO4. The combination results are listed in Ta-
ble 2. In tables of this paper, ?Grd? and ?ILP?
stand for greedy and ILP combination methods re-
spectively, and ?+MEW? means the combination
is performed with MEW strategy.
P (%) R(%) F1
Grd 82.68 73.36 77.74
ILP 82.21 73.93 77.85
Grd+MEW 81.30 75.38 78.23
ILP+MEW 81.27 75.74 78.41
Table 2: The results of combining outputs of full
parsing based system on test set.
Er FO1 FO2 FO3 FO4
Grd 1? F1 0.31 0.16 0.30 0.23
ILP 1? F1 0.33 0.10 0.27 0.30
Table 3: The minimum error weights for the re-
sults in Table 2.
From Table 2 and Table 1, we can see that, with-
out MEW strategy, the F1 score of combination
result is about 2.3% higher than the best individ-
ual output. With MEW strategy, the F1 score is
improved about 0.5% further. That is to say, with
MEW strategy, the benefit of combination is im-
proved by about 20%. Therefore, the effect of
MEW is very encouraging.
Here the error function for MEW training is
chosen to be 1 ? F1. And the trained weights
for greedy and ILP methods are listed in Table 3
1367
separately. In tables of this paper, the column Er
corresponds to the error function used for MEW
strategy.
7.4 Combining all outputs
We have also combined all five outputs. The re-
sults are listed in Table 4. Compared with the re-
sults in Table 2, we can see that the combination
results is largely improved, especially the recall.
P (%) R(%) F1
Grd 83.64 75.32 79.26
ILP 83.31 75.71 79.33
Grd+MEW 83.34 77.47 80.30
ILP+MEW 83.02 78.03 80.45
Table 4: The results of combining all outputs on
test set.
From Table 4 and Table 1 we can see that with-
out MEW strategy, the F1 score of combination
result is about 3.8% higher than the best individ-
ual output. With MEW, the F1 score is improved
further by more than 1%. That means the bene-
fit of combination is improved by over 25% with
MEW strategy.
Here the error function for MEW training is still
1 ? F1, and the trained weights are listed in Ta-
ble 5.
Er FO1 FO2 FO3 FO4 SO
Grd 1? F1 0.23 0.12 0.23 0.20 0.22
ILP 1? F1 0.24 0.08 0.22 0.21 0.25
Table 5: The minimum error weights for the re-
sults in Table 4.
7.5 Using alternative error functions for
minimum error weights training
In previous experiments, we use 1 ? F1 as error
function. As pointed out in Section 6, the def-
inition of error function is very general. So we
have experimented with two other error functions,
which are 1 ? Precision, and 1 ? Recall. Ob-
viously, these two error functions favor precision
and recall separately. The results of combining
all five outputs using these two error functions are
listed in Table 6, and the trained weights are listed
in Table 7.
From Table 6 and Table 4, we can see that when
1 ? Precison is used as error function, the pre-
Er P (%) R(%) F1
Grd+MEW 1? P 85.31 73.42 78.92
ILP+MEW 1? P 85.62 72.76 78.67
Grd+MEW 1?R 81.94 77.55 79.68
ILP+MEW 1?R 79.74 78.34 79.03
Table 6: The results of combining all outputs with
alternative error functions.
Er FO1 FO2 FO3 FO4 SO
Grd 1? P 0.25 0.24 0.22 0.22 0.07
ILP 1? P 0.30 0.26 0.20 0.15 0.09
Grd 1?R 0.21 0.10 0.17 0.15 0.37
ILP 1?R 0.24 0.04 0.10 0.22 0.39
Table 7: The minimum error weights for the re-
sults in Table 6.
cision of combination result is largely improved.
But the recall decreases a lot. Similar effect of the
error function 1?Recall is also observed.
The results of this subsection reflect the flex-
ibility of MEW strategy. This flexibility comes
from the generality of the definition of error func-
tion. The choice of error function gives us some
control over the results we want to get. We can
define different error functions to favor precision,
or recall, or some error counts such as the number
of misclassified arguments.
7.6 Discussion
In this paper, the greedy and ILP combination
methods conform to the same simple constraints
specified in Section 5. From the experiment
results, we can see that ILP method generates
slightly better results than greedy method.
In Subsection 7.4, we see that combining all
outputs using ILP method with MEW strategy
yields 4.90% improvement on F1 score over the
best individual output FO1. In order to under-
stand each output?s contribution to the improve-
ment over FO1. We compare the differences be-
tween outputs.
Let CO denote the set of correct arguments in
an output O. Then we get the following statistics
when comparing two outputs A and B: (1) the
number of common correct arguments in A and
B, i.e., |CA ? CB| ; (2) the number of correct ar-
guments in A and not in B, i.e., |CA \CB|; (3) the
number of correct arguments in B and not in A,
i.e., |CB \ CA|. The comparison results between
1368
some outputs on test set are listed in Table 8. In
this table, UF stands for the union of the 4 outputs
FO1 ? FO4.
A B |CA ? CB | |CA \ CB | |CB \ CA|
FO2 5498 508 372
FO3 5044 962 552
FO4 4815 1191 512FO1
SO 4826 1180 920
UF SO 5311 1550 435
Table 8: Comparison between outputs on test set.
From Table 8 we can see that the output SO
has 4826 common correct arguments with FO1,
which is relatively small. And, more importantly,
SO contains 920 correct arguments not in FO1,
which is much more than any other output con-
tains. Therefore, SO is more complementary to
FO1 than other outputs. On the contrary, FO2 is
least complementary to FO1. Even compared with
the union of FO1 ? FO4, SO still contains 435
correct arguments not in the union. This shows
that the output of shallow parsing based system is
a good complement to the outputs of full parsing
based system. This explains why recall is largely
improved when SO is combined in Subsection 7.4.
From the analysis above we can also see that the
weights in Table 5 are quite reasonable. In Ta-
ble 5, SO is assigned the largest weight and FO2
is assigned the smallest weight.
In Subsection 7.3, the MEW strategy improves
the benefit of combination by about 20%. And in
Subsection 7.4, the MEW strategy improves the
benefit of combination by over 25%. This shows
that the MEW strategy is very effective for Chi-
nese SRL combination.
To our best knowledge, no results on Chinese
SRL combination has been reported in the litera-
ture. Therefore, to compare with previous results,
the top two results of single SRL system in the
literature and the result of our combination sys-
tem on this data set are listed in Table 9. For the
results in Table 9, the system of Sun et al uses
sentences with golden POS tags as input. Xue?s
system and our system both use sentences with
automatic POS tags as input. The result of Sun
et al (2009) is the best reported result on this data
set in the literature.
POS P (%) R(%) F1
(Xue, 2008) auto 76.8 62.5 68.9
(Sun et al, 2009) gold 79.25 69.61 74.12
Ours auto 83.02 78.03 80.45
Table 9: Previous best single system?s results and
our combination system?s result on this data set.
8 Conclusions
In this paper, we propose a minimum error
weighting strategy for SRL combination and in-
vestigate the benefit that Chinese SRL can get
from combination. We assign different weights to
different system outputs and add a weighted merg-
ing stage to conventional SRL combination sys-
tem architecture. And we also propose a method
to train these weights on development set. We
evaluate the MEW strategy on Chinese Propbank
data set with greedy and ILP combination meth-
ods.
Our experiments have shown that the MEW
strategy is very effective for Chinese SRL combi-
nation, and the benefit of combination can be im-
proved over 25% with this strategy. And also, the
MEW strategy is very flexible. With different def-
initions of error function, this strategy can favor
precision, or recall, or F1 score. The experiments
have also shown that Chinese SRL can benefit a
lot from combination, especially when systems
based on different syntactic views are combined.
The SRL result with the highest F1 score in this
paper is generated by ILP combination together
with MEW strategy. In fact, the MEW strategy is
easy to incorporate with other combination meth-
ods, just like incorporating with the greedy and
ILP combination methods in this paper.
Acknowledgment
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053, 90820303 and 60736014,
the National Key Technology R&D Program un-
der Grant No. 2006BAH03B02, the Hi-Tech Re-
search and Development Program (?863? Pro-
gram) of China under Grant No. 2006AA010108-
4, and also supported by the China-Singapore In-
stitute of Digital Media (CSIDM) project under
grant No. CSIDM-200804.
1369
References
Daniel Bikel. 2004. Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4):480-511.
Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a Hybrid Con-
volution Tree Kernel for Semantic Role Labeling.
ACM Transactions on Asian Language Information
Processing, 2008, 7(4).
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking. In
Proceedings of COLING/ACL-2006.
Weiwei Ding and Baobao Chang. 2008. Improving
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings of
EMNLP-2008.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
CoNLL-2009.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL-
2003.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of CoNLL-2005 shared task.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proceedings of
NAACL-2001.
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving Nominal SRL
in Chinese Language with Verbal SRL Information
and Automatic Predicate Recognition. In Proceed-
ings of EMNLP-2009.
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A Robust Combination Strat-
egy for Semantic Role Labeling. In Proceedings of
EMNLP-2005.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized parsing. In Proceedings of ACL-
2007.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic Role Labeling Using Different Syntactic
Views. In Proceedings of ACL-2005.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING-2004.
Honglin Sun and Daniel Jurafsky. 2004. Shallow
semantic parsing of Chinese. In Proceedings of
NAACL-2004.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese Semantic Role Labeling with Shal-
low Parsing. In Proceedings of EMNLP-2009.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artificial Intel-
ligence Research (JAIR), 29:105-151.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2): 145-159.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL-2003.
Nianwen Xue. 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2): 225-255.
Yaxiang Yuan. 1993. Numerical Methods for Nonlin-
ear Programming. Shanghai Scientific and Techni-
cal Pulishers, Shanghai.
1370
Coling 2010: Poster Volume, pages 1292?1300,
Beijing, August 2010
Phrase Structure Parsing with Dependency Structure 
Zhiguo Wang and Chengqing Zong
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{zgwang, cqzong}@nlpr.ia.ac.cn 
Abstract
In this paper we present a novel phrase 
structure parsing approach with the help 
of dependency structure. Different with 
existing phrase parsers, in our approach 
the inference procedure is guided by 
dependency structure, which makes the 
parsing procedure flexibly.  The 
experimental results show our approach is 
much more accurate. With the help of 
golden dependency trees, F1 score of our 
parser achieves 96.08% on Penn English 
Treebank and 90.61% on Penn Chinese 
Treebank. With the help of N-best 
dependency trees generated by modified 
MSTParser, F1 score achieves 90.54% 
for English and 83.93% for Chinese. 
1 Introduction 
Over the past few years, several high-precision 
phrase parsers have been presented, and most of 
them are employing probabilistic context-free 
grammar (PCFG). As we all know, the basic 
PCFG has the problems that the independence 
assumption is too strong and lacks of lexical 
conditioning (Jurafsky and Martin, 2007). 
Although researchers have proposed various 
models and inference algorithms aiming to solve 
these problems, the performance of existing 
phrase parsers is still remained to further 
improve. Most of the existing approaches can be 
classified into two categories: unlexicalized 
PCFG based (Johnson, 1998; Klein and 
Manning, 2003; Levy and Manning, 2003; 
Matsuzaki et al, 2005; Petrov et al, 2006) and 
lexicalized PCFG based (Collins, 1999a; 
Charniak, 1997; Bikel, 2004; Charniak and 
Johnson, 2005). 
Unlexicalized PCFG based approach attempts 
to weaken the independence assumption by 
annotating non-terminal symbols with labels of 
ancestor, siblings and even the latent annotations 
encoded by local information. In lexicalized 
PCFG based approach, researchers believe that 
the forms of a constituent and its sub-
constituents are determined more by the 
constituent?s head than any other of its lexical 
items (Charniak, 1997), so they annotate non-
terminal symbols with the head words 
information. 
Both of the two PCFG based approaches have 
improved the basic PCFG based parsers 
significantly. However, neither of them has been 
guided by enough linguistic priori knowledge. 
Their parsing procedures are too mechanical. 
Because of this, the efficiency is always worse, 
and much more artificial ambiguities, which are 
different from linguistic ambiguities (Krotov et 
al., 1998; Johnson, 1998), are generated. We 
believe parsing procedure guided by more 
linguistic priori knowledge will help to 
overcome the drawbacks in some extent. From 
our intuition, dependency structure, another type 
of syntactic structure with much linguistic 
knowledge, will be a good candidate to guide 
phrase parsing procedure.  
In this paper we present a novel approach to 
using dependency structure to guide phrase 
parsing. This novel approach has its virtues from 
multiple angles. First, dependency structure 
offers a good compromise between the 
conflicting demands of analysis depth, which 
makes it much easier to get through hand 
annotating than phrase structure (Nivre, 2004). 
So, when we want to build a phrase structure 
corpus, we can build a dependency structure 
corpus first, and get the corresponding phrase 
structure automatically with the help of 
dependency structure. Second, many parsing 
algorithms with linear-time complexity used in 
dependency parsers can still achieve the state-
of-the-art results (Johansson, 2007), but almost 
all phrase parsers with high-precision have no 
efficient algorithms superior to cubic-time 
complexity. So, in order to get an efficient 
1292
parser, we can first get a dependency structure 
through linear-time algorithm, and then obtain 
the phrase structure with the help of dependency 
structure more efficiently. Third, the lexicalized 
PCFG based parsers which just bring the head 
words into account have got a highly improved 
performance. It gives us reasons to believe 
dependency structure which takes the 
relationship of all the words will bring phrase 
parser a great help. 
Remainder of this paper is organized as 
follows: Section 2 introduces the related work. 
Section 3 gives a consistency between 
dependency structure and phrase structure, and 
presents an approach to parsing phrase structure 
with dependency structure. In Section 4, we 
discuss the experiments and analysis. Finally, 
we conclude this paper and point out some 
future work in Section 5. 
2 Related work 
Unlexicalized PCFG based parsers (Johnson, 
1998; Klein and Manning, 2003; Levy and 
Manning, 2003; Matsuzaki et al, 2005; Petrov 
et al, 2006) are the most successful parsing 
tools. They regard parsing as a pure machine 
learning question. However, they haven?t taken 
any extra linguistic priori knowledge directly 
into account. Lexicalized PCFG based parsers 
(Collins, 1999a; Charniak, 1997; Bikel, 2004; 
Charniak and Johnson, 2005) just bring a little 
linguistic priori knowledge (head word 
information) into learning phase. In inference 
phase, both of the unlexicalized PCFG based 
approach and lexicalized PCFG based approach 
are using the pure searching algorithms, which 
try to parse a sentence monotonously, either 
from left to right or from right to left. From 
these states, we can find that manners of current 
parsers are too mechanical. Because of this, the 
efficiency of phrase parsers is always worse, and 
much more artificial ambiguities are generated. 
There have been some work (Collins et al, 
1999b; Xia and Palmer, 2001) about converting 
dependency structures to phrase structures. 
Collins et al (1999b) proposed an algorithm to 
convert the Czech dependency Treebank into a 
phrase structure Treebank and do dependency 
parsing through Collins (1999a)?s model. 
Results showed the accuracy of dependency 
parsing for Czech was improved largely. Xia 
and Palmer (2001) proposed a more generalized 
algorithm according to X-bar theory and Collins 
et al (1999b), and they did some experiments on 
Penn Treebank. The results showed their 
algorithm produced phrase structures that were 
very close to the ones in Penn Treebank. 
However, we have to point out that they only 
computed the unlabeled performance but lost all 
the exact syntactic symbols. Different from tree-
transformed PCFG based approach and 
lexicalized PCFG based approach, both of 
Collins et al (1999b) and Xia and Palmer (2001) 
attempted to build some heuristic rules through 
linguistic theory, but didn?t try to learn anything 
from Treebank. 
Li and Zong (2005) presented a hierarchical 
parsing algorithm for long complex Chinese 
sentences with the help of punctuations. They 
first divided a long sentence into short ones 
according to punctuation marks, then parsed the 
short ones into sub-trees individually, and at last 
combined all the sub-trees into a whole tree. 
Experimental results showed the parsing time 
was reduced largely, and performance was 
improved too. Although the procedure of their 
parser is more close to human beings? manner, it 
appears a little shallow just using the 
punctuation marks. 
In this paper our motivations are to bring 
more linguistic priori knowledge into phrase 
parsing procedure with the help of dependency 
structure, and make the parsing procedure 
flexibly. 
Matsuzaki et al (2005) defined a generative 
model called PCFG with latent annotations 
(PCFG-LA). Using EM-algorithm each non-
terminal symbols was annotated with a latent 
variable, and a fine-grained model can be got.  
In order to get a more compact PCFG-LA model, 
Petrov et al (2006) presented a split-and-merge 
method which can get PCFG-LA model 
hierarchically, and their final result 
outperformed state-of-the-art phrase parsers. To 
make the parsing process of hierarchical PCFG-
LA model more efficient, Petrov and Klein 
(2007) presented a coarse-to-fine inference 
algorithm. In Section 4 of this paper, we try to 
combine the hierarchical PCFG-LA model in 
learning phase and coarse-to-fine method in 
inference phase into our parser in order to get an 
accurate and efficient parser. 
1293
3 Our framework 
In this section, we first compare phrase structure 
with dependency structure of the same sentence, 
and get a consistent relationship among them. 
Then, based on this relationship, we present an 
inference framework to make the parsing 
procedure flexible and more efficient.  
3.1 Analysis on consistency between phrase 
structure and dependency structure 
Phrase structure and dependency structure are 
two different ways to represent syntactic 
structures of sentences. Phrase structure 
represents sentences by nesting of multi-word 
constituents, while dependency structure 
represents sentences as trees, whose nodes are 
words and edges represent the relations among 
words.
As we know, there are two kinds of 
dependency structures, projective structure and 
non-projective structure. For free-word order 
languages, non-projectivity is a common 
phenomenon, e.g. Czech. For languages like 
English and Chinese, the dependency structures 
are often projective trees. In this paper, we only 
consider English parsing based on Penn 
Treebank (PTB) and Chinese parsing based on 
Penn Chinese Treebank (PCTB), so we just 
research the consistency between phrase 
structure and projective dependency structure 
through PTB/PCTB.  
Information carried by the two structures isn?t 
equal. Phrase structure is more flexible, carries 
more information, and even contains all the 
information of dependency structure. So the task 
to convert a phrase structure to dependency 
structure is more straight, e.g. Nivre and Scholz 
(2004), Johansson and Nugues (2007). However, 
the reverse procedure is much more difficult, 
because dependency structure lacks the syntactic 
symbols, which are indispensable in phrase 
structure.
join
Vinken will board as 29
the director Nov
a nonexecutive
(a) Dependency structure
(1)
(2)
(3)
S
NP VP
NNP
Vinken
MD VP
will VB NP PP NP
join
DT NN
the board
IN
as
NP NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
(b) Phrase structure
(1)
(2)
(3)
Figure 1. The consistency between phrase 
structure and dependency structure 
Although the two structures are completely 
different, they have consistency in some deep 
level. In this paper we analyze the consistency 
from a practical perspective in order to do 
phrase parsing with the help of dependency 
structure. Having investigated the two kinds of 
trees with dependency structure and phrase 
structure, we find a consistency1 that each sub-
tree in dependency structure must correspond to 
a sub-tree in phrase structure who dominates all 
the words appearing in dependency sub-tree. 
Figure 1 shows this relationship more intuitively. 
The dependency sub-tree surrounded by circle 
(1) in Figure 1(a) is a one-layer sub-tree, and has 
a corresponding phrase sub-tree surrounded by 
circle (1) in Figure 1(b). Both of the two sub-
trees dominate the same word ?Vinken?. This 
consistency is also satisfied in other cases, e.g. 
two-layer sub-tree surrounded by circle (3) and 
three-layer sub-tree surrounded by circle (2) in 
Figure 1(a). These dependency sub-trees 
respectively have their corresponding phrase 
sub-trees dominating the same words in Figure 
1(b).
This consistency brings us inspiration to make 
use of dependency structure for phrase parsing. 
In other words, in our method when a phrase 
sub-tree is generated from a dependency sub-
tree, it must dominate all the same words with 
ones in the corresponding dependency sub-tree. 
3.2 Inference framework 
1 Be aware that the consistency is irreversible and not every 
phrase sub-tree has its corresponding dependency sub-tree. 
1294
As we mentioned in Section 2, most of current 
inference algorithms are monotonous, which 
generate much more artificial ambiguities. For 
example, in Figure 1, if a sub-tree only 
dominating ?board? and ?as? is built (actually it 
is not occurred in golden tree) an artificial 
ambiguity is generated, and it thus will further 
bring a worse effect to other parts. The final 
precision will certainly descend. However, if we 
are given a corresponding dependency structure, 
those errors will be avoided. The consistency 
analyzed above tells us that there isn?t a sub-tree 
dominating only ?board? and ?as? in 
dependency tree, so the two words can?t build a 
sub-tree independently in phrase parsing. 
According to this strategy, we design an 
inference framework for phrase parsing. 
NP
NNP
Vinken
MD
will
VB
join
DT NN
the board
IN
as
NNP CD
Nov 29
DT JJ NN
a nonexecutive director
NP
NNP
Vinken
MD
will
VB
NP
NP
join
DT NN
the board
IN
as
NP
NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
join
Vinken will board as 29
the director Nov
a nonexecutive
NP
NNP
Vinken
MD
will
VB
NP
PP
NP
join
DT NN
the board
IN
as
NP
NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
join
Vinken will board as 29
the director Nov
a nonexecutive
S
NP VP
NNP
Vinken
MD VP
will VB NP PP NP
join
DT NN
the board
IN
as
NP NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
join
Vinken will board as 29
the director Nov
a nonexecutive
(a) fill cell[i,i] for each word
(b) fill spans guided by two-layer dependency sub-trees
(0) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10)
d[3,4]
d[6,8]
d[9,10]
d[5,8]
d[0,10]
cell[3,4]
cell[6,8]
cell[9,10]
cell[5,8]
cell[0,10]
(c) fill spans guided by three-layer dependency sub-trees
(d) fill spans guided by four-layer dependency sub-trees
Figure 2. Parsing procedure of our 
inference framework guided by 
dependency structure
Our inference framework parses a sentence 
flexibly with a traditional inference algorithm. 
The following terms will help to explain our 
work. A key data structure is cell[i,j], which is 
used to store phrase sub-trees spanning words 
from positions i to j of the input sentence. d[i,j]
is a dependency sub-tree spanning words from 
positions i to j. cells[i,j] is an array to store all 
the cells which can be combined to build 
cell[i,j]. The pseudo-code of our inference 
framework is shown in Algorithm1. The line 
indicated by (1) and (2) gives us freedom to 
select any kinds of inference algorithms and 
matching parsing models. 
Algorithm 1
InferenceFramework(sentence S, dependency tree D)
 initialize a List for the input sentence 
for each word wi in S do
fill cell[i, i] and add it to a list L
 parse the cells[] hierarchically 
for each d[s, t] of D in topological order do
fill cells[s, t] with spans in L
fill cell[s, t] with cells[s, t] through  
traditional inference algorithm   (1) 
add cell[s, t] to L
 extract the best tree 
estimate all trees in cell[0, n]
through parsing model             (2) 
return the best phrase tree
Now, let?s illustrate the flexible parsing 
procedure step by step through an example. 
Please see Figure 2.  For simplicity, we just 
draw sub-trees of the final best tree, and ignore 
all the others. Figure 2(a) shows the procedure 
of filling cell[i,i] for each word. In Figure 2(b), 
there are three two-layer dependency sub-trees 
d[3,4], d[6,8] and d[9,10]. So we try to generate 
phrase sub-trees for cell[3,4], cell[6,8] and 
cell[9,10], which have been annotated with bold 
edges. For example, we use sub-trees contained 
in cell[6,6], cell[7,7] and cell[8,8] to 
1295
build new sub-trees for cell[6,8]. Figure 2(c) 
and Figure 2(d) show the same procedure for 
parsing with the help of three-layer dependency 
sub-trees and four-layer dependency sub-trees 
individually. The generated phrase sub-trees are 
all annotated with bold edges in the figure. 
Obviously, the biggest dependency sub-tree is 
the whole dependency tree of sentence. In this 
example, when the four-layer dependency sub-
tree is processed, the whole phrase trees are built. 
Usually, more than one phrase trees with the 
similar skeletons are generated. So we use a 
model to evaluate candidate results, and get out 
the one with the highest score as the final result.
Benefiting from the dependency structure, we 
can parse a sentence flexibly. Comparing with 
previous work on converting dependency 
structure to phrase structure (Collins et al, 
1999b; Xia and Palmer, 2001), we make use of 
Treebank knowledge more sufficient with the 
help of traditional parsing technology. The 
search space has been pruned tremendously. As 
we know, the traditional parsing approach often 
tries to search all the n*(n+1)/2  cells for input 
sentence which has n words, but our parsing 
framework search cells intelligently with the 
help of corresponding dependency structure. 
Let?s get a view of this through the sentence 
shown in Figure 2. From the whole parsing 
procedure shown in Figure 2, our framework 
just tries to fill 16 cells, which are cell[i,i] for 
each word, cell[3,4], cell[6,8], cell[9,10],
cell[5,8] and cell[0,10] hierarchically, but 
traditional parsing approach would try to fill all 
66 cells. So 75.76% searching space has been 
pruned.
4 Experiments and results 
In order to evaluate the effectiveness of our 
approach, we have done some experiments both 
for English parsing and Chinese parsing. 
4.1 Preparation
To make comparison with previous work fairly, 
our experiments are based on general Treebank 
according to standard settings. We choose Penn 
English Treebank for English parsing 
experiments and Penn Chinese Treebank for 
Chinese. Table 1 shows the standard settings we 
take.
? ???? ????? ???? ? ??????? ?
PU NP NP NP PU
VP PUVP
IP
?????????????? ? ??????? ?
PUIPPU PUIP
IP
(b) Golden phrase tree
(c) Parsing result of PCFG model and CYK algorithm
? ???? ????? ???? ? ????? ?
PU NP NP NP PU
VP PUVP
IP
(d) Parsing result after pruning strategy added
??
NP
? ???? ????? ???? ? ??????? ?
PU NP NP NP PU
VP PUVP
IP
(e) Parsing result of PCFG-LA model
(a) Golden dependency tree
??/hold
??/ceremony ??/today ?/in ?
??/Shanghai??/collaborate ??/project ??/signing? ?
?/America
?/China ?/high
??/technology
Figure 4. An example showing 
experimental results
English Chinese
Train Set Sections 2-21 Art. 1-270, 400-1151 
Dev Set Section 22 Articles 301-325 
Test Set Section 23 Articles 271-300 
Table 1. Experimental settings 
Because the two Treebanks are in type of 
phrase structure, we should get dependency 
structures corresponding with them. There are 
two ways to accomplish this work. First, use 
converting tools to get dependency trees directly 
through converting the original Treebanks, and 
the generated trees are always considered as 
golden trees during dependency parsing. Second, 
use a dependency parser with state-of-the-art 
1296
performance to parse all the sentences 
automatically. In this paper, we design two 
groups of experiments, as following: 
(1) Phrase parsing with the help of golden 
dependency trees. 
(2) Phrase parsing with the help of N-best 
dependency trees generated automatically. 
4.2 Phrase parsing with golden dependency 
trees
In order to verify how much dependency 
structure can help phrase parsing and get an 
upper bound of our approach, we do phrase 
parsing with the help of golden dependency 
trees in this subsection. 
Based on the parsing framework shown in 
Figure 3, we only use the basic PCFG in 
learning phase and our inference framework 
with basic CYK algorithm in inference phase. 
The parsing results are shown with the mark (1) 
in Table 2 for English and Table 3 for Chinese 
respectively. 
Having investigated the generated trees with 
golden trees, we find the consistency of 
dependency structure and phrase structure is 
broken by some trees. Let?s get a view of this 
through an example from Penn Chinese 
Treebank. In Figure 4(a), the dependency sub-
tree surrounded by circle tells us that there must 
be a phrase sub-tree which dominate the word 
sequence of ??????????
? ?  ? ?  ? (the signing ceremony of 
collaborating in high technology between 
America and China), and the golden phrase tree 
shown in Figure 4(b) has a corresponding sub-
tree surrounded by circle indeed. However, the 
parsing tree generated by our approach shown in 
Figure 4(c) doesn?t conform. There are three 
sub-trees dominating the word sequence 
mutually, but they don?t construct a whole one. 
In our opinion, the contradiction derived from 
binarizing process of CYK 2 . The binary trees 
generated by our algorithm have consisted with 
the consistency originally, but after debinarizing 
process the consistency is broken. 
Trying to check our opinion, we add a 
pruning strategy to the original inference 
2 The premise of using CYK is that all the rules must have 
CNF form. So we usually bring some medial nodes to 
binarize rules gathered from Treebank. 
algorithm to prune all the medial nodes which 
may break the consistency during parsing 
procedure. With the help of pruning strategy, the 
performances of English and Chinese are all 
improved further. Corresponding figures are 
shown in Table 2 and Table 3 with the mark (2). 
The parsing result of above example is shown in 
Figure 4(d) and the error appearing in Figure 4(c) 
is corrected naturally after the pruning strategy 
added.
Comparing with previous work which have 
done much work in learning phase, our 
algorithm achieves such amazing results only 
using basic PCFG model. From this aspect, our 
inference framework is much more effective. 
However, there are still some errors our 
approach can?t deal with. For example, in Figure 
4(d) the sub-tree rooted at NP and dominating 
word sequence of  ????????? (hold 
in Shanghai today) is separated by two sub-trees. 
The reason is that the model (basic PCFG) we 
use in learning phase is too coarse to 
disambiguate sufficiently. So we don?t pin all 
hopes in inference phase. We also modify the 
model in learning phase. PCFG-LA is one of the 
most successful models in phrase parsing, so we 
choose PCFG-LA as the model in learning phase. 
After this modification, performance of our 
approach has been improved delightedly. F1 
score is 96.08% for English and 90.06% for 
Chinese. The line marked with (3) in Table 2 
and Table 3 shows more details. 
4.3 Phrase parsing with N-best dependency 
trees generated automatically 
The experimental results shown in subsection 
4.2 bring us confidence that do phrase parsing 
with the help of dependency structure is a highly 
effective approach. However, we don?t usually 
have golden dependency structures, and a more 
acceptable way is using a dependency parser to 
generate dependency trees automatically. In this 
subsection we explore feasibility and 
effectiveness of phrase parsing with the help of 
dependency trees generated automatically. As 
we all know, even state-of-the-art dependency 
parser cannot generate totally correct result. So in 
order to make our system more robust we use N-
best dependency structures to guide phrase 
parsing procedure. 
1297
length<=40 all sentences ?
Precision Recall F1 Precision Recall F1 
(1) Using PCFG and CYK 90.28 88.41 89.34 90.11 88.32 89.21 
(2) Using pruning strategy 90.69 89.53 90.11 90.51 89.45 89.97 
(3) Using PCFG-LA 96.28 95.97 96.13 96.25 95.91 96.08
Table 2. Parsing performance (%) for English with the help of golden dependency tree.  
length<=40 all sentences ?
Precision Recall F1 Precision Recall F1 
(1) Using PCFG and CYK 86.89 78.25 82.34 85.56 77.43 81.29 
(2) Using pruning strategy 87.65 82.33 84.91 86.39 81.45 83.85 
(3) Using PCFG-LA 91.51 91.26 91.38 90.43 90.79 90.61
Table 3. Parsing performance (%) for Chinese with the help of golden dependency tree.  
We choose MSTParser 3  which is the most 
famous dependency parser and modify it to 
generate N-best dependency trees. The oracle 
unlabeled accuracy of N-best dependency trees 
generated from 1-order model is shown in 
Table 4. To show the effectiveness of our 
approach, we choose Berkeleyparser 4  as the 
baseline parser, take the same configuration and 
combine it into our general parsing framework 
shown in Figure 3. 
The experiment of parsing with golden 
dependency structure gets an amazing 
performance. It brings us a new way to build 
PTB/PCTB style phrase structure corpus. 
Because dependency structure is much easier to 
get through hand annotating than phrase 
structure, we can build a dependency structure 
corpus first, and then get phrase structure 
corpus through our approach guided by the 
dependency structure corpus. 
The experiment of parsing with N-best 
dependency structures generated automatically 
uplifts the parsing performance to a new height. 
It brings us a more applied parsing tool for 
other NLP applications. 
Considering the number of dependency 
structures (N-best) will affect the final result, 
we make use of the development set shown in 
Table1 to turning parameters. We parse the 
development set many times with different 
number of dependency structures. The F1 
scores are shown in Figure 5 for English and 
Figure 6 5  for Chinese. From Figure 5 and 
Figure 6, we can find when we use 10-best 
dependency structures the performance is better. 
So we choose 10-best dependency trees for the 
test set. 
From the experiments in Section 4.2, we can 
find that even using the golden dependency 
structure we can?t get totally correct phrase 
structure. The reason is that although every 
dependency sub-tree has its corresponding 
phrase sub-tree, not every phrase sub-tree has 
its corresponding dependency sub-tree. So the 
remainder errors can?t be solved only by 
dependency structure and a better way is to 
modify the parsing model. 
The final performances of test set comparing 
with previous work are shown in Table 5 and 
Table 6. We can easily find that our approach 
has outperformed all the parsers which aren?t 
improved through reranking stage or semi-
supervised approach. Although there is still a 
margin between our parser and reranked parser 
or semi-supervised parser, we believe that the 
parsing performance can be improved further if 
we bring the reranking or semi-supervised 
approaches into our parsing framework.
5 Conclusion and Future work  
In this paper, we present a novel phrase parsing 
approach with the help of dependency structure. 
Based on the consistency between phrase 
structure and dependency structure, we propose 
a novel inference framework. Guided by the 
inference framework, inference algorithms 
parse sentences hierarchically with the help of 
dependency structures. Experimental results 
show that our approach can efficiently get 
better performance with both golden 
dependency structure and N-best dependency 
4.4 Discussion 
3 http://www.ryanmcd.com/MSTParser/MSTParser.html 
4 http://code.google.com/p/berkeleyparser/ 
5 F1 score at n=0 is the result of Berkeley parser running 
on my machine 
1298







          
1EHVW
)
OHQJWK  DOOVHQWHQFHV
Figure 6. F1 scores (%) of Dev Set for Chinese 
with the help of N-best dependency trees 

Coling 2010: Poster Volume, pages 1336?1344,
Beijing, August 2010
Exploring the Use of Word Relation Features
for Sentiment Classification 
Rui Xia and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{rxia, cqzong}@nlpr.ia.ac.cn 
Abstract
Word relation features, which encode 
relation information between words, are 
supposed to be effective features for 
sentiment classification. However, the 
use of word relation features suffers 
from two issues. One is the sparse-data 
problem and the lack of generalization 
performance; the other is the limitation 
of using word relations as additional 
features to unigrams. To address the two 
issues, we propose a generalized word 
relation feature extraction method and 
an ensemble model to efficiently inte-
grate unigrams and different type of 
word relation features. Furthermore, 
aimed at reducing the computation 
complexity, we propose two fast feature 
selection methods that are specially de-
signed for word relation features. A 
range of experiments are conducted to 
evaluate the effectiveness and efficiency 
of our approaches. 
1 Introduction 
The task of text sentiment classification has be-
come a hotspot in the field of natural language 
processing in recent years (Pang and Lee, 2008). 
The dominating text representation method in 
sentiment classification is known as the bag-of-
words (BOW) model. Although BOW is quite 
simple and efficient, a great deal of the informa-
tion from original text is discarded, word order 
is disrupted and syntactic structures are broken. 
Therefore, more sophisticated features with a 
deeper understanding of the text are required for 
sentiment classification tasks. 
With the attempt to capture the word relation 
information behind the text, word relation (WR) 
features, such as higher-order n-grams and word 
dependency relations, have been employed in 
text representation for sentiment classification 
(Dave et al, 2003; Gamon, 2004; Joshi and 
Penstein-Ros?, 2009). 
However, in most of the literature, the per-
formance of individual WR feature set was poor, 
even inferior to the traditional unigrams. For 
this reason, WR features were commonly used 
as additional features to supplement unigrams, 
to encode more word order and word relation 
information. Even so, the performance of joint 
features was still far from satisfactory (Dave et 
al., 2003; Gamon, 2004; Joshi and Penstein-
Ros?, 2009).  
We speculate that the poor performance is 
possibly due to the following two reasons: 1) in 
WR features, the data are sparse and the fea-
tures lack generalization capability; 2) the use 
of joint features of unigrams and WR features 
has its limitation.  
On one hand, there were attempts at finding 
better generalized WR (GWR) features. Gamon 
(2004) back off words in n-grams (and semantic 
relations) to their respective POS tags (e.g., 
great-movie to adjective-noun); Joshi and Ros? 
(2009) propose a method by only backing off 
the head word in dependency relation pairs to its 
POS tag (e.g., great-movie to great-noun), 
which are supposed to be more generalized than 
word pairs. Based on Joshi and Ros??s method, 
we back off the word in each word relation pairs 
to its corresponding POS cluster, making the 
feature space smarter and more effective. 
On the other hand, we find that from uni-
grams to WR features, relevance between fea-
tures is reduced and the independence is in-
1336
creased. Although the discriminative model 
(e.g., SVM) is proven to be more effective on 
unigrams (Pang et al, 2002) for its ability of 
capturing the complexity of more relevant fea-
tures, WR features are more inclined to work 
better in the generative model (e.g., NB) since 
the feature independence assumption holds well 
in this case. 
Based on this finding, we therefore intuitively 
seek, instead of jointly using unigrams and 
GWR features, to efficiently integrate them to 
synthesize a more accurate classification proce-
dure. We use the ensemble model to fuse differ-
ent types of features under distinct classification 
models, with an attempt to overcome individual 
drawbacks and benefit from each other?s merit, 
and finally to enhance the overall performance. 
Furthermore, feature reduction is another im-
portant issue of using WR features. Due to the 
huge dimension of WR feature space, traditional 
feature selection methods in text classification 
perform inefficiently. However, to our knowl-
edge, no related work has focused on feature 
selection specially designed for WR features. 
Taking this point into consideration, we pro-
pose two fast feature selection methods (FMI 
and FIG) for GWR features with a theoretical 
proof. FMI and FIG regard the importance of a 
GWR feature as two component parts, and take 
the sum of two scores as the final score. FMI 
and FIG remain a close approximation to MI 
and IG, but speed up the computation by at most 
10 times. Finally, we apply FMI and FIG to the 
ensemble model, reducing the computation 
complexity to a great extent. 
The remainder of this paper is organized as 
follows. In Section 2, we introduce the approach 
to extracting GWR features. In Section 3, we 
present the ensemble model for integrating dif-
ferent types of features. In Section 4, the fast 
feature selection methods for WR features are 
proposed. Experimental results are reported in 
Section 5. Section 6 draws conclusions and out-
lines directions for future work. 
2 Generalized Word Relation Features 
A straightforward method for extracting WR 
features is to simply map word pairs into the 
feature vector. However, due to the sparse-data 
problem and the lack of generalization ability, 
the performance of WR is discounted. Consider 
the following two pieces of text: 
1) Avatar is a great movie. I definitely rec-
ommend it. 
2) I definitely recommend this book. It is great.
We lay the emphasis on the following word 
pairs: great-movie, great-it, it-recommend, and 
book-recommend. Although these features are 
good indicators of sentiment, due to the sparse-
data problem, they may not contribute as impor-
tantly as we have expected in machine learning 
algorithms. Moreover, the effects of those fea-
tures would be greatly reduced when they are 
not captured in the test dataset (for example, a 
new feature great-song in the test set would 
never benefit from great-movie and great-it).
Joshi and Ros? (2009) back off the head word 
in each of the relation pairs to its POS tag. Tak-
ing great-movie for example, the back-off fea-
ture will be great-noun. With such a transforma-
tion, original features like great-movie, great-
book and other great-noun pairs are regarded as 
one feature, hence, the learning algorithms 
could learn a weight for a more general feature 
that has stronger evidence of association with 
the class, and any new test sentence that con-
tains an unseen noun in a similar relationship 
with the adjective great (e.g., great-song) will 
receive some weight in favor of the class label. 
With the attempt to make a further generali-
zation, we conduct a POS clustering. Consider-
ing the effect of different POS tags in both uni-
grams and word relations, the POS tags are 
categorized as shown in Table 1. 
POS-cluster Contained POS tags 
J JJ, JJS, JJR 
R RB, RBS, RBR 
V VB, VBZ, VBD, VBN, VBG, VBP
N NN, NNS, NNP, NNPS, PRP 
O The other POS tags 
Table 1: POS Clustering (the Penn Corpus Style) 
Since adjectives and adverbs have the highest 
correlation with sentiment, and some verbs and 
nouns are also strong indicators of sentiment, 
we therefore put them into separate clusters. All 
the other tags are categorized to one cluster be-
cause they contain a lot of noise rather than use-
ful information. In addition, we assign pronouns 
to POS-cluster N, aimed at capturing the gener-
ality in WR features like great-movie and great-
it, or book-recommend and it-recommend.
1337
Taking ?Avatar is a great movie? for example, 
different types of WR features are presented in 
Table 2, where Uni denotes unigrams; WR-Bi 
indicates traditional bigrams; WR-Dp indicates 
word pairs of dependency relation; GWR-Bi 
and GWR-Dp respectively denote generalized 
bigrams and dependency relations. 
WR types WR features 
WR-Bi Avatar-is, is-a, a-great, great-movie 
WR-Dp 
Avatar-is, a-movie, great-movie, 
movie-is 
GWR-Bi 
Avatar-V, is-O, a-J, great-N, 
N-is, V-a, O-great, J-movie
GWR-Dp 
Avatar-V, a-N, great-N, movie-V, 
N-is, O-movie, J-movie
Table 2: Different types of WR features 
3 An Ensemble Model for Integrating 
WR Features 
3.1 Joint Features, Good Enough? 
Although the unigram feature space is simple, 
and the WR features are more sophisticated, the 
latter was mostly used as extra features in addi-
tion to the former, rather than to substitute it. 
Even so, in most of the literature, the improve-
ments of joint features are still not as good as 
we had expected. For example, Dave et al 
(2003) try to extract a refined subset of WR 
pairs (adjective-noun, subject-verb, and verb-
object pairs) as additional features to traditional 
unigrams, but do not get significant improve-
ments. In the experiments of Joshi and Ros? 
(2009), the improvements of unigrams together 
with WR features (even generalized WR 
features) are also not remarkable (sometimes 
even worse) compared to simple unigrams. 
One possible explanation might be that dif-
ferent types of features have distinct distribu-
tions, and therefore would probably yield vary 
performance on different machine learning al-
gorithms. For example, the generative model is 
optimal if the distribution is well estimated; 
otherwise the performance will drop signifi-
cantly (for instance, NB performs poorly unless 
the feature independence assumption holds 
well). While on the contrary, the discriminative 
model such as SVM is good at representing the 
complexity of relevant features. 
Let us review the results reported by Pang 
and Lee (2002) that compare different classifi-
cation algorithms: SVM performs significantly 
better than NB on unigrams; while the outcome 
is the opposite on bigrams. It is possibly due to 
that from unigrams to bigrams, the relevance 
between features is reduced (bigrams cover 
some relevance of unigram pairs), and the inde-
pendence between features increases. 
Since GWR features are less relevant and 
more independent in comparison, it is reason-
able for us to infer that these features would 
work better on NB than on SVM. We therefore 
intuitively seek to employ the ensemble model 
for sentiment classification tasks, with an at-
tempt to efficiently integrate different types of 
features under distinct classification models. 
3.2 Model Formulation 
The ensemble model (Kittler, 1998), which 
combines the outputs of several base classifiers 
to form an integrated output, has become an 
effective classification method for many do-
mains.
For our ensemble task, we train six base clas-
sifiers (the NB and SVM model respectively on 
the Uni, GWR-Bi and GWR-Dp features). By 
mapping the probabilistic outputs (for C  classes) 
of D base classifiers into the meta-vector 
11 1 1? [ , , , , , , ],C kj D DCo o o o ox ! ! ! !  (1) 
the weighted ensemble is formulized by 
1 1
? ?( ) ,
D D
j j k kj k k
k k
O g o xX X q 
 
  ? ?x D j  (2) 
where  is the weight assigned to the -th
ss
mization 
, we use descent 
 defined as 
k
X k
base cla ifier. 
3.3 Weight Opti
Inspired by linear regression
methods to seek optimization according to cer-
tain criteria. We employ two criteria, namely 
the perceptron criterion and the minimum clas-
sification error (MCE) criterion. 
The perceptron cost function is
1, ,
1
1 N
? ?max ( ) ( ) .
ip j i y ij C
i
J g g
N 
? ? ? ?? ?? x x!  (3) 
The minimization of 
p
J is approximately equal 
sc
 1992) 
is
function is given by 
to seek a minimum mi lassification rate. 
The MCE criterion (Juang and Katagiri,
supposed to be more relevant to the classifica-
tion error. A short version of MCE criterion 
1338
1 1
1
? ?( ) ( ( ) max ( ))
N C
mce i
i j
J I y j g g
N
E
 
   ?? x x  (4) j k
k jv
where  is the sigmoid function. 
For both criteria, stochastic gradient descent 
. SGD uses 
( )E <
(SGD) is utilized for optimization
approximate gradients estimated from subsets of 
the training data and updates the parameters in 
an online manner: 
( 1) ( ) ( )
h h
h
J
k k kX X I s   . (5) 
Xs
functions are respectively
The gradients of perceptron and MCE cost 
p
1
1
? ?( )
N
h
ih
J
x x
NX q 
s
 
s ?  (6) i iD s h D yq 
where
i
, and 
1, ,
?arg max ( )
i j
j C
s g

 x
!
MC ( ))
y i
Js E
1
1
? ? ? ?( )(1 ( )
i i i i
N
y i h D s h D y
ih
l l x x
NX q  q 
  
s ? x x  (7) 
where x?  and 
As for perceptron criterion, we employ the 
average perceptron (AvgP) (Freund and 
Sc
In the past decade, feature selection (FS) studies 
n.
pose a 
fast feature selection method that is specially 
designed for GWR features. In our method, the 
re  (e.g., great-
? ?( ) ( ( ) max ( ))
ij i y i k ih j
l g gE
v
  x x
, ;
?arg max ( )
i
i j i
C j y
s g
v
x
!
.
1,j

hapire, 1999), a variation of perceptron model 
that averages the weights of all iteration loops, 
to improve the generalization performance. 
4 Feature Selection for WR Features 
mainly focus on topical text classificatio
(Yang and Pedersen, 1997) investigate five FS 
metrics and reported that good FS methods 
(such as IG and CHI) can improve the categori-
zation accuracy with an aggressive feature re-
moval. In sentiment classification tasks, tradi-
tional FS methods were also proven to be effec-
tive (Ng et al, 2006; Li et al, 2009). 
With regard to WR features, since the dimen-
sion of feature space has sharply increased, the 
amount of computation is considerably large 
when employing traditional FS methods. 
4.1 Fast MI and Fast IG 
In order to address this problem, we pro
importance of a GWR featu ws
movie) is considered as two component parts: 
the non-back-off word w  (great) and the POS 
pairs s  (J-N). We calculate the score of w  and 
s  respectively using existing FS methods, and 
take the sum of them as the final score. By as-
suming the two parts are mutually independent, 
the im ortance of a relation feature can be taken 
separately. We now give a theoretical support. 
First, the mutual information between a rela-
tion feature ws  and class 
k
c  is defined as 
p
( , )
( , ) log .
( ) ( )
k
k
k
P ws c
I ws c
P ws P c
  (8) 
If w  and s  are independent, they are condi-
tionally inde ndent. Thus e have pe w
( | )
( , ) log
( )
( | ) ( | )
log
( ) ( )
k k
P w c P s c
P w P s
x
( | ) ( | )
log log
( ) ( )
( , ) ( , ).
k
k
k k
k k
P ws c
I ws c
P ws
P w c P s c
P w P s
I w c I s c

 
 
 (9) 
ula (9) indicates that under the assum
tion that two component parts  and 
Form p-
w s  of a 
relation feature  are mutually
the mutual in
ws
formation of the relation feature 
independent,
( , )
k
I ws c  equals the sum of two component 
parts ( , )
k
I w c  and ( , )
k
I s c .
Since the aver  mutual information across 
all classes ( )
age
I ws  is the probabilistic sum of 
ss, it can be written as: each cla
.( ) ( ) ( )I ws I w I sx   (10) 
d Pe
 weighted average of 
Yang an dersen (1997) show that the in-
formation gain ( )G t  is the
( , )
k
I t c  and ( , )
k
I t c
can cons
. Therefore, with the sa
ider the infor
ula
t IG (FIG) respectively. Now let 
 of the independ-
ence assumption. In fact in a rel
tw
me 
mation gain of reason, we 
a relation feature ( )G ws  as the sum of two com-
ponent parts: 
( ) ( )G w G sx   (11) 
We refer to Form (10) and (11) as fast MI 
(FMI) and fas
us look back at the rationality
( )G ws
ation feature, 
o component parts are hardly independent 
since they are ?related?. Nonetheless, if we con-
1339
sider a GWR feature as a combination of the 
non-back-off word and the POS pairs, the as-
sumption will be easier to satisfy. Taking great-
movie (great-N) for example, compared to great
and N, great and J-N are more independent (J-N 
covers some relation information), therefore it is 
more feasible to take ( ) (J-N)G great G  as an 
approximation of ( -N)G great .
Laying aside the assumption, we place em-
phasis on the advantage of FIG (FMI) in com-
putational efficiency. A ension 
of the unigrams feature sp
ssuming the dim
ace is , and ignor-
in
escribed in section 3.2. In 
-
-
on
ense
 the effective-
lection. 
, 2004) is used in 
ment-level polarit
positive and 1,000 
N
N
g the data-sparse problem, the dimension of 
the GWR feature space is 2 5 Nq q  (backing off 
head/modifier word to 5 POS-cluster). Tradi-
tional IG (MI) feature selection needs to calcu-
late the score of all 10 Nq  features, while FIG 
(FMI) only needs to comp  words and 
25 POS pairs. That is to say, FIG (FMI) can 
speed up the computation of traditional IG (MI) 
by at most 10 times. 
4.2 Integration with the Ensemble Model 
We now present how FMI (FIG) is applied to 
the ensemble model d
ute for 
each of the six base-classifiers described in Sec
tion 3.2, feature selection is performed (tradi
tional IG on unigrams, FIG on GWR features).  
Note that when performing FIG on individual 
GWR feature sets, the computation of non-
back-off word ( )G w , is taken care of by having 
already computed IG on unigrams. Thus, we
ly need to compute the score of 25 POS pairs. 
From this point of view, FIG (FMI) is quite 
suitable for the mble model. 
5 Experiments
We first present the performance of system per-
formance, and then demonstrate
ness of fast feature se
5.1 Experimental Setup 
Datasets: The Cornell movie-review dataset 1
introduced by (Pang and Lee
our experiments. It is a docu
dataset that contains 1,000 
y
negative processed reviews. 
1 http://www.cs.cornell.edu/people/pabo/movie-review-data/
We also use the dataset2 introduced in (Joshi 
and Penstein-Ros?, 2009) for comparison. It is a 
su
t and the E-product dataset is 
at
l (McCallum 
ts are 
d ?enko, 2004) is employed. 
Ta
e accuracy. 
gan-
per-
bset (200 sentences each for 11 different 
products) of the product review dataset released 
by (Hu and Liu, 2004). We will refer to it E-
product dataset. 
The Movie dataset is a domain-specific docu-
ment-level datase
sentence-level and cross-domain. We conduct 
experiments on both of them to evaluate our 
approach in a wide range of tasks. 
Classifier: We implement the NB classifier 
based on a multinomial event mode
and Nigam, 1998) with Laplace smoothing. The 
tool LIBSVM3 is chosen as the SVM classifier. 
Setting of kernel function is linear kernel, the 
penalty parameter is set to one, and the Platt?s 
probabilistic output for SVM is applied to ap-
proximate the posterior probabilities. Term 
presence is used as the feature weighting. 
Implementation: The Movie dataset is evenly 
divided into 5 folds, and all the experimen
conducted with a 5-fold cross validation. Fol-
lowing the settings by Joshi and Ros?, an 11-
fold cross validation is applied to E-product 
dataset, where each test fold contains all the 
sentences for one of the 11 products, and the 
sentences for the remaining 10 products are 
used for training. 
For ensemble learning, the stacking frame-
work (D?eroski an
king the Movie dataset for example, in each 
loop of the 5-fold cross validation, the probabil-
istic outputs of the test fold are considered as 
test samples for ensemble leaning; and an inner 
4-fold leave-one-out procedure is applied to the 
training data, where samples in each fold are 
trained on the remaining three folds to obtain 
the probabilistic outputs which serve as training 
samples for ensemble learning. 
All the performance in the remaining tables 
and figures is in terms of averag
5.2 Results of Classification Accuracy 
The results of classification accuracy are or
ized in three parts. We first compare the 
formance of individual WR and GWR; secondly 
we compare joint features and the ensemble 
2 http://www.cs.cmu.edu/~maheshj/datasets/acl09short.html
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
1340
model; thirdly we compare different ensemble 
strategies; finally we make a comparison with 
some related work. 
5.2.1 WR vs. GWR 
Table 3 presents the re
feature sets. Four types 
sults of individual WR 
of WR features, includ-
ing WR-Bi, WR-Dp, GWR-Bi and GWR-Dp, 
are examined under two classification models 
on two datasets. For each of the results, we re-
port the best accuracy under feature selection. 
Model WR Feature Movie E-product
WR-Bi 83.05 63.27 
GWR-Bi 85.55 65.17 
WR-Dp 82.15 65.14 
SVM
GWR-Dp 83.40 67.09 
WR-Bi 84.60 66.86 
GWR-Bi 85.45 67.50 
WR-Dp 83.90 65.68 
NB
GWR-Dp 83.65 67.41 
Table 3: Acc ) of I al WR e 
Sets
formance of individua R and WR. With the 
SV
uracies (% ndividu  Featur
At first, we place the emphasis on the per-
l GW
M model, the performance of GWR features 
is remarkable compared to traditional WR pairs. 
Specifically, on the Movie dataset, GWR-Bi 
outperforms WR-Bi by 2.50%, and GWR-Dp 
outperforms WR-Dp by 1.35%; on the E-
product dataset, the improvements are 1.90% 
and 1.95%. Under the NB model, on the Movie 
dataset, GWR-Bi outperforms WR-Bi by 0.85%; 
on the E-product dataset, GWR-Bi outperforms 
WR-Bi by 0.64% and GWR-Dp outperforms 
WR-Dp by 1.73%. One exception is GWR-Dp 
on the Movie dataset, but the decline is slight 
(0.25%).
WR Feature Movie E-product 
WR-Bi 386k 21k 
GWR-Bi 152k 16k 
WR-Dp 455k 24k 
GWR-Dp 151k 16k 
Table ion of ual Fe Space 
fe -
ag
nt
in 
 s4: Dimen  Individ ature 
Secondly, we compare the dimensions of dif-
rent feature space. Table 4 presents the aver
e size of different types of feature spaces on 
two datasets. On the Movie dataset, the size of 
GWR feature space has been significantly re-
duced (386k vs. 152k in Bi; 455k vs. 151k in 
Dp). On the E-product dataset, since the training 
set are made up by 10 different domains, data 
are quite sparse, therefore, the extent of dimen-
sion reduction is not as sound as that on Movie 
dataset, but still considerable (21k vs. 16k in Bi; 
24k vs. 16k in Dp). 
5.2.2 Joint Features vs. Ensemble Model 
The performance of individual feature sets, joi
feature set and ensemble model is reported 
Table 5. Uni, GWR-Bi and GWR-Dp are used 
as individual features sets in the ensemble 
model, and Joint Features denote the union of 
three individual sets. For feature selection, IG is 
used in Joint Features, and FIG is used in the 
ensemble model. The reported results are in 
terms of the best accuracy under feature selec-
tion.
Feature and Model Movie E-product
SVM 85.20 67.77 
Uni
NB 84.10 66.18 
SVM 85.55 65.17 
GWR-Bi 
NB 85.45 67.50 
SVM 83.40 67.09 
GWR-Dp 
NB 83.65 67.41 
SVM 86.10 66.55 
Joint Features 
NB 85.20 67.64 
AvgP 88.60 70.14 
E
M  
nsemble Model
CE 88.55 70.18 
Table 5: Accuracies  Co t Fe
Joint Feature nsem odel
-
vidual emon-
str
els on different feature 
se
 (%) of mponen atures, 
s and E ble M
To begin with, we observe the results of indi
feature sets. Although we have d
ated that GWR features are more effective 
than WR, it is a pity that they do not show sig-
nificant superiority (sometimes even worse) 
compared to unigrams. That is to say, although 
GWR features encode more generalized word 
relation information than WR features, the role 
of unigrams still can not be replaced. This is in 
accordance with that, WR (GWR) features are 
used as additional features to assist unigrams in 
most of the literature.  
Secondly, we focus on the performance of 
two classification mod
ts. SVM seems to work better than NB on 
unigrams (more than 1%); while on GWR-Bi 
and GWR-Dp feature sets, NB tends to be over-
all effective. This has confirmed our speculation 
that WR features perform better under NB than 
under SVM (since independence between fea-
tures increases) and strengthened the confidence 
1341
of our motivation to ensemble different types of 
features under distinct classification models. 
Finally, we make a comparison of Joint Fea-
tures and Ensemble model. Observing the re-
su
he result of Joint Features is even 
w
ifferent
lts on the Movie dataset, Joint Features ex-
ceed individual feature sets, but the improve-
ments are not remarkable (less than 1 percent-
age compared to the best individual score). 
While the results of the ensemble model, as we 
have expected, are fairly good. AvgP and MCE 
respectively get the scores of 0.886 and 0.8855, 
robustly higher than that of Joint Features 
(0.8610 and 0.8520 respectively under SVM 
and NB). 
On the E-product dataset, it is quite surpris-
ing that t
orse than some of the individual features sets. 
This also confirms that Joint Features are some-
times not so effective at exploring different 
types of features. With regard to the ensemble 
model, AvgP gets an accuracy of 0.7014 and 
MCE achieves the best score (0.7018), consis-
tently superior to the results of Joint Features. 
5.2.3 Different Ensemble Strategies 
We also examine the performance of d
strategies. In Table 6, three ensemble strategies 
are compared, where  ?(Uni & Bi & Dp ) @ 
SVM? denotes ensemble of three kinds of fea-
ture sets with the fixed SVM classifier,  ?Uni @ 
(NB & SVM)? denotes ensemble of two classi-
fiers on fixed unigram features, and ?(Uni & Bi 
& Dp ) @ (NB & SVM)? denotes ensemble of 
both classifiers and feature sets. 
Ensemble Strategy Movie E-product
AveP 86.60 69.50 (Uni & Bi & Dp )  
E@ SVM MC 86.60 69.59 
AveP 87.75 68.95 Uni
@ ( M) NB & SV MCE 87.80 69.14 
AveP 88.60 70.14 (Uni &  Dp ) 
@ (NB & SVM) 
 Bi &
MCE 88.55 70.18 
Table 6: Accuracies  Di Ens
Strategies.
f 
ensemble of either s or classifiers is 
ro
rams
ovie 
e-
lin
t result (0.679) on joint features of 
un
 for GWR 
of MI and 
-
se
 (%) of fferent emble 
Seen from Table 5 and 6, the performance o
 feature set
bustly better than any individual classifier, as 
well as the joint features on both datasets. With 
regard to ensemble of both feature sets and clas-
sification algorithms, it is the most effective 
compared to the above two ensemble strategies. 
This is in accordance with our motivation de-
scribed in Section 3.1. 
5.2.4 Comparison with Related Work 
We take the performance of SVM on unig
as the baseline for comparison. On the M
dataset, Pang and Lee (2004) and Ng et al 
(2006) reported the baseline accuracy of 0.871. 
But our baseline is 2 percentages lower (0.852). 
It is mainly due to that: 1) 0.871 was obtained 
by a 10-fold cross validation, and our result is 
get by 5-fold cross validation; 2) the result of 
the tool LibSVM is inferior of SVMlight by al-
most 1-2 percentages, since the penalty parame-
ter in LibSVM is fixed, while in SVMlight, the 
value is automatically adapted; 3) the baseline 
in Ng et al (2006) is obtained with length nor-
malization which play a role in performance. 
Ng et al reported the state of art best per-
formance (0.905), which outperforms the bas
e (0.871) by 3.4%. Our best result of ensem-
ble model (0.886) gets a comparable improve-
ment (3.40%) compared to our obtained base-
line (0.852).  
On the E-product dataset, Joshi and Ros? re-
ported the bes
igrams and their proposed GWR features. 
This is in accordance with our result of Joint 
Features (0.6655 by SVM and 0.6764 by NB). 
The superiority of our ensemble result is quite 
significant (0.7014 by AvgP and 0.7018 by 
MCE).
5.3 Results of Feature Selection 
In this part, we examine FMI and FIG
feature selection. The performance 
IG are also presented for comparison. The re-
sults on the Movie and E-product datasets are 
displayed in Figures 1 and 2 respectively. Due 
to space limit, we only report the results of 
GWR-Bi features for Movie and GWR-Dp fea-
tures for E-product. In each of the figures, the 
results under NB and SVM are both presented. 
At first, we observe the results of feature se-
lection for GWR-Bi features on the Movie data
t. At first glance, IG and FIG have roughly the 
same performance. IG-based methods are 
shown to be quite effective in GWR feature re-
duction. For example under the NB model, top 
2.5% (4000) GWR-Bi features ranked by IG 
and FIG achieve accuracies of 0.849 and 0.842 
1342
respectively, even better than the score with all 
features (0.8415).
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 150,000
0.6
0.65
0.7
0.75
0.8
0.85
Movie: Bi-wpc @ SVM
Feature number
Ac
cu
ra
cy
IG
FIG
MI
FMI
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 150,000
0.6
0.65
0.7
0.75
0.8
0.85
Movie: Bi-wpc @ NB
Feature number
Ac
cu
ra
cy
IG
FIG
MI
FMI
Figure 1: Feature Selection for GWR-Bi Features on 
the Movie Dataset 
0 2000 4000 6000 8000 10000 12000 14000 16000
0.45
0.5
0.55
0.6
0.65
0.7
E-product: Dp-wpc @ SVM
Feature number
Ac
cu
ra
cy
IG
FIG
MI
FMI
0 2000 4000 6000 8000 10000 12000 14000 16000
0.45
0.5
0.55
0.6
0.65
0.7
E-product: Dp-wpc @ NB
Feature number
Ac
cu
ra
cy WR features for sentiment classification. We 
have proposed a GWR feature extraction ap-
proach and an ensemble model to efficiently 
integrate different types of features. Moreover, 
we have proposed two fast feature selection 
methods (FMI and FIG) for GWR features. 
Individual GWR features outperform tr
IG
FIG
MI
FMI
Figure 2: Feature Selection for GWR-Dp features on 
We then ob  finer granu-
la
er-
fo
 size of E-
pr
omparisons are made ac-
co
he compu
ble model, when per-
o
6 Conclusions and Future Work 
adi-
tio
proved to be a good solution for se-
lecting GWR features. It is also worthy noting 
stu
the E-product dataset
serve IG vs. FIG in a
rity. When the selected features are few (less 
than 5%), IG performs significantly better than 
FIG, while the latter gradually approaches the 
former when the feature number increases: as it 
comes to 10-15%, their performance is quite 
close. From then on, FIG is consistently compa-
rable to IG, even sometimes slightly better.  
With regard to MI and FMI, although the p
rmance compared to IG and FIG is rather poor 
(the reason has been intensively studied by 
Yang and Pedersen, 1997). Our focus is the 
ability of FMI for approximating MI. From this 
point of view, FMI is by contrast effective, es-
pecially with more than 1/3 features. 
Compared to the Movie dataset, the
oduct dataset is much smaller, and the data 
are much sparser. Nevertheless, IG and FIG are 
still effective. On one hand, top 1.25% (2000) 
features ranked by IG yield a result better than 
(or comparable to) that with all features. On the 
other hand, FIG is still competent to be a good 
approximation to IG. 
All of the above c
rding to accuracies, and we now pay attention 
to computational efficiency. Taking the Movie 
dataset for example, IG needs to compute scores 
of information gain for all 152k  features, while 
FIG only needs to comput 5 5k q  scores, 
saving more than 70% of t tational 
load; on the E-product dataset, although the data 
are sparse, the rate of computation reduction is 
still significant (62.5%). 
Note that in the ensem
e 42
f rming FIG for individual GWR feature set, 
part of its inherent complexity is already taken 
care of by having already computed IG on Uni 
feature set, and we only need to compute the 
scores for 25 POS pairs. From this perspective, 
FIG is even more attractive in the ensemble 
model. 
The focus of this paper is exploring the use of 
nal WR features significantly, but they still 
can not totally substitute unigrams. The ensem-
ble model is quite effective at integrating uni-
grams and different types of WR feature, and 
the performance is significantly better than joint 
features.
FIG is 
that FIG is a general feature selection method 
for bigram features, even outside the scope of 
sentiment classification and text classification.  
In the future, we plan to make an in-depth
dy about why individual WR features are 
inferior to unigrams, and how to make the joint 
features more effective. We also plan to extend 
the use of GWR features to the task of transfer 
learning, which we think is a promising direc-
tion for future work. 
1343
Acknowledgment 
We thank Yufeng Chen, Shoushan Li, Ping Jian 
and the anonymous reviewers for valuable com-
ments and helpful suggestions. The research 
work has been partially funded by the Natural 
Science Foundation of China under Grant No. 
60975053, 90820303 and 60736014, the Na-
tional Key Technology R&D Program under 
Grant No. 2006BAH03B02, the Hi-Tech Re-
search and Development Program (?863? Pro-
gram) of China under Grant No. 
2006AA010108-4, and also supported by the 
China-Singapore Institute of Digital Media 
(CSIDM) project under grant No. CSIDM-
200804. 
References
Kushal Dave, Steve Lawrence and David M. Pen-
nock, 2003. Mining the Peanut Gallery: Opinion 
Extraction and Semantic Classification of Product 
Reviews. In Proceedings of the international 
World Wide Web Conference (WWW), pages 
519-528. 
Sa?o D?eroski and Bernard ?enko, 2004. Is combin-
ing classifiers with stacking better than selecting 
the best one? Machine Learning, 54 (3). pages 
255-273. 
Yoav Freund and Robert E. Schapire, 1999. Large 
margin classification using the perceptron algo-
rithm. Machine Learning, 37 (3). pages 277-296. 
Michael Gamon, 2004. Sentiment classification on 
customer feedback data: noisy data, large feature 
vectors, and the role of linguistic analysis. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING). pages 841-847. 
Minqing Hu and Bing Liu, 2004. Mining and sum-
marizing customer reviews. In Proceedings of the 
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168-177. 
Mahesh Joshi and Carolyn Penstein-Ros?, 2009. 
Generalizing dependency features for opinion 
mining. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 313-316. 
Biing-Hwang Juang and Shigeru Katagiri, 1992. 
Discriminative learning for minimum error classi-
fication. IEEE Transactions on Signal Processing, 
40 (12). pages 3043-3054. 
J Kittler, 1998. Combining classifiers: A theoretical 
framework. Pattern Analysis and Applications, 1 
(1). pages 18-27. 
Shoushan Li, Rui Xia, Chengqing Zong and Chu-
Ren Huang, 2009. A framework of feature selec-
tion methods for text categorization. In Proceed-
ings of the Joint Conference of the 47th Annual 
Meeting of the Association for Computational 
Linguistics (ACL), pages 692-700. 
Andrew McCallum and Kamal Nigam, 1998. A com-
parison of event models for naive bayes text clas-
sification. In Proceedings of the AAAI workshop 
on learning for text categorization. 
Vincent Ng, Sajib Dasgupta and S. M. Niaz Arifin, 
2006. Examining the Role of Linguistic Knowl-
edge Sources in the Automatic Identification and 
Classification of Reviews. In Proceedings of the 
COLING/ACL, pages 611-618. 
Bo Pang and Lillian Lee, 2004. A Sentimental Edu-
cation: Sentiment Analysis Using Subjectivity 
Summarization Based on Minimum Cuts. In Pro-
ceedings of the Association for Computational 
Linguistics (ACL), pages 271-278. 
Bo Pang and Lillian Lee, 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2 (1-2). pages 1-135. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan, 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 79-86. 
Yiming Yang and Jan O. Pedersen, 1997. A com-
parative study on feature selection in text catego-
rization. In Proceedings of the 14th International 
Conference on Machine Learning (ICML), pages 
412-420. 
1344
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 398?408, Dublin, Ireland, August 23-29 2014.
 Dynamically Integrating Cross-Domain Translation Memory into 
Phrase-Based Machine Translation during Decoding 
 
 
Kun Wang?        Chengqing Zong?        Keh-Yih Su? 
?National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
?Institute of Information Science, Academia Sinica, Taiwan 
?
{kunwang, cqzong}@nlpr.ia.ac.cn 
?
kysu@iis.sinica.edu.tw 
 
 
Abstract 
Our previous work focuses on combining translation memory (TM) and statistical machine translation 
(SMT) when the TM database and the SMT training set are the same. However, the TM database will 
deviate from the SMT training set in the real task when time goes by. In this work, we concentrate on 
the task when the TM database and the SMT training set are different and even from different domains. 
Firstly, we dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real 
application. Secondly, we propose an improved integrated model to distinguish the original and the new-
ly-added phrase-pairs. Thirdly, a simple but effective TM adaptation method is adopted to favor the 
consistent translations in cross-domain test. Our experiments have shown that merging the TM phrase-
pairs achieves significant improvements. Furthermore, the proposed approaches are significantly better 
than the TM, the SMT and previous integration works for both in-domain and cross-domain tests. 
1 Introduction 
Since the translation memory (TM) system and the statistical machine translation (SMT) system com-
plement each other in those matched sub-segments and unmatched sub-segments (Wang et al., 2013), 
combining them can improve the output quality significantly, especially when high-similarity fuzzy 
matches are available. Therefore, combining TM and SMT is drawing more and more attention in re-
cent years (He et al., 2010a; 2010b; 2011; Koehn and Senellart, 2010; Zhechev and van Genabith, 
2010; Ma et al., 2011; Dara et al., 2013; Wang et al., 2013). 
Those previous works on combining TM and SMT can be classified into four categories: (1) select-
ing the better translation sentence from TM and SMT (He et al., 2010a; 2010b; Dara et al., 2013); (2) 
incorporating TM matched sub-segments into SMT in a pipelined manner (Koehn and Senellart, 2010; 
He et al., 2011; Ma et al., 2011); (3) only enhancing the SMT phrase table with new TM phrase-pairs 
(Bi?ici and Dymetman, 2008; Simard and Isabelle, 2009); and (4) incorporating the associated TM 
information with each source phrase to guide the SMT decoding (Wang et al., 2013). 
However, all previous works mentioned above only focus on the case in which the TM database and 
the SMT training set share the same data-set. Nonetheless, in real applications, the TM database will 
deviate from the SMT training set when time goes by, because the TM database will be dynamically 
enlarged when more translations are generated by the human translator. Therefore, this paper will con-
centrate on a more realistic case, in which the TM database and the SMT training set are different and 
even from different domains. 
When the TM database and the SMT training set share the same data-set, the integrated model 
(Wang et al., 2013) can avoid the drawbacks of the pipeline approaches and outperforms the other ap-
proaches significantly. However, this integrated model only refers to the TM information but not 
adopts the matched TM phrase-pairs as candidates during decoding. Therefore, many TM phrase-pairs 
cannot be covered by the SMT phrase table when the TM database and the SMT training set are dif-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
398
ferent. It is thus impossible to generate those unseen TM target phrases. This problem would even get 
worse when the TM database and the SMT training set are from different domains. 
To make the integrated model meet the real application, we dynamically merge the matched TM 
phrase-pairs into the SMT phrase table. In addition, an improved integrated model is proposed to dis-
tinguish the original SMT phrase-pairs and the newly-added ones extracted from TM. Furthermore, a 
simple but effective TM adaptation method is adopted to favor the consistent translation in cross-
domain test. To our best knowledge, this is the first unified framework for integrating TM into SMT 
during decoding when the TM database and the SMT training set are different (even from different 
domains). 
On the TM database which consists of Chinese?English computer technical documents, our experi-
ments have shown that merging the matched TM phrase-pairs achieves significant improvement when 
the fuzzy match score is above 0.5. Besides, the proposed approaches are significantly better than ei-
ther the SMT or the TM systems for both the in-domain and the cross-domain tests when the fuzzy 
match score is above 0.4. Furthermore, the proposed approaches also outperform previous integration 
works significantly in all test conditions. 
2 Integrated Model 
Wang et al. (2013) incorporated the TM information into the phrase-based SMT, and re-defined the 
translation problem as: 
 ?          ( |                         )  
Where   denotes the given source sentence,   is a corresponding target translation, and  ? is the final 
result; [                       ]  is the associated information of the best TM sentence-pairs; 
     and      are the corresponding TM source and target sentences, respectively;      denotes its 
corresponding fuzzy match score (from 0 to 1);     is the monolingual alignment information between 
  and     ; and      denotes the bilingual word alignment information between      and     . 
With the TM information, this problem can be simplified to: 
  ?        { (  ?
 | ? ( )
 ( ))  ?        ? ( )  (  |    ) 
 
   }  (1) 
Where  ? ( ) and   ? denote the k-th associated source and target phrases, respectively;     ? ( ) and 
     ?( ) are the corresponding TM source and target phrases associated with the given source phrase 
 ? ( ) (total K phrases without insertion).   is the corresponding TM target phrase matching status for 
the current target candidate   ?, which reflects the quality of the given candidate;    is the linking sta-
tus vector of  ? ( ) (the aligned source phrase, within  ? ( )
 ( )  of   ?), which indicates the matching and 
linking status in the source side (and is closely related to the matching status of the target side).      
is uniformly divided into ten fuzzy match intervals and the index   specifies the corresponding interval. 
In Equation (1), the first factor is just the typical phrase-based SMT model, and the second factor 
 (  |    ) is the information derived from the TM sentence pair. Afterwards, the factor  (  |    ) 
was further derived with TM matching status as follows: 
  (  |    )  {
 (    |                          )
  (    |                     )
  (    |                )
} (2) 
Where the first factor reflects the TM content matching status, the second factor is the relationship 
between various TM target phrases, and the third factor is the reordering information implied by TM. 
Equation (2) is adopted to guide the SMT decoding, and is denoted as the integrated Model-III in 
(Wang et al., 2013) (also called Model-III in this paper thereafter). 
For space limitation, only those features which are also adopted in our additional introduced proba-
bility factor (to be specified later) will be briefly introduced here: 
Target Phrase Content Matching Status (TCM): It indicates the content matching status between   ? 
and      ?( ) , and reflects the quality of   ? . It is a member of {Same, High, Low, NA (Not-
Applicable)}. 
399
Source Phrase Content Matching Status (SCM): It indicates the content matching status between 
 ? ( )  and     ? ( ) , and affects the matching status of   ?  and      ?( )  greatly. It is a member of 
{Same, High, Low, NA}. 
Number of Linking Neighbors (NLN): Usually, the context of a source phrase would affect its target 
translation. The more similar the context is, the more likely that the translation is the same. NLN is 
adopted to measure the context similarity. 
3 Proposed Approaches 
3.1 Merging the TM Phrase-Pairs 
Since all TM phrase-pairs are only referred while re-scoring the SMT candidates in Model-III, they are 
not regarded as candidates during decoding. When the TM database and the SMT training set are the 
same, this restriction is reasonable because the SMT phrase table can cover all the continuous TM 
phrase pairs within the phrase length limit. However, this would not be true when the TM database 
and the SMT training set are different. Therefore, the SMT phrase table should be further enhanced 
with those matched new TM phrase pairs in this case.  
According to their relations with the SMT phrase table, TM phrase pairs can be classified into three 
different categories: (1) the whole TM phrase-pair can be found in the original SMT phrase table; (2) 
only TM source phrase exists in the original SMT phrase table, but its corresponding target phrase 
does not; (3) even TM source phrase cannot be found in the original SMT phrase table. Since the first 
category has been covered by the original SMT phrase table, only the phrase-pairs from the second 
and the third categories should be added into the SMT phrase table dynamically for each input sen-
tence. To distinguish those newly added phrase-pairs from the original SMT phrase-pairs, we use eight 
additional feature weights    for the translation probability (lexical and phrase transfer in both direc-
tions) and two more feature weights for the phrase penalty (details will be specified later in Section 4). 
The above approach is inspired by the work of (Bi?ici and Dymetman, 2008). However, there are 
three differences between our approach and theirs. Firstly, we add all those matched TM phrase-pairs 
(include all associated sub-phrase pairs), while Bi?ici and Dymetman (2008) only added the longest 
matched one; Secondly, we add all the possible TM target phrase-pairs for a given TM source phrase 
while they extracted only one TM target phrase regardless of the existence of multiple TM target can-
didates; Lastly, we use different feature weights to distinguish those newly added TM phrase-pairs 
from the original SMT phrase-pairs, while they treated them equally. 
3.2 Distinguishing the TM Phrase-Pairs 
As mentioned in Section 3.1, we need to merge those TM matched phrase pairs into the SMT phrase 
table when the TM database and the SMT training set are different. However, the original integrated 
Model-III does not distinguish the newly added TM phrase-pairs from those original SMT phrase-
pairs in  (  |    ). Therefore, we introduce two new features Source Phrase Origin (SPO) and 
Target Phrase Origin (TPO), which are a member of {Original, Newly-Added}, to the original Mod-
el-III in (Wang et al., 2013) to favor the newly added TM phrase-pairs, and re-derive  (  |    ) as 
follows (assume that TPO is only dependent on SPO, NLN and  ): 
 
 (  |    ) 
  ([               ] |[                       ]   ) 
 
{
 
 
 (    |                          )
  (    |                     )
  (    |                )
  (    |           ) }
 
 
 
(2) 
The additional factor  (    |           ) in the above equation is added to handle those newly 
added TM phrase-pairs. This would be the proposed Distinguishing Model. For the phrases from the 
original SMT phrase table, both the SPO and TPO features would be ?Original?; for the phrases from 
the second category mentioned in Section 3.1, the SPO would be ?Original? but the TPO would be 
?Newly-Added?; for the phrases from the third category, both the SPO and TPO features would be 
?Newly-Added?. 
400
3.3 TM Adaptation 
In real applications, the TM database is usually not big enough to train an SMT system when it is ap-
plied to a special technical domain other than the news domain. Besides, many professional translators 
do not want to expose the whole TM database to the SMT system providers (Cancedda, 2012). In this 
situation, we will be forced to first train an SMT model on an out domain (usually the news domain) 
which possesses a lot of training data, and then fix the obtained phrase-based SMT model. Afterwards, 
we incorporate it on line with an additional TM database which is from another in domain. 
To simulate the above scenario, we will thus train our integrated model on the out domain. However, 
we have a domain-mismatch problem for this cross-domain test. Generally, in the technical domain, 
which is suitable for TM application, the translations (especially for technical terms) are much more 
consistent than that in the news domain. That is, the same source phrase in various places tends to 
have exactly the same translation in technical domains. Therefore, when we use Distinguishing Model 
to perform forced decoding, the obtained results would possess different statistics among the in-
domain development set and the out-domain training set. For example, at interval [0.9, 1.0), when 
SCM is ?Same?, 94.6% of TCM are ?Same? in the development set (in), while this ratio is only 65.1%  
in the training set (out). Therefore, the factor  (    |                          ) from the 
test set will possess a different probability distribution in comparison with that from the training set. 
However, the development set is not big enough (only a few hundreds sentence-pairs at each interval) 
to re-train all TM factors of the proposed model. Therefore, we simply add the following h1 feature to 
reflect the tendency of having high translation consistency in the development set: 
  ( ?  ?  ) {
                              
                                                         
 
Where  ? and  ? denote the source phrase, the target candidate, respectively. 
Furthermore, various source synonyms might generate the same translation (Zhu et al., 2013). 
Therefore, even SCM?Same, we still favor the SMT phrase-pair candidate which exactly matches TM 
target phrase. For example, if source words are synonyms such as ???? (want) and ??? (want), ??
?? (if) and ??? (if), ???? (at once) and ???? (at once), the target translations would be the same. 
Therefore, the issue of having high translation consistency in the technical domain is also applied. We 
thus further add the following h2 feature to reflect the tendency of having high translation consistency 
in this case (?High? and ?Low? are grouped into ?Other? for the SCM): 
  ( ?  ?  ) {
                               
                                                          
 
Afterwards, the associated feature weights are tuned on the development set. 
4 Experiments 
4.1 Experimental Setup 
We use the same TM data-set adopted by Wang et al. (2013), which is a Chinese?English TM data-
base consisting of computer technical documents. It includes about 267k sentence pairs. All the exper-
iments are conducted around this TM data-set. To compare the performances under different condi-
tions, the same development set and the test set will be shared by both in-domain and cross-domain 
tests. Since the associated SMT training-set and TM database will vary under different experimental 
configurations, they will be specified later in each sub-section. 
In this work, the translation memory system (denoted as TM) and the phrase-based machine transla-
tion system (denoted as SMT) are adopted as our two baseline systems. Following (Wang et al., 2013), 
for TM, the word-based fuzzy match score is adopted as the similarity measure; also, for the phrase-
based SMT system, the same Moses toolkit (Koehn et al., 2007) and the same set of following features 
are adopted: the phrase translation model, the language model, the distance-based reordering model, 
the lexicalized reordering model and the word penalty. The system configurations are as follows: GI-
ZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, ?intersec-
tion? refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use SRI Language Model 
401
toolkit (Stolcke, 2002) to train a 5-gram model with modified Kneser-Ney smoothing (Kneser and Ney, 
1995; Chen and Goodman, 1998) on the target-side (English) training corpus. All the feature weights 
and the weight for each probability factor are tuned on the development set with minimum-error-rate 
training (MERT) (Och, 2003). The maximum phrase length is set to 7 in our experiments. 
To compare our proposed models with those state-of-the-art methods, we re-implement two XML-
Markup approaches (Koehn and Senellart, 2010; and the upper bound version of (Ma et al, 2011)) and 
the Model-III (Wang et al., 2013) as three baseline systems, and denote them as Koehn-10, Ma-11-U 
and Model-III, respectively. Similar to (Wang et al., 2013), we only re-implement the XML-Markup 
method used in (Ma et al, 2011), but not their discriminative learning method. 
Following (Wang et al., 2013), we also train the TCM, LTC and CPM factors in the SMT training 
set with cross-fold translation. Since the TPO factor (conditioning on NLN and Distinguishing Model) 
is based on Model-III, we first use Model-III to generate the desired results on the development set via 
forced decoding, and then generate the training samples of TPO factor for Distinguishing Model.  
In this work, the translation performance is measured with case-insensitive BLEU-4 score (Papineni 
et al., 2002) and TER score (Snover et al., 2006). Statistical significance tests are conducted with re-
sampling (1,000 times) approach (Koehn, 2004) in 95% confidence level. 
4.2 In-Domain Translation Results 
In the in-domain test, the original TM dataset is first randomly divided into two parts. The first part is 
then adopted as the new TM database, while the second part is adopted as the SMT training set. The 
detailed corpus statistics is shown in Table 1. Since the TM database is different from that adopted in 
(Wang et al., 2013), the statistics shown in Table 2 at each interval is also different from theirs.  
All matched TM phrase-pairs are extracted according to the word alignment generated from the 
phrase-based SMT system. Since there are not enough samples to estimate the translation probabilities 
for those newly added TM phrase-pairs, we use the following method to assign the translation proba-
bilities. For those TM phrase-pairs that only their source phrases exist in the original SMT phrase table 
(the second category mentioned in Section 3.1), as their source phrases have already existed in the 
SMT phrase table, there is at least one associated target phrase in the original SMT phrase table. For 
each new TM phrase-pair, we thus directly assign the maximum probability among its associated orig-
inal target phrases to it. For those TM phrase-pairs that even their source phrase cannot be found in the 
original SMT phrase table (the third category), as there is no corresponding phrase-pair in the original 
SMT phrase table, we will simply assign probability ?1.0? (this value is not important as its associated 
weight will be tuned later) as their four translation probabilities. To distinguish those newly added 
phrase-pairs from the original SMT phrase-pairs, we use eight additional feature weights for the trans-
lation probability and two more feature weights for the phrase penalty. 
To evaluate the effectiveness of adding TM phrase-pairs, we compare the cases of whether merging 
TM phrase-pairs or not for both SMT and Model-III. Table 3 and Table 4 give the translation results in 
BLEU and TER, respectively. ?SMT? and ?Model-III? denote that we do not merge the TM phrase-
pairs into the SMT phrase table during decoding. That is, they only use the original SMT phrase table. 
  #Sentences #Chn. Words #Chn. VOC. #Eng. Words #Eng. VOC. 
New TM Database 130,953 1,808,992 30,164 1,811,413 30,807 
SMT Training Set 130,953 1,814,524 29,792 1,815,615 30,516 
Table 1: Corpus Statistics for In-Domain Tests 
Intervals 
[0.9, 
1.0) 
[0.8, 
0.9) 
[0.7, 
0.8) 
[0.6, 
0.7) 
[0.5, 
0.6) 
[0.4, 
0.5) 
[0.3, 
0.4) 
(0.0, 
0.3) 
(0.0, 
1.0) 
#Sentences 147 255 244 355 488 514 419 154 2,576 
#Words 2,431 3,438 3,299 4,674 6,125 7,525 7,082 4,074 38,648 
W/S 16.5 13.5 13.5 13.2 12.6 14.6 16.9 26.5 15.0 
Table 2: Corpus Statistics for In-Domain Test-Set (W/S: the average #words per sentence) 
402
?SMT+? and ?Model-III+? mean that we merge the TM phrase-pairs into the SMT phrase table dynam-
ically. In these tables, ?+? indicates that those newly added TM phrase-pairs significantly improve the 
translation results (?SMT? vs. ?SMT+?, ?Model-III? vs. ?Model-III+?, and ?Model-III? vs. ?Distin-
guishing?). 
It can be seen that adding TM phrase-pairs significantly improve the translation results when the 
fuzzy match score is above 0.5 (comparing SMT with SMT+, and Model-III with Model-III+). For ex-
ample, at interval [0.9, 1.0), those added TM phrase-pairs significantly improve the SMT system from 
63.65 to 73.55, and Model-III from 80.69 to 86.40. However, if Model-III+ is compared with Model-III, 
the improvements from merging the TM phrase-pairs get less when the fuzzy match score decreases, 
because the matched TM parts are fewer at low fuzzy match intervals. 
Also, with the same original SMT phrase table, Model-III exceeds the SMT system at each interval.  
For example, at interval [0.9, 1.0), the TM information significantly improve the translation result 
from 63.65 to 80.69. It thus shows that the TM information is very useful. However, it is still worse 
than the TM in TER (13.32 vs. 10.42). On the other hand, although Model-III has greatly exceeded the 
SMT at each interval, Model-III+ still significantly outperforms Model-III at most intervals. Therefore, 
the benefit of utilizing TM information and the benefit of adding TM phrase-pairs are not covered by 
each other and can be jointly enjoyed. Take the interval [0.9, 1.0) as an example, the TM information 
first improve the translation results from 63.65 (SMT) to 80.69 (Model-III), and then the added TM 
phrase-pairs further boosts it to 86.40 (Model-III+). 
Besides, Table 3 and Table 4 also present the translation results of our other two baselines (Koehn-
10 and Ma-11-U), and the proposed Distinguishing Model. Scores marked with  ?*?  indicate  that  
they are significantly better (p < 0.05) than both the TM and the SMT+ baselines, and those marked 
with ?#? are significantly better (p < 0.05) than Koehn-10. Scores marked with ?$? are significantly 
better than Model-III+. The bold entries are the best result at each interval. 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Koehn-10 Ma-11-U 
[0.9, 1.0) 79.89 63.65  73.55 + 80.69  86.40 +*# 86.69 +*# 82.21 67.58 
[0.8, 0.9) 72.65 60.75  74.04 + 78.95 * 83.35 +*# 83.44 +*# 79.50 * 67.03 
[0.7, 0.8) 59.59 60.57  65.52 + 68.55 * 71.37 +*# 72.06 +*# 67.52 62.60 
[0.6, 0.7) 41.57 53.38  56.14 + 55.61 # 57.75 +*# 58.73 +*#$ 51.83 56.74 
[0.5, 0.6) 25.17 45.60  46.95 + 47.40 # 48.39 +*# 48.27 *# 39.08 47.94 
[0.4, 0.5) 14.62 41.81  42.03  42.60 # 42.30 # 43.04 *#$ 31.60 42.93 
[0.3, 0.4) 7.50 35.95  35.49  36.10 # 35.31 # 35.34 # 25.25 36.58 
(0.0, 0.3) 4.94 32.64  33.22  33.45 # 33.23 # 33.23 # 23.70 33.10 
(0.0, 1.0) 31.11 46.68  49.41 + 51.00 *# 52.26 +*# 52.56 +*#$ 44.28 48.91 
Table 3: In-Domain Translation Results (BLEU). Scores marked with ?+? indicates that those newly 
added TM phrase-pairs significantly (p < 0.05) improve the translation results (?SMT? vs. ?SMT+?, 
?Model-III? vs. ?Model-III+?, and ?Model-III? vs. ?Distinguishing?). Scores marked with ?*? are sig-
nificantly better (p < 0.05) than both TM and SMT+ systems, and those marked with ?#? are signifi-
cantly better (p < 0.05) than Koehn-10. Scores marked with ?$? are significantly better  (p < 0.05) than 
Model-III+ (?Model-III+? vs. ?Distinguishing?) 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Koehn-10 Ma-11-U 
[0.9, 1.0) 10.42  27.14  17.64 + 13.32  8.76 +*# 8.22 +*# 12.95 23.94 
[0.8, 0.9) 16.07  28.73  17.66 + 14.69 * 10.46 +*# 10.49 +*# 14.72 * 23.83 
[0.7, 0.8) 28.68  29.47  24.99 + 22.01 * 20.15 +*# 19.33 +*# 23.96 27.43 
[0.6, 0.7) 48.59  33.76  31.53 + 31.57 # 29.77 +*# 28.95 +*#$ 36.89 30.98 
[0.5, 0.6) 63.13  40.57  39.00 + 38.79 # 38.00 *# 38.51 # 47.08 38.44 
[0.4, 0.5) 74.02  44.09  43.66  42.84 *# 43.43 # 42.88 *#$ 55.35 42.31 
[0.3, 0.4) 81.09  50.00  50.63  50.04 # 50.70 # 50.90 # 63.28 48.83 
(0.0, 0.3) 84.34  55.58  56.66  54.68 # 55.96 *# 55.96 *# 68.00 54.51 
(0.0, 1.0) 58.58  40.88  38.55 + 37.26 *# 36.47 +*# 36.28 +*# 45.63 38.73 
Table 4: In-Domain Translation Results (TER). The marks are the same as that in Table 3. 
403
In comparison with the TM and the SMT+ systems, Model-III+ is significantly better than both of 
them in either BLEU or TER scores when the fuzzy match score is above 0.5; also, Distinguishing 
Model outperforms both the TM and the SMT+ systems in either BLEU or TER scores when the fuzzy 
match score is above 0.4. Furthermore, the improvements from both Model-III+ and Distinguishing 
Model get less when the fuzzy match score decreases, as the TM information is less reliable at low 
fuzzy match intervals. 
Across all intervals (the last row in the table), Distinguishing Model not only achieves the best 
BLEU score (52.56), but also gets the best TER score (36.28). At those intervals when the fuzzy 
match score is above 0.4, Model-III+ and Distinguishing Model are the best two in either BLEU or 
TER scores. Besides, Distinguishing Model slightly exceeds Model-III+ at most intervals. However, 
both Model-III+ and Distinguishing Model achieve significant improvements over the TM and the 
SMT+. 
Compared with previous works, it can be seen that both Model-III+ and Distinguishing Model sig-
nificantly outperform Koehn-10 in either BLEU or TER scores at all intervals, and are significantly 
better than Model-III when the fuzzy match score is above 0.6. Furthermore, the proposed approaches 
(both Model-III+ and Distinguishing Model) achieve a much better TER score than the TM system 
does at the interval [0.9, 1.0); while Model-III and Koehn-10 are worse than the TM system at this 
interval. Also, both Model-III+ and Distinguishing Model exceed Ma-11-U at most intervals. There-
fore, it can be concluded that the proposed models outperform previous approaches significantly in 
this scenario. 
To further verify the proposed approaches in this case, we swap the TM database and the SMT 
training set and re-run the experiments. Similar and significant improvements are still observed: both 
Model-III+ and the Distinguishing Model achieve significant improvements over the TM and the 
SMT+. All those results have shown that the proposed approaches are robust. 
In real environments, the SMT training set and the TM database could be the same before transla-
tion projects starts. However, the TM database will gradually deviate from the SMT training set while 
the translation task progresses.  Nonetheless, our experiments have shown that the proposed Distin-
guishing Model is effective even when the TM database and the SMT training set are totally different 
(which would be the extreme case for real applications). Therefore, it can be concluded that this pro-
posed approach is robust. 
4.3 Cross-Domain Translation Results 
To evaluate the cross domain performance, we adopt the news corpora about computer and science 
from CWMT09 (Liu and Zhao, 2009) as the SMT training set, and adopt the whole TM dataset as the 
TM database. The SMT training set includes about 404k bilingual sentence-pairs (which includes 
about 9M Chinese words and 8.7M English words). Corpus statistics is shown in Table 5. Since the 
TM database and the test set (also the development set) are the same as that in (Wang et al., 2013), the 
statistics at each interval is the same as theirs but different from Table 2. 
The training procedure is the same as that mentioned in the last sub-section. Table 6 and Table 7 
present the translation results of TM, SMT, SMT+, two baselines (Koehn-10 and Model-III), and three 
proposed approaches (Model-III+, Distinguishing and Adaptation). The Adaptation approach means 
that we add two consistent related features based on Distinguishing Model (Section 3.3). All the for-
mats are the same as that adopted in Table 3 and Table 4. Besides, scores marked by ?&? are signifi-
cantly better than Distinguishing Model. 
Comparing the TM with the SMT, the performance of in-domain TM significantly exceeds that of 
out-domain SMT. Since the fuzzy match intervals are divided according to the TM database, the trans-
lation result of the SMT system at interval [0.8, 0.9) even slightly outperforms that at interval [0.9, 
1.0). Besides, adding TM phrase-pairs significantly improves the translation results when the fuzzy 
match score is above 0.5 (SMT vs. SMT+, and Model-III vs. Model-III+). Furthermore, the benefit of 
utilizing TM information and the benefit of adding TM phrase-pairs are not covered by each other, and 
can be jointly enjoyed. Furthermore, compared with TM, SMT, SMT+ and Model-III, both Model-III+ 
and Distinguishing Model achieve better translation results when the fuzzy match score is above 0.4. 
All observed trends are similar to that in the last sub-section. 
 
404
   #Sentences #Chn. Words #Chn. VOC. #Eng. Words #Eng. VOC. 
TM Database 261,906 3,623,516 43,112 3,627,028 44,221 
SMT Training Set 404,172 9,007,614 102,073 8,737,801 107,883 
Table 5: Corpus Statistics for Cross-Domain Tests 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Adaptation Koehn-10 
[0.9, 1.0) 81.31 30.87 64.74 + 64.79 82.28 + 83.19 +*$ 84.89 *#$& 81.52 
[0.8, 0.9) 73.25 31.94 60.13 + 61.91 74.21 + 74.72 +* 79.78 *#$& 76.47 * 
[0.7, 0.8) 63.62 30.63 51.64 + 51.44 62.94 + 63.32 + 67.74 *$& 67.12 *$& 
[0.6, 0.7) 43.64 28.95 39.94 + 38.28 46.28 +* 46.46 +* 49.49 *$& 48.47 * 
[0.5, 0.6) 27.37 27.61 32.49 + 28.85 34.50 +* 34.87 +* 37.12 *#$& 35.25 * 
[0.4, 0.5) 15.43 27.16 27.35 27.30 # 27.47 # 27.82 # 28.80 *#$& 25.10 
[0.3, 0.4) 8.24 23.85 22.66 23.81 # 22.41 # 22.41 # 22.95 # 20.72 
(0.0, 0.3) 4.13 24.64 24.25 24.24 # 23.65 # 24.12 # 24.31 # 18.79 
(0.0, 1.0) 40.17 28.30 40.59 + 40.47 47.37 +* 47.70 +*#$ 49.79 *#$& 47.09 * 
Table 6: Cross-Domain Translation Results (BLEU). The marks are the same as that in Table 3. Be-
sides, scores marked by ?$? are significantly better  (p < 0.05) than Model-III+, and those marked by 
?&? are significantly better than ?Distinguishing? (?Adaptation? vs. ?Distinguishing?). 
Intervals TM SMT SMT+ Model-III Model-III+ Distinguishing Adaptation Koehn-10 
[0.9, 1.0) 9.79 54.54 27.07 + 27.09 11.81 + 11.01 + 9.58 #$& 13.51 
[0.8, 0.9) 16.21 52.86 29.33 + 28.04 17.13 + 17.47 + 13.80 *#$& 17.29 
[0.7, 0.8) 27.79 52.42 36.48 + 35.56 27.07 + 26.40 +$ 23.04 *$& 24.31 *$& 
[0.6, 0.7) 46.40 54.74 47.39 + 48.06 41.13 +* 40.36 +*$ 37.45 *#$& 40.16 * 
[0.5, 0.6) 62.59 57.18 53.08 + 56.78 51.77 +* 51.60 +* 48.08 *#$& 51.57 
[0.4, 0.5) 73.93 57.19 56.57 57.19 # 56.82 # 56.53 # 54.42 *#$& 61.32 
[0.3, 0.4) 79.86 60.62 61.16 61.35 # 61.31 # 61.31 # 60.33 #$& 68.82 
(0.0, 0.3) 85.31 63.62 62.81 62.22 # 63.04 # 62.07 # 61.87 # 74.85 
(0.0, 1.0) 50.51 56.42 46.89 + 47.38 # 41.63 +*# 41.27 +*#$ 38.87 *#$& 43.95 * 
Table 7: Cross-Domain Translation Results (TER). The marks are the same as that in Table 6. 
However, both Model-III+ and Distinguishing Model are worse than Koehn-10 at some high fuzzy 
match intervals. The reason is that the TM factors are trained on the news domain but the test set is 
from computer technical domain. Therefore, it is not strange that the Adaptation approach achieves the 
best translation results at all intervals in either BLEU or TER when the fuzzy match score is above 0.4. 
At most intervals, the Adaptation approach significantly outperforms Koehn-10 in either BLEU or 
TER, especially for the high fuzzy match intervals such as [0.9, 1.0) and [0.8, 0.9). Furthermore, the 
Adaptation approach achieves better TER than the TM system and Koehn-10 at intervals [0.9, 1.0) and 
[0.8, 0.9). All obtained results have shown that the Adaptation approach is effective and robust for 
cross-domain test. Moreover, it can be seen that the h1 feature (mentioned in Section 3.3) is more ef-
fective than the h2 feature. 
5 Related Work 
According to the way of combination, those previous works can be classified into four categories (as 
specified in Section 1). The first category uses a classifier (or a re-ranker) to judge whether TM or 
SMT gives a better translation sentence, and then delivers the better one to the post-editor (He et al., 
2010a; He et al., 2010b; Dara et al., 2013). Since the outputs of SMT and TM are not merged but only 
re-ranked, the possible improvement resulted from those approaches is quite limited. 
The second category incorporates TM matched parts into the SMT input sentence in a pipelined 
manner (Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; He et al., 2011; Ma et al., 
2011). These approaches usually translate the sentence in two stages: (1) first determine whether the 
405
extracted TM sentence pair should be adopted or not, and then merge the relevant translations of 
matched parts into the input sentence; (2) then force the SMT system to only translate those un-
matched parts at decoding. There are three drawbacks for this kind of pipeline approaches (Wang et al., 
2013). Firstly, whether those matched parts should be adopted or not is determined at the sentence lev-
el. Secondly, they select only one TM target phrase before decoding. Thirdly, they do not utilize the 
SMT probabilistic information for the matched parts. 
The third category mainly adds the longest matched TM phrase pairs into the SMT phrase table 
(Bi?ici and Dymetman, 2008; Simard and Isabelle, 2009), and associates them with a fixed large prob-
ability value to favor the TM target phrase. However, they only add one aligned target phrase for each 
matched source phrase and did not distinguish the original and the newly-added phrase-pairs. 
The last category incorporates the associated TM information of each source phrase into the SMT 
during decoding (Wang et al., 2013). This category can avoid the drawbacks of the pipeline approach-
es, and thus achieves superior results when the TM database and the SMT training set are the same. 
However, they only refer to the TM information and do not regard the TM phrase-pairs as candidates 
during decoding. Therefore, the superiority of this approach disappears when the TM database and the 
SMT training set are different, because many TM phrase-pairs cannot be found in the original SMT 
phrase table in this case. 
Our approach combines the strength of both the third and the last categories. During decoding, the 
associated TM information is referred to re-score the SMT candidates. At the same time, all matched 
TM phrase-pairs are dynamically merged into the phrase table. Moreover, this is the first unified 
framework for integrating TM into SMT at decoding when the TM database and the SMT training set 
are different. Although some previous works of the second and third categories can be also applied 
when the TM database and the SMT training set are different, they did not explicitly focus on and test 
this case.  
Last, since the example-based machine translation (EBMT, [Nagao, 1984]) is similar to that of us-
ing TM, some approaches (Watanabe and Sumita, 2003; Smith and Clark, 2009; Dandapat et al., 2011; 
2012; Phillips, 2011) also combined EBMT with SMT. It would be interesting to compare our ap-
proaches with theirs in the future. 
6 Conclusion 
Combining TM and SMT can greatly improve the translation performance and reduce human post-
editing effort. In comparison with those previous approaches, our work makes the following contribu-
tions: 
(1) Dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real ap-
plication;  
(2) Propose an improved integrated model to distinguish the original SMT phrase-pairs from the 
newly-added ones extracted from TM;  
(3) Adopt a simple but effective TM adaptation method to favor the consistent translation in cross-
domain test. 
This is the first work adopting a unified framework to integrate the TM information into the SMT 
model during decoding when the TM database and the SMT training set are different. On the TM da-
tabase which consists of Chinese?English computer technical documents, our experiments have shown 
that merging the TM phrase-pairs achieves significant improvements when the fuzzy match score is 
above 0.5. Furthermore, the proposed approaches are significantly better than either the SMT or the 
TM systems for both the in-domain and the cross-domain tests. Last, the proposed approaches outper-
form previous works significantly in all test conditions. 
Acknowledgements 
This research work was partially funded by the Natural Science Foundation of China under Grant No. 
61333018, the Hi-Tech Research and Development Program (?863? Program) of China under Grant 
No. 2012AA011101, the Key Project of Knowledge Innovation Program of Chinese Academy of Sci-
ences under Grant No. KGZD-EW-501, and Toshiba (China) R&D Center. 
 
406
Reference 
Ergun Bi?ici and Marc Dymetman. 2008. Dynamic translation memory: using statistical machine translation to 
improve translation memory fuzzy matches. In Proceedings of the 9th International Conference on Intelligent 
Text Processing and Computational Linguistics (CICLing 2008), pages 454?465. 
Nicola Cancedda. 2012. Private Access to Phrase Tables for Statistical Machine Translation. In Proceedings of 
the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 23?27. 
Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University Center for Research in Computing Technology. 
Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation, In Proceedings of 
the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 263?270. 
Sandipan Dandapat, Sara Morrissey, Andy Way, and Mikel L Forcada. 2011. Using example-based MT to sup-
port statistical MT when translating homogeneous data in resource-poor settings, In Proceedings of the 15th 
Annual Meeting of the European Association for Machine Translation (EAMT 2011), pages 201?208. 
Sandipan Dandapat, Sara Morrissey, Andy Way, and Joseph Van Genabith. 2012. Combining EBMT, SMT, TM 
and IR technologies for quality and scale, In Proceedings of the Joint Workshop on Exploiting Synergies be-
tween Information Retrieval and Machine Translation (ESIRMT) and Hybrid Approaches to Machine Trans-
lation (HyTra), pages 48?58. 
Aswarth Dara, Sandipan Dandapat, Declan Groves, and Josef van Genabith. TMTprime: a recommender system 
for MT and TM integration. In Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13. 
Yifan He, Yanjun Ma, Josef van Genabith and Andy Way, 2010a. Bridging SMT and TM with translation rec-
ommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics 
(ACL), pages 622?630. 
Yifan He, Yanjun Ma, Andy Way, and Josef Van Genabith. 2010b. Integrating N-best SMT outputs into a TM 
system, In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), 
pages 374?382. 
Yifan He, Yanjun Ma, Andy Way and Josef van Genabith. 2011. Rich linguistic features for translation memory-
inspired consistent translation. In Proceedings of the Thirteenth Machine Translation Summit, pages 456?463. 
Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceed-
ings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181?184. 
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388?395, Barcelona, 
Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke 
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer and Ond?ej Bojar. 2007. Moses: Open source 
toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 
177?180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of 
the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on 
Human Language Technology, pages 48?54. 
Philipp Koehn and Jean Senellart. 2010. Convergence of translation memory and statistical machine translation. 
In AMTA Workshop on MT Research and the Translation Industry, pages 21?31. 
Liu, Qun and Hongmei Zhao. 2009. Report on CWMT2009 MT Translation Evaluation. In Proceedings of the 
5th China Workshop on Machine Translation (CWMT2009), pages 1?31, Nanjing, China. 
Yanjun Ma, Yifan He, Andy Way and Josef van Genabith. 2011. Consistent translation using dis-criminative 
learning: a translation memory-inspired approach. In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1239?1248, Portland, Oregon. 
Makoto Nagao, 1984. A framework of a mechanical translation between Japanese and English by anal-ogy prin-
ciple. In: Banerji, Alick Elithorn and  Ran-an (ed). Artifiical and Human Intelligence: Edited Review Papers 
Presented at the International NATO Symposium on Artificial and Human Intelligence. North-Holland, Am-
sterdam, 173?180. 
407
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st 
Annual Meeting of the Association for Computational Linguistics (ACL), pages 160?167. 
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. 
Computational Linguistics, 29 (1). pages 19?51. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 311?318. 
Aaron B. Phillips, 2011. Cunei: open-source machine translation with relevance-based models of each transla-
tion instance. Machine Translation, 25 (2). pages 166-177. 
Michel Simard and Pierre Isabelle. 2009. Phrase-based machine translation in a computer-assisted translation 
environment. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120?127. 
James Smith and Stephen Clark. 2009. EBMT for SMT: a new EBMT-SMT hybrid. In Proceedings of the 3rd 
International Workshop on Example-Based Machine Translation (EBMT'09), pages 3?10, Dublin, Ireland. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of Association for Ma-chine Translation in the 
Americas (AMTA-2006), pages 223?231. 
Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, pages 311?318. 
Taro Watanabe, Eiichiro Sumita. 2003. Example-based decoding for statistical machine translation, In Proceed-
ing of Machine Translation Summit IX, pages 410?417. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2013. Integrating translation memory into phrase-based machine 
translation during decoding. In Proceedings of the 51st Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 11?21. 
Ventsislav Zhechev and Josef van Genabith. 2010. Seeding statistical machine translation with translation 
memory output through tree-based structural alignment. In Proceedings of the 4th Workshop on Syntax and 
Structure in Statistical Translation, pages 43?51. 
Xiaoning Zhu, Zhongjun He, Hua Wu, Haifeng Wang, Conghui Zhu, and Tiejun Zhao. 2013. Improving pivot-
based statistical machine translation using random walk. In Proceedings of the 2013 Conference on Empirical 
Methods in Natural Language Processing, pages 524?534. 
408
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 304?314,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Inference for Bilingual Semantic Role Labeling
Tao Zhuang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{tzhuang, cqzong}@nlpr.ia.ac.cn
Abstract
We show that jointly performing semantic role
labeling (SRL) on bitext can improve SRL
results on both sides. In our approach, we
use monolingual SRL systems to produce ar-
gument candidates for predicates in bitext at
first. Then, we simultaneously generate SRL
results for two sides of bitext using our joint
inference model. Our model prefers the bilin-
gual SRL result that is not only reasonable on
each side of bitext, but also has more consis-
tent argument structures between two sides.
To evaluate the consistency between two argu-
ment structures, we also formulate a log-linear
model to compute the probability of aligning
two arguments. We have experimented with
our model on Chinese-English parallel Prop-
Bank data. Using our joint inference model,
F1 scores of SRL results on Chinese and En-
glish text achieve 79.53% and 77.87% respec-
tively, which are 1.52 and 1.74 points higher
than the results of baseline monolingual SRL
combination systems respectively.
1 Introduction
In recent years, there has been an increasing inter-
est in SRL on several languages. However, little
research has been done on how to effectively per-
form SRL on bitext, which has important applica-
tions including machine translation (Wu and Fung,
2009). A conventional way to perform SRL on bi-
text is performing SRL on each side of bitext sep-
arately, as has been done by Fung et al (2007) on
Chinese-English bitext. However, it is very difficult
to obtain good SRL results on both sides of bitext
in this way. The reason is that even the state-of-
the-art SRL systems do not have very high accuracy
on both English text (Ma`rquez et al, 2008; Pradhan
et al, 2008; Punyakanok et al, 2008; Toutanova et
al., 2008), and Chinese text (Che et al, 2008; Xue,
2008; Li et al, 2009; Sun et al, 2009).
On the other hand, the semantic equivalence be-
tween two sides of bitext means that they should
have consistent predicate-argument structures. This
bilingual argument structure consistency can guide
us to find better SRL results. For example, in Fig-
ure 1(a), the argument structure consistency can
guide us to choose a correct SRL result on Chinese
side. Consistency between two argument structures
is reflected by sound argument alignments between
them, as shown in Figure 1(b). Previous research has
shown that bilingual constraints can be very help-
ful for parsing (Burkett and Klein, 2008; Huang et
al., 2008). In this paper, we show that the bilingual
argument structure consistency can be leveraged to
substantially improve SRL results on both sides of
bitext.
Formally, we present a joint inference model to
preform bilingual SRL. Using automatic word align-
ment on bitext, we first identify a pair of predicates
that align with each other. And we use monolin-
gual SRL systems to produce argument candidates
for each predicate. Then, our model jointly generate
SRL results for both predicates from their argument
candidates, using integer linear programming (ILP)
technique. An overview of our approach is shown in
Figure 2.
Our joint inference model consists of three com-
ponents: the source side, the target side, and the ar-
304
In recent years the pace of opening up to the outside of China `s construction market    has   further    accelerated
[    AM-TMP    ]  [                                                        A1                                                 ]          [   A2  ]   [   Pred   ]
R1:    [                A1                  ]    [ AM-TMP ]    [            C-A1           ]     [ AM-ADV ]    [Pred]
R2:    [                                                     A1                                           ]     [ AM-ADV ]    [Pred]
?? ?? ?? ?? ? ? ? ?? ?? ??? ??
      zhongguo jianzhu shichang      jinnian lai        dui wai kaifang bufa         jinyibu         jiakuai
[    AM-TMP   ] [                                                        A1                                                  ]         [   A2   ]  [    Pred    ]
In recent years the pace of opening up to the outside of China `s construction market    has   further    accelerated
?? ?? ?? ?? ? ? ? ?? ?? ??? ??
[ A1 ] [ AM-TMP ] [ C-A1          ]    [AM-ADV]    [Pred]
(a) Word alignment and SRL results for a Chinese-English predicate pair.
(b) Argument alignments for a Chinese-English predicate pair.
Figure 1: An example from Chinese-English parallel PropBank. In (a), the SRL results are generated by the state-
of-the-art monolingual SRL systems. The English SRL result is correct. But it is to more difficult to get correct
SRL result on Chinese side, because the AM-TMP argument embeds into a discontinuous A1 argument. The Chinese
SRL result in the row marked by ?R1? is correct and consistent with the result on English side. Whereas the result in
the row marked by ?R2? is incorrect and inconsistent with the result on English side, with the circles showing their
inconsistency. The argument structure consistency can guide us to choose the correct Chinese SRL result.
Monolingual
SRL System
Monolingual
SRL System
Our Joint
Inference
Model
Source-side
 Predidate
Target side
Predicate
Source-side
SRL 
Candidates
Target-side
SRL 
Candidates BilingualSRL Result
Figure 2: Overview of our approach.
gument alignment between two sides. These three
components correspond to three interrelated factors:
the quality of the SRL result on source side, the qual-
ity of the SRL result on target side, and the argu-
ment structure consistency between the SRL results
on both sides. To evaluate the consistency between
the two argument structures in our joint inference
model, we formulate a log-linear model to compute
the probability of aligning two arguments. Experi-
ments on Chinese-English parallel PropBank shows
that our model significantly outperforms monolin-
gual SRL combination systems on both Chinese and
English sides.
The rest of this paper is organized as follows: Sec-
tion 2 introduces related work. Section 3 describes
how we generate SRL candidates on each side of bi-
text. Section 4 presents our joint inference model.
Section 5 presents our experiments. And Section 6
concludes our work.
2 Related Work
Some existing work on monolingual SRL combina-
tion is related to our work. Punyakanok et al (2004;
2008) formulated an ILP model for SRL. Koomen
et al (2005) combined several SRL outputs using
ILP method. Ma`rquez et al (2005) and Pradhan et
al. (2005) proposed combination strategies that are
not based on ILP method. Surdeanu et al (2007)
did a complete research on a variety of combination
strategies. Zhuang and Zong (2010) proposed a min-
imum error weighting combination strategy for Chi-
nese SRL combination.
Research on SRL utilizing parallel corpus is also
related to our work. Pado? and Lapata (2009) did
research on cross-lingual annotation projection on
English-German parallel corpus. They performed
SRL only on the English side, and then mapped
the English SRL result to German side. Fung et
al. (2007) did pioneering work on studying argu-
ment alignment on Chinese-English parallel Prop-
Bank. They performed SRL on Chinese and En-
glish sides separately. Then, given the SRL result
on both sides, they automatically induced the argu-
ment alignment between two sides.
The major difference between our work and all
existing research is that our model performs SRL in-
ference on two sides of bitext simultaneously. In our
305
model, we jointly consider three interrelated factors:
SRL result on the source side, SRL result on the tar-
get side, and the argument alignment between them.
3 Generating Candidates for Inference
3.1 Monolingual SRL System
As shown in Figure 2, we need to use a monolin-
gual SRL system to generate candidates for our joint
inference model. We have implemented a monolin-
gual SRL system which utilize full phrase-structure
parse trees to perform SRL. In this system, the whole
SRL process is comprised of three stages: pruning,
argument identification, and argument classification.
In the pruning stage, the heuristic pruning method
in (Xue, 2008) is employed. In the argument iden-
tification stage, a number of argument locations are
identified in a sentence. In the argument classifica-
tion stage, each location identified in the previous
stage is assigned a semantic role label. Maximum
entropy classifier is employed for both the argument
identification and classification tasks. And Zhang
Le?s MaxEnt toolkit1 is used for implementation.
We use the monolingual SRL system described
above for both Chinese and English SRL tasks. For
the Chinese SRL task, the features used in this paper
are the same with those used in (Xue, 2008). For
the English SRL task, the features used are the same
with those used in (Pradhan et al, 2008).
3.2 Output of the Monolingual SRL System
The maximum entropy classifier in our monolingual
SRL system can output classification probabilities.
We use the classification probability of the argument
classification stage as an argument?s probability. As
illustrated in Figure 3, in an individual system?s out-
put, each argument has three attributes: its location
in sentence loc, represented by the number of its first
word and last word; its semantic role label l; and its
probability p.
So each argument outputted by a system is a triple
(loc, l, p). For example, the A0 argument in Figure 3
is ((0, 2),A0, 0.94). Because these outputs are to be
combined, we call such triple a candidate.
1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit
.html
Sent: 	??]?????I	n??O:
Args: [ A0 ] [Pred] [ A1 ]
loc: (0, 2) (4, 7)
l: A0 A1
p: 0.94 0.92
Figure 3: Three attributes of an output argument: location
loc, label l, and probability p.
3.3 Generating and Merging Candidates
To generate candidates for joint inference, we need
to have multiple SRL results on each side of bi-
text. Therefore, for both Chinese and English SRL
systems, we use the 3-best parse trees of Berkeley
parser (Petrov and Klein, 2007) and 1-best parse
trees of Bikel parser (Bikel, 2004) and Stanford
parser (Klein and Manning, 2003) as inputs. All the
three parsers are multilingual parsers. The second
and third best parse trees of Berkeley parser are used
for their good quality. Therefore, each monolingual
SRL system produces 5 different outputs.
Candidates from different outputs may have the
same loc and l but different p. So we merge all
candidates with the same loc and l into one by av-
eraging their probabilities. For a merged candidate
(loc, l, p), we say that p is the probability of assign-
ing l to loc.
4 Joint Inference Model
Our model can be conceptually decomposed to three
components: the source side, the target side, and the
argument alignment. The objective function of our
joint inference model is the weighted sum of three
sub-objectives:
max Os + ?1Ot + ?2Oa (1)
where Os and Ot represent the quality of the SRL
results on source and target sides, and Oa represents
the soundness of the argument alignment between
the SRL results on two sides, ?1, ?2 are positive
weights corresponding to the importance of Ot and
Oa respectively.
4.1 Components of Source and Target Sides
4.1.1 Source Side Component
The source side component aims to improve the
SRL result on source side. This is equivalent to a
306
monolingual SRL combination problem.
For convenience, we denote the whole semantic
role label set for source language as {ls1, l
s
2, . . . , l
s
Ls},
in which ls1 ? l
s
6 stand for the key argument labels
A0 ? A5 respectively. Suppose there are Ns differ-
ent locations, denoted as locs1, . . . , loc
s
Ns , among all
candidates on the source side. The probability of as-
signing lsj to loc
s
i is p
s
ij . An indicator variable xij is
defined as:
xij = [loc
s
i is assigned label l
s
j ].
Then the source side sub-objective Os in equation
(1) is the sum of arguments? probabilities on source
side:
Os =
Ns?
i=1
Ls?
j=1
(psij ? Ts)xij (2)
where Ts is a bias to prevent including too many can-
didates in solution (Surdeanu et al, 2007).
We consider the following two linguistically mo-
tivated constraints:
1. No duplication: There is no duplication for key
arguments: A0 ? A5.
2. No overlapping: Arguments cannot overlap
with each other.
In (Punyakanok et al, 2004), several more con-
straints are considered. According to (Surdeanu
et al, 2007), however, no significant performance
improvement can be obtained by considering more
constraints than the two above. So we do not con-
sider other constraints.
The inequalities in (3) make sure that each locsi is
assigned at most one label.
?1 ? i ? Ns :
Ls?
j=1
xij ? 1 (3)
The inequalities in (4) satisfy the No duplication
constraint.
?1 ? j ? 6 :
Ns?
i=1
xij ? 1 (4)
For any source side location locsi , let Ci denote
the index set of the locations that overlap with it.
Then the No overlapping constraint means that if
locsi is assigned a label, i.e.,
?Ns
j=1 xij = 1, then
for any u ? Ci, locsu cannot be assigned any label,
i.e.,
?Ns
j=1 xuj = 0. A common technique in ILP
modeling to form such a constraint is to use a suf-
ficiently large auxiliary constant M . And the con-
straint is formulated as:
?1 ? i ? Ns :
?
u?Ci
Ls?
j=1
xuj ? (1?
Ls?
j=1
xij)M (5)
In this case,M only needs to be larger than the num-
ber of candidates to be combined. In this paper,
M = 500 is large enough.
4.1.2 Target Side Component
In principle, the target side component of our joint
inference model is the same with the source side
component.
The whole semantic role label set for target lan-
guage is denoted by {lt1, l
t
2, . . . , l
t
Lt}. There are
Nt different locations, denoted as loct1, . . . , loc
t
Nt ,
among all candidates in the target side. And lt1 ? l
t
6
stand for the key argument labels A0 ? A5 respec-
tively. The probability of assigning ltj to loc
t
k is p
t
kj .
An indicator variable ykj is defined as:
ykj = [loc
t
k is assigned label l
t
j ].
Then the target side sub-objective Ot in equation (1)
is:
Ot =
Nt?
k=1
Lt?
j=1
(ptkj ? Tt)ykj (6)
The constraints on target side are as follows:
Each loctk is assigned at most one label:
?1 ? k ? Nt :
Lt?
j=1
ykj ? 1 (7)
The No duplication constraint:
?1 ? j ? 6 :
Nt?
k=1
ykj ? 1 (8)
The No overlapping constraint:
?1 ? k ? Nt :
?
v?Ck
Lt?
j=1
yvj ? (1?
Lt?
j=1
ykj)M (9)
In (9), Ck denotes the index set of the locations that
overlap with loctk, and the constant M is set to 500
in this paper.
307
4.2 Argument Alignment
The argument alignment component is the core of
our joint inference model. It gives preference to the
bilingual SRL results that have more consistent ar-
gument structures.
For a source side argument argsi = (loc
s
i , l
s) and
a target side argument argtk = (loc
t
k, l
t), let zik be
the following indicator variable:
zik = [arg
s
i aligns with arg
t
k].
We use paik to represent the probability that arg
s
i and
argtk align with each other, i.e., p
a
ik = P (zik = 1).
We call paik the argument alignment probability
between argsi and arg
t
k.
4.2.1 Argument Alignment Probability Model
We use a log-linear model to compute the argu-
ment alignment probability paik between arg
s
i and
argtk. Let (s, t) denote a bilingual sentence pair and
wa denote the word alignment on (s, t). Our log-
linear model defines a distribution on zik given the
tuple tup = (argsi , arg
t
k, wa, s, t):
P (zik|tup) ? exp(w
T?(tup))
where ?(tup) is the feature vector. With this model,
paik can be computed as p
a
ik = P (zik = 1|tup).
In order to study the argument alignment in cor-
pus and to provide training data for our log-linear
model, we have manually aligned the arguments in
60 files (chtb 0121.fid to chtb 0180.fid) of Chinese-
English parallel PropBank. On this data set, we get
the argument alignment matrix in Table 1.
Ch\En A0 A1 A2 A3 A4 AM* NUL
A0 492 30 4 0 0 0 46
A1 98 853 43 2 0 0 8
A2 9 57 51 1 0 47 0
A3 1 0 2 6 0 0 0
A4 0 0 2 0 3 0 0
AM* 0 2 39 0 0 895 221
NUL 53 14 27 0 0 45 0
Table 1: The argument alignment matrix on manually
aligned corpus.
Each entry in Table 1 is the number of times for
which one type of Chinese argument aligns with one
type of English argument. AM* stands for all ad-
juncts types like AM-TMP, AM-LOC, etc., and NUL
means that the argument on the other side cannot be
aligned with any argument on this side. For exam-
ple, the number 46 in the A0 row and NUL column
means that Chinese A0 argument cannot be aligned
with any argument on English side for 46 times in
our manually aligned corpus.
We use the following features in our model.
Word alignment feature: If there are many word-
to-word alignments between argsi and arg
t
k, then
it is very probable that argsi and arg
t
k would align
with each other. We adopt the method used in (Pado?
and Lapata, 2009) to measure the word-to-word
alignments between argsi and arg
t
k. And the word
alignment feature is defined as same as the word
alignment-based word overlap in (Pado? and Lapata,
2009). Note that this is a real-valued feature.
Head word alignment feature: The head word
of an argument is usually more representative than
other words. So we use whether the head words of
argsi and arg
t
k align with each other as a binary fea-
ture. The use of this feature is inspired by the work
in (Burkett and Klein, 2008).
Semantic role labels of two arguments: From Ta-
ble 1, we can see that semantic role labels of two ar-
guments are a good indicator of whether they should
align with each other. For example, a Chinese A0
argument aligns with an English A0 argument most
of the times, and never aligns with an English AM*
argument in Table 1. Therefore, the semantic role
labels of argsi and arg
t
k are used as a feature.
Predicate verb pair: Different predicate pairs have
different argument alignment patterns. Let?s take the
Chinese predicate O/zengzhang and the English
predicate grow as an example. The argument align-
ment matrix for all instances of the Chinese-English
predicate pair (zengzhang, grow) in our manually
aligned corpus is shown in Table 2.
CH \EN A0 A1 A2 AM* NUL
A0 0 16 0 0 0
A1 0 0 12 0 0
AM* 0 0 4 7 10
NUL 0 0 0 2 0
Table 2: The argument alignment matrix for the predicate
pair (zengzhang, grow).
From Table 2 we can see that all A0 arguments of
zengzhang align with A1 arguments of grow. This
308
is very different from the results in Table 1, where a
Chinese A0 argument tends to align with an English
A0 argument. This phenomenon shows that a pred-
icate pair can determine which types of arguments
should align with each other. Therefore, we use the
predicate pair as a feature.
4.2.2 Argument Alignment Component
The argument alignment sub-objective Oa in
equation (1) is the sum of argument alignment prob-
abilities:
Oa =
Ns?
i=1
Nt?
k=1
(paik ? Ta)zik (10)
where Ta is a bias to prevent including too many
alignments in final solution, and paik is computed
using the log-linear model described in subsec-
tion 4.2.1.
Oa reflects the consistency between argument
structures on two sides of bitext. Larger Oa means
better argument alignment between two sides, thus
indicates more consistency between argument struc-
tures on two sides.
The following constraints are considered:
1. Conformity with bilingual SRL result. For
all candidates on both source and target sides, only
those that are chosen to be arguments on each side
can be aligned.
2. One-to-many alignment limit. Each argument
can not be aligned with more than 3 arguments.
3. Complete argument alignment. Each argument
on source side must be aligned with at least one ar-
gument on target side, and vice versa.
The Conformity with bilingual SRL result con-
straint is necessary to validly integrate the bilingual
SRL result with the argument alignment. This con-
straint means that if argsi and arg
t
k align with each
other, i.e., zik = 1, then locsi must be assigned
a label on source side, i.e.,
?Ls
j=1 xij = 1, and
loctk must be assigned a label on target side, i.e.,?Lt
j=1 ykj = 1. So this constraint can be represented
as:
?1 ? i ? Ns, 1 ? k ? Nt :
Ls?
j=1
xij ? zik (11)
?1 ? k ? Nt, 1 ? i ? Ns :
Lt?
j=1
ykj ? zik (12)
The One-to-many alignment limit constraint
comes from our observation on manually aligned
corpus. We have found that no argument aligns with
more than 3 arguments in our manually aligned cor-
pus. This constraint can be represented as:
?1 ? i ? Ns :
Nt?
k=1
zik ? 3 (13)
?1 ? k ? Nt :
Ns?
i=1
zik ? 3 (14)
The Complete argument alignment constraint
comes from the semantic equivalence between two
sides of bitext. For each source side location locsi ,
if it is assigned a label, i.e.,
?Ls
j=1 xij = 1, then it
must be aligned with some arguments on target side,
i.e.,
?Nt
k=1 zik ? 1. This can be represented as:
?1 ? i ? Ns :
Nt?
k=1
zik ?
Ls?
j=1
xij (15)
Similarly, each target side argument must be aligned
to at least one source side argument. This can be
represented as:
?1 ? k ? Nt :
Ns?
i=1
zik ?
Lt?
j=1
ykj (16)
4.3 Complete Argument Alignment as a Soft
Constraint
Although the hard Complement argument alignment
constraint is ideally reasonable, in real situations this
constraint does not always hold. The manual argu-
ment alignment result shown in Table 1 indicates
that in some cases an argument cannot be aligned
with any argument on the other side (see the NUL
row and column in Table 1). Therefore, it would
be reasonable to change the hard Complement argu-
ment alignment constraint to a soft one. To do so,
we need to remove the hard Complement argument
alignment constraint and add penalty for violation of
this constraint.
If an argument does not align with any argument
on the other side, we say it aligns with NUL. And we
define the following indicator variables:
zi,NUL = [argsi aligns with NUL], 1 ? i ? Ns.
309
zNUL,k = [argtk aligns with NUL], 1 ? k ? Nt.
Then
?Ns
i=1 zi,NUL is the number of source side ar-
guments that align with NUL. And
?Nt
k=1 zNUL,k is
the number of target side arguments that align with
NUL. For each argument that aligns with NUL, we
add a penalty ?3 to the argument alignment sub-
objective Oa. Therefore, the sub-objective Oa in
equation (10) is changed to:
Oa =
Ns?
i=1
Nt?
k=1
(paik ? Ta)zik
??3(
Ns?
i=1
zi,NUL +
Nt?
k=1
zNUL,k) (17)
From the definition of zi,NUL, it is obvious that,
for any 1 ? i ? Ns, zi,NUL and zik(1 ? k ? Nt)
have the following relationship: If
?Nt
k=1 zik ? 1,
i.e., argsi aligns with some arguments on target side,
then zi,NUL = 0; Otherwise, zi,NUL = 1. These
relationships can be captured by the following con-
straints:
?1 ? i ? Ns, 1 ? k ? Nt : zi,NUL ? 1?zik (18)
?1 ? i ? Ns :
Nt?
k=1
zik + zi,NUL ? 1 (19)
Similarly, for any 1 ? k ? Nt, zNUL,k and
zik(1 ? i ? Ns) observe the following constraints:
?1 ? k ? Nt, 1 ? i ? Ns : zNUL,k ? 1? zik
(20)
?1 ? k ? Nt :
Ns?
i=1
zik + zNUL,k ? 1 (21)
4.4 Models Summary
So far, we have presented two versions of our joint
inference model. The first version treats Comple-
ment argument alignment as a hard constraint. We
will refer to this version as Joint1. The objective
function of Joint1 is defined by equations (1, 2, 6,
10). And the constraints of Joint1 are defined by
equations (3-5, 7-9, 11-16).
The sencond version treats Complement argument
alignment as a soft constraint. We will refer to this
version as Joint2. The objective function of Joint2
is defined by equations (1, 2, 6, 17). And the con-
straints of Joint2 are defined by equations (3-5, 7-9,
11-14, 18-21).
Our baseline models are monolingual SRL com-
bination models. We will refer to the source side
combination model as SrcCmb. The objective of Sr-
cCmb is to maximize Os, which is defined in equa-
tion (2). And the constraints of SrcCmb are defined
by equations (3-5). Similarly, we will refer to the tar-
get side combination model as TrgCmb. The objec-
tive of TrgCmb is to maximize Ot defined in equa-
tion (6). And the constraints of TrgCmb are defined
by equations (7-9). In this paper, we employ lp-
solve2 to solve all ILP models.
5 Experiments
5.1 Experimental Setup
In our experiments, we use the Xinhua News por-
tion of Chinese and English data in LDC OntoNotes
Release 3.0. This data is a Chinese-English parallel
proposition bank described in (Palmer et al, 2005).
It contains parallel proposition annotations for 325
files (chtb 0001.fid to chtb 0325.fid) from Chinese-
English parallel Treebank. The English part of this
data contains proposition annotations only for ver-
bal predicates. Therefore, we only consider verbal
predicates in this paper.
We employ the GIZA++ toolkit (Och and Ney,
2003) to perform automatic word alignment. Be-
sides the parallel PropBank data, we use additional
4,500K Chinese-English sentence pairs3 to induce
word alignments for both directions, with the default
GIZA++ settings. The alignments are symmetrized
using the intersection heuristic (Och and Ney, 2003),
which is known to produce high-precision align-
ments.
We use 80 files (chtb 0001.fid to chtb 0080.fid)
as test data, and 40 files (chtb 0081.fid to
chtb 0120.fid) as development data. Although our
joint inference model needs no training, we still
need to train a log-linear argument alignment prob-
ability model, which is used in the joint inference
model. As specified in subsection 4.2.1, the train-
2http://lpsolve.sourceforge.net/
3These data includes the following LDC corpus:
LDC2002E18, LDC2003E07, LDC2003E14, LDC2005T06,
LDC2004T07, LDC2000T50.
310
ing set for the argument alignment probability model
consists of 60 files (chtb 0121.fid to chtb 0180.fid)
with manual argument alignment. Unfortunately,
the quality of automatic word alignment on one-
to-many Chinese-English sentence pairs is usually
very poor. So we only include one-to-one Chinese-
English sentence pairs in all data. And not all predi-
cates in a sentence pair can be included. Only bilin-
gual predicate pairs are included. A bilingual pred-
icate pair is defined to be a pair of predicates in bi-
text which align with each other in automatic word
alignment. Table 3 shows how many sentences and
predicates are included in each data set.
Test Dev Train
Articles 1-80 81-120 121-180
Chinese Sentences 1067 578 778
English Sentences 1182 620 828
Bilingual pairs 821 448 614
Chinese Predicates 3792 2042 2572
English Predicates 2864 1647 1860
Bilingual pairs 1476 790 982
Table 3: Sentence and predicate counts.
Our monolingual SRL systems are trained sep-
arately. Our Chinese SRL system is trained on
640 files (chtb 0121.fid to chtb 0931.fid) in Chinese
Propbank 1.0. Because Xinhua News is a quite dif-
ferent domain from WSJ, the training set for our En-
glish SRL system includes not only Sections 02?21
of WSJ data in English Propbank, but also 205 files
(chtb 0121.fid to chtb 0325.fid) in the English part
of parallel PropBank. For Chinese, the syntactic
parsers are trained on 640 files (chtb 0121.fid to
chtb 0931.fid) plus the broadcast news portion of
Chinese Treebank 6.0. For English, the syntactic
parsers are trained on the following data: Sections
02?21 of WSJ data in English Treebank, 205 files
(chtb 0121.fid to chtb 0325.fid) of Xinhua News
data in OntoNotes 3.0, and the Sinorama data in
OntoNotes 3.0. We treat discontinuous and corefer-
ential arguments in accordance to the CoNLL-2005
shared task (Carreras and Ma`rquez, 2005). The first
part of a discontinuous argument is labeled as it is,
and the second part is labeled with a prefix ?C-?.
All coreferential arguments are labeled with a prefix
?R-?.
5.2 Tuning Parameters in Models
The models Joint1, Joint2, SrcCmb, and TrgCmb
have different parameters. For each model, we have
automatically tuned its parameters on development
set using Powell?s Mothod (Brent, 1973). Powell?s
Method is a heuristic optimization algorithm that
does not require the objective function to have an ex-
plicit analytical formula. For a monolingual model
like SrcCmb or TrgCmb, our objective is to maxi-
mize the F1 score of the model?s result on develop-
ment set. But a joint model, like Joint1 or Joint2,
generates SRL results on both sides of bitext. So
our objective is to maximize the sum of the two F1
scores of the model?s results for both Chinese and
English on development set. For all models, we re-
gard the parameters to be tuned as variables. Then
we optimize our objective using Powell?s Method.
The solution of this optimization is the values of pa-
rameters. To avoid finding poor local optimum, we
perform the optimization 30 times with different ini-
tial parameter values, and choose the best solution
found. The final parameter values are listed in Ta-
ble 4.
Model Ts Tt Ta ?1 ?2 ?3
SrcCmb 0.21 - - - - -
TrgCmb - 0.32 - - - -
Joint1 0.17 0.22 0.36 0.96 1.04 -
Joint2 0.15 0.26 0.42 1.02 1.21 0.15
Table 4: Parameter values in models.
5.3 Individual SRL Outputs? Performance
As specified in subsection 3.3, the monoligual SRL
system uses different parse trees to generate multi-
ple SRL outputs. The performance of these outputs
on test set is shown in Table 5. In Table 5, O1?O3
are the outputs using 3-best parse trees of Berkeley
parser respectively, O4 and O5 are the outputs us-
ing the best parse trees of Stanford parser and Bikel
parser respectively.
As specified in subsection 5.1, only a small part
of English SRL training data is in the same domain
with test data. Therefore, the English SRL result in
Table 5 is not very impressive. But the Chinese SRL
result is pretty good.
311
Side Outputs P (%) R(%) F1
O1 79.84 71.95 75.69
O2 78.53 70.32 74.20
Chinese O3 78.41 69.99 73.96
O4 73.21 67.13 70.04
O5 75.32 63.78 69.07
O1 77.13 70.42 73.62
O2 75.88 69.06 72.31
English O3 75.74 68.65 72.02
O4 71.57 66.11 68.73
O5 73.12 68.04 70.49
Table 5: The results of individual monolingual SRL out-
puts on test set.
5.4 Effects of Different Constraints
The One-to-many limit and Complete argument
alignment constraints in subsection 4.2.2 comes
from our empirical knowledge. To investigate the
effect of these two constraits, we remove them from
our joint inference models one by one, and observe
the performance variations on test set. The results
are shown in Table 6. In Table 6, ?c2? refers to the
One-to-many limit constraint, ?c3? refers to the Com-
plete argument alignment constraint, and ?-? means
removing. For example, ?Joint1 - c2? means remov-
ing the constraint ?c2? from the model Joint1. Recall
that the only difference between Joint1 and Joint2 is
that ?c3? is a hard constraint in Joint1, but a soft con-
straint in Joint2. Therefore, ?Joint2 - c3? and ?Joint2
- c2 - c3? do not appear in Table 6, because they are
the same with ?Joint1 - c3? and ?Joint1 - c2 - c3?
respectively.
Model Side P (%) R(%) F1
Joint1
Chinese
82.95 75.21 78.89
Joint1 - c2 81.46 75.97 78.62
Joint1 - c3 82.36 74.68 78.33
Joint1 - c2 - c3 82.04 74.67 78.18
Joint2 83.35 76.04 79.53
Joint2 - c2 82.41 76.03 79.09
Joint1
English
79.38 75.16 77.21
Joint1 - c2 78.51 75.22 76.83
Joint1 - c3 78.66 74.55 76.55
Joint1 - c2 - c3 78.37 74.37 76.32
Joint2 79.64 76.18 77.87
Joint2 - c2 78.41 75.89 77.13
Table 6: Results of different joint models on test set.
From Table 6, we can see that the constraints ?c2?
and ?c3? both have positive effect in our joint in-
ference model, because removing any one of them
causes performance degradation. And removing
?c3? from Joint1 causes more performance degrada-
tion than removing ?c2?. This means that ?c3? plays
a more important role than ?c2? in our joint inference
model. Indeed, by treating ?c3? as a soft constraint,
the model Joint2 has the best performance on both
sides of bitext.
5.5 Final Results
We use Joint2 as our final joint inference model.
And as specified in subsection 4.4, our baselines are
monolingual SRL combination models: SrcCmb for
Chinese, and TrgCmb for English. Note that SrcCmb
and TrgCmb are basically the same as the state-of-
the-art combination model in (Surdeanu et al, 2007)
with No overlapping and No duplication constraints.
The final results on test set are shown in Table 7.
Side Model P (%) R(%) F1
Chinese
SrcCmb 82.58 73.92 78.01
Joint2 83.35 76.04 79.53
English
TrgCmb 79.02 73.44 76.13
Joint2 79.64 76.18 77.87
Table 7: Comparison between monolingual combination
model and our joint inference model on test set.
From Table 5 and Table 7, we can see that SrcCmb
and TrgCmb improve F1 scores over the best indi-
vidual SRL outputs by 2.32 points and 2.51 points
on Chinese and English seperately. Thus they form
strong baselines for our joint inference model. Even
so, our joint inference model still improves F1 score
over SrcCmb by 1.52 points, and over TrgCmb by
1.74 points.
From Table 7, we can see that, despite only part of
training data for English SRL system is in-domain,
our joint inference model still produces good En-
glish SRL result. And the F1 score of Chinese SRL
result reaches 79.53%, which represents the state-
of-the-art Chinese SRL performance to date.
6 Conclusions
In this paper, we propose a joint inference model
to perform bilingual SRL. Our joint inference
model incorporates not only linguistic constraints on
312
source and target sides of bitext, but also the bilin-
gual argument structure consistency requirement on
bitext. Experiments on Chinese-English parallel
PropBank show that our joint inference model is
very effective for bilingual SRL. Compared to state-
of-the-art monolingual SRL combination baselines,
our joint inference model substantially improves
SRL results on both sides of bitext. In fact, the so-
lution of our joint inference model contains not only
the SRL results on bitext, but also the optimal argu-
ment alignment between two sides of bitext. This
makes our model especially suitable for application
in machine translation, which needs to obtain the ar-
gument alignment.
Acknowledgments
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053 and 60736014, the National
Key Technology R&D Program under Grant No.
2006BAH03B02. We would like to thank Jiajun
Zhang for helpful discussions and the anonymous
reviewers for their valuable comments.
References
Daniel Bikel. 2004. Intricacies of Collins Parsing Model.
Computational Linguistics, 30(4):480-511.
Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Prentice-Hall, Englewood Cliffs,
NJ.
David Burkett, and Dan Klein. 2008. Two Languages
are Better than One (for Syntactic Parsing). In Pro-
ceedings of EMNLP-2008, pages 877-886.
Xavier Carreras, and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152-164.
Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a Hybrid Convo-
lution Tree Kernel for Semantic Role Labeling. ACM
Transactions on Asian Language Information Process-
ing, 2008, 7(4): 1-23.
Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai
Wu. 2007. Learning Bilingual Semantic Frames:
Shallow Semantic Parsing vs. Semantic Role Projec-
tion. In Proceedings of the 11th Conference on The-
oretical and Methodological Issues in Machine Trans-
lation, pages 75-84.
Liang Huang, Wenbin Jiang, Qun Liu. 2009.
Bilingually-Constrained (Monolingual) Shift-Reduce
Parsing. In Proceedings of EMNLP-2009, pages 1222-
1231.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-2003,
pages 423-430.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-
tau Yih. 2005. Generalized Inference with Multiple
Semantic Role Labeling Systems. In Proceedings of
CoNLL-2005 shared task, pages 181-184.
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving Nominal SRL in
Chinese Language with Verbal SRL Information and
Automatic Predicate Recognition. In Proceedings of
EMNLP-2009, pages 1280-1288.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,
Suzanne Stevenson. 2008. Semantic Role Labeling:
An Introduction to the Special Issue. Computational
Linguistics, 34(2):145-159.
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi
Turmo. 2005. A Robust Combination Strategy for
Semantic Role Labeling. In Proceedings of EMNLP-
2005, pages 644-651.
Frans J. Och, and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19-51.
Sebastian Pado?, and Mirella Lapata. 2009. Cross-lingual
Annotation Projection of Semantic Roles. Journal of
Artificial Intelligence Research (JAIR), 36:307-340.
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-
ing Chen, Benjamin Snyder. 2005. A Parallel Propo-
sition Bank II for Chinese and English. In Frontiers
in Corpus Annotation, Workshop in conjunction with
ACL-05, pages 61-67.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized parsing. In Proceedings of ACL-
2007, pages 46-54.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Martin, and Daniel Jurafsky. 2005. Semantic Role
Labeling Using Different Syntactic Views. In Pro-
ceedings of ACL-2005, pages 581-588.
Sameer S. Pradhan, Wayne Ward, James H. Martin.
2008. Towards Robust Semantic Role Labeling. Com-
putational Linguistics, 34(2):289-310.
Vasin Punyakanok, Dan Roth, Wen-tauYih. 2008. The
Importance of Syntactic Parsing and Inference in Se-
mantic Role Labeling. Computational Linguistics,
34(2):257-287.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING-2004, pages 1346-1352.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese Semantic Role Labeling with Shallow
313
Parsing. In Proceedings of EMNLP-2009, pages 1475-
1483.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artificial Intel-
ligence Research (JAIR), 29:105-151.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Seman-
tic Role Labeling. Computational Linguistics, 34(2):
145-159.
Dekai Wu, and Pascale Fung. 2009. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceedings of
NAACL-2009, pages 13-16.
Nianwen Xue. 2008. Labeling Chinese Predicates with
Semantic Roles. Computational Linguistics, 34(2):
225-255.
Tao Zhuang, and Chengqing Zong. 2010. A Minimum
Error Weighting Combination Strategy for Chinese Se-
mantic Role Labeling. In Proceedings of COLING-
2010, pages 1362-1370.
314
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204?215,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Augmenting String-to-Tree Translation Models with Fuzzy Use of 
Source-side Syntax 
Jiajun Zhang, Feifei Zhai and Chengqing Zong 
Institute of Automation, Chinese Academy of Sciences 
Beijing, China 
{jjzhang, ffzhai, cqzong}@nlpr.ia.ac.cn 
 
 
 
 
 
Abstract 
Due to its explicit modeling of the 
grammaticality of the output via target-side 
syntax, the string-to-tree model has been 
shown to be one of the most successful 
syntax-based translation models. However, 
a major limitation of this model is that it 
does not utilize any useful syntactic 
information on the source side. In this 
paper, we analyze the difficulties of 
incorporating source syntax in a string-to-
tree model. We then propose a new way to 
use the source syntax in a fuzzy manner, 
both in source syntactic annotation and in 
rule matching. We further explore three 
algorithms in rule matching: 0-1 matching, 
likelihood matching, and deep similarity 
matching. Our method not only guarantees 
grammatical output with an explicit target 
tree, but also enables the system to choose 
the proper translation rules via fuzzy use of 
the source syntax. Our extensive 
experiments have shown significant 
improvements over the state-of-the-art 
string-to-tree system. 
1 Introduction 
In recent years, statistical translation models based 
upon linguistic syntax have shown promising 
progress in improving translation quality. It 
appears that encoding syntactic annotations on 
either side or both sides in translation rules can 
increase the expressiveness of rules and can 
produce more accurate translations with improved 
reordering.  
One of the most successful syntax-based models 
is the string-to-tree model (Galley et al, 2006; 
Marcu et al, 2006; Shen et al, 2008; Chiang et al, 
2009). Since it explicitly models the 
grammaticality of the output via target-side syntax, 
the string-to-tree model (Xiao et al, 2010) 
significantly outperforms both the state-of-the-art 
phrase-based system Moses (Koehn et al, 2007) 
and the formal syntax-based system Hiero (Chiang, 
2007). However, there is a major limitation in the 
string-to-tree model: it does not utilize any useful 
source-side syntactic information, and thus to some 
extent lacks the ability to distinguish good 
translation rules from bad ones. 
The source syntax is well-known to be helpful in 
improving translation accuracy, as shown 
especially by tree-to-string systems (Quirk et al, 
2005; Liu et al, 2006; Huang et al, 2006; Mi et al, 
2008; Zhang et al, 2009).  The tree-to-string 
systems are simple and efficient, but they also have 
a major limitation: they cannot guarantee the 
grammaticality of the translation output because 
they lack target-side syntactic constraints.  
Thus a promising solution is to combine the 
advantages of the tree-to-string and string-to-tree 
approaches. A natural idea is the tree-to-tree model 
(Ding and Palmer, 2005; Cowan et al, 2006; Liu et 
al., 2009). However, as discussed by Chiang 
(2010), while tree-to-tree translation is indeed 
promising in theory, in practice it usually ends up 
over-constrained. Alternatively, Mi and Liu (2010) 
proposed to enhance the tree-to-string model with 
target dependency structures (as a language model). 
In this paper, we explore in the other direction: 
based on the strong string-to-tree model which 
builds an explicit target syntactic tree during 
decoding rather than apply only a syntactic 
language model, we aim to find a useful way to 
incorporate the source-side syntax. 
204
First, we give a motivating example to show the 
importance of the source syntax for a string-to-tree 
model. Then we discuss the difficulties of 
integrating the source syntax into the string-to-tree 
model. Finally, we propose our solutions. 
Figure 1 depicts a standard process that 
transforms a Chinese string into an English tree 
using several string-to-tree translation rules. The 
tree with solid lines is produced by the baseline 
string-to-tree system. Although the yield is 
grammatical, the translation is not correct since the 
system mistakenly applies rule r2, thus translating 
the Chinese preposition ? (h? ) in the example 
sentence into the English conjunction and. As a 
result, the Chinese prepositional phrase ?? ?? 
??  ?? (?with terrorist networks?) is wrongly 
translated as a part of the relevant noun phrase 
(?[Hussein] and terrorists networks?). Why does 
this happen? We find that r2 occurs 103316 times 
in our training data, while r3 occurs only 1021 
times. Thus, without source syntactic clues, the 
Chinese word ? (h ? ) is converted into the 
conjunction and in most cases. In general, this 
conversion is correct when the word?(h?) is used 
as a conjunction. But?(h?) is a preposition in the 
source sentence. If we are given this source 
syntactic clue, rule r3 will be preferred. This 
example motivates us to provide a moderate 
amount of source-side syntactic information so as 
to obtain the correct English tree with dotted lines 
(as our proposed system does). 
A natural question may arise that is it easy to 
incorporate source syntax in the string-to-tree 
model? To the best of our knowledge, no one has 
studied this approach before. In fact, it is not a 
trivial question if we look into the string-to-tree 
model. We find that the difficulties lie in at least 
three problems: 1) For a string-to-tree rule such as 
r6 in figure 1, how should we syntactically annotate 
its source string? 2) Given the source-annotated 
string-to-tree rules, how should we match these 
rules according to the test source tree during 
decoding? 3) How should we binarize the source-
annotated string-to-tree rules for efficient decoding? 
For the first problem, one may require the 
source side of a string-to-tree rule to be a 
constituent. However, such excessive constraints 
will exclude many good string-to-tree rules whose 
source strings are not constituents. Inspired by 
Chiang (2010), we adopt a fuzzy way to label 
every source string with the complex syntactic 
categories of SAMT (Zollmann and Venugopal, 
2006). This method leads to a one-to-one 
correspondence between the new rules and the 
string-to-tree rules. We will detail our fuzzy 
labeling method in Section 2. 
For the second problem, it appears simple and 
intuitive to match rules by requiring a rule?s source 
syntactic category to be the same as the category of 
the test string. However, this hard constraint will 
greatly narrow the search space during decoding. 
Continuing to pursue the fuzzy methodology, we 
adopt a fuzzy matching procedure to enable 
matching of all the rules whose source strings 
match the test string, and then determine the 
degree of matching between the test source tree 
and each rule. We will discuss three fuzzy 
matching algorithms, from simple to complex, in 
Section 3. 
The third question is a technical problem, and 
we will give our solution in Section 4. 
Our method not only guarantees the 
grammaticality of the output via the target tree 
structure, but also enables the system to choose 
appropriate translation rules during decoding 
through source syntactic fuzzy labeling and fuzzy 
matching.  
The main contributions of this paper are as 
follows: 
1) We propose a fuzzy method for both source 
syntax annotation and rule matching for 
augmenting string-to-tree models. 
2) We design and investigate three fuzzy rule 
matching algorithms: 0-1 matching, 
likelihood matching, and deep similarity 
matching. 
We hope that this paper will demonstrate how to 
effectively incorporate both source and target 
syntax into a translation model with promising 
results. 
2 Rule Extraction 
Since we annotate the source side of each string-to-
tree rule with source parse tree information in a 
fuzzy way, we will henceforward denote the 
source-syntax-decorated string-to-tree rule as a 
fuzzy-tree to exact-tree rule. We first briefly 
review issues of string-to-tree rule extraction; then 
we discuss how to augment the string-to-tree rules 
to yield fuzzy-tree to exact-tree rules. 
205
 Figure 1:  Two alternative derivations for a sample string-to-tree translation. The rules used are listed on the right. 
The target yield of the tree with solid lines is hussein and terrorist networks established relations. The target yield 
of the tree with dotted lines is hussein established relations with terrorist networks. 
 
2.1 String-to-Tree Rule Extraction 
Galley et al (2004) proposed the GHKM algorithm 
for extracting (minimal) string-to-tree translation 
rules from a triple (f, et, a), where f is the source-
language sentence, et is a target-language parse tree 
whose yield e is the translation of f, and a is the set 
of word alignments between e and f. The basic idea 
of GHKM is to obtain the set of minimally-sized 
translation rules which can explain the mappings 
between source string and target parse tree. The 
minimal string-to-tree rules are extracted in three 
steps: (1) frontier set computation; (2) 
fragmentation; and (3) extraction. 
  The frontier set (FS) is the set of potential points 
at which to cut the graph G constructed by the 
triple (f, et, a) into fragments. A node satisfying the 
word alignment is a frontier. Bold italic nodes in 
the English parse tree in Figure 2 are all frontiers. 
   Given the frontier set, a well-formed 
fragmentation of G is generated by restricting each 
fragment to take only nodes in FS as the root and 
leaf nodes. 
   With fragmentation completed, the rules are 
extracted through a depth-first traversal of te : for 
each frontier being visited, a rule is extracted. 
These extracted rules are called minimal rules 
(Galley et al, 2004). For example, rules r ra i? in 
Figure 2 are part of the total of 13 minimal rules.  
To improve the rule coverage, SPMT models 
can be employed to obtain phrasal rules (Marcu et 
at., 2006). In addition, the minimal rules which 
share the adjacent tree fragments can be connected 
together to form composed rules (Galley et al, 
2006). In Figure 2, jr  is a rule composed by 
combining cr and gr . 
2.2 Fuzzy-tree to Exact-tree Rule Extraction 
Our fuzzy-tree to exact-tree rule extraction works 
on word-aligned tree-to-tree data (Figure 2 
illustrates a Chinese-English tree pair).  Basically, 
the extraction algorithm includes two parts: 
(1) String-to-tree rule extraction (without 
considering the source parse tree); 
(2) Decoration of the source side of the string-to-
tree rules with syntactic annotations. 
We use the same algorithm introduced in the 
previous section for extracting the base string-to-
tree rules. The source-side syntactic decoration is 
much more complicated. 
The simplest way to decorate, as mentioned in 
the Introduction, is to annotate the source-side of a 
string-to-tree rule with the syntactic tag that 
exactly covers the source string. This is what the 
exact tree-to-tree procedure does (Liu et al, 2009). 
However, many useful string-to-tree rules will 
become invalid if we impose such a tight 
restriction. For example, in Figure 2, the English 
phrase discuss ? them is a VP, but its Chinese 
counterpart is not a constituent. Thus we will miss 
the rule rh although it is a useful reordering rule. 
According to the analysis of our training data, the 
rules with rigid source-side syntactic constraints 
account for only about 74.5% of the base string-to-
tree rules. In this paper, we desire more general 
applicability. 
206
IP
NP VP
ADJP PP VP
AD P NP VP NP
PN VV NN
PN
?
?? ?
?? ?? ??
i
the
happy
to
discuss
matter
am
with them
NPIN
PP
NNDT
NPVB
VPTO
 VPJJ
  ADJPVBP
VP
S
NP
FW
rb: ?? JJ(happy)
String-to-Tree rules:
ra: ? FW(i)
rm: ?{P} IN(with)
rd: ?? NP(them)re: ?? VB(discuss)rf: ?? NP(DT(the) NN(matter))rg: x0 x1 PP(x0:IN x1:NP)rh: x2 x0 x1 VP(x0:VB x1:NP x2:PP)ri: x0 VP(TO(to) x0:VP)
rj: ? x0 PP(IN(with) x0:VP)
Fuzzy-tree to exact-tree rules:
rk: ?{PN} FW(i)
rl: ??{AD} JJ(happy)
rc: ? IN(with)
rn: x2 x0 x1{PP*VP} VP(x0:VB x1:NP x2:PP)
ro: x0{PP*VP} VP(TO(to) x0:VP)
...
...
 Figure 2:  A sample Chinese-English tree pair for rule extraction. The bold italic nodes in the target English tree are 
frontiers. Note that string-to-tree rules are extracted without considering source-side syntax (upper-right). The new 
fuzzy-tree to exact-tree rules are extracted with both-side parse trees (bottom-right). 
 
Inspired by (Zollmann and Venugopal, 2006; 
Chiang, 2010), we resort to SAMT-style syntactic 
categories in the style of categorial grammar (Bar-
Hillel, 1953). The annotation of the source side of 
string-to-tree rules is processed in three steps: (1) 
If the source-side string corresponds to a syntactic 
category C in the source parse tree, we label the 
source string with C. (2) Otherwise, we check if 
there exists an extended category of the forms 
C1*C2, C1/C2 or C2\C11, indicating respectively that 
the source string spans two adjacent syntactic 
categories, a partial syntactic category C1 missing a 
C2 on the right, or a partial C1 missing a C2 on the 
left. (3) If the second step fails, we check if there is 
an extended category of the forms C1*C2*C3 or 
C1..C2, showing that the source string spans three 
adjacent syntactic categories or a partial category 
with C1 and C2 on each side. In the worst case, 
C1..C2 can denote every source string, thus all of 
the decorations in our training data can be 
explained within the above three steps. Using the 
SAMT-style grammar, each source string can be 
associated with a syntactic category. Thus our 
fuzzy-tree to exact-tree extraction does not lose 
                                                          
1 The kinds of categories are checked in order. This means that 
if C1*C2, C1/C2 can both describe the same source string, we will choose C1*C2. 
any rules as compared with string-to-tree 
extraction. For example, rule ro in Figure 2 uses the 
product category *PP VP  on the source side. 
A problem may arise: How should we handle the 
situation where several rules are observed which 
only differ in their source-side syntactic categories? 
For example, besides the rule rm in Figure 2, we 
encountered rules like ? ? ? ?CC IN with??  in the 
training data. Which source tag should we retain? 
We do not make a partial choice in the rule 
extraction phase. Instead, we simply make a union 
of the relevant rules and retain the respective tag 
counts. Applying this strategy, the rule takes the 
form of ? ? ? ?: 6, : 4P CC IN with?? 2, indicating that 
the source-side preposition tag appears six times 
while the conjunction occurs four times. Note that 
the final rule format used in translation depends on 
the specific fuzzy rule matching algorithm adopted. 
3 Fuzzy Rule Matching Algorithms 
The extracted rules will ultimately be applied to 
derive translations during decoding. One way to 
apply the fuzzy-tree to exact-tree rules is to narrow 
the rule search space. Given a test source sentence 
                                                          
2 6 and 4 are not real counts. They are used for illustration 
only. 
207
with its parse tree, we can according to this 
strategy choose only the rules whose source syntax 
matches the test source tree.  However, this 
restriction will rule out many potentially correct 
rules. In this study, we keep the rule search space 
identical to that of the string-to-tree setting, and 
postpone the use of source-side syntax until the 
derivation stage. During derivation, a fuzzy 
matching algorithm will be adopted to compute a 
score to measure the compatibility between the 
rule and the test source syntax. The translation 
model will learn to distinguish good rules from bad 
ones via the compatibility scores. 
   In this section, three fuzzy matching algorithms, 
from simple to complex, are investigated in order. 
3.1 0-1 Matching 
0-1 matching is a straightforward approach that 
rewards rules whose source syntactic category 
exactly matches the syntactic category of the test 
string and punishes mismatches. It has mainly been 
employed in hierarchical phrase-based models for 
integrating source or both-side syntax (Marton and 
Resnik, 2008; Chiang et al, 2009; Chiang, 2010). 
Since it is verified to be very effective in 
hierarchical models, we borrow this idea in our 
source-syntax-augmented string-to-tree translation.  
In 0-1 matching, the rule?s source side must 
contain only one syntactic category, but a rule may 
have been decorated with more than one syntactic 
category on the source side. Thus we have to 
choose the most reliable category and discard the 
others. Here, we select the one with the highest 
frequency. For example, the tag P in the rule 
? ? ? ?: 6, : 4P CC IN with??  appears more frequently, 
so the final rule used in 0-1 matching will be 
? ? ? ?P IN with?? . Accordingly, we design two 
features: 
1. match_count calculates in a derivation the 
number of rules whose source-side syntactic 
category matches the syntactic category of the 
test string. 
2. unmatch_count counts the number of 
mismatches. 
For example, in the derivations of Figure 1, we 
know the Chinese word?(h?)  is a preposition in 
this sentence (and thus can be written as P(?)), 
therefore, match_count += 1 if the above rule 
? ? ? ?P IN with?? is employed. 
These two features are integrated into the log-
linear translation model and the corresponding 
feature weights will be tuned along with other 
model features to learn which rules are preferred. 
3.2 Likelihood Matching 
It appears intuitively that the 0-1 matching 
algorithm does not make full use of the source-side 
syntax because it keeps only the most-frequent 
syntactic label and discards some potentially useful 
information. Therefore, it runs the risk of treating 
all the discarded source syntactic categories of the 
rule as equally likely. For example, there is an 
extracted rule as follows: 
? ? ? ?:11233, :11073, : 65DEC DEG DEV IN of??  
 0-1 matching converts it into ? ? ? ?DEC IN of?? . 
The use of this rule will be penalized if the 
syntactic category of the test string ?(d?) is parsed 
as DEG or DEV. On one hand, the frequency of the 
tag DEG is just slightly less than that of DEC, but 
the 0-1 matching punishes the former while 
rewarding the latter. On the other hand, the 
frequency of DEG is much more than that of DEV, 
but they are penalized equally. It is obvious that 
the syntactic categories are not finely distinguished. 
   Considering this situation, we propose the 
likelihood matching algorithm. First, we compute 
the likelihood of the rule?s source syntactic 
categories. Since we need to deal with the potential 
problem that the rule is hit by the test string but the 
syntactic category of the test string is not in the 
category set of the rule?s source side, we apply the 
m-estimate of probability (Mitchell, 1997) to 
calculate a smoothed likelihood 
c
c
n mplikelihood n m
?? ?                     (1) 
in which nc is the count of each syntactic category 
c in a specific rule, n denotes the total count of the 
rule, m is a constant called the equivalent sample 
size, and p is the prior probability of the category c. 
In our work, we set the constant m=1 and the prior 
p to 1/12599 where 12599 is the total number of 
source-side syntactic categories in our training data.  
For example, the rule ? ? ? ?: 6, : 4P CC IN with??  
becomes ? ? ? ?: 0.545, : 0.364, 7.2 -6P CC e IN with? ?  
after likelihood computation. Then, if we apply 
likelihood matching in the derivations in Figure 1 
where the test string is? and its syntax is P(?), 
208
the matching score with the above rule will be 
0.545. When the test Chinese word ? is parsed as 
a category other than P or CC, the matching score 
with the above rule will be 7.2e-6. 
   Similar to 0-1 matching, likelihood matching will 
serve as an additional model feature representing 
the compatibility between categories and rules. 
3.3 Deep Similarity Matching 
Considering the two algorithms above, we can see 
that the purpose of fuzzy matching is in fact to 
calculate a similarity. 0-1 matching assigns 
similarity 1 for exact matches and 0 for mismatch, 
while likelihood matching directly utilizes the 
likelihood to measure the similarity. Going one 
step further, we adopt a measure of deep similarity, 
computed using latent distributions of syntactic 
categories. Huang et al (2010) proposed this 
method to compute the similarity between two 
syntactic tag sequences, used to impose soft 
syntactic constraints in hierarchical phrase-based 
models. Analogously, we borrow this idea to 
calculate the similarity between two SAMT-style 
syntactic categories, and then apply it to calculate 
the degree of matching between a translation rule 
and the syntactic category of a test source string 
for purposes of fuzzy matching. We call this 
procedure deep similarity matching. 
Instead of directly using the SAMT-style 
syntactic categories, we represent each category by 
a real-valued feature vector. Suppose there is a set 
of n latent syntactic categories ? ?1, , nV v v? ?  (n=16 
in our experiments). For each SAMT-style 
syntactic category, we compute its distribution of 
latent syntactic categories ? ? ? ? ? ?? ?1 , ,c c c nP V P v P v?? ? .  
For example, ? ? ? ?* 0.4, 0.2, 0.3, 0.1VP NPP V ??  means that 
the latent syntactic categories v1, v2, v3, v4 are 
distributed as p(v1)=0.4, p(v2)=0.2, p(v3)=0.3 and 
p(v4)=0.1 for the SAMT-style syntactic category 
VP*NP. Then we further transform the distribution 
to a normalized feature vector 
? ? ? ? ? ?c cF c P V P V?? ? ?  to represent the SAMT-style 
syntactic category c. 
With the real-valued vector representation for 
each SAMT-style syntactic category, the degree of 
similarity between two syntactic categories can be 
simply computed as a dot-product of their feature 
vectors: 
? ? ? ? ? ? ? ?
1
' 'i i
i n
F c F c f c f c
? ?
? ? ??? ??                   (2) 
This computation yields a similarity score ranging 
from 0 (totally different syntactically) to 1 (totally 
identical syntactically). 
Since we can now compute the similarity of any 
syntactic category pair, we are currently ready to 
compute the matching degree between the 
syntactic category of a test source string and a 
fuzzy-tree to exact-tree rule. To do this, we first 
convert the original fuzzy-tree to exact-tree rule to 
the rule of likelihood format without any 
smoothing. For example, the rule 
? ? ? ?: 6, : 4P CC IN with? ? becomes 
? ? ? ?: 0.6, : 0.4P CC IN with?? after conversion. We 
then denote the syntax of a rule?s source-side RS 
by weighting all the SAMT-style categories in RS 
? ? ? ? ? ?RS
c RS
F RS P c F c
?
? ?? ?                     (3) 
where ? ?RSP c  is the likelihood of the category c. 
Finally, the deep similarity between a SAMT-style 
syntactic category tc of a test source string and a 
fuzzy-tree to exact-tree rule is computed as follows: 
? ? ? ? ? ?,DeepSim tc RS F tc F RS? ?? ?                   (4) 
This deep similarity score will serve as a useful 
feature in the string-to-tree model which will 
enable the model to learn how to take account of 
the source-side syntax during translation. 
We have ignored the details of latent syntactic 
category induction in this paper. In brief, the set of 
latent syntactic categories is automatically induced 
from a source-side parsed, word-aligned parallel 
corpus. The EM algorithm is employed to induce 
the parameters. We simply follow the algorithm of 
(Huang et al, 2010), except that we replace the tag 
sequence with SAMT-style syntactic categories.  
4 Rule Binarization 
In the baseline string-to-tree model, the rules are 
not in Chomsky Normal Form. There are several 
ways to ensure cubic-time decoding. One way is to 
prune the extracted rules using a scope-3 grammar 
and do SCFG decoding without binarization 
(Hopkins and Lengmead, 2010). The other, and 
most popular way is to binarize the translation 
rules (Zhang et al, 2006). We adopt the latter 
approach for efficient decoding with integrated n-
gram language models since this binarization 
technique has been well studied in string-to-tree 
209
translation. However, when the rules? source string 
is decorated with syntax (fuzzy-tree to exact-tree 
rules), how should we binarize these rules? 
    We use the rule rn in Figure 2 for illustration: ? ? ? ?2 0 1 0 1 2: * : : :nr x x x PP VP VP x VB x NP x PP? . 
Without regarding the source-side syntax, we 
obtain the following two binarized rules: ? ?
? ?
0 1
0 1
2 0*1 0*1 * 2
0 1 * 0 1
1: : :
2 : : :
x x
x x
B x x VP x V x PP
B x x V x VB x NP
?
?
 
Since the source-side syntax PP*VP in rule rn 
only accounts for the entire source side, it is 
unclear how to annotate the source side of a partial 
rule such as the second binary rule B2.  
Analyzing the derivation process, we observe 
that a partial rule such as binary rule B2 never 
appears in the final derivation unless the rooted 
binary rule B1 also appears in the derivation. 
Based on this observation, we design a heuristic3 
strategy: we simply attach the syntax PP*VP in the 
rooted binary rule B1, and do not decorate other 
binary rules with source syntax. Thus rule rn will 
be binarized as: 
? ? ? ? ? ?
? ? ? ?
0 1
0 1
2 0*1 0*1 * 2
0 1 * 0 1
1 * : :
2 : :
x x
x x
x x PP VP VP x V x PP
x x V x VB x NP
?
?
 
5 Translation Model and Decoding 
The proposed translation system is an 
augmentation of the string-to-tree model. In the 
baseline string-to-tree model, the decoder searches 
for the optimal derivation *d  that parses a source 
string f into a target tree et from all possible 
derivations D: 
? ?? ? ? ?
? ?
*
1 2
3
arg max log
|
LMd D
d p d d
d R d f
? ? ? ?
?
?
? ?
? ?
                  (5) 
where the first element is a language model score 
in which ? ?d?  is the target yield of derivation d ; 
the second element is the translation length penalty; 
the third element is used to control the derivation 
length; and the last element is a translation score 
that includes six features: 
                                                          
3 We call it heuristic because there may be other syntactic 
annotation strategies for the binarized rules. It should be noted 
that our strategy makes the annotated binarized rules 
equivalent to the original rule. 
? ? ? ? ? ?
? ? ? ?
? ? ? ?
4 5
6 7
8 9
| log | ( ) log | ( )
log | ( ) log ( ) | ( )
log ( ) | ( ) _
r d
lex
lex
R d f p r root r p r lhs r
p r rhs r p lhs r rhs r
p rhs r lhs r is comp
? ?
? ?
? ? ?
?
? ?
? ?
? ?
?
(6) 
In equation (6), the first three elements denote the 
conditional probability of the rule given the root, 
the source-hand side, and the target-hand side. The 
next two elements are bidirectional lexical 
translation probabilities. The last element is the 
preferred binary feature for learning: either the 
composed rule or the minimal rule. 
    In our source-syntax-augmented model, the 
decoder also searches for the best derivation. With 
the help of the source syntactic information, the 
derivation rules in our new model are much more 
distinguishable than that in the string-to-tree model: 
? ?? ? ? ?
? ?
*
1 2
3
arg max log
|
LMd D
d p d d
d R d f
? ? ? ?
?
?
? ?
? ?
            (7) 
Here, all elements except the last one are the same 
as in the string-to-tree model. The last item is: 
? ? ? ?
? ? ? ?? ?
? ? ? ?? ?
? ? ? ? ? ?? ?
10
11
12 13
| |
log ,
log ,
01
r d
R d f R d f
DeepSim DeepSim tag r
likelihood likelihood tag r
match unmatch
? ?
? ?
? ? ? ? ?
?
?
?
?
? ?
?      (8) 
The 0-1 matching4 is triggered only when we set 
? ?01 1? ? . The other two fuzzy matching algorithms 
are triggered in a similar way. 
During decoding, we use a CKY-style parser 
with beam search and cube-pruning (Huang and 
Chiang, 2007) to decode the new source sentences. 
6 Experiments 
6.1 Experimental Setup 
The experiments are conducted on Chinese-to-
English translation, with training data consisting of 
about 19 million English words and 17 million 
Chinese words5. We performed bidirectional word 
alignment using GIZA++, and employed the grow-
diag-final balancing strategy to generate the final 
                                                          
4  In theory, the features unmatch_count, match_count and 
derivation_length are linearly dependent, so the 
unmatch_count is redundant. In practice, since the derivation 
may include glue rules which are not scored by fuzzy 
matching. Thus, "unmatch_count + match_count + 
glue_rule_number = derivation_length". 
5  LDC catalog number: LDC2002E18, LDC2003E14, 
LDC2003E07, LDC2004T07 and LDC2005T06. 
210
symmetric word alignment. We parsed both sides 
of the parallel text with the Berkeley parser (Petrov 
et al, 2006) and trained a 5-gram language model 
with the target part of the bilingual data and the 
Xinhua portion of the English Gigaword corpus. 
    For tuning and testing, we use NIST MT 
evaluation data for Chinese-to-English from 2003 
to 2006 (MT03 to MT06). The development data 
set comes from MT06 in which sentences with 
more than 20 words are removed to speed up 
MERT6 (Och, 2003). The test set includes MT03 
to MT05. 
   We implemented the baseline string-to-tree 
system ourselves according to (Galley et al, 2006; 
Marcu et al, 2006). We extracted minimal GHKM 
rules and the rules of SPMT Model 1 with source 
language phrases up to length L=4. We further 
extracted composed rules by composing two or 
three minimal GHKM rules. We also ran the state-
of-the-art hierarchical phrase-based system Joshua 
(Li et al, 2009) for comparison. In all systems, we 
set the beam size to 200. The final translation 
quality is evaluated in terms of case-insensitive 
BLEU-4 with shortest length penalty. The 
statistical significance test is performed using the 
re-sampling approach (Koehn, 2004). 
6.2 Results 
Table 1 shows the translation results on 
development and test sets. First, we investigate the 
performance of the strong baseline string-to-tree 
model (s2t for short). As the table shows, s2t 
outperforms the hierarchical phrase-based system 
Joshua by more than 1.0 BLEU point in all 
translation tasks. This result verifies the superiority 
of the baseline string-to-tree model. 
   With the s2t system providing a baseline, we 
further study the effectiveness of our source-
syntax-augmented string-to-tree system with 
fuzzy-tree to exact-tree rules (we use FT2ET to 
denote our proposed system). The last three lines 
in Table 1 show that, for each fuzzy matching 
algorithm, our new system TF2ET performs 
significantly better than the baseline s2t system, 
with an improvement of more than 0.5 absolute 
BLEU points in all tasks. This result demonstrates 
the success of our new method of incorporating 
source-side syntax in a string-to-tree model. 
                                                          
6 The average decoding speed is about 50 words per minute in 
the baseline string-to-tree system and our proposed systems. 
System MT06
(dev)
MT03 MT04 MT05
Joshua 29.42 28.62 31.52 31.39 
s2t 30.84 29.75 32.68 32.41 
0-1 31.61** 30.60** 33.45** 33.37**
LH 31.35* 30.34* 33.21* 33.05*
 
FT2ET
DeepSim 31.77** 30.82** 33.69** 33.50**
Table 1: Results (in BLEU scores) of different 
translation models in multiple tasks. LH=likelihood. 
*or**=significantly better than s2t system (p<0.05 or 
0.01 respectively). 
 
 Very similar 
? ? ? ?'F c F c?? ? >0.9 
Very dissimilar 
? ? ? ?'F c F c?? ? <0.1
ADJP JJ;  AD\ADJP VP;  ADVP\NP 
NP DT*NN;  LCP*P*NP CP;  BA*CP 
Table 2: Example of similar and dissimilar categories. 
 
Specifically, the FT2ET system with deep 
similarity matching obtains the best translation 
quality in all tasks and surpasses the baseline s2t 
system by 0.93 BLEU points in development data 
and by more than 1.0 BLEU point in test sets. The 
0-1 matching algorithm is simple but effective, and 
it yields quite good performance (line 3). The 
contribution of 0-1 matching as reflected in our 
experiments is consistent with the conclusions of 
(Marton and Resnik, 2008; Chiang, 2010). By 
contrast, the system with likelihood matching does 
not perform as well as the other two algorithms, 
although it also significantly improves the baseline 
s2t in all tasks. 
6.3 Analysis and Discussion 
We are a bit surprised at the large improvement 
gained by the 0-1 matching algorithm. This 
algorithm has several advantages: it is simple and 
easy to implement, and enhances the translation 
model by enabling its rules to take account of the 
source-side syntax to some degree. However, a 
major deficiency of this algorithm is that it does 
not make full use of the source side syntax, since it 
retains only the most frequent SAMT-style 
syntactic category to describe the rule?s source 
syntax. Thus this algorithm penalizes all the other 
categories equally, although some may be more 
frequent than others, as in the case of DEG and 
DEV in the rule 
? ? ? ?:11233, :11073, : 65DEC DEG DEV IN of?? .  
To some extent, the likelihood matching 
algorithm solves the main problem of 0-1 matching. 
211
Instead of rewarding or penalizing, this algorithm 
uses the likelihood of the syntactic category to 
approximate the degree of matching between the 
test source syntactic category and the rule. For a 
category not in the rule?s source syntactic category 
set, the likelihood algorithm computes a smoothed 
likelihood. However, the likelihood algorithm does 
not in fact lead to very promising improvement. 
We conjecture that this disappointing performance 
is due to the simple smoothing method we 
employed. Future work will investigate more fully. 
Compared with the above two matching 
algorithms, the deep similarity matching algorithm 
based on latent syntactic distribution is much more 
beautiful in theory. This algorithm can successfully 
measure the similarity between any two SAMT-
style syntactic categories (Table 2 gives some 
examples of similar and dissimilar category pairs).  
Then it can accurately compute the degree of 
matching between a test source syntactic category 
and a fuzzy-tree to exact-tree rule. Thus this 
algorithm obtains the best translation quality. 
However, the deep similarity matching algorithm 
has two practical shortcomings. First, it is not easy 
to determine the number of latent categories. We 
have to conduct multiple experiments to arrive at a 
number which can yield a tradeoff between 
translation quality and model complexity. In our 
work, we have tried the numbers n=4, 8, 16, 32, 
and have found n=16 to give the best tradeoff. The 
second shortcoming is that the induction of latent 
syntactic categories has been very time consuming, 
since we have applied the EM algorithm to the 
entire source-parsed parallel corpus. Even with 
n=8, it took more than a week to induce the latent 
syntactic categories on our middle-scale training 
data when using a Xeon four-core computer 
( 2.5 2 16GHz CPU GB? ? memory). When the training 
data contains tens of millions of sentence pairs, the 
computation time may no longer be tolerable. 
Table 3 shows some translation examples for 
comparison. In the first example, the Chinese 
preposition word ? is mistakenly translated into 
English conjunction word and in Joshua and 
baseline string-to-tree system s2t, however, our 
source-syntax-augmented system FT2ET-DeepSim 
correctly converts the Chinese word ?  into 
English preposition with and finally yield the right 
translation. In the second example, our proposed 
system moves the prepositional phrase at an early 
date after the sibling verb phrase. It is more 
reasonable compared with the baseline system s2t. 
In the third example, the proposed system FT2ET-
DeepSim successfully recognizes the Chinese long 
prepositional phrase ? ? ?? ?? ??? ?? ?
? ? ?? ??? ?? ? and short verb phrase ?, 
and obtains the correct phrase reordering at last. 
7 Related Work 
Several studies have tried to incorporate source or 
target syntax into translation models in a fuzzy 
manner. 
Zollmann and Venugopal (2006) augment the 
hierarchical string-to-string rules (Chiang, 2005) 
with target-side syntax. They annotate the target 
side of each string-to-string rule using SAMT-style 
syntactic categories and aim to generate the output 
more syntactically. Zhang et al (2010) base their 
approach on tree-to-string models, and generate 
grammatical output more reliably with the help of 
tree-to-tree sequence rules. Neither of them builds 
target syntactic trees using target syntax, however. 
Thus they can be viewed as integrating target 
syntax in a fuzzy manner. By contrast, we base our 
approach on a string-to-tree model which does 
construct target syntactic trees during decoding. 
(Marton and Resnik, 2008; Chiang et al, 2009 
and Huang et al, 2010) apply fuzzy techniques for 
integrating source syntax into hierarchical phrase-
based systems (Chiang, 2005, 2007). The first two 
studies employ 0-1 matching and the last tries deep 
similarity matching between two tag sequences. By 
contrast, we incorporate source syntax into a 
string-to-tree model. Furthermore, we apply fuzzy 
syntactic annotation on each rule?s source string 
and design three fuzzy rule matching algorithms. 
Chiang (2010) proposes a method for learning to 
translate with both source and target syntax in the 
framework of a hierarchical phrase-based system. 
He not only executes 0-1 matching on both sides of 
rules, but also designs numerous features such as 
. 'X Xroot  which counts the number of rules whose 
source-side root label is X  and target-side root 
label is 'X .  This fuzzy use of source and target 
syntax enables the translation system to learn 
which tree labels are similar enough to be 
compatible, which ones are harmful to combine, 
and which ones can be ignored. The differences 
between us are twofold: 1) his work applies fuzzy 
syntax in both sides, while ours bases on the string- 
212
Source sentence ?? ? [? ?? ???] ?? ? ?? 
Reference hussein also established ties with terrorist networks 
Joshua hussein also and terrorist networks established relations 
s2t hussein also and terrorist networks established relations 
 
 
1 
FT2ET- DeepSim hussein also established relations with terrorist networks 
Source sentence ? [? ?] [??] [??] [? ? ?? ?? ? ?? ??] 
Reference .. to end years of bloody conflict between israel and palestine as soon as possible 
.. to end at an early date years of bloody conflict between israel and palestine 
Joshua ? in the early period to end years of blood conflict between israel and palestine 
s2t ? at an early date to end years of blood conflict between israel and palestine 
 
 
 
2 
FT2ET- DeepSim ? to end years of blood conflict between israel and palestine at an early date 
Source sentence ?? [? ? ?? ?? ??? ?? ?? ? ?? ??? ?? ?] [?] ? 
 
Reference 
the europen union said in a joint statement issued after its summit meeting with china ?s 
premier wen jiabao ? 
in a joint statement released after the summit with chinese premier wen jiabao , the 
europen union said ? 
Joshua the europen union with chinese premier wen jiabao in a joint statement issued after the 
summit meeting said ? 
s2t the europen union in a joint statement issued after the summit meeting with chinese 
premier wen jiabao said ? 
 
 
 
 
 
3 
FT2ET- DeepSim the europen union said in a joint statement issued after the summit meeting with chinese 
premier wen jiabao ? 
 
Table 3: Some translation examples produced by Joshua, string-to-tree system s2t and source-syntax-augmented 
string-to-tree system FT2ET with deep similarity matching algorithm 
 
to-tree model and applies fuzzy syntax on source 
side; and 2) we not only adopt the 0-1 fuzzy rule 
matching algorithm, but also investigate likelihood 
matching and deep similarity matching algorithms. 
8 Conclusion and Future Work 
In this paper, we have proposed a new method for 
augmenting string-to-tree translation models with 
fuzzy use of the source syntax. We first applied a 
fuzzy annotation method which labels the source 
side of each string-to-tree rule with SAMT-style 
syntactic categories. Then we designed and 
explored three fuzzy rule matching algorithms: 0-1 
matching, likelihood matching, and deep similarity 
matching. The experiments show that our new 
system significantly outperforms the strong 
baseline string-to-tree system. This substantial 
improvement verifies that our fuzzy use of source 
syntax is effective and can enhance the ability to 
choose proper translation rules during decoding 
while guaranteeing grammatical output with 
explicit target trees. We believe that our work may 
demonstrate effective ways of incorporating both-
side syntax in a translation model to yield 
promising results. 
   Next, we plan to further study the likelihood 
fuzzy matching and deep similarity matching 
algorithms in order to fully exploit their potential. 
For example, we will combine the merits of 0-1 
matching and likelihood matching so as to avoid 
the setting of parameter m in likelihood matching. 
We also plan to explore another direction: we will 
annotate the source side of each string-to-tree rule 
with subtrees or subtree sequences. We can then 
apply tree-kernel methods to compute a degree of 
matching between a rule and a test source subtree 
or subtree sequence. 
Acknowledgments 
The research work has been funded by the Natural 
Science Foundation of China under Grant No. 
60975053, 61003160 and 60736014 and supported 
by the External Cooperation Program of the 
Chinese Academy of Sciences. We would also like 
to thank Mark Seligman and Yu Zhou for revising 
the early draft, and anonymous reviewers for their 
valuable suggestions.  
 
 
213
References  
Yehoshua Bar-Hillel, 1953. A quasi-arithmetical 
notation for syntactic description. Language, 29 (1). 
pages 47-58. 
David Chiang, 2005. A hiearchical phrase-based model 
for statistical machine translation. In Proc. of ACL 
2005, pages 263-270. 
David Chiang, 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2). 
pages 201-228. 
David Chiang, 2010. Learning to translate with source 
and target syntax. In Proc. of ACL 2010, pages 
1443-1452. 
David Chiang, Kevin Knight and Wei Wang, 2009. 
11,001 new features for statistical machine 
translation. In Proc. of NAACL 2009, pages 218-
226. 
Brooke Cowan, Ivona Kucerova and Michael Collins, 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP, pages 232-241. 
Yuan Ding and Martha Palmer, 2005. Machine 
translation using probabilistic synchronous 
dependency insertion grammars. In Proc. of ACL 
2005, pages 541-548. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu, 2004. What?s in a translation rule. In Proc. 
of HLT-NAACL 2004, pages 273?280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer, 2006. Scalable inference and training of 
context-rich syntactic translation models. In Proc. 
of ACL-COLING 2006. 
Mark Hopkins and Greg Langmead, 2010. SCFG 
decoding without binarization. In Proc. of EMNLP 
2010, pages 646-655. 
Liang Huang and David Chiang, 2007. Forest rescoring: 
Faster decoding with integrated language models. 
In Proc. of ACL 2007, pages 144-151. 
Liang Huang, Kevin Knight and Aravind Joshi, 2006. A 
syntax-directed translator with extended domain of 
locality. In Proc. of AMTA 2006, pages 65-73. 
Zhongqiang Huang, Martin Cmejrek and Bowen Zhou, 
2010. Soft syntactic constraints for hierarchical 
phrase-based translation using latent syntactic 
distributions. In Proc. of EMNLP 2010, pages 138-
147. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin and Evan Herbst, 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proc. of ACL 2007, pages 177-180. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of EMNLP 
2004, pages 388?395. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri 
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren N.G. Thornton, Jonathan Weese and Omar F. 
Zaidan, 2009. Joshua: An open source toolkit for 
parsing-based machine translation. In Proc. of ACL 
2009, pages 135-139. 
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006, pages 
609-616. 
Yang Liu, Yajuan Lv and Qun Liu, 2009. Improving 
tree-to-tree translation with packed forests. In Proc. 
of ACL-IJCNLP 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight, 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Yuval Marton and Philip Resnik, 2008. Soft syntactic 
constraints for hierarchical phrased-based 
translation. In Proc. of ACL-08: HLT. pages 1003?
1011. 
Haitao Mi, Liang Huang and Qun Liu, 2008. Forest-
based translation. In Proc. of ACL-08: HLT. pages 
192?199. 
Haitao Mi and Qun Liu, 2010. Constituency to 
dependency translation with forests. In Proc. of 
ACL 2010, pages 1433-1442. 
Tom M. Mitchell, 1997. Machine learning. Mac Graw 
Hill. 
Franz Josef Och, 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 
2003, pages 160-167. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440. 
Chris Quirk, Arul Menezes and Colin Cherry, 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL 2005, 
pages 271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel, 2008. A 
new string-to-dependency machine translation 
algorithm with a target dependency language 
model. In Proc. of ACL-08: HLT, pages 577-585. 
Tong Xiao, Jingbo Zhu, Muhua Zhu and and Huizhen 
Wang, 2010. Boosting-based System Combination 
for Machine Translation. In Proc. of ACL 2010, 
pages 739-748. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight, 2006. Synchronous binarization for 
machine translation. In Proc. of HLT-NAACL 2006, 
pages 256-263. 
214
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew 
Lim Tan, 2009. Forest-based tree sequence to 
string translation model. In Proc. of ACL-IJCNLP 
2009, pages 172-180. 
Hui Zhang, Min Zhang, Haizhou Li and Chng Eng 
Siong, 2010. Non-isomorphic forest pair 
translation. In Proc. of EMNLP 2010, pages 440-
450. 
Andreas Zollmann and Ashish Venugopal, 2006. Syntax 
augmented machine translation via chart parsing. 
In Proc. of Workshop on Statistical Machine 
Translation 2006, pages 138-141. 
 
 
215
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 363?373,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Multi-Predicate Semantic Role Labeling
Haitong Yang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{htyang, cqzong}@nlpr.ia.ac.cn
Abstract
The current approaches to Semantic Role
Labeling (SRL) usually perform role clas-
sification for each predicate separately and
the interaction among individual predi-
cate?s role labeling is ignored if there is
more than one predicate in a sentence. In
this paper, we prove that different predi-
cates in a sentence could help each other
during SRL. In multi-predicate role label-
ing, there are mainly two key points: argu-
ment identification and role labeling of the
arguments shared by multiple predicates.
To address these issues, in the stage of
argument identification, we propose nov-
el predicate-related features which help re-
move many argument identification errors;
in the stage of argument classification, we
adopt a discriminative reranking approach
to perform role classification of the shared
arguments, in which a large set of glob-
al features are proposed. We conducted
experiments on two standard benchmarks:
Chinese PropBank and English PropBank.
The experimental results show that our
approach can significantly improve SRL
performance, especially in Chinese Prop-
Bank.
1 Introduction
Semantic Role Labeling (SRL) is a kind of shal-
low semantic parsing task and its goal is to rec-
ognize some related phrases and assign a joint
structure (WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW) to each predicate of a sen-
tence (Gildea and Jurafsky, 2002). Because of
the ability of encoding semantic information, SR-
L has been applied in many tasks of NLP, such as
question and answering (Narayanan and Haraba-
gir, 2004), information extraction (Surdeanu et
The justices will be forced to reconsider  the questions.
[      A1      ] [  Pred  ]
[      A0      ] [    Pred    ] [      A1      ]
Figure 1: A sentence from English PropBank,
with an argument shared by multiple predicates
al., 2003; Christensen et al., 2005), and machine
translation (Wu and Fung, 2009; Liu and Gildea,
2010; Xiong et al., 2012; Zhai et al., 2012).
Currently, an SRL system works as follows:
first identify argument candidates and then per-
form classification for each argument candidate.
However, this process only focuses on one inde-
pendent predicate without considering the internal
relations of multiple predicates in a sentence. Ac-
cording to our statistics, more than 80% sentences
in Propbank carry multiple predicates. One exam-
ple is shown in Figure 1, in which there are two
predicates ?Force? and ?Reconsider?. Moreover,
the constituent ?the justices? is shared by the two
predicates and is labeled as A1 for ?Force? but as
A0 for ?Reconsider?. We call this phenomenon of
the shared arguments Role Transition . Intuitive-
ly, all predicates in a sentence are closely related to
each other and the internal relations between them
would be helpful for SRL.
This paper has made deep investigation on
multi-predicate semantic role labeling. We think
there are mainly two key points: argument identi-
fication and role labeling of the arguments shared
by multiple predicates. We adopt different strate-
gies to address these two issues.
During argument identification, there are a large
number of identification errors caused by the poor
performance of auto syntax trees. However, many
of these errors can be removed, if we take other
predicates into consideration. To achieve this pur-
pose, we propose novel predicates-related features
which have been proved to be effective to recog-
363
nize many identification errors. After these fea-
tures added, the precision of argument identifica-
tion improves significantly by 1.6 points and 0.9
points in experiments on Chinese PropBank and
English PropBank respectively, with a slight loss
in recall.
Role labeling of the shared arguments is anoth-
er key point. The predicates and their shared argu-
ment could be considered as a joint structure, with
strong dependencies between the shared argumen-
t?s roles. If we consider linguistic basis for joint
modeling of the shared argument?s roles, there are
at least two types of information to be captured.
The first type of information is the compatibility
of Role Transition among the shared argument?s
roles. A noun phrase may be labeled as A0 for a
predicate and at the same time, it can be labeled
as A1 for another predicate. However, there are
few cases that a noun phrase is labeled as A0 for a
predicate and as AM-ADV for another predicate
at the same time. Secondly, joint modeling the
shared arguments could explore global informa-
tion. For example, in ?The columbia mall is ex-
pected to open?, there are two predicates ?expect?
and ?open? and a shared argument ?the columbi-
a mall?. Because this shared argument is before
?open? and the predicate ?open? is in active voice,
a base classifier often incorrectly label this argu-
ment A0 for ?open?. But if we observe that the ar-
gument is also an argument of ?expect?, it should
be labeled as A1 for ?expect? and ?open?.
Motivated by the above observations, we at-
tempt to jointly model the shared arguments? roles.
Specifically, we utilize the discriminative rerank-
ing approach that has been successfully employed
in many NLP tasks. Typically, this method first
creates a list of n-best candidates from a base sys-
tem, and then reranks them with arbitrary features
(both local and global), which are either not com-
putable or are computationally intractable within
the base model.
We conducted experiments on Chinese Prop-
Bank and English PropBank. Results show that
compared with a state-of-the-art base model, the
accuracy of our joint model improves significant-
ly by 2.4 points and 1.5 points on Chinese Prop-
Bank and English PropBank respectively, which
suggests that there are substantial gains to be made
by jointly modeling the shared arguments of mul-
tiple predicates.
Our contributions can be summarized as fol-
lows:
? To the best of our knowledge, this is the first
work to investigate the mutual effect of mul-
tiple predicates? semantic role labeling.
? We present a rich set of features for argument
identification and shared arguments? classifi-
cation that yield promising performance.
? We evaluate our method on two standard
benchmarks: Chinese PropBank and English
PropBank. Our approach performs well in
both, which suggests its good universality.
The remainder of this paper is organized as fol-
lows. Section 2 gives an overview of our approach.
We discuss the mutual effect of multi-predicate?
argument identification and argument classifica-
tion in Section 3 and Section 4 respectively. The
experiments and results are presented in Section
5. Some discussion and analysis can be found in
Section 6. Section 7 discusses the related work-
s. Finally, the conclusion and future work are in
Section 8.
2 Approach Overview
As illustrated in Figure 2, our approach follows the
standard separation of the task of semantic role la-
beling into two phases: Argument Identification
and Argument Classification . We investigate the
effect of multiple predicates in Argument Identi-
fication and Argument Classification respectively.
Specifically, in the stage of Argument Identifica-
tion, we introduce new features related to predi-
cates which are effective to recognize many argu-
ment identification errors. In the stage of Argu-
ment Classification, we concentrate on the classi-
fication of the arguments shared by multiple pred-
icates. We first use a base model to generate n-
best candidates for the shared arguments and then
construct a joint model to rerank the n-best list, in
which a rich set of global features are proposed.
3 Argument Identification
In this section, we investigate multi-predicate? mu-
tual effects in Argument Identification. Argument
Identification is to recognize the arguments from
all candidates of each predicate. Here, we use
the Maximum Entropy (ME) classifier to perform
binary classification. As a discriminative model,
ME can easily incorporate arbitrary features and
364
C andidates
Phase 1: 
Argument Identification Base 
Features
 New 
Features
Cl assifier
Refined Argument 
C andidates
Phase 2 : 
Argument Classification
Joint Model
B ase Model
N - Best list
Final Results
Is - Shared
Figure 2: The overview of our approach
achieve good performance. The model is formu-
lated as follows:
p(y|x) =
1
Z(x)
exp(
?
i
?
i
f
i
(x, y)) (1)
in which x is the input sample, y(0 or 1) is the out-
put label, f(x, y) are feature functions and Z(x)
is a normalization term as follows:
Z(x) =
?
y
exp(
?
i
?
i
f
i
(x, y))
3.1 Base Features
Xue (2008) took a critical look at the features used
in SRL and achieved good performance. So, we
use the same features in Xue (2008) as the base
features:
? Predicate lemma
? Path from node to predicate
? Head word
? Head word?s part-of-speech
? Verb class (Xue, 2008)
? Predicate and Head word combination
? Predicate and Phrase type combination
? Verb class and Head word combination
? Verb class and Phrase type combination
3.2 Additional Features
In the SRL community, it is widely recognized
that the overall performance of a system is large-
ly determined by the quality of syntactic parsers
(Gildea and Palmer, 2002), which is particularly
notable in the identification stage. Unfortunate-
ly, the state-of-the-art auto parsers fall short of the
demands of applications. Moreover, when there
are multiple predicates, or even multiple clauses
in a sentence, the problem of syntactic ambiguity
increases drastically (Kim et al., 2000). For ex-
ample, in Figure 3, there is a sentence with two
consecutive predicates ?/? (is) and ? ?? (devel-
op). Compared with the gold tree, the auto tree is
less preferable, which makes the classifier easily
mistake ??Q? (building) as an argument of ? 
?? (develop) with base features. But this identifi-
cation error can be removed if we note that there
is another predicate ?/? (is) before ? ?? (devel-
IP
NP VP
V V NP
DNP NP
VC VP
N N D EG
IP
NP VP
VC NP
CP NP
IP DEC
V V NP
??
?
?? ??
?
????
??
??
?
?? ?
????
( a ) ( b)
?? ? ?? ?? ? ????
Building is an economic activity of developing Pudong .
Figure 3: An example from Chinese PropBank.
Tree (a) is the gold syntax tree and (b) is parsed by
a state of-the-art parser Berkeley parser. On tree
(b), ??Q? (building) is mistaken as an argument
of ? ?? (develop) with base features.
365
op). Similar examples with the pattern ?NP +/ +
VV? can be found in PropBank, in which the sub-
ject NP of the sentence is usually not an argument
of the latter predicate. Thus, ?/? (is) is an effec-
tive clue to detect this kind of identification error.
It is challenging to obtain a fully correct syntax
tree for a complex sentence with multiple predi-
cates. Therefore, base features that heavily rely
on syntax trees often fail in discriminating argu-
ments from candidates as demonstrated in Figure
3. However, by considering the elements of neigh-
boring predicates, we could capture useful clues
like in the above example and remove many iden-
tification errors. Below, we define novel predi-
caterelated features to encode these ?clues? to re-
fine candidates.
There are mainly five kinds of features as fol-
lows.
? Is the given predicate the nearest one?
This is a binary feature that indicates whether
the predicate is the nearest one to the candi-
date.
? Local adjunct
This is a binary feature designed for adjective
and adverbial phrases. Some adjunct phras-
es, such as ??? (only), have a limited sphere
of influence. If the candidate is ?local? but
the given predicate is not the nearest one, the
candidate is often not an argument for the
given predicate. To collect local adjuncts, we
traverse the whole training set to get the ini-
tial lexicon of adjuncts and refine it manually.
? Cut-Clause
This type of feature is a binary feature de-
signed to distinguish identification errors of
noun phrase candidates. If a noun phrase can-
didate is separated from the given predicate
by a clause consisting of a NP and VP, the
candidate is usually not the argument of the
given predicate.
? Different Relative Positions with Conjunc-
tions
This is a binary feature that describes whether
the candidate and the predicate are located in
different positions as separated by conjunc-
tions such as ?F/? (but). Conjunctions are
often used to concatenate two clauses, but the
first clause commonly describes one proposi-
tion and the second clause describes anoth-
er one. Thus, if the candidate and the given
predicate have different positions relative to
the conjunctions, the candidate is usually not
the argument of the given predicate.
? Consecutive Predicates Sequence
When multiple predicates appear in a sen-
tence consecutively, parse errors frequently
occurs due to the problems of syntactic am-
biguity as demonstrated in Figure 2. To in-
dicate such errors, sequence features of the
candidates and consecutive predicates are de-
fined specifically. For instance, for the candi-
date ??Q? (building) of ? ?? (develop),
the features are ?cand-/- ?? and ?cand-
/-VV?, in which we use ?cand? to represent
the position of the candidate.
4 Argument Classification
In this section, we investigate multi-predicate? mu-
tual effects in Argument Classification. Argument
Classification is to assign a label to each argumen-
t candidate recognized by the phase of Argument
Identification.
4.1 Base Model
A conventional method in Argument Classifica-
tion is to assign a label to each argument candidate
by a classifier independently. We call this kind of
method Base Model. In the base model, we still
adopt ME (1) as our classifier; all base features of
Argument Identification are contained (shown in
subsection 3.1). In addition, there are some other
features:
? Position: the relative position of the candi-
date argument compared to the predicate
? Subcat frame: the syntactic rule that expands
the parent of the verb
? The first and the last word of the candidate
? Phrase type: the syntactic tag of the candidate
argument
? Subcat frame+: the frame that consists of the
NPs (Xue, 2008).
366
4.2 Joint Model
As discussed briefly in Section 1, there are many
dependencies between the shared arguments? la-
beling for different predicates, but the base model
completely ignores such useful information. To
incorporate these dependencies, we employ the
discriminative reranking method. Here, we first
establish a unified framework for reranking. For
an input x, the generic reranker selects the best
output y
?
among the set of candidates GEN(x)
according to the scoring function:
y
?
= argmax
y?GEN(x)
score(y) (2)
In our task, GEN(x) is a set of the n-best can-
didates generated from the base model. As usual,
we calculate the score of a candidate by the dot
product between a high dimensional feature and a
weight W:
score(y) = W ? f(y) (3)
We estimate the weight W using the aver-
aged perceptron algorithm (Collins, 2002a) which
is well known for its fast speed and good per-
formance in similar large-parameter NLP tasks
(Huang, 2008). The training algorithm of the
generic averaged perceptron is shown in Table 1.
In line 5, the algorithm updates W with the differ-
ence (if any) between the feature representations
of the best scoring candidate and the gold candi-
date. We also use a refinement called ?averaged
parameters? that the final weight vector W is the
average of weight vectors over T iterations and N
samples. This averaging effect has been shown to
reduce overfitting and produces more stable results
(Collins, 2002a).
Pseudocode: Averaged Structured Perceptron
1: Input: training data(x
t
, y
?
t
) for t = 1, ..., T ;
2: w?
(0)
? 0; v ? 0; i? 0
3: for n in 1,...,N do
4: for t in 1, ..., T do
5: w?
(i+1)
? update w?
(i)
according to (x
t
, y
?
t
)
6: v ? v + w?
i+1
7: i? i+ 1
8: w? ? v//(N ? T )
9: return w?
Table 1: The perceptron training algorithm
4.3 Features for Joint Model
Here, we introduce features used in the joint mod-
el. For clear illustration, we describe these fea-
tures in the context of the example in Figure 1.
Role Transition (RT): a binary feature to in-
dicate whether the transitions among roles of the
candidate are reasonable. Because all roles are as-
signed to the same candidate, all role transitions
should be compatible. For instance, if an argu-
ment is labeled as AM-TMP for one predicate, it
cannot be labeled as AM-LOC for another pred-
icate. This feature is constructed by traversing
the training data to ascertain whether transitions
between all roles are reasonable. In Table 2, we
list some role transitions which are obtained from
the training data of experiments on Chinese Prop-
Bank.
Roles and Predicates? Sequence (RPS): a
joint feature template that concatenates roles and
the given predicates. For the gold candidate
?Arg1, Arg0?, the feature is ?Arg1-force, Arg0-
reconsider?.
Roles and Predicates? Sequence with Rela-
tive Orders (RPSWR): the template is similar to
the above one except that relative orders between
roles and predicates are added. If the shared argu-
ment is before the given predicate, the feature is
described as ?Role-Predicate?; otherwise, the fea-
ture is ?Predicate-Role?. And, if the predicate?s
voice is passive, the order is reversed. Thus, for
the gold candidate ?Arg1, Arg0?, this feature is
?force-Arg1, Arg0-reconsider?.
Roles and Phrase Type Sequence (RPTS)
Roles and Head Word Sequence (RHWS)
Roles and Head Word?s POS Sequence
(RHWPS)
These three features are utilized to explore the
shared argument?s relations with roles.
Time and Location Class (TLC): We find
there are much confusions between AM-TMP and
AM-LOC in the base model. To fix these errors,
we add two features: Time and Location Class.
For these features, we just collect phrases labeled
as AM-TMP and AM-LOC from the training da-
ta. When the argument belongs to Time or Loca-
tion Class, we add a sequence template consisting
of ?Role-Time? for Time Class or ?Role-Location?
for Location Class. For the gold candidate ?Arg1,
Arg0?, the feature is ?Arg1-none, Arg0-none? be-
cause ?the justices? belongs neither to Time Class
nor to Location Class.
367
Role Arg0 Arg1 Arg2 AM-LOC AM-TMP AM-ADV AM-MNR AM-TPC
Arg0 + + + + + + + +
Arg1 + + + + - + + +
Arg2 + + + + - - - +
AM-LOC + + + + - + - +
AM-TMP + - - - + + - -
AM-ADV + - + - + + - -
AM-MNR + + - - - - + -
AM-TPC + + + + + + - +
Table 2: Some role transitons from Chinese PropBank. ?+? means reasonable role transition and ?-?
means illegal.
5 Experiments
5.1 Experimental Setting
To evaluate the performance of our approach, we
have conducted on two standard benchmarks: Chi-
nese PropBank and English PropBank. The exper-
imental setting is as follows:
Chinese:
We use Chinese Proposition Bank 1.0. All data
are divided into three parts. 648 files (from cht-
b 081.fid to chtb 899.fid) are used as the training
set. 40 files (from chtb 041.fid to chtb 080.fid)
constitutes the development set. The test set con-
sists of 72 files (chtb 001.fid to chtb 040.fid and
chtb 900.fid to chtb 931.fid). This data setting is
the same as in (Xue, 2008; Sun et al., 2009). We
adopt Berkeley Parser
1
to carry out auto parsing
for SRL and the parser is retrained on the training
set. We used n =10 joint assignments for training
the joint model and testing.
English:
We choose English Propbank as the evaluation
corpus. According to the traditional partition, the
training set consists of the annotations in Sections
2 to 21, the development set is Section 24, and
the test set is Section 23. This data setting is the
same as in (Xue and Palmer, 2004; Toutanova et
al., 2005). We adopt Charniak Parser
2
to carry out
auto parsing for SRL and the parser is retrained on
the training set. We used n =10 joint assignments
for training the joint model and testing.
5.2 Experiment on Argument Identification
We first investigate the performance of our ap-
proach in Argument Identification.
For the task of Argument Identification (AI), we
1
http://code.google.com/p/berkeleyparser/
2
https://github.com/BLLIP/bllip-parser
adopt auto parser to produce auto parsing trees for
SRL. The results are shown in Table 3. We can
see that in the experiment of Chinese, the F1 score
reaches to 78.79% with base features. While after
additional predicates-related features are added,
the precision has improved by 1.6 points with s-
light loss in recall, which leads to the improve-
ment of 0.6 points in F1. The similar effect oc-
curred in the experiment of English. After addi-
tional features added in the identification module,
the precision is improved by about 0.9 points with
a slight loss in recall, leading to an improvement
of 0.3 points in F1. However, the improvemen-
t in English is slight smaller than in Chinese. We
think the main reason is that there are less parse er-
rors in English than in Chinese. All results demon-
strate that the novel predicted-related features are
effective in recognizing many identification errors
which are difficult to discriminate with base fea-
tures.
P(%) R(%) F
1
(%)
Ch
Base 84.36 73.90 78.79
+Additional 85.97 73.72 79.38*
En
Base 82.86 76.83 79.73
+Additional 83.75 76.69 80.06
Table 3: Comparison with Base Features in Ar-
gument Identification. Scores marked by ?*? are
significantly better (p < 0.05) than base features.
5.3 Experiment on Argument Classification
5.3.1 Results
Errors produced in AI will influenced the evalu-
ation of Argument Classification (AC). So, to e-
valuate fairly we assume that the argument con-
stituents of a predicate are already known, and the
368
Num Acc(%)
Ch
Shared 2060 91.36
All 8462 92.77
En
Shared 2015 93.85
All 14061 92.30
Table 4: Performance of the Base Model in Argu-
ment Classification
Methods Acc(%)
Ch
Base 91.36
Joint 93.74*
En
Base 93.85
Joint 95.33*
Table 5: Comparison with Base Model on shared
arguments. Scores marked by ?*? are significantly
better (p < 0.05) than base model.
task is only to assign the correct labels to the con-
stituents. The evaluation criterion is Accuracy.
The results of the base model are shown in Ta-
ble 4. We first note that in testing set, there are a
large number of shared arguments, which weigh-
s about one quarter of all arguments in Chinese
and 14% in English. Therefore, the fine process-
ing of these arguments is essential for argumen-
t classification. However, the base model cannot
handle these shared arguments so well in Chinese
that the accuracy of the shared arguments is lower
by about 1.4 points than the average value of all
arguments. Nevertheless, from Table 5 we can see
that our joint model?s accuracy on the shared argu-
ments reaches 93.74%, 2.4 points higher than the
base model in Chinese. Although the base mod-
el obtain good performance on shared arguments
of English, our joint model?s performance reach-
es 95.33%, 1.5 points higher than the base mod-
el. This indicates that even though the base model
is optimized to utilize a large set of features and
achieves the state-of-the-art performance, it is still
advantageous to model the joint information of the
shared arguments.
Another point to note is that our joint model in
resolving English SRL task is not so good as in
Chinese SRL. There are mainly two reasons. The
first reason is that the shared arguments occur less
in English than in Chinese so that training sam-
ples are insufficient for our discriminative model.
The second reason is the annotation of some in-
transitive verbs. In English PropBank, there is a
class of intransitive verbs such as ?land? (known
as verbs of variable behavior), for which the ar-
gument can be tagged as either ARG0 or ARG1.
Here, we take examples from the guideline
3
of En-
glish PropBank to explain.
?A bullet (ARG1) landed at his feet?
?He (ARG0) landed?
In the above examples, the two arguments and
the predicate ?land? have the same relative order
and voice but the arguments have different label-
s for their respective predicates. In fact, accord-
ing to the intention of the annotator, ARG0 and
ARG1 are both correct. Unfortunately, in English
PropBank, there is only one gold label for each ar-
gument, which leads to much noise for our joint
model. Moreover, such situations are not rare in
the corpus.
5.3.2 Feature Performance
We investigate effects of the features of joint mod-
el to the performance and results are shown in Ta-
ble 6. Each row shows the improvement over the
baseline when that feature is used in the joint mod-
el. We can see that features proposed are beneficial
to the performance of the joint model. But some
features like ?RPS? and ?RPSRO? play a more im-
portant role.
Features Chinese English
base 91.36 93.85
RT 91.70 94.10
RPS 92.30 94.70
RPSRO 92.24 94.50
RPTS 91.80 94.18
RHWS 91.63 93.95
RHWPS 91.43 94.23
TCL 91.93 94.23
All 93.74 95.33
Table 6: Features performance in the Joint Model.
We use first letter of words to represent features.
5.4 SRL Results
We also conducted the complete experiment on the
auto parse trees. The results are shown in Table
7. In experiments on Chinese PropBank, we can
see that after novel predicate-related features are
added in the stage of Argument Identification, our
model outperforms the base model by 0.5 points
3
http://verbs.colorado.edu/propbank/EPB-
AnnotationGuidelines.pdf
369
F1
(%)
Chinese
Base 74.04
Base + AI 74.50
Base + AI + AC 75.31
English
Base 76.44
Base + AI 76.70
Base + AI + AC 77.00
Table 7: Results on auto parse trees. Base mean-
s the baseline system, +AI meaning predcates-
related features added in AI, + AC meaning joint
module added.
Methods F
1
(%)
Chinese
Xue(2008) 71.90
Sun et al.(2009) 74.12
Ours 75.31
English
Surdeanu and Turmo(2005) 76.46
Ours 77.00
Table 8: Comparison with Other Methods
in F1. Furthermore, after incorporating the joint
module, the performance goes up to 75.31%, 1.3
points higher than the base model. We obtain sim-
ilar observations in experiments on English Prop-
Bank, but due to reasons illustrated in Subsection
5.3, the performance of our method is slight better
than the base model.
We compare our method with others and the re-
sults are shown in Table 8. In Chinese, Xue (2008)
and Sun et al. (2009) are pioneer works in Chinese
SRL. Our approach outperforms these approaches
by about 3.4 and 1.9 F1 points respectively. In
English SRL, we compare out method with Sur-
deanu and Turmo (2005) which is best result ob-
tained with single parse tree as the input in CON-
LL 2005 SRL evaluation. Our approach is better
than their approach which ignores the relation of
multiple predicates? SRL.
6 Discussion and Analysis
In this section, we discuss some case studies that
illustrate the advantages of our model. Some ex-
amples from our experiments are shown in Table
9. In example (1), the argument is a preposition-
al phrase ?( n? ] t I? Y? ? ??
(at the same time of compulsory education) and
shared by two predicates ??0? (witness) and ?i
'? (expand). In the corpus, a prepositional phrase
is commonly labeled as ARGM-LOC and ARGM-
TMP. Thus, the base model labeled the argument
into these classes but one as ARGM-LOC, another
as ARGM-TMP. Unfortunately, ARGM-LOC for
??0? (witness) is wrong while our joint model
outputs both correct answers, which benefits from
the role transition feature. From Table 1, we can
see that the role transition between ARGM-TMP
and ARGM-LOC is impossible, which lowers the
score of candidates containing both ARGM-LOC
and ARGM-TMP in the joint model. Thus, the
joint model is more likely to output the gold can-
didate.
In example (2), the argument is ?w? ?:
:? (Hailar Airport) and shared by two predicates
?i?? (expand) and ?:? (become). Because of
the high similarity of the features in the base mod-
el, the argument for both predicates is classified
into the same class ARG0, but the label for ?i??
(expand) is wrong. Nevertheless, our joint mod-
el obtains both correct labels, which benefits from
the global features. After searching the training
data, we find some similar examples to this one,
such as ?0? ?L ? ? i? ?  l
?? (The railway operation mileage is expanded to
120 kilometers), in which ?0? ?L ?? (the
railway operation mileage) is labeled as ARG1 for
?i?? (expand) but ARG0 for ??? (to). We think
these samples provide evidence for our joint mod-
el while these information has not been captured
by the base model.
In example (3), the argument is ??? ?
?? ? ? ' ??? (a large group with high
reputation) and shared by predicates ??U? (de-
velop) and ?:? (become). Different from the
above cases in which only one label is wrong in
the base model, both labels for ??U? (develop)
and ?:? (become) are misclassified by the base
model. However, our method still gets correct an-
swers for both predicates, which also benefits from
the global features.
7 Related work
Our work is related to semantic role labeling
and discriminative reranking. In this section, we
briefly review these two types of work.
On Semantic Role Labeling
Gildea and Jurafsky (2002) first presented a sys-
tem based on a statistical classifier which is trained
on a hand-annotated corpora FrameNet. In their
pioneering work, they used a gold or autoparsed
syntax tree as the input and then extracted vari-
ous lexical and syntactic features to identify the
370
Examples Base Ours
1. (n?]tI?Y???-I
LA Joint Model to Identify and Align Bilingual
Named Entities
Yufeng Chen?
National Laboratory of Pattern
Recognition, Institute of Automation,
Chinese Academy of Sciences
Chengqing Zong??
National Laboratory of Pattern
Recognition, Institute of Automation,
Chinese Academy of Sciences
Keh-Yih Su?
Behavior Design Corporation
In this article, an integrated model is derived that jointly identifies and aligns bilingual named
entities (NEs) between Chinese and English. The model is motivated by the following obser-
vations: (1) whether an NE is translated semantically or phonetically depends greatly on its
entity type, (2) entities within an aligned pair should share the same type, and (3) the initially
detected NEs can act as anchors and provide further information while selecting NE candidates.
Based on these observations, this article proposes a translation mode ratio feature (defined as
the proportion of NE internal tokens that are semantically translated), enforces an entity type
consistency constraint, and utilizes additional new NE likelihoods (based on the initially detected
NE anchors).
Experiments show that this novel method significantly outperforms the baseline. The type-
insensitive F-score of identified NE pairs increases from 78.4% to 88.0% (12.2% relative im-
provement) in our Chinese?English NE alignment task, and the type-sensitive F-score increases
from 68.4% to 83.0% (21.3% relative improvement). Furthermore, the proposed model demon-
strates its robustness when it is tested across different domains. Finally, when semi-supervised
learning is conducted to train the adopted English NE recognition model, the proposed model
also significantly boosts the English NE recognition type-sensitive F-score.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
? No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China.
E-mail: chenyf@nlpr.ia.ac.cn.
?? No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China.
E-mail: cqzong@nlpr.ia.ac.cn.
? Hsinchu, Taiwan. E-mail: bdc.kysu@gmail.com.
Submission received: 9 October 2010; revised submission received: 15 February 2012; accepted for
publication: 27 March 2012.
doi:10.1162/COLI a 00122
Computational Linguistics Volume 39, Number 2
1. Introduction
Named entities (NEs), especially person names (PER), location names (LOC), and
organization names (ORG), deliver essential context and meaning in human languages.
Therefore, NE translation plays a critical role in trans-lingual language processing tasks,
such as machine translation (MT) and cross-lingual information retrieval. To learn NE
translation knowledge, bilingual NE alignment (which links source NEs and target
NEs to generate desired NE pairs) is the first step in producing the NE translation
table (which can then be used to train the NE translation model). Furthermore, with
additional alignment constraints from the other language, the alignment module can
also refine those initially recognized NEs, and thus can be adopted to conduct semi-
supervised learning to learn monolingual NE recognition models from a large untagged
bilingual corpus.
Because NE alignment can only be conducted after its associated NEs have been
identified, the NE recognition errors propagate into the alignment stage. The type-
insensitive inclusion rate1 of the initial recognition stage thus significantly limits the
final alignment performance. One way to alleviate this error propagation problem is to
jointly perform NE recognition and alignment. Such a combined approach is usually
infeasible, however, due to the high computational cost of evaluating alignment scores
for a large number2 of NE pair candidates.
In order to make the problem computationally tractable, a sequential approach is
usually used to first identify NEs and then align them. Two such kinds of sequential
strategies that alleviate the error propagation problem have been proposed. The first
strategy, named asymmetry alignment (Al-Onaizan and Knight 2002; Moore 2003;
Feng, Lv, and Zhou 2004; Lee, Chang, and Jang 2006), identifies NEs only on the source
side and then finds their corresponding NEs on the target side. Although this approach
avoids the NE recognition errors resulting from the target side, which would otherwise
be brought into the alignment process, the NE recognition errors from the source side
continue to affect alignment.
To further reduce the errors from the source side, the second strategy, denoted
symmetry alignment (Huang, Vogel, and Waibel 2003), expands the NE candidate
sets in both languages before conducting the alignment. This is achieved by using the
original results as anchors, and enlarging or shrinking the boundaries of the anchors to
generate new candidates. This strategy fails to work if the NE anchor has already been
missed in the initial NE recognition stage, however. In our data set (1,000 Chinese?
English sentence pairs randomly selected from the Chinese News Translation Text
corpus [LDC2005T06]), this strategy significantly improves the type-insensitive NE pair
inclusion rate from 83.9% to 96.1%;3 in the meantime, the type-insensitive Chinese NE
(CNE) recognition inclusion rate rises from 88.7% to 95.9%, and that of English NE
(ENE) from 92.8% to 97.2%. This strategy is thus adopted in this article.
Although the symmetric expansion strategy has substantially alleviated the prob-
lem of error propagation, the final alignment accuracy, in terms of type-sensitive F-score
1 This is the percentage of desired NE pairs that are included within the given candidate set, and is the
upper bound for NE alignment performance (type-insensitive means disregarding NE types).
2 This number will dramatically increase if the combined approach is adopted, as every possible string
will become a NE candidate.
3 This figure is based on the expanded candidates that are constructed without any range limitation. In
this case, Inclusion rate = 1 ? [Missing Rate of Initial NEs]. In addition, because not every Chinese NE is
linked in the given sentence pair, the Inclusion rate of Chinese NEs is even lower than that of NE pairs
(95.9% vs. 96.1%).
230
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
(achieved by the approach proposed by Huang, Vogel, and Waibel [2003]) continues
to be as low as 68.4% (see in Table 3 in Section 4.3). After having examined the data,
we found the following: (1) How a given NE is translated, either semantically (called
translation) or phonetically (called transliteration), depends greatly on its associated
entity type.4 The translation mode ratio, which is the percentage of NE internal tokens
that are translated semantically, thus can help to identify the NE type. (2) Entities within
an aligned pair should share the same type, and this restriction should be integrated
into the NE alignment model as a constraint. (3) In prior work, the initially identified
monolingual NEs were used only to construct the candidate set without playing any
role in final NE identification. Indeed, these monolingual NEs do carry other useful in-
formation and can act as anchors to giveNE likelihoods, which can provide additional
scope preference information to those regenerated candidates.
Based on these observations, we propose a novel joint model that adopts the transla-
tion mode ratio, enforces the entity type consistency constraint, and also utilizes the NE
likelihoods. This proposed approach jointly identifies and aligns bilingual NEs under
an integrated framework, which consists of three stages: Initial NE Recognition, NE-
Candidate Set Expansion, and NE Re-identification & Alignment. The Initial NE Recog-
nition stage identifies the initial NEs and their associated NE types in both the source
and target. In the next stage, NE Candidate Set Expansion regenerates the candidate
sets in both languages in order to remedy the initial NE recognition errors. In the final
stage, NE Re-identification & Alignment jointly recognizes and aligns bilingual NEs via
the proposed joint model. The experimental results validate our proposed three-step
method.
The integrated model that jointly identifies and aligns bilingual named entities
between Chinese and English was originally introduced in Chen, Zong, and Su (2010).
In this article, the problem has been re-formulated and derived. The new derivation
starts from two given NE sequences, whereas the original derivation only begins with
one given NE pair. We also give more details of the problem study, model analysis, and
experiments. Moreover, we report additional experiments, which include those that
study the effect of adopting different initial NE recognizers and the effectiveness of the
proposed model across different domains. Finally, a complete error analysis is given in
the current version.
The remainder of this article is organized as follows: Section 2 motivates the pro-
posed method. Afterwards, the proposed model is formally introduced in Section 3.
Section 4 describes experiments conducted on various configurations of the method.
The associated error analysis and discussion of results are presented in Section 5.
Section 6 gives applications of the proposed model. We review related work in Section 7.
Finally, conclusions are drawn in Section 8.
2. Motivation
By examining the NEs initially recognized in aligned sentence pairs, we have the
following two observations: (1) Alignment can help fix those NEs that are initially in-
correctly recognized when they are not the correct counterparts of each other. Therefore,
4 The proportions of semantic translation (which denote the ratios of semantically translated words
among all the associated words within NEs) for PER, LOC, and ORG are approximately 0%, 28.6%, and
74.8%, respectively, in the Chinese?English name entity list (2005T34) released by the Linguistic Data
Consortium (LDC). Because titles, such as ?sir? and ?chairman,? are not considered part of person
names in this corpus, all PERs are transliterated.
231
Computational Linguistics Volume 39, Number 2
alignment and recognition should be jointly optimized. (2) Alignment cannot help in
determining the appropriate scope when each word within an NE (or within its larger
context window covering the NE) is correctly matched to its counterpart. Therefore, the
information of those initial NEs should be utilized to decide the appropriate NE scope.
The following two sections further elaborate on these two observations.
2.1 Alignment Helps NE Recognition
In NE recognition, both boundary identification and type classification are required.
The complexity of these tasks varies with different languages, however. For example,
Chinese NE boundaries are not obvious because adjacent words are not separated by
spaces. In contrast, English NE boundaries are easier to identify with explicit words and
capitalization clues. On the other hand, classification of English NE type is considered
more challenging (Ji and Grishman 2006).
Because alignment would force the NEs in the linked NE pair to share the same
semantic meaning, the NE that is more reliably identified in one language can be
used to identify its less reliable counterpart in the other language. This benefit, which
is observed in both NE boundary identification and type classification, indicates that
alignment can be used to locate those NEs that are initially incorrectly recognized. For
example, once the correct boundaries are drawn in one language, word equivalences
inside an aligned NE pair can help identify NE boundaries in the language that does
not have explicit clues (e.g., Chinese). As shown in Example (1), even though the desired
Chinese NE ????????? is only partially recognized as ?????? in the initial
recognition stage, it can be recovered if the English counterpart North Korean [no?s]
Central News Agency is given. The reason for this is that News Agency is better aligned
to ?????, rather than be deleted, which would occur if ?????? is chosen as the
corresponding Chinese NE.
On the other hand, type consistency constraints can help correct the NE type that
is less reliably identified. Moreover, in identifying the NE type, it helps if we know
whether a word is translated or transliterated. As illustrated in Example (2), the word
lake in the English NE is linked to the Chinese character ???, and this mapping is
found to be a translation, not a transliteration. Because translation rarely occurs for
personal names (Chen, Yang, and Lin 2003), the desired NE type ?LOC? should be
shared between the English NE Lake Constance and its corresponding Chinese NE ???
???.? As a result, the original incorrect type ?PER? of the given English NE is fixed; it
thus corroborates the need for using the translation mode ratio and NE type consistency
constraint.
232
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
2.2 Initial NEs Carry NE Scope Information
In Huang, Vogel, and Waibel (2003), initial NE information was discarded once the
new candidate set was generated. Alignment scores alone, however, are incapable of
selecting the appropriate NE scope in some cases. For instance, when each word in an
NE is correctly matched to its counterpart, the alignment score might still prefer an
incorrect NE pair with smaller scope, even after the normalization of alignment terms
has been considered (explained in Section 4.3). On the other hand, when words sur-
rounding the desired NE are also correctly matched to their counterpart, the incorrect
NE pair with larger scope might be chosen. As illustrated in Example (3), the desired NE
pair {????::[Germany]} is initially correctly recognized, although the wrong NE pair
{??????::[Germany economy]} is finally selected from the regenerated candidate set
if only alignment scores are used in the final selection process. This is because the extra
words ???? and economy are a perfect translation of each other, thus resulting in the
incorrect pair {??????::[Germany economy]}, which receives a higher alignment
score than does the correct NE pair {????::[Germany]}.
It must also be noted that an NE alignment model usually ignores a fair amount of
potentially useful information that is commonly used in monolingual NE recognition
models. For example, the bigrams of text surrounding NEs are frequently adopted in
a monolingual NE recognition model, but not in a bilingual alignment model. Other
233
Computational Linguistics Volume 39, Number 2
examples include case information, part-of-speech (POS) triggers, gazetteer features,
and external macro context features mentioned in Zhou and Su (2006). The alignment
model fails to consider the monolingual context surrounding NEs while determining
NE scope, resulting in the error mentioned in Example 3. By ignoring the initial NEs
after their corresponding candidate sets have been generated, we lose the information
provided by these initial NEs that is otherwise available to the alignment model.
Therefore, though the initially detected NEs might be unreliable by themselves, they
should act as anchors to provide scope preference information, even after the expanded
candidate set has been generated from these NEs.
3. The Proposed Joint Model
Given a Chinese?English sentence pair (Sc,Se), with its initial (denoted by lower
case letter b, for ?beginning?) Chinese NEs [Cbi,Tci]
Nc
i=1,Nc ? 1 and English NEs
[Ebj,Tej]
Ne
j=1,Ne ? 1 , where Nc and Ne are the numbers of initially recognized NEs of
Chinese and English, respectively; Tci and Tej are the original NE types assigned to Cbi
and Ebj , respectively. (For the reader?s convenience, all adopted notations are listed in
Table 1 for quick reference.) We first regenerate two NE candidate sets (by enlarging
and shrinking the boundaries of those initial NEs) to include, we hope, the correct
corresponding candidates that failed to be recognized in the first stage. Let CKc1 and E
Ke
1
denote the two sets that include those regenerated candidates for Chinese and English
NEs, respectively (Kc and Ke are their set-sizes), and K = min (Nc,Ne). Then, the total K
pairs of the final Chinese and English NEs will be extracted from the Cartesian product
of CKc1 and E
Ke
1 . Here, only NE pairs in one-to-one mappings will be extracted, as most
applications are only interested in this kind of correspondence. Therefore, we will let CK1
and EK1 denote the two extracted candidate sets to be linked, and these sets will consist
of non-overlapping NE candidates from CKc1 and E
Ke
1 , respectively (for conciseness,
we will not explicitly distinguish between the indices CK1 and C
Kc
1 , or between
EK1 and E
Ke
1 ).
Let
{
Ca(k),Ek
}
denote a specific NE-linking-pair (where a(k) and k are the associated
indices of those regenerated Chinese and English NEs within CK1 and E
K
1 , respectively);
the subscript a(k) denotes that Ca(k) is aligned to Ek. Let Tk be the NE type to be re-
assigned and shared by Ca(k) and Ek (as they should denote the same entity). Assuming
that only one-to-one mappings of NE pairs will be extracted, the problem of getting
Table 1
Adopted notations (keywords have been italicized).
Symbols Symbols
Chinese Sentence Sc English Sentence Se
Initial/Beginning Chinese NE Cb Initial/Beginning English NE Eb
Regenerated Chinese NE C Regenerated English NE E
Initial Chinese NE Type Tc Initial English NE Type Te
Re-assigned NE Type T NE Sequence Alignment As
Internal Component Alignment A Internal Translation Mode M
Chinese Component cp English Word ew
Translation Mode Ratio ? Chinese Character cc
Left Distance dL Right Distance dR
Initial/Beginning NE Length Lb Regenerated NE Length L
234
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
the final desired aligned NE pairs
{
C?a(k),E
?
k ,T
?
k
}K
k=1
is that of finding the most likely
allowable combination of NE pairs (and their re-assigned NE types), given all the initial
NEs (i.e., [Cbi,Tci]
Nc
i=1 and [Ebj,Tej]
Ne
j=1) and the sentences that include them (i.e., Sc and
Se). This can be formulated as follows:
{
C?a(k),E
?
k ,T
?
k
}K
k=1
= arg max
{Ca(k),Ek}
K
k=1
?
?
?
max
TK1
P
(
{
Ca(k),Ek,Tk
}K
k=1
|[Cbi,Tci]Nci=1,Sc, [Ebj,Tej]
Ne
j=1,Se
)
?
?
?
(1)
For any given NE?pair sequence
{
Ca(k),Ek
}K
k=1
, the internal max operator will first
operate over each re-assigned type-sequences TK1 (where T belongs to PER, LOC, ORG
for each given NE pair included in the NE?pair sequence). The outer argmax operator
will cross every admissible NE?pair sequence.
This formulation implies that recognition and alignment are executed jointly
with respect to CK1 and E
K
1 , without making any independence assumptions among
those NE pairs included in the associated NE?pair sequence. This equation is
thus computationally infeasible due to a large search space. Therefore, it is fur-
ther simplified and derived as follows by first explicitly denoting the link between
Ca(k) and Ek as AS,k =
{
Ca(k),Ek
}
. Let {AS,k}
K
k=1 (abbreviated as A
K
S,1) denote one possi-
ble alignment between CK1 and E
K
1 . We will then have K! different possible align-
ments between them (i.e., total factorial of K different AKS,1). Finally, let [Cba(k),Tca(k)]
K
k=1
and [Ebk,Tek]
K
k=1 denote those initially recognized corresponding NEs that gener-
ate CK1 and E
K
1 (i.e., their associated anchors), respectively. Then we can replace
{
Ca(k),Ek, Tk}
K
k=1 in Equation (1) by [AS,k,Ca(k),Ek,Tk]
K
k=1, and derive the original prob-
ability P
(
{
Ca(k),Ek,Tk
}K
k=1
|[Cbi,Tci]Nci=1,Sc, [Ebj,Tej]
Ne
j=1,Se
)
as follows.
P
(
{
Ca(k),Ek,Tk
}K
k=1
|[Cbi,Tci]Nci=1,Sc, [Ebj,Tej]
Ne
j=1,Se
)
? P
(
[AS,k,Ca(k),Ek,Tk]
K
k=1|[Cba(k),Tca(k)]
K
k=1,Sc, [Ebk,Tek]
K
k=1,Se
)
?
K
?
k=1
[P
(
[AS,k,Ca(k),Ek,Tk]|[Cba(k),Tca(k)],Sc, [Ebk,Tek],Se
)
]
(2)
where P
(
[AS,k,Ca(k),Ek,Tk]|[Cba(k),Tca(k)],Sc, [Ebk,Tek],Se
)
can be further decomposed as
follows.
P
(
[AS,k,Ca(k),Ek,Tk]|[Cba(k),Tca(k)],Sc, [Ebk,Tek],Se
)
? P(AS,k|Ca(k),Ek,Tk) ? P
(
Tk|Tca(k),Tek,Sc,Se
)
? P(Ca(k)|Cba(k),Tca(k),Tk,Sc) ? P(Ek|Ebk,Tek,Tk,Se)
(3)
In Equation (3), P(AS,k|Ca(k),Ek,Tk) and P
(
Tk|Tca(k),Tek,Sc,Se
)
are defined as
NE Alignment Probability and NE Type Re-assignment Probability, respectively, for
finding the final alignment AKS,1 among C
K
1 and E
K
1 that have been selected. Both
P(Ca(k)|Cba(k),Tca(k),Tk,Sc) and P(Ek|Ebk,Tek,Tk,Se) are called NE likelihoods, and are
used to assign preference to each selected CK1 and E
K
1 , based on the initial NEs (which
act as anchors). For brevity, we drop the associated subscripts hereafter, if there is
235
Computational Linguistics Volume 39, Number 2
no confusion. These probabilities will be further described in Sections 3.1 and 3.2.
Finally, the joint identification and alignment framework that incorporates the initial
NE recognition process, candidate set construction, and the associated search process
are given in Section 3.3.
3.1 Bilingual Related Probabilities
The NE alignment probability represents the likelihood of a specific alignment AS,k,
given C and E and their associated T. Because Chinese word segmentation in-
troduces errors, especially for transliterated words, the NE alignment probability
P(AS,k|Ca(k),Ek,Tk) in Equation (3) is derived from E (i.e., starting from the English part).
In addition, because internal component alignment (denoted as A, to be defined later)
within a given NE pair carries important information (as illustrated in Section 2), the
internal component alignment will be introduced as follows.
P(AS,k|Ca(k),Ek,Tk) =
?
A
P(A|C,E,T)
? max
A
P(A|C,E,T) (4)
= max
A
[ 1
R
? P(A|E,T)]
where R =
?
A P(A|E,T) is a normalization value,
5 which will be ignored for simplicity,
leaving only the probability P(A|E,T) to be derived.
Let A be configured as A ? ?[cpa(n), ewn,Mn]Nn=1, ??, where [cpa(n), ewn,Mn] denotes
a linked pair of a Chinese component cpa(n) (which might contain several Chinese char-
acters) and an English word ewn within C and E, respectively, with their translation
mode Mn to be either translation (abbreviated as TS) or transliteration (abbreviated as
TL). We assume that there are N component transformations in total, including NTS
translation transformations [cpa(n), ewn,TS]
NTS
n=1 and NTL transliteration transformations
[cpa(n), ewn,TL]
NTL
n=1, such that N = NTS +NTL. Moreover, because the statistical distribu-
tion of internal translation mode varies greatly across various NE types (as illustrated in
footnote [4] of this article), the associated translation mode ratio ? = (NTS/N) is an impor-
tant feature and is included in the internal component alignment specified previously.
For example, if the A between ??????? and Constance Lake is [????, Constance,
TL] and [?, Lake, TS] (NTS = NTL = 1), then its associated translation mode ratio will be
0.5 (i.e., ? = 1/2).
Therefore, the internal alignment probability P(A|E,T) will be further deduced by
introducing the translation mode Mn and the translation mode ratio ? as follows:
P(A|E,T) ? P([cpa(n), ewn,Mn]Nn=1, ?|E,T)
?
N
?
n=1
[P(cpa(n)|Mn, ewn,T) ? P(Mn|ewn,T)] ? P(?|T) (5)
5 The summation will be taken over various A that can generate C.
236
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Combining Equations (4) and (5), the NE alignment probability P(AS,k|Ca(k),Ek,Tk),
which integrates internal component alignment information such as translation mode
ratio and NE type constraint, is finally obtained as follows.
P(AS,k|Ca(k),Ek,Tk)
? 1R ? maxA
[
NA
?
n=1
[P(cpA,a(n)|MA,n, ewA,n,T) ? P(MA,n|ewA,n,T)] ? P(?A|T)
]
(6)
where R is the normalization factor defined by Equation (4), and will be ignored in
the final selection. In Equation (6), the mappings between internal elements is trained
from the syllable/word alignment of NE pairs of different NE types. For translitera-
tion, the model adopted in Huang, Vogel, and Waibel (2003), which first romanizes
Chinese characters and then transliterates them into English characters, is used in
estimating P(cpa(n)|TL, ewn,T). For translation, conditional probability is directly used
for P(cpa(n)|TS, ewn,T).
On the other hand, the NE type re-assignment probability P (T|Tc,Te,Sc,Se), proposed
in Equation (3), is derived as follows.
P (T|Tc,Te,Sc,Se) ? P (T|Tc,Te) (7)
As Equation (7) shows, both the initially assigned Chinese NE type Tc and the initially
assigned English NE type Te are adopted to jointly identify their shared NE type T.
3.2 Monolingual NE Likelihoods
The monolingual related probabilities in Equation (3) represent the likelihood that a
regenerated NE candidate is the true NE, given its originally detected NE. For Chinese,
we derive the likelihood as follows.
P(C|Cb,Tc,T,Sc)
? P
(
dL, dR,String[C]|Lb,Tc,T
)
? P(dL|Lb,Tc,T) ? P(dR|Lb,Tc,T) ?
?L
l=1 P(ccl|ccl?1,T)
(8)
Here Lb is the length (in characters) of the original recognized Chinese NE Cb. Let dL and
dR denote the left and right distance, respectively (which are based on the numbers of
Chinese characters), that C shrinks/enlarges from the left and the right boundaries of
its anchor Cb. In Example (1) in Section 2.1, in the case where the given Cb and C are
?????? and ????????, respectively, then dL and dR are ?1 and +3, respec-
tively. Let String[C] denote the associated Chinese string of C, ccl denote the l-th Chinese
character within that string, and L denote the total number of Chinese characters within
C. Then we will have a range of bigram probabilities for candidates with different
lengths. Therefore, it is systematically biased6 (in probability value) towards candidates
with shorter lengths. On the English side, following Equation (8), P(E|Eb,Te,T,Se) can
be derived similarly; the unit is a word, however, rather than a character.
6 This bias is introduced by the conditional independence assumption made while decomposing
P
(
String[C]|T
)
into
?L
l=1 P(ccl|ccl?1, T).
237
Computational Linguistics Volume 39, Number 2
In summary, with factors dL and dR, the proposed NE likelihood is able to assign
scope preference to each regenerated NE based on its associated initial NE. The initial
NE therefore still plays a role in the final selection process, even after its related candi-
date set has been generated, which is important when all words involved are correctly
matched to their counterparts, as explained in Section 2.2. In contrast, Huang, Vogel,
and Waibel (2003) adopt only type-dependent bigrams as the NE likelihood. The initial
NE thus will not play any role in the final selection process after its related candidate
set has been generated. The scope preference information carried by the initial NE is
therefore not utilized in their model.
Having integrated all related probabilities (Equations (6), (7), and (8)) together,
we now have the final desired model. For simplicity, all the probabilities involved are
estimated by the Good-Turing smoothing technique (Chen and Goodman 1998) unless
otherwise specified.
3.3 Framework for Jointly Identifying and Aligning Bilingual NEs
In jointly identifying and aligning bilingual NEs, a three-stage framework is adopted:
(A) Initial NE Recognition, generating the initial NE anchors with off-the-shelf pack-
ages, (B) NE Candidate Set Expansion, expanding the associated NE Candidate set to
remedy the errors made in the previous stage, and (C) NE Re-identification & Align-
ment, extracting the final NE pairs from the Cartesian product of source and target
candidate sets (created in the second stage) via a search process. Figure 1 presents the
detailed procedure of this framework.
1 For each given bilingual sentence pair:
2 (A) Initial NE Recognition: The initial Chinese NEs and English NEs are first identified by their
corresponding NE recognition toolkits, respectively.
3 (B) NE Candidate Set Expansion: To rescue those NEs whose boundaries are incorrectly identified in
the previous stage?for each initially detected NE, several NE candidates will be regenerated from the
original NE by allowing its boundaries to be shrunk or enlarged within a pre-specified range.
4 (B.1) Create both C and E candidate sets, which are expanded from those initial NEs recognized in
the previous stage.
5 (B.2) Construct a NE pair candidate set (namedNE-Pair-Candidate set) by generating a Cartesian
product of C and E candidate sets created in the above step.
6 (C)NE Re-identification & Alignment: Rank each candidate in the NE-Pair-Candidate set constructed
above with the score specified by the proposed model. Let Nc and Ne be the numbers of those initial
Chinese and English NEs in the first stage, respectively, and set K = min(Nc,Ne). Extract top K final
NE pairs (with their re-assigned NE types) with the highest scores from the NE-Pair-Candidate set.
7 (C.1) FOR each NE pair in the NE-Pair-Candidate set created above:
FOR each re-assigned NE type within {PER, LOC, ORG}
Evaluate the score for the given candidate pair and the given NE type according to the
proposed model.
END FOR
Find the re-assigned NE type with the highest score, then attach it and its corresponding
score to the given NE pair
END FOR (C.1)
8 (C.2) Conduct a beam search process to select the top K non-overlapping NE pairs from the NE-Pair-
Candidate set with the scores assigned above. The searching process will keep removing those over-
lapping NEs from the candidate list before each state is branched.
Figure 1
A framework for jointly identifying and aligning bilingual NEs.
238
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Example 4 illustrates how the framework works. For simplicity, we will allow the
Chinese NE to enlarge/shrink its boundaries to four characters on each side, and only
allow two words for English.
Example 4. An example of candidate set construction
Each NE and its type in this example is separated by ?/?. Only partial and relevant
information is shown here.
(A.1) A Chinese tagged sentence: ??????????/PER? ????????????
?. . .??;
(A.2) Initial Chinese NE (Cb): ??????/PER?, Nc = 1;
(A.3) An English tagged sentence: ?The report said the [Galapagos/PER] [National
Park/ORG] and local fishermen were working together . . .?;
(A.4) Initial English NEs (Eb): [Galapagos/PER], [National Park / ORG], Ne = 2;
(B.1.1) Regenerated Chinese candidate set (CK1 ): {???????, ???????????, ???,
??????????, ????, ???, ???, ???, ???, ??????????????, ????
?????, . . . .}. Total 62 C candidates will be generated.
(B.1.2) Regenerated English candidate set (EK1 ): {[Galapagos], [National Park], [Galapagos Na-
tional Park], [said the Galapagos National Park], [National Park and local], [National],
[Park], . . .}. Total 24 E candidates will be generated.
(B.2) NE-Pair-Candidate set: {???????::[Galapagos], ???????????::[Galapagos
National Park], ???????::[National Park], ???????????::[Park], . . . . . .}.
Total 1,488 (62 ? 24) NE pairs will be generated.
(C) K = min(Nc,Ne) = 1. Therefore, only {???????????::[Galapagos National
Park], ORG} will be extracted.
This example shows that the desired Chinese NE ??????????? is partially
recognized as ??????? initially with an incorrect NE type PER. In addition, the
desired English NE [Galapagos National Park] is split into [Galapagos] and [National
Park], initially two NEs. After each NE Candidate set has been expanded, the desired
NEs ??????????? and [Galapagos National Park] are included in C and E
candidate sets, respectively. The desired NE pair {???????????::[Galapagos
National Park], ORG} can thus be located.
If we allow the boundaries to be enlarged/shrunk without any limitation during
the expansion step, all NEs that are initially incorrectly recognized can then be in-
cluded. The generated search space, however, would be too large to be tractable. In
our observation, four Chinese characters for both shrinking and enlarging, and two
English words for shrinking and three for enlarging, are found to be adequate in
most cases. (Note that only the candidate that contains at least one original character/
word is allowed.) Under this condition, the inclusion rates for NEs with correct
boundaries can be increased to 94.6% (from 88.7%) for Chinese, and 96.3% (from
92.8%) for English, respectively; the NE pair inclusion rate can even be increased to
95.3% from 83.9%. Because the inclusion rate achieved by this strategy (with limited
range) is only 0.8% lower than that obtained without any range limitation (which
is 96.1%, as some NEs might have been completely missed in the first stage), this
setting is adopted in this article to reduce the search space. Even with this expansion
strategy, however, those missing and spurious (false positive) errors still cannot be
remedied, because we will neither create additional anchors nor delete any existing
anchor.
239
Computational Linguistics Volume 39, Number 2
4. Experiments on Various Configurations
To evaluate the proposed approach, prior work (Huang, Vogel, and Waibel 2003) is
re-implemented as our baseline (see Section 4.2). This is because the work not only
adopts the same candidate set expansion strategy mentioned previously, but also uti-
lizes monolingual information when selecting NE pairs (only a simple bigram model is
used, however). This is in contrast to other works (Feng, Lv, and Zhou 2004; Lee, Chang,
and Jang 2006), which only used alignment scores.
The same training and test sets are used for the various experiment configura-
tions. The adopted training set includes two parts. The first part consists of 110,874
aligned sentence pairs from newswire data in the Foreign Broadcast Information Service
(LDC2003E147) corpus, which is denoted as Training Set I. The average length of the
Chinese sentences in this data set is 74.6 characters, and the average length of the
English sentences is 30.2 words. Training Set I is initially tagged by Chinese/English
NE taggers, and then reference NE boundaries and types are manually labeled. The
second part of the training set is the LDC2005T348 bilingual NE pair list with a total
of 218,772 NE pairs, which is denoted as Training Set II. The required features (e.g.,
NE type and translation-mode) are then manually labeled throughout the two training
sets. Because Training Set II only contains isolated NE pairs that are not associated with
their surrounding context, Training Set I is thus required to train those context-related
parameters.
In the baseline system, translation cost and transliteration cost models are trained
on Training Set II, and tagging cost is trained on Training Set I. For the proposed
approach, the NE likelihoods are trained on Training Set I, and Training Set II is used to
train the parameters relating to the NE alignment probability.
For the test set, 300 sentence pairs are randomly selected from the Linguistic Data
Consortium (LDC) Chinese?English News Text (LDC2005T06) corpus, which contains
at least one NE pair in each sentence. The average length of Chinese sentences is 59.4
characters, and the average length of English sentences is 24.8 words. The answer
keys to NE recognition and alignment are annotated manually, and used as the gold
standard to calculate the metrics of precision (P), recall (R), and F-score (F) for both
NE recognition and alignment. A total of 765 Chinese NEs and 747 English NEs are
manually identified in the test set, in which there are 718 reference NE pairs (including
214 PER pairs, 371 LOC pairs, and 133 ORG pairs). NE alignment result is a subset of
NE recognition results, because not all those recognized NEs can be aligned.
The development set for feature selection and weight training is composed of 200
sentence pairs selected from the LDC2005T06 corpus, which includes 482 manually
tagged NE pairs. The average length of Chinese sentences is 56.4 characters, and the
average length of English sentences is 23.2 words. There is no overlap between the
training, development, and test sets.
These data sets will be adopted in a series of experiments that investigate the
proposed model. Among them, the results of initial NE recognition are given in Sec-
tion 4.1, and those related to the baseline system are given in Section 4.2. In Section 4.3,
a series of experiments are conducted to examine the effect of various features adopted
in the proposed model. The weighted version of the proposed model is also tested.
7 FBIS multilingual text (http://projects.ldc.upenn.edu/TIDES/mt2003.html).
8 The LDC2005T34 data set consists of proofread bilingual entries: 73,352 person names, 76,460 location
names, and 68,960 organization names.
(http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T34).
240
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 2
Initial type-sensitive Chinese/English NER performance.
NE type P (%) R (%) F (%)
PER 80.2/79.2 87.7/85.3 83.8/82.1
LOC 89.8/85.9 87.3/81.5 88.5/83.6
ORG 78.6/82.9 82.8/79.6 80.6/81.2
ALL 83.4/82.1 86.0/82.6 84.7/82.3
Furthermore, the effectiveness of adopting different initial NE recognizers is shown in
Section 4.4, and the effectiveness of the proposed model across different domains is
illustrated in Section 4.5. Finally, the result of directly using all available features under
a Maximum Entropy framework without developing a principled model is given in
Section 4.6.
4.1 Initial NE Recognizers
Both the baseline alignment system and the proposed model share the same Initial
NE Recognition subtask. The systems adopt the Chinese NE recognizer reported in
Wu, Zhao, and Xu (2005), which is a hybrid statistical model incorporating multi-
knowledge sources, and the English NE recognizer included in the publicly available
Mallet toolkit9 (McCallum 2002) to generate initial NEs. These two initial NE recog-
nizers are adopted because their performance is comparable to other state-of-the-art
systems (Gao, Li, Wu, and Huang 2005; Zhou and Su 2006). The NE recognition baseline
performances reported subsequently are provided by these two packages. A total of 789
Chinese NEs and 752 English NEs are recognized.
Table 2 shows the initial NE recognition (NER) performance for both Chinese and
English (the highest performance in each column is in bold). It is observed that the
F-score of ORG type is the lowest among all NE types for both English and Chinese.
This is because many organization names are only partially recognized or missed alto-
gether. In addition, the precision rate of PER type is lowest among all English NE types
because many location names or abbreviated organization names tend to be incorrectly
recognized as person names in English. In general, the initial Chinese NER outperforms
the initial English NER, as the NE type classification turns out to be a more difficult
problem for this English NER system.
4.2 The Baseline System
The model of Huang, Vogel, and Waibel (2003) is re-implemented in our environment as
the baseline system, and is briefly sketched here for the reader?s convenience. There are
three cost features in Huang?s alignment model: (1) transliteration cost, which measures
the phonetic similarity of the aligned NEs; (2) translation cost, which is similar to IBM
model-1 (Brown et al 1993); and (3) tagging cost, which evaluates bigram probabilities
of the aligned NEs based on the same NE type.
9 http://mallet.cs.umass.edu/index.php/Main Page.
241
Computational Linguistics Volume 39, Number 2
Table 3
NEA type-insensitive (type-sensitive) performance on the test set.
Model P (%) R (%) F (%)
ExpB (Baseline) 77.1 (67.1) 79.7 (69.8) 78.4 (68.4)
Exp1 (B-Probabilities) 76.2 (72.3) 78.5 (74.6) 77.3 (73.4)
Exp2 (B-Probabilities N-Alignment) 77.7 (73.5) 79.9 (75.7) 78.8 (74.6)
Exp3 (N-Full Model) 83.7 (78.1) 86.2 (80.7) 84.9 (79.4)
Exp4 (MERT-W) 85.9 (80.5) 88.4 (83.0) 87.1 (81.7)
In our experiments, the translation cost of the baseline system is trained on Training
Set I (with 110,874 aligned sentence pairs) by the GIZA++ toolkit (Och and Ney 2003),
the transliteration cost is trained on all person names (all are transliterated) and translit-
erated location and organization names included in Training Set II. The tagging cost is
trained on the tagging result of Training Set I by the initial NE detection system.
When those initially identified NEs are directly used for alignment, only a 64.1%
F-score (regarding their NE types) is obtained from this baseline system. This relatively
poor performance is mainly due to errors in the initial NE recognition stage that are
brought into the subsequent alignment stage. To diminish the accumulative effect of
errors, the same expansion strategy described in Section 3.3 is then adopted to enlarge
the possible NE candidate set. However, only a slight improvement is obtained (from
64.1% to 68.4% for type-sensitive F-score), as shown in Table 3 in Section 4.3. Therefore,
it is conjectured that the baseline alignment model is unable to perform well if the
features proposed in this article are not adopted.
4.3 The Re-identification and Alignment Joint Model
To examine the individual effect of features adopted in the model, a series of experi-
ments are first conducted on the development set. All features mentioned in Section 3
are verified by their contributions and are then adopted for further experiments on the
test set. Table 3 lists only the representative performance of NE alignment (NEA) on
the test set, and gives two performance measures for the experiments. The first one
(named type-insensitive) only checks the scope of each NE without taking its associated
NE type into account (which is the approach adopted in most of the literature on NE
recognition), and is reported as the main metric in Table 3. The second one (named
type-sensitive) also evaluates the associated NE type of each NE. To evaluate the type-
sensitive performance for NE pairs with correct boundaries, we give one point to any
NE pair that also possesses the correct type-tags on both sides, and give 0.5% if only
one side is correct. Of course, zero points are given if both types are incorrect or the
boundary of any NE is incorrect. With the rules specified herein, the type-sensitive
results are also given within the parentheses in Table 3, and a large degradation is
observed. The configurations of various experiments are listed as follows.
ExpB: This is the baseline system (Huang, Vogel, and Waibel 2003), which is re-
implemented in our environment for comparison.
Exp1: Exp1 (named B-Probabilities) adopts all bilingual related probabilities involved in
Equations (6) and (7), to show the full power of bilingual probabilities.
242
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Exp2: Furthermore, because the NE alignment probability would favor the candidates
with fewer components,10 it is further normalized by converting Equation (6) into
the following form:
max
A
?
?
?
?
?
?
?
NA
?
n=1
P(cpA,a(n)|MA,n, ewA,n,T) ? P(MA,n|ewA,n,T)
?
?
1
NA
? P(?A|T)
?
?
?
?
?
The experiment covering complete bilingual probabilities, with the normalized
versions of Equations (6) and (7), is denoted as Exp2 (named B-Probabilities
N-Alignment).
Table 3 indicates that Exp2 achieves the best performance (both type-insensitive
and type-sensitive) among different combinations of the bilingual-related prob-
abilities. Due to its effectiveness, the normalized bilingual probabilities are hence
adopted in all subsequent experiments (i.e., all are based on Exp2).
Exp3: Exp3 (named N-Full Model) manifests the full power of the proposed recognition
and alignment joint model, by integrating all monolingual options into Exp2.
Note that we use the SRI Language Modeling Toolkit11 (Stolcke 2002) to train
various character-/word-based bigram models on different NE types. They are
trained with modified Kneser-Ney smoothing (Kneser and Ney 1995, Chen and
Goodman 1998). Note that monolingual bigrams are also normalized with their
numbers.
As Exp3 shows, the best configuration is to take advantage of all features proposed
in this article and to normalize all feature probabilities. This configuration will thus be
taken for further improvement in the following sections.
So far the proposed model weighs all features equally. It is reasonable to expect that
features should be weighted differently according to their contribution, however. Those
weighting coefficients can be learned from the development set via the well-known
Minimum Error Rate Training approach (Schlu?ter and Ney 2001; Och 2003) (commonly
abbreviated as MERT). To save computational cost, we only re-evaluate the scores of
candidate pairs in a pre-generated pool, instead of regenerating new candidate pairs
each time when Wt (the vector of weighting coefficients at i-th iteration) is updated to
Wt+1 (of the next iteration). For each sentence pair, its corresponding pool is first created
by using W0 (i.e., Exp3) to generate the top 50 NE pairs
12 resulting from the beam search
process. Subsequently, when we switch Wt to Wt+1, we only re-score (and then re-rank)
those candidate pairs inside the pool according to Wt+1.
Exp 4 presents the weighted version of the proposed joint model obtained from
MERT training (MERT-W, N-Full Model, abbreviated as MERT-W). The result demon-
strates that MERT is effective and useful. Entries in bold indicate that the model signif-
icantly outperforms the baseline system. (All statistical significance tests in this article
are measured with 95% confidence level on 1,000 re-sampling batches [Zhang, Vogel,
and Waibel 2004]).
10 It is biased by the E word-count due to the sufficiency (Freedman 2005; Liese and Miescke 2008)
assumption made during decomposition.
11 http://www.speech.sri.com/projects/srilm/.
12 Because most sentence pairs possess less than four NE pairs under Exp3, 50 NE pairs should be sufficient.
243
Computational Linguistics Volume 39, Number 2
Table 4
NER type-insensitive (type-sensitive) performance of different English NE recognizers.
English NE recognizers P (%) R (%) F (%)
Mallet Toolkit 91.8 (82.1) 92.4 (82.6) 92.1 (82.3)
Stanford NE recognizer 93.7 (84.7) 91.4 (82.3) 92.5 (83.5)
Minor Third 90.8 (82.1) 89.5 (80.7) 90.1 (81.4)
Table 5
NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer
(Wu?s system) and different English NE recognizers.
NE alignment on P (%) R (%) F (%) Upper bound (%)
different recognizers
Mallet Toolkit 85.9 (80.5) 88.4 (83.0) 87.1 (81.7) 95.3
Stanford NE recognizer 85.9 (80.2) 88.4 (82.7) 87.1 (81.4) 95.0
Minor Third 85.7 (80.2) 88.1 (82.7) 86.9 (81.4) 94.2
Compared to the baseline system, the MERT-W version has substantially raised
the test set type-insensitive F-score of identified NE pairs from 78.4% to 87.1% (11.1%
relative improvement), and the type-sensitive F-score from 68.4% to 81.7% (19.4%
relative improvement). Therefore, this MERT-W version is adopted in all further
experiments.
4.4 Effect of Adopting Different Initial NE Recognizers
To study whether the final performance of NE alignment is sensitive to the choice of
initial NE recognizers, we investigate the final alignment performance across different
Chinese and English NE recognizers.
First, we test the NE alignment performance with the same Chinese NE recognizer
(Wu?s system, adopted earlier) but with different English NE recognizers that include
the Mallet toolkit (used before), the Stanford NE recognizer (Finkel, Grenager, and
Manning 2005), and Minor Third (Cohen 2004). Table 4 shows the type-insensitive and
type-sensitive (within parentheses) results. Table 5 shows the effect on NE alignment
performance. From Tables 4 and 5, we find that NE alignment performance is actually
not sensitive to the NE recognition result. Although the performance of different NE
recognizers are various (type-insensitive13 F-scores are 90.1%, 92.1%, and 92.5%, re-
spectively), the gaps among their corresponding NE alignment results are negligible
(type-sensitive F-scores of weighted versions are 81.7%, 81.4%, and 81.4%, respectively),
as their candidate sets are enlarged based on initially recognized NEs. It is also note-
worthy that although the F-score of the Stanford NE recognizer is higher than that
of the Mallet toolkit, its corresponding NE alignment performance is lower than the
model based on the Mallet toolkit. We conjecture that the lower recall of Stanford NE
13 Because the initial NE recognizer mainly provides NE anchors, NE type is less relevant to the following
alignment.
244
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 6
NER type-insensitive (type-sensitive) performance of different Chinese NE recognizers.
Chinese NE recognizers P (%) R (%) F (%)
Wu?s System 86.2 (83.4) 88.7 (86.0) 87.4 (84.7)
BaseNER 88.3 (85.9) 84.9 (82.5) 86.6 (84.2)
S-MSRSeg 86.8 (84.7) 81.5 (79.5) 84.1 (82.1)
Table 7
NEA type-insensitive (type-sensitive) performance with the same English NE recognizer (Mallet
system) and different Chinese NE recognizers.
NE alignment on P (%) R (%) F (%) Upper bound (%)
different recognizers
Wu?s System 85.9 (80.5) 88.4 (83.0) 87.1 (81.7) 95.3
BaseNER 85.6 (79.9) 88.1 (82.4) 86.8 (81.1) 94.2
S-MSRSeg 84.5 (78.9) 87.1 (81.4) 85.8 (80.1) 93.3
recognizer leads to lower NE alignment performance because the recall of NER is closely
related to the NE pair inclusion rate (please refer to footnote 1), which is the upper
bound of its corresponding NE alignment performance.
Similarly, we test the NE alignment performance with the same English NE rec-
ognizer (Mallet) but with different Chinese NE recognizers, including Wu?s system (as
before), BaseNER (Zhao and Kit 2008), and S-MSRSeg (Gao, Li, Wu, and Huang 2005).
The comparisons are given in Tables 6 and 7. From these tables we also see that the
NE alignment result is not sensitive to the NE recognition result (84.1% to 87.4% type-
insensitive for NER vs. 80.1% to 81.7% type-sensitive in F-score for NEA), although the
performance of NE alignment is related to the recall of the Chinese NE recognizer (the
weaker side). We also note that the type-insensitive F-score performance gap among
various English NE recognizers in Table 5 is less than that of the Chinese NE recognizers
in Table 7 (0.2% vs. 1.3%), which is mainly due to the different gaps among their original
performances (2.4% vs. 3.3%, shown by Tables 4 and 6).
Furthermore, NE alignment based on the worst Chinese NE recognizer (S-MSRSeg)
and the worst English NE recognizer (Minor Third) is conducted in Table 8. From Table 8
we see that the final NE alignment performance is primarily determined by the weaker
side, which is the one that gives the lower recognition recall rate. In this particular
case, the performance of the combination of S-MSRSeg and Minor Third (79.9% type-
sensitive F-score) is mainly driven by the performance of the Chinese S-MSRSeg (80.1%
Table 8
NEA type-insensitive (type-sensitive) performance with a different English NE recognizer and
another Chinese NE recognizer.
NE alignment on P (%) R (%) F (%) Upper bound (%)
different recognizers
Wu & Mallet 85.9 (80.5) 88.4 (83.0) 87.1 (81.7) 95.3
S-MSRSeg & Minor 84.7 (78.6) 87.3 (81.3) 86.0 (79.9) 93.9
245
Computational Linguistics Volume 39, Number 2
Table 9
Initial NE recognition type-insensitive (type-sensitive) performance across various domains.
Different domains Language P (%) R (%) F (%)
News (Table 2) Chinese (Wu) 86.2 (83.4) 88.7 (86.0) 87.4 (84.7)
English (Mallet) 91.8 (82.1) 92.4 (82.6) 92.1 (82.3)
HK Hansards Chinese (Wu) 91.4 (88.5) 89.1 (87.3) 90.2 (87.9)
English (Mallet) 93.5 (90.4) 94.3 (91.2) 93.9 (90.8)
Computer Chinese (Wu) 82.7 (81.4) 87.9 (86.5) 85.2 (83.9)
English (Mallet) 76.6 (72.9) 88.9 (85.2) 82.3 (78.6)
Table 10
The superiority of our joint model on three different domains indicated by type-insensitive
(type-sensitive) performance (those significant entries are marked in comparison with baseline).
Different domains Model P (%) R (%) F (%)
News (Table 3) Baseline 77.1 (67.1) 79.7 (69.8) 78.4 (68.4)
Proposed Model 85.9 (80.5) 88.4 (83.0) 87.1 (81.7)
HK Hansards Baseline 86.3 (83.3) 87.1 (84.1) 86.7 (83.7)
Proposed Model 88.2 (86.5) 89.1 (87.3) 88.6 (86.9)
Computer Baseline 69.4 (66.1) 80.3 (77.1) 74.5 (70.3)
Proposed Model 75.5 (72.4) 86.2 (83.1) 79.6 (76.5)
in Table 7). This is because the NE pair inclusion rate is usually dominated by the
weaker side.
4.5 Effectiveness of the Proposed Model Across Different Domains
To test the effectiveness of the joint model across domains, we compare the baseline
and our joint model on three different domains (News, HK Hansards, and Com-
puter Technology). To do this, two other test sets are selected from HK Hansards
(LDC2004T08) and from the computer domain (training data in CWMT08),14 respec-
tively (the test set used in the previous sections is from the News domain). Each of
these new test sets also includes 300 randomly selected sentence pairs.
Table 9 shows the initial NE recognition performance across those three different
domains. Also, it is clear from Table 10 that our joint model outperforms the baseline in
all three domains, which indicates that the advantage of our joint model holds over
various domains. On the other hand, the smaller improvement observed in the HK
Hansards domain might be due to the possibly easier task of initial NE recognition and
NE alignment.15 (Note that the baseline performance in this domain is much higher
than others?with an NE alignment type-sensitive F-score of 83.7% compared with
14 http://nlpr-web.ia.ac.cn/cwmt-2008.
15 Note that 40.3% sentence pairs in the HK Hansards corpus contains only one NE pair (alignment would
be trivial in this case); this ratio is 15.7% and 27.0% for News and Computer domains, respectively.
246
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 11
Comparison between a ME framework and the derived model on the same test set.
Model Data set?Size 400 4,000 40,000 90,412
ME Framework (Maxent) 38.9 (0%) 51.6 (0%) 63.8 (0%) 69.5 (0%)
ME Framework (YASMET) ?2.4 (?6.2%) ?1.2 (?2.3%) ?1.2 (?1.9%) ?1.6 (?2.3%)
Weighted-Joint-Model +2.6 (+6.9%) +3.5 (+6.9%) +3.4 (+5.3%) +2.9 (+4.2%)
68.4% and 70.3% in News and Computer domains, respectively). Therefore, those novel
features of our joint model are not crucial in easy cases.
4.6 Maximum Entropy Framework with Primitive Features
We propose and derive the model described previously in a principled manner. One
might wonder, however, whether it is worthwhile to derive such a model after all related
features have been proposed, as all proposed features can also be directly integrated
into the well-known maximum entropy (ME) framework (Berger, Della Pietra, and
Della Pietra 1996) without making any assumptions. To show that not only features,
but also the adopted model contributes to performance improvement, we build an
ME model that directly adopts all primitive features mentioned previously as its input
(including the internal component alignment-pair, initial and final NE type, NE bigram-
based string, and left/right distance), without involving any related probabilities
derived in the proposed model.
Because an ME approach can be trained only on linked NE pairs, those sentence
pairs that include at least one NE pair are first extracted from Training Set I. A total of
90,412 sentence pairs are obtained, as some sentence pairs only have either Chinese or
English NEs, and 298,302 NE pairs are identified. This ME method is implemented with
the YASMET16 package, and is tested under various training-set sizes (400, 4,000, 40,000,
and 90,412 sentence pairs). Because the NEs of the bilingual NE pair list (Training Set II)
do not contain their corresponding sentences, the ME approach lacks the necessary
context to extract specific ME features and hence this list is left out of our training data
for both the baseline ME model and our joint model.
In order to compare different ME approaches, we also try Zhang?s Maxent pack-
age17 with five classes (i.e., PER, LOC, ORG, Incorrect-Boundaries, Correct-Boundaries-
Incorrect-Type). A five-class approach outperforms a three-class approach (YASMET)
in this case (it has many more features as well). Table 11 shows only the type-sensitive
F-scores evaluated on the same test set to save space. The data within the parentheses
are relative improvements, and entries in bold indicate that the performance of the
derived model is statistically better than that of the ME models.
The improvement indicated in Table 11 clearly illustrates the benefit of deriving the
model. Because a reasonably derived model not only shares the same training set with
the primitive ME approach, but also enjoys the additional knowledge introduced by
the human researcher (i.e., the assumptions/constraints implied by the model), it is not
surprising that a good model does perform well, and the relative improvement becomes
more noticeable when the training set becomes smaller.
16 http://www.fjoch.com/YASMET.html.
17 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html.
247
Computational Linguistics Volume 39, Number 2
When we take a closer look at the 18 instances where our model correctly identifies
the NE alignments and Maxent fails to do so on the test set, we find that the internal
component alignment-pair feature in the Maxent approach to be dominant in causing
56% of the errors (10 out of 18). In contrast, our corresponding internal mapping prob-
ability P(cpa(n)|Mn, ewn,T) in Equation (5) makes the correct decisions 90% of the time
(9 out of 10). In fact, even P(cpa(n)|ewn,T) (the simpler form) makes correct predictions
80% of the time (8 out of 10).
One example of the ten errors made by Maxent is ???????? (Bolivia holds),
which is incorrectly linked to Bolivia holds as a NE pair by the ME approach, whereas
our model correctly aligns ?????? with Bolivia. This is because ?????? is
transliterated into Bolivia, but ???? is translated into holds. Given T = LOC, the in-
ternal mapping probability P(cpa(n)|ewn,T) thus disfavors the translation mode between
???? and hold within an LOC NE pair, and prefers the correct result. This example
illustrates the utility of the explicit dependency constraint imposed by the model, which
is not possible in the ME approach.
5. Discussion and Error Analysis
Although the proposed model substantially improves alignment performance, errors do
remain. Therefore, we would like to know what the limitations of the proposed model
are, and what kinds of problems still remain?an essential component in finding future
directions for further improvements. In the test set, a total of 718 NE reference-pairs and
739 aligned NE pairs are generated from the proposed joint model (MERT-W version).
Among the generated NE pairs, there are 104 (out of 739) boundary errors (regardless of
their re-assigned types), or 14.1%. Also, among the remaining 635 NE pairs with correct
boundaries, 41 (6.5%) are re-assigned to the incorrect NE type. Boundary identification,
therefore, is still a crucial problem.
Before investigating the errors made by MERT-W, we would like to understand its
limits. As mentioned in Section 3.3, the inclusion rate of those desired NE pairs (within
the Cartesian product of expanded candidate sets CK1 and E
K
1 ) is the upper bound for
the system adopted in the final selection stage, which in the current setting is 95.3%.
In comparison, the type-insensitive F-score of MERT-W is 87.1%, indicating that there
is still a significant 8.2% scope for improvement, even though a great improvement has
already been made over the baseline system. We examine this gap and propose solutions
to address the errors in the following section.
5.1 Classification of Type-Insensitive NE Pair Errors
There are 111 type-insensitive NE pair errors in the test set (104 boundary errors plus 7
others not included in the output list due to missing anchors), and these can be classified
into the following six main categories.
(I) Reference Inconsistency (11%): The NE references from Chinese and English
are not correctly matched, which rules out the possibility of generating the
correct NE pair.
(II) Missing Anchor (14%): Although the NE reference is consistent, not all their
associated NE anchors are generated in the initial recognition stage, which
cannot be remedied by the expansion strategy adopted in this article.
248
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
(III) Over-generating Anchors (10%): Similarly, additional spurious NE anchors
are also generated in the initial recognition stage, and result in incorrect
NE pairs.
The remaining cases with correct corresponding anchors are further classified as
follows.
(IV) Inconsistent Components (12%): Although their corresponding anchors are
correctly generated, the internal components of those reference pairs are
originally unmatched due to deletions or insertions occurring in the NE
translation. For example, in the reference {???????::[Berlingen]}, the
Chinese component ??? (town) is originally unmatched because its
English correspondent town does not exist in the given English sentence.
Therefore, only one incorrect pair {??????::[Berlingen]} is generated.
Other cases with all matched components are further classified in the
following.
(V) Expansion Limitation (5%): Even though all the internal components are
matched, the desired candidate (i.e., the reference) is still not covered by the
candidate set after its expansion.
(VI) Others (48%): Even if all internal components are matched and their
references are also included in the candidate set, some errors still remain,
mainly due to the limitations of the current model. These cases account for
the majority of the errors that will be further analyzed subsequently in the
article.
Table 12 shows the distribution of the six defined categories. {?CNE?::[ENE]} is
a specified NE pair, and the unmatched components are underlined. The numbers in
Table 12
Distribution of various error categories (type-insensitive).
Consistency
Problem
Anchor
Problem
Error
Categories
Reference NE pair Initially
Recognized NEs
Final Output Percentage
Inconsistent
References
(11%)
NA (I) Reference
Inconsistency
?????::[ ];
??::[Northam]
CNE: ????
ENE: [Northam]
??
??::[Northam]
11% (12)
Consistent
References
(89%)
Incorrect
Anchors
(24%)
(II) Missing
Anchor
????::[ASEAN] CNE: ?? ENE:
[ASEAN]
No such
alignment
14% (16)
(III) Over-
generating
Anchors
?????::[Lee
Nam-shin]
CNE: ?????
ENE: [Lee
Nam-shin];
[South]
???::[South] 10% (11)
Correct
Anchors
(65%)
(IV) Inconsistent
Components
?????
??::[Berlingen]
CNE: ?????
ENE: [Berlingen]
???
??::[Berlingen]
12% (13)
(V) Expansion
Limitation
(Matched
Components,
Excluded
Reference)
????????
???
??::[British
Pharmaceutical
Firm
GlaxoSmithKline]
CNE: ????; ??
???; ?????
ENE: [British
Pharmaceutical];
[Firm
GlaxoSmithKline]
??
??::[British]
5% (6)
(VI) Others
(Matched
Components,
Included
Reference)
?????::[South
and North
Koreas]
CNE: ??? ENE:
[North Koreas]
??
??::[North
Koreas]
48% (53)
249
Computational Linguistics Volume 39, Number 2
parentheses in the last column denote the number of NE pairs of the corresponding
category. Among all categories, Category (I) errors (Reference Inconsistency, 11%) are
irrelevant to the alignment model, and are attributed to the asymmetrical distribution
of bilingual NEs (corresponding NEs might sometimes be missed or replaced by the
pronoun it). As illustrated in Table 12, CNE ????? (Fossett) is initially recognized as
???? and finally linked to an irrelevant ENE Northam (??). This occurs because their
corresponding counterparts Fossett and ???? do not appear in the original sentence
pair, and our alignment model assumes that the linking between NEs is a one-to-
one mapping. One possible simple solution would be to set a minimal threshold on
alignment scores that filters out such spurious linking. This may introduce the risk that
some correct NE pairs might be pruned away at the same time, however.
Category (II) errors (Missing Anchor, 14%) are due to the absence of the associated
NE anchors in the initial recognition stage. As an example in Table 12, the corresponding
anchor of the Chinese NE ???? (ASEAN) is not initially identified. Because each
candidate set is generated from the given anchor, a missing anchor implies that its
associated candidate set will not exist, thereby making it impossible to generate the
corresponding NE pair. Although increasing the number of output anchors generated
from the initial recognition stage can relieve this problem, doing so makes the
subsequent alignment task harder. Additionally, the spurious anchors generated might
introduce even more errors.
The errors in Category (III) (Over-generating Anchors, 10%) are due to spurious
anchors generated in the initial recognition stage. For instance, the CNE ????? is
originally aligned with the ENE Lee Nam-shin by transliteration. A spurious ENE South is
also identified in the initial stage, however. This spurious ENE South is then incorrectly
linked to a virtual CNE with the highest score, ??,? which is a sub-string of the desired
CNE ????,? and also a Chinese translation for south. This prevents the correct NE
pair from being generated. Both missing and over-generating anchor problems are
largely dependent on the NE recognition toolkits adopted in the initial stage. Using the
current expansion strategy, the initial NE recognizers with lower recall (or precision)
tend to result in worse NE-pair recall (or precision) in the final alignment stage.
Category (IV) errors (Inconsistent Components, 12%) are caused by internal com-
ponents within NEs that were not originally matched. Because words in NEs are not
always translated literally, there are insertions and deletions during NE translation.
As an example, shown in Table 12 and illustrated previously, the incorrect result
{??????::[Berlingen]} is generated for its reference {???????::[Berlingen]}, as
its Chinese component ??? (town) is originally unmatched. In the worse case, those
unmatched components could interleave with matched components within the NE pair,
and thus prevent some matched components from being included. For example, in
the reference {?????????::[European Commission]}, both the Chinese compo-
nents ??? (alliance) and ???? (execution) have no counterparts, and they would have
prevented the matched portion {?????::[Commission]} from being included in the
final output. As a result, only {???::[European]} is eventually extracted. To tackle this
problem of component insertions/deletions that sometimes occur in English?Chinese
translation, the alignment model should be further enhanced to allow the component to
be linked to an empty element, NULL. Introducing this freedom, however, might have
the side effect of including additional spurious Chinese characters (or English words).
Further study is required to justify this idea; given that this category accounts for only
12% of the errors, we propose to defer this for later studies.
Furthermore, Category (V) errors (Expansion Limitation, 5%) are caused by the
problem that the desired candidate (i.e., reference) is excluded during the candidate
250
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
set expansion stage. Table 12 shows that the final output of the reference {??????
???????::[British Pharmaceutical Firm GlaxoSmithKline]} is {????::[British]}.
This reference has three Chinese initial anchors: ???? (British), ????? (Glaxo) and
????? (SmithKline), and it also has two English initial anchors: British Pharmaceuti-
cal and Firm GlaxoSmithKline. Because only four characters are allowed for boundary
enlarging/shrinking for Chinese anchors (three words for English anchors), the refer-
ence CNE is beyond the scope of any Chinese initial anchor during the expansion stage.
Therefore, it could not be included in the candidate set for final selection. In addition,
the adjacent Chinese component ????? could not be recovered due to the translation
re-ordering of the Chinese components?its counterpart Glaxo is far apart from British
in the given sentence. Similarly, the adjacent English words Pharmaceutical Firm could
not be recovered, as its counterpart ???? is far apart from ???.? Although loosening
the constraint during the expansion stage can increase the reference coverage, it must
be weighed against the corresponding lower precision.
Finally, for those NE pairs with aligned components and included references, the
proposed model still makes a significant number of mistakes. These kinds of errors,
Category (VI) (Others, 48%), account for the largest portion among all errors. There-
fore, they are further hierarchically classified in Table 13 according to their associated
transformation types and origins.
The incorrect NE pairs with aligned components and included references (i.e., [VI]
Other Category in Table 12) are first classified by their corresponding transformation
types: (VI.A) Abnormal Transformation (27%), whose transformation types are not as-
sumed by the model (i.e., neither normally translated nor normally transliterated); and
(VI.B) Normal Transformation (21%), whose components are either normally translated
or normally transliterated. A detailed explanation is given as follows.
Table 13
Distribution of Category (VI) error classes (type-insensitive).
Transformation
Type
Classes Reference NE pair Initially Recognized NEs Final Output Percentage
(VI.A)
Abnormal
Transformation
(27%)
(VI.A.1)
English
Acronym
??????
?::[ICC]
CNE: ????????;
?????
ENE: [ICC]
?????::[ICC] 11% (12)
(VI.A.2)
Chinese
Abbreviation
????
??::[Peace-
keeping
Troop]
CNE: ????
ENE: [Peace-keeping
Troop]
????::[Troop] 8% (9)
(VI.A.3)
Irregular
Translation
????::[Akihito] CNE: ???? ENE:
[Akihito]
????::[Hirohito] 8% (9)
(VI.B)
Normal
Transformation
(21%)
(VI.B.1) Bias
from NE
likelihoods
Translation:
?????
::[South and
North Koreas]
Transliteration:
??????
::[Beatrix]
Translation:
CNE: ???
ENE: [North Koreas]
Transliteration:
CNE: ????; ????
ENE: [Beatrix]
Translation:
????
::[North Koreas]
Transliteration:
?????
::[Beatrix]
18% (20)
(VI.B.2) Bias
from
Bilingual
Probabilities
Translation:
?????
::[World Cup]
Transliteration:
??????
::[Islamabad]
Translation:
CNE: ?????
ENE: [World Cup]
Transliteration:
CNE: ??????
ENE: [Islamabad]
Translation:
???????
::[championship
at World Cup]
Transliteration:
???????
::[Islamabad]
3% (3)
251
Computational Linguistics Volume 39, Number 2
(VI.A) Abnormal Transformation (27%): This category includes transformation
types that are not assumed by our alignment model. It can be further divided into
three classes according to their origins: (VI.A.1) English Acronym (11%), whose ENE is an
acronym; (VI.A.2) Chinese Abbreviation (8%), whose CNE is an abbreviation; and (VI.A.3)
Irregular Translation (8%), whose components are transformed neither semantically nor
phonetically. These cases are interesting and are illustrated herein.
In the row (VI.A.1) (English Acronym, 11%) of Table 13, a Chinese NE ????
???? (International Criminal Court) is tagged as ???????/ORG,? whereas
its English counterpart is the acronym ICC. Linking ???????? to ICC is thus
beyond the ability of our model. On the other hand, Chinese NEs are also occasionally
abbreviated. For example, in Class (VI.A.2) (Chinese Abbreviation, 8%), ???? is the
Chinese abbreviation of ?????? (Peace-keeping), which is also difficult to align
to its English counterpart. Such acronym and abbreviation cases are not rare in NE
translation. We believe that an expansion table (or even anaphora analysis) for acronyms
and abbreviations can help handle such issues.
It is also known that some loanwords or out-of-vocabulary terms are translated
neither semantically nor phonetically. As an example for Class (VI.A.3) (Irregular Trans-
lation, 8%), CNE ???? (which is the name of a Japanese emperor, and consists of
Japanese kanji characters) is incorrectly linked to an English word Hirohito (whose
Chinese translation should be ????), although it should be linked to ENE Akihito.
In this example, the Japanese kanji ???? is directly adopted as the corresponding
Chinese characters (as those characters are originally borrowed from Chinese), which
would be pronounced as ming-ren in Chinese and thus deviates significantly from the
English pronunciation of Akihito. Therefore, it is translated neither semantically nor
phonetically. This phenomenon mainly occurs in loanwords or out-of-vocabulary terms
and the model would have to be extended to cover those new conversion types. Such an
extension is very likely to be language-pair dependent (e.g., with an additional Japanese
phonetic table for cases such as the given example), however.
(VI.B) Normal Transformation (21%): Components of this category are translated
normally. It can be further divided into two classes according to their sources: (VI.B.1)
Bias from NE likelihoods (18%), which prefers the incorrect NE pair scope due to its
associated monolingual likelihood scores, and (VI.B.2) Bias from Bilingual Probabilities
(3%), which introduces extra non-NE words in the output due to high alignment scores
of words that are adjacent to the NE. Further illustration is given as follows.
As Class (VI.B.1) (Bias from NE likelihoods, 18%) shows in Table 13, the Chinese NE
????? and the English NE South and North Koreas are initially recognized as ???
(Korea) and North Koreas, respectively; the model finally chooses a partial alignment
result {????::[North Koreas]}. In this case, every component in either the CNE or
the ENE is well matched to its counterpart. Therefore, there is no significant differ-
ence among the alignment scores of various NE pair candidates with different scopes
(such as {???::[Koreas]}, {????::[North Koreas]}, and {?????::[South and North
Koreas]}, etc.)
The same situation also appears in the transliteration case. For example, the final
output of the reference {??????::[Beatrix]} is {?????::[Beatrix]}, and it has two
Chinese initial anchors: ???? (pronounced as bi-cui in Chinese) and ???? (ke-si).
According to the training data, both ??? (ke) and ???? (ke-si) could be aligned to
the English letter x. There is no significant difference, therefore, in the alignment scores
between {?????::[Beatrix]} and {??????::[Beatrix]}.
Because the NE alignment feature has only negligible discrimination power in these
cases (as described in Section 2.2), monolingual likelihood scores dominate the scope
252
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
preference. Addressing this shortcoming is beyond the capability of the alignment
model, and the adjacent contextual (non-NE) bigrams, to be proposed later, can only
correct 5 of the 20 errors in this class. No easy and effective solution for this kind of
problem can currently be found.
Contrary to this case, Class (VI.B.2) (Bias from Bilingual Probabilities, 3%) accounts
for those cases where incorrect NE pairs are selected due to the bilingual align-
ment score. For example, the final output of the reference {?????::[World Cup]} is
{???????::[championship at World Cup]}. Although both desired CNE and ENE
have already been correctly recognized in the initial stage, the bilingual NE alignment
feature prefers to include the additional Chinese common noun ???? and an extra
English word championship, because they are a perfect mutual translation. At first glance,
it seems that we could use the lower casing of championship as a feature. Other refer-
ences with lower case words could also be found (e.g., {??????????::[WTO
ministerial meeting]}), however. Therefore, additional features such as their relative
positions are also required.
The same situation also occurs in transliteration. For example, the Chinese NE
?????? (yi-si-lan-bao, in Chinese pronunciation) and the English NE Islamabad
are both correctly recognized in the initial stage. The model chooses a longer align-
ment result {???????::[Islamabad]} in the final stage, however. In this case, the
Chinese character ??? (land, pronounced as di in Chinese) could also be phoneti-
cally aligned to syllable ?d? with high probability. Therefore, there is no significant
difference in the alignment scores between ??????::[Islamabad] and ?????
??::[Islamabad]. We may need to resort to using a richer bilingual context (i.e., ??
?????????; In the map of Islamabad . . .) as features to resolve this issue. If its
Chinese adjacent contextual word ???? (map, pronounced as di-tu) could be aligned
to the corresponding English word map, and given that ???? and map are common
nouns in their respective languages, it is possible to determine that this extra Chinese
character ??? should not be linked to the ENE Islamabad.
Addressing these problems requires that both translation and transliteration models
be more complex and must use additional features (possibly knowledge-rich features).
Because this class accounts for only 3% of the errors, we leave the problem for future
work.
5.1.1 Features Contributing to Boundary Errors. Among the 111 alignment errors analyzed,
76 of them18 have their references covered by the expanded candidate set. The scores
of their associated features are then further inspected to determine which features
contribute to the errors. This is assessed by counting the number of times (denoted
by #Worst) that a specific weighted feature-score gets the worst difference when those
incorrect NE pairs are compared to their corresponding references. A large #Worst
would imply that this feature should get more attention in pursuing further perfor-
mance improvement.
The top four related statistics are given in Table 14, which indicates that F1 (Nor-
malized TS/TL Transformation) is the most dominant feature in making those errors.
Following this, F2 (Normalized Translation Mode), F7 (Normalized Chinese Bigram),
and F10 (Normalized English Bigram) are on the second tier. Both F1 and F2 are related
to alignment, which coincides with our observation that alignment-related Categories
18 This is the number of those entries under Categories (III), (IV), and (VI) in Table 12 after subtracting one
(76 = 11 + 13 + 53 ? 1), as one reference in Category (III) cannot be found after expansion.
253
Computational Linguistics Volume 39, Number 2
Table 14
Top four worst-case statistics of features for NE boundary errors.
Features #Worst
F1: [
?N
n=1 P(cpa(n)|Mn, ewn,T)]
1/N (Normalized TS/TL Transformation) 17
F2: [
?N
n=1 P(Mn|ewn, T)]
1/N (Normalized Translation Mode) 10
F7: [
?L
l=1 P(ccl|ccl?1,T)]
1/L (Normalized Chinese Bigram) 10
F10: [
?N
n=1 P(ewn|ewn?1,T)]
1/N(Normalized English Bigram) 9
(i.e., IV, VI.A, and VI.B.2) occupy the largest portion of errors (62% of 76 inspected
errors). The errors dominated by F7 and F10 are further discussed as follows.
Among the ten errors dominated by F7, except for three (in which one is due to
a spurious anchor, and two are due to abnormal transformations), all others selected
the sub-strings of their corresponding CNE references. Furthermore, each selected sub-
string included the Chinese bigrams that appear more frequently than those within the
remaining sub-string (of its reference CNE). In other words, F7 tends to select only the
portion with high frequency bigrams when all related components are aligned. For
example, for the reference {??????????::[National Center for Tobacco-Free
Kids]}, only {????::[Center]} is extracted, as this Chinese bigram is frequently used
in various organization names. Further inspection of those seven cases (which prefer
the sub-string) reveals that three of them are with unmatched components (Category
IV); therefore only four of them are, in fact, due to the problem of F7.
On the other hand, among the nine errors caused mainly by F10, only three of them
chose the sub-strings of their corresponding ENE reference, and the remaining six errors
selected the strings unrelated to the reference due to spurious anchors and an acronym.
It therefore seems that different languages possess different error patterns.
The problem of preferring a more frequent sub-string cannot be solved by normaliz-
ing related bigrams. This is because the current bigram model does not consider the im-
plied restriction on the context surrounding the given CNE. In other words, a given CNE
also implies that its left and right adjacent characters should not be a part of CNE (or its
left and right adjacent characters must be in a non-NE region). In our data set, two adja-
cent non-NE Chinese characters (or words for English) are found to be sufficient for both
left and right contexts. Therefore, the following additional terms are further proposed to
take care of this issue: P(cc?1|cc0,T) ? P(cc0|cc1,T) ? P(ccL+2|ccL+1,T) ? P(ccL+1|ccL,T),
where ccL1 is the given CNE, cc0 and ccL+1 are its left and right adjacent non-NE char-
acters, respectively. This formula can be easily derived from P(C|Cb,Tc,T,Sc), similar
to Equation (8). The derivation also applies to English. To test this supposition, the
related experiment (Exp4 [MERT-W, N+Full Model] specified in Table 3) is updated as
Experiment 5 with the probability features shown here.
Exp5: This experiment (named MERT-W, N-Full Model+Contextual-Bigram, and
denoted by MERT-W-CB) replaces [
?L
l=1 P(ccl|ccl?1,T)]
1
L in the original Exp4 with
P(cc?1|cc0,T) ? P(cc0|cc1,T) ? P(ccL+2|ccL+1,T) ? P(ccL+1|ccL,T) ? [
?L
l=1
P(ccl|ccl?1,T)]
1
L
The same is done for English.
Table 15 shows the performance on the test set (data from Exp4 are also listed for
comparison). The entries in bold indicate statistically significant improvements over
254
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 15
Effect of adjacent contextual (non-NE) bigrams on the test set.
Model P(%) R(%) F(%)
Exp4 (MERT-W) 85.9 (80.5) 88.4 (83.0) 87.1 (81.7)
Exp5 (MERT-W-CB): Add Contextual- 86.7 (81.7) 89.3 (84.1) 88.0 (83.0)
Bigram to Original N-Bigram
their counterparts. Results show that the performance has indeed improved, and six of
the targeted seven cases (four CNE and three ENE errors, as mentioned previously)
have been corrected (the remaining error is due to data sparseness and cannot be
corrected). According to coefficient weighting by MERT process, P(cc0|cc1,T) is more
important than P(cc?1|cc0,T), indicating that the closer a non-NE character is to the
given NE, the more influential it is. This observation confirms our intuition about the
context effect. A similar trend is also observed for other contextual non-NE characters.
5.2 NE Type Errors
In addition to the 111 boundary errors just analyzed, there are also NE type errors.
Among those 635 NE pairs with correct boundaries in the test set, there are 41 (6.5%)
NE type errors in Exp4 (MERT-W). Among them, 175 PER, 248 LOC, and 212 ORG
NE types are assigned. The associated confusion matrix of various NE types is shown
in Table 16 (the numbers within the parentheses are the relative ratios of their output
types). All except 5 of the 41 NE type errors originated from transliterated NE pairs (not
shown in the table). This is consistent with our observation that even a human annotator
finds it challenging to identify correct types for transliterated NE pairs in the absence of
context.
Table 16 shows that PER has the highest error rate, LOC follows as the second,
and ORG is a distant third. In addition, PER and LOC are the types that are most often
confused with one another. These observations match the distribution of transliterations
in each type (the transliteration mode ratios for PER, LOC, and ORG are 100%, 71.4%,
and 25.2%, respectively), as it is very difficult to determine the type when a NE is
transliterated without context.
To solve NE type errors originating from transliteration, the adjacent contextual
non-NE characters are also helpful. For example, {?????::[Myers]} is incorrectly
identified as LOC, when in fact it should have been PER in the context ???????
(president Myers). The left adjacent contextual bigrams ???? (president) should indicate
that the following NE is likely to be PER. Table 15 shows that an additional four type
errors are also corrected apart from the six boundary errors. Therefore, in comparison
with the original MERT-W, this new version (MERT-W-CB) gains more in type-sensitive
Table 16
Distribution of the NE type errors (MERT-W).
NE type Reference PER Type Reference LOC Type Reference ORG Type
Output Type = PER 153 (87.4%) 16 (9.1%) 6 (3.4%)
Output Type = LOC 14 (5.6%) 232 (93.5%) 2 (0.8%)
Output Type = ORG 0 (0%) 3 (1.4%) 209 (98.6%)
255
Computational Linguistics Volume 39, Number 2
F-score (an increase of 1.5%, from 81.5% to 83.0%) than in type-insensitive F-score
(an increase of 0.9%, from 87.1% to 88.0%).
The adjacent contextual non-NE characters have only limited power in disam-
biguating NE types, however. For instance, the reference {?????::[Kabul]} is incor-
rectly identified as PER, which should be LOC in the context ?????? (at Kabul).
Because ????? follows a preposition ??? (at) in the associated context, it indicates
that ????? is a location name. We can also easily find counterexamples, however,
such as ??????? (around Bush), in which ???? (Bush) is PER, not LOC. In
fact, both PER and LOC can be freely exchanged in this situation in either a Chinese
or English context. Therefore, more complicated syntactic or semantic information is
required in some cases involving transliteration.
Another interesting statistic not shown in Table 16 is the distribution of errors versus
initial NE types (i.e., Tc and Te) assigned in the first stage. Among 41 errors, 22 (54%)
have both incorrect Tc and Te, 16 (39%) have correct Tc but wrong Te, and only 3 (7%)
have correct Te but incorrect Tc. This distribution shows that Tc is more reliably assigned
in the first stage than is Te, which confirms the observation in Ji and Grishman (2006)
that English NE type assignment is more challenging.
5.2.1 Features Contributing to Type Errors. Similarly, the study for #Worst on weighted
feature-scores is also performed under MERT-W-CB (i.e., Exp5). There are 37 type errors
with correct boundaries, and we have examined that F4 (NE Type Re-assignment) is
the most dominant feature in making these errors. Because this feature LogP(T|Tc,Te)
(Equation [7]) always assigns an incorrect type when both Tc and Te are incorrect (11 out
of 13 F4 errors belong to this class), it is not surprising that it is ranked at the top
(22 [59%] out of a total of 37 errors have both incorrect Tc and Te). In addition, the feature
LogP(?A|T) in Equation (6) always prefers PER (which are always completely translit-
erated in the corpus) when all components in an NE are transliterated (i.e., ?A = 0).
Therefore, the feature is ranked second (31 out of 37 errors are complete transliterations,
although only 10 of them should have been assigned PER). It is found that all nine cases
in this category are not PER (only two of them are not transliterated), which further
supports our analysis. Solving these type errors requires that these two features be
conditioned on more features, and requires further study. Finally, the top four features in
making type errors are related to alignment (Equations [6] and [7]), which indicates that
monolingual lexicon information (both English and Chinese bigrams) is more reliable
in deciding NE type.
6. Applications of the Proposed Model
It would be interesting to know how the proposed model performs in real applications.
Because MERT-W performs best in our tests, it is adopted in this study on real applica-
tions. Section 6.1 presents the effectiveness of improving NE recognition, and Section 6.2
shows how the improved NE recognition can be used in learning a monolingual NE
recognition model (also NE translation table/models) in a semi-supervised manner.
6.1 On Improving Monolingual NE Recognition
As explained in Section 2.1, the alignment result can also be used to refine the initially
recognized NEs. The improvements that MERT-W made in refining the boundaries and
NE types of those initially recognized Chinese/English NEs are shown in Tables 17 and
18, respectively. For comparison, the rows associated with the initial recognizers and the
256
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 17
Type-insensitive improvement for Chinese/English NER.
NE type Model P (%) R (%) F (%)
PER Initial 85.9/91.3 89.3/91.1 87.6/91.2
Baseline 88.1 (+2.2)/ 92.9 (+1.6) 89.8 (+0.5)/ 92.2 (+1.1) 89.0 (+1.4)/ 92.6 (+1.4)
MERT-W 90.3 (+4.4)/ 94.5 (+3.2) 90.5 (+1.2)/ 93.0 (+1.9) 90.4 (+2.8)/ 93.8 (+2.6)
LOC Initial 90.9/93.6 91.4/93.4 91.1/93.5
Baseline 91.8 (+0.9)/ 94.7 (+1.1) 91.8 (+0.4)/ 94.2 (+0.8) 91.8 (+0.7)/ 94.5 (+1.0)
MERT-W 94.2 (+3.3)/ 95.8 (+2.2) 93.2 (+1.8)/ 95.2 (+1.8) 93.6 (+2.5)/ 95.5 (+2.0)
ORG Initial 81.1/88.9 83.9/87.7 82.5/88.3
Baseline 84.4 (+3.3)/ 90.7 (+1.8) 86.6 (+2.7)/ 89.2 (+1.5) 85.5 (+3.0)/ 90.0 (+1.7)
MERT-W 86.8 (+5.7)/ 92.8 (+3.9) 88.7 (+4.8)/ 89.9 (+2.2) 87.8 (+5.3)/ 91.4 (+3.1)
ALL Initial 86.0/92.2 88.7/92.2 87.3/92.5
Baseline 88.8 (+2.8)/ 93.6 (+1.4) 89.6 (+0.9)/ 93.9 (+1.1) 89.2 (+1.9)/ 93.7 (+1.2)
MERT-W 91.4 (+5.4)/ 95.2 (+3.0) 91.1 (+2.4)/ 94.7 (+1.9) 91.2 (+3.9)/ 94.9 (+2.4)
alignment baseline systems are also given in the two tables (as before, the entries in bold
indicate that differences are statistically significant). In addition, figures in parentheses
indicate the corresponding differences in performance compared to the initial version
(shown in Table 2).
Table 17 shows that both the baseline and MERT-W systems have significantly im-
proved the initial NE recognition type-insensitive results for both Chinese and English.
It also shows that MERT-W significantly outperforms the baseline. In particular, Chinese
ORG is observed to yield the largest improvement among NE types in both Chinese and
English, which matches our previous observations that the boundary of a Chinese ORG
is difficult to identify using only the information from the Chinese sentence.
The type-sensitive results are given in Table 18, which shows that MERT-W also
significantly improves the initial NE recognition results for both Chinese and English.
Table 18
Type-sensitive improvement for Chinese/English NER.
NE type Model P (%) R (%) F (%)
PER Initial 80.2/79.2 87.7/85.3 83.8/82.1
Baseline 79.4 (?0.8)/ 78.6 (?0.6) 88.1 (+0.4)/ 86.6 (+1.3) 83.5 (?0.3)/ 82.4 (+0.3)
MERT-W 85.6 (+5.4)/ 85.6 (+6.4) 89.9 (+2.2)/ 87.9 (+2.6) 87.7 (+3.9)/ 86.7 (+4.6)
LOC Initial 89.8/85.9 87.3/81.5 88.5/83.6
Baseline 90.5 (+0.7)/ 86.5 (+0.6) 83.6 (?3.7)/ 80.4 (?1.1) 86.9 (?1.6)/ 83.3 (?0.3)
MERT-W 93.8 (+4.0)/ 89.3 (+3.4) 87.1 (?0.2)/ 84.1(+2.7) 90.3 (+1.8)/ 86.6 (+3.0)
ORG Initial 78.6/82.9 82.8/79.6 80.6/81.2
Baseline 79.5 (?0.9)/ 82.2 (?0.7) 80.9 (?1.9)/ 80.1 (+0.5) 80.2 (?0.4)/ 81.1 (?0.1)
MERT-W 85.6 (+7.0)/ 86.8 (+3.9) 88.4 (+5.6)/ 88.7 (+9.1) 87.0 (+6.4)/ 87.7 (+6.5)
ALL Initial 83.4/82.1 86.0/82.6 84.7/82.3
Baseline 83.2 (?0.2)/ 82.2 (+0.1) 83.9 (?2.1)/ 82.3 (?0.3) 83.5 (?1.2)/ 82.2 (?0.1)
MERT-W 88.7 (+5.3)/ 87.3 (+5.2) 88.4 (+2.4)/ 86.6 (+4.0) 88.6 (+3.9)/ 86.9 (+4.6)
257
Computational Linguistics Volume 39, Number 2
Note that English ORG yields the largest gain among NE types in both Chinese and
English, again supporting our earlier observation that an English ORG cannot be easily
identified when only the English sentence is available. It must be noted that the baseline
alignment model deteriorates the original NE recognition in overall performance (even
though it can correct some NE initial boundary errors as shown in Table 17), because it
does not use the features/constraints proposed in the joint model.
6.2 On Learning NE Recognition Models via Semi-Supervised Learning
In many NLP applications, the associated process will be considerably simplified if
the included NEs can be identified first. Therefore, it is important to have a good NE
recognizer that has been well trained. Various domains frequently have different sets
of NEs, however, and new NEs also emerge over time. We thus need to periodically
update the NE recognition model (also the NE translation table/model, if it is for MT),
which necessitates the need to ensure short training times (including set-up time and
human effort). This requirement can be addressed well in a semi-supervised learning
set-up where parameters/tables are learned from a large unlabeled corpus with a small
(albeit human) annotated seed set.
Under the semi-supervised learning framework, however, maximizing likelihood
does not imply a minimizing of error rate at the same time. Without additional con-
straints, a monolingual NE model is usually unable to converge to the desired point in
the parameter space. On the other hand, as shown in the previous section, the align-
ment module can further refine the initially recognized NEs with additional mapping
constraints from the other language. The proposed joint model thus can be used to train
the monolingual NE recognition model via semi-supervised learning on a large un-
labeled bilingual corpus. In other words, when semi-supervised learning is conducted
for learning the NE recognition model, MERT-W is expected to guide the search pro-
cess for convergence towards the human annotation. This advantage is important for
regularly updating the NE recognition and translation table/models.
We now outline our semi-supervised learning procedure as follows.
(1) A small pre-labeled corpus acts as seed data. Based on these seed data,
train the Chinese/English NE recognition toolkit (described in Section 4.1)
and the adopted NE alignment model.
(2) Perform the Chinese/English NE recognition toolkits and the NE
alignment model, trained in Step (1), on a large unlabeled
(sentence-aligned) bilingual corpus to report those NE pairs that they
identify.
(3) Denote the located NE pairs as correctly labeled and combine them with
the seed data as new labeled training data.
(4) Re-train the Chinese/English NE recognition toolkit and also re-train the
NE alignment model on the newly labeled training data.
(5) Repeat Steps (2) through (4) until convergence. The final Chinese/English
NE recognizer and the NE alignment model are compared to their initial
versions.
Because the adopted Chinese NE recognizer (Wu, Zhao, and Xu 2005) cannot be
re-trained (because it is not an open source toolkit), only the English NE recognizer and
258
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Table 19
English NE recognition on test data after semi-supervised learning.
Model Seed Size (sentence pairs) 100 400 4,000 40,000
Initial-NER 36.7 58.6 71.4 79.1
(0%) (0%) (0%) (0%)
NER-Only ?2.3 ?0.5 ?0.3 ?0.1
(?6.3%) (?0.8%) (?0.4%) (?0.1%)
NER+Alignment-Baseline +4.9 +3.4 +1.7 +0.7
(+13.4%) (+5.8%) (+2.4%) (+0.9%)
NER+Alignment-Weighted +10.7 +8.7 +4.8 +2.3
(+29.2%) (+14.8%) (+6.7%) (+2.9%)
the alignment model are updated during training iterations. In our experiments, 50,412
sentence pairs are first extracted from Training Set I as unlabeled data. Various labeled
data sets are then extracted from the remaining data as seed corpora with different sizes
(100, 400, 4,000, and 40,000 sentence pairs). The test set is still the same 300 sentence
pairs that were adopted previously.
Table 19 shows the type-sensitive F-score of English NER on the test set at conver-
gence.19 The Initial-NER in Table 19 indicates the initial performance of the NER model
re-trained from different seed corpora. Three different approaches are tested: (1) English
NER only (NER-Only), (2) the alignment baseline model (NER+Alignment-Baseline),
and (3) our weighted joint model MERT-W (NER+Alignment-Weighted). The first case
performs semi-supervised learning only on English data without involving alignment,
whereas the last two cases include alignment, and both the English NER model and
the alignment model are re-trained during iterations. The numbers in parentheses
show relative improvements over Initial-NER. The entries in bold indicate statistically
significant improvements over Initial-NER.
As Table 19 shows, using the NER model alone, the performance may drop after
convergence. This is because maximizing likelihood does not imply minimizing the
error rate. With additional mapping constraints from the other language, however, the
alignment module can guide the search process to converge to a more desirable point in
the parameter space. It must be noted here that the contribution of additional constraints
increases with smaller seed corpora, because constraints become more important when
the labeled data set is smaller.
Table 20 shows only the type-sensitive F-score of NE pair alignment on the test set
before and after convergence due to space constraints. Two different models are tested:
(1) the alignment baseline model (NER+Alignment-Baseline), and (2) our weighted joint
model (NER+Alignment-Joint). The data in parentheses indicate relative improvements
over the performance before training. The entries in bold indicate statistically significant
improvements over the model before training.
As Table 20 demonstrates, the alignment performance can also be improved with
semi-supervised learning. Note that the improvement is greater on smaller data sets,
which is common in most semi-supervised learning tasks.
19 The iteration process will stop when the last two consecutive iterations share more than 99.9% of their
output.
259
Computational Linguistics Volume 39, Number 2
Table 20
NE alignment on test data after semi-supervised learning.
Model Seed Size (sentence pairs) 100 400 4,000 40,000
NER+Alignment-Baseline 33.5 50.2 62.6 67.3
(0%) (0%) (0%) (0%)
NER+Alignment-Baseline (After) +4.2 +3.0 +1.8 +1.2
(+12.5%) (+6.0%) (+2.9%) (+1.8%)
NER+Alignment-Joint 38.1 56.3 67.7 72.5
(0%) (0%) (0%) (0%)
NER+Alignment-Joint (After) +10.1 +8.7 +6.0 +3.3
(+26.5%) (+15.5%) (+8.9%) (+4.6%)
As shown by Tables 19 and 20, the proposed joint model gains more from the
learning process in comparison with the alignment-baseline model, because informa-
tion from the aligned sentence is utilized more effectively. Results demonstrate that
the proposed joint model, combined with semi-supervised learning, offers significant
improvement for semi-automatically updating the NE recognition model and the NE
translation table. Additionally, the impact is greater when less time is available for
labeling seed data.
7. Related Work
There is significant work on identifying NEs within monolingual texts across languages,
such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999)
and Chinese (Chen et al 1998a; Sun, Zhou, and Gao 2003), to name a few. Various
approaches to identifying NEs have also been proposed, such as hidden Markov
models (Bikel et al 1997; Bikel, Schwartz, and Weischedel 1999), conditional random
fields (McCallum and Li 2003; Jiao et al 2006), modified transformation-based
learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al 2002),
AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised
learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features
including local information (e.g., token, part-of-speech) and global information (e.g.,
label consistency, context features) from monolingual resources have been adopted
(Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual
NE alignment for NE recognition, Huang and Vogel (2004) used an iterative process
to extract a smaller but cleaner NE translation dictionary and then used the dictionary
to improve the monolingual NE annotation quality. Ji and Grishman (2007) adopted
several heuristic rules for using bilingual-text information to correct NE recognition
errors.
In aligning bilingual NEs from two given NE lists, the NE translation model is
usually adopted. Typically, an NE is either transliterated or semantically translated. For
transliteration, Knight and Graehl (1998) were pioneers in adopting the probabilistic
model to align the components within an NE pair. Since then, similar approaches have
been applied to various language pairs such as English/Arabic (Stalls and Knight
1998), English/Chinese (Chen et al 1998b; Wan and Verspoor 1998; Lin and Chen
2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004;
260
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002),
and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li,
Zhang, and Su (2004), and Li et al (2007) presented a joint source channel model for
transliteration, and automated the semantic transliteration process, which takes origin
and gender into account for personal names.
In contrast, research on automatic NE semantic translation is less common. Zhang
et al (2005) proposed a phrase-based context-dependent joint probability model for
semantic translation, which is similar to phrase-level translation models in statistical
MT (Zong and Seligman 2005; Hu, Zong, and Xu 2006). Chen, Yang, and Lin (2003) and
Chen et al (2006) studied formulation and transformation rules for English?Chinese
NEs. They adopted a frequency-based approach for extracting key words of NEs with or
without dictionary assistance and constructed transformation rules from the bilingual
NE corpus. Their studies focused on transformation rules with particular attention
to distinguishing translated parts from transliterated parts; the performance of rule-
application in NE translation was not described, however. Chen and Zong (2008) pro-
posed a chunk-based probabilistic translation model for organization names, although,
its application to person and location names has not been studied.
Because new NEs emerge from time to time and can be transformed in various ways
(translation, transliteration, or other abnormal types), the NE transliteration/translation
models mentioned usually lead to unsatisfactory results, especially for infrequently
occurring NEs. Recent studies have therefore focused on extracting new NE pairs from
either bilingual corpora or Web resources, so that the corresponding human translation
can be directly adopted (or used for training). To do this, however, NE alignment is an
essential tool. Due to the relatively poor quality of Web data, such alignment approaches
are usually limited unless significant effort is devoted to data cleaning; therefore, we do
not discuss these approaches.
To extract the NE pairs from bilingual corpora, all NE alignment approaches we
found in the literature are conducted after an initial NE identification stage so that
the complexity of the task can be reduced. The associated cost is that those initial NE
recognition errors propagate into the following alignment stage. Both symmetric (which
first identifies NEs in both languages) and asymmetric (which first identifies NEs in only
one language) strategies have been proposed to mitigate this problem (Moore 2003), and
are described here.
For the symmetric strategy, Huang and Vogel (2004) proposed to extract the NE
translation dictionary from the bilingual corpus, and then used it to improve the
NE annotation performance iteratively. Huang, Vogel, and Waibel (2003) described
a multi-feature NE alignment model to extract NE equivalences (with translation,
transliteration, and tagging features), from which a NE translation dictionary was then
constructed. Kumano et al (2004) proposed a method to extract English?Chinese NE
pairs from a content-aligned corpus. This approach tries to find the correspondences
between bilingual NE groups based on the similarity in their order of appearance in
each document. Additionally, an abridged version of our work has been presented in
our ACL-10 paper (Chen, Zong, and Su 2010). Among those symmetric approaches,
only Huang, Vogel, and Waibel and Chen, Zong, and Su adopt the expansion strategy,
described below.
For the asymmetric strategy, Al-Onaizan and Knight (2002) proposed an algorithm
to translate NEs from Arabic to English using monolingual and bilingual resources.
Given an Arabic NE, they used transliteration models (including a phonetic-based and
a spelling-based model), a bilingual dictionary, and an English news corpus to first
generate a list of English candidates, which were then re-scored by a Web resource.
261
Computational Linguistics Volume 39, Number 2
Moore (2003) developed an approach to learning phrase translations from a parallel
corpus based on a sequence of cost models. A maximum entropy model for NE align-
ment was presented in Feng, Lv, and Zhou (2004). Lee, Chang, and Jang (2006) proposed
to align bilingual NEs in a bilingual corpus by incorporating a statistical model with
multiple sources. Turning to comparable corpora, Shao and Ng (2004) presented a
hybrid method to mine new translations from Chinese?English comparable corpora,
combining both transliteration and context information. Sproat, Tao, and Zhai (2006)
investigated the Chinese?English NE transliteration equivalence within comparable
corpora.
Although these asymmetry strategies can prevent NE recognition errors on the
target side from affecting alignment, errors on the source side continue to propagate
to later stages. To reduce error propagation from both the source and the target, Huang,
Vogel, and Waibel (2003) proposed to first identify the NEs in both the source and target,
and then enlarge the obtained NE candidate sets for both languages before conducting
alignment. Based on the observation that NE boundaries are frequently identified in-
correctly, the enlarging procedure is done by treating the original recognition results as
anchors and then increasing the number of candidates by expanding or shrinking the
boundaries of those originally recognized NEs in both languages.
Our approach also adopts the expansion strategy. It differs from the works of Huang
et al (2003) and others in several ways, however. First, in all the alignment papers
mentioned here, the adopted probabilities are directly used as features for log-linear
combination or ME training without derivation. In contrast, our work fully derives a
probabilistic joint model, for both identification and alignment, in a principled way.
Second, unlike previous approaches that discard the information of initially identified
NE anchors after the anchors have been expanded, our approach uses this information
in the final selection process. Third, we propose new features, such as translation mode
and its ratio, boundary shifting distance, and contextual bigrams. Fourth, we introduce
a normalization step that removes the systematic bias preferring shorter NEs. Fifth, the
effect of each individual feature, the influence of adopting different NE recognizers, the
effectiveness across different domains, the effect of using a derived model (compared
to ME), and the effect of the alignment model in semi-supervised learning are studied.
Finally, the causes of alignment errors and type re-assignment errors are extensively
investigated and categorized.
8. Conclusion
This article develops a novel and principled model for jointly conducting NE recogni-
tion and alignment. To the best of our knowledge, this is the first work that formally
captures the interactions between NE recognition and NE alignment. The joint model
not only greatly improves NE alignment performance, but also significantly boosts NE
recognition performance.
Our experiments show that the new NE likelihoods are more effective than the
bigram model used in the baseline system. Moreover, both the translation mode ratio
and the entity type consistency constraint are critical in identifying the associated
NE boundaries and types, as evidenced by the 21.3% relative improvement on type-
sensitive F-score (from 68.4% to 83.0%) in our Chinese?English NE alignment task. The
superiority of the proposed model has been shown to hold over the various domains
tested.
Furthermore, the joint alignment model can also be used to refine the initially
recognized NEs. This is achieved by utilizing additional mapping information from the
262
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
other language. In our experiments, when semi-supervised learning is conducted to
train the adopted English NE model (with only 100 seed sentence pairs), the proposed
model greatly boosts the English NE recognition type-sensitive F-score from 36.7% to
47.4% (29.2% relative improvement) in the test set.
Finally, the proposed model does not utilize language-dependent features. For
example, Chinese characters and English words adopted in the model are visible units
in the given languages, and no language-dependent features, such as morpheme/part-
of-speech (or prefix/suffix), are used. In addition, the model does not use linguistic
rules or tree banks. Therefore, although our experiments are conducted on Chinese?
English language pairs, it is expected that the proposed approach can be applied to
other language pairs with little adaptation effort.
Acknowledgments
This research has been funded by the Natural
Science Foundation of China under grant
nos. 61003160 and 60975053 and supported
by the Hi-Tech Research and Development
Program (?863? Program) of China under
grant no. 2011AA01A207. Thanks are also
given to the authors? associate, Tao Zhuang,
for his great help on the publication version.
References
Al-Onaizan, Yaser and Kevin Knight.
2002. Translating named entities using
monolingual and bilingual resources. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 400?408, Philadelphia, PA.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Bikel, Daniel M., Scott Miller, Richard
Schwartz, and Ralph Weischedel.
1997. Nymble: A high-performance
learning name-finder. In Proceedings
of the Fifth Conference on Applied Natural
Language Processing, pages 194?201,
Washington, DC.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name. Machine
Learning, 34(1?3):211?231.
Black, W. J. and Argyrios Vasilakopoulos.
2002. Language independent named
entity classification by modified
transformation-based learning and by
decision tree induction. In Proceedings
of the Sixth Conference on Natural Language
Learning, pages 159?162, Taipei.
Borthwick, A. 1999. A Maximum Entropy
Approach to Named Entity Recognition. Ph.D.
thesis, New York University.
Brown, Perer F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Carreras, X., L. Marquez, and L. Padro. 2002.
Named entity extraction using adaboost.
In Proceedings of the Sixth Conference on
Natural Language Learning (CoNLL-2002),
pages 167?170, Taipei.
Chen, Hsin-His, Yung-Wei Ding, Shih-Chung
Tsai, and Guo-Wei Bian. 1998a. Description
of the NTU system used for met2. In
Proceedings of the 7th Message Understanding
Conference (MUC-7), pages 121?129,
Fairfax, VA.
Chen, Hsin-His, S.-J. Huang, Y.-W. Ding,
and S.-C. Tsai. 1998b. Proper name
translation in cross-language information
retrieval. In Proceedings of the 17th COLING
and 36th ACL Conference, pages 232?236,
Montreal.
Chen, Hsin-His, W.-C. Lin, C. Yang,
and W.-H. Lin. 2006. Translating/
transliterating named entities for
multilingual information access. Journal
of the American Society for Information
Science and Technology (Special Issue on
Multilingual Information Systems),
57(5):645?659.
Chen, Hsin-His, Changhua Yang, and
Ying Lin. 2003. Learning formulation and
transformation rules for multilingual
named entities. In Proceedings of the
ACL 2003 Workshop on Multilingual and
Mixed-language Named Entity Recognition,
pages 1?8, Sapporo.
Chen, Stanley F. and Joshua Goodman.
1998. An empirical study of smoothing
techniques for language modeling.
Technical Report TR-10-98, Computer
Science Group, Harvard University,
Cambridge, MA.
263
Computational Linguistics Volume 39, Number 2
Chen, Yufeng and Chengqing Zong. 2008.
A structure-based model for Chinese
organization name translation. ACM
Transactions on Asian Language Information
Processing (TALIP), 7(1):1?30.
Chen, Yufeng, Chengqing Zong, and Keh-Yih
Su. 2010. On jointly recognizing and
aligning bilingual named entities. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 631?639, Uppsala.
Chinchor, Nancy. 1998. Overview of
muc-7/met-2. In Proceedings of Message
Understanding Conference MUC-7,
pages 1?4, Fairfax, VA.
Cohen, William W. 2004. MinorThird: methods
for identifying names and ontological relations
in text using heuristics for inducing
regularities from data. Available at
http://minorthird.sourceforge.net.
Collins, Michael. 2002. Ranking algorithms
for named-entity extraction: Boosting and
the voted perceptron. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 489?496,
Philadelphia, PA.
Feng, Donghui, Yajuan Lv, and Ming Zhou.
2004. A new approach for English-Chinese
named entity alignment. In Proceedings of
the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2004),
pages 372?379, Barcelona.
Finkel, Jenny Rose, Trond Grenager, and
Christopher Manning. 2005. Incorporating
non-local information into information
extraction systems by Gibbs sampling.
In Proceedings of the 43nd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 363?370, Ann Arbor, MI.
Freedman, D. A. 2005. Statistical Models:
Theory and Practice. Cambridge
University Press.
Gao, Jianfeng, Mu Li, Andi Wu, and
Chang-Ning Huang. 2005. Chinese
word segmentation and named entity
recognition: A pragmatic approach.
Computational Linguistics, 31(4):531?574.
Gao, Wei, Kam-Fam Wong, and Wai Lam.
2004. Transliteration of foreign names
for OOV problem. In Proceedings of the
1st International Joint Conference on
Natural Language Processing (IJCNLP),
pages 110?119, Sanya.
Hu, Rile, Chengqing Zong, and Bo Xu. 2006.
An approach to automatic acquisition of
translation templates based on phrase
structure extraction and alignment. IEEE
Transactions on Audio, Speech, and Language
Processing, 14(5):1656?1663.
Huang, Fei and Stephan Vogel. 2004.
Improved named entity translation and
bilingual named entity extraction. In
Proceedings of the 4th IEEE International
Conference on Multimodal Interface,
pages 253?258, Pittsburgh, PA.
Huang, Fei, Stephan Vogel, and Alex Waibel.
2003. Automatic extraction of named
entity translingual equivalence based
on multi-feature cost minimization.
In Proceedings of ACL?03, Workshop on
Multilingual and Mixed-language Named
Entity Recognition, pages 9?16, Sapporo.
Ji, Heng and Ralph Grishman. 2006.
Analysis and repair of name tagger
errors. In Proceedings of COLING/ACL
2006, pages 420?427, Sydney.
Ji, Heng and Ralph Grishman. 2007.
Collaborative entity extraction and
translation. In Proceedings of International
Conference on Recent Advances in Natural
Language Processing, pages 73?84, Borovets.
Jiao, Feng, Shaojun Wang, Chi H. Lee,
Russell Greiner, and Dale Schuurmans.
2006. Semi-supervised conditional
random fields for improved sequence
segmentation and labeling. In Proceedings
of the 21st International Conference on
Computational Linguistics, pages 209?216,
Sydney.
Kneser, Reinhard and Hermann Ney.
1995. Improved backing-off for m-gram
language modeling. In Proceedings
of the IEEE International Conference on
Acoustics, Speech, and Signal Processing,
pages 181?184, Detroit, MI.
Knight, Kevin and Jonathan Graehl. 1998.
Machine transliteration. Computational
Linguistics, 24(4):599?612.
Krishman, Vijay and Christopher D.
Manning. 2006. An effective two-stage
model for exploiting non-local
dependencies in named entity recognition.
In Proceedings of 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1,121?1,128, Sydney.
Kumano, T., H. Kashioka, H. Tanaka,
and T. Fukusima. 2004. Acquiring
bilingual named entity translations from
content-aligned corpora. In Proceedings
of the First International Joint Conference
on Natural Language Processing,
pages 177?186, Hainan Island.
Lee, Chun-Jen and Jason S. Chang.
2003. Acquisition of English-Chinese
transliterated word pairs from
parallel aligned texts using a statistical
machine transliteration model.
In Proceedings of HLT-NAACL 2003
264
Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities
Workshop on Building and Using Parallel
Texts: Data Driven Machine Translation
and Beyond, pages 96?103, Edmonton.
Lee, Chun-Jen, Jason S. Chang, and
Jyh-Shing R. Jang. 2003. A statistical
approach to Chinese-to-English
back transliteration. In Proceedings
of the 17th Pacific Asia Conference on
Language, Information, and Computation,
pages 310?318, Singapore.
Lee, Chun-Jen, Jason S. Chang, and
Jyh-Shing R. Jang. 2006. Alignment of
bilingual named entities in parallel
corpora using statistical models and
multiple knowledge sources. ACM
Transactions on Asian Language Information
Processing (TALIP), 5(2):121?145.
Lee, Jae Sung and Key-Sun Choi. 1997.
A statistical method to generate
various foreign word transliterations in
multilingual information retrieval system.
In Proceedings of the 2nd International
Workshop on Information Retrieval with
Asian Languages (IRAL), pages 123?128,
Tsukuba.
Li, Haizhou, Khe Chai Sim, Jin Shea Kuo,
and Minghui Dong. 2007. Semantic
transliteration of personal names. In
Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 120?127, Prague.
Li, Haizhou, Min Zhang, and Jian Su. 2004.
A joint source channel model for machine
transliteration. In Proceedings of 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 159?166, Barcelona.
Liao, Wenhui and Sriharsha
Veeramachaneni. 2009. A simple
semi-supervised algorithm for named
entity recognition. In Proceedings of the
NAACL HLT Workshop on Semi-Supervised
Learning for Natural Language Processing,
pages 58?65, Boulder, CO.
Liese, Friedrich and Klaus-J. Miescke. 2008.
Statistical Decision Theory: Estimation,
Testing, and Selection. Springer, Berlin.
Lin, Wei-Hao and Hsin-Hsi Chen. 2002.
Backward transliteration by learning
phonetic similarity. In Proceedings of the
Sixth Conference on Natural Language
Learning, pages 139?145, Taipei.
McCallum, Andrew and Wei Li. 2003.
Early results for named entity recognition
with conditional random fields, feature
induction and Web-enhanced lexicons.
In Proceedings of the Conference on
Computational Natural Language Learning
(CoNLL 2003), pages 188?191, Edmonton.
McCallum, Andrew Kachites. 2002. MALLET:
A Machine Learning for Language Toolkit.
Available at http://mallet.cs.
umass.edu.
Mikheev, A., C. Grover, and M. Moens. 1998.
Description of the LTG system used for
MUC-7. In Proceedings of the 7th Message
Understanding Conference (MUC-7),
pages 1?12, Fairfax, VA.
Moore, R. C. 2003. Learning translations of
named-entity phrases from parallel
corpora. In Proceedings of the 10th
Conference of the European Chapter of ACL,
pages 259?266, Budapest.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Conference
of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Oh, Jong-Hoon and Key-Sun Choi. 2002.
An English-Korean transliteration model
using pronunciation and contextual rules.
In Proceedings of the 19th International
Conference on Computational Linguistics,
pages 758?764, Taipei.
Oh, Jong-Hoon and Key-Sun Choi. 2005.
An ensemble of grapheme and phoneme
for machine transliteration. In Proceedings
of the Second International Joint Conference
on Natural Language Processing,
pages 450?461, Jeju Island.
Pervouchine, Vladimir, Haizhou Li, and
Bo Lin. 2009. Transliteration alignment.
In Proceedings of the 47th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 136?144, Singapore.
Schlu?ter, Ralf and Hermann Ney. 2001.
Model-based MCE bound to the true Bayes
error. IEEE Signal Processing Letters,
8(5):131?133.
Shao, Li and Hwee Tou Ng. 2004. Mining
new word translations from comparable
corpora. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 618?624,
Geneva.
Sproat, Richard, Tao Tao, and ChengXiang
Zhai. 2006. Named entity transliteration
with comparable corpora. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 73?80, Sydney.
Stalls, B. G. and Kevin Knight. 1998.
Translating names and technical
265
Computational Linguistics Volume 39, Number 2
terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational
Approaches to Semitic Languages,
pages 34?41, Montreal.
Stolcke, Andreas. 2002. SRILM?An
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, pages 901?904,
Denver, CO.
Sun, Jian, Ming Zhou, and Jianfeng Gao.
2003. A class-based language model
approach to Chinese named entity
identification. Computational Linguistics
and Chinese Language Processing, 8(2):1?28.
Tsuji, Keita. 2002. Automatic extraction of
translational Japanese-Katakana and
English word pairs from bilingual corpora.
International Journal of Computer Processing
of Oriental Languages, 15(3):261?279.
Wan, S. and C. M. Verspoor. 1998. Automatic
English-Chinese name transliteration for
development of multilingual resources.
In Proceedings of the 17th COLING and
36th ACL, pages 1,352?1,356, Montreal.
Wong, Yingchuan and Hwee Tou Ng.
2007. One class per named entity:
Exploiting unlabeled text for named
entity recognition. In Proceedings of
the International Joint Conferences on
Artificial Intelligence, pages 1,763?1,768,
Hyderabad.
Wu, D., G. Ngai, M. Carpuat, J. Larsen, and
Y. Yang. 2002. Boosting for named
entity recognition. In Proceedings of the
Sixth Conference on Natural Language
Learning, pages 195?198, Taipei.
Wu, Youzheng, Jun Zhao, and Bo Xu. 2005.
Chinese named entity recognition model
based on multiple features. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2005),
pages 427?434, Vancouver.
Zhang, Min, Haizhou Li, Jian Su, and
Hendra Setiawan. 2005. A phrase-based
context-dependent joint probability model
for named entity translation. In Proceedings
of the Second International Joint Conference
on Natural Language Processing,
pages 600?611, Jeju Island.
Zhang, Ying, S. Vogel, and A. Waibel. 2004.
Interpreting BLEU/NIST scores: How
much improvement do we need to have
a better system? In Proceedings of the
4th International Conference on Language
Resources and Evaluation, pages 2,051?2,054,
Lisbon.
Zhao, Hai and Chunyu Kit. 2008.
Unsupervised segmentation helps
supervised learning of character tagging
for word segmentation and named entity
recognition. In the Sixth SIGHAN
Workshop on Chinese Language Processing
(SIGHAN-6), pages 106?111, Hyderabad.
Zhou, GuoDong and Jian Su. 2006. Machine
learning-based named entity recognition
via effective integration of various
evidences. Natural Language Engineering,
11(2):189?206.
Zong, Chengqing and Mark Seligman.
2005. Toward practical spoken language
translation. Machine Translation,
19(2):113?137.
266
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 631?639,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
On Jointly Recognizing and Aligning Bilingual Named Entities 
 
Yufeng Chen, Chengqing Zong 
Institute of Automation, Chinese Academy of Sciences 
Beijing, China 
{chenyf,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation 
Hsinchu, Taiwan, R.O.C.  
bdc.kysu@gmail.com 
 
  
Abstract 
We observe that (1) how a given named en-
tity (NE) is translated (i.e., either semanti-
cally or phonetically) depends greatly on its 
associated entity type, and (2) entities within 
an aligned pair should share the same type. 
Also, (3) those initially detected NEs are an-
chors, whose information should be used to 
give certainty scores when selecting candi-
dates. From this basis, an integrated model is 
thus proposed in this paper to jointly identify 
and align bilingual named entities between 
Chinese and English. It adopts a new map-
ping type ratio feature (which is the propor-
tion of NE internal tokens that are semanti-
cally translated), enforces an entity type con-
sistency constraint, and utilizes additional 
monolingual candidate certainty factors 
(based on those NE anchors). The experi-
ments show that this novel approach has sub-
stantially raised the type-sensitive F-score of 
identified NE-pairs from 68.4% to 81.7% 
(42.1% F-score imperfection reduction) in 
our Chinese-English NE alignment task.  
1 Introduction 
In trans-lingual language processing tasks, such 
as machine translation and cross-lingual informa-
tion retrieval, named entity (NE) translation is 
essential. Bilingual NE alignment, which links 
source NEs and target NEs, is the first step to 
train the NE translation model.  
Since NE alignment can only be conducted af-
ter its associated NEs have first been identified, 
the including-rate of the first recognition stage 
significantly limits the final alignment perform-
ance. To alleviate the above error accumulation 
problem, two strategies have been proposed in 
the literature. The first strategy (Al-Onaizan and 
Knight, 2002; Moore, 2003; Feng et al, 2004; 
Lee et al, 2006) identifies NEs only on the 
source side and then finds their corresponding 
NEs on the target side. In this way, it avoids the 
NE recognition errors which would otherwise be 
brought into the alignment stage from the target 
side; however, the NE errors from the source 
side still remain.  
To further reduce the errors from the source 
side, the second strategy (Huang et al, 2003) 
expands the NE candidate-sets in both languages 
before conducting the alignment, which is done 
by treating the original results as anchors, and 
then re-generating further candidates by enlarg-
ing or shrinking those anchors' boundaries. Of 
course, this strategy will be in vain if the NE an-
chor is missed in the initial detection stage. In 
our data-set, this strategy significantly raises the 
NE-pair type-insensitive including-rate 1  from 
83.9% to 96.1%, and is thus adopted in this paper. 
Although the above expansion strategy has 
substantially alleviated the error accumulation 
problem, the final alignment accuracy is still not 
good (type-sensitive F-score only 68.4%, as indi-
cated in Table 2 in Section 4.2). After having 
examined the data, we found that: (1) How a 
given NE is translated, either semantically 
(called translation) or phonetically (called trans-
literation), depends greatly on its associated en-
tity type2. The mapping type ratio, which is the 
percentage of NE internal tokens which are 
translated semantically, can help with the recog-
nition of the associated NE type; (2) Entities 
within an aligned pair should share the same type, 
and this restriction should be integrated into NE 
alignment as a constraint; (3) Those initially 
identified monolingual NEs can act as anchors to 
give monolingual candidate certainty scores 
                                                 
1 Which is the percentage of desired NE-pairs that are in-
cluded in the expanded set, and is the upper bound on NE 
alignment performance (regardless of NE types).  
2 The proportions of semantic translation, which denote the 
ratios of semantically translated words among all the asso-
ciated NE words, for person names (PER), location names 
(LOC), and organization names (ORG) approximates 0%, 
28.6%, and 74.8% respectively in Chinese-English name 
entity list (2005T34) released by the Linguistic Data Con-
sortium (LDC). Since the title, such as ?sir? and ?chairman?, 
is not considered as a part of person names in this corpus, 
PERs are all transliterated there. 
 631
(preference weightings) for the re-generated can-
didates. 
Based on the above observation, a new joint 
model which adopts the mapping type ratio, en-
forces the entity type consistency constraint, and 
also utilizes the monolingual candidate certainty 
factors is proposed in this paper to jointly iden-
tify and align bilingual NEs under an integrated 
framework. This framework is decomposed into 
three subtasks: Initial Detection, Expansion, and 
Alignment&Re-identification. The Initial Detec-
tion subtask first locates the initial NEs and their 
associated NE types inside both the Chinese and 
English sides. Afterwards, the Expansion subtask 
re-generates the candidate-sets in both languages 
to recover those initial NE recognition errors. 
Finally, the Alignment&Re-identification subtask 
jointly recognizes and aligns bilingual NEs via 
the proposed joint model presented in Section 3. 
With this new approach, 41.8% imperfection re-
duction in type-sensitive F-score, from 68.4% to 
81.6%, has been observed in our Chinese-
English NE alignment task. 
2 Motivation 
The problem of NE recognition requires both 
boundary identification and type classification. 
However, the complexity of these tasks varies 
with different languages. For example, Chinese 
NE boundaries are especially difficult to identify 
because Chinese is not a tokenized language. In 
contrast, English NE boundaries are easier to 
identify due to capitalization clues. On the other 
hand, classification of English NE types can be 
more challenging (Ji et al, 2006). Since align-
ment would force the linked NE pair to share the 
same semantic meaning, the NE that is more re-
liably identified in one language can be used to 
ensure its counterpart in another language. This 
benefits both the NE boundary identification and 
type classification processes, and it hints that 
alignment can help to re-identify those initially 
recognized NEs which had been less reliable. 
As shown in the following example, although 
the desired NE ????????? is recognized 
partially as ?????? in the initial recognition 
stage, it would be more preferred if its English 
counterpart ?North Korean's Central News 
Agency? is given. The reason for this is that 
?News Agency? would prefer to be linked to ??
???, rather than to be deleted (which would 
happen if ?????? is chosen as the corre-
sponding Chinese NE).  
 
(I) The initial NE detection in a Chinese sentence: 
???  <ORG>????</ORG> ???????...  
(II) The initial NE detection of its English counterpart: 
Official <ORG>North Korean's Central News Agency 
</ORG> quoted the navy's statement? 
(III) The word alignment between two NEs: 
  
(VI) The re-identified Chinese NE boundary after alignment:  
??? <ORG>???????</ORG> ??????...  
As another example, the word ?lake? in the 
English NE is linked to the Chinese character 
??? as illustrated below, and this mapping is 
found to be a translation and not a transliteration. 
Since translation rarely occurs for personal 
names (Chen et al, 2003), the desired NE type 
?LOC? would be preferred to be shared between 
the English NE ?Lake Constance? and its corre-
sponding Chinese NE ???????. As a result, 
the original incorrect type ?PER? of the given 
English NE is fixed, and the necessity of using 
mapping type ratio and NE type consistency con-
straint becomes evident. 
(I) The initial NE detection result in a Chinese sentence: 
?  <LOC>?????</LOC> ?????????? 
(II) The initial NE detection of its English counterpart: 
The captain of a ferry boat who works on <PER>Lake Con-
stance </PER>? 
(III) The word alignment between two NEs: 
  
(VI) The re-identified English NE type after alignment: 
The captain of a ferry boat who works on <LOC>Lake 
Constance</LOC>? 
3 The Proposed Model 
As mentioned in the introduction section, given a 
Chinese-English sentence-pair ( , , with its 
initially recognized Chinese NEs 
)CS ES
1, ,Si i iCNE CType S? 1? ? ?
1[ , ] ,Tj j jENE EType T? ?
 and English NEs 
 (  and 1 ieCTyp jEty
iCNE
pe
EN
 are 
original NE types assigned to  and , 
respectively), we will first re-generate two NE 
candidate-sets from them by enlarging and 
shrinking the boundaries of those initially recog-
nized NEs. Let 
jE
1 C
KR  and CNE 1 EKRENE
C
 denote 
these two re-generated candidate sets for Chi-
nese and English NEs respectively ( K  and EK  
are their set-sizes), and ? ?min ,K S T? , then a 
total K  pairs of final Chinese and English NEs 
will be picked up from the Cartesian product of 
 632
1 C
KRCNE  and 1 EKRENE
( ,RCNE R? ?
[ ]kRENE
RType
? RE
iCNE
, according to their associ-
ated linking score, which is defined as follows. 
Let  denote the asso-
ciated linking score for a given candidate-pair 
 and , where  and  are 
the associated indexes of the re-generated Chi-
nese and English NE candidates, respectively. 
Furthermore, let  be the NE type to be re-
assigned and shared by RCNE  and  
(as they possess the same meaning). Assume 
that  and  are derived from ini-
tially recognized  and , respectively, 
and 
[ ]kre ENE
k
k? [ ]kNE
EN
Sco
kRCNE? ?
RCNE
)k
k? ?
k? ?
jE
[ ]k
RENE[ ]k
ICM  denotes their internal component map-
ping, to be defined in Section 3.1, then  
 is  defined as follows: [ ]( ,k RENE? ?
[ ]
, ,
k k
IC k
i i
RENE
M RType
NE CType
)kNEScore RC
,
max
IC kM RType
Score RCN
P? [ ]
( , )
,     , , ,[ , ],
k k
j j
E
RCNE RENE
C CS ENE EType ES
? ?
? ????? ??
| ????
 (1)                  
Here, the ?max? operator varies over each 
possible internal component mapping ICM  and 
re-assigned type (PER, LOC, and ORG). For 
brevity, we will drop those associated subscripts 
from now on, if there is no confusion. 
The associated probability factors in the above 
linking score can be further derived as follows. 
? ?
?
?
?
, , ,, ,    [ , ],
  , ,
  , ,
  , ,
IC
IC
CNE CType CSP M RType ENE EType ES
P M RTyp ENE
P RCNE CS RType
P RENE E ES RType
P RType Type EType
? ? ? ???
?
?
?
?
?
?
?
,
, ,
| ,
| ,
| ,
RCNE RENE
e RCNE R
CNE CType
NE EType
CNE ENE C
??
   (2) 
In the above equation, 
? ?, ,e RCNE
? | , ,ENE C
| ,CType
| ,NE EType
ICP M RTyp RENE
?
 and 
 are the Bilin-
gual Alignment Factor and the Bilingual Type 
Re-assignment Factor respectively, to represent 
the bilingual related scores (Section 3.1). Also, 
and 
 are Monolin-
gual Candidate Certainty Factors (Section 3.2) 
used to assign preference to each selected  
and , based on the initially recognized 
NEs (which act as anchors).  
,P RType CNE Type EType
? ?, ,P RCNE CNE CS RType
? ?, ,P RENE E ES RType
RENE
RCNE
3.1 Bilingual Related Factors 
The bilingual alignment factor mainly represents 
the likelihood value of a specific internal com-
ponent mapping ICM , given a pair of possible 
NE configurations RCNE  and  and their 
associated . Since Chinese word segmen-
tation is problematic, especially for transliterated 
words, the bilingual alignment factor 
RENE
RType
? ?, ,CNE REICP M RType R NE  in Eq (2) is derived 
to be conditioned on RE  (i.e., starting from 
the English part). 
NE
We define the internal component mapping 
ICM  to be [ ] 1[ , , ] ,NIC n n n nM cpn ew Mtype ?? ? ??? ?
[ ][ , , ]n n new Mtype
ncpn
, 
where  denotes a linked pair 
consisting of a Chinese component 
cpn? ?
? ?
[ ]new RCNE
 
(which might contain several Chinese characters) 
and an English word  within  and 
 respectively, with their internal mapping 
type 
RENE
nMtype
TLN
2[ ,n ew
 to be either translation (abbreviated 
as TS) or transliteration (abbreviated as TL). In 
total, there are N  component mappings, with 
 translation mappings  
and  transliteration mappings 
TSN
cpn
1 1[ ][ , , TSNn ncpn ew TS? ?
2 2[ ] 1, ] TLNn nTL
1 1]n ?
? ? ? TS TLN N N? ?, so that .  
Moreover, since the mapping type distribu-
tions of various NE types deviate greatly from 
one another, as illustrated in the second footnote, 
the associated mapping type ratio ? ?/TSN N? ?  is 
thus an important feature, and is included in the 
internal component mapping configuration speci-
fied above. For example, the ICM  between ???
???? and ?Constance Lake? is [????, 
Constance, TL] and [?, Lake, TS], so its asso-
ciated mapping type ratio will be ?0.5? (i.e., 1/2). 
Therefore, the internal mapping 
 is further deduced by in-
troducing the internal mapping type 
( | ,ICP M RType RENE)
nMtype  and 
the mapping type ratio ?  as follows: 
[ ] 1
[ ]
1 [ ]
( | , )
([ , , ] , | , )
( | , , )
( | , )
( | )
IC
N
n n n n
N n n n
n n n
P M RType RENE
P cpn ew Mtype RType RENE
P cpn Mtype ew RType
P Mtype ew RType
P RType
?
?
? ? ?
? ?
?
?
? ?? ? ??? ?? ?
?
?    (3) 
In the above equation, the mappings between 
internal components are trained from the sylla-
ble/word alignment of NE pairs of different NE 
types. In more detail? for transliteration, the 
model adopted in (Huang et al, 2003), which 
first Romanizes Chinese characters and then 
transliterates them into English characters, is 
 633
used for . For transla-
tion, conditional probability is directly used for 
.  
[ ]( | , ,n n nP cpn TL ew RType? ?
[ ]( | , , )n n nTS ew RType
)
?
P cpn? ?
Lastly, the bilingual type re-assignment factor 
 proposed in 
Eq (2) is derived as follows: 
? | , , ,P RType CNE ENE CType EType
? ?
? ?
| , , ,
| ,
P RType RCNE RENE CType EType
P RType CType EType?        (4) 
As Eq (4) shows, both the Chinese initial NE 
type and English initial NE type are adopted to 
jointly identify their shared NE type RType .  
3.2 Monolingual Candidate Certainty Factors 
On the other hand, the monolingual candidate 
certainty factors in Eq (2) indicate the likelihood 
that a re-generated NE candidate is the true NE 
given its originally detected NE. For Chinese, it 
is derived as follows: 
?
11
( , , , )
, , [ ] , ,
( , , )
 ( , , )
( | , )
C
C
C
M
m mm
P RCNE CNE CType CS RType
P LeftD RightD Str RCNE Len CType RType
P LeftD Len CType RType
P RightD Len CType RType
P cc cc RType??
?
?
?
??
|
|
|
|
?
  (5) 
Where, the subscript C  denotes Chinese, and 
 is the length of the originally recognized 
Chinese NE CN .  and  denote the 
left and right distance (which are the numbers of 
Chinese characters) that R  shrinks/enlarges 
from the left and right boundary of its anchor 
, respectively. As in the above example, 
assume that CN  and  are ?????? 
and ???????? respectively, Le  and 
 will be ?-1? and ?+3?. Also,  
stands for the associated Chinese string of , 
 denotes the m-th Chinese character within 
that string, and 
CLen
CNE
RightD
mcc
E
E
LeftD
R
RightD
CNE
CNE
ftD
Str R
R
[ ]CNE
CNE
M denotes the total number of 
Chinese characters within .  RCNE
On the English side, following Eq (5), 
? ?| , , ,P RENE ENE EType ES RType
ftD
E RENE
LeftD RightD
mcc
 can be derived 
similarly, except that Le  and  will be 
measured in number of English words. For in-
stance, with   EN  and  as  ?Lake Con-
stance? and ?on Lake Constance? respectively, 
 and  will be ?+1? and ?0?. Also, 
the bigram unit  of the Chinese NE string is 
replaced by the English word unit .  
RightD
new
All the bilingual and monolingual factors 
mentioned above, which are derived from Eq (1), 
are weighted differently according to their con-
tributions. The corresponding weighting coeffi-
cients are obtained using the well-known Mini-
mum Error Rate Training (Och, 2003; com-
monly abbreviated as MERT) algorithm by 
minimizing the number of associated errors in 
the development set. 
3.3 Framework for the Proposed Model  
The above model is implemented with a three-
stage framework: (A) Initial NE Recognition; (B) 
NE-Candidate-Set Expansion; and (C) NE 
Alignment&Re-identification. The Following 
Diagram gives the details of this framework: 
 
For each given bilingual sentence-pair: 
(A) Initial NE Recognition: generates the ini-
tial NE anchors with off-the-self packages. 
(B) NE-Candidate-Set Expansion: For each 
initially detected NE, several NE candi-
dates will be re-generated from the origi-
nal NE by allowing its boundaries to be 
shrunk or enlarged within a pre-specified 
range.  
(B.1) Create both RCNE and RENE 
candidate-sets, which are ex-
panded from those initial NEs 
identified in the previous stage.  
(B.2) Construct an NE-pair candidate-
set (named NE-Pair-Candidate-
Set), which is the Cartesian 
product of the RCNE and RENE 
candidate-sets created above.  
(C) NE Alignment&Re-identification: Rank 
each candidate in the NE-Pair-Candidate-
Set constructed above with the linking 
score specified in Eq (1). Afterwards, con-
duct a beam search process to select the 
top K non-overlapping NE-pairs from this 
set. 
Diagram 1. Steps to Generate the Final NE-Pairs 
 
It is our observation that, four Chinese charac-
ters for both shrinking and enlarging, two Eng-
lish words for shrinking and three for enlarging 
are enough in most cases. Under these conditions, 
the including-rates for NEs with correct bounda-
ries are raised to 95.8% for Chinese and 97.4% 
for English; and even the NE-pair including rate 
is raised to 95.3%. Since the above range limita-
tion setting has an including-rate only 0.8% 
lower than that can be obtained without any 
range limitation (which is 96.1%), it is adopted 
in this paper to greatly reduce the number of NE-
pair-candidates. 
 634
4 Experiments 
To evaluate the proposed joint approach, a prior 
work (Huang et al, 2003) is re-implemented in 
our environment as the baseline, in which the 
translation cost, transliteration cost and tagging 
cost are used. This model is selected for com-
parison because it not only adopts the same can-
didate-set expansion strategy as mentioned above, 
but also utilizes the monolingual information 
when selecting NE-pairs (however, only a simple 
bi-gram model is used as the tagging cost in their 
paper). Note that it enforces the same NE type 
only when the tagging cost is evaluated: 
11
11
min [ log( ( | , ))
                     log( ( | , ))]
RType
M
tag m mm
N
n nn
C P cc cc RType
P ew ew RType
??
??
? ?
?
?
? . 
To give a fairer comparison, the same train-
ing-set and testing-set are adopted. The training-
set includes two parts. The first part consists of 
90,412 aligned sentence-pairs newswire data 
from the Foreign Broadcast Information Service 
(FBIS), which is denoted as Training-Set-I. The 
second Part of the training set is the 
LDC2005T34 bilingual NE dictionary3, which is 
denoted as Training-Set-II. The required feature 
information is then manually labeled throughout 
the two training sets.  
In our experiments, for the baseline system, 
the translation cost and the transliteration cost 
are trained on Training-Set-II, while the tagging 
cost is trained on Training-Set-I. For the pro-
posed approach, the monolingual candidate cer-
tainty factors are trained on Training-Set-I, and 
Training-Set-II is used to train the parameters 
relating to bilingual alignment factors.  
For the testing-set, 300 sentence pairs are ran-
domly selected from the LDC Chinese-English 
News Text (LDC2005T06). The average length 
of the Chinese sentences is 59.4 characters, while 
the average length of the English sentences is 
24.8 words. Afterwards, the answer keys for NE 
recognition and alignment were annotated manu-
ally, and used as the gold standard to calculate 
metrics of precision (P), recall (R), and F-score 
(F) for both NE recognition (NER) and NE 
alignment (NEA). In Total 765 Chinese NEs and 
747 English NEs were manually labeled in the 
testing-set, within which there are only 718 NE 
pairs, including 214 PER, 371 LOC and 133 
ORG NE-pairs. The number of NE pairs is less 
                                                 
3 The LDC2005T34 data-set consists of proofread bilingual 
entries: 73,352 person names, 76,460 location names and 
68,960 organization names. 
than that of NEs, because not all those recog-
nized NEs can be aligned. 
Besides, the development-set for MERT 
weight training is composed of 200 sentence 
pairs selected from the LDC2005T06 corpus, 
which includes 482 manually tagged NE pairs. 
There is no overlap between the training-sets, the 
development-set and the testing-set.  
4.1 Baseline System 
Both the baseline and the proposed models share 
the same initial detection subtask, which adopts 
the Chinese NE recognizer reported by Wu et al  
(2005), which is a hybrid statistical model incor-
porating multi-knowledge sources, and the Eng-
lish NE recognizer included in the publicly 
available Mallet toolkit4 to generate initial NEs. 
Initial Chinese NEs and English NEs are recog-
nized by these two available packages respec-
tively.  
 
NE-type P (%): C/E R (%): C/E F (%): C/E
PER 80.2 / 79.2 87.7 / 85.3 83.8 / 82.1
LOC 89.8 / 85.9 87.3 / 81.5 88.5/ 83.6
ORG 78.6 / 82.9 82.8 / 79.6 80.6 / 81.2
ALL 83.4 / 82.1 86.0 / 82.6 84.7 / 82.3
Table 1. Initial Chinese/English NER 
 
Table 1 shows the initial NE recognition per-
formances for both Chinese and English (the 
largest entry in each column is highlighted for 
visibility). From Table 1, it is observed that the 
F-score of ORG type is the lowest among all NE 
types for both English and Chinese. This is be-
cause many organization names are partially rec-
ognized or missed. Besides, not shown in the 
table, the location names or abbreviated organi-
zation names tend to be incorrectly recognized as 
person names. In general, the initial Chinese 
NER outperforms the initial English NER, as the 
NE type classification turns out to be a more dif-
ficult problem for this English NER system. 
When those initially identified NEs are di-
rectly used for baseline alignment, only 64.1% F 
score (regard of their name types) is obtained. 
Such a low performance is mainly due to those 
NE recognition errors which have been brought 
into the alignment stage.  
To diminish the effect of errors accumulating, 
which stems from the recognition stage, the base-
line system also adopts the same expansion strat-
egy described in Section 3.3 to enlarge the possi-
                                                 
4 http://mallet.cs.umass.edu/index.php/Main_Page 
 635
ble NE candidate set. However, only a slight im-
provement (68.4% type-sensitive F-score) is ob-
tained, as shown in Table 2. Therefore, it is con-
jectured that the baseline alignment model is un-
able to achieve good performance if those fea-
tures/factors proposed in this paper are not 
adopted. 
4.2 The Recognition and Alignment Joint 
Model 
To show the individual effect of each factor in 
the joint model, a series of experiments, from 
Exp0 to Exp11, are conducted. Exp0 is the basic 
system, which ignores monolingual candidate 
certainty scores, and also disregards mapping 
type and NE type consistency constraint by ig-
noring  and [ ]( | ,n nP Mtype ew RType) ( | )P RType? , 
and also replacing P  
with  in Eq (3).  
[ ], ,n n new RType( |cpn? ?
[ ]( | )n nP cpn ew? ?
)
)
)
)
)n
Mtype
To show the effect of enforcing NE type con-
sistency constraint on internal component map-
ping, Exp1 (named Exp0+RType) replaces 
 in Exp0 with 
; On the other hand, Exp2 
(named Exp0+MappingType) shows the effect of 
introducing the component mapping type to Eq 
(3) by replacing  in Exp0 by 
; Then 
Exp3 (named Exp2+MappingTypeRatio) further 
adds 
[ ]( |n nP cpn ew? ?
[ ]( |n nP cpn ew? ?
( |n nP cpn Mtype? ?
( |P RTy
, RType
P c
[ ],ew
)pe
[ ]( |n npn ew? ?
) (n P Mtype e? [ ]|n w
?  to Exp2, to manifest the con-
tribution from the mapping type ratio. In addition, 
Exp4 (named Exp0+RTypeReassignment) adds 
the NE type reassignment score, Eq (4), to Exp0 
to show the effect of enforcing NE-type consis-
tency. Furthermore, Exp5 (named All-BiFactors) 
shows the full power of the set of proposed bi-
lingual factors by turning on all the options men-
tioned above. As the bilingual alignment factors 
would favor the candidates with shorter lengths, 
[ ] 1([ , , ] , | , ),Nn n n nP cpn ew Mtype RType RENE?? ? ? Eq (3), 
is further normalized into the following form: 
1
[ ]
1
[ ]
( | , , ) ( | ),
( | , )
N N
n n n
n
n n
P cpn Mtype ew RType P RType
P Mtype ew RType
?? ??
? ?? ? ?? ?? ??? ?
?
and is shown by Exp6 (named All-N-BiFactors). 
To show the influence of additional informa-
tion carried by those initially recognized NEs, 
Exp7 (named Exp6+LeftD/RightD) adds left and 
right distance information into Exp6, as that 
specified in Eq (5). To study the monolingual bi-
gram capability, Exp8 (named Exp6+Bigram) 
adds the NEtype dependant bigram model of 
each language to Exp6. We use SRI Language 
Modeling Toolkit5 (SRILM) (Stolcke, 2002) to 
train various character/word based bi-gram mod-
els with different NE types. Similar to what we 
have done on the bilingual alignment factor 
above, Exp9 (named Exp6+N-Bigram) adds the 
normalized NEtype dependant bigram to Exp6 
for removing the bias induced by having differ-
ent NE lengths. The normalized Chinese NEtype 
dependant bigram score is defined as 
1
11[ ( | , )
M ]Mm mm P cc cc RType??? . A Similar trans-
formation is also applied to the English side. 
Lastly, Exp10 (named Fully-JointModel) 
shows the full power of the proposed Recogni-
tion and Alignment Joint Model by adopting all 
the normalized factors mentioned above. The 
result of a MERT weighted version is further 
shown by Exp11 (named Weighted-JointModel). 
 
Model P (%) R (%) F (%)
Baseline 77.1  (67.1) 
79.7 
(69.8) 
78.4 
(68.4) 
Exp0 
(Basic System) 
67.9 
 (62.4) 
70.3 
(64.8) 
69.1 
(63.6) 
Exp1 
(Exp0 + Rtype) 
69.6 
 (65.7) 
71.9 
(68.0) 
70.8 
(66.8) 
Exp2 
(Exp0 + MappingType) 
70.5 
 (65.3) 
73.0 
(67.5) 
71.7 
(66.4) 
Exp3 
(Exp2 + MappingTypeRatio)
72.0 
(68.3) 
74.5 
(70.8) 
73.2 
(69.5) 
Exp4 
(Exp0 + RTypeReassignment)
70.2 
(66.7) 
72.7 
(69.2) 
71.4 
(67.9) 
Exp5 
(All-BiFactors) 
76.2 
 (72.3) 
78.5 
(74.6) 
77.3 
(73.4) 
Exp6 
(All-N-BiFactors) 
77.7 
(73.5) 
79.9 
(75.7) 
78.8 
(74.6) 
Exp7 
(Exp6 + LeftD/RightD) 
83.5 
(77.7) 
85.8 
(80.1) 
84.6 
(78.9) 
Exp8  
(Exp6 + Bigram) 
80.4 
(75.5) 
82.7 
(77.9) 
81.5 
(76.7) 
Exp9 
(Exp6 + N-Bigram) 
82.7 
(77.1) 
85.1 
(79.6) 
83.9 
(78.3) 
Exp10 
(Fully-JointModel) 
83.7 
(78.1) 
86.2 
(80.7) 
84.9 
(79.4) 
Exp11 
(Weighted-Joint Model) 
85.9 
(80.5) 
88.4 
(83.0) 
87.1 
(81.7) 
Table 2. NEA Type-Insensitive (Type-Sensitive) 
Performance  
 
Since most papers in the literature are evalu-
ated only based on the boundaries of NEs, two 
kinds of performance are thus given here. The 
first one (named type-insensitive) only checks 
the scope of each NE without taking its associ-
ated NE type into consideration, and is reported 
                                                 
5   http://www.speech.sri.com/projects/srilm/ 
 636
as the main data at Table 2. The second one 
(named type-sensitive) would also evaluate the 
associated NE type of each NE, and is given 
within parentheses in Table 2. A large degrada-
tion is observed when NE type is also taken into 
account. The highlighted entries are those that 
are statistically better6 than that of the baseline 
system. 
4.3 ME Approach with Primitive Features 
Although the proposed model has been derived 
above in a principled way, since all these pro-
posed features can also be directly integrated 
with the well-known maximum entropy (ME) 
(Berger et al, 1996) framework without making 
any assumptions, one might wonder if it is still 
worth to deriving a model after all the related 
features have been proposed. To show that not 
only the features but also the adopted model con-
tribute to the performance improvement, an ME 
approach is tested as follows for comparison. It 
directly adopts all those primitive features men-
tioned above as its inputs (including internal 
component mapping, initial and final NE type, 
NE bigram-based string, and left/right distance), 
without involving any related probability factors 
derived within the proposed model.  
This ME method is implemented with a public 
package YASMET7, and is tested under various 
training-set sizes (400, 4,000, 40,000, and 90,412 
sentence-pairs). All those training-sets are ex-
tracted from the Training-Set-I mentioned above 
(a total of 298,302 NE pairs included are manu-
ally labeled). Since the ME approach is unable to 
utilize the bilingual NE dictionary (Training-Set-
II), for fair comparison, this dictionary was also 
not used to train our models here. Table 3 shows 
the performance (F-score) using the same test-
ing-set. The data within parentheses are relative 
improvements. 
 
Model 400 4,000 40,000 90,412
ME framework 36.5 (0%) 
50.4 
(0%) 
62.6 
(0%) 
67.9 
(0%) 
Un-weighted- 
JointModel 
+4.6 
(+12.6%) 
+4.5 
(+8.9%) 
+4.3 
(+6.9%) 
+4.1 
(+6.0%)
Weighted- 
JointModel 
+5.0 
(+13.7%) 
+4.7 
(+9.3%) 
+4.6 
(+7.3%) 
+4.5 
(+6.6%)
Table 3. Comparison between ME Framework 
and Derived Model on the Testing-Set 
 
                                                 
6 Statistical significance test is measured on 95% confidence 
level on 1,000 re-sampling batches (Zhang et al, 2004) 
7 http://www.fjoch.com/YASMET.html 
The improvement indicated in Table 3 clearly 
illustrates the benefit of deriving the model 
shown in Eq (2). Since a reasonably derived 
model not only shares the same training-set with 
the primitive ME version above, but also enjoys 
the additional knowledge introduced by the hu-
man (i.e., the assumptions/constraints implied by 
the model), it is not surprising to find out that a 
good model does help, and that it also becomes 
more noticeable as the training-set gets smaller.  
5 Error Analysis and Discussion 
Although the proposed model has substantially 
improved the performance of both NE alignment 
and recognition, some errors still remain. Having 
examined those type-insensitive errors, we found 
that they can be classified into four categories: 
(A) Original NEs or their components are al-
ready not one-to-one mapped (23%). (B) NE 
components are one-to-one linked, but the asso-
ciated NE anchors generated from the initial rec-
ognition stage are either missing or spurious 
(24%). Although increasing the number of output 
candidates generated from the initial recognition 
stage might cover the missing problem, possible 
side effects might also be expected (as the com-
plexity of the alignment task would also be in-
creased). (C) Mapping types are not assumed by 
the model (27%). For example, one NE is abbre-
viated while its counterpart is not; or some loan-
words or out-of-vocabulary terms are translated 
neither semantically nor phonetically. (D) Wrong 
NE scopes are selected (26%). Errors of this type 
are uneasy to resolve, and their possible solutions 
are beyond the scope of this paper. 
Examples of above category (C) are interest-
ing and are further illustrated as follows. As an 
instance of abbreviation errors, a Chinese NE 
??????? (GlaxoSmithKline Factory)? is 
tagged as ???? /PRR ??? /n?, while its 
counterpart in the English side is simply abbrevi-
ated as ?GSK? (or  replaced by a pronoun ?it? 
sometimes). Linking ????? to ?GSK? (or to 
the pronoun ?it?) is thus out of reach of our 
model. It seems an abbreviation table (or even 
anaphora analysis) is required to recover these 
kind of errors.  
As an example of errors resulting from loan-
words; Japanese kanji ???? (the name of a 
Japanese emperor) is linked to the English word 
?Akihito?. Here the Japanese kanji ???? is di-
rectly adopted as the corresponding Chinese 
characters (as those characters were originally 
borrowed from Chinese), which would be pro-
 637
nounced as ?Mingren? in Chinese and thus devi-
ates greatly from the English pronunciation of 
?Akihito?. Therefore, it is translated neither se-
mantically nor phonetically. Further extending 
the model to cover this new conversion type 
seems necessary; however, such a kind of exten-
sion is very likely to be language pair dependent. 
6 Capability of the Proposed Model 
In addition to improving NE alignment, the pro-
posed joint model can also boost the perform-
ance of NE recognition in both languages. The 
corresponding differences in performance (of the 
weighted version) when compared with the ini-
tial NER ( ,   and P? R? F? ) are shown in Table 4. 
Again, those marked entries indicate that they are 
statistically better than that of the original NER.  
 
NEtype P? (%): C/E R? (%): C/E F? (%): C/E
PER +5.4 / +6.4 +2.2 / +2.6 +3.9 / +4.6 
LOC +4.0 / +3.4 -0.2 / +2.7 +1.8 / +3.0 
ORG +7.0 / +3.9 +5.6 / +9.1 +6.2 / +6.4 
ALL +5.3 /+5.2 +2.4 / +4.0 +3.9 / +4.6 
Table 4. Improvement in Chinese/English NER 
 
The result shows that the proposed joint model 
has a clear win over the initial NER for either 
Chinese or English NER. In particular, ORG 
seems to have yielded the greatest gain amongst 
NE types, which matches our previous observa-
tions that the boundaries of Chinese ORG are 
difficult to identify with the information only 
coming from the Chinese sentence, while the 
type of English ORG is uneasy to classify with 
the information only coming from the English 
sentence.  
Though not shown in the tables, it is also ob-
served that the proposed approach achieves a 
28.9% reduction on the spurious (false positive) 
and partial tags over the initial Chinese NER, as 
well as 16.1% relative error reduction compared 
with the initial English NER. In addition, total 
27.2% wrong Chinese NEs and 40.7% wrong 
English NEs are corrected into right NE types. 
However, if the mapping type ratio is omitted, 
only 21.1% wrong Chinese NE types and 34.8% 
wrong English NE types can be corrected. This 
clearly indicates that the ratio is essential for 
identifying NE types. 
With the benefits shown above, the alignment 
model could thus be used to train the monolin-
gual NE recognition model via semi-supervised 
learning. This advantage is important for updat-
ing the NER model from time to time, as various 
domains frequently have different sets of NEs 
and new NEs also emerge with time. 
Since the Chinese NE recognizer we use is not 
an open source toolkit, it cannot be used to carry 
out semi-supervised learning. Therefore, only the 
English NE recognizer and the alignment model 
are updated during training iterations. In our ex-
periments, 50,412 sentence pairs are first ex-
tracted from Training-Set-I as unlabeled data. 
Various labeled data-sets are then extracted from 
the remaining data as different seed corpora (100, 
400, 4,000 and 40,000 sentence-pairs). Table 5 
shows the results of semi-supervised learning 
after convergence for adopting only the English 
NER model (NER-Only), the baseline alignment 
model (NER+Baseline), and our un-weighted 
joint model (NER+JointModel) respectively. The 
Initial-NER row indicates the initial performance 
of the NER model re-trained from different seed 
corpora. The data within parentheses are relative 
improvement over Initial-NER. Note that the 
testing set is still the same as before.  
As Table 5 shows, with the NER model alone, 
the performance may even deteriorate after con-
vergence. This is due to the fact that maximizing 
likelihood does not imply minimizing the error 
rate. However, with additional mapping con-
straints from the aligned sentence of another lan-
guage, the alignment module could guide the 
searching process to converge to a more desir-
able point in the parameter space; and these addi-
tional constraints become more effective as the 
seed-corpus gets smaller. 
 
Model 100 400 4,000 40,000
Initial-NER 36.7 (0%) 
58.6 
(0%) 
71.4 
(0%) 
79.1 
(0%) 
NER-Only -2.3 (-6.3%)
-0.5 
(-0.8%) 
-0.3 
(-0.4%) 
-0.1 
(-0.1%)
NER+Baseline +4.9 (+13.4%)
+3.4 
(5.8%) 
+1.7 
(2.4%) 
+0.7 
(0.9%)
NER+Joint 
 Model 
+10.7 
(+29.2%)
+8.7 
(+14.8%) 
+4.8 
(+6.7%) 
+2.3 
(+2.9%)
Table 5. Testing-Set Performance for Semi-
Supervised Learning of English NE Recognition  
7 Conclusion 
In summary, our experiments show that the new 
monolingual candidate certainty factors are more 
effective than the tagging cost (only bigram 
model) adopted in the baseline system. Moreover, 
both the mapping type ratio and the entity type 
consistency constraint are very helpful in identi-
fying the associated NE boundaries and types. 
After having adopted the features and enforced 
 638
 the constraint mentioned above, the proposed 
framework, which jointly recognizes and aligns 
bilingual named entities, achieves a remarkable 
42.1% imperfection reduction on type-sensitive 
F-score (from 68.4% to 81.7%) in our Chinese-
English NE alignment task. 
Although the experiments are conducted on 
the Chinese-English language pair, it is expected 
that the proposed approach can also be applied to 
other language pairs, as no language dependent 
linguistic feature (or knowledge) is adopted in 
the model/algorithm used. 
Acknowledgments 
The research work has been partially supported 
by the National Natural Science Foundation of 
China under Grants No. 60975053, 90820303, 
and 60736014, the National Key Technology 
R&D Program under Grant No. 2006BAH03B02, 
and also the Hi-Tech Research and Development 
Program (?863? Program) of China under Grant 
No. 2006AA010108-4. 
References 
Al-Onaizan, Yaser, and Kevin Knight. 2002. Translat-
ing Named Entities Using Monolingual and Bilin-
gual resources. In Proceedings of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), pages 400-408. 
Berger, Adam L., Stephen A. Della Pietra and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy 
Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39-72, March. 
Chen, Hsin-His, Changhua Yang and Ying Lin. 2003. 
Learning Formulation and Transformation Rules 
for Multilingual Named Entities. In Proceedings of 
the ACL 2003 Workshop on Multilingual and 
Mixed-language Named Entity Recognition, pages 
1-8. 
Feng, Donghui, Yajuan Lv and Ming Zhou. 2004. A 
New Approach for English-Chinese Named Entity 
Alignment. In Proceedings of the Conference on 
Empirical Methods in Natural Language Process-
ing (EMNLP 2004), pages 372-379. 
Huang, Fei, Stephan Vogel and Alex Waibel. 2003. 
Automatic Extraction of Named Entity Translin-
gual Equivalence Based on Multi-Feature Cost 
Minimization. In Proceedings of ACL?03, Work-
shop on Multilingual and Mixed-language Named 
Entity Recognition. Sappora, Japan. 
Ji, Heng and Ralph Grishman. 2006. Analysis and 
Repair of Name Tagger Errors. In Proceedings of 
COLING/ACL 06, Sydney, Australia. 
Lee, Chun-Jen, Jason S. Chang and Jyh-Shing R. Jang. 
2006. Alignment of Bilingual Named Entities in 
Parallel Corpora Using Statistical Models and Mul-
tiple Knowledge Sources. ACM Transactions on 
Asian Language Information Processing (TALIP), 
5(2): 121-145. 
Moore, R. C.. 2003. Learning Translations of Named-
Entity Phrases from Parallel Corpora. In Proceed-
ings of 10th Conference of the European Chapter 
of ACL, Budapest, Hungary. 
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Conference of the Associa-
tion for Computational Linguistics (ACL). July 8-
10, 2003. Sapporo, Japan. Pages: 160-167. 
Stolcke, A. 2002. SRILM -- An Extensible Language 
Modeling Toolkit. Proc. Intl. Conf. on Spoken 
Language Processing, vol. 2, pp. 901-904, Denver. 
Wu, Youzheng, Jun Zhao and Bo Xu. 2005. Chinese 
Named Entity Recognition Model Based on Multi-
ple Features. In Proceedings of HLT/EMNLP 2005, 
pages 427-434. 
Zhang, Ying, Stephan Vogel, and Alex Waibel, 2004. 
Interpreting BLEU/NIST Scores: How Much Im-
provement Do We Need to Have a Better System? 
In Proceedings of the 4th International Conference 
on Language Resources and Evaluation, pages 
2051--2054.  
639
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 159?164,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Evaluation of Chinese Translation Output:                                    
Word-Level or Character-Level? 
 
Maoxi Li    Chengqing Zong Hwee Tou Ng 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of 
Sciences, Beijing, China, 100190 
Department of Computer Science 
National University of Singapore 
13 Computing Drive, Singapore 117417 
  {mxli, cqzong}@nlpr.ia.ac.cn                      nght@comp.nus.edu.sg 
 
 
Abstract 
Word is usually adopted as the smallest unit in 
most tasks of Chinese language processing. 
However, for automatic evaluation of the quali-
ty of Chinese translation output when translat-
ing from other languages, either a word-level 
approach or a character-level approach is possi-
ble. So far, there has been no detailed study to 
compare the correlations of these two ap-
proaches with human assessment. In this paper, 
we compare word-level metrics with character-
level metrics on the submitted output of Eng-
lish-to-Chinese translation systems in the 
IWSLT?08 CT-EC and NIST?08 EC tasks. Our 
experimental results reveal that character-level 
metrics correlate with human assessment better 
than word-level metrics. Our analysis suggests 
several key reasons behind this finding. 
1 Introduction 
White space serves as the word delimiter in Latin 
alphabet-based languages. However, in written 
Chinese text, there is no word delimiter. Thus, in 
almost all tasks of Chinese natural language 
processing (NLP), the first step is to segment a 
Chinese sentence into a sequence of words. This is 
the task of Chinese word segmentation (CWS), an 
important and challenging task in Chinese NLP. 
Some linguists believe that word (containing at 
least one character) is the appropriate unit for Chi-
nese language processing. When treating CWS as a 
standalone NLP task, the goal is to segment a sen-
tence into words so that the segmentation matches 
the human gold-standard segmentation with the 
highest F-measure, but without considering the 
performance of the end-to-end NLP application 
that uses the segmentation output. In statistical 
machine translation  (SMT), it can happen that the 
most accurate word segmentation as judged by the 
human gold-standard segmentation may not 
produce the best translation output (Zhang et al, 
2008). While state-of-the-art Chinese word 
segmenters achieve high accuracy, some errors still 
remain.  
Instead of segmenting a Chinese sentence into 
words, an alternative is to split a Chinese sentence 
into characters, which can be readily done with 
perfect accuracy. However, it has been reported 
that a Chinese-English phrase-based SMT system 
(Xu et al, 2004) that relied on characters (without 
CWS) performed slightly worse than when it used 
segmented words. It has been recognized that vary-
ing segmentation granularities are needed for SMT 
(Chang et al, 2008).  
To evaluate the quality of Chinese translation 
output, the International Workshop on Spoken 
Language Translation in 2005 (IWSLT'2005) used 
the word-level BLEU metric (Papineni et al, 
2002). However, IWSLT'08 and NIST'08 adopted 
character-level evaluation metrics to rank the sub-
mitted systems. Although there is much work on 
automatic evaluation of machine translation (MT), 
whether word or character is more suitable for au-
tomatic evaluation of Chinese translation output 
has not been systematically investigated.  
In this paper, we utilize various machine transla-
tion evaluation metrics to evaluate the quality of 
Chinese translation output, and compare their cor-
relation with human assessment when the Chinese 
translation output is segmented into words versus 
characters. Since there are several CWS tools that 
can segment Chinese sentences into words and 
their segmentation results are different, we use four 
representative CWS tools in our experiments. Our 
experimental results reveal that character-level me-
159
trics correlate with human assessment better than 
word-level metrics. That is, CWS is not essential 
for automatic evaluation of Chinese translation 
output. Our analysis suggests several key reasons 
behind this finding. 
2 Chinese Translation Evaluation 
Automatic MT evaluation aims at formulating au-
tomatic metrics to measure the quality of MT out-
put. Compared with human assessment, automatic 
evaluation metrics can assess the quality of MT 
output quickly and objectively without much hu-
man labor. 
 
  
Figure 1. An example to show an MT system translation 
and multiple reference translations being segmented into 
characters or words. 
 
To evaluate English translation output, automat-
ic MT evaluation metrics take an English word as 
the smallest unit when matching a system transla-
tion and a reference translation. On the other hand, 
to evaluate Chinese translation output, the smallest 
unit to use in matching can be a Chinese word or a 
Chinese character. As shown in Figure 1, given an 
English sentence ?how much are the umbrellas?? a 
Chinese system translation (or a reference transla-
tion) can be segmented into characters (Figure 1(a)) 
or words (Figure 1(b)). 
A variety of automatic MT evaluation metrics 
have been developed over the years, including 
BLEU (Papineni et al, 2002), NIST (Doddington, 
2002), METEOR (exact) (Banerjee and Lavie, 
2005), GTM (Melamed et al, 2003), and TER 
(Snover et al, 2006). Some automatic MT evalua-
tion metrics perform deeper linguistic analysis, 
such as part-of-speech tagging, synonym matching, 
semantic role labeling, etc. Since part-of-speech 
tags are only defined for Chinese words and not for 
Chinese characters, we restrict the automatic MT 
evaluation metrics explored in this paper to those 
metrics listed above which do not require part-of-
speech tagging. 
3 CWS Tools 
Since there are a number of CWS tools and they 
give different segmentation results in general, we 
experimented with four different CWS tools in this 
paper. 
 
ICTCLAS: ICTCLAS has been successfully used 
in a commercial product (Zhang et al, 2003). The 
version we adopt in this paper is ICTCLAS2009. 
 
NUS Chinese word segmenter (NUS): The NUS 
Chinese word segmenter uses a maximum entropy 
approach to Chinese word segmentation, which 
achieved the highest F-measure on three of the four 
corpora in the open track of the Second Interna-
tional Chinese Word Segmentation Bakeoff (Ng 
and Low, 2004; Low et al, 2005). The segmenta-
tion standard adopted in this paper is CTB (Chi-
nese Treebank).  
 
Stanford Chinese word segmenter 
(STANFORD): The Stanford Chinese word seg-
menter is another well-known CWS tool (Tseng et 
al., 2005). The version we used was released on 
2008-05-21 and the standard adopted is CTB. 
 
Urheen: Urheen is a CWS tool developed by 
(Wang et al, 2010a; Wang et al, 2010b), and it 
outperformed most of the state-of-the-art CWS 
systems in the CIPS-SIGHAN?2010 evaluation. 
This tool is trained on Chinese Treebank 6.0.  
4 Experimental Results 
4.1  Data 
To compare the word-level automatic MT evalua-
tion metrics with the character-level metrics, we 
conducted experiments on two datasets, in the spo-
ken language translation domain and the newswire 
translation domain.  
Translation: ?_?_?_?_?_?_? 
Ref 1:  ?_?_?_?_?_?_?_? 
?? 
Ref 7:  ?_?_?_?_?_?_?_?_?_?_? 
(a) Segmented into characters. 
Translation: ??_?_?_?_?_? 
Ref  1:   ??_??_??_?_? 
?? 
Ref  7:   ??_??_?_??_?_??_? 
(b) Segmented into words by Urheen. 
160
The IWSLT'08 English-to-Chinese ASR chal-
lenge task evaluated the translation quality of 7 
machine translation systems (Paul, 2008). The test 
set contained 300 segments with human assess-
ment of system translation quality. Each segment 
came with 7 human reference translations. Human 
assessment of translation quality was carried out 
on the fluency and adequacy of the translations, as 
well as assigning a rank to the output of each sys-
tem. For the rank judgment, human graders were 
asked to "rank each whole sentence translation 
from best to worst relative to the other choices" 
(Paul, 2008). Due to the high manual cost, the flu-
ency and adequacy assessment was limited to the 
output of 4 submitted systems, while the human 
rank assessment was applied to all 7 systems. 
Evaluation based on ranking is reported in this pa-
per. Experimental results on fluency and adequacy 
judgment also agree with the results on human 
rank assessment, but are not included in this paper 
due to length constraint. 
The NIST'08 English-to-Chinese translation task 
evaluated 127 documents with 1,830 segments. 
Each segment has 4 reference translations and the 
system translations of 11 MT systems, released in 
the corpus LDC2010T01. We asked native speak-
ers of Chinese to perform fluency and adequacy 
judgment on a five-point scale. Human assessment 
was done on the first 30 documents (355 segments) 
(document id ?AFP_ENG_20070701.0026? to 
?AFP_ENG_20070731.0115?). The method of 
manually scoring the 11 submitted Chinese system 
translations of each segment is the same as that 
used in (Callison-Burch et al, 2007). The adequa-
cy score indicates the overlap of the meaning ex-
pressed in the reference translations with a system 
translation, while the fluency score indicates how 
fluent a system translation is. 
4.2  Segment-Level Consistency or Correla-
tion 
For human fluency and adequacy judgments, the 
Pearson correlation coefficient is used to compute 
the segment-level correlation between human 
judgments and automatic metrics. Human rank 
judgment is not an absolute score and thus Pearson 
correlation coefficient cannot be used. We calcu-
late segment-level consistency as follows:  
    -  
    -  
The consistent number of pair wise comparisons
The total number of pair wise comparisons? ?  
Ties are excluded in pair-wise comparison. 
Table 1 and 2 show the segment-level consisten-
cy or correlation between human judgments and 
automatic metrics. The ?Character? row shows the 
segment-level consistency or correlation between 
human judgments and automatic metrics after the 
system and reference translations are segmented 
into characters. The ?ICTCLAS?, ?NUS?, 
?STANFORD?, and ?Urheen? rows show the 
scores when the system and reference translations 
are segmented into words by the respective Chi-
nese word segmenters.  
The character-level metrics outperform the best 
word-level metrics by 2?5% on the IWSLT?08 
CT-EC task, and 4?13% on the NIST?08 EC task. 
 
Method BLEU NIST METEOR GTM 1? TER 
Character 0.69  0.73  0.74  0.71 0.60 
ICTCLAS 0.64  0.70  0.69  0.66 0.57 
NUS 0.64 0.71 0.70 0.65 0.55 
STANFORD 0.64  0.69  0.69  0.64 0.54 
Urheen 0.63  0.70  0.68  0.65 0.55 
Table 1. Segment-level consistency on IWSLT?08 CT-
EC. 
 
Method BLEU NIST METEOR GTM 1? TER 
Character 0.63 0.61 0.65 0.61 0.60 
ICTCLAS 0.49 0.56 0.59 0.55 0.51 
NUS 0.49 0.57 0.58 0.54 0.51 
STANFORD 0.50 0.57 0.59 0.55 0.50 
Urheen 0.49 0.56 0.58 0.54 0.51 
Table 2. Average segment-level correlation on NIST?08 
EC. 
4.3  System-Level Correlation 
We measure correlation at the system level using 
Spearman's rank correlation coefficient. The sys-
tem-level correlations of word-level metrics and 
character-level metrics are summarized in Table 3 
and 4.  
Because there are only 7 systems that have hu-
man assessment in the IWSLT?08 CT-EC task, the 
gap between character-level metrics and word-
level metrics is very small. However, it still shows 
that character-level metrics perform no worse than 
word-level metrics. For the NIST?08 EC task, the 
system translations of the 11 submitted MT sys-
tems were assessed manually. Except for the GTM 
metric, character-level metrics outperform word-
161
level metrics. For BLEU and TER, character-level 
metrics yield up to 6?9% improvement over word-
level metrics. This means the character-level me-
trics reduce about 2?3 erroneous system rankings. 
When the number of systems increases, the differ-
ence between the character-level metrics and word-
level metrics will become larger. 
 
 Method BLEU NIST METEOR GTM 1? TER 
Character 0.96  0.93  0.96  0.93 0.96 
ICTCLAS 0.96  0.93  0.89  0.93 0.96 
NUS 0.96 0.93 0.89 0.86 0.96 
STANFORD 0.96  0.93  0.89  0.86 0.96 
Urheen 0.96  0.93  0.89  0.86 0.96 
Table 3. System-level correlation on IWSLT?08 CT-EC. 
 
 Method BLEU NIST METEOR GTM 1? TER 
Character 0.97 0.98 1.0 0.99 0.86 
ICTCLAS 0.91 0.96 0.99 0.99 0.81 
NUS 0.91 0.96 0.99 0.99 0.79 
STANFORD 0.89 0.97 0.99 0.99 0.77 
Urheen 0.91 0.96 0.99 0.99 0.79 
Table 4. System-level correlation on NIST?08 EC. 
5 Analysis 
We have analyzed the reasons why character-level 
metrics better correlate with human assessment 
than word-level metrics. 
Compared to word-level metrics, character-level 
metrics can capture more synonym matches. For 
example, Figure 1 gives the system translation and 
a reference translation segmented into words: 
Translation: ??_?_?_?_?_? 
Reference: ??_??_??_?_? 
The word ??? is a synonym for the word ??
??, and both words are translations of the English 
word ?umbrella?. If a word-level metric is used, 
the word ??? in the system translation will not 
match the word ???? in the reference translation. 
However, if the system and reference translation 
are segmented into characters, the word ??? in the 
system translation shares the same character ??? 
with the word ??? ? in the reference. Thus 
character-level metrics can better capture synonym 
matches. 
We can classify the semantic relationships of 
words that share some common characters into 
three types: exact match, partial match, and no 
match. The statistics on the output translations of 
an MT system are shown in Table 5. It shows that 
?exact match? accounts for 71% (29/41) and ?no 
match? only accounts for 7% (3/41). This means 
that words that share some common characters are 
synonyms in most cases. Therefore, character-level 
metrics do a better job at matching Chinese transla-
tions. 
 
Total  
count  
Exact  
match  
Partial  
match  No match  
41  29  9  3  
Table 5. Statistics of semantic relationships on words 
sharing some common characters. 
 
Another reason why word-level metrics perform 
worse is that the segmented words in a system 
translation may be inconsistent with the segmented 
words in a reference translation, since a statistical 
word segmenter may segment the same sequence 
of characters differently depending on the context 
in a sentence. For example: 
Translation: ?_?_?? _?_? 
Reference:   ?_?_?_ ? _?_??_? 
Here the word ???? is the Chinese translation 
of the English word ?Kyoto?.  However, it is seg-
mented into two words, ??? and ???, in the ref-
erence translation by the same CWS tool. When 
this happens, a word-level metric will fail to match 
them in the system and reference translation. While 
the accuracy of state-of-the-art CWS tools is high, 
segmentation errors still exist and can cause such 
mismatches. 
To summarize, character-level metrics can 
capture more synonym matches and the resulting 
segmentation into characters is guaranteed to be 
consistent, which makes character-level metrics 
more suitable for the automatic evaluation of 
Chinese translation output. 
6 Conclusion 
In this paper, we conducted a detailed study of the 
relative merits of word-level versus character-level 
metrics in the automatic evaluation of Chinese 
translation output. Our experimental results have 
shown that character-level metrics correlate better 
with human assessment than word-level metrics.  
Thus, CWS is not needed for automatic evaluation 
162
of Chinese translation output. Our study provides 
the needed justification for the use of character-
level metrics in evaluating SMT systems in which 
Chinese is the target language. 
Acknowledgments 
This research was done for CSIDM Project No. 
CSIDM-200804 partially funded by a grant from 
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA) 
of Singapore. This research has also been funded 
by the Natural Science Foundation of China under 
Grant No. 60975053, 61003160, and 60736014, 
and also supported by the External Cooperation 
Program of the Chinese Academy of Sciences. We 
thank Kun Wang, Daniel Dahlmeier, Matthew 
Snover, and Michael Denkowski for their kind as-
sistance. 
References  
Satanjeev Banerjee and Alon Lavie, 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization, pages 65-72, Ann 
Arbor, Michigan, USA. 
Chris Callison-Burch, Cameron Fordyce, Philipp 
Koehn, Christof Monz and Josh Schroeder, 2007. 
(Meta-) Evaluation of Machine Translation. 
Proceedings of the Second Workshop on Statistical 
Machine Translation, pages 136-158, Prague, Czech 
Republic. 
Pi-Chuan Chang, Michel Galley and Christopher D. 
Manning, 2008. Optimizing Chinese Word 
Segmentation for Machine Translation Performance. 
Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 224-232, Columbus, 
Ohio, USA. 
George Doddington, 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram Co-
occurrence Statistics. Proceedings of the Second 
International Conference on Human Language 
Technology Research (HLT'02), pages 138-145, San 
Diego, California, USA. 
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo, 2005. 
A Maximum Entropy Approach to Chinese Word 
Segmentation. Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, pages 
161-164, Jeju Island, Korea. 
I. Dan Melamed, Ryan Green and Joseph P. Turian, 
2003. Precision and Recall of Machine Translation. 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for 
Computational Linguistics (HLT-NAACL 2003) - 
short papers, pages 61-63, Edmonton, Canada. 
Hwee Tou Ng and Jin Kiat Low, 2004. Chinese Part-of-
Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2004), pages 277-
284, Barcelona, Spain. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu, 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, pages 311-318, 
Philadelphia, Pennsylvania, USA. 
Michael Paul, 2008. Overview of the IWSLT 2008 
Evaluation Campaign. Proceedings of IWSLT 2008, 
pages 1-17, Hawaii, USA. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciulla and Ralph Makhoul, 
2006. A Study of Translation Edit Rate with Targeted 
Human Annotation. Proceedings of the Association 
for Machine Translation in the Americas, pages 223-
231, Cambridge. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel 
Jurafsky and Christopher Manning, 2005. A 
Conditional Random Field Word Segmenter for 
Sighan Bakeoff 2005. Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing, pages 168-171, Jeju Island, Korea. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010a. A 
Character-Based Joint Model for Chinese Word 
Segmentation. Proceedings of the 23rd International 
Conference on Computational Linguistics (COLING 
2010), pages 1173-1181, Beijing, China. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010b. A 
Character-Based Joint Model for CIPS-SIGHAN 
Word Segmentation Bakeoff 2010. Proceedings of 
CIPS-SIGHAN Joint Conference on Chinese 
Language Processing (CLP2010), pages 245-248, 
Beijing, China. 
Jia Xu, Richard Zens and Hermann Ney, 2004. Do We 
Need Chinese Word Segmentation for Statistical 
Machine Translation? Proceedings of the ACL 
SIGHAN Workshop 2004, pages 122-128, Barcelona, 
Spain. 
Hua-Ping Zhang, Qun Liu, Xue-Qi Cheng, Hao Zhang 
and Hong-Kui Yu, 2003. Chinese Lexical Analysis 
163
Using Hierarchical Hidden Markov Model. 
Proceedings of the Second SIGHAN Workshop on 
Chinese Language Processing, pages 63-70, Sapporo, 
Japan. 
Ruiqiang Zhang, Keiji Yasuda and Eiichiro Sumita, 
2008. Chinese Word Segmentation and Statistical 
Machine Translation. ACM Transactions on Speech 
and Language Processing, 5 (2). pages 1-19. 
 
 
164
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?21,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Translation Memory into Phrase-Based 
Machine Translation during Decoding 
 
 
Kun Wang?        Chengqing Zong?        Keh-Yih Su? 
?National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
?Behavior Design Corporation, Taiwan 
?
{kunwang, cqzong}@nlpr.ia.ac.cn, 
?
kysu@bdc.com.tw 
 
  
 
Abstract 
Since statistical machine translation (SMT) 
and translation memory (TM) complement 
each other in matched and unmatched regions, 
integrated models are proposed in this paper to 
incorporate TM information into phrase-based 
SMT. Unlike previous multi-stage pipeline 
approaches, which directly merge TM result 
into the final output, the proposed models refer 
to the corresponding TM information associat-
ed with each phrase at SMT decoding. On a 
Chinese?English TM database, our experi-
ments show that the proposed integrated Mod-
el-III is significantly better than either the 
SMT or the TM systems when the fuzzy match 
score is above 0.4. Furthermore, integrated 
Model-III achieves overall 3.48 BLEU points 
improvement and 2.62 TER points reduction 
in comparison with the pure SMT system. Be-
sides, the proposed models also outperform 
previous approaches significantly.  
1 Introduction 
Statistical machine translation (SMT), especially 
the phrase-based model (Koehn et al, 2003), has 
developed very fast in the last decade. For cer-
tain language pairs and special applications, 
SMT output has reached an acceptable level, es-
pecially in the domains where abundant parallel 
corpora are available (He et al, 2010). However, 
SMT is rarely applied to professional translation 
because its output quality is still far from satis-
factory. Especially, there is no guarantee that a 
SMT system can produce translations in a con-
sistent manner (Ma et al, 2011). 
In contrast, translation memory (TM), which 
uses the most similar translation sentence (usual-
ly above a certain fuzzy match threshold) in the 
database as the reference for post-editing, has 
been widely adopted in professional translation 
field for many years (Lagoudaki, 2006). TM is 
very useful for repetitive material such as updat-
ed product manuals, and can give high quality 
and consistent translations when the similarity of 
fuzzy match is high. Therefore, professional 
translators trust TM much more than SMT. 
However, high-similarity fuzzy matches are 
available unless the material is very repetitive. 
In general, for those matched segments1, TM 
provides more reliable results than SMT does. 
One reason is that the results of TM have been 
revised by human according to the global context, 
but SMT only utilizes local context. However, 
for those unmatched segments, SMT is more re-
liable. Since TM and SMT complement each 
other in those matched and unmatched segments, 
the output quality is expected to be raised signif-
icantly if they can be combined to supplement 
each other. 
In recent years, some previous works have in-
corporated TM matched segments into SMT in a 
pipelined manner (Koehn and Senellart, 2010; 
Zhechev and van Genabith, 2010; He et al, 2011; 
Ma et al, 2011). All these pipeline approaches 
translate the sentence in two stages. They first 
determine whether the extracted TM sentence 
pair should be adopted or not. Most of them use 
fuzzy match score as the threshold, but He et al 
(2011) and Ma et al (2011) use a classifier to 
make the judgment. Afterwards, they merge the 
relevant translations of matched segments into 
the source sentence, and then force the SMT sys-
tem to only translate those unmatched segments 
at decoding. 
There are three obvious drawbacks for the 
above pipeline approaches. Firstly, all of them 
determine whether those matched segments 
                                                 
1 We mean ?sub-sentential segments? in this work. 
11
should be adopted or not at sentence level. That 
is, they are either all adopted or all abandoned 
regardless of their individual quality. Secondly, 
as several TM target phrases might be available 
for one given TM source phrase due to insertions, 
the incorrect selection made in the merging stage 
cannot be remedied in the following translation 
stage. For example, there are six possible corre-
sponding TM target phrases for the given TM 
source phrase ???4 ?5 ??6? (as shown in 
Figure 1) such as ?object2 that3 is4 associated5?, 
and ?an1 object2 that3 is4 associated5  with6?, etc. 
And it is hard to tell which one should be adopt-
ed in the merging stage. Thirdly, the pipeline 
approach does not utilize the SMT probabilistic 
information in deciding whether a matched TM 
phrase should be adopted or not, and which tar-
get phrase should be selected when we have mul-
tiple candidates. Therefore, the possible im-
provements resulted from those pipeline ap-
proaches are quite limited. 
On the other hand, instead of directly merging 
TM matched phrases into the source sentence, 
some approaches (Bi?ici and Dymetman, 2008; 
Simard and Isabelle, 2009) simply add the long-
est matched pairs into SMT phrase table, and 
then associate them with a fixed large probability 
value to favor the corresponding TM target 
phrase at SMT decoding. However, since only 
one aligned target phrase will be added for each 
matched source phrase, they share most draw-
backs with the pipeline approaches mentioned 
above and merely achieve similar performance. 
To avoid the drawbacks of the pipeline ap-
proach (mainly due to making a hard decision 
before decoding), we propose several integrated 
models to completely make use of TM infor-
mation during decoding. For each TM source 
phrase, we keep all its possible corresponding 
target phrases (instead of keeping only one of 
them). The integrated models then consider all 
corresponding TM target phrases and SMT pref-
erence during decoding. Therefore, the proposed 
integrated models combine SMT and TM at a 
deep level (versus the surface level at which TM 
result is directly plugged in under previous pipe-
line approaches). 
On a Chinese?English computer technical 
documents TM database, our experiments have 
shown that the proposed Model-III improves the 
translation quality significantly over either the 
pure phrase-based SMT or the TM systems when 
the fuzzy match score is above 0.4. Compared 
with the pure SMT system, the proposed inte-
grated Model-III achieves 3.48 BLEU points im-
provement and 2.62 TER points reduction over-
all. Furthermore, the proposed models signifi-
cantly outperform previous pipeline approaches. 
2 Problem Formulation 
Compared with the standard phrase-based ma-
chine translation model, the translation problem 
is reformulated as follows (only based on the 
best TM, however, it is similar for multiple TM 
sentences): 
  (1) 
Where  is the given source sentence to be trans-
lated,  is the corresponding target sentence and  
is the final translation;  
are the associated information of the best TM 
sentence-pair;  and  denote the corre-
sponding TM sentence pair;  denotes its 
associated fuzzy match score (from 0.0 to 1.0); 
 is the editing operations between  and ; 
and  denotes the word alignment between 
 and . 
Let  and  denote the k-th associated 
source phrase and target phrase, respectively. 
Also,  and  denote the associated source 
phrase sequence and the target phrase sequence, 
respectively (total  phrases without insertion). 
Then the above formula (1) can be decomposed 
as below: 
 
(2) 
Afterwards, for any given source phrase , 
we can find its corresponding TM source phrase 
 and all possible TM target phrases (each 
of them is denoted by ) with the help of 
corresponding editing operations  and word 
alignment . As mentioned above, we can 
have six different possible TM target phrases for 
the TM source phrase ??? 4 ? 5 ?? 6?. This 
??0                    ?1  ??2  ??3  ??4  ?5  ??6  ?7
??0  ?1  ??2  ?3  ??4             ??5  ?6  ??7  ?8
gets0  n1  obj ct2  that3  is4  associated5  with6  the7  annotation8  label9  .10
Source
TM Source
TM Target
 
Figure 1: Phrase Mapping Example 
12
is because there are insertions around the directly 
aligned TM target phrase. 
In the above Equation (2), we first segment the 
given source sentence into various phrases, and 
then translate the sentence based on those source 
phrases. Also,  is replaced by , as they 
are actually the same segmentation sequence. 
Assume that the segmentation probability 
 is a uniform distribution, with the corre-
sponding TM source and target phrases obtained 
above, this problem can be further simplified as 
follows: 
 
(3) 
Where  is the corresponding TM phrase 
matching status for , which is a vector consist-
ing of various indicators (e.g., Target Phrase 
Content Matching Status, etc., to be defined lat-
er), and reflects the quality of the given candi-
date;  is the linking status vector of  (the 
aligned source phrase of  within ), and indi-
cates the matching and linking status in the 
source side (which is closely related to the status 
in the target side); also,  indicates the corre-
sponding TM fuzzy match interval specified later.  
In the second line of Equation (3), we convert 
the fuzzy match score  into its correspond-
ing interval , and incorporate all possible com-
binations of TM target phrases. Afterwards, we 
select the best one in the third line. Last, in the 
fourth line, we introduce the source matching 
status and the target linking status (detailed fea-
tures would be defined later). Since we might 
have several possible TM target phrases , 
the one with the maximum score will be adopted 
during decoding. 
The first factor  in the above for-
mula (3) is just the typical phrase-based SMT 
model, and the second factor  (to be 
specified in the Section 3) is the information de-
rived from the TM sentence pair. Therefore, we 
can still keep the original phrase-based SMT 
model and only pay attention to how to extract 
useful information from the best TM sentence 
pair to guide SMT decoding. 
3 Proposed Models 
Three integrated models are proposed to incorpo-
rate different features as follows: 
3.1 Model-I 
In this simplest model, we only consider Target 
Phrase Content Matching Status (TCM) for . 
For , we consider four different features at the 
same time: Source Phrase Content Matching 
Status (SCM), Number of Linking Neighbors 
(NLN), Source Phrase Length (SPL), and Sen-
tence End Punctuation Indicator (SEP). Those 
features will be defined below.  is 
then specified as: 
 
All features incorporated in this model are speci-
fied as follows: 
TM Fuzzy Match Interval (z): The fuzzy match 
score (FMS) between source sentence  and TM 
source sentence  indicates the reliability of 
the given TM sentence, and is defined as (Sikes, 
2007): 
 
Where  is the word-based 
Levenshtein Distance (Levenshtein, 1966) be-
tween  and . We equally divide FMS into 
ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 
0.9) etc., and the index  specifies the corre-
sponding interval. For example, since the fuzzy 
match score between  and  in Figure 1 is 
0.667, then . 
Target Phrase Content Matching Status 
(TCM): It indicates the content matching status 
between   and , and reflects the quality 
of . Because  is nearly perfect when FMS 
is high, if the similarity between    and  
is high, it implies that the given  is possibly a 
good candidate. It is a member of {Same, High, 
Low, NA (Not-Applicable)}, and is specified as: 
(1) If  is not null: 
(a) if , ; 
(b) else if , ; 
(c) else, ; 
(2) If  is null, ; 
Here  is null means that either there is no 
corresponding TM source phrase  or 
there is no corresponding TM target phrase 
13
 aligned with . In the example of 
Figure 1, assume that the given  is ??? 5  
? 6  ?? 7? and  is ?object that is associated?. 
If  is ?object2 that3 is4 associated5?, 
; if  is ?an1 object2 that3 
is4 associated5?, . 
Source Phrase Content Matching Status 
(SCM): Which indicates the content matching 
status between  and , and it affects 
the matching status of  and  greatly. 
The more similar  is to , the more 
similar   is to . It is a member of {Same, 
High, Low, NA} and is defined as: 
(1) If  is not null: 
(a) if , ; 
(b) else if , 
; 
(c) else, ; 
(2) If  is null, ; 
Here  is null means that there is no corre-
sponding TM source phrase  for the giv-
en source phrase . Take the source phrase  
 ??? 5 ? 6 ?? 7? in Figure 1 for an ex-
ample, since its corresponding  is ??? 4 
? 5 ?? 6?, then . 
Number of Linking Neighbors (NLN): Usually, 
the context of a source phrase would affect its 
target translation. The more similar the context 
are, the more likely that the translations are the 
same. Therefore, this NLN feature reflects the 
number of matched neighbors (words) and it is a 
vector of <x, y>. Where ?x? denotes the number 
of matched source neighbors; and ?y? denotes 
how many those neighbors are also linked to tar-
get words (not null), which also affects the TM 
target phrase selection. This feature is a member 
of {<x, y>: <2, 2>, <2, 1>, <2, 0>, <1, 1>, <1, 0>, 
<0, 0>}. For the source phrase ??? 5 ? 6 ??
7? in Figure 1, the corresponding TM source 
phrase is ??? 4 ? 5 ?? 6? . As only their 
right neighbors ??8? and ??7? are matched, and 
??7? is aligned with ?.10?, NLN will be <1, 1>. 
Source Phrase Length (SPL): Usually the long-
er the source phrase is, the more reliable the TM 
target phrase is. For example, the corresponding 
 for the source phrase with 5 words 
would be more reliable than that with only one 
word. This feature denotes the number of words 
included in , and is a member of {1, 2, 3, 4, 
?5}. For the case ??? 5 ? 6 ?? 7?, SPL will 
be 3.  
Sentence End Punctuation Indicator (SEP): 
Which indicates whether the current phrase is a 
punctuation at the end of the sentence, and is a 
member of {Yes, No}. For example, the SEP for 
??? 5 ? 6 ?? 7? will be ?No?. It is intro-
duced because the SCM and TCM for a sen-
tence-end-punctuation are always ?Same? re-
gardless of other features. Therefore, it is used to 
distinguish this special case from other cases. 
3.2 Model-II 
As Model-I ignores the relationship among vari-
ous possible TM target phrases, we add two fea-
tures TM Candidate Set Status (CSS) and Long-
est TM Candidate Indicator (LTC) to incorporate 
this relationship among them.  Since CSS is re-
dundant after LTC is known, we thus ignore it 
for evaluating TCM probability in the following 
derivation: 
 
The two new features CSS and LTC adopted in 
Model-II are defined as follows: 
TM Candidate Set Status (CSS): Which re-
stricts the possible status of , and is a 
member of {Single, Left-Ext, Right-Ext, Both-Ext, 
NA}. Where ?Single? means that there is only 
one  candidate for the given source 
phrase ; ?Left-Ext? means that there are 
multiple  candidates, and all the candi-
dates are generated by extending only the left 
boundary; ?Right-Ext? means that there are mul-
tiple  candidates, and all the candidates 
are generated by only extending to the right; 
?Both-Ext? means that there are multiple  
candidates, and the candidates are generated by 
extending to both sides; ?NA? means that 
 is null. 
For ??? 4 ? 5 ?? 6? in Figure 1, the 
linked TM target phrase is ?object2 that3 is4 asso-
ciated5?, and there are 5 other candidates by ex-
tending to both sides. Therefore, 
. 
Longest TM Candidate Indicator (LTC): 
Which indicates whether the given  is the 
longest candidate or not, and is a member of 
{Original, Left-Longest, Right-Longest, Both-
Longest, Medium, NA}. Where ?Original? means 
that the given  is the one without exten-
sion; ?Left-Longest? means that the given 
14
 is only extended to the left and is the 
longest one; ?Right-Longest? means that the giv-
en  is only extended to the right and is 
the longest one; ?Both-Longest? means that the 
given  is extended to both sides and is the 
longest one; ?Medium? means that the given 
 has been extended but not the longest 
one; ?NA? means that  is null. 
For  ?object2 that3 is4 associated5? in 
Figure 1, ; for  ?an1 ob-
ject2 that3 is4 associated5?, ; 
for the longest  ?an1 object2 that3 is4 as-
sociated5 with6 the7?, . 
3.3 Model-III 
The abovementioned integrated models ignore 
the reordering information implied by TM. 
Therefore, we add a new feature Target Phrase 
Adjacent Candidate    Relative   Position   
Matching    Status (CPM) into Model-II and 
Model-III is given as: 
 
We assume that CPM is independent with SPL 
and SEP, because the length of source phrase 
would not affect reordering too much and SEP is 
used to distinguish the sentence end punctuation 
with other phrases.  
The new feature CPM adopted in Model-III is 
defined as: 
Target Phrase Adjacent Candidate Relative 
Position Matching Status (CPM): Which indi-
cates the matching status between the relative 
position of 
 
and the relative position of  
. It checks if  are 
positioned in the same order with 
, and reflects the quality of 
ordering the given target candidate . It is a 
member of {Adjacent-Same, Adjacent-Substitute, 
Linked-Interleaved, Linked-Cross, Linked-
Reversed, Skip-Forward, Skip-Cross, Skip-
Reversed, NA}. Recall that 
 
is always right ad-
jacent to , then various cases are defined as 
follows: 
(1) If both  and  are not null: 
(a) If  is on the right of  
and they are also adjacent to each other: 
i. If the right boundary words of  and 
 are the same, and the left 
boundary words of  and  are 
the same, ; 
ii. Otherwise, ; 
(b) If  is on the right of  
but they are not adjacent to each other, 
; 
(c) If  is not on the right of 
: 
i. If there are cross parts between  
and , ; 
ii. Otherwise, ; 
(2) If   is null but  is not null, 
then find the first which is 
not null (  starts from 2)2: 
(a) If  is on the right of , 
; 
(b) If  is not on the right of 
: 
i. If there are cross parts between  
and , ; 
ii. Otherwise, . 
(3) If  is null, . 
In Figure 1, assume that ,  and 
 are ?gets an?, ?object that is associat-
ed with? and ?gets0 an1?, respectively. For 
 ?object2 that3 is4 associated5?, because 
 is on the right of  and they are 
adjacent pair, and both boundary words (?an? 
and ?an1?; ?object? and ?object2?) are matched, 
; for  ?an1 object2 
that3 is4 associated5?, because there are cross 
parts ?an1? between  and , 
. On the other hand, as-
sume that ,  and  are ?gets?, ?ob-
ject that is associated with? and ?gets0?, respec-
tively. For  ?an1 object2 that3 is4 associ-
ated5?, because  and  are adja-
cent pair, but the left boundary words of  and 
 (?object? and ?an1?) are not matched, 
; for  ?object2 
that3 is4 associated5?, because  is on the 
right of  but they are not adjacent pair, 
therefore, . One more 
example, assume that ,  and  are 
?the annotation label?, ?object that is associated 
with? and ?the7 annotation8 label9?, respectively. 
For  ?an1 object2 that3 is4 associated5?, 
because  is on the left of , and 
there are no cross parts, .  
                                                 
2 It can be identified by simply memorizing the index of 
nearest non-null  during search. 
15
4 Experiments 
4.1 Experimental Setup 
Our TM database consists of computer domain 
Chinese-English translation sentence-pairs, 
which contains about 267k sentence-pairs. The 
average length of Chinese sentences is 13.85 
words and that of English sentences is 13.86 
words. We randomly selected a development set 
and a test set, and then the remaining sentence 
pairs are for training set. The detailed corpus sta-
tistics are shown in Table 1. Furthermore, devel-
opment set and test set are divided into various 
intervals according to their best fuzzy match 
scores. Corpus statistics for each interval in the 
test set are shown in Table 2.  
For the phrase-based SMT system, we adopted 
the Moses toolkit (Koehn et al, 2007). The sys-
tem configurations are as follows: GIZA++ (Och 
and Ney, 2003) is used to obtain the bidirectional 
word alignments. Afterwards, ?intersection? 3 
refinement (Koehn et al, 2003) is adopted to ex-
tract phrase-pairs. We use the SRI Language 
Model toolkit (Stolcke, 2002) to train a 5-gram 
model with modified Kneser-Ney smoothing 
(Kneser and Ney, 1995; Chen and Goodman, 
1998) on the target-side (English) training corpus. 
All the feature weights and the weight for each 
probability factor (3 factors for Model-III) are 
tuned on the development set with minimum-
error-rate training (MERT) (Och, 2003). The 
maximum phrase length is set to 7 in our exper-
iments. 
In this work, the translation performance is 
measured with case-insensitive BLEU-4 score 
(Papineni et al, 2002) and TER score (Snover et 
al., 2006). Statistical significance test is conduct-
ed with re-sampling (1,000 times) approach 
(Koehn, 2004) in 95% confidence level. 
4.2 Cross-Fold Translation 
To estimate the probabilities of proposed models, 
the corresponding phrase segmentations for bi-
lingual sentences are required. As we want to 
check what actually happened during decoding in 
the real situation, cross-fold translation is used to 
obtain the corresponding phrase segmentations. 
We first extract 95% of the bilingual sentences as 
a new training corpus to train a SMT system. 
Afterwards, we generate the corresponding 
phrase segmentations for the remaining 5% bi-
                                                 
3 ?grow-diag-final? and ?grow-diag-final-and? are also test-
ed. However, ?intersection? is the best option in our exper-
iments, especially for those high fuzzy match intervals.  
lingual sentences with Forced Decoding (Li et 
al., 2000; Zollmann et al, 2008; Auli et al, 2009; 
Wisniewski et al, 2010), which searches the best 
phrase segmentation for the specified output. 
Having repeated the above steps 20 times4, we 
obtain the corresponding phrase segmentations 
for the SMT training data (which will then be 
used to train the integrated models). 
Due to OOV words and insertion words, not 
all given source sentences can generate the de-
sired results through forced decoding. Fortunate-
ly, in our work, 71.7% of the training bilingual 
sentences can generate the corresponding target 
results. The remaining 28.3% of the sentence 
pairs are thus not adopted for generating training 
samples. Furthermore, more than 90% obtained 
source phrases are observed to be less than 5 
words, which explains why five different quanti-
zation levels are adopted for Source Phrase 
Length (SPL) in section 3.1. 
4.3 Translation Results 
After obtaining all the training samples via cross-
fold translation, we use Factored Language 
Model toolkit (Kirchhoff et al, 2007) to estimate 
the probabilities of integrated models with Wit-
ten-Bell smoothing (Bell et al, 1990; Witten et 
al., 1991) and Back-off method. Afterwards, we 
incorporate the TM information  for 
each  phrase  at  decoding.   All  experiments  are 
                                                 
4  This training process only took about 10 hours on our 
Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of 
RAM).  
  Train Develop Test 
#Sentences 261,906 2,569 2,576 
#Chn. Words 3,623,516 38,585 38,648 
#Chn. VOC. 43,112 3,287 3,460 
#Eng. Words 3,627,028 38,329 38,510 
#Eng. VOC. 44,221 3,993 4,046 
Table 1: Corpus Statistics 
Intervals #Sentences #Words W/S 
[0.9, 1.0) 269 4,468 16.6 
[0.8, 0.9) 362 5,004 13.8 
[0.7, 0.8) 290 4,046 14.0 
[0.6, 0.7) 379 4,998 13.2 
[0.5, 0.6) 472 6,073 12.9 
[0.4, 0.5) 401 5,921 14.8 
[0.3, 0.4) 305 5,499 18.0 
(0.0, 0.3) 98 2,639 26.9 
(0.0, 1.0) 2,576 38,648 15.0 
Table 2: Corpus Statistics for Test-Set 
16
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 
[0.9, 1.0) 81.31 81.38 85.44  * 86.47  *# 89.41  *# 82.79 77.72 82.78 
[0.8, 0.9) 73.25 76.16 79.97  * 80.89  * 84.04  *# 79.74  * 73.00 77.66 
[0.7, 0.8) 63.62 67.71 71.65  * 72.39  * 74.73  *# 71.02  * 66.54 69.78 
[0.6, 0.7) 43.64 54.56 54.88    # 55.88  *# 57.53  *# 53.06 54.00 56.37 
[0.5, 0.6) 27.37 46.32 47.32  *# 47.45  *# 47.54  *# 39.31 46.06 47.73 
[0.4, 0.5) 15.43 37.18 37.25    # 37.60    # 38.18  *# 28.99 36.23 37.93 
[0.3, 0.4) 8.24 29.27 29.52    # 29.38    # 29.15    # 23.58 29.40 30.20 
(0.0, 0.3) 4.13 26.38 25.61    # 25.32    # 25.57    # 18.56 26.30 26.92 
(0.0, 1.0) 40.17 53.03 54.57  *# 55.10  *# 56.51  *# 50.31 51.98 54.32 
Table 3: Translation Results (BLEU%). Scores marked by ?*? are significantly better (p < 0.05) than both TM 
and SMT systems, and those marked by ?#? are significantly better (p < 0.05) than Koehn-10. 
Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 
[0.9, 1.0) 9.79 13.01 9.22      # 8.52    *# 6.77    *# 13.01 18.80 11.90 
[0.8, 0.9) 16.21 16.07 13.12  *# 12.74  *# 10.75  *# 15.27 20.60 14.74 
[0.7, 0.8) 27.79 22.80 19.10  *# 18.58  *# 17.11  *# 21.85 25.33 21.11 
[0.6, 0.7) 46.40 33.38 32.63    # 32.27  *# 29.96  *# 35.93 35.24 31.76 
[0.5, 0.6) 62.59 39.56 38.24  *# 38.77  *# 38.74  *# 47.37 40.24 38.01 
[0.4, 0.5) 73.93 47.19 47.03    # 46.34  *# 46.00  *# 56.84 48.74 46.10 
[0.3, 0.4) 79.86 55.71 55.38    # 55.44    # 55.87    # 64.55 55.93 54.15 
(0.0, 0.3) 85.31 61.76 62.38    # 63.66    # 63.51    # 73.30 63.00 60.67 
(0.0, 1.0) 50.51 35.88 34.34  *# 34.18  *# 33.26  *# 40.75 38.10 34.49 
Table 4: Translation Results (TER%). Scores marked by ?*? are significantly better (p < 0.05) than both TM and 
SMT systems, and those marked by ?#? are significantly better (p < 0.05) than Koehn-10. 
conducted using the Moses phrase-based decoder 
(Koehn et al, 2007). 
Table 3 and 4 give the translation results of 
TM, SMT, and three integrated models in the test 
set. In the tables, the best translation results (ei-
ther in BLEU or TER) at each interval have been 
marked in bold. Scores marked by ?*? are signif-
icantly better (p < 0.05) than both the TM and 
the SMT systems. 
It can be seen that TM significantly exceeds 
SMT at the interval [0.9, 1.0) in TER score, 
which illustrates why professional translators 
prefer TM rather than SMT as their assistant tool. 
Compared with TM and SMT, Model-I is signif-
icantly better than the SMT system in either 
BLEU or TER when the fuzzy match score is 
above 0.7; Model-II significantly outperforms 
both the TM and the SMT systems in either 
BLEU or TER when the fuzzy match score is 
above 0.5; Model-III significantly exceeds both 
the TM and the SMT systems in either BLEU or 
TER when the fuzzy match score is above 0.4. 
All these improvements show that our integrated 
models have combined the strength of both TM 
and SMT.  
However, the improvements from integrated 
models get less when the fuzzy match score de-
creases. For example, Model-III outperforms 
SMT 8.03 BLEU points at interval [0.9, 1.0), 
while the advantage is only 2.97 BLEU points at 
interval [0.6, 0.7). This is because lower fuzzy 
match score means that there are more un-
matched parts between  and ; the output of 
TM is thus less reliable. 
Across all intervals (the last row in the table), 
Model-III not only achieves the best BLEU score 
(56.51), but also gets the best TER score (33.26). 
If intervals are evaluated separately, when the 
fuzzy match score is above 0.4, Model-III out-
performs both Model-II and Model-I in either 
BLEU or TER. Model-II also exceeds Model-I in 
either BLEU or TER. The only exception is at 
interval [0.5, 0.6), in which Model-I achieves the 
best TER score. This might be due to that the 
optimization criterion for MERT is BLEU rather 
than TER in our work. 
4.4 Comparison with Previous Work 
In order to compare our proposed models with 
previous work, we re-implement two XML-
Markup approaches: (Koehn and Senellart, 2010) 
and (Ma et al 2011), which are denoted as 
Koehn-10 and Ma-11, respectively. They are 
selected because they report superior perfor-
mances in the literature. A brief description of 
them is as follows: 
17
Source 
?? 0 ?? 1 ? 2 ?? 3 ?? 4 ?5 internet6 explorer7 ? 8 ?? 9 internet10 ?? 11 ??? 12 
? 13 ? 14 ?? 15 ?16 ?? 17 ? 18 ? 19 ?? 20 ?? 21 ?? 22 ?23 
Reference 
if0 you1 disable2 this3 policy4 setting5 ,6 internet7 explorer8 does9 not10 check11 the12 internet13 
for14 new15 versions16 of17 the18 browser19 ,20 so21 does22 not23 prompt24 users25 to26 install27 
them28 .29 
TM 
Source 
?? 0 ? 1 ?? 2 ? 3 ?? 4 ?? 5 ?6 internet7 explorer8 ? 9 ?? 10 internet11 ?? 12 ??
? 13 ? 14 ? 15 ?? 16 ?17 ?? 18 ? 19 ? 20 ?? 21 ?? 22 ?? 23 ?24 
TM 
Target 
if0 you1 do2 not3 configure4 this5 policy6 setting7 ,8 internet9 explorer10 does11 not12 check13 the14 
internet15 for16 new17 versions18 of19 the20 browser21 ,22 so23 does24 not25 prompt26 users27 to28 
install29 them30 .31 
TM 
Alignment 
0-0 1-3 2-4 3-5 4-6 5-7 6-8 7-9 8-10 9-11 11-15 13-21 14-19 15-17 16-18 17-22 18-23 19-24 
21-26 22-27 23-29 24-31 
SMT 
if you disable this policy setting , internet explorer does not prompt users to install internet for 
new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion] 
Koehn-10 
if you do you disable this policy setting , internet explorer does not check the internet for new 
versions of the browser , so does not prompt users to install them .    [Insert two spurious target 
words] 
Ma-11 
if you disable this policy setting , internet explorer does not prompt users to install internet for 
new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-
tion] 
Model-I 
if you disable this policy setting , internet explorer does not prompt users to install new ver-
sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 
wrong permutation] 
Model-II 
if you disable this policy setting , internet explorer does not prompt users to install new ver-
sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 
wrong permutation] 
Model-III 
if you disable this policy setting , internet explorer does not check the internet for new versions 
of the browser , so does not prompt users to install them .    [Exactly the same as the reference] 
Figure 2: A Translation Example at Interval [0.9, 1.0] (with FMS=0.920) 
Koehn et al (2010) first find out the un-
matched parts between the given source sentence 
and TM source sentence. Afterwards, for each 
unmatched phrase in the TM source sentence, 
they replace its corresponding translation in the 
TM target sentence by the corresponding source 
phrase in the input sentence, and then mark the 
substitution part. After replacing the correspond-
ing translations of all unmatched source phrases 
in the TM target sentence, an XML input sen-
tence (with mixed TM target phrases and marked 
input source phrases) is thus obtained. The SMT 
decoder then only translates the un-
matched/marked source phrases and gets the de-
sired results. Therefore, the inserted parts in the 
TM target sentence are automatically included. 
They use fuzzy match score to determine wheth-
er the current sentence should be marked or not; 
and their experiments show that this method is 
only effective when the fuzzy match score is 
above 0.8. 
Ma et al (2011) think fuzzy match score is not 
reliable and use a discriminative learning method 
to decide whether the current sentence should be 
marked or not. Another difference between Ma-
11 and Koehn-10 is how the XML input is con-
structed. In constructing the XML input sentence, 
Ma-11 replaces each matched source phrase in 
the given source sentence with the corresponding 
TM target phrase. Therefore, the inserted parts in 
the TM target sentence are not included. In Ma?s 
another paper (He et al, 2011), more linguistic 
features for discriminative learning are also add-
ed. In our work, we only re-implement the XML-
Markup method used in (He et al, 2011; Ma et al 
2011), but do not implement the discriminative 
learning method. This is because the features 
adopted in their discriminative learning are com-
plicated and difficult to re-implement. However, 
the proposed Model-III even outperforms the 
upper bound of their methods, which will be dis-
cussed later.  
Table 3 and 4 give the translation results of 
Koehn-10 and Ma-11 (without the discriminator). 
Scores marked by ?#? are significantly better (p 
< 0.05) than Koehn-10. Besides, the upper bound 
of (Ma et al 2011) is also given in the tables, 
which is denoted as Ma-11-U. We calculate this 
18
upper bound according to the method described 
in (Ma et al, 2011). Since He et al, (2011) only 
add more linguistic features to the discriminative 
learning method, the upper bound of (He et al, 
2011) is still the same with (Ma et al, 2011); 
therefore, Ma-11-U applies for both cases. 
It is observed that Model-III significantly ex-
ceeds Koehn-10 at all intervals. More important-
ly, the proposed models achieve much better 
TER score than the TM system does at interval 
[0.9, 1.0), but Koehn-10 does not even exceed 
the TM system at this interval. Furthermore, 
Model-III is much better than Ma-11-U at most 
intervals. Therefore, it can be concluded that the 
proposed models outperform the pipeline ap-
proaches significantly.  
Figure 2 gives an example at interval [0.9, 1.0), 
which shows the difference among different sys-
tem outputs. It can be seen that ?you do? is re-
dundant for Koehn-10, because they are inser-
tions and thus are kept in the XML input. How-
ever, SMT system still inserts another ?you?, 
regardless of ?you do? has already existed. This 
problem does not occur at Ma-11, but it misses 
some words and adopts one wrong permutation. 
Besides, Model-I selects more right words than 
SMT does but still puts them in wrong positions 
due to ignoring TM reordering information. In 
this example, Model-II obtains the same results 
with Model-I because it also lacks reordering 
information. Last, since Model-III considers both 
TM content and TM position information, it 
gives a perfect translation. 
5 Conclusion and Future Work 
Unlike the previous pipeline approaches, which 
directly merge TM phrases into the final transla-
tion result, we integrate TM information of each 
source phrase into the phrase-based SMT at de-
coding. In addition, all possible TM target 
phrases are kept and the proposed models select 
the best one during decoding via referring SMT 
information. Besides, the integrated model con-
siders the probability information of both SMT 
and TM factors. 
The experiments show that the proposed 
Model-III outperforms both the TM and the SMT 
systems significantly (p < 0.05) in either BLEU 
or TER when fuzzy match score is above 0.4. 
Compared with the pure SMT system, Model-III 
achieves overall 3.48 BLEU points improvement 
and 2.62 TER points reduction on a Chinese?
English TM database. Furthermore, Model-III 
significantly exceeds all previous pipeline ap-
proaches. Similar improvements are also ob-
served on the Hansards parts of LDC2004T08 
(not shown in this paper due to space limitation). 
Since no language-dependent feature is adopted, 
the proposed approaches can be easily adapted 
for other language pairs. 
Moreover, following the approaches of 
Koehn-10 and Ma-11 (to give a fair comparison), 
training data for SMT and TM are the same in 
the current experiments. However, the TM is 
expected to play an even more important role 
when the SMT training-set differs from the TM 
database, as additional phrase-pairs that are un-
seen in the SMT phrase table can be extracted 
from TM (which can then be dynamically added 
into the SMT phrase table at decoding time). Our 
another study has shown that the integrated mod-
el would be even more effective when the TM 
database and the SMT training data-set are from 
different corpora in the same domain (not shown 
in this paper). In addition, more source phrases 
can be matched if a set of high-FMS sentences, 
instead of only the sentence with the highest 
FMS, can be extracted and referred at the same 
time. And it could further raise the performance. 
Last, some related approaches (Smith and 
Clark, 2009; Phillips, 2011) combine SMT and 
example-based machine translation (EBMT) 
(Nagao, 1984). It would be also interesting to 
compare our integrated approach with that of 
theirs. 
 
Acknowledgments 
 
The research work has been funded by the Hi-
Tech Research and Development Program 
(?863? Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501.  
The authors would like to thank the anony-
mous reviewers for their insightful comments 
and suggestions. Our sincere thanks are also ex-
tended to Dr. Yanjun Ma and Dr. Yifan He for 
their valuable discussions during this study.  
References  
Michael Auli, Adam Lopez, Hieu Hoang and Philipp 
Koehn, 2009. A systematic analysis of translation 
model search spaces. In Proceedings of the Fourth 
Workshop on Statistical Machine Translation, pag-
es 224?232. 
19
Timothy C. Bell, J.G. Cleary and Ian H. Witten, 1990. 
Text compression: Prentice Hall, Englewood Cliffs, 
NJ. 
Ergun Bi?ici and Marc Dymetman. 2008. Dynamic 
translation memory: using statistical machine trans-
lation to improve translation memory fuzzy match-
es. In Proceedings of the 9th International Confer-
ence on Intelligent Text Processing and Computa-
tional Linguistics (CICLing 2008), pages 454?465. 
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language 
modeling. Technical Report TR-10-98, Harvard 
University Center for Research in Computing 
Technology. 
Yifan He, Yanjun Ma, Josef van Genabith and Andy 
Way, 2010. Bridging SMT and TM with transla-
tion recommendation. In Proceedings of the 48th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 622?630. 
Yifan He, Yanjun Ma, Andy Way and Josef van 
Genabith. 2011. Rich linguistic features for transla-
tion memory-inspired consistent translation. In 
Proceedings of the Thirteenth Machine Translation 
Summit, pages 456?463. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech and Signal Processing, pages 
181?184. 
Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh. 
2007. Factored language models tutorial. Technical 
report, Department of Electrical Engineering, Uni-
versity of Washington, Seattle, Washington, USA.  
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in 
Natural Language Processing (EMNLP), pages 
388?395, Barcelona, Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer and Ond?ej Bojar. 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proceedings of the ACL 2007 Demo 
and Poster Sessions, pages 177?180. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pages 48?54. 
Philipp Koehn and Jean Senellart. 2010. Convergence 
of translation memory and statistical machine 
translation. In AMTA Workshop on MT Research 
and the Translation Industry, pages 21?31. 
Elina Lagoudaki. 2006. Translation memories survey 
2006: Users? perceptions around tm use. In Pro-
ceedings of the ASLIB International Conference 
Translating and the Computer 28, pages 1?29. 
Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-Hui 
Lee. 2000. Automatic verbal information verifica-
tion for user authentication. IEEE transactions on 
speech and audio processing, Vol. 8, No. 5, pages 
1063?6676. 
Vladimir Iosifovich Levenshtein. 1966. Binary codes 
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10 (8). pages 707?
710. 
Yanjun Ma, Yifan He, Andy Way and Josef van 
Genabith. 2011. Consistent translation using dis-
criminative learning: a translation memory-inspired 
approach. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1239?1248, Portland, Oregon. 
Makoto Nagao, 1984. A framework of a mechanical 
translation between Japanese and English by anal-
ogy principle. In: Banerji, Alick Elithorn and  Ran-
an (ed). Artifiical and Human Intelligence: Edited 
Review Papers Presented at the International 
NATO Symposium on Artificial and Human Intelli-
gence. North-Holland, Amsterdam, 173?180. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160?167. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29 (1). pages 
19?51. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL), pages 311?
318. 
Aaron B. Phillips, 2011. Cunei: open-source machine 
translation with relevance-based models of each 
translation instance. Machine Translation, 25 (2). 
pages 166-177. 
Richard Sikes. 2007, Fuzzy matching in theory and 
practice. Multilingual, 18(6):39?43. 
Michel Simard and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted 
translation environment. In Proceedings of the 
Twelfth Machine Translation Summit (MT Summit 
XII), pages 120?127. 
James Smith and Stephen Clark. 2009. EBMT for 
SMT: a new EBMT-SMT hybrid. In Proceedings 
of the 3rd International Workshop on Example-
20
Based Machine Translation (EBMT'09), pages 3?
10, Dublin, Ireland. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
study of translation edit rate with targeted human 
annotation. In Proceedings of Association for Ma-
chine Translation in the Americas (AMTA-2006), 
pages 223?231. 
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 311?318. 
Guillaume Wisniewski, Alexandre Allauzen and 
Fran?ois Yvon, 2010. Assessing phrase-based 
translation models with oracle decoding. In Pro-
ceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
933?943. 
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: estimating the probabilities of 
novel events in adaptive test compression. IEEE 
Transactions on Information Theory, 37(4): 1085?
1094, July. 
Ventsislav Zhechev and Josef van Genabith. 2010. 
Seeding statistical machine translation with transla-
tion memory output through tree-based structural 
alignment. In Proceedings of the 4th Workshop on 
Syntax and Structure in Statistical Translation, 
pages 43?51. 
Andreas Zollmann, Ashish Venugopal, Franz Josef 
Och and Jay Ponte, 2008. A systematic comparison 
of phrase-based, hierarchical and syntax-
augmented statistical MT. In Proceedings of the 
22nd International Conference on Computational 
Linguistics (Coling 2008), pages 1145?1152. 
 
 
21
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Handling Ambiguities of Bilingual Predicate-Argument Structures for 
Statistical Machine Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Predicate-argument structure (PAS) has been 
demonstrated to be very effective in improving 
SMT performance. However, since a source-
side PAS might correspond to multiple differ-
ent target-side PASs, there usually exist many 
PAS ambiguities during translation. In this pa-
per, we group PAS ambiguities into two types: 
role ambiguity and gap ambiguity. Then we 
propose two novel methods to handle the two 
PAS ambiguities for SMT accordingly: 1) in-
side context integration; 2) a novel maximum 
entropy PAS disambiguation (MEPD) model. 
In this way, we incorporate rich context in-
formation of PAS for disambiguation. Then 
we integrate the two methods into a PAS-
based translation framework. Experiments 
show that our approach helps to achieve sig-
nificant improvements on translation quality. 
1 Introduction 
Predicate-argument structure (PAS) depicts the 
relationship between a predicate and its associat-
ed arguments, which indicates the skeleton struc-
ture of a sentence on semantic level. Basically, 
PAS agrees much better between two languages 
than syntax structure (Fung et al, 2006; Wu and 
Fung, 2009b). Considering that current syntax-
based translation models are always impaired by 
cross-lingual structure divergence (Eisner, 2003; 
Zhang et al, 2010), PAS is really a better repre-
sentation of a sentence pair to model the bilin-
gual structure mapping. 
However, since a source-side PAS might 
correspond to multiple different target-side PASs, 
there usually exist many PAS ambiguities during 
translation. For example, in Figure 1, (a) and (b) 
carry the same source-side PAS <[A0]1 
[Pred(?)]2 [A1]3> for Chinese predicate ???. 
However, in Figure 1(a), the corresponding 
target-side-like PAS is <[X1] [X2] [X3]>, while in 
Figure 1(b), the counterpart target-side-like PAS1 
is <[X2] [X3] [X1]>. This is because the two 
PASs play different roles in their corresponding 
sentences. Actually, Figure 1(a) is an independ-
ent PAS, while Figure 1(b) is a modifier of the 
noun phrase ??? ? ????. We call this kind 
of PAS ambiguity role ambiguity. 
??  ?  ??? ?? ???
[           A0         ]1 [     A1    ]3[Pred]2
?
being , should  ?two major countries
[           X3            ][X2]
China and Russia
[          X1           ]
? ?
?? ?? ? ???
[ A0 ]1 [          A1         ]3[Pred]2
flood  prevention is the  primary  mission
[           X1          ] [ X2 ] [              X3              ]
??? ? ?? ? ??? ? ? ? ?
[      A0      ]1 [    A1   ]3[Pred]2
the location of the olympic village for athletesis the best
[     X3    ][X2][                    X1                     ]
(a)
(c)
(b)
 
Figure 1. An example of ambiguous PASs. 
Meanwhile, Figure 1 also depicts another kind 
of PAS ambiguity. From Figure 1, we can see 
that (a) and (c) get the same source-side PAS and 
target-side-like PAS. However, they are different 
because in Figure 1(c), there is a gap string ?? 
???? between [A0] and [Pred]. Generally, the 
gap strings are due to the low recall of automatic 
semantic role labeling (SRL) or complex sen-
tence structures. For example, in Figure 1(c), the 
gap string ?? ???? is actually an argument 
?AM-PRP? of the PAS, but the SRL system has 
                                                 
1We use target-side-like PAS to refer to a list of general 
non-terminals in target language order, where a non-
terminal aligns to a source argument. 
1127
ignored it. We call this kind of PAS ambiguity 
gap ambiguity. 
During translation, these PAS ambiguities will 
greatly affect the PAS-based translation models. 
Therefore, in order to incorporate the bilingual 
PAS into machine translation effectively, we 
need to decide which target-side-like PAS should 
be chosen for a specific source-side PAS. We 
call this task PAS disambiguation. 
In this paper, we propose two novel methods 
to incorporate rich context information to handle 
PAS ambiguities. Towards the gap ambiguity, 
we adopt a method called inside context 
integration to extend PAS to IC-PAS. In terms of 
IC-PAS, the gap strings are combined effectively 
to deal with the gap ambiguities. As to the role 
ambiguity, we design a novel maximum entropy 
PAS disambiguation (MEPD) model to combine 
various context features, such as context words 
of PAS. For each ambiguous source-side PAS, 
we build a specific MEPD model to select 
appropriate target-side-like PAS for translation. 
We will detail the two methods in Section 3 and 
4 respectively. 
Finally, we integrate the above two methods 
into a PAS-based translation framework (Zhai et 
al. 2012). Experiments show that the two PAS 
disambiguation methods significantly improve 
the baseline translation system. The main 
contribution of this work can be concluded as 
follows: 
1) We define two kinds of PAS ambiguities: 
role ambiguity and gap ambiguity. To our 
best knowledge, we are the first to handle 
these PAS ambiguities for SMT. 
2) Towards the two different ambiguities, we 
design two specific methods for PAS 
disambiguation: inside context integration 
and the novel MEPD model.  
2 PAS-based Translation Framework 
PAS-based translation framework is to perform 
translation based on PAS transformation (Zhai et 
al., 2012). In the framework, a source-side PAS 
is first converted into target-side-like PASs by 
PAS transformation rules, and then perform 
translation based on the obtained target-side-like 
PASs. 
2.1 PAS Transformation Rules 
PAS transformation rules (PASTR) are used to 
convert a source-side PAS into a target one. 
Formally, a PASTR is a triple <Pred, SP, TP>: 
? Pred means the predicate where the rule is 
extracted. 
? SP denotes the list of source elements in 
source language order. 
? TP refers to the target-side-like PAS, i.e., a 
list of general non-terminals in target 
language order. 
For example, Figure 2 shows the PASTR 
extracted from Figure 1(a). In this PASTR, Pred 
is Chinese verb ???, SP is the source element 
list <[A0]1 [Pred]2 [A1]3>, and TP is the list of 
non-terminals <X1 X2 X3>. The same subscript in 
SP and TP means a one-to-one mapping between 
a source element and a target non-terminal. Here, 
we utilize the source element to refer to the 
predicate or argument of the source-side PAS. 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
Figure 2. An example PASTR. 
2.2 PAS Decoding 
The PAS decoding process is divided into 3 steps: 
(1) PAS acquisition: perform semantic role 
labeling (SRL) on the input sentences to achieve 
their PASs, i.e., source-side PASs; 
(2) Transformation: use the PASTR to match 
the source-side PAS i.e., the predicate Pred and 
the source element list SP. Then by the matching 
PASTRs, transform source-side PASs to target-
side-like PASs. 
(3) Translation: in this step, the decoder first 
translates each source element respectively, and 
then a CKY-style decoding algorithm is adopted 
to combine the translation of each element and 
get the final translation of the PAS.  
2.3 Sentence Decoding with the PAS-based 
translation framework 
Sometimes, the source sentence cannot be fully 
covered by the PAS, especially when there are 
several predicates. Thus to translate the whole 
sentence, Zhai et al (2012) further designed an 
algorithm to decode the entire sentence.  
In the algorithm, they organized the space of 
translation candidates into a hypergraph. For the 
span covered by PAS (PAS span), a multiple-
branch hyperedge is employed to connect it to 
the PAS?s elements. For the span not covered by 
PAS (non-PAS span), the decoder considers all 
the possible binary segmentations of it and uti-
lizes binary hyperedges to link them. 
1128
During translation, the decoder fills the spans 
with translation candidates in a bottom-up man-
ner. For the PAS span, the PAS-based translation 
framework is adopted. Otherwise, the BTG sys-
tem (Xiong et al, 2006) is used. When the span 
covers the whole sentence, we get the final trans-
lation result. 
 
Obviously, PAS ambiguities are not 
considered in this framework at all. The target-
side-like PAS is selected only according to the 
language model and translation probabilities, 
without considering any context information of 
PAS. Consequently, it would be difficult for the 
decoder to distinguish the source-side PAS from 
different context. This harms the translation 
quality. Thus to overcome this problem, we de-
sign two novel methods to cope with the PAS 
ambiguities: inside-context integration and a 
maximum entropy PAS disambiguation (MEPD) 
model. They will be detailed in the next two sec-
tions. 
3 Inside Context Integration 
In this section, we integrate the inside context of 
the PAS into PASTRs to do PAS disambiguation. 
Basically, a PAS consists of several elements (a 
predicate and several arguments), which are ac-
tually a series of continuous spans. For a specific 
PAS <E1,?, En>, such as the source-side PAS 
<[A0][Pred][A1]> in Figure 2, its controlled range 
is defined as: 
( ) { ( ), [1, ]}irange PAS s E i n= ? ?  
where s(Ei) denotes the span of element Ei. Fur-
ther, we define the closure range of a PAS. It 
refers to the shortest continuous span covered by 
the entire PAS: 
0( ) ( )
_ min , max
nj s E j s E
closure range j j
? ?
? ?= ? ?? ?
 
Here, E0 and En are the leftmost and rightmost 
element of the PAS respectively. The closure 
range is introduced here because adjacent source 
elements in a PAS are usually separated by gap 
strings in the sentence. We call these gap strings 
the inside context (IC) of the PAS, which satisfy: 
_ ( ) ( ( ) ( ) )closure range PAS IC PAS range PAS= ? ?  
The operator ?  takes a list of neighboring spans 
as input2, and returns their combined continuous 
span. As an example, towards the PAS ?<[A0] 
[Pred][A1]>? (the one for Chinese predicate ??
(shi)?) in Figure 3, its controlled range is 
{[3,5],[8,8],[9,11]} and its closure range is [3,11]. 
The IC of the PAS is thus {[6,7]}. 
To consider the PAS?s IC during PAS trans-
formation process, we incorporate its IC into the 
extracted PASTR. For each gap string in IC, we 
abstract it by the sequence of highest node cate-
gories (named as s-tag sequence). The s-tag se-
quence dominates the corresponding syntactic 
tree fragments in the parse tree. For example, in 
Figure 3, the s-tag sequence for span [6,8] is ?PP 
VC?. Thus, the sequence for the IC (span [6,7]) 
in Figure 3 is ?PP?. We combine the s-tag se-
quences with elements of the PAS in order. The 
resulting PAS is called IC-PAS, just like the left 
side of Figure 4(b) shows. 
[           A0           ] [        PP        ]
???3 ???7 ?8 ?10
de wei-zhiao-yun-cun
??5?4 ?6
dui yun-dong-yuan shi
?9 ?11
zui hao de
NN DEC NN
NP
P NN
PP
VC AD VA DEC
CP
VP
IP
??1
VV
biao-shi
VP
,2
PU
?0
PN
ta
?
PU
IP
DNP
[Pred] [      A1     ]  
Figure 3. The illustration of inside context (IC). The 
subscript in each word refers to its position in sen-
tence. 
Differently, Zhai et al (2012) attached the IC 
to its neighboring elements based on parse trees. 
For example, in Figure 3, they would attach the 
gap string ??(dui) ???(yun-dong-yuan)? to the 
PAS?s element ?Pred?, and then the span of 
?Pred? would become [6,8]. Consequently, the 
span [6,8] will be translated as a whole source 
element in the decoder. This results in a bad 
translation because the gap string ??(dui) ???
(yun-dong-yuan)? and predicate ??(shi)? should 
be translated separately, just as Figure 4(a) 
shows. Therefore, we can see that the attachment 
decision in (Zhai et al, 2012) is sometimes un-
reasonable and the IC also cannot be used for 
PAS disambiguation at all. In contrast, our meth-
                                                 
2 Here, two spans are neighboring means that the beginning 
of the latter span is the former span?s subsequent word in 
the sentence. For example, span [3,6] and [7,10] are neigh-
boring spans. 
1129
od of inside context integration is much flexible 
and beneficial for PAS disambiguation. 
(a)
(b)
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
??? ??? ? ?
[            A0            ]1 [      A1     ]4[Pred]3
[the location of the olympic village]1 [for athletes]2[is]3 [the best]4
[         PP         ]2
de wei-zhiao-yun-cun
??? ?
dui yun-dong-yuan shi
? ?
zui hao de
 
Figure 4. Example of IC-PASTR. (a) The aligned 
span of each element of the PAS in Figure 3; (b) The 
extracted IC-PASTR from (a). 
Using the IC-PASs, we look for the aligned 
target span for each element of the IC-PAS. We 
demand that every element and its corresponding 
target span must be consistent with word align-
ment. Otherwise, we discard the IC-PAS. After-
wards, we can easily extract a rule for PAS trans-
formation, which we call IC-PASTR. As an ex-
ample, Figure 4(b) is the extracted IC-PASTR 
from Figure 4(a). 
Note that we only apply the source-side PAS 
and word alignment for IC-PASTR extraction. 
By contrast, Zhai et al (2012) utilized the result 
of bilingual SRL (Zhuang and Zong, 2010b). 
Generally, bilingual SRL could give a better 
alignment between bilingual elements. However, 
bilingual SRL usually achieves a really low re-
call on PASs, about 226,968 entries in our train-
ing set while it is 882,702 by using monolingual 
SRL system. Thus to get a high recall for PASs, 
we only utilize word alignment instead of captur-
ing the relation between bilingual elements. In 
addition, to guarantee the accuracy of IC-
PASTRs, we only retain rules with more than 5 
occurrences. 
4 Maximum Entropy PAS Disambigua-
tion (MEPD) Model 
In order to handle the role ambiguities, in this 
section, we concentrate on utilizing a maximum 
entropy model to incorporate the context infor-
mation for PAS disambiguation. Actually, the 
disambiguation problem can be considered as a 
multi-class classification task. That is to say, for 
a source-side PAS, every corresponding target-
side-like PAS can be considered as a label. For 
example, in Figure 1, for the source-side PAS 
?[A0]1[Pred]2[A1]3?, the target-side-like PAS 
?[X1] [X2] [X3]? in Figure 1(a) is thus a label and 
?[X2] [X3] [X1]? in Figure 1(b) is another label of 
this classification problem. 
The maximum entropy model is the classical 
way to handle this problem: 
exp( ( , , ( ), ( )))
( | , ( ), ( ))
exp( ( , , ( ), ( )))
i i i
tp i i i
h sp tp c sp c tp
P tp sp c sp c tp
h sp tp c sp c tp?
?
??
= ?
? ?
 
where sp and tp refer to the source-side PAS (not 
including the predicate) and the target-side-like 
PAS respectively. c(sp) and c(tp) denote the sur-
rounding context of sp and tp. hi is a binary fea-
ture function and ?i is the weight of hi. 
We train a maximum entropy classifier for 
each sp via the off-the-shelf MaxEnt toolkit 3 . 
Note that to avoid sparseness, sp does not in-
clude predicate of the PAS. Practically, the pred-
icate serves as a feature of the MEPD model. As 
an example, for the rule illustrated in Figure 4(b), 
we build a MEPD model for its source element 
list sp <[A0] [PP] [Pred] [A1]>, and integrate the 
predicate ??(shi)? into the MEPD model as a 
feature. 
In detail, we design a list of features for each 
pair <sp, tp> as follows: 
?   Lexical Features. These features include 
the words immediately to the left and right of sp, 
represented as w-1 and w+1. Moreover, the head 
word of each argument also serves as a lexical 
feature, named as hw(Ei). For example, Figure 3 
shows the context of the IC-PASTR in Figure 
4(b), and the extracted lexical features of the in-
stance are: w-1=? , w+1=? , hw([A0]1)=??
(wei-zhi), hw([A1]4)=?(hao). 
?   POS Features. These features are defined 
as the POS tags of the lexical features, p-1, p+1 
and phw(Ei) respectively. Thus, the correspond-
ing POS features of Figure 4 (b) are: p-1=PU, 
p+1=PU, phw([A0]1)=NN, phw([A1]4)=VA. 
?   Predicate Feature. It is the pair of source 
predicate and its corresponding target predicate. 
For example, in Figure 4(b), the source and tar-
get predicate are ??(shi)? and ?is? respectively. 
The predicate feature is thus ?PredF=?(shi)+is?. 
The target predicate is determined by: 
_ ( )
- arg max ( | - )j
j t range PAS
t pred p t s pred
?
=  
where s-pred is the source predicate and t-pred 
is the corresponding target predicate. 
                                                 
3http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm
l 
1130
t_range(PAS) refers to the target range covering 
all the words that are reachable from the PAS via 
word alignment.  tj refers to the jth word in 
t_range(PAS). The utilized lexical translation 
probabilities are from the toolkit in Moses 
(Koehn et al, 2007). 
?   Syntax Features. These features include 
st(Ei), i.e., the highest syntax tag for each argu-
ment, and fst(PAS) which is the lowest father 
node of sp in the parse tree. For example, for the 
rule shown in Figure 4(b), syntax features are 
st([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IP 
respectively.  
Using these features, we can train the MEPD 
model. We set the Gaussian prior to 1.0 and per-
form 100 iterations of the L-BFGS algorithm for 
each MEPD model. At last, we build 160 and 
215 different MEPD classifiers, respectively, for 
the PASTRs and IC-PASTRs. Note that since the 
training procedure of maximum entropy classifi-
er is really fast, it does not take much time to 
train these classifiers. 
5 Integrating into the PAS-based Trans-
lation Framework 
In this section, we integrate our method of PAS 
disambiguation into the PAS-based translation 
framework when translating each test sentence. 
For inside context integration, since the format 
of IC-PASTR is the same to PASTR4, we can 
use the IC-PASTR to substitute PASTR for 
building a PAS-based translation system directly. 
We use ?IC-PASTR? to denote this system. In 
addition, since our method of rule extraction is 
different from (Zhai et al, 2012), we also use 
PASTR to construct a translation system as the 
baseline system, which we call ?PASTR?. 
On the basis of PASTR and IC-PASTR, we 
further integrate our MEPD model into transla-
tion. Specifically, we take the score of the MEPD 
model as another informative feature for the de-
coder to distinguish good target-side-like PASs 
from bad ones. The weights of the MEPD feature 
can be tuned by MERT (Och, 2003) together 
with other translation features, such as language 
model. 
6 Related Work 
The method of PAS disambiguation for SMT is 
relevant to the previous work on context depend-
                                                 
4 The only difference between IC-PASTR and PASTR is 
that there are many syntactic labels in IC-PASTRs.  
ent translation. 
Carpuat and Wu (2007a, 2007b) and Chan et 
al. (2007) have integrated word sense disambig-
uation (WSD) and phrase sense disambiguation 
(PSD) into SMT systems. They combine rich 
context information to do disambiguation for 
words or phrases, and achieve improved transla-
tion performance. 
Differently, He et al (2008), Liu et al (2008) 
and Cui et al (2010) designed maximum entropy 
(ME) classifiers to do better rule section for hier-
archical phrase-based model and tree-to-string 
model respectively. By incorporating the rich 
context information as features, they chose better 
rules for translation and yielded stable improve-
ments on translation quality. 
Our work differs from the above work in the 
following two aspects: 1) in our work, we focus 
on the problem of disambiguates on PAS; 2) we 
define two kinds of PAS ambiguities: role 
ambiguity and gap ambiguity. 3) towards the two 
different ambiguities, we design two specific 
methods for PAS disambiguation: inside context 
integration and the novel MEPD model. 
In addition, Xiong et al (2012) proposed an 
argument reordering model to predict the relative 
position between predicates and arguments. They 
also combine the context information in the 
model. But they only focus on the relation be-
tween the predicate and a specific argument, ra-
ther than the entire PAS. Different from their 
work, we incorporate the context information to 
do PAS disambiguation based on the entire PAS. 
This is very beneficial for global reordering dur-
ing translation (Zhai et al, 2012). 
7 Experiment 
7.1 Experimental Setup  
We perform Chinese-to-English translation to 
demonstrate the effectiveness of our PAS disam-
biguation method. The training data contains 
about 260K sentence pairs5. To get accurate SRL 
results, we ensure that the length of each sen-
tence in the training data is among 10 and 30 
words. We run GIZA++ and then employ the 
grow-diag-final-and (gdfa) strategy to produce 
symmetric word alignments. The development 
set and test set come from the NIST evaluation 
test data (from 2003 to 2005). Similar to the 
training set, we also only retain the sentences 
                                                 
5 It is extracted from the LDC corpus. The LDC category 
number : LDC2000T50, LDC2002E18, LDC2003E07, 
LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 
and LDC2005T34. 
1131
whose lengths are among 10 and 30 words. Fi-
nally, the development set includes 595 sentenc-
es from NIST MT03 and the test set contains 
1,786 sentences from NIST MT04 and MT05. 
We train a 5-gram language model with the 
Xinhua portion of English Gigaword corpus and 
target part of the training data. The translation 
quality is evaluated by case-insensitive BLEU-4 
with shortest length penalty. The statistical sig-
nificance test is performed by the re-sampling 
approach (Koehn, 2004). 
We perform SRL on the source part of the 
training set, development set and test set by the 
Chinese SRL system used in (Zhuang and Zong, 
2010b). To relieve the negative effect of SRL 
errors, we get the multiple SRL results by 
providing the SRL system with 3-best parse trees 
of Berkeley parser (Petrov and Klein, 2007), 1-
best parse tree of Bikel parser (Bikel, 2004) and 
Stanford parser (Klein and Manning, 2003). 
Therefore, at last, we can get 5 SRL result for 
each sentence. For the training set, we use these 
SRL results to do rule extraction respectively. 
We combine the obtained rules together to get a 
combined rule set. We discard the rules with 
fewer than 5 appearances. Using this set, we can 
train our MEPD model directly. 
As to translation, we match the 5 SRL results 
with transformation rules respectively, and then 
apply the resulting target-side-like PASs for de-
coding. As we mentioned in section 2.3, we use 
the state-of-the-art BTG system to translate the 
non-PAS spans. 
source-side PAS counts number of classes 
[A0] [Pred(?)] [A1] 245 6 
[A0] [Pred(?)] [A1] 148 6 
[A0] [AM-ADV] [Pred(?)] [A1] 68 20 
[A0] [Pred(??)] [A1] 66 6 
[A0] [Pred(?)] [A1] 42 6 
[A0] [Pred(??)] [A1] 32 4 
[A0] [AM-ADV] [Pred(?)] [A1] 32 19 
[A0] [Pred(??)] [A1] 29 4 
[AM-ADV] [Pred(?)] [A1] 26 6 
[A2] [Pred(?)] [A1] 16 5 
Table 1. The top 10 frequent source-side PASs in the 
dev and test set. 
7.2 Ambiguities in Source-side PASs 
We first give Table 1 to show some examples of 
role ambiguity. In the table, for instance, the se-
cond line denotes that the source-side PAS ?[A0] 
[Pred(?)] [A1]? appears 148 times in the devel-
opment and test set al together, and it corre-
sponds to 6 different target-side-like PASs in the 
training set. 
As we can see from Table 1, all the top 10 
PASs correspond to several different target-side-
like PASs. Moreover, according to our statistics, 
among all PASs appearing in the development 
set and test set, 56.7% of them carry gap strings. 
These statistics demonstrate the importance of 
handling the role ambiguity and gap ambiguity in 
the PAS-based translation framework. Therefore, 
we believe that our PAS disambiguation method 
would be helpful for translation. 
7.3 Translation Result  
We compare the translation result using PASTR, 
IC-PASTR and our MEPD model in this section. 
The final translation results are shown in Table 2. 
As we can see, after employing PAS for transla-
tion, all systems outperform the baseline BTG 
system significantly. This comparison verifies 
the conclusion of (Zhai et al, 2012) and thus also 
demonstrates the effectiveness of PAS. 
MT system Test set 
n-gram precision 
1 2 3 4 
BTG 32.75 74.39 41.91 24.75 14.91 
PASTR 33.24* 75.28 42.62 25.18 15.10 
PASTR+MEPD 33.78* 75.32 43.08 25.75 15.58 
IC-PASTR 33.95*# 75.62 43.36 25.92 15.58 
IC-PASTR+MEPD 34.19*# 75.66 43.40 26.15 15.92 
Table 2. Result of baseline system and the MT sys-
tems using our PAS-based disambiguation method. 
The ?*? and ?#? denote that the result is significantly 
better than BTG and PASTR respectively (p<0.01).  
Specifically, after integrating the inside con-
text information of PAS into transformation, we 
can see that system IC-PASTR significantly out-
performs system PASTR by 0.71 BLEU points. 
Moreover, after we import the MEPD model into 
system PASTR, we get a significant improve-
ment over PASTR (by 0.54 BLEU points). These 
comparisons indicate that both the inside context 
integration and our MEPD model are beneficial 
for the decoder to choose better target-side-like 
PAS for translation. 
On the basis of IC-PASTR, we further add our 
MEPD model into translation and get system IC-
PASTR+MEPD. We can see that this system 
further achieves a remarkable improvement over 
system PASTR (0.95 BLEU points).  
However, from Table 2, we find that system 
IC-PASTR+MEPD only outperforms system IC-
PASTR slightly (0.24 BLEU points). The result 
seems to show that our MEPD model is not such 
1132
useful after using IC-PASTR. We will explore 
the reason in section 7.5. 
7.4 Effectiveness of Inside Context Integra-
tion 
The method of inside context integration is used 
to combine the inside context (gap strings) into 
PAS for translation, i.e., extend the PASTR to 
IC-PASTR. In order to demonstrate the effec-
tiveness of inside context integration, we first 
give Table 3, which illustrates statistics on the 
matching PASs. The statistics are conducted on 
the combination of development set and test set. 
Transformation 
Rules 
Matching PAS 
None Gap PAS Gap PAS Total 
PASTR 1702 1539 3241 
IC-PASTR 1546 832 2378 
Table 3. Statistics on the matching PAS. 
In Table 3, for example, the line for PASTR 
means that if we use PASTR for the combined 
set, 3241 PASs (column ?Total?) can match 
PASTRs in total. Among these matching PASs, 
1539 ones (column ?Gap PAS?) carry gap strings, 
while 1702 ones do not (column ?None Gap 
PAS?). Consequently, since PASTR does not 
consider the inside context during translation, the 
Gap PASs, which account for 47% (1539/3241) 
of all matching PASs, might be handled inappro-
priately in the PAS-based translation framework. 
Therefore, integrating the inside context into 
PASTRs, i.e., using the proposed IC-PASTRs, 
would be helpful for translation. The translation 
result shown in Table 2 also demonstrates this 
conclusion. 
(a) reference
(c) translation result using IC-PASTR
[for economic recovery , especially of investment confidence is]
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[ a good sign ] [ for economic recovery , especially of investment confidence ]this is
? ? ? ? ??? ?? ?? ? ??? ?? ?? ??  ? 
[a good sign]this
(b) translation result using PASTR
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[a good sign]this is [for economic recovery and the restoration of investors ' confidence]
[  A0  ] [                            Pred                             ] [      A1      ]
 
Figure 5. Translation examples to verify the effec-
tiveness of inside context.  
From Table 3, we can also find that the num-
ber of matching PASs decreases after using IC-
PASTR. This is because IC-PASTR is more spe-
cific than PASTR. Therefore, for a PAS with 
specific inside context (gap strings), even if the 
matched PASTR is available, the matched IC-
PASTR might not. This indicates that comparing 
with PASTR, IC-PASTR is more capable of dis-
tinguishing different PASs. Therefore, based on 
this advantage, although the number of matching 
PASs decreases, IC-PASTR still improves the 
translation system using PASTR significantly. Of 
course, we believe that it is also possible to inte-
grate the inside context without decreasing the 
number of matching PASs and we plan this as 
our future work. 
We further give a translation example in Fig-
ure 5 to illustrate the effectiveness of our inside 
context integration method. In the example, the  
automatic SRL system ignores the long preposi-
tion phrase ?? ???? ???? ?? ???
?? for the PAS. Thus, the system using PASTRs 
can only attach the long phrase to the predicate 
??? according to the parse tree, and meanwhile, 
make use of a transformation rule as follows: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
In this way, the translation result is very bad, just 
as Figure 5(b) shows. The long preposition 
phrases are wrongly positioned in the translation. 
In contrast, after inside context integration, we 
match the inside context during PAS transfor-
mation. As Figure 5(c) shows, the inside context 
helps to selects a right transformation rule as fol-
lows and gets a good translation result finally. 
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
 
7.5 Effectiveness of the MEPD Model 
The MEPD model incorporates various context 
features to select better target-side-like PAS for 
translation. On the basis of PASTR and IC-
PASTR, we build 160 and 215 different MEPD 
classifies, respectively, for the frequent source-
side PASs. 
In Table 2, we have found that our MEPD 
model improves system IC-PASTR slightly. We 
conjecture that this phenomenon is due to two 
possible reasons. On one hand, sometimes, many 
PAS ambiguities might be resolved by both in-
side context and the MEPD model. Therefore, 
the improvement would not be such significant 
1133
when we combine these two methods together. 
On the other hand, as Table 3 shows, the number 
of matching PASs decreases after using IC-
PASTR. Since the MEPD model works on PASs, 
its effectiveness would also weaken to some ex-
tent. Future work will explore this phenomenon 
more thoroughly. 
PASTR
Ref
PASTR 
+ MEPD
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...  [the hague]     [is]      [the last leg]  .
...  ,  [??]    [?]    [? ?? ??]  ?
...  [the hague]   [is]   [his last stop]  .
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...   [is]    [his last leg of]    [the hague] .
 
Figure 6. Translation examples to demonstrate the 
effectiveness of our MEPD model. 
Now, we give Figure 6 to demonstrate the ef-
fectiveness of our MEPD model. From the Fig-
ure, we can see that the system using PASTRs 
selects an inappropriate transformation rule for 
translation: 
[X1] [X3] [A0]1 [Pred]2 [A1]3 [X2] 
source-side PAS(?) target-side-like PAS
 
This rule wrongly moves the subject ???
(Hague)? to the end of the translation. We do not 
give the translation result of the BTG system 
here because it makes the same mistake. 
Conversely, by considering the context infor-
mation, the PASTR+MEPD system chooses a 
correct rule for translation: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
As we can see, the used rule helps to keep the 
SVO structure unchanged, and gets the correct 
translation. 
8 Conclusion and Future Work 
In this paper, we focus on the problem of ambi-
guities for PASs. We first propose two ambigui-
ties: gap ambiguity and role ambiguity. Accord-
ingly, we design two novel methods to do effi-
cient PAS disambiguation: inside-context inte-
gration and a novel MEPD model. For inside 
context integration, we abstract the inside con-
text and combine them into the PASTRs for PAS 
transformation. Towards the MEPD model, we 
design a maximum entropy model for each ambi-
tious source-side PASs. The two methods suc-
cessfully incorporate the rich context information 
into the translation process. Experiments show 
that our PAS disambiguation methods help to 
improve the translation performance significantly.  
In the next step, we will conduct experiments 
on other language pairs to demonstrate the effec-
tiveness of our PAS disambiguation method. In 
addition, we also will try to explore more useful 
and representative features for our MEPD model. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. We thank the anonymous 
reviewers for their valuable comments and sug-
gestions. 
References  
Wilker Aziz, Miguel Rios, and Lucia Specia. (2011). 
Shallow semantic trees for smt. In Proceedings of 
the Sixth Workshop on Statistical Machine Trans-
lation, pages 316?322, Edinburgh, Scotland, July. 
Daniel Bikel. (2004). Intricacies of Collins parsing 
model. Computational Linguistics, 30(4):480-511. 
David Chiang, (2007). Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2):201?
228. 
Marine Carpuat and Dekai Wu. 2007a. How phrase-
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In 
11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 43?52. 
Marine Carpuat and Dekai Wu. 2007b. Improving 
statistical machine translation using word sense 
disambiguation. In Proceedings of EMNLP-CoNLL 
2007, pages 61?72. 
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 
2007. Word sense disambiguation improves statis-
tical machine translation. In Proc. ACL 2007, pag-
es 33?40. 
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou and 
Tiejun Zhao. A Joint Rule Selection Model for 
Hierarchical Phrase-Based Translation. In Proc. 
of ACL 2010. 
1134
Jason Eisner. (2003). Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003. 
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and 
Dekai Wu. (2006). Automatic learning of chinese 
english semantic structure mapping. In IEEE/ACL 
2006 Workshop on Spoken Language Technology 
(SLT 2006), Aruba, December. 
Pascale Fung, Zhaojun Wu, Yongsheng Yang and 
Dekai Wu. (2007). Learning bilingual semantic 
frames: shallow semantic sarsing vs. semantic sole 
projection. In Proceedings of the 11th Conference 
on Theoretical and Methodological Issues in Ma-
chine Translation, pages 75-84. 
Qin Gao and Stephan Vogel. (2011). Utilizing target-
side semantic role labels to assist hierarchical 
phrase-based machine translation. In Proceedings 
of Fifth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, pages 107?115, 
Portland, Oregon, USA, June 2011. Association for 
Computational Linguistics 
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexi-
calized rule selection. In Proc. of Coling 2008, 
pages 321?328. 
Franz Josef Och. (2003). Minimum error rate training 
in statistical machine translation. In Proc. of ACL 
2003, pages 160?167. 
Franz Josef Och and Hermann Ney. (2004). The 
alignment template approach to statistical machine 
translation. Computational Linguistics, 30:417?449. 
Dan Klein and Christopher D. Manning. (2003). Ac-
curate unlexicalized parsing. In Proc. of ACL-2003, 
pages 423-430. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
(2003). Statistical phrase-based translation. In Pro-
ceedings of NAACL 2003, pages 58?54, Edmonton, 
Canada, May-June. 
Philipp Koehn. (2004). Statistical significance tests 
for machine translation evaluation. In Proceedings 
of EMNLP 2004, pages 388?395, Barcelona, Spain, 
July. 
P Koehn, H Hoang, A Birch, C Callison-Burch, M 
Federico, N Bertoldi, B Cowan, W Shen, C Moran 
and R Zens, (2007). Moses: Open source toolkit for 
statistical machine translation. In Proc. of ACL 
2007. pages 177?180, Prague, Czech Republic, 
June. Association for Computational Linguistics. 
Mamoru Komachi and Yuji Matsumoto. (2006). 
Phrase reordering for statistical machine translation 
based on predicate-argument structure. In Proceed-
ings of the International Workshop on Spoken 
Language Translation: Evaluation Campaign on 
Spoken Language Translation, pages 77?82. 
Ding Liu and Daniel Gildea. (2008). Improved tree-
to-string transducer for machine Translation. In 
Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 62?69, Columbus, 
Ohio, USA, June 2008. 
Ding Liu and Daniel Gildea. (2010). Semantic role 
features for machine translation. In Proc. of Coling 
2010, pages 716?724, Beijing, China, August. 
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 
Maximum Entropy based Rule Selection Model for 
Syntax-based Statistical Machine Translation. In 
Proc. of EMNLP 2008.  
Yang Liu, Qun Liu and Shouxun Lin. (2006). Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. (2006). SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. (2002). Bleu: a method for automat-
ic evaluation of machine translation. In Proc. ACL 
2002, pages 311?318, Philadelphia, Pennsylvania, 
USA, July. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan 
Klein. (2006). Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440, Sydney, Australia, July. Association for Com-
putational Linguistics. 
Andreas Stolcke. (2002). Srilm ? an extensible lan-
guage modelling toolkit. In Proceedings of the 7th 
International Conference on Spoken Language 
Processing, pages 901?904, Denver, Colorado, 
USA, September. 
Dekai Wu and Pascale Fung. (2009a). Can semantic 
role labelling improve smt. In Proceedings of the 
13th Annual Conference of the EAMT, pages 218?
225, Barcelona, May. 
Dekai Wu and Pascale Fung. (2009b). Semantic roles 
for smt: A hybrid two-pass model. In Proc. NAACL 
2009, pages 13?16, Boulder, Colorado, June. 
ShuminWu and Martha Palmer. (2011). Semantic 
mapping using automatic word alignment and se-
mantic role labelling. In Proceedings of Fifth 
Workshop on Syntax, Semantics and Structure in 
Statistical Translation, pages 21?30, Portland, Or-
egon, USA, June 2011. 
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime 
Tsukada, and Masaaki Nagata. (2011). Extracting 
preordering rules from predicate-argument struc-
tures. In Proc. IJCNLP 2011, pages 29?37, Chiang 
Mai, Thailand, November.  
1135
Deyi Xiong, Qun Liu, and Shouxun Lin. (2006). Max-
imum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
521?528, Sydney, Australia, July. 
Deyi Xiong, Min Zhang, and Haizhou Li. (2012). 
Modelling the translation of predicate-argument 
structure for smt. In Proc. of ACL 2012, pages 
902?911, Jeju, Republic of Korea, 8-14 July 2012. 
Nianwen Xue. (2008). Labelling chinese predicates 
with semantic roles. Computational Linguistics, 
34(2): 225-255. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. Machine Translation by Modeling Predicate- 
Argument Structure Transformation. In Proc. of 
COLING 2012. 
Hui Zhang, Min  Zhang, Haizhou Li and Eng Siong 
Chng. (2010). Non-isomorphic Forest Pair Transla-
tion. In Proceedings of EMNLP 2010, pages 440-
450, Massachusetts, USA, 9-11 October 2010.  
Tao Zhuang, and Chengqing Zong. (2010a). A mini-
mum error weighting combination strategy for chi-
nese semantic role labelling. In Proceedings of 
COLING-2010, pages 1362-1370. 
Tao Zhuang and Chengqing Zong. (2010b). Joint in-
ference for bilingual semantic role labelling. In 
Proceedings of EMNLP 2010, pages 304?314, 
Massachusetts, USA, 9-11 October 2010.  
1136
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1425?1434,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning a Phrase-based Translation Model from Mon-
olingual Data with Application to Domain Adaptation 
 
Jiajun Zhang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing, China 
{jjzhang, cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Currently, almost all of the statistical ma-
chine translation (SMT) models are trained 
with the parallel corpora in some specific 
domains. However, when it comes to a lan-
guage pair or a different domain without 
any bilingual resources, the traditional SMT 
loses its power. Recently, some research 
works study the unsupervised SMT for in-
ducing a simple word-based translation 
model from the monolingual corpora. It 
successfully bypasses the constraint of 
bitext for SMT and obtains a relatively 
promising result. In this paper, we take a 
step forward and propose a simple but effec-
tive method to induce a phrase-based model 
from the monolingual corpora given an au-
tomatically-induced translation lexicon or a 
manually-edited translation dictionary. We 
apply our method for the domain adaptation 
task and the extensive experiments show 
that our proposed method can substantially 
improve the translation quality. 
1 Introduction 
During the last decade, statistical machine trans-
lation has made great progress. Novel translation 
models, such as phrase-based models (Koehn et 
a., 2007), hierarchical phrase-based models 
(Chiang, 2007) and linguistically syntax-based 
models (Liu et a., 2006; Huang et al, 2006; Gal-
ley, 2006; Zhang et al 2008; Chiang, 2010; 
Zhang et al, 2011; Zhai et al, 2011, 2012) have 
been proposed and achieved higher and higher 
translation performance. However, all of these 
state-of-the-art translation models rely on the 
parallel corpora to induce translation rules and 
estimate the corresponding parameters.  
It is unfortunate that the parallel corpora are 
very expensive to collect and are usually not 
available for resource-poor languages and for 
many specific domains even in a resource-rich 
language pair. 
Recently, more and more researchers concen-
trated on taking full advantage of the monolin-
gual corpora in both source and target languages, 
and proposed methods for bilingual lexicon in-
duction from non-parallel data (Rapp, 1995, 
1999; Koehn and Knight, 2002; Haghighi et al, 
2008; Daum? III and Jagarlamudi, 2011) and 
proposed unsupervised statistical machine trans-
lation (bilingual lexicon is a byproduct) with 
only monolingual corpora (Ravi and Knight, 
2011; Nuhn et al, 2012; Dou and Knight, 2012). 
In the bilingual lexicon induction (Koehn and 
Knight, 2002; Haghighi et al, 2008; Daum? III 
and Jagarlamudi, 2011), with the help of the or-
thographic and context features, researchers 
adopted an unsupervised method, such as canon-
ical correlation analysis (CCA) model, to auto-
matically induce the word translation pairs be-
tween two languages from non-parallel data only 
requiring that the monolingual data in each lan-
guage are from a fairly comparable domain. 
The unsupervised statistical machine transla-
tion method (Ravi and Knight, 2011; Nuhn et al, 
2012; Dou and Knight, 2012) viewed the trans-
lation task as a decipherment problem and de-
signed a generative model with the objective 
function to maximize the likelihood of the 
source language monolingual data. To tackle the 
large-scale vocabulary, they mainly considered 
the word-based model (e.g. IBM Model 3) and 
applied the Bayesian method with Gibbs sam-
pling or slice sampling. Finally, they used the 
learned translation model directly to translate 
unseen data (Ravi and Knight, 2011; Nuhn et al, 
2012) or incorporated the learned bilingual lexi-
con as a new in-domain translation resource into 
the phrase-based model which is trained with 
out-of-domain data to improve the domain adap-
tation performance in machine translation (Dou 
and Knight, 2012).  
We can easily see that these unsupervised 
methods can only induce the word-based transla-
tion rules (bilingual lexicon) at present. It is a 
big challenge that whether we can induce phrase 
1425
1, word reordering example:
?   ??   ?  ??   ?? ||| the purpose of the invention is to ||| 0-0 0-3 1-4 2-2 3-1 4-5 4-6
2, idiom example:
??   ??   ? ||| distinguish the true from the false ||| 0-0 1-2 1-5 2-1 2-4
3, unknown word translation:
??   ???   ??   ? ||| of the light-emitting diode chip ||| 0-2 1-2 2-4 3-0 3-1
 
Table 1: Examples of new translation knowledge learned with the proposed phrase pair induction method. For 
the three fields separated by ?|||?, the first two are respectively Chinese and English phrase, and the last one is 
the word alignment between these two phrases. 
 
level translation rules and learn a phrase-based 
model from the monolingual corpora. 
In this paper, we focus on exploring this di-
rection and propose a simple but effective meth-
od to induce the phrase-level translation rules 
from monolingual data. The main idea of our 
method is to divide the phrase-level translation 
rule induction into two steps: bilingual lexicon 
induction and phrase pair induction.  
Since many researchers have studied the bi-
lingual lexicon induction, in this paper, we 
mainly concentrate ourselves on phrase pair in-
duction given a probabilistic bilingual lexicon 
and two in-domain large monolingual data 
(source and target language). In addition, we 
will further introduce how to refine the induced 
phrase pairs and estimate the parameters of the 
induced phrase pairs, such as four standard 
translation features and phrase reordering feature 
used in the conventional phrase-based models 
(Koehn et al, 2007). The induced phrase-based 
model will be used to help domain adaptation 
for machine translation. 
In the rest of this paper, we first explain with 
examples to show what new translation 
knowledge can be learned with our proposed 
phrase pair induction method (Section 2), and 
then we introduce the approach for probabilistic 
bilingual lexicon acquisition in Section 3. In Sec-
tion 4 and 5, we respectively present our method 
for phrase pair induction and introduce an ap-
proach for phrase pair refinement and parameter 
estimation. Section 6 will show the detailed ex-
periments for the task of domain adaptation. We 
will introduce some related work in Section 7 
and conclude this paper in Section 8. 
2 What Can We Learn with Phrase 
Pair   Induction? 
Readers may doubt that if phrase pair induction 
is performed only using bilingual lexicon and 
monolingual data, what new translation 
knowledge can be learned? 
The bilingual lexicon can only express the 
translation equivalence between source- and tar-
get-side word pair and has little ability to deal 
with word reordering and idiom translation. In 
contrast, phrase pair induction can make up for 
this deficiency to some extent. Furthermore, our 
method is able to learn some unknown word 
translations. 
From the induced phrase pairs with our meth-
od, we have conducted a deep analysis and find 
that we can learn three kinds of new translation 
knowledge: 1) word reordering in a phrase pair; 
2) idioms; and 3) unknown word translations. 
Table 1 gives examples for each of the three 
kinds. For the first example, the source and tar-
get phrase are extracted respectively from mono-
lingual data, each word in the source phrase has 
a translation in the target phrase, but the word 
order is different. The word order encoded in a 
phrase pair is difficult to learn in a word-based 
SMT.  In the second example, the italic source 
word corresponds to two target words (in italic), 
and the phrase pair is an idiom which cannot be 
learned from word-based SMT. In the third ex-
ample, as we learn from the source and target 
monolingual text that the words around the italic 
ones are translations with each other, thus we 
cannot only extract a new phrase pair but also 
learn a translation pair of unknown words in 
italic. 
3 Probabilistic Bilingual Lexicon Ac-
quisition 
In order to induce the phrase pairs from the in-
domain monolingual data for domain adaptation, 
the probabilistic bilingual lexicon is essential. 
In this paper, we acquire the probabilistic bi-
lingual lexicon from two approaches: 1) build a 
bilingual lexicon from large-scale out-of-domain 
parallel data; 2) adopt a manually collected in-
domain lexicon. This paper uses Chinese-to-
English translation as a case study and electronic 
data is the in-domain data we focus on.  
1426
In Chinese-to-English translation, there are 
lots of parallel data on News. Here, we utilize 
about 2.08 million sentence pairs1 in News do-
main to learn a probabilistic bilingual lexicon. 
Basically, we can use GIZA++ (Och, 2003) to 
get the probabilistic lexicon. However, the prob-
lem is that each source-side word associates too 
many possible translations which contain much 
noise. For instance, in the lexicon obtained with 
GIZA++, each source-side word has about 13 
translations on average. The noise of the lexicon 
can influence the accuracy of the induced phrase 
pairs to a large extent. To learn a lexicon with a 
high precision, we follow Munteanu and Marcu 
(2006) to apply Log-Likelihood-Ratios (Dunning, 
1993; Melamed, 2000; Moore, 2004a, 2004b) to 
estimate how strong the association is between a 
source-side word and its aligned target-side word. 
We employ the same algorithm used in (Munte-
anu and Marcu, 2006) which first use the GI-
ZA++ (with grow-diag-final-and heuristic) to 
obtain the word alignment between source and 
target words, and then calculate the association 
strength between the aligned words. After using 
the log-likelihood-ratios algorithm2, we obtain a 
probabilistic bilingual lexicon with bidirectional 
translation probabilities from the out-of-domain 
data. In the final lexicon, the number of average 
translations is only 5. We call this lexicon LLR-
lex. 
   In the electronic domain, we manually collect-
ed a lexicon which contains about 140k entries. 
It should be noted that there is no translation 
probability in this lexicon. In order to assign 
probabilities to each entry, we apply the Corpus 
Translation Probability which used in (Wu et al, 
2008): given an in-domain source language 
monolingual data, we translate this data with the 
phrase-based model trained on the out-of-domain 
News data, the in-domain lexicon and the in-
domain target language monolingual data (for 
language model estimation).  With the source 
language data and its translation, we estimate the 
bidirectional translation probabilities for each 
entry in the original lexicon. For the entries 
whose translation probabilities are not estimated, 
we just assign a uniform probability. That is if a 
source word has n translations, then the transla-
tion probability of target word given the source 
word is 1/n. We call this lexicon Domain-lex. 
                                                 
1 LDC category numbers are: LDC2000T50, LDC2003E14, 
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, 
LDC2005T10 and LDC2005T34. 
2  Following Moore (2004b), we use the threshold 10 on 
LLR to filter out unlikely translations. 
  We combine LLR-lex and Domain-lex to obtain 
the final probabilistic bilingual lexicon for phrase 
pair induction. 
4 Phrase Pair Induction Method 
Given a probabilistic bilingual lexicon and two 
monolingual data, we present a simple but effec-
tive method for phrase pair induction in this sec-
tion. 
 
 
Figure 1: a na?ve algorithm for phrase pair induction. 
4.1 A Na?ve Method 
We first introduce a relatively na?ve way to ex-
tract phrase pairs from the given resources. For a 
source phrase (word sequence), we can reorder 
the words in the phrase (permutation) first, and 
then obtain the target phrases with the bilingual 
lexicon (translation), and finally check if the tar-
get phrase is in the target monolingual data. The 
algorithm is given in Figure 1. 
Figure 1 shows that the na?ve algorithm is very 
easy to implement. However, the time complexi-
ty is too high. For each source phrase jis  (with 
? ?1 !j i? ?  permutations), suppose a source word 
has C translations on average and checking 
whether the target phrase ''jit  in T needs time 
? ?O T , then, phrase pair induction for a single 
source phrase needs time ? ?? ?1 1 !j iO C T j i? ? ? ?. 
It is very time consuming. One may design 
smarter algorithms. For example, one can collect 
distinct n-grams from source and target monolin-
gual data. Then, for a source-side phrase with 
length L, one can find the best translation candi-
date using the probabilistic bilingual lexicon 
from the target-side phrases with the same length 
L. The biggest disadvantage of these algorithms 
is that they can only induce phrase pair (with the 
Input:   Probabilistic bilingual lexicon V (each source word 
s maps a translation set V[s]) 
            Source language monolingual data S={sn} n=1...N 
            Target language monolingual data T={tm} m=1...M 
Output: Phrase pairs  P 
 
1: For each distinct source-side phrase jis  in S:  
2:       If each jk is s? in V: 
3: Collect [ ] jk k iV s ?  
4: For each permutation ''jis  of jis :  
5:        If ''jit  in T:     ' '[ ] ' [ , ]k kt V s k i j? ?  
6:  Add phrase pair ? ?'',j ji is t into P 
1427
same length) encoding word reordering, but can-
not learn phrase pairs in different length. Fur-
thermore, they cannot learn idioms and unknown 
word translations from monolingual data. Obvi-
ously, these kind of approaches is not optimal. 
4.2 Phrase Pair Induction with Inverted 
Index 
In order to make the phrase pair induction both 
effective and efficient, we propose a method 
using inverted index data structure which is usu-
ally a central component of a typical search en-
gine.  
The inverted index is employed to represent 
the target language monolingual data. For a tar-
get language word, the inverted index not only 
records the sentence position in monolingual 
data, but also records the word position in a sen-
tence. Some examples are shown in Table 2. By 
doing this, we do not need to iterate all the per-
mutations of source language phrase jis  to ex-
plore possible phrase pairs encoding word reor-
dering. Furthermore, it is possible to learn idiom 
translation and unknown word translations. We 
will elaborate how to induce phrase pairs with 
the help of inverted index. 
Target Language 
Word 
Position 
communication (2,5), (106,20), ?, (23022, 12) 
? ? 
zoom (90,2), (280,21), ?, (90239,15) 
Table 2: Some examples of inverted index for tar-
get language words, (2,5) means that ?communica-
tion? occurs at the 5th word of the 2nd sentence in the 
target monolingual data. 
The new algorithm for phrase pair induction is 
presented in Figure 2. Line 1 iterates all the dis-
tinct phrases in the source-side monolingual data. 
It can be implemented by collecting all the dis-
tinct n-grams in which n is the phrase length we 
are interested in (3 to 7 in this paper). For each 
distinct source-side phrase, Line 2-5 efficiently 
collects all the positions in the target monolin-
gual data for the translations of each word in the 
source phrase. Line 6 sorts the positions so that 
we can easily find the position sequence belong-
ing to a same sentence. Line 8-9 discards all the 
position sub-sequences that lack translations for 
more than one source-side words. That is to say 
we allow at most one unknown word in an in-
duced phrase pair in order to make the induction 
more accurate. Line 10 and Line 12 is the core 
of this algorithm. We first define a constraint 
before detailing the algorithm. 
Figure 2: Phrase pair induction using inverted index. 
Constraint: we require that there exists at 
most one phrase in a target sentence that is the 
translation of the source-side phrase. 
According to our analysis, it is not often to 
find that two phrases (length larger than 2) in a 
same sentence have the same meaning. Even if it 
happens, it is reasonable to keep the one with the 
highest probability. Given a position sequence 
belonging to a same sentence, Line 10 smoothes 
the probability of the single word gap according 
to the probabilities of the around words. Single 
word gap means that this word is not aligned but 
its left and right words are aligned with the 
words of the source-side phrase. Suppose the 
target sub-sequence is 
i i r jt t t?
 and 
i rt ?  is the 
only word that is not aligned with source-side 
words. We smooth the probability ? ?|i rp t null?  
as follows: 
? ?
? ? ? ?? ?
? ? ? ?
1 11 1
min | , |
, 1 1
2|
| |
,
2
i j
i r i r
i t j t
i r
i r t i r t
p t s p t s
if r or r j
p t null
p t s p t s
otherwise? ? ? ?
?
? ? ? ?
?
? ? ? ??
? ?
? ?
?
?
        (1) 
The above formula means that if the left or the 
right side only has one word, then the smoothed 
probability is one half of the minimum of the 
probabilities of the two neighbors, otherwise the 
smoothed probability is the average of the prob-
abilities of the two neighbors. This smoothing 
strategy encourages that if more words around 
the un-aligned word are translations of the 
source-side phrase, then the gap word is more 
likely to belong to the translations of the source-
side phrase. 
Input:   Probabilistic bilingual lexicon V (each source word s 
maps a translation set V[s]) 
            Source language monolingual data S={sn} n=1...N 
            Inverted index representing target language monolin-
gual data IMap 
Output: Phrase pairs P 
1: For each distinct source-side phrase jis  in S:  
2:      positionArray = [] 
3:      For each jk is s? : 
4:            For each [ ]kt V s? : 
5:       add  IMap[ t ]  into positionArray 
6:      Sort  positionArray 
7:      For each sequence in a same sentence in positionArray:  
8:              If more than 1 word in jis has no trans in the seq: 
9:                    Discard this seq and continue 
10:             Probability smoothing for single word gap 
11:             For all continuous position sub-sequence: 
12:                  Find the one kht  with maximum probability 
13:                 Add phrase pair ? ?,j ki hs t into P 
1428
After probability smoothing of the single gap 
word, we are ready to extract the candidate 
translation of the source-side phrase. Similar 
with Line 9 in Figure 2, we further filter the tar-
get continuous phrase if more than one word in 
source-side phrase has no translation in this tar-
get phrase. After that, we just choose the contin-
uous target phrase with the largest probability if 
two or more continuous target phrases exist in 
the same target sentence. The probability of a 
target-side phrase given the source-side phrase is 
computed similar to that of (Koehn et al, 2003) 
except that we impose length normalization: 
? ? ? ?? ? ? ?? ?
1
,1
1| , |
| ,
nn
lex i j
i j ai
p t s a p t s
j i j a ? ??
? ?
? ?? ? ??? ?
??
         (2) 
where the alignment a is produced using 
probabilistic bilingual lexicon. If a target word 
in t is a gap word, we suppose there is a word 
alignment between the target gap word and the 
source-side null.  
Similarly, we can compute the probability of 
source-side phrase given the target-side phrase 
? ?| ,lexp s t a . Then, we find the target-side phrase 
which has the biggest value of 
? ? ? ?| , | ,lex lexp t s a p s t a? . Line 13 in Figure 2 col-
lects the induced phrase pairs. 
For the time complexity, it depends on the 
length of positionArray, since the time complex-
ity of the core algorithm (Line 7-13) is propor-
tional to the length of positionArray. If posi-
tionArray contains almost all the positions in the 
target monolingual data T, then the worst time 
complexity will be ? ?logO T T  (for array sort). 
However, we find in the target monolingual data 
(1 million sentences) that each distinct word 
happens 110 times on average. Then, for a 
sources-side phrase with 7 words, the average 
length of positionArray will be 3850, since each 
source word has averagely 5 target translations 
(mentioned in Section 3). Therefore, the algo-
rithm is relatively efficient in the average case. 
5 Phrase Pair Refinement and Parame-
terization 
5.1 Phrase Pair Refinement 
Some of the phrase pairs induced in Section 4 
may contain noise. According to our analysis, 
we find that the biggest problem is that in the 
target-side of the phrase pair, there are two or 
more identical words aligned to the same source-
side word. For example, we extract a phrase pair 
as follows: 
?  ??  ??
of  business information of 
In the above phrase pair, there are two words 
?of? in the target side and the first one is redun-
dant. The phrase pair induction algorithm pre-
sented in Section 4 cannot deal with this situa-
tion. In this section, we propose a simple ap-
proach to handle this problem. For each entry in 
LLR-lex, such as (?, of), we can learn two kinds 
of information from the out-of-domain word-
aligned sentence pairs: one is whether the target 
translation is before or after the translation of the 
preceding source-side word (Order); the other is 
whether the target translation is adjacent with 
the translation of the preceding source-side word 
(Adjacency). If the source-side word is the be-
ginning of the phrase, we calculate the corre-
sponding information with the succeeding word 
instead of the preceding word. For the entries in 
Domain-lex, we constrain that the target transla-
tion should be adjacent with the translations of 
its source-side neighbors and translation order is 
the same with the source-side words. 
With the Order and Adjacency information, 
we first check the order information, and then 
check the adjacency information if the dupli-
cates cannot be handled using order information. 
For example, since (?, of) is an entry in LLR-
lex and we have learned that ?of? is much more 
likely to be behind the translation of the suc-
ceeding word. Thus, the first word ?of? can be 
discarded. This refinement can be applied before 
finding the phrase pair with maximum probabil-
ity (Line 12 in Figure 2) so that the duplicate 
words do not affect the calculation of translation 
probability of phrase pair. 
5.2 Translation Probability Estimation 
It is well known that in the phrase-based SMT 
there are four translation probabilities and the 
reordering probability for each phrase pair. 
   The translation probabilities in the traditional 
phrase-based SMT include bidirectional phrase 
translation probabilities and bidirectional lexical 
weights. For the lexical weights, we can use the 
? ?| ,lexp s t a  and ? ?| ,lexp t s a computed in the 
above section without length normalization. 
However, for the phrase-level probability, we 
cannot use maximum likelihood estimation since 
the phrase pairs are not extracted from parallel 
sentences. 
1429
 In this paper, we borrow and extend the idea of 
(Klementiev et al, 2012) to calculate the phrase-
level translation probability with context infor-
mation in source and target monolingual corpus. 
The value is calculated using a vector space 
model. With source and target vocabularies 
? ?1 2, , , Ns s s  and ? ?1 2, , , Mt t t , the source-side 
phrase s and target-side phrase t can be respec-
tively represented in an N- and M-dimensional 
vector. The k-th component of s?s contextual 
vector is computed using the method of (Fung 
and Yee, 1998) as follows: 
? ?? ?, maxlog / 1k s k kw n n n? ? ?               (3) 
where 
,s kn
and 
kn denotes the number of times ks  
occurs in the context of s and in the entire source 
language monolingual data, and 
maxn is the max-
imum number of occurrence of any source-side 
word in the source language monolingual data. 
The k-th element of t?s vector can be computed 
with the same method. We finally normalize 
these vectors with L2-norm. 
   With the s?s and t?s contextual vector represen-
tations, we calculate two similarities: 1) project 
s?s vector into target side t  with the lexical 
mapping p(t|s), and then get the similarity by 
computing the cosine of two angles between t?s 
and t ?s vectors; 2) project t?s vector into source 
side s  with the lexical mapping p(s|t), and then 
obtain the similarity between s?s and s ?s vectors. 
These two contextual similarities will serve as 
two phrase-level translation probabilities. 
5.3 Reordering Probability Estimation 
For the reordering probabilities of newly induced 
phrase pairs, we can also follow Klementiev et al 
(2012) to estimate these probabilities using 
source and target monolingual data. The method 
is to calculate six probabilities for monotone, 
swap or discontinuous cases. For the phrase pair 
(? ?? ?? , business information of), we 
find a source sentence containing ? ?? ??, 
and find a target sentence containing business 
information of. If there is another phrase pair 
? ?,s t ,  t  exactly follows business information of 
and s  occurs in the same source sentence with 
? ?? ??, then we compare the position 
relationship between s  and ? ?? ??. We 
increment the swap count if s  is just before ? 
?? ??. After counting, we finally use max-
imum likelihood estimation method to compute 
the reordering probabilities. 
6 Related Work 
As far as we know, few researchers study phrase 
pair induction from only monolingual data. 
   There are three research works that are most 
related with ours. One is using an in-domain 
probabilistic bilingual lexicon to extract sub-
sentential parallel fragments from comparable 
corpora (Munteanu and Marcu, 2006; Quirk et al, 
2007; Cettolo et al, 2010). Munteanu and Marcu 
(2006) first extract the candidate parallel sen-
tences from the comparable corpora and further 
extract the accurate sub-sentential bilingual 
fragments from the candidate parallel sentences 
using the in-domain probabilistic bilingual lexi-
con. Compared with their work, our focus is to 
induce phrase pairs directly from monolingual 
data rather than comparable data. Thus, finding 
the candidate parallel sentences is not possible in 
our situation. 
Another is to make full use of monolingual da-
ta with transductive learning (Ueffing et al, 2007; 
Schwenk, 2008; Wu et al, 2008; Bertoldi and 
Federico, 2009). For the target-side monolingual 
data, they just use it to train language model, and 
for the source-side monolingual data, they em-
ploy a baseline (word-based SMT or phrase-
based SMT trained with small-scale bitext) to 
first translate the source sentences, combining 
the source sentence and its target translation as a 
bilingual sentence pair, and then train a new 
phrase-base SMT with these pseudo sentence 
pairs. This method cannot learn idiom transla-
tions and unknown word translations. 
The third is to estimate the translation parame-
ters and reordering parameters using monolin-
gual data given the phrase pairs (Klementiev et 
al., 2012). Their work supposes the phrase pairs 
are already given and then corresponding param-
eters can be learned with monolingual data. Dif-
ferent from their work, we concentrate ourselves 
on inducing phrase pairs from monolingual data 
and then borrow some ideas from theirs for pa-
rameter estimation. Furthermore, we extend their 
contextual similarity between source and target 
phrases to both directions. 
7 Experiments 
7.1 Experimental Setup  
Our purpose is to induce phrase pairs to improve 
translation quality for domain adaptation. We 
have introduced the out-of-domain data and the 
electronic in-domain lexicon in Section 3. Here 
we introduce other information about the in-
1430
domain data. Besides the in-domain lexicon, we 
have collected respectively 1 million monolin-
gual sentences in electronic area from the web. 
They are neither parallel nor comparable because 
we cannot even extract a small number of paral-
lel sentence pairs from this monolingual data 
using the method of (Munteanu and Marcu, 
2006). We further employ experts to translate 
2000 Chinese electronic sentences into English. 
The first half is used as the tuning set (elec1000-
tune) and the second half is employed as the test-
ing set (elec1000-test). 
   We construct two kinds of phrase-based mod-
els using Moses (Koehn et al, 2007): one uses 
out-of-domain data and the other uses in-domain 
data. For the out-of-domain data, we build the 
phrase table and reordering table using the 2.08 
million Chinese-to-English sentence pairs, and 
we use the SRILM toolkit (Stolcke, 2002) to 
train the 5-gram English language model with 
the target part of the parallel sentences and the 
Xinhua portion of the English Gigaword. For the 
in-domain electronic data, we first consider the 
lexicon as a phrase table in which we assign a 
constant 1.0 for each of the four probabilities, 
and then we combine this initial phrase table and 
the induced phrase pairs to form the new phrase 
table. The in-domain reordering table is created 
for the induced phrase pairs. An in-domain 5-
gram English language model is trained with the 
target 1 million monolingual data. 
   We use BLEU (Papineni et al, 2002) score 
with shortest length penalty as the evaluation 
metric and apply the pairwise re-sampling ap-
proach (Koehn, 2004) to perform the signifi-
cance test. 
7.2 Experimental Results  
In this section, we first conduct experiments to 
figure out how the translation performance de-
grades when the domain changes. To better illus-
trate the comparison, we first use News data to 
evaluate the NIST evaluation tests and then use 
the same News data to evaluate the electronic 
test sets. For the NIST evaluation, we employ 
Chinese-to-English NIST MT03 as the tuning set 
and NIST MT05 as the test set. Table 3 gives the 
results. It is obvious that, it is relatively high 
when using the News training data to evaluate 
the same News test set. However, when the test 
domain is changed, the translation performance 
decreases to a large extent. 
Given the in-domain bilingual lexicon and two 
monolingual data, previous works also proposed 
some good methods to explore the potential of 
the given data to improve the translation quality. 
Here, we implement their approaches and use 
them as our strong baseline. Wu et al (2008) 
regards the in-domain lexicon with corpus trans-
lation probability as another phrase table and 
further use the in-domain language model be-
sides the out-of-domain language model. Table 4 
gives the results. We can see from the table that 
the domain lexicon is much helpful and signifi-
cantly outperforms the baseline with more than 
4.0 BLEU points. When it is enhanced with the 
in-domain language model, it can further im-
prove the translation performance by more than 
2.5 BLEU points. This method has made good 
use of in-domain lexicon and the target-side in-
domain monolingual data, but it does not take 
full advantage of the in-domain source-side 
monolingual data. 
In order to use source-side monolingual data, 
Ueffing et al (2007), Schwenk (2008), Wu et al 
(2008) and Bertoldi and Federico (2009) em-
ployed the transductive learning to first translate 
the source-side monolingual data using the best 
configuration (baseline+in-domain lexicon+in-
domain language model) and obtain 1-best trans-
lation for each source-side sentence. With the 
source-side sentences and their translations, the 
new phrase table and reordering table are built. 
Then, these resources are added into the best 
configuration. The experimental results are pre-
sented in the last low of Table 4. From the results, 
we see that transductive learning can further im-
prove the translation performance significantly 
by 0.6 BLEU points. 
In tranductive learning, in-domain lexicon and 
both-side monolingual data have been explored. 
However, this method does not take full ad-
vantage of both-side monolingual data because it 
uses source and target monolingual data individ-
ually. In our method, we explore fully the source 
and target monolingual data to induce translation 
equivalence on the phrase level. In order to make 
the phrase pair induction more efficient, we first 
sort all the sentences in the both-side monolin-
gual data according to the word hit rate in the 
bilingual lexicon. Then, we conduct six sets of 
experiments respectively on the first 100k, 200k, 
300k, 500k and whole 1m sentences. All the ex-
periments are run based on the configuration 
with BLEU 13.41 in Table 4, and we call this 
configuration BestConfig. Note that the unknown 
words are only allowed if the source-side of a 
phrase pair has more than 3 words. Table 5 
shows the results. 
1431
 Training Data Tune Data (NIST MT03) Test Data (NIST MT05) 
2.08M sentence pairs in News 
35.79 34.26 
Tune Data (elec1000-tune) Test Data (elec1000-test) 
7.93 6.69 
Table 3: Experimental results using News training data to test NIST evaluation data and electronic data (numbers 
denote BLEU score points in percent). 
 
Method Tune (elec1000-tune) Test (elec1000-test) 
Baseline 7.93 6.69 
baseline + in-domain lexicon 10.97 10.87 
baseline + in-domain lexicon + in-
domain language model 
13.72 13.41++ 
Transductive Learning 14.13 14.01* 
Table 4: Experimental results using News training data, in-domain lexicon, language model and transductive 
learning. Bold figures mean that the results are statistically significant better than the baseline with p<0.01, and 
?++? denotes the result is statistically significant better than baseline+in-domain lexicon. ?*? means that the 
result is statistically significant better than 13.41 with p<0.05. 
 
Method Tune (BLEU %) Test (BLEU %) 
BestConfig 13.72 13.41 
+phrase pair induction (100k) 14.23 14.06 
+phrase pair induction (200k) 14.45 14.24 
+phrase pair induction (300k) 14.76 14.83++ 
+phrase pair induction (500k) 14.98 15.16++ 
+phrase pair induction (1m) 15.11 15.30++ 
Table 5: Experimental results of our phrase pair induction method. Bold figures denotes the corresponding 
method significantly outperform the BestConfig with p<0.05. Bold and Italic figures means the results are sig-
nificantly better than that of BestConfig with p<0.01. ?++? denotes that the corresponding approach performs 
significantly better than Transductive Learning with p<0.01. 
 
Method Before Filtering After Filtering 
+phrase pair induction (100k) 72,615 8,724 
+phrase pair induction (200k) 108,948 12,328 
+phrase pair induction (300k) 136,529 17,505 
+phrase pair induction (500k) 150,263 19,862 
+phrase pair induction (1m) 169,172 21,486 
Table 6: the number of phrase pairs induced with different size of monolingual data. 
 
  We can see from the table that our method ob-
tains the best translation performance. When us-
ing the first 100k sentences for phrase pair induc-
tion, it obtains a significant improvement over 
the BestConfig by 0.65 BLEU points and can 
outperform the transductive learning method.  
When we use more monolingual data, the per-
formance becomes even better.  The method of 
phrase pair induction using 300k sentences per-
forms quite well. It outperforms the BestConfig 
significantly with an improvement of 1.42 BLEU 
points and it also performs much better than 
transductive learning method with gains of 0.82 
BLEU points. With the monolingual data larger 
and larger, the gains become smaller and smaller 
because the word hit rate gets lower and lower. 
These experimental results empirically show the 
effectiveness of our proposed phrase pair induc-
tion method. 
   A question remains that how many new phrase 
pairs are induced with different size of monolin-
gual data. Here, we give respectively the statis-
tics before and after filtering with the 1000 test 
sentences. Table 6 shows the statistics. We can 
see from the table that lots of new phrase pairs 
can be induced since the source and target mono-
lingual data is in the same domain. However, 
since the source and target monolingual data is 
1432
far from parallel, most of the phrase pairs are not 
long. For example, in the 108,948 distinct phrase 
pairs, we find that the phrase pair distribution 
according to source-side length is (3:50.6%, 
4:35.6%, 5:3.3%, 6:9.8%, 7:0.7%). It is easy to 
see that the phrase pairs whose source-side 
length longer than 4 account for only a very 
small part. 
8 Conclusion and Future Work 
This paper proposes a simple but effective meth-
od to induce phrase pairs from monolingual data. 
Given the probabilistic bilingual lexicon and 
both-side monolingual data in the same domain, 
the method employs inverted index structure to 
represent the target-side monolingual data, and 
induce the translations for each distinct source-
side phrase with the help of the bilingual lexicon. 
We further propose an approach to refine the re-
sult phrase pairs to make them more accurate. 
We also introduce how to estimate the translation 
and reordering parameters for the induced phrase 
pairs with monolingual data. Extensive experi-
ments on domain adaptation have shown that our 
method can significantly outperform previous 
methods which also focus on exploring the in-
domain lexicon and monolingual data. 
However, through the analysis we find that our 
induced phrase pairs still contain some noise, 
such as the words in source- and target-side of 
the phrase pair are all aligned but the target-side 
phrase expresses the different meaning. Further-
more, our proposed method cannot learn expres-
sions which are not lexical translations but are 
semantic ones. In the future, we will study fur-
ther on these phenomena and propose new meth-
ods to handle these problems. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101 and 
2012AA011102, and also supported by the Key 
Project of Knowledge Innovation of Program of 
Chinese Academy of Sciences under Grant No. 
KGZD-EW-501. We would also like to thank the 
anonymous reviewers for their valuable sugges-
tions.  
References  
Nicola Bertoldi and Marcello Federico, 2009. Domain 
adaptation for statistical machine translation with 
monolingual resources. In Proc. of the Fourth 
Workshop on Statistical Machine Translation, 
pages 182-189. 
Mauro Cettolo, Marcello Federico and Nicola 
Bertoldi, 2010. Mining parallel fragments from 
comparable texts. In Proc. of the seventh 
International Workshop on Spoken Language 
Translation (IWSLT), pages 227-234. 
David Chiang, 2007. Hierarchical phrase-based 
translation. computational linguistics, 33 (2). 
pages 201-228. 
David Chiang, 2010. Learning to translate with source 
and target syntax. In Proc. of ACL 2010, pages 
1443-1452. 
Hal Daum? III and Jagadeesh Jagarlamudi, 2011. 
Domain adaptation for machine translation by 
mining unseen words. In Proc. of ACL-HLT 
2011. 
Qing Dou and Kevin Knight, 2012. Large Scale 
Decipherment for Out-of-Domain Machine 
Translation. In Proc. of EMNLP-CONLL 2012. 
Ted Dunning, 1993. Accurate methods for the 
statistics of surprise and coincidence. 
computational linguistics, 19 (1). pages 61-74. 
Pascale Fung and Lo Yuen Yee, 1998. An IR 
approach for translating new words from 
nonparallel, comparable texts. In Proc. of ACL-
COLING 1998., pages 414-420. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve DeNeefe, Wei Wang and 
Ignacio Thayer, 2006. Scalable inference and 
training of context-rich syntactic translation 
models. In Proc. of COLING-ACL 2006, pages 
961-968. 
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick 
and Dan Klein, 2008. Learning bilingual 
lexicons from monolingual corpora. In Proc. of 
ACL-08: HLT, pages 771-779. 
Liang Huang, Kevin Knight and Aravind Joshi, 2006. 
A syntax-directed translator with extended 
domain of locality. In Proc. of AMTA 2006, 
pages 1-8. 
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch and David Yarowsky, 2012. Toward 
statistical machine translation without parallel 
corpora. In Proc. of EACL 2012., pages 130-140. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of 
EMNLP 2004., pages 388-395, Barcelona, Spain, 
July 25th-26th, 2004. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Marcello Federico, Nicola Bertoldi, Brooke 
Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ond?ej Bojar, Alexandra 
Constantin and Evan Herbst, 2007. Moses: Open 
source toolkit for statistical machine translation. 
In Proc. of ACL on Interactive Poster and 
Demonstration Sessions 2007., pages 177-180, 
Prague, Czech Republic, June 27th-30th, 2007. 
Philipp Koehn and Kevin Knight, 2002. Learning a 
translation lexicon from monolingual corpora. In 
1433
Proc. of the ACL-02 workshop on Unsupervised 
lexical acquisition, pages 9-16. 
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING-ACL 2006, 
pages 609-616. 
I. Dan Melamed, 2000. Models of translational 
equivalence among words. computational 
linguistics, 26 (2). pages 221-249. 
Rorbert C. Moore, 2004a. Improving IBM word-
alignment model 1. In Proc. of ACL 2004. 
Rorbert C. Moore, 2004b. On log-likelihood-ratios 
and the significance of rare events. In Proc. of 
EMNLP 2004., pages 333-340. 
Dragos Stefan Munteanu and Daniel Marcu, 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora. In Proc. of ACL-COLING 
2006. 
Malte Nuhn, Arne Mauser and Hermann Ney, 2012. 
Deciphering Foreign Language by Combining 
Language Models and Context Vectors. In Proc. 
of ACL 2012. 
Franz Josef Och and Hermann Ney., 2003. A 
systematic comparison of various statistical 
alignment models. computational linguistics, 29 
(1). pages 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu, 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proc. of ACL 2002., pages 311-318. 
Chris Quirk, Raghavendra Udupa and Arul Menezes, 
2007. Generative models of noisy translations 
with applications to parallel fragment extraction. 
In Proc. of the Machine Translation Summit XI, 
pages 377-384. 
Reinhard Rapp, 1995. Identifying word translations in 
non-parallel texts. In Proc. of ACL 1995, pages 
320-322. 
Reinhard Rapp, 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. In Proc. of ACL 1999, pages 
519-526. 
Sujith Ravi and Kevin Knight, 2011. Deciphering 
foreign language. In Proc. of ACL 2011., pages 
12-21. 
Holger Schwenk, 2008. Investigations on largescale 
lightly-supervised training for statistical machine 
translation. In Proc. of IWSLT 2008, pages 182-
189. 
Andreas Stolcke, 2002. SRILM-an extensible 
language modeling toolkit. In Proc. of 7th 
International Conference on Spoken Language 
Processing, pages 901-904, Denver, Colorado, 
USA, September 16th-20th, 2002. 
Nicola Ueffing, Gholamreza Haffari and Anoop 
Sarkar, 2007. Transductive learning for statistical 
machine translation. In Proc. of ACL 2007. 
Hua Wu, Haifeng Wang and Chengqing Zong, 2008. 
Domain adaptation for statistical machine 
translation with domain dictionary and 
monolingual corpora. In Proc. of COLING 2008., 
pages 993-1000. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong, 2011.  Simple but effective approaches to 
improving tree-to-tree model.  In Proc. of MT 
Summit XIII 2011, pages 261-268. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong, 2012. Tree-based translation without using 
parse trees. In Proc. of COLING 2012, pages 
3037-3054. 
Jiajun Zhang, Feifei Zhai and Chengqing Zong, 2011. 
Augmenting string-to-tree translation models 
with fuzzy use of the source-side syntax. In Proc. 
of EMNLP 2011, pages 204-215. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li, 2008. A tree 
sequence alignment-based tree-to-tree translation 
model. In Proc. of ACL-08: HLT, pages 559-567. 
 
1434
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 370?374,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Novel Translation Framework Based on Rhetorical Structure Theory 
 
 
Mei Tu             Yu Zhou           Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation,  
Chinese Academy of Sciences 
{mtu,yzhou,cqzong}@nlpr.ia.ac.cn 
  
 
 
 
Abstract 
Rhetorical structure theory (RST) is widely 
used for discourse understanding, which repre-
sents a discourse as a hierarchically semantic 
structure. In this paper, we propose a novel 
translation framework with the help of RST. In 
our framework, the translation process mainly 
includes three steps: 1) Source RST-tree ac-
quisition: a source sentence is parsed into an 
RST tree; 2) Rule extraction: translation rules 
are extracted from the source tree and the tar-
get string via bilingual word alignment; 3) 
RST-based translation: the source RST-tree 
is translated with translation rules. Experi-
ments on Chinese-to-English show that our 
RST-based approach achieves improvements 
of 2.3/0.77/1.43 BLEU points on 
NIST04/NIST05/CWMT2008 respectively. 
1 Introduction 
For statistical machine translation (SMT), a cru-
cial issue is how to build a translation model to 
extract as much accurate and generative transla-
tion knowledge as possible. The existing SMT 
models have made much progress. However, 
they still suffer from the bad performance of un-
natural or even unreadable translation, especially 
when the sentences become complicated. We 
think the deep reason is that those models only 
extract translation information on lexical or syn-
tactic level, but fail to give an overall under-
standing of source sentences on semantic level of 
discourse. In order to solve such problem, (Gong 
et al, 2011; Xiao et al, 2011; Wong and Kit, 
2012) build discourse-based translation models 
to ensure the lexical coherence or consistency. 
Although some lexicons can be translated better 
by their models, the overall structure still re-
mains unnatural. Marcu et al  (2000) design a 
discourse structure transferring module, but leave 
much work to do, especially on how to integrate 
this module into SMT and how to automatically 
analyze the structures. Those reasons urge us to 
seek a new translation framework under the idea 
of ?translation with overall understanding?. 
Rhetorical structure theory (RST) (Mann and 
Thompson, 1988) provides us with a good per-
spective and inspiration to build such a frame-
work. Generally, an RST tree can explicitly show 
the minimal spans with semantic functional in-
tegrity, which are called elementary discourse 
units (edus) (Marcu et al, 2000), and it also de-
picts the hierarchical relations among edus. Fur-
thermore, since different languages? edus are 
usually equivalent on semantic level, it is intui-
tive to create a new framework based on RST by 
directly mapping the source edus to target ones. 
Taking the Chinese-to-English translation as 
an example, our translation framework works as 
the following steps:  
1) Source RST-tree acquisition: a source 
sentence is parsed into an RST-tree;  
2) Rule extraction: translation rules are ex-
tracted from the source tree and the target string 
via bilingual word alignment;  
3) RST-based translation:  the source RST-
tree is translated into target sentence with ex-
tracted translation rules. 
Experiments on Chinese-to-English sentence-
level discourses demonstrate that this method 
achieves significant improvements. 
2 Chinese RST Parser  
2.1 Annotation of Chinese RST Tree 
Similar to (Soricut and Marcu, 2003), a node of 
RST tree is represented as a tuple R-[s, m, e], 
which means the relation R controls two seman-
tic spans U1 and U2 , U1 starts from word position 
s and stops at word position m. U2 starts from 
m+1 and ends with e. Under the guidance of def-
inition of RST, Yue (2008) defined 12 groups1 of 
                                                 
1They are Parallel, Alternative, Condition, Reason, Elabo-
ration, Means, Preparation, Enablement, Antithesis, Back-
ground, Evidences, Others. 
370
rhetorical relations for Chinese particularly, upon 
which our Chinese RST parser is developed. 
Figure 1 illustrates an example of Chinese 
RST tree and its alignment to the English string. 
There are two levels in this tree. The Antithesis 
relation controls U1 from 0 to 9 and U2 from 10 
to 21. Thus it is written as Antithesis-[0,9,21]. 
Different shadow blocks denote the alignments 
of different edus. Links between source and tar-
get words are alignments of cue words. Cue 
words are viewed as the strongest clues for rhe-
torical relation recognition and always found at 
the beginning of text (Reitter, 2003), such as ??
?(although), ??(because of)?. With the cue 
words included, the relations are much easier to 
be analyzed. So we focus on the explicit relations 
with cue words in this paper as our first try. 
2.2 Bayesian Method for Chinese RST Parser 
For Chinese RST parser, there are two tasks. One 
is the segmentation of edu and the other is the 
relation tagging between two semantic spans. 
Feature Meaning 
F1(F6) left(right) child is a syntactic sub-tree? 
F2(F5) left(right) child ends with a punctuation? 
F3(F4) cue words of left (right) child. 
F7 left and right children are sibling nodes? 
F8(F9) syntactic head symbol of left(right) child. 
Table 1: 9 features used in our Bayesian model 
Inspired by the features used in English RST 
parser (Soricut and Marcu, 2003; Reitter, 2003; 
Duverle and Prendinger, 2009; Hernault et al, 
2010a), we design a Bayesian model to build a 
joint parser for segmentation and tagging simul-
taneously. In this model, 9 features in Table 1 are 
used. In the table, punctuations include comma, 
semicolons, period and question mark. We view 
explicit connectives as cue words in this paper. 
Figure 2 illustrates the conditional independ-
ences of 9 features which are denoted with F1~F9. 
 
The segmentation and parsing conditional 
probabilities are computed as follows: 
P (mjF 91 ) = P (mjF 31 ; F8) (1)
P (ejF 91 ) = P (ejF 74 ; F9) (2)
P (ReljF 91 ) = P (ReljF 43 ) (3) 
where Fn represents the nth  feature , F ln means 
features from n  to l . Rel  is short for relation. (1) 
and (2) describe the conditional probabilities of 
m and e. When using Formula (3) to predict the 
relation, we search all the cue-words pair, as 
shown in Figure 1, to get the best match. When 
training, we use maximum likelihood estimation 
to get al the associated probabilities. For decod-
ing, the pseudo codes are given as below. 
 
m e
F1 F2 F3
Rel
F4 F5 F6 F7F8 F9
 
Figure 2: The graph for conditional independences 
of 9 features. 
1: Nodes={[]} 
2: Parser(0,End)  
3: Parser(s,e): // recursive parser function 
4:    if s > e or e is -1: return -1; 
5:    m = GetMaxM(s,e)  //compute m through Formu-
la(1);if no cue words found, 
then m=-1; 
6:      e? = GetMaxE(s,m,e)  //compute e? through F (2); 
7:    if m or e? equals to -1: return -1; 
8:   Rel=GetRelation(s,m,e?) //compute relation by F 
(3) 
9:    push [Rel,s,m,e?] into Nodes   
10:  Parser(s,m)  
11:  Parser(m+1,e?) 
12:  Parser(e?+1,e) 
13:   Rel=GetRelation(s,e?,e) 
14:   push [Rel,s,e?,e] into Nodes 
15:   return e 
J?sh?     l?b?    du? me?yu?n  de m?ngy?   hu?l? xi?ji?ng  le    ,
??       ??       ?    ??          ?   ??          ??     ??      ?    ?
  0               1           2         3              4        5                6            7          8     9
y?uy?  g?o t?ngzh?ng ,
 ??       ?    ??            ?
   10          11      12            13
q?    sh?j?   hu?l?    y?  sh?  sh?ngsh?ng de    .
?    ??      ??      ?   ?           ??          ?    ?
14      15           16        17   18             19            20   21   
Although the rupee's nominal rate against the dollar was held down , India's real exchange rate rose because of  high inflation . 
Reason 
Antithesis
U1:[0,9] U2:[10,21]
U1:[10,13] U2:[14,21]
Cue-words pair matching set of cue words for span [0,9] and [10,21]:{??/??,??/NULL,NULL/??}
Cue-words pair matching set of cue words for span [10,13] and [14,21]:{??/NULL}
RST-based Rules:  Antithesis:: ??[X]/[Y] => Although[X]/[Y] ;  Reason::??[X]/[Y] => [Y]/because of[X]
Example 1:
Figure 1: An example of Chinese RST tree and its word alignment of the corresponding English string. 
371
For example in Figure 1, for the first iteration, 
s=0 and m will be chosen from {1-20}. We get 
m=9 through Formula (1). Then, similar with m, 
we get e=21 through Formula (2). Finally, the 
relation is figured out by Formula (3). Thus, a 
node is generated. A complete RST tree con-
structs until the end of the iterative process for 
this sentence. This method can run fast due to the 
simple greedy algorithm.  It is plausible in our 
cases, because we only have a small scale of 
manually-annotated Chinese RST corpus, which 
prefers simple rather than complicated models.  
3 Translation Model 
3.1 Rule Extraction 
As shown in Figure 1, the RST tree-to-string 
alignment provides us with two types of transla-
tion rules. One is common phrase-based rules, 
which are just like those in phrase-based model 
(Koehn et al, 2003). The other is RST tree-to-
string rule, and it?s defined as, 
relation ::U1(?;X)=U2(?; Y )
) U1(tr(?); tr(X)) ? U2(tr(?); tr(Y ))
 
where the terminal characters ? and ? represent 
the cue words which are optimum match for 
maximizing Formula (3). While the non-
terminals X and Y represent the rest of the se-
quence. Function tr(? ) means the translation 
of ?. The operator ~ is an operator to indicate 
that the order of tr(U1) and tr(U2) is monotone or 
reverse. During rules? extraction, if the mean 
position of all the words in tr(U1) precedes that 
in tr(U2), ~ is monotone. Otherwise, ~ is reverse.   
For example in Figure 1, the Reason relation 
controls U1:[10,13] and U2:[14,21]. Because the 
mean position of tr(U2) is before that of tr(U1), 
the reverse order is selected. We list the RST-
based rules for Example 1 in Figure 1. 
3.2 Probabilities Estimation  
For the phrase-based translation rules, we use 
four common probabilities and the probabilities? 
estimation is the same with those in (Koehn et al, 
2003). While the probabilities of RST-based 
translation rules are given as follows,  
(1) P(rejrf ;Rel) = Count(re;rf ;relation)Count(rf ;relation)
:  where re  
is the target side of the rule, ignorance of the or-
der, i.e. U1(tr(?); tr(X)) ? U2(tr(?); tr(Y )) with 
two directions, rf is the source side, i.e. 
U1(?;X)=U2(?;Y ), and Rel  means the relation 
type.  
(2) P(?jre; rf ;Rel) = Count(?;re;rf ;relation)Count(re;rf ;relation)
: 
? 2 fmonotone; reverseg. It is the conditional 
probability of re-ordering. 
4 Decoding 
The decoding procedure of a discourse can be 
derived from the original decoding formula 
eI1 = argmaxeI1P (eI1jfJ1 )
. Given the rhetorical 
structure of a source sentence and the corre-
sponding rule-table, the translating process is to 
find an optimal path to get the highest score un-
der structure constrains, which is, 
argmaxesfP (esj; ft)g
= argmaxesf
Y
fn2ft
P (eu1; eu2; ? jfn)g
 
where ft  is a source  RST tree combined by a set 
of node fn . es is the target string combined by 
series of en  (translations of fn ).  fn  consists of  
U1 and U2. eu1  and eu2  are translations of U1 and 
U2 respectively. This global optimization prob-
lem is approximately simplified to local optimi-
zation to reduce the complexity,  
Y
fn2ft
argmaxenfP(eu1; eu2; ? jfn)g
 
In our paper, we have the following two ways 
to factorize the above formula, 
Decoder 1: 
P (eu1; eu2; ? jfn)
= P (ecp; eX ; eY ; ? jfcp; fX ; fY )
= P (ecpjfcp)P (? jecp; fcp)P (eX jfX )P (eY jfY )
= P (rejrf ;Rel)P (? jre; rf ; Rel)P (eX jfX )P (eY jfY )   
where eX, eY are the translation of non-terminal 
parts. fcp  and ecp  are cue-words pair of source 
and target sides. The first and second factors are 
just the probabilities introduced in Section 3.2. 
After approximately simplified to local optimiza-
tion, the final formulae are re-written as, 
argmaxrfP (rejrf ; Rel)P (? jre; rf ; Rel)g (4)
argmaxeX fP (eX jfX )g (5)
argmaxeY fP (eY jfY )g (6)
 
Taking the source sentence with its RST tree 
in Figure 1 for instance, we adopt a bottom-up 
manner to do translation recursively. Suppose the 
best rules selected by (4) are just those written in 
the figure, Then span [11,13] and [14,21] are 
firstly translated by (5) and (6). Their translations 
are then re-packaged by the rule of Reason-
[10,13,21]. Iteratively, the translations of span 
[1,9] and [10,21] are re-packaged by the rule of 
Antithesis-[0,9,21] to form the final translation.  
372
Decoder 2 : Suppose that the translating process 
of two spans U1 and U2 are independent of each 
other, we rewrite P(eu1; eu2; ? jfn)   as follows, 
P (eu1; eu2; ? jfn)
= P (eu1; eu2; ? jfu1; fu2)
= P (eu1jfu1)P (eu2jfu2)P (? jrf ; Rel)
= P (eu1jfu1)P (eu2jfu2)
X
re
P (? jre; rf ; Rel)P (rejrf ; Rel)
after approximately simplified to local optimization, 
the final formulae are re-written as below,
 
argmaxeu1fPr(eu1jfu1)g (7)
argmaxeu2fPr(eu2jfu2)g (8)
argmaxrf
X
e
Pr(? jre; rf ; Rel)Pr(rejrf ; Rel)g (9)
 
We also adopt the bottom-up manner similar 
to Decoder 1. In Figure 1, U1 and U2 of Reason 
node are firstly translated. Their translations are 
then re-ordered. Then the translations of two 
spans of Antithesis node are re-ordered and con-
structed into the final translation. In Decoder 2, 
the minimal translation-unit is edu. While in De-
coder 1, an edu is further split into cue-word part 
and the rest part to obtain the respective transla-
tion.  
In our decoders, language model(LM) is used 
for translating edus in Formula(5),(6),(7),(8), but 
not for reordering the upper spans because with 
the bottom-to-up combination, the spans become 
longer and harder to be judged by a traditional 
language model. So we only use RST rules to 
guide the reordering.  But LM will be properly 
considered in our future work. 
5 Experiment 
5.1 Setup 
In order to do Chinese RST parser, we annotated 
over 1,000 complicated sentences on CTB (Xue 
et al, 2005), among which 1,107 sentences are 
used for training, and 500 sentences are used for 
testing. Berkeley parser2 is used for getting the 
syntactic trees.  
The translation experiment is conducted on 
Chinese-to-English direction. The bilingual train-
ing data is from the LDC corpus3. The training 
corpus contains 2.1M sentence pairs. We obtain 
the word alignment with the grow-diag-final-and 
strategy by GIZA++4. A 5-gram language model 
is trained on the Xinhua portion of the English 
                                                 
2
 http://code.google.com/p/berkeleyparser/ 
3
 LDC category number : LDC2000T50, LDC2002E18, 
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, 
LDC2005T10 and LDC2005T34 
4 http://code.google.com/p/giza-pp/ 
Gigaword corpus. For tuning and testing, we use 
NIST03 evaluation data as the development set, 
and extract the relatively long and complicated 
sentences from NIST04, NIST05 and CWMT085  
evaluation data as the test set. The number and 
average word-length of sentences are 511/36, 
320/34, 590/38 respectively. We use case-
insensitive BLEU-4 with the shortest length pen-
alty for evaluation.  
To create the baseline system, we use the 
toolkit Moses6 to build a phrase-based translation 
system. Meanwhile, considering that Xiong et al 
(2009) have presented good results by dividing 
long and complicated sentences into sub-
sentences only by punctuations during decoding, 
we re-implement their method for comparison. 
5.2 Results of Chinese RST Parser 
Table 2 shows the results of RST parsing. On 
average, our RS trees are 2 layers deep. The 
parsing errors mostly result from the segmenta-
tion errors, which are mainly caused by syntactic 
parsing errors. On the other hand, the polyse-
mous cue words, such as ??(but, and, thus)? 
may lead ambiguity for relation recognition, be-
cause they can be clues for different relations.  
Task Precision Recall F1 
Segmentation 0.74 0.83 0.78 
Labeling 0.71 0.78 0.75 
Table 2: Segmentation and labeling result. 
5.3 Results of Translation 
Table 3 presents the translation comparison re-
sults. In this table, XD represents the method in 
(Xiong et al, 2009). D1 stands for Decoder-1, 
and D2 for Decoder-2. Values with boldface are 
the highest scores in comparison. D2 performs 
best on the test data with 2.3/0.77/1.43/1.16 
points. Compared with XD, our results also out-
perform by 0.52 points on the whole test data. 
Observing and comparing the translation re-
sults, we find that our translation results are more 
readable by maintaining the semantic integrality 
of the edus and by giving more appreciate reor-
ganization of the translated edus. 
Testing Set Baseline XD D1 D2 
NIST04 29.39 31.52 31.34 31.69 
NIST05 29.86 29.80 30.28 30.63 
CWMT08 24.31 25.24 25.74 25.74 
ALL 27.85 28.49 28.66 29.01 
Table 3: Comparison with related models. 
                                                 
5
 China Workshop on Machine Translation 2008 
6
 www.statmt.org/moses/index.php?n=Main.HomePage 
373
6 Conclusion and Future Work 
In this paper, we present an RST-based transla-
tion framework for modeling semantic structures 
in translation model, so as to maintain the se-
mantically functional integrity and hierarchical 
relations of edus during translating. With respect 
to the existing models, we think our translation 
framework works more similarly to what human 
does, and we believe that this research is a cru-
cial step towards discourse-oriented translation. 
In the next step, we will study on the implicit 
discourse relations for Chinese and further modi-
fy the RST-based framework. Besides, we will 
try to combine other current translation models 
such as syntactic model and hierarchical model 
into our framework. Furthermore, the more accu-
rate evaluation metric for discourse-oriented 
translation will be further studied. 
 
Acknowledgments 
The research work has been funded by the Hi-Tech 
Research and Development Program (?863? Pro-
gram) of China under Grant No. 2011AA01A207, 
2012AA011101, and 2012AA011102 and also sup-
ported by the Key Project of Knowledge Innova-
tion Program of Chinese Academy of Sciences un-
der Grant No.KGZD-EW-501. 
References  
David A Duverle and Helmut Prendinger. 2009. A 
novel discourse parser based on support vector ma-
chine classification. In Proceedings of the Joint 
Conference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Volume 
2-Volume 2, pages 665?673. Association for Com-
putational Linguistics. 
Zhengxian Gong, Min Zhang, and Guodong Zhou. 
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, pages 909?919. Association for Computa-
tional Linguistics. 
Hugo Hernault, Danushka Bollegala, and Mitsuru 
Ishizuka. 2010a. A sequential model for discourse 
segmentation. Computational Linguistics and Intel-
ligent Text Processing, pages 315?326. 
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, 
et al 2010b. Hilda: A discourse parser using sup-
port vector machine classification. Dialogue & 
Discourse, 1(3). 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology 
Volume 1, pages 48?54. Association for Computa-
tional Linguistics. 
William C Mann and Sandra A Thompson. 1986. 
Rhetorical structure theory: Description and con-
struction of text structures. Technical report, DTIC 
Document. 
William C Mann and Sandra A Thompson. 1987. 
Rhetorical structure theory: A framework for the 
analysis of texts. Technical report, DTIC Docu-
ment. 
William C Mann and Sandra A Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243?281. 
Daniel Marcu, Lynn Carlson, and Maki Watanabe. 
2000. The automatic translation of discourse struc-
tures. In Proceedings of the 1st North American 
chapter of the Association for Computational Lin-
guistics conference, pages 9?17. Morgan Kauf-
mann Publishers Inc. 
David Reitter. 2003. Simple signals for complex rhet-
orics: On rhetorical analysis with rich-feature sup-
port vector models. Language, 18:52. 
Radu Soricut and Daniel Marcu. 2003. Sentence level 
discourse parsing using syntactic and lexical in-
formation. In Proceedings of the 2003 Conference 
of the North American Chapter of the Association 
for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 149?156. As-
sociation for Computational Linguistics. 
Billy TM Wong and Chunyu Kit. 2012. Extending 
machine translation evaluation metrics with lexical 
cohesion to document level. In Proceedings of the 
2012 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning, page 1060?1068. As-
sociation for Computational Linguistics. 
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 
2011. Document-level consistency verification in 
machine translation. In Machine Translation Sum-
mit, volume 13, pages 131?138. 
Hao Xiong, Wenwen Xu, Haitao Mi, Yang Liu, and 
Qun Liu. 2009. Sub-sentence division for tree-
based machine translation. In Proceedings of the 
ACL-IJCNLP 2009 Conference Short Papers, pag-
es 137?140. Association for Computational Lin-
guistics. 
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta 
Palmer. 2005. The Penn Chinese treebank: Phrase 
structure annotation of a large corpus. Natural 
Language Engineering, 11(2):207. 
Ming Yue. 2008. Rhetorical structure annotation of 
Chinese news commentaries. Journal of Chinese 
Information Processing, 4:002. 
374
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521?525,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
 
Dual Training and Dual Prediction for Polarity Classification 
 
Rui Xia, Tao Wang, Xuelei Hu 
Department of Computer Science 
Nanjing University of  
Science and Technology 
rxia@njust.edu.cn,  
linclonwang@163.com, 
xlhu@njust.edu.cn 
Shoushan Li 
NLP Lab 
Department of  
Computer Science 
Soochow University 
shoushan.li 
@gmail.com 
Chengqing Zong 
National Lab of  
Pattern Recognition 
Institute of Automation 
CAS 
cqzong 
@nlpr.ia.ac.cn 
 
 
Abstract 
Bag-of-words (BOW) is now the most popular 
way to model text in machine learning based 
sentiment classification. However, the perfor-
mance of such approach sometimes remains 
rather limited due to some fundamental defi-
ciencies of the BOW model. In this paper, we 
focus on the polarity shift problem, and pro-
pose a novel approach, called dual training and 
dual prediction (DTDP), to address it. The 
basic idea of DTDP is to first generate artifi-
cial samples that are polarity-opposite to the 
original samples by polarity reversion, and 
then leverage both the original and opposite 
samples for (dual) training and (dual) predic-
tion. Experimental results on four datasets 
demonstrate the effectiveness of the proposed 
approach for polarity classification.  
1 Introduction 
The most popular text representation model in 
machine learning based sentiment classification 
is known as the bag-of-words (BOW) model, 
where a piece of text is represented by an unor-
dered collection of words, based on which stand-
ard machine learning algorithms are employed as 
classifiers. Although the BOW model is simple 
and has achieved great successes in topic-based 
text classification, it disrupts word order, breaks 
the syntactic structures and discards some kinds 
of semantic information that are possibly very 
important for sentiment classification. Such dis-
advantages sometimes limit the performance of 
sentiment classification systems.  
A lot of subsequent work focused on feature 
engineering that aims to find a set of effective 
features based on the BOW representation. How-
ever, there still remain some problems that are 
not well addressed. Out of them, the polarity 
shift problem is the biggest one. 
We refer to ?polarity shift? as a linguistic phe-
nomenon that the sentiment orientation of a text 
is reversed (from positive to negative or vice ver-
sa) because of some particular expressions called 
polarity shifters. Negation words (e.g., ?no?, ?not? 
and ?don?t?) are the most important type of po-
larity shifter. For example, by adding a negation 
word ?don?t? to a positive text ?I like this book? 
in front of ?like?, the orientation of the text is 
reversed from positive to negative.  
Naturally, handling polarity shift is very im-
portant for sentiment classification. However, the 
BOW representations of two polarity-opposite 
texts, e.g., ?I like this book? and ?I don?t like this 
book?, are considered to be very similar by most 
of machine learning algorithms. Although some 
methods have been proposed in the literature to 
address the polarity shift problem (Das and Chen, 
2001; Pang et al, 2002; Na et al, 2004; Kenndey 
and Inkpen, 2006; Ikeda et al, 2008; Li and 
Huang, 2009; Li et al, 2010), the state-of-the-art 
results are still far from satisfactory. For example, 
the improvements are less than 2% after consid-
ering polarity shift in Li et al (2010). 
In this work, we propose a novel approach, 
called dual training and dual prediction (DTDP), 
to address the polarity shift problem. By taking 
advantage of the unique nature of polarity classi-
fication, DTDP is motivated by first generating 
artificial samples that are polarity-opposite to the 
original ones. For example, given the original 
sample ?I don?t like this book. It is boring,? its 
polarity-opposite version, ?I like this book. It is 
interesting?, is artificially generated. Second, the 
original and opposite training samples are used 
together for training a sentiment classifier (called 
dual training), and the original and opposite test 
samples are used together for prediction (called 
dual prediction). Experimental results prove that 
the procedure of DTDP is very effective at cor-
recting the training and prediction errors caused 
521
 by polarity shift, and it beats other alternative 
methods of considering polarity shift. 
2 Related Work 
The lexicon-based sentiment classification sys-
tems can be easily modified to include polarity 
shift. One common way is to directly reverse the 
sentiment orientation of polarity-shifted words, 
and then sum up the orientations word by word 
(Hu and Liu, 2004; Kim and Hovy, 2004; Po-
lanyi and Zaenen, 2004; Kennedy and Inkpen, 
2006). Wilson et al (2005) discussed other com-
plex negation effects by using conjunctive and 
dependency relations among polarity words. Alt-
hough handling polarity shift is easy and effec-
tive in term-counting systems, they rarely outper-
form the baselines of machine learning methods 
(Kennedy, 2006). 
The machine learning methods are generally 
more effective for sentiment classification. How-
ever, it is difficult to handle polarity shift based 
on the BOW model. Das and Chen (2001) pro-
posed a method by simply attaching ?NOT? to 
words in the scope of negation, so that in the text 
?I don?t like book?, the word ?like? is changed to 
a new word ?like-NOT?. There were also some 
attempts to model polarity shift by using more 
complex linguistic features (Na et al, 2004; 
Kennedy and Inkpen, 2006). But the improve-
ments upon the baselines of machine learning 
systems are very slight (less than 1%). 
Ikeda et al (2008) proposed a machine learn-
ing method, to model polarity-shifters for both 
word-wise and sentence-wise sentiment classifi-
cation, based on a dictionary extracted from 
General Inquirer. Li and Huang (2009) proposed 
a method first to classify each sentence in a text 
into a polarity-unshifted part and a polarity-
shifted part according to certain rules, then to 
represent them as two bag-of-words for senti-
ment classification. Li et al (2010) further pro-
posed a method to separate the shifted and un-
shifted text based on training a binary detector. 
Classification models are then trained based on 
each of the two parts. An ensemble of two com-
ponent parts is used at last to get the final polari-
ty of the whole text. 
3 The Proposed Approach 
We first present the method for generating artifi-
cial polarity-opposite samples, and then intro-
duce the algorithm of dual training and dual pre-
diction (DTDP). 
3.1 Generating Artificial Polarity-Opposite 
Samples 
Given an original sample and an antonym dic-
tionary (e.g., WordNet 1 ), a polarity-opposite 
sample is generated artificially according to the 
following rules: 
1) Sentiment word reversion: All sentiment 
words out of the scope of negation are re-
versed to their antonyms; 
2) Handling negation: If there is a negation 
expression, we first detect the scope of nega-
tion, and then remove the negation words 
(e.g., ?no?, ?not?, and ?don?t?). The senti-
ment words in the scope of negation are not 
reversed; 
3) Label reversion: The class label of the la-
beled sample is also reversed to its opposite 
(i.e., Positive to Negative, or vice versa) as 
the class label of newly generated samples 
(called polarity-opposite samples). 
Let us use a simple example to explain the 
generation process. Given the original sample: 
The original sample 
Text:   I don?t like this book. It is boring. 
Label: Negative 
According to Rule 1, ?boring? is reversed to 
its antonym ?interesting?; According to Rule 2, 
the negation word ?don?t? is removed, and ?like? 
is not reversed; According to Rule 3, the class 
label Negative is reversed to Positive. Finally, an 
artificial polarity-opposite sample is generated: 
The generated opposite sample 
Text:   I like this book. It is interesting. 
Label: Positive 
All samples in the training and test set are re-
versed to their polarity-opposite versions. We 
refer to them as ?opposite training set? and ?op-
posite test set?, respectively. 
3.2 Dual Training and Dual Prediction 
In this part, we introduce how to make use of the 
original and opposite training/test data together 
for dual training and dual prediction (DTDP). 
Dual Training: Let D = f(xi; yi)gNi=1 and 
~D = f(~xi; ~yi)gNi=1 be the original and opposite 
training set respectively, where x  denotes the 
feature vector, y  denotes the class label, and N  
denotes the size of training set. In dual training, 
D [ ~D  are used together as training data to learn 
                                                 
1 http://wordnet.princeton.edu/ 
522
 a classification model. The size of training data 
is doubled in dual training. 
Suppose the example in Section 3.1 is used as 
one training sample. As far as only the original 
sample (?I don?t like this book. It is boring.?) is 
considered, the feature ?like? will be improperly 
recognized as a negative indicator (since the 
class label is Negative), ignoring the expression 
of negation. Nevertheless, if the generated oppo-
site sample (?I like this book. It is interesting.?) 
is also used for training, ?like? will be learned 
correctly, due to the removal of negation in sam-
ple reversion. Therefore, the procedure of dual 
training can correct some learning errors caused 
by polarity shift. 
Dual Prediction: Given an already-trained 
classification model, in dual prediction, the orig-
inal and opposite test samples are used together 
for prediction. In dual prediction, when we pre-
dict the positive degree of a test sample, we 
measure not only how positive the original test 
sample is, but also how negative the opposite 
sample is.  
Let x  and ~x  denote the feature vector of the 
original and opposite test samples respectively; 
let pd(cjx)  and pd(cj~x)  denote the predictions of 
the original and opposite test sample, based on 
the dual training model. The dual predicting 
function is defined as: 
pd(+jx; ~x) = (1?a)pd(+jx)+apd(?j~x), 
pd(?jx; ~x) = (1?a)pd(?jx)+apd(+j~x), 
where a  (06 a6 1 ) is the weight of the oppo-
site prediction.  
Now suppose the example in Section 3.1 is a 
test sample. As far as only the original test sam-
ple (?I don?t like this book. It is boring.?) is used 
for prediction, it is very likely that it is falsely 
predicted as Positive, since ?like? is a strong pos-
itive feature, despite that it is in the scope of ne-
gation. While in dual prediction, we still measure 
the ?sentiment-opposite? degree of the opposite 
test sample (?I like this book. It is interesting.?). 
Since negation is removed, it is very likely that 
the opposite test sample is assigned with a high 
positive score, which could compensate the pre-
diction errors of the original test sample. 
Final Output: It should be noted that alt-
hough the artificially generated training and test-
ing data are helpful in most cases, they still pro-
duce some noises (e.g., some poorly generated 
samples may violate the quality of the original 
data set). Therefore, instead of using all dual 
predictions as the final output, we use the origi-
nal prediction po(cjx)  as an alternate, in case that 
the dual prediction pd(cjx; ~x)  is not enough con-
fident, according to a confidence threshold t . The 
final output is defined as: 
pf(cjx) =
? pd(cjx; ~x); if?p > tpo(cjx); if?p < t
 
where ?p= pd(cjx; ~x)?po(cjx). 
4 Experimental Study 
4.1 Datasets 
The Multi-Domain Sentiment Datasets2 are used 
for evaluations. They consist of product reviews 
collected from four different domains: Book, 
DVD, Electronics and Kitchen. Each of them 
contains 1,000 positive and 1,000 negative re-
views. Each of the datasets is randomly spit into 
5 folds, with four folds serving as training data, 
and the remaining one fold serving as test data. 
All of the following results are reported in terms 
of an average of 5-fold cross validation. 
4.2 Evaluated Systems 
We evaluate four machine learning systems that 
are proposed to address polarity shift in docu-
ment-level polarity classification: 
1) Baseline: standard machine learning meth-
ods based on the BOW model, without han-
dling polarity shift;  
2) Das-2001: the method proposed by Das and 
Chen (2001), where ?NOT? is attached to the 
words in the scope of negation as a prepro-
cessing step; 
3) Li-2010: the approach proposed by Li et al 
(2010). The details of the algorithm is intro-
duced in related work; 
4) DTDP: our approach proposed in Section 3. 
The WordNet dictionary is used for sample 
reversion. The empirical value of the param-
eter a  and t  are used in the evaluation.  
4.3 Comparison of the Evaluated Systems 
In table 1, we report the classification accuracy 
of four evaluated systems using unigram features. 
We consider two widely-used classification algo-
rithms: SVM and Na?ve Bayes. For SVM, the 
LibSVM toolkit3 is used with a linear kernel and 
the default penalty parameter. For Na?ve Bayes, 
the OpenPR-NB toolkit4 is used. 
                                                 
2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
4 http://www.openpr.org.cn  
523
 Dataset 
SVM Na?ve Bayes 
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP 
Book 0.745 0.763 0.760 0.800 0.779 0.783 0.792 0.814 
DVD 0.764 0.771 0.795 0.823 0.795 0.793 0.810 0.820 
Electronics 0.796 0.813 0.812 0.828 0.815 0.827 0.824 0.841 
Kitchen 0.822 0.820 0.844 0.849 0.830 0.847 0.840 0.859 
Avg. 0.782 0.792 0.803 0.825 0.804 0.813 0.817 0.834 
Table 1: Classification accuracy of different systems using unigram features 
Dataset 
SVM Na?ve Bayes 
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP 
Book 0.775 0.777 0.788 0.818 0.811 0.815 0.822 0.840 
DVD 0.790 0.793 0.809 0.828 0.824 0.826 0.837 0.868 
Electronics 0.818 0.834 0.841 0.848 0.841 0.857 0.852 0.866 
Kitchen 0.847 0.844 0.870 0.878 0.878 0.879 0.883 0.896 
Avg. 0.808 0.812 0.827 0.843 0.839 0.844 0.849 0.868 
Table 2: Classification accuracy of different systems using both unigram and bigram features 
Compared to the Baseline system, the Das-
2001 approach achieves very slight improve-
ments (less than 1%). The performance of Li-
2010 is relatively effective: it improves the aver-
age score by 0.21% and 0.13% on SVM and Na-
?ve Bayes, respectively. Yet, the improvements 
are still not satisfactory. 
As for our approach (DTDP), the improve-
ments are remarkable. Compared to the Baseline 
system, the average improvements are 4.3% and 
3.0% on SVM and Na?ve Bayes, respectively. In 
comparison with the state-of-the-art (Li-2010), 
the average improvement is 2.2% and 1.7% on 
SVM and Na?ve Bayes, respectively. 
We also report the classification accuracy of 
four systems using both unigrams and bigrams 
features for classification in Table 2. From this 
table, we can see that the performance of each 
system is improved compared to that using uni-
grams. It is now relatively difficult to show im-
provements by incorporating polarity shift, be-
cause using bigrams already captured a part of 
negations (e.g., ?don?t like?).  
The Das-2001 approach still shows very lim-
ited improvements (less than 0.5%), which 
agrees with the reports in Pang et al (2002). The 
improvements of Li-2010 are also reduced: 1.9% 
and 1% on SVM and Na?ve Bayes, respectively.  
Although the improvements of the previous 
two systems are both limited, the performance of 
our approach (DTDP) is still sound. It improves 
the Baseline system by 3.7% and 2.9% on SVM 
and Na?ve Bayes, respectively, and outperforms 
the state-of-the-art (Li-2010) by 1.6% and 1.9% 
on SVM and Na?ve Bayes, respectively. 
5 Conclusions 
In this work, we propose a method, called dual 
training and dual prediction (DTDP), to address 
the polarity shift problem in sentiment classifica-
tion. The basic idea of DTDP is to generate arti-
ficial samples that are polarity-opposite to the 
original samples, and to make use of both the 
original and opposite samples for dual training 
and dual prediction. Experimental studies show 
that our DTDP algorithm is very effective for 
sentiment classification and it beats other alterna-
tive methods of considering polarity shift.  
One limitation of current work is that the tun-
ing of parameters in DTDP (such as a  and t ) is 
not well discussed. We will leave this issue to an 
extended version. 
Acknowledgments 
The research work is supported by the Jiangsu 
Provincial Natural Science Foundation of China 
(BK2012396), the Research Fund for the Doc-
toral Program of Higher Education of China 
(20123219120025), and the Open Project Pro-
gram of the National Laboratory of Pattern 
Recognition (NLPR). This work is also partly 
supported by the Hi-Tech Research and Devel-
opment Program of China (2012AA011102 and 
2012AA011101), the Program of Introducing 
Talents of Discipline to Universities (B13022), 
and the Open Project Program of the Jiangsu Key 
Laboratory of Image and Video Understanding 
for Social Safety (30920130122006).  
524
 References  
S. Das and M. Chen. 2001. Yahoo! for Amazon: 
Extracting market sentiment from stock mes-
sage boards. In Proceedings of the Asia Pacif-
ic Finance Association Annual Conference. 
M. Hu and B. Liu. 2004. Mining opinion features 
in customer reviews. In Proceedings of the 
National Conference on Artificial Intelligence 
(AAAI). 
D. Ikeda, H. Takamura L. Ratinov M. Okumura. 
2008. Learning to Shift the Polarity of Words 
for Sentiment Classification. In Proceedings 
of the International Joint Conference on Natu-
ral Language Processing (IJCNLP).  
S. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceeding of the Inter-
national Conference on Computational Lin-
guistics (COLING). 
A. Kennedy and D. Inkpen. 2006. Sentiment 
classification of movie reviews using contex-
tual valence shifters. Computational Intelli-
gence, 22:110?125. 
S. Li and C. Huang. 2009. Sentiment classifica-
tion considering negation and contrast transi-
tion. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Com-
putation (PACLIC). 
S. Li, S. Lee, Y. Chen, C. Huang and G. Zhou. 
2010. Sentiment Classification and Polarity 
Shifting. In Proceeding of the International 
Conference on Computational Linguistics 
(COLING). 
J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 
2004. Effectiveness of simple linguistic pro-
cessing in automatic sentiment classification 
of product reviews. In Proceeding of the Con-
ference of the International Society for 
Knowledge Organization. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. 
Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings 
of the Conference on Empirical Methods in 
Natural Language Processing (EMNLP). 
L. Polanyi and A. Zaenen. 2004. Contextual lex-
ical valence shifters. In Proceedings of the 
AAAI Spring Symposium on Exploring Attitude 
and Affect in Text, AAAI technical report. 
P. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceeding of the 
Annual Meeting of the Association for Compu-
tational Linguistics (ACL). 
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. 
Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of 
the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). 
 
525
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 623?627,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lattice-based Framework for Joint Chinese Word Segmentation, 
POS Tagging and Parsing 
Zhiguo Wang1, Chengqing Zong1 and Nianwen Xue2 
1National Laboratory of Pattern Recognition, 
Institute of Automation, Chinese Academy of Sciences, Beijing, China, 100190 
2Computer Science Department, Brandeis University, Waltham, MA 02452 
{zgwang, cqzong}@nlpr.ia.ac.cn   xuen@brandeis.edu 
 
Abstract 
For the cascaded task of Chinese word seg-
mentation, POS tagging and parsing, the pipe-
line approach suffers from error propagation 
while the joint learning approach suffers from 
inefficient decoding due to the large combined 
search space. In this paper, we present a novel 
lattice-based framework in which a Chinese 
sentence is first segmented into a word lattice, 
and then a lattice-based POS tagger and a lat-
tice-based parser are used to process the lattice 
from two different viewpoints: sequential POS 
tagging and hierarchical tree building. A strat-
egy is designed to exploit the complementary 
strengths of the tagger and parser, and encour-
age them to predict agreed structures. Experi-
mental results on Chinese Treebank show that 
our lattice-based framework significantly im-
proves the accuracy of the three sub-tasks. 
1 Introduction 
Previous work on syntactic parsing generally 
assumes a processing pipeline where an input 
sentence is first tokenized, POS-tagged and then 
parsed (Collins, 1999; Charniak, 2000; Petrov 
and Klein, 2007). This approach works well for 
languages like English where automatic tokeni-
zation and POS tagging can be performed with 
high accuracy without the guidance of the high-
level syntactic structure. Such an approach, how-
ever, is not optimal for languages like Chinese 
where there are no natural delimiters for word 
boundaries, and word segmentation (or tokeniza-
tion) is a non-trivial research problem by itself. 
Errors in word segmentation would propagate to 
later processing stages such as POS tagging and 
syntactic parsing. More importantly, Chinese is a 
language that lacks the morphological clues that 
help determine the POS tag of a word. For ex-
ample, ??  (?investigate/investigation?) can 
either be a verb (?investigate?) or a noun (?inves-
tigation?), and there is no morphological varia-
tion between its verbal form and nominal form. 
This contributes to the relatively low accuracy 
(95% or below) in Chinese POS tagging when 
evaluated as a stand-alone task (Sun and Uszko-
reit, 2012), and the noun/verb ambiguity is a ma-
jor source of error.  
More recently, joint inference approaches 
have been proposed to address the shortcomings 
of the pipeline approach. Qian and Liu (2012) 
proposed a joint inference approach where syn-
tactic parsing can provide feedback to word 
segmentation and POS tagging and showed that 
the joint inference approach leads to improve-
ments in all three sub-tasks. However, a major 
challenge for joint inference approach is that the 
large combined search space makes efficient de-
coding and parameter estimation very hard.  
In this paper, we present a novel lattice-based 
framework for Chinese. An input Chinese sen-
tence is first segmented into a word lattice, 
which is a compact representation of a small set 
of high-quality word segmentations. Then, a lat-
tice-based POS tagger and a lattice-based parser 
are used to process the word lattice from two 
different viewpoints. We next employ the dual 
decomposition method to exploit the comple-
mentary strengths of the tagger and parser, and 
encourage them to predict agreed structures. Ex-
perimental results show that our lattice-based 
framework significantly improves the accuracies 
of the three sub-tasks  
2 The Lattice-based Framework 
Figure 1 gives the organization of the framework. 
There are four types of linguistic structures: a 
Chinese sentence, the word lattice, tagged word 
sequence and parse tree of the Chinese sentence. 
An example for each structure is provided in 
Figure 2. We can see that the terminals and pre-
terminals of a parse tree constitute a tagged word 
sequence. Therefore, we define a comparator 
between a tagged word sequence and a parse tree: 
if they contain the same word sequence and POS 
tags, they are equal, otherwise unequal. 
623
Figure 1 also shows the workflow of the 
framework. First, the Chinese sentence is seg-
mented into a word lattice using the word seg-
mentation system. Then the word lattice is fed 
into the lattice-based POS tagger to produce a 
tagged word sequence   and into the lattice-
based parser to separately produce a parse tree  . 
We then compare   with   to see whether they 
are equal. If they are equal, we output   as the 
final result. Otherwise, the guidance generator 
generates some guidance orders based on the 
difference between   and  , and guides the tag-
ger and the parser to process the lattice again. 
This procedure may iterate many times until the 
tagger and parser predict equal structures. 
 
 
The motivation to design such a framework is 
as follows. First, state-of-the-art word segmenta-
tion systems can now perform with high accura-
cy. We can easily get an F1 score greater than 
96%, and an oracle (upper bound) F1 score 
greater than 99%  for the word lattice (Jiang et 
al., 2008). Therefore, a word lattice provides us a 
good enough search space to allow sufficient 
interaction among word segmentation, POS tag-
ging and parsing systems. Second, both the lat-
tice-based POS tagger and the lattice-based pars-
er can select word segmentation from the word 
lattice and predict POS tags, but they do so from 
two different perspectives. The lattice-based POS 
tagger looks at a path in a word lattice as a se-
quence and performs sequence labeling based on 
linear local context, while the lattice-based pars-
er builds the parse trees in a hierarchical manner. 
They have different strengths with regard to 
word segmentation and POS tagging. We hypo-
thesize that exploring the complementary 
strengths of the tagger and parser would improve 
each of the sub-tasks. 
We build a character-based model (Xue, 2003) 
for the word segmentation system, and treat 
segmentation as a sequence labeling task, where 
each Chinese character is labeled with a tag. We 
use the tag set provided in Wang et al (2011) 
and use the same feature templates. We use the 
Maximum Entropy (ME) model to estimate the 
feature weights. To get a word lattice, we first 
generate N-best word segmentation results, and 
then compact the N-best lists into a word lattice 
by collapsing all the identical words into one 
edge. We also assign a probability to each edge, 
which is calculated by multiplying the tagging 
probabilities of each character in the word. 
    The goal of the lattice-based POS tagger is to 
predict a tagged word sequence   for an input 
word lattice  :   = argmax ?    ( ) ?  ( ) 
where     ( ) represents the set of all possible 
tagged word sequences derived from the word 
lattice  .  ( ) is used to map   onto a global fea-
ture vector, and   is the corresponding weight 
vector. We use the same non-local feature tem-
plates used in Jiang et al (2008) and a similar 
decoding algorithm. We use the perceptron algo-
rithm (Collins, 2002) for parameter estimation. 
Goldberg and Elhadad (2011) proposed a lat-
tice-based parser for Heberw based on the 
PCFG-LA model (Matsuzaki et al, 2005). We 
adopted their approach, but found the un-
weighted word lattice their parser takes as input 
to be ineffective for our Chinese experiments. 
Instead, we use a weighted lattice as input and 
weigh each edge in the lattice with the word 
probability. In our model, each syntactic catego-
ry   is split into multiple subcategories  [ ] by 
labeling a latent annotation  . Then, a parse tree 
????????????? 
Brown?s group will leave Shanghai to Guangzhou tonight. 
(a) Chinese Sentence 
 
 (b) Word Lattice 
?? ?????????
NR NRVVNRPNTP PU
??
NN
Brown .GuangzhougoShanghaileavetonightingroup  
(c) Tagged Word Sequence 
Brown
.
Guangzhou
go
Shanghai
leavetonight
ingroup
?? ?
?? ?
??
?
NR P
NT
NP
PP
VV
NR
NP
VP
PUNP
IP
VP
??
NN
NP NP
?
?
VV
NR
NP
VP
VP
 
(d) Parse Tree 
Figure 2: Linguistic structure examples. 
Chinese Sentence
Word Segmentation
Word Lattice
Lattice-based Parser Lattice-based POS Tagger
Guidance Generator
Parse Tree Tagged Word 
Sequence
The Final Parse Tree
No
Yes
Equal?
 
Figure 1: The lattice-based framework. 
624
  is refined into  [ ], where X is the latent an-
notation vector for all non-terminals in  . The 
probability of  [ ] is calculated as:  ( [ ]) =   ( [ ] ?  [ ] [ ]) ?  ( [ ] ?  )?  ( ) 
where the three terms are products of all syntac-
tic rule probabilities, lexical rule probabilities 
and word probabilities in  [ ] respectively. 
3 Combined Optimization Between The 
Lattice-based POS Tagger and The 
Lattice-based Parser  
We first define some variables to make it easier 
to compare a tagged word sequence   with a 
parse tree  . We define   as the set of all POS 
tags. For  , we define  ( ,  , )=1 if   contains a 
POS tag  ?   spanning from the i-th character 
to the j-th character, otherwise  ( ,  , ) = 0. We 
also define  ( ,  , #) = 1 if   contains the word 
spanning from the i-th character to the j-th cha-
racter, otherwise  ( ,  , #) = 0. Similarly, for  , 
we define  ( ,  , )=1 if   contains a POS tag  ?   spanning from the i-th character to the j-th 
character, otherwise  ( ,  ,  ) = 0. We also define  ( ,  , #)  = 1 if   contains the word spanning 
from the i-th character to the j-th character, oth-
erwise  ( ,  , #) = 0. Therefore,   and   are equal, 
only if  ( ,  ,  ) =  ( ,  ,  )  for all  ? [0,  ] ,  ? [ + 1,  ] and  ?  ? #, otherwise unequal. 
Our framework expects the tagger and the 
parser to predict equal structures and we formu-
late it as a constraint optimization problem:    ,   = argmax ,    ( ) +   ( ) 
Such that for all  ? [0, ] ,  ? [ + 1, ]  and  ?  ? #:  ( ,  ,  ) =  ( ,  , ) 
 
where   ( ) =  ?  ( )  is a scoring function 
from the viewpoint of the lattice-based POS tag-
ger, and   ( ) = log  ( ) is a scoring function 
from the viewpoint of the lattice-based parser.  
The dual decomposition (a special case of La-
grangian relaxation) method introduced in Ko-
modakis et al (2007) is suitable for this problem. 
Using this method, we solve the primal con-
straint optimization problem by optimizing the 
dual problem. First, we introduce a vector of La-
grange multipliers  ( ,  ,  )  for each equality 
constraint. Then, the Lagrangian is formulated as:  ( ,  ,  ) =   ( ) +   ( ) +   ( ,  , )( ( ,  ,  )?  ( ,  , )) , ,  
By grouping the terms that depend on   and  , 
we rewrite the Lagrangian as  ( , , ) =    ( ) +   ( ,  , ) ( ,  , ) , ,   +   ( )?  ( ,  , ) ( ,  , ) , ,   
Then, the dual objective is  ( ) = max ,  ( , , ) = max    ( ) +   ( ,  , ) ( ,  , ) , ,  + max    ( )?  ( ,  , ) ( ,  , ) , ,   
The dual problem is to find min  ( ). 
    We use the subgradient method (Boyd et al, 
2003) to minimize the dual. Following Rush et al 
(2010), we define the subgradient of   ( ) as:  ( ,  , ) =  ( ,  , )?  ( ,  ,  )  for all ( ,  , ) 
Then, adjust  ( ,  ,  ) as follows:   ( ,  , ) =  ( ,  , )?  ( ( ,  ,  )?  ( ,  , )) 
where  >0 is a step size. 
 
Algorithm 1 presents the subgradient method 
to solve the dual problem. The algorithm initia-
lizes the Lagrange multiplier values with 0 (line 
1) and then iterates many times. In each iteration, 
the algorithm finds the best   ( )  and   ( )  by 
running the lattice-based POS tagger (line 3) and 
the lattice-based parser (line 4). If   ( ) and    ( ) 
share the same tagged word sequence (line 5), 
then the algorithm returns the solution (line 6). 
Otherwise, the algorithm adjusts the Lagrange 
multiplier values based on the differences be-
tween    ( ) and   ( ) (line 8). A crucial point is 
that the argmax problems in line 3 and line 4 can 
be solved efficiently using the original decoding 
algorithms, because the Lagrange multiplier can 
be regarded as adjustments for lexical rule prob-
abilities and word probabilities.  
4 Experiments 
We conduct experiments on the Chinese Tree-
bank Version 5.0 and use the standard data split 
Algorithm 1: Combined Optimization 
1: Set  ( )( ,  , )=0, for all  ( ,  , ) 
2: For k=1 to K 
3:     ( ) ? argmax    ( ) + ?   (   )( ,  , ) ( ,  , )  , ,    
4:     ( ) ? argmax    ( )? ?   (   )( ,  ,  ) ( ,  ,  )  , ,   
5:   If  ( )( ,  ,  ) =  ( )( ,  ,  ) for all ( ,  ,  )  
6:      Return (  ( ),   ( )) 
7:   Else  
8:       ( )( ,  ,  ) =  (   )( ,  ,  ) ?  ( ( )( ,  ,  )?  ( )( ,  , ))  
 
625
(Petrov and Klein, 2007). The traditional evalua-
tion metrics for POS tagging and parsing are not 
suitable for the joint task. Following with Qian 
and Liu (2012), we redefine precision and recall 
by computing the span of a constituent based on 
character offsets rather than word offsets.  
4.1 Performance of the Basic Sub-systems 
We train the word segmentation system with 100 
iterations of the Maximum Entropy model using 
the OpenNLP toolkit. Table 1 shows the perfor-
mance. It shows that our word segmentation sys-
tem is comparable with the state-of-the-art sys-
tems and the upper bound F1 score of the word 
lattice exceeds 99.6%. This indicates that our 
word segmentation system can provide a good 
search space for the lattice-based POS tagger and 
the lattice-based parser. 
 
To train the lattice-based POS tagger, we gen-
erate the word lattice for each sentence in the 
training set using cross validation approach. We 
divide the entire training set into 18 folds on av-
erage (each fold contains 1,000 sentences). For 
each fold, we segment each sentence in the fold 
into a word lattice by compacting 20-best seg-
mentation list produced with a model trained on 
the other 17 folds. Then, we train the lattice-
based POS tagger with 20 iterations of the aver-
age perceptron algorithm. Table 2 presents the 
joint word segmentation and POS tagging per-
formance and shows that our lattice-based POS 
tagger obtains results that are comparable with 
state-of-the-art systems. 
 
We implement the lattice-based parser by 
modifying the Berkeley Parser, and train it with 
5 iterations of the split-merge-smooth strategy 
(Petrov et al, 2006). Table 3 shows the perfor-
mance, where the ?Pipeline Parser? represents 
the system taking one-best segmentation result 
from our word segmentation system as input and 
?Lattice-based Parser? represents the system tak-
ing the compacted word lattice as input. We find 
the lattice-based parser gets better performance 
than the pipeline system among all three sub-
tasks. 
 
4.2 Performance of the Framework 
For the lattice-based framework, we set the max-
imum iteration in Algorithm 1 as K = 20. The 
step size   is tuned on the development set and 
empirically set to be 0.8. Table 4 shows the pars-
ing performance on the test set. It shows that the 
lattice-based framework achieves improvement 
over the lattice-based parser alone among all 
three sub-tasks: 0.16 points for word segmenta-
tion, 1.19 points for POS tagging and 1.65 points 
for parsing. It also outperforms the lattice-based 
POS tagger by 0.65 points on POS tagging accu-
racy. Our lattice-based framework also improves 
over the best joint inference parsing system 
(Qian and Liu, 2012) by 0.57 points. 
 
5 Conclusion  
In this paper, we present a novel lattice-based 
framework for the cascaded task of Chinese 
word segmentation, POS tagging and parsing. 
We first segment a Chinese sentence into a word 
lattice, then process the lattice using a lattice-
based POS tagger and a lattice-based parser. We 
also design a strategy to exploit the complemen-
tary strengths of the tagger and the parser and 
encourage them to predict agreed structures. Ex-
perimental results show that the lattice-based 
framework significantly improves the accuracies 
of the three tasks. The parsing accuracy of the 
framework also outperforms the best joint pars-
ing system reported in the literature. 
  P R F 
(Qian and Liu, 
2012) 
 
Seg. 97.56 98.36 97.96 
POS 93.43 94.2 93.81 
Parse 83.03 82.66 82.85 
Lattice-based  
Framework 
Seg. 97.82 97.9 97.86 
POS 94.36 94.44 94.40 
Parse 83.34 83.5 83.42 
 Table 4: Lattice-based framework evaluation. 
  P R F 
Pipeline Parser 
 
Seg. 96.97 98.06 97.52 
POS 92.01 93.04 92.52 
Parse 80.86 81.47 81.17 
 
Lattice-based 
 Parser 
Seg. 97.73 97.66 97.70 
POS 93.24 93.18 93.21 
Parse 81.83 81.71 81.77 
 Table 3: Parsing evaluation. 
 P R F (Kruengkrai et al, 2009) 93.28 94.07 93.67 
(Zhang and Clark, 2010) - - 93.67 
(Qian and Liu, 2012) 93.1 93.96 93.53 
(Sun, 2011) - - 94.02 
Lattice-based POS tagger 93.64 93.87 93.75 
Table 2: POS tagging evaluation. 
  P R F 
(Kruengkrai et al, 2009) 97.46 98.29 97.87 
(Zhang and Clark, 2010) - - 97.78 
(Qian and Liu, 2012) 97.45 98.24 97.85 
(Sun, 2011) - - 98.17 
Our Word Seg. System 96.97 98.06 97.52 
Word Lattice Upper Bound 99.55 99.75 99.65 
Table 1: Word segmentation evaluation. 
626
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program ("863" 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. This work is also supported 
in part by the DAPRA via contract HR0011-11-
C-0145 entitled "Linguistic Resources for Multi-
lingual Processing". 
References  
S. Boyd, L. Xiao and A. Mutapcic. 2003. Subgradient 
methods. Lecture notes of EE392o, Stanford Uni-
versity. 
E. Charniak. 2000. A maximum?entropy?inspired 
parser. In NAACL ?00, page 132?139. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. In Proc. of 
EMNLP2002, pages 1-8. 
Yoav Goldberg and Michael Elhadad. 2011. Joint 
Hebrew segmentation and parsing using a PCFG-
LA lattice parser. In Proc. of ACL2011. 
Wenbin Jiang, Haitao Mi and Qun Liu. 2008. Word 
lattice reranking for Chinese word segmentation 
and part-of-speech tagging. In Proc. of Coling 2008, 
pages 385-392. 
Komodakis, N., Paragios, N., and Tziritas, G. 2007. 
MRF optimization via dual decomposition: Mes-
sage-passing revisited. In ICCV 2007. 
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. 
Torisawa and H. Isahara. 2009. An error-driven 
word-character hybrid model for joint Chinese 
word segmentation and POS tagging. In Proc. of 
ACL2009, pages 513-521. 
Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. In 
Proc. of ACL2005, pages 75-82. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL2006, 
pages 433-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proc. of NAACL2007, 
pages 404-411. 
Xian Qian and Yang Liu. 2012. Joint Chinese Word 
segmentation, POS Tagging Parsing. In Proc. of 
EMNLP 2012, pages 501-511. 
Alexander M. Rush, David Sontag, Michael Collins 
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural 
language processing. In Proc. of EMNLP2010, 
pages 1-11. 
Weiwei Sun. 2011. A stacked sub-word model for 
joint Chinese word segmentation and part-of-
speech tagging. In Proc. of ACL2011, pages 1385-
1394. 
Weiwei Sun and Hans Uszkoreit. Capturing paradig-
matic and syntagmatic lexical relations: Towards 
accurate Chinese part-of-speech tagging. In Proc. 
of ACL2012. 
Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka, 
Wenliang Chen, Yujie Zhang and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation 
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proc. of 
IJCNLP2011, pages 309-317. 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Yue Zhang and Stephen Clark. 2010. A fast decoder 
for joint word segmentation and POS-tagging using 
a single discriminative model. In Proc. of 
EMNLP2010, pages 843-852. 
627
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111?121,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Bilingually-constrained Phrase Embeddings for Machine Translation
Jiajun Zhang
1
, Shujie Liu
2
, Mu Li
2
, Ming Zhou
2
and Chengqing Zong
1
1
National Laboratory of Pattern Recognition, CASIA, Beijing, P.R. China
{jjzhang,cqzong}@nlpr.ia.ac.cn
2
Microsoft Research Asia, Beijing, P.R. China
{shujliu,muli,mingzhou}@microsoft.com
Abstract
We propose Bilingually-constrained Re-
cursive Auto-encoders (BRAE) to learn
semantic phrase embeddings (compact
vector representations for phrases), which
can distinguish the phrases with differ-
ent semantic meanings. The BRAE is
trained in a way that minimizes the seman-
tic distance of translation equivalents and
maximizes the semantic distance of non-
translation pairs simultaneously. After
training, the model learns how to embed
each phrase semantically in two languages
and also learns how to transform semantic
embedding space in one language to the
other. We evaluate our proposed method
on two end-to-end SMT tasks (phrase ta-
ble pruning and decoding with phrasal se-
mantic similarities) which need to mea-
sure semantic similarity between a source
phrase and its translation candidates. Ex-
tensive experiments show that the BRAE
is remarkably effective in these two tasks.
1 Introduction
Due to the powerful capacity of feature learn-
ing and representation, Deep (multi-layer) Neural
Networks (DNN) have achieved a great success in
speech and image processing (Kavukcuoglu et al,
2010; Krizhevsky et al, 2012; Dahl et al, 2012).
Recently, statistical machine translation (SMT)
community has seen a strong interest in adapting
and applying DNN to many tasks, such as word
alignment (Yang et al, 2013), translation confi-
dence estimation (Mikolov et al, 2010; Liu et al,
2013; Zou et al, 2013), phrase reordering predic-
tion (Li et al, 2013), translation modelling (Auli et
al., 2013; Kalchbrenner and Blunsom, 2013) and
language modelling (Duh et al, 2013; Vaswani et
al., 2013). Most of these works attempt to im-
prove some components in SMT based on word
embedding, which converts a word into a dense,
low dimensional, real-valued vector representation
(Bengio et al, 2003; Bengio et al, 2006; Collobert
and Weston, 2008; Mikolov et al, 2013).
However, in the conventional (phrase-based)
SMT, phrases are the basic translation units. The
models using word embeddings as the direct in-
puts to DNN cannot make full use of the whole
syntactic and semantic information of the phrasal
translation rules. Therefore, in order to success-
fully apply DNN to model the whole translation
process, such as modelling the decoding process,
learning compact vector representations for the ba-
sic phrasal translation units is the essential and
fundamental work.
In this paper, we explore the phrase embedding,
which represents a phrase (sequence of words)
with a real-valued vector. In some previous works,
phrase embedding has been discussed from differ-
ent views. Socher et al (2011) make the phrase
embeddings capture the sentiment information.
Socher et al (2013a) enable the phrase embed-
dings to mainly capture the syntactic knowledge.
Li et al (2013) attempt to encode the reordering
pattern in the phrase embeddings. Kalchbrenner
and Blunsom (2013) utilize a simple convolution
model to generate phrase embeddings from word
embeddings. Mikolov et al (2013) consider a
phrase as an indivisible n-gram. Obviously, these
methods of learning phrase embeddings either fo-
cus on some aspects of the phrase (e.g. reordering
pattern), or impose strong assumptions (e.g. bag-
of-words or indivisible n-gram). Therefore, these
phrase embeddings are not suitable to fully repre-
sent the phrasal translation units in SMT due to the
lack of semantic meanings of the phrase.
Instead, we focus on learning phrase embed-
dings from the view of semantic meaning, so
that our phrase embedding can fully represent the
phrase and best fit the phrase-based SMT. As-
suming the phrase is a meaningful composition
111
of its internal words, we propose Bilingually-
constrained Recursive Auto-encoders (BRAE) to
learn semantic phrase embeddings. The core idea
behind is that a phrase and its correct translation
should share the same semantic meaning. Thus,
they can supervise each other to learn their seman-
tic phrase embeddings. Similarly, non-translation
pairs should have different semantic meanings,
and this information can also be used to guide
learning semantic phrase embeddings.
In our method, the standard recursive auto-
encoder (RAE) pre-trains the phrase embedding
with an unsupervised algorithm by minimizing the
reconstruction error (Socher et al, 2010), while
the bilingually-constrained model learns to fine-
tune the phrase embedding by minimizing the se-
mantic distance between translation equivalents
and maximizing the semantic distance between
non-translation pairs.
We use an example to explain our model. As
illustrated in Fig. 1, the Chinese phrase on the
left and the English phrase on the right are trans-
lations with each other. If we learn the embedding
of the Chinese phrase correctly, we can regard it
as the gold representation for the English phrase
and use it to guide the process of learning English
phrase embedding. In the other direction, the Chi-
nese phrase embedding can be learned in the same
way. This procedure can be performed with an
co-training style algorithm so as to minimize the
semantic distance between the translation equiva-
lents
1
. In this way, the result Chinese and English
phrase embeddings will capture the semantics as
much as possible. Furthermore, a transformation
function between the Chinese and English seman-
tic spaces can be learned as well.
With the learned model, we can accurately mea-
sure the semantic similarity between a source
phrase and a translation candidate. Accordingly,
we evaluate the BRAE model on two end-to-
end SMT tasks (phrase table pruning and decod-
ing with phrasal semantic similarities) which need
to check whether a translation candidate and the
source phrase are in the same meaning. In phrase
table pruning, we discard the phrasal translation
rules with low semantic similarity. In decoding
with phrasal semantic similarities, we apply the
semantic similarities of the phrase pairs as new
features during decoding to guide translation can-
1
For simplicity, we do not show non-translation pairs
here.
source phrase embedding ps  
?? ? ??? France and Russia 
target phrase embedding pt  
Figure 1: A motivation example for the BRAE
model.
didate selection. The experiments show that up to
72% of the phrase table can be discarded without
significant decrease on the translation quality, and
in decoding with phrasal semantic similarities up
to 1.7 BLEU score improvement over the state-of-
the-art baseline can be achieved.
In addition, our semantic phrase embeddings
have many other potential applications. For in-
stance, the semantic phrase embeddings can be
directly fed to DNN to model the decoding pro-
cess. Besides SMT, the semantic phrase embed-
dings can be used in other cross-lingual tasks (e.g.
cross-lingual question answering) and monolin-
gual applications such as textual entailment, ques-
tion answering and paraphrase detection.
2 Related Work
Recently, phrase embedding has drawn more and
more attention. There are three main perspectives
handling this task in monolingual languages.
One method considers the phrases as bag-of-
words and employs a convolution model to trans-
form the word embeddings to phrase embeddings
(Collobert et al, 2011; Kalchbrenner and Blun-
som, 2013). Gao et al (2013) also use bag-of-
words but learn BLEU sensitive phrase embed-
dings. This kind of approaches does not take the
word order into account and loses much informa-
tion. Instead, our bilingually-constrained recur-
sive auto-encoders not only learn the composition
mechanism of generating phrases from words, but
also fine tune the word embeddings during the
model training stage, so that we can induce the full
information of the phrases and internal words.
Another method (Mikolov et al, 2013) deals
with the phrases having a meaning that is not a
simple composition of the meanings of its indi-
vidual words, such as New York Times. They first
find the phrases of this kind. Then, they regard
these phrases as indivisible units, and learn their
embeddings with the context information. How-
112
ever, this kind of phrase embedding is hard to cap-
ture full semantics since the context of a phrase
is limited. Furthermore, this method can only ac-
count for a very small part of phrases, since most
of the phrases are compositional. In contrast, our
method attempts to learn the semantic vector rep-
resentation for any phrase.
The third method views any phrase as the mean-
ingful composition of its internal words. The re-
cursive auto-encoder is typically adopted to learn
the way of composition (Socher et al, 2010;
Socher et al, 2011; Socher et al, 2013a; Socher
et al, 2013b; Li et al, 2013). They pre-train the
RAE with an unsupervised algorithm. And then,
they fine-tune the RAE according to the label of
the phrase, such as the syntactic category in pars-
ing (Socher et al, 2013a), the polarity in sentiment
analysis (Socher et al, 2011; Socher et al, 2013b),
and the reordering pattern in SMT (Li et al, 2013).
This kind of semi-supervised phrase embedding is
in fact performing phrase clustering with respect
to the phrase label. For example, in the RAE-
based phrase reordering model for SMT (Li et
al., 2013), the phrases with the similar reorder-
ing tendency (e.g. monotone or swap) are close
to each other in the embedding space, such as the
prepositional phrases. Obviously, this kind meth-
ods of semi-supervised phrase embedding do not
fully address the semantic meaning of the phrases.
Although we also follow the composition-based
phrase embedding, we are the first to focus on
the semantic meanings of the phrases and propose
a bilingually-constrained model to induce the se-
mantic information and learn transformation of the
semantic space in one language to the other.
3 Bilingually-constrained Recursive
Auto-encoders
This section introduces the Bilingually-
constrained Recursive Auto-encoders (BRAE),
that is inspired by two observations. First, the
recursive auto-encoder provides a reasonable
composition mechanism to embed each phrase.
And the semi-supervised phrase embedding
(Socher et al, 2011; Socher et al, 2013a; Li et
al., 2013) further indicates that phrase embedding
can be tuned with respect to the label. Second,
even though we have no correct semantic phrase
representation as the gold label, the phrases
sharing the same meaning provide an indirect but
feasible way.
x1 x2 x3 x4 
y1=f(W(1)[x1; x2]+b) 
y2=f(W(1)[y1; x3]+b) 
y3=f(W(1)[y2; x4]+b) 
Figure 2: A recursive auto-encoder for a four-
word phrase. The empty nodes are the reconstruc-
tions of the input.
We will first briefly present the unsupervised
phrase embedding, and then describe the semi-
supervised framework. After that, we introduce
the BRAE on the network structure, objective
function and parameter inference.
3.1 Unsupervised Phrase Embedding
3.1.1 Word Vector Representations
In phrase embedding using composition, the word
vector representation is the basis and serves as the
input to the neural network. After learning word
embeddings with DNN (Bengio et al, 2003; Col-
lobert and Weston, 2008; Mikolov et al, 2013),
each word in the vocabulary V corresponds to a
vector x ? R
n
, and all the vectors are stacked into
an embedding matrix L ? R
n?|V |
.
Given a phrase which is an ordered list of m
words, each word has an index i into the columns
of the embedding matrix L. The index i is used to
retrieve the word?s vector representation using a
simple multiplication with a binary vector e which
is zero in all positions except for the ith index:
x
i
= Le
i
? R
n
(1)
Note that n is usually set empirically, such as n =
50, 100, 200. Throughout this paper, n = 3 is used
for better illustration as shown in Fig. 1.
3.1.2 RAE-based Phrase Embedding
Assuming we are given a phrase w
1
w
2
? ? ?w
m
,
it is first projected into a list of vectors
(x
1
, x
2
, ? ? ? , x
m
) using Eq. 1. The RAE learns
the vector representation of the phrase by recur-
sively combining two children vectors in a bottom-
up manner (Socher et al, 2011). Fig. 2 illustrates
an instance of a RAE applied to a binary tree, in
113
which a standard auto-encoder (in box) is re-used
at each node. The standard auto-encoder aims at
learning an abstract representation of its input. For
two children c
1
= x
1
and c
2
= x
2
, the auto-
encoder computes the parent vector y
1
as follows:
p = f(W
(1)
[c
1
; c
2
] + b
(1)
) (2)
Where we multiply the parameter matrix W
(1)
?
R
n?2n
by the concatenation of two children
[c
1
; c
2
] ? R
2n?1
. After adding a bias term b
(1)
,
we apply an element-wise activation function such
as f = tanh(?), which is used in our experiments.
In order to apply this auto-encoder to each pair of
children, the representation of the parent p should
have the same dimensionality as the c
i
?s.
To assess how well the parent?s vector repre-
sents its children, the standard auto-encoder recon-
structs the children in a reconstruction layer:
[c
?
1
; c
?
2
] = f
(2)
(W
(2)
p+ b
(2)
) (3)
Where c
?
1
and c
?
2
are reconstructed children, W
(2)
and b
(2)
are parameter matrix and bias term for re-
construction respectively, and f
(2)
= tanh(?).
To obtain the optimal abstract representation of
the inputs, the standard auto-encoder tries to min-
imize the reconstruction errors between the inputs
and the reconstructed ones during training:
E
rec
([c
1
; c
2
]) =
1
2
||[c
1
; c
2
]? [c
?
1
; c
?
2
]||
2
(4)
Given y
1
= p, we can use Eq. 2 again to com-
pute y
2
by setting the children to be [c
1
; c
2
] =
[y
1
;x
3
]. The same auto-encoder is re-used until
the vector of the whole phrase is generated.
For unsupervised phrase embedding, the only
objective is to minimize the sum of reconstruction
errors at each node in the optimal binary tree:
RAE
?
(x) = argmin
y?A(x)
?
s?y
E
rec
([c
1
; c
2
]
s
) (5)
Where x is the list of vectors of a phrase, andA(x)
denotes all the possible binary trees that can be
built from inputs x. A greedy algorithm (Socher
et al, 2011) is used to generate the optimal binary
tree y. The parameters ? = (W, b) are optimized
over all the phrases in the training data.
3.2 Semi-supervised Phrase Embedding
The above RAE is completely unsupervised and
can only induce general representations of the
Reco nstr uctio n Erro r  Pred ictio n Erro r  
W (1)  
W (2)  W (label)  
Figure 3: An illustration of a semi-supervised
RAE unit. Red nodes show the label distribution.
multi-word phrases. Several researchers extend
the original RAEs to a semi-supervised setting so
that the induced phrase embedding can predict a
target label, such as polarity in sentiment analysis
(Socher et al, 2011), syntactic category in parsing
(Socher et al, 2013a) and phrase reordering pat-
tern in SMT (Li et al, 2013).
In the semi-supervised RAE for phrase embed-
ding, the objective function over a (phrase, label)
pair (x, t) includes the reconstruction error and the
prediction error, as illustrated in Fig. 3.
E(x, t; ?) = ?E
rec
(x, t; ?)+(1??)E
pred
(x, t; ?)
(6)
Where the hyper-parameter ? is used to balance
the reconstruction and prediction error. For label
prediction, the cross-entropy error is usually used
to calculate E
pred
. By optimizing the above ob-
jective, the phrases in the vector embedding space
will be grouped according to the labels.
3.3 The BRAE Model
We know from the semi-supervised phrase embed-
ding that the learned vector representation can be
well adapted to the given label. Therefore, we can
imagine that learning semantic phrase embedding
is reasonable if we are given gold vector represen-
tations of the phrases.
However, no gold semantic phrase embedding
exists. Fortunately, we know the fact that the
two phrases should share the same semantic rep-
resentation if they express the same meaning. We
can make inference from this fact that if a model
can learn the same embedding for any phrase pair
sharing the same meaning, the learned embedding
must encode the semantics of the phrases and the
corresponding model is our desire.
As translation equivalents share the same se-
mantic meaning, we employ high-quality phrase
translation pairs as training corpus in this
work. Accordingly, we propose the Bilingually-
constrained Recursive Auto-encoders (BRAE),
114
Sour ce Recons truction Err o r  
Sour ce Prediction Err o r  
W s (1)  
W s (2)  W s (label)  
Ta rg et Recons truction Err o r  
W t (1)  
W t (2)  
W t (label)  Ta rg et Prediction Err o r  
Source Language Phrase Target Language Phrase 
Figure 4: An illustration of the bilingual-
constrained recursive auto-encoders. The two
phrases are translations with each other.
whose basic goal is to minimize the semantic dis-
tance between the phrases and their translations.
3.3.1 The Objective Function
Unlike previous methods, the BRAE model jointly
learns two RAEs (Fig. 4 shows the network struc-
ture): one for source language and the other for
target language. For a phrase pair (s, t), two kinds
of errors are involved:
1. reconstruction errorE
rec
(s, t; ?): how well
the learned vector representations p
s
and p
t
repre-
sent the phrase s and t respectively?
E
rec
(s, t; ?) = E
rec
(s; ?) + E
rec
(t; ?) (7)
2. semantic error E
sem
(s, t; ?): what is the
semantic distance between the learned vector rep-
resentations p
s
and p
t
?
Since word embeddings for two languages are
learned separately and locate in different vector
space, we do not enforce the phrase embeddings
in two languages to be in the same semantic vector
space. We suppose there is a transformation be-
tween the two semantic embedding spaces. Thus,
the semantic distance is bidirectional: the distance
between p
t
and the transformation of p
s
, and that
between p
s
and the transformation of p
t
. As a re-
sult, the overall semantic error becomes:
E
sem
(s, t; ?) = E
sem
(s|t, ?) + E
sem
(t|s, ?) (8)
Where E
sem
(s|t, ?) = E
sem
(p
t
, f(W
l
s
p
s
+ b
l
s
))
means the transformation of p
s
is performed as
follows: we first multiply a parameter matrix W
l
s
by p
s
, and after adding a bias term b
l
s
we apply
an element-wise activation function f = tanh(?).
Finally, we calculate their Euclidean distance:
E
sem
(s|t, ?) =
1
2
||p
t
? f(W
l
s
p
s
+ b
l
s
)||
2
(9)
E
sem
(t|s, ?) can be calculated in exactly the same
way. For the phrase pair (s, t), the joint error is:
E(s, t; ?) = ?E
rec
(s, t; ?) + (1??)E
sem
(s, t; ?)
(10)
The hyper-parameter ? weights the reconstruction
and semantic error. The final BRAE objective over
the phrase pairs training set (S, T ) becomes:
J
BRAE
=
1
N
?
(s,t)?(S,T )
E(s, t; ?)+
?
2
||?||
2
(11)
3.3.2 Max-Semantic-Margin Error
Ideally, we want the learned BRAE model can
make sure that the semantic error for the positive
example (a source phrase s and its correct transla-
tion t) is much smaller than that for the negative
example (the source phrase s and a bad translation
t
?
). However, the current model cannot guarantee
this since the above semantic error E
sem
(s|t, ?)
only accounts for positive ones.
We thus enhance the semantic error with both
positive and negative examples, and the corre-
sponding max-semantic-margin error becomes:
E
?
sem
(s|t, ?) = max{0, E
sem
(s|t, ?)
? E
sem
(s|t
?
, ?) + 1}
(12)
It tries to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously. Using the above error function, we need
to construct a negative example for each positive
example. Suppose we are given a positive exam-
ple (s, t), the correct translation t can be converted
into a bad translation t
?
by replacing the words
in t with randomly chosen target language words.
Then, a negative example (s, t
?
) is available.
3.3.3 Parameter Inference
Like semi-supervised RAE (Li et al, 2013), the
parameters ? in our BRAE model can also be di-
vided into three sets:
?
L
: word embedding matrix L for two lan-
guages (Section 3.1.1);
?
rec
: recursive auto-encoder parameter matrices
W
(1)
, W
(2)
, and bias terms b
(1)
, b
(2)
for two lan-
guages (Section 3.1.2);
?
sem
: transformation matrix W
l
and bias term
b
l
for two directions in semantic distance compu-
tation (Section 3.3.1).
115
To have a deep understanding of the parameters,
we rewrite Eq. 10:
E(s, t; ?) = ?(E
rec
(s; ?) + E
rec
(t; ?))
+ (1? ?)(E
?
sem
(s|t, ?) + E
?
sem
(t|s, ?))
= (?E
rec
(s; ?
s
) + (1? ?)E
?
sem
(s|t, ?
s
))
+ (?E
rec
(t; ?
t
) + (1? ?)E
?
sem
(t|s, ?
t
))
(13)
We can see that the parameters ? can be divided
into two classes: ?
s
for the source language and ?
t
for the target language. The above equation also
indicates that the source-side parameters ?
s
can be
optimized independently as long as the semantic
representation p
t
of the target phrase t is given to
compute E
sem
(s|t, ?) with Eq. 9. It is similar for
the target-side parameters ?
t
.
Assuming the target phrase representation p
t
is available, the optimization of the source-side
parameters is similar to that of semi-supervised
RAE. We apply the Stochastic Gradient Descent
(SGD) algorithm to optimize each parameter:
?
s
= ?
s
? ?
?J
s
??
s
(14)
In order to run SGD algorithm, we need to solve
two problems: one for parameter initialization and
the other for partial gradient calculation.
In parameter initialization, ?
rec
and ?
sem
for the
source language is randomly set according to a
normal distribution. For the word embedding L
s
,
there are two choices. First, L
s
is initialized ran-
domly like other parameters. Second, the word
embedding matrix L
s
is pre-trained with DNN
(Bengio et al, 2003; Collobert and Weston, 2008;
Mikolov et al, 2013) using large-scale unlabeled
monolingual data. We prefer to the second one
since this kind of word embedding has already
encoded some semantics of the words. In this
work, we employ the toolkit Word2Vec (Mikolov
et al, 2013) to pre-train the word embedding for
the source and target languages. The word em-
beddings will be fine-tuned in our BRAE model to
capture much more semantics.
The partial gradient for one instance is com-
puted as follows:
?J
s
??
s
=
?E(s|t, ?
s
)
??
s
+ ??
s
(15)
Where the source-side error given the target phrase
representation includes reconstruction error and
updated semantic error:
E(s|t, ?
s
) = ?E
rec
(s; ?
s
) + (1??)E
?
sem
(s|t, ?
s
)
(16)
Given the current ?
s
, we first construct the binary
tree (as illustrated in Fig. 2) for any source-side
phrase using the greedy algorithm (Socher et al,
2011). Then, the derivatives for the parameters in
the fixed binary tree will be calculated via back-
propagation through structures (Goller and Kuch-
ler, 1996). Finally, the parameters will be updated
using Eq. 14 and a new ?
s
is obtained.
The target-side parameters ?
t
can be optimized
in the same way as long as the source-side phrase
representation p
s
is available. It seems a para-
dox that updating ?
s
needs p
t
while updating ?
t
needs p
s
. To solve this problem, we propose an
co-training style algorithm which includes three
steps:
1. Pre-training: applying unsupervised phrase
embedding with standard RAE to pre-train the
source- and target-side phrase representations p
s
and p
t
respectively (Section 2.1.2);
2. Fine-tuning: with the BRAE model, us-
ing target-side phrase representation p
t
to update
the source-side parameters ?
s
and obtain the fine-
tuned source-side phrase representation p
?
s
, and
meanwhile using p
s
to update ?
t
and get the fine-
tuned p
?
t
, and then calculate the joint error over the
training corpus;
3. Termination Check: if the joint error
reaches a local minima or the iterations reach
the pre-defined number (25 is used in our exper-
iments), we terminate the training procedure, oth-
erwise we set p
s
= p
?
s
, p
t
= p
?
t
, and go to step
2.
4 Experiments
With the semantic phrase embeddings and the vec-
tor space transformation function, we apply the
BRAE to measure the semantic similarity between
a source phrase and its translation candidates in
the phrase-based SMT. Two tasks are involved in
the experiments: phrase table pruning that dis-
cards entries whose semantic similarity is very low
and decoding with the phrasal semantic similari-
ties as additional new features.
4.1 Hyper-Parameter Settings
The hyper-parameters in the BRAE model include
the dimensionality of the word embedding n in Eq.
1, the balance weight ? in Eq. 10, ?s in Eq. 11,
and the learning rate ? in Eq. 14.
For the dimensionality n, we have tried three
settings n = 50, 100, 200 in our experiments. We
116
empirically set the learning rate ? = 0.01. We
draw ? from 0.05 to 0.5 with step 0.05, and ?s
from {10
?6
, 10
?5
, 10
?4
, 10
?3
, 10
?2
}. The over-
all error of the BRAE model is employed to guide
the search procedure. Finally, we choose ? =
0.15, ?
L
= 10
?2
, ?
rec
= 10
?3
and ?
sem
= 10
?3
.
4.2 SMT Setup
We have implemented a phrase-based translation
system with a maximum entropy based reordering
model using the bracketing transduction grammar
(Wu, 1997; Xiong et al, 2006).
The SMT evaluation is conducted on Chinese-
to-English translation. Accordingly, our BRAE
model is trained on Chinese and English. The
bilingual training data from LDC
2
contains 0.96M
sentence pairs and 1.1M entity pairs with 27.7M
Chinese words and 31.9M English words. A 5-
gram language model is trained on the Xinhua por-
tion of the English Gigaword corpus and the En-
glish part of bilingual training data. The NIST
MT03 is used as the development data. NIST
MT04-06 and MT08 (news data) are used as the
test data. Case-insensitive BLEU is employed
as the evaluation metric. The statistical signif-
icance test is performed by the re-sampling ap-
proach (Koehn, 2004).
In addition, we pre-train the word embedding
with toolkit Word2Vec on large-scale monolingual
data including the aforementioned data for SMT.
The monolingual data contains 1.06B words for
Chinese and 1.12B words for English. To ob-
tain high-quality bilingual phrase pairs to train
our BRAE model, we perform forced decoding
for the bilingual training sentences and collect the
phrase pairs used. After removing the duplicates,
the remaining 1.12M bilingual phrase pairs (length
ranging from 1 to 7) are obtained.
4.3 Phrase Table Pruning
Pruning most of the phrase table without much
impact on translation quality is very important
for translation especially in environments where
memory and time constraints are imposed. Many
algorithms have been proposed to deal with this
problem, such as significance pruning (Johnson et
al., 2007; Tomeh et al, 2009), relevance prun-
ing (Eck et al, 2007) and entropy-based pruning
2
LDC category numbers: LDC2000T50, LDC2002L27,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2005T34.
(Ling et al, 2012; Zens et al, 2012). These algo-
rithms are based on corpus statistics including co-
occurrence statistics, phrase pair usage and com-
position information. For example, the signifi-
cance pruning, which is proven to be a very ef-
fective algorithm, computes the probability named
p-value, that tests whether a source phrase s and a
target phrase t co-occur more frequently in a bilin-
gual corpus than they happen just by chance. The
higher the p-value, the more likely of the phrase
pair to be spurious.
Our work has the same objective, but instead of
using corpus statistics, we attempt to measure the
quality of the phrase pair from the view of seman-
tic meaning. Given a phrase pair (s, t), the BRAE
model first obtains their semantic phrase represen-
tations (p
s
, p
t
), and then transforms p
s
into target
semantic space p
s
?
, p
t
into source semantic space
p
t
?
. We finally get two similarities Sim(p
s
?
, p
t
)
and Sim(p
t
?
, p
s
). Phrase pairs that have a low
similarity are more likely to be noise and more
prone to be pruned. In experiments, we discard
the phrase pair whose similarity in two directions
are smaller than a threshold
3
.
Table 1 shows the comparison results between
our BRAE-based pruning method and the signif-
icance pruning algorithm. We can see a common
phenomenon in both of the algorithms: for the first
few thresholds, the phrase table becomes smaller
and smaller while the translation quality is not
much decreased, but the performance jumps a lot
at a certain threshold (16 for Significance pruning,
0.8 for BRAE-based one).
Specifically, the Significance algorithm can
safely discard 64% of the phrase table at its thresh-
old 12 with only 0.1 BLEU loss in the overall
test. In contrast, our BRAE-based algorithm can
remove 72% of the phrase table at its threshold
0.7 with only 0.06 BLEU loss in the overall eval-
uation. When the two algorithms using a similar
portion of the phrase table
4
(35% in BRAE and
36% in Significance), the BRAE-based algorithm
outperforms the Significance algorithm on all the
test sets except for MT04. It indicates that our
BRAE model is a good alternative for phrase table
pruning. Furthermore, our model is much more in-
3
To avoid the situation that all the translation candidates
for a source phrase are pruned, we always keep the first 10
best according to the semantic similarity.
4
In the future, we will compare the performance by en-
forcing the two algorithms to use the same portion of phrase
table
117
Method Threshold PhraseTable MT03 MT04 MT05 MT06 MT08 ALL
Baseline 100% 35.81 36.91 34.69 33.83 27.17 34.82
BRAE
0.4 52% 35.94 36.96 35.00 34.71 27.77 35.16
0.5 44% 35.67 36.59 34.86 33.91 27.25 34.89
0.6 35% 35.86 36.71 34.93 34.63 27.34 35.05
0.7 28% 35.55 36.62 34.57 33.97 27.10 34.76
0.8 20% 35.06 36.01 34.13 33.04 26.66 34.04
Significance
8 48% 35.86 36.99 34.74 34.53 27.59 35.13
12 36% 35.59 36.73 34.65 34.17 27.16 34.72
16 25% 35.19 36.24 34.26 33.32 26.55 34.09
20 18% 35.05 36.09 34.02 32.98 26.37 33.97
Table 1: Comparison between BRAE-based pruning and Significance pruning of phrase table. Threshold
means similarity in BRAE and negative-log-p-value in Significance. ?ALL? combines the development
and test sets. Bold numbers denote that the result is better than or comparable to that of baseline. n = 50
is used for embedding dimensionality.
tuitive because it is directly based on the semantic
similarity.
4.4 Decoding with Phrasal Semantic
Similarities
Besides using the semantic similarities to prune
the phrase table, we also employ them as two in-
formative features like the phrase translation prob-
ability to guide translation hypotheses selection
during decoding. Typically, four translation prob-
abilities are adopted in the phrase-based SMT, in-
cluding phrase translation probability and lexical
weights in both directions. The phrase transla-
tion probability is based on co-occurrence statis-
tics and the lexical weights consider the phrase as
bag-of-words. In contrast, our BRAE model fo-
cuses on compositional semantics from words to
phrases. Therefore, the semantic similarities com-
puted using our BRAE model are complementary
to the existing four translation probabilities.
The semantic similarities in two directions
Sim(p
s
?
, p
t
) and Sim(p
t
?
, p
s
) are integrated into
our baseline phrase-based model. In order to in-
vestigate the influence of the dimensionality of the
embedding space, we have tried three different set-
tings n = 50, 100, 200.
As shown in Table 2, no matter what n is, the
BRAE model can significantly improve the trans-
lation quality in the overall test data. The largest
improvement can be up to 1.7 BLEU score (MT06
for n = 50). It is interesting that with dimen-
sionality growing, the translation performance is
not consistently improved. We speculate that us-
ing n = 50 or n = 100 can already distinguish
good translation candidates from bad ones.
4.5 Analysis on Semantic Phrase Embedding
To have a better intuition about the power of the
BRAE model at learning semantic phrase embed-
dings, we show some examples in Table 3. Given
the BRAE model and the phrase training set, we
search from the set the most semantically similar
English phrases for any new input English phrase.
The input phrases contain different number of
words. The table shows that the unsupervised
RAE can at most capture the syntactic property
when the phrases are short. For example, the
unsupervised RAE finds do not want for the in-
put phrase do not agree. When the phrase be-
comes longer, the unsupervised RAE cannot even
capture the syntactic property. In contrast, our
BRAE model learns the semantic meaning for
each phrase no matter whether it is short or rel-
atively long. This indicates that the proposed
BRAE model is effective at learning semantic
phrase embeddings.
5 Discussions
5.1 Applications of The BRAE model
As the semantic phrase embedding can fully rep-
resent the phrase, we can go a step further in the
phrase-based SMT and feed the semantic phrase
embeddings to DNN in order to model the whole
translation process (e.g. derivation structure pre-
diction). We will explore this direction in our fu-
ture work. Besides SMT, the semantic phrase em-
beddings can be used in other cross-lingual tasks,
such as cross-lingual question answering, since
the semantic similarity between phrases in differ-
ent languages can be calculated accurately.
In addition to the cross-lingual applications, we
believe the BRAE model can be applied in many
118
Method n MT03 MT04 MT05 MT06 MT08 ALL
Baseline 35.81 36.91 34.69 33.83 27.17 34.82
BRAE
50 36.43 37.64 35.35 35.53 28.59 35.84
+
100 36.45 37.44 35.58 35.42 28.57 36.03
+
200 36.34 37.35 35.78 34.87 27.84 35.62
+
Table 2: Experimental results of decoding with phrasal semantic similarities. n is the embedding dimen-
sionality. ?+? means that the model significantly outperforms the baseline with p < 0.01.
New Phrase Unsupervised RAE BRAE
military force
core force military power
main force military strength
labor force armed forces
at a meeting
to a meeting at the meeting
at a rate during the meeting
a meeting , at the conference
do not agree
one can accept do not favor
i can understand will not compromise
do not want not to approve
each people in this nation
each country regards every citizen in this country
each country has its all the people in the country
each other , and people all over the country
Table 3: Semantically similar phrases in the training set for the new phrases.
monolingual NLP tasks which depend on good
phrase representations or semantic similarity be-
tween phrases, such as named entity recognition,
parsing, textual entailment, question answering
and paraphrase detection.
5.2 Model Extensions
In fact, the phrases having the same meaning are
translation equivalents in different languages, but
are paraphrases in one language. Therefore, our
model can be easily adapted to learn semantic
phrase embeddings using paraphrases.
Our BRAE model still has some limitations.
For example, as each node in the recursive auto-
encoder shares the same weight matrix, the BRAE
model would become weak at learning the seman-
tic representations for long sentences with tens of
words. Improving the model to semantically em-
bed sentences is left for our future work.
6 Conclusions and Future Work
This paper has explored the bilingually-
constrained recursive auto-encoders in learning
phrase embeddings, which can distinguish phrases
with different semantic meanings. With the ob-
jective to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously, the learned model can semantically embed
any phrase in two languages and can transform
the semantic space in one language to the other.
Two end-to-end SMT tasks are involved to test
the power of the proposed model at learning the
semantic phrase embeddings. The experimental
results show that the BRAE model is remarkably
effective in phrase table pruning and decoding
with phrasal semantic similarities.
We have also discussed many other potential ap-
plications and extensions of our BRAE model. In
the future work, we will explore four directions.
1) we will try to model the decoding process with
DNN based on our semantic embeddings of the
basic translation units. 2) we are going to learn
semantic phrase embeddings with the paraphrase
corpus. 3) we will apply the BRAE model in other
monolingual and cross-lingual tasks. 4) we plan to
learn semantic sentence embeddings by automati-
cally learning different weight matrices for differ-
ent nodes in the BRAE model.
Acknowledgments
We thank Nan Yang for sharing the baseline
code and anonymous reviewers for their valu-
able comments. The research work has been
partially funded by the Natural Science Founda-
tion of China under Grant No. 61333018 and
61303181, and Hi-Tech Research and Develop-
ment Program (863 Program) of China under
Grant No. 2012AA011102.
119
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137?186.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language
Processing, 20(1):30?42.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 678?
683.
Matthias Eck, Stephen Vogal, and Alex Waibel. 2007.
Estimating phrase pair relevance for translation
model pruning. In MTSummit XI.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347?352.
John Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700?1709.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha?el Mathieu, and Yann L Cun.
2010. Learning convolutional feature hierarchies for
visual recognition. In Advances in neural informa-
tion processing systems, pages 1090?1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for itg-based translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Wang Ling, Joao Grac?a, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 962?971.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 791?801.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
120
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for
statistical machine translation. In Proceedings of
Summit XII, pages 144?151.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387?1392.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of ACL-
COLING, pages 505?512.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 972?983.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
121
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 850?860,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Enhancing Grammatical Cohesion: 
Generating Transitional Expressions for SMT 
 
Mei Tu             Yu Zhou           Chengqing Zong 
National Laboratory of Pattern Recognition,  
Institute of Automation,  
Chinese Academy of Sciences 
{mtu,yzhou,cqzong}@nlpr.ia.ac.cn 
 
  
 
 
Abstract 
Transitional expressions provide glue that 
holds ideas together in a text and enhance the 
logical organization, which together help im-
prove readability of a text. However, in most 
current statistical machine translation (SMT) 
systems, the outputs of compound-complex 
sentences still lack proper transitional expres-
sions. As a result, the translations are often 
hard to read and understand. To address this 
issue, we propose two novel models to en-
courage generating such transitional expres-
sions by introducing the source compound-
complex sentence structure (CSS). Our models 
include a CSS-based translation model, which 
generates new CSS-based translation rules, 
and a generative transfer model, which en-
courages producing transitional expressions 
during decoding. The two models are integrat-
ed into a hierarchical phrase-based translation 
system to evaluate their effectiveness. The ex-
perimental results show that significant im-
provements are achieved on various test data 
meanwhile the translations are more cohesive 
and smooth.  
1 Introduction 
During the last decade, great progress has been 
made on statistical machine translation (SMT) 
models. However, these translations still suffer 
from poor readability, especially translations of 
compound-complex sentences. One of the main 
reasons may be that most existing models con-
centrate more on producing well-translated local 
sentence fragments, but largely ignore global 
cohesion between the fragments. Generally, co-
hesion, including lexical and grammatical cohe-
sion, contributes much to the understandability 
and smoothness of a text.  
Recently, researchers have begun addressing 
the lexical cohesion of SMT (Gong et al, 2011; 
Xiao et al, 2011; Wong and Kit, 2012; Xiong, 
2013). These efforts focus mainly on the co-
occurrence of lexical items in a similar environ-
ment. Grammatical cohesion1 (Halliday and Has-
san, 1976) in SMT has been little mentioned in 
previous work. Translations without grammatical 
cohesion is hard to read, mostly due to loss of 
cohesive and transitional expressions between 
two sentence fragments. Thus, generating transi-
tional expressions is necessary for achieving 
grammatical cohesion. However, it is not easy to 
produce such transitional expressions in SMT. 
As an example, consider the Chinese-to-English 
translation in Figure 1.  
Source Chinese sentence:  
  [??         ??     ??     ?   ??     ??      ?]1   [  ??          
Alth ugh   reduce  pollution  of   calls    continue  ,           public      
   ??       ??   ?]2   [??       ??    ??       ?        ??                               
gr wing    angry  ,         pollution  still    become   more   worse 
   ?   ?]3  [??  ??         ??                         ?    ??? ?]4
already   ,   more   show  environment  protection  of    urgent .
Target English golden translation:
Despite frequent calls for cutting pollution, and 
growing public anger, the proble  has only g t worse, 
which increasingly shows the urgency of environmental 
protection.
Figure 1: An example of Chinese-to-English transla-
tion. The English translation sentence has three transi-
tional phrases: Despite, and, which. 
 
There are 4 sub-sentences separated by com-
mas in the Chinese sentence. We have tried to 
translate the Chinese sentence using many well-
                                                 
1
 Grammatical cohesion can make relations among sentenc-
es more explicit. There are various grammatically cohesive 
devices (reference, substitution ellipsis and conjunction) 
that tie fragments together in a cohesive way.    
850
known online translators, but find that it is very 
difficult to generate the target transitional ex-
pressions, especially when there is no explicit 
connective word in the source sentence, such as 
generating ?and ? and ?which? in Figure 1. 
Fortunately, the functional relationships be-
tween two neighboring source sub-sentences 
provide us with a good perspective and the inspi-
ration to generate those transitional phrases. Fig-
ure 1 shows that the first and the second Chinese 
sub-sentences form a parallel relation. Thus, 
even though there is no distinct connective word 
at the beginning of the second source sub-
sentence, a good translator is still able to insert or 
generate an ?and? as a connection word to make 
the target translation more cohesive.  
Based on the above analysis, this paper focus-
es on the target grammatical cohesion in SMT to 
make the translation more understandable, espe-
cially for languages with great difference in lin-
guistic structure like Chinese and English. To the 
best of our knowledge, our work is the first at-
tempt to generate target transitional expressions 
for SMT grammatical cohesion by introducing 
the functional relationships of source sentences. 
In this work, we propose two models. One is a 
new translation model that is utilized to generate 
new translation rules combined with the infor-
mation of source functional relationships. The 
other is a generative transfer model that encour-
ages producing transitional phrases during de-
coding. Our experimental results on Chinese-to-
English translation demonstrate that the transla-
tion readability is greatly improved by introduc-
ing the cohesive information. 
The remainder of the paper is organized as 
follows. In Section 2, we describe the functional 
relationships of Chinese compound-complex sen-
tences. In Section 3, we present our models and 
show how to integrate the models into an SMT 
system. Our experimental results are reported in 
Section 4. A survey of related work is conducted 
in Section 5, and we conclude our work and out-
line the future work in Section 6.  
2 Chinese Compound-Complex Sen-
tence Structure 
To acquire the functional relationships of a Chi-
nese compound-complex sentence, Zhou (2004) 
proposed a well-annotated scheme to build the 
Compound-complex Sentence Structure (CSS). 
The structure explicitly shows the minimal se-
mantic spans, called elementary units (eus), and 
also depicts the hierarchical relations among eus. 
There are 11 common types of functional rela-
tionships 2  annotated in the Tsinghua Chinese 
Treebank (Zhou, 2004).  
Under the annotation scheme of the Tsinghua 
Chinese Treebank, the Chinese sentence of ex-
ample in Figure 1 is represented as the tree 
shown in Figure 2. In this example, each sub-
sentence is an eu. eu1 and eu2 are combined with 
a parallel relationship, followed by eu3 with an 
adversative relationship. eu1, eu2, and eu3 form a 
large semantic span3, connected with eu4 by a 
consequence relationship. All of the eus are or-
ganized into various functional relationships and 
finally form a hierarchical tree. 
par llel-[(1,1), (2,2)]
dversative-[(1,2),(3,3)]
consequence-[(1,3),(4,4)]
??  ?? 
??  ?   
??  ?   ?
??    ??   
??  ?  
??? ?
eu1
eu2
eu3
eu4
??  ??      
??  ?  
?? ?? ?
?? ?? 
??     ?
 
Figure 2: The compound-complex sentence 
structure of the Chinese sentence in Figure 1. 
Formally, given a compound-complex sen-
tence structure (CSS), each node in the CSS can 
be represented as a tuple
1 1[( , ),...( , ),..., ( , )]? l l L LR s e s e s e. R represents the 
relationship, which has L children. For each 
child of R , a pair ( , )lls e records its start and end 
eus. For example, adversative-[(1,2), (3,3)] in 
Figure 2 means that two children are controlled 
by the relationship adversative, and the left child 
consists of eu1 and eu2, while the right child con-
tains only eu3.  
CSS has much in common with Rhetorical 
Structure (Mann and Thompson, 1988) in Eng-
lish, which also describe the semantic relation 
between discourse units. But the Rhetorical 
Structure involves much richer relations on the 
document-level, and little corpus is open for 
Chinese.    
In the following, we will describe in detail 
how to utilize such CSS information for model-
ling in SMT.  
                                                 
2 They are parallel, consequence, progressive, alternative, 
causal, purpose, hypothesis, condition, adversative, expla-
nation, and flowing relationships. 
3 A semantic span can include one or more eus. 
851
3 Modelling 
Our purpose is to enhance the grammatical cohe-
sion by exploiting the source CSS information. 
Therefore, theoretically, the conditional probabil-
ity of a target translation es conditioned on the 
source CSS-based tree ft is given by ( | )s tP e f , 
and the final translation se  is obtained with the 
following formula: 
argmax{P( | )} (1)?
S
s s te
e e f
  
    Following Och and Ney (2002), our model is 
framed as a log-linear model: 
 exp ( , )( | ) (2)
exp ( , )
?
??
? ?? ?
s
k k k s t
s t
k k k s t
hP
he
e fe f
e' f
 
 
where ( , )s th e f is a feature with weight? . Then, 
the best translation is: 
 
argmaxexp ( , ) (3)
s
s k k k s th?? ?ee e f
 
Our models make use of CSS with two strate-
gies:  
1) CSS-based translation model: following 
formula (1), we obtain the cohesion information 
by modifying the translation rules with their 
probabilities ( | )s tP e f  based on word align-
ments between the source CSS-tree and the tar-
get string; 
 2) CSS-based transfer model: following 
formula (3), we introduce a transfer score to en-
courage the decoder to generate transitional 
words and phrases; the score is utilized as an ad-
ditional feature ( , )k s th e f  in the log-linear model.  
3.1 CSS-based Translation Model 
For the existing translation models, the entire 
training process is conducted at the lexical or 
syntactic level without grammatically cohesive 
information. As a result, it is difficult to utilize 
such cohesive information during decoding. In-
stead, we reserve the cohesive information in the 
training process by converting the original source 
sentence into tagged-flattened CSS and then per-
form word alignment and extract the translation 
rules from the bilingual flattened source CSS and 
the target string.  
As introduced in Section 2, a CSS consists of 
nodes, and a node can be represented as a tuple
1 1[( , ),...( , ),...,( , )]L Ll lR s e s e s e? . In this represen-
tation, the relationship R is the most important 
factor because different relationships directly 
reflect different cohesive expressions. In addition, 
the children?s positions always play a strong role 
in choosing cohesive expressions because transi-
tional expressions vary for children with differ-
ent positions. For example, when translating the 
last child of a parallel relation, we always use 
word ?and? as the transitional expression seen in 
Figure 3, but we will not use it for the first child 
of a parallel relation. Therefore, in the training 
process we just keep the information of relation-
ships and children?s positions when converting 
Despite    frequent    calls   for   cutting   pollution  ,   and   growing   public   anger   ,
<Parallel  @B>  ??  ??   ??  ?   ??  ??  ?          <Parallel  @E>  ??      ??   ??  ?
parallel
??     ??    ??    ?     ??     ??  ? ??    ??    ??     ?
Despite    frequent    calls   for   cutting   pollution  ,   and   growing   public   anger   ,
(a)
(b)
Original hierarchical rules:
                               [X] ?? |||  and growing [X]
Modified hierarchical rules:
                       <parallel  @E >  [X] ??  |||  and growing [X]
(c)
 
Figure 3: An example of modifying translation rules. @B means the current structure information 
comes from the first child, and @E means from the last child.  
852
the source CSS to a tagged-flattened string. 
 Considering that the absolute position (index 
of the eu, such as 1, 2, 3) is somehow sparse in 
the corpus, we employ the relative position in-
stead. B (Beginning) represents the first child of 
a relationship, E (End) means the last child of a 
relationship, and M (Middle) represents all the 
middle children.  
Under this agreement, the original Chinese 
CSS-based tree will be converted to a new 
tagged-flattened string. Note the converting ex-
ample from Figure 3(a) to Figure 3(b): node par-
allel-[(1,1), (2,2)] (see Figure 2) is converted to 
a flat string. Its first child is represented as <par-
allel, @B> with the semantic span, while the last 
child is <parallel, @E> with the corresponding 
semantic span. 
We then perform word alignment on the modi-
fied bilingual sentences, and extract the new 
translation rules based on the new alignment, as 
shown in Figure 3(b) to Figure 3(c). Now the 
newly extracted rule ?<parallel, @E > [X] ?? 
||| and growing [X] ? is tagged with cohesive in-
formation. Thus, if the similar relationship paral-
lel occurs in the test source sentence, this type of 
rule is more likely to be chosen to generate the 
cohesive word ?and? during decoding because it 
is more discriminating than the original rules ([X] 
?? ||| and growing [X]). The conditional prob-
abilities of the new translation rules are calculat-
ed following (Chiang, 2005). 
3.2 CSS-based Transfer model  
In general, according to formula (3), the transla-
tion quality based on the log-linear model is re-
lated tightly with the features chosen. Most trans-
lation systems adopt the features from a transla-
tion model, a language model, and sometimes a 
reordering model. To give a bonus to generating 
cohesive expressions during decoding, we have 
designed a special additional feature. The addi-
tional feature is represented as a probability cal-
culated by a transfer model. 
Given the source CSS information, we want 
our transfer model to predict the most possible 
cohesive expressions. For example, given two 
semantic spans with a parallel relationship and 
many translation candidates, our transfer model 
is expected to assign higher scores to those with 
transitional expressions such as ?and? or ?as well 
as?. 
Let 
0 1, ,... nw w w?w  represent the transitional 
expressions observed in the target string. Our 
transfer model can be represented as a condition-
al probability: 
( | ) (4)P CSSw  
    By deriving each node of the CSS, we can 
obtain a factored formula: 
,( | ) ( | , ) (5)i j ij i jP CSS P R RP??w w 
where 
ijw
is the transitional expression produced 
by the thj child of the thi node of the CSS. iR is 
the relationship type of the thi node. For the thj
child in the thi  node, 
jRP
is its relative position 
(B, M or E) introduced in Section 3.1.  
    The process of training this transfer model and 
smoothing is similar to the process of training a 
language model. We obtain the factored transfer 
probability as follows, 
1
1
0 0
( | , )
( | , ) ( | , , ) (6)
ij i j
i
n
k
j k
k
i j
P R RP
P w R RP P w w R RP?
?
? ?
w  
where  
 
0 0 ,... (7)nij nw w w? ?w
 
Following (Bilmes and Kirchhoff, 2003), the 
conditional probabilities 10( | , , )ik jkP w w R RP? in 
formula (6) are estimated in the same way as a 
factored language model, which has the ad-
vantage of easily incorporating various linguistic 
information. 
Considering that 
ijw
 commonly appears at the 
beginning of the target translation of a source 
semantic span such as ?which ??, namely, the 
left-frontier phrases, we focus only on the left-
frontier phrases when training this model. Note 
that if there exists a target word before a left 
frontier, and this word is aligned to NULL, we 
will expand the left frontier to this word. The 
expansion process will be repeated until there is 
no such word. For example, if we take the CSS 
and the alignment in Figure 3(a) for training, the 
left frontier of the second child will be expanded 
from ?growing? to ?and?. In addition, taking the 
tri-gram left-frontier phrase for example, we can 
obtain a training sample such as 
ijw
= and grow-
ing public, R=parallel, RP = E. 
By learning such probabilities for different 
transitional expressions conditioned on different 
relationships, we are able to capture the inner 
connection between the source CSS and the pro-
jected target cohesive phrases. Thus, during de-
coding, if we add the probability generated by 
the transfer model of ( | )P CSSw as a feature in 
853
formula (3), it will certainly contribute to select-
ing more cohesive candidates.  
3.3 Elementary-Unit Cohesion Constraint 
As mentioned in Section 3.2, in the transfer 
model, the transitional phrases are expected to 
occur at the left frontier of a projected span on 
target side. In fact, this depends on the assump-
tion that the projected translations of any two 
disjoint source semantic spans are also disjoint to 
keep their own semantic integrity. We call this 
assumption the integrity assumption. This as-
sumption is intuitive and supported by statistics. 
After analyzing 1,007 golden aligned Chinese-
English sentence-pairs, we find that approxi-
mately 90% of the pairs comply with the as-
sumption. However, in real automatically aligned 
noisy data, the ratio of complying pairs reduces 
to 71%4. Two projected translations that violate 
the integrity assumption may mutually overlap, 
which causes our confusion on where to extract 
the transitional phrases. In this case, extracted 
transitional phrases are likely to be wrong. 
    To increase the chance of extracting correct 
transitional phrases, the alignment results must 
be modified to reduce the impact of incorrect 
alignment. We propose a dynamic cleaning 
method to ensure that the most expressive transi-
tional phrases fall in the accessible extraction 
range before training the transfer model. 
3.3.1 EUC and non-EUC 
As we have defined in Section 2, the minimal 
semantic span is called elementary unit (eu). If 
the source eu and its projected target span com-
ply with the integrity assumption, we say that 
such an eu and its projected span have Elemen-
tary-Unit-Cohesion (EUC). We define EUC 
formally as follows. 
Given two elementary units 
Aeu  and Beu , 
and their projected target spans 
Aps and Bps
bound by the word alignment, the alignment 
complies with EUC only if there is no overlap 
between 
Aps  and Bps . Otherwise, the alignment 
is called non-EUC. The common EUC and non-
EUC cases are illustrated in Figure 4. 
EUC is the basic case for the integrity as-
sumption. For the best cases, the elementary 
units comply with EUC, and thus the semantic 
                                                 
4 The aligning tool is GIZA++ with 5 iterations of Model 1, 
5 iterations of HMM, and 10 iterations of Model 4. The 
GIZA++ code can be downloaded from 
https://code.google.com/p/giza-pp/ 
spans combined by elementary units are certainly 
subject to the integrity assumption.  
 
uA euB
psA psB 
(a) mono EUC case 
euA euB
psApsB  
(b) swap EUC case euA euB
psA psB 
(c) non-EUC case 
Figure.4 The schematic diagram of EUC cases 
and non-EUC case.  
3.3.2 A Dynamic Cleaning Method 
An intuitive method to clean the alignment re-
sults is to drop off the noisy word-to-word links 
that cause non-EUC. Considering that the drop-
ping process is a post-editing method for the 
original alignment obtained by a state-of-the-art 
aligner such as GIZA++, we do not expect over-
deleting. Therefore, we tend to take a relatively 
conservative strategy to minimize the deleting 
operation. 
Given a sentence-pair (f, e), suppose that 
0{ ,..., ,..., }i If f f?f  is divided into M elemen-
tary units 
0{ ,..., ,..., }m MU u u u? , and e has N 
words, that is, 
0{ ,..., ,..., }n Ne e e?e . If A  is the 
word alignment of (f, e), then the goal is to con-
struct the maximum subset *A A? under the 
condition that *A  is the word alignment with the 
constraint of EU. The search process can be de-
scribed as the pseudo code in Figure 5. 
In Figure 5, we scan each target word and each 
source eu to assign each word to a unique eu un-
der the EUC constraint with the lowest cost. 
Function cost( , )n m  in line 6 computes the 
counts of deleted links that force the thn target 
word to align only to words in the range of the 
thm eu. For example, if the thn target word is 
aligned to the thi , ( 1)thi ? , and ( 2)thi ? word in 
source side, while the thi word belongs to 
1`mu
 
and the ( 1)thi ?  and ( 2)thi ?  words belong to 
2mu
, then 
1cost( , ) 2mn u ?
, and 
2cost( , ) 1mn u ?
. 
In line 6, Score[n][m] saves a list of scores, each 
score computed by adding the current cost(n, m) 
with the history score of each list of Score[n-1]. 
854
Before the next iteration, the bad branches are 
pruned, as seen in line 5. We adopt the following 
two ways to prune:  
(1) EUC constraint: if the current link violates 
EUC alignment, delete it. 
(2) Keep the hypothesis with a fixed maximum 
size to avoid too large a searching space. 
 
 
 
Figure 5. The pseudo code of dynamic cleaning 
method.  
4 Experiments 
4.1 Experimental Setup 
To obtain the CSSs of Chinese sentences, we use 
the Chinese parser proposed in (Tu et al, 2013a). 
Their parser first segments the compound-
complex sentence into a series of elementary 
units, and then builds structure of the hierarchical 
relationships among these elementary units. 
Their parser was reported to achieve an F-score 
for elementary unit segmentation of approxi-
mately 0.89. The progressive, causal, and condi-
tion terms of functional relationships can be rec-
ognized with precisions of 0.86, 0.8, and 0.75, 
respectively, while others, such as purpose, par-
allel, and flowing, achieve only 0.5, 0.59 and 
0.62, respectively.  
The translation experiments have been con-
ducted in the Chinese-to-English direction. The 
bilingual training data for translation model and 
CSS-based transfer model is FBIS corpus with 
approximately 7.1 million Chinese words and 9.2 
million English words. We obtain the word 
alignment with the grow-diag-final-and strategy 
with GIZA++. Before training the CSS-based 
transfer model, the alignment for transfer model 
is modified by our dynamic cleaning method. 
During the cleaning process, the maximum size 
of hypothesis is limited to 5. A 5-gram language 
model is trained with SRILM5 on the combina-
tion of the Xinhua portion of the English Giga-
word corpus combined with the English part of 
FBIS. For tuning and testing, we use NIST03 
evaluation data as the development set. 
NIST04/05/06, CWMT08-Development 6  and 
CWMT08-Evaluation data are used for testing 
under the measure metric of BLEU-4 (Papineni 
et al 2002) with the shortest length penalty.  
Table 1 shows how the CSS is distributed in 
all testing sets. According to the statistics in Ta-
ble 1, we see that CSS is really widely distribut-
ed in the NIST and CWMT corpora, which im-
plies that the translation quality may benefit sub-
stantially from the CSS information, if it is well 
considered in SMT.  
 
4.2 Extracted Transitional Expressions 
Eleven types of Chinese functional relationships 
and their English left-frontier phrases (tri-gram) 
learned by our transfer model are given in Table 
2.  
The results in Table 2 show that some left-
frontier phrases reflect the source functional rela-
tionship well, especially for those with better 
precision of relationship recognition, such as 
progressive, causal and condition. Conversely, 
lower precision of relationship recognition may 
weaken the learning ability of the transfer model. 
For example, noisy left-frontier phrases are easi-
ly generated under relationships such as parallel 
and purpose. 
                                                 
5 http://www.speech.sri.com/projects/srilm/ 
6 The China Workshop on Machine Translation 
//Pseudo code for dynamic cleaning                             
1: Score [N+1][M]={[0]}N M?          /* initialize  
                                      cumulative cost score chart*/ 
2: Path [M]=[[]]                  /*initialize tracking path*/ 
3: forn = 1 N? :{           /*  scan target words*/ 
4:   for 0 1m M? ? ? :{        /*scan source U set */ 
5:     PrunePath();   
                 /* prune invalid  path and high-cost path*/ 
6:     Score[n][m]=GetScore(Score[n-1], cost(n, m)) 
       /*compute current cumulative cost score by previ-
ous score and current cost*/ 
7:      SaveCurrentPath(Path[m]);  
/*add current index to Path*/ 
8:  }//end m    
9:}//end n  
10: OptimalPath = 
[ ]argmax{ [ ][ ]}Path m Score N m
; 
 
 Total CSS Ratio(%) 
NIST04 1,788 1,307 73.1 
NIST05 1,082 849 78.5 
NIST06 1,000 745 74.5 
CWMT08-Dev. 1,006 818 81.3 
CWMT08-Eval. 1,006 818 81.3 
Table 1. The numbers of sentences and the 
CSS ratios of all sentences. CWMT08-Dev. is 
short for CWMT08 Development data and 
CWMT08-Eval. is CWMT08 Evaluation da-
ta. 
855
 4.3 Results on SMT with Different Strategies 
For this work, we use an in-house decoder to 
build the SMT baseline; it combines the hierar-
chical phrase-based translation model (Chiang, 
2005; Chiang, 2007) with the BTG (Wu, 1996) 
reordering model (Xiong et al, 2006; Zens and 
Ney, 2006; He et al, 2010).  
To test the effectiveness of the proposed mod-
els, we have compared the translation quality of 
different integration strategies. First, we adopted 
only the tagged-flattened rules in the hierarchical 
translation system. Next, we added the log prob-
ability generated by the transfer model as a fea-
ture into the baseline features. The baseline fea-
tures include bi-directional phrase translation 
probabilities, bi-directional lexical translation 
probabilities, the BTG re-ordering features, and 
the language model feature. The tri-gram left-
frontier phrase was adopted in the experiment. 
Then the probability generated by the transfer 
model with EUC constraint is added. Finally, we 
incorporated the tagged-flattened rules and the 
additional transfer model feature together.  
Table 3 shows the results of these different in-
tegrated strategies. In Table 3, almost all BLEU 
scores are improved, no matter what strategy is 
used. In particular, the best performance marked 
in bold is as high as 1.24, 0.94, and 0.82 BLEU 
points, respectively, over the baseline system on 
NIST04, CWMT08 Development, and CWMT08 
Evaluation data. The strategy of ?TFS+ Flat-
tened Rule? is the most stable. Meanwhile the 
?Flattened Rule? achieves better performance 
than ?TFS?. The merits of ?Flattened Rule? are 
two-fold: 1) In training process, the new word 
alignment upon modified sentence pairs can 
align transitional expressions to flattened CSS 
tags; 2) In decoding process, the CSS-based rules 
are more discriminating than the original rules, 
which is more flexible than ?TFS?.  From the 
table, we cannot conclude that the EUC con-
straint will certainly promote translation quality, 
but the transfer model performs better with the 
constraint on most testing sets. 
4.4 Analysis of Different Effects of Different 
N-grams 
As mentioned in Section 4.3, we have noted the 
effectiveness of tri-gram transfer model, which 
means 2n ? in formula (7). In fact, the lengths of 
common transitional expressions vary from one 
word to several words. To evaluate the effects of 
different n-grams for our proposed transfer mod-
el, we compared the uni-/bi-/tri-gram transfer 
models in SMT, and illustrate the results in Fig-
Relation Left-frontier phrases (tri-gram) 
parallel as well as;   at the same; ? 
progressive but will also; in addition to;? 
causal 
therefore , the;   for this reason;   as a 
result; because it is;   so it is;? 
condition as long as;   only when the? 
hypothesis if we do; if it is;  if the us; ? 
alternative regardless of whether;? 
purpose 
it is necessary;  
further promote the ;? 
explanation that is ,;  the first is; first is the;? 
adversative however , the ;  but it is; ? 
flowing this is a; which is an; ? 
consequence so that the; to ensure that? 
Table 2. Chinese functional relations and their 
corresponding English left-frontier phrases 
learned by our transfer model. The noun phrases 
starting with a definite / indefinite word are fil-
tered because they are unlikely to be the transi-
tional phrases. 
 
 
 
NIST04 NIST05 NIST06 
CWMT08?s 
Dev. 
CWMT08?s 
Eval. 
Baseline   33.42   31.99   33.88       26.14       23.88 
+Flattened Rule   34.54**   32.32   34.58**       26.79**       24.70** 
+TFS (without EUC)   33.93**   32.04   34.40*       26.44       24.58** 
+TFS   33.84**   32.63*   34.15       27.08**       24.65** 
+TFS+ Flattened Rule   34.66**   32.54 34.52**       26.87**       24.49** 
       + Flattened Rule: only use the tagged-flattened translation rules 
       + TFS:  only use the transfer model score as an additional feature (based on 3-gramtransitional phrase) 
       + TFS + Flattened Rule: both are used 
       *: value with * means that it is significantly better than the baseline with p<0.05 
       **: value with ** means that it is significantly better than the baseline with p<0.01 
Table 3. BLEU scores of the testing sets with different integrating strategies 
856
ure 6. In this experiment, the CSS-based transla-
tion rules and the CSS-based transfer model are 
both incorporated. Considering time and compu-
ting resources, in the rest of our paper, our analy-
sis is conducted on NIST05 and NIST06.  
We choose 0,1, 2n ?  in this experiment for 
that the common English transitional expressions 
are primarily conjunctions, most of which are 
less than 4 words. Results in Figure 6 show that 
the uni-gram and tri-gram transitional expres-
sions seem more fitting for our transfer model. 
One possible reason is that uni-gram or tri-gram 
conjunctions are more utilized in an English text. 
In a conjunction expression list proposed by 
(Williams, 1983) which summarizes the differ-
ent kinds of conjunctions based on the work of 
Halliday and Hassan (1976), we obtain the statis-
tical results on uni-/bi-/tri-gram expressions, 
which are about 52.1%/16.9%/23.9% respective-
ly. 
 
4.5 Experiments on Big Training Data 
To further evaluate the effectiveness of the pro-
posed models, we also conducted an experiment 
on a larger set of bilingual training data from the 
LDC corpus7 for translation model and transfer 
model. The training corpus contains 2.1M sen-
tence pairs with approximately 27.7M Chinese 
words and 31.9M English words. All the other 
settings were the same as the SMT experiments 
of sub-section 4.3. The final BLEU scores on 
NIST05 and NIST06 are given in Table 4.  
The results in Table 4 further verify the effec-
tiveness of our proposed models. The best per-
formance with bold marking scored as high as 
0.83 and 0.64 BLEU points, respectively over the 
                                                 
7 LDC category number: LDC2000T50, DC2002E18, 
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, 
LDC2005T10 and LDC2005T34. 
baseline system on NIST05 and NIST06 evalua-
tion data.  
 
4.6 Translation Examples  
Two SMT examples of Chinese-to-English are 
given in Table 5. We observe that compared to 
the baseline, our approach has obvious ad-
vantages on translating the implicit relations, due   
to generating translational expressions on target 
side. Moreover, with the transitional expressions, 
cohesion of the entire translation improves. No-
tably, the transitional expressions in this work 
like ?including, there are, the core of which? are 
not linguistic conjunctions. We would like to call 
them ?generalized? conjunctions, because they 
tie semantic fragments together, analogously to 
linguistic conjunctions. 
5 Related Work  
Improving cohesion for complex sentences or 
discourse translation has attracted much attention 
in recent years. Such research efforts can be 
roughly divided into two groups: 1) research on 
lexical cohesion, which mainly contributes to the 
selection of generated target words; 2) efforts to 
improve the grammatical cohesion, such as dis-
ambiguation of references and connectives.  
In lexical cohesion work, (Gong et al, 2011; 
Xiao et al, 2011; Wong and Kit, 2012) built dis-
course-based models to ensure lexical cohesion 
or consistency. In (Xiong et al, 2013a), three 
different features were designed to capture the 
lexical cohesion for document-level machine 
translation. (Xiong et al, 2013b) incorporated 
lexical-chain-based models (Morris and Hirst, 
1991) into machine translation. They generated 
the target lexical chains based on the source 
 
Figure 6.  Different translation qualities along 
with different n-grams for transfer model.  
30
31
32
33
34
35
NIST05 NIST06
BLEU 
Testing Set 
Uni-gram
Bi-gram
Tri-gram
 NIST05 NIST06 
Baseline    35.20     35.52 
+Flattened Rule    36.03** 36.10* 
+TFS    35.56* 36.04* 
+TFS +Flattened Rule    36.02**    36.16** 
+ Flattened Rule: only use the tagged-flattened transla-
tion rules 
 + TFS:  only use the transfer model score as an addi-
tional feature (3-gram transitional phrase) 
+ TFS + Flattened Rule: both are used 
*: value with * means that it is significantly better than 
the baseline with p<0.05 
**: value with ** means that it is significantly better 
than the baseline with p<0.01 
Table 4. BLEU scores on the large-scale training 
data.  
857
chains via maximum entropy classifiers, and 
used the target chains to work on the word selec-
tion. 
 Limited work has been conducted on gram-
matical cohesion. (Marcu et al, 2000) designed a 
discourse structure transfer module, but it fo-
cused on converting the semantic structure rather 
than actual translation. (Tu et al, 2013b) provid-
ed a Rhetorical-Structure-Theory-based tree-to-
string translation method for complex sentences 
with explicit relations inspired by (Marcu et al, 
2000), but their models worked only for explicit 
functional relations, and they were concerned 
mainly with the translation integrity of semantic 
span rather than cohesion. (Meyer and Popescu-
Belis, 2012) used sense-labeled discourse con-
nectives for machine translation from English to 
French. They added the labels assigned to con-
nectives as an additional input to an SMT system, 
but their experimental results show that the im-
provements under the evaluation metric of BLEU 
were not significant. (Nagard and Koehn, 2010) 
addresses the problems of reference or anaphora 
resolution inspired by work of Mitkov et al 
(1995). 
To the best of our knowledge, our work is the 
first attempt to exploit the source functional rela-
tionship to generate the target transitional ex-
pressions for grammatical cohesion, and we have 
successfully incorporated the proposed models 
into an SMT system with significant improve-
ment of BLEU metrics. 
6 Conclusion 
In this paper, we focus on capturing cohesion 
information to enhance the grammatical cohesion 
of machine translation. By taking the source CSS 
into consideration, we build bridges to connect 
the source functional relationships in CSS to tar-
get transitional expressions; such a process is 
very similar to human translating. 
    Our contributions can be summarized as: 1) 
the new translation rules are more discriminative 
and sensitive to cohesive information by convert-
ing the source string into a CSS-based tagged-
flattened string; 2) the new additional features 
embedded in the log-linear model can encourage 
the decoder to produce transitional expressions. 
The experimental results show that significant 
improvements have been achieved on various 
test data, meanwhile the translations are more 
cohesive and smooth, which together demon-
strate the effectiveness of our proposed models.  
In the future, we will extend our methods to 
other translation models, such as the syntax-
based model, to study how to further improve the 
performance of SMT systems. Besides, more 
language pairs with various linguistic structures 
will be taken into consideration.  
Acknowledgement 
We would like to thank Jiajun Zhang for provid-
ing the BTG-based hierarchical decoder. The 
research work has been partially funded by the 
Natural Science Foundation of China under 
Grant No. 61333018, the Hi-Tech Research and 
Development Program (?863? Program) of China 
under Grant No. 2012AA011101, and also 
the Key Project of Knowledge Innovation Pro-
gram of Chinese Academy of Sciences under 
Grant No. KGZD-EW-501 as well.  
     
Source ?????????????????? ????????????????? ? ? 
Reference 
In the past three years, the sequencing of three chromosomes has been completed, including 
chromosomes 20 , 21 , and 22 . 
Baseline 
In the past three years , now has three terms of the completion of the chromosomes , 20 , 21 
and 22 . 
Improved 
In the past three years , there are three chromosomes to accomplish , including 20 , 21 and 
22 . 
Source ??????????????????????????????????? 
Reference 
The above-mentioned propositions constitute the basic connotation of this one-china principle 
with safeguarding china ' s sovereignty and territorial integrity as its core . 
Baseline 
The above-mentioned propositions constitute the basic meaning of the one-china principle is 
the core of safeguard china ' s sovereignty and territorial integrity . 
Improved 
The above-mentioned propositions constitute the basic meaning of the one-china principle , 
the core of which is to safeguard china ' s sovereignty and territorial integrity . 
Table 5. Examples of baseline and the improved system outputs. 
 
858
References  
Jeff A. Bilmes and Katrin Kirchhoff. Factored lan-
guage models and generalized parallel backoff. In 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for Compu-
tational Linguistics on Human Language Technol-
ogy: companion volume of the Proceedings of 
HLT-NAACL 2003--short papers-Volume 2: 4-6. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 263?
270. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, pages 
33(2):201?228. 
Zhengxian Gong, Min Zhang, and Guodong Zhou. 
Cache-based document-level statistical machine 
translation, 2011, Edinburgh, Scotland, UK. In 
Proceedings of the 2011 Conference on Empirical 
Methods in Natural Language Processing, pages 
909?919. 
Liane Guillou. 2013. Analysing lexical consistency in 
translation. In Proceedings of the Workshop on 
Discourse in Machine Translation, pages 10?18, 
Sofia 
Michael A.K. Halliday, Hasan R. Cohesion in English. 
1976. London: Longman. 
Zhongjun He, Yao Meng, and Hao Yu. 2010b. Maxi-
mum Entropy Based Phrase Reordering for Hier-
archical Phrase-based Translation. In Proc. of the 
Conf. on Empirical Methods for Natural Language 
Processing (EMNLP), pages 555?563. 
Annie Louis and Ani Nenkova. 2012. A coherence 
model based on syntactic patterns. In Proceedings 
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1157?
1168, Jeju Island, Korea, July.  
William C Mann and Sandra A Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243?281. 
Ruslan Mitkov, Sung-Kwon Choi, and Randall Sharp. 
1995. Anaphora resolution in Machine Transla-
tion. In Proceedings of the Sixth International 
Conference on Theoretical and Methodological Is-
sues in Machine Translation. 
Thomas Meyer and Andrei Popescu-Belis. Using 
sense-labeled discourse connectives for statistical 
machine translation, 2012, In Proceedings of the 
Joint Workshop on Exploiting Synergies between 
Information Retrieval and Machine Translation 
(ESIRMT) and Hybrid Approaches to Machine 
Translation (HyTra), pages:129-138. 
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indica-
tor of the structure of text. Comput. Linguist., 
17(1):21?48, March. 
Ronan L Nagard and Philipp Koehn. 2010, Aiding 
pronoun translation with co-reference resolution, 
In proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, 
pages 252-261. 
Franz J Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statisti-
cal machine translation. In Proc. of ACL, pages 
295?302. 
Kishore Papineni, Salim Roukos, Todd Ward, et al 
2002, BLEU: a method for automatic evaluation 
of machine translation. In proceedings of the 40th 
annual meeting on association for computational 
linguistics. pages: 311-318. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
Miltsakaki, Livio Robaldo, Aravind Joshi, and 
Bonnie Webber. 2008. The Penn Discourse Tree-
bank 2.0. In Proceedings of the 6th International 
Conference on Language Resources and Evalua-
tion (LREC 2008). 
Williams Ray. Teaching the Recognition of Cohesive 
Ties in Reading a Foreign, 1983. Reading in a 
foreign language, 1(1), pages: 35-52. 
Radu Soricut and Daniel Marcu. 2003. Sentence level 
discourse parsing using syntactic and lexical in-
formation. In Proceedings of the 2003 Conference 
of the North American Chapter of the Association 
for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 149?156. 
Mei Tu, Yu Zhou, and Chengqing Zong. 2013a, A 
Novel Translation Framework Based on Rhetori-
cal Structure Theory. In Proceedings of the 51st 
Annual Meeting of the Association for Computa-
tional Linguistics, short paper, Sofia, Bulgaria, 
pages 370?374. 
Mei Tu, Yu Zhou, Chengqing Zong. 2013b, Automat-
ically Parsing Chinese Discourse Based on Maxi-
mum Entropy. In The 2nd Conference on Natural 
Language Processing & Chinese Computing. 
Ashish Vaswani, Liang Huang and David Chiang, 
Huang L, Chiang D. 2012, Smaller alignment 
models for better translations: unsupervised word 
alignment with the l 0-norm. In Proceedings of the 
50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 
1,pages 311-319. 
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 
Document-level consistency verification in ma-
chine translation. September 2011, Xiamen, China. 
In Proceedings of the 2011 MT summit XIII, pag-
es 131?138. 
859
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the 44th Annual Meeting of the Association for 
Computational Linguistics, pages 521?528. 
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv, 
and Qun Liu. 2013 (a). Modeling lexical cohesion 
for document-level machine translation. In Pro-
ceedings of the Twenty-Third International Joint 
Conference on Artificial Intelligence (IJCAI-13), 
Beijing, China, August. 
Deyi Xiong, Ding Yang, Min Zhang and Chew Lim 
Tan, 2013 (b). Lexical Chain Based Cohesion 
Models for Document-Level Statistical Machine 
Translation. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages: 1563-1573. 
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine 
translation. In Proceedings of theWorkshop on 
Statistical Machine Translation, pages 55?63. 
Qiang Zhou, 2004, Annotation Scheme for Chinese 
Treebank, Journal of Chinese Information Pro-
cessing, 18(4): 1-8. 
 
 
 
 
 
 
860
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779?784,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
RNN-based Derivation Structure Prediction for SMT
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{ffzhai, jjzhang, yzhou, cqzong}@nlpr.ia.ac.cn
Abstract
In this paper, we propose a novel deriva-
tion structure prediction (DSP) model
for SMT using recursive neural network
(RNN). Within the model, two steps are
involved: (1) phrase-pair vector represen-
tation, to learn vector representations for
phrase pairs; (2) derivation structure pre-
diction, to generate a bilingual RNN that
aims to distinguish good derivation struc-
tures from bad ones. Final experimental
results show that our DSP model can sig-
nificantly improve the translation quality.
1 Introduction
Derivation structure is important for SMT decod-
ing, especially for the translation model based
on nested structures of languages, such as BTG
(bracket transduction grammar) model (Wu, 1997;
Xiong et al, 2006), hierarchical phrase-based
model (Chiang, 2007), and syntax-based model
(Galley et al, 2006; Marcu et al, 2006; Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008;
Zhang et al, 2011; Zhai et al, 2013). In general,
derivation structure refers to the tuple that records
the used translation rules and their compositions
during decoding, just as Figure 1 shows.
Intuitively, a good derivation structure usually
yields a good translation, while bad derivations al-
ways result in bad translations. For example in
Figure 1, (a) and (b) are two different derivations
for Chinese sentence ??? ? ?9 ?1 
 ?
!?. Comparing the two derivations, (a) is more
reasonable and yields a better translation. How-
ever, (b) wrongly translates phrase ?? ?9? to
?and Sharon? and combines it with [??;Bush]
incorrectly, leading to a bad translation.
To explore the derivation structure?s potential
on yielding good translations, in this paper, we
propose a novel derivation structure prediction
(DSP) model for SMT decoding.
(a) (b)
??
Bush
??? ??
held a talk
? ??
with Sharon
?? ? ??
held a talk
? ??
with Sharon
??
Bush
?? ? ??
held a talk
? ??with Sharon
??
Bush
Bush and
? ??
Sharon
??
??
Bush
??? ??
held a talk
? ??
and Sharon
? ??
and Sharon
?? ? ??
held a talk
Figure 1: Two different derivation structures of
BTG translation model. In the structure, leaf
nodes denote the used translation rules. For each
node, the first line is the source string, while the
second line is its corresponding translation.
The proposed DSP model is built on recur-
sive neural network (RNN). Within the model,
two steps are involved: (1) phrase-pair vector
representation, to learn vector representations for
phrase pairs; (2) derivation structure prediction,
to build a bilingual RNN that aims to distinguish
good derivation structures from bad ones. Ex-
tensive experiments show that the proposed DSP
model significantly improves the translation qual-
ity, and thus verify the effectiveness of derivation
structure on indicating good translations.
We make the following contributions in this
work:
? We propose a novel RNN-based model to do
derivation structure prediction for SMT de-
coding. To our best knowledge, this is the
first work on this issue in SMT community;
? In current work, RNN has only been verified
to be useful on monolingual structure learn-
ing (Socher et al, 2011a; Socher et al, 2013).
We go a step further, and design a bilingual
RNN to represent the derivation structure;
? To train the RNN-based DSP model, we pro-
pose a max-margin objective that prefers gold
derivations yielded by forced decoding to
n-best derivations generated by the conven-
tional BTG translation model.
779
2 The DSP Model
The basic idea of DSP model is to represent the
derivation structure by RNN (Figure 2). Here, we
build the DSP model for BTG translation model,
which is naturally compatible with RNN. We be-
lieve that the DSP model is also beneficial to other
translation models. We leave them as our future
work.
2.1 Phrase-Pair Vector Representation
Phrase pairs, i.e., the used translation rules, are the
leaf nodes of derivation structure. Hence, to repre-
sent the derivation structure by RNN, we need first
to represent the phrase pairs. To do this, we use
two unsupervised recursive autoencoders (RAE)
(Socher et al, 2011b), one for the source phrase
and the other for the target phrase. We call the unit
of the two RAEs the Leaf Node Network (LNN).
Using n-dimension word embedding, RAE can
learn a n-dimension vector for any phrase. Mean-
while, RAE will build a binary tree for the phrase,
as Figure 2 (in box) shows, and compute a re-
construction error to evaluate the vector. We use
E(T
ph
) to denote the reconstruction error given by
RAE, where ph is the phrase and T
ph
is the corre-
sponding binary tree. In RAE, higher error corre-
sponds to worse vector. More details can be found
in (Socher et al, 2011b).
Given a phrase pair (sp, tp), we can use LNN
to generate two n-dimension vectors, representing
sp and tp respectively. Then, we concatenate the
two vectors directly, and get a vector r ? R
2n
to
represent phrase pair (sp, tp) (shown in Figure
2). The vector r is evaluated by combining the
reconstruction error on both sides:
E(T
sp
, T
tp
) =
1
2
[E(T
sp
) + E(T
tp
) ?
N
s
N
t
]
(1)
where T
sp
and T
tp
are the binary trees for sp and
tp. N
s
and N
t
denote the number of nodes in T
sp
and T
tp
. Note that in order to unify the errors on
the two sides, we use ratio N
s
/N
t
to eliminate the
influence of phrase length.
Then, according to Equation (1), we compute
an LNN score to evaluate the vector of all phrase
pairs, i.e., leaf nodes, in derivation d:
LNN(d) = ?
?
(sp,tp)
E(T
sp
, T
tp
)
(2)
where (sp, tp) is the used phrase pair in derivation
d. Obviously, the derivation with better phrase-
pair representations will get a higher LNN score.
??
? ?? with Sharon
?? ? ?? held a talk
Bush
Figure 2: Illustration of DSP model, based on the
derivation structure in Figure 1(a).
The LNN score will serve as part of the DSP
model for predicting good derivation structures.
2.2 Derivation Structure Prediction
Using the vector representations of phrase pairs,
we then build a Derivation Structure Network
(DSN) for prediction (Figure 2).
In DSN, the derivation structure is repre-
sented by repeatedly applying unit neural net-
work (UNN, Figure 3) at each non-leaf node. The
UNN receives two node vectors r
1
? R
2n
and
r
2
? R
2n
as input, and induces a vector p ? R
2n
to represent the parent node.
r1 r2
p
score
Figure 3: The unit neural network used in DSN.
For example, in Figure 2, node [? ?9; with
Sharon] serves as the first child with vector r
1
,
and node [?1
?!; held a talk] as the second
child with vector r
2
. The parent node vector p,
representing [? ?9 ?1 
 ?!; held a talk
with Sharon], is computed by merging r
1
and r
2
:
p = f(W
UNN
[r
1
; r
2
] + b
UNN
) (3)
where [r
1
; r
2
] ? R
4n?1
is the concatenation of r
1
and r
2
, W
UNN
? R
2n?4n
and b
UNN
? R
2n?1
are
the network?s parameter weight matrix and bias
term respectively. We use tanh(?) as function f .
Then, we compute a local score using a simple
inner product with a row vector W
score
UNN
? R
1?2n
:
s(p) = W
score
UNN
? p (4)
The score measures how well the two child nodes
r
1
and r
2
are merged into the parent node p.
As we all know, in BTG derivations, we have
two different ways to merge translation candi-
dates, monotone or inverted, meaning that we
780
merge two candidates in a monotone or inverted
order. We believe that different merging or-
der (monotone or inverted) needs different UNN.
Hence, we keep two different ones in DSN, one for
monotone order (with parameter W
mono
, b
mono
,
and W
score
mono
), and the other for inverted (with pa-
rameter W
inv
, b
inv
, and W
score
inv
). The idea is that
the merging order of the two candidates will de-
termine which UNN will be used to generate their
parent?s vector and compute the score in Equa-
tion (4). Using a set of gold derivations, we can
train the network so that correct order will receive
a high score by Equation (4) and incorrect one will
receive a low score.
Thus, when we merge the candidates of two ad-
jacent spans during BTG-based decoding, the lo-
cal score in Equation (4) is useful in two aspects:
(1) for the same merging order, it evaluates how
well the two candidates are merged; (2) for the dif-
ferent order, it compares the candidates generated
by monotone order and inverted order.
Further, to assess the entire derivation structure,
we apply UNN to each node recursively, until the
root node. The final score utilized for derivation
structure prediction is the sum of all local scores:
DSN(d) =
?
p
s(p) (5)
where d denotes the derivation structure and p is
the non-leaf node in d. Obviously, by this score,
we can easily assess different derivations. Good
derivations will get higher scores while bad ones
will get lower scores.
Li et al (2013) presented a network to predict
how to merge translation candidates, in monotone
or inverted order. Our DSN differs from Li?s work
in two points. For one thing, DSN can not only
predict how to merge candidates, but also evaluate
whether two candidates should be merged. For an-
other, DSN focuses on the entire derivation struc-
ture, rather than only the two candidates for merg-
ing. Therefore, the translation decoder will pursue
good derivation structures via DSN. Actually, Li?s
work can be easily integrated into our work. We
leave it as our future work.
3 Training
In this section, we present the method of training
the DSP model. The parameters involved in this
process include: word embedding, parameters of
the two unsupervised RAEs in LNN, and parame-
ters in DSN.
3.1 Max-Margin Framework
In DSP model, our goal is to assign higher scores
to gold derivations, and lower scores to bad ones.
To reach this goal, we adopt a max-margin frame-
work (Socher et al, 2010; Socher et al, 2011a;
Socher et al, 2013) for training.
Specifically, suppose we have a training data
like (u
i
,G(u
i
),A(u
i
)), where u
i
is the input
source sentence, G(u
i
) is the gold derivation set
containing all gold derivations of u
i
1
, and A(u
i
)
is the possible derivation set that contains all
possible derivations of u
i
. We want to minimize
the following regularized risk function:
J(?) =
1
N
N
?
i=1
R
i
(?) +
?
2
? ? ?
2
, where
R
i
(?) = max
?
d?A(u
i
)
(
s
(
?, u
i
,
?
d
)
+ ?
(
?
d,G(u
i
)
)
)
? max
d?G(u
i
)
(
s
(
?, u
i
, d
)
)
(6)
Here, ? is the model parameter. s(?, u
i
, d) is the
DSP score for sentence u
i
?s derivation d. It is
computed by summing LNN score (Equation (2))
and DSN score (Equation (5)):
s(?, u, d) = LNN
?
(d) +DSN
?
(d) (7)
?(
?
d,G(u
i
)) is the structure loss margin, which
penalizes derivation
?
d more if it deviates more
from gold derivations. It is formulated as:
?
(
?
d,G(u
i
)
)
=
?
pi?
?
d
?
s
?{pi 6? G(u
i
)}+ ?
t
Dist(y(
?
d), ref)
(8)
The margin includes two parts. For the first part,
pi is the source span in derivation
?
d, ? {?} is an
indicator function. We use the first part to count
the number of source spans in derivation
?
d, but
not in gold derivations. The second part is for
target side. Dist(y(
?
d), ref) computes the edit-
distance between the translation result y(
?
d) de-
fined by derivation
?
d and the reference translation
ref . Obviously, this margin can effectively esti-
mate the difference between derivation
?
d and gold
derivations, both on source side and target side.
Note that ?
s
and ?
t
are only two hyperparameters
for scaling. They are independent of each other,
and we set ?
s
= 0.1 and ?
t
= 0.1 respectively.
1
We investigate the general case here and suppose that
one sentence could have several different gold derivations.In
the experiment, we only use one gold derivation for simple
implementation.
781
3.2 Learning
As the risk function, Equation (6) is not differ-
entiable. We train the model via the subgradient
method (Ratliff et al, 2007; Socher et al, 2013).
For parameter ?, the subgriadient of J(?) is:
?J
??
=
1
N
?
i
?s(?, u
i
,
?
d
m
)
??
?
?s(?, u
i
, d
m
)
??
+??
where
?
d
m
is the derivation with the highest DSP
score, and d
m
denotes the gold derivation with the
highest DSP score. We adopt the diagonal vari-
ant of AdaGrad (Duchi et al, 2011; Socher et al,
2013) to minimize the risk function for training.
3.3 Training Instances Collection
In order to train the model, we need to collect the
gold derivation set G(u
i
) and possible derivation
set A(u
i
) for input sentence u
i
.
For G(u
i
) , we define it by force decoding
derivation (FDD). Basically, FDD refers to the
derivation that produces the exact reference trans-
lation (single reference in our training data). For
example, since ?Bush held a talk with Sharon? is
the reference of test sentence ??? ? ?9 ?
1
?!?, then Figure 1(a) is one of the FDDs.
As FDD can produce reference translation, we be-
lieve that FDD is of high quality, and take them as
gold derivations for training.
For A(u
i
), it should contain all possible deriva-
tions of u
i
. However, it is too difficult to obtain
all derivations. Thus, we use n-best derivations of
SMT decoding to simulate the complete derivation
space, and take them as the derivations in A(u
i
).
4 Integrating the DSP Model into SMT
To integrate the DSP model into decoding, we take
it (named DSP feature) as one of the features in the
log-linear framework of SMT. During decoding,
the DSP feature is distributed to each node in the
derivation structure. For the leaf node, the score
in Equation (2), i.e., LNN score, serves as the fea-
ture. For the non-leaf node, Equation (4) plays
the role. In order to give positive feature value to
the log-linear framework (for logarithm), we nor-
malize the DSP scores to [0,1] during decoding.
Due to the length limit, we ignore the specific nor-
malization methods here. We just preform some
simple transformations (such as adding a constant,
computing reciprocal), and convert the scores pro-
portionally to [0,1] at last.
5 Experiments
5.1 Experimental Setup
To verify the effectiveness of our DSP model, we
perform experiments on Chinese-to-English trans-
lation. The training data contains about 2.1M sen-
tence pairs with about 27.7M Chinese words and
31.9M English words
2
. We train a 5-gram lan-
guage model by the Xinhua portion of Gigaword
corpus and the English part of the training data.
We obtain word alignment by GIZA++, and adopt
the grow-diag-final-and strategy to generate the
symmetric alignment. We use NIST MT 2003 data
as the development set, and NIST MT04-08
3
as
the test set. We use MERT (Och, 2004) to tune pa-
rameters. The translation quality is evaluated by
case-insensitive BLEU-4 (Papineni et al, 2002).
The statistical significance test is performed by
the re-sampling approach (Koehn, 2004). The
baseline system is our in-house BTG system (Wu,
1997; Xiong et al, 2006; Zhang and Zong, 2009).
To train the DSP model, we first use Word2Vec
4
toolkit to pre-train the word embedding on large-
scale monolingual data. The used monolingual
data contains about 1.06B words for Chinese and
1.12B words for English. The dimensionality of
our vectors is 50. The detiled training process is
as follows:
(1) Using the BTG system to perform force de-
coding on FBIS part of the bilingual training data
5
,
and collect the sentences succeeded in force de-
coding (86,902 sentences in total)
6
. We then col-
lect the corresponding force decoding derivations
as gold derivations. Here, we only use the best
force decoding derivation for simple implementa-
tion. In future, we will try to use multiple force
decoding derivations for training.
(2) Collecting the bilingual phrases in the leaf
nodes of gold derivations. We train LNN by these
phrases via L-BFGS algorithm. Finally, we get
351,448 source phrases to train the source side
RAE and 370,948 target phrases to train the tar-
get side RAE.
2
LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,
LDC2005T10 and LDC2005T34.
3
For MT06 and MT08, we only use the part of news data.
4
https://code.google.com/p/word2vec/
5
Here we only use the high quality corpus FBIS to guar-
antee the quality of force decoding derivation.
6
Many sentence pairs fail in forced decoding due to many
reasons, such as reordering limit, noisy alignment, and phrase
length limit (Yu et al, 2013).
782
(3) Decoding the 86902 sentences by the BTG
system to get n-best translations and correspond-
ing derivations. The n-best derivations are used to
simulate the entire derivation space. We retain at
most 200-best derivations for each sentence.
(4) Leveraging force decoding derivations and
n-best derivations to train the DSP model. Note
that all parameters, including word embedding and
parameters in LNN and DSN, are tuned together in
this step. It takes about 15 hours to train the entire
network using a 16-core, 2.9 GHz Xeon machine.
5.2 Experimental Results
We compare baseline BTG system and the DSP-
augmented BTG system in this section. The final
translation results are shown in Table 1.
After integrating the DSP model into BTG sys-
tem, we get significant improvement on all test
sets, about 1.0 BLEU points over BTG system on
average. This comparison strongly demonstrates
that our DSP model is useful and will be a good
complement to current translation models.
Systems
BLEU(%)
MT04 MT05 MT06 MT08 Aver
BTG 36.91 34.69 33.83 27.17 33.15
BTG+DSP 37.41 35.77 35.08 28.42 34.17
Table 1: Final translation results. Bold numbers
denote that the result is significantly better than
baseline BTG system (p < 0.05). Column ?Aver?
gives the average BLEU points of the 4 test sets.
To have a better intuition for the effectiveness
of our DSP model, we give a case study in Figure
4. It depicts two derivations built by BTG system
and BTG+DSP system respectively.
From Figure 4(b), we can see that BTG system
yields a bad translation due to the bad derivation
structure. In the figure, BTG system makes three
mistakes. It attaches candidates [??; achieve-
ments], [? ? ; has reached] and [#\?;
singapore] to the big candidate [?UTransactions of the Association for Computational Linguistics, 1 (2013) 243?254. Action Editor: Philipp Koehn.
Submitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Unsupervised Tree Induction for Tree-based Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation,  
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
  
 
 
 
 
Abstract 
In current research, most tree-based translation 
models are built directly from parse trees. In 
this study, we go in another direction and build 
a translation model with an unsupervised tree 
structure derived from a novel non-parametric 
Bayesian model. In the model, we utilize 
synchronous tree substitution grammars (STSG) 
to capture the bilingual mapping between 
language pairs. To train the model efficiently, 
we develop a Gibbs sampler with three novel 
Gibbs operators. The sampler is capable of 
exploring the infinite space of tree structures by 
performing local changes on the tree nodes. 
Experimental results show that the string-to-
tree translation system using our Bayesian tree 
structures significantly outperforms the strong 
baseline string-to-tree system using parse trees. 
1 Introduction 
In recent years, tree-based translation models1 are 
drawing more and more attention in the 
community of statistical machine translation 
(SMT). Due to their remarkable ability to 
incorporate context structure information and long 
distance reordering into the translation process, 
tree-based translation models have shown 
promising progress in improving translation 
quality (Liu et al, 2006, 2009; Quirk et al, 2005; 
Galley et al, 2004, 2006; Marcu et al, 2006; Shen 
et al, 2008; Zhang et al, 2011b). 
However, tree-based translation models always 
suffer from two major challenges: 1) They are 
usually built directly from parse trees, which are 
generated by supervised linguistic parsers. 
                                                          
1 A tree-based translation model is defined as a model 
using tree structures on one side or both sides. 
However, for many language pairs, it is difficult to 
acquire such corresponding linguistic parsers due 
to the lack of Tree-bank resources for training. 2) 
Parse trees are actually only used to model and 
explain the monolingual structure, rather than the 
bilingual mapping between language pairs. This 
indicates that parse trees are usually not the 
optimal choice for training tree-based translation 
models (Wang et al, 2010). 
Based on the above analysis, we can conclude 
that the tree structure that is independent from 
Tree-bank resources and simultaneously considers 
the bilingual mapping inside the bilingual sentence 
pairs would be a good choice for building tree-
based translation models. 
Therefore, complying with the above conditions, 
we propose an unsupervised tree structure for tree-
based translation models in this study. In the 
structures, tree nodes are labeled by combining the 
word classes of their boundary words rather than 
by syntactic labels, such as NP, VP. Furthermore, 
using these node labels, we design a generative 
Bayesian model to infer the final tree structure 
based on synchronous tree substitution grammars 
(STSG) 2 . STSG is derived from the word 
alignments and thus can grasp the bilingual 
mapping effectively. 
Training the Bayesian model is difficult due to 
the exponential space of possible tree structures for 
each training instance. We therefore develop an 
efficient Gibbs sampler with three novel Gibbs 
operators for training. The sampler is capable of 
exploring the infinite space of tree structures by 
performing local changes on the tree nodes. 
                                                          
2 We believe it is possible to design a model to infer the 
node label and tree structure jointly. We plan this as 
future work, and here, we focus only on inferring the 
tree structure in terms of the node labels derived from 
word classes. 
243
The tree structure formed in this way is 
independent from the Tree-bank resources and 
simultaneously exploits the bilingual mapping 
effectively. Experiments show that the proposed 
unsupervised tree (U-tree) is more effective and 
reasonable for tree-based translation than the parse 
tree. 
The main contributions of this study are as 
follows: 
1) Instead of the parse tree, we propose a 
Bayesian model to induce a U-tree for tree-
based translation. The U-tree exploits the 
bilingual mapping effectively and does not 
rely on any Tree-bank resources. 
2) We design a Gibbs sampler with three novel 
Gibbs operators to train the Bayesian model 
efficiently. 
The remainder of the paper is organized as 
follows. Section 2 introduces the related work. 
Section 3 describes the STSG generation process, 
and Section 4 depicts the adopted Bayesian model. 
Section 5 describes the Gibbs sampling algorithm 
and Gibbs operators. In Section 6, we analyze the 
achieved U-trees and evaluate their effectiveness. 
Finally, we conclude the paper in Section 7. 
2 Related Work 
In this study, we move in a new direction to build a 
tree-based translation model with effective 
unsupervised U-tree structures. 
For unsupervised tree structure induction, 
DeNero and Uszkoreit (2011) adopted a parallel 
parsing model to induce unlabeled trees of source 
sentences for syntactic pre-reordering. Our 
previous work (Zhai et al, 2012) designed an EM-
based method to construct unsupervised trees for 
tree-based translation models. This work differs 
from the above work in that we design a novel 
Bayesian model to induce unsupervised U-trees, 
and prior knowledge can be encoded into the 
model more freely and effectively. 
Blunsom et al (2008, 2009, 2010) utilized 
Bayesian methods to learn synchronous context 
free grammars (SCFG) from a parallel corpus. The 
obtained SCFG is further used in a phrase-based 
and hierarchical phrase-based system (Chiang, 
2007). Levenberg et al (2012) employed a 
Bayesian method to learn discontinuous SCFG 
rules. This study differs from their work because 
we concentrate on constructing tree structures for 
tree-based translation models. Our U-trees are 
learned based on STSG, which is more appropriate 
for tree-based translation models than SCFG. 
Burkett and Klein (2008) and Burkett et al 
(2010) focused on joint parsing and alignment. 
They utilized the bilingual Tree-bank to train a 
joint model for both parsing and word alignment. 
Cohn and Blunsom (2009) adopted a Bayesian 
method to infer an STSG by exploring the space of 
alignments based on parse trees. Liu et al (2012) 
re-trained the linguistic parsers bilingually based 
on word alignment. Burkett and Klein (2012) 
utilized a transformation-based method to learn a 
sequence of monolingual tree transformations for 
translation. Compared to their work, we do not rely 
on any Tree-bank resources and focus on 
generating effective unsupervised tree structures 
for tree-based translation models. 
Zollmann and Venugopal (2006) substituted the 
non-terminal X in hierarchical phrase-based model 
by extended syntactic categories. Zollmann and 
Vogel (2011) further labeled the SCFG rules with 
POS tags and unsupervised word classes. Our work 
differs from theirs in that we present a Bayesian 
model to learn effective STSG translation rules and 
U-tree structures for tree-based translation models, 
rather than designing a labeling strategy for 
translation rules. 
3 The STSG Generation Process 
In this work, we induce effective U-trees for the 
string-to-tree translation model, which is based on 
a synchronous tree substitution grammar (STSG) 
between source strings and target tree fragments. 
We take STSG as the generation grammar to match 
the translation model. Typically, such an STSG3 is 
a 5-tuple as follows: 
( , , , , )s t t tG N S P ? ?  
where: 
i s?  and t?  represent the set of source and 
target words, respectively, 
i tN  is the set of target non-terminals, 
i t tS N?  is the start root non-terminal, and 
i P  is the production rule set. 
                                                          
3 Generally, an STSG involves tree fragments on both 
sides. Here we only consider the special case where the 
source side is actually a string. 
244
Apart from the start non-terminal tS , we define 
all the other non-terminals in tN  by word classes. 
Inspired by (Zollmann and Vogel, 2011), we 
divide these non-terminals into three categories: 
one-word, two-word and multi-word non-terminals. 
The one-word non-terminal is a word class, such as 
C, meaning that it dominates a word whose word 
class is C. Two-word non-terminals are used to 
stand for two word strings. They are labeled in the 
form of C1+C2, where C1 and C2 are the word 
classes of the two words separately. Accordingly, 
multi-word non-terminals represent the strings 
containing more than two words. They are labeled 
as C1?Cn, demanding that the word classes of the 
leftmost word and the rightmost word are C1 and 
Cn, respectively. 
We use POS tag to play the role of word class4. 
For example, the head node of the rule in Figure 1 
is a multi-word non-terminal PRP?RB. It requires 
that the POS tags of the leftmost and rightmost 
word must be PRP and RB, respectively. Xiong et 
al. (2006) showed that the boundary word is an 
effective indicator for phrase reordering. Thus, we 
believe that combining the word class of boundary 
words can denote the whole phrase well. 
PRP...RB
we
PRP
VBP:x0 RB:x1
VBP+RB
??   x1   x0
wo-men
 
Figure 1. An example of an STSG production rule. 
Each production rule in P  consists of a source 
string and a target tree fragment. In the target tree 
fragment, each internal node is labeled with a non-
terminal in tN , and each leaf node is labeled with 
either a target word in t?  or a non-terminal in tN . 
The source string in a production rule comprises 
source words and variables. Each variable 
corresponds to a leaf non-terminal in the target tree 
fragment. In the STSG, the production rule is used 
to rewrite the root node into a string and a tree 
fragment. For example, in Figure 1, the rule 
rewrites the head node PRP?RB into the 
corresponding string and fragment. 
An STSG derivation refers to the process of 
generating a specific source string and target tree 
                                                          
4 The demand of a POS tagger impairs the independence 
from manual resources to some extent. In future, we 
plan to design a method to learn effective unsupervised 
labels for the non-terminals. 
structure by production rules. This process begins 
with the start non-terminal tS  and an empty source 
string. We repeatedly choose production rules to 
rewrite the leaf non-terminals and expand the 
string until no leaf non-terminal is left. Finally, we 
acquire a source string and a target tree structure 
defined by the derivation. The probability of a 
derivation is given as follows: 
  
1
( ) ( | )
n
i i
i
p d p r N
 
 ?  (1) 
where the derivation comprises a sequence of rules 
d=(r1,?,rn), and Ni represents the root node of rule 
ri. Hence, for a specific bilingual sentence pair, we 
can generate the best target-side tree structure 
based on the STSG, independent from the Tree-
bank resources. The STSG used in the above 
process is learned by the Bayesian model that is 
detailed in the next section. 
Actually, SCFG can also be used to build the U-
trees. We do not use SCFG because most of the 
tree-based models are based on STSG. In our 
Bayesian model, the U-trees are optimized through 
selecting a set of STSG rules. These STSG rules 
are consistent with the translation rules used in the 
tree-based models. 
Another reason is that STSG has a stronger 
expressive power on tree construction than SCFG. 
In a STSG-based U-tree or a STSG rule, although 
not linguistically informed, the nodes labeled by 
POS tags are also effective on distinguishing 
different ones. However, with SCFG, we have to 
discard all the internal nodes (i.e., flattening the U-
trees or rules) to express the same sequence, 
leading to a poor ability of distinguishing different 
U-trees and production rules. Thus, using STSG, 
we can build more specific U-trees for translation.  
In addition, we find that the Bayesian SCFG 
grammar cannot even significantly outperform the 
heuristic SCFG grammar (Blunsom et al 2009)5. 
This would indicate that the SCFG-based 
derivation tree as by-product is also not such good 
for tree-based translation models. Considering the 
above reasons, we believe that the STSG-based 
learning procedure would result in a better 
translation grammar for tree-based models. 
                                                          
5 In (Blunsom et al, 2009), for Chinese-to-English 
translation, the Bayesian SCFG grammar only 
outperform the heuristic SCFG grammar by 0.1 BLEU 
points on NIST MT 2004 and 0.6 BLEU points on NIST 
MT 2005 in the NEWS domain. 
245
4 Bayesian Model 
In this section, we present a Bayesian model to 
learn STSG defined in section 3. In the model, we 
use ?N to denote the probability distribution 
( | )p r N  in Equation (1). ?N follows a multinomial 
distribution and we impose a Dirichlet prior (DP) 
on it: 
  
0 0
| ~ ( )
| , ~ ( , ( | ) )
N
N N N
r N Multi
P DP P N
T
T D D <  (2) 
where 0 ( | )P N<  (base distribution) is used to assign 
prior probabilities to the STSG production rules. ?N 
controls the model?s tendency to either reuse 
existing rules or create new ones using the base 
distribution 0 ( | )P N< . 
Instead of denoting the multinomial distribution 
explicitly with a specific ?N, we integrate over all 
possible values of ?N to achieve the probabilities of 
rules. This integration results in the following 
conditional probability for rule ri given the 
previously observed rules r-i = r1 ,?, ri-1: 
 
0
0
( | )
( | , , , ) i
i
r N ii
i N i
N N
n P r N
p r r N P
n
DD D




   (3) 
Where n-i ri  denotes the number of ri in ir , and n
-i 
N  
represents the total count of rules rewriting non-
terminal N in ir . Thanks to the exchangeability of 
the model, all permutations of the rules are actually 
equiprobable. This means that we can compute the 
probability of each rule based on the previous and 
subsequent rules (i.e. consider each rule as the last 
one). This characteristic allows us to design an 
efficient Gibbs sampling algorithm to train the 
Bayesian model. 
4.1  Base Distribution 
The base distribution 0 ( | )P r N  is designed to 
assign prior probabilities to the STSG production 
rules. Because each rule r consists of a target tree 
fragment frag and a source string str in the model, 
we follow Cohn and Blunsom (2009) and 
decompose the prior probability 0 ( | )P r N  into two 
factors as follows: 
  0 ( | ) ( | ) ( | )P r N P frag N P str frag ?  (4) 
where ( | )P frag N  is the probability of 
producing the target tree fragment frag. To 
generate frag, Cohn and Blunsom (2009) used a 
geometric prior to decide how many child nodes to 
assign each node. Differently, we require that each 
multi-word non-terminal node must have two child 
nodes. This is because the binary structure has 
been verified to be very effective for tree-based 
translation (Wang et al, 2007; Zhang et al, 2011a).  
The generation process starts at root node N. At 
first, root node N is expanded into two child nodes. 
Then, each newly generated node will be checked 
to expand into two new child nodes with 
probability pexpand. This process repeats until all the 
new non-terminal nodes are checked. Obviously, 
pexpand controls the scale of tree fragments, where a 
large pexpand corresponds to large fragments
6. The 
new terminal nodes (words) are drawn uniformly 
from the target-side vocabulary, and the non-
terminal nodes are created by asking two questions 
as follows: 
1) What type is the node, one-word, two-
word or multi-word non-terminal? 
2) What tag is used to label the node? 
The answer to question 1) is chosen from a 
uniform distribution, i.e., the probability is 1/3 for 
each type of non-terminal. The entire generation 
process is in a top-down manner, i.e., generating a 
parent node first and then its children. 
With respect to question 2), because the father 
node has determined the POS tags of boundary 
words, we only need one POS tag to generate the 
label of the current node. For example, in Figure 1, 
as the father node PRP?RB demands that the POS 
tag of the rightmost word is RB, the right child of 
PRP?RB must also satisfy this condition. 
Therefore, we choose a POS tag VBP and obtain 
the label VBP+RB. The POS tag is drawn 
uniformly from the POS tag set. If the current node 
is a one-word non-terminal, question 2) is 
unnecessary. Similarly, with respect to the two-
word non-terminal node, questions 1) and 2) are 
both unnecessary for its two child nodes because 
they have already been defined by their father node. 
As an example of the generative process, the 
tree fragment in Figure 1 is created as follows: 
a. Determine that the left child of PRP?RB is 
a one-word non-terminal (labeled with PRP); 
b. Expand PRP and generate the word ?we? for 
PRP; 
                                                          
6 In our experiment, we set pexpand to 1/3 to encourage 
small tree fragments.  
246
c. Determine that the right child of PRP?RB is 
a two-word non-terminal; 
d. Utilize the predetermined RB and a POS tag 
VBP to form the tag of the two-word non-
terminal: VBP+RB; 
e. Expand VBP+RB (to VBP and RB); 
f. Do not expand VBP and RB. 
( | )P str frag  in Equation (4) is the probability of 
generating the source string, which contains 
several source words and variables. Inspired by 
(Blunsom et al, 2009) and (Cohn and Blunsom, 
2009), we define ( | )P str frag  as follows: 
 
var
1
1 1
( | ) ( ;1)
| |
poisson sw
sw
sw
c
c
s i
P str frag P c
c i 
 u u? ?  (5) 
where csw is the number of words in the source 
string. ?s means the source vocabulary set. Further, 
cvar denotes the number of variables, which is 
determined by the tree fragment frag. 
As shown in Equation(5), we first determine 
how many source words to generate using a 
Poisson distribution Ppoisson(csw;1), which imposes a 
stable preference for short source strings. Then, we 
draw each source word from a uniform distribution 
over ?s. Afterwards, we insert the variables into 
the string. The variables are inserted one at a time 
using a uniform distribution over the possible 
positions. This factor discourages more variables.  
For the example rule in Figure 1, the generative 
process of the source string is: 
a. Decide to generate one source word;  
b. Generate the source word ??? (wo-men) ?;  
c. Insert the first variable after the word;  
d. Insert the second variable between the word 
and the first variable. 
Intuitively, a good translation grammar should 
carry both small translation rules with enough 
generality and large rules with enough context 
information. DeNero and Klein (2007) proposed 
this statement, and Cohn and Blunsom (2009) has 
verified it in their experiments with parse trees. 
Our base distribution is also designed based on 
this intuition. Considering the two factors in our 
base distribution, we penalize both large target tree 
fragments with many nodes and long source strings 
with many words and variables. The Bayesian 
model tends to select both small and frequent 
STSG production rules to construct the U-trees. 
With these types of trees, we can extract small 
rules with good generality and simultaneously 
obtain large rules with enough context information 
by composition. We will show the effectiveness of 
our U-trees in the verification experiment. 
5 Model Training by Gibbs Sampling 
In this section, we introduce a collapsed Gibbs 
sampler, which enables us to train the Bayesian 
model efficiently. 
5.1 Initialization State 
At first, we use random binary trees to initialize the 
sampler. To get the initial U-trees, we recursively 
and randomly segment a sentence into two parts 
and simultaneously create a tree node to dominate 
each part. The created tree nodes are labeled by the 
non-terminals described in section 3. 
Using the initial target U-trees, source sentences 
and word alignment, we extract minimal GHKM 
translation rules7 in terms of frontier nodes (Galley 
et al, 2004). Frontier nodes are the tree nodes that 
can map onto contiguous substrings on the source 
side via word alignment. For example, the bold 
italic nodes with shadows in Figure 2 are frontier 
nodes. In addition, it should be noted that the word 
alignment is fixed8, and we only explore the entire 
space of tree structures in our sampler. Differently, 
Cohn and Blunsom (2009) designed a sampler to 
infer an STSG by fixing the tree structure and 
exploring the space of alignment. We believe that 
it is possible to investigate the space of both tree 
structure and alignment simultaneously. This 
subject will be one of our future work topics. 
For each training instance (a pair of source 
sentence and target U-tree structure), the extracted 
GHKM minimal translation rules compose a 
unique STSG derivation9. Moreover, all the rules 
developed from the training data constitute an 
initial STSG for the Gibbs sampler. 
                                                          
7 We attach the unaligned word to the lowest frontier 
node that can cover it in terms of word alignment. 
8 The sampler might reinforce the frequent alignment 
errors (AE), which would harm the translation model 
(TM). Actually, the frequent AEs also greatly impair the 
conventional TM. Besides, our sampler encourages the 
correct alignments and simultaneously discourages the 
infrequent AEs. Thus, compared with the conventional 
TMs, we believe that our final TM would not be worse 
due to AEs. Our final experiments verify this point and 
we will conduct a much detailed analysis in future. 
9 We only use the minimal GHKM rules (Galley et al, 
2004) here to reduce the complexity of the sampler. 
247
jin-tian jian-mianwo-men zai-ci
PRP+VBP
today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
PRP...RB
NN...RB
 
Figure 2. Illustration of an initial U-tree structure. The 
bold italic nodes with shadows are frontier nodes. 
Under this initial STSG, the sampler modifies 
the initial U-trees (initial sample) to create a series 
of new ones (new samples) by the Gibbs operators. 
Consequently, new STSGs are created based on the 
new U-trees simultaneously and used for the next 
sampling operation. Repeatedly and after a number 
of iterations, we can obtain the final U-trees for 
building translation models. 
5.2 The Gibbs Operators 
In this section, we develop three novel Gibbs 
operators for the sampler. They explore the entire 
space of the U-tree structures by performing local 
changes on the tree nodes. 
For a U-tree of a given sentence, we define s-
node as the non-root node covering at least two 
words. Thus, the set of s-node contains all the tree 
nodes except the root node, the pre-terminal nodes 
and leaf nodes, which we call non-s-node. For 
example, in Figure 2, PRB?RB and PRP+VBP are 
s-nodes, while NN and NN?RB are non-s-nodes. 
Since the POS tag sequence of the sentence is 
fixed, all non-s-nodes would stay unchanged in all 
possible U-trees of the sentence. Based on this fact, 
our Gibbs operators work only on s-nodes. 
Further, we assign 3 descendant candidates (DC) 
for each s-node: its left child, right child and its 
sibling. For example, in Figure 3, the 3 DCs for the 
s-node are node PRP, VBP and RB respectively. 
According to the different DCs it governs, every s-
node might be in one of the two different states: 
1) Left state: as Figure 3(a) shows, the s-node 
governs the left two DCs, PRP and VBP, 
and is labeled PRP+VBP. 
2) Right state: as Figure 3(b) shows, the s-node 
governs the right two DCs, VBP and RB, and 
is labeled VBP+RB. 
For a specific U-tree, the states of s-nodes are fixed. 
Thus, by changing an s-node?s state, we can easily 
transform this U-tree to another one, i.e., from the 
current sample to a new one. 
To formulate the U-tree transformation process, 
we associate a binary variable ??{0,1} with each 
s-node, indicating whether the s-node is in the left 
?  or right state ?  Then we can change 
the U-tree by changing value of the ? parameters. 
Our first Gibbs operator, Rotate, just works by 
sampling value of the ?parameters, one at a time, 
and changing the U-tree accordingly. For example, 
in Figure 3(a), the s-node is currently in the left 
VWDWH? :HVDPSOHWKH?RIWKLVQRGHDQGLI
WKHVDPSOHGYDOXHRI?LVZHNHHSWKHVWUXFWXUH
unchanged, i.e., in the left state. Otherwise, we 
change its state to the right state ? , and 
transform the U-tree to Figure 3(b) accordingly. 
jian-mianwo-men zai-ci
s-node
we
PRP
meet
VBP
again
RB
?? ?? ??
PRP...RB
PRP+VBP
jian-mianwo-men zai-ci
s-node
we
PRP
meet
VBP
again
RB
?? ?? ??
PRP...RB
VBP+RB
(b) ?=1(a) ?=0
Rotate
 
Figure 3. Illustration of the Rotate operator. In the 
figure, (a) and (b) denote the s-node?s left state and right 
state respectively. The bold italic nodes with shadows in 
the figure are frontier nodes. 
Obviously, towards an s-node for sampling, the 
two values of ? would define two different U-trees. 
Using the GHKM algorithm (Galley et al 2004), 
we can get two different STSG derivations from 
the two U-trees based on the fixed word alignment. 
Each derivation carries a set of STSG rules (i.e., 
minimal GHKM translation rules) of its own. In 
the two derivations, the STSG rules defined by the 
two states include the one rooted at the s-node?s 
lowest ancestor frontier node, and the one rooted at 
the s-node if it is a frontier node. For instance, in 
Figure 3(a), as the s-node is not a frontier node, the 
left state (? ) defines only one rule: 
 
0 2 1
0 1 2
:
... ( ( : : ) : )
leftr x x x
PRP RB PRP VBP x PRP x VBP x RB
o
  
Differently, in Figure 3(b), the s-node is a 
frontier node and thus the right state (? 1) defines 
two rules: 
248
 
0 0 1 0 1
1 1 0 0 1
: ... ( : : )
: ( : : )
right
right
r x x PRP RB x PRP x VBP RB
r x x VBP RB x VBP x RB


o 
o   
Using these STSG rules, the two derivations are 
evaluated as follows (We use the value of ? to 
denote the corresponding STSG derivation): 
0 1
0 1 0
( 0) ( | )
( 1) ( , | )
( | ) ( | , )
left
right right
right right right
p p r r
p p r r r
p r r p r r r


 
 
  
<  v
<  v
 
 
Where r  refers to the conditional context, i.e., the 
set of all other rules in the training data. All the 
probabilities in the above formulas are computed 
by Equation(3). We then normalize the two scores 
and sample a value of ? based on them. With the 
Bayesian model described in section 4, the sampler 
ZLOOSUHIHUWKH?WKDWSURGXFHVVPDOODQGIUHTXHQW
STSG rules. This tendency results in more frontier 
nodes in the U-tree (i.e., the s-node tends to be in 
the state that is a frontier node), which will factor 
the training instance into more small STSG rules. 
In this way, the overall likelihood of the bilingual 
data is improved by the sampler. 
Theoretically, the Rotate operator is capable of 
arriving at any possible U-tree from the initial U-
tree. This is because we can first convert the initial 
U-tree to a left branch tree by the Rotate operator, 
and then transform it to any other U-tree. However, 
it may take a long time to do so. Thus, to speed up 
the structure transformation process, we employ a 
Two-level-Rotate operator, which takes a pair of s-
nodes in a parent-child relationship as a unit for 
sampling. Similar to the Rotate operator, we also 
assign a binary variable ??{0,1} to each unit and 
update the U-tree by sampling the value of ?. The 
method of sampling ? is similar to the one used for 
?. Figure 4 shows an example of the operator. As 
shown in Figure 4(a), the unit NN?VBP and 
PRP+VBP is in the left state (?=0), and governs 
the left three descendants: NN, PRP, and VBP. By 
the Two-level-Rotate operator, we can convert the 
unit to Figure 4(b), i.e., the ULJKWVWDWH?=1). Just as 
Figure 4(b) shows, the governed descendants of the 
unit are turned to PRP, VBP, and RB. 
It may be confusing when choosing the parent-
child s-node pair for sampling because the parent 
node always faces two choices: combining the left 
child or right child for sampling. To avoid 
confusion, we split the Two-level-Rotate operator 
into two operators: Two-level-left-Rotate operator, 
which works with the parent node and its left child, 
and Two-level-right-Rotate operator, which only 
considers the parent node and its right child 10 . 
Therefore, the operator used in Figure 4 is a Two-
level-right-Rotate operator. 
jin-tian jian-mianwo-men zai-ci
PRP+VBP
Today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
NN...VBP
NN...RB
jin-tian jian-mianwo-men zai-ci
VBP+RB
Today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
PRP...RB
NN...RB
(a) ?=0 (b) ?=1
Two-level-right-Rotate
 
Figure 4. Illustration of the Two-level-Rotate operator. 
The bold italic nodes with shadows in the Figure are 
frontier nodes. 
During sampling, for each training instance, the 
sampler first applies the Two-level-left-Rotate 
operator to all candidate pairs of s-nodes (parent s-
node and its left child s-node) in the U-tree. After 
that, the Two-level-right-Rotate operator is applied 
to all the candidate pairs of s-nodes (parent s-node 
and its right child s-node). Then, we use the Rotate 
operator on every s-node in the U-tree. By utilizing 
the operators separately, we can guarantee that our 
sampler satisfies detailed balance. We visit all the 
training instances in a random order (one iteration). 
After a number of iterations, we can obtain the 
final U-tree structures and build the tree-based 
translation model accordingly. 
6 Experiments 
6.1 Experimental Setup 
The experiments are conducted on Chinese-to-
English translation. The training data are the FBIS 
corpus with approximately 7.1 million Chinese 
words and 9.2 million English words. We obtain 
the bidirectional word alignment with GIZA++, 
and then adopt the grow-diag-final-and strategy to 
obtain the final symmetric alignment. We train a 5-
gram language model on the Xinhua portion of the 
English Gigaword corpus and the English part of 
                                                          
10 We can also take more nodes as a unit for sampling, 
but this would make the algorithm much more complex. 
249
the training data. For tuning and testing, we use the 
NIST MT 2003 evaluation data as the development 
set, and use the NIST MT04 and MT05 data as the 
test set. We use MERT (Och, 2004) to tune 
parameters. Since MERT is prone to search errors, 
we run MERT 5 times and select the best tuning 
parameters in the tuning set. The translation quality 
is evaluated by case-insensitive BLEU-4 with the 
shortest length penalty. The statistical significance 
test is performed by the re-sampling approach 
(Koehn, 2004). 
To create the baseline system, we use the open-
source Joshua 4.0 system (Ganitkevitch et al, 2012) 
to build a hierarchical phrase-based (HPB) system, 
and a syntax-augmented MT (SAMT) 11  system 
(Zollmann and Venugopal, 2006) respectively. 
The translation system used for testing the 
effectiveness of our U-trees is our in-house string-
to-tree system (abbreviated as s2t). The system is 
implemented based on (Galley et al, 2006) and 
(Marcu et al 2006). In the system, we extract both 
the minimal GHKM rules (Galley et al, 2004), and 
the rules of SPMT Model 1 (Galley et al, 2006) 
with phrases up to length L=5 on the source side. 
We then obtain the composed rules by composing 
two or three adjacent minimal rules. 
To build the above s2t system, we first use the 
parse tree, which is generated by parsing the 
English side of the bilingual data with the Berkeley 
parser (Petrov et al, 2006). Then, we binarize the 
English parse trees using the head binarization 
approach (Wang et al, 2007) and use the resulting 
binary parse trees to build another s2t system. 
For the U-trees, we run the Gibbs sampler for 
1000 iterations on the whole corpus. The sampler 
uses 1,087s per iteration, on average, using a single 
core, 2.3 GHz Intel Xeon machine. For the 
hyperparameters, we set ? to 0.1 and pexpand = 1/3 
to give a preference to the rules with small 
fragments. We built an s2t translation system with 
the achieved U-trees after the 1000th iteration. We 
only use one sample to extract the translation 
grammar because multiple samples would result in 
a grammar that would be too large. 
                                                          
11 From (Zollmann and Vogel, 2011), we find that the 
performance of SAMT system is similar with the 
method of labeling SCFG rules with POS tags. Thus, to 
be convenient, we only conduct experiments with the 
SAMT system. 
6.2 Analysis of The Gibbs Sampler 
To evaluate the effectiveness of the Gibbs sampler, 
we explore the change of the training data?s 
likelihood with increasing sampling iterations. 
1.239E+08
1.243E+08
1.247E+08
1.251E+08
1.255E+08
1.259E+08
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations 
N
e
g
a
ti
v
e
-L
o
g
 L
ik
e
li
h
o
o
d random 1
random 2
random 3
 
Figure 5. Histograms of the training data?s likelihood vs. 
the number of sampling iterations. In the figure, random 
1 to 3 refers to three independent runs of the sampler 
with different initial U-trees as initialization states. 
Figure 5 depicts the negative-log likelihood of 
the training data after several sampling iterations. 
The results show that the overall likelihood of the 
training data is improved by the sampler. Moreover, 
comparing the three independent runs, we see that 
although the sampler begins with different initial 
U-trees, the training data?s likelihood is always 
similar during sampling. This demonstrates that 
our sampler is not sensitive to the random initial 
U-trees and can always arrive at a good final state 
beginning from different initialization states. Thus, 
we only utilize the U-trees from random 1 for 
further analysis hereafter. 
1.035E+07
1.040E+07
1.045E+07
1.050E+07
1.055E+07
1.060E+07
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations 
T
o
ta
l 
N
u
m
b
e
r 
o
f 
F
ro
n
ti
e
r 
N
o
d
e
s
random 1
random 2
random 3
 
Figure 6. The total number of frontier nodes for the 
three independent runs. 
6.3 Analysis of the U-tree Structure 
Acquiring better U-trees for translation is our final 
purpose. However, are the U-trees achieved by the 
250
Gibbs sampler appropriate for the tree-based 
translation model? 
To answer this question, we first analyze the 
effect of the sampler on the U-trees. Figure 6 
shows the total number of frontier nodes in the 
training data during sampling. The results show 
that the number of frontier nodes increases with 
increased sampling. This tendency indicates that 
our sampler prefers the tree structure with more 
frontier nodes. Consequently, the final U-tree 
structures can always be factored into many small 
minimal translation rules. Just as we have argued 
in section 4.1, this is beneficial for a good 
translation grammar. 
To demonstrate the above analysis, Figure 7 
shows a visual comparison between our U-tree 
(from random 1) and the binary parse tree (found 
by head binarization). Because the traditional parse 
tree is not binarized, we do not consider it for this 
analysis. Figure 7 shows that whether it is the 
target tree fragment or the source string of the rule, 
our U-trees always tend to obtain the smaller 
ones12. This comparison verifies that our Bayesian 
tree induction model is effective in shifting the tree 
structures away from complex minimal rules, 
which tend to negatively affect translation. 
0
200k
400k
600k
800k
1000k
2 3 4 5 6 7 8 9 10 >=11
U-Tree
binary parse tree
Number of Nodes in the Target Tree Fragment
N
um
be
r 
of
 R
ul
es
Number of Words and Variables in the Source String
0
300k
600k
900k
1200k
1 2 3 4 5 6 7
N
um
be
r 
of
 R
ul
es
 
Figure 7. Histograms over minimal translation rule 
statistics comparing our U-trees and binary parse trees. 
                                                          
12 Binary parse trees get more tree fragments with two 
nodes than U-trees. This is because there are many 
unary edges in the binary parse trees, while no unary 
edge exists in our U-trees. 
Specifically, we show an example of a binary 
parse tree and our U-tree in Figure 8. The example 
U-tree is more conducive to extracting effective 
translation rules. For example, to translate the 
Chinese phrase ?? ??, we can extract a rule (R2 
in Figure 9) directly from the U-tree because the 
phrase ?? ?? is governed by a frontier node, i.e., 
node ?VBD+RB?. However, because no node 
governs ?? ?? in the binary parse tree, we can 
only obtain a rule (R1 in Figure 9) with many extra 
nodes and edges, such as node CD in R1. Due to 
these extra things, R1 is too large to show good 
generality. 
was
QP
dollarsUS1500only
VBD NNSNNPCDRB
NP
NP
? ???????
NP-COMP
(a) binary parse tree
(b) U-tree
was dollarsUS1500only
VBD NNSNNPCDRB
? ???????
VBD+RB NNP+NNS
CD...NNS
VBD...NNS
 
Figure 8. Example of different tree structures. The node 
NP-COMP is achieved by head binarization. The bold 
italic nodes with shadows denote frontier nodes. 
was QP
only
VBD
CD:x0RB
NP
NP
NP-COMP:x1
was only
VBD RB
? ?VBD+RB
? x1x0?
R1:
R2:
 
Figure 9. Example rules to translate the Chinese phrase 
??  ? .? R1 is extracted from Figure 8(a), i.e., the 
binary parse tree. R2 is from Figure 8(b), i.e., the U-tree. 
251
Based on the above analysis, we can conclude 
that our proposed U-tree structures are conducive 
to extracting small, minimal translation rules. This 
indicates that the U-trees are more consistent with 
the word alignment and are good at capturing 
bilingual mapping information. Therefore, because 
parse trees are always constrained by cross-lingual 
structure divergence, we believe that the proposed 
U-trees would result in a better translation 
grammar. We demonstrate this conclusion in the 
next sub-section. 
6.4 Final Translation Results 
The final translation results are shown in Table 1. 
In the table, lines 3-6 refer to the string-to-tree 
systems built with different types of tree structures. 
Table 1 shows that all our s2t systems 
outperform the Joshua (HPB) and Joshua (SAMT) 
system significantly. This comparison verifies the 
superiority of our in-house s2t system. Moreover, 
the results shown in Table 1 also demonstrate the 
effectiveness of head binarization, which helps to 
improve the s2t system using parse trees in all 
translation tasks. 
To test the effectiveness of our U-trees, we give 
the s2t translation system using the U-trees (from 
random 1). The results show that the system using 
U-trees achieves the best translation result from all 
of the systems. It surpasses the s2t system using 
parse trees by 1.47 BLEU points on MT04 and 
1.44 BLEU points on MT05. Moreover, even using 
the binary parse trees, the achieved s2t system is 
still lower than our U-tree-based s2t system by 
0.97 BLEU points on the combined test set. From 
the translation results, we can validate our former 
analysis that the U-trees generated by our Bayesian 
tree induction model are more appropriate for 
string-to-tree translation than parse trees. 
System MT04 MT05 All 
Joshua (HPB) 31.73 28.82 30.64 
Joshua (SAMT) 32.48 29.77 31.56 
s2t (parse-tree) 33.73* 30.25* 32.75* 
s2t (binary-parse-tree) 34.09* 30.99*# 32.92* 
s2t (U-tree) 35.20*# 31.69*# 33.89*# 
Table 1. Results (in case-insensitive BLEU-4 scores) of 
s2t systems using different types of trees. The ?*? and 
?#? denote that the results are significantly better than 
the Joshua (SAMT) system and the s2t system using 
parse trees (p<0.01). 
6.5 Large Data 
We also conduct an experiment on a larger 
bilingual training data from the LDC corpus13. The 
training corpus contains 2.1M sentence pairs with 
approximately 27.7M Chinese words and 31.9M 
English words. Similarly, we train a 5-gram 
language model using the Xinhua portion of the 
English Gigaword corpus and the English part of 
the training corpus. With the same settings as 
before, we run the Gibbs sampler for 1000 
iterations and utilize the final U-tree structure to 
build a string-to-tree translation system. 
The final BLEU score results are shown in Table 
2. In the scenario with a large data, the string-to-
tree system using our U-trees still significantly 
outperforms the system using parse trees. 
System MT04 MT05 All 
Joshua (HPB) 34.55 33.11 34.01 
Joshua (SAMT) 34.76 33.72 34.37 
s2t (parse-tree) 36.40* 34.53* 35.70* 
s2t (binary-parse-tree) 37.38*# 35.14*# 36.54*# 
s2t (U-tree) 38.02*# 36.12*# 37.34*# 
Table 2. Results (in case-insensitive BLEU-4 scores) for 
the large training data. The meaning of ?*? and ?#? are 
similar to Table 1. 
7 Conclusion and Future Work 
In this paper, we explored a new direction to build 
a tree-based model based on unsupervised 
Bayesian trees rather than supervised parse trees. 
To achieve this purpose, we have made two major 
efforts in this paper: 
(1) We have proposed a novel generative 
Bayesian model to induce effective U-trees for 
tree-based translation. We utilized STSG in the 
model to grasp bilingual mapping information. We 
further imposed a reasonable hierarchical prior on 
the tree structures, encouraging small and frequent 
minimal rules for translation. 
(2) To train the Bayesian tree induction 
model efficiently, we developed a Gibbs sampler 
with three novel Gibbs operators. The operators are 
designed specifically to explore the infinite space 
of tree structures by performing local changes on 
the tree structure. 
                                                          
13 LDC category number : LDC2000T50, LDC2002E18, 
LDC2003E07, LDC2004T07, LDC2005T06, 
LDC2002L27, LDC2005T10 and LDC2005T34. 
252
Experiments on the string-to-tree translation 
model demonstrated that our U-trees are better 
than the parse trees. The translation results verify 
that the well-designed unsupervised trees are 
actually more appropriate for tree-based translation 
than parse trees. Therefore, we believe that the 
unsupervised tree structure would be a promising 
research direction for tree-based translation. 
In future, we plan to testify our sampler with 
various initial trees, such as the tree structure 
formed by (Zhang et al, 2008). We also plan to 
perform a detailed empirical comparison between 
STST and SCFG under our settings. Moreover, we 
will further conduct experiments to compare our 
methods with other relevant works, such as (Cohn 
and Blunsom, 2009) and (Burkett and Klein, 2012). 
Acknowledgments 
We would like to thank Philipp Koehn and three 
anonymous reviewers for their valuable comments 
and suggestions. The research work has been 
funded by the Hi-Tech Research and Development 
Program (?863? Program) of China under Grant 
No. 2011AA01A207, 2012AA011101, and 
2012AA011102. 
References 
Phil Blunsom, Trevor Cohn, Miles Osborne. 2008. 
Bayesian synchronous grammar induction. In 
Advances in Neural Information Processing Systems, 
volume 21, pages 161-168. 
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles 
Osborne. 2009. A gibbs sampler for phrasal 
synchronous grammar induction. In Proc. of ACL 
2009, pages 782-790. 
Phil Blunsom and Trevor Cohn. 2010. Inducing 
synchronous grammars with slice sampling. In Proc. 
of NAACL 2010, pages 238-241. 
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic Parsing). In Proc. of 
EMNLP 2008, pages 877-886. 
David Burkett, John Blitzer, and Dan Klein. 2010. Joint 
parsing and alignment with weakly synchronized 
grammars. In Proc. of NAACL 2010, pages 127-135.  
David Burkett and Dan Klein. 2012. Transforming trees 
to improve syntactic convergence. In Proc. of 
EMNLP 2012, pages 863-872. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2). pages 
201-228. 
Dekai Wu. 1996. A polynomial-time algorithm for 
statistical machine translation. In Proc. of ACL 1996, 
pages 152-158. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23:377-404. 
Trevor Cohn and Phil Blunsom. 2009. A bayesian 
model of syntax-directed tree to string grammar 
induction. In Proc. of EMNLP 2009, pages 352-361. 
Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 
2010. Inducing tree-substitution grammars. Journal 
of Machine Learning Research, pages 3053-3096. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP 2006, pages 232-241. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL 2007, pages 17-24. 
John DeNero and Jakob Uszkoreit. 2011. Inducing 
sentence structure from parallel corpora for 
reordering. In Proc. of EMNLP 2011, pages 193-203. 
Chris Dyer. 2010. Two monolingual parses are better 
than one (synchronous parse). In Proc. of NAACL 
2010, pages 263-266. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule. In Proc. of 
HLT-NAACL 2004, pages 273?280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inference and training of 
context-rich syntactic translation models. In Proc. of 
ACL-COLING 2006, pages 961-968. 
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post and Adam Lopez. 2011. Joshua 3.0: 
syntax-based machine translation with the thrax 
Grammar Extractor. In Proc of WMT11, pages 478-
484. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. A 
syntax-directed translator with extended domain of 
locality. In Proc. of AMTA 2006, pages 65-73. 
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.  
Statistical phrase-based translation, In Proc. of 
HLT/NAACL 2003, pages 48-54.  
253
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of EMNLP 
2004, pages 388?395. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
RichDUG =HQV &KULV '\HU DQG 2QG?HM %RMDU. 2007. 
Moses: open source toolkit for statistical machine 
translation. In Proc. of ACL 2007, pages 177-180. 
Abby Levenberg, Chris Dyer and Phil Blunsom. 2012. 
A bayesian model for learning SCFGs with 
discontiguous Rules. In Proc. of EMNLP 2012, pages 
223-232. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri 
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren N.G. Thornton, Jonathan Weese and Omar F. 
Zaidan. 2009. Joshua: An open source toolkit for 
parsing-based machine translation. In Proc. of ACL 
2009, pages 135-139. 
Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Re-
training monolingual parser bilingually for syntactic 
SMT. In Proc. of EMNLP 2012, pages 854-862. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006, pages 
609-616. 
Yang Liu, Yajuan Lv and Qun Liu. 2009. Improving 
tree-to-tree translation with packed forests. In Proc. 
of ACL-IJCNLP 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phrases. 
In Proc. of EMNLP 2006, pages 44-52. 
Franz Och, 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 2003, 
pages 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic 
evaluation of machine translation. In Proc. of ACL 
2002, pages 311-318. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: syntactically 
informed phrasal SMT. In Proc. of ACL 2005, pages 
271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation 
algorithm with a target dependency language model. 
In Proc. of ACL-08, pages 577-585. 
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. 
Binarizing syntax trees to improve syntax-based 
machine translation accuracy. In Proc. of EMNLP 
2007, pages 746-754. 
Wei Wang, Jonathan May, Kevin Knight, and Daniel 
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. 
Computational Linguistics, 36(2):247?277. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. 2012. Tree-based translation without using 
parse trees. In Proc. of COLING 2012, pages 3037-
3054. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous binarization for machine 
translation. In Proc. of HLT-NAACL 2006, pages 
256-263. 
Hao Zhang, Daniel Gildea, and David Chiang. 2008. 
Extracting synchronous grammars rules from word 
level alignments in linear time. In Proc. of COLING 
2008, pages 1081-1088. 
Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu.  
2011a. Binarized forest to string translation. In Proc. 
of ACL 2011, pages 835-845. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew 
Lim Tan. 2009. Forest-based tree sequence to string 
translation model. In Proc. of ACL-IJCNLP 2009, 
pages 172-180. 
Jiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b. 
Augmenting string-to-tree translation models with 
fuzzy use of source-side syntax. In Proc. of EMNLP 
2011, pages 204-215. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew 
Lim Tan and Sheng Li. 2007. A tree-to-tree 
alignment-based model for statistical Machine 
translation. MT-Summit-07. pages 535-542 
Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew 
Lim Tan and Sheng Li. 2008. A tree sequence 
alignment-based tree-to-tree translation model. In 
Proc. of ACL 2008, pages 559-567. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax 
augmented machine translation via chart parsing. In 
Proc. of Workshop on Statistical Machine 
Translation 2006, pages 138-141. 
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine 
translation. In Proc. of ACL 2011, pages 1-11. 
254
Large-scale Word Alignment Using Soft Dependency Cohesion 
Constraints 
Zhiguo Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{zgwang, cqzong}@nlpr.ia.ac.cn 
  
 
 
Abstract 
Dependency cohesion refers to the 
observation that phrases dominated by 
disjoint dependency subtrees in the source 
language generally do not overlap in the 
target language. It has been verified to be a 
useful constraint for word alignment. 
However, previous work either treats this 
as a hard constraint or uses it as a feature in 
discriminative models, which is ineffective 
for large-scale tasks. In this paper, we take 
dependency cohesion as a soft constraint, 
and integrate it into a generative model for 
large-scale word alignment experiments. 
We also propose an approximate EM 
algorithm and a Gibbs sampling algorithm 
to estimate model parameters in an 
unsupervised manner. Experiments on 
large-scale Chinese-English translation 
tasks demonstrate that our model achieves 
improvements in both alignment quality 
and translation quality. 
1 Introduction 
Word alignment is the task of identifying word 
correspondences between parallel sentence pairs. 
Word alignment has become a vital component of 
statistical machine translation (SMT) systems, 
since it is required by almost all state-of-the-art 
SMT systems for the purpose of extracting phrase 
tables or even syntactic transformation rules 
(Koehn et al, 2007; Galley et al, 2004). 
During the past two decades, generative word 
alignment models such as the IBM Models (Brown 
et al, 1993) and the HMM model (Vogel et al, 
1996) have been widely used, primarily because 
they are trained on bilingual sentences in an 
unsupervised manner and the implementation is 
freely available in the GIZA++ toolkit (Och and 
Ney, 2003). However, the word alignment quality 
of generative models is still far from satisfactory 
for SMT systems. In recent years, discriminative 
alignment models incorporating linguistically 
motivated features have become increasingly 
popular (Moore, 2005; Taskar et al, 2005; Riesa 
and Marcu, 2010; Saers et al, 2010; Riesa et al, 
2011). These models are usually trained with 
manually annotated parallel data. However, when 
moving to a new language pair, large amount of 
hand-aligned data are usually unavailable and 
expensive to create.  
A more practical way to improve large-scale 
word alignment quality is to introduce syntactic 
knowledge into a generative model and train the 
model in an unsupervised manner (Wu, 1997; 
Yamada and Knight, 2001; Lopez and Resnik, 
2005; DeNero and Klein, 2007; Pauls et al, 2010). 
In this paper, we take dependency cohesion (Fox, 
2002) into account, which assumes phrases 
dominated by disjoint dependency subtrees tend 
not to overlap after translation. Instead of treating 
dependency cohesion as a hard constraint (Lin and 
Cherry, 2003) or using it as a feature in 
discriminative models (Cherry and Lin, 2006b), we 
treat dependency cohesion as a distortion 
constraint, and integrate it into a modified HMM 
word alignment model to softly influence the 
probabilities of alignment candidates.  We also 
propose an approximate EM algorithm and an 
explicit Gibbs sampling algorithm to train the 
model in an unsupervised manner. Experiments on 
a large-scale Chinese-English translation task 
demonstrate that our model achieves 
improvements in both word alignment quality and 
machine translation quality. 
The remainder of this paper is organized as 
follows: Section 2 introduces dependency cohesion 
291
Transactions of the Association for Computational Linguistics, 1 (2013) 291?300. Action Editor: Chris Callison-Burch.
Submitted 5/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
constraint for word alignment. Section 3 presents 
our generative model for word alignment using 
dependency cohesion constraint. Section 4 
describes algorithms for parameter estimation. We 
discuss and analyze the experiments in Section 5. 
Section 6 gives the related work. Finally, we 
conclude this paper and mention future work in 
Section 7. 
2 Dependency Cohesion Constraint for 
Word Alignment 
Given a source (foreign) sentence ?1
? = ?1, ?2, ? , ?? 
and a target (English) sentence ?1
? = ?1, ?2, ? , ?? , 
the alignment ? between ?1
?
and ?1
?  is defined as a 
subset of the Cartesian product of word positions: 
? ? {(?, ?): ? = 1,? , ?; ? = 1,? , ?} 
When given the source side dependency tree ?, we 
can project dependency subtrees in ?  onto the 
target sentence through the alignment ? . 
Dependency cohesion assumes projection spans of 
disjoint subtrees tend not to overlap. Let ?(??) be 
the subtree of ? rooted at ??, we define two kinds 
of projection span for the node ??: subtree span and 
head span. The subtree span is the projection span 
of the total subtree ?(??), while the head span is 
the projection span of the node ?? itself. Following 
Fox (2002) and Lin and Cherry (2003), we 
consider two types of dependency cohesion: head-
modifier cohesion and modifier-modifier cohesion. 
Head-modifier cohesion is defined as the subtree 
span of a node does not overlap with the head span 
of its head (parent) node, while modifier-modifier 
cohesion is defined as subtree spans of two nodes 
under the same head node do not overlap each 
other. We call a situation where cohesion is not 
maintained crossing. 
Using the dependency tree in Figure 1 as an 
example, given the correct alignment ?R?, the 
subtree span of ??/have? is [8, 14] , and the head 
span of its head node ???/one of? is [3, 4]. They 
do not overlap each other, so the head-modifier 
cohesion is maintained. Similarly, the subtree span 
of ???/few? is [6, 6], and it does not overlap the 
subtree span of  ??/have?, so a modifier-modifier 
cohesion is maintained. However, when ?R? is 
replaced with the incorrect alignment ?W?, the 
subtree span of ??/have? becomes [3, 14], and it 
overlaps the head span of its head ???/one of?, 
so a head-modifier crossing occurs. Meanwhile, 
the subtree spans of the two nodes ??/have? and 
??? /few? overlap each other, so a modifier-
modifier crossing occurs.  
 
Fox (2002) showed that dependency cohesion is 
generally maintained between English and French. 
To test how well this assumption holds between 
Chinese and English, we measure the dependency 
cohesion between the two languages with a 
manually annotated bilingual Chinese-English data 
set of 502 sentence pairs 1 . We use the head-
modifier cohesion percentage (HCP) and the 
modifier-modifier cohesion percentage (MCP) to 
measure the degree of cohesion in the corpus. HCP 
(or MCP) is used for measuring how many head-
modifier (or modifier-modifier) pairs are actually 
cohesive. Table 1 lists the relative percentages in 
both Chinese-to-English (ch-en, using Chinese side 
dependency trees) and English-to-Chinese (en-ch, 
using English side dependency trees) directions. 
As we see from Table 1, dependency cohesion is 
                                                          
1 The data set is the development set used in Section 5. 
?
?
? ? ?
?
? ??
?
?
?
?
?
??
?
Australia
is
one
of
the
few
countries
that
have
diplomatic
relations
with
North
Korea
.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
 
Figure 1: A Chinese-English sentence pair 
including the word alignments and the Chinese 
side dependency tree. The Chinese and English 
words are listed horizontally and vertically, 
respectively. The black grids are gold-standard 
alignments. For the Chinese word ??/have?, 
we give two alignment positions, where ?R? is 
the correct alignment and ?W? is the incorrect 
alignment. 
292
generally maintained between Chinese and English. 
So dependency cohesion would be helpful for 
word alignment between Chinese and English. 
However, there are still a number of crossings. If 
we restrict alignment space with a hard cohesion 
constraint, the correct alignments that result in 
crossings will be ruled out directly. In the next 
section, we describe an approach to integrating 
dependency cohesion constraint into a generative 
model to softly influence the probabilities of 
alignment candidates. We show that our new 
approach addresses the shortcomings of using 
dependency cohesion as a hard constraint. 
 
3 A Generative Word Alignment Model 
with Dependency Cohesion Constraint  
The most influential generative word alignment 
models are the IBM Models 1-5 and the HMM 
model (Brown et al, 1993; Vogel et al, 1996; Och 
and Ney, 2003). These models can be classified 
into sequence-based models (IBM Models 1, 2 and 
HMM) and fertility-based models (IBM Models 3, 
4 and 5). The sequence-based model is easier to 
implement, and recent experiments have shown 
that appropriately modified sequence-based model 
can produce comparable performance with 
fertility-based models (Lopez and Resnik, 2005; 
Liang et al, 2006; DeNero and Klein, 2007; Zhao 
and Gildea, 2010; Bansal et al, 2011). So we built 
a generative word alignment model with 
dependency cohesion constraint based on the 
sequence-based model. 
3.1 The Sequence-based Alignment Model 
According to Brown et al (1993) and Och and Ney 
(2003), the sequence-based model is built as a 
noisy channel model, where the source sentence ?1
? 
and the alignment ?1
? are generated conditioning on 
the target sentence ?1
? . The model assumes each 
source word is assigned to exactly one target word, 
and defines an asymmetric alignment for the 
sentence pair as ?1
? = ?1, ?2, ? , ?? , ? , ??, where each 
?? ? [0, ?] is an alignment from the source position j 
to the target position ?? , ?? = 0  means ??  is not 
aligned with any target words. The sequence-based 
model divides alignment procedure into two stages 
(distortion and translation) and factors as: 
?(?1
?, ?1
?|?1
? ) = ? ??(??|???1, ?)??(??|???)
?
?=1       (1) 
where ??  is the distortion model and ??  is the 
translation model. IBM Models 1, 2 and the HMM 
model all assume the same translation model 
 ??(??|???) . However, they use three different 
distortion models. IBM Model 1 assumes a 
uniform distortion probability 1/(I+1), IBM Model 
2 assumes ??(??|?) that depends on word position j 
and HMM model assumes ??(??|???1, ?)  that 
depends on the previous alignment ???1. Recently, 
tree distance models (Lopez and Resnik, 2005; 
DeNero and Klein, 2007) formulate the distortion 
model as ??(??|???1, ?) , where the distance 
between ??  and ???1  are calculated by walking 
through the phrase (or dependency) tree T. 
3.2 Proposed Model 
To integrate dependency cohesion constraint into a 
generative model, we refine the sequence-based 
model in two ways with the help of the source side 
dependency tree ??.  
First, we design a new word alignment order. In 
the sequence-based model, source words are 
aligned from left to right by taking source sentence 
as a linear sequence. However, to apply 
dependency cohesion constraint, the subtree span 
of a head node is computed based on the 
alignments of its children, so children must be 
aligned before the head node. Riesa and Marcu 
(2010) propose a hierarchical search procedure to 
traverse all nodes in a phrase structure tree. 
Similarly, we define a bottom-up topological order 
(BUT-order) to traverse all words in the source 
side dependency tree ?? . In the BUT-order, tree 
nodes are aligned bottom-up with ?? as a backbone. 
For all children under the same head node, left 
children are aligned from right to left, and then 
right children are aligned from left to right. For 
example, the BUT-order for the following 
dependency tree is  ?C B E F D A H G?.  
 
A HGFEDCB
 
ch-en en-ch 
HCP MCP HCP MCP 
88.43 95.82 81.53 91.62 
Table 1: Cohesion percentages (%) of a manually 
annotated data set between Chinese and English. 
293
For the sake of clarity, we define a function to 
map all nodes in ??  into their BUT-order, and 
notate it as BUT(??) = ?1, ?2, ? , ?? , ? , ?? , where ?? 
means the j-th node in BUT-order is the ??-th word 
in the original source sentence. We arrange 
alignment sequence ?1
?  according the BUT-order 
and notate it as ?[1,?] = ??1 , ? , ??? , ? , ??? , where 
??? is the aligned position for a node ???. We also 
notate the sub-sequence ??? , ? , ???as ?[?,?]. 
Second, we keep the same translation model as 
the sequence-based model and integrate the 
dependency cohesion constraints into the distortion 
model. The main idea is to influence the distortion 
procedure with the dependency cohesion 
constraints. Assume node ??  and node ??  are a 
head-modifier pair in ??, where ?? is the head and 
??  is the modifier. The head-modifier cohesion 
relationship between them is notated as ??,? ?
{????????, ????????} . When the head-modifier 
cohesion is maintained ??,? = ????????, otherwise 
??,? = ???????? . We represent the set of head-
modifier cohesion relationships for all the head-
modifier pairs in ?? as: 
      ? = {??,? | ? ? [1, ?], ? ? [1, ?], ? ? ?,  
    ?? and ?? are a head-modifier pair in ??} 
The set of head-modifier cohesion relationships for 
all the head-modifier pairs taking ??  as the head 
node can be represented as: 
      ?? = {??,? | ? ? [1, ?], ? ? ?,   
               ?? and ?? are a head-modifier pair in ??} 
Obviously, ? = ? ??
?
?=0 . 
Similarly, we assume node ?? and node ?? are a 
modifier-modifier pair in ?? . To avoid repetition, 
we assume ??  is the node sitting at the position 
after  ??  in BUT-order and call ??  as the higher-
order node of the pair. The modifier-modifier 
cohesion relationship between them is notated as 
??,? ? {????????, ????????} . When the modifier-
modifier cohesion is maintained ??,? = ???????? , 
otherwise ??,? = ????????. We represent the set of 
modifier-modifier cohesion relationships for all the 
modifier-modifier pairs in ?? as: 
      ? = {??,? | ? ? [1, ?], ? ? [1, ?], ? ? ?,  
         ?? and ?? are a modifier-modifier pair in ??} 
The set of modifier-modifier cohesion 
relationships for all the modifier-modifier pairs 
taking ??  as the higher-order node can be 
represented as: 
      ?? = {??,? | ? ? [1, ?], ? ? ?,   
                ?? and ?? are a modifier-modifier pair in ??} 
Obviously, ? = ? ??
?
?=0 . 
With the above notations, we formulate the 
distortion probability for a node ???  as 
?? (??? , ??? ,???|?[1,??1]). 
 
According to Eq. (1) and the two improvements, 
we formulated our model as: 
?(?1
?, ?[1,?]|?1
? , ??) = ?(?[1,?], ?,?, ?1
? , |?1
? , ??) 
? ? ?? (??? , ??? ,???|?[1,??1]) ?? (???|????
)??????(??)   
   (2) 
Here, we use the approximation symbol, 
because the right hand side is not guaranteed to 
be normalized. In practice, we only compute 
ratios of these terms, so it is not actually a 
problem. Such model is called deficient (Brown 
et al, 1993), and many successful unsupervised 
models are deficient, e.g., IBM model 3 and 
IBM model 4.  
3.3 Dependency Cohesive Distortion Model 
We assume the distortion procedure is influenced 
by three factors: words distance, head-modifier 
cohesion and modifier-modifier cohesion. 
Therefore, we further decompose the distortion 
model ?? into three terms as follows: 
?? (??? , ??? ,???|?[1,??1]) 
= ? (???|?[1,??1]) ? (???|?[1,?]) ? (???|?[1,?], ???) 
? ??? (???|????1 , ?) ??? (???|?[1,?]) ??? (???|?[1,?]) 
(3) 
where ??? is the words distance term, ??? is  the 
head-modifier cohesion term and ???  is the 
modifier-modifier cohesion term. 
The word distance term ??? has been verified to 
be very useful in the HMM alignment model. 
However, in our model, the word distance is 
calculated based on the previous node in BUT-
order rather than the previous word in the original 
sentence. We follow the HMM word alignment 
model (Vogel et al, 1996) and parameterize ??? in 
terms of the jump width: 
???(?|?
?, ?) =
?(????)
? ?(??????)???
       (4) 
where ?(?) is the count of jump width. 
294
The head-modifier cohesion term ??? is used to 
penalize the distortion probability according to 
relationships between the head node and its 
children (modifiers). Therefore, we define ???  as 
the product of probabilities for all head-modifier 
pairs taking ??? as head node: 
??? (???|?[1,?]) = ? ?? (???,?|??, ????
, ???)???,?????
 (5) 
where ???,? ? {????????, ????????}  is the head-
modifier cohesion relationship between ???  and 
one of its child ?? ,  ??  is the corresponding 
probability, ????
and ???  are the aligned words for 
??? and ??.  
Similarly, the modifier-modifier cohesion term 
???  is used to penalize the distortion probability 
according to relationships between ???  and its 
siblings. Therefore, we define  ??? as the product 
of probabilities for all the modifier-modifier pairs 
taking ??? as the higher-order node: 
??? (???|?[1,?]) = ? ?? (???,?|??, ????
, ???)???,?????
  
 (6) 
where ???,? ? {????????, ????????} is the modifier-
modifier cohesion relationship between  ???  and 
one of its sibling ?? , ??  is the corresponding 
probability, ????
 and ??? are the aligned words for 
??? and ??.  
Both  ?? and ??  in Eq. (5) and Eq. (6) are 
conditioned on three words, which would make 
them very sparse. To cope with this problem, we 
use the word clustering toolkit, mkcls (Och et al, 
1999), to cluster all words into 50 classes, and 
replace the three words with their classes. 
4 Parameter Estimation 
To align sentence pairs with the model in Eq. (2), 
we have to estimate some parameters: ??, ???, ?? 
and ?? . The traditional approach for sequence-
based models uses Expectation Maximization (EM) 
algorithm to estimate parameters. However, in our 
model, it is hard to find an efficient way to sum 
over all the possible alignments, which is required 
in the E-step of EM algorithm. Therefore, we 
propose an approximate EM algorithm and a Gibbs 
sampling algorithm for parameter estimation. 
4.1 Approximate EM Algorithm 
The approximate EM algorithm is similar to the 
training algorithm for fertility-based alignment 
models (Och and Ney, 2003). The main idea is to 
enumerate only a small subset of good alignments 
in the E-step, then collect expectation counts and 
estimate parameters among the small subset in M-
step. Following with Och and Ney (2003), we 
employ neighbor alignments of the Viterbi 
alignment as the small subset. Neighbor 
alignments are obtained by performing one swap 
or move operation over the Viterbi alignment.  
Obtaining the Viterbi alignment itself is not so 
easy for our model. Therefore, we take the Viterbi 
alignment of the sequence-based model (HMM 
model) as the starting point, and iterate the hill-
climbing algorithm (Brown et al, 1993) many 
times to get the best alignment greedily. In each 
iteration, we find the best alignment with Eq. (2) 
among neighbor alignments of the initial point, and 
then make the best alignment as the initial point for 
the next iteration. The algorithm iterates until no 
update could be made. 
4.2 Gibbs Sampling Algorithm 
Gibbs sampling is another effective algorithm for 
unsupervised learning problems. As is described in 
the literatures (Johnson et al, 2007; Gao and 
Johnson, 2008), there are two types of Gibbs 
samplers: explicit and collapsed. An explicit 
sampler represents and samples the model 
parameters in addition to the word alignments, 
while in a collapsed sampler the parameters are 
integrated out and only alignments are sampled. 
Mermer and Sara?lar (2011) proposed a collapsed 
sampler for IBM Model 1. However, their sampler 
updates parameters constantly and thus cannot run 
efficiently on large-scale tasks. Instead, we take 
advantage of explicit Gibbs sampling to make a 
highly parallelizable sampler. Our Gibbs sampler 
is similar to the MCMC algorithm in Zhao and 
Gildea (2010), but we assume Dirichlet priors 
when sampling model parameters and take a 
different sampling approach based on the source 
side dependency tree. 
Our sampler performs a sequence of consecutive 
iterations. Each iteration consists of two sampling 
steps. The first step samples the aligned position 
for each dependency node according to the BUT-
order.  Concretely, when sampling the aligned 
295
position ???
(?+1)
 for node ???  on iteration ?+1,  the 
aligned positions for ?[1,??1] are fixed on the new 
sampling results ?[1,??1]
(?+1)
on iteration ?+1, and the 
aligned positions for ?[?+1,?]  are fixed on the old 
sampling results ?[?+1,?]
(?)
 on iteration ? . Therefore, 
we sample the aligned position ???
(?+1)
 as follows: 
???
(?+1)
  ~   ? (???|?[1,??1]
(?+1)
, ?[?+1,?]
(?)
, ?1
?, ?1
?)
=
? (?1
?, ??????
|?1
? )
? ? (?1
? , ??????
|?1
? )????{0,1,?,?}
 
(7) 
where ??????
= ?[1,??1]
(?+1)
? ??? ? ?[?+1,?]
(?)
, the numerator 
is the probability of aligning ???  with ????
 (the 
alignments for other nodes are fixed at ?[1,??1]
(?+1)
and 
?[?+1,?]
(?)
) calculated with Eq. (2), and the 
denominator is the summation of the probabilities 
of aligning ??? with each target word. The second 
step of our sampler calculates parameters ??, ???, 
??  and ??  using their counts, where all these 
counts can be easily collected during the first 
sampling step. Because all these parameters follow 
multinomial distributions, we consider Dirichlet 
priors for them, which would greatly simplify the 
inference procedure. 
In the first sampling step, all the sentence pairs 
are processed independently. So we can make this 
step parallel and process all the sentence pairs 
efficiently with multi-threads. When using the 
Gibbs sampler for decoding, we just ignore the 
second sampling step and iterate the first sampling 
step many times.  
5 Experiments 
We performed a series of experiments to evaluate 
our model. All the experiments are conducted on 
the Chinese-English language pair. We employ 
two training sets: FBIS and LARGE. The size and 
source corpus of these training sets are listed in 
Table 2. We will use the smaller training set FBIS 
to evaluate the characters of our model and use the 
LARGE training set to evaluate whether our model 
is adaptable for large-scale task. For word 
alignment quality evaluation, we take the hand-
aligned data sets from SSMT20072, which contains 
                                                          
2 http://nlp.ict.ac.cn/guidelines/guidelines-2007-
SSMT(English).doc 
505 sentence pairs in the testing set and 502 
sentence pairs in the development set. Following 
Och and Ney (2003), we evaluate word alignment 
quality with the alignment error rate (AER), where 
lower AER is better. 
Because our model takes dependency trees as 
input, we parse both sides of the two training sets, 
the development set and the testing set with 
Berkeley parser (Petrov et al, 2006), and then 
convert the generated phrase trees into dependency 
trees according to Wang and Zong (2010; 2011). 
Our model is an asymmetric model, so we perform 
word alignment in both forward (Chinese?English) 
and reverse (English?Chinese) directions. 
 
5.1 Effectiveness of Cohesion Constraints 
In Eq. (3), the distortion probability ??  is 
decomposed into three terms: ??? , ???  and ??? . 
To study whether cohesion constraints are effective 
for word alignment, we construct four sub-models 
as follows:  
(1) wd: ?? = ???;  
(2) wd-hc: ?? = ??? ? ???;  
(3) wd-mc: ?? = ??? ? ???; 
(4) wd-hc-mc: ?? = ??? ? ??? ? ???. 
We train these four models with the approximate 
EM and the Gibbs sampling algorithms on the 
FBIS training set. For approximate EM algorithm, 
we first train a HMM model (with 5 iterations of 
IBM model 1 and 5 iterations of HMM model), 
then train these four sub-models with 10 iterations 
of the approximate EM algorithm. For Gibbs 
sampling, we choose symmetric Dirichlet priors 
identically with all hyper-parameters equals 0.0001 
to obtain a sparse Dirichlet prior. Then, we make 
the alignments produced by the HMM model as the 
initial points, and train these sub-models with 20 
iterations of the Gibbs sampling.  
AERs on the development set are listed in Table 
3. We can easily find: 1) when employing the 
head-modifier cohesion constraint, the wd-hc 
model yields better AERs than the wd model; 2) 
Train Set Source Corpus # Words 
FBIS FBIS newswire data Ch: 7.1M 
En: 9.1M 
 
LARGE 
LDC2000T50, LDC2003E14, 
LDC2003E07, LDC2004T07, 
LDC2005T06, LDC2002L27, 
LDC2005T10, LDC2005T34 
 
Ch: 27.6M 
En: 31.8M 
Table 2: The size and the source corpus of the two 
training sets. 
296
when employing the modifier-modifier cohesion 
constraint, the wd-mc model also yields better 
AERs than the wd model; and 3) when employing 
both head-modifier cohesion constraint and 
modifier-modifier cohesion constraint together, the 
wd-hc-mc model yields the best AERs among the 
four sub-models. So both head-modifier cohesion 
constraint and modifier-modifier cohesion 
constraint are helpful for word alignment. Table 3 
also shows that the approximate EM algorithm 
yields better AERs in the forward direction than 
reverse direction, while the Gibbs sampling 
algorithm yields close AERs in both directions. 
 
5.2 Comparison with State-of-the-Art Models 
To show the effectiveness of our model, we 
compare our model with some of the state-of-the-
art models. All the systems are listed as follows: 
1) IBM4: The fertility-based model (IBM model 4) 
which is implemented in GIZA++ toolkit. The 
training scheme is 5 iterations of IBM model 1, 
5 iterations of the HMM model and 10 
iterations of IBM model 4. 
2) IBM4-L0: A modification to the GIZA++ 
toolkit which extends IBM models with ?0 -
norm (Vaswani et al, 2012). The training 
scheme is the same as IBM4. 
3) IBM4-Prior: A modification to the GIZA++ 
toolkit which extends the translation model of 
IBM models with Dirichlet priors (Riley and 
Gildea, 2012). The training scheme is the same 
as IBM4. 
4) Agree-HMM: The HMM alignment model by 
jointly training the forward and reverse models 
(Liang et al, 2006), which is implemented in 
the BerkeleyAligner. The training scheme is 5 
iterations of jointly training IBM model 1 and 5 
iterations of jointly training HMM model. 
5) Tree-Distance: The tree distance alignment 
model proposed in DeNero and Klein (2007), 
which is implemented in the BerkeleyAligner. 
The training scheme is 5 iterations of jointly 
training IBM model 1 and 5 iterations of jointly 
training the tree distance model. 
6) Hard-Cohesion: The implemented ?Cohesion 
Checking Algorithm? (Lin and Cherry, 2003) 
which takes dependency cohesion as a hard 
constraint during beam search word alignment 
decoding. We use the model trained by the 
Agree-HMM system to estimate alignment 
candidates.  
We also build two systems for our soft 
dependency cohesion model: 
7) Soft-Cohesion-EM: the wd-hc-mc sub-model 
trained with the approximate EM algorithm as 
described in sub-section 5.1. 
8) Soft-Cohesion-Gibbs: the wd-hc-mc sub-model 
trained with the Gibbs sampling algorithm as 
described in sub-section 5.1. 
We train all these systems on the FBIS training 
set, and test them on the testing set. We also 
combine the forward and reverse alignments with 
the grow-diag-final-and (GDFA) heuristic (Koehn 
et al, 2007). All AERs are listed in Table 4. We 
find our soft cohesion systems produce better 
AERs than the Hard-Cohesion system as well as 
the other systems. Table 5 gives the head-modifier 
cohesion percentage (HCP) and the modifier-
modifier cohesion percentage (MCP) of each 
system. We find HCPs and MCPs of our soft 
cohesion systems are much closer to the gold-
standard alignments.  
 
To evaluate whether our model is adaptable for 
large-scale task, we retrained these systems using 
the LARGE training set. AERs on the testing set 
are listed in Table3 6. Compared with Table 4, we 
                                                          
3 Tree-Distance system requires too much memory to run on 
our server when using the LARGE data set, so we can?t get the 
result. 
  forward reverse GDFA 
IBM4 42.90 42.81 44.32 
IBM4-L0 42.59 41.04 43.19 
IBM4-Prior 41.94 40.46 42.44 
Agree-HMM 38.03 37.91 41.01 
Tree-Distance 34.21 37.22 38.42 
Hard-Cohesion 37.32 38.92 38.92 
Soft-Cohesion-EM 33.65 34.74 35.85 
Soft-Cohesion-Gibbs 34.45 33.72 34.46 
Table 4: AERs on the testing set (trained on the 
FBIS data set). 
  
EM Gibbs 
forward reverse forward reverse 
wd 26.12  28.66  27.09  26.40  
wd-hc 24.67  25.86  26.24  24.39  
wd-mc 24.49  26.53  25.51  25.40  
wd-hc-mc 23.63  25.17  24.65  24.33  
Table 3: AERs on the development set (trained 
on the FBIS data set). 
297
find all the systems yield better performance when 
using more training data. Our soft cohesion 
systems still produce better AERs than other 
systems, suggesting that our soft cohesion model is 
very effective for large-scale word alignment tasks. 
 
 
5.3 Machine Translation Quality Comparison 
We then evaluate the effect of word alignment on 
machine translation quality using the phrase-based 
translation system Moses (Koehn et al, 2007). We 
take NIST MT03 test data as the development set, 
NIST MT05 test data as the testing set. We train a 
5-gram language model with the Xinhua portion of 
English Gigaword corpus and the English side of 
the training set using the SRILM Toolkit (Stolcke, 
2002).  
We train machine translation models using 
GDFA alignments of each system. BLEU scores 
on NIST MT05 are listed in Table 7, where BLEU 
scores are calculated using lowercased and 
tokenized data (Papineni et al, 2002). Although 
the IBM4-L0, Agree-HMM, Tree-Distance and 
Hard-Cohesion systems improve word alignment 
than IBM4, they fail to outperform the IBM4 
system on machine translation. The BLEU score of 
our Soft-Cohesion-EM system is better than the 
IBM4 system when using the FBIS training set, but 
worse when using the LARGE training set. Our 
Soft-Cohesion-Gibbs system produces the best 
BLEU score when using both training sets. We 
also performed a statistical significance test using 
bootstrap resampling with 1000 samples (Koehn, 
2004; Zhang et al, 2004). Experimental results 
show the Soft-Cohesion-Gibbs system is 
significantly better (p<0.05) than the IBM4 system. 
The IBM4-Prior system slightly outperforms IBM4, 
but it?s not significant. 
 
6 Related Work 
There have been many proposals of integrating 
syntactic knowledge into generative alignment 
models. Wu (1997) proposed the inversion 
transduction grammar (ITG) to model word 
alignment as synchronous parsing for a sentence 
pair. Yamada and Knight (2001) represented 
translation as a sequence of re-ordering operations 
over child nodes of a syntactic tree. Gildea (2003) 
introduced a ?loosely? tree-based alignment 
technique, which allows alignments to violate 
syntactic constraints by incurring a cost in 
probability. Pauls et al (2010) gave a new instance 
of the ITG formalism, in which one side of the 
synchronous derivation is constrained by the 
syntactic tree. 
Fox (2002) measured syntactic cohesion in gold 
standard alignments and showed syntactic 
cohesion is generally maintained between English 
and French. She also compared three variant 
syntactic representations (phrase tree, verb phrase 
flattening tree and dependency tree), and found the 
dependency tree produced the highest degree of 
cohesion. So Cherry and Lin (2003; 2006a) used 
dependency cohesion as a hard constraint to 
restrict the alignment space, where all potential 
alignments violating cohesion constraint are ruled 
  
forward reverse 
HCP MCP HCP MCP 
IBM4 60.53 63.94 56.15 64.80 
IBM4-L0 60.57 62.53 66.49 65.68 
IBM4-Prior 66.48 74.65 67.19 72.32 
Agree-HMM 75.52 66.61 73.88 66.07 
Tree-Distance 81.37 74.69 78.00 71.73 
Hard-Cohesion 98.70 97.43 98.25 97.84 
Soft-Cohesion-EM 85.21 81.96 82.96 81.36 
Soft-Cohesion-Gibbs 88.74 85.55 87.81 84.83 
gold-standard 88.43 95.82 81.53 91.62 
Table 5: HCPs and MCPs on the development 
set. 
  FBIS LARGE 
IBM4 30.7 33.1 
IBM4-L0 30.4 32.3 
IBM4-Prior 30.9 33.2 
Agree-HMM 27.2 30.1 
Tree-Distance 28.2 N/A 
Hard-Cohesion 30.4 32.2 
Soft-Cohesion-EM 30.9 33.1 
Soft-Cohesion-Gibbs   31.6*   33.9* 
Table 7: BLEU scores, where * indicates 
significantly better than IBM4 (p<0.05). 
  forward reverse GDFA 
IBM4 37.45 39.18 40.52 
IBM4-L0 38.17 38.88 39.82 
IBM4-Prior 35.86 36.71 37.08 
Agree-HMM 35.58 35.73 39.10 
Hard-Cohesion 35.04 37.59 37.63 
Soft-Cohesion-EM 30.93 32.67 33.65 
Soft-Cohesion-Gibbs 32.07 32.68 32.28 
Table 6: AERs on the testing set (trained on the 
LARGE data set). 
298
out directly. Although the alignment quality is 
improved, they ignored situations where a small set 
of correct alignments can violate cohesion. To 
address this limitation, Cherry and Lin (2006b) 
proposed a soft constraint approach, which took 
dependency cohesion as a feature of a 
discriminative model, and verified that the soft 
constraint works better than the hard constraint. 
However, the training procedure is very time-
consuming, and they trained the model with only 
100 hand-annotated sentence pairs. Therefore, their 
method is not suitable for large-scale tasks. In this 
paper, we also use dependency cohesion as a soft 
constraint. But, unlike Cherry and Lin (2006b), we 
integrate the soft dependency cohesion constraint 
into a generative model that is more suitable for 
large-scale word alignment tasks. 
7 Conclusion and Future Work  
We described a generative model for word 
alignment that uses dependency cohesion as a soft 
constraint. We proposed an approximate EM 
algorithm and an explicit Gibbs sampling 
algorithm for parameter estimation in an 
unsupervised manner. Experimental results 
performed on a large-scale data set show that our 
model improves word alignment quality as well as 
machine translation quality. Our experimental 
results also indicate that the soft constraint 
approach is much better than the hard constraint 
approach.  
It is possible that our word alignment model can 
be improved further. First, we generated word 
alignments in both forward and reverse directions 
separately, but it might be helpful to use 
dependency trees of the two sides simultaneously. 
Second, we only used the one-best automatically 
generated dependency trees in the model. However, 
errors are inevitable in those trees, so we will 
investigate how to use N-best dependency trees or 
dependency forests (Hayashi et al, 2011) to see if 
they can improve our model. 
Acknowledgments 
We would like to thank Nianwen Xue for 
insightful discussions on writing this article. We 
are grateful to anonymous reviewers for many 
helpful suggestions that helped improve the final 
version of this article. The research work has been 
funded by the Hi-Tech Research and Development 
Program ("863" Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. This work is also supported in 
part by the DAPRA via contract HR0011-11-C-
0145 entitled "Linguistic Resources for 
Multilingual Processing".  
 
References  
Mohit Bansal, Chris Quirk, and Robert Moore, 2011. 
Gappy Phrasal Alignment By Agreement. In Proc. of 
ACL 2011. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer, 1993. The 
mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 19 
(2). pages 263-311. 
C. Cherry and D. Lin, 2003. A probability model to 
improve word alignment. In Proc. of ACL '03, pages 
88-95. 
C. Cherry and D. Lin, 2006a. A comparison of 
syntactically motivated word alignment spaces. In 
Proc. of EACL '06, pages 145-152. 
C. Cherry and D. Lin, 2006b. Soft syntactic constraints 
for word alignment through discriminative training. 
In Proc. of COLING/ACL '06, pages 105-112. 
John DeNero and Dan Klein, 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL '07, pages 17. 
C. Dyer, J. Clark, A. Lavie and N.A. Smith, 2011. 
Unsupervised word alignment with arbitrary features. 
In Proc. of ACL '11, pages 409-419. 
Heidi J. Fox, 2002. Phrasal cohesion and statistical 
machine translation. In Proc. of EMNLP '02, pages 
304-3111. 
Michel Galley, Mark Hopkins, Kevin Knight, Daniel 
Marcu, 2004. What's in a translation rule? In Proc. of 
NAACL '04, pages 344-352. 
J. Gao and M. Johnson, 2008. A comparison of 
Bayesian estimators for unsupervised Hidden 
Markov Model POS taggers. In Proc. of EMNLP '08, 
pages 344-352. 
Daniel Gildea, 2003. Loosely Tree-Based Alignment for 
Machine Translation. In Proc. of ACL'03, pages 80-
87. 
299
K. Hayashi, T. Watanabe, M. Asahara and Y. 
Matsumoto, 2011. Third-order Variational Reranking 
on Packed-Shared Dependency Forests. In Proc. of 
EMNLP '11. 
M. Johnson, T. Griffiths and S. Goldwater, 2007. 
Bayesian inference for PCFGs via Markov chain 
Monte Carlo. In Proc. of NAACL '07, pages 139-146. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of 
EMNLP'04. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran 
and R. Zens, 2007. Moses: Open source toolkit for 
statistical machine translation. In Proc. of ACL '07, 
Demonstration Session, pages 177-180. 
Percy Liang, Ben Taskar and Dan Klein, 2006. 
Alignment by agreement. In Proc. of HLT-NAACL 
06, pages 104-111. 
D. Lin and C. Cherry, 2003. Word alignment with 
cohesion constraint. In Proc. of NAACL '03, pages 
49-51. 
Adam Lopez and Philip Resnik, 2005. Improved HMM 
alignment models for languages with scarce 
resources. In ACL Workshop on Building and Using 
Parallel Texts '05, pages 83-86. 
Cos k?un Mermer and Murat Sara?lar, 2011. Bayesian 
word alignment for statistical machine translation. In 
Proc. of ACL '11, pages 182-187. 
R.C. Moore, 2005. A discriminative framework for 
bilingual word alignment. In Proc. of EMNLP '05, 
pages 81-88. 
F.J. Och, C. Tillmann and H. Ney, 1999. Improved 
alignment models for statistical machine translation. 
In Proc. of EMNLP/WVLC '99, pages 20-28. 
Franz Josef Och and Hermann Ney, 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29 (1). pages 19-51. 
K. Papineni, S. Roukos, T. Ward and W.J. Zhu, 2002. 
BLEU: a method for automatic evaluation of 
machine translation. In Proc. of ACL '02, pages 311-
318. 
Adam Pauls, Dan Klein, David Chiang and Kevin 
Knight, 2010. Unsupervised Syntactic Alignment 
with Inversion Transduction Grammars. In Proc. of 
NAACL '10. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of ACL 2006. 
Jason Riesa and Daniel Marcu, 2010. Hierarchical 
search for word alignment. In Proc. of ACL '10, 
pages 157-166. 
Jason Riesa, Ann Irvine and Daniel Marcu, 2011. 
Feature-Rich Language-Independent Syntax-Based 
Alignment for Statistical Machine Translation. In 
Proc. of EMNLP '11. 
Darcey Riley and Daniel Gildea, 2012. Improving the 
IBM Alignment Models Using Variational Bayes. In 
Proc. of ACL '12. 
M. Saers, J. Nivre and D. Wu, 2010. Word alignment 
with stochastic bracketing linear inversion 
transduction grammar. In Proc. of NAACL '10, pages 
341-344. 
A. Stolcke, 2002. SRILM-an extensible language 
modeling toolkit. In ICSLP '02. 
B. Taskar, S. Lacoste-Julien and D. Klein, 2005. A 
discriminative matching approach to word alignment. 
In Proc. of EMNLP '05, pages 73-80. 
Ashish Vaswani, Liang Huang, and David Chiang, 2012. 
Smaller alignment models for better translations: 
unsupervised word alignment with the l0 norm. In 
Proc. ACL'12, pages 311?319. 
Stephan Vogel, Hermann Ney and Christoph Tillmann, 
1996. HMM-based word alignment in statistical 
translation. In Proc. of COLING-96, pages 836-841. 
D. Wu, 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23 (3). pages 377-403. 
Zhiguo Wang, Chengqing Zong, 2010. Phrase Structure 
Parsing with Dependency Structure, In Proc. of 
COLING 2010, pages 1292-1300. 
Zhiguo Wang, Chengqing Zong, 2011. Parse Reranking 
Based on Higher-Order Lexical Dependencies, In 
Proc. Of IJCNLP 2011, pages 1251-1259. 
Kenji Yamada and Kevin Knight, 2001. A syntax-based 
statistical translation model. In Proc. of ACL '01, 
pages 523-530. 
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. 
Interpreting BLEU/NIST scores: How much 
improvement do we need to have a better system? In 
Proc. of LREC. 
Shaojun Zhao and Daniel Gildea, 2010. A fast fertility 
hidden Markov model for word alignment using 
MCMC. In Proc. of EMNLP '10, pages 596-605. 
 
300
 A Character-Based Joint Model 
for CIPS-SIGHAN Word Segmentation Bakeoff 2010 
Kun Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn 
Keh-Yih Su 
Behavior Design Corporation
 
kysu@bdc.com.tw 
 
Abstract 
This paper presents a Chinese Word 
Segmentation system for the closed track 
of CIPS-SIGHAN Word Segmentation 
Bakeoff 2010. This system adopts a 
character-based joint approach, which 
combines a character-based generative 
model and a character-based discrimina-
tive model. To further improve the cross-
domain performance, we use an addi-
tional semi-supervised learning proce-
dure to incorporate the unlabeled corpus. 
The final performance on the closed 
track for the simplified-character text 
shows that our system achieves compa-
rable results with other state-of-the-art 
systems. 
1 Introduction 
The character-based tagging approach (Xue, 
2003) has become the dominant technique for 
Chinese word segmentation (CWS) as it can tol-
erate out-of-vocabulary (OOV) words. In the last 
few years, this method has been widely adopted 
and further improved in many previous works 
(Tseng et al, 2005; Zhang et al, 2006; Jiang et 
al., 2008). Among various character-based tag-
ging approaches, the character-based joint model 
(Wang et al, 2010) achieves a good balance be-
tween in-vocabulary (IV) words recognition and 
OOV words identification. 
In this work, we adopt the character-based 
joint model as our basic system, which combines 
a character-based discriminative model and a 
character-based generative model. The genera-
tive module holds a robust performance on IV 
words, while the discriminative module can 
handle the extra features easily and enhance the 
OOV words segmentation. However, the per-
formance of out-of-domain text is still not satis-
factory as that of in-domain text, while few pre-
vious works have paid attention to this problem. 
To further improve the performance of the ba-
sic system in out-of-domain text, we use a semi-
supervised learning procedure to incorporate the 
unlabeled corpora of Literature (Unlabeled-A) 
and Computer (Unlabeled-B). The final results 
show that our system performs well on all four 
testing-sets and achieves comparable segmenta-
tion results with other participants. 
2 Our system 
2.1 Character-Based Joint Model 
The character-based joint model in our system 
contains two basic components:  
? The character-based discriminative model.  
? The character-based generative model. 
The character-based discriminative model 
(Xue, 2003) is based on a Maximum Entropy 
(ME) framework (Ratnaparkhi, 1998) and can be 
formulated as follows: 
2
1 1 1 2
1
( ) ( ,
n
n n k
k k k
k
P t c P t t c +? ?
=
?? )  (1) 
Where tk is a member of {Begin, Middle, End, 
Single} (abbreviated as B, M, E and S from now 
on) to indicate the corresponding position of 
character ck in its associated word. For example, 
the word ???? (Beijing City)? will be as-
signed with the corresponding tags as: ?? /B 
(North) ?/M (Capital) ?/E (City)?.  
This discriminative module can flexibly in-
corporate extra features and it is implemented 
with the ME package1 given by Zhang Le. All 
training experiments are done with Gaussian 
prior 1.0 and 200 iterations. 
The character-based generative module is a 
character-tag-pair-based trigram model (Wang et 
al., 2009) and can be expressed as below: 
1
1
1
([ , ] ) ([ , ] [ , ] ).
n
n
i i
i
P c t P c t c t ??
=
?? 2i  (2) 
In our experiments, SRI Language Modeling  
Toolkit2 (Stolcke, 2002) is used to train the gen-
erative trigram model with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). 
The character-based joint model combines the 
above discriminative module and the generative 
module with log-linear interpolation as follows: 
1
2
2
1 2
( ) log( ([ , ] [ , ] ))
(1 ) log( ( , ))
k
k k
k
k k k
Score t P c t c t
P t t c
?
?
?
?
+
? ?
= ?
+ ? ?
k
 (3) 
Where the parameter (0.0 1.0)? ?? ?  is the 
weight for the generative model. Score(tk) will 
be directly used during searching the best se-
quence. We set an empirical value ( 0.3? = ) to 
this model as there is no development-set for 
various domains. 
2.2 Features 
In this work, the feature templates adopted in the 
character-based discriminative model are very 
simple and are listed below: 
1
1 1
2 1 0 1
( ) ( 2, 1,0,12);
( ) ( 2, 1,0,1);
( ) ;
( ) ( ) ( ) ( ) ( ) ( )
n
n n
a C n
b C C n
c C C
d T C T C T C T C T C
+
?
? ?
= ? ?
= ? ?
2
 
In the above templates, Cn represents a char-
acter and the index n indicates the position. For 
example, when we consider the third character 
??? in the sequence ???????, template (a) 
results in the features as following: C-2=?, C-1=
?, C0=?, C1=?, C2=?, and template (b) gen-
erates the features as: C-2C-1=??, C-1C0=??, 
                                                 
1 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
2 http://www.speech.sri.com/projects/srilm/ 
C0C1=??, C1C2=??, and template (c) gives 
the feature C-1C1=??.  
Template (d) is the feature of character type. 
Five types classes are defined: dates (???, ???, 
???, the Chinese character for ?year?, ?month? 
and ?day? respectively) represents class 0; for-
eign alphabets represent class 1; Arabic and 
Chinese numbers represent class 2; punctuation 
represents class 3 and other characters represent 
class 4. For example, when we consider the 
character ??? in the sequence ?????Q?, 
the feature T C  will 
be set to ?20341?. 
2 1 0 1 2( ) ( ) ( ) ( ) ( )T C T C T C T C? ?
When training the character-based discrimina-
tive module, we convert all the binary features 
into real-value features, and set the real-value of 
C0 to be 2.0, the value of C-1C0 and C0C1 to be 
3.0, and the values of all other features to be 1.0. 
This method sounds a little strange because it is 
equal to duplicate some features for the maxi-
mum entropy training. However, it effectively 
improves the performance in our previous works. 
2.3 Restrictions in constructing lattice 
As the closed track allows the participants to use 
the character type information, we add some re-
strictions to our system when constructing the 
character-tag lattice. When we consider a char-
acter in the sequence, the type information of 
both the previous and the next character would 
be taken into account. The restrictions are list as 
follows: 
z If the previous, the current and the next 
characters are all English or numbers, we 
would fix the current tag to be ?M?; 
z If the previous and the next characters are 
both English or numbers, while the current 
character is a connective symbol such as ?-?, 
?/?, ?_?, ?\? etc., we would also fix the cur-
rent tag to be ?M?; 
z Otherwise, all four tags {B, E, M, S} would 
be given to the current character. 
It is shown that in the Computer domain these 
simple restrictions not only greatly reduce the 
number of words segmented, but also speed up 
the system. 
Domain Mark OOV Rate R P F1 ROOV RIV 
Literature A 0.069 0.937 0.937 0.937 0.652 0.958 
Computer B 0.152 0.941 0.940 0.940 0.757 0.974 
Medicine C 0.110 0.930 0.917 0.923 0.674 0.961 
Finance D 0.087 0.957 0.956 0.957 0.813 0.971 
Table 1: Official segmentation results of our system. 
Algorithm 1: Semi-Supervised Learning 
Given: 
z Labeled training corpus: L 
z Unlabeled training corpus: U  
1: Use L to train a segmenter S ;  0
2: Use S  to segment the unlabeled corpus U  
and then get labeled corpus U ; 
0
0
3: for i  to K  do = 1
4: Add U  to L and get a new corpus Li;i?1
Use Li to train a new segmenter Si; 5: 
6: Use Si to segment the unlabeled corpus 
 and then get labeled corpus Ui; U
7:     if convergence criterion meets 
8:          break 
8: end for 
Output: the last segmenter S  K
 
2.4 Semi-Supervised Learning 
In the last decade, Chinese word segmentation 
has been improved significantly and gets a high 
precision rate in performance. However, the per-
formance for out-of-domain text is still unsatis-
factory at the present. Also, few works have paid 
attention to the cross-domain problem in Chi-
nese word segmentation task so far. 
Self-training and Co-training are two simple 
semi-supervised learning methods to incorporate 
unlabeled corpus (Zhu, 2006). In this work, we 
use an iterative self-training method to incorpo-
rate the unlabeled data. A segmenter is first 
trained with the labeled corpus. Then this seg-
menter is used to segment the unlabeled data. 
Then the predicted data is added to the original 
training corpus as a new training-set. The seg-
menter will be re-trained and the procedure re-
peated. To simplify the task, we fix the weight  
0.3? =  for the generative module of our joint 
model in the training iterations. The procedure is 
shown in Algorithm 1. The iterations will not be 
ended until the similarity of two segmentation 
results Ui?1 and Ui reach a certain level. Here we 
used F-score to measure the similarity between 
?1 and Ui: treat Ui?1 as the benchmark, Ui as a 
testing-set. From our observation, this method 
converges quickly in only 3 or 4 iterations for 
both Literature and Computer corpora. 
Ui
3 Experiments and Discussion 
3.1 Results 
In this CIPS-SIGHAN bakeoff, we only partici-
pate the closed track for simplified-character text. 
There are two kinds of training corpora:  
z Labeled corpus from News Domain 
z Unlabeled corpora from Literature Do-
main (Unlabeled-A) and Computer Do-
main (Unlabeled-B). 
Also, the testing corpus covers four domains: 
Literature (Testing-A), Computer (Testing-B), 
Medicine (Testing-C) and Finance (Testing-D). 
As there are only two unlabeled corpora for 
Domain A and B, we thus adopt different strate-
gies for each testing-set: 
z Testing-A: Character-Based Joint Model 
with semi-supervised learning, training 
on Labeled corpus and Unlabeled-A; 
z Testing-B: Character-Based Joint Model 
with semi-supervised learning, training 
on Labeled corpus and Unlabeled-B; 
z Testing-C and D: Character-Based Joint 
Model, training on Labeled corpus; 
Table 1 shows that our system achieves F-
scores for various testing-sets: 0.937 (A), 0.940 
(B), 0.923 (C) and 0.957 (D), which are compa-
rable with other systems. Among those four test-
ing domains, our system performs unsatisfactor-
ily on Testing-C (Medicine) even the OOV rate 
of this domain is not the highest. There are pos-
sible reasons for this result: (1) Semi-supervised 
learning is not conducted for this domain; (2) the 
statistical property between News and Medicine 
are significantly different. 
Domain Model F1 ROOV 
J + R + S 0.937 0.652 
J + S 0.937 0.646 
J + R 0.936 0.646 
A 
J 0.936 0.642 
J + R + S 0.940 0.757 
J + S 0.931 0.721 
J + R 0.938 0.744 
B 
J 0.927 0.699 
J + R 0.923 0.674 C 
J 0.923 0.674 
J + R 0.957 0.813 
D 
J 0.954 0.786 
Table 2: Performance of various approaches 
J: Baseline, the character-based joint model 
R: Adding restrictions in constructing lattice 
S: Conduct Semi-Supervised Learning 
 
3.2 Discussion 
The aim of restrictions in constructing lattice is 
to improve the performance of English and nu-
merical expressions, both of which appear fre-
quently in Computer and Finance domain. 
Therefore, the improvements gained from these 
restrictions are significantly in these two do-
mains (as shown in Table 2). 
Besides, the adopted semi-supervised learning 
procedure improves the performance in Domain 
A and B., but the improvement is not significant. 
Semi-supervised learning aims to incorporate 
large amounts of unlabeled data. However, the 
size of unlabeled corpora provided here is too 
small. The semi-supervised learning procedure is 
expected to be more effective if a large amount 
of unlabeled data is available. 
4 Conclusion 
Our system is based on a character-based joint 
model, which combines a generative module and 
a discriminative module. In addition, we applied 
a semi-supervised learning method to the base-
line approach to incorporate the unlabeled cor-
pus. Our system achieves comparable perform-
ance with other participants. However, cross-
domain performance is still not satisfactory and 
further study is needed. 
Acknowledgement 
The research work has been partially funded by  
the Natural Science Foundation of China under 
Grant No. 60975053, 90820303 and 60736014, 
the National Key Technology R&D Program 
under Grant No. 2006BAH03B02, and also the 
Hi-Tech Research and Development Program 
(?863? Program) of China under Grant No. 
2006AA010108-4 as well. 
References 
Stanley F. Chen and Joshua Goodman, 1998. An em-
pirical study of smoothing techniques for language 
modeling. Technical Report TR-10-98, Harvard 
University Center for Research in Computing 
Technology. 
Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu, 
2008. A Cascaded Linear Model for Joint Chinese 
Word Segmentation and Part-of-Speech Tagging. 
In Proceedings of ACL, pages 897-904. 
Adwait Ratnaparkhi, 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Uni-
versity of Pennsylvania. 
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Proc-
essing, pages 311-318. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. A 
Conditional Random Field Word Segmenter for 
Sighan Bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Process-
ing, pages 168-171. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2009. 
Which is more suitable for Chinese word segmen-
tation, the generative model or the discriminative 
one? In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computa-
tion (PACLIC23), pages 827-834. 
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010. 
A Character-Based Joint Model for Chinese Word 
Segmentation. To appear in COLING 2010. 
Nianwen Xue, 2003. Chinese Word Segmentation as 
Character Tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Ruiqiang Zhang, Genichiro Kikui and Eiichiro 
Sumita, 2006. Subword-based Tagging for Confi-
dence-dependent Chinese Word Segmentation. In 
Proceedings of the COLING/ACL, pages 961-968. 
Xiaojin Zhu, 2006. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison. 
Treebank Conversion based Self-training Strategy for Parsing 
Zhiguo Wang and Chengqing Zong
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{zgwang, cqzong}@nlpr.ia.ac.cn 
Abstract
In this paper, we propose a novel self-
training strategy for parsing which is 
based on Treebank conversion (SSPTC). 
In SSPTC, we make full use of the 
strong points of Treebank conversion 
and self-training, and offset their 
weaknesses with each other. To provide 
good parse selection strategies which are 
needed in self-training, we score the 
automatically generated parse trees with 
parse trees in source Treebank as a 
reference. To maintain the constituency 
between source Treebank and conversion 
Treebank which is needed in Treebank 
conversion, we get the conversion trees 
with the help of self-training. In our 
experiments, SSPTC strategy is utilized 
to parse Tsinghua Chinese Treebank 
with the help of Penn Chinese Treebank. 
The results significantly outperform the 
baseline parser. 
1 Introduction 
Syntax parsing is one of the most fundamental 
tasks in natural language processing (NLP) and 
has attracted extensive attention during the past 
few decades. In statistical area, according to the 
type of data used in training stage, the parsing 
approaches can be classified into three 
categories: supervised, semi-supervised and 
unsupervised. In supervised parsing approach, a 
high-performance parser can be built when given 
sufficient labeled data (Charniak, 2000; Collins, 
2003; Henderson, 2004). The semi-supervised 
approach utilizes some labeled data to annotate 
unlabeled data, then uses the annotated data to 
improve original model, e.g., self-training 
(McClosky et al, 2006) and co-training (Hwa et 
al., 2003). In unsupervised parsing, the labeled 
data was not employed and all annotations and 
grammars are discovered automatically from 
unlabeled data. 
State-of-the-art supervised parsers (Charniak, 
2000; Collins, 2003; Henderson, 2004) require 
large amounts of manually annotated training 
data, such as the Penn Treebank (Marcus et al, 
1993), to achieve high performance. However, it 
is quite costly and time-consuming to create 
high quality labeled data. So it becomes a key 
bottleneck for supervised approach to acquire 
sufficient labeled training data. Self-training is 
an effective strategy to overcome this shortage. 
It tries to enlarge the training set with 
automatically annotated unlabeled data and 
trains a parser with the enlarged training set.  
During the last few decades, many Treebanks 
annotated with different grammar formalisms 
are released (Zhou, 2004; Xue et al, 2005). 
Although they are annotated with different 
schemes, they have some linguistic consistency 
in some extent. Intuitively, we can convert 
Treebank annotated with one grammar 
formalisms into another Treebank annotated 
with grammar formalism that we are interested 
in. For simplicity, we call the first source 
Treebank, and the second target Treebank. And 
we call this strategy as Treebank conversion. 
Although both self-training and Treebank 
conversion can overcome the limitation of 
labeled data shortage for supervised parsing in 
some extent, they all have drawbacks. For self-
training, the quality of automatically annotated 
unlabeled data will affect the performance of 
semi-supervised parsers highly. For example, 
McClosky et al (2006) shows that when the 
parser-best list is used for self-training, the 
parsing performance isn?t improved, but after 
using reranker-best list, the retrained parser 
achieves an absolute 1.1% improvement. For 
Treebank conversion, different types among 
Treebanks make the converting procedure very 
complicated, and it is very hard to get a 
conversion Treebank constituent with target 
Treebank.
To overcome the limitations mentioned above, 
we propose a Treebank conversion based self-
training strategy for parsing, which tries to 
combine self-training and Treebank conversion 
together.
Remainder of this paper is organized as 
follows. In Section 2, we introduce some related 
work. Section 3 describes details of our SSPTC 
strategy. In Section 4, we propose a head finding 
method for Task21 in CLP2010. The 
experiments and analysis is given in Section 5. 
The last section draws conclusions and describes 
the future work. 
2 Related Work 
With the development of statistical parsing 
approaches, large scale corpus has become an 
indispensable resource. Because of the limited 
amount of existing labeled training data and the 
hardness of constructing corpus, many strategies 
have been proposed and experimented to 
overcome the contradiction. 
Self-training is one of the most successful 
strategies. McClosky et al (2006) shows that 
self-training effectively improves the accuracy 
of English parsing. First, they trained a two-
stage reranking parser(Charniak and Johnson, 
2005) using Penn Treebank (PTB)(Marcus et al, 
1993) and parsed 1,750k unlabeled sentences 
from North American News Text corpus 
(NANC). Then they combined the labeled 
NANC sentences with PTB together as training 
set and retrained the first stage of the parser. The 
final result got a 1.1% improvement over the 
previous best parser for section 23 of the Penn 
Treebank. Huang and Harper (2009) combined 
self-training into a PCFG-LA based parser both 
for English and Chinese. Experimental result 
showed that self-training contributed 0.83% 
absolute improvement using only 210k 
unlabeled sentences with a single generative 
parser. For the Chinese parsing, self-training 
contributed 1.03% absolute improvement. 
Treebank Conversion is another potential 
strategy to reuse existing source Treebanks for 
the study of target grammar parsing. Wang et al 
(1994) proposed a Treebank conversion 
algorithm for corpus sharing. They employed a 
parser with target grammar formalism to get N-
best parse list for each sentence in source 
Treebank, selected the best conversion tree from 
the list using their algorithm, then inserted the 
conversion trees into training set, and finally 
retrained the parser with the enlarged training set. 
Experimental result shows their algorithm is 
effective. Collins et al (1999) performed 
statistical constituency parsing of Czech on a 
Treebank that was converted from the Prague 
Dependency Treebank under the guidance of 
conversion rules and heuristic rules, and the final 
performance was also improved. Xia and Palmer 
(2001) proposed three methods to convert 
dependency trees into phrase structure trees with 
some hand-written heuristic rules. For 
acquisition of better conversion rules, Xia et al 
(2008) proposed a method to automatically 
extract conversion rules from a target Treebank. 
Niu et al (2009) tried to exploit heterogeneous 
Treebanks for parsing. They proposed a 
grammar formalism conversion algorithm to 
convert dependency formalism Treebank into 
phrase structure formalism, and did phrase 
structure parsing with the conversion trees. Their 
experiments are done in Chinese parsing, and the 
final performance is improved indeed. 
In summary, from the existing work we are 
confident that the strategies of self-training and 
Treebank conversion are effective to improve 
the performance of parser. 
3 Our Strategy 
3.1 Parsing Algorithm 
Although self-training and Treebank Conversion 
are effective for training set enlarging, they all 
have drawbacks. Self-training needs some parse 
selection strategies to select higher quality 
parsers. Treebank Conversion needs us to 
maintain the consistency between conversed 
Treebank and target Treebank. On the other 
hand, self-training strategy provides us a good 
idea to get annotated trees consistent with target 
grammar formalism, and the parse trees in 
source side provide a reference for higher 
quality parsers selecting. So we can combine 
self-training and Treebank Conversion together, 
use self-training strategy to get converted 
candidates for sentences in source Treebank, and 
select higher quality parses according to trees in 
source Treebank. We call this strategy Treebank 
Conversion based Self-training, and show more 
details in Algorithm 1. 
In Algorithm 1, target Treebank tT  and source 
Treebank sT  are input first (line 1). Then tT  is 
split into two parts: training set trainT  and 
development set devT  (line 3). And we train an 
initial parser with trainT and devT  in line 4. From 
line 6 to line 12, we train parsers with SSPTC 
strategy Iter times iteratively. Let is tT o be the 
automatically converted Treebank from source 
Treebank to target Treebank grammar formalism 
during the i-th iteration. From line 8 to line 11, 
we try to get a conversion tree with target 
grammar for each of the N sentences in source 
Treebank. We get N-best parse list kParseList  for 
sentence ks with 1iParser   (line 9), select the 
parse ? kp  with the highest score from kParseList
(line 10), and insert it into is tT o  (line 11). This 
procedure runs iteratively until all the trees in 
source Treebank have been converted, finally, 
we train a new parser iParser  with trainT , devT  and 
i
s tT o (line 12). 
3.2 Parse selection 
In line 10 of Algorithm 1, we select the highest 
quality parse ? kp  from kParseList according to 
function ( , )s s tScore p p o , where sp denotes a tree 
in source Treebank and s tp o denotes a 
conversion tree with target Treebank grammar 
formalism for sp . ( , )s s tScore p p o  compares 
s tp o  with sp  and computes a score for s tp o
taken sp  as a reference. According to the idea 
proposed in Wang et al (1994), we use the 
number of aligned constituents in the source and 
target trees to construct ( , )s s tScore p p o . We 
propose two types of ( , )s s tScore p p o as follows. 
(1) Unlabeled aligned constituents F1 score 
(UAF)
First, we define a constituent as tag[i,j], which 
represents a non-terminal node labeled with tag
and spanning words from positions i to j of the 
input sentence. A non-terminal node in s tp o
aligns with a non-terminal node in sp  when they 
span the same words. If two nodes are aligned, 
we call them an aligned constituent and denote 
the aligned relationship as [ , ] [ , ]s ttag i j tag i j? .
For example in Figure 1, there are three aligned 
constituents between the source Treebank tree 
and the conversion tree, and we can denote them 
as [0, 7] [0, 7]s tIP dj? , [0, 2] [0, 2]s tNR sp? and
[2, 6] [2, 6]s tNR np? , respectively. 
When given sp and s tp o , we can easily 
collect all the aligned constituents. So we define 
Unlabeled aligned constituents Precision (UAP) 
and Unlabeled aligned constituents Recall (UAR) 
as follows. 
,
,
( [ , ] [ , ])
( [ , ])
s t
i j
t
i j
Count tag i j tag i j
UAP
Count tag i j
?
 
?
?
,
,
( [ , ] [ , ])
( [ , ])
s t
i j
s
i j
Count tag i j tag i j
UAR
Count tag i j
?
 
?
?
Algorithm 1 
1: Input: tT and sT
2:  initialize 
3: { , ( )}train dev tT T Split Tm
4: 0 ( , )train devParser Train T Tm
5:   Iter iterations 
6: for im 1? Iter do
7:    is tT Io m
8:    for k m 1? N do
9:        1( , )k i kParseList Nbest Parser sm
10:       ,? arg max ( , )j kk s k jp ParseListp Score p p? 
11:       ?is t kT po m
12:   ( , , )ii train dev s tParser Train T T T om
13: return IterParser
Then Unlabeled aligned constituents F1 score 
(UAF) is defined as: 
,
,
2
( , )
2 ( [ , ] [ , ])
( ( [ , ]) ( [ , ]))
s s t
s t
i j
s t
i j
UAP UAR
Score p p
UAP UAR
Count tag i j tag i j
Count tag i j Count tag i j
o
u u
 

u ?
 

?
?
    (1) 
(2) Labeled aligned constituents F1 score 
(LAF)
In the last subsection, we define ( , )s s tScore p p o
according to UAF. In fact, the tags of 
constituents bring us much information to score 
conversion trees. So we 
define ( , )s s tScore p p o with Labeled aligned 
constituents F1 score (LAF) in this subsection. 
Because the annotation schemes are different, 
constituent tags in source Treebank may be 
much more different from target Treebank. The 
number of such tags may be drastically different 
and the mapping may not be one-to-one. To 
eliminate the contradiction, we assume that each 
tag in source Treebank can be converted into 
every tag in target Treebank with various 
probabilities. So there is a converting matrix 
representing the converting probabilities, and we 
can calculate the converting matrix through 
source Treebank and N-best conversion trees. 
Given the source Treebank and N-best 
conversion trees, first we align all the 
constituents, then collect all the aligned tags and 
compute the converting probability as the 
following equation.  
( )
( )
( )
s t
s t
s
Count tag tag
p tag tag
Count tag
?
o  
          (2) 
Finally, we modify UAF computed by 
equation (1) into LAF as below. 
,
,
( )
2 (1 ( )) ( [ , ] [ , ])
( ( [ , ]) ( [ , ]))
,
s t s t
i j
s t
i j
s s tScore
p tag tag Count tag i j tag i j
Count tag i j Count tag i j
p p
J
o  
u  u o u ?

?
?
                                                                           (3) 
In equation (3), J  is a tunable variable, which 
is used to weight the converting probability. 
Especially, LAF will be transferred into UAF 
when J =0.
3.3 Corpus weighting technique 
In line 12 of Algorithm 1, we train a new parser 
with target Treebank and conversion trees. 
However, the errors in automatically conversion 
trees are unavoidable and they would limit the 
accuracy of the self-trained model. So we have 
to take some measures to weight the gold target 
Treebank and the automatically conversion trees. 
McClosky et al (2006) and Niu et al (2009) 
take the strategy that duplicates the gold 
Treebank data many times. However, this 
strategy isn?t suitable for PCFG-LA 
parser 1 (Matsuzaki et al, 2005; Petrov et al, 
2006), because PCFG-LA employs an EM 
algorithm in training stage, so duplicating gold 
Treebank would increase the training time 
tremendously. Instead, according to Huang and 
Harper (2009), we weight the posterior 
probabilities computed for the gold and 
automatically converted trees to balance their 
importance. 
Let ( | )count A tEo be the count of rule 
A Eo  in a parse tree t . tT  and s tT o  are the sets 
of target Treebank and automatically converted 
trees from source Treebank respectively. The 
posterior probability of rule A Eo  (with 
weighting parameterD ) can be expressed as: 
1 We will use BerkeleyParser as our baseline parser, 
which is a PCFG-LA based parser. 
?? ?? ?? ? ?? ?? ??
NR NR NN CC NN NN VV
?? ?? ?? ? ?? ?? ??
nS nS vN cC n vN v
NR [0,2] NR [2,6] VP [6,7]
VP [0,6]
IP [0,7]
sp [0,2] np [4,6]
np [2,6]
vp [2,7]
dj [0,7]
(a) parse tree in source Treebank
(b) conversion tree with target Treebank grammar
Figure 1: source tree and its conversion 
tree with target grammar formalism 
( )
( | ) ( | )
( ( | ) ( | ))
t s t
t s t
t T t T
t T t T
p A
Count A t Count A t
Count A t Count A t
E
E
E D E
E D E
o
o
? ?
? ?
o  
o  o
o  o
? ?
? ? ?
(4)
4 Head Finding 
In Task21 of CLP2010, we are required to find 
heads for each constituent. Our method is to 
make head finding as a post procedure after 
parsing.
We treat head finding problem as a 
classification problem, which is to classify each 
context-free production into categories labelled 
with their heads. For example, there are three 
types of heads: -0, -0-2 and -2 for 
vp vp wP vpo , so we try to classify this 
production into categories labelled with -0, -0-2 
and -2. First, we scan the train set and collect all 
the heads for each context-free production. Then 
we train a Maxent classifier to classify each 
context-free production into categories. We take 
the same feature templates for the classification 
as Chen et al (2009) did, which is described in 
Table 1. 
The head finding procedure proceeds in a 
bottom-up fashion, so that we can make use of 
heads of productions in lower layers as features 
for classification of the higher layers. 
To evaluate the accuracy of our head finding 
method, we randomly select a development set, 
remove all head information and use our Maxent 
classifier to retrieve the heads. Experimental 
results show the accuracy has reached 98.28%. 
However, the final performance would drop 
much when the parse trees are generated 
automatically. Because the automatically 
generated parse trees would bring many errors, 
and the post procedure of head finding can?t 
correct the errors. 
5 Experiments and Analysis 
5.1 Data Preparation 
In order to evaluate the effectiveness of our 
approach, we do experiments for Chinese 
parsing using Tsinghua Chinese Treebank 
(TCTB) on target side and Penn Chinese 
Treebank (PCTB) on source side. We divide the 
training portion of the Tsinghua Chinese 
Treebank provided by CLP2010 into three parts 
as follows: 500 trees are randomly extracted as 
development set, another 500 as validating set 
and the rest trees are taken as training set. For 
trees in PCTB, all the empty-node and function 
tag information are removed. All the ParseVal 
measures reported in this paper are evaluated by 
the EVALB tool2.
5.2 Experiments 
In order to get a good final accuracy, we choose 
BerkeleyParser 3 , which is a state-of-the-art 
unlexicalized parser, and train a model with the 
training set as our baseline. The F1 score of 
validating set parsed by baseline parser is 
85.72%. In the following of this subsection, we 
try to combine our strategies into the baseline 
parser and evaluate the effectiveness. Because 
mult-time iterations can?t improve parsing 
performance tremendously but cost much time 
during our experiments, we take Iter=1 here. 
(1) Corpus weighting experiment 
To evaluate the corpus weighting strategy, we 
take sentences (ignore the tree structure) in 
PCTB as unlabeled data, and train a parser with 
self-training strategy. F1 scores of validating set 
varying with D in equation (4) are shown in 
Figure 2. From Figure 2, we find that the F1 
score varies with D , and reaches 86.46% 
2 http://nlp.cs.nyu.edu/evalb/ 
3 http://code.google.com/p/berkeleyparser/ 
Feature templates 
The label of the current constituent; 
The label of the left most child, the middle child and the right most child; 
The head word of the left most child, the middle child and the right most child; 
The POS tag of the head word of the left most child, the middle child and the right most child; 
Bigram of label, head word and POS tag of head word of the children: L/M, M/R; 
Trigram of label, head word and POS tag of head word of the children: L/M/R; 
The number of children; 
Table 1: Feature Templates for Head Finding 
when D =1. The 0.74 absolute improvement 
comparing with the baseline certifies the 
effectiveness of our corpus weighting strategy. 
(2) Parse selection experiments 
In this subsection we evaluate our parse 
selection strategies with the help of PCTB. 
According to Algorithm 1, we train an initial 
parser with training set and development set. 
Then we generate 50-best parses list with the 
initial parser for each sentence in PCTB, and 
select a higher-score parse for each sentence 
through our parse selection strategies to build a 
conversion Treebank. Finally, we retrain a parser 
with training set and the conversion Treebank 
with the help of corpus weighting strategy. 
Figure 3 shows F1 scores of validating set 
using UAF to select higher quality parses. 
When D =0.3, F1 score reaches 86.92%. The 
improvement over baseline is 1.2 percentage 
points. Comparing with the highest F1 score of 
self-training, we got 0.46 more improvement. So 
our parse selection strategy with UAF is 
effective.
Because the highest F1 score is at the point 
D =0.3 in Figure 3, we choose D =0.3 to 
evaluating LAF strategy. Figure 4 shows F1 
scores on validating set using LAF. The highest 
F1 score is 87.44% at the pointJ =6, and it gets 
1.72 percentage points improvement over 
baseline. Comparing with UAF, LAF gets 0.52 
more improvement. So we can conclude that the 
parse selection strategy with LAF is much more 
effective.
5.3 Discussion 
Table 2 reports the highest performances of 
various strategies. From the table we can easily 
find that all strategies outperform the baseline 
parser. Corpus weighting experiment tells us that 
balancing the importance of gold target 
Treebank and conversion trees is helpful for the 
final performance. Using UAF to select 
conversion trees can get more improvement than 
self-training which just selects the best-first trees. 
This fact proves that our SSPTC strategy is 
reasonable and effective. Making use of LAF, 
we get more improvement than UAF. It tells us 
that exploiting source Treebank deeply can bring 
us more useful knowledge which is helpful to 
develop high-performance parser. 
6 Experiments for Task 2 of CLP2010 
Task 2 of CLP2010 includes two sub-tasks: sub-
sentence parsing and complete sentence parsing. 
For each sub-task, there are two tracks: closed 
track and open track. To accomplish tasks in 
closed track, we make use of our baseline parser 
shown in section 5 and train it with different 
parameters and data set. For open track, we 
make use of our SSPTC strategy and train it with 
different parameters and data set. We tuned the 
parameters on the development set and selected 
Strategy F1 score 
Baseline 85.72% 
Corpus weighting 86.46% 
UAF 86.92% 
LAF 87.44% 
Table 2: F1 scores of various strategies 
Figure 4: F1 score of LAF strategy 
Figure 2: F1 score of self-training 
Figure 3: F1 score of UAF strategy 
some configurations which achieve higher 
performance on the development set(more 
details have been shown in section 5). The final 
parameters and training data of our systems are 
shown in Table 34. We also make use of the 
approach explained in section 4 for the head 
finding procedure. 
The parsing results of our systems on the test 
set can be found on the official ranking report. 
Our systems training with SSPTC strategy bring 
us an amazing performance which outperforms 
other systems in both the two sub-tasks. 
7 Conclusion and Future work 
In this paper, we propose a novel self-training 
strategy for parsing which is based on Treebank 
conversion. Benefiting from SSPTC strategy, we 
have gotten higher quality parse trees with the 
help of source Treebank, and gotten conversion 
Treebank with target Treebank grammar 
formalism simply and consistently. The parsing 
results on validating set show SSPTC is 
effective. We apply SSPTC to the test set of 
Task 2 in CLP2010, and get 1.275 percentage 
points improvement over baseline parser using 
the parameters tuned on validating set.  
4 The parsing result for system b in open track of sub-
task1 has been submitted mistakenly, so the figures of 
this system on the official ranking report have no 
reference value. 
5 The F1 score of baseline parser is 75.24%, and it 
reaches 76.51% using TCBS strategy. 
All the delightful results tell us that SSPTC is 
a promoting strategy for parsing. However, there 
is much knowledge in source Treebank remained 
to further exploit, e.g. the POS tags in source 
Treebank is a good resource to improve the POS 
tagging accuracy of target Treebank. So, in the 
next step we will exploit source Treebank deeply 
and try to get more knowledge from it for 
parsing.
Acknowledgement 
The research work has been partially funded by 
the Natural Science Foundation of China under 
Grant No. 6097 5053, 90820303 and 60736014, 
the National Key Technology R&D Program 
under Grant No. 2006BAH03B02, the Hi-Tech 
Research and Development Program (?863? 
Program) of China under Grant No. 
2006AA010108-4, and also supported by the 
China-Singapore Institute of Digital Media 
(CSIDM) project under grant No. CSIDM-
200804. 
References 
Eugene Charniak, 2000. A maximum-entropy-
inspired parser. In NAACL-2000  
Eugene Charniak and Mark Johnson, 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative 
reranking. In ACL-05. 
Xiao Chen, Changning Huang, Mu Li and Chunyu 
Kit, 2009. Better Parser Combination. In CIPS.  
 Sub-task Track  ID Parser Parameters Train data
 a  Berkeley -- TS
  Closed 
 b  Berkeley -- TS && VS
a SSPTC  0.3  5D J  TS && PTCB
  Sub-task 1
Open
b SSPTC  0.3  5D J   TS && VS && PTCB 
a   Berkeley -- TS
  Closed
b   Berkeley -- TS && VS
a SSPTC  0.3  6D J  TS && PTCB
b SSPTC  0.3  5D J   TS && VS && PTCB
c SSPTC  0.3  5D J  TS && PTCB
 Sub-task 2
Open
d SSPTC  0.3  3D J  TS && PTCB
Table 3: The configurations of our systems. The abbreviations in the last column mean 
training set(TS) and validating set(VS) explaining in section 5.1. 
Michael Collins, 2003. Head-driven statistical models 
for natural language parsing. Computational 
Linguistics, 29 (4). pages 589-637.  
M Collins, J Hajic, L Ramshaw and C Tillman, 1999. 
A statistical parser for Czech. In ACL-99. J 
Henderson, 2004. Discriminative training of a 
neural network statistical parser. 
Zhongqiang Huang and Mary Harper, 2009. Self-
Training PCFG grammars with latent annotations 
across languages. ACL-09. 
R Hwa, M Osborne, A Sarkar and M Steedman, 2003. 
Corrected co-training for statistical parsers. 
Citeseer.
MP Marcus, B Santorini and MA Marcinkiewicz, 
1993. Building a large annotated corpus of English: 
The Penn Treebank. Computational Linguistics, 19 
(2). pages 313-330. 
Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii, 
2005. Probabilistic CFG with latent annotations. In 
ACL-05.  
David McClosky, Eugene Charniak and Mark 
Johnson, 2006. Effective self-training for parsing. 
In ACL-06.  
Zheng-Yu Niu, Haifeng Wang and Hua Wu, 2009. 
Exploiting heterogeneous treebanks for parsing. In 
ACL-09, pages 46-54. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In ACL-06.  
Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su, 
1994. An automatic treebank conversion algorithm 
for corpus sharing. In ACL-94. 
Fei Xia and Martha Palmer, 2001. Converting 
dependency structures to phrase structures. In The 
1st Human Language Technology Conference 
(HLT-2001). 
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha 
Palmer and Dipti Misra Sharma, 2008. Towards a 
multi-representational treebank. Proc. of the 7th 
Int'lWorkshop on Treebanks and Linguistic 
Theories (TLT-7). pages 207-238. 
Qiang Zhou, 2004. Annotation Scheme for Chinese 
Treebank. Journal of Chinese Information 
Processing, 18 (004). 
