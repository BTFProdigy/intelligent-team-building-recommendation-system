Learning Named Entity Hyponyms for Question Answering
Paul McNamee
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
paul.mcnamee@jhuapl.edu
Rion Snow
Stanford AI Laboratory
Stanford University
Stanford, CA 94305, USA
rion@cs.stanford.edu
Patrick Schone
Department of Defense
Fort George G. Meade, MD 20755-6000
pjschon@tycho.ncsc.mil
James Mayfield
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
james.mayfield@jhuapl.edu
Abstract
Lexical mismatch is a problem that con-
founds automatic question answering sys-
tems. While existing lexical ontologies such
as WordNet have been successfully used to
match verbal synonyms (e.g., beat and de-
feat) and common nouns (tennis is-a sport),
their coverage of proper nouns is less ex-
tensive. Question answering depends sub-
stantially on processing named entities, and
thus it would be of significant benefit if
lexical ontologies could be enhanced with
additional hypernymic (i.e., is-a) relations
that include proper nouns, such as Edward
Teach is-a pirate. We demonstrate how a re-
cently developed statistical approach to min-
ing such relations can be tailored to iden-
tify named entity hyponyms, and how as a
result, superior question answering perfor-
mance can be obtained. We ranked candi-
date hyponyms on 75 categories of named
entities and attained 53% mean average pre-
cision. On TREC QA data our method pro-
duces a 9% improvement in performance.
1 Introduction
To correctly extract answers, modern question an-
swering systems depend on matching words be-
tween questions and retrieved passages containing
answers. We are interested in learning hypernymic
(i.e., is-a) relations involving named entities because
we believe these can be exploited to improve a sig-
nificant class of questions.
For example, consider the following questions:
? What island produces Blue Mountain coffee?
? In which game show do participants compete
based on their knowledge of consumer prices?
? What villain is the nemesis of Dudley Do-
Right?
Knowledge that Jamaica is an island, that The Price
is Right is a game show, and that Snidely Whiplash
is a villain, is crucial to answering these questions.
Sometimes these relations are evident in the same
context as answers to questions, for example, in
?The island of Jamaica is the only producer of Blue
Mountain coffee?; however, ?Jamaica is the only
producer of Blue Mountain coffee? should be suf-
ficient, despite the fact that Jamaica is an island is
not observable from the sentence.
The dynamic nature of named entities (NEs)
makes it difficult to enumerate all of their evolv-
ing properties; thus manual creation and curation
of this information in a lexical resource such as
WordNet (Fellbaum, 1998) is problematic. Pasca
and Harabagiu discuss how insufficient coverage of
named entities impairs QA (2001). They write:
?Because WordNet was not designed
as an encyclopedia, the hyponyms of con-
cepts such as composer or poet are illus-
trations rather than an exhaustive list of
instances. For example, only twelve com-
poser names specialize the concept com-
poser ... Consequently, the enhancement
of WordNet with NE information could
help QA.?
799
The chief contribution of this study is demonstrat-
ing that an automatically mined knowledge base,
which naturally contains errors as well as correctly
distilled knowledge, can be used to improve QA per-
formance. In Section 2 we discuss prior work in
identifying hypernymic relations. We then explain
our methods for improved NE hyponym learning
and its evaluation (Section 3) and apply the relations
that are discovered to enhance question answering
(Section 4). Finally we discuss our results (Section
5) and present our conclusions (Section 6).
2 Hyponym Induction
We review several approaches to learning is-a rela-
tions.
2.1 Hearst Patterns
The seminal work in the field of hypernym learn-
ing was done by Hearst (1992). Her approach was
to identify discriminating lexico-syntactic patterns
that suggest hypernymic relations. For example, ?X,
such as Y?, as in ?elements, such as chlorine and
fluorine?.
2.2 KnowItAll
Etzioni et al developed a system, KnowItAll, that
does not require training examples and is broadly
applicable to a variety of classes (2005). Starting
with seed examples generated from high precision
generic patterns, the system identifies class-specific
lexical and part-of-speech patterns and builds a
Bayesian classifier for each category. KnowItAll
was used to learn hundreds of thousands of class
instances and clearly has potential for improving
QA; however, it would be difficult to reproduce the
approach because of information required for each
class (i.e., specifying synonyms such as town and
village for city) and because it relies on submitting a
large number of queries to a web search engine.
2.3 Query Logs
Pasca and Van Durme looked at learning entity class
membership for five high frequency classes (com-
pany, country, city, drug, and painter), using search
engine query logs (2007). They reported precision
at 50 instances between 0.50 and 0.82.
2.4 Dependency Patterns
Snow et al have described an approach with several
desirable properties: (1) it is weakly-supervised and
only requires examples of hypernym/hyponym rela-
tions and unannotated text; (2) the method is suit-
able for both common and rare categories; and, (3)
it achieves good performance without post filtering
using the Web (2005; 2006). Their method relies
on dependency parsing, a form of shallow parsing
where each word modifies a single parent word.
Hypernym/hyponym word pairs where the words1
belong to a single WordNet synset were identified
and served to generate training data in the follow-
ing way: making the assumption that when the two
words co-occur, evidence for the is-a relation is
present, sentences containing both terms were ex-
tracted from unlabeled text. The sentences were
parsed and paths between the nouns in the depen-
dency trees were calculated and used as features in a
supervised classifier for hypernymy.
3 Learning Named Entity Hyponyms
The present work follows the technique described
by Snow et al; however, we tailor the approach in
several ways. First, we replace the logistic regres-
sion model with a support vector machine (SVM-
Light). Second, we significantly increase the size
of training corpora to increase coverage. This ben-
eficially increases the density of training and test
vectors. Third, we include additional features not
based on dependency parses (e.g., morphology and
capitalization). Fourth, because we are specifically
interested in hypernymic relations involving named
entities, we use a bootstrapping phase where train-
ing data consisting primarily of common nouns are
used to make predictions and we then manually ex-
tract named entity hyponyms to augment the train-
ing data. A second learner is then trained using the
entity-enriched data.
3.1 Data
We rely on large amounts of text; in all our exper-
iments we worked with a corpus from the sources
given in Table 1. Sentences that presented difficul-
ties in parsing were removed and those remaining
1Throughout the paper, use of the term word is intended to
include named entities and other multiword expressions.
800
Table 1: Sources used for training and learning.
Size Sentences Genre
TREC Disks 4,5 81 MB 0.70 M Newswire
AQUAINT 1464 MB 12.17 M Newswire
Wikipedia (4/04) 357 MB 3.27 M Encyclopedia
Table 2: Characteristics of training sets.
Pos. Pairs Neg. Pairs Total Features
Baseline 7975 63093 162528
+NE 9331 63093 164298
+Feat 7975 63093 162804
were parsed with MINIPAR (Lin, 1998). We ex-
tracted 17.3 million noun pairs that co-occurred in
at least one sentence. All pairs were viewed as po-
tential hyper/hyponyms.
Our three experimental conditions are summa-
rized in Table 2. The baseline model used 71068
pairs as training data; it is comparable to the
weakly-supervised hypernym classifier of Snow et
al. (2005), which used only dependency parse fea-
tures, although here the corpus is larger. The entity-
enriched data extended the baseline training set by
adding positive examples. The +Feat model uses ad-
ditional features besides dependency paths.
3.2 Bootstrapping
Our synthetic data relies on hyper/hyponym pairs
drawn from WordNet, which is generally rich in
common nouns and lacking in proper nouns. But
certain lexical and syntactic features are more likely
to be predictive for NE hyponyms. For example, it
is uncommon to precede a named entity with an in-
definite article, and certain superlative adjectives are
more likely to be used to modify classes of entities
(e.g., ?the youngest coach?, ?the highest peak?). Ac-
cordingly we wanted to enrich our training data with
NE exemplars.
By manually reviewing highly ranked predictions
of the baseline system, we identified 1356 additional
pairs to augment the training data. This annotation
took about a person-day. We then rescanned the cor-
pus to build training vectors for these co-occurring
nouns to produce the +NE model vectors.
Table 3: Features considered for +Feat model.
Feature Comment
Hypernym con-
tained in hyponym
Sands Hotel is-a hotel
Length in chars /
words
Chars: 1-4, 5-8, 9-16, 17+
Words: 1, 2, 3, 4, 5, 6, 7+
Has preposition Treaty of Paris; Statue of Liberty
Common suffixes -ation, -ment, -ology, etc...
Figurative term Such as goal, basis, or problem
Abstract category Like person, location, amount
Contains digits Usually not a good hyponym
Day of week;
month of year
Indiscriminately co-occurs with
many nouns.
Presence and depth
in WordNet graph
Shallow hypernyms are unlikely to
have entity hyponyms. Presence in
WN suggests word is not an entity.
Lexname of 1st
synset in WordNet
Root classes like person, location,
quantity, and process.
Capitalization Helps identify entities.
Binned document
frequency
Partitioned by base 10 logs
3.3 Additional Features
The +Feat model incorporated an additional 276 bi-
nary features which are listed in Table 3. We consid-
ered other features such as the frequency of patterns
on the Web, but with over 17 million noun pairs this
was computationally infeasible.
3.4 Evaluation
To compare our different models we created a test
set of 75 categories. The classes are diverse and
include personal, corporate, geographic, political,
artistic, abstract, and consumer product entities.
From the top 100 responses of the different learn-
ers, a pool of candidate hyponyms was created, ran-
domly reordered, and judged by one of the authors.
To assess the quality of purported hyponyms we
used average precision, a measure in ranked infor-
mation retrieval evaluation, which combines preci-
sion and recall.
Table 4 gives average precision values for the
three models on 15 classes of mixed difficulty2. Per-
formance varies considerably based on the hyper-
nym category, and for a given category, by classifier.
N is the number of known correct instances found in
the pool that belong to a given category.
Aggregate performance, as mean average preci-
sion, was computed over all 75 categories and is
2These are not the highest performing classes
801
Table 4: Average precision on 15 categories.
N Baseline +NE +Feat
chemical element 78 0.9096 0.9781 0.8057
african country 48 0.8581 0.8521 0.4294
prep school 26 0.6990 0.7098 0.7924
oil company 132 0.6406 0.6342 0.7808
boxer 109 0.6249 0.6487 0.6773
sculptor 95 0.6108 0.6375 0.8634
cartoonist 58 0.5988 0.6109 0.7097
volcano 119 0.5687 0.5516 0.7722
horse race 23 0.4837 0.4962 0.7322
musical 80 0.4827 0.4270 0.3690
astronaut 114 0.4723 0.5912 0.5738
word processor 26 0.4437 0.4426 0.6207
chief justice 115 0.4029 0.4630 0.5955
perfume 43 0.2482 0.2400 0.5231
pirate 10 0.1885 0.3070 0.2282
Table 5: Mean average precision over 75 categories.
Baseline +NE +Feat
MAP 0.4801 0.5001 (+4.2%) 0.5320 (+10.8%)
given in Table 5. Both the +NE and +Feat models
yielded improvements that were statistically signif-
icant at a 99% confidence level. The +Feat model
gained 11% over the baseline condition. The maxi-
mum F-score for +Feat is 0.55 at 70% recall.
Mean average precision emphasizes precision at
low ranks, so to capture the error characteristics at
multiple operating points we present a precision-
recall graph in Figure 1. The +NE and +Feat models
both attain superior performance at all but the lowest
recall levels. For question answering this is impor-
tant because it is not known which entities will be
the focus of a question, so the ability to deeply mine
various entity classes is important.
Table 6 lists top responses for four categories.
3.5 Discussion
53% mean average precision seems good, but is it
good enough? For automated taxonomy construc-
tion precision of extracted hyponyms is critically
important; however, because we want to improve
question answering we prefer high recall and can
tolerate some mistakes. This is because only a small
set of passages that are likely to contain an answer
are examined in detail, and only from this subset
of passages do we need to reason about potential
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Level
P
r
e
c
i
s
i
o
n
Feat
Ent
Baseline
Figure 1: Precision-recall graph for three classifiers.
hyponyms. In the next section we describe an ex-
periment which confirms that our learned entity hy-
ponyms are beneficial.
4 QA Experiments
4.1 QACTIS
To evaluate the usefulness of our learned NE hy-
ponyms for question answering, we used the QAC-
TIS system (Schone et al, 2005). QACTIS was
fielded at the 2004-2006 TREC QA evaluations and
placed fifth at the 2005 workshop. We worked with
a version of the software from July 2005.
QACTIS uses WordNet to improve matching of
question and document words, and a resource, the
Semantic Forest Dictionary (SFD), which contains
many hypernym/hyponym pairs. The SFD was pop-
ulated through both automatic and manual means
(Schone et al, 2005), and was updated based on
questions asked in TREC evaluations through 2004.
4.2 Experimental Setup
We used factoid questions from the TREC 2005-
2006 QA evaluations (Voorhees and Dang, 2005)
and measured performance with mean reciprocal
rank (MRR) and percent correct at rank 1.
All runs made use of WordNet 2.0, and we ex-
amined several other sources of hypernym knowl-
802
Table 6: Top responses for four categories using the +Feat model. Starred entries were judged incorrect.
Sculptor Horse Race Astronaut Perfume
1 Evelyn Beatrice Longman Tevis Cup Mark L Polansky * Avishag
2 Nancy Schon Kenilworth Park Gold Cup Richard O Covey Ptisenbon
3 Phidias Cox Plate George D Nelson Poeme
4 Stanley Brandon Kearl Grosser Bugatti Preis Guion Bluford Jr Parfums International
5 Andy Galsworthy Melbourne Cup Stephen S Oswald Topper Schroeder
6 Alexander Collin * Great Budda Hall Eileen Collins * Baccarin
7 Rachel Feinstein Travers Stakes Leopold Eyharts Pink Lady
8 Zurab K Tsereteli English Derby Daniel M Tani Blue Waltz
9 Bertel Thorvaldsen * Contrade Ronald Grabe WCW Nitro
10 Cildo Meireles Palio * Frank Poole Jicky
Table 7: Additional knowledge sources by size.
Classes Class Instances
Baseline 76 11,066
SFD 1,140 75,647
SWN 7,327 458,370
+Feat 44,703 1,868,393
edge. The baseline condition added a small subset
of the Semantic Forest Dictionary consisting of 76
classes seen in earlier TREC test sets (e.g., nation-
alities, occupations, presidents). We also tested: (1)
the full SFD; (2) a database from the Stanford Word-
net (SWN) project (Snow et al, 2006); and, (3) the
+Feat model discussed in Section 3. The number of
classes and entries of each is given in Table 7.
4.3 Results
We observed that each source of knowledge benefit-
ted questions that were incorrectly answered in the
baseline condition. Examples include learning a me-
teorite (Q84.1), a university (Q93.3), a chief oper-
ating officer (Q108.3), a political party (Q183.3), a
pyramid (Q186.4), and a movie (Q211.5).
In Table 8 we compare performance on questions
from the 2005 and 2006 test sets. We assessed
performance primarily on test questions that were
deemed likely to benefit from hyponym knowledge
? questions that had a readily discernible category
(e.g., ?What film ...?, ?In what country ...?) ? but we
also give results on the entire test set.
The WordNet-only run suffers a large decrease
compared to the baseline. This is expected because
WordNet lacks coverage of entities and the baseline
condition specifically populates common categories
of entities that have been observed in prior TREC
evaluations. Nonetheless, WordNet is useful to the
system because it addresses lexical mismatch that
does not involve entities.
The full SFD, the SWN, and the +Feat model
achieved 17%, 2%, and 9% improvements in answer
correctness, respectively. While no model had ex-
posure to the 2005-2006 TREC questions, the SFD
database was manually updated based on training
on the TREC-8 through TREC-2004 data sets. It
approximates an upper bound on gains attributable
to addition of hyponym knowledge: it has an un-
fair advantage over the other models because recent
question sets use similar categories to those in ear-
lier TRECs. Our +Feat model, which has no bias
towards TREC questions, realizes larger gains than
the SWN. This is probably at least in part because it
produced a more diverse set of classes and a signif-
icantly larger number of class instances. Compared
to the baseline condition the +Feat model sees a 7%
improvement in mean reciprocal rank and a 9% im-
provement in correct first answers; both results rep-
resent a doubling of performance compared to the
use of WordNet alne. We believe that these results
illustrate clear improvement attributable to automat-
ically learned hyponyms.
The rightmost columns in Table 8 reveal that the
magnitude of improvements, when measured over
all questions, is less. But the drop off is consistent
with the fact that only one third of questions have
clear need for entity knowledge.
5 Discussion
Although there is a significant body of work in auto-
mated ontology construction, few researchers have
examined the relationship between their methods
803
Table 8: QA Performance on TREC 2005 & 2006 Data
Hyponym-Relevant Subset (242) All Questions (734)
MRR % Correct MRR % Correct
WN-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%)
Baseline 0.348 26.4 0.342 26.4
SFD 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%)
SWN 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%)
Feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)
for knowledge discovery and improved question-
answering performance. One notable study was con-
ducted by Mann (2002). Our work differs in two
ways: (1) his method for identifying hyponyms was
based on a single syntactic pattern, and (2) he looked
at a comparatively simple task ? given a question
and one answer sentence containing the answer, ex-
tract the correct named entity answer.
Other attempts to deal with lexical mismatch in
automated QA include rescoring based on syntactic
variation (Cui et al, 2005) and identification of ver-
bal paraphrases (Lin and Pantel, 2001).
The main contribution of this paper is showing
that large-scale, weakly-supervised hyponym learn-
ing is capable of producing improvements in an end-
to-end QA system. In contrast, previous studies have
generally presented algorithmic advances and show-
cased sample results, but failed to demonstrate gains
in a realistic application. While the hypothesis that
discovering is-a relations for entities would improve
factoid QA is intuitive, we believe these experiments
are important because they show that automatically
distilled knowledge, even when containing errors
that would not be introduced by human ontologists,
is effective in question answering systems.
6 Conclusion
We have shown that highly accurate statistical learn-
ing of named entity hyponyms is feasible and that
bootstrapping and feature augmentation can signif-
icantly improve classifier accuracy. Mean aver-
age precision of 53% was attained on a set of 75
categories that included many fine-grained entity
classes. We also demonstrated that mining knowl-
edge about entities can be directly applied to ques-
tion answering, and we measured the benefit on
TREC QA data. On a subset of questions for
which NE hyponyms are likely to help we found that
learned hyponyms generated a 9% improvement in
performance compared to a strong baseline.
References
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng
Chua. 2005. Question answering passage retrieval using
dependency relations. In SIGIR 2005, pages 400?407.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana M.
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised Named-Entity
Extraction from the Web: An Experimental Study. Artificial
Intelligence, 165(1):191?134.
Christine Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In ACL 1992, pages 539?545.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of minipar.
In Workshop on the Evaluation of Parsing Systems.
Gideon S. Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In COLING-02 on SEMANET,
pages 1?7.
Marius Pasca and Benjamin Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from query
logs. In IJCAI-07, pages 2832?2837.
Marius Pasca and Sanda M. Harabagiu. 2001. The informa-
tive role of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on WordNet and
Other Lexical Resources.
Patrick Schone, Gary Ciany, Paul McNamee, James Mayfield,
and Thomas Smith. 2005. QACTIS-based Question An-
swering at TREC 2005. In Proceedings of the 14th Text RE-
trieval Conference.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery. In
NIPS 17.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In ACL
2006, pages 801?808.
804
Proceedings of NAACL HLT 2009: Short Papers, pages 25?28,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Translation Corpus Source and Size in Bilingual Retrieval
Paul McNamee and James Mayfield
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
{paul.mcnamee,james.mayfield}@jhuapl.edu
Charles Nicholas
Dept. of Computer Science and Electrical Engineering
UMBC
Baltimore, MD 21250, USA
nicholas@umbc.edu
Abstract
This paper explores corpus-based bilingual re-
trieval where the translation corpora used vary
by source and size. We find that the quality of
translation alignments and the domain of the
bitext are important. In some settings these
factors are more critical than corpus size. We
also show that judicious choice of tokeniza-
tion can reduce the amount of bitext required
to obtain good bilingual retrieval performance.
1 Introduction
Large parallel corpora are an increasingly available
commodity. Such texts are the fuel of statistical
machine translation systems and are used in appli-
cations such as cross-language information retrieval
(CLIR). Several beliefs are commonly held regard-
ing the relationship between parallel text quality and
size for CLIR. It is thought that larger texts should
be better, because the problems of data sparseness
and untranslatable terms are reduced. Similarly, par-
allel text from a domain more closely related to a
document collection should lead to better bilingual
retrieval performance, again because better lexical
translations are available.
We compared four sources of parallel text us-
ing CLEF document collections in eight languages
(Braschler and Peters, 2004). English topic sets
from 2000 to 2007 were used. Corpus-based trans-
lation of query terms was performed and documents
were ranked using a statistical language model ap-
proach to retrieval (Ponte and Croft, 1998). Exper-
iments were conducted using unlemmatized words
and character 5-grams. No use was made of pre-
translation query expansion or automated relevance
feedback.
2 Translation Corpora
Information about the four parallel texts used in our
experiments is provided in Table 1. We restricted
our focus to Dutch (NL), English (EN), Finnish (FI),
French (FR), German (DE), Italian (IT), Portuguese
(PT), Spanish (ES), and Swedish (SV). These lan-
guages are covered by each parallel corpus.
2.1 Bible
The bible corpus is based on the 66 books in the Old
and New Testaments. Alignments at the verse level
were used; there are 31103 verses in the English text.
2.2 JRC-Acquis v3
This parallel text is based on EU laws comprising the
Acquis Communautaire and translations are avail-
able in 22 languages. The English portion of the
acquis data includes 1.2 million aligned passages
containing over 32 million words, which is approxi-
mately 40 times larger than the Biblical text. Align-
ments were provided with the corpus and were pro-
duced by the Vanilla algorithm.1 The alignments are
at roughly the sentence level, but only 85% corre-
spond to a single sentence in both languages.
2.3 Europarl v3
The Europarl corpus was assembled to support ex-
periments in statistical machine translation (Koehn,
2005). The documents consist of transcribed dia-
logue from the official proceedings of the European
Parliament. We used the precomputed alignments
that are provided with the corpus, and which are
based on the algorithm by Gale and Church (1991).
The alignments are believed to be of high quality.
1Available from http://nl.ijs.si/telri/vanilla/
25
Name Words Wrds/doc Alignments Genre Source
bible 785k 25.3 Near Perfect Religious http://unbound.biola.edu/
acquis 32M 26.3 Good EU law (1958 to 2006) http://wt.jrc.it/lt/acquis/
europarl 33M 25.5 Very Good Parliamentary oration
(1996 to 2006)
http://www.statmt.org/europarl/
ojeu 84M 34.5 Fair Governmental affairs
(1998 to 2004)
Derived from documents at
http://europea.eu.int/
Table 1: Parallel texts used in experiments.
2.4 Official Journal of the EU
The Official Journal of the European Union covers a
wide range of topics such as agriculture, trade, and
foreign relations. We constructed this parallel cor-
pus by downloading documents dating from January
1998 through April 2004 and converting the texts
from Adobe?s Portable Document Format (PDF) to
ISO-8859-1 encoded text using pdftotext. The doc-
uments were segmented into pages and into para-
graphs consisting of a small number of sentences
(typically 1 to 3); however this process was compli-
cated by the fact that many documents have outline
or tabular formatting. Alignments were produced
using Church?s char align software (1993).
Due to complexities of decoding the PDF, some of
the accented characters were not extracted properly,
but this is a problem mostly for the earlier material
in the collection. In total about 85 million words of
text per language was obtained, which is over twice
the size of either the acquis or europarl collections.
3 Translation
Using the pairwise-aligned corpora described above,
parallel indexes for each corpus were created using
words and 5-grams. Query translation was accom-
plished as follows. For each query term s, source
language documents from the aligned collection that
contain s are identified. If no document contains this
term, then it is left untranslated. Each target lan-
guage term t appearing in the corresponding docu-
ments is scored:
Score(t) = (Fl(t)? Fc(t))? IDF (t)1.25 (1)
where Fl and Fc are relative document frequencies
based on local subset of documents and the whole
corpus. IDF (t) is the inverse document frequency,
or log2( Ndf(t)). The candidate translation with thehighest score replaced the original query term and
the transformed query vector is used for retrieval
against the target language collection.
This is a straightforward approach to query trans-
lation. More sophisticated methods have been pro-
posed, including bidirectional translation (Wang and
Oard, 2006) and use of more than one translation
candidate per query term (Pirkola et al, 2003).
Subword translation, the direct translation of
character n-grams, offers several advantages over
translating words (McNamee and Mayfield, 2005).
N-grams provide morphological normalization,
translations of multiword expressions are suggested
by translation of word-spanning n-grams, and out-
of-vocabulary (OOV) words can be be partly trans-
lated with n-gram fragments. Additionally, there are
few OOV n-grams, at least for n = 4 and n = 5.
4 Experimental Results
We describe two experiments. The first examines
the efficacy of the different translation resources and
the second measures the relationship between cor-
pus size and retrieval effectiveness. English was the
sole source language.
4.1 Translation Resources
First the relationship between translation source and
bilingual retrieval effectiveness is studied. Table 2
reports mean average precision when word-based to-
kenization and translation was performed for each
of the target collections. For comparison the cor-
responding performance using topics in the target
language (mono) is also given. As expected, the
smallest bitext, bible, performs the worst. Averaged
across the eight languages only 39% relative effec-
tiveness is seen compared to monolingual perfor-
mance. Reports advocating the use of religious texts
for general purpose CLIR may have been overly op-
timistic (Chew et al, 2006). Both acquis and eu-
roparl are roughly 40 times larger in size than bible
26
Target mono bible acquis europarl ojeu
DE 0.3303 0.1338 0.1802 0.2427 0.1937
ES 0.4396 0.1454 0.2583 0.3509 0.2786
FI 0.3406 0.1288 0.1286 0.2135 0.1636
FR 0.3638 0.1651 0.2508 0.2942 0.2600
IT 0.3749 0.1080 0.2365 0.2913 0.2405
NL 0.3813 0.1502 0.2474 0.2974 0.2484
PT 0.3162 0.1432 0.2009 0.2365 0.2157
SV 0.3387 0.1509 0.2111 0.2447 0.1861
Average 0.3607 0.1407 0.2142 0.2714 0.2233
39.0% 59.4% 75.3% 61.9%
Table 2: Mean average precision for word-based transla-
tion of English topics using different corpora.
Target mono bible acquis europarl ojeu
DE 0.4201 0.1921 0.2952 0.3519 0.3169
ES 0.4609 0.2295 0.3661 0.4294 0.3837
FI 0.5078 0.1886 0.3552 0.3744 0.3743
FR 0.3930 0.2203 0.3013 0.3523 0.3334
IT 0.3997 0.2110 0.2920 0.3395 0.3160
NL 0.4243 0.2132 0.3060 0.3603 0.3276
PT 0.3524 0.1892 0.2544 0.2931 0.2769
SV 0.4271 0.1653 0.3016 0.3203 0.2998
Average 0.4232 0.2012 0.3090 0.3527 0.3286
47.5% 73.0% 83.3% 77.6%
Table 3: Mean average precision using 5-gram transla-
tions of English topics using different corpora.
and both do significantly better; however europarl is
clearly superior and achieves 75% of monolingual
effectiveness. Though nearly twice the size, ojeu
fails to outperform europarl and just barely beats
acquis. Likely reasons for this include difficulties
properly converting the ojeu data to text, problem-
atic alignments, and the substantially greater length
of the aligned passages.
The same observations can be seen from Table 3
where 5-grams were used for tokenization and trans-
lation instead of words. The level of performance
with 5-grams is higher and these improvements are
statistically significant with p < 0.01 (t-test).2 Av-
eraged across the eight languages gains from 30% to
47% were seen using 5-grams, depending on the re-
source. As a translation resource europarl still out-
performs the other sources in each of the eight lan-
guages and the relative ordering of {europarl, ojeu,
acquis, bible} is the same in both cases.
2Except in four cases: mono: In ES & IT p < 0.05; bible:
5-grams were not significantly different than words in FI & SV
4.2 Size of Parallel Text
To investigate how corpus size effects bilingual
retrieval we subsampled europarl and used these
smaller subcorpora for translation. The entire cor-
pus is 33 million words in size, and samples of 1%,
2%, 5%, 10%, 20%, 40%, 60%, and 80% were made
based on counting documents, which for europarl
is equivalent to counting sentences. Samples were
taken by processing the data in chronological order.
In Figure 1 (a-d) the effect of using larger parallel
corpora is plotted for four languages. Mean average
precision is on the vertical axes, and for visual effect
the chart for each language pair uses the same scale.
The general shape of the curves is to rise quickly as
increasing subsets from 1% to 10% are used and to
flatten as size increases further. Curves for the other
four languages (not shown) are quite similar. The
deceleration of improvement with increasing cor-
pus size can be explained by Heap?s Law. Similar
results have been obtained in the few studies that
have sought to quantify bilingual retrieval perfor-
mance as a function of translation resource size (Xu
and Weischedel, 2000; Demner-Fushman and Oard,
2003). In the higher complexity languages such as
German and Finnish, n-grams appear to be gaining
a slight improvement even when the entire corpus is
used; vocabulary size is greater in those languages.
The data for the 0% condition were based on
cognate matches for words and ?cognate n-grams?
that require no translation. The figure reveals that
even very small amounts of parallel text quickly im-
prove performance. The 2% condition is roughly the
size of bible, but is higher performing, likely due
to a better domain match.3 Using a subsample of
only 5% of available data from the highest perform-
ing translation resource, europarl, 5-grams outper-
formed plain words using any amount of bitext.
5 Conclusion
We examined issues in corpus-based bilingual re-
trieval, including the importance of parallel corpus
selection and size, and the relative effectiveness of
alternative tokenization methods. Size is not the
only important factor in corpus-based bilingual re-
3For example, the Biblical text does not contain the words
nuclear or energy and thus is greatly disadvantaged for a topic
about nuclear power.
27
(a) German (b) Spanish
(c) Finnish (d) French
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
????? ?????? ?????
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
????? ?????? ?????
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
?????? ?????? ??????
???
???
???
???
???
???
???
???
???
?? ?? ?? ?? ?? ???
????? ?????? ?????
Figure 1: Performance improvement with corpus growth.
trieval, the quality of alignments, compatibility in
genre, and choice of tokenization are also important.
We found that character 5-gram tokenization out-
performs words when used both for translation and
document indexing. Large relative improvements
(over 30%) were observed with 5-grams, and when
only limited parallel data is available for translation,
n-grams are markedly more effective than words.
Future work could address some limitations of the
present study by using bidirectional translation mod-
els, considering other language families and source
languages other than English, and applying query
expansion techniques.
References
Martin Braschler and Carol Peters. 2004. Cross-
language evaluation forum: Objectives, results,
achievements. Inf. Retr., 7(1-2):7?31.
P. A. Chew, S. J. Verzi, T. L. Bauer, and J. T. Mc-
Clain. 2006. Evaluation of the Bible as a resource for
cross-language information retrieval. In Workshop on
Multilingual Language Resources and Interoperabil-
ity, pages 68?74.
Kenneth Ward Church. 1993. Char align: A program for
aligning parallel texts at the character level. In Pro-
ceedings ACL, pages 1?8.
Dina Demner-Fushman and Douglas W. Oard. 2003.
The effect of bilingual term list size on dictionary-
based cross-language information retrieval. In HICSS,
pages 108?117.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings ACL, pages 177?184.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Paul McNamee and James Mayfield. 2005. Translating
pieces of words. In ACM SIGIR, pages 643?644.
Ari Pirkola, Deniz Puolama?ki, and Kalervo Ja?rvelin.
2003. Applying query structuring in cross-language
retrieval. Inf. Process. Manage, 39(3):391?402.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In ACM
SIGIR, pages 275?281.
Jianqiang Wang and Douglas W. Oard. 2006. Combin-
ing bidirectional translation and synonymy for cross-
language information retrieval. In ACM SIGIR, pages
202?209.
Jinxi Xu and Ralph Weischedel. 2000. Cross-lingual in-
formation retrieval using hidden Markov models. In
EMNLP, pages 85?103.
28
 	
  Named Entity Recognition using Hundreds of Thousands of Features
James Mayfield and Paul McNamee and Christine Piatko
The Johns Hopkins University Applied Physics Laboratory
11100 Johns Hopkins Road, Laurel, Maryland 20723-6099 USA
{mayfield,mcnamee,piatko}@jhuapl.edu
Abstract
We present an approach to named entity recog-
nition that uses support vector machines to cap-
ture transition probabilities in a lattice. The
support vector machines are trained with hun-
dreds of thousands of features drawn from the
CoNLL-2003 Shared Task training data. Mar-
gin outputs are converted to estimated prob-
abilities using a simple static function. Per-
formance is evaluated using the CoNLL-2003
Shared Task test set; Test B results were F?=1
= 84.67 for English, and F?=1 = 69.96 for Ger-
man.
1 Introduction
Language independence is difficult to achieve in named
entity recognition (NER) because different languages ap-
pear to require different features. Most NER systems (or
taggers) are severely limited in the number of features
they may consider, because the computational expense of
handling large numbers of features is high, and because
the risk of overtraining increases with the number of fea-
tures. Thus, the feature set must be finely tuned to be
effective. Such constrained feature sets are naturally lan-
guage dependent.
Increasing the number of features that a tagger can han-
dle would ameliorate this problem, because the designer
could select many relatively simple features in lieu of a
few highly tuned features. Because support vector ma-
chines (SVMs) (Vapnik, 1995) can handle large numbers
of parameters efficiently while simultaneously limiting
overtraining, they are good candidates for application to
named entity recognition. This paper proposes a novel
way to use SVMs for named entity recognition called
SVM-Lattice, describes a large feature space that we used
on the CoNLL-2003 Shared Task (Tjong Kim Sang and
De Meulder, 2003), and presents results from that task.
2 Model
We are interested in a lattice-based approach to named
entity recognition. In this approach, each sentence is pro-
cessed individually. A lattice is built with one column
per word of the sentence (plus a start state). Each column
contains one vertex for each possible tag. Each vertex in
one column is connected by an edge to every vertex in the
next column that may legitimately follow it (some tran-
sitions, such as from I-LOC to B-PER are disallowed).
Given such a lattice, our task is first to assign probabili-
ties to each of the arcs, then to find the highest likelihood
path through the lattice based on those probabilities. This
path corresponds to the highest likelihood tagging of the
sentence.
Hidden Markov models break the probability calcula-
tions into two pieces: transition probabilities (the proba-
bility of moving from one vertex to another independent
of the word at the destination node), and emission proba-
bilities (the probability that a given word would be gener-
ated from a certain state independent of the path taken to
get to that state). These probability distributions are cal-
culated separately because the training data are typically
too sparse to support a reasonable maximum likelihood
estimate of the joint probability. However, there is no
reason that these two distributions could not be combined
given a suitable estimation technique.
A support vector machine is a binary classifier that uses
supervised training to predict whether a given vector is in
a target class. All SVM training and test data occupy
a single high-dimensional vector space. In its simplest
form, training an SVM amounts to finding the hyperplane
that separates the positive training samples from the neg-
ative samples by the largest possible margin. This hyper-
plane is then used to classify the test vectors; those that
lie on one side of the hyperplane are classified as mem-
bers of the positive class, while others are classified as
members of the negative class. In addition to the clas-
sification decision, the SVM also produces a margin for
each vector?its distance from the hyperplane.
SVMs have two useful properties for our purposes.
First, they can handle very high dimensional spaces, as
long as individual vectors are sparse (i.e., each vector has
extent along only a small subset of the dimensions). Sec-
ondly, SVMs are resistant to overtraining, because only
the training vectors that are closest to the hyperplane
(called support vectors) dictate the parameters for the hy-
perplane. So SVMs would seem to be ideal candidates
for estimating lattice probabilities.
Unfortunately, SVMs do not produce probabilities, but
rather margins. In fact, one of the reasons that SVMs
work so well is precisely because they do not attempt to
model the entire distribution of training points. To use
SVMs in a lattice approach, then, a mechanism is needed
to estimate probability of category membership given a
margin.
Platt (1999) suggests such a method. If the range of
possible margins is partitioned into bins, and positive and
negative training vectors are placed into these bins, each
bin will have a certain percentage of positive examples.
These percentages can be approximated by a sigmoid
function: P (y = 1 | f) = 1/(1 + exp(Ax + b)). Platt
gives a simple iterative method for estimating sigmoid
parameters A and B, given a set of training vectors and
their margins.
This approach can work well if a sufficient number of
positive training vectors are available. Unfortunately, in
the CoNLL-2003 shared task, many of the possible label
transitions have few exemplars. Two methods are avail-
able to handle insufficient training data: smoothing, and
guessing.
In the smoothing approach, linear interpolation is used
to combine the model for the source to target pair that
lacks sufficient data with the model made from a com-
bination of all transitions going to the target label. For
example, we could smooth the probabilities derived for
the I-ORG to I-LOC transition with the probability that
any tag would transition to the I-LOC state at the same
point in the sentence.
The second approach is to guess at an appropriate
model without examining the training data. While in the-
ory this could prove to be a terrible approach, in practice
for the Shared Task, selection of fixed sigmoid parame-
ters works better than using Platt?s method to train the
parameters. Thus, we fix A = ?2 and b = 0. We con-
tinue to believe that Platt?s method or something like it
will ultimately lead to superior performance, but our cur-
rent experiments use this untrained model.
Our overall approach then is to use SVMs to estimate
lattice transition probabilities. First, due to the low fre-
quency of B-XXX tags in the training data, we convert
each B-XXX tags to the corresponding I-XXX tag; thus,
our system never predicts B-XXX tags. Then, we featur-
ize the training data, forming sparse vectors suitable for
input to our SVM package, SVMLight 5.00 (Joachims,
1999). Our feature set is described in the following sec-
tion. Next, we train one SVM for each transition type
seen in the training data. We used a cubic kernel for all
of our experiments; this kernel gives a consistent boost
over a linear kernel, while still training in a reasonable
amount of time. If we were to use Platt?s approach, the re-
sulting classifiers would be applied to further (preferably
held-out) training data to produce a set of margins, which
would be used to estimate appropriate sigmoid parame-
ters for each classifier. Sigmoid estimates that suffered
from too few positive input vectors would be replaced
by static estimates, and the sigmoids would optionally be
smoothed.
To evaluate a test set, the test input is featurized using
the same features as were used with the training data, re-
sulting in a separate vector for each word of the input.
Each classifier built during the training phase is then ap-
plied to each test vector to produce a margin. The margin
is mapped to a probability estimate using the static sig-
moid described above. When all of the probabilities have
been estimated and applied to the lattice, a Viterbi-like
algorithm is used to find the most likely path through the
lattice. This path identifies the final tag for each word of
the input sentence.
3 Features
The advantage of the ability to handle large numbers of
features is that we do not need to consider how well a
feature is likely to work in a particular language before
proposing it. We use the following features:
1. the word itself, both unchanged and lower-cased;
2. the character 3-grams and 4-grams that compose the
word;
3. the word?s capitalization pattern and digit pattern;
4. the inverse of the word?s length;
5. whether the word contains a dash;
6. whether the word is inside double quote marks;
7. the inverse of the word?s position in the sentence,
and of the position of that sentence in the document;
8. the POS, CHUNK and LEMMA features from the
training data;
9. whether the word is part of any entity, according
to a previous application of the TnT-Subcat tagger
(Brants, 2000) (see below) trained on the tag set {O,
I-ENTITY} (Test A F?=1 performance was 94.70
English and 74.33 German on this tag set); and
Run Description Test LOC MISC ORG PER Overall
1. Tnt Test A 86.67 79.60 73.04 88.54 82.90
Test B 81.28 68.98 65.71 82.84 75.54
2. Tnt + subcat Test A 91.46 81.41 80.63 91.64 87.49
Test B 85.71 68.41 73.82 87.95 80.68
3. SVM-Lattice Test A 92.14 84.86 83.70 93.73 89.63
Test B 87.09 72.81 78.84 90.40 83.92
4. SVM-Lattice+ Test A 93.75 86.02 85.90 93.91 90.85
Test B 88.77 74.19 79.00 90.67 84.67
Table 1: English evaluation results. F?=1 measures for subcategories, and overall.
Run Description Test LOC MISC ORG PER Overall
1. Tnt Test A 59.51 49.58 48.71 53.77 53.29
Test B 66.16 46.45 50.00 64.51 59.01
2. Tnt + subcat Test A 67.62 54.97 56.18 65.04 61.46
Test B 66.13 46.01 55.35 74.07 62.90
3. SVM-Lattice Test A 67.04 54.18 65.77 64.01 63.48
Test B 68.47 51.88 60.67 73.07 65.47
4. SVM-Lattice+ Test A 72.58 58.13 65.76 74.92 68.72
Test B 73.60 50.98 63.69 80.20 69.96
Table 2: German evaluation results. F?=1 measures for subcategories, and overall.
10. the maximum likelihood estimate, based on the
training data, of the word?s prior probability of being
in each class.
In some runs, we also use:
11. the tag assigned by a previous application of the
SVM-Lattice tagger, or by another tagger.
Each of these features is applied not just to the word
being featurized, but also to a range of words on either
side of it. We typically use a range of three (or, phrased
differently, a centered window of seven). We also ap-
plied some of these features to the environment of the
first occurrence of the word in the document. For ex-
ample, if the first occurrence of ?Bush? in the document
were followed by ?League,? then the second occurrence
of ?Bush? would receive the feature ?first-occurrence-is-
followed-by-league.?
Some values of the above features will be encountered
during testing but not during training. For example, a
word that occurs in the test set but not the training set will
lack a known value for the first feature in the list above.
To handle these cases, we assign any feature that appears
only once in the training data to a special ?never-before-
seen? class. This gives us examples at training time of
unseen features, which we can then train on.
Using the Shared Task English training data, this ap-
proach to featurization leads to a feature space of well
over 600,000 features, while the German data results in
over a million features. Individual vectors typically have
extent along a few hundred of these features.
There is a significant practical consideration in apply-
ing the method. The vectors produced by the featur-
izer for input to the SVM package are voluminous, lead-
ing to significant I/O costs, and slowing tag assignment.
Two methods might ameliorate this problem. First, sim-
ple compression techniques would be quite effective in
reducing file sizes, if the SVM package would support
them. Secondly, most vectors represent negative exam-
ples; a portion of these could probably be eliminated en-
tirely without significantly affecting system performance.
We have done no tuning of our feature set, preferring
to spend our time adding new features and relying on the
SVMs to ignore useless features. This is advantageous
when applying the technique to a language that we do
not understand (such as any of the world?s various non-
English languages).
4 Results
We evaluated our approach using the CoNLL-2003 En-
glish and German training and test sets, and the conll-
eval scoring software. We ran two baseline tests using
Thorsten Brants? TnT tagger (2000), and two tests of
SVM-Lattice:
1. TnT: The TnT tagger applied as distributed.
2. TnT+subcat: The TnT tagger applied to a refined
tag set. Each tag type was subcategorized into about
forty subtag types; each instance of a tag in the text
was then replaced by the appropriate subtag. For ex-
ample, a number (e.g., 221) that was part of a loca-
tion received an I-LOC-alldigits tag; a location with
an initial capital letter (e.g., Baker) received an I-
LOC-initcap tag; and one of the 30 most common
words (e.g., of) that was part of a location received a
(word-specific) I-LOC-of tag. This run served both
to calibrate the SVM-Lattice performance scores,
and to provide input for the SVM-Lattice+ run be-
low.
3. SVM-Lattice: Features 1-10 (listed above in the
Features section)
4. SVM-Lattice+: Features 1-11, using the output of
runs SVM-Lattice and TnT+subcat as input fea-
tures.
Scores for each English test are shown in Table 1; Ger-
man tests are shown in Table 2. Table 3 shows the re-
sults of the SVM-Lattice+ run in more detail. The results
show that the technique performs well, at least compared
with the baseline technique provided with the CoNLL-
2003 data (whose English Test B F?=1 measure is 59.61
English and 30.30 German).
5 Conclusion
The SVM-Lattice approach appears to give good results
without language-specific tuning; it handily outperforms
the CoNLL-2003 Shared Task baseline, and beats a basic
HMM tagger as well. Use of SVMs allows the introduc-
tion of a large number of features. These features can
be introduced with little concern for dependency among
features, and without significant knowledge of the target
language. It is likely that our results reflect some degree
of overfitting, given the large number of parameters we
use; however, we suspect this effect is not large. Thus,
the SVM-Lattice technique is particularly well suited to
language-neutral entity recognition. We expect it will
also perform well on other tasks that can be cast as tag-
ging problems, such as part-of-speech tagging and syn-
tactic chunking.
Acknowledgments
Significant theoretical and implementation contributions
were made to this work by Claudia Pearce, for which we
are grateful.
We gratefully acknowledge the provision of the
Reuters Corpus Vol. 1: English language, 1996-08-20
to 1997-08-19 by Reuters Limited.
English devel. Precision Recall F?=1
LOC 94.42% 93.09% 93.75
MISC 88.80% 83.41% 86.02
ORG 85.24% 86.58% 85.90
PER 92.79% 95.06% 93.91
overall 90.97% 90.73% 90.85
English test Precision Recall F?=1
LOC 88.22% 89.33% 88.77
MISC 74.89% 73.50% 74.19
ORG 79.31% 78.69% 79.00
PER 89.71% 91.65% 90.67
overall 84.45% 84.90% 84.67
German devel. Precision Recall F?=1
LOC 72.77% 72.40% 72.58
MISC 71.00% 49.21% 58.13
ORG 72.57% 60.11% 65.76
PER 83.70% 67.81% 74.92
overall 75.48% 63.07% 68.72
German test Precision Recall F?=1
LOC 75.08% 72.17% 73.60
MISC 63.62% 42.54% 50.98
ORG 69.20% 58.99% 63.69
PER 86.53% 74.73% 80.20
overall 75.97% 64.82% 69.96
Table 3: Results for the development and test evaluations
for the English and German tasks.
References
Thorsten Brants. 2000. TnT-A statistical part-of-speech
tagger. In Proceedings of ANLP-2000. Seattle, Wash-
ington.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In C. Burges B. Scho?lkopf and
A. Smola, editors, Support Vector Learning. MIT
Press.
John C. Platt. 1999. Probabilistic Outputs for Sup-
port Vector Machines and Comparisons to Regular-
ized Likelihood Methods. In B. Scholkopf A. Smola,
P. Bartlett and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT Press.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 Shared Task: Language
Independent Named Entity Recognition. In Proceed-
ings of CoNLL-2003. Edmonton, Canada.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277?285,
Beijing, August 2010
Entity Disambiguation for Knowledge Base Population
?Mark Dredze and ?Paul McNamee and ?Delip Rao and ?Adam Gerber and ?Tim Finin
?Human Language Technology Center of Excellence, Center for Language and Speech Processing
Johns Hopkins University
?University of Maryland ? Baltimore County
mdredze,mcnamee,delip,adam.gerber@jhu.edu, finin@umbc.edu
Abstract
The integration of facts derived from information extraction
systems into existing knowledge bases requires a system to
disambiguate entity mentions in the text. This is challeng-
ing due to issues such as non-uniform variations in entity
names, mention ambiguity, and entities absent from a knowl-
edge base. We present a state of the art system for entity dis-
ambiguation that not only addresses these challenges but also
scales to knowledge bases with several million entries using
very little resources. Further, our approach achieves perfor-
mance of up to 95% on entities mentioned from newswire
and 80% on a public test set that was designed to include
challenging queries.
1 Introduction
The ability to identify entities like people, orga-
nizations and geographic locations (Tjong Kim
Sang and De Meulder, 2003), extract their at-
tributes (Pasca, 2008), and identify entity rela-
tions (Banko and Etzioni, 2008) is useful for sev-
eral applications in natural language processing
and knowledge acquisition tasks like populating
structured knowledge bases (KB).
However, inserting extracted knowledge into a
KB is fraught with challenges arising from nat-
ural language ambiguity, textual inconsistencies,
and lack of world knowledge. To the discern-
ing human eye, the ?Bush? in ?Mr. Bush left
for the Zurich environment summit in Air Force
One.? is clearly the US president. Further con-
text may reveal it to be the 43rd president, George
W. Bush, and not the 41st president, George H.
W. Bush. The ability to disambiguate a polyse-
mous entity mention or infer that two orthograph-
ically different mentions are the same entity is
crucial in updating an entity?s KB record. This
task has been variously called entity disambigua-
tion, record linkage, or entity linking. When per-
formed without a KB, entity disambiguation is
called coreference resolution: entity mentions ei-
ther within the same document or across multi-
ple documents are clustered together, where each
cluster corresponds to a single real world entity.
The emergence of large scale publicly avail-
able KBs like Wikipedia and DBPedia has spurred
an interest in linking textual entity references to
their entries in these public KBs. Bunescu and
Pasca (2006) and Cucerzan (2007) presented im-
portant pioneering work in this area, but suffer
from several limitations including Wikipedia spe-
cific dependencies, scale, and the assumption of
a KB entry for each entity. In this work we in-
troduce an entity disambiguation system for link-
ing entities to corresponding Wikipedia pages de-
signed for open domains, where a large percent-
age of entities will not be linkable. Further, our
method and some of our features readily general-
ize to other curated KB. We adopt a supervised
approach, where each of the possible entities con-
tained within Wikipedia are scored for a match to
the query entity. We also describe techniques to
deal with large knowledge bases, like Wikipedia,
which contain millions of entries. Furthermore,
our system learns when to withhold a link when
an entity has no matching KB entry, a task that
has largely been neglected in prior research in
cross-document entity coreference. Our system
produces high quality predictions compared with
recent work on this task.
2 Related Work
The information extraction oeuvre has a gamut of
relation extraction methods for entities like per-
sons, organizations, and locations, which can be
classified as open- or closed-domain depending
on the restrictions on extractable relations (Banko
and Etzioni, 2008). Closed domain systems ex-
tract a fixed set of relations while in open-domain
systems, the number and type of relations are un-
bounded. Extracted relations still require process-
ing before they can populate a KB with facts:
namely, entity linking and disambiguation.
277
Motivated by ambiguity in personal name
search, Mann and Yarowsky (2003) disambiguate
person names using biographic facts, like birth
year, occupation and affiliation. When present
in text, biographic facts extracted using regular
expressions help disambiguation. More recently,
the Web People Search Task (Artiles et al, 2008)
clustered web pages for entity disambiguation.
The related task of cross document corefer-
ence resolution has been addressed by several
researchers starting from Bagga and Baldwin
(1998). Poesio et al (2008) built a cross document
coreference system using features from encyclo-
pedic sources like Wikipedia. However, success-
ful coreference resolution is insufficient for cor-
rect entity linking, as the coreference chain must
still be correctly mapped to the proper KB entry.
Previous work by Bunescu and Pasca (2006)
and Cucerzan (2007) aims to link entity men-
tions to their corresponding topic pages in
Wikipedia but the authors differ in their ap-
proaches. Cucerzan uses heuristic rules and
Wikipedia disambiguation markup to derive map-
pings from surface forms of entities to their
Wikipedia entries. For each entity in Wikipedia,
a context vector is derived as a prototype for the
entity and these vectors are compared (via dot-
product) with the context vectors of unknown en-
tity mentions. His work assumes that all entities
have a corresponding Wikipedia entry, but this as-
sumption fails for a significant number of entities
in news articles and even more for other genres,
like blogs. Bunescu and Pasca on the other hand
suggest a simple method to handle entities not in
Wikipedia by learning a threshold to decide if the
entity is not in Wikipedia. Both works mentioned
rely on Wikipedia-specific annotations, such as
category hierarchies and disambiguation links.
We just recently became aware of a system
fielded by Li et al at the TAC-KBP 2009 eval-
uation (2009). Their approach bears a number
of similarities to ours; both systems create candi-
date sets and then rank possibilities using differing
learning methods, but the principal difference is in
our approach to NIL prediction. Where we simply
consider absence (i.e., the NIL candidate) as an-
other entry to rank, and select the top-ranked op-
tion, they use a separate binary classifier to decide
whether their top prediction is correct, or whether
NIL should be output. We believe relying on fea-
tures that are designed to inform whether absence
is correct is the better alternative.
3 Entity Linking
We define entity linking as matching a textual en-
tity mention, possibly identified by a named en-
tity recognizer, to a KB entry, such as a Wikipedia
page that is a canonical entry for that entity. An
entity linking query is a request to link a textual
entity mention in a given document to an entry in
a KB. The system can either return a matching en-
try or NIL to indicate there is no matching entry.
In this work we focus on linking organizations,
geo-political entities and persons to a Wikipedia
derived KB.
3.1 Key Issues
There are 3 challenges to entity linking:
Name Variations. An entity often has multiple
mention forms, including abbreviations (Boston
Symphony Orchestra vs. BSO), shortened forms
(Osama Bin Laden vs. Bin Laden), alternate
spellings (Osama vs. Ussamah vs. Oussama),
and aliases (Osama Bin Laden vs. Sheikh Al-
Mujahid). Entity linking must find an entry de-
spite changes in the mention string.
Entity Ambiguity. A single mention, like
Springfield, can match multiple KB entries, as
many entity names, like people and organizations,
tend to be polysemous.
Absence. Processing large text collections vir-
tually guarantees that many entities will not ap-
pear in the KB (NIL), even for large KBs.
The combination of these challenges makes
entity linking especially challenging. Consider
an example of ?William Clinton.? Most read-
ers will immediately think of the 42nd US pres-
ident. However, the only two William Clintons in
Wikipedia are ?William de Clinton? the 1st Earl
of Huntingdon, and ?William Henry Clinton? the
British general. The page for the 42nd US pres-
ident is actually ?Bill Clinton?. An entity link-
ing system must decide if either of the William
Clintons are correct, even though neither are ex-
act matches. If the system determines neither
278
matches, should it return NIL or the variant ?Bill
Clinton?? If variants are acceptable, then perhaps
?Clinton, Iowa? or ?DeWitt Clinton? should be
acceptable answers?
3.2 Contributions
We address these entity linking challenges.
Robust Candidate Selection. Our system is
flexible enough to find name variants but suffi-
ciently restrictive to produce a manageable can-
didate list despite a large-scale KB.
Features for Entity Disambiguation. We de-
veloped a rich and extensible set of features based
on the entity mention, the source document, and
the KB entry. We use a machine learning ranker
to score each candidate.
Learning NILs. We modify the ranker to learn
NIL predictions, which obviates hand tuning and
importantly, admits use of additional features that
are indicative of NIL.
Our contributions differ from previous efforts
(Bunescu and Pasca, 2006; Cucerzan, 2007) in
several important ways. First, previous efforts de-
pend on Wikipedia markup for significant perfor-
mance gains. We make no such assumptions, al-
though we show that optional Wikipedia features
lead to a slight improvement. Second, Cucerzan
does not handle NILs while Bunescu and Pasca
address them by learning a threshold. Our ap-
proach learns to predict NIL in a more general
and direct way. Third, we develop a rich fea-
ture set for entity linking that can work with any
KB. Finally, we apply a novel finite state machine
method for learning name variations. 1
The remaining sections describe the candidate
selection system, features and ranking, and our
novel approach learning NILs, followed by an
empirical evaluation.
4 Candidate Selection for Name Variants
The first system component addresses the chal-
lenge of name variants. As the KB contains a large
number of entries (818,000 entities, of which 35%
are PER, ORG or GPE), we require an efficient se-
lection of the relevant candidates for a query.
Previous approaches used Wikipedia markup
for filtering ? only using the top-k page categories
1http://www.clsp.jhu.edu/ markus/fstrain
(Bunescu and Pasca, 2006) ? which is limited to
Wikipedia and does not work for general KBs.
We consider a KB independent approach to selec-
tion that also allows for tuning candidate set size.
This involves a linear pass over KB entry names
(Wikipedia page titles): a naive implementation
took two minutes per query. The following sec-
tion reduces this to under two seconds per query.
For a given query, the system selects KB entries
using the following approach:
? Titles that are exact matches for the mention.
? Titles that are wholly contained in or contain
the mention (e.g., Nationwide and Nationwide In-
surance).
? The first letters of the entity mention match the
KB entry title (e.g., OA and Olympic Airlines).
? The title matches a known alias for the entity
(aliases described in Section 5.2).
? The title has a strong string similarity score
with the entity mention. We include several mea-
sures of string similarity, including: character
Dice score > 0.9, skip bigram Dice score > 0.6,
and Hamming distance <= 2.
We did not optimize the thresholds for string
similarity, but these could obviously be tuned to
minimize the candidate sets and maximize recall.
All of the above features are general for any
KB. However, since our evaluation used a KB
derived from Wikipedia, we included a few
Wikipedia specific features. We added an entry if
its Wikipedia page appeared in the top 20 Google
results for a query.
On the training dataset (Section 7) the selection
system attained a recall of 98.8% and produced
candidate lists that were three to four orders of
magnitude smaller than the KB. Some recall er-
rors were due to inexact acronyms: ABC (Arab
Banking; ?Corporation? is missing), ASG (Abu
Sayyaf; ?Group? is missing), and PCF (French
Communist Party; French reverses the order of the
pre-nominal adjectives). We also missed Interna-
tional Police (Interpol) and Becks (David Beck-
ham; Mr. Beckham and his wife are collectively
referred to as ?Posh and Becks?).
279
4.1 Scaling Candidate Selection
Our previously described candidate selection re-
lied on a linear pass over the KB, but we seek
more efficient methods. We observed that the
above non-string similarity filters can be pre-
computed and stored in an index, and that the skip
bigram Dice score can be computed by indexing
the skip bigrams for each KB title. We omitted
the other string similarity scores, and collectively
these changes enable us to avoid a linear pass over
the KB. Finally we obtained speedups by serving
the KB concurrently2. Recall was nearly identical
to the full system described above: only two more
queries failed. Additionally, more than 95% of
the processing time was consumed by Dice score
computation, which was only required to cor-
rectly retrieve less than 4% of the training queries.
Omitting the Dice computation yielded results in
a few milliseconds. A related approach is that of
canopies for scaling clustering for large amounts
of bibliographic citations (McCallum et al, 2000).
In contrast, our setting focuses on alignment vs.
clustering mentions, for which overlapping parti-
tioning approaches like canopies are applicable.
5 Entity Linking as Ranking
We select a single correct candidate for a query
using a supervised machine learning ranker. We
represent each query by a D dimensional vector
x, where x ? RD, and we aim to select a sin-
gle KB entry y, where y ? Y , a set of possible
KB entries for this query produced by the selec-
tion system above, which ensures that Y is small.
The ith query is given by the pair {xi, yi}, where
we assume at most one correct KB entry.
To evaluate each candidate KB entry in Y we
create feature functions of the form f(x, y), de-
pendent on both the example x (document and en-
tity mention) and the KB entry y. The features
address name variants and entity disambiguation.
We take a maximum margin approach to learn-
ing: the correct KB entry y should receive a
higher score than all other possible KB entries
y? ? Y, y? 6= y plus some margin ?. This learning
2Our Python implementation with indexing features and
four threads achieved up to 80? speedup compared to naive
implementation.
constraint is equivalent to the ranking SVM algo-
rithm of Joachims (2002), where we define an or-
dered pair constraint for each of the incorrect KB
entries y? and the correct entry y. Training sets pa-
rameters such that score(y) ? score(y?) + ?. We
used the library SVMrank to solve this optimiza-
tion problem.3 We used a linear kernel, set the
slack parameter C as 0.01 times the number of
training examples, and take the loss function as
the total number of swapped pairs summed over
all training examples. While previous work used
a custom kernel, we found a linear kernel just as
effective with our features. This has the advan-
tage of efficiency in both training and prediction 4
? important considerations in a system meant to
scale to millions of KB entries.
5.1 Features for Entity Disambiguation
200 atomic features represent x based on each
candidate query/KB pair. Since we used a lin-
ear kernel, we explicitly combined certain fea-
tures (e.g., acroynym-match AND known-alias) to
model correlations. This included combining each
feature with the predicted type of the entity, al-
lowing the algorithm to learn prediction functions
specific to each entity type. With feature combina-
tions, the total number of features grew to 26,569.
The next sections provide an overview; for a de-
tailed list see McNamee et al (2009).
5.2 Features for Name Variants
Variation in entity name has long been recog-
nized as a bane for information extraction sys-
tems. Poor handling of entity name variants re-
sults in low recall. We describe several features
ranging from simple string match to finite state
transducer matching.
String Equality. If the query name and KB en-
try name are identical, this is a strong indication of
a match, and in our KB entry names are distinct.
However, similar or identical entry names that
refer to distinct entities are often qualified with
parenthetical expressions or short clauses. As
an example, ?London, Kentucky? is distinguished
3www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
4Bunescu and Pasca (2006) report learning tens of thou-
sands of support vectors with their ?taxonomy? kernel while
a linear kernel represents all support vectors with a single
weight vector, enabling faster training and prediction.
280
from ?London, Ontario?, ?London, Arkansas?,
?London (novel)?, and ?London?. Therefore,
other string equality features were used, such as
whether names are equivalent after some transfor-
mation. For example, ?Baltimore? and ?Baltimore
City? are exact matches after removing a common
GPE word like city; ?University of Vermont? and
?University of VT? match if VT is expanded.
Approximate String Matching. Many entity
mentions will not match full names exactly. We
added features for character Dice, skip bigram
Dice, and left and right Hamming distance scores.
Features were set based on quantized scores.
These were useful for detecting minor spelling
variations or mistakes. Features were also added if
the query was wholly contained in the entry name,
or vice-versa, which was useful for handling ellip-
sis (e.g., ?United States Department of Agricul-
ture? vs. ?Department of Agriculture?). We also
included the ratio of the recursive longest com-
mon subsequence (Christen, 2006) to the shorter
of the mention or entry name, which is effective at
handling some deletions or word reorderings (e.g.,
?Li Gong? and ?Gong Li?). Finally, we checked
whether all of the letters of the query are found in
the same order in the entry name (e.g., ?Univ Wis-
consin? would match ?University of Wisconsin?).
Acronyms. Features for acronyms, using dic-
tionaries and partial character matches, enable
matches between ?MIT? and ?Madras Institute of
Technology? or ?Ministry of Industry and Trade.?
Aliases. Many aliases or nicknames are non-
trivial to guess. For example JAVA is the
stock symbol for Sun Microsystems, and ?Gin-
ger Spice? is a stage name of Geri Halliwell. A
reasonable way to do this is to employ a dictio-
nary and alias lists that are commonly available
for many domains5.
FST Name Matching. Another measure of sur-
face similarity between a query and a candidate
was computed by training finite-state transducers
similar to those described in Dreyer et al (2008).
These transducers assign a score to any string pair
by summing over all alignments and scoring all
5We used multiple lists, including class-specific lists (i.e.,
for PER, ORG, and GPE) lists extracted from Freebase (Bol-
lacker et al, 2008) and Wikipedia redirects. PER, ORG, and
GPE are the commonly used terms for entity types for peo-
ple, organizations and geo-political regions respectively.
contained character n-grams; we used n-grams of
length 3 and less. The scores are combined using a
global log-linear model. Since different spellings
of a name may vary considerably in length (e.g.,
J Miller vs. Jennifer Miller) we eliminated the
limit on consecutive insertions used in previous
applications.6
5.3 Wikipedia Features
Most of our features do not depend on Wikipedia
markup, but it is reasonable to include features
from KB properties. Our feature ablation study
shows that dropping these features causes a small
but statistically significant performance drop.
WikiGraph statistics. We added features de-
rived from the Wikipedia graph structure for an
entry, like indegree of a node, outdegree of a node,
and Wikipedia page length in bytes. These statis-
tics favor common entity mentions over rare ones.
Wikitology. KB entries can be indexed with hu-
man or machine generated metadata consisting of
keywords or categories in a domain-appropriate
taxonomy. Using a system called Wikitology,
Syed et al (2008) investigated use of ontology
terms obtained from the explicit category system
in Wikipedia as well as relationships induced from
the hyperlink graph between related Wikipedia
pages. Following this approach we computed top-
ranked categories for the query documents and
used this information as features. If none of the
candidate KB entries had corresponding highly-
ranked Wikitology pages, we used this as a NIL
feature (Section 6.1).
5.4 Popularity
Although it may be an unsafe bias to give prefer-
ence to common entities, we find it helpful to pro-
vide estimates of entity popularity to our ranker
as others have done (Fader et al, 2009). Apart
from the graph-theoretic features derived from the
Wikipedia graph, we used Google?s PageRank to
by adding features indicating the rank of the KB
entry?s corresponding Wikipedia page in a Google
query for the target entity mention.
6Without such a limit, the objective function may diverge
for certain parameters of the model; we detect such cases and
learn to avoid them during training.
281
5.5 Document Features
The mention document and text associated with a
KB entry contain context for resolving ambiguity.
Entity Mentions. Some features were based on
presence of names in the text: whether the query
appeared in the KB text and the entry name in the
document. Additionally, we used a named-entity
tagger and relation finder, SERIF (Boschee et al,
2005), identified name and nominal mentions that
were deemed co-referent with the entity mention
in the document, and tested whether these nouns
were present in the KB text. Without the NE anal-
ysis, accuracy on non-NIL entities dropped 4.5%.
KB Facts. KB nodes contain infobox attributes
(or facts); we tested whether the fact text was
present in the query document, both locally to a
mention, or anywhere in the text. Although these
facts were derived from Wikipedia infoboxes,
they could be obtained from other sources as well.
Document Similarity We measured similarity
between the query document and the KB text in
two ways: cosine similarity with TF/IDF weight-
ing (Salton and McGill, 1983); and using the Dice
coefficient over bags of words. IDF values were
approximated using counts from the Google 5-
gram dataset as by Klein and Nelson (2008).
Entity Types. Since the KB contained types
for entries, we used these as features as well as
the predicted NE type for the entity mention in
the document text. Additionally, since only a
small number of KB entries had PER, ORG, or
GPE types, we also inferred types from Infobox
class information to attain 87% coverage in the
KB. This was helpful for discouraging selection
of eponymous entries named after famous enti-
ties (e.g., the former U.S. president vs. ?John F.
Kennedy International Airport?).
5.6 Feature Combinations
To take into account feature dependencies we cre-
ated combination features by taking the cross-
product of a small set of diverse features. The
attributes used as combination features included
entity type; a popularity based on Google?s rank-
ings; document comparison using TF/IDF; cov-
erage of co-referential nouns in the KB node
text; and name similarity. The combinations were
cascaded to allow arbitrary feature conjunctions.
Thus it is possible to end up with a feature kbtype-
is-ORG AND high-TFIDF-score AND low-name-
similarity. The combined features increased the
number of features from roughly 200 to 26,000.
6 Predicting NIL Mentions
So far we have assumed that each example has a
correct KB entry; however, when run over a large
corpus, such as news articles, we expect a signifi-
cant number of entities will not appear in the KB.
Hence it will be useful to predict NILs.
We learn when to predict NIL using the SVM
ranker by augmenting Y to include NIL, which
then has a single feature unique to NIL answers.
It can be shown that (modulo slack variables) this
is equivalent to learning a single threshold ? for
NIL predictions as in Bunescu and Pasca (2006).
Incorporating NIL into the ranker has several
advantages. First, the ranker can set the thresh-
old optimally without hand tuning. Second, since
the SVM scores are relative within a single exam-
ple and cannot be compared across examples, set-
ting a single threshold is difficult. Third, a thresh-
old sets a uniform standard across all examples,
whereas in practice we may have reasons to favor
a NIL prediction in a given example. We design
features for NIL prediction that cannot be cap-
tured in a single parameter.
6.1 NIL Features
Integrating NIL prediction into learning means
we can define arbitrary features indicative of NIL
predictions in the feature vector corresponding to
NIL. For example, if many candidates have good
name matches, it is likely that one of them is cor-
rect. Conversely, if no candidate has high entry-
text/article similarity, or overlap between facts
and the article text, it is likely that the entity is
absent from the KB. We included several features,
such as a) the max, mean, and difference between
max and mean for 7 atomic features for all KB
candidates considered, b) whether any of the can-
didate entries have matching names (exact and
fuzzy string matching), c) whether any KB en-
try was a top Wikitology match, and d) if the top
Google match was not a candidate.
282
Micro-Averaged Macro-Averaged
Best Median All Features Best Features Best Median All Features Best Features
All 0.8217 0.7108 0.7984 0.7941 0.7704 0.6861 0.7695 0.7704
non-NIL 0.7725 0.6352 0.7063 0.6639 0.6696 0.5335 0.6097 0.5593
NIL 0.8919 0.7891 0.8677 0.8919 0.8789 0.7446 0.8464 0.8721
Table 1: Micro and macro-averaged accuracy for TAC-KBP data compared to best and median reported performance.
Results are shown for all features as well as removing a small number of features using feature selection on development data.
7 Evaluation
We evaluated our system on two datasets: the
Text Analysis Conference (TAC) track on Knowl-
edge Base Population (TAC-KBP) (McNamee and
Dang, 2009) and the newswire data used by
Cucerzan (2007) (Microsoft News Data).
Since our approach relies on supervised learn-
ing, we begin by constructing our own training
corpus.7 We highlighted 1496 named entity men-
tions in news documents (from the TAC-KBP doc-
ument collection) and linked these to entries in
a KB derived from Wikipedia infoboxes. 8 We
added to this collection 119 sample queries from
the TAC-KBP data. The total of 1615 training ex-
amples included 539 (33.4%) PER, 618 (38.3%)
ORG, and 458 (28.4%) GPE entity mentions. Of
the training examples, 80.5% were found in the
KB, matching 300 unique entities. This set has a
higher number of NIL entities than did Bunescu
and Pasca (2006) (10%) but lower than the TAC-
KBP test set (43%).
All system development was done using a train
(908 examples) and development (707 examples)
split. The TAC-KBP and Microsoft News data
sets were held out for final tests. A model trained
on all 1615 examples was used for experiments.
7.1 TAC-KBP 2009 Experiments
The KB is derived from English Wikipedia pages
that contained an infobox. Entries contain basic
descriptions (article text) and attributes. The TAC-
KBP query set contains 3904 entity mentions for
560 distinct entities; entity type was only provided
for evaluation. The majority of queries were for
organizations (69%). Most queries were missing
from the KB (57%). 77% of the distinct GPEs
in the queries were present in the KB, but for
7Data available from www.dredze.com
8http://en.wikipedia.org/wiki/Help:Infobox
PERs and ORGs these percentages were signifi-
cantly lower, 19% and 30% respectively.
Table 1 shows results on TAC-KBP data us-
ing all of our features as well a subset of features
based on feature selection experiments on devel-
opment data. We include scores for both micro-
averaged accuracy ? averaged over all queries
? and macro-averaged accuracy ? averaged over
each unique entity ? as well as the best and me-
dian reported results for these data (McNamee
and Dang, 2009). We obtained the best reported
results for macro-averaged accuracy, as well as
the best results for NIL detection with micro-
averaged accuracy, which shows the advantage of
our approach to learning NIL. See McNamee et
al. (2009) for additional experiments.
The candidate selection phase obtained a re-
call of 98.6%, similar to that of development data.
Missed candidates included Iron Lady, which
refers metaphorically to Yulia Tymoshenko, PCC,
the Spanish-origin acronym for the Cuban Com-
munist Party, and Queen City, a former nickname
for the city of Seattle, Washington. The system re-
turned a mean of 76 candidates per query, but the
median was 15 and the maximum 2772 (Texas). In
about 10% of cases there were four or fewer can-
didates and in 10% of cases there were more than
100 candidate KB nodes. We observed that ORGs
were more difficult, due to the greater variation
and complexity in their naming, and that they can
be named after persons or locations.
7.2 Feature Effectiveness
We performed two feature analyses on the TAC-
KBP data: an additive study ? starting from a
small baseline feature set used in candidate selec-
tion we add feature groups and measure perfor-
mance changes (omitting feature combinations),
and an ablative study ? starting from all features,
remove a feature group and measure performance.
283
Class All non-NIL NIL
Baseline 0.7264 0.4621 0.9251
Acronyms 0.7316 0.4860 0.9161
NE Analysis 0.7661 0.7181 0.8022
Google 0.7597 0.7421 0.7730
Doc/KB Text Similarity 0.7313 0.6699 0.7775
Wikitology 0.7318 0.4549 0.9399
All 0.7984 0.7063 0.8677
Table 2: Additive analysis: micro-averaged accuracy.
Table 2 shows the most significant features in
the feature addition experiments. The baseline
includes only features based on string similarity
or aliases and is not effective at finding correct
entries and strongly favors NIL predictions. In-
clusion of features based on analysis of named-
entities, popularity measures (e.g., Google rank-
ings), and text comparisons provided the largest
gains. The overall changes are fairly small,
roughly ?1%; however changes in non-NIL pre-
cision are larger.
The ablation study showed considerable redun-
dancy across feature groupings. In several cases,
performance could have been slightly improved
by removing features. Removing all feature com-
binations would have improved overall perfor-
mance to 81.05% by gaining on non-NIL for a
small decline on NIL detection.
7.3 Experiments on Microsoft News Data
We downloaded the evaluation data used in
Cucerzan (2007)9: 20 news stories from MSNBC
with 642 entity mentions manually linked to
Wikipedia and another 113 mentions not having
any corresponding link to Wikipedia.10 A sig-
nificant percentage of queries were not of type
PER, ORG, or GPE (e.g., ?Christmas?). SERIF
assigned entity types and we removed 297 queries
not recognized as entities (counts in Table 3).
We learned a new model on the training data
above using a reduced feature set to increase
speed.11 Using our fast candidate selection sys-
tem, we resolved each query in 1.98 seconds (me-
dian). Query processing time was proportional to
9http://research.microsoft.com/en-us/um/people/silviu/WebAssistant/TestData/
10One of the MSNBC news articles is no longer available
so we used 759 total entities.
11We removed Google, FST and conjunction features
which reduced system accuracy but increased performance.
Num. Queries Accuracy
Total Nil All non-NIL NIL
NIL 452 187 0.4137 0.0 1.0
GPE 132 20 0.9696 1.00 0.8000
ORG 115 45 0.8348 0.7286 1.00
PER 205 122 0.9951 0.9880 1.00
All 452 187 0.9469 0.9245 0.9786
Cucerzan (2007) 0.914 - -
Table 3: Micro-average results for Microsoft data.
the number of candidates considered. We selected
a median of 13 candidates for PER, 12 for ORG
and 102 for GPE. Accuracy results are in Table
3. The high results reported for this dataset over
TAC-KBP is primarily because we perform very
well in predicting popular and rare entries ? both
of which are common in newswire text.
One issue with our KB was that it was derived
from infoboxes in Wikipedia?s Oct 2008 version
which has both new entities, 12 and is missing en-
tities.13 Therefore, we manually confirmed NIL
answers and new answers for queries marked as
NIL in the data. While an exact comparison is not
possible (as described above), our results (94.7%)
appear to be at least on par with Cucerzan?s sys-
tem (91.4% overall accuracy).With the strong re-
sults on TAC-KBP, we believe that this is strong
confirmation of the effectiveness of our approach.
8 Conclusion
We presented a state of the art system to disam-
biguate entity mentions in text and link them to
a knowledge base. Unlike previous approaches,
our approach readily ports to KBs other than
Wikipedia. We described several important chal-
lenges in the entity linking task including han-
dling variations in entity names, ambiguity in en-
tity mentions, and missing entities in the KB, and
we showed how to each of these can be addressed.
We described a comprehensive feature set to ac-
complish this task in a supervised setting. Impor-
tantly, our method discriminately learns when not
to link with high accuracy. To spur further re-
search in these areas we are releasing our entity
linking system.
122008 vs. 2006 version used in Cucerzan (2007) We
could not get the 2006 version from the author or the Internet.
13Since our KB was derived from infoboxes, entities not
having an infobox were left out.
284
References
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalu-
ation and the plan for the second. In WWW.
Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Association for Computational Linguistics.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In SIGMOD Management of Data.
E. Boschee, R. Weischedel, and A. Zamanian. 2005.
Automatic information extraction. In Conference
on Intelligence Analysis.
Razvan C. Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In European Chapter of the Assocation
for Computational Linguistics (EACL).
Peter Christen. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Australian National Uni-
versity.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Empirical Methods in
Natural Language Processing (EMNLP).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity dis-
ambiguation to arbitrary web text. In WikiAI09
Workshop at IJCAI 2009.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Knowledge Discovery
and Data Mining (KDD).
Martin Klein and Michael L. Nelson. 2008. A com-
parison of techniques for estimating IDF values to
generate lexical signatures for the web. In Work-
shop on Web Information and Data Management
(WIDM).
Fangtao Li, Zhicheng Zhang, Fan Bu, Yang Tang,
Xiaoyan Zhu, and Minlie Huang. 2009. THU
QUANTA at TAC 2009 KBP and RTE track. In Text
Analysis Conference (TAC).
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Conference
on Natural Language Learning (CONLL).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In
Knowledge Discovery and Data Mining (KDD).
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Text Analysis Conference (TAC).
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus
Dreyer. 2009. HLTCOE approaches to knowledge
base population at TAC 2009. In Text Analysis Con-
ference (TAC).
Marius Pasca. 2008. Turning web text and search
queries into factual knowledge: hierarchical class
attribute extraction. In National Conference on Ar-
tificial Intelligence (AAAI).
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob
Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
Stanley Yong, Wai Keong, Gideon Mann, Alessan-
dro Moschitti, Simone Ponzetto, Jason Smith, Josef
Steinberger, Michael Strube, Jian Su, Yannick Ver-
sley, Xiaofeng Yang, and Michael Wick. 2008. Ex-
ploiting lexical and encyclopedic resources for en-
tity disambiguation: Final report. Technical report,
JHU CLSP 2007 Summer Workshop.
Gerard Salton and Michael McGill. 1983. Introduc-
tion to Modern Information Retrieval. McGraw-
Hill Book Company.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Confer-
ence on Natural Language Learning (CONLL).
Zareen Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents.
In Proceedings of the Second International Confer-
ence on Weblogs and Social Media. AAAI Press.
285
Coling 2010: Poster Volume, pages 1050?1058,
Beijing, August 2010
Streaming Cross Document Entity Coreference Resolution
Delip Rao and Paul McNamee and Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University
delip,mcnamee,mdredze@jhu.edu
Abstract
Previous research in cross-document en-
tity coreference has generally been re-
stricted to the offline scenario where the
set of documents is provided in advance.
As a consequence, the dominant approach
is based on greedy agglomerative cluster-
ing techniques that utilize pairwise vec-
tor comparisons and thus require O(n2)
space and time. In this paper we ex-
plore identifying coreferent entity men-
tions across documents in high-volume
streaming text, including methods for uti-
lizing orthographic and contextual infor-
mation. We test our methods using several
corpora to quantitatively measure both the
efficacy and scalability of our streaming
approach. We show that our approach
scales to at least an order of magnitude
larger data than previous reported meth-
ods.
1 Introduction
A key capability for successful information ex-
traction, topic detection and tracking, and ques-
tion answering is the ability to identify equiva-
lence classes of entity mentions. An entity is a
real-world person, place, organization, or object,
such as the person who serves as the 44th pres-
ident of the United States. An entity mention is
a string which refers to such an entity, such as
?Barack Hussein Obama?, ?Senator Obama? or
?President Obama?. The goal of coreference res-
olution is to identify and connect all textual entity
mentions that refer to the same entity.
The first step towards this goal is to identify all
references within the same document, or within
document coreference resolution. A document of-
ten has a leading canonical reference to the entity
(?Barack Obama?) followed by additional expres-
sions for the same entity (?President Obama.?)
An intra-document coreference system must first
identify each reference, often relying on named
entity recognition, and then decide if these refer-
ences refer to a single individual or multiple enti-
ties, creating a coreference chain for each unique
entity. Feature representations include surface
form similarity, lexical context of mentions, po-
sition in the document and distance between ref-
erences. A variety of statistical learning meth-
ods have been applied to this problem, including
use of decision trees (Soon et al, 2001; Ng and
Cardie, 2002), graph partitioning (Nicolae and
Nicolae, 2006), maximum-entropy models (Luo
et al, 2004), and conditional random fields (Choi
and Cardie, 2007).
Given pre-processed documents, in which enti-
ties have been identified and entity mentions have
been linked into chains, we seek to identify across
an entire document collection all chains that re-
fer to the same entity. This task is called cross
document coreference resolution (CDCR). Sev-
eral of the challenges associated with CDCR dif-
fer from the within document task. For example,
it is unlikely that the same document will discuss
John Phillips the American football player and
John Phillips the musician, but it is quite proba-
ble that documents discussing each will appear in
the same collection. Therefore, while matching
entities with the same mention string can work
well for within document coreference, more so-
phisticated approaches are necessary for the cross
document scenario where a one-entity-per-name
assumption is unreasonable.
One of the most common approaches to both
within document and cross document corefer-
ence resolution has been based on agglomerative
clustering, where vectors might be bag-of-word
contexts (Bagga and Baldwin, 1998; Mann and
1050
Yarowsky, 2003; Gooi and Allan, 2004; Chen
and Martin, 2007). These algorithms creates a
O(n2) dependence in the number of mentions ?
for within document ? and documents ? for cross
document. This is a reasonable limitation for
within document, since the number of references
will certainly be small; we are unlikely to en-
counter a document with millions of references.
In contrast to the small n encountered within a
document, we fully expect to run a CDCR sys-
tem on hundreds of thousands or millions of doc-
uments. Most previous approaches cannot handle
collections of this size.
In this work, we present a new method for
cross document coreference resolution that scales
to very large corpora. Our algorithm operates in
a streaming setting, in which documents are pro-
cessed one at a time and only a single time. This
creates a linear (O(n)) dependence on the num-
ber of documents in the collection, allowing us
to scale to millions of documents and millions
of unique entities. Our algorithm uses stream-
ing clustering with common coreference similar-
ity computations to achieve large scale. Further-
more, our method is designed to support both
name disambiguation and name variation.
In the next section, we give a survey of related
work. In Section 3 we detail our streaming setup,
giving a description of the streaming algorithm
and presenting efficient techniques for represent-
ing clusters over streams and for computing simi-
larity. Section 4 describes the data sets on which
we evaluate our methods and presents results. We
conclude with a discussion and description of on-
going work.
2 Related Work
Traditional approaches to cross document coref-
erence resolution have first constructed a vector
space representation derived from local (or global)
contexts of entity mentions in documents and then
performed some form of clustering on these vec-
tors. This is a simple extension of Firth?s distribu-
tional hypothesis applied to entities (Firth, 1957).
We describe some of the seminal work in this area.
Some of the earliest work in CDCR was by
Bagga and Baldwin (1998). Key contributions
of their research include: promotion of a set-
theoretic evaluation measure, B-CUBED; intro-
duction of a data set based on 197 New York
Times articles which mention a person named
John Smith; and, use of TF/IDF weighted vec-
tors and cosine similarity in single-link greedy ag-
glomerative clustering.
Mann and Yarowsky (2003) extended Bagga
and Baldwin?s work and contributed several inno-
vations, including: use of biographical attributes
(e.g., year of birth, occupation), and evaluation us-
ing pseudonames. Pseudonames are sets of artifi-
cially conflated names that are used as an efficient
method for producing a set of gold-standard dis-
ambiguations.1 Mann and Yarowsky used 4 pairs
of conflated names in their evaluation. Their sys-
tem did not perform as well on named entities with
little available biographic information.
Gooi and Allan (2004) expanded on the use
of pseudonames by semi-automatically creating
a much larger evaluation set, which they called
the ?Person-X? corpus. They relied on automated
named-entity tagging and domain-focused text re-
trieval. This data consisted of 34,404 documents
where a single person mention in each document
was rewritten as ?Person X?. Besides their novel
construction of a large-scale resource, they in-
vestigated several minor variations in clustering,
namely (a) use of Kullback-Leibler divergence as
a distance measure, (b) use of 55-word snippets
around entity mentions (vs. entire documents or
extracted sentences), and (c) scoring clusters us-
ing average-link instead of single- or complete-
link.
Finally, in more recent work, Chen and Martin
(2007) explore the CDCR task in both English and
Chinese. Their work focuses on use of both lo-
cal, and document-level noun-phrases as features
in their vector-space representation.
There have been a number of open evaluations
of CDCR systems. For example, the Web People
Search (WePS) workshops (Artiles et al, 2008)
have created a task for disambiguating personal
names from HTML pages. A set of ambiguous
names is chosen and each is submitted to a popular
web search engine. The top 100 pages are then
manually clustered.We discuss several other data
1See Sanderson (2000) for use of this technique in word
sense disambiguation.
1051
sets in Section 4.2
All of the papers mentioned above focus on dis-
ambiguating personal names. In contrast, our sys-
tem can also handle organizations and locations.
Also, as was mentioned earlier, we are commit-
ted to a scenario where documents are presented
in sequence and entities must be disambiguated
instantly, without the benefit of observing the en-
tire corpus. We believe that such a system is bet-
ter suited to highly dynamic environments such as
daily news feeds, blogs, and tweets. Additionally,
a streaming system exposes a set of known entity
clusters after each document is processed instead
of waiting until the end of the stream.
3 Approach
Our cross document coreference resolution sys-
tem relies on a streaming clustering algorithm
and efficient calculation of similarity scores. We
assume that we receive a stream of corefer-
ence chains, along with entity types, as they
are extracted from documents. We use SERIF
(Ramshaw and Weischedel, 2005), a state of the
art document analysis system which performs
intra-document coreference resolution. BBN de-
veloped SERIF to address information extraction
tasks in the ACE program and it is further de-
scribed in Pradhan et al (2007).
Each unique entity is represented by an entity
cluster c, comprised of entity chains from many
documents that refer to the same entity. Given
an entity coreference chain e, we identify the best
known entity cluster c. If a suitable entity cluster
is not found, a new entity cluster is formed.
An entity cluster is selected for a given corefer-
ence chain using several similarity scores, includ-
ing document context, predicted entity type, and
orthographic similarity between the entity men-
tion and previously discovered references in the
entity cluster. An efficient implementation of the
similarity score allows the system to identify the
top k most likely mentions without considering all
m entity clusters. The final output of our sys-
tem is a collection of entity clusters, each con-
taining a list of coreference chains and their doc-
uments. Additionally, due to its streaming nature,
2We preferred other data sets to the WePS data in our
evaluation because it is not easily placed in temporal order.
the system can be examined at any time to produce
this information based on only the documents that
have been processed thus far.
In the next sections, we describe both the clus-
tering algorithm and efficient computation of the
entity similarity scores.
3.1 Clustering Algorithm
We use a streaming clustering algorithm to cre-
ate entity clusters as follows. We observe a set
of points from a potentially infinite set X , one at
a time, and would like to maintain a fixed number
of clusters while minimizing the maximum cluster
radius, defined as the radius of the smallest ball
containing all points of the cluster. This setup is
well known in the theory and information retrieval
community and is referred to as the dynamic clus-
tering problem (Can and Ozkarahan, 1987).
Others have attempted to use an incremen-
tal clustering approach, such as Gooi and Al-
lan (2004) (who eventually prefer a hierarchi-
cal clustering approach), and Luo et al (2004),
who use a Bell tree approach for incrementally
clustering within document entity mentions. Our
work closely follows the Doubling Algorithm of
Charikar et al (1997), which has better perfor-
mance guarantees for streaming data. Streaming
clustering means potentially linear performance in
the number of observations since each document
need only be examined a single time, as opposed
to the quadratic cost of agglomerative clustering.3
The Doubling Algorithm consists of two stages:
update and merge. Update adds points to existing
clusters or creates new clusters while merge com-
bines clusters to prevent the clusters from exceed-
ing a fixed limit. New clusters are created accord-
ing to a threshold set using development data. We
selected a threshold of 0.5 since it worked well in
preliminary experiments. Since the number of en-
tities grows with time, we have skipped the merge
step in our initial experiments so as not to limit
cluster growth.
We use a dynamic caching scheme which backs
the actual clusters in a disk based index, but re-
3It is possible to implement hierarchical agglomerative
clustering in O(n logm) time where n is the number of
points and m in the number of clusters. However this is still
superlinear and expensive in situations where m continually
increases like in streaming coreference resolution.
1052
1 2 3 4 5 6log(Rank)0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
log(
Fre
que
ncy
)
PERLOCORG
Figure 1: Frequency vs. rank for 567k people,
136k organizations, and 25k locations in the New
York Times Annotated Corpus (Sandhaus, 2008).
tains basic cluster information in memory (see be-
low). Doing so improves paging performance as
observed in Omiecinski and Scheuermann (1984).
Motivated by the Zipfian distribution of named en-
tities in news sources (Figure 1), we organize our
cluster store using an LRU policy, which facili-
tates easy access to named entities that were ob-
served in the recent past. We obtain additional
performance gains by hashing the clusters based
on the constituent mention string (details below).
This allows us to quickly retrieve a small but re-
lated number of clusters, k. It is always the case
that k << m, the current number of clusters.
3.2 Candidate Cluster Selection
As part of any clustering algorithm, each new item
must be compared against current clusters. As we
see more documents, the number of unique clus-
ters (entities) grows. Therefore, we need efficient
methods to select candidate clusters.
To select the top candidate clusters, we obtain
those that have high orthographic similarity with
the head name mention in the coreference chain e.
We compute this similarity using the dice score on
either word unigrams or character skip bigrams.
For each entity mention string associated with a
cluster c, we generate all possible n-grams using
one of the above two policies. We then index the
cluster by each of its n-grams in a hash maintained
in memory. In addition, we keep the number of n-
grams generated for each cluster.
When given a new head mention e for a coref-
erence chain, we generate all of the n-grams and
look up clusters that contain these n-grams using
the hash. We then compute the dice score:
dice(e, c) = |{ngram(e)} ? {ngram(c)}||{ngram(e)} ? {ngram(c)}| ,
where {ngram(e)} are the set of n-grams in entity
mention e and {ngram(c)} are the set of n-grams
for all entity mentions in cluster c. Note that we
can calculate the numerator (the intersection) by
looking up the n-grams of e in the hash and count-
ing matches with c. The denominator is equivalent
to the number of n-grams unique to e and to c plus
the number that are shared. The number that are
shared is the intersection. The number unique to
e is the total number of n-grams in e minus the in-
tersection. The final term, the number unique to c,
is computed by taking the total number of n-grams
in c (a single integer stored in memory) minus the
intersection.
Through this strategy, we can select only those
clusters that have the highest orthographic simi-
larity to e without requiring the cluster contents,
which may not be stored in memory. In our exper-
iments, we evaluate settings where we select all
candidates with non-zero score and a pruned set of
the top k dice score candidates. We also include
in the n-gram list known aliases to facilitate or-
thographically dissimilar, but reasonable matches
(e.g., IBM or ?Big Blue? for ?International Busi-
ness Machines, Inc.?).4
For further efficiency, we keep separate caches
for each named entity type.5 We then select the
appropriate cache based on the automatically de-
termined type of the named entity provided by the
named entity tagger, which also prevents spurious
matches of non-matching entity types.
3.3 Similarity Metric
After filtering by orthographic information to
quickly obtain a small set of candidate clusters,
a full similarity score is computed for the current
4We generated alias lists for entities from Freebase.
5Persons (PER), organizations (ORG), and locations
(LOC).
1053
entity coreference chain and each retrieved can-
didate cluster. These computations require infor-
mation about each cluster, so the cluster?s suffi-
cient statistics are loaded using the LRU cache de-
scribed above.
We define several similarity metrics between
coreference chains and clusters to deal with both
name variation and disambiguation. For name
variation, we define an orthographic similarity
metric to match similar entity mention strings. As
before, we use word unigrams and character skip
bigrams. For each of these methods, we compute
a similarity score as dice(e, c) and select the high-
est scoring cluster.
To address name disambiguation, we use two
types of context from the document. First, we use
lexical features represented as TF/IDF weighted
vectors. Second, we consider topic features, in
which each word in a document is replaced with
the topic inferred from a topic model. This yields
a distribution over topics for a given document.
We use an LDA (Blei et al, 2003) model trained
on the New York Times Annotated Corpus (Sand-
haus, 2008). We note that LDA can be computed
over streams (Yao et al, 2009).
To compare context vectors we use cosine sim-
ilarity, where the cluster vector is the average of
all document vectors assigned to the cluster. Note
that the filtering step in Section 3.2 returns only
those candidates with some orthographic similar-
ity with the coreference chain, so a similarity met-
ric that uses context only is still restricted to ortho-
graphically similar entities.
Finally, we consider a combination of ortho-
graphic and context similarity as a linear combi-
nation of the two metrics as:
score(e, c) = ? dice(e, c) + (1? ?)cosine(e, c) .
We set ? = 0.8 based on initial experiments.
4 Evaluation
We used several corpora to evaluate our meth-
ods, including two data sets commonly used in the
coreference community. We also created a new
test set using artificially conflated names. And fi-
nally to test scalability, we ran our algorithm over
a large text collection that, while it did not have
Attribute smith nytac ace08 kbp09
Total Documents 197 1.85M 10k 1.2M
Annotated Docs 197 19,360 415 **
Annotated Entities 35 200 3,943 **
Table 1: Data sets used in our experiments. For
the kbp09 data we did not have annotations.
ground truth entity clusters, was useful for com-
puting other performance statistics. Properties for
each data set are given in Table 1.
4.1 John Smith corpus
Bagga and Baldwin (1998) evaluated their disam-
biguation system on a set of 197 articles from the
New York Times that mention a person named
?John Smith?. This data exhibits no name variants
and is strictly a disambiguation task. We include
this data (smith) to allow comparison to previous
work.
4.2 NYTAC Pseudo-name corpus
To study the effects of word sense ambiguity
and disambiguation several researchers have ar-
tificially conflated dissimilar words together and
then attempted to disambiguate them (Sanderson,
2000). The obvious advantage is cheaply obtained
ground truth for disambiguation.
The same trick has also been employed in per-
son name disambiguation (Mann and Yarowsky,
2003; Gooi and Allan, 2004). We adopt the same
method on a somewhat larger scale using annota-
tions from the New York Times Annotated Corpus
(NYTAC) (Sandhaus, 2008), which annotates doc-
uments based on whether or not they mention an
entity. The NYTAC data contains documents from
20 years of the New York Times and contains rich
metadata and document-level annotations that in-
dicate when an entity is mentioned in the docu-
ment using a standard lexicon of entities. (Note
that mention strings are not tagged.) Using these
annotations we created a set of 100 pairs of con-
flated person names.
The names were selected to be medium fre-
quency (i.e., occurring in between 50 and 200 ar-
ticles) and each pair matches in gender. The first
50 pairs are for names that are topically similar,
for example, Tim Robbins and Tom Hanks (both
actors); Barbara Boxer and Olympia Snowe (both
1054
smith nytac ace08
Approach P R F P R F P R F
Baseline 1.000 0.178 0.302 1.000 0.010 0.020 1.000 0.569 0.725
ExactMatch 0.233 1.000 0.377 0.563 0.897 0.692 0.977 0.697 0.814
Ortho 0.603 0.629 0.616 0.611 0.784 0.687 0.975 0.694 0.811
BoW 0.956 0.367 0.530 0.930 0.249 0.349 0.989 0.589 0.738
Topic 0.847 0.592 0.697 0.815 0.244 0.363 0.983 0.605 0.750
Ortho+BoW 0.603 0.634 0.618 0.801 0.601 0.686 0.976 0.691 0.809
Ortho+Topic 0.603 0.634 0.618 0.800 0.591 0.680 0.975 0.704 0.819
Table 2: Best B3 performance on the smith, nytac, and ace08 test sets.
US politicians). We imagined that this would be
a more challenging subset because of presumed
lexical overlap. The second set of 50 name pairs
were arbitrarily conflated. We sub-selected the
data to ensure that no two entities in our collec-
tion co-occur in the same document and this left
us with 19,360 documents for which ground-truth
was known. In each document we rewrote the
conflated name mentions using a single gender-
neutral name; any middle initials or names were
discarded.
4.3 ACE 2008 corpus
The NIST ACE 2008 (ace08) evaluation studied
several related technologies for information ex-
traction, including named-entity recognition, re-
lation extraction, and cross-document coreference
for person names in both English and Arabic. Ap-
proximately 10,000 documents from several gen-
res (predominantly newswire) were given to par-
ticipants, who were expected to cluster person and
organization entities across the entire collection.
However, only a selected set of about 400 docu-
ments were annotated and used to evaluate sys-
tem performance. Baron and Freedman (2008)
describe their work in this evaluation, which in-
cluded a separate task for within-document coref-
erence.
4.4 TAC-KBP 2009 corpus
The NIST TAC 2009 Knowledge Base Popula-
tion track (kbp09) (McNamee and Dang, 2009)
conducted an evaluation of a system?s ability to
link entity mentions to corresponding Wikipedia-
derived knowledge base nodes. The TAC-KBP
task focused on ambiguous person, organization,
and geo-political entities mentioned in newswire,
and required systems to cope with name variation
(e.g., ?Osama Bin Laden? / ?Usama Bin Laden?
or ?Mark Twain? / ?Samuel Clemens?) as well as
name disambiguation. Furthermore, the task re-
quired detection of when no appropriate KB entry
exists, which is a departure from the conventional
disambiguation problem. The collection contains
over 1.2 million documents, primarily newswire.
Wikipedia was used as a surrogate knowledge
base, and it has been used in several previous stud-
ies (e.g., Cucerzan (2007)). This task is closely re-
lated to CDCR, as mentions that are aligned to the
same knowledge base entry create a coreference
cluster. However, there are no actual CDCR anno-
tations for this corpus, though we used it nonethe-
les as a benchmark corpus to evaluate speed and
to demonstrate scalability.
5 Discussion
5.1 Accuracy
In Table 2 we report cross document coreference
resolution performance for a variety of experi-
mental conditions using the B3 method, which
includes precision, recall, and calculated F?=1
values. For each of the three evaluation corpora
(smith, nytac, and ace08) we report values for two
baseline methods and for similarity metrics us-
ing different types of features. The first baseline,
called Baseline, places each coreference chain in
its own cluster while the second baseline, called
ExactMatch, merges all mentions that match ex-
actly orthographically into the same cluster.
Use of name similarity scores as features (in ad-
dition to their use for candidate cluster selection)
is indicated by rows labeled Ortho. Use of lexi-
cal features is indicated by BoW and use of topic
model features by Topic.
Using topic models as features was more help-
ful than lexical contexts on the smith corpus.
1055
#coref chains Baseline ExactMatch Ortho BOW Topics Ortho+BOW Ortho+Topics
1K
10K
20K
30K
40K
50K
60K
70K
80K
90K
100K
110K
120K
130K
140K
1000 702 699 925 857 699 697
10000 4563 4530 7964 7956 4514 4518
20000 8234 8202 15691 15073 8159 8163
30000 11745 11682 23138 21878 11608 11611
40000 15041 14964 30900 28500 14869 14863
50000 18110 18016 38248 34758 17910 17903
60000 20450 20377 44735 40081 20241 20228
70000 22845 22780 51190 45722 22615 22603
80000 25062 25026 57440 51104 24832 24818
90000 27389 27358 64140 56581 27145 27126
100000 29797 29782 71034 62228 29546 29511
110000 32147 32139 77705 67853 31882 31840
120000 34567 34589 84309 73397 34284 34235
130000 36817 36874 90465 78676 36543 36486
140000 38826 38901 96225 83525 38539 38482
0
37500
75000
112500
150000
1K 20K 40K 60K 80K 100K 120K 140K
#
 
o
f
 
c
l
u
s
t
e
r
s
 
p
r
o
d
u
c
e
d
# of entity chains seen
Baseline ExactMatch
Ortho BOW
Topics Ortho+BOW
Ortho+Topics
Figure 2: Number of clusters produced vs. num-
ber of entity chains observed in the stream. Num-
ber of entity chains is proportional to the number
of documents.
When used alone topic beats BoW, but in com-
bination with the ortho features performance is
equivalent. For both nytac and ace08 heavy re-
liance on orthographic similarity proved hard to
beat. On the ace08 corpus Baron and Freedman
(2008) report B3 F-scores of 83.2 for persons and
67.8 for organizations, and our streaming results
appear to be comparable to their offline method.
The cluster growth induced by the various mea-
sures can be seen in Figure 2. The two base-
line methods, Baseline and ExactMatch, provide
bounds on the cluster growth with all other meth-
ods falling in between.
5.2 Hashing Strategies for Candidate
Selection
Table 3 containsB3 F-scores when different hash-
ing strategies are employed for candidate selec-
tion. The trend appears to be that stricter match-
ing outperforms fuzzier matching; full mentions
tended to beat words, which beat use of the char-
acter bigrams. This agrees with the results de-
scribed in the previous section, which show heavy
reliance on orthographic similarity.
5.3 Timing Results
Figure 3 shows how processing time increases
with the number of entities observed in the ace08
#chains 1 5 10 20
1000 2 0 0 1
2000 2 0 0 1
3000 2 0 0 1
4000 2 0 0 1
5000 2 2 0 4
6000 2 2 0 4
7000 2 2 0 4
8000 2 2 0 4
9000 2 2 0 4
10000 2 2 0 4
11000 2 2 0 4
12000 2 2 0 5
13000 2 2 0 6
14000 2 2 0 6
15000 2 2 0 6
16000 2 2 0 6
17000 2 2 0 6
18000 2 2 0 6
19000 2 2 1 7
20000 2 2 1 7
21000 2 2 2 7
22000 2 2 2 7
23000 2 2 3 7
24000 2 2 3 7
25000 2 2 3 7
26000 2 2 3 7
27000 2 2 4 8
28000 2 2 4 9
29000 2 2 5 10
30000 2 2 8 11
31000 2 2 8 12
32000 2 2 8 13
33000 2 2 8 14
34000 2 2 8 15
35000 2 2 8 16
36000 2 2 8 17
37000 2 2 8 18
38000 2 2 9 19
39000 2 2 9 20
40000 2 2 9 21
41000 2 2 10 22
42000 2 2 10 23
43000 2 2 11 24
44000 2 2 12 25
45000 2 2 12 26
46000 2 2 13 27
47000 2 3 14 28
48000 2 3 15 29
49000 2 4 16 30
50000 2 5 17 32
51000 2 6 18 34
52000 2 7 19 36
53000 2 7 20 38
54000 2 7 21 40
55000 2 8 22 41
56000 2 8 23 43
57000 2 9 24 45
58000 2 10 25 47
59000 2 10 26 48
60000 2 11 27 50
61000 2 12 28 52
62000 2 13 29 54
63000 2 13 30 56
64000 2 14 31 58
65000 2 15 32 60
66000 3 16 33 62
67000 3 17 34 63
68000 3 18 35 65
69000 3 19 36 67
70000 3 19 37 68
71000 4 20 38 70
72000 4 21 39 72
73000 4 22 40 73
74000 5 23 41 75
75000 6 24 42 77
76000 6 25 43 79
77000 6 26 44 81
78000 6 27 45 83
79000 7 28 46 85
80000 7 29 47 87
81000 8 30 48 89
82000 8 31 49 91
83000 9 32 50 93
84000 9 33 51 95
85000 9 34 52 97
86000 9 35 53 99
87000 10 36 54 101
88000 10 37 55 102
89000 10 38 56 104
90000 11 39 57 106
91000 13 40 58 108
92000 14 41 59 111
93000 15 42 60 113
94000 15 43 61 115
95000 16 44 62 117
96000 17 45 63 119
97000 17 46 64 121
98000 18 47 65 123
99000 19 48 66 125
100000 20 49 67 127
101000 21 50 68 129
102000 21 51 69 131
103000 22 52 70 133
104000 23 53 71 136
105000 24 54 72 138
106000 25 55 73 140
107000 25 56 74 142
108000 26 57 75 144
109000 27 58 76 146
110000 28 59 77 148
111000 28 60 78 150
112000 29 61 79 153
113000 30 62 80 156
114000 31 63 81 158
115000 32 64 83 161
116000 33 66 84 163
117000 34 67 85 165
118000 35 68 86 167
119000 36 69 87 169
120000 37 70 88 171
121000 38 71 90 174
122000 39 72 92 177
123000 40 73 93 179
124000 41 74 94 181
125000 42 75 95 183
126000 43 76 96 186
127000 44 77 99 188
128000 45 78 100 191
129000 46 79 101 196
130000 47 80 102 198
131000 48 81 103 201
132000 49 82 104 204
133000 50 83 105 207
134000 51 84 106 210
135000 52 85 107 214
136000 53 86 109 217
137000 54 87 110 219
138000 55 89 112 222
139000 56 90 113 225
140000 57 91 115 228
141000 58 92 117 231
142000 59 93 118 234
143000 62 94 120 237
143442 62 94 121 238
1
10
100
1000
1000 25000 49000 73000 97000 121000
T
i
m
e
 
(
s
e
c
s
)
# of chains processed
1 5 10 20
Figure 3: Elapsed processing time as a function of
bounding the number of candidate clusters consid-
ered for an entity. When fewer candidates are con-
sidered, clustering decisions can be made much
faster.
document stream. We experimented with using an
upper bound on the number of candidate clusters
to consider for an entity.
Figure 4 compares the efficiency of using three
different methods for candidate cluster identifica-
tion. The most restrictive hashing strategy, using
exact mention strings, is the most efficient, fol-
lowed by the use of words, then the use of charac-
ter skip bigrams. This makes intuitive sense ? the
strictest matching reduces the number of candi-
date clusters that have to be considered when pro-
cessing an entity.6
The ace08 corpus contained over 10,000 doc-
uments and is one of the largest CDCR test sets.
In Figure 5 we show how processing time grows
when processing the kbp09 corpus. Doubling the
number of entities processed increases the runtime
by about a factor of 5. The curve is not linear
due to the increasing number of entity cluster?s
that must be considered. Future work will exam-
ine how to keep the number of clusters considered
constant over time, such as ignoring older entities.
6 Conclusion
We have presented a new streaming cross doc-
ument coreference resolution system. Our ap-
proach is substantially faster than previous sys-
6In the limit, if names were unique, hashing on strings
would completely solve the CDCR problem and processing
an entity would be O(1)
1056
smith nytac ace08
Approach bigrams words mention bigrams words mention bigrams words mention
Ortho 0.382 0.553 0.616 0.120 0.695 0.687 0.540 0.797 0.811
BoW 0.480 0.530 0.467 0.344 0.339 0.349 0.551 0.700 0.738
Topic 0.697 0.661 0.579 0.071 0.620 0.363 0.544 0.685 0.750
Ortho+BoW 0.389 0.554 0.618 0.340 0.691 0.686 0.519 0.783 0.809
Ortho+Topic 0.398 0.555 0.618 0.120 0.477 0.680 0.520 0.776 0.819
Table 3: B3 F-scores using different hashing strategies for candidate selection. Name/cluster similarity
could be based on character skip bigrams, words appear in names, or exact matching of mention.
#chains mention string word bigram
1000 2 1 1
2000 2 2 5
3000 2 2 6
4000 2 4 7
5000 3 5 9
6000 4 6 11
7000 5 6 13
8000 5 7 15
9000 6 7 17
10000 6 7 19
11000 7 9 21
12000 7 10 23
13000 7 11 25
14000 9 13 27
15000 9 14 29
16000 10 15 31
17000 10 16 33
18000 11 17 36
19000 12 18 39
20000 12 19 42
21000 13 21 44
22000 13 23 47
23000 13 24 50
24000 14 26 53
25000 15 28 56
26000 16 30 60
27000 16 32 64
28000 17 34 68
29000 17 36 71
30000 18 39 75
31000 19 40 79
32000 21 43 83
33000 22 45 88
34000 23 47 92
35000 24 49 96
36000 26 51 100
37000 26 53 104
38000 27 55 108
39000 27 57 112
40000 28 59 117
41000 29 63 121
42000 30 66 125
43000 31 70 129
44000 33 73 133
45000 34 77 137
46000 35 80 142
47000 36 84 146
48000 36 87 150
49000 38 90 154
50000 39 94 158
51000 40 98 162
52000 41 101 167
53000 42 104 171
54000 44 107 175
55000 45 111 179
56000 46 115 183
57000 48 119 187
58000 49 122 192
59000 49 126 196
60000 50 130 200
61000 51 134 204
62000 52 139 208
63000 53 143 212
64000 56 146 217
65000 57 150 222
66000 59 155 227
67000 60 158 232
68000 62 163 237
69000 62 167 242
70000 63 170 247
71000 65 173 252
72000 66 178 257
73000 67 183 262
74000 67 186 267
75000 68 191 273
76000 69 194 277
77000 70 198 282
78000 70 202 287
79000 71 207 292
80000 72 211 297
81000 73 215 302
82000 73 219 307
83000 74 224 312
84000 74 229 317
85000 75 233 322
86000 76 238 328
87000 77 243 333
88000 77 246 338
89000 78 250 343
90000 78 254 348
91000 79 258 353
92000 80 262 358
93000 80 268 363
94000 81 273 368
95000 82 277 373
96000 82 281 378
97000 83 284 384
98000 84 288 389
99000 84 293 394
100000 85 297 400
101000 85 300 406
102000 85 303 413
103000 85 307 419
104000 85 310 425
105000 86 314 431
106000 87 319 438
107000 89 325 444
108000 90 330 450
109000 91 334 456
110000 91 339 463
111000 92 343 469
112000 92 348 476
113000 93 352 482
114000 94 357 489
115000 95 362 495
116000 95 366 501
117000 96 369 507
118000 96 373 513
119000 96 377 520
120000 97 382 526
121000 97 387 532
122000 100 392 539
123000 100 395 546
124000 101 399 552
125000 101 403 558
126000 102 408 564
127000 102 412 571
128000 103 417 577
129000 104 421 583
130000 104 425 589
131000 105 430 596
132000 106 435 602
133000 108 441 609
134000 109 446 615
135000 111 452 622
136000 112 457 628
137000 113 461 634
138000 113 467 640
139000 114 472 648
140000 115 479 655
141000 116 484 662
142000 117 489 669
143000 118 494 676
143442 118 497 679
1
10
100
1000
1000 25000 49000 73000 97000 121000
T
i
m
e
 
(
s
e
c
s
)
# of coref chains processed
mention string word bigram
Figure 4: Comparison of three hashing strategies
for identifying candidate clusters for a given en-
tity. The more restrictive strategies lead to faster
processing as fewer candidates are considered.
tems, and our experiments have demonstrated
scalability to an order of magnitude larger data
than previously published evaluations. Despite its
speed and simplicity, we still obtain competitive
results on a variety of data sets as compared with
batch systems. In future work, we plan to investi-
gate additional similarity metrics that can be com-
puted efficiently, as well as experiments on web
scale corpora.
References
Artiles, Javier, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search: results of the first evalua-
tion and the plan for the second. In World Wide Web
(WWW).
Bagga, Amit and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Conference on Computational
Linguistics (COLING).
Baron, Alex and Marjorie Freedman. 2008. Who
#coref chains processed Time (secs)
1K 1.5
100K 10
200K 40
400K 120
600K 700
900K 920
1.1M 1200
1
10
100
1000
10000
1K 100K 200K 400K 600K 900K 1.1M
T
i
m
e
 
(
s
e
c
s
)
# of coref chains processed
Figure 5: The number of coreference chains pro-
cessed over time in the kbp09 corpus. The pro-
cessing of over 1 million coreference chains is at
least an order of magnitude larger than previous
systems reported.
is Who and What is What: Experiments in cross-
document co-reference. In Empirical Methods in
Natural Language Processing (EMNLP).
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research (JMLR), 3:993?1022.
Can, F. and E. Ozkarahan. 1987. A dynamic clus-
ter maintenance system for information retrieval. In
Conference on Research and Development in Infor-
mation Retrieval (SIGIR).
Charikar, Moses, Chandra Chekuri, Toma?s Feder, and
Rajeev Motwani. 1997. Incremental clustering and
dynamic information retrieval. In ACM Symposium
on Theory of Computing (STOC).
Chen, Ying and James Martin. 2007. Towards ro-
bust unsupervised personal name disambiguation.
In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Choi, Y. and C. Cardie. 2007. Structured local training
and biased potential functions for conditional ran-
dom fields with application to coreference resolu-
tion. In North American Chapter of the Association
1057
for Computational Linguistics (NAACL), pages 65?
72.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on wikipedia data. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 708?716.
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1?32.
Oxford: Philological Society.
Gooi, Chung Heong and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for
Computational Linguistics (NAACL).
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Association for Computational Linguistics (ACL).
Mann, Gideon S. and David Yarowsky. 2003. Unsu-
pervised personal name disambiguation. In Confer-
ence on Natural Language Learning (CONLL).
McNamee, Paul and Hoa Dang. 2009. Overview of
the TAC 2009 knowledge base population track. In
Text Analysis Conference (TAC).
Ng, V. and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
Nicolae, C. and G. Nicolae. 2006. Bestcut: A
graph algorithm for coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 275?283. Association for Compu-
tational Linguistics.
Omiecinski, Edward and Peter Scheuermann. 1984. A
global approach to record clustering and file reorga-
nization. In Conference on Research and Develop-
ment in Information Retrieval (SIGIR).
Pradhan, S.S., L. Ramshaw, R. Weischedel,
J. MacBride, and L. Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events
in ontonotes. In International Conference on
Semantic Computing (ICSC).
Ramshaw, L. and R. Weischedel. 2005. Information
extraction. In IEEE ICASSP.
Sanderson, Mark. 2000. Retrieving with good sense.
Information Retrieval, 2(1):45?65.
Sandhaus, Evan. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics.
Yao, L., D. Mimno, and A. McCallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In Knowledge discovery
and data mining (KDD).
1058
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32?35,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
KELVIN: a tool for automated knowledge base construction
Paul McNamee, James Mayfield
Johns Hopkins University
Human Language Technology Center of Excellence
Tim Finin, Tim Oates
University of Maryland
Baltimore County
Dawn Lawrie
Loyola University Maryland
Tan Xu, Douglas W. Oard
University of Maryland
College Park
Abstract
We present KELVIN, an automated system for
processing a large text corpus and distilling a
knowledge base about persons, organizations,
and locations. We have tested the KELVIN
system on several corpora, including: (a) the
TAC KBP 2012 Cold Start corpus which con-
sists of public Web pages from the University
of Pennsylvania, and (b) a subset of 26k news
articles taken from English Gigaword 5th edi-
tion.
Our NAACL HLT 2013 demonstration per-
mits a user to interact with a set of search-
able HTML pages, which are automatically
generated from the knowledge base. Each
page contains information analogous to the
semi-structured details about an entity that are
present in Wikipedia Infoboxes, along with
hyperlink citations to supporting text.
1 Introduction
The Text Analysis Conference (TAC) Knowledge
Base Population (KBP) Cold Start task1 requires
systems to take set of documents and produce a
comprehensive set of <Subject, Predicate, Object>
triples that encode relationships between and at-
tributes of the named-entities that are mentioned in
the corpus. Systems are evaluated based on the fi-
delity of the constructed knowledge base. For the
2012 evaluation, a fixed schema of 42 relations (or
slots), and their logical inverses was provided, for
example:
? X:Organization employs Y:Person
1See details at http://www.nist.gov/tac/2012/
KBP/task_guidelines/index.html
? X:Person has-job-title title
? X:Organization headquartered-in Y:Location
Multiple layers of NLP software are required for
this undertaking, including at the least: detection of
named-entities, intra-document co-reference resolu-
tion, relation extraction, and entity disambiguation.
To help prevent a bias towards learning about
prominent entities at the expense of generality,
KELVIN refrains from mining facts from sources
such as documents obtained through Web search,
Wikipedia2, or DBpedia.3 Only facts that are as-
serted in and gleaned from the source documents are
posited.
Other systems that create large-scale knowledge
bases from general text include the Never-Ending
Language Learning (NELL) system at Carnegie
Mellon University (Carlson et al, 2010), and the
TextRunner system developed at the University of
Washington (Etzioni et al, 2008).
2 Washington Post KB
No gold-standard KBs were available to us to assist
during the development of KELVIN, so we relied on
qualitative assessment to gauge the effectiveness of
our extracted relations ? by manually examining ten
random samples for each relations, we ascertained
that most relations were between 30-80% accurate.
Although the TAC KBP 2012 Cold Start task was a
pilot evaluation of a new task using a novel evalua-
tion methodology, the KELVIN system did attain the
highest reported F1 scores.4
2http://en.wikipedia.org/
3http://www.dbpedia.org/
40.497 0-hop & 0.363 all-hops, as reported in the prelimi-
nary TAC 2012 Evaluation Results.
32
During our initial development we worked with
a 26,143 document collection of 2010 Washington
Post articles and the system discovered 194,059 re-
lations about 57,847 named entities. KELVIN learns
some interesting, but rather dubious relations from
the Washington Post articles5
? Sen. Harry Reid is an employee of the ?Repub-
lican Party.? Sen. Reid is also an employee of
the ?Democratic Party.?
? Big Foot is an employee of Starbucks.
? MacBook Air is a subsidiary of Apple Inc.
? Jill Biden is married to Jill Biden.
However, KELVIN also learns quite a number of
correct facts, including:
? Warren Buffett owns shares of Berkshire Hath-
away, Burlington Northern Santa Fe, the Wash-
ington Post Co., and four other stocks.
? Jared Fogle is an employee of Subway.
? Freeman Hrabowski works for UMBC,
founded the Meyerhoff Scholars Program, and
graduated from Hampton University and the
University of Illinois.
? Supreme Court Justice Elena Kagan attended
Oxford, Harvard, and Princeton.
? Southwest Airlines is headquartered in Texas.
? Ian Soboroff is a computer scientist6 employed
by NIST.7
3 Pipeline Components
3.1 SERIF
BBN?s SERIF tool8 (Boschee et al, 2005) provides
a considerable suite of document annotations that
are an excellent basis for building a knowledge base.
The functions SERIF can provide are based largely
5All 2010 Washington Post articles from English Gigaword
5th ed. (LDC2011T07).
6Ian is the sole computer scientist discovered in processing
a year of news. In contrast, KELVIN found 52 lobbyists.
7From Washington Post article (WPB ENG 20100506.0012
in LDC2011T07).
8Statistical Entity & Relation Information Finding.
Slotname Count
per:employee of 60,690
org:employees 44,663
gpe:employees 16,027
per:member of 14,613
org:membership 14,613
org:city of headquarters 12,598
gpe:headquarters in city 12,598
org:parents 6,526
org:country of headquarters 4,503
gpe:headquarters in country 4,503
Table 1: Most prevalent slots extracted by SERIF from
the Washington Post texts.
Slotname Count
per:title 44,896
per:employee of 39,101
per:member of 20,735
per:countries of residence 8,192
per:origin 4,187
per:statesorprovinces of residence 3,376
per:cities of residence 3,376
per:country of birth 1,577
per:age 1,233
per:spouse 1,057
Table 2: Most prevalent slots extracted by FACETS from
the Washington Post texts.
on the NIST ACE specification,9 and include: (a)
identifying named-entities and classifying them by
type and subtype; (b) performing intra-document
co-reference analysis, including named mentions,
as well as co-referential nominal and pronominal
mentions; (c) parsing sentences and extracting intra-
sentential relations between entities; and, (d) detect-
ing certain types of events.
In Table 1 we list the most common slots SERIF
extracts from the Washington Post articles.
3.2 FACETS
FACETS, another BBN tool, is an add-on pack-
age that takes SERIF output and produces role and
argument annotations about person noun phrases.
FACETS is implemented using a conditional-
9The principal types of ACE named-entities are per-
sons, organizations, and geo-political entities (GPEs).
GPEs are inhabited locations with a government. See
http://www.itl.nist.gov/iad/mig/tests/ace/
2008/doc/ace08-evalplan.v1.2d.pdf.
33
Figure 1: Simple rendering of KB page about former Florida congressman Joe Scarborough. Many facts are correct
? he lived in and was employed by the State of Florida; he has a brother George; he was a member of the Republican
House of Representatives; and, he is employed by MSNBC.
exponential learner trained on broadcast news. The
attributes FACETS can recognize include general at-
tributes like religion and age (which anyone might
have), as well as role-specific attributes, such as
medical specialty for physicians, or academic insti-
tution for someone associated with an university.
In Table 2 we report the most prevalent slots
FACETS extracts from the Washington Post.10
3.3 CUNY toolkit
To increase our coverage of relations we also in-
tegrated the KBP Slot Filling Toolkit (Chen et al,
2011) developed at the CUNY BLENDER Lab.
Given that the KBP toolkit was designed for the tra-
ditional slot filling task at TAC, this primarily in-
volved creating the queries that the tool expected as
input and parallelizing the toolkit to handle the vast
number of queries issued in the cold start scenarios.
To informally gauge the accuracy of slots
extracted from the CUNY tool, some coarse as-
sessment was done over a small collection of 807
New York Times articles that include the string
?University of Kansas.? From this collection, 4264
slots were identified. Nine different types of slots
were filled in order of frequency: per:title (37%),
per:employee of (23%), per:cities of residence
(17%), per:stateorprovinces of residence (6%),
10Note FACETS can independently extract some slots that
SERIF is capable of discovering (e.g., employment relations).
org:top members/employees (6%), org:member of
(6%), per:countries of residence (2%), per:spouse
(2%), and per:member of (1%). We randomly sam-
pled 10 slot-fills of each type, and found accuracy
to vary from 20-70%.
3.4 Coreference
We used two methods for entity coreference. Un-
der the theory that name ambiguity may not be a
huge problem, we adopted a baseline approach of
merging entities across different documents if their
canonical mentions were an exact string match af-
ter some basic normalizations, such as removing
punctuation and conversion to lower-case charac-
ters. However we also used the JHU HLTCOE
CALE system (Stoyanov et al, 2012), which maps
named-entity mentions to the TAC-KBP reference
KB, which was derived from a 2008 snapshot of En-
glish Wikipedia. For entities that are not found in the
KB, we reverted to exact string match. CALE entity
linking proved to be the more effective approach for
the Cold Start task.
3.5 Timex2 Normalization
SERIF recognizes, but does not normalize, temporal
expressions, so we used the Stanford SUTime pack-
age, to normalize date values.
34
Figure 2: Supporting text for some assertions about Mr. Scarborough. Source documents are also viewable by
following hyperlinks.
3.6 Lightweight Inference
We performed a small amount of light inference to
fill some slots. For example, if we identified that
a person P worked for organization O, and we also
extracted a job title T for P, and if T matched a set
of titles such as president or minister we asserted
that the tuple <O, org:top members employees, P>
relation also held.
4 Ongoing Work
There are a number of improvements that we are un-
dertaking, including: scaling to much larger corpora,
detecting contradictions, expanding the use of infer-
ence, exploiting the confidence of extracted infor-
mation, and applying KELVIN to various genres of
text.
5 Script Outline
The KB generated by KELVIN is best explored us-
ing a Wikipedia metaphor. Thus our demonstration
consists of a web browser that starts with a list of
moderately prominent named-entities that the user
can choose to examine (e.g., investor Warren Buf-
fett, Supreme Court Justice Elena Kagan, Southwest
Airlines Co., the state of Florida). Selecting any
entity takes one to a page displaying its known at-
tributes and relations, with links to documents that
serve as provenance for each assertion. On every
page, each entity is hyperlinked to its own canon-
ical page; therefore the user is able to browse the
KB much as one browses Wikipedia by simply fol-
lowing links. A sample generated page is shown in
Figure 1 and text that supports some of the learned
assertions in the figure is shown in Figure 2. We
also provide a search interface to support jump-
ing to a desired entity and can demonstrate access-
ing the data encoded in the semantic web language
RDF (World Wide Web Consortium, 2013), which
supports ontology browsing and executing complex
SPARQL queries (Prud?Hommeaux and Seaborne,
2008) such as ?List the employers of people living
in Nebraska or Kansas who are older than 40.?
References
E. Boschee, R. Weischedel, and A. Zamanian. 2005. Au-
tomatic information extraction. In Proceedings of the
2005 International Conference on Intelligence Analy-
sis, McLean, VA, pages 2?4.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Z. Chen, S. Tamang, A. Lee, X. Li, and H. Ji. 2011.
Knowledge Base Population (KBP) Toolkit @ CUNY
BLENDER LAB Manual.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, Decem-
ber.
E Prud?Hommeaux and A. Seaborne. 2008. SPARQL
query language for RDF. Technical report, World
Wide Web Consortium, January.
Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W.
Oard, Dawn Lawrie, Tim Oates, and Tim Finin. 2012.
A context-aware approach to entity linking. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction, AKBC-WEKEX ?12, pages 62?67, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
World Wide Web Consortium. 2013. Resource Descrip-
tion Framework Specification. ?http://http://
www.w3.org/RDF/. ?[Online; accessed 8 April,
2013]?.
35
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 65?74,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Language Identification for Creating Language-Specific Twitter Collections
Shane Bergsma? Paul McNamee?,? Mossaab Bagdouri? Clayton Fink? Theresa Wilson?
?Human Language Technology Center of Excellence, Johns Hopkins University
?Johns Hopkins University Applied Physics Laboratory, Laurel, MD
?Department of Computer Science, University of Maryland, College Park, MD
sbergsma@jhu.edu, mcnamee@jhu.edu, mossaab@umd.edu, clayton.fink@jhuapl.edu, taw@jhu.edu
Abstract
Social media services such as Twitter offer an
immense volume of real-world linguistic data.
We explore the use of Twitter to obtain authen-
tic user-generated text in low-resource lan-
guages such as Nepali, Urdu, and Ukrainian.
Automatic language identification (LID) can
be used to extract language-specific data from
Twitter, but it is unclear how well LID per-
forms on short, informal texts in low-resource
languages. We address this question by an-
notating and releasing a large collection of
tweets in nine languages, focusing on confus-
able languages using the Cyrillic, Arabic, and
Devanagari scripts. This is the first publicly-
available collection of LID-annotated tweets
in non-Latin scripts, and should become a
standard evaluation set for LID systems. We
also advance the state-of-the-art by evaluat-
ing new, highly-accurate LID systems, trained
both on our new corpus and on standard ma-
terials only. Both types of systems achieve
a huge performance improvement over the
existing state-of-the-art, correctly classifying
around 98% of our gold standard tweets. We
provide a detailed analysis showing how the
accuracy of our systems vary along certain di-
mensions, such as the tweet-length and the
amount of in- and out-of-domain training data.
1 Introduction
Twitter is an online social-networking service that
lets users send and receive short texts called tweets.
Twitter is enormously popular; more than 50 mil-
lion users log in daily and billions of tweets are sent
each month.1 Tweets are publicly-available by de-
1http://mashable.com/2011/09/08/
Twitter-has-100-million-active-users/
fault and thus provide an enormous and growing free
resource of authentic, unedited text by ordinary peo-
ple. Researchers have used Twitter to study how hu-
man language varies by time zone (Kiciman, 2010),
census area (Eisenstein et al, 2011), gender (Burger
et al, 2011), and ethnicity (Fink et al, 2012). Twit-
ter also provides a wealth of user dialog, and a vari-
ety of dialog acts have been observed (Ritter et al,
2010) and predicted (Ritter et al, 2011).
Of course, working with Twitter is not all roses
and rainbows. Twitter is a difficult domain because
unlike, for example, news articles, tweets are short
(limited to 140 characters), vary widely in style,
and contain many spelling and grammatical errors.
Moreover, unlike articles written by a particular
news organization, a corpus constructed from Twit-
ter will contain tweets in many different languages.
This latter point is particularly troubling because
the majority of language-processing technology is
predicated on knowing which language is being pro-
cessed. We are pursuing a long-term effort to build
social media collections in a variety of low-resource
languages, and we need robust language identifica-
tion (LID) technology. While LID is often viewed
as a solved problem (McNamee, 2005), recent re-
search has shown that LID can be made arbitrarily
difficult by choosing domains with (a) informal writ-
ing, (b) lots of languages to choose from, (c) very
short texts, and (d) unbalanced data (Hughes et al,
2006; Baldwin and Lui, 2010). Twitter exhibits all
of these properties. While the problem of LID on
Twitter has been considered previously (Tromp and
Pechenizkiy, 2011; Carter et al, 2013), these studies
have only targeted five or six western European lan-
guages, and not the diversity of languages and writ-
ing systems that we would like to process.
65
Our main contribution is the release of a large col-
lection of tweets in nine languages using the Cyril-
lic, Arabic, and Devanagari alphabets. We test dif-
ferent methods for obtaining tweets in a given tar-
get language (?2). We then use an online crowd-
sourcing platform to have these tweets annotated by
fluent speakers of that language (?3). We generate
over 18,000 triple-consensus tweets, providing the
first publicly-available collection of LID-annotated
tweets in non-Latin scripts. The annotated cor-
pus is available online at: http://apl.jhu.edu/
?
paulmac/lid.html. We anticipate our multilin-
gual Twitter collection becoming a standard evalua-
tion set for LID systems.
We also implement two LID approaches and eval-
uate these approaches against state-of-the-art com-
petitors. ?4.1 describes a discriminative classifier
that leverages both the tweet text and the tweet meta-
data (such as the user name, location, and landing
pages for shortened URLs). ?4.2 describes an effi-
cient tool based on compression language models.
Both types of systems achieve a huge improvement
over existing state-of-the-art approaches, including
the Google Compact Language Detector (part of the
Chrome browser), and a recent LID system from
Lui and Baldwin (2011). Finally, we provide further
analysis of our systems in this unique domain, show-
ing how accuracy varies with the tweet-length and
the amount of in-domain and out-of-domain train-
ing data. In addition to the datasets, we are releasing
our compression language model tool for public use.
2 Acquiring Language-Specific Tweets
We use two strategies to collect tweets in specific
languages: (?2.1) we collect tweets by users who
follow language-specific Twitter sources, and (?2.2)
we use the Twitter API to collect tweets from users
who are likely to speak the target language.
2.1 Followers of Language-Specific Sources
Our first method is called the Sources method and
involves a three-stage process. First, Twitter sources
for the target language are manually identified.
Sources are Twitter users or feeds who: (a) tweet
in the target language, (b) have a large number of
followers, and (c) act as hubs (i.e., have a high
followers-to-following ratio). Twitter sources are
typically news or media outlets (e.g. BBC News),
celebrities, politicians, governmental organizations,
but they may just be prominent bloggers or tweeters.
Once sources are identified, we use the Twitter
API (dev.twitter.com) to query each source for
its list of followers. We then query the user data for
the followers in batches of 100 tweets. For users
whose data is public, a wealth of information is
returned, including the total number of tweets and
their most recent tweet. For users who had tweeted
above a minimum number of times. and whose
most-recent-tweet tweet was in the character set for
the target language, we obtained their most recent
100-200 tweets and added them to our collection.2
While we have used the above approach to ac-
quire data in a number of different languages, for the
purposes of our annotated corpus (?3), we select the
subsets of users who exclusively follow sources in
one of our nine target languages (Table 1). We also
filter tweets that do not contain at least one charac-
ter in the target?s corresponding writing system (we
plan to address romanized tweets in future work).
2.2 Direct Twitter-API Collection
While we are most interested in users who follow
news articles, we also tested other methods for ob-
taining language-specific tweets. First, we used the
Twitter API to collect tweets from locations where
we expected to get some number of tweets in the tar-
get language. We call this method the Twit-API col-
lection method. To geolocate our tweets, the Twit-
ter API?s geotag method allowed us to collect tweets
within a specified radius of a given set of coordi-
nates in latitude and longitude. To gather a sam-
ple of tweets in our target languages, we queried
for tweets from cities with populations of at least
200,000 where speakers of the target language are
prominent (e.g., Karachi, Pakistan for Urdu; Tehran,
Iran for Farsi; etc.). We collected tweets within a ra-
dius of 25 miles of the geocoordinates. We also used
the Search API to persistently poll for tweets from
users identified by Twitter as being in the queried
location. For Urdu, we also relied on the language-
2Tromp and Pechenizkiy (2011) also manually identified
language-specific Twitter feeds, but they use tweets from these
sources directly as gold standard data, while we target the users
who simply follow such sources. We expect our approach to
obtain more-authentic and less-edited user language.
66
identification code returned by the API for each
tweet; we filter all our geolocated Urdu tweets that
are not marked as Urdu.
We also obtained tweets through an information-
retrieval approach that has been used elsewhere for
creating minority language corpora (Ghani et al,
2001). We computed the 25 most frequent unique
words in a number of different languages (that is,
words that do not occur in the vocabularies of other
languages). Unfortunately, we found no way to en-
force that the Twitter API return only tweets con-
taining one or more of our search terms (e.g., re-
turned tweets for Urdu were often in Arabic and did
not contain our Urdu search terms). There is a lack
of documentation on what characters are supported
by the search API; it could be that the API cannot
handle certain of our terms. We thus leave further
investigation of this method for future work.
3 Annotating Tweets by Language
The general LID task is to take as input some piece
of text, and to produce as output a prediction of what
language the text is written in. Our annotation and
prediction systems operate at the level of individual
tweets. An alternative would have been to assume
that each user only tweets in a single language, and
to make predictions on an aggregation of multiple
tweets. We operate on individual tweets mainly be-
cause (A) we would like to quantify how often users
switch between languages and (B) we are also inter-
ested in domains and cases where only tweet-sized
amounts of text are available. When we do have
multiple tweets per user, we can always aggregate
the scores on individual predictions (?6 has some ex-
perimental results using prediction aggregation).
Our human annotation therefore also focuses on
validating the language of individual tweets. Tweets
verified by three independent annotators are ac-
cepted into our final gold-standard data.
3.1 Amazon Mechanical Turk
To access annotators with fluency in each language,
we crowdsourced the annotation using Amazon Me-
chanical Turk (mturk.com). AMT is an online la-
bor marketplace that allows requesters to post tasks
for completion by paid human workers. Crowd-
sourcing via AMT has been shown to provide high-
quality data for a variety of NLP tasks (Snow et al,
2008; Callison-Burch and Dredze, 2010), including
multilingual annotation efforts in translation (Zaidan
and Callison-Burch, 2011b), dialect identification
(Zaidan and Callison-Burch, 2011a), and building
bilingual lexicons (Irvine and Klementiev, 2010).
3.2 Annotation Task
From the tweets obtained in ?2, we took a random
sample in each target language, and posted these
tweets for annotation on AMT. Each tweet in the
sample was assigned to a particular AMT job; each
job comprised the annotation of 20 tweets. The job
description requested workers that are fluent in the
target language and gave an example of valid and
invalid tweets in that language. The job instructions
asked workers to mark whether each tweet was writ-
ten for speakers of the target language. If the tweet
combines multiple languages, workers were asked
to mark as the target language if ?most of the text is
in [that language] excluding URLs, hash-tags, etc.?
Jobs were presented to workers as HTML pages with
three buttons alongside each tweet for validating the
language. For example, for Nepali, a Worker can
mark that a tweet is ?Nepali?, ?Not Nepali?, or ?Not
sure.? We paid $0.05 per job and requested that each
job be completed by three workers.
3.3 Quality Control
To ensure high annotation quality, we follow our
established practices in only allowing our tasks to
be completed by workers who have previously com-
pleted at least 50 jobs on AMT, and who have had at
least 85% of their jobs approved. Our jobs also dis-
play each tweet as an image; this prevents workers
from pasting the tweet into existing online language
processing services (like Google Translate).
We also have control tweets in each job to allow
us to evaluate worker performance. A positive con-
trol is a tweet known to be in the target language;
a negative control is a tweet known to be in a dif-
ferent language. Between three to six of the twenty
tweets in each job were controls. The controls are
taken from the sources used in our Sources method
(?2.1); e.g., our Urdu controls come from sources
like BBC Urdu?s Twitter feed. To further validate
the controls, we also applied our open-domain LID
system (?4.2) and filtered any Source tweets whose
67
Language Method Purity Gold Tweets
Arabic Sources 100% 1174
Farsi Sources 100% 2512
Urdu Sources 55.4% 1076
Arabic Twit-API 99.9% 1254
Farsi Twit-API 99.7% 2366
Urdu Twit-API 61.0% 1313
Hindi Sources 97.5% 1214
Nepali Sources 97.3% 1681
Marathi Sources 91.4% 1157
Russian Sources 99.8% 2005
Bulgarian Sources 92.2% 1886
Ukrainian Sources 14.3% 631
Table 1: Statistics of the Annotated Multilingual Twitter
Corpus: 18,269 total tweets in nine languages.
predicted language was not the expected language.
Our negative controls are validated tweets in a lan-
guage that uses the same alphabet as the target (e.g.,
our negative controls for Ukrainian were taken from
our LID-validated Russian and Bulgarian sources).
We collect aggregate statistics for each Worker
over the control tweets of all their completed jobs.
We conservatively discard any annotations by work-
ers who get below 80% accuracy on either the posi-
tive or negative control tweets.
3.4 Dataset Statistics
Table 1 gives the number of triple-validated ?Gold?
tweets in each language, grouped into those using
the Arabic, Devanagari and Cyrillic writing sys-
tems. The Arabic data is further divided into tweets
acquired using the Sources and Twit-API methods.
Table 1 also gives the Purity of the acquired re-
sults; that is, the percentage of acquired tweets that
were indeed in the target language. The Purity
is calculated as the number of triple-verified gold
tweets divided by the total number of tweets where
the three annotators agreed in the annotation (thus
triply-marked either Yes, No, or Not sure).
For major languages (e.g. Arabic and Russian),
we can accurately obtain tweets in the target lan-
guage, perhaps obviating the need for LID. For the
Urdu sets, however, a large percentage of tweets are
not in Urdu, and thus neither collection method is
reliable. An LID tool is needed to validate the data.
A native Arabic speaker verified that most of our
invalid Urdu tweets were Arabic. Ukrainian is the
most glaringly impure language that we collected,
with less than 15% of our intended tweets actually
in Ukrainian. Russian is widely spoken in Ukraine
and seems to be the dominant language on Twitter,
but more analysis is required. Finally, Marathi and
Bulgarian also have significant impurities.
The complete annotation of all nine languages
cost only around $350 USD. While not insignificant,
this was a small expense relative to the total human
effort we are expending on this project. Scaling our
approach to hundreds of languages would only cost
on the order of a few thousand dollars, and we are
investigating whether such an effort could be sup-
ported by enough fluent AMT workers.
4 Language Identification Systems
We now describe the systems we implemented
and/or tested on our annotated data. All the ap-
proaches are supervised learners, trained from a col-
lection of language-annotated texts. At test time, the
systems choose an output language based on the in-
formation they have derived from the annotated data.
4.1 LogR: Discriminative LID
We first adopt a discriminative approach to LID.
Each tweet to be classified has its relevant informa-
tion encoded in a feature vector, x?. The annotated
training data can be represented as N pairs of la-
bels and feature vectors: {(y1, x?1), ..., (yN , x?N )}.
To train our model, we use (regularized) logistic re-
gression (a.k.a. maximum entropy) since it has been
shown to perform well on a range of NLP tasks
and its probabilistic outputs are useful for down-
stream processing (such as aggregating predictions
over multiple tweets). In multi-class logistic regres-
sion, the probability of each class takes the form of
exponential functions over features:
p(y = k|x?) = exp(w?k ? x?)?
j exp(w?j ? x?)
For LID, the classifier predicts the language k that
has the highest probability (this is also the class with
highest weighted combination of features, w?k ? x?).
The training procedure tunes the weights to optimize
for correct predictions on training data, subject to a
tunable L2-regularization penalty on the weight vec-
tor norm. For our experiments, we train and test our
logistic regression classifier (LogR) using the effi-
cient LIBLINEAR package (Fan et al, 2008).
68
We use two types of features in our classifier:
Character Features encode the character
N-grams in the input text; characters are the
standard information source for most LID systems
(Cavnar and Trenkle, 1994; Baldwin and Lui, 2010).
We have a unique feature for each unique N-gram in
our training data. N-grams of up-to-four characters
were optimal on development data. Each feature
value is the (smoothed) log-count of how often
the corresponding N-gram occurs in that instance.
Prior to extracting the N-grams, we preprocess each
tweet to remove URLs, hash-tags, user mentions,
punctuation and we normalize all digits to 0.
Meta features encode user-provided information
beyond the tweet text. Similar information has pre-
viously been used to improve the accuracy of LID
classifiers on European-language tweets (Carter et
al., 2013). We have features for the tokens in
the Twitter user name, the screen name, and self-
reported user location. We also have features for
prefixes of these tokens, and flags for whether the
name and location are in the Latin script. Our meta
features also include features for the hash-tags, user-
mentions, and URLs in the tweet. We provide fea-
tures for the protocol (e.g. http), hostname, and top-
level domain (e.g. .com) of each link in a tweet. For
shortened URLs (e.g. via bit.ly), we query the
URL server to obtain the final link destination, and
provide the URL features for this destination link.
4.2 PPM: Compression-Based LID
Our next tool uses compression language models,
which have been proposed for a variety of NLP
tasks including authorship attribution (Pavelec et al,
2009), text classification (Teahan, 2000; Frank et al,
2000), spam filtering (Bratko et al, 2006), and LID
(Benedetto et al, 2002). Our method is based on the
prediction by partial matching (PPM) family of al-
gorithms and we use the PPM-A variant (Cleary et
al., 1984). The algorithm processes a string and de-
termines the number of bits required to encode each
character using a variable-length context. It requires
only a single parameter, the maximal order, n; we
use n = 5 for the experiments in this paper. Given
training data for a number of languages, the method
seeks to minimize cross-entropy and thus selects the
Language Wikip. All
Arabic 372 MB 1058 MB
Farsi 229 MB 798 MB
Urdu 30 MB 50 MB
Hindi 235 MB 518 MB
Nepali 31 MB 31 MB
Marathi 32 MB 66 MB
Russian 563 MB 564 MB
Bulgarian 301 MB 518 MB
Ukrainian 461 MB 463 MB
Table 2: Size of other PPM training materials.
language which would most compactly encode the
text we are attempting to classify.
We train this method both on our Twitter data and
on large collections of other material. These ma-
terials include corpora obtained from news sources,
Wikipedia, and government bodies. For our ex-
periments we divide these materials into two sets:
(1) just Wikipedia and (2) all sources, including
Wikipedia. Table 2 gives the sizes of these sets.
4.3 Comparison Systems
We compare our two new systems with the best-
available commercial and academic software.
TextCat: TextCat3 is a widely-used stand-alone
LID program. Is is an implementation of the
N-gram-based algorithm of Cavnar and Trenkle
(1994), and supports identification in ?about 69 lan-
guages? in its downloadable form. Unfortunately,
the available models do not support all of our target
languages, nor are they compatible with the standard
UTF-8 Unicode character encoding. We therefore
modified the code to process UTF-8 characters and
re-trained the system on our Twitter data (?5).
Google CLD: Google?s Chrome browser includes
a tool for language-detection (the Google Compact
Language Detector), and this tool is included as a li-
brary within Chrome?s open-source code. Mike Mc-
Candless ported this library to its own open source
project.4 The CLD tool makes predictions using text
4-grams. It is designed for detecting the language
of web pages, and can take meta-data hints from the
domain of the webpage and/or the declared webpage
3http://odur.let.rug.nl/vannoord/TextCat/
4http://code.google.com/p/
chromium-compact-language-detector/
69
Dataset Train Development Test
Arabic 2254 1171 1191
Devanagari 2099 991 962
Cyrillic 2243 1133 1146
Table 3: Number of tweets used in experiments, by writ-
ing system/classification task
encoding, but it also works on stand-alone text.5 We
use it in its original, unmodified form. While there
are few details in the source code itself, the train-
ing data for this approach was apparently obtained
through Google?s internal data collections.
Lui and Baldwin ?11: Lui and Baldwin (2011) re-
cently released a stand-alone LID tool, which they
call langid.py.6 They compared this system to
state-of-the-art LID methods and found it ?to be
faster whilst maintaining competitive accuracy.? We
use this system with its provided models only, as
the software readme notes ?training a model for
langid.py is a non-trivial process, due to the large
amount of computations required.? The sources of
the provided models are described in Lui and Bald-
win (2011). Although many languages are sup-
ported, we restrict the system to only choose be-
tween our data?s target languages (?5).
5 Experiments
The nine languages in our annotated data use one of
three different writing systems: Arabic, Devanagari,
or Cyrillic. We therefore define three classification
tasks, each choosing between three languages that
have the same writing system. We divide our an-
notated corpus into training, development and test
data for these experiments (Table 3). For the Ara-
bic data, we merge the tweets obtained via our two
collection methods (?2); for Devanagari/Cyrillic, all
tweets are obtained using the Sources method. We
ensure that tweets by a unique Twitter user occur
in at most only one of the sets. The proportion of
each language in each set is roughly the same as the
proportions of gold tweets in Table 1. All of our
Twitter-trained systems learn their models from this
training data, while all hyperparameter tuning (such
5Google once offered an online language-detection API, but
this service is now deprecated; moreover, it was rate-limited and
not licensed for research use (Lui and Baldwin, 2011).
6https://github.com/saffsd/langid.py
System Arab. Devan. Cyrill.
Trained on Twitter Corpus:
LogR: meta 79.8 74.7 82.0
LogR: chars 97.1 96.2 96.1
LogR: chars+meta 97.4 96.9 98.3
PPM 97.1 95.3 95.8
TextCat 96.3 89.1 90.3
Open-Domain: Trained on Other Materials:
Google CLD 90.5 N/A 91.4
Lui and Baldwin ?11 91.4 78.4 88.8
PPM (Wikip.) 97.6 95.8 95.7
PPM (All) 97.6 97.1 95.8
Trained on both Twitter and Other Materials:
PPM (Wikip.+Twit) 97.9 97.0 95.9
PPM (All+Twit) 97.6 97.9 96.0
Table 4: LID accuracy (%) of different systems on held-
out tweets. High LID accuracy on tweets is obtainable,
whether training in or out-of-domain.
as tuning the regularization parameter of the LogR
classifier) is done on the development set. Our eval-
uation metric is Accuracy: what proportion of tweets
in each held-out test set are predicted correctly.
6 Results
For systems trained on the Twitter data, both our
LogR and PPM system strongly outperform TextCat,
showing the effectiveness of our implemented ap-
proaches (Table 4). Meta features improve LogR
on each task. For systems trained on external data,
PPM strongly outperforms other systems, making
fewer than half the errors on each task. We also
trained PPM on both the relatively small number of
Twitter training samples and the much larger number
of other materials. The combined system is as good
or better than the separate models on each task.
We get more insight into our systems by seeing
how they perform as we vary the amount of train-
ing data. Figure 1 shows that with only a few hun-
dred annotated tweets, the LogR system gets over
90% accuracy, while performance seems to plateau
shortly afterwards. A similar story holds as we
vary the amount of out-of-domain training data for
the PPM system; performance improves fairly lin-
early as exponentially more training data is used, but
eventually begins to level off. Not only is PPM an
effective system, it can leverage a lot of training ma-
70
 50
 60
 70
 80
 90
 100
 10  100  1000
Ac
cu
ra
cy
 (%
)
Number of training tweets
Arabic
Devanagari
Cyrillic
Figure 1: The more training data the better, but accuracy
levels off: learning curve for LogR-chars (note log-scale).
 50
 60
 70
 80
 90
 100
 100  1000  10000 100000 1e+06  1e+07
Ac
cu
ra
cy
 (%
)
Number of characters of training data
Arabic
Devanagari
Cyrillic
Figure 2: Accuracy of PPM classifier using varying
amounts of Wikipedia training text (also on log-scale).
terials in order to obtain its high accuracy.
In Figure 3, we show how the accuracy of our sys-
tems varies over tweets grouped into bins by their
length. Performance on short tweets is much worse
than those closer to 140 characters in length.
We also examined aggregating predictions over
multiple tweets by the same user. We extracted all
users with ?4 tweets in the Devanagari test set (87
users in total). We then averaged the predictions of
the LogR system on random subsets of a user?s test
tweets, making a single decision for all tweets in a
subset. We report the mean accuracy of running this
approach 100 times with random subsets of 1, 2, 3,
and all 4 tweets used in the prediction. Even with
only 2 tweets per user, aggregating predictions can
reduce relative error by almost 60% (Table 5).
Encouraged by the accuracy of our systems on an-
notated data, we used our PPM system to analyze
a large number of un-annotated tweets. We trained
PPM models for 128 languages using data that in-
cludes Wikipedia (February 2012), news (e.g., BBC
News, Voice of America), and standard corpora such
 88
 90
 92
 94
 96
 98
 100
 40  60  80  100  120  140
Ac
cu
ra
cy
 (%
)
Avgerage length of tweet (binned)
Arabic
Devanagari
Cyrillic
Figure 3: The longer the tweet, the better: mean accuracy
of LogR by average length of tweet, with tweets grouped
into five bins by length in characters.
Number of Tweets 1 2 3 4
Accuracy 97.0 98.7 98.8 98.9
Table 5: The benefits of aggregating predictions by user:
Mean accuracy of LogR-chars as you make predictions
on multiple Devanagari tweets at a time
as Europarl, JRC-Acquis, and various LDC releases.
We then made predictions in the TREC Tweets2011
Corpus.7
We observed 65 languages in roughly 10 million
tweets. We calculated two other proportions using
auxiliary data:8 (1) the proportion of Wikipedia arti-
cles written in each language, and (2) the proportion
of speakers that speak each language. We use these
proportions to measure a language?s relative repre-
sentation on Twitter: we divide the tweet-proportion
by the Wikipedia and speaker proportions. Table 6
shows some of the most over-represented Twitter
languages compared to Wikipedia. E.g., Indonesian
is predicted to be 9.9 times more relatively com-
mon on Twitter than Wikipedia. Note these are pre-
dictions only; some English tweets may be falsely
marked as other languages due to English impurities
in our training sources. Nevertheless, the good rep-
resentation of languages with otherwise scarce elec-
tronic resources shows the potential of using Twitter
to build language-specific social media collections.
7http://trec.nist.gov/data/tweets/ This corpus, de-
veloped for the TREC Microblog track (Soboroff et al, 2012), contains
a two-week Twitter sample from early 2011. We processed all tweets
that were obtained with a ?200? response code using the twitter-corpus-
tools package.
8From http://meta.wikimedia.org/wiki/List_of_
Wikipedias_by_speakers_per_article
71
Language Num. % of Tweets/ Tweets/
Tweets Tot. Wikip. Speakers
Indonesian 1055 9.0 9.9 3.1
Thai 238 2.0 5.7 1.9
Japanese 2295 19.6 5.0 8.8
Korean 446 3.8 4.0 3.2
Swahili 46 0.4 3.4 0.4
Portuguese 1331 11.4 3.2 2.8
Marathi 58 0.5 2.9 0.4
Malayalam 30 0.3 2.2 0.4
Nepali 23 0.2 2.1 0.8
Macedonian 61 0.5 1.9 13.9
Bengali 25 0.2 1.9 0.1
Turkish 174 1.5 1.7 1.1
Arabic 162 1.4 1.6 0.3
Chinese 346 3.0 1.4 0.2
Spanish 696 5.9 1.4 0.7
Telugu 39 0.3 1.4 0.3
Croatian 79 0.7 1.3 6.1
English 2616 22.3 1.2 2.1
Table 6: Number of tweets (1000s) and % of total for lan-
guages that appear to be over-represented on Twitter (vs.
proportion of Wikipedia and proportion of all speakers).
7 Related Work
Researchers have tackled language identification us-
ing statistical approaches since the early 1990s.
Cavnar and Trenkle (1994) framed LID as a text
categorization problem and made their influential
TextCat tool publicly-available. The related problem
of identifying the language used in speech signals
has also been well-studied; for speaker LID, both
phonetic and sequential information may be help-
ful (Berkling et al, 1994; Zissman, 1996). Insights
from LID have also been applied to related problems
such as dialect determination (Zaidan and Callison-
Burch, 2011a) and identifying the native language
of non-native speakers (Koppel et al, 2005).
Recently, LID has received renewed interest as a
mechanism to help extract language-specific corpora
from the growing body of linguistic materials on the
web (Xia et al, 2009; Baldwin and Lui, 2010). Work
along these lines has found LID to be far from a
solved problem (Hughes et al, 2006; Baldwin and
Lui, 2010; Lui and Baldwin, 2011); the web in gen-
eral has exactly the uneven mix of style, languages,
and lengths-of-text that make the real problem quite
difficult. New application areas have also arisen,
each with their own unique challenges, such as LID
for search engine queries (Gottron and Lipka, 2010),
or person names (Bhargava and Kondrak, 2010).
The multilinguality of Twitter has led to the de-
velopment of ways to ensure language purity. Rit-
ter et al (2010) use ?a simple function-word-driven
filter. . . to remove non-English [Twitter] conversa-
tions,? but it?s unclear how much non-English sur-
vives the filtering and how much English is lost.
Tromp and Pechenizkiy (2011) and Carter et al
(2013) perform Twitter LID, but only targeting six
common European languages. We focus on low-
resource languages, where training data is scarce.
Our data and systems could enable better LID
for services like indigenoustweets.com, which
aims to ?strengthen minority languages through so-
cial media.?
8 Conclusions
Language identification is a key technology for ex-
tracting authentic, language-specific user-generated
text from social media. We addressed a previously
unexplored issue: LID performance on Twitter text
in low-resource languages. We have created and
made available a large corpus of human-annotated
tweets in nine languages and three non-Latin writ-
ing systems, and presented two systems that can pre-
dict tweet language with very high accuracy.9 While
challenging, LID on Twitter is perhaps not as diffi-
cult as first thought (Carter et al, 2013), although
performance depends on the amount of training data,
the length of the tweet, and whether we aggregate
information across multiple tweets by the same user.
Our next step will be to develop a similar approach
to handle romanized text. We also plan to develop
tools for identifying code-switching (switching lan-
guages) within a tweet.
Acknowledgments
We thank Chris Callison-Burch for his help with the
crowdsourcing. The first author was supported by the
Natural Sciences and Engineering Research Council of
Canada. The third author was supported by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015
9The annotated corpus and PPM system are available online
at: http://apl.jhu.edu/
?
paulmac/lid.html
72
References
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In
Proc. HLT-NAACL, pages 229?237.
Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.
2002. Language trees and zipping. Physical Review
Letters, 88(4):2?5.
Kay Berkling, Takayuki Arai, and Etienne Barnard.
1994. Analysis of phoneme-based features for lan-
guage identification. In Proc. ICASSP, pages 289?
292.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Andrej Bratko, Gordon V. Cormack, Bogdan Filipic,
Thomas R. Lynam, and Blaz Zupan. 2006. Spam
filtering using statistical data compression models.
JMLR, 6:2673?2698.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proc. NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk, pages 1?12.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proc. Symposium on
Document Analysis and Information Retrieval, pages
161?175.
John G. Cleary, Ian, and Ian H. Witten. 1984. Data
compression using adaptive coding and partial string
matching. IEEE Transactions on Communications,
32:396?402.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Clayton Fink, Jonathon Kopecky, Nathan Bos, and Max
Thomas. 2012. Mapping the Twitterverse in the devel-
oping world: An analysis of social media use in Nige-
ria. In Proc. International Conference on Social Com-
puting, Behavioral Modeling, and Prediction, pages
164?171.
Eibe Frank, Chang Chui, and Ian H. Witten. 2000. Text
categorization using compression models. In Proc.
DCC-00, IEEE Data Compression Conference, Snow-
bird, US, pages 200?209. IEEE Computer Society
Press.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2001.
Automatic web search query generation to create mi-
nority language corpora. In Proceedings of the 24th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?01, pages 432?433, New York, NY, USA. ACM.
Thomas Gottron and Nedim Lipka. 2010. A comparison
of language identification approaches on short, query-
style texts. In Proc. ECIR, pages 611?614.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew Mackinlay. 2006. Reconsid-
ering language identification for written language re-
sources. In Proc. LREC, pages 485?488.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proc. NAACL HLT 2010 Workshop
on Creating Speech and Language Data with Ama-
zon?s Mechanical Turk, pages 108?113.
Emre Kiciman. 2010. Language differences and meta-
data features on Twitter. In Proc. SIGIR 2010 Web
N-gram Workshop, pages 47?51.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. KDD, pages 624?628.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Proc.
IJCNLP, pages 553?561.
Paul McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. J.
Comput. Sci. Coll., 20(3):94?101.
D. Pavelec, L. S. Oliveira, E. Justino, F. D. Nobre Neto,
and L. V. Batista. 2009. Compression and stylometry
for author identification. In Proc. IJCNN, pages 669?
674, Piscataway, NJ, USA. IEEE Press.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Proc.
HLT-NAACL, pages 172?180.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proc. EMNLP, pages 583?593.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP, pages 254?263.
Ian Soboroff, Dean McCullough, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Richard McCreadie. 2012.
Evaluating real-time search over tweets. In Proc.
ICWSM.
73
William John Teahan. 2000. Text classification and
segmentation using minimum cross-entropy. In Proc.
RIAO, pages 943?961.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proc. EACL, pages 870?878.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal content.
In Proc. ACL, pages 37?41.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proc. ACL, pages 1220?1229.
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of tele-
phone speech. IEEE Transactions on Speech and Au-
dio Processing, 4(1):31?44.
74
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45?53,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP,
and FrameNet Annotation Standards
Jacqueline Aguilar and Charley Beller and Paul McNamee and Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD, USA
Stephanie Strassel and Zhiyi Song and Joe Ellis
University of Pennsylvania
Linguistic Data Consortium (LDC)
Philadelphia, PA, USA
Abstract
The resurgence of effort within computa-
tional semantics has led to increased in-
terest in various types of relation extrac-
tion and semantic parsing. While var-
ious manually annotated resources exist
for enabling this work, these materials
have been developed with different stan-
dards and goals in mind. In an effort
to develop better general understanding
across these resources, we provide a sum-
mary overview of the standards underly-
ing ACE, ERE, TAC-KBP Slot-filling, and
FrameNet.
1 Overview
ACE and ERE are comprehensive annotation stan-
dards that aim to consistently annotate Entities,
Events, and Relations within a variety of doc-
uments. The ACE (Automatic Content Extrac-
tion) standard was developed by NIST in 1999 and
has evolved over time to support different evalua-
tion cycles, the last evaluation having occurred in
2008. The ERE (Entities, Relations, Events) stan-
dard was created under the DARPA DEFT pro-
gram as a lighter-weight version of ACE with the
goal of making annotation easier, and more con-
sistent across annotators. ERE attempts to achieve
this goal by consolidating some of the annotation
type distinctions that were found to be the most
problematic in ACE, as well as removing some
more complex annotation features.
This paper provides an overview of the relation-
ship between these two standards and compares
them to the more restricted standard of the TAC-
KBP slot-filling task and the more expansive stan-
dard of FrameNet. Sections 3 and 4 examine Rela-
tions and Events in the ACE/ERE standards, sec-
tion 5 looks at TAC-KBP slot-filling, and section
6 compares FrameNet to the other standards.
2 ACE and ERE Entity Tagging
Many of the differences in Relations and Events
annotation across the ACE and ERE standards
stem from differences in entity mention tagging.
This is simply because Relation and Event tagging
relies on the distinctions established in the entity
tagging portion of the annotation process. For ex-
ample, since ERE collapses the ACE Facility and
Location Types, any ACE Relation or Event that
relied on that distinction is revised in ERE. These
top-level differences are worth keeping in mind
when considering how Events and Relations tag-
ging is approached in ACE and ERE:
? Type Inventory: ACE and ERE share the Per-
son, Organization, Geo-Political Entity, and
Location Types. ACE has two additional
Types: Vehicle and Weapon. ERE does not
account for these Types and collapses the Fa-
cility and Location Types into Location. ERE
also includes a Title Type to address titles,
honorifics, roles, and professions (Linguis-
tic Data Consortium, 2006; Linguistic Data
Consortium, 2013a).
? Subtype Annotation: ACE further classifies
entity mentions by including Subtypes for
each determined Type; if the entity does not
fit into any Subtype, it is not annotated. ERE
annotation does not include any Subtypes.
? Entity Classes: In addition to Subtype, ACE
also classifies each entity mention according
45
1996 1998 2000 2002 2004 2006 2008 2010 2012
F
r
a
m
e
N
e
t
p
r
o
j
e
c
t
c
r
e
a
t
e
d
A
C
E
d
e
v
e
l
o
p
e
d
m
o
s
t
c
o
m
p
r
e
h
e
n
s
i
v
e
A
C
E
c
o
r
p
u
s
l
a
s
t
A
C
E
e
v
a
l
fi
r
s
t
T
A
C
-
K
B
P
E
R
E
c
r
e
a
t
e
d
Figure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standards
to entity class (Specific, Generic, Attributive,
and Underspecified).
? Taggability: ACE tags Attributive, Generic,
Specific, and Underspecified entity mentions.
ERE only tags Specific entity mentions.
? Extents and Heads: ACE marks the full noun
phrase of an entity mention and tags a head
word. ERE handles tagging based on the
mention level of an entity; in Name mentions
(NAM) the name is the extent, in Nominal
mentions (NOM) the full noun phrase is the
extent, in Pronoun mentions (PRO) the pro-
noun is the extent.
? Tags: ERE only specifies Type and Men-
tion level (NAM, NOM, PRO). ACE speci-
fies Type, Subtype, Entity Class (Attributive,
Generic, Specific, Underspecified), and Men-
tion Level (NAM, NOM, PRO, Headless).
3 Relations in ACE and ERE
In the ACE and ERE annotation models, the goal
of the Relations task is to detect and character-
ize relations of the targeted types between enti-
ties (Linguistic Data Consortium, 2008; Linguistic
Data Consortium, 2013c). The purpose of this task
is to extract a representation of the meaning of the
text, not necessarily tied to underlying syntactic
or lexical semantic representations. Both models
share similar overarching guidelines for determin-
ing what is taggable. For relations the differences
lie in the absence or presence of additional fea-
tures, syntactic classes, as well as differences in
assertion, trigger words, and minor subtype varia-
tions.
3.1 Similarities in Relations Annotation
In addition to comprising similar Types (both
models include Physical and Part.Whole Types as
well as slightly different Types to address Affilia-
tion and Social relations) used to characterize each
relation, ACE and ERE share important similar-
ities concerning their relation-tagging guidelines.
These include:
? Limiting relations to only those expressed in
a single sentence
? Tagging only for explicit mention
? No ?promoting? or ?nesting? of taggable en-
tities. In the sentence, Smith went to a hotel
in Brazil, (Smith, hotel) is a taggable Phys-
ical.Located relation, but (Smith, Brazil) is
not. This is because in order to tag this as
such, one would have to promote ?Brazil?.
? Tagging for past and former relations
? Two different Argument slots (Arg1 and
Arg2) are provided for each relation to cap-
ture the importance of Argument ordering.
? Arguments can be more than one token (al-
though ACE marks the head as well)
? Using ?templates? for each relation
Type/Subtype (e.g., in a Physical.Located
relation, the Person that is located some-
where will always be assigned to Arg1 and
the place in which the person is located will
always be assigned to Arg2).
? Neither model tags for negative relations
? Both methods contain argument span bound-
aries. That is, the relations should only in-
clude tagged entities within the extent of a
sentence.
3.2 Differences in Assertion, Modality, and
Tense
A primary difference between these two annota-
tion models is a result of ERE only annotating as-
serted events while ACE also includes hypothet-
icals. ACE accounts for these cases by including
two Modality attributes: ASSERTED and OTHER
46
(Linguistic Data Consortium, 2008). For exam-
ple, in the sentence, We are afraid that Al-Qaeda
terrorists will be in Baghdad, ACE would tag this
as an OTHER attribute, where OTHER pertains to
situations in ?some other world defined by coun-
terfactual constraints elsewhere in the context?,
whereas ERE would simply not tag a relation in
this sentence. Additionally, while both ACE and
ERE tag past and former relations, ACE goes fur-
ther to mark the Tense of each relation by means
of four attributes: Past, Future, Present and Un-
specified.
3.3 Syntactic Classes
ACE further justifies the tagging of each Relation
through Syntactic Classes. The primary function
of these classes is to serve as a sanity check on
taggability and as an additional constraint for tag-
ging. These classes include: Possessive, Prepo-
sition, PreMod, Coordination, Formulaic, Partic-
ipal, Verbal, Relations Expressed by Verbs, and
Other. Syntactic classes are not present in ERE
relations annotation.
3.4 Triggers
Explicit trigger words do not exist in ACE relation
annotation; instead, the model annotates the full
syntactic clause that serves as the ?trigger? for the
relation. ERE attempts to minimize the annotated
span by allowing for the tagging of an optional
trigger word, defined as ?the smallest extent of text
that indicates a relation Type and Subtype? (Lin-
guistic Data Consortium, 2013c). These triggers
are not limited to a single word, but can also be
composed of a phrase or any extent of the text that
indicates a Type/Subtype relation, left to the dis-
cretion of the annotator. It is common for preposi-
tions to be triggers, as in John is in Chicago. How-
ever, sometimes no trigger is needed because the
syntax of the sentence is such that it indicates a
particular relation Type/Subtype without a word to
explicitly signal the relation.
3.5 Types and Subtypes of Relations
There are three types of relations that contain var-
ied Subtypes between ERE and ACE. These are
the Physical, Part-Whole, Social and Affiliation
Types. The differences are a result of ERE collaps-
ing ACE Types and Subtypes into more concise, if
less specific, Type groups.
Physical Relation Type Differences The main
differences in the handling of the physical rela-
tions between ACE and ERE are shown in Table
1. ACE only marks Location for PERSON enti-
ties (for Arg1). ERE uses Location for PERSON
entities being located somewhere as well as for
a geographical location being part of another ge-
ographical location. Additionally, ACE includes
?Near? as a Subtype. This is used for when an en-
tity is explicitly near another entity, but neither en-
tity is a part of the other or located in/at the other.
ERE does not have an equivalent Subtype to ac-
count for this physical relation. Instead, ERE in-
cludes ?Origin? as a Subtype. This is used to de-
scribe the relation between a PER and an ORG.
ACE does not have a Physical Type equivalent,
but it does account for this type of relation within
a separate General Affiliation Type and ?Citizen-
Resident-Religion-Ethnicity? Subtype.
Part-Whole Relation Differences In Table 2,
note that ACE has a ?Geographical? Subtype
which captures the location of a FAC, LOC, or
GPE in or at, or as part of another FAC, LOC,
or GPE. Examples of this would be India con-
trolled the region or a phrase such as the Atlanta
area. ERE does not include this type of annota-
tion option. Instead, ERE tags these regional re-
lations as Physical.Located. ACE and ERE do
share a ?Subsidiary? Subtype which is defined in
both models as a ?category to capture the own-
ership, administrative and other hierarchical rela-
tionships between ORGs and/or GPEs? (Linguis-
tic Data Consortium, 2008; Linguistic Data Con-
sortium, 2013c).
Social and Affiliation Relation Differences
The most evident discrepancy in relation anno-
tation between the two models lies in the So-
cial and Affiliation Relation Types and Subtypes.
For social relations, ACE and ERE have three
Subtypes with similar goals (Business, Family,
Unspecified/Lasting-Personal) but ERE has an ad-
ditional ?Membership? Subtype, as shown in Ta-
ble 3. ACE addresses all ?Membership? relations
in its Affiliation Type. ERE also includes the ?So-
cial.Role? Subtype in order to address the TITLE
entity type, which only applies to ERE. How-
ever, both models agree that the arguments for
each relation must be PERSON entities and that
they should not include relationships implied from
interaction between two entities (e.g., President
47
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Physical Located PER, GPE, LOC GPE, LOC
Physical Origin PER, ORG GPE, LOC
ACE
Physical Located PER FAC, LOC, GPE
Physical Near PER, FAC, GPE, LOC FAC, GPE, LOC
Table 1: Comparison of Permitted Relation Arguments for the Physical Type Distinction in the ERE and
ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Part-Whole Subsidiary ORG ORG, GPE
ACE
Part-Whole Geographical FAC, LOC, GPE FAC, LOC, GPE
Part-Whole Subsidiary ORG ORG, GPE
Table 2: Comparison of Permitted Relation Arguments for the Part-Whole Type and Subtype Distinctions
in the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Social Business PER PER
Social Family PER PER
Social Membership PER PER
Social Role TTL PER
Social Unspecified PER PER
ACE
Personal-Social Business PER PER
Personal-Social Family PER PER
Personal-Social Lasting-Personal PER PER
Table 3: Comparison of Permitted Relation Arguments for the Social Type and Subtype Distinctions in
the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Affiliation Employment/Membership PER, ORG,
GPE
ORG, GPE
Affiliation Leadership PER ORG, GPE
ACE
ORG-Affiliation Employment PER ORG, GPE
ORG-Affiliation Ownership PER ORG
ORG-Affiliation Founder PER, ORG ORG, GPE
ORG-Affiliation Student-Alum PER ORG.Educational
ORG-Affiliation Sports-Affiliation PER ORG
ORG-Affiliation Investor-Shareholder PER, ORG,
GPE
ORG, GPE
ORG-Affiliation Membership PER, ORG,
GPE
ORG
Agent-Artifact User-Owner-Inventor-
Manufacturer
PER, ORG,
GPE
FAC
Gen-Affiliation Citizen-Resident-Religion-
Ethnicity
PER PER.Group,
LOC, GPE,
ORG
Gen-Affiliation Org-Location-Origin ORG LOC, GPE
Table 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctions
in the ERE and ACE Guidelines
48
Clinton met with Yasser Arafat last week would
not be considered a social relation).
As for the differences in affiliation relations,
ACE includes many Subtype possibilities which
can more accurately represent affiliation, whereas
ERE only observes two Affiliation Subtype op-
tions (Table 4).
4 Events in ACE and ERE
Events in both annotation methods are defined as
?specific occurrences?, involving ?specific partic-
ipants? (Linguistic Data Consortium, 2005; Lin-
guistic Data Consortium, 2013b). The primary
goal of Event tagging is to detect and character-
ize events that include tagged entities. The central
Event tagging difference between ACE and ERE
is the level of specificity present in ACE, whereas
ERE tends to collapse tags for a more simplified
approach.
4.1 Event Tagging Similarities
Both annotation schemas annotate the same ex-
act Event Types: LIFE, MOVEMENT, TRANS-
ACTION, BUSINESS, CONFLICT, CONTACT,
PERSONNEL, and JUSTICE events. Both anno-
tation ontologies also include 33 Subtypes for each
Type. Furthermore, both rely on the expression
of an occurrence through the use of a ?Trigger?.
ACE, however, restricts the trigger to be a single
word that most clearly expresses the event occur-
rence (usually a main verb), while ERE allows for
the trigger to be a word or a phrase that instanti-
ates the event (Linguistic Data Consortium, 2005;
Linguistic Data Consortium, 2013b). Both meth-
ods annotate modifiers when they trigger events
as well as anaphors, when they refer to previously
mentioned events. Furthermore, when there is
any ambiguity about which trigger to select, both
methods have similar rules established, such as
the Stand-Alone Noun Rule (In cases where more
than one trigger is possible, the noun that can be
used by itself to refer to the event will be selected)
and the Stand-Alone Adjective Rule (Whenever a
verb and an adjective are used together to express
the occurrence of an Event, the adjective will be
chosen as the trigger whenever it can stand-alone
to express the resulting state brought about by the
Event). Additionally, both annotation guidelines
agree on the following:
? Tagging of Resultative Events (states that re-
sult from taggable Events)
? Nominalized Events are tagged as regular
events
? Reported Events are not tagged
? Implicit events are not tagged
? Light verbs are not tagged
? Coreferential Events are tagged
? Tagging of multi-part triggers (both parts are
tagged only if they are contiguous)
4.2 Event Tagging Differences
One of the more general differences between ERE
and ACE Event tagging is the way in which each
model addresses Event Extent. ACE defines the
extent as always being the ?entire sentence within
which the Event is described? (Linguistic Data
Consortium, 2005). In ERE, the extent is the
entire document unless an event is coreferenced
(in which case, the extent is defined as the ?span
of a document from the first trigger for a par-
ticular event to the next trigger for a particular
event.? This signifies that the span can cross
sentence boundaries). Unlike ACE, ERE does
not delve into indicating Polarity, Tense, Gener-
icity, and Modality. ERE simplifies any anno-
tator confusion engendered by these features by
simply not tagging negative, future, hypotheti-
cal, conditional, uncertain or generic events (al-
though it does tag for past events). While ERE
only tags attested Events, ACE allows for irrealis
events, and includes attributes for marking them
as such: Believed Events; Hypothetical Events;
Commanded and Requested Events; Threatened,
Proposed and Discussed Events; Desired Events;
Promised Events; and Otherwise Unclear Con-
structions. Additionally both ERE and ACE tag
Event arguments as long as the arguments occur
within the event mention extent (another way of
saying that a taggable Event argument will occur
in the same sentence as the trigger word for its
Event). However, ERE and ACE have a diverging
approach to argument tagging:
? ERE is limited to pre-specified arguments for
each event and relation subtype. The pos-
sible arguments for ACE are: Event partici-
pants (limited to pre-specified roles for each
event type); Event-specific attributes that are
associated with a particular event type (e.g.,
the victim of an attack); and General event
attributes that can apply to most or all event
types (e.g., time, place).
49
? ACE tags arguments regardless of modal cer-
tainty of their involvement in the event. ERE
only tags asserted participants in the event.
? The full noun phrase is marked in both ERE
and ACE arguments, but the head is only
specified in ACE. This is because ACE han-
dles entity annotation slightly differently than
ERE does; ACE marks the full noun phrase
with a head word for entity mention, and ERE
treats mentions differently based on their syn-
tactic features (for named or pronominal en-
tity mentions the name or pronominal itself
is marked, whereas for nominal mentions the
full noun phrase is marked).
Event Type and Subtype Differences Both an-
notation methods have almost identical Event
Type and Subtype categories. The only differences
between both are present in the Contact and Move-
ment Event Types.
A minor distinction in Subtype exists as a re-
sult of the types of entities that can be trans-
ported within the Movement Type category. In
ACE, ARTIFACT entities (WEAPON or VEHI-
CLE) as well as PERSON entities can be trans-
ported, whereas in ERE, only PERSON entities
can be transported. The difference between the
Phone-Write and Communicate Subtypes merely
lies in the definition. Both Subtypes are the de-
fault Subtype to cover all Contact events where
a ?face-to-face? meeting between sender and re-
ceiver is not explicitly stated. In ACE, this contact
is limited to written or telephone communication
where at least two parties are specified to make
this event subtype less open-ended. In ERE, this
requirement is simply widened to comprise elec-
tronic communication as well, explicitly including
those via internet channels (e.g., Skype).
5 TAC-KBP
After the final ACE evaluation in 2008 there was
interest in the community to form an evaluation
explicitly focused on knowledge bases (KBs) cre-
ated from the output of extraction systems. NIST
had recently started the Text Analysis Conference
series for related NLP tasks such as Recognizing
Textual Entailment, Summarization, and Question
Answering. In 2009 the first Knowledge Base
Population track (TAC-KBP) was held featuring
two initial tasks: (a) Entity Linking ? linking en-
tities to KB entities, and (b) Slot Filling ? adding
information to entity profiles that is missing from
the KB (McNamee et al., 2010). Due to its gener-
ous license and large scale, a snapshot of English
Wikipedia from late 2008 has been used as the ref-
erence KB in the TAC-KBP evaluations.
5.1 Slot Filling Overview
Unlike ACE and ERE, Slot Filling does not have
as its primary goal the annotation of text. Rather,
the aim is to identify knowledge nuggets about a
focal named entity using a fixed inventory of re-
lations and attributes. For example, given a fo-
cal entity such as former Ukrainian prime minister
Yulia Tymoshenko, the task is to identify attributes
such as schools she attended, occupations, and im-
mediate family members. This is the same sort
of information commonly listed about prominent
people in Wikipedia Infoboxes and in derivative
databases such as FreeBase and DBpedia.
Consequently, Slot Filling is somewhat of a hy-
brid between relation extraction and question an-
swering ? slot fills can be considered as the cor-
rect responses to a fixed set of questions. The rela-
tions and attributes used in the 2013 task are pre-
sented in Table 5.
5.2 Differences with ACE-style relation
extraction
Slot Filling in TAC-KBP differs from extraction in
ACE and ERE in several significant ways:
? information is sought for named entities,
chiefly PERs and ORGs;
? the focus is on values not mentions;
? assessment is more like QA; and,
? events are handled as uncorrelated slots
In traditional IE evaluation, there was an
implicit skew towards highly attested in-
formation such as leader(Bush, US), or
capital(Paris, France). In contrast, TAC-KBP
gives full credit for finding a single instance of a
correct fill instead of every attestation of that fact.
Slot Filling assessment is somewhat simpler
than IE annotation. The assessor must decide
if provenance text is supportive of a posited fact
about the focal entity instead of annotating a doc-
ument with all evidenced relations and events for
any entity. For clarity and to increase assessor
agreement, guidelines have been developed to jus-
tify when a posited relation is deemed adequately
supported from text. Additionally, the problem of
50
Relations Attributes
per:children org:shareholders per:alternate names org:alternate names
per:other family org:founded by per:date of birth org:political religious affiliation
per:parents org:top members employees per:age org:number of employees members
per:siblings org:member of per:origin org:date founded
per:spouse org:members per:date of death org:date dissolved
per:employee or member of org:parents per:cause of death org:website
per:schools attended org:subsidiaries per:title
per:city of birth org:city of headquarters per:religion
per:stateorprovince of birth org:stateorprovince of headquarters per:charges
per:country of birth org:country of headquarters
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
per:city of death
per:stateorprovince of death
per:country of death
Table 5: Relation and attributes for PERs and ORGs.
slot value equivalence becomes an issue - a sys-
tem should be penalized for redundantly asserting
that a person has four children named Tim, Beth,
Timothy, and Elizabeth, or that a person is both a
cardiologist and a doctor.
Rather than explicitly modeling events, TAC-
KBP created relations that capture events, more
in line with the notion of Infobox filling or ques-
tion answering (McNamee et al., 2010). For exam-
ple, instead of a criminal event, there is a slot fill
for charges brought against an entity. Instead of a
founding event, there are slots like org:founded by
(who) and org:date founded (when). Thus a state-
ment that ?Jobs is the founder and CEO of Apple?
is every bit as useful for the org:founded by rela-
tion as ?Jobs founded Apple in 1976.? even though
the date is not included in the former sentence.
5.3 Additional tasks
Starting in 2012 TAC-KBP introduced the ?Cold
Start? task, which is to literally produce a KB
based on the Slot Filling schema. To date, Cold
Start KBs have been built from collections of
O(50,000) documents, and due to their large size,
they are assessed by sampling. There is also
an event argument detection evaluation in KBP
planned for 2014.
Other TAC-KBP tasks have been introduced in-
cluding determining the timeline when dynamic
slot fills are valid (e.g., CEO of Microsoft), and
targeted sentiment.
6 FrameNet
The FrameNet project has rather different moti-
vations than either ACE/ERE or TAC-KBP, but
shares with them a goal of capturing informa-
tion about events and relations in text. FrameNet
stems from Charles Fillmore?s linguistic and lex-
icographic theory of Frame Semantics (Fillmore,
1976; Fillmore, 1982). Frames are descriptions
of event (or state) types and contain information
about event participants (frame elements), infor-
mation as to how event types relate to each other
(frame relations), and information about which
words or multi-word expressions can trigger a
given frame (lexical units).
FrameNet is designed with text annotation in
mind, but unlike ACE/ERE it prioritizes lexico-
graphic and linguistic completeness over ease of
annotation. As a result Frames tend to be much
finer grained than ACE/ERE events, and are more
numerous by an order of magnitude. The Berkeley
FrameNet Project (Baker et al., 1998) was devel-
oped as a machine readable database of distinct
frames and lexical units (words and multi-word
constructions) that were known to trigger specific
frames.
1
FrameNet 1.5 includes 1020 identified
frames and 11830 lexical units.
One of the most widespread uses of FrameNet
has been as a resource for Semantic Role Label-
ing (SRL) (Gildea and Jurafsky, 2002). FrameNet
related SRL was promoted as a task by the
SENSEVAL-3 workshop (Litkowski, 2004), and
the SemEval-2007 workshop (Baker et al., 2007).
(Das et al., 2010) is a current system for automatic
FrameNet annotation.
The relation and attribute types of TAC-KBP
and the relation and event types in the ACE/ERE
standards can be mapped to FrameNet frames.
The mapping is complicated by two factors.
The first is that FrameNet frames are gener-
ally more fine-grained than the ACE/ERE cate-
gories. As a result the mapping is sometimes
one-to-many. For example, the ERE relation Af-
1
This database is accessible via webpage (https:
//framenet.icsi.berkeley.edu/fndrupal/)
and as a collection of XML files by request.
51
Relations
FrameNet ACE ERE TAC-KBP
Kinship Personal-Social.Family Social.Family per:children
per:other family
per:parents
per:siblings
per:spouse
Being Employed ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member of
Membership org:member of
Being Located Physical.Located Physical.Located org:city of headquarters
org:stateorprovince of headquarters
org:country of headquarters
Events
FrameNet ACE ERE
Contacting Phone-Write Communicate
Extradition Justice-Extradition Justice-Extradition
Attack Conflict-Attack Conflict-Attack
Being Born Life-Be Born Life-Be Born
Attributes
FrameNet TAC-KBP
Being Named per:alternate names
Age per:age
Table 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBP
filiation.Employment/Membership covers both
the Being Employed frame and the Member-
ship frame. At the same time, while TAC-
KBP has only a handful of relations relative to
FrameNet, some of these relations are more fine-
grained than the analogous frames or ACE/ERE
relations. For example, the frame Kinship, which
maps to the single ERE relation Social.Family,
maps to five TAC-KBP relations, and the Be-
ing Located, which maps to the ACE/ERE rela-
tion Being.Located, maps to three TAC-KBP re-
lations. Rough mappings from a selection of rela-
tions, events, and attributes are given in Table 6.
The second complication arises from the fact
that FrameNet frames are more complex objects
than ERE/ACE events, and considerably more
complex than TAC-KBP relations. Rather than the
two entities related via a TAC-KBP or ACE/ERE
relation, some frames have upwards of 20 frame
elements. Table 7 shows in detail the mapping be-
tween frame elements in the Extradition frame and
ACE?s and ERE?s Justice-Extradition events. The
?core? frame elements map exactly to the ERE
event, the remaining two arguments in the ACE
event map to two non-core frame elements, and
the frame includes several more non-core elements
with no analogue in either ACE or ERE standards.
7 Conclusion
The ACE and ERE annotation schemas have
closely related goals of identifying similar in-
formation across various possible types of docu-
ments, though their approaches differ due to sepa-
rate goals regarding scope and replicability. ERE
differs from ACE in collapsing different Type dis-
tinctions and in removing annotation features in
order to eliminate annotator confusion and to im-
FrameNet ACE ERE
Authorities Agent-Arg Agent-Arg
Crime jursidiction Destination-Arg Destination-Arg
Current jursidiction Origin-Arg Origin-Arg
Suspect Person-Arg Person-Arg
Reason Crime-Arg
Time Time-Arg
Legal Basis
Manner
Means
Place
Purpose
Depictive
Table 7: Mapping between frame elements of Ex-
tradition (FrameNet), and arguments of Justice-
Extradition (ACE/ERE): A line divides core frame
elements (above) from non-core (below).
prove consistency, efficiency, and higher inter-
annotator agreement. TAC-KPB slot-filling shares
some goals with ACE/ERE, but is wholly fo-
cused on a set collection of questions (slots to
be filled) concerning entities to the extent that
there is no explicit modeling of events. At the
other extreme, FrameNet seeks to capture the
full range of linguistic and lexicographic varia-
tion in event representations in text. In general, all
events, relations, and attributes that can be repre-
sented by ACE/ERE and TAC-KBP standards can
be mapped to FrameNet representations, though
adjustments need to be made for granularity of
event/relation types and granularity of arguments.
Acknowledgements
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
52
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1, pages 86?90. Associ-
ation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic
structure extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of NAACL-HLT, pages 948?
956. Association for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lancec Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ace) program- tasks, data, and evaluation. In
Proceedings of LREC 2004: Fourth International
Conference on Language Resources and Evaluation,
Lisbon, May 24-30.
Charles J Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences, 280(1):20?32.
Charles Fillmore. 1982. Frame semantics. In Linguis-
tics in the morning calm, pages 111?137. Hanshin
Publishing Co.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Linguistic Data Consortium. 2005. ACE (automatic
content extraction) English annotation guidelines
for events. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 5.4.3 2005.07.01.
Linguistic Data Consortium. 2006. ACE (automatic
content extraction) English annotation guidelines
for entities. https://www.ldc.upenn.edu/
collaborations/past-projects/ace,
Version 5.6.6 2006.08.01.
Linguistic Data Consortium. 2008. ACE (automatic
content extraction) English annotation guidelines for
relations. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 6.0 2008.01.07.
Linguistic Data Consortium. 2013a. DEFT ERE anno-
tation guidelines: Entities v1.1, 05.17.2013.
Linguistic Data Consortium. 2013b. DEFT ERE anno-
tation guidelines: Events v1.1. 05.17.2013.
Linguistic Data Consortium. 2013c. DEFT ERE anno-
tation guidelines: Relations v1.1. 05.17.2013.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of LREC.
53
