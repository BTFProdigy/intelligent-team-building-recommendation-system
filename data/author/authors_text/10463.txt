Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 43?50,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
The Development of the Index Thomisticus Treebank Valency Lexicon 
 
 
Barbara McGillivray 
University of Pisa 
Italy 
b.mcgillivray@ling.unipi.it
Marco Passarotti 
Catholic University of the Sacred Heart 
Milan, Italy 
marco.passarotti@unicatt.it
 
  
Abstract 
We present a valency lexicon for Latin verbs 
extracted from the Index Thomisticus Tree-
bank, a syntactically annotated corpus of Me-
dieval Latin texts by Thomas Aquinas. 
In our corpus-based approach, the lexicon re-
flects the empirical evidence of the source 
data. Verbal arguments are induced directly 
from annotated data. 
The lexicon contains 432 Latin verbs with 270 
valency frames. The lexicon is useful for NLP 
applications and is able to support annotation. 
 
1 Introduction 
Over the last decades, annotated corpora and 
computational lexicons have gained an increas-
ing role among language resources in computa-
tional linguistics: on the one hand, they are used 
to train Natural Language Processing (NLP) 
tools such as parsers and PoS taggers; on the 
other hand, they are developed through auto-
matic procedures of linguistic annotation and 
lexical acquisition. 
The relation between annotated corpora and 
computational lexicons is circular: as a matter of 
fact, if linguistic annotation of textual data is 
supported and improved by the use of lexicons, 
these latter can be induced from annotated data 
in a corpus-based fashion. 
In the field of cultural heritage and in particu-
lar that of classical languages studies, much ef-
fort has been devoted throughout the years to the 
digitization of texts, but only recently have some 
projects begun to annotate them above the mor-
phological level. 
Concerning lexicology and lexicography of 
classical languages, a long tradition has produced 
and established many dictionaries, thesauri and 
lexicons, providing examples from real texts. 
Nevertheless, nowadays it is possible and indeed 
necessary to match lexicons with data from (an-
notated) corpora, and viceversa. This requires the 
scholars to exploit the vast amount of textual 
data from classical languages already available in 
digital format,1 and particularly those annotated 
at the highest levels. The evidence provided by 
the texts themselves can be fully represented in 
lexicons induced from these data. Subsequently, 
these lexicons can be used to support the textual 
annotation itself in a virtuous circle. 
This paper reports on the creation of a valency 
lexicon induced from the Index Thomisticus 
Treebank, a syntactically annotated corpus of 
Medieval Latin texts by Thomas Aquinas. The 
paper is organised as follows: section 2 describes 
the available Latin treebanks, their annotation 
guidelines and gives some specific information 
on the Index Thomisticus treebank; section 3 
deals with the notion of valency, while section 4 
describes the state of the art on valency lexicons; 
section 5 illustrates the procedures of acquisition 
and representation of our valency lexicon; fi-
nally, section 6 draws some conclusions and de-
scribes future work. 
2 Latin Treebanks 
Latin is a richly inflected language, showing: 
- discontinuous constituents (?non-
projectivity?): this means that phrasal con-
stituents may not be continuous, but broken 
up by words of other constituents. An exam-
ple is the following sentence by Ovid 
(Metamorphoses, I.1-2): ?In nova fert ani-
mus mutatas dicere formas corpora? (?My 
mind leads me to tell of forms changed into 
new bodies?). In this sentence, both the 
nominal phrases ?nova corpora? and ?muta-
tas formas? are discontinuous; 
- moderately free word-order: for instance, the 
order of the words in a sentence like ?au-
                                                          
1 See, for instance, the Perseus Digital Library (Crane et al, 
2001), or data repositories such as LASLA (Denooz, 1996). 
43
daces fortuna iuvat? (?fortune favours the 
bold?) could be changed into ?fortuna au-
daces iuvat?, or ?fortuna iuvat audaces?, 
without affecting the meaning of the sen-
tence. 
These features of Latin influenced the choice 
of Dependency Grammars (DG)2 as the most 
suitable grammar framework for building Latin 
annotated corpora like treebanks. 
While since the 1970s the first treebanks were 
annotated via Phrase Structure Grammar (PSG)-
based schemata (as in IBM, Lancaster and, later 
on, Penn treebanks), in the past decade many 
projects of dependency treebanks development 
have started, such as the ALPINO treebank for 
Dutch (Van der Beek et al, 2002), the Turin 
University Treebank for Italian (Lesmo et al, 
2002), or the Danish Dependency Treebank 
(Kromann, 2003). On the one hand, this is due to 
the fact that the first treebanks were mainly Eng-
lish language corpora. PSG were a suitable 
framework for a poorly inflected language like 
English, showing a fixed word-order and few 
discontinuous constituents. Later on, the syntac-
tic annotation of moderately free word-order lan-
guages required the adoption of the DG frame-
work, which is more appropriate than PSG for 
such a task. On the other hand, Carroll et al 
(1998) showed that inter-annotator agreement 
was significantly better for dependency tree-
banks, indicating that phrase structure annotation 
was requiring too many irrelevant decisions (see 
also Lin, 1995). 
Although much Latin data is nowadays avail-
able in digital format, the first two projects for 
the development of Latin treebanks have only 
recently started: namely the Latin Dependency 
Treebank (LDT) at the Tufts University in Bos-
ton (within the Perseus Digital Library) based on 
texts of the Classical era (Bamman, 2006), and 
the Index Thomisticus Treebank (IT-TB) at the 
Catholic University of the Sacred Heart in Milan, 
based on the Opera omnia of Thomas Aquinas 
(Passarotti, 2007). 
Taking into account the above mentioned fea-
tures of Latin, both the treebanks independently 
chose the DG framework as the most suitable 
one for data annotation. The same approach was 
later on followed by a third Latin treebank now 
                                                          
2 With Tesni?re (1959) as a common background, there are 
many different current DG flavours. See for instance the 
following: Dependency Unification Grammar (Hellwig, 
1986), Functional Generative Description (Sgall, Haji?ov? 
and Panevov?, 1986), Meaning Text Theory (Mel??uk, 
1988), Word Grammar (Hudson, 1990). 
available, which is ongoing at the University of 
Oslo in the context of the PROIEL project 
(Pragmatic Resources in Old Indo-European 
Languages): the aim of PROIEL is the syntactic 
annotation of the oldest extant versions of the 
New Testament in Indo-European languages, 
including Greek, Latin, Gothic, Armenian and 
Church Slavonic (Haug and J?hndal, 2008). 
2.1 Annotation Guidelines 
Since LDT and IT-TB were the first projects of 
their kind for Latin, no prior established guide-
lines were available to rely on for syntactic anno-
tation. 
Therefore, the so-called ?analytical layer? of 
annotation of the Prague Dependency Treebank 
(PDT) for Czech (Haji? et al, 1999) was chosen 
and adapted to specific or idiosyncratic construc-
tions of Latin. These constructions (such as the 
ablative absolute or the passive periphrastic) 
could be syntactically annotated in several dif-
ferent ways and are common to Latin of all eras. 
Rather than have each treebank project decide 
upon and record each decision for annotating 
them, LDT and IT-TB decided to pool their re-
sources and create a single annotation manual 
that would govern both treebanks (Bamman et 
al., 2007a; Bamman et al, 2007b; Bamman et al, 
2008). 
As we are dealing with Latin dialects sepa-
rated by 13 centuries, sharing a single annotation 
manual is very useful for comparison purposes, 
such as checking annotation consistency or dia-
chronically studying specific syntactic construc-
tions. In addition, the task of data annotation 
through these common guidelines allows annota-
tors to base their decisions on a variety of exam-
ples from a wider range of texts and combine the 
two datasets in order to train probabilistic de-
pendency parsers. 
Although the PROIEL annotation guidelines 
are grounded on the same grammar framework 
as the LDT and IT-TB, they differ in a number of 
details, some of which are described in Passarotti 
(forthcoming). 
2.2 The Index Thomisticus Treebank 
The Index Thomisticus (IT) by Roberto Busa SJ 
(1974-1980) was begun in 1949 and is consid-
ered a groundbreaking project in computational 
linguistics. It is a database containing the Opera 
omnia of Thomas Aquinas (118 texts) as well as 
61 texts by other authors related to Thomas, for a 
total of around 11 million tokens. The corpus is 
morphologically tagged and lemmatised. 
44
Early in the 1970?s Busa started to plan a pro-
ject aimed at both the morphosyntactic disam-
biguation of the IT lemmatisation and the syntac-
tic annotation of its sentences. Today, these tasks 
are performed by the IT-TB project, which is 
part of the wider ?Lessico Tomistico Bicul-
turale?, a project whose target is the development 
of a lexicon from the IT texts.3
Presently, the size of the IT-TB is 46,456 to-
kens, for a total of 2,103 parsed sentences ex-
cerpted from the Scriptum super Sententiis Mag-
istri Petri Lombardi. 
3 Valency 
As outlined above, the notion of valency is gen-
erally defined as the number of complements 
required by a word: these obligatory comple-
ments are usually named ?arguments?, while the 
non-obligatory ones are referred to as ?adjuncts?. 
Although valency can refer to different parts of 
speech (usually verbs, nouns and adjectives), 
scholars have mainly focused their attention on 
verbs, so that the notion of valency often coin-
cides with verbal valency. 
Valency is widely used in DG formalisms, but 
it also figures in PSG-based formalisms like 
HPSG and LFG. 
While Karl B?hler can be considered as the 
pioneer of the modern theory of valency,4 Lucien 
Tesni?re is widely recognised as its real founder. 
Tesni?re views valency as a quantitative quality 
of verbs, since only verbs constrain both the 
quantity and the quality (i.e. nouns and adverbs) 
of their obligatory arguments; through a meta-
phor borrowed from drama, Tesni?re classifies 
dependents into actants (arguments) and circon-
stants (adjuncts): ?Le noeud verbal [?] exprime 
tout un petit drame. Comme un drame en effet, il 
comporte obligatoirement un proc?s, et le plus 
souvent des acteurs et des circonstances. Trans-
pos?s du plan de la r?alit? dramatique sur celui 
de la syntaxe structurale, le proc?s, les acteurs et 
les circonstances deviennent respectivement le 
verbe, les actants et les circonstants? (Tesni?re, 
1959: 102).5  
                                                          
3 http://itreebank.marginalia.it. 
4 In the Sprachtheorie, he writes that ?die W?rter einer bes-
timmten Wortklasse eine oder mehrere Leerstellen um sich 
er?ffnen, die durch W?rter bestimmter anderer Wortklassen 
ausgef?llt werden m?ssen? (B?hler, 1934: 173) (?words of a 
certain word-class open up around themselves one or sev-
eral empty spaces that have to be filled by words of certain 
other word-classes?; our translation). 
5 ?The verbal node expresses a whole little drama. As a 
drama, it implies a process and, most of the times, actors 
Arguments can be either obligatory or op-
tional, depending on which sense of the verb is 
involved. For example, the seem sense of the 
verb appear requires two obligatory arguments 
in active clauses, as in the following sentence: 
?That lawyer appears to love his work?. Here the 
second argument (?to love his work?) cannot be 
left out without changing the meaning of the 
verb. On the other hand, optional arguments are 
recorded into the verbal argument structure itself, 
althought they may not appear at the clausal 
level. For instance, in the following sentence the 
object required by the verb eat is missing, but the 
sentence is still acceptable: ?He eats (some-
thing)?. 
Optionality can also act at the communicative 
level as well as at the structural one. For in-
stance, adjuncts can be necessary for communi-
cative intelligibility in particular contexts, as in 
the following sentence: ?I met James at the Mar-
quee club?, where the locative adverbial (?at the 
Marquee club?) is required to answer a question 
like ?Where did you meet James??. On the other 
hand, structural optionality depends on the fea-
tures of the language and applies at the clausal 
level. For instance, as a poorly inflected lan-
guage, English requires the subject of a predicate 
to be expressed in declarative and interrogative 
main clauses, so that a sentence like the follow-
ing is ungrammatical if the subject is missing: 
?[I] slept all morning?. 
Given the so-called ?syntax-semantics inter-
face? (Levin, 1993), arguments are generally 
associated with a predicate sense rather than a 
predicate form, and are structured in sequences 
called ?subcategorization frames? (SCFs) or 
?complementation patterns?. For example, there 
is a semantic difference between the bill sense 
and the attack sense of the verb charge in Eng-
lish, as in the following sentences: 
- (a) ?The hotel charges 80 euros for a night?. 
- (b) ?The army charged the enemy?. 
In these sentences, the two predicate senses 
show two different SCFs: 
- (a) [Subj_NP, Pred, Obj_NP, Obj_PP-for] 
- (b) [Pred, Obj_NP] 
Arguments are also selected by verbs accord-
ing to lexical-semantic properties, called ?selec-
tional preferences? (SPs) or ?selectional restric-
tions?. For example, a sentence like ?*The train 
flew to Rome? is ungrammatical, since it violates 
                                                                                        
and circumstances. Transposed from the dramatic reality to 
structural syntax, the process, the actors and the circum-
stances respectively become the verb, the actants and the 
circumstants? (our translation). 
45
the SP of the verb fly on its subject and can only 
be accepted in a metaphorical context. 
4 Valency Lexicons 
Over the past years, several valency lexicons 
have been built within different theoretical 
frameworks: these lexicons have an important 
role in the NLP community thanks to their wide 
applications in NLP components, such as pars-
ing, word sense disambiguation, automatic verb 
classification and selectional preference acquisi-
tion. 
As shown in Ure?ov? (2004), a valency lexi-
con can also help the task of linguistic annotation 
(as in treebank development), providing annota-
tors with essential information about the number 
and types of arguments realized at the syntactic 
level for a specific verb, along with semantic 
information on the verb?s lexical preferences. 
In the phase of lexicon creation, both intui-
tion-based and corpus-based approaches can be 
pursued, according to the role played by human 
intuition and empirical evidence extracted from 
annotated corpora such as treebanks. 
For instance, lexicons like PropBank (Kings-
bury and Palmer, 2002), FrameNet (Ruppenhofer 
et al, 2006) and PDT-Vallex (Haji? et al, 2003) 
have been created in an intuition-based fashion 
and then checked and improved with examples 
from corpora. 
On the other side, research in lexical acqui-
sition has recently made available a number of 
valency lexicons automatically acquired from 
annotated corpora, such as VALEX (Korhonen, 
et al, 2006) and LexShem (Messiant et al, 
2008). Unlike the fully intuition-based ones, 
these lexicons aim at systematically reflecting 
the evidence provided by data, with very little 
human intervention. The role of intuition is 
therefore left to the annotation phase (where the 
annotator interprets the corpus data), and not ex-
tended to the development of the lexicon itself. 
Corpus-based lexicons show several advan-
tages if compared with traditional human-
developed dictionaries. Firstly, they systemati-
cally reflect the evidence of the corpus they were 
extracted from, while acquiring information spe-
cific to the domain of the corpus. Secondly, 
unlike manually built lexicons, they are not 
prone to human errors that are difficult to detect, 
such as omissions and inconsistencies. In addi-
tion, such lexicons usually display statistical in-
formation in their entries, such as the actual fre-
quency of subcategorization frames as attested in 
the original corpus. Finally, they are less costly 
than hand-crafted lexical resources in terms of 
time, money and human resources. 
While several subcategorization lexicons 
have been compiled for modern languages, much 
work in this field still remains to be done on 
classical languages such as Greek and Latin. Re-
garding Latin, Happ reports a list of Latin verbs 
along with their valencies (Happ, 1976: 480-
565). Bamman and Crane (2008) describe a ?dy-
namic lexicon? automatically extracted from the 
Perseus Digital Library, using the LDT as a 
training set. This lexicon displays qualitative and 
quantitative information on subcategorization 
patterns and selectional preferences of each word 
as it is used in every Latin author of the corpus. 
Relying on morphological tagging and statistical 
syntactic parsing of such a large corpus, their 
approach finds the most common arguments and 
the most common lexical fillers of these argu-
ments, thus reducing the noise caused by the 
automatic pre-processing of the data. 
5 The Index Thomisticus Treebank 
Valency Lexicon 
We propose a corpus-based valency lexicon for 
Latin verbs automatically induced from IT-TB 
data. The automatic procedure allows both the 
extension of this work to the LDT (thanks to the 
common annotation guidelines) and the updating 
of the lexicon as the treebank size increases. 
First, we automatically extract the argu-
ments of all the occurrences of verbal lemmata in 
the treebank, along with their morphological fea-
tures and lexical fillers. 
In the IT-TB, verbal arguments are anno-
tated using the following tags: Sb (Subject), Obj 
(Object), OComp (Object Complement) and 
Pnom (Predicate Nominal); adjuncts are anno-
tated with the tag Adv (Adverbial). The differ-
ence between Obj and Adv corresponds to the 
that between direct or indirect arguments (except 
subjects) and adjuncts. A special kind of Obj is 
the determining complement of the object, which 
is tagged with OComp, such as senatorem in the 
phrase ?aliquem senatorem facere? (?to nominate 
someone senator?). Conversely, the determining 
complement of the subject is tagged as Pnom, as 
in ?aliquis senator fit? (?someone becomes sena-
tor?).6
                                                          
6 As in the PDT, all of the syntactic tags can be appended 
with a suffix in the event that the given node is member of a 
coordinated construction (_Co), an apposition (_Ap) or a 
parenthetical statement (_Pa). 
46
In order to retrieve the arguments realised 
for each verbal occurrence in the treebank, spe-
cific database queries have been created to 
search for the nodes depending on a verbal head 
through the functional tags listed above. 
The head-dependent relation can be either 
direct or indirect, since intermediate nodes may 
intervene. These nodes are prepositions (tag 
AuxP), conjunctions (tag AuxC) and coordinat-
ing or apposing elements (respectively, tags Co-
ord and Apos). 
For example, see the following sentences: 
- [1] ?primo determinat formam baptismi;?7 
(?at first it determines the form of the bap-
tism;?) 
- [2] ?ly aliquid autem, et ly unum non deter-
minant aliquam formam vel naturam;?8 (?the 
?something? and the ?one? do not determine 
any form or nature?) 
Figure 1 reports the tree of sentence [1], 
where the Obj relation between the verbal head 
determinat and the dependent formam is direct. 
 
 
Figure 1. 
Tree of sentence [1] 
 
Figure 2 shows the tree of sentence [2]. In 
this tree, two coordinated subjects (aliquid and 
unum) and two coordinated objects (formam and 
naturam) depend on the common verbal head 
determinant through two different Coord nodes 
(et and vel)9. 
                                                          
7 Thomas, Super Sententiis Petri Lombardi, IV, Distinctio 3, 
Quaestio 1, Prologus, 41-6, 42-2. The edition of the text 
recorded in the IT is Thomas (1856-1858). 
8 Thomas, Super Sententiis Petri Lombardi, III, Distinctio 6, 
Quaestio 2, Articulus 1, Responsio ad Argumentum 7, 4-5, 
6-1. 
9 Following PDT-style, the distributed determination ali-
quam, which modifies both the coordinated objects formam 
 
Figure 2 
Tree of sentence [2] 
 
In the case of indirect relation, the interme-
diate nodes need to be detected and extracted, in 
order to be inserted into the lexicon as subcate-
gorization structures containing the syntactic 
roles of the verbal arguments. To represent these 
structures, we distinguished two major types of 
them: subcategorization frames (SCFs) and sub-
categorization classes (SCCs). 
An SCF contains the sequence of functional 
labels of verbal arguments as they appear in the 
sentence order, whereas an SCC reports the sub-
categorization elements disregarding their linear 
order in the sentence. SCFs and SCCs play a dif-
ferent role in our lexicon. On the one hand, SCFs 
are very detailed patterns useful for diachronic 
and/or comparative studies on linear order. On 
the other hand, SCCs are more general and make 
the data in the lexicon comparable with the sub-
categorization structures as usually defined in the 
literature and in other valency lexicons. For each 
of these structures we then created the following 
sub-types, ranging from the most specific to the 
least specific one. 
SCF1: subcategorization frame marking the 
full path between the verbal head (referred to as 
?V?) and each of its argument nodes in the tree. 
SCF1 also assigns the same index to those argu-
ment nodes linked by coordinating or apposing 
elements. For instance, the SCF1 of the verbal 
                                                                                        
and naturam, depends on the coordinating node vel. For 
more details, see Hajic et al (1999), 236-238. 
47
head determino10 in sentence [1] is ?V + Obj?, 
while in sentence [2] is ?(Coord)Sb_Co(1) + (Co-
ord)Sb_Co(1) + V + (Coord)Obj_Co(2) + (Co-
ord)Obj_Co(2)?. In the latter, the intermediate 
nodes Coord are in square brackets and indices 1 
and 2 link the coordinated nodes. These indices 
have been adopted in order to disambiguate sub-
categorization structures where more Obj_Co 
tags can refer to different verbal arguments. For 
instance, in a sentence like ?I give X and Y to W 
and Z?, both the tranferred objects (X and Y) and 
the receivers (W and Z) are annotated with 
Obj_Co. Using indices, the subcategorization 
structure of the verb give in this sentence appears 
as follows: ?Sb + V + (Coord)Obj_Co(1) + (Co-
ord)Obj_Co(1) + (Coord)Obj_Co(2) + (Co-
ord)Obj_Co(2)?. The indices cannot be applied a 
priori to subsequent arguments, since Latin, al-
lowing discontinuous constituents, can show 
cases where coindexed nodes are separated by 
other lexical items in the linear order. 
SCC1: the subcategorization class associated 
with SCF1. The SCC1 of the verb determino in 
[1] is ?{Obj}?, while in [2] is ?{(Coord)Sb_Co(1), 
(Coord)Sb_Co(1), (Coord)Obj_Co(2), (Co-
ord)Obj_Co(2)}?. 
SCF2: a subcategorization frame containing 
only the labels and the indices of the arguments, 
but not the full path. So, the SCF2 of determino 
in [1] is ?V + Obj?, while in [2] is ?Sb_Co(1) + 
Sb_Co(1) + V + Obj_Co(2) + Obj_Co(2)?. 
SCC2: the subcategorization class associated 
with SCF2. For determino, this is ?{Obj}? in [1] 
and ?{Sb_Co(1), Sb_Co(1), Obj_Co(2), Obj_Co(2)}? 
in [2]. 
SCC3: a subcategorization frame containing 
only the argument labels. The SCC3 of determino 
is ?{Obj}? in [1] and ?{Sb, Obj}? in [2], showing 
that in this sentence determino is used as a biar-
gumental verb, regardless of the number of lexi-
cal fillers realised for each of its arguments at the 
surface level. 
6 Conclusion and future work 
Presently, the size of the IT-TB valency lexicon 
is 432 entries (i.e. verbal lemmata, corresponding 
to 5966 wordforms), with 270 different SCF1s. In 
the near future, the lexicon will be enriched with 
valency information for nouns and adjectives. 
The corpus-based approach we followed in-
duces verbal arguments directly from annotated 
data, where the arguments may be present or not, 
                                                                                                                    
10 Determino is the lemma of both the wordforms determi-
nat (sentence [1]) and determinant (sentence [2]). 
depending on the features of the texts. Therefore, 
the lexicon reflects the empirical evidence given 
by the data it was extracted from, encouraging 
linguistic studies on the particular language do-
main of our corpus. 
In addition to the syntactic information re-
ported in the different types of SCFs and SCCs, 
it is possible at each stage to include both the 
morphological features and the lexical fillers of 
verbal arguments, helping define verbal selec-
tional preferences. 
The lexicon may also be useful for improv-
ing the performance of statistical parsers, enrich-
ing the information acquired by parsers on verbal 
entries. On the other hand, moving from parser 
performance to lexicon development, the lexicon 
can be induced from automatically parsed texts 
when an accurate parsing system is available. 
The syntactic and lexical data recorded in the 
lexicon are also important in further semantic 
NLP applications, such as word sense disam-
biguation, anaphora and ellipsis resolution, and 
selectional preference acquisition. Following a 
widespread approach in valency lexicons, a close 
connection between valency frames and word 
senses will be followed in the description of lexi-
con entries: this means that each headword entry 
of our lexicon will consist of one or more SCFs 
and SCCs, one for each sense of the word. 
We plan to make the lexicon available on-
line through a graphical interface usable also 
during the annotation procedures, as has been 
already done for the PDT via the tree editor 
TrEd.11 In this way, the consistency of the anno-
tation process can be tested and enforced thanks 
to the information stored in the lexicon. 
In order to test the accuracy of our system, it 
will be also necessary to evaluate the quality of 
our valency lexicon against the Perseus ?dy-
namic lexicon?, Happ?s list and other existing 
resources for Latin, such as traditional dictionar-
ies and thesauri. A comparison with the lexicon 
by Perseus is also very interesting in a contras-
tive diachronic perspective, as it may show im-
portant linguistic differences between Classical 
and Medieval Latin. 
Acknowledgments 
We would like to thank Paolo Ruffolo for his 
help in designing the database architecture. 
References 
11 TrEd is freely available at 
http://ufal.mff.cuni.cz/~pajas/tred/. 
48
David Bamman. 2006. The Design and Use of Latin 
Dependency Treebank. In Jan Haji? and Joakim 
Nivre (eds.), TLT 2006. Proceedings of the Fifth 
Workshop on Treebanks and Linguistic Theories. 
December 1-2, 2006, Prague, Czech Republic, In-
stitute of Formal and Applied Linguistics, Prague, 
Czech Republic, 67-78. 
David Bamman and Gregory Crane. 2008. Building a 
Dynamic Lexicon from a Digital Library. In Pro-
ceedings of the 8th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries (JCDL 2008), Pittsburgh. 
David Bamman, Marco Passarotti, Gregory Crane and 
Savina Raynaud. 2007a. Guidelines for the Syntac-
tic Annotation of Latin Treebanks, ?Tufts Univer-
sity Digital Library?. Available at: 
http://dl.tufts.edu/view_pdf.jsp?urn=tufts:facpubs:d
bamma01-2007.00002. 
David Bamman, Marco Passarotti, Gregory Crane and 
Savina Raynaud. 2007b. A Collaborative Model of 
Treebank Development. In Koenraad De Smedt, 
Jan Haji? and Sandra K?bler (eds.), Proceedings of 
the Sixth International Workshop on Treebanks 
and Linguistic Theories. December 7-8, 2007, Ber-
gen, Norway, Northern European Association for 
Language Technology (NEALT) Proceedings Se-
ries, Vol. 1, 1-6. 
David Bamman, Marco Passarotti, Roberto Busa and 
Gregory Crane. 2008. The annotation guidelines of 
the Latin Dependency Treebank and Index Thomis-
ticus Treebank. The treatment of some specific 
syntactic constructions in Latin. In Proceedings of 
the Sixth International Conference on Language 
Resources and Evaluation (LREC 2008). May 28-
30, 2008, Marrakech, Morocco, European Lan-
guage Resources Association (ELRA), 2008. 
Karl B?hler. 1934. Sprachtheorie: die Darstellungs-
funktion der Sprache, Jena: Gustav Fischer, Stutt-
gart. 
Roberto Busa. 1974?1980. Index Thomisticus: sancti 
Thomae Aquinatis operum omnium indices et con-
cordantiae, in quibus verborum omnium et singu-
lorum formae et lemmata cum suis frequentiis et 
contextibus variis modis referuntur quaeque / con-
sociata plurium opera atque electronico IBM 
automato usus digessit Robertus Busa SJ, From-
mann-Holzboog, Stuttgart-Bad Cannstatt. 
Gregory R. Crane, Robert F. Chavez, Anne Mahoney, 
Thomas L. Milbank, Jeff A. Rydberg-Cox, David 
A. Smith and Clifford E. Wulfman. 2001. Drudg-
ery and deep thought: Designing a digital library 
for the humanities. In Communications of the 
ACM, 44(5), 34-40. 
John Carroll, Ted Briscoe and Antonio Sanfilippo. 
1998. Parser Evaluation: a Survey and a New Pro-
posal. In Proceedings of the First International 
Conference on Language Resources and Evalua-
tion (LREC 1998). May 28-30, 1998, Granada, 
Spain, 447-454. 
Joseph Denooz. 1996. La banque de donn?es du labo-
ratoire d'analyse statistique des langues anciennes 
(LASLA). ? Le M?di?viste et l'ordinateur ?, 33, 14-
20. 
Jan Haji?, Jarmila Panevov?, Eva Bur??ov?, Zde?ka 
Ure?ov? and Alla B?mov?. 1999. Annotations at 
Analytical Level. Instructions for annotators, Insti-
tute of Formal and Applied Linguistics, Prague, 
Czech Republic. Available at: 
http://ufal.mff.cuni.cz/pdt2.0/doc/manuals/en/a-
layer/pdf/a-man-en.pdf. 
Jan Haji?, Jarmila Panevov?, Zde?ka Ure?ov?, Alla 
B?mov?, Veronika Kol?rov?-Rezn?ckov? and Petr 
Pajas. 2003. PDT-VALLEX: Creating a Large 
Coverage Valency Lexicon for Treebank Annota-
tion. In Joakim Nivre and Erhard Hinrichs (eds.), 
TLT 2003 ? Proceedings of the Second Workshop 
on Treebanks and Linguistic Theories, volume 9 of 
Mathematical Modelling in Physics, Engineering 
and Cognitive Sciences, V?xj? University Press, 
V?xj?, Sweden, 57-68. 
Heinz Happ. 1976. Grundfragen einer Dependenz-
Grammatik des Lateinischen, Vandenhoeck & Ru-
precht, Goettingen. 
Dag Haug and Marius J?hndal. 2008. Creating a Par-
allel Treebank of the Old Indo-European Bible 
Translations. In Proceedings of the Language 
Technology for Cultural Heritage Data Workshop 
(LaTeCH 2008), Marrakech, Morocco, 1st June 
2008, 27-34. 
Peter Hellwig. 1986. Dependency Unification Gram-
mar, In Proceedings of the 11th International Con-
ference on Computational Linguistics, Universit?t 
Bonn, Bonn, 195-198. 
Richard Hudson. 1990. English Word Grammar, 
Blackwell Publishers Ltd, Oxford, UK. 
Paul Kingsbury and Martha Palmer. 2002. From 
Treebank to Propbank. In Proceedings of the Third 
International Conference on Language Resources 
and Evaluation (LREC 2002), Las Palmas ? Gran 
Canaria, Spain. 
Anna Korhonen, Yuval Krymolowski and Ted 
Briscoe. 2006. A Large Subcategorization Lexicon 
for Natural Language Processing Applications. In 
Proceedings of the Fifth International Conference 
on Language Resources and Evaluation (LREC 
2006), Genoa, Italy. 
Matthias T. Kromann. 2003. The Danish Dependency 
Treebank and the underlying linguistic theory. In 
Joakim Nivre and Erhard Hinrichs (eds.), TLT 
2003 ? Proceedings of the Second Workshop on 
Treebanks and Linguistic Theories, volume 9 of 
Mathematical Modelling in Physics, Engineering 
and Cognitive Sciences, V?xj? University Press, 
V?xj?, Sweden. 
Leonardo Lesmo, Vincenzo Lombardo and Cristina 
Bosco. 2002. Treebank Development: the TUT 
Approach. In Rajeev Sangal and Sushma M. 
Bendre (eds.), Recent Advances in Natural 
Language Processing. Proceedings of 
International Conference on Natural Language 
49
Processing (ICON 2002), Vikas Publ. House, New 
Delhi, 61-70. 
Beth Levin. 1993. English verb classes and 
alternations: a preliminary investigation, 
University of Chicago Press, Chicago. 
Dekang Lin. 1995. A dependency-based method for 
evaluating broadcoverage parsers. In Proceedings 
of the IJCAI-95, Montreal, Canada, 1420-1425. 
Igor Mel??uk. 1988. Dependency Syntax: Theory and 
Practice, State University Press of New York, Al-
bany/NY. 
Cedric Messiant, Anna Korhonen and Thierry 
Poibeau. 2008. LexSchem: A Large 
Subcategorization Lexicon for French Verbs. In 
Proceedings of the Sixth International Conference 
on Language Resources and Evaluation (LREC 
2008). May 28-30, 2008, Marrakech, Morocco, 
European Language Resources Association 
(ELRA), 2008. 
Jarmila Panevov?. 1974-1975. On Verbal Frames in 
Functional Generative Description. Part I, ?Prague 
Bulletin of Mathematical Linguistics?, 22, 3-40; 
Part II, ?Prague Bulletin of Mathematical Linguis-
tics?, 23, 17-52. 
Marco Passarotti. 2007. Verso il Lessico Tomistico 
Biculturale. La treebank dell?Index Thomisticus. In 
Raffaella Petrilli and Diego Femia (eds.), Il filo del 
discorso. Intrecci testuali, articolazioni linguisti-
che, composizioni logiche. Atti del XIII Congresso 
Nazionale della Societ? di Filosofia del Linguag-
gio, Viterbo, 14-16 Settembre 2006, Aracne Editri-
ce, Pubblicazioni della Societ? di Filosofia del 
Linguaggio, 04, Roma, 187-205. 
Marco Passarotti. Forthcoming. Theory and Practice 
of Corpus Annotation in the Index Thomisticus 
Treebank. In Proceedings of the Conference 
?Trends in Computational and Formal Philology - 
Venice Padua, May 22-24, 2008?. 
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L. 
Petruck, Christopher R. Johnson and Jan 
Scheffczyk. 2006. FrameNet II. Extendend Theory 
and Practice. E-book available at 
http://framenet.icsi.berkeley.edu/index.php?option
=com_wrapper&Itemid=126. 
Petr Sgall, Eva Haji?ov? and Jarmila Panevov?. 1986. 
The Meaning of the Sentence in its Semantic and 
Pragmatic Aspects, D. Reidel, Dordrecht, NL. 
Lucien Tesni?re. 1959. ?l?ments de syntaxe struc-
turale, Editions Klincksieck, Paris, France. 
Thomas Aquinas. 1856-1858. Sancti Thomae 
Aquinatis, doctoris angelici, Ordinis 
praedicatorum Commentum in quatuor libros 
Sententiarum magistri Petri Lombardi, adjectis 
brevibus adnotationibus, Fiaccadori, Parma. 
Zdenka Ure?ov?. 2004. The Verbal Valency in the 
Prague Dependency Treebank from the Annotator's 
Point of View. Jazykovedn? ?stav ?. ?t?ra, SAV, 
Bratislava, Slovakia. 
Leonoor Van der Beek, Gosse Bouma, Rob Malouf 
and Gertjan van Noord. 2002. The Alpino 
Dependency Treebank. In Mariet Theune, Anton 
Nijholt and Hendri Hondorp (eds.), Proceedings of 
the Twelfth Meeting of Computational Linguistics 
in the Netherlands (CLIN 2001), Rodopi, 
Amsterdam, 8-22. 
50
Proceedings of the ACL 2010 Student Research Workshop, pages 73?78,
Uppsala, Sweden, 13 July 2010.
c
?2010 Association for Computational Linguistics
Automatic Selectional Preference Acquisition for Latin verbs
Barbara McGillivray
University of Pisa
Italy
b.mcgillivray@ling.unipi.it
Abstract
We present a system that automatically
induces Selectional Preferences (SPs) for
Latin verbs from two treebanks by using
Latin WordNet. Our method overcomes
some of the problems connected with data
sparseness and the small size of the input
corpora. We also suggest a way to evalu-
ate the acquired SPs on unseen events ex-
tracted from other Latin corpora.
1 Introduction
Automatic acquisition of semantic information
from corpora is a challenge for research on low-
resourced languages, especially when semanti-
cally annotated corpora are not available. Latin is
definitely a high-resourced language for what con-
cerns the number of available texts and traditional
lexical resources such as dictionaries. Neverthe-
less, it is a low-resourced language from a compu-
tational point of view (McGillivray et al, 2009).
As far as NLP tools for Latin are concerned,
parsing experiments with machine learning tech-
niques are ongoing (Bamman and Crane, 2008;
Passarotti and Ruffolo, forthcoming), although
more work is still needed in this direction, espe-
cially given the small size of the training data.
As a matter of fact, only three syntactically an-
notated Latin corpora are available (and still in
progress): the Latin Dependency Treebank (LDT,
53,000 tokens) for classical Latin (Bamman and
Crane, 2006), the Index Thomisticus Treebank (IT-
TB, 54,000 tokens) for Thomas Aquinas?s works
(Passarotti, 2007), and the PROIEL treebank (ap-
proximately 100,000 tokens) for the Bible (Haug
and J?ndal, 2008). In addition, a Latin version
of WordNet ? Latin WordNet (LWN; Minozzi,
(2009) ? is being compiled, consisting of around
10,000 lemmas inserted in the multilingual struc-
ture of MultiWordNet (Bentivogli et al, 2004).
The number and the size of these resources are
small when compared with the corpora and the
lexicons for modern languages, e. g. English.
Concerning semantic processing, no seman-
tically annotated Latin corpus is available yet;
building such a corpus manually would take con-
siderable time and energy. Hence, research in
computational semantics for Latin would benefit
from exploiting the existing resources and tools
through automatic lexical acquisition methods.
In this paper we deal with automatic acquisition
of verbal selectional preferences (SPs) for Latin,
i. e. the semantic preferences of verbs on their ar-
guments: e. g. we expect the object position of the
verb edo ?eat? to be mostly filled by nouns from the
food domain. For this task, we propose a method
inspired by Alishahi (2008) and outlined in an ear-
lier version on the IT-TB in McGillivray (2009).
SPs are defined as probability distributions over
semantic features extracted as sets of LWN nodes.
The input data are two subcategorization lexicons
automatically extracted from the LDT and the IT-
TB (McGillivray and Passarotti, 2009).
Our main contribution is to create a new tool for
semantic processing of Latin by adapting compu-
tational techniques developed for extant languages
to the special case of Latin. A successful adapta-
tion is contingent on overcoming corpus size dif-
ferences. The way our model combines the syntac-
tic information contained in the treebanks with the
lexical semantic knowledge from LWN allows us
to overcome some of the difficulties related to the
small size of the input corpora. This is the main
difference from corpora for modern languages, to-
gether with the absence of semantic annotation.
Moreover, we face the problem of evaluating our
system?s ability to generalize over unseen cases by
using text occurrences, as access to human linguis-
tic judgements is denied for Latin.
In the rest of the paper we will briefly summa-
rize previous work on SP acquisition and motivate
73
our approach (section 2); we will then describe our
system (section 3), report on first results and evalu-
ation (section 4), and finally conclude by suggest-
ing future directions of research (section 5).
2 Background and motivation
The state-of-the-art systems for automatic acqui-
sition of verbal SPs collect argument headwords
from a corpus (for example, apple, meat, salad as
objects of eat) and then generalize the observed
behaviour over unseen cases, either in the form of
words (how likely is it to find sausage in the object
position of eat?) or word classes (how likely is it
to find VEGETABLE, FOOD, etc?).
WN-based approaches translate the generaliza-
tion problem into estimating preference probabil-
ities over a noun hierarchy and solve it by means
of different statistical tools that use the input data
as a training set: cf. inter al. Resnik (1993), Li
and Abe (1998), Clark and Weir (1999). Agirre
and Martinez (2001) acquire SPs for verb classes
instead of single verb lemmas by using a semanti-
cally annotated corpus and WN.
Distributional methods aim at automatically in-
ducing semantic classes from distributional data in
corpora by means of various similarity measures
and unsupervised clustering algorithms: cf. e. g.
Rooth et al (1999) and Erk (2007). Bamman and
Crane (2008) is the only distributional approach
dealing with Latin. They use an automatically
parsed corpus of 3.5 million words, then calculate
SPs with the log-likelihood test, and obtain an as-
sociation score for each (verb, noun) pair.
The main difference between these previous
systems and our case is the size of the input cor-
pus. In fact, our dataset consists of subcatego-
rization frames extracted from two relatively small
treebanks, amounting to a little over 100,000 word
tokens overall. This results in a large number of
low-frequency (verb, noun) associations, which
may not reflect the actual distributions of Latin
verbs. This state improves if we group the obser-
vations into clusters. Such a method, proposed by
Alishahi (2008), proved effective in our case.
The originality of this approach is an incre-
mental clustering algorithm for verb occurrences
called frames which are identified by specific syn-
tactic and semantic features, such as the number
of verbal arguments, the syntactic pattern, and the
semantic properties of each argument, i. e. the
WN hypernyms of the argument?s fillers. Based
on a probabilistic measure of similarity between
the frames? features, the clustering produces larger
sets called constructions. The constructions for a
verb contribute to the next step, which acquires
the verb?s SPs as semantic profiles, i. e. probabil-
ity distributions over the semantic properties. The
model exploits the structure of WN so that predic-
tions over unseen cases are possible.
3 The model
The input data are two corpus-driven subcate-
gorization lexicons which record the subcatego-
rization frames of each verbal token occurring
in the corpora: these frames contain morpho-
syntactic information on the verb?s arguments, as
well as their lexical fillers. For example, ?eo
+ A (in)Obj[acc]{exsilium}? represents an active
occurrence of the verb eo ?go? with a prepositional
phrase introduced by the preposition in ?to, into?
and composed by an accusative noun phrase filled
by the lemma exsilium ?exile?, as in the sentence
1
(1) eat
go:SBJV.PRS.3SG
in
to
exsilium
exile:ACC.N.SG
?he goes into exile?.
We illustrate how we adapted Alishahi?s defini-
tions of frame features and formulae to our case.
Alishahi uses a semantically annotated English
corpus, so she defines the verb?s semantic prim-
itives, the arguments? participant roles and their
semantic categories; since we do not have such an-
notation, we used the WN semantic information.
The syntactic feature of a frame (ft
1
) is the
set of syntactic slots of its verb?s subcategoriza-
tion pattern, extracted from the lexicons. In the
above example, ?A (in)Obj[acc]?. In addition, the
first type of semantic features of a frame (ft
2
)
collects the semantic properties of the verb?s ar-
guments as the set of LWN synonyms and hy-
pernyms of their fillers. In the previous exam-
ple this is {exsilium ?exile?, proscriptio ?proscrip-
tion?, rejection, actio, actus ?act?}.
2
The second
type of semantic features of a frame (ft
3
) col-
lects the semantic properties of the verb in the
form of the verb?s synsets. In the above example,
these are all synsets of eo ?go?, among which ?{eo,
gradior, grassor, ingredior, procedo, prodeo,
1
Cicero, In Catilinam, II, 7.
2
We listed the LWN node of the lemma exsilium, followed
by its hypernyms; each node ? apart from rejection, which
is English and is not filled by a Latin lemma in LWN ? is
translated by the corresponding node in the English WN.
74
vado}? (?{progress, come on, come along, ad-
vance, get on, get alng, shape up}? in the En-
glish WN).
3.1 Clustering of frames
The constructions are incrementally built as new
frames are included in them; a new frame F is as-
signed to a construction K if F probabilistically
shares some features with the frames in K so that
K = argmax
k
P (k|F ) = argmax
k
P (k)P (F |k),
where k ranges over the set of all constructions,
including the baseline k
0
= {F}. The prior
probability P (k) is calculated from the number of
frames contained in k divided by the total number
of frames. Assuming that the frame features are
independent, the posterior probability P (F |k) is
the product of three probabilities, each one corre-
sponding to the probability that a feature displays
in k the same value it displays in F : P
i
(ft
i
(F )|k)
for i = 1, 2, 3:
P (F |k) =
?
i=1,2,3
P
i
(ft
i
(F )|k)
We estimated the probability of a match be-
tween the value of ft
1
in k and the value of ft
1
in F as the sum of the syntactic scores between
F and each frame h contained in k, divided the
number n
k
of frames in k:
P (ft
1
(F )|k) =
?
h?k
synt score(h, F )
n
k
where the syntactic score synt score(h, F ) =
|SCS(h)?SCS(F )|
|SCS(F )|
calculates the number of syntac-
tic slots shared by h and F over the number of
slots in F . P (ft
1
(F )|k) is 1 when all the frames
in k contain all the syntactic slots of F .
For each argument position a, we estimated the
probability P (ft
2
(F )|k) as the sum of the seman-
tic scores between F and each h in k:
P (ft
2
(F )|k) =
?
h?k
sem score(h, F )
n
k
where the semantic score sem score(h, F ) =
|S(h)?S(F )|
|S(F )|
counts the overlap between the seman-
tic properties S(h) of h (i. e. the LWN hyper-
nyms of the fillers in h) and the semantic prop-
erties S(F ) of F (for argument a), over |S(F )|.
P (ft
3
(F )|k) =
?
h?k
syns score(h, F )
n
k
where the synset score syns score(h, F ) =
|Synsets(verb(h))?Synsets(verb(F ))|
|Synsets(verb(F ))|
calculates the
overlap between the synsets for the verb in h and
the synsets for the verb in F over the number of
synsets for the verb in F .
3
We introduced the syntactic and synset scores in
order to account for a frequent phenomenon in our
data: the partial matches between the values of the
features in F and in k.
3.2 Selectional preferences
The clustering algorithm defines the set of con-
structions in which the generalization step over
unseen cases is performed. SPs are defined as
semantic profiles, that is, probability distributions
over the semantic properties, i. e. LWN nodes. For
example, we get the probability of the node actio
?act? in the position ?A (in)Obj[acc]? for eo ?go?.
If s is a semantic property and a an argument
position for a verb v, the semantic profile P
a
(s|v)
is the sum of P
a
(s, k|v) over all constructions k
containing v or a WN-synonym of v, i. e. a verb
contained in one or more synsets for v. P
a
(s, k|v)
is approximated as
P (k,v)P
a
(s|k,v)
P (v)
, where P (k, v)
is estimated as
n
k
?freq(k,v)
?
k
?
n
k
?
?freq(k
?
,v)
To estimate P
a
(s|k, v) we consider each frame
h in k and account for: a) the similarity between v
and the verb in h; b) the similarity between s and
the fillers of h. This is achieved by calculating a
similarity score between h, v, a and s, defined as:
syns score(v, V (h)) ?
?
f
|s ? S(f)|
N
fil
(h, a)
(1)
where V (h) in (1) contains the verbs of h,
N
fil
(h, a) counts the a-fillers in h, f ranges in the
set of a-fillers in h, S(f) contains the semantic
properties for f and |s?S(f)| is 1 when s appears
in S(f) and 0 otherwise.
P
a
(s|k, v) is thus obtained by normalizing the
sum of these similarity scores over all frames in
k, divided by the total number of frames in k con-
taining v or its synonyms.
The similarity scores weight the contributions
of the synonyms of v, whose fillers play a role in
the generalization step. This is our innovation with
respect to Alishahi (2008)?s system. It was intro-
duced because of the sparseness of our data, where
3
The algorithm uses smoothed versions of all the previous
formulae by adding a very small constant so that the proba-
bilities are never 0.
75
k h
1
induco + P Sb[acc]{forma}
introduco + P Sb{PR}
introduco + P Sb{forma}
addo +P Sb{praesidium}
2
induco + A Obj[acc]{forma}
immitto + A Obj[acc]{PR},Obj[dat]{antrum}
introduco + A Obj[acc]{NP}
3
introduco + A (in)Obj[acc]{finis},Obj[acc]{copia},Sb{NP}
induco + A (in)Obj[acc]{effectus},Obj[acc]{forma}
4
introduco + A Obj[acc]{forma}
induco + A Obj[acc]{perfectio},Sb[nom]{PR}
5
induco + A Obj[acc]{forma}n
immitto + A Obj[acc]{PR},Obj[dat]{antrum}
introduco + A Obj[acc]{NP}
Table 1: Constructions (k) for the frames (h) con-
taining the verb introduco ?bring in?.
many verbs are hapaxes, which makes the gener-
alization from their fillers difficult.
4 Results and evaluation
The clustering algorithm was run on 15509 frames
and it generated 7105 constructions. Table 1 dis-
plays the 5 constructions assigned to the 9 frames
where the verb introduco ?bring in, introduce? oc-
curs. Note the semantic similarity between addo
?add to, bring to?, immitto ?send against, insert?,
induco ?bring forward, introduce? and introduco,
and the similarity between the syntactic patterns
and the argument fillers within the same construc-
tion. For example, finis ?end, borders? and ef-
fectus ?result? share the semantic properties AT-
TRIBUTE, COGNITIO ?cognition?, CONSCIENTIA
?conscience?, EVENTUM ?event?, among others.
The vast majority of constructions contain less
than 4 frames. This contrasts with the more gen-
eral constructions found by Alishahi (2008) and
can be explained by several factors. First, the cov-
erage of LWN is quite low with respect to the
fillers in our dataset. In fact, 782 fillers out of
2408 could not be assigned to any LWN synset;
for these lemmas the semantic scores with all the
other nouns are 0, causing probabilities lower than
the baseline; this results in assigning the frame to
the singleton construction consisting of the frame
itself. The same happens for fillers consisting of
verbal lemmas, participles, pronouns and named
entities, which amount to a third of the total num-
ber. Furthermore, the data are not tagged by sense
and the system deals with noun ambiguity by list-
ing together all synsets of a word n (and their hy-
pernyms) to form the semantic properties for n:
consequently, each sense contributes to the seman-
tic description of n in relation to the number of
hypernyms it carries, rather than to its observed
semantic property probability
actio ?act? 0.0089
actus ?act? 0.0089
pars ?part? 0.0089
object 0.0088
physical object 0.0088
instrumentality 0.0088
instrumentation 0.0088
location 0.0088
populus ?people? 0.0088
plaga ?region? 0.0088
regio ?region? 0.0088
arvum ?area? 0.0088
orbis ?area? 0.0088
external body part ? 0.0088
nympha ?nymph?, ?water? 0.0088
latex ?water? 0.0088
lympha ?water? 0.0088
intercapedo ?gap, break? 0.0088
orificium ?opening? 0.0088
Table 2: Top 20 semantic properties in the seman-
tic profile for ascendo ?ascend? + A (de)Obj[abl].
frequency. Finally, a common problem in SP ac-
quisition systems is the noise in the data, including
tagging and metaphorical usages. This problem
is even greater in our case, where the small size
of the data underestimates the variance and there-
fore overestimates the contribution of noisy obser-
vations. Metaphorical and abstract usages are es-
pecially frequent in the data from the IT-TB, due
to the philosophical domain of the texts.
As to the SP acquisition, we ran the system
on all constructions generated by the clustering.
We excluded the pronouns occurring as argument
fillers, and manually tagged the named entities.
For each verb lemma and slot we obtained a proba-
bility distribution over the 6608 LWN noun nodes.
Table 2 displays the 20 semantic properties
with the highest SP probabilities as ablative argu-
ments of ascendo ?ascend? introduced by de ?down
from?, ?out of?. This semantic profile was cre-
ated from the following fillers for the verbs con-
tained in the constructions for ascendo and its
synonyms: abyssus ?abyss?, fumus ?smoke?, lacus
?lake?, machina ?machine?, manus ?hand?, negoti-
atio ?business?, mare ?sea?, os ?mouth?, templum
?temple?, terra ?land?. These nouns are well repre-
sented by the semantic properties related to water
and physical places. Note also the high rank of
general properties like actio ?act?, which are asso-
ciated to a large number of fillers and thus gener-
ally get a high probability.
Regarding evaluation, we are interested in test-
ing two properties of our model: calibration
and discrimination. Calibration is related to the
model?s ability to distinguish between high and
low probabilities. We verify that our model is
76
adequately calibrated, since its SP distribution is
always very skewed (cf. figure 1). Therefore,
the model is able to assign a high probability to
a small set of nouns (preferred nouns) and a low
probability to a large set of nouns (the rest), thus
performing better than the baseline model, defined
as the model that assigns the uniform distribution
over all nouns (4724 LWN leaf nodes). Moreover,
our model?s entropy is always lower than the base-
line: 12.2 vs. the 6.9-11.3 range; by the maximum
entropy principle, this confirms that the system
uses some information for estimating the proba-
bilities: LWN structure, co-occurrence frequency,
syntactic patterns. However, we have no guaran-
tee that the model uses this information sensibly.
For this, we test the system?s discrimination po-
tential, i. e. its ability to correctly estimate the SP
probability of each single LWN node.
noun SP probability
pars ?part? 0.0029
locus ?place? 0.0026
forma ?form? 0.0023
ratio ?account??reason?, ?opinion? 0.0023
respectus ?consideration? 0.0022
caput ?head?, ?origin? 0.0022
anima ?soul? 0.0021
animus ?soul?, ?spirit? 0.0020
figura ?form?, ?figure? 0.0020
spiritus ?spirit? 0.0020
causa cause? ? 0.0020
corpus ?body? 0.0019
sententia ?judgement? 0.0019
finitio ?limit?, ?definition? 0.0019
species ?sight?, ?appearance? 0.0019
Table 3: 15 nouns with the highest probabilities as
accusative objects of dico ?say?.
Figure 1: Decreasing SP probabilities of the LWN
leaf nodes for the objects of dico ?say?.
Table 3 displays the 15 nouns with the highest
probabilities as direct objects for dico ?say?. From
table 3 ? and the rest of the distribution, repre-
sented in figure 1 ? we see that the model assigns
a high probability to most seen fillers for dico in
the corpus: anima ?soul?, corpus ?body?, locus
?place?, pars ?part?, etc.
For what concerns evaluating the SP probabil-
ity assigned to nouns unseen in the training set,
Alishahi (2008) follows the approach suggested
by Resnik (1993), using human plausibility judge-
ments on verb-noun pairs. Given the absence of
native speakers of Latin, we used random occur-
rences in corpora, considered as positive examples
of plausible argument fillers; on the other hand, we
cannot extract non-plausible fillers from a corpus
unless we use a frequency-based criterion. How-
ever, we can measure how well our system predicts
the probability of these unseen events.
As a preliminary evaluation experiment, we
randomly selected from our corpora a list of 19
high-frequency verbs (freq.>51) and 7 medium-
frequency verbs (11<freq.<50), for each of which
we chose an interesting argument slot. Then we
randomly extracted one filler for each such pair
from two collections of Latin texts (Perseus Dig-
ital Library and Corpus Thomisticum), provided
that it was not in the training set. The semantic
score in equation 1 on page 3 is then calculated
between the set of semantic properties of n and
that for f , to obtain the probability of finding the
random filler n as an argument for a verb v.
For each of the 26 (verb, slot) pairs, we looked
at three measures of central tendency: mean, me-
dian and the value of the third quantile, which
were compared with the probability assigned by
the model to the random filler. If this probabil-
ity was higher than the measure, the outcome was
considered a success. The successes were 22 for
the mean, 25 for the median and 19 for the third
quartile.
4
For all three measures a binomial test
found the success rate to be statistically significant
at the 5% level. For example, table 3 and figure
1 show that the filler for dico+A Obj[acc] in the
evaluation set ? sententia ?judgement? ? is ranked
13th within the verb?s semantic profile.
5 Conclusion and future work
We proposed a method for automatically acquiring
probabilistic SP for Latin verbs from a small cor-
pus using the WN hierarchy; we suggested some
4
The dataset consists of all LWN leaf nodes n, for which
we calculated P
a
(n|v). By definition, if we divide the dataset
in four equal-sized parts (quartiles), 25% of the leaf nodes
have a probability higher than the value at the third quartile.
Therefore, in 19 cases out of 26 the random fillers are placed
in the high-probability quarter of the plot, which is a good
result, since this is where the preferred arguments gather.
77
new strategies for tackling the data sparseness in
the crucial generalization step over unseen cases.
Our work also contributes to the state of the art in
semantic processing of Latin by integrating syn-
tactic information from annotated corpora with the
lexical resource LWN. This demonstrates the use-
fulness of the method for small corpora and the
relevance of computational approaches for histor-
ical linguistics.
In order to measure the impact of the frame
clusters for the SP acquisition, we plan to run the
system for SP acquisition without performing the
clustering step, thus defining all constructions as
singleton sets containing one frame each. Finally,
an extensive evaluation will require a more com-
prehensive set, composed of a higher number of
unseen argument fillers; from the frequencies of
these nouns, it will be possible to directly compare
plausible arguments (high frequency) and implau-
sible ones (low frequency). For this, a larger auto-
matically parsed corpus will be necessary.
6 Acknowledgements
We wish to thank Afra Alishahi, Stefano Minozzi
and three anonymous reviewers.
References
E. Agirre and D. Martinez. 2001. Learning class-to-
class selectional preferences. In Proceedings of the
ACL/EACL 2001 Workshop on Computational Nat-
ural Language Learning (CoNLL-2001), pages 1?8.
A. Alishahi. 2008. A probabilistic model of early ar-
gument structure acquisition. Ph.D. thesis, Depart-
ment of Computer Science, University of Toronto.
D. Bamman and G. Crane. 2006. The design and use
of a Latin dependency treebank. In Proceedings of
the Fifth International Workshop on Treebanks and
Linguistic Theories, pages 67?78.
?
UFAL MFF UK.
D. Bamman and G. Crane. 2008. Building a dynamic
lexicon from a digital library. In Proceedings of the
8th ACM/IEEE-CS Joint Conference on Digital Li-
braries, pages 11?20.
L. Bentivogli, P. Forner, and and Pianta E. Magnini,
B. 2004. Revising wordnet domains hierarchy: Se-
mantics, coverage, and balancing. In Proceedings of
COLING Workshop on Multilingual Linguistic Re-
sources, pages 101?108.
S. Clark and D. Weir. 1999. An iterative approach
to estimating frequencies over a semantic hierarchy.
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora. University of Maryland,
pages 258?265.
K. Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 216?223.
D. T. T. Haug and M. L. J?ndal. 2008. Creating a par-
allel treebank of the old Indo-European Bible trans-
lations. In Proceedings of Language Technologies
for Cultural Heritage Workshop, pages 27?34.
H. Li and N. Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
B. McGillivray and M. Passarotti. 2009. The devel-
opment of the Index Thomisticus Treebank Valency
Lexicon. In Proceedings of the Workshop on Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education,
pages 33?40.
B. McGillivray, M. Passarotti, and P. Ruffolo. 2009.
The Index Thomisticus treebank project: Annota-
tion, parsing and valency lexicon. TAL, 50(2):103?
127.
B. McGillivray. 2009. Selectional Preferences from
a Latin treebank. In Przepi?orkowski A. Passarotti,
M., S. Raynaud, and F. van Eynde, editors, Proceed-
ings of the Eigth International Workshop on Tree-
banks and Linguistic Theories (TLT8), pages 131?
136. EDUCatt.
S. Minozzi. 2009. The Latin Wordnet project.
In P. Anreiter and M. Kienpointner, editors, Pro-
ceedings of the 15th International Colloquium on
Latin Linguistics (ICLL), Innsbrucker Beitraege zur
Sprachwissenschaft.
M. Passarotti and P. Ruffolo. forthcoming. Parsing
the Index Thomisticus Treebank. some preliminary
results. In P. Anreiter and M. Kienpointner, edi-
tors, Proceedings of the 15th International Collo-
quium on Latin Linguistics, Innsbrucker Beitr?age
zur Sprachwissenschaft.
M. Passarotti. 2007. Verso il Lessico Tomistico Bi-
culturale. La treebank dell?Index Thomisticus. In
R. Petrilli and D. Femia, editors, Atti del XIII Con-
gresso Nazionale della Societ`a di Filosofia del Lin-
guaggio, pages 187?205.
P. Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D.
thesis, University of Pennsylvania.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and
F. Beil. 1999. Inducing a semantically annotated
lexicon via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics, pages 104?111.
78
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 49?52
Manchester, August 2008
Semantic structure from Correspondence Analysis
Barbara McGillivray
Dipartimento di Linguistica
Universit`a di Pisa
Pisa, Italy
barbara.mcgillivray@aksis.uib.no
Christer Johansson
Dept. of Linguistics
University of Bergen
Bergen, Norway
christer.johansson@uib.no
Daniel Apollon
Text Technology Lab.
Aksis, UNIFOB
Bergen, Norway
daniel.apollon@aksis.uib.no
Abstract
A common problem for clustering tech-
niques is that clusters overlap, which
makes graphing the statistical structure in
the data difficult. A related problem is
that we often want to see the distribution
of factors (variables) as well as classes
(objects). Correspondence Analysis (CA)
offers a solution to both these problems.
The structure that CA discovers may be an
important step in representing similarity.
We have performed an analysis for Italian
verbs and nouns, and confirmed that simi-
lar structures are found for English.
1 Introduction
Over the past years, distributional methods have
been used to explore the semantic behaviour of
verbs, looking at their contexts in corpora (Lan-
dauer and Laham, 1998; Redington and Finch,
1998; Biemann, 2006, inter al.). We follow a gen-
eral approach suggested already by Firth (1957),
to associate distributional similarity with semantic
similarity.
One question concerns the syntax-semantics in-
terface. Results using distributions of verbs in con-
text had an impact on verb classification (Levin,
1993), automatic verb clustering (Schulte im
Walde, 2003), and selectional preference acquisi-
tion (Resnik, 1993; Li and Abe, 1995; McCarthy,
2001; Agirre and Martinez, 2001, inter al.).
In automatic verb clustering, verbs are repre-
sented by vectors of a multidimensional space
whose dimensions (variables) are identified by
some linguistic features, ranging, for example,
from subcategorization frames to participation in
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
diathesis alternations and lexical selectional pref-
erences. The verbs cluster on co-occurrence with
the features chosen, and such information provide
a generalisation over the verbs with respect to the
variables.
In the case of selectional preference acquisition,
a verb (or a verb class) is associated to a class of
nouns that can be the lexical fillers of a case frame
slot for the verb. This allows us to calculate the
association strength between the verb and its filler
nouns. The generalisation step is performed for the
case frame instances (observations) and produces
more abstract noun classes that can be applied to
unseen cases. This often utilizes hierarchies of ex-
isting thesauri or wordnets.
We propose a method that uses Correspondence
Analysis (CA) to study the distribution (and asso-
ciated semantic behaviour) of a list of verbs with
nouns occurring in a particular syntactic relation,
for example their subjects. This is collected from
a corpus, and reflects usage in that corpus. Un-
like clustering methods, this technique does not
imply an exclusive choice between a) classifying
verbs on the basis of the noun fillers in their syn-
tactic frame, or b) associating noun classes to verbs
(sometimes mediated by a semantic hierarchy). In-
stead, this approach yields a geometric representa-
tion of the relationships between the nouns and the
verbs in a common dual space (biplot). CA aims
to find an overall structure (if any) of the data. The
method emphasizes unusual observations, as de-
viance from the expected is what creates the axes
of the analysis. CA generalizes over the actual oc-
currences of verb-noun pairs in the corpus, and vi-
sualizes the shape of the correspondence space.
When associating verbs with nouns, CA takes as
input a contingency table (here rows correspond to
the verbs, and columns correspond to their subject
fillers). Each verb is a row point in the multidimen-
sional noun space, and each noun is a column point
in the multidimensional verb space. The CA goals
49
are to reduce the dimension of the dual original
space, and to find an optimal subspace that is the
closest to this cloud of points in the ?
2
-metric. The
best subspace is determined by finding the smallest
number of orthogonal axes that describe the most
variance from the original cloud.
Finally the coordinates of both row and col-
umn points of the ?
2
contingency table are pro-
jected onto this optimal subspace, simultaneously
displaying row and column points. If we consider
those points that are well represented, the closer
they are in this geometric representation, the more
similar their original distributions are. In this way,
we can detect not only that there is a relationship
between the verb (e.g. explode) and the noun (e.g.
bomb), but also how each word relates to each
other word.
2 Correspondence Analysis
CA is a data analytic technique developed by
Benz?ecri in the 1960s, which has been widely used
in describing large contingency tables and binary
data. At the heart of CA is Singular Value Decom-
position (SVD), from which many other methods
were derived (Biplot, Classical Principal Compo-
nent Analysis, PCA and more).
Compared to usual clustering methods, CA
gives a more fine-grained view of the spread of
the input points. Benz?ecri (1973) points out that
CA is more efficient than clustering in terms of de-
composition of variance. Secondly, CA represents
possible regions in space with varying density, and
produces a flexible ?compound clustering? on both
objects and variables. Verb-nouns association pro-
files may not cluster in distinct space regions, but
may be evenly distributed, follow a gradient-like
distribution, or show overlapping clusters. In such
difficult cases for clustering, CA is able to offer
a representation of the geometry of the input pro-
files. Finally, CA offers the possibility of recon-
structing the original space from the output sub-
space.
Let us consider a data matrix M whose size is
(r, c), the (i, j)
th
entry of M containing the num-
ber of occurrences of verb j with noun i as its sub-
ject in a corpus. We calculate the relative frequen-
cies by dividing each entry M(i, j) by the sum of
row i, i. e. the frequency of noun i, to get the ma-
trix of row profiles R(i, j). Therefore, the more
similar two row profiles i
1
and i
2
are, the more
these two nouns can be considered as distributional
synonyms.
The next step implies comparing the row pro-
files with the average distribution where each entry
(i, j) is the product of the frequency of noun i by
the frequency of verb j divided by the grand total
N of the table. This comparison is calculated us-
ing the ?
2
-distance (i.e. a weighted Euclidean dis-
tance), which eliminates effects of high frequency
alone. The next formula shows calculations for
rows. Calculations for columns are analogous.
?
2
(i
1
, i
2
) =
c
?
j=1
(R(i
1
, j) ?R(i
2
, j))
2
?
r
i=1
M(i, j)
The ?
2
-distance between a profile point and the av-
erage profile (barycentre) is called inertia of the
profile point and the total inertia measures how
the individual profiles p
i
are spread around the
barycentre:
Inertia =
1
N
r
?
i=1
c
?
j=1
M(i, j)?
2
(p
i
, p?)
CA then searches for the optimal subspace S that
minimises the distance from the profile points.
Once specified its dimension k ? min(r ? 1, c ?
1), S is found by applying the Singular Value De-
composition (SVD) to matrix R ? 1p? , which de-
composes it as the product N ?D ?M : where D is
a diagonal matrix with positive coefficients ?
1
?
?
2
? . . . ? ?
k
(singular values) and N and M are
orthonormal matrices (N
T
N = M
T
M = I). The
rows of M are the orthonormal basis vectors that
define S (called principal axes of inertia) and the
rows of matrix F = N ? D are the projections of
the row profiles onto S. For k = 2, this allows us
to plot the new coordinates in a two-dimensional
space and get the correspondence analysis of the
row profiles.
The total inertia is decomposed into the direc-
tion of the principal axes of inertia. The first axis
represents the direction where the inertia of the
cloud is the maximum; the second axis maximises
the inertia among all the directions orthogonal to
the first axis, and so on.
The geometry of column profiles can be anal-
ysed similarly, because the two problems are di-
rectly linked and two transition formulae can be
used to pass from one coordinate system to the
other, explaining the French name ?analyse des
correspondances?.
As a result, both analyses decompose the same
inertia into the same system of principal axes. This
allows us to merge the two representations in one
single geometric display showing at the same time
50
the projections of row and column points in the
subspace.
In addition to this dual space representation, CA
gives a system of diagnostic measures for each of
the two dual spaces:
? contributions of the rows (and columns) to the
axes, i. e. the inertia of the points projected onto
the axes, which contributes to the principal inertia;
? contributions of the axes to the row (and col-
umn) points;
? quality of representation (cumulative sum of
contributions of the axes for each point); this high-
lights well represented points.
3 Explorations
We performed a CA using the Matlab Analytica
Toolbox developed by Daniel Apollon. We tested
this technique first on the Italian newspaper corpus
LA REPUBBLICA, which consists of 450 million
word tokens. This corpus was syntactically parsed
using the MALT dependency parser (Nivre, 2006).
A list of 196 verbs was compiled following the list
of German verbs contained in (Schulte im Walde,
2003) and adapting it to Italian. Looking at the
syntactic analyses of the corpus where the verbs of
the list showed a subcategorization frame contain-
ing a subject slot, their lexical subject fillers were
automatically extracted. The matrix M , whose
2553 row entries correspond to the nouns extracted
as subject fillers, was then used as input for the CA
(|M | = 196? 2553 = 500388).
Starting from the quality of representation
scores of this analysis, we isolated a set of points
with increasingly good representation, ending with
an extremely faithful and low dimensional rep-
resentation. We called this method ?incremental
pruning?. Figure 1 shows the dual display of the
analysis for the Italian data in a two dimensional
space, after filtering out those points showing a
quality of representation below a threshold of 30%.
We can conceptualize the data set C after a CA
as the cumulative effect of three different underly-
ing phenomena: K, R and E.
K can be seen as a reduction of the latent struc-
ture of C; it contains its core structure as it has
been underlined by the analysis and left after prun-
ing.
R refers to the residual variance, not included in
the core analysis. It contains the most predictable
points
1
, which are plotted near to origin (barycen-
1
In our data: pronouns she, I, he, every-, no-, some-body,
Figure 1: Correspondence graph for Italian
tre of the data cloud). These points give a small
contribution to the inertia of the principal axes.
E contains the error in the representation, as
well as badly represented points.
Points far from the origin display strong struc-
ture; they may correspond to rare words used in
special contexts. Figure 1 shows that words related
to destruction
2
are aligned in the same direction,
whereas the second vector is mainly constituted by
nouns and verbs that have to do with the political
and legal area
3
. The first principal axis accounts
for nearly 16% of the total inertia, whereas the
second axis accounts for 12%. The first six axes
accounts for over 70% of variation. Many words
were not well represented, but contribute to vari-
ance.
We confirmed our method on English, using the
British National Corpus
4
. A similar structure was
found. We restrict ourselves to reproduce the graph
for Italian.
who, nouns with partly pronominal qualities husband, wife,
friend, sir, son, father, mother, fact, event.
2
Along the y-axis from top down to the middle, we find the
nouns flame, extend, stick of dynamite, excavator, chemother-
apy, effusion, blaze, seism, demon, dynamite, fire, aviation,
earthquake, artificer, explosive device, insect, gas, landslide,
virus, rain, bulldozer, hurricane, wave, speculation, artillery,
remorse, bomb, missile, violence, revolution, etc.
3
Along the x-axis we find, from left to the middle,
the nouns order, regulations, norm, code, legislation, rules,
treaty, constitution, circular letter, system, directive, law, de-
cree, article, judgement, amendment, court, etc.
4
via sketchengine http://www.sketchengine.co.uk
51
4 Conclusion
CA detects a structure for Italian verb-noun cor-
respondences in LA REPUBBLICA (? 450 mil-
lion words). A similar structure was confirmed us-
ing BNC for English. Both global and local struc-
tures are found, which gives possibilities to rep-
resent lexical units with reference to both princi-
pal axes and word similarity. The main dimen-
sions of the Italian corpus are topical (crime re-
lated vs. natural catastrophes, and laws vs. po-
litical institutions). Semantic relatedness were ob-
served in closely mapped words. Both global and
local structure is found, and we can speculate that
this helps representing lexical units in semantic la-
beling (Giuglea and Moschitti, 2006) for machine
learning tasks. We can conceptualize text graphs in
two distinct usages: knowledge re-presenting (e.g.
FrameNet) and visualizing relations in a data set.
Our method belongs in the second category.
Acknowledgements
The first author is a MULTILINGUA fellow at
Uni. Bergen, financially supported by a Marie
Curie action (European Commission). We wish to
thank the anonymous reviewers who gave impor-
tant leads to future research.
References
Agirre, Eneko and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proc. of
the ACL/EACL Workshop on Computational Natural
Language Learning, pages 1?8, Toulouse, France.
Benz?ecri, Jean-Paul. 1973. L?Analyse des Donn?ees,
volume 1. Dunod.
Biemann, Chris. 2006. Chinese Whispers ? an
Efficient Graph Clustering Algorithm and its Ap-
plication to Natural Language Processing Prob-
lems. In Proc. of the HLT-NAACL-06 Workshop on
Textgraphs-06, pages 73?80, New York, USA.
Firth, John R., 1957. Studies in Linguistic Analysis,
chapter A synopsis of linguistic theory 1930-1955.
Philological Society.
Giuglea, Ana-Maria and Alessandro Moschitti. 2006.
Semantic Role Labeling via FrameNet, VerbNet and
PropBank. In Proc. of the 21st Int. Conf. on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
929?936, Sydney, Australia.
Landauer, Thomas K., Peter W. Foltz and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, 25:259?284.
Levin, Beth. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Li, Hang and Naoki Abe. 1995. Generalizing case
frames using a thesaurus and the MDL principle. In
Proc. of Recent Advances in Natural Language Tech-
nology, pages 239?248.
McCarthy, Diana. 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
Springer.
Redington, Martin, Nick Chater and Steven Finch.
1998. Distributional information: A powerful cue
for acquiring syntactic categories. Cognitive Sci-
ence, 22:425?469.
Resnik, Philip. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Schulte im Walde, Sabine. 2003. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Ph.D. thesis, Institut f?ur Maschinelle
Sprachverarbeitung, Universit?at Stuttgart.
52
