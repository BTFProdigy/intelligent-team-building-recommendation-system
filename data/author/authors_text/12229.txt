Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Web Selectors for the Disambiguation of All Words
Hansen A. Schwartz and Fernando Gomez
School of Electrical Engineering and Computer Science
University of Central Florida
Orlando, FL 32816, USA
{hschwartz, gomez}@cs.ucf.edu
Abstract
This research examines a word sense dis-
ambiguation method using selectors acquired
from the Web. Selectors describe words which
may take the place of another given word
within its local context. Work in using Web se-
lectors for noun sense disambiguation is gen-
eralized into the disambiguation of verbs, ad-
verbs, and adjectives as well. Additionally,
this work incorporates previously ignored ad-
verb context selectors and explores the effec-
tiveness of each type of context selector ac-
cording to its part of speech. Overall results
for verb, adjective, and adverb disambigua-
tion are well above a random baseline and
slightly below the most frequent sense base-
line, a point which noun sense disambigua-
tion overcomes. Our experiments find that,
for noun and verb sense disambiguation tasks,
each type of context selector may assist target
selectors in disambiguation. Finally, these ex-
periments also help to draw insights about the
future direction of similar research.
1 Introduction
The great amount of text on the Web has emerged
as an unprecedented electronic source of natural
language. Recently, word sense disambiguation
systems have fostered the size of the Web in or-
der to supplant the issue of limited annotated data
availability for supervised systems (Mihalcea, 2002;
Agirre and Martinez, 2004). Some unsupervised or
minimally supervised methods use the Web more di-
rectly in disambiguation algorithms that do not use
a training set for the specific target words.
One such minimally supervised method uses se-
lectors acquired from the Web for noun sense disam-
biguation by comparing the selectors of a given sen-
tence to a target noun within the sentence (Schwartz
and Gomez, 2008). Although this work found strong
results, many aspects of the use of selectors was left
unexplored. For one, the method was only applied
to noun sense disambiguation, focusing on the well-
developed noun hypernym hierarchy within Word-
Net (Miller et al, 1993). Additionally, the role of
different types of selectors was not extensively ex-
plored, and adverb selectors were not used at all. We
seek to address those issues.
In this paper, we extend our method of using se-
lectors from the Web for noun sense disambigua-
tion into a more robust method of disambiguating
words of all parts of speech. After a brief back-
ground on selectors and related work, we explain
the acquisition and empirical application of selec-
tors from nouns, verbs, adjectives, pronouns/proper
nouns, and adverbs. Finally, results are presented
from the SemEval-2007 coarse grained all-words
task (Navigli et al, 2007), and we explore the influ-
ence of various types of selectors on the algorithm
in order to draw insight for future improvement of
Web-based methods.
2 Background
In this section we describe related research in selec-
tors and solving the problem of word sense disam-
biguation (WSD). Specifically, two types of WSD
research are examined: works that used the Web in
direct manner, and works which applied a similarity
or relatedness measure.
2.1 Selectors
The term selector comes from (Lin, 1997), and
refers to a word which can take the place of another
given word within the same local context. Although
28
Lin searched a dependency relationship database in
order to match local context, it is not yet possible to
parse dependency relationships of the entire Web. In
turn, one must search for text as local context. For
example, in the sentence below, the local context for
?strikers? would be composed of ?he addressed the?
and ?at the rally.?.
He addressed the strikers at the rally.
Previously, we introduced the idea of using selec-
tors of other words in a sentence in addition to se-
lectors of the target, the word being disambiguated
(Schwartz and Gomez, 2008). Words taking the
place of a target word are referred to as target selec-
tors and words which take the place of other words
in a sentence are referred to as context selectors.
Context selectors can be classified further based on
their part of speech. In our example, if ?striker? was
the target word, the verb context selectors would be
verbs replacing ?addressed? and the noun context se-
lectors would be nouns replacing ?rally?.
Similarity is used to measure the relationship be-
tween a target word and its target selectors, while
relatedness measures the relationship between a tar-
get word and context selectors from other parts of
the sentence. Thus, the use of selectors in disam-
biguating words relies on a couple assumptions:
1. Concepts which appear in matching syntactic
constructions are similar.
2. Concepts which appear in the context of a given
target word are related to the correct sense of
the target word.
Note that ?concept? and ?word sense? are used in-
terchangeably throughout this paper. This idea of
distinguishing similarity and relatedness has an ex-
tensive history (Rada et al, 1989; Resnik, 1999; Pat-
wardhan et al, 2003; Budanitsky and Hirst, 2006),
but most algorithms only find a use for one or the
other.
2.2 Related Word Sense Disambiguation
A key aspect of using selectors for disambiguation is
the inclusion of context in the Web search queries.
This was done in works by (Martinez et al, 2006)
and (Yuret, 2007), which substituted relatives or
similar words in place of the target word within a
given context. The context, restricted with a win-
dow size, helped to limit the results from the Web.
These works followed (Mihalcea and Moldovan,
1999; Agirre et al, 2001) in that queries were con-
structed through the use of a knowledge-base, fill-
ing the queries with pre-chosen words. We also use
context in the web search, but we acquire words
matching a wildcard in the search rather than incor-
porate a knowledge-base to construct queries with
pre-chosen relatives. Consequently, the later half of
our algorithm uses a knowledge-base through simi-
larity and relatedness measures.
Some recent works have used similarity or relat-
edness measures to assist with WSD. Particuarly,
(Patwardhan et al, 2003) provide evaluations of var-
ious relatedness measures for word sense disam-
biguation based on words in context. These evalu-
ations helped us choose the similarity and related-
ness measures to use in this work. Other works,
such as (Sinha and Mihalcea, 2007), use similar-
ity or relatedness measures over graphs connecting
words within a sentence. Likewise, (Navigli and Ve-
lardi, 2005) analyze the connectivity of concepts in
a sentence among Structured Semantic Interconnec-
tions (SSI), graphs of relationships based on many
knowledge sources. These works do not use selec-
tors or the Web. Additionally, target selectors and
context selectors provide an application for the dis-
tinction between similarity and relatedness not used
in these other methods.
Several ideas distinguish this current work from
our research described in (Schwartz and Gomez,
2008). The most notable aspect is that we have gen-
eralized the overall method of using Web selectors
into disambiguating verbs, adverbs, and adjectives
in addition to nouns. Another difference is the in-
clusion of selectors for adverbs. Finally, we also ex-
plore the actual impact that each type of selector has
on the performance of the disambiguation algorithm.
3 Approach
In this section we describe the Web Selector algo-
rithm such that verbs, adjectives, and adverbs are
disambiguated in addition to nouns. The algorithm
essentially runs in two steps: acquisition of selectors
and application of selectors.
29
3.1 Acquisition of Selectors
Selectors are acquired for all appropriate parts of
speech. Whether the selectors are used as target
selectors or context selectors depends on the target
word with which they are being applied. Thus, one
process can be used to acquire all noun, verb, adjec-
tive, and adverb selectors. Additionally, noun selec-
tors can be acquired for pronouns and proper nouns
(referred to as ?pro? selectors). These are regular
nouns found to replace a pronoun or proper noun
within their local context.
The first step in acquisition is to construct a query
with a wildcard in place of the target. In our ex-
ample, with ?address? as the target, the query is ?he
* the strikers at the rally.? Yahoo! Web Services1
provides the functionality for searching the web for
phrases with wildcards. Selectors are extracted from
the samples returned from the web search by match-
ing the words which take the place of the wildcard.
All words not found in WordNet under the same
part of speech as the target are thrown out as well
as phrases longer than 4 words or those containing
punctuation.
The system enters a loop where it:
? searches the web with a given query, and
? extracts selectors from the web samples.
The query is truncated and the search is repeated un-
til a goal for the number of selectors was reached
or the query becomes too short. This approach, de-
tailed in (Schwartz and Gomez, 2008), removes se-
lect punctuation, determiners, and gradually short-
ens the query one word at a time. Selectors retrieved
from a larger query are removed from the results of
smaller queries as the smaller queries should sub-
sume the larger query results. Some selectors re-
trieved for the example, with their corresponding
web query are listed in Table 1.
3.2 Similarity and Relatedness
To apply selectors in disambiguation, similarity and
relatedness measures are used to compare the selec-
tors with the target word. We incorporate the use
of a few previously defined measures over WordNet
(Miller et al, 1993). The WordNet::Similarity pack-
age provides a flexible implementation of many of
these measures (Pedersen et al, 2004). We config-
ured WordNet::Similarity for WordNet version 2.1,
1http://developer.yahoo.com/search/
He addressed the * at the rally
crowd:1
He addressed * at the rally
student:1, supporter:2
He addressed * at the
Council:1, Muslim:1, Saturday:1, Ugandan:1,
analyst:2, attendee:20, audience:3, class:2,
consumer:1, council:1, delegate:64, diplomat:2,
employee:2, engineer:1, fan:1, farmer:1,
globalization:1, graduate:5, guest:2, hundred:3,
investor:1, issue:1, journalist:9, lawmaker:11,
legislator:1, member:6, midshipman:1,
mourner:1, official:2, parliamentarian:1,
participant:17, patient:1, physician:18,
reporter:8, sailor:1, secretary:1, soldier:3,
staff:3, student:20, supporter:8, thousand:3,
today:2, trader:1, troops:2, visitor:1, worker:1
He * the strikers at the
treat:2
He * the strikers at
get:1, keep:1, price:1, treat:1
Table 1: Lists of selectors for the target words ?striker?
and ?address? returned by corresponding web queries.
the same version used to annotate our chosen exper-
imental corpus.
A relatedness measure was used with context se-
lectors, and we chose the adapted Lesk algorithm
(Banerjee and Pedersen, 2002). An important char-
acteristic of this measure is that it can handle multi-
ple parts of speech. For target selectors we sought
to use measures over the WordNet ontology in order
to most closely measure similarity. An information-
content (IC) measure (Resnik, 1999) was used for
target selectors of nouns and verbs. However, be-
cause IC measures do not work with all parts of
speech, we used the adapted Lesk algorithm as an
approximation of similarity for adjectives and ad-
verbs. Note that finding the best relatedness or sim-
ilarity measure was outside the scope of this paper.
The following function, based on Resnik?s word
similarity (Resnik, 1999), is used to find the max
similarity or relatedness between a concept and a
word (specifically between a sense of the target
word, ct and a selector, ws).
maxsr(ct, ws) = maxcs?ws[meas(ct, cs)]
where cs is a sense of the selector and meas is a
similarity or relatedness measure.
30
Figure 1: General flow in applying selectors to word
sense disambiguation. Note that the target selectors may
be any part of speech.
3.3 Application of Selectors
Next, we briefly describe the empirical basis for
scoring senses of the target word. This step is out-
lined in Figure 1. The occurrences of selectors can
be converted to a probability of a selector, ws ap-
pearing in a web query, q:
p(ws, q)
The senses of the target word are compared with
each selector. For a given sense of the target word,
ct, the similarity or relatedness from a selector and
query is computed as:
SR(ct, ws, q) = p(ws, q) ?maxsr(ct, ws)senses(ws)
where senses(ws) is the number of senses of the
selector.
As the queries get shorter, the accuracy of the se-
lectors becomes weaker. In turn, the SR value from
selectors is scaled by a ratio of the web query length,
wql, to the original sentence length, sl. This scaling
is applied when the SR values for one target word
sense are summed:
sum(ct, T ) =
?
q?qs(T )
?
ws?sels(q)
SR(ct, ws, q)? wqlsl
where qs(T ) represents the set of queries for a selec-
tor type, T , and ws ranges over all selectors found
with q, denoted sels(q).
The general approach of disambiguation is to find
the sense of a target word which is most similar to all
target selectors and most related to all context selec-
tors. This follows our assumptions about selectors
given in the background section. Thus, similarity
and relatedness values from different selector types
(represented as Types) must be combined. By ag-
gregating the normalized sums from all types of se-
lectors, we get a combined similarity/relatedness for
a given target word sense:
CSR(ct) =
?
T?Types
scale(T ) ? sum(ct, T )max
ci?wt
[sum(ci, T )]
where wt represents the set of all senses belonging to
the target word, and scale(T ) is a coefficient used to
weight each type of selector. This term is important
in this work, because our experiments explore the
impact of various selector types.
The top sense is then chosen by looking at the
CSR of all senses. For some situations, specifically
when other senses have a score within 5% of the
top CSR, the difference between concepts is very
small. In these cases, the concept with the lowest
sense number in WordNet is chosen from among the
top scoring senses.
4 Experiments
Our experiments are run over the SemEval2007 Task
7: coarse-grained English all-words. The sense in-
ventory was created by mapping senses in WordNet
2.1 to the Oxford Dictionary of English (Navigli et
al., 2007). The corpus was composed of five docu-
ments with differing domains resulting in 2269 an-
notated word instances. Our system runs on fine-
grained WordNet senses, but evaluation is done by
checking if the predicted fine-grained sense maps to
the correct coarse-grained sense. Many issues as-
sociated with fine-grained annotation, such as those
brought up in (Ide and Wilks, 2006) are avoided
through the use of this corpus.
First, we apply the generalized Web selector algo-
rithm in a straight-forward manner to the entire task.
Then, we delve into analyzing the acquired selectors
and the influence of each type of context selector in
order to gain insights into future related work.
31
BLRand MED WS BLMFS
53.43 70.21 76.02 78.89
Table 2: Results as F1 Values of our system, WS,
compared with baselines: random, BLRand; most fre-
quent sense, BLMFS ; median system performance at Se-
mEval07, MED.
UPV-WSD NUS-PT SSI
78.63 82.50 83.21
Table 3: Results as F1 Values of top performing systems
for the SemEval07 Task07 (UPV = (Buscaldi and Rosso,
2007), NUS-PT = (Chan et al, 2007), and SSI = a task
organizer?s system (Navigli and Velardi, 2005)).
4.1 Evaluating All Words
In this section, we seek to apply the algorithm to all
instances of the testing corpus in order to compare
with baselines and other disambiguation algorithms.
Unless stated otherwise, all results are presented as
F1 values, where F1 = 2? P?RP+R . For SemEval2007,all systems performed better than the random base-
line of 53.43%, but only 4 of 13 systems achieved
an F1 score higher than the MFS baseline of 78.89%
(Navigli et al, 2007).
Table 2 lists the results of applying the general-
ized Web selector algorithm described in this paper
in a straight-forward manner, such that all scale(T )
are set to 1. We see that this version of the system
performs better than the median system in the Se-
mEval07 task, but it is a little below the MFS base-
line. A comparison with top systems is seen in Table
3. Our overall results were just below that of the top
system not utilizing training data, (UPV-WSD (Bus-
caldi and Rosso, 2007)), and a little over 6 percent-
age points below the top supervised system (NUS-
PT (Chan et al, 2007)).
The results are broken down by part of speech
in Table 4. We see that adjective disambiguation
was the furthest above our median point of refer-
ence, and noun disambiguation results were above
the MFS baseline. On the other hand, our adverb
disambiguation results appear weakest compared to
the baselines. Note that we previously reported a
noun sense disambiguation F1 value of 80.20% on
the same corpus (Schwartz and Gomez, 2008). Cur-
rent results differ because the previous work used
N V A R
MED 70.76 62.10 71.55 74.04
WS 78.52 68.36 81.21 75.48
BLMFS 77.44 75.30 84.25 87.50
insts 1108 591 362 208
Table 4: Results as F1 values (precision = recall) of our
system by parts of speech (N = noun, V = verb, A = ad-
jective, R = adverb). insts = disambiguation instances of
each part of speech. For other keys see Table 2.
different scale(T ) values as well as a custom noun
similarity measure.
4.2 Selector Acquisition Analysis
We examine the occurrences of acquired selectors.
Listed as the column headings of Table 5, selectors
are acquired for five parts of speech (pro is actually
a combination of two parts of speech: pronoun and
proper noun). The data in Table 5 is based on re-
sults from acquiring selectors for our experimental
corpus. The information presented includes:
insts instances which the algorithm attempts
to acquire selectors
% w/ sels percentage of instances for which
selectors were acquired
sels/inst average number of selectors for an
instance (over all insts)
unique/inst average number of unique selectors for
an instance (over all insts)
insts/sent average instances in a sentence
noun verb adj. adverb pro
insts 1108 591 362 208 370
% w/ sels 54.5 65.8 61.0 57.2 27.0
sels/inst 36.5 51.2 29.5 17.7 15.9
unique/inst 11.6 13.1 8.4 4.1 5.6
insts/sent 4.5 2.4 1.5 0.8 1.5
Table 5: Various statistics on the acquired selectors for
the SemEval07 Task 7 broken down by part of speech.
Row descriptions are in the text.
The selector acquisition data provides useful in-
formation. In general, % w/ sels was low from be-
ing unable to find text on the Web matching local
context (even with truncated queries). The lowest
% w/ sels, found for pro, was expected consider-
ing only nouns which replace the original words are
32
used (pronouns acquired were thrown out since they
are not compatible with the relatedness measures).
There was quite a variation in the sels/inst depending
on the type, and all of these numbers are well below
the upper-bound of 200 selectors acquired before the
algorithm stops searching. It turned out that only
15.9% of the instances hit this mark. This means
that most instances stopped acquiring selectors be-
cause they hit the minimum query length (5 words).
In fact, the average web query to acquire at least one
selector had a length of 6.7 words, and the bulk of
selectors came from shorter queries (with less con-
text from shorter queries, the selectors returned are
not as strong). We refer to the combination of quan-
tity and quality issues presented above, in general,
as the quality selector sparsity problem.
Although quality and quantity were not ideal,
when one considers data from the sentence level,
things are more optimistic. The average sentence
had 10.7 instances (of any part of speech listed),
so when certain selector types were missing, oth-
ers were present. As explained previously, the tar-
get selector and context selector distinction is made
after the acquisition of selectors. Thus, each in-
stance is used as both (exception: pro instances were
never used as target selectors since they were not
disambiguated) . Employing this fact, more infor-
mation can be discovered. For example, the aver-
age noun was disambiguated with 36.5 target selec-
tors, 122.9 verb context selectors (51.2 sels/inst *
2.4 insts/sent), 44.3 adjective context selectors, 14.2
adverb context selectors, and 23.9 pro context se-
lectors. Still, with the bulk of those selectors com-
ing from short queries, the reliability of the selectors
was not strong.
4.3 Exploring the Influence of Selector Types
This section explores the influence of each context
selector on the disambiguation algorithm, by chang-
ing the value of scale(T ) in the previously listed
CSR function.
Examining Table 6 reveals precision results
when disambiguating instances with target selec-
tors, based only on the target word?s similarity with
target selectors. This serves as a bearing for inter-
preting results of context selector variation.
We tested how well each type of context selec-
tor complements the target selectors. Accordingly,
wsd prec. % insts.
N 64.08 348
V 52.86 227
A 77.36 106
R 58.39 56
Table 6: Precision when disambiguating with target se-
lectors only. All instances contain target selectors and
multiple senses in WordNet. (insts. = number of in-
stances disambiguated.)
wsd noun verb adj. adverb pro
N 272 186 120 84 108
V 211 167 110 80 103
A 97 78 50 40 34
R 47 44 30 17 26
Table 7: Instance occurrences used for disambiguation
when experimenting with all types of context selectors
(listed as columns). The rows represent the four parts of
speech disambiguated.
scale(target) was set to 1, and scale(T ) for all
other context types were set to 0. In order to limit ex-
ternal influences, we did not predict words with only
one sense in WordNet or instances where the CSR
was zero (indicating no selectors). Additionally, we
only tested on examples which had at least one tar-
get selector and at least one selector of the specific
type being examined. This restriction ensures we are
avoiding some of the quality selector sparsity prob-
lem described in the analysis. Nevertheless, results
are expected to be a little lower than our initial tests
as we are ignoring other types of selectors and not
including monosemous words according to Word-
Net. Table 7 lists the instance occurrences for each
of the four parts of speech that were disambiguated,
based on these restrictions.
Figures 2 through 5 show graphs of the precision
score while increasing the influence of each context
selector type. Each graph corresponds to the disam-
biguation of a different part of speech, and each line
in a graph represents one of the five types of context
selectors:
1. noun context
2. verb context
3. adjective context
4. adverb context
5. pro context
33
62
64
66
68
70
72
74
76
78
0.25 1 4 16
scale(T ) value
noun
verbadjective
adverbpro
Figure 2: The noun sense disambiguation precision when
varying the scale(T ) value for each type of context selec-
tor. scale(target) is always 1.
The lines are formed with a Bezier curve algorithm2
on the precision data. The horizontal line represents
the precision of only using the target selectors to dis-
ambiguate instances with target selectors. Precision
either decreases or remains the same if any graph
line was extended past the right-most boundary.
When examining the figures, one should note
when the precision increases as the scale value in-
creases. This indicates that increases in influence of
the particular type of context selector improved the
results. The x-axis increases exponentially, since we
would like a ratio of scale(T ) to scale(target), and
at x = 1 the context selector has the same influence
as the target selector.
We see that all types of context selectors improve
the results for noun and verb sense disambiguation.
Thus, our inclusion of adverb context selectors was
worthwhile. It is difficult to draw a similar conclu-
sion from the adverb and adjective disambiguation
graphs (Figures 4 and 5), although it still appears
that the noun context selectors are helpful for both
and the pro context selectors are helpful for the ad-
jective task. We also note that most selector types
2http://www.gnuplot.info/docs/node124.html
40
45
50
55
60
65
0.25 1 4 16
scale(T ) value
noun
verbadjective
adverbpro
Figure 3: The verb sense disambiguation precision when
varying the scale(T ) value for each type of context se-
lector. scale(target) is 1.
achieve highest precision above a scale value of 1,
indicating that the context selector should have more
influence than the target selectors. This is proba-
bly due to the existence of more selectors from con-
text than those from the target word. The results of
adverb disambiguation should be taken lightly, be-
cause there were not many disambiguation instances
that fit the restrictions (see Table 7).
4.4 Discussion of Future Work
Based on the results of our analysis and experiments,
we list two avenues of future improvement:
1. Automatic Alternative Query Construction:
This idea is concerned with the quality and
quantity of selectors acquired for which there
is currently a trade-off. As one shortens the
query to receive more quantity, the quality
goes down due to a less accurate local context.
One may be able to side-step this trade-off by
searching with alternative queries which cap-
ture just as much local context. For example,
the query ?He * the strikers at the rally? can
be mapped into the passive transformation ?the
strikers were * at the rally by him?. Query
34
60
62
64
66
68
70
72
74
76
78
80
0.25 1 4 16
scale(T ) value
noun
verbadjective
adverbpro
Figure 4: The adjective sense disambiguation precision
when varying the scale(T ) value for each type of context
selector. scale(target) is 1.
reconstruction can be accomplished by using
a constituent-based parser, which will help to
produce syntactic alternations and other trans-
formations such as the dative.
2. Improving Similarity and Relatedness: Noun
sense disambiguation was the only subtask to
pass the MFS baseline. One reason we suspect
for this is that work in similarity and related-
ness has a longer history over nouns than over
other parts of speech (Budanitsky and Hirst,
2006). Additionally, the hypernym (is-a) re-
lationship of the noun ontology of WordNet
captures the notion of similarity more clearly
than the primary relationships of other parts of
speech in WordNet. Accordingly, future work
should look into specific measures of similarity
for each part of speech, and further improve-
ment to relatedness measures which function
accross different parts of speech. A subtle piece
of this type of work may find a way to effec-
tively incorporate pronouns in the measures, al-
lowing less selectors to be thrown out.
35
40
45
50
55
60
65
0.25 1 4 16
scale(T ) value
noun
verbadjective
adverbpro
Figure 5: The adverb sense disambiguation precision
when varying the scale(T ) value for each type of con-
text selector. scale(target) is 1.
5 Conclusion
We found the use of Web selectors to be a worth-
while approach to the disambiguation of other parts
of speech in addition to nouns. However, results
for verb, adjective, and adverb disambiguation were
slightly below the most frequent sense baseline, a
point which noun sense disambiguation overcomes.
The use of this type of algorithm is still rich with
avenues yet to be taken for improvement.
Future work may address aspects at all levels of
the algorithm. To deal with a quality selector spar-
sity problem, a system might automatically form
alternative web queries utilizing a syntactic parser.
Research may also look into defining similarity mea-
sures for adjectives and adverbs, and refining the
similarity measures for nouns and verbs. Neverthe-
less, without these promising future extensions the
system still performs well, only topped by one other
minimally supervised system.
6 Acknowledgement
This research was supported by the NASA Engi-
neering and Safety Center under Grant/Cooperative
Agreement NNX08AJ98A.
35
References
Eneko Agirre and David Martinez. 2004. Unsupervised
WSD based on automatically retrieved examples: The
importance of bias. In Proceedings of EMNLP 2004,
pages 25?32, Barcelona, Spain, July.
Eneko Agirre, Olatz Ansa, and David Martinez. 2001.
Enriching wordnet concepts with topic signatures. In
In Proceedings of the NAACL workshop on WordNet
and Other Lexical Resources: Applications, Exten-
sions and Customizations, Pittsburg, USA.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics. Mexico City, Mexico.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Davide Buscaldi and Paolo Rosso. 2007. UPV-WSD :
Combining different WSD methods by means of fuzzy
borda voting. In Proceedings of SemEval-2007, pages
434?437, Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
NUS-PT: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of Proceedings of SemEval-2007, pages 253?
256, Prague, Czech Republic, June.
Nancy Ide and Yorick Wilks, 2006. Word Sense Dis-
ambiguation: Algorithms And Applications, chapter 3:
Making Sense About Sense. Springer.
Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th annual meeting on Association for
Computational Linguistics, pages 64?71.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42?50.
Rada Mihalcea and Dan I. Moldovan. 1999. An auto-
matic method for generating sense tagged corpora. In
Proceedings of AAAI-99, pages 461?466.
Rada Mihalcea. 2002. Bootstrapping large sense tagged
corpora. In Proceedings of the 3rd International
Conference on Languages Resources and Evaluations
LREC 2002, Las Palmas, Spain, May.
George Miller, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: A knowledge-based ap-
proach to word sense disambiguation. IEEE Trans.
Pattern Anal. Mach. Intell., 27(7):1075?1086.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-grained
english all-words task. In Proceedings of SemEval-
2007, pages 30?35, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Siddharth Patwardhan, S. Banerjee, and T. Pedersen.
2003. Using Measures of Semantic Relatedness
for Word Sense Disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics, pages
241?257, Mexico City, Mexico, February.
Ted Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the NAACL Demonstrations, pages 38?41,
Boston, MA, May.
R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. In IEEE Transactions on Systems, Man and Cy-
bernetics, volume 19, pages 17?30.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Hansen A. Schwartz and Fernando Gomez. 2008. Ac-
quiring knowledge from the web to be used as selec-
tors for noun sense disambiguation. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 105?112,
Manchester, England, August.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. Irvine, CA, Septem-
ber.
Deniz Yuret. 2007. KU: Word sense disambiguation by
substitution. In Proceedings of SemEval-2007, pages
207?214, Prague, Czech Republic, June.
36
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146?1151,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Developing Age and Gender Predictive Lexica over Social Media
Maarten Sap
1
Gregory Park
1
Johannes C. Eichstaedt
1
Margaret L. Kern
1
David Stillwell
3
Michal Kosinski
3
Lyle H. Ungar
2
and H. Andrew Schwartz
2
1
Department of Psychology, University of Pennsylvania
2
Computer & Information Science, University of Pennsylvania
3
Psychometrics Centre, University of Cambridge
maarten@sas.upenn.edu
Abstract
Demographic lexica have potential for
widespread use in social science, economic,
and business applications. We derive predic-
tive lexica (words and weights) for age and
gender using regression and classification
models from word usage in Facebook, blog,
and Twitter data with associated demographic
labels. The lexica, made publicly available,
1
achieved state-of-the-art accuracy in language
based age and gender prediction over Face-
book and Twitter, and were evaluated for
generalization across social media genres as
well as in limited message situations.
1 Introduction
Use of social media has enabled the study of psycho-
logical and social questions at an unprecedented scale
(Lazer et al., 2009). This allows more data-driven dis-
covery alongside the typical hypothesis-testing social
science process (Schwartz et al., 2013b). Social me-
dia may track disease rates (Paul and Dredze, 2011;
Google, 2014), psychological well-being (Dodds et al.,
2011; De Choudhury et al., 2013; Schwartz et al.,
2013a), and a host of other behavioral, psychological
and medical phenomena (Kosinski et al., 2013).
Unlike traditional hypothesis-driven social science,
such large-scale social media studies rarely take into
account?or have access to?age and gender informa-
tion, which can have a major impact on many ques-
tions. For example, females live almost five years
longer than males (cdc, 2014; Marengoni et al., 2011).
Men and women, on average, differ markedly in their
interests and work preferences (Su et al., 2009). With
age, personalities gradually change, typically becom-
ing less open to experiences but more agreeable and
conscientious (McCrae et al., 1999). Additionally, so-
cial media language varies by age (Kern et al., 2014;
Pennebaker and Stone, 2003) and gender (Huffaker and
Calvert, 2005). Twitter may have a male bias (Mislove
et al., 2011), while social media in general skew to-
wards being young and female (pew, 2014).
Accessible tools to predict demographic variables
can substantially enhance social media?s utility for so-
1
download at http://www.wwbp.org/data.html
cial science, economic, and business applications. For
example, one can post-stratify population-level results
to reflect a representative sample, understand variation
across age and gender groups, or produce personalized
marketing, services, and sentiment recommendations;
a movie may be generally disliked, except by people in
a certain age group, whereas a product might be used
primarily by one gender.
This paper describes the creation of age and gen-
der predictive lexica from a dataset of Facebook users
who agreed to share their status updates and reported
their age and gender. The lexica, in the form of words
with associated weights, are derived from a penalized
linear regression (for continuous valued age) and sup-
port vector classification (for binary-valued gender). In
this modality, the lexica are simply a transparent and
portable means for distributing predictive models based
on words. We test generalization and adapt the lex-
ica to blogs and Twitter, plus consider situations when
limited messages are available. In addition to use in
the computational linguistics community, we believe
the lexicon format will make it easier for social sci-
entists to leverage data-driven models where manually
created lexica currently dominate
2
(Dodds et al., 2011;
Tausczik and Pennebaker, 2010).
2 Related Work
Online behavior is representative of many aspects of
a user?s demographics (Pennacchiotti and Popescu,
2011; Rao et al., 2010). Many studies have used lin-
guistic cues (such as ngrams) to determine if someone
belongs to a certain age group, be it on Twitter or an-
other social media platform (Al Zamal et al., 2012;
Argamon et al., 2009; Nguyen et al., 2013; Rangel
and Rosso, 2013). Gender prediction has been studied
across blogs (Burger and Henderson, 2006; Goswami
et al., 2009), Yahoo! search queries (Jones et al., 2007),
and Twitter (Burger et al., 2011; Nguyen et al., 2013;
Liu and Ruths, 2013; Rao et al., 2010). Because Twit-
ter does not make gender or age available, such work
infers gender and age by leveraging profile informa-
tion, such as gender-discriminating names or crawling
for links to publicly available data (e.g. Burger et al.,
2
The LIWC lexicon, derived manually based on psycho-
logical theory, (Pennebaker et al., 2001) had 1136 citations in
2013 alone.
1146
2011).
While many studies have examined prediction of age
or gender, none (to our knowledge) have released a
model to the public, much less in the form of a lexi-
con. Additionally, most works in age prediction clas-
sify users into bins rather than predicting a continuous
real-valued age as we do (exceptions: Nguyen et al.,
2013; Jones et al., 2007). People have also used online
media to infer other demographic-like attributes such
as native language (Argamon et al., 2009), origin (Rao
et al., 2010), and location (Jones et al., 2007). An ap-
proach similar to the one presented here could be used
to create lexica for any of these outcomes.
While lexica are not often used for demographics,
data-driven lexicon creation over social media has been
well studied for sentiment, in which univariate tech-
niques (e.g. point-wise mutual information) domi-
nate
3
. For example, Taboada et al. (2011) expanded
an initial lexicon by adding on co-occurring words.
More recently, Mohammad?s sentiment lexicon (Mo-
hammad et al., 2013) was found to be the most in-
formative feature for the top system in the SemEval-
2013 social media sentiment analysis task (Wilson et
al., 2013). Approaches like point-wise mutual infor-
mation take a univariate view on words?i.e. the weight
given to one feature (word) is not affected by other
features. Since language is highly collinear, we take
a multivariate lexicon development approach, which
takes covariance into account (e.g. someone who men-
tions ?hair? often is more likely to mention ?brushing?,
?style?, and ?cut?; weighting these words in isolation
might ?double-count? some information).
3 Method
Primary data. Our primary dataset consists of Face-
book messages from users of the MyPersonality appli-
cation (Kosinski and Stillwell, 2012). Messages were
posted between January 2009 and October 2011. We
restrict our analysis to those Facebook users meeting
certain criteria: they must indicate English as a primary
language, have written at least 1,000 words in their sta-
tus updates, be younger than 65 years old (data beyond
this age becomes very sparse), and indicate their gen-
der and age. This resulted in a dataset of N = 75,394
users, who wrote over 300 million words collectively.
We split our sample into training and test sets. Our
primary test set consists of a 1,000 randomly selected
Facebook users, while the training set that we used for
creating the lexica was a subset (N = 72,874) of the
remaining users.
Additional data To evaluate our predictive lexica in
differing situations, we utilize three additional datasets:
3
Note that the point-wise information-derived sentiment
lexica are often used as features in a supervised model, essen-
tially dimensionally reducing a large set of words into posi-
tive and negative sentiment, while our lexica represent the
predictive model itself.
stratified Facebook data, blogs, and tweets. The strat-
ified Facebook data (exclusively used for testing) con-
sists of equal proportions of 1,520 males and females
across 12 4-year age bins starting at 13 and ending at
60.
4
This roughly matchs the size of the main test set.
Seeking out-of-domain data, we downloaded age
and gender annotated blogs from 2004 (Schler et al.,
2006) (also used in Goswami et al., 2009) and gender
labeled tweets (Volkova et al., 2013). Limiting the sam-
ple to users who wrote at least 1000 words, the total
number of bloggers is 15,006, of which 50.6% are fe-
male and only 15% are over 27 (reflecting the younger
population standard in social media). From this we use
a randomly selected 1,000 bloggers as a blogger test set
and the remaining 14,006 bloggers for training. Sim-
ilarly for the Twitter dataset, we use 11,000 random
gender-only annotated users, in which 51.9% are fe-
male. We again randomly select 1,000 users as a test
set for gender prediction and use the remaining 10,000
for training.
3.1 Lexicon Creation
We present a method of weighted lexicon creation by
using the coefficients from linear multivariate regres-
sion and classification models. Before delving into the
creation process, consider that a weighted lexicon is of-
ten applied as the sum of all weighted word relative
frequencies over a document:
usage
lex
=
?
word?lex
w
lex
(word) ?
freq(word, doc)
freq(?, doc)
where w
lex
(word) is the lexicon (lex) weight for the
word, freq(word, doc) is frequency of the word in the
document (or for a given user), and freq(?, doc) is the
total word count for that document (or user).
Further consider how one applies linear multivariate
models in which the goal is to optimize feature coeffi-
cients that best fit the continuous outcome (regression)
or separate two classes (classification):
y = (
?
f?features
w
f
? x
f
) + w
0
where x
f
is the value for a feature (f ), w
f
is the fea-
ture coefficient, and w
0
is the intercept (a constant fit
to shift the data such that it passes through the origin).
In the case of regression, y is the outcome value (e.g.
age) while in classification y is used to separate classes
(e.g. >= 0 is female, < 0 is male). If all features are
word relative frequencies (
freq(word,doc)
freq(?,doc)
) then many
multivariate modeling techniques can simply be seen
as learning a weighted lexicon plus an intercept
5
.
4
65 females and 65 males in each of the first 11 bins:
[13,16], [17,20], . . . , [53, 56]; the last bin ([57, 60]) contained
45 males and 45 females. The [61.64] bin was excluded as it
was much smaller.
5
included in the lexicon distribution
1147
age gender
model\corpus randFB stratFB randBG randFB stratFB randBG randT
r mae r mae r mae acc acc acc acc
baseline 0 6.14 0 11.62 0 6.11 .617 .500 .508 .518
FB
lex
.835 3.40 .801 6.94 .710 5.76 .917 .913 .774 .856
BG
lex
.664 4.26 .656 11.39 .768 3.63 .838 .803 .824 .834
FB+BG
lex
.831 3.42 .795 7.06 .762 3.76 .913 .909 .822 .858
T
lex
.816 .820 .763 .889
FB+BG+T
lex
.919 .910 .820 .900
Table 1: Prediction accuracies for age (Pearson correlation coefficient(r); mean absolute error (mae) in years)
and gender (accuracy %). Baseline for age is mean age of training sample; for gender, it is the most frequent
class (female). Lexica tested include those derived from Facebook (FB
lex
), blogs (BG
lex
), and Twitter (T
lex
).
We evaluate over a random Facebook sample (randFB), a stratified Facebook sample (stratFB), a random blogger
sample (randBG), and a random twitter sample (randT). All results were a significant (p < 0.001) improvement
over the baseline.
In practice, we learn our 1gram coefficients (i.e. lex-
icon weights) from ridge regression (Hoerl and Ken-
nard, 1970) for age (continuous variable) and from sup-
port vector classification (Fan et al., 2008) for gender
(binary variable). Ridge regression uses an L2 (?||?||
2
)
penalization to avoid overfitting (Hoerl and Kennard,
1970). Although some words no doubt have a non-
linear relationship with age (e.g., ?fiance? peaks in the
20s), we still find high accuracy from a linear model
(see Table 1) and it allows for a distribution of the
model in the accessible form of a lexicon. For gender
prediction, we use an SVM with a linear kernel with
L1 penalization (?||?||
1
) (Tibshirani, 1996). Because
the L1 penalization zeros-out many coefficients, it has
the added advantage of effectively reducing the size of
the lexica. Using the training data, we test a variety al-
gorithms including the lasso, elastic net regression, and
L2 penalized SVMs in order to decide which learning
algorithms to use.
To extract the words (1grams) to use as features
and which make up lexica, we use the Happier Fun
Tokenizer,
6
which handles social media content and
markup such as emoticons or hashtags. For our main
user-level models, word usage is aggregated as the rel-
ative frequency (
freq(word,user)
freq(?,user)
). Due to the sparse
and large vocabulary of social media data, we limit the
1grams to those used by at least 1% of users.
4 Evaluation
We evaluate our predictive lexica across held-out user
data. First, we see how well lexica derived from Face-
book users predict a random set of additional users.
Then, we explore generalization of the models in vari-
ous other settings: on a stratified Facebook test sample,
blogs, and Twitter. Finally, we compare lexica fit to a
restricted number of messages per user.
Results of our evaluation over Facebook users are
shown in Table 1 (randFB columns). Accuracies for
age are reported as Pearson correlation coefficients (r)
6
downloaded from http://www.wwbp.org/data.html
and mean absolute errors (mae), measured in years.
For gender, we use an accuracy % (number-correct over
test-size). As baselines, we use the mean for age (23.0
years old) and the most frequent class (female) for gen-
der. We see that for both age and gender, accuracies are
substantially higher than the baseline. These accuracies
were just below with no significant difference previous
state-of-the-art results (Schwartz et al., 2013; r = 0.84
for age and 91.9% accuracy for gender).
7
Because of the nature of our datasets (the Face-
book data is private) and task (user-level predictions),
comparable previous studies are nearly nonexistent.
Nonetheless, the Twitter data was a random subset of
users based on the (Burger et al., 2011) dataset exclud-
ing non-English tweets, making it somewhat compa-
rable. In this case, the lexica outperformed previous
results for gender prediction of Twitter users, which
ranged from 75.5% to 87% (Burger et al., 2011; Ciot
et al., 2013; Liu and Ruths, 2013; Al Zamal et al.,
2012). However, the lexica were unable to match the
92.0% accuracy Burger et al. (2011) achieved when
using profile information in addition to text. No other
similar studies ? to the best of our knowledge ? have
been conducted.
Application in other settings. While Facebook is the
ideal setting to apply our lexica, we hope that they gen-
eralize to other situations. To evaluate their utility in
other settings, we first tested them over a gender and
age stratified Facebook sample. Our random sample,
like all of Facebook, is biased toward the young; this
stratified test sample contains equal numbers of males
and females, ages 13 to 60. Next, we use the lexica to
predict data from other domains: blogs (Schler et al.,
2006) and Twitter (Volkova et al., 2013). In this case,
our goal was to account for the content and stylistic
variation that may be specific to Facebook.
7
Adding 2 and 3-grams increases the performance of our
model (r = 0.85, 92.7%), just above our previous results
(Schwartz et al., 2013b). However, with the accessibility of
single word lexica in mind, this current work focuses on fea-
tures based entirely on 1grams.
1148
# Msgs: all 100 20 5 1
age .831 .820 .688 .454 .156
gender .919 .901 .796 .635 .554
Table 2: Prediction accuracies for age (Pearson correla-
tion) and gender (accuracy %) when reducing the num-
ber of messages from each user.
Results over these additional datasets are shown in
Table 1 (stratFB, randBG, and randT columns). The
performance decreases as expected since these datasets
have differing distributions, but it is still substantially
above mean and most frequent class baselines on the
stratified dataset. Over blogs and Twitter, both age
and gender prediction accuracies drop to a greater de-
gree (when only using the Facebook-trained models),
suggesting stylistic or content differences between the
domains. However, when using lexica created with
data from across multiple domains, the results in Face-
book, blogs, and Twitter remain in line with results
from models created specifically over their respective
domains. In light of this result, we release the FB+BG
age & FB+BG+T gender models as lexica (available at
www.wwbp.org/data.html).
Limiting messages per user. As previously noted,
some applications of demographic estimation require
predictions over more limited messages. We explore
the accuracy of user-level age and gender predictions
as the number of messages per user decreases in Ta-
ble 2. For these tests we used the FB+BG age &
FB+BG+T gender lexica. Confirming findings by Van
Durme (2012), the fewer posts one has for each user,
the less accurate the gender and age predictions. Still,
given the average user posted 205 messages, it seems
that not all messages from a user are necessary to make
a decent inference on their age and gender. Future work
may explore models developed specifically for these
limited situations.
5 Conclusion
We created publicly available lexica (words and
weights) using regression and classification models
over language usage in social media. Evaluation of the
lexica over Facebook yielded accuracies in line with
state-of-the-art age (r = 0.831) and gender (91.9% ac-
curacy) prediction. By deriving the lexica from Face-
book, blogs, and Twitter, we found the predictive power
generalized across all three domains with little sacrifice
to any one domain, suggesting the lexica may be used
in additional social media domains. We also found the
lexica maintain reasonable accuracy when writing sam-
ples were somewhat small (e.g. 20 messages) but other
approaches may be best when dealing with more lim-
ited data.
Given that manual lexica are already extensively em-
ployed in social sciences such as psychology, eco-
nomics, and business, using lexical representations of
data-driven models allows the utility of our models to
extend beyond the borders of the field of NLP.
Acknowledgement
Support for this work was provided by the Templeton
Religion Trust and by Martin Seligman of the Univer-
sity of Pennsylvania?s Positive Psychology Center.
References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.
Shlomo Argamon, Moshe Koppel, James W Pen-
nebaker, and Jonathan Schler. 2009. Automatically
profiling the author of an anonymous text. Commu-
nications of the ACM, 52(2):119?123.
John D Burger and John C Henderson. 2006. An ex-
ploration of observable features related to blogger
age. In AAAI Spring Symposium: Computational
Approaches to Analyzing Weblogs, pages 15?20.
John D Burger, John C Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309. Association for Computational Linguis-
tics.
2014. Faststats: How healthy are we. http://www.
cdc.gov/nchs/fastats/healthy.htm.
Accessed on March 12, 2014.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Predicting postpartum changes in
emotion and behavior via social media. In Pro-
ceedings of the 2013 ACM annual conference on
Human factors in computing systems, pages 3267?
3276. ACM.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one, 6(12):e26752.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Inc. Google. 2014. Google flu trends. http://
www.google.org/flutrends. Accessed on
March 12, 2014.
1149
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Third International AAAI Conference on We-
blogs and Social Media.
Arthur E Hoerl and Robert W Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55?67.
David A Huffaker and Sandra L Calvert. 2005.
Gender, identity, and language use in teenage
blogs. Journal of Computer-Mediated Communica-
tion, 10(2):00?00.
Rosie Jones, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2007. I know what you did last summer:
query logs and user privacy. In Proceedings of the
sixteenth ACM conference on Conference on infor-
mation and knowledge management, pages 909?914.
ACM.
Margaret L Kern, Johannes C Eichstaedt, H Andrew
Schwartz, Gregory Park, Lyle H Ungar, David J
Stillwell, Michal Kosinski, Lukasz Dziurzynski, and
Martin EP Seligman. 2014. From sooo excited!!!
to so proud: Using language to study development.
Developmental psychology, 50(1):178?188.
Michal Kosinski and David J Still-
well. 2012. mypersonality project.
http://www.mypersonality.org/wiki/.
Michal Kosinski, David J Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. volume
110, pages 5802?5805. National Acad Sciences.
David Lazer, Alex Pentland, Lada Adamic, Sinan Aral,
Albert-Laszlo Barabasi, Devon Brewer, Nicholas
Christakis, Noshir Contractor, James Fowler, Myron
Gutmann, Tony Jebara, Gary King, Michael Macy,
Deb Roy, and Marshall Van Alstyne. 2009. Com-
putational social science. Science, 323(5915):721?
723.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In Analyzing Microtext: 2013 AAAI Spring
Symposium.
Alessandra Marengoni, Sara Angleman, Ren?e Melis,
Francesca Mangialasche, Anita Karp, Annika Gar-
men, Bettina Meinow, and Laura Fratiglioni. 2011.
Aging with multimorbidity: a systematic review of
the literature. Ageing research reviews, 10(4):430?
439.
Robert R McCrae, Paul T Costa, Margarida Pedroso
de Lima, Ant?onio Sim?oes, Fritz Ostendorf, Alois
Angleitner, Iris Maru?si?c, Denis Bratko, Gian Vitto-
rio Caprara, Claudio Barbaranelli, et al. 1999. Age
differences in personality across the adult life span:
parallels in five cultures. Developmental Psychol-
ogy, 35(2):466?477.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J Niels Rosenquist. 2011.
Understanding the demographics of twitter users.
ICWSM, 11:5th.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. arXiv
preprint arXiv:1308.6242.
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. how old do you think i am?:
A study of language and age in twitter. In Proceed-
ings of the Seventh International AAAI Conference
on Weblogs and Social Media.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation. In ICWSM.
James W Pennebaker and Lori D Stone. 2003. Words
of wisdom: language use over the life span. Journal
of Personality and Social Psychology, 85(2):291?
301.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. 71:2001.
2014. Social networking fact sheet. http:
//www.pewinternet.org/fact-sheets/
social-networking-fact-sheet/. Ac-
cessed on August 26, 2014.
Francisco Rangel and Paolo Rosso. 2013. Use of lan-
guage and author profiling: Identification of gender
and age. Natural Language Processing and Cogni-
tive Science, page 177.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W Pennebaker. 2006. Effects of age
and gender on blogging. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, volume 6, pages 199?205.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Megha Agrawal,
Gregory J Park, Shrinidhi K Lakshmikanth, Sneha
Jha, Martin EP Seligman, Lyle Ungar, et al. 2013a.
Characterizing geographic variation in well-being
using tweets. In ICWSM.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David J Stillwell, Martin EP Seligman, et al.
2013b. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one, 8(9):e73791.
1150
Rong Su, James Rounds, and Patrick Ian Armstrong.
2009. Men and things, women and people: a meta-
analysis of sex differences in interests. Psychologi-
cal Bulletin, 135(6):859?884.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of language
and social psychology, 29(1):24?54.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267?288.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 48?58. Association
for Computational Linguistics.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of the
2013 Conference on Empirical Methods on Natural
Language Processing.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval, volume 13.
1151
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 392?395,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UCF-WS: Domain Word Sense Disambiguation using Web Selectors
Hansen A. Schwartz and Fernando Gomez
School of Electrical Engineering and Computer Science
University of Central Florida
Orlando, FL 32816
{hschwartz,gomez}@cs.ucf.edu
Abstract
This paper studies the application of the
Web Selectors word sense disambiguation
system on a specific domain. The system
was primarily applied without any domain
tuning, but the incorporation of domain
predominant sense information was ex-
plored. Results indicated that the system
performs relatively the same with domain
predominant sense information as without,
scoring well above a random baseline, but
still 5 percentage points below results of
using the first sense.
1 Introduction
We explore the use of the Web Selectors word
sense disambiguation system for disambiguating
nouns and verbs of a domain text. Our method to
acquire selectors from the Web for WSD was first
described in (Schwartz and Gomez, 2008). The
system is extended for the all-words domain task
by including part of speech tags from the Stanford
Parser (Klein and Manning, 2003). Additionally, a
domain adaptation technique of using domain pre-
dominant senses (Koeling et al, 2005) is explored,
but our primary goal is concerned with evaluating
the performance of the existing Web Selectors sys-
tem on domain text.
In previous studies, the Web Selectors system
was applied to text of a general domain. However,
the system was not directly tuned for the general
domain. The system may perform just as strong
for domain WSD since the selectors, which are the
core of disambiguation, can come from any do-
main present on the Web. In this paper, we study
the application of the Web Selectors WSD algo-
rithm to an all-words task on a specific domain,
the SemEval 2010: Task 17 (Agirre et al, 2010).
2 Web Selectors
Selectors are words which take the place of a given
target word within its local context (Lin, 1997). In
the case of acquiring selectors from the Web, we
search with the text of local context (Schwartz and
Gomez, 2008). For example, if one was search-
ing for selectors of ?channel? in the sentence, ?The
navigation channel undergoes major shifts from
north to south banks?, then a search query would
be:
The navigation * undergoes major shifts from
north to south banks .
where * represents a wildcard to match every se-
lector. The query is shortened to produce more
results until at least 300 selectors are acquired or
the query is less than 6 words. The process of
acquiring selectors repeats for every content word
of the sentence. Example selectors that might be
returned for ?channel? include ?route?, ?pathway?,
and ?passage?.
Selectors serve for the system to essentially
learn the areas or concepts of WordNet that the
sense of a word should be similar or related. The
target noun or verb is disambiguated by comparing
its senses with all selectors for itself (target selec-
tors), as well as with context selectors for other
nouns, verbs, adjective, adverbs, proper nouns,
and pronouns in the sentence. Figure 1 shows the
overall process undertaken to rank the senses of
an ambiguous word. A similarity measure is used
when comparing with target selectors and a relat-
edness measure is used when comparing with con-
text selectors. Referring to our previous example,
the senses of ?channel? are compared to its own
(target) selectors via similarity measures, while
relatedness measures are used for the context se-
lectors: noun selectors of ?navigation?, ?shifts?,
?north?, ?south?, and ?banks?; the verb selectors of
392
Figure 1: The overall process undertaken to disambiguate a word using Web selectors.
?undergoes?; plus the adjective selectors of ?ma-
jor?. Adverbs, proper nouns, and pronouns are not
present in the sentence, and so no selectors from
those parts of speech are considered.
For this study, we implemented the Web Selec-
tors system that was presented in (Schwartz and
Gomez, 2009). This generalized version of the
system may annotate verbs in addition to nouns,
and it includes the previously unused context se-
lectors of adverbs. We used the path-based sim-
ilarity measure of (Jiang and Conrath, 1997) for
target selectors, and the gloss-based relatedness
measure of (Banerjee and Pedersen, 2002) for con-
text selectors.
The incorporation of a part of speech tagger was
a necessary addition to the existing system. Previ-
ous evaluations of Web Selectors relied on the test-
ing corpus to provide part of speech (POS) tags
for content words. In the case of SemEval-2010
Task 17, words were only marked as targets, but
their POS was not included. We used the POS
tags from the Stanford Parser (Klein and Manning,
2003). We chose this system since the dependency
relationship output was also useful for our domain
adaptation (described in section 2.1). A modifica-
tion was made to the POS tags given the knowl-
edge that the testing corpus only included nouns
and verbs as targets. Any target that was not ini-
tially tagged as a noun or verb was reassigned as
a noun, if the word existed as a noun in WordNet
(Miller et al, 1993), or as a verb if not.
2.1 Domain Adaptation
Overall, the Web Selectors system is not explicitly
tuned to the general domain. Selectors themselves
can be from any domain. However, sense tagged
data may be used indirectly within the system.
First, the similarity and relatedness measures used
in the system may rely on SemCor data (Miller et
al., 1994). Also, the system breaks ties by choos-
ing the most frequent sense according to WordNet
frequency data (based on SemCor). These two as-
pects of the system can be seen as tuned to the
general domain, and thus, they are likely aspects
of the system for adaptation to a specific domain.
For this work, we focused on domain-adapting
the tie breaker aspect of the Web Selectors sys-
tem. The system defines a tie occurring when mul-
tiple sense choices are scored within 5% of the top
sense choice. In order to break the tie, the system
normally chooses the most frequent sense among
the tied senses. However, it would be ideal to
break the tie by choosing the most prevalent sense
over the testing domain. Because sense tagged do-
main data is not typically available, Koeling et al
(2005) presented the idea of estimating the most
frequent sense of a domain by calculating sense
prevalence scores from unannotated domain text.
Several steps are taken to calculate the preva-
lence scores. First, a dependency database is cre-
ated, listing the frequencies that each dependency
relationship appears. In our case, we used the
Stanford Parser (Klein and Manning, 2003) on the
background data provided by the task organizers.
From the dependency database, a thesaurus is cre-
ated based on the method of (Lin, 1998). In our ap-
proach, we considered the following relationships
from the dependency database:
subject (agent, csubj, subjpass, nsubj, nsubjpass,
xsubj)
direct object (dobj)
indirect object (iobj)
393
adjective modifier (amod)
noun modifier (nn)
prepositional modifier (any preposition, exclud-
ing prep of and prep for)
(typed dependency names listed in parenthesis)
Finally, a prevalence score is calculated for each
sense of a noun or verb by finding the similarity
between it and the top 50 most similar words ac-
cording to the automatically created thesaurus. As
Koeling et al did, we use the similarity measure
of (Jiang and Conrath, 1997).
3 Results and Discussion
The results of our system are given in Table 1. The
first set of results (WS) was a standard run of the
system without any domain adaptation, while the
second set (WS
dom
) was from a run including the
domain prevalence scores in order to break ties.
The results show our domain adaptation technique
did not lead to improved results. Overall, WS re-
sults came in ranked thirteenth among twenty-nine
participating system results.
We found that using the prevalence scores alone
to pick a sense (i.e. the ?predominant sense?) re-
sulted in an F score of 0.514 (PS in Table 1).
Koeling et al (2005) found the predominant
sense to perform significantly better than the first
sense baseline (1sense: equivalent to most fre-
quent sense for the English WordNet) on specific
domains (32% error reduction on a finance do-
main, and 62% error reduction on a sports do-
main). Interestingly, there was no significant error
reduction over the 1sense for this task, implying
either that the domain was more difficult to adapt
to or that our implementation of the predominant
sense algorithm was not as strong as that use by
Koeling et al In any case, this lack of significant
error reduction over the 1sense may explain why
our WS
dom
results were not stronger than the WS
results. In WS
dom
, prevalence scores were used
instead of 1sense to break ties.
We computed a few figures to gain more in-
sights on the system?s handling of domain data.
Noun precision was 0.446 while verb precision
was 0.449. It was unexpected for verb disam-
biguation results to be as strong as nouns because
a previous study using Web Selectors found noun
sense disambiguation clearly stronger than verb
sense disambiguation on a coarse-grained corpus
P R F P
n
P
v
rand 0.23 0.23 0.23
1sense 0.505 0.505 0.505
WS 0.447 0.441 0.444 .446 .449
WS
dom
0.440 0.434 0.437 .441 .438
PS 0.514 0.514 0.514 .53 .44
Table 1: (P)recision, (R)ecall, and (F)-score of
various runs of the system on the Task 17 data.
P
n
and P
v
correspond to precision results broken
down by nouns and verbs.
P
en1
P
en2
P
en3
WS 0.377 0.420 0.558
WS
dom
0.384 0.415 0.531
Table 2: Precision scores based on the three docu-
ments of the English testing corpora (?en1?, ?en2?,
and ?en3?).
(Schwartz and Gomez, 2009). Ideally, our results
for noun disambiguation would have been stronger
than the the 1sense and PS results. In order to
determine the effect of the POS tagger (parser in
this case) on the error, we determined 1.6% of the
error was due to the wrong POS tag at (0.9% of
all instances). Lastly, Table 2 shows the precision
scores for each of the three documents from which
the English testing corpus was created. Without
understanding the differences between the testing
documents it is difficult to explain why the preci-
sion varies, but the figures may be useful for com-
parisons by others.
Several aspects of the test data were unexpected
for our system. Some proper nouns were consid-
ered as target words. Our system was not orig-
inally intended to annotate proper nouns, but we
were able to adjust it to treat them simply as nouns.
To be sure this treatment was appropriate, we also
submitted results where proper nouns were ex-
cluded, and got a precision of 0.437 and recall
of 0.392. One would expect the precision to in-
crease at the expense of recall if the proper nouns
were more problematic for the system than other
instances. This was not the case, and we conclude
our handling of proper nouns was appropriate.
Unfortunately, another unexpected aspect of the
data was not handled correctly by our system. Our
system only considered senses from one form of
the target word according to WordNet, while the
key included multiple forms of a word. For exam-
ple, the key indicated low tide-1 was the answer to
394
an instance where our system had only considered
senses of ?tide?. We determined that for 10.2%
of the instances that were incorrect in our WS re-
sults we did not even consider the correct sense
as a possible prediction due to using an inventory
from only one form of the word. Since this issue
mostly applied to nouns it may explain the obser-
vation that the noun disambiguation performance
was not better than the verb disambiguation per-
formance as was expected.
4 Conclusion
In this paper we examined the application of the
Web Selectors WSD system to the SemEval-2010
Task 17: All-words WSD on a Specific Domain. A
primary goal was to apply the pre-existing system
with minimal changes. To do this we incorporated
automatic part of speech tags, which we found
only had a small impact on the error (incorrectly
tagged 0.9% of all target instances). Overall, the
results showed the system to perform below the
1sense baseline for both nouns and verbs. This is a
lower relative performance than past studies which
found the disambiguation performance above the
1sense for nouns. One reason for the lower noun
performance is that for 10.2 % of our errors, the
system did not consider the correct sense choice
as a possibility. Future versions of the system will
need to expand the sense inventory to include other
forms of a word (example: ?low tide? when disam-
biguating ?tide?).
Toward domain adaptation, we ran an exper-
iment in which one aspect of our system was
tuned to the domain by using domain prevalence
scores (or ?predominant senses?). We found no im-
provement from using this adaptation technique,
but we also discovered that results entirely based
on predictions of the domain predominant senses
were only minimally superior to 1sense (F-score
of 0.514 versus 0.505 for 1sense). Thus, future
studies will examine better implementation of the
predominant sense algorithm, as well as explore
other complimentary techniques for domain adap-
tation: customizing similarity measures for the
domain, or restricting areas of WordNet as sense
choices based on the domain.
Acknowledgement
This research was supported by the NASA
Engineering and Safety Center under
Grant/Cooperative Agreement NNX08AJ98A.
References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of SemEval-2010. Association for Computa-
tional Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Proceedings of the Third In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, Mexico City,
Mexico.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING X, Taiwan.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems 15, pages 3?10.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of
the conference on Human Language Technology
and Experimental Methods in NLP, pages 419?426,
Morristown, NJ, USA.
Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th annual meeting on Association
for Computational Linguistics, pages 64?71.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL 98, pages 768?774, Montreal, Canada. Morgan
Kaufmann.
George Miller, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
George A. Miller, Martin Chodorow, Shari L, Claudia
Leacock, and Robert G. Thomas. 1994. Using a se-
mantic concordance for sense identification. In In
Proc. of ARPA Human Language Technology Work-
shop.
Hansen A. Schwartz and Fernando Gomez. 2008. Ac-
quiring knowledge from the web to be used as se-
lectors for noun sense disambiguation. In CoNLL
2008: Proceedings of the Twelfth Conference on
Computational Natural Language Learning, pages
105?112, Manchester, England, August.
Hansen A. Schwartz and Fernando Gomez. 2009.
Using web selectors for the disambiguation of all
words. In Proceedings of the NAACL-2009 Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 28?36, Boulder,
Colorado, June.
395
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 679?683,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Penn: Using Word Similarities to better Estimate Sentence Similarity
Sneha Jha and H. Andrew Schwartz and Lyle H. Ungar
University of Pennsylvania
Philadelphia, PA, USA
{jhasneha, hansens, ungar}@seas.upenn.edu
Abstract
We present the Penn system for SemEval-
2012 Task 6, computing the degree of seman-
tic equivalence between two sentences. We
explore the contributions of different vector
models for computing sentence and word sim-
ilarity: Collobert and Weston embeddings as
well as two novel approaches, namely eigen-
words and selectors. These embeddings pro-
vide different measures of distributional simi-
larity between words, and their contexts. We
used regression to combine the different simi-
larity measures, and found that each provides
partially independent predictive signal above
baseline models.
1 Introduction
We compute the semantic similarity between pairs
of sentences by combining a set of similarity met-
rics at various levels of depth, from surface word
similarity to similarities derived from vector mod-
els of word or sentence meaning. Regression is then
used to determine optimal weightings of the differ-
ent similarity measures. We use this setting to as-
sess the contributions from several different word
embeddings.
Our system is based on similarities computed us-
ing multiple sets of features: (a) naive lexical fea-
tures, (b) similarity between vector representations
of sentences, and (c) similarity between constituent
words computed using WordNet, using the eigen-
word vector representations of words , and using se-
lectors, which generalize words to a set of words that
appear in the same context.
2 System Description
This section briefly describes the feature sets used to
arrive at a similarity measure between sentences. We
compare the use of word similarities based on three
different embeddings for words neural embeddings
using recursive autoencoders, eigenwords and selec-
tors.
2.1 Neural Models of Word Representation
An increasingly popular approach is to learn repre-
sentational embeddings for words from a large col-
lection of unlabeled data (typically using a genera-
tive model), and to use these embeddings to augment
the feature set of a supervised learner. These models
are based on the distributional hypothesis in linguis-
tics that words that occur in similar contexts tend
to have similar meanings. The similarities between
these vectors indicate similarity in the meanings of
corresponding words.
The state of the art model in paraphrase detection
uses an unsupervised recursive autoencoder (RAE)
model based on an unfolding objective that learn
feature vectors for phrases in syntactic parse trees
(Socher et al, 2011). The idea of neural language
models is to jointly learn an embedding of words
into an n-dimensional vector space that capture dis-
tributional syntactic and semantic information via
the words co-occurrence statistics. Further details
and evaluations of these embeddings are discussed
in Turian et al (2010).
Once the distributional syntactic and semantic
matrix is learned on an unlabeled corpus, one can
use it for subsequent tasks by using each words vec-
tor to represent that word. For initial word embed-
dings, we used the 100-dimensional vectors com-
679
puted via the unsupervised method of Collobert and
Weston (2008). These word embeddings are matri-
ces of size |V | ? n where |V | is the size of the vo-
cabulary and n is the dimensionality of the semantic
space. This matrix usually captures co-occurrence
statistics and its values are learned. We used the
embeddings provided by Socher et al (2011). Al-
though the original paper employed a dynamic pool-
ing layer in addition to the RAE that captures the
global structure of the similarity matrix, we found
the resulting sentence-level RAE itself was useful.
In turn, we use these vector representations at the
sentence level where the cosine similarity between
the sentence vectors serves as a measure of sentence
similarity. All parameters for the RAE layer are kept
same as described by Socher et al (2011).
2.2 Eigenword Similarity
Recent spectral methods use large amounts of un-
labeled data to learn word representations, which
can then be used as features in supervised learners
for linguistic tasks. Eigenwords, a spectral method
for computing word embeddings based on context
words that characterize the meanings of words, can
be efficiently computed by a set of methods based on
singular value decomposition (Dhillon et al, 2011).
Such representations are dense, low dimensional
and real-valued like the vector representations in the
previous section except that they are induced us-
ing eigen-decomposition of the word co-occurrence
matrix instead of neural networks. This method
uses Canonical Correlation Analysis (CCA) be-
tween words and their immediate contexts to es-
timate word representations from unlabeled data.
CCA is the analog to Principal Component Analysis
(PCA) for pairs of matrices. It computes the direc-
tions of maximal correlation between a pair of matri-
ces. CCAs invariance to linear data transformations
enables proofs showing that keeping the dominant
singular vectors faithfully captures any state infor-
mation. (For this work, we used the Google n-gram
collection of web three-grams as the unlabeled data.)
Each dimension of these representations captures la-
tent information about a combination of syntactic
and semantic word properties. In the original paper,
the word embeddings are context-specific. For this
task, we only use context-oblivious embeddings i.e.
one embedding per word type for this task, based
on their model. Word similarity can then be cal-
culated as cosine similarity between the eigenword
representation vectors for any two words.
To move from word-level similarity to sentence-
level a few more steps are necessary. We adapted
the method of matrix similarity given by Stevenson
and Greenwood (2005). One calculates similarity
between all pairs of words, and each sentence is rep-
resented as a binary vector (with elements equal to 1
if a word is present and 0 otherwise). The similarity
between these sentences vectors ~a and~b is given by:
s(~a,~b) =
~aW~b
|~a||~b|
(1)
where W is a semantic similarity matrix contain-
ing information about the similarity of word pairs.
Each element in matrix W represents the similarity
of words according to some lexical or spectral simi-
larity measure.
2.3 Selector Similarity
Another novel method to account for the similarity
between words is via comparison of Web selectors
(Schwartz and Gomez, 2008). Selectors are words
that take the place of an instance of a target word
within its local context. For example, in ?he ad-
dressed the strikers at the rally?, selectors for ?strik-
ers? might be ?crowd?, ?audience?, ?workers?, or ?stu-
dents? words which can realize the same constituent
position as the target word. Since selectors are de-
termined based on the context, a set of selectors is an
abstraction for the context of a word instance. Thus,
comparing selector sets produces a measure of word
instance similarity. A key difference between selec-
tors and the eigenwords used in this paper are that
selectors are instance specific. This has the benefit
that selectors can distinguish word senses, but the
drawback that each word instance requires its own
set of selectors to be acquired.
Although selectors have previously only been
used for worse sense disambiguation, one can also
use them to compute similarity between two word
instances by taking the cosine similarity of vectors
containing selectors for each instance. In our case,
we compute the cosine similarity for each pair of
noun instances and populate the semantic similarity
matrix in formula (1) to generate a sentence-level
680
similarity estimate. Combining web selector- based
word similarity features with the word embeddings
from the neural model gave us the best overall per-
formance on the aggregated view of the data sets.
2.4 Other Similarity Metrics
Knowledge-Based. We use WordNet to calculate
semantic distances between all open-class words in
the sentence pairs. There are three classifications
of similarity metrics over WordNet: path-based,
information- content based, and gloss-based (Ped-
erson et al, 2004). We chose to incorporate those
measures performing best in the Schwartz & Gomez
(2011) application-oriented evaluation: (a) the path-
based measure of Schwartz & Gomez (2008); (b)
the information-content measure of Jiang & Conrath
(1997) utilizing the difference in information con-
tent between concepts and their point of intersection;
(c) the gloss-based measure of Patwardhan & Peder-
sen (2006). By including metrics utilizing different
sources of information, we suspect they will each
have something novel to contribute.
Because WordNet provides similarity between
concepts (word senses), we take the maximum simi-
larity between all senses of each word to be the sim-
ilarity between the two words. Such similarity can
then be computed between multiple pairs of words
to populate the semantic similarity matrix W in for-
mula (1) and generate sentence-level similarity esti-
mates as described above. The information-content
and path-based measures are restricted to compar-
ing nouns and verbs and only across the same part
of speech. On the other hand, the gloss-based mea-
sure, which relies on connections through concept
definitions, is more general and can compare words
across parts of speech.
Surface Metrics. We added the following set of
lexical features to incorporate some surface infor-
mation lost in the vector-based representations.
? difference in the lengths of the two sentences
? average length of the sentences
? number of common words based on exact
string match
? number of content words in common
? number of common words in base form
? number of similar numerals in the sentences
3 Evaluation and Results
We combine the similarity metrics discussed previ-
ously via regression (Pedregosa et al, 2011). We
included the following sets of features:
? System-baseline: surface metrics, knowledge-
based metrics. (discussed in section 2.4).
? Neu: Neural Model similarity (section 2.1)
? Ew: Eigenword similarity (section 2.2)
? Sel: Selector similarity (section 2.3)
To capture possible non-linear relations, we added
a squared and square-rooted column corresponding
to each feature in the feature matrix. We also tried
to combine all the features to form composite mea-
sures by defining multiple interaction terms. Both
these sets of additional features improved the per-
formance of our regression model. We used all fea-
tures to train both a linear regression model and a
regularized model based on ridge regression. The
regularization parameter for ridge regression was set
via cross-validation over the training set. All pre-
dictions of similarity values were capped within the
range [0,1]. Our systems were trained on the follow-
ing data sets:
? MSR-Paraphrase, Microsoft Research Para-
phrase Corpus-750 pairs of sentences.
? MSR-Video, Microsoft Research Video De-
scription Corpus-750 pairs of sentences.
? SMT-Europarl, WMT2008 development data
set (Europarl section)-734 pairs of sentences.
Our performance in the official submission for the
SemEval task can be seen in Table 1. LReg indi-
cates the run with linear regression, ELReg adds
the eigenwords feature and ERReg also uses eigen-
words but with ridge regression. At the time of sub-
mission, we were not ready to test with the selector
features yet. Ridge regression consistently outper-
formed linear regression for every run of our sys-
tem, but overall Pearson score for our system using
linear regression scored the highest. Table 2 presents
a more thorough examination of results.
681
MSRpar MSRvid SMT-eur On-WN SMT-news ALLnrm Mean ALL
task-baseline .4334 .2996 .4542 .5864 .3908 .6732 (85) .4356 (70) .3110 (87)
LReg .5460 .7818 .3547 .5969 .4137 .8043 (36) .5699 (41) .6497 (33)
ELReg .5480 .7844 .3513 .6040 .3607 .8048 (34) .5654 (44) .6622 (27)
ERReg .5610 .7857 .3568 .6214 .3732 .8083 (28) .5755 (37) .6573 (28)
Table 1: Pearson?s r scores for the official submission. ALLnrm: Pearson correlation after the system outputs for each
dataset are fitted to the gold standard using least squares, and corresponding rank. Mean: Weighted mean across the
5 datasets, where the weight depends on the number of pairs in the dataset. ALL: Pearson correlation with the gold
standard for the five datasets, and corresponding rank. Parentheses indicate official rank out of 87 systems.
MSRpar MSRvid SMT-eur On-WN SMT-news Mean ALL
system-baseline .5143 .7736 .3574 .5017 .3867 .5343 .6542
+Neu .5243 .7811 .3772 .4860 .3410 .5318 .6643
+Ew .5267 .7787 .3853 .5237 .4495 .5560 .6724
+Sel .4973 .7684 .3129 .4812 .4016 .5306 .6492
+Neu, +Ew .5481 .7831 .2751 .5576 .3424 .5404 .6647
+Neu, +Sel .5230 .7775 .3724 .5327 .3787 .5684 .6818
+Ew, +Sel .5239 .7728 .2842 .5191 .4038 .5320 .6554
+Neu, +Ew, +Sel .5441 .7835 .2644 .5877 .3578 .5472 .6645
Table 2: Pearson?s r scores for runs based on various combinations of features. Mean: Weighted mean across the 5
datasets, where the weight depends on the number of pairs in the dataset. ALL: Pearson correlation with the gold
standard for the five datasets, and corresponding rank.
Discussion. In the aggregate, we see that each of
the similarity metrics has the ability to improve re-
sults when used with the right combination of other
features. For example, while selector similarity by
itself does not seem to help overall, using this met-
ric in conjunction with the neural model of similar-
ity gives us our best results. Interestingly, the oppo-
site is true of eigenword similarity, where the best
results are seen when they are independent of selec-
tors or the neural models. The decreased correla-
tions can be accounted for by the new features intro-
ducing over fitting, and one should note that no such
reductions in performance are significant compared
to the baseline, where as our best performance is a
significant (p < 0.05) improvement.
There are a few potential directions for future im-
provements. We did not tune our system differently
for different data sets although there is evidence of
specific features favoring certain data sets. In the
case of the neural model of similarity we expect
that deriving phrase level representations from the
sentences and utilizing the dynamic pooling layer
should give us a more thorough measure of simi-
larity beyond the sentence-level vectors we used in
this work. For eigenwords, we would like to experi-
ment with context-aware vectors as was described in
(Dhillon et. al, 2011). Lastly, we were only able to
acquire selectors for nouns, but we believe introduc-
ing selectors for other parts of speech will increase
the power of the selector similarity metric.
4 Conclusion
In this paper, we described two novel word-level
similarity metrics, namely eigenword similarity and
selector similarity, that leverage Web-scale corpora
in order to build word-level vector representations.
Additionally, we explored the use of a vector-model
at the sentence-level by unfolding a neural model of
semantics. We utilized these metrics in addition to
knowledge-based similarity, and surface-level simi-
larity metrics in a regression system to estimate sim-
ilarity at the sentence level. The performance of the
features varies significantly across corpora but at the
aggregate, eigenword similarity, selector similarity,
and the neural model of similarity all are shown to be
capable of improving performance beyond standard
surface-level and WordNet similarity metrics alone.
682
References
Eneko Agirre, Daniel Cer, Mona Diab and Aitor
Gonzalez. 2012. The SemEval-2012 Task-6 : A
Pilot on Semantic Textual Similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing : deep
neural networks with multitask learning. In Inter-
national Conference on Machine Learning. Pages
160-167.
Paramveer Dhillon, Dean Foster and Lyle Ungar.
2011. Multiview learning of word embeddings via
CCA. In Proceedings of Neural Information Pro-
cessing Systems.
Jay Jiang and David Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics, pages
1933.
Dekang Lin. 1997. Using syntactic dependency as
local context to resolve word sense ambiguity. In
Proceedings of the 35th annual meeting of Associa-
tion for Computational Linguistics, pages 64-71.
Ted Pedersen, Siddharth Patwardhan and Jason
Michelizzi. 2004. WordNet::Similarity-measuring
the relatedness of concepts. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V.
Michel, B. Thirion, G. Grisel, M. Blondel, P. Pret-
tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A.
Passos, D. Cournapeau, M. Brucher, M. Perrot, E.
Duchesnay. 2011. Scikit-learn: Machine Learning
in Python. Journal of Machine Learning Research.
Vol 12.2825-2830
Hansen A. Schwartz and Fernando Gomez. 2008.
Acquiring knowledge from the web to be used as se-
lectors for noun sense disambiguation. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning.
Hansen A. Schwartz and Fernando Gomez. 2011.
Evaluating semantic metrics on tasks of concept
similarity. In Proceedings of the twenty-fourth
Florida Artificial Intelligence Research Society.
Palm Beach, Florida: AAAI Press.
Richard Socher, Eric H. Huang, Jeffrey Penning-
ton, Andrew Y. Ng and Christopher Manning. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 379386.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proceed-
ings of the annual meeting of Association for Com-
putational Linguistics.
683
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 296?305, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Choosing the Right Words:
Characterizing and Reducing Error of the Word Count Approach
H. Andrew Schwartz,1 Johannes Eichstaedt,1 Lukasz Dziurzynski,1 Eduardo Blanco,2
Margaret L. Kern,1 Stephanie Ramones,1 Martin Seligman,1 and Lyle Ungar1
1University of Pennsylvania
2Lymba Corporation
hansens@seas.upenn.edu
Abstract
Social scientists are increasingly using the
vast amount of text available on social me-
dia to measure variation in happiness and
other psychological states. Such studies count
words deemed to be indicators of happiness
and track how the word frequencies change
across locations or time. This word count ap-
proach is simple and scalable, yet often picks
up false signals, as words can appear in differ-
ent contexts and take on different meanings.
We characterize the types of errors that occur
using the word count approach, and find lex-
ical ambiguity to be the most prevalent. We
then show that one can reduce error with a
simple refinement to such lexica by automat-
ically eliminating highly ambiguous words.
The resulting refined lexica improve precision
as measured by human judgments of word oc-
currences in Facebook posts.
1 Introduction
Massive social media corpora, such as blogs, tweets,
and Facebook statuses have recently peaked the in-
terest of social scientists. Compared to traditional
samples in tens or hundreds, social media sample
sizes are orders of magnitude larger, often contain-
ing millions or billions of posts or queries. Such text
provides potential for unobtrusive, inexpensive, and
real-time measurement of psychological states (such
as positive or negative affect) and aspects of sub-
jective well-being (such as happiness and engage-
ment). Social scientists have recently begun to use
social media text in a variety of studies (Cohn et
al., 2004; Kramer, 2010; Tausczik and Pennebaker,
2010; Kamvar and Harris, 2011; Dodds et al, 2011;
Golder and Macy, 2011).
One of the most popular approaches to estimate
psychological states is by using the word count
method (Pennebaker et al, 2007), where one tracks
the frequency of words that have been judged to be
associated with a given state. Greater use of such
words is taken to index the prevalence of the cor-
responding state. For example, the use of the word
?happy? is taken to index positive emotion, and ?an-
gry? to index negative emotion. The most widely
used tool to carry out such analysis, and the one we
investigate in this paper, is Pennebaker?s Linguistic
Inquiry and Word Count, (LIWC) (Pennebaker et al,
2001; Pennebaker et al, 2007). LIWC, originally de-
veloped to analyze writing samples for emotion and
control, has grown to include a variety of lexica for
linguistic and psychosocial topics including positive
and negative emotions, pronouns, money, work, and
religion. The word count approach has high appeal
to social scientists in need of a tool to approach so-
cial media, and although others have been used (see,
for example (Gottschalk and Bechtel, 1998; Bollen
et al, 2010), LIWC?s lexica are generally perceived
as a ?tried-and-tested? list of words (Miller, 2011).
Unfortunately, the word count approach has some
drawbacks when used as indicators for psycholog-
ical states. Words are the unit of measurement, but
words can carry many different meanings depending
on context. Consider the Facebook posts below con-
taining instances of ?play?, a word associated with
positive emotion in LIWC.
296
1. so everyone should come to the play tomor-
row...
2. Does anyone what type of file i need to convert
youtube videos to play on PS3???
3. Time to go play with Chalk from the Easter
Bunny!
Out of the three instances, only (3) seems to com-
municate positive emotion. In (1), ?play? is used as
a noun rather than the expected verb, while in (2),
?play? is a verb but it is used in a sense that is not
directly associated with positive emotion. (1) and
(2) demonstrate how lexical ambiguities (i.e. multi-
ple parts-of-speech or word senses) can affect accu-
racy of words in a lexicon. Additionally, even when
appearing as the expected part of speech and word
sense, signal from a word may change due to its con-
text, such as being within the scope of a negation as
in (4), or describing something desired as in (5).
4. ...all work no play :-(
5. i sure wish i had about 50 hours a day to play
cod
Our goal is to characterize the errors of the widely
used word count approach, and show that such lex-
ica can be significantly improved by employing an
ambiguity metric to refine such lexica. Rather than
work on a new method of measuring psychological
states, we work within the bounds of word count and
ask how accurate it is and whether we can improve
it without sacrificing its simplicity and scalability.
We attempt to reduce the erroneous signal of
the word count approach while maintaining legiti-
mate signal simply by refining the lexicon. In other
words, we would like to move closer to the goal in
Figure 1, by eliminating words that often carry er-
roneous signal such as ?play?, and keeping words
which often carry the sought-after signal, such as
?cheerful?. The difficulty in doing this is that we do
not have the data to tell us which words are most
likely to carry signal (even if we had such data we
would like to develop a method that could be applied
to any newly created lexica). Instead we leverage
part-of-speech and word sense data to help us deter-
mine which words are lexically ambiguous.
Figure 1: The relationship between text expressing posi-
tive emotion (POSEMO) and text containing LIWC terms
for POSEMO.
Our approach of eliminating ambiguous words
increases the precision at the expense of recall, a
reasonable trade-off in social media where we are
working with millions or even billions of word in-
stances. Additionally, it is minimally-supervised, in
that we do not require training data on human-state;
instead we use existing hand-labeled corpora, such
as SemCor (Miller et al, 1993), for word sense in-
formation. Not requiring training data also means
our refinement is flexible; it can be applied to mul-
tiple domains and lexica, it makes few assumptions
that might introduce problems of over-fitting, and it
is parsimonious in that it merely improves an estab-
lished approach.
This paper makes two primary contributions: (1)
an analysis of the types of errors common for the
word count approach (Section 3), and (2) a general
method for refining psychosocial lexica based on the
ambiguity of words (Section 4). Before describing
these contributions, we discuss related work, mak-
ing the case for using social media in social science
and surveying some work in computational linguis-
tics. We then evaluate both the original LIWC lex-
icon and our refinement of it against human judg-
ments of expression of positive and negative emo-
tions on hand-annotated Facebook posts, and show
the benefit of lexicon refinement for estimating well-
being over time for large aggregates of posts. Fi-
nally, we discuss the implications of our work and
possible future directions.
297
2 Background
Compared to traditional approaches in the social sci-
ences, large scale analysis of social media is cheap,
near real-time, unobtrusive, and gives high cover-
age. We outline these advantages below.
Inexpensive Extracting information from sources
such Facebook and Twitter is vastly cheaper than the
more conventional polling done by companies such
as Gallup ? and by many social science researchers.
Social media data does not require phone calls to be
made or doors to be knocked on. For example, a rep-
resentative survey asking 1,000 people by a leading
polling company costs to the order of $10,0001. In
contrast, once the software exists, social media data
from tens of millions of users can be obtained and
analyzed at a fraction of the cost.
Temporal Resolution Much of the attraction of
social media stems from the fact that it captures
a written live stream of collective thought. When
Google relied on search queries to monitor health-
seeking behavior to predict influenza epidemics, the
reporting lag was a mere day, whereas traditional
CDC surveillance systems take 1-2 weeks to pub-
lish their data (Ginsberg et al, 2009). Infrastructure
based on social media and Internet use data allows
reporting and analysis systems with little to no re-
porting lag. Additionally, traditional survey designs
are typically only designed to assess psychological
states at a given point in time.
Unobtrusive Estimation Traditional self-report
survey approaches, even those implemented on the
web, suffer from social desirability, priming, and
other biases. For example, Kahneman et al (Kah-
neman et al, 2006) found that the order in which
questions are asked on questionnaires can determine
how they are answered. By looking directly into the
social worlds, many of these self-report biases can
be avoided. The traces of human interactions in so-
cial media represent the goings-on in their original
ecologies of meaning and signification. This ap-
proach diminishes the inferential distance between
the context of the phenomena and the context of
measurement ? and thus decreases the room for sys-
tematic distortion of signal.
1Gallup, Personal correspondence.
2.1 The Word Count Approach
As previously noted, the word count approach is
most often used by social scientists through the tool
known as Linguistic Inquiry and Word Count or
LIWC (Pennebaker et al, 2007). The LIWC2007
dictionary is composed of almost 4,500 words and
word stems organized across one or more word cat-
egories, including 406 positive emotion words and
499 negative emotion words. When long form texts
are analyzed with LIWC, the program simply re-
turns the percentages of words belonging to the dif-
ferent analytical categories ? the simplicity of this
approach makes it popular with non-technical social
scientists.
LIWC?s positive and negative emotion lexica have
recently begun to be used on ?short form? writing in
social media. For example, Golder and Macy (2011)
used LIWC to study diurnal and seasonal variation
in mood in a collection of 400 million Twitter mes-
sages. Kramer (2010) proposed the ?Gross National
Happiness? index and Kivran-Swaine and Naaman
(2011) examined associations between user expres-
sions of positive and negative emotions and the size
and density of social networks. A comprehensive
review can be found in Tausczik and Pennebaker
(2010).
To our knowledge there is only one work which
has evaluated LIWC?s accuracy over social media.
Bantum and Owen (2009) evaluated LIWC on a set
of posts to an Internet-based breast cancer support
group. By annotating expression of emotion within
this text, they were able to produce accuracy figures
of sensitivity (much like recall) and predictive va-
lidity (precision). Sensitivity measured how often
a word (in context) expressing positive or negative
emotion was captured by LIWC. Predictive validity
measured how often a word (in context) captured
by LIWC as measuring positive or negative emotion
was indeed expressing positive or negative emotion.
While they found a recall of 0.88, the precision was
only 0.31 ? that is, only 31% of instances contain-
ing words indexed by LIWC actually conveyed the
associated emotion. We contend that this is a major
drawback for applying LIWC to social media, be-
cause while it is not important to catch every expres-
sion of emotion out of a million Tweets, it is impor-
tant that when something is captured it is an accurate
298
estimate of the true state.
2.2 Related Work in Computational
Linguistics
Researchers have been exploring the use of lexica
that define the subjective orientation of words for
tasks such as sentiment or subjectivity analysis. A
common weakly-supervised approach starts with a
small set of sentiment knowledge (seed words as-
sociated with a given sentiment) and expands the
words into a large lexicon (Hatzivassiloglou and
McKeown, 1997; Kamps and Marx, 2002; Kim and
Hovy, 2004; Kanayama and Nasukawa, 2006; Bac-
cianella et al, 2010). We take a different approach.
Rather than expanding lexica, we start with a large
set of words and refine the set. The refinement in-
creases precision at the cost of recall, which is a
reasonable exchange when we are looking at mil-
lions or even billions of word instances. Standard
applications of sentiment analysis, such as annotat-
ing movie reviews, may not be as inclined to skip
instances, since they want to make predictions for
items which have very few reviews.
Another line of work in sentiment analysis has
created lexicons using supervised learning. One of
the first works to do so was by Pang and colleagues
(2002), who used data including author ratings of
reviews, such as IMDB movie reviews. The author
ratings become training data for sentiment classifi-
cation. Pang et al showed that human-created lexi-
cons did not perform as well as lexicons based on
simple word statistics over the training data. In-
terestingly, they found that words like ?still? were
most predictive of positive movie reviews, and that
punctuation marks of ?!? and ??? were strong signs
of negative movie reviews. Unfortunately, training
data for subjective well-being or happiness is not
yet available, preventing the use of such supervised
learning methods. Additionally, this work seeks to
experiment within the bounds of what social sci-
entists are in fact using (with publications in high-
impact venues such as Science). We thus take a dif-
ferent approach, and automatically improve human
created lexicons.
Wiebe and Cardie (2005) generalized the task of
sentiment analysis to that of discovering subjectiv-
ity such as ?opinions, emotions, sentiments, specu-
lations, evaluations, etc.?. More recently, Wilson et
POSEMO NEGEMO
term frequency term frequency
like 774,663 hate 167,109
love 797,833 miss 158,274
good 571,687 bad 151,496
friend* 406,568 bore* 140,684
happy 384,797 shit* 114,923
LOL 370,613 hurt* 98,291
well* 284,002 craz* 94,518
great 263,113 lost 94,059
haha* 240,587 damn* 93,666
best 227,381 fuck 90,212
better 212,547 stupid* 85,587
fun 216,432 kill* 83,593
please* 174,597 hell 80,046
hope 170,998 fuckin* 79,959
thank 161,827 wrong* 70,714
Table 1: Most frequent POSEMO and NEGEMO terms in
LIWC in the 12.7 million Facebook posts. ?*? indicates a
wildcard, so that ?well*? matches ?wellness?.
al. (2009) contended that the context may neutralize
or change the polarity of the subjective orientation
of a word. It is difficult to determine where concepts
of happiness such as quality of relationships or de-
gree of achievement in life fit in with subjectivity.
Thus, we do not claim to be measuring subjectivity
and instead we use the general term of ?psychologi-
cal state?, referring to ?the way something [a person]
is with respect to its main attributes? (Miller, 1993).
To the best of our knowledge, while part-of-
speech tagging and word sense disambiguation are
staple tasks in the computational linguistics commu-
nity, the utility of a lexical ambiguity metric has yet
to be explored.
3 Annotation and Analysis of Errors from
the Word Count Method
One objective of our work is to document and de-
scribe how often different types of errors occur when
using the word count approach on social media. To
do this, we first judged a sample of 1,000 instances
of LIWC terms occurring in Facebook posts to indi-
cate whether they contribute signal towards the as-
sociated LIWC category (i.e. positive emotion). We
then took instances that were deemed to carry erro-
neous signal and annotated them with a label for the
299
category agreement instances base rate
POSEMO 0.742 500 .654
NEGEMO 0.746 500 .697
TOTAL 0.744 1,000 .676
random 0.343 - -
Table 2: Inter-annotator agreement over 1,000 instances
of LIWC terms in Facebook posts. Base rate is the aver-
age of how often an annotator answered true.
type of signal error. This section describes the pro-
cess we used in generating these annotations and the
results we found.
3.1 Annotation Process
Annotating social media instances of lexica terms
provides insight into how well the word count ap-
proach works, and also yields a ?ground truth? for
evaluating our lexicon refinement methods. We ran-
domly selected for labeling a sample of 1,000 sta-
tus updates containing words from a given lexicon
drawn from a collection of 12.7 million Facebook
status updates provided by the Cambridge myPer-
sonality project (Kosinski and Stillwell, 2012).
We used terms from the LIWC positive emotion
(POSEMO) and negative emotion (NEGEMO) lex-
ica, which are the same lexica used by the works of
Kramer (2010), Kivran-Swaine and Naaman (2011),
and Golder and Macy (2011). Table 1 lists the
most frequent POSEMO and NEGEMO terms in our
Facebook sample.
As mentioned above, we did two types of annota-
tions. First, we judged whether each given instance
of a word conveyed the correct associated type of
emotion. The second task took a sample of instances
judged to have incorrect signal and labeled them
with a reason for the error; We refer to this as signal
error type.
For the first task, we had three human judges inde-
pendently evaluate the 1,000 status update instances
as to whether they were indeed correct signal. The
question the judges were told to answer was ?Does
the word contribute to the associated psychological-
state (POSEMO or NEGEMO) within the sentence
it appears??. In other words, ?would the sentence
convey less [positive emotion or negative emotion]
without this word??. Subjective feedback from the
judges indicated that it was often difficult to make
a decision, so we used three judges per instance. In
the case of conflict between judges, the ?correct? la-
bel for validation of the refined lexicon was defined
to be the majority vote. A sampling of Facebook sta-
tuses demonstrates a mixed picture of relevance for
the unrefined LIWC dictionaries:
1. has had a very good day (?good? - POSEMO)
2. is so very bored. (?bore*? - NEGEMO)
3. damn, that octopus is good, lol (?damn? -
NEGEMO)
4. thank you for his number (?numb*? -
NEGEMO)
5. I got pranked sooooo bad (?bad? - NEGEMO)
6. don?t be afraid to fail (?afraid? - NEGEMO)
7. I wish I could . . . and we could all just be happy
(?happy? - POSEMO)
Some posts clearly use positive or negative lexicon
words such as (1) and (2). Curse words can signify
negative emotion or emphasize the opposite state as
in (3), which is clearly emphasizing positive emo-
tion here. Example (5) demonstrates the word sense
issue we discussed previously. Words with wild-
cards that expand into other words with different
meanings can be particularly problematic, as the ex-
panded word can be far more frequent ? and very
different in meaning ? from the original word. For
example, ?numb*? matches ?number? in 4.
A different problem occurs when the context
of the word changes its implication for the emo-
tional state of the writer. This can either occur
through negation such as in (6) where ?afraid? sig-
nals NEGEMO, but is negated with ?don?t? or the
signal can be changed indirectly through a variety of
words indicating that the writer desires (and hence
lacks) the state, as in (7) where someone is wishing
to be ?happy?.
Table 2 shows the agreement between an-
notators calculated as
?
i agree(A
(i)
1 ,A
(i)
2 ,A
(i)
3 )
1,000 , where
agree(A1, A2, A3) was 1 when all three annota-
tions matched and 0 otherwise. Given the aver-
age positive base rate across annotators was 0.676
the chance that all three reviewers agree accord-
ing to chance (random agreement) is calculated as
300
category precision instances
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
Table 4: Accuracy of LIWC POSEMO and NEGEMO
lexica over Facebook posts.
0.6763+(1?0.676)3 = 0.343, the probability of all
three answering yes plus the probability of all three
answering no.
For the second task, we selected 100 instances
judged to be incorrect signal from the first task, and
labeled them according to the best reason for the
mistake. This task required more linguistic exper-
tise and was performed by a single annotator. La-
bels and descriptions are given in Table 3, which
breaks down the cases into lexical ambiguity, direct
or indirect negation, and other reasons such as the
stemming issue (stem plus wildcard expanding into
words indicating a different (or no) emotional state).
3.2 Analysis of Errors
Before discussing the types of errors we found when
using the word count approach, we examine LIWC?s
overall accuracy on our dataset. Table 4 shows the
precision broken down for both the positive emotion
(POSEMO) and the negative emotion (NEGEMO)
lexica. We see that the precision for NEGEMO is
slightly higher than POSEMO, indicating the terms
in that category may be more likely to indicate their
associated state.
Although the overall accuracy seems decent, one
should keep in mind our subjective judgement crite-
ria were quite tolerant, allowing any amount of con-
tribution of the corresponding signal to be consid-
ered accurate. For example, a salutation like ?Happy
New Year? was judged to be a correct use of ?happy?
to signal POSEMO, even though it clearly does not
have as strong a signal as someone saying ?I feel
deliriously happy?.
Frequencies of signal errors are given in Table
5. The most common signal error was wrong word
sense, where the word did not signal emotional
state and some other sense or definition of the word
was intended (e.g. ?u feel like ur living in a mu-
sic video?; corresponding to the sense ?to inhabit?
rather than the intended sense, ?to have life; be
category label frequency
Lexical Ambiguity
Wrong POS 15
Wrong WS 38
Signal Negation
Strict Negation 16
Desiring 6
Other
Stem Issue 5
Other 24
Table 5: Frequency of the signal error types.
alive? (Miller, 1993)). Other common signal errors
include strict negation where the word is canceled
out by a clear negative quantifier (e.g. ?Don?t be
afraid to fail?) and wrong part of speech where the
word is signaling a different part of speech than the
emotion (e.g. ?well, we cant afford to go to NYC?).
There were also various other signal error types that
include stem issues where the stem matched clearly
unintended words, desiring statuses where the status
is commenting on wanting the emotion instead of
experiencing it and other less prevalent issues such
as non-English language post, memes, or clear sar-
casm.
4 Method for Refining Lexica
The idea behind our refinement method is to remove
words that are likely to carry erroneous signal about
the underlying state or emotion of the person writ-
ing the tweet or Facebook post.2 We do so in an
indirect fashion, without actually using training data
of which posts are, in fact indicative of positive or
negative emotion. Instead, we focus on reducing er-
rors that are due to lexical ambiguity. By remov-
ing words that are often used with multiple parts of
speech or multiple senses, we can tilt the balance to-
ward precision at some cost in recall (losing some
signal from the ambiguous words). This makes the
word count approach more suitable for use in the
massive corpora afforded by social media.
4.1 Lexical Ambiguity
We address lexical ambiguity at the levels of both
part of speech (POS) and word sense. As a metric
of inverse-ambiguity, we determine the probability
that a random instance is the most frequent sense
(mfs) of the most frequent part of speech (mfp) of
2Refinement tool is available at wwbp.org.
301
category label description examples
Lexical Ambiguity
Wrong POS Not a valid signal because it is
the wrong POS
so everyone should come to the
play tomorrow...
Wrong WS Not a valid signal because it is
the wrong word sense (includes
metaphorical senses)
Does anyone what type of file i
need to convert youtube videos
to play on PS3???
Signal Negation
Strict Negation Within the scope of a negation,
where there is a clear negative
quantifier
...all work no play :-(
Desiring Within the scope of a desire /
wishing for something
i sure wish i had about 50 hours
a day to play cod
Other
Stem Issue Clearly not intended to be
matched with the given stem
numb* for NEGEMO match-
ing number
Other Any other issue or difficult to
classify
Table 3: Signal error types.
the word, denoted TSP (for top sense probability).
Given a wordw, we consider all parts of speech ofw
(POS(w)) and all senses for the most frequent part
of speech (senses(mfp(w))):
pmfp(w) =
max
[wpos?POS(w)]
fp(wpos)
?
wpos?POS(w)
fp(wpos)
pmfs(w) =
max
[wsense?senses(mfp(w))]
fs(wsense)
?
wsense?senses(mfp(w))
fs(wsense)
TSP (w) = (pmfp(w) ? pmfs(w))
2 (1)
Here, fp and fs represent the frequencies of a cer-
tain part-of-speech and a certain sense of a word,
respectively. This is the squared-probability that an
instance of w is the top sense ? the most-frequent
part-of-speech and the most-frequency sense of that
part-of-speech. The probability is squared because
both the word in the lexicon and the word occurring
in context should be the top sense (two independent
probabilities: given an instance of a word in a cor-
pus, and another instance of the word in the lexicon,
what is the probability that both are the top POS
and sense?). Frequency data is provided for parts-
of-speech from the Google N-Grams 2.0 (Lin et al,
2010) and for word senses from SemCor (Miller et
al., 1993). This aspect of the refinement is inspired
by the most frequent sense heuristic for word sense
disambiguation (McCarthy et al, 2004; Yarowsky,
1993), in which the sense of a word is chosen with-
out regard to the context, but rather is simply based
on the frequencies of senses in corpora. In our case,
we restrict ourselves this way in order for the appli-
cation of the lexicon to remain unchanged.
For some words, we were unable to find sense fre-
quency data. We decided to keep such terms, on
the assumption that a lack in available frequency in-
formation implies that the word is not very ambigu-
ous. Many of these terms include Web speak such as
?haha? or ?lol?, which we believe can carry a strong
signal for positive and negative emotion.
Lastly, since TSP is only a metric for the in-
verse ambiguity of a word, we must apply a thresh-
old to determine which words to keep. We denote
this threshold as ?, and the description of the refined
lexicon for a category, cat, is below.
lex?(cat) = {w|w ? cat ? TSP (w) > ?}
4.2 Handling Stems
Some lexica, such as the LIWC dictionary, include
word stems that are intended to match multiple
forms of a word. Stems are marked by the suffix
?*?. LIWC describes the application of stems as fol-
lows ?the asterisk, then, denotes the acceptance of
all letters, hyphens, or numbers following its ap-
302
lex cat prec size
full
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
lex0.10
POSEMO 70.9% 392
NEGEMO 71.6% 423
both 71.3% 815
lex0.50
POSEMO 75.7% 239
NEGEMO 78.9% 232
both 77.3% 471
lex0.90
POSEMO 72.5% 109
NEGEMO 78.1% 128
both 75.5% 237
Table 6: Precision (prec) and instance subset size (size)
of refinements to the LIWC POSEMO and NEGEMO lex-
ica with various ? thresholds (0.10, 0.50, 0.90)
pearance.?3 This presents a problem because, while
the creators of such lexica obviously intended stems
to match multiple forms of a word, stems also often
match completely different words, such as ?numb*?
matching ?number? or ?won*? matching ?won?t?.
We identified how often unintended matches hap-
pen in Section 3. Finding that the stemming issues
were not the biggest problem, here, we just describe
how they fit into our lexical ambiguity metric, rather
than describe a technique to rid the lexicon of stem-
ming problems. One approach might be to deter-
mine how ambiguous a stem is ? i.e. determine
how many words, parts-of-speech, and senses a stem
could be expanded into, but this ignores the fact that
the dictionary creators obviously intended the stem
to match multiple words. Instead, we expand stems
into all words that they match and replace them into
the lexica.
We base our expansion on the actual terms used
in social media. We find all words matching stems
among 1 million randomly selected Twitter mes-
sages posted over a 6-month period (August 2009
- February 2010), and restrict to those occurring at
least 20 times. Then, each word stem in the lexicon
is replaced with the expanded set of matching words.
Figure 2: The relationship between precision and size
when increasing the TSP threshold (?).
5 Evaluation
We evaluate our refinement by comparing against
human judgements of the emotion conveyed by
words in individual posts. In the case of hu-
man judgements, we find that the subset of human-
annotated instances matching the refined lexica are
more accurate than the complete set.
In section 3 we discussed the method we used to
judge instances of LIWC POSEMO and NEGEMO
words as to whether they contributed the associated
affect. Each of the 1,000 instances in our evaluation
corpus were judged three times such that the major-
ity was taken as truth. In order to validate our refined
lexica, we find the accuracy (precision) of the subset
of instances which contain the refined lexica terms.
Table 6 shows the change in precision when us-
ing the refined lexica. size represents the number of
instances from the full evaluation corpus matching
words in the refined lexica. One can see that ini-
tially precision increase as the size becomes smaller.
This is more clearly seen in Figure 2. As discussed
in the method section, our goal with the refine-
ment is improving precision, making lexica more
suitable to applications over massive social media
where one can more readily afford to skip instances
(i.e. smaller size) in order to achieve more accu-
racy. Still, removing more ambiguous words does
3?How it works?: http://www.liwc.net/howliwcworks.php
303
not guarantee improved precision at capturing the
intended psychological state; it is possible that that
all senses of an ambiguous word do in fact carry in-
tended signal or that the intended sense a low ambi-
guity word is not the most frequent.
Our maximum precision occurs with a threshold
of 0.50, where things somewhat level-out. This rep-
resents approximately a 23% reduction in error, and
verifies that we can increase precision through the
automatic lexicon refinement based on lexical ambi-
guity.
6 Conclusions
Social scientists and other researchers are starting
to measure psychological states such as happiness
through text in Facebook and Twitter. We have
shown that the widely used word count method,
where one simply counts occurrences of positive or
negative words, can often produce noisy and inaccu-
rate estimates of expressions of psychological states.
We characterized and measured the frequency of
different types of errors that occur using this ap-
proach, and found that when counting words without
considering context, it is lexical ambiguities (unin-
tended parts-of-speech or word senses) which cause
the most errors. We proposed a method for refin-
ing lexica by removing those words most likely to
be ambiguous, and showed that we can significantly
reduce error as measured by human judgements.
Acknowledgments
Support for this research was provided by the
Robert Wood Johnson Foundation?s Pioneer Portfo-
lio, through a grant to Martin Seligman, ?Exploring
Concepts of Positive Health?. We thank the review-
ers for their constructive and insightful comments.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, may. European Language Resources
Association (ELRA).
Erin O.C. Bantum and J.E. Owen. 2009. Evaluating the
validity of computerized content analysis programs for
identification of emotional expression in cancer narra-
tives. Psychological assessment, 21(1):79.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2010.
Twitter mood predicts the stock market. Computer and
Information Science, 1010:1?8.
Michael A. Cohn, M.R. Mehl, and J.W. Pennebaker.
2004. Linguistic markers of psychological change sur-
rounding september 11, 2001. Psychological Science,
15(10):687.
Peter Sheridan Dodds, Kameron Decker Harris, Isabel M
Kloumann, Catherine A Bliss, and Christopher M
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and twitter. Diversity, page 26.
Jeremy Ginsberg, M.H. Mohebbi, R.S. Patel, L. Bram-
mer, M.S. Smolinski, L. Brilliant, et al 2009. De-
tecting influenza epidemics using search engine query
data. Nature, 457(7232):1012?4.
Scott A. Golder and M.W. Macy. 2011. Diurnal and
seasonal mood vary with work, sleep, and daylength
across diverse cultures. Science, 333(6051):1878?
1881.
Louis A. Gottschalk and RJ Bechtel. 1998. Psychiatric
content analysis and diagnosis (pcad2000).
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Annual Meeting of the Association for Com-
putational Linguistics, pages 174?181.
Daniel Kahneman, A.B. Krueger, D. Schkade,
N. Schwarz, and A.A. Stone. 2006. Would you
be happier if you were richer? a focusing illusion.
Science, 312(5782):1908.
Jaap Kamps and Maarten Marx. 2002. Words with atti-
tude. In 1st International WordNet Conference, pages
332?341, Mysore, India.
Sepandar D. Kamvar and J. Harris. 2011. We feel fine
and searching the emotional web. In Proceedings
of the fourth ACM international conference on Web
search and data mining, pages 117?126. ACM.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 355?363, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Funda Kivran-Swaine and M. Naaman. 2011. Network
properties and social sharing of emotions in social
awareness streams. In Proceedings of the ACM 2011
304
conference on Computer supported cooperative work,
pages 379?382. ACM.
Michal. Kosinski and David J. Stillwell. 2012.
mypersonality research wiki. mypersonality project.
http://www.mypersonality.org/wiki/.
Adam D.I. Kramer. 2010. An unobtrusive behavioral
model of gross national happiness. In Proceedings of
the 28th international conference on Human factors in
computing systems, pages 287?290. ACM.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kaufman.
George A. Miller. 1993. Five papers on wordnet. Tech-
nical Report, Princeton University.
Greg Miller. 2011. Social scientists wade into the tweet
stream. Science, 333(6051):1814?1815.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Word Journal Of The International Lin-
guistic Association.
James W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development and
psychometric properties of liwc2007. Austin, TX,
LIWC. Net.
Yla R. Tausczik and J.W. Pennebaker. 2010. The psy-
chological meaning of words: Liwc and computerized
text analysis methods. Journal of Language and So-
cial Psychology, 29(1):24.
Janyce Wiebe and Claire Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. lan-
guage resources and evaluation. In Language Re-
sources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433, September.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the workshop on Human Language
Technology, HLT ?93, pages 266?271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
305
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 105?112
Manchester, August 2008
Acquiring Knowledge from the Web to be used as Selectors for Noun
Sense Disambiguation
Hansen A. Schwartz and Fernando Gomez
School of Electrical Engineering and Computer Science
University of Central Florida
{hschwartz, gomez}@cs.ucf.edu
Abstract
This paper presents a method of acquiring
knowledge from the Web for noun sense
disambiguation. Words, called selectors,
are acquired which take the place of an
instance of a target word in its local con-
text. The selectors serve for the system to
essentially learn the areas or concepts of
WordNet that the sense of a target word
should be a part of. The correct sense
is chosen based on a combination of the
strength given from similarity and related-
ness measures over WordNet and the prob-
ability of a selector occurring within the lo-
cal context. Our method is evaluated using
the coarse-grained all-words task from Se-
mEval 2007. Experiments reveal that path-
based similarity measures perform just as
well as information content similarity mea-
sures within our system. Overall, the re-
sults show our system is out-performed
only by systems utilizing training data or
substantially more annotated data.
1 Introduction
Recently, the Web has become the focus for many
word sense disambiguation (WSD) systems. Due
to the limited amount of sense tagged data avail-
able for supervised approaches, systems which are
typically referred to as unsupervised, have turned
to the use of unannotated corpora including the
Web. The advantage of these systems is that they
can disambiguate all words, and not just a set of
words for which training data has been provided.
In this paper we present an unsupervised system
which uses the Web in a novel fashion to perform
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
sense disambiguation of any noun, incorporating
both similarity and relatedness measures.
As explained in (Brody et al, 2006), there are
generally two approaches to unsupervised WSD.
The first is referred to as token based, which com-
pares the relatedness of a target word to other
words in its context. The second approach is type
based, which uses or identifies the most common
sense of a word over a discourse or corpus, and an-
notates all instances of a word with the most com-
mon sense. Although the type based approach is
clearly bound to fail occasionally, it is commonly
found to produce the strongest results, rivaling su-
pervised systems (McCarthy et al, 2004). We
identify a third approach through the use of selec-
tors, first introduced by (Lin, 1997), which help
to disambiguate a word by comparing it to other
words that may replace it within the same local
context.
We approach the problem of word sense dis-
ambiguation through a relatively straightforward
method that incorporates ideas from the token,
type, and selector approaches. In particular, we
expand the use of selectors in several ways. First,
we revise the method for acquiring selectors to be
applicable to the web, a corpus that is, practically
speaking, impossible to parse in whole. Second,
we describe a path-based similarity measure that
is more suited for a portion of our method than the
relatedness measures used by token based systems.
Finally, we expand the use of selectors to help with
disambiguating nouns other than the one replaced.
2 Background
2.1 Word Sense Disambiguation
A popular approach to using the web or unanno-
tated corpora for word sense disambiguation in-
volves the use of monosemous relatives. Monose-
mous relatives are words which are similar to a
105
sense of the target word, but which only have one
sense. By searching text for these words, one can
build training data for each sense of a target word.
This idea was proposed by (Leacock et al, 1998).
More recently, the idea has been used to auto-
matically create sense tagged corpora (Mihalcea,
2002; Agirre and Martinez, 2004) . These meth-
ods queried large corpora with relatives rather than
with the context.
With some resemblances to our approach, (Mar-
tinez et al, 2006) present the relatives in context
method. A key similarity of this method with ours
is the use of context in the web queries. They pro-
duce queries with relatives in place of the target
word in a context with a window size of up to 6.
Similarly, (Yuret, 2007) first chooses substitutes
and determines a sense by looking at the proba-
bility of a substitute taking the place of the target
word within the Web1T corpus. The number of
hits each query has on the web is then used to pick
the correct sense. Our approach differs from these
in that we acquire words(selectors) from the web,
and proceed to choose a sense based on similarity
measures over WordNet (Miller et al, 1993). We
also attempt to match the context of the entire sen-
tence if possible, and we are more likely to receive
results from longer queries by including the wild-
card instead of pre-chosen relatives.
We adopted the term selector from (Lin, 1997)
to refer to a word which takes the place of another
in the same local context. Lin searched a local con-
text database, created from dependency relation-
ships over an unannotated corpora in order to find
selectors. In this case, the local context was repre-
sented by the dependency relationships. Given that
the task of producing a dependency parse database
of the Web is beyond our abilities, we search for
the surrounding local context as text in order to
retrieve selectors for a given word. Another dif-
ference is that we compare the relatedness of se-
lectors of other words in the sentence to the target
word, and we also incorporate a path-based simi-
larity measure along with a gloss-based relatedness
measure.
2.2 Similarity and Relatedness Measures
Semantic similarity and relatedness measures have
an extensive history. The measures reported in this
work were included based on appropriateness with
our approach and because of past success accord-
ing to various evaluations (Patwardhan et al, 2003;
Budanitsky and Hirst, 2006).
Many similarity measures have been created
which only use paths in the WordNet ontology.
One approach is to simply compute the length
of the shortest path between two concepts over
the hypernym/hyponym relationship (Rada et al,
1989). Other methods attempt to compensate for
the uniformity problem, the idea that some areas of
the ontology are more dense than others, and thus
all edges are not equal. (Wu and Palmer, 1994)
uses the path length from the root to the lowest
common subsumer(LCS) of two concepts scaled
by the distance from the LCS to each concept. An-
other method, by (Leacock et al, 1998), normal-
izes path distance based on the depth of hierar-
chy. Our method attempts to produce a normalized
depth based on the average depth of all concepts
which are leaf nodes below the lowest common
subsumer in a tree.
We employ several other measures in our sys-
tem. These measures implement various ideas
such as information content (Jiang and Conrath,
1997; Lin, 1997) and gloss overlaps (Banerjee and
Pedersen, 2003). For our work the path-based and
information content measures are referred to as
similarity measures, while the gloss-based meth-
ods are referred to as relatedness measures. Re-
latedness measures can be used to compare words
from different parts of speech. In past evaluations
of token based WSD systems, information con-
tent and gloss-based measures perform better than
path-based measures (Patwardhan et al, 2003; Bu-
danitsky and Hirst, 2006).
3 Method
The general idea of our method is to find the sense
of a target noun which is most similar to all se-
lectors which can replace the target and most re-
lated to other words in context and their selectors.
Our method requires that a test sentence has been
part-of-speech tagged with noun, verb, and adjec-
tive POS, and we use the selectors from all of these
parts of speech as well as noun selectors of pro-
nouns and proper nouns. In this work, we only dis-
ambiguate nouns because similarity measures for
target selectors are based heavily on the depth that
is present in the WordNet noun ontology. How-
ever, we are still able to use verb and adjective se-
lectors from the context through relatedness mea-
sures working over all parts of speech listed. The
method can be broken into two steps:
106
1. Acquire probabilities of selectors occurring
for all nouns, verbs, adjectives, pronouns and
proper nouns from the Web.
2. Rank the senses of a target noun according to
similarity with its own selectors and related-
ness with other selectors in the context.
These steps are described in detail below. Finally,
we also describe a similarity measure we employ.
3.1 Acquiring Selectors
We acquire target selectors and context selectors
from the Web. Target selectors are those words
which replace the current target word in the local
context, while context selectors are words which
may replace other words in the local context.
There are four different types of context selectors:
noun context selectors essentially the target se-
lectors for other nouns of the sentence.
verb context selectors verbs which are found to
replace other verbs in the sentence.
adjective context selectors adjectives which re-
place other adjectives in the sentence.
pro context selectors nouns which replace pro-
nouns and proper nouns.
A query must be created based on the original
sentence and target word. This is fairly straightfor-
ward as the target word is removed and replaced
with a * to indicate the wildcard. For example,
when searching for selectors of ?batter? from ?She
put the batter in the refrigerator.?, a query of ?She
put the * in the refrigerator.? is used. The queries
are sent through the Yahoo! Search Web Services1
in order to retrieve matching text on the web.
The selectors are extracted from the samples re-
turned from the web by matching the wildcard of
the query to the sample. The wildcard match is
thrown out if any of the following conditions are
true: longer than 4 words, contains any punctua-
tion, is composed only of pronouns or the origi-
nal word. Keep in mind we acquire the nouns that
replace the pronouns of the original sentence, so
a selector is never a pronoun. WordNet is used
to determine if the phrase is a compound and the
base morphological form of the head word. Re-
sults containing head words not found in WordNet
are filtered out. Proper nouns are used if they are
found in WordNet. Finally, the list of selectors is
1http://developer.yahoo.com/search/
adjusted so no single word takes up more than 30%
of the list.
The Web is massive, but unfortunately it is not
large enough to find results when querying with
a whole sentence a majority of the time. There-
fore, we perform truncation of the query to acquire
more selectors. For this first work with selectors
from the web, we chose to create a simple trunca-
tion focused just on syntax in order to run quickly.
The steps below are followed and the final step is
repeated until a stop condition is met.
i Shorten to a size of 10 words.
ii Remove end punctuation, if not preceded by *.
iii Remove front punctuation, if not proceeded by *.
iv Remove determiners (the, a, an, this, that) preceding *.
v Remove a single word.
When removing a single word, the algorithm at-
tempts to keep the * in the center. Figure 1 demon-
strates the loop that occurs until a stop condition
is met: enough selectors are found or the query
has reached a minimum size. Since a shorter query
should return the same results as a longer query, we
filter the selectors from longer query results out of
the shorter results. It is important that the criteria
to continue searching is based on the number of se-
lectors and not on the number of samples, because
many samples fail to produce a selector.Validation
experiments were performed to verify that each
step of truncation was helpful in returning more re-
sults with valid selectors, although the results are
not reported as the focus is on the method in gen-
eral. Selectors are tied to the queries used to ac-
quire them in order to help emphasize results from
longer queries.
The steps to acquire all types of selectors (tar-
get or any in context) are the same. The part of
speech only plays a part in determining the base
form or compounds when using WordNet. Note
that all selectors for each noun, verb, adjective, and
pronoun/proper can be acquired in one pass, so that
duplicate queries are not sent to the Web. When the
process is complete we have a probability value for
each selector word (w
s
) to occur in a local context
given by the acquisition query (q). The probability
of w
s
appearing in q is denoted as:
p
occ
(w
s
, q)
3.2 Ranking Senses
There are essentially two assumptions made in or-
der to rank the senses of a noun.
107
Figure 1: The overall process undertaken to disambiguate a noun. (Note that selectors only need to be
acquired once for each sentence since they can be reused for each target noun.)
1. Similar concepts (or noun senses) appear in
similar syntactic constructions.
2. The meaning of a word is often related to
other words in its context
The first assumption implies the use of a similarity
measure with target selectors. The meaning of the
target selectors should be very similar to that of
the original word, and thus we compare similarity
between all target selectors with each sense of the
original word.
The second assumption reflects the information
provided by context selectors, for which we use a
relatedness measure to compare with the original
word. Note that because context selectors may be
of a different part of speech, we should be sure this
measure is able to handle multiple parts of speech.
Regardless of the similarity or relatedness mea-
sure used, the value produced is applied the same
for both target selectors and context selectors. We
are comparing the senses (or concepts) of the origi-
nal target word with all of the selectors. To find the
similarity or relatedness of two words, rather than
two concepts, one can use the maximum value over
all concepts of the selector word and all the senses
of the target word, (Resnik, 1999, word similarity):
wsr(w
t
, w
s
) = max
c
t
,c
s
[srm(c
t
, c
s
)]
where srm is a similarity or relatedness measure
and c
t
, c
s
represent a sense (concept) of the tar-
get word (w
t
) and selector word (w
s
) respectively.
We would like to get a value for each sense of a
target word if possible, so we derive similarity or
relatedness between one concept and one word as:
cwsr(c
t
, w
s
) = max
c
s
[srm(c
t
, c
s
)]
Intuitively, combining cwsr with p
occ
is the ba-
sis for scoring the senses of each noun. However,
we also take several others values into accout, in
order to learn most effectively from Web selectors.
The score is scaled by the number of senses of the
selector and the length of the query used to ac-
quire it. This gives less ambiguous selectors and
those selectors with a most similar local context
a stronger role. These values are represented by
senses(w
s
) and qweight = current length
original length
:
score(c
t
, w
s
, q)
= p
occ
(w
s
, q) ? cwsr(c
t
, w
s
) ?
qweight
senses(w
s
)
The scores are summed with:
sum
type
(c
t
) =
?
q
?
w
s
score(c
t
, w
s
, q)
where q ranges over all queries for a type(type) of
selector, and w
s
ranges over all selectors acquired
with query q.
Overall, the algorithm gives a score to each
sense by combining the normalized sums from all
types of the selectors:
Score(c
t
) =
?
type
sum
type
(c
t
)
max
c?w
t
[sum
type
(c)]
? scale
type
where typ ranges over a type of selector (target,
noun context, verb context, adjective context, pro
context), c ranges over all senses of the target word
(w
t
), and scale
type
is a constant for each type of
selector. We experimented with different values
over 60 instances of the corpus to decide on a scale
value of 1 for target selectors, a value of 0.5 for
108
noun and verb context selectors, and a value of
0.1 for adjective and pro context selectors. This
weights the scores that come from target selectors
equal to that of noun and verb context selectors,
while the adjective and pro selectors only play a
small part.
Finally, the senses are sorted based on their
Score, and we implement the most frequent sense
heuristic as a backoff strategy. All those senses
within 5% of the top sense?s Score, are re-sorted,
ranking those with lower sense numbers in Word-
Net higher. The highest ranking sense is taken to
be the predicted sense.
3.3 Similarity Measure
We use the notion that similarity is a specific type
of relatedness (Rada et al, 1989; Patwardhan et
al., 2003). For our purposes, a similarity measure
is used for nouns which may take the place of a
target word within its local context, while words
which commonly appear in other parts of the local
context are measured by relatedness. In particular,
the similarity measure places emphasis strictly on
the is-a relationship. As an example, ?bottle? and
?water? are related but not similar, while ?cup?
and ?bottle? are similar. Because of this distinc-
tion, we would classify our path-based measure as
a similarity measure.
A well known problem with path-based mea-
sures is the assumption that the links between con-
cepts are all uniform (Resnik, 1999). As a re-
sponse to this problem, approaches based on in-
formation content are used, such as (Resnik, 1999;
Jiang and Conrath, 1997; Lin, 1997). These mea-
sures still use the is-a relationship in WordNet, but
they do not rely directly on edges to determine the
strength of a relationship between concepts. (Pat-
wardhan et al, 2003) shows that measures based
on information content or even gloss based mea-
sures generally perform best for comparing a word
with other words in its context for word sense dis-
ambiguation. However, these measures may not
be as suited for relating one word to other words
which may replace it (target selectors). Therefore,
our similarity measure examines the use of links in
WordNet, and attempts to deal with the uniformity
problem by normalizing depths based on average
leaf node depth.
All types of relatedness measures return a value
representing the strength of the relation between
the two concepts. These values usually range be-
tween 0 and 1. Note that concepts are not the
same as words, and the example above assumes
one chooses the sense of ?water? as a liquid and
the sense of ?bottle? and ?cup? as a container. Our
similarity measure is based on finding the normal-
ized depth (nd) of a concept (c) in the WordNet
Hierarchy:
nd(c) =
depth(c)
ald(c)
Where depth is the length from the concept to the
root, and ald returns the average depth of all de-
scendants (hyponyms) that do not have hyponyms
themselves (average leaf depth):
ald(c) =
?
L?lnodes(c)
depth(l)
|lnodes(c)|
To be clear, lnodes returns a list of only those
nodes without hyponyms that are themselves hy-
ponyms of c. We chose to only use the leaf depth
as opposed to all depths of descendants, because
ald produces a value representing maximum depth
for that branch in the tree, which is more appropri-
ate for normalization.
Like other similarity measures, for any two con-
cepts we compute the lowest (or deepest) common
subsumer, lcs, which is the deepest node in the hi-
erarchy which is a hypernym of both concepts. The
similarity between two concepts is then given by
the normalized depth of their lcs:
sim(c
1
, c
2
) = nd(lcs(c
1
, c
2
))
Thus, a concept compared to itself will have a
score of 1, while the most dissimilar concepts will
have a score of 0. Following (Wu and Palmer,
1994; Lin, 1997) we scale the measure by each
concept?s nd as follows:
scaled sim(c
1
, c
2
) =
2 ? sim(c
1
, c
2
)
nd(c
1
) + nd(c
2
)
where our normalized depth replaces the depth or
information content value used by the past work.
4 Evaluation
We evaluated our algorithm using the SemEval
2007 coarse-grained all-words task. In order to
achieve a coarse grained sense inventory WordNet
2.1 senses were manually mapped to the top-level
of the Oxford Dictionary of English by an expert
lexicographer. This task avoids the issues of a fine
granular sense inventory, which provides senses
109
type insts avgSels
target 1108 68.5
noun context 1108 68.5
verb context 591 70.1
adj context 362 37.3
pro context 372 31.9
Table 1: Total word instances for which selectors
were acquired (insts), and average number of se-
lectors acquired for use in each instance (avgSels).
that are difficult even for humans to distinguish.
Additionally, considering how recent the event oc-
curred, there is a lot of up-to-date data about the
performance of other disambiguation systems to
compare with. (Navigli et al, 2007)
Out of 2269 noun, verb, adjective, or adverb in-
stances we are concerned with disambiguating the
1108 noun instances from the 245 sentences in the
corpus . These noun instances represent 593 differ-
ent words. Since we did not use the coarse-grained
senses within our algorithm, the predicted senses
were correct if they mapped to the correct coarse-
grained sense. The average instance had 2.5 possi-
ble coarse-grained senses. The average number of
selectors acquired for each word is given in Table
1. The bottom of Table 2 shows the random base-
line as well as a baseline using the most frequent
sense (MFS) heuristic. As previously mentioned,
many supervised systems only perform marginally
better than the MFS. For the SemEval workshop,
only 6 of 15 systems performed better than this
baseline on the nouns (Navigli et al, 2007), all of
which used MFS as a back off strategy and an ex-
ternal sense tagged data set. Our results are pre-
sented as precision (P), recall (R), and F1 value
(F1 = 2 ? P?R
P+R
).
4.1 Results and Discussion
Table 2 shows the results when using various simi-
larity for the target selectors. We selected gloss-
based measures (Banerjee and Pedersen, 2003;
Patwardhan et al, 2003) due to the need for han-
dling multiple parts of speech for the context se-
lectors. Functionality for our use of many dif-
ferent relatedness measurements was provided by
WordNet::Similarity (Pedersen et al, 2004). Our
method performs better than the MFS baseline,
and clearly better than the random baseline. As
one can see, the scaled sim (path2) similarity
measure along with the gloss based relatedness
gloss1 gloss2
path1 78.8 78.3
path2 80.2 78.6
path3 78.7 78.6
IC1 78.6 79.3
IC2 78.5 79.2
IC3 78.0 78.1
gloss1 78.4 80.0
gloss2 78.6 78.9
MFS baseline 77.4
random baseline 59.1
Table 2: Performance of our method, given by F1
values (precision = recall), with various similarity
measures for target selectors: path1= sim (nor-
malized depth), path2 = scaled sim, path3 = (Wu
and Palmer, 1994), IC1 = (Resnik, 1999), IC2 =
(Lin, 1997), IC3 = (Jiang and Conrath, 1997), and
relatedness measures for context selectors: gloss1
= (Banerjee and Pedersen, 2003), gloss2 = (Pat-
wardhan et al, 2003). Baselines: MFS = most fre-
quent sense, random = random choice of sense.
measure of (Banerjee and Pedersen, 2003) gave
the best results. Note that the path-based and in-
formation content measures, in general, performed
equally.
We experimented with using the gloss-based re-
latedness measures in place of similarity measures.
The idea was that one measure could be used for
both target selectors and context selectors. As one
can gather from the bottom of table 2, for the most
part, the measures performed equally. The experi-
mental runtime of the path-based and information
content measures was roughly one-fourth that of
the gloss-based measures.
Table 3 presents results from experiments where
we only attempted to annotate instances with over
a minimum number of target selectors (tMin) and
context selectors (cMin). We use steps of four for
target selectors and steps of ten for context selec-
tors, reflecting a ratio of roughly 2 target selectors
for every 5 context selectors. It was more common
for an instance to not have any target selectors than
to not have context selectors, so we present results
with only a tMin or cMin. The main goal of these
experiments was simply to determine if the algo-
rithm performed better on instances that we were
able to acquire more selectors. We were able to see
this was the case as the precision improved at the
expense of recall from avoiding the noun instances
110
tMin cMin A P R F1
0 0 1108 80.2 80.2 80.2
4 0 658 84.4 50.1 62.9
16 0 561 85.2 43.1 57.2
0 10 982 81.1 71.9 76.2
0 40 908 81.3 66.6 73.3
4 10 603 85.4 46.4 60.1
8 20 554 85.3 42.6 56.9
12 30 516 86.4 40.2 54.9
16 40 497 86.5 38.8 53.5
Table 3: Number attempted (A), Precision (P),
Recall (R) and F1 values of our method with re-
strictions on a minimum number of target selectors
(tMin) and context selectors (cMin).
sel noMFS 1SPD
80.2 79.6 79.8
Table 4: Results of a variety of experiments using
path2 and gloss1 from the previous table. noMFS
= no use of most frequent sense, 1SPD = use of 1
sense per discourse.
that did not have many selectors.
Table 4 shows the results when we modify the
method in a few ways. All these results use
the path2 (scaled sim) and gloss1 (Banerjee and
Pedersen, 2003) measures. The results of Ta-
ble 2 include first sense heuristic used as a back-
off strategy for close calls, when multiple senses
have a score within 0.05 of each other. There-
fore, we experiment without this heuristic pre-
sented as noMFS, and found our method still per-
forms strongly. We also implemented one sense
per discourse, reported as 1SPD. Our experimental
corpus had five documents, and for each document
we calculated the most commonly predicted sense
and used that for all occurrences of the word within
the document. Interestingly, this strategy does not
seem to improve the results in our method.
4.2 Comparison with other systems
Table 5 shows the results of our method (sel) com-
pared with a few systems participating in the Se-
mEval coarse-grained all-words task. These re-
sults include the median of all participating sys-
tems, the top system not using training data (UPV-
WSD) (Buscaldi and Rosso, 2007), and the top
system using training data (NUS-PT) (Chan et
al., 2007). The best performance reported on the
sel med UPV-WSD NUS-PT SSI
80.2 71.1 79.33 82.31 84.12
Table 5: Comparison of noun F1 values with
various participants in the SemEval2007 coarse-
grained all-words task.
nouns for the SemEval coarse-grained task, was
actually from a system by the authors of the task
(SSI) (Navigli and Velardi, 2005). All systems
performing better than the MFS used the heuris-
tic as a backoff strategy when unable to output a
sense (Navigli et al, 2007). Also, the systems per-
forming better than ours (including SSI) used more
sources of sense annotated data.
5 Conclusion
We have presented a method for acquiring knowl-
edge from the Web for noun sense disambiguation.
Rather than searching the web with pre-chosen rel-
atives, we search with a string representing the lo-
cal context of a target word. This produces a list
of selectors, words which may replace the target
word within its local context. The selectors are
then compared with the senses of the target word
via similarity and relatedness measures to choose
the correct sense. By searching with context in-
stead of simply relatives, we are able to insure
more relevant results from the web. Additionally,
this method has an advantage over methods which
use relatives and context in that it does not restrict
the results to include pre-chosen words.
We also show that different types of similarity
and relatedness measures are appropriate for dif-
ferent roles in our disambiguation algorithm. We
found a path-based measure to be best with tar-
get selectors while a slower gloss-based method
was appropriate for context selectors in order to
handle multiple POS. For many tasks, information
content based measures perform better than path-
based measures. However, we found a path-based
measure to be just as strong if not stronger in our
approach.
Results of our evaluation using the SemEval
coarse-grained all-words task showed strength in
the use of selectors from the Web for disambigua-
tion. Our system was out-performed only by sys-
tems using training data or substantially more an-
notated data. Future work may improve results
through the use of sense tagged corpora, a gram-
matical parse, or other methods commonly used in
111
WSD. Additionally, better precision was achieved
when requiring a minimum number of selectors,
giving promise to improved results with more
work in acquiring selectors. This paper has shown
an effective and novel method of noun sense dis-
ambiguation through the use of selectors acquired
from the web.
6 Acknowledgement
This research was supported in part by the
NASA Engineering and Safety Center under
Grant/Cooperative Agreement NNX08AJ98A.
References
Agirre, Eneko and David Martinez. 2004. Unsuper-
vised wsd based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25?32, Barcelona, Spain, July.
Association for Computational Linguistics.
Banerjee, S. and T. Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In
Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805?
810, Acapulco.
Brody, Samuel, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics, pages 97?104, Sydney,
Australia.
Budanitsky, Alexander and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Buscaldi, Davide and Paolo Rosso. 2007. Upv-wsd
: Combining different wsd methods by means of
fuzzy borda voting. In Proceedings of SemEval-
2007, pages 434?437, Prague, Czech Republic, June.
Chan, Yee Seng, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of Proceedings of SemEval-2007, pages
253?256, Prague, Czech Republic, June.
Jiang, Jay J. and David W. Conrath. 1997. Semantic
similarity on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING X, Taiwan.
Leacock, Claudia, Martin Chodorow, and George A.
Miller. 1998. Using corpus statistics and wordnet re-
lations for sense identification. Computational Lin-
guistics, 24(1):147?165.
Lin, Dekang. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th annual meeting on Association
for Computational Linguistics, pages 64?71.
Martinez, David, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense
disambiguation. In Proceedings of the 2006 Aus-
tralasian Language Technology Workshop, pages
42?50.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
279?286, Barcelona, Spain, July.
Mihalcea, Rada. 2002. Bootstrapping large sense
tagged corpora. In Proceedings of the 3rd Inter-
national Conference on Languages Resources and
Evaluations LREC 2002, Las Palmas, Spain, May.
Miller, George, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
Navigli, Roberto and Paola Velardi. 2005. Structural
semantic interconnections: A knowledge-based ap-
proach to word sense disambiguation. IEEE Trans.
Pattern Anal. Mach. Intell., 27(7):1075?1086.
Navigli, Roberto, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval-2007, pages 30?35, Prague, Czech Repub-
lic, June.
Patwardhan, S., S. Banerjee, and T. Pedersen. 2003.
Using Measures of Semantic Relatedness for Word
Sense Disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, pages 241?
257, Mexico City, Mexico, February.
Pedersen, T., S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics Demonstrations,
pages 38?41, Boston, MA, May.
Rada, R., H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. In IEEE Transactions on Systems, Man and
Cybernetics, volume 19, pages 17?30.
Resnik, Philip. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its applica-
tion to problems of ambiguity in natural language.
Journal of Artificial Intelligence Research, 11:95?
130.
Wu, Zhibiao and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd. Annual Meeting of the Association for Com-
putational Linguistics, pages 133 ?138, New Mexico
State University, Las Cruces, New Mexico.
Yuret, Deniz. 2007. Ku: Word sense disambiguation
by substitution. In Proceedings of SemEval-2007,
pages 207?214, Prague, Czech Republic, June.
112
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Acquiring Applicable Common Sense Knowledge from the Web
Hansen A. Schwartz and Fernando Gomez
School of Electrical Engineering and Computer Science
University of Central Florida
Orlando, FL 32816, USA
{hschwartz, gomez}@cs.ucf.edu
Abstract
In this paper, a framework for acquiring com-
mon sense knowledge from the Web is pre-
sented. Common sense knowledge includes
information about the world that humans use
in their everyday lives. To acquire this knowl-
edge, relationships between nouns are re-
trieved by using search phrases with automat-
ically filled constituents. Through empirical
analysis of the acquired nouns over Word-
Net, probabilities are produced for relation-
ships between a concept and a word rather
than between two words. A specific goal of
our acquisition method is to acquire knowl-
edge that can be successfully applied to NLP
problems. We test the validity of the acquired
knowledge by means of an application to the
problem of word sense disambiguation. Re-
sults show that the knowledge can be used to
improve the accuracy of a state of the art un-
supervised disambiguation system.
1 Introduction
Common sense knowledge (CSK) is the knowledge
we use in everyday life without necessarily being
aware of it. Panton et al (2006) of the Cyc project,
define common sense as ?the knowledge that every
person assumes his neighbors also possess?. Al-
though the term common sense may be understood
as a process such as reasoning, we are referring only
to knowledge. It is CSK that tells us keys are kept in
one?s pocket and keys are used to open a door, but
CSK does not hold that keys are kept in a kitchen
sink or that keys are used to turn on a microwave,
although all are possible.
To show the need for this information more
clearly we provide a couple sentences:
She put the batter in the refrigerator. (1)
He ate the apple in the refrigerator. (2)
In (1), we are dealing with lexical ambiguity. There
is little doubt for us to determine just what the ?bat-
ter? is (food/substance used in baking). However, a
computer must determine that it is not someone who
swings a bat in baseball that is being put into a re-
frigerator, although it is entirely possible to do (de-
pending on the size of the refrigerator). This demon-
strates how CSK can be useful in solving word sense
disambiguation. We know it is common for food to
be found in a refrigerator and so we easily resolve
batter as a food/substance rather than a person.
CSK can also help to solve syntactic ambiguity.
The problem of prepositional phrase attachment oc-
curs in sentences similar to (2). In this case, it is
difficult for a computer to determine if ?he? is in the
refrigerator eating an apple or if the ?apple? which
he ate was in the refrigerator. Like the previous ex-
ample, the knowledge that food is commonly found
in a refrigerator and people are not, leads us to un-
derstand that ?in the refrigerator? should be attached
to the noun phrase ?the apple? and not as a modifier
of the verb phrase ?ate?.
Unfortunately, there are not many sources of CSK
readily available for use in computer algorithms.
Those sets of knowledge that are available, such
as the CYC project (Lenat, 1995) or ConceptNet
(Liu and Singh, 2004) rely on manually provided
or crafted data. Our aim is to develop an auto-
matic approach to acquire CSK1 by turning to the
vast amount of unannotated text that is available on
the Web. In turn, we present a method to automat-
ically retrieve and analyze phrases from the Web.
1data available at: http://eecs.ucf.edu/?hschwartz/CSK/
1
We employ the use of a syntactic parser to accu-
rately match syntactic patterns of phrases acquired
from the Web. The data is analyzed over WordNet
(Miller et al, 1993) in order to induce knowledge
about word senses or concepts rather than words. Fi-
nally, we evaluate whether the knowledge by apply-
ing it to the problem of word sense disambiguation.
2 Background
The particular type of CSK that we experiment with
in this paper is described formally as follows:
A relationship, e1Re2, exists between entities
e1 and e2 if one finds ?e1 is R e2.?
Some examples include: ?a cup is on a table? and
?food is in a refrigerator?, which would result in re-
lationships: cupontable and foodinrefrigerator. The
next section attempts to make the relationship more
clear, as we provide a brief linguistic background of
prepositions and relationships.
2.1 Prepositions and Relationships
Prepositions state a relationship between two enti-
ties (Quirk et al, 1985). One of the entities is typ-
ically a constituent of the sentence while the other
is the complement to the preposition. For exam-
ple, consider the relationship between ?furniture?
and ?house? in the following sentences:
The furniture is...
...at the house.
...on the house.
...in the house.
?The furniture? is the subject of the sentence, while
?the house? is a prepositional complement. Notice
that the meaning is different for each sentence de-
pending on the actual preposition (?at?, ?on?, or ?in?),
and thus furniture relates to house in three different
ways. Although each relationship between furniture
and house is possible, only one would be considered
CSK to most people: furnitureinhouse.
We focus on prepositions which indicate a posi-
tive spacial relationship given by Quirk et al (1985).
There are three types of such relationships: ?at a
point?, ?on a line or surface?, and ?in an area or vol-
ume?. In particular, we concentrate on the 1 to 3
dimensional relationships given in Table 1, denoted
on and in throughout the paper. At, the 0 dimen-
sional relationship, occurred far less frequently. The
dims description prepositions
1 or 2 on surface or line on, onto, atop, upon,
on top of, down on
2 or 3 in area or volume in, into, inside,
within, inside of
Table 1: Spatial dimensions (dims) and corresponding
prepositions.
sentences below exemplify each of the 1 to 3 dimen-
sional relationships:
on surface The keyboard is on the table.
on line The beach is on US 1.
in area The bank is in New York.
in volume The vegetables are in the bowl.
2.2 Related Work
As a prevalent source of lexical knowledge, dictio-
nary definitions may be regarded as common sense.
However, some definitions may be considered expert
knowledge rather than CSK. The scope of definitions
certainly do not provide all necessary information
(such as keys are commonly kept in one?s pocket).
We examine WordNet in particular because the hy-
pernym relation has been developed extensively for
nouns. The noun ontology is used in our work to
help induce relationships involving concepts (senses
of nouns) rather than just among words. This notion
of inducing CSK among concepts, rather than words,
is a key difference between our work and similar re-
search.
The work on VerbOcean is similar to our research
in the use of the Web for acquiring relationships
(Chklovski and Pantel, 2004). They used patterns
of phrases in order to search the Web for semantic
relations among verbs. The knowledge they acquire
falls into the category of CSK, but the specific re-
lationships are different than ours in that they are
among verb word forms and senses are not resolved.
ConceptNet was created based on the OpenMind
Commonsense project (Liu and Singh, 2004). The
project acquired knowledge through an interface on
the Web by having users play games and answer
questions about words. A contribution of Concept-
Net is that it has a wide range of relations. While
WordNet provides connections between concepts
(senses of words), ConceptNet only provides rela-
tionships between word forms.
2
Concept AnalysisNoun Acquisition
web search
parse and match
Word
probabilities:
nounA
[in|on]
nounB
WordNet
Ontology
determine
concept
probabilities
concept
probabilities:
conceptA
[in|on] 
nounB
a chosen
nounB
create 
web queries
search
phrases
for CSK
Figure 1: The overall common sense knowledge acquisition framework under the assumption that one is acquiring
concepts (WordNet synsets) in a relationship with a given nounB (word).
A project in progress for over twenty years, CYC
has been acquiring common sense knowledge about
everyday objects and actions stored in 106 axioms
(Lenat, 1995). The axioms, handcrafted by workers
at CYCcorp, represent knowledge rooted in propo-
sitions. There are three layers of information: the
first two, access and physical, contain meta data,
while the third, logical layer, stores high level im-
plicit meanings. Only a portion of CYC is available
to the public.
Our method for acquiring knowledge is somewhat
similar to that of (Hearst, 1992). Patterns are built
manually. However, we do not use our manually
constructed patterns (referred to as search phrases)
to query the Web. Instead the search phrases are ab-
stract patterns that are used to automatically gener-
ate more specific web queries by filling constituents
based on lists of words.
The SemEval-2007 Task 4 presents a good
overview of work in noun-noun relationships (Girju
et al, 2007). Our work is related in that the rela-
tionships we acquire are between nominals, and in
order to build their corpus Girju et al queried the
web with patterns like that of Hearst?s work (Hearst,
1992). The SemEval task was to choose or clas-
sify relationships, rather than acquire and apply rela-
tionships. Additionally, the relationship classes they
use are not necessarily within the scope of common
sense knowledge.
Similar to our research, in (Agirre et al, 2001)
knowledge is acquired about WordNet concepts.
They find topics signatures, sets of related words,
based on data from the Web and use them for word
sense disambiguation. However, the type of rela-
tionship between words of a topic signature and the
WordNet concept is not made explicit, and the au-
thors find the topic signatures are not very effective
for word sense disambiguation.
Finally, we note one approach to using the Web
for NLP applications is to acquire knowledge on the
fly. Previous work has approached solutions to word
sense disambiguation by acquiring words or phrases
directly based on the sentences or words being dis-
ambiguated (Martinez et al, 2006; Schwartz and
Gomez, 2008). These methods dynamically acquire
the data at runtime, rather than automatically create
a common sense database of relations that is readily
available. Additionally, in our current approach, we
are able to acquire explicit CSK relationships.
3 Common Sense Acquisition
The two major phases of our framework, ?Noun Ac-
quisition? and ?Concept Analysis?, are outlined in
Figure 1 and described within this section.
3.1 Noun Acquisition
The first step of our method is to acquire nouns
(as words) from the Web which are in a relation-
ship with other nouns. A Web search is performed
in order to retrieve samples of text matching a web
query created from a search phrase for the relation-
ship. Each sample is syntactically parsed to verify
a match with the corresponding web query, and the
noun(s) filling a missing constituent of the parse are
recorded.
The framework itself is very flexible, and it can
handle the acquisition of words from other parts of
speech. However, to be clear, we focus the explana-
tion on the use of the framework to acquire specific
types of relationships between nouns. Below we de-
scribe the procedures in more detail.
3
3.1.1 Creating Web Queries
Web queries are created semi-automatically by
defining these parameters of a search phrase:
nounA the first noun phrase
nounB the second noun phrase
prep preposition, if any, used in the phrase
verb verb, if any, used in the phrase.
Table 2 lists all of the search phrases we use, one of
which we use as an example throughout this section:
place nounA prep nounB
The verb, ?place? in this case, is statically defined as
part of the search phrase.
Prepositions were chosen to describe the type of
relationship we were seeking to acquire as described
in the background section. We limited ourselves to
the ?on? and ?in? relationships since these were the
most common.
on = (on, onto, atop, upon, on top of, down on)
in = (in, into, inside, within, inside of )
When noun parameters are provided, determiners
or possessive pronouns selected from the list below
are included. This provides greater accuracy in our
search results.
det = (the, a/an, this, that, my, your, his, her)
Finally, the undefined parameters are replaced
with a ?*?. Below is a web query created from our
search phrase where nounB is ?refrigerator?, prep is
?in?, det is ?the?, and nounA is undefined:
place * in the refrigerator
3.1.2 Searching the Web
Given a nounB, The search algorithm can be sum-
marized through the pseudocode below.
for each search phrase
for each prep
for each det
query = create query(search phrase,
prep, det, nounB));
samples = websearch(query);
The searches were carried out through the Google
Search API2, or the Yahoo! Search Web Services3.
Each search phrase, listed in Table 2, was run until
a maximum of 2000 results were returned. Dupli-
cate samples were removed to reduce the effects of
websites replicating the text of one another.
2no longer supported by Google
3http://developer.yahoo.com/search/
relation search phrase voice
nounA is located prep nounB
on, in nounA is found prep nounB passive
nounA is situated prep nounB
nounA is prep nounB
put nounA prep nounB
place nounA prep nounB
on, in lay nounA prep nounB active
set nounA prep nounB
locate nounA prep nounB
position nounA prep nounB
hang nounA prep nounB
on mount nounA prep nounB active
attach nounA prep nounB
Table 2: Search phrases and relationships used for acqui-
sition of CSK.
3.1.3 Parse and Match
The results we want to achieve in this step should
describe a relationship:
nounA is [in | on] nounB
We use Charniak?s parser (Charniak, 2000) on both
the web query and the results returned from the web
in order to ensure accuracy. To demonstrate this pro-
cess, we extend our example, ?place * in the refrig-
erator?.
First, we get a parse with * (nounA) represented
as ?something?.
(VP (VB place)
(NP (NN something))
(PP (IN in) (NP (DT the) (NN refrigerator))))
We now know the constituent(s) which replace ?(NN
something)? will be our nounA. For example, in the
following parse ?batter? is resolved as nounA.
(S1 (S (NP (PRP He))
(VP (AUX was) (VP (VBN told) (S (VP (TO to)
(VP (VB place)
(NP (DT the) (JJ mixed) (NN batter))
(PP (IN in) (NP (DT the) (NN refrigerator))))]
The head noun of the matching phrase is determined,
which is ?batter? in the phrase ?(DT the) (JJ mixed)
(NN batter)?. Words are only recorded if they are
present as a noun in WordNet. If the noun phrase
contains a compound noun found in WordNet, then
the compound noun is recorded instead.
The parse also helps to eliminate bad results. For
the following sentence, the verb phrase does not
4
match the parse of the web query due to an extra PP,
and therefore we do not pull out ?for several hours?
as nounA.
(S1 (S (VP (VP (VB Mix)
(NP (DT the) (NN sugar))
(PRT (RP in))
(PP (TO to) (NP (DT the) (NN dough))))
(CC and)
(VP (VB place)
(PP (IN for) (NP (JJ several) (NNS hours)))
(PP (IN in) (NP (DT the) (NN refrigerator)))))))
One may note that this malformed sentence is com-
municating that ?dough? is placed in the refrigerator,
but the method does not handle this.
At the end of the noun acquisition phase, we are
left with frequency counts of nouns being retrieved
from a context matching the syntactic structure of
a web query. This can easily be represented as the
probability of a noun, nA, being returned to a query
for the relationship, R, with noun nB.
pw(nA,R, nB)
This value along with the other steps we have gone
over are stored in a MySQL relational database4.
One could trace a relationship probability between
nouns back to the web results which were matched
to a web query, and even determine the abstract
search phrase which produced the web query.
3.2 Concept Analysis
A focus of this work is on going beyond relation-
ships between words. We would like to acquire
knowledge about specific concepts in WordNet. In
particular, we are trying to induce:
conceptA is [in | on] nounB.
where conceptA is a concept in WordNet (such as a
sense of nounA), and nounB remains simply a word.
For the analysis, we rely on the vast amount of
nouns we are able to acquire in order to create proba-
bilities for relationships of conceptARnounB. To get
a grasp of the idea in general, consider ?table? as a
nounB of interest. By examining all possible hyper-
nyms of all senses of each nounA one will find it
is common for abstract entities to be ?in a table?
(i.e. data in a table), artifacts to be ?on a table? (i.e.
4http://www.mysql.com
cup on a table), and physical things (including living
things) to be ?at a table? (i.e. the employees at the
table). The same idea could be applied in reverse if
one acquires knowledge for a set of nounAs. How-
ever, this paper only focuses on acquiring knowl-
edge for the nounB constituent in a search phrase.
To begin with, one should note that concepts in
WordNet are represented as synsets. A synset is
a group of word-senses that have the same mean-
ing. For example, (batter-1, hitter-1, slugger-1,
batsman-1) is a synset with the meaning ?(baseball)
a ballplayer who is batting?. We use WordNet ver-
sion 3.0 in order to take advantage of the latest up-
dates and corrections to the noun ontology. Since a
word has multiple senses, we represent the probabil-
ity that a word-sense, nAs, resulted from a query for
a relationship, R with nounB as:
pns(nAs,R, nB) = pw(lemma(nAs),R, nB)senses(lemma(nAs))
where senses returns the number of senses of the
word (lemma) within the word-sense nAs. We
can then extend the probability to apply to a synset,
syns, as:
psyn(syns,R, nB) =
?
nAs?syns
pns(nAs,R, nB)
Finally, we define a recursive function based on
the idea that a concept subsumes all concepts below
it (hyponyms) in the WordNet ontology:
Pc(cA,R, nB) = psyn(syns(cA),R, nB)
+ ?
h?hypos(cA)
Pc(h,R, nB)
where cA is a concept/node in WordNet, syns re-
turns the synset which represents the concept, and
hypos returns the set of all direct hyponyms within
the WordNet ontology. For example, (money-3) is
a (currency-1), so Pc(currency-1,R, nB) receives
psyn((money-3),R, nB) among others. This type
of calculation over WordNet follows much like
that of Resnik?s (1999) information-content calcu-
lation. Note that the function no longer recurs
when reaching a concept with no hyponyms and that
Pc(entity-1,R, nB) is always 1 (entity-1 is the root
node). Pc now represents a probability for the rela-
tionship: conceptARnounB.
5
nounB #nounAs nounB #nounAs
basket 3300 boat 2787
bookcase 260 bottle 4742
bowl 5252 cabin 720
cabinet 1474 canoe 163
car 5534 ceiling 1187
city 1432 desk 4770
drawer 1638 dresser 698
floor 2850 house 4627
jar 4462 kitchen 2948
pocket 4771 refrigerator 2897
road 5493 room 5023
shelf 2581 ship 1469
sink 296 sofa 509
table 5312 truck 528
van 301 wall 2285
Table 3: List of nouns which fill the nounB constituent
in a search phrase, and the corresponding occurrences of
nounAs acquired for each.
4 Evaluation
Our evaluation focuses on the applicability of the
acquired CSK. We acquired relationships for the 30
nouns listed in Table 3. These nouns represent all
possible words to fill the nounB constituent of a
search phrase. The corresponding #nounAs indi-
cates the number of nounAs that were acquired from
the Web for each nounB. For example, 4771 nounAs
were acquired for ?pocket?. This means 4771 results
from the web matched the parse of a web query for
?pocket? and contained a nounA in WordNet (keep-
ing in mind duplicates Web text were removed).
Delving deeper into our example, below are
the top 20 nounAs found for the relationship
nounAinpocket.
money, hand, cash, firework, something, dol-
lar, ball, hands, key, coin, pedometer, card,
battery, item, phone, penny, music, buck, im-
plant, wallet
As described in the concept analysis section, occur-
rences of each nounA for a given nounB lead to pw
values, which in turn are used to produce Pc values
for concepts in WordNet. The application of CSK
utilizes these probabilities rather than simply lists of
words or even lists of concepts. However, challenges
were encountered during the noun acquisition step
before the probabilities were produced.
Many challenges of the noun acquisition step
were overcome through the use of a parser. For ex-
ample, phrases such as ?Palestine is on the road to
becoming...? could be eliminated since the parser
marks the prepositional phrase ?to becoming? as be-
ing attached to ?the road?. Thus, the parse of the
web sample does not match the parse of the web
query used to acquire it. Other times, noun-noun re-
lationships were common simply because many web
pages seem to copy the text of others. This prob-
lem was handled through the elimination of dupli-
cate text samples from the Web. In the end, only
about one in four results from the Web were actually
used. Numbers in Table 3 reflect the result of these
eliminations.
Some issues of the acquisition step were not di-
rectly addressed in this paper. A domain may tend
to be more prevalent on the Internet and skew the
CSK, such as fireworkinpocket. Another example,
babyinbasket was very common due to biblical ref-
erences. Fictional works and metaphors also pro-
vided uncommon relationships dispersed within the
results. Additionally, the parser makes mistakes. It
was the hope that the concept analysis step would
help to mitigate some noise from these problems.
A final issue was the bottleneck of limited queries
per day by the search engines, which restricted us to
testing on only the 30 nouns listed.
4.1 Disambiguation System
The CSK is not intended to be used by itself for dis-
ambiguation. It would be far from accurate to as-
sume the sense of a noun can be disambiguated sim-
ply by observing its relationship with one other noun
in the sentence. For example, one of the test sen-
tences incorporated the relationship noteinpocket.
Multiple senses of note are likely to be found in a
pocket (i.e. the senses referring to ?a brief written
record?, ?a short personal letter?, or ?a piece of pa-
per money?). In other cases, a relationship may not
be found for any sense of a target word. Therefore,
our knowledge is intended to be used as a reference,
consulted by a disambiguation system.
We integrate our knowledge into a state of the art
?all-words? word sense disambiguation algorithm.
These algorithms are considered unsupervised or
6
minimally supervised, because they do not require
specific training data that is designed for instances
of words in the testing data. In other words, these
systems are designed to handle any word they come
across. Our knowledge can supplement such a sys-
tem, because the data can be acquired automatically
for an unlimited number of nouns, assuming limit-
less web query restrictions.
The basis of our disambiguation system is the
publicly available GWSD system (Sinha and Mihal-
cea, 2007). Sinha and Mihalcea report higher re-
sults on the Senseval-2 and Senseval-3 datasets than
any of the participating unsupervised system. Ad-
ditionally, GWSD is compatible with WordNet 3.0
and its output made it easy to integrate our knowl-
edge. Sense predictions from four different graph
metrics are produced, and we are able to incorporate
our knowledge as another prediction within a voting
scheme.
Considering the role of our knowledge as a refer-
ence, in some cases we would like the CSK to sug-
gest multiple senses and in others none. For each
target noun instance in the corpus, we lookup the
Pc(c,R, nB) value, where c is the WordNet concept
that corresponds to a sense of the target noun. We
choose nB by matching the phrase ?in|on det nB?
within the sentence. The system suggests all senses
with a Pc value greater than 0.75 of the maximum Pc
value over all senses. If no senses have a Pc value
then no senses are suggested.
During voting, tallies of predictions and sugges-
tions are taken for each sense of a noun. Ties are
broken by choosing the lowest sense number among
all those involved in the tie. Note that this is differ-
ent than choosing the most frequent sense (i.e. the
lowest sense number from all senses), in that only
the top predicted senses are considered. This same
type of voting is used with and without the CSK sug-
gestions.
4.2 Experimental Corpus
A goal of our work was to acquire data which could
be applied to NLP problems. We focus particularly
on the difficult problem of word sense disambigua-
tion. Due to the lack of sense tagged data, we were
unable to find an annotated corpus with instances
of all the nouns in Table 3 as prepositional com-
plements. This was not surprising considering one
of the reasons that minimally supervised approaches
have become more popular is that they do not require
hand-tagged training data (Mihalcea, 2002; Diab,
2004; McCarthy et al, 2004).
We created a corpus from sentences in Wikipedia
which contained the phrase ?in|on det lemma?,
where det is a determiner or possessive pronoun,
lemma is a noun from Table 3, and in|on is a prepo-
sition for either relationship described earlier. Be-
low we have provided an example from our corpus
where the knowledge from ?pocket? can be applied
to disambiguate ?key?.
Now Tony?s key to the flat is in the pocket of his
raincoat, so on returning to his flat some time
later he realizes that he cannot get inside.
The corpus5 contained a total of 342 sen-
tences, with one target noun annotated per sen-
tence. The target nouns were selected to poten-
tially fill the nounA constituent in the relationship
nounARnounB, and they were assigned all appro-
priate WordNet 3.0 senses. Considering the fine-
grained nature of WordNet (Ide and Wilks, 2006),
26.3% of the instances were annotated with multi-
ple senses. We also restricted the corpus to only
include polysemous nouns, or nouns which had an
additional sense beyond the senses assigned to it.
Inter-annotator agreement was used to validate
the corpus. Because the corpus was built by an
author of the work, we asked a non-author to re-
annotate the corpus without knowledge of the orig-
inal annotations. This second annotator was told to
choose all appropriate senses just as did the original
annotator. Agreement was calculated as:
agree =
(?
i?C
|S1i ? S2i|
|S1i ? S2i|
)
? 342
where S1 and S2 are the two sets of sense annota-
tions, and i is an instance of the corpus, C.
The agreement and other data concerning corpus
annotation can be found in Table 4. As a point of
comparison, the Senseval 3 all-words task had a 75%
agreement on nouns (Snyder and Palmer, 2004). A
second evaluation of agreement was also done. The
non-author annotations were treated as if they came
5available at: http://eecs.ucf.edu/?hschwartz/CSK/
7
insts agree F1h F1rnd F1MFS
on 131 79.9 84.7 28.2 71.0
in 211 80.8 91.9 27.2 67.8
both 342 80.5 89.2 27.6 69.0
Table 4: Experimental corpus data for each relation-
ship (on, in). insts: number of annotated instances;
agree: inter-annotator agreement %; F1 values (precision
= recall): h: human annotation, rnd: random baseline,
MFS: most frequent sense baseline.
without CSK with CSK
F1all F1indeg F1all F1indeg
on 62.6 63.4 64.9 67.2
in 68.7 69.7 71.6 72.5
both 66.4 67.3 69.0 70.5
ties 37 0 66 72
Table 5: F1 values (precision = recall) on our experimen-
tal corpus with and without CSK. F1all: using all 4 graph
metrics; F1indeg: using only the indegree metric; ties:
number of instances where tie votes occurred.
from a disambiguation system in order to get a hu-
man upper-bound of performance. Just as the auto-
matic system handled tie votes, when one word had
multiple sense annotations, the annotation with the
lowest sense number was used. This performance
upper-bound is shown as F1h in Table 4.
4.3 Results
Our disambiguation results are presented in Table
5. We found that, in all cases, including CSK im-
proved results. It turned out that 54.7% of the noun
instances received at least one suggestion from the
CSK, and 24.5% of the instances received multiple
suggestions. It is not clear why the on results were
slightly below that for in. We suspect the on por-
tion of the corpus was slightly more difficult be-
cause the human annotation (F1h) found a similar
phenomenon.
One observation we made when setting up the
test was that the indegree metric alone performed
slightly better than using the votes of all four met-
rics. This was not surprising considering Sinha and
Mihalcea found the indegree metric by itself to per-
form only slightly below a combination of metrics
on the senseval data (Sinha and Mihalcea, 2007).
Therefore, Table 5 also reports the use of the inde-
gree metric by itself or with CSK, F1indeg. In these
cases we saw the greatest improvements of using
CSK, producing an an error reduction of about 4.5%
and outperforming the F1MFS value.
Several additional experiments were performed.
Note that even during ties, the chosen sense was
taken from the predictions and suggestions. When
we instead incorporated an MFS backoff strategy for
ties, our top results for F1indeg with CSK dropped to
70.2. We also ran a precision test with no predictions
made for tie votes, and found a precision of 71.9%
on the 270 instances that did not have a tie for top
votes (this also used the indegree metric with CSK).
All results supported our goal of acquiring CSK that
was applicable to word sense disambiguation.
5 Conclusion
We found our acquired CSK to be useful when incor-
porated into a word sense disambiguation system,
finding an error reduction of around 4.5% for top re-
sults. Relationships between nouns were acquired
from the Web through a unique search method of
filling constituents in a search phrase. Samples re-
turned from the Web were restricted by a require-
ment to match the syntactic parse of a web query.
The resulting data was analyzed over WordNet to
produce probabilities of relationships in the form of
conceptARnounB, where conceptA is a concept in
WordNet rather than an ambiguous noun.
In our effort to validate the knowledge through ap-
plication, many steps along the way were left open
for future investigations. First, there is a need to ex-
haustively search for CSK of all nouns and to acquire
other forms of CSK. With this improvement CSK
could be tested on a standard corpus, rather than
a corpus focused on select nouns and prepositional
phrases. Looking into acquisition improvements, a
study of the effectiveness of the parse would be ben-
eficial. Finally, the applicability of the knowledge
may be increased through a more complex concept
analysis or utilizing a more advanced voting scheme.
6 Acknowledgement
This research was supported by the NASA Engi-
neering and Safety Center under Grant/Cooperative
Agreement NNX08AJ98A.
8
References
Eneko Agirre, Olatz Ansa, and David Martinez. 2001.
Enriching wordnet concepts with topic signatures. In
In Proceedings of the NAACL workshop on WordNet
and Other Lexical Resources: Applications, Exten-
sions and Customizations, Pittsburg, USA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Conference on Empirical
Methods in Natural Language Processing (EMNLP-
04), Barcelona, Spain.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL?04), pages 303?310.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of SemEval-
2007, pages 13?18, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING-92), pages 539?545.
Nancy Ide and Yorick Wilks, 2006. Word Sense Dis-
ambiguation: Algorithms And Applications, chapter 3:
Making Sense About Sense. Springer.
Douglas B. Lenat. 1995. CYC: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
H. Liu and P Singh. 2004. Conceptnet: A practical com-
monsense reasoning toolkit. BT Technology Journal,
22:211?226.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42?50.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 279?286, Barcelona, Spain, July. Association
for Computational Linguistics.
Rada Mihalcea. 2002. Bootstrapping large sense tagged
corpora. In Proceedings of the 3rd International
Conference on Languages Resources and Evaluations
LREC 2002, Las Palmas, Spain, May.
George Miller, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
Kathy Panton, Cynthia Matuszek, Douglas Lenat, Dave
Schneider, Michael Witbrock, Nick Siegel, and Blake
Shepard. 2006. Common sense reasoning : From cyc
to intelligent assistant. In Y. Cai and J. Abascal, edi-
tors, Ambient Intelligence in Everyday Life, pages 1?
31.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammaer
of the English Language. Longman.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Hansen A. Schwartz and Fernando Gomez. 2008. Ac-
quiring knowledge from the web to be used as selec-
tors for noun sense disambiguation. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 105?112,
Manchester, England, August.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. Irvine, CA, Septem-
ber.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In ACL Senseval-3 Workshop,
Barcelona, Spain, July.
9
