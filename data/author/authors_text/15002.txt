Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 592?597,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying the Semantic Orientation of Foreign Words
Ahmed Hassan
EECS Department
University of Michigan
Ann Arbor, MI
hassanam@umich.edu
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Rahul Jha
EECS Department
University of Michigan
Ann Arbor, MI
rahuljha@umich.edu
Dragomir Radev
EECS Department and School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present a method for identifying the pos-
itive or negative semantic orientation of for-
eign words. Identifying the semantic orienta-
tion of words has numerous applications in the
areas of text classification, analysis of prod-
uct review, analysis of responses to surveys,
and mining online discussions. Identifying
the semantic orientation of English words has
been extensively studied in literature. Most of
this work assumes the existence of resources
(e.g. Wordnet, seeds, etc) that do not exist
in foreign languages. In this work, we de-
scribe a method based on constructing a mul-
tilingual network connecting English and for-
eign words. We use this network to iden-
tify the semantic orientation of foreign words
based on connection between words in the
same language as well as multilingual connec-
tions. The method is experimentally tested us-
ing a manually labeled set of positive and neg-
ative words and has shown very promising re-
sults.
1 Introduction
A great body of research work has focused on iden-
tifying the semantic orientation of words. Word po-
larity is a very important feature that has been used
in several applications. For example, the problem
of mining product reputation from Web reviews has
been extensively studied (Turney, 2002; Morinaga
et al, 2002; Nasukawa and Yi, 2003; Popescu and
Etzioni, 2005; Banea et al, 2008). This is a very
important task given the huge amount of product re-
views written on the Web and the difficulty of man-
ually handling them. Another interesting applica-
tion is mining attitude in discussions (Hassan et al,
2010), where the attitude of participants in a discus-
sion is inferred using the text they exchange.
Due to its importance, several researchers have
addressed the problem of identifying the semantic
orientation of individual words. This work has al-
most exclusively focused on English. Most of this
work used several language dependent resources.
For example Turney and Littman (2003) use the en-
tire English Web corpus by submitting queries con-
sisting of the given word and a set of seeds to a
search engine. In addition, several other methods
have used Wordnet (Miller, 1995) for connecting se-
mantically related words (Kamps et al, 2004; Taka-
mura et al, 2005; Hassan and Radev, 2010).
When we try to apply those methods to other lan-
guages, we run into the problem of the lack of re-
sources in other languages when compared to En-
glish. For example, the General Inquirer lexicon
(Stone et al, 1966) has thousands of English words
labeled with semantic orientation. Most of the lit-
erature has used it as a source of labeled seeds or
for evaluation. Such lexicons are not readily avail-
able in other languages. Another source that has
been widely used for this task is Wordnet (Miller,
1995). Even though other Wordnets have been built
for other languages, their coverage is very limited
when compared to the English Wordnet.
In this work, we present a method for predicting
the semantic orientation of foreign words. The pro-
592
Figure 1: Sparse Foreign Networks are connected to
Dense English Networks. Dashed nodes represent la-
beled positive and negative seeds.
posed method is based on creating a multilingual
network of words that represents both English and
foreign words. The network has English-English
connections, as well as foreign-foreign connections
and English-foreign connections. This allows us to
benefit from the richness of the resources built for
the English language and in the meantime utilize
resources specific to foreign languages. Figure 1
shows a multilingual network where a sparse foreign
network and a dense English network are connected.
We then define a random walk model over the multi-
lingual network and predict the semantic orientation
of any given word by comparing the mean hitting
time of a random walk starting from it to a positive
and a negative set of seed English words.
We use both Arabic and Hindi for experiments.
We compare the performance of several methods us-
ing the foreign language resources only and the mul-
tilingual network that has both English and foreign
words. We show that bootstrapping from languages
with dense resources such as English is useful for
improving the performance on other languages with
limited resources.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work.
We define our problem and explain our approach in
Section 3. Results and discussion are presented in
Section 4. We conclude in Section 5.
2 Related Work
The problem of identifying the polarity of individual
words is a well-studied problem that attracted sev-
eral research efforts in the past few years. In this
section, we survey several methods that addressed
this problem.
The work of Hatzivassiloglou and McKeown
(1997) is among the earliest efforts that addressed
this problem. They proposed a method for identify-
ing the polarity of adjectives. Their method is based
on extracting all conjunctions of adjectives from a
given corpus and then they classify each conjunc-
tive expression as either the same orientation such
as ?simple and well-received? or different orienta-
tion such as ?simplistic but well-received?. Words
are clustered into two sets and the cluster with the
higher average word frequency is classified as posi-
tive.
Turney and Littman (2003) identify word polar-
ity by looking at its statistical association with a set
of positive/negative seed words. They use two sta-
tistical measures for estimating association: Point-
wise Mutual Information (PMI) and Latent Seman-
tic Analysis (LSA). Co-occurrence statistics are col-
lected by submitting queries to a search engine. The
number of hits for positive seeds, negative seeds,
positives seeds near the given word, and negative
seeds near the given word are used to estimate the
association of the given word to the positive/negative
seeds.
Wordnet (Miller, 1995), thesaurus and co-
occurrence statistics have been widely used to mea-
sure word relatedness by several semantic orienta-
tion prediction methods. Kamps et al (2004) use the
length of the shortest-path in Wordnet connecting
any given word to positive/negative seeds to iden-
tify word polarity. Hu and Liu (2004) use Word-
net synonyms and antonyms to bootstrap from words
with known polarity to words with unknown polar-
ity. They assign any given word the label of its syn-
onyms or the opposite label of its antonyms if any of
them are known.
Kanayama and Nasukawa (2006) used syntactic
features and context coherency, defined as the ten-
dency for same polarities to appear successively,
to acquire polar atoms. Takamura et al (2005)
proposed using spin models for extracting seman-
tic orientation of words. They construct a network
of words using gloss definitions, thesaurus and co-
occurrence statistics. They regard each word as an
electron. Each electron has a spin and each spin has
a direction taking one of two values: up or down.
593
Two neighboring spins tend to have the same orien-
tation from an energetic point of view. Their hypoth-
esis is that as neighboring electrons tend to have the
same spin direction, neighboring words tend to have
similar polarity. Hassan and Radev (2010) use a ran-
dom walk model defined over a word relatedness
graph to classify words as either positive or negative.
Words are connected based on Wordnet relations as
well as co-occurrence statistics. They measure the
random walk mean hitting time of the given word to
the positive set and the negative set. They show that
their method outperforms other related methods and
that it is more immune to noisy word connections.
Identifying the semantic orientation of individ-
ual words is closely related to subjectivity analy-
sis. Subjectivity analysis focused on identifying
text that presents opinion as opposed to objective
text that presents factual information (Wiebe, 2000).
Some approaches to subjectivity analysis disregard
the context phrases and words appear in (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea
et al, 2008), while others take it into considera-
tion (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005).
3 Approach
The general goal of this work is to mine the seman-
tic orientation of foreign words. We do this by cre-
ating a multilingual network of words. In this net-
work two words are connected if we believe that they
are semantically related. The network has English-
English, English-Foreign and Foreign-Foreign con-
nections. Some of the English words will be used as
seeds for which we know the semantic orientation.
Given such a network, we will measure the mean
hitting time in a random walk starting at any given
word to the positive set of seeds and the negative set
of seeds. Positive words will be more likely to hit the
positive set faster than hitting the negative set and
vice versa. In the rest of this section, we define how
the multilingual word network is built and describe
an algorithm for predicting the semantic orientation
of any given word.
3.1 Multilingual Word Network
We build a network G(V,E) where V = Ven ? Vfr
is the union of a set of English and foreign words.
E is a set of edges connecting nodes in V . There
are three types of connections: English-English con-
nections, Foreign-Foreign connections and English-
Foreign connections.
For the English-English connections, we use
Wordnet (Miller, 1995). Wordnet is a large lexical
database of English. Words are grouped in synsets
to express distinct concepts. We add a link between
two words if they occur in the same Wordnet synset.
We also add a link between two words if they have a
hypernym or a similar-to relation.
Foreign-Foreign connections are created in a sim-
ilar way to the English connections. Some other lan-
guages have lexical resources based on the design of
the Princeton English Wordnet. For example: Euro
Wordnet (EWN) (Vossen, 1997), Arabic Wordnet
(AWN) (Elkateb, 2006; Black and Fellbaum, 2006;
Elkateb and Fellbaum, 2006) and the Hindi Word-
net (Narayan et al, 2002; S. Jha, 2001). We also use
co-occurrence statistics similar to the work of Hatzi-
vassiloglou and McKeown (1997).
Finally, to connect foreign words to English
words, we use a foreign to English dictionary. For
every word in a list of foreign words, we look up
its meaning in a dictionary and add an edge between
the foreign word and every other English word that
appeared as a possible meaning for it.
3.2 Semantic Orientation Prediction
We use the multilingual network we described above
to predict the semantic orientation of words based
on the mean hitting time to two sets of positive and
negative seeds. Given the graph G(V,E), we de-
scribed in the previous section, we define the transi-
tion probability from node i to node j by normaliz-
ing the weights of the edges out from i:
P (j|i) = Wij/
?
k
Wik (1)
The mean hitting time h(i|j) is the average num-
ber of steps a random walker, starting at i, will take
to enter state j for the first time (Norris, 1997). Let
the average number of steps that a random walker
starting at some node i will need to enter a state
594
k ? S be h(i|S). It can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h(j|S) + 1 otherwise
(2)
where pij is the transition probability between
node i and node j.
Given two lists of seed English words with known
polarity, we define two sets of nodes S+ and S?
representing those seeds. For any given word w, we
calculate the mean hitting time between w and the
two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as
negative, otherwise it is classified as positive. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). Sev-
eral other similarity measures may be used to predict
whether a given word is closer to the positive seeds
list or the negative seeds list (e.g. average shortest
path length (Kamps et al, 2004)). However hit-
ting time has been shown to be more efficient and
more accurate (Hassan and Radev, 2010) because it
measures connectivity rather than distance. For ex-
ample, the length of the shortest path between the
words ?good? and ?bad? is only 5 (Kamps et al,
2004).
4 Experiments
4.1 Data
We used Wordnet (Miller, 1995) as a source of syn-
onyms and hypernyms for linking English words in
the word relatedness graph. We used two foreign
languages for our experiments Arabic and Hindi.
Both languages have a Wordnet that was constructed
based on the design the Princeton English Wordnet.
Arabic Wordnet (AWN) (Elkateb, 2006; Black and
Fellbaum, 2006; Elkateb and Fellbaum, 2006) has
17561 unique words and 7822 synsets. The Hindi
Wordnet (Narayan et al, 2002; S. Jha, 2001) has
56,928 unique words and 26,208 synsets.
In addition, we used three lexicons with words la-
beled as either positive or negative. For English, we
used the General Inquirer lexicon (Stone et al, 1966)
as a source of seed labeled words. The lexicon con-
tains 4206 words, 1915 of which are positive and
2291 are negative. For Arabic and Hindi we con-
structed a labeled set of 300 words for each language
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT-FR HT-FR+EN
Figure 2: Accuracy of the proposed method and baselines
for both Arabic and Hindi.
for use in evaluation. Those sets were labeled by two
native speakers of each language. We also used an
Arabic-English and a Hindi-English dictionaries to
generate Foreign-English links.
4.2 Results and Discussion
We performed experiments on the data described in
the previous section. We compare our results to
two baselines. The first is the SO-PMI method de-
scribed in (Turney and Littman, 2003). This method
is based on finding the semantic association of any
given word to a set of positive and a set of negative
words. It can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(3)
where w is a word with unknown polarity,
hitsw,pos is the number of hits returned by a com-
mercial search engine when the search query is the
given word and the disjunction of all positive seed
words. hitspos is the number of hits when we
search for the disjunction of all positive seed words.
hitsw,neg and hitsneg are defined similarly. We used
7 positive and 7 negative seeds as described in (Tur-
ney and Littman, 2003).
The second baseline constructs a network of for-
eign words only as described earlier. It uses mean
hitting time to find the semantic association of any
given word. We used 10 fold cross validation for this
experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use
the hitting time as before to predict semantic orien-
595
tation. We used the English words from (Stone et
al., 1966) as seeds and the labeled foreign words
for evaluation. We will refer to this system as
HT-FR + EN.
Figure 2 compares the accuracy of the three meth-
ods for Arabic and Hindi. We notice that the
SO-PMI and the hitting time based methods per-
form poorly on both Arabic and Hindi. This is
clearly evident when we consider that the accuracy
of the two systems on English was 83% and 93% re-
spectively (Turney and Littman, 2003; Hassan and
Radev, 2010). This supports our hypothesis that
state of the art methods, designed for English, per-
form poorly on foreign languages due to the limited
amount of resources available in foreign languages
compared to English. The figure also shows that the
proposed method, which combines resources from
both English and foreign languages, performs sig-
nificantly better. Finally, we studied how much im-
provement is achieved by including links between
foreign words from global Wordnets. We found out
that it improves the performance by 2.5% and 4%
for Arabic and Hindi respectively.
5 Conclusions
We addressed the problem of predicting the seman-
tic orientation of foreign words. All previous work
on this task has almost exclusively focused on En-
glish. Applying off-the-shelf methods developed for
English to other languages does not work well be-
cause of the limited amount of resources available
in foreign languages compared to English. We pro-
posed a method based on the construction of a multi-
lingual network that uses both language specific re-
sources as well as the rich semantic relations avail-
able in English. We then use a model that computes
the mean hitting time to a set of positive and neg-
ative seed words to predict whether a given word
has a positive or a negative semantic orientation.
We showed that the proposed method can predict
semantic orientation with high accuracy. We also
showed that it outperforms state of the art methods
limited to using language specific resources.
Acknowledgments
This research was funded in part by the Office
of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the ofcial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Elkateb S. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Black, W. and C. Fellbaum. 2006. Introducing the
arabic wordnet project. In Third International Word-
Net Conference.
Black. W. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Elkateb, S. and C. Fellbaum. 2006. Building a word-
net for arabic. In Fifth International Conference on
Language Resources and Evaluation.
Black W. Vossen P. Farwell D. Rodrguez H. Pease A.
Alkhalifa M. Elkateb, S. 2006. Arabic wordnet and
the challenges of arabic. In Arabic NLP/MT Confer-
ence.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
596
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and P. Bhattacharyya. 2002. An experience in build-
ing the indo wordnet - a wordnet for hindi. In First
International Conference on Global WordNet.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
P. Pande P. Bhattacharyya S. Jha, D. Narayan. 2001. A
wordnet for hindi. In International Workshop on Lexi-
cal Resources in Natural Language Processing.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL?02, pages 417?424.
P. Vossen. 1997. Eurowordnet: a multilingual database
for information retrieval. In DELOS workshop on
Cross-language Information Retrieval.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
597
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 249?254,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Random Walk Factoid Annotation for Collective Discourse
Ben King Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI
benking@umich.edu
rahuljha@umich.edu
Dragomir R. Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Robert Mankoff ?
The New Yorker Magazine
New York, NY
bob mankoff
@newyorker.com
Abstract
In this paper, we study the problem of au-
tomatically annotating the factoids present
in collective discourse. Factoids are in-
formation units that are shared between
instances of collective discourse and may
have many different ways of being realized
in words. Our approach divides this prob-
lem into two steps, using a graph-based
approach for each step: (1) factoid dis-
covery, finding groups of words that corre-
spond to the same factoid, and (2) factoid
assignment, using these groups of words
to mark collective discourse units that con-
tain the respective factoids. We study this
on two novel data sets: the New Yorker
caption contest data set, and the crossword
clues data set.
1 Introduction
Collective discourse tends to contain relatively
few factoids, or information units about which the
author speaks, but many nuggets, different ways
to speak about or refer to a factoid (Qazvinian and
Radev, 2011). Many natural language applications
could be improved with good factoid annotation.
Our approach in this paper divides this problem
into two subtasks: discovery of factoids, and as-
signment of factoids. We take a graph-based ap-
proach to the problem, clustering a word graph to
discover factoids and using random walks to as-
sign factoids to discourse units.
We also introduce two new datasets in this pa-
per, covered in more detail in section 3. The
New Yorker cartoon caption dataset, provided
by Robert Mankoff, the cartoon editor at The
New Yorker magazine, is composed of reader-
submitted captions for a cartoon published in the
magazine. The crossword clue dataset consists
?Cartoon Editor, The New Yorker magazine
Figure 1: The cartoon used for the New Yorker
caption contest #331.
of word-clue pairs used in major American cross-
word puzzles, with most words having several
hundred different clues published for it.
The term ?factoid? is used as in (Van Halteren
and Teufel, 2003), but in a slightly more abstract
sense in this paper, denoting a set of related words
that should ideally refer to a real-world entity, but
may not for some of the less coherent factoids.
The factoids discovered using this method don?t
necessarily correspond to the factoids that might
be chosen by annotators.
For example, given two user-submitted cartoon
captions
? ?When they said, ?Take us to your leader,? I
don?t think they meant your mother?s house,?
? and ?You?d better call your mother and tell
her to set a few extra place settings,?
a human may say that they share the factoid called
?mother.? The automatic methods however, might
say that these captions share factoid3, which is
identified by the words ?mother,? ?in-laws,? ?fam-
ily,? ?house,? etc.
The layout of this paper is as follows: we review
related work in section 2, we introduce the datasets
249
in detail in section 3, we describe our methods in
section 4, and report results in section 5.
2 Related Work
The distribution of factoids present in text collec-
tions is important for several NLP tasks such as
summarization. The Pyramid Evaluation method
(Nenkova and Passonneau, 2004) for automatic
summary evaluation depends on finding and an-
notating factoids in input sentences. Qazvinian
and Radev (2011) also studied the properties of
factoids present in collective human datasets and
used it to create a summarization system. Hennig
et al (2010) describe an approach for automati-
cally learning factoids for pyramid evaluation us-
ing a topic modeling approach.
Our random-walk annotation technique is sim-
ilar to the one used in (Hassan and Radev, 2010)
to identify the semantic polarity of words. Das
and Petrov (2011) also introduced a graph-based
method for part-of-speech tagging in which edge
weights are based on feature vectors similarity,
which is like the corpus-based lexical similarity
graph that we construct.
3 Data Sets
We introduce two new data sets in this paper, the
New Yorker caption contest data set, and the cross-
word clues data set. Though these two data sets are
quite different, they share a few important char-
acteristics. First, the discourse units tend to be
short, approximately ten words for cartoon cap-
tions and approximately three words for crossword
clues. Second, though the authors act indepen-
dently, they tend to produce surprisingly similar
text, making the same sorts of jokes, or referring
to words in the same sorts of ways. Thirdly, the
authors often try to be non-obvious: obvious jokes
are often not funny, and obvious crossword clues
make a puzzle less challenging.
3.1 New Yorker Caption Contest Data Set
The New Yorker magazine holds a weekly con-
test1 in which they publish a cartoon without
a caption and solicit caption suggestions from
their readers. The three funniest captions are se-
lected by the editor and published in the follow-
ing weeks. Figure 1 shows an example of such
a cartoon, while Table 1 shows examples of cap-
tions, including its winning captions. As part of
1http://www.newyorker.com/humor/caption
I don?t care what planet they are from, they can pass on the
left like everyone else.
I don?t care what planet they?re from, they should have the
common courtesy to dim their lights.
I don?t care where he?s from, you pass on the left.
If he wants to pass, he can use the right lane like everyone
else.
When they said, ?Take us to your leader,? I don?t think they
meant your mother?s house.
They may be disappointed when they learn that ?our leader?
is your mother.
You?d better call your mother and tell her to set a few extra
place settings.
If they ask for our leader, is it Obama or your mother?
Which finger do I use for aliens?
I guess the middle finger means the same thing to them.
I sense somehow that flipping the bird was lost on them.
What?s the Klingon gesture for ?Go around us, jerk??
Table 1: Captions for contest #331. Finalists are
listed in italics.
this research project, we have acquired five car-
toons along with all of the captions submitted in
the corresponding contest.
While the task of automatically identifying the
funny captions would be quite useful, it is well be-
yond the current state of the art in NLP. A much
more manageable task, and one that is quite impor-
tant for the contest?s editor is to annotate captions
according to their factoids. This allows the orga-
nizers of the contest to find the most frequently
mentioned factoids and select representative cap-
tions for each factoid.
On average, each cartoon has 5,400 submitted
captions, but for each of five cartoons, we sam-
pled 500 captions for annotation. The annotators
were instructed to mark factoids by identifying
and grouping events, objects, and themes present
in the captions, creating a unique name for each
factoid, and marking the captions that contain each
factoid. One caption could be given many differ-
ent labels. For example, in cartoon #331, such fac-
toids may be ?bad directions?, ?police?, ?take me
to your leader?, ?racism?, or ?headlights?. After
annotating, each set of captions contained about
60 factoids on average. On average a caption was
annotated with 0.90 factoids, with approximately
80% of the discourse units having at least one fac-
toid, 20% having at least two, and only 2% hav-
ing more than two. Inter-annotator agreement was
moderate, with an F1-score (described more in
section 5) of 0.6 between annotators.
As van Halteren and Teufel (2003) also found
250
0 20 40 600
20
40
60
(a)
0 5 10 15 20 250
50
100
150
(b)
Figure 2: Average factoid frequency distributions
for cartoon captions (a) and crossword clues (b).
0 100 200 300 400 5000
20
40
60
(a)
0 100 200 300 400 5000
5
10
(b)
Figure 3: Growth of the number of unique factoids
as the size of the corpus grows for cartoon captions
(a) and crossword clues (b).
when examining factoid distributions in human-
produced summaries, we found that the distribu-
tion of factoids in the caption set for each car-
toon seems to follow a power law. Figure 2 shows
the average frequencies of factoids, when ordered
from most- to least-frequent. We also found a
Heap?s law-type effect in the number of unique
factoids compared to the size of the corpus, as in
Figure 3.
3.2 Crossword Clues Data Set
Clues in crossword puzzles are typically obscure,
requiring the reader to recognize double mean-
ings or puns, which leads to a great deal of diver-
sity. These clues can also refer to one or more
of many different senses of the word. Table 2
shows examples of many different clues for the
word ?tea?. This table clearly illustrates the differ-
ence between factoids (the senses being referred
to) and nuggets (the realization of the factoids).
The website crosswordtracker.com col-
lects a large number of clues that appear in dif-
ferent published crossword puzzles and aggregates
them according to their answer. From this site, we
collected 200 sets of clues for common crossword
answers.
We manually annotated 20 sets of crossword
clues according to their factoids in the same fash-
ion as described in section 3.1. On average each
set of clues contains 283 clues and 15 different
factoids. Inter-annotator agreement on this dataset
was quite high with an F1-score of 0.96.
Clue Sense
Major Indian export drink
Leaves for a break? drink
Darjeeling, e.g. drink
Afternoon social event
4:00 gathering event
Sympathy partner film
Mythical Irish queen person
Party movement political movement
Word with rose or garden plant and place
Table 2: Examples of crossword clues and their
different senses for the word ?tea?.
4 Methods
4.1 Random Walk Method
We take a graph-based approach to the discovery
of factoids, clustering a word similarity graph and
taking the resulting clusters to be the factoids. Two
different graphs, a word co-occurrence graph and
a lexical similarity graph learned from the corpus,
are compared. We also compare the graph-based
methods against baselines of clustering and topic
modeling.
4.1.1 Word Co-occurrence Graph
To create the word co-occurrence graph, we create
a link between every pair of words with an edge
weight proportional to the number of times they
both occur in the same discourse unit.
4.1.2 Corpus-based Lexical Similarity Graph
To build the lexical similarity graph, a lexical sim-
ilarity function is learned from the corpus, that
is, from one set of captions or clues. We do this
by computing feature vectors for each lemma and
using the cosine similarity between these feature
vectors as a lexical similarity function. We con-
struct a word graph with edge weights propor-
tional to the learned similarity of the respective
word pairs.
We use three types of features in these feature
vectors: context word features, context part-of-
speech features, and spelling features. Context
features are the presence of each word in a win-
dow of five words (two words on each side plus the
word in question). Context part-of-speech features
are the part-of-speech labels given by the Stan-
ford POS tagger (Toutanova et al, 2003) within
the same window. Spelling features are the counts
of all character trigrams present in the word.
Table 3 shows examples of similar word pairs
from the set of crossword clues for ?tea?. From
251
Figure 4: Example of natural clusters in a subsection of the word co-occurrence graph for the crossword
clue ?astro?.
Word pair Sim.
(white-gloves, white-glove) 0.74
(may, can) 0.57
(midafternoon, mid-afternoon) 0.55
(company, co.) 0.46
(supermarket, market) 0.53
(pick-me-up, perk-me-up) 0.44
(green, black) 0.44
(lady, earl) 0.39
(kenyan, indian) 0.38
Table 3: Examples of similar pairs of words as cal-
culated on the set of crossword clues for ?tea?.
this table, we can see that this method is able
to successfully identify several similar word pairs
that would be missed by most lexical databases:
minor lexical variations, such as ?pick-me-up? vs.
?perk-me-up?; abbreviations, such as ?company?
and ?co.?; and words that are similar only in this
context, such as ?lady? and ?earl? (referring to
Lady Grey and Earl Grey tea).
4.1.3 Graph Clustering
To cluster the word similarity graph, we use the
Louvain graph clustering method (Blondel et al,
2008), a hierarchical method that optimizes graph
modularity. This method produces several hierar-
chical cluster levels. We use the highest level, cor-
responding to the fewest number of clusters.
Figure 4 shows an example of clusters found
in the word graph for the crossword clue ?as-
tro?. There are three obvious clusters, one for the
Houston Astros baseball team, one for the dog in
the Jetsons cartoon, and one for the lexical prefix
?astro-?. In this example, two of the clusters are
connected by a clue that mentions multiple senses,
?Houston ballplayer or Jetson dog?.
4.1.4 Random Walk Factoid Assignment
After discovering factoids, the remaining task is
to annotate captions according to the factoids they
contain. We approach this problem by taking ran-
dom walks on the word graph constructed in the
previous sections, starting the random walks from
words in the caption and measuring the hitting
times to different clusters.
For each discourse unit, we repeatedly sam-
ple words from it and take Markov random walks
starting from the nodes corresponding to the se-
lected and lasting 10 steps (which is enough to en-
sure that every node in the graph can be reached).
After 1000 random walks, we measure the aver-
age hitting time to each cluster, where a cluster is
considered to be reached by the random walk the
first time a node in that cluster is reached. Heuris-
tically, 1000 random walks was more than enough
to ensure that the factoid distribution had stabi-
lized in development data.
The labels that are applied to a caption are the
labels of the clusters that have a sufficiently low
hitting time. We perform five-fold cross valida-
tion on each caption or set of clues and tune the
threshold on the hitting time such that the aver-
age number of labels per unit produced matches
the average number of labels per unit in the gold
annotation of the held-out portion.
For example, a certain caption may have the fol-
lowing hitting times to the different factoid clus-
ters:
factoid1 0.11
factoid2 0.75
factoid3 1.14
factoid4 2.41
If the held-out portion has 1.2 factoids per cap-
tion, it may be determined that the optimal thresh-
252
old on the hitting times is 0.8, that is, a threshold
of 0.8 produces 1.2 factoids per caption in the test-
set on average. In this case factoid1 and factoid2
would be marked for this caption, since the hitting
times fall below the threshold.
4.2 Clustering
A simple baseline that can act as a surrogate for
factoid annotation is clustering of discourse units,
which is equivalent to assigning exactly one fac-
toid (the name of its cluster) to each discourse
unit. As our clustering method, we use C-Lexrank
(Qazvinian and Radev, 2008), a method that has
been well-tested on collective discourse.
4.3 Topic Model
Topic modeling is a natural way to approach the
problem of factoid annotation, if we consider the
topics to be factoids. We use the Mallet (McCal-
lum, 2002) implementation of Latent Dirichlet Al-
location (LDA) (Blei et al, 2003). As with the ran-
dom walk method, we perform five-fold cross val-
idation, tuning the threshold for the average num-
ber of labels per discourse unit to match the aver-
age number of labels in the held-out portion. Be-
cause LDA needs to know the number of topics
a priori, we set the number of topics to be equal
to the true number of factoids. We also use the
average number of unique factoids in the held-out
portion as the number of LDA topics.
5 Evaluation and Results
We evaluate this task in a way similar to pairwise
clustering evaluation methods, where every pair of
discourse units that should share at least one fac-
toid and does is a true positive instance, every pair
that should share a factoid and does not is a false
negative, etc. From this we are able to calculate
precision, recall, and F1-score. This is a reason-
able evaluation method, since the average number
of factoids per discourse unit is close to one. Be-
cause the factoids discovered by this method don?t
necessarily match the factoids chosen by the an-
notators, it doesn?t make sense to try to measure
whether two discourse units share the ?correct?
factoid.
Tables 4 and 5 show the results of the various
methods on the cartoon captions and crossword
clues datasets, respectively. On the crossword
clues datasets, the random-walk-based methods
are clearly superior to the other methods tested,
whereas simple clustering is more effective on the
Method Prec. Rec. F1
LDA 0.318 0.070 0.115
C-Lexrank 0.131 0.347 0.183
Word co-occurrence graph 0.115 0.348 0.166
Word similarity graph 0.093 0.669 0.162
Table 4: Performance of various methods annotat-
ing factoids for cartoon captions.
Method Prec. Rec. F1
LDA 0.315 0.067 0.106
C-Lexrank 0.702 0.251 0.336
Word co-occurrence graph 0.649 0.257 0.347
Word similarity graph 0.575 0.397 0.447
Table 5: Performance of various methods annotat-
ing factoids for crossword clues.
cartoon captions dataset.
In some sense, the two datasets in this paper
both represent difficult domains, ones in which
authors are intentionally obscure. The good re-
sults acheived on the crossword clues dataset in-
dicate that this obscurity can be overcome when
discourse units are short. Future work in this
vein includes applying these methods to domains,
such as newswire, that are more typical for sum-
marization, and if necessary, investigating how
these methods can best be applied to domains with
longer sentences.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Vincent D Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics: Theory and Experiment,
2008(10):P10008.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 395?403. As-
sociation for Computational Linguistics.
Leonhard Hennig, Ernesto William De Luca, and Sahin
Albayrak. 2010. Learning summary content units
with topic modeling. In Proceedings of the 23rd
253
International Conference on Computational Lin-
guistics: Posters, COLING ?10, pages 391?399,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method.
Vahed Qazvinian and Dragomir R Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 689?696. Association for Computational Lin-
guistics.
Vahed Qazvinian and Dragomir R Radev. 2011.
Learning from collective human behavior to intro-
duce diversity in lexical choice. In Proceedings of
the 49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies, pages 1098?1108.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Hans Van Halteren and Simone Teufel. 2003. Exam-
ining the consensus between human summaries: ini-
tial experiments with factoid analysis. In Proceed-
ings of the HLT-NAACL 03 on Text summarization
workshop-Volume 5, pages 57?64. Association for
Computational Linguistics.
254
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?577,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A System for Summarizing Scientific Topics Starting from Keywords
Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI, USA
rahuljha@umich.edu
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we investigate the problem
of automatic generation of scientific sur-
veys starting from keywords provided by
a user. We present a system that can take
a topic query as input and generate a sur-
vey of the topic by first selecting a set
of relevant documents, and then selecting
relevant sentences from those documents.
We discuss the issues of robust evalua-
tion of such systems and describe an eval-
uation corpus we generated by manually
extracting factoids, or information units,
from 47 gold standard documents (surveys
and tutorials) on seven topics in Natural
Language Processing. We have manually
annotated 2,625 sentences with these fac-
toids (around 375 sentences per topic) to
build an evaluation corpus for this task.
We present evaluation results for the per-
formance of our system using this anno-
tated data.
1 Introduction
The rise of the number of publications in all sci-
entific fields is making it more and more difficult
to get quickly acquainted with the new develop-
ments in a new area. One way to wade through this
huge amount of scholarly information is to consult
topical surveys written by experts in an area. For
example, for machine translation, one might read
(Lopez, 2008)1. Such surveys can be very help-
ful when available, but unfortunately, may not be
available for all areas. Additionally, the manual
surveys quickly go out of date within a few years
of publication as additional papers are published
in the field.
1Adam Lopez. 2008. Statistical machine translation.
ACM Comput. Surv. 40, 3, Article 8
Thus, a system that can generate such surveys
automatically would be a useful tool. Short sum-
maries in the form of abstracts are available for
individual papers, but no such information is avail-
able for scientific topics. In this paper, we ex-
plore strategies for generating and evaluating such
surveys of scientific topics automatically starting
from a phrase representing a topic area. We evalu-
ate our system on a set of topics in the field of Nat-
ural Language Processing. In earlier work, (Teufel
and Moens, 2002) have examined the problem
of summarizing scientific articles using rhetorical
analysis of sentences. Nanba and Okumura (1999)
have also discussed the problem of generating sur-
veys of multiple papers. Mohammad et al (2009)
presented experiments on generating surveys of
scientific topics starting from papers to be summa-
rized. More recently, Hoang and Kan (2010) have
presented initial results on automatically generat-
ing related work section for a target paper by tak-
ing a hierarchical topic tree as an input.
In this paper, we tackle the more challenging
problem of summarizing a topic starting from a
topic query. Our system takes as an input a string
describing the topic area, selects the relevant pa-
pers from a corpus of papers, and then selects sen-
tences from the citing sentences to these papers to
generate a survey of the topic. A sample output of
our system for the topic of ?Word Sense Disam-
biguation? is shown in Figure 1.
2 Candidate Document Selection
Given a query representing the topic to be sum-
marized, our first task is to find the set of rele-
vant documents from the corpus. The simplest
way to do this for a corpus of scientific publica-
tions is to do a query search using exact match or
a standard TF*IDF system such Lucene, rank the
documents using either citation counts or pager-
ank in the bibliometric citation network, and se-
lect the top n documents. However, comparing
572
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a
tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al , 1998), and semi-supervised sense disambiguation
(Yarowsky, 1995).
Most researchers working on word sense disambiguation (WSD) use manually sense tagged data such as SemCor (Miller et al , 1993) to train statistical
classifiers, but also use the information in SemCor on the overall sense distribution for each word as a backoff model.
Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation.
Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002;
Mihalcea and Moldovan, 2001; Yarowsky et al , 2001).
For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al , 1991; Dagan, 1991; Dagan and Itai, 1994;
Ide, 2000; Resnik and Yarowsky, 1999).
Figure 1: A sample output survey of our system on the topic of ?Word Sense Disambiguation? produced
by paper selection using Restricted Expansion and sentence selection using Lexrank. In our evaluations,
this survey achieved a pyramid score of 0.82 and Unnormalized RU score of 0.31.
Document selection algorithm CG5 CG10 CG20
Title match sorted with citation count 1.82 2.75 3.29
Title match sorted with pagerank 1.77 2.55 3.34
Citation expansion sorted with citation
count 0.53 1.20 2.29
Citation expansion sorted with pagerank 0.20 0.78 1.99
TF*IDF ranked 0.14 0.14 0.56
TF*IDF sorted with citation count 0.44 2.25 3.18
TF*IDF sorted with pagerank 1.54 2.22 2.85
Restricted Expansion 2.52 3.91 6.01
Table 1: Comparison of different methods for
document selection by measuring the Cumulative
Gain (CG) of top 5, 10 and 20 results.
the results of these techniques with the papers cov-
ered by gold standard surveys on a few topics, we
found that some important papers are missed by
these simple approaches. One reason for this is
that early papers in a field might use non-standard
terms in the absence of a stable, accepted termi-
nology. Some early Word Sense Disambiguation
papers, for example, refer to the problem as Lex-
ical Ambiguity Resolution. Additionally, papers
might use alternative forms or abbreviations of
topics in their titles and abstracts, e.g. for input
query ?Semantic Role Labelling?, papers such as
(Dahlmeier et al, 2009) titled ?Joint Learning of
Preposition Senses and Semantic Roles of Prepo-
sitional Phrases? and (Che and Liu, 2010) titled
?Jointly Modeling WSD and SRL with Markov
Logic? might be missed.
To find these papers, we add a simple heuristic
called Restricted Expansion. In this method, we
first create a base set B, by finding papers with an
exact match to the query. This is a high precision
set since a paper with a title that contains the ex-
act query phrase is very likely to be relevant to the
topic. We then find additional papers by expand-
ing in the citation network around B, that is, by
finding all the papers that are cited by or cite the
papers in B, to create an extended set E. From
this combined set (B ?E), we create a new set F
by filtering out the set of papers that are not cited
by or cite a minimum threshold tinit of papers in
B. If the total number of papers is lower than fmin
or higher than fmax, we iteratively increase or de-
crease t till fmin ? |F | ? fmax. This method
allows us to increase our recall without losing pre-
cision. The values for our current experiments are:
tinit = 5, fmin = 150, fmax = 250.
Authors Year Size
Surveys
ACL Wiki 2012 4
Roberto Navigli 2009 68
Eneko Agirre; Philip Edmonds 2006 28
Xiaohua Zhou; Hyoil Han 2005 6
Nancy Ide; Jean Vronis 1998 41
Tutorials
Sanda Harabagiu 2011 45
Diana McCarthy 2011 120
Philipp Koehn 2008 17
Rada Mihalcea 2005 186
Table 2: The set of surveys and tutorials col-
lected for the topic of ?Word Sense Disambigua-
tion?. Sizes for surveys are expressed in number
of pages, sizes for tutorials are expressed in num-
ber of slides.
To evaluate different methods of candidate doc-
ument selection, we use Cumulative Gain (CG),
where the weight for each paper is estimated by
the fraction of surveys it appears in. Table 1
shows the average Cumulative Gain of top 5, 10
and 20 documents for each of eight methods we
tried. Restricted Expansion outperformed every
other method. Once we obtain a set of papers to
be summarized, we select the top n most cited pa-
pers in the document set as the papers to be sum-
marized, and extract the set of citing sentences S
from all the papers in the document set to these n
papers. S is the input for our sentence selection
algorithms, described in Section 4.
573
Factoid S1 S2 S3 S4 S5 T1 T2 T3 T4 Factoid Weight
definition of wsd X X X X X X X X X 9
wordnet X X X X X X X X 8
knowledge based wsd X X X X X X X 7
supervised wsd X X X X X X X 7
senseval X X X X X X X 7
definition of word senses X X X X X X 7
knowledge based wsd using machine readable dictionaries X X X X X X 6
unsupervised wsd X X X X X X 6
bootstrapping algorithms X X X X X X 6
supervised wsd using decision lists X X X X X X 6
Table 3: Top 10 factoids for the topic of ?Word Sense Disambiguation? and their distribution across
various data sources.
3 Evaluation Data for Survey Generation
We use the ACL Anthology Network (AAN) as the
corpus for our experiments (Radev et al, 2013).
We built a factoid inventory for seven topics in
NLP based on manual written surveys in the fol-
lowing way. For each topic, we found at least 3
recent tutorials and 3 recent surveys on the topic
and extracted the factoids that are covered in each
of them. Table 2 shows the complete list of ma-
terial collected for the topic of ?Word Sense Dis-
ambiguation?. We found around 80 factoids per
topic on an average. Once the factoids were ex-
tracted, each factoid was assigned a weight based
on the number of documents it appears in, and any
factoids with weight one were removed. Table 3
shows the top ten factoids in the topic of Word
Sense Disambiguation along with their distribu-
tion across the different surveys and tutorials and
final weight.
For each of the topics, we used the method de-
scribed in Section 2 to create a candidate docu-
ment set and extracted the candidate citing sen-
tences to be used as the input for the content se-
lection component. Each sentence in each topic
was then annotated by a human judge against the
factoid list for that topic. A sentence is allowed
to have zero or more than one factoid. The human
assessors were graduate students in Computer Sci-
ence who have taken a basic ?Natural Language
Processing? course or an equivalent course. On an
average, 375 citing sentences were annotated for
each topic, with 2,625 sentences being annotated
in total. We present all our experimental results on
this large annotated corpora which is also available
for download 2.
4 Content Models
Once we have the set of input sentences, our sys-
tem must select the sentences that should be part
2http://clair.si.umich.edu/corpora/survey data/
of the survey. For this task, we experimented with
three content models, described below.
4.1 Centroid
The centroid of a set of documents is a set of words
that are statistically important to the cluster of doc-
uments. Centroid based summarization of a docu-
ment set involves first creating the centroid of the
documents, and then judging the salience of each
document based on its similarity to the centroid
of the document set. In our case, the input citing
sentences represent the documents from which we
extract the centroid. We use the centroid imple-
mentation from the publicly available summariza-
tion toolkit, MEAD (Radev et al, 2004).
4.2 Lexrank
LexRank (Erkan and Radev, 2004) is a network
based content selection algorithm that works by
first building a graph of all the documents in a
cluster. The edges between corresponding nodes
represent the cosine similarity between them.
Once the network is built, the algorithm computes
the salience of sentences in this graph based on
their eigenvector centrality in the network.
4.3 C-Lexrank
C-Lexrank is another network based content selec-
tion algorithm that focuses on diversity (Qazvinian
and Radev, 2008). Given a set of sentences, it first
creates a network using these sentences and then
runs a clustering algorithm to partition the net-
work into smaller clusters that represent different
aspects of the paper. The motivation behind the
clustering is to include more diverse facts in the
summary.
5 Experiments and Results
To do an evaluation of our different content selec-
tion methods, we first select the documents using
our Restricted Expansion method, and then pick
574
Topic Rand Cent LR C-LR
Summarization 0.68 0.61 0.91 0.82
Question Answering 0.52 0.50 0.65 0.56
Word Sense Disambiguation 0.78 0.73 0.82 0.76
Named Entity Recognition 0.90 0.90 0.94 0.94
Sentiment Analysis 0.75 0.78 0.77 0.78
Semantic Role Labeling 0.78 0.79 0.88 0.94
Dependency Parsing 0.67 0.38 0.71 0.53
Average 0.72 0.68 0.81? 0.76
Table 4: Results of pyramid evaluation for each
of the three methods and the random baseline on
each topic.
the citing sentences to be used as the input to the
summarization module as described in Section 2.
Given this input, we generate 500 word summaries
for each of the seven topics using the four meth-
ods: Centroid, Lexrank, C-Lexrank and a random
baseline.
For each summary, we compute two evaluation
metrics. The first is the Pyramid score (Nenkova
and Passonneau, 2004) computed by treating the
factoids as Summary Content Units (SCU?s). The
Pyramid scores for each summary is shown in Ta-
ble 4. The second metric is an Unnormalized Rel-
ative Utility score (Radev and Tam, 2003), com-
puted using the factoid scores of sentences based
on the method presented in (Qazvinian, 2012). We
call this Unnormalized RU since we are not able to
normalize the scores with human generated gold
summaries. The results for Unnormalized RU are
shown in Table 5. The parameter ? is the RU
penalty for including a redundant sentence sub-
sumed by an earlier sentence. If the summary
chooses a sentence si with score worig that is sub-
sumed by an earlier summary sentence, the score
is reduced as wsubsumed = (? ? worig). We ap-
proximate subsumption by marking a sentence sj
as being subsumed by si if Fj ? Fi, where Fi and
Fj are sets of factoids covered in each sentence.
Topic Rand Cent LR C-LR
Summarization 0.16 0.57 0.29 0.17
Question Answering 0.32 0.39 0.48 0.30
Word Sense Disambiguation 0.28 0.33 0.31 0.30
Named Entity Recognition 0.36 0.38 0.34 0.31
Sentiment Analysis 0.23 0.34 0.48 0.33
Semantic Role Labeling 0.11 0.17 0.16 0.21
Dependency Parsing 0.16 0.05 0.30 0.15
Average 0.23 0.32 0.34? 0.25
Table 5: Results of Unnormalized Relative Utility
evaluation for the three methods and random base-
line using ? = 0.5.
The reason for the relatively high scores for the
random baseline is that our process to select the
initial set of sentences eliminates many bad sen-
tences. For example, for a subset of 5 topics,
the total input set contains 1508 sentences, out of
which 922 of the sentences (60%) have at least one
factoid. This makes it highly likely to pick good
content sentences even when we are picking sen-
tences at random.
We find that the Lexrank method outperforms
other sentence selection methods on both evalua-
tion metrics. The higher performance of Lexrank
compared to Centroid is consistent with earlier
published results (Erkan and Radev, 2004). The
reason for the low performance of C-Lexrank as
compared to Lexrank on this data set can be at-
tributed to the fact that the input sentence set is
derived from a much more diverse set of papers
which can have a high diversity in lexical choice
when describing the same factoid. Thus simple
lexical similarity is not enough to find good clus-
ters in this sentence set.
The lower Unnormalized RU scores compared
to Pyramid scores indicate that we are selecting
sentences containing highly weighted factoids, but
we do not select the most informative sentences
that contain a large number of factoids. This
also shows that we select some redundant factoids,
since Unnormalized RU contains a penalty for re-
dundancy. This is again, explained by the fact
that the simple lexical diversity based model in C-
Lexrank is not able to detect the same factoids be-
ing present in two sentences. Despite these short-
comings, our system works quite well in terms
of content selection for unseen topics, Figure 2
shows the top 5 sentences for the query ?Condi-
tional Random Fields?.
6 Conclusion and Future Work
In this paper, we described a pipeline for the gen-
eration of scientific surveys starting from a topic
query. Our system is divided into two components.
The first component finds the set of papers from
the corpus relevant to the query using a simple
heuristic called Restricted Expansion. The second
component selects sentences from these papers to
generate a survey of the topic. One of the main
contributions of this work is a manually annotated
data set for evaluating both the tasks. We collected
47 gold standard documents (surveys and tutori-
als) on seven topics in Natural Language Process-
ing and extracted factoids for each topic. Each
factoid is given an importance score based on the
number of gold standard documents it appears in.
575
In recent years, conditional random fields (CRFs) (Lafferty et al , 2001)
have shown success on a number of natural language processing (NLP)
tasks, including shallow parsing (Sha and Pereira, 2003), named entity
recognition (McCallum and Li, 2003) and information extraction from
research papers (Peng and McCallum, 2004).
In natural language processing, two aspects of CRFs have been
investigated sufficiently: one is to apply it to new tasks, such as named
entity recognition (McCallum and Li, 2003; Li and McCallum, 2003;
Settles, 2004), part-of-speech tagging (Lafferty et al, 2001), shallow
parsing (Sha and Pereira, 2003), and language modeling (Roark et al,
2004); the other is to exploit new training methods for CRFs, such as
improved iterative scaling (Lafferty et al, 2001), L-BFGS (McCallum,
2003) and gradient tree boosting (Dietterich et al, 2004)
NP chunks are very similar to the ones of Ramshaw and Marcus (1995).
CRFs have shown empirical successes recently in POS tagging (Lafferty
et al , 2001), noun phrase segmentation (Sha and Pereira, 2003) and
Chinese word segmentation (McCallum and Feng, 2003)
CRFs have been successfully applied to a number of real-world tasks,
including NP chunking (Sha and Pereira, 2003), Chinese word
segmentation (Peng et al, 2004), information extraction (Pinto et al,
2003; Peng and McCallum, 2004), named entity identification (McCallum
and Li, 2003; Settles, 2004), and many others.
Figure 2: A sample output survey produced by
our system on the topic of ?Conditional Random
Fields? using Restricted Expansion and Lexrank.
Additionally, we manually annotated 2,625 input
sentences, about 375 sentences per topic, with the
factoids extracted from the gold standard docu-
ments for each topic. Using this corpus, we pre-
sented experimental results for the performance of
our document selection component and three sen-
tence selection strategies.
Our results indicate three main directions for
future work. We plan to look at better models
of diversity in sentence selection, since methods
based on simple lexical similarity do not seem to
work well. The low factoid recall shown by low
unnormalized RU scores suggests integrating the
full text of papers with citation based summaries
which might help us find factoids such as topic
definitions that are unlikely to be present in citing
sentences. A final goal would be to improve the
readability and coherence of our system output.
Acknowledgments
We thank Vahed Qazvinian, Wanchen Lu, Ben
King, and Shiwali Mohan for extremely useful
discussions and help with the data annotation.
This research is supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
References
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ?10,
pages 427?435, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 584?592, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of the 16th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-99), pages 926?931.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics - Human Language Technologies (HLT-
NAACL ?04).
Vahed Qazvinian and Dragomir R. Radev. 2008.
Scientific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING-08), Manchester, UK.
Vahed Qazvinian. 2012. Using Collective Discourse
to Generate Surveys of Scientific Paradigms. Ph.D.
thesis.
Dragomir R. Radev and Daniel Tam. 2003. Sum-
marization evaluation using relative utility. In
CIKM2003, pages 508?511.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
576
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources
and Evaluation, pages 1?26.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
577
Heterogeneous Networks and Their Applications: Scientometrics, Name
Disambiguation, and Topic Modeling
Ben King, Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI
{benking,rahuljha}@umich.edu
Dragomir R. Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present heterogeneous networks as a way to
unify lexical networks with relational data. We
build a unified ACL Anthology network, tying
together the citation, author collaboration, and
term-cooccurence networks with affiliation and
venue relations. This representation proves to
be convenient and allows problems such as name
disambiguation, topic modeling, and the mea-
surement of scientific impact to be easily solved
using only this network and off-the-shelf graph
algorithms.
1 Introduction
Graph-based methods have been used to great ef-
fect in NLP, on problems such as word sense disam-
biguation (Mihalcea, 2005), summarization (Erkan
and Radev, 2004), and dependency parsing (McDon-
ald et al., 2005). Most previous studies of networks
consider networks with only a single type of node,
and in some cases using a network with a single type
of node can be an oversimplified view if it ignores
other types of relationships.
In this paper we will demonstrate heterogeneous
networks, networks with multiple different types of
nodes and edges, along with several applications of
them. The applications in this paper are not pre-
sented so much as robust attempts to out-perform the
current state-of-the-art, but rather attempts at being
competitive against top methods with little effort be-
yond the construction of the heterogeneous network.
Throughout this paper, we will use the data from
the ACL Anthology Network (AAN) (Bird et al.,
2008; Radev et al., 2013), which contains additional
metadata relationships not found in the ACL Anthol-
ogy, as a typical heterogeneous network. The results
in this paper should be generally applicable to other
heterogeneous networks.
1.1 Heterogeneous AAN schema
We build a heterogeneous graph G(V,E) from
AAN, where V is the set of vertices and E is the
set of edges connecting vertices. A vertex can be
one of five semantic types: {paper, author, venue,
institution, term}. An edge can also be one of five
types, each connecting different types of vertices:
? author ? [writes] ? paper
? paper ? [cites] ? paper
? paper ? [published in] ? venue1
? author ? [affiliated with] ? institution2
? paper ? [contains] ? term
All of this data, except for the terms, is available
for all papers in the 2009 release of AAN. Terms are
extracted from titles by running TextRank (Mihal-
cea and Tarau, 2004) on NP-chunks from titles and
manually filtering out bad terms.
We show the usefulness of this representation
in several applications: the measurement of scien-
tific impact (Section 2), name disambiguation (Sec-
tion 3), and topic modeling (Section 4). The hetero-
geneous network representation provides a simple
framework for combining lexical networks (like the
term co-occurence network) with metadata relations
from a source like AAN and allows us to begin to
develop NLP-aware methods for problems like sci-
entometrics and name disambiguation, which are not
usually framed in an NLP perspective.
1For a joint meeting of venues A and B publishing a paper
x, two edges (x,A) and (x,B) are created.
2Author-affiliation edges are weighted according to the
number of papers an author has published from an institution.
1
Transactions of the Association for Computational Linguistics, 2 (2014) 1?14. Action Editor: Lillian Lee.
Submitted 3/2013; Revised 6/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
2 Scientific Impact Measurement
The study of scientometrics, which attempts to
quantify the scientific impact of papers, authors, etc.
has received much attention recently, even within
the NLP community. In the past few years, there
have been many proposed measures of scientific im-
pact based on relationships between entities. Intu-
itively, a model that can take into account many dif-
ferent types of relationships between entities should
be able to measure scientific impact more accu-
rately than simpler measures like citation counts or
h-index.
We propose using Pagerank on the heterogeneous
AAN (Page et al., 1999) to measure scientific impact.
Since changes in the network schema can affect the
relative rankings between different types of entities,
this method is probably not appropriate for compar-
ing entities of two different types against each other.
But between nodes of the same type, this measure is
an appropriate (and as we will show, accurate) way
to compare impacts.
We see this method as a first logical step in the
direction of heterogeneous network-based sciento-
metrics. This method could easily be extended to
use a directed schema (Kurland and Lee, 2005) or a
schema that is aware of the lexical content of citation
sentences, such as sentiment-based signed networks
(Hassan et al., 2012).
Determining the intrinsic quality of scientific im-
pact measures can be difficult since there is no
way to collect gold standard measurements for real-
world entities. Previous studies have attempted to
show that their measures give high scores to a few
known high-impact entities, e.g. Nobel prize win-
ners (Hirsch, 2005), or have performed a statistical
component analysis to find the most important mea-
sures in a group of related statistics (Bollen et al.,
2009). Our approach, instead, is to generate real-
istic data from synthetic entities whose impacts are
known.
We had considered alternative formulations that
did not rely on synthetic data, but each of them
presented problems. When we attempted manual
prominence annotation for AAN data, the inter-
judge agreement (measured by Spearman correla-
tion) in our experiments ranged from decent (0.9
in the case of institutions) to poor (0.3 for authors)
to nearly random (0.03 for terms), far too low to
use in most cases. We also considered evaluating
prominence measures by their ability to predict fu-
ture citations to an entity. Citations are often used
as a proxy for impact, but our measurements have
found that correlation between past citations and fu-
ture citations is too high for citation prediction to be
a meaningful evaluation3.
2.1 Creating a synthetic AAN
In network theory, a common technique for testing
network algorithms when judgments of real-world
data are expensive or impossible to obtain is to test
the algorithm on a synthetic network. To create such
a synthetic network, the authors define a simple, but
realistic generative process by which the real-world
networks of interest may arise. The properties of
the network are measured to ensure that it replicates
certain observable behaviors of the real-world net-
work. They can then test network algorithms to see
how well they are able to recover the hidden param-
eters that generated the synthetic network. (Pastor-
Satorras and Vespignani, 2001; Clauset et al., 2009;
Karrer and Newman, 2011)
We take a two-step approach to generating this
synthetic data, first generating entities with known
impacts, and second, linking these entities together
according to their latent impacts. Our heuristic is
that high impact entities should be linked to other
high impact entities and vice-versa. As in the net-
work theory literature, we must show that this data
reflects important properties observed in the true
AAN.
One such property is that the number of citations
per paper follows a power law distribution (Redner,
1998). We observe this behavior in AAN along with
several other small-world behaviors, such as a small
diameter, a small average shortest path length, and a
high clustering coefficient in the coauthorship graph.
We strive to replicate these properties in our syn-
thetic data.
3Most existing impact measurements require access to at
least one year?s worth of citation information. The Spearman
correlation between the number of citations received after one
year and after five years is 0.79 with correlation between suc-
cessive years as high as 0.99. Practically this means that the
measures that best correlate with citations after five years are
exactly those that best correlate with citations after one year.
2
Since scientific impact measures attempt to quan-
tify the true impact of entities, we can use these mea-
sures to help understand how the true impact mea-
sures are distributed across different entities. In fact,
citation counts, being a good estimate of impact, can
be used to generate these latent impact variables for
each entity. For each type of entity (papers, authors,
institutions, venues, and terms), we create a latent
impact by sampling from the appropriate citation
count distribution. After sampling, all the impacts
are normalized to fall in the [0, 1] interval, with the
highest-impact entity of each type having a latent
impact of 1. Additive smoothing is used to avoid
having an impact of 0.
Once we have created the entities, our method
for placing edges is most similar to the Erdo?s-
Re?yni method for creating random graphs (Erdo?s
and Re?nyi, 1960), in which edges are distributed
uniformly at random between pairs of vertices. In-
stead of distributing links uniformly, links between
entities are sampled proportionally to I(a)I(b)(1 ?
(I(a) ? I(b))2), where I(x) is the latent impact of
entity x.
We tried several other formulae that failed to
replicate the properties of the real AAN. The
I(a)I(b) part of the formula above reflects a pref-
erence for nodes of any type to connect with high-
impact entities (e.g., major conferences receive
many submissions even though most submissions
will be rejected), but the 1 ? (I(a) ? I(b))2 part
also reflects the reality that entities of similar promi-
nence are most likely to attach to each other (e.g.,
well-known authors publish in major conferences,
while less well-known authors may publish mostly
in lesser-known workshops).
Using this distribution, we randomly sample links
between papers and authors; authors and institu-
tions; papers and venues; and papers and terms. The
only exception to this was paper-to-paper citation
links, for which we did not expect this same be-
havior to apply, as low-impact papers regularly cite
high-impact papers, but not vice-versa. To model ci-
tations, we selected citing papers uniformly at ran-
dom and cited papers in proportion to their impacts.
(Albert and Baraba?si, 2002)
Finally, we generated a network equal in size to
AAN, that is, with the exact same numbers of pa-
pers, authors, etc. and the exact same number of
Relationship True value Synth. value
Paper-citations power
law coeff. 1.82 2.12
Diameter 9 8
Avg. shortest path 4.27 4.05
Collaboration network
clustering coeff. 0.34 0.26
Table 1: Network properties of the synthetic AAN
compared with the true AAN.
paper-author links, paper-venue links, etc. Table 1
compares the observed properties of the true AAN
with the observed properties of this synthetic version
of AAN. None of the statistics are exact matches, but
when building random graphs, it is not uncommon
for measures to differ by many orders of magnitude,
so a model that has measures that are on the same
order of magnitude as the observed data is generally
considered to be a decent model (Newman and Park,
2003).
2.2 Measuring impact on the synthetic AAN
This random network is, of course, still imperfect
in some regards. First of all, it has no time aspect,
so it is not possible for impact to change over time,
which means we cannot test against some impact
measures that have a time component like CiteR-
ank (Maslov and Redner, 2008). Second, there are
some constraints present in the real world that are
not enforced here. Because the edges are randomly
selected, some papers have no venues, while others
have multiple venues. There is also nothing to en-
force certain consistencies, such as authors publish-
ing many papers from relatively few institutions, or
repeatedly collaborating with the same authors.
We had also considered using existing random
graph models such as the Baraba?si-Albert model
(Baraba?si and Albert, 1999), which are known to
produce graphs that exhibit power law behavior.
These models, however, do not provide a way to re-
spect the latent impacts of the entities, as they add
links in proportion only to the number of existing
links a node has.
We measure the quality of impact measures by
comparing ranked lists: the ordering of the entities
3
Paper measure Agreement
Heterogeneous network Pagerank 0.773
Citation network Pagerank 0.558
Citation count 0.642
Author measure Agreement
Heterogeneous network Pagerank 0.461
Coauthorship network Pagerank 0.244
h-index (Hirsch, 2005) 0.292
Aggregated citation count 0.236
i10-index 0.235
Institution measure Agreement
Heterogeneous network Pagerank 0.373
h-index (Mitra, 2006) 0.334
Aggregated citation count 0.327
Venue measure Agreement
Heterogeneous network Pagerank 0.449
h-index (Braun et al., 2006) 0.425
Aggregated citation count 0.370
Impact factor 0.092
Venue citation network Pagerank (Bollen
et al., 2006) 0.366
Table 2: Agreement of various impact measures
with the true latent impact.
by their true (but hidden) impact against their order-
ing according to the impact measure. The agree-
ment between these lists is measured by Kendall?s
Tau. Table 2 compares several well-known impact
measures with our impact measure, Pagerank cen-
trality on the heterogeneous AAN network. We find
that some popular methods, such as h-index (Hirsch,
2005) are too coarse to accurately capture much
of the underlying variation. There is a version of
Kendall?s Tau that accounts for ties, and while this
metric slightly helps the coarser measures, Pagerank
on the heterogeneous network is still the clear win-
ner.
When comparing different ordering methods, it
is natural to wonder which of entities the orderings
disagree on. In general, non-heterogeneous mea-
sures like h-index or collaboration network Pager-
ank, which only focus on one type of relationship
can suffer when the entity in question has an impor-
tant relationship of another type. For example, if an
author is highly cited, but mostly works alone, his
1985 1990 1995 2000 2005 2010
20
40
60
80
100
120
Re
lat
ive
Pa
ge
ran
k
ACL
EMNLP
COLING
NAACL
Figure 1: Evolution of conference impacts. The y-
axis measures relative Pagerank, the entity?s Pager-
ank relative to the average Pagerank in that year.
contribution would be undervalued in the collabo-
ration network, but would be more accurate in the
heterogeneous network.
The majority of the differences between the im-
pact measures, though, tend to be in how they han-
dle entities of low prominence. It seems that, for the
most part, there is relatively little disagreement in
the orderings of high-impact entities between differ-
ent impact measures. That is, most highly prominent
entities tend to be highly rated by most measures.
But when an author or a paper, for example, only has
one or two citations, it can be advantageous to look
at more types of relationships than just citations.
The paper may be written by an otherwise prominent
author, or published at a well-known venue, and hav-
ing many types of relations at its disposal can help a
method like heterogeneous network Pagerank better
distinguish between two low-prominence entities.
2.3 Top-ranked entities according to
heterogeneous network PageRank
Table 3 shows the papers, authors, institutions,
venues, and terms that received the highest Pager-
ank in the heterogeneous AAN. It is obvious that the
top-ranked entities in this network are not simply the
most highly cited entities.
This ranking also does not have any time bias
toward the entities that are currently prominent, as
some of the top authors were more prolific in previ-
ous decades than at the current time. We also see
this effect with COLING, which for many of the
early years, is the only venue in the ACL Anthology.
4
Top Papers Top Authors Top Institutions Top Venues TopTerms
? Building A Large Annotated Corpus OfEnglish: The Penn Treebank 4 15 Jun?ichi Tsujii 4 8
Carnegie Mellon
University 4 1 COLING ? translation
? The Mathematics Of Statistical MachineTranslation: Parameter Estimation 4 7
Aravind K.
Joshi 4 1
University of
Edinburgh 5 1 ACL 4 3 speech
? Attention, Intentions, And The Structure OfDiscourse 4 18
Ralph
Grishman 5 2
University of
Pennsylvania 4 2 HLT 5 1 parsing
? A Maximum Entropy Approach To NaturalLanguage Processing 4 75 Hitoshi Isahara 5 2
Massachusetts
Institute of
Technology
4 4 EACL 5 1 machinetranslation
? BLEU: a Method for Automatic Evaluationof Machine Translation 4 20
Yuji
Matsumoto 4 12
Saarland
University 4 7 LREC 4 3 generation
? A Maximum-Entropy-Inspired Parser 4 7 Kathleen R.McKeown 5 2
IBM T.J. Watson
Research Center ? NAACL 4 3 evaluation
4 2 A Stochastic Parts Program And NounPhrase Parser For Unrestricted Text 4 13 Eduard Hovy 4 39 CNRS 5 3 EMNLP 4 6 grammar
5 1 A Systematic Comparison of VariousStatistical Alignment Models 4 10
Christopher D.
Manning 4 26
University of
Tokyo 5 5
Computational
Linguistics 4 16 dialogue
4 4
Transformation-Based Error-Driven
Learning and Natural Language Processing:
a Case Study in Part-of-Speech Tagging
4 93 Yorick Wilks 5 4 StanfordUniversity 4 4 IJCNLP 4 10
knowl-
edge
4 1 A Maximum Entropy Model forPart-of-Speech Tagging 5 9 Hermann Ney 4 3 BBN Technologies 4 1
Workshop on
Speech and
Natural
Language
4 1 discourse
Table 3: The entities of each type receiving the highest scores from the heterogeneous network Pagerank
impact measure along with their respective changes in ranking when compared to a simple citation count
measure.
One possible way to address this is to use a narrower
time window when creating the graph, such as only
including edges from the previous five years. We
apply this technique in the following section.
2.4 Entity impact evolution
The heterogeneous graph formalism also provides a
natural way to study the evolution of impact over
time, as in (Hall et al., 2008), but at a much finer
granularity. Hall et al. measured the year-by-year
prominence of statistical topics, but we can measure
year-by-year prominence for any entity in the graph.
To measure the evolution of impacts over the
years, we iteratively create year-by-year versions of
the heterogeneous AAN. Each of these graphs con-
tains all entities along with all edges occurring in a
five year window. Due to space, we cannot com-
prehensively exhibit this technique and the data it
produces, but as a brief example, in Figure 1, we
show how the impacts of some major NLP confer-
ences changes over time.
The graph shows that NAACL and EMNLP have
been steadily gaining prominence since their intro-
ductions, but also shows that ACL has had to make
up a lot of ground since 1990 to surpass COLING.
We also notice that all the major conferences have
grown in impact since 2005, and believe that as the
field continues to grow, the major conferences will
continue to become more and more important.
3 Name Disambiguation
We frame network name disambiguation in a link
prediction setting (Taskar et al., 2003; Liben-Nowell
and Kleinberg, 2007). The problems of name dis-
ambiguation and link prediction share many char-
acteristics, and we have found that if two ambigu-
ous name nodes are close enough to be selected by a
link-prediction method, then they likely correspond
to the same real-world author.
We intend to show that the heterogeneous biblio-
graphic network can be used to better disambiguate
author names than the author collaboration network.
The heterogeneous network for this problem con-
tains papers, authors, terms, venues, and institutions.
We compare several well-known network similarity
measures from link prediction by transforming the
5
Network Distance Measure Precision Recall F1-score Rand index Purity NMI
Heterogeneous Truncated Commute Time 0.59 0.78 0.63 0.63 0.71 0.43
Heterogeneous Shortest Path 0.90 0.79 0.83 0.87 0.94 0.76
Heterogeneous PropFlow 0.89 0.83 0.84 0.87 0.93 0.77
Coauthorship Truncated Commute Time 0.47 0.80 0.54 0.47 0.60 0.18
Coauthorship Shortest Path 0.54 0.73 0.60 0.61 0.67 0.31
Coauthorship PropFlow 0.57 0.76 0.64 0.66 0.71 0.43
Coauthorship GHOST 0.89 0.60 0.69 0.81 0.94 0.63
Table 4: Performance of different networks and distance measures on the author name disambiguation task.
The performance measures are averaged over the sets of two, three, and four authors. Rand index is from
(Rand, 1971) and NMI is an abbreviation for normalized mutual information (Strehl and Ghosh, 2003)
similarities to distances and inducing clusters of au-
thors based on these distances.
We compare three distance measures: shortest
path, truncated commute time (Sarkar et al., 2008),
and PropFlow (Lichtenwalter et al., 2010). Short-
est path distance can be a useful metric for author
disambiguation because it is small when two am-
biguous nodes are neighbors in the graph or share
a neighbor. Its downside is that it only considers one
path between nodes, the shortest, and cannot take
advantage of the fact that there may be many short
paths between two nodes.
Truncated commute time is a variant of commute
time where all paths longer than some threshold are
truncated. The truncation threshold l should be set
such that no semantically meaningful path is trun-
cated. We use a value of ten for l in the heteroge-
neous graph and three in the coauthorship graph4.
The advantage of truncated commute time over or-
dinary commute time is simpler calculation, as no
paths longer than l need be considered. The down-
side of this method is that large branching factors
tend to lead to less agreement between commute
time and truncated commute time.
PropFlow is a quantity that measures the proba-
bility that a non-intersecting random walk starting at
node a reaches node b in l steps or fewer, where l is
again a threshold. As before, l should be a bound on
the length of semantically meaningful paths, so we
use the same values for l as with truncated commute
time. Of course, PropFlow is not a metric, which is
4This is a standard coauthorship graph with the edge weights
equal to the number of publications shared between authors.
The heterogeneous network does not have author-to-author
links, as authors are linked by paper nodes.
required for some clustering methods. We use the
following equation to transform PropFlow to a met-
ric: d(a, b) = 1PropF low(a,b) ? 1.
With each of the distance measures, we apply
the same clustering method: partitioning around
medoids, with the number of clusters automatically
determined using the gap statistic method (Tibshi-
rani et al., 2001). We create the null distribution
needed for the gap statistic method by many itera-
tions of randomly sampling distances from the com-
plete distance matrix between all nodes in the graph.
The gap statistic method automatically selects the
number of clusters from two, three, or four author
clusters.
We compare our methods against GHOST (Fan et
al., 2011), a high-performance author disambigua-
tion method based on the coauthorship graph.
3.1 Data
To generate name disambiguation data, we use the
pseudoword method of (Gale et al., 1992). Specif-
ically, we choose two or more completely random
authors and conflate them by giving all instances
of both authors the same name. We let each paper
written by this pseudoauthor be an instance to be
clustered. The clusters produced by any author dis-
ambiguation method can then be compared against
the papers actually written by each of the two au-
thors. This method, of course, relies on having all of
the underlying authors completely disambiguated,
which AAN provides.
This method is used to create 100 distambiguation
sets with two authors, 100 for three authors, and 100
for four authors.
6
3.2 Results
Table 4 shows the performance of author name dis-
ambiguation with different networks and distance
metrics. F1-score is the measure that is most of-
ten used to compare author disambiguation methods.
Both PropFlow and shortest path similarity on the
heterogeneous network perform quite well accord-
ing this measure, as well as the other reported mea-
sures. While comparable recall can be achieved us-
ing only the coauthorship graph, the heterogeneous
graph allows for much higher precision.
4 Random walk topic model
Here we present a topic model based entirely on
graph random walks. This method is not truly a
statistical model as there are no statistical parame-
ters being learned, but rather a topic-discovery and
-assignment method, attempting to solve the same
problem as statistical topic models such as proba-
bilistic latent semantic analysis (pLSA) (Hofmann,
1999) or latent Dirichlet allocation (LDA) (Blei et
al., 2003). In the absence of better terminology, we
use the name random walk topic model.
While this method does not have the robust math-
ematical foundation that statistical topic models pos-
sess, in its favor it has modularity, simplicity, and
interpretability. This language model is modular as
it completely separates the discovery of topics from
the association of topics with entities. It is sim-
ple because it requires only a clustering algorithm
and random walk algorithms, instead of complex in-
ference algorithms. The method also does not re-
quire any modification if the topology of the net-
work changes, whereas statistical models may need
an entirely different inference procedure if, e.g., au-
thor topics are desired in addition to paper topics.
Thirdly this method is easily interpretable with top-
ics provided by clustering in the word-relatedness
graph and topic association based on random walks
from entities to topics.
4.1 Topics from word graph clustering
From the set of ACL anthology titles, we create
two graphs: (1) a word relatedness graph by cre-
ating a weighted link between each pair of words
corresponding to the PropFlow (Lichtenwalter et al.,
2010) measure between them on the full heteroge-
neous graph and (2) a word co-occurence graph by
creating a weighted link between each pair of words
corresponding to the number of titles in which both
words occur.
Both of these graphs are then clustered using
Graph Factorization Clustering (GFC). GFC is a soft
clustering algorithm for graphs that models graph
edges as a mixture of latent node-cluster association
variables. (Yu et al., 2006)
Given a word graph G with vertices V and ad-
jacency matrix [w]ij , GFC attempts to fit a bipar-
tite graph K(V,U) with adjacency matrix [b]ij onto
this data, with the m nodes of U representing the
clusters. Whereas in G, similarity between two
words i and j can be measured with wij , we can
similarly measure their similarity in K with w?ij =?m
p=1
bipbjp
?p where ?p =
?n
i=1 bip is the degree of
vertex p ? U .
Essentially the bipartite graph attempts to approx-
imate the transition probability between i and j inG
with the sum of transition probabilities from i to j
through any of the m nodes in U . Yu, et al. (2006)
present an algorithm for minimizing the divergence
distance `(X,Y) =?ij(xijlog xijyij ? xij + yij) be-
tween [w]ij and [w?]ij .
We run GFC with this distance metric and m =
100 clusters on the word graph until convergence
(change in log-likelihood < 0.1%). After conver-
gence, the nodes in U become the clusters and the
weights bip (constrained to sum to 1 for each clus-
ter) become the topic-word association scores.
Examples of some topics found by this method
are shown in Table 5. From manual inspection of
these topics, we found them to be very much like
topics created by statistical topic models. We find
instances of all the types of topics listed in (Mimno
et al., 2011): chained, intruded, random, and unbal-
anced. For an evaluation of these topics see Sec-
tion 4.3.1.
4.2 Entity-topic association
To associate entities with topics, we first create
the heterogeneous network as in previous sections,
adding links between papers and their title words,
along with links between words and the topics that
were discovered in the previous section. Word-topic
links are also weighted according to the weights
7
Word sense induction sense disambiguation word induction unsupervised clustering senses based similarity chinese
CRFs + their applications entity named recognition random conditional fields chinese entities biomedical segmentation
Dependency parsing parsing dependency projective probabilistic incremental deterministic algorithm data syntactic trees
Tagging models tagging model latent markov conditional random parsing unsupervised segmentation
Multi-doc summarization summarization multi document text topic based query extractive focused summaries
Chinese word segmentation word segmentation chinese based alignment character tagging bakeoff model crf
Lexical semantics lexical semantic distributional similarity wordnet resources lexicon acquistion semantics representation
Cross-lingual IR cross lingual retrieval document language linguistic multi person multilingual coreference
Generation for summar. sentence based compression text summarization ordering approach ranking generation
Spoken language speech recognition automatic prosodic tagging spontaneous news broadcast understanding conversational
French function words de la du des le automatique analyse une en pour
Question answering question answering system answer domain retrieval web based open systems
Unsupervised learning unsupervised discovery learning induction knowledge graph acquisition concept clustering pattern
SVMs for NLP support vector machines errors space classification correcting word parsing detecting
MaxEnt models entropy maximum approach based attachment model models phrase prepositional disambiguation
Dialogue systems dialogue spoken systems human conversational multi interaction dialogues utterances multimodal
Semantic role-labeling semantic role labeling parsing syntactic features ill dependency formed framenet
SMT based translation machine statistical phrase english approach learning reordering model
Coreference resolution resolution coreference anaphora reference pronoun ellipsis ambiguity resolving approach pronominal
Semi- and weak-supervision learning supervised semi classification active data clustering approach graph weakly
Information retrieval based retrieval similarity models semantic space model distance measures document
Discourse discourse relations structure rhetorical coherence temporal representation text connectives theory
CFG parsing context free grammars parsing linear probabilistic rewriting grammar systems optimal
Min. risk train. and decod. minimum efficient training error rate translation risk bayes decoding statistical
Phonology phoneme conversion letter phonological grapheme rules applying transliteration syllable sound
Sentiment sentiment opinion reviews classification mining polarity analysis predicting product features
Neural net speech recog. speech robust recognition real network time neural networks language environments
Finite state methods state finite transducers automata weighted translation parsing incremental minimal construction
Mechanical Turk mechanical turk automatic evaluation amazon techniques data articles image scientific
Table 5: Top 10 words for several topics created by the co-occurence random walk topic model. The left
column is a manual label.
Topic 59 Topic 82
translation 0.1953 parsing 0.1715
machine 0.1802 dependency 0.1192
statistical 0.0784 projective 0.0138
Machine Translation 0.0018 K-best Spanning Tree Parsing 0.0025
Better Hypothesis Testing for Statistical
Machine Translation: Controlling for
Optimizer Instability
0.0016 Pseudo-Projective Dependency Parsing 0.0024
Filtering Antonymous, Trend- Contrasting, and
Polarity-Dissimilar Distributional Paraphrases
for Improving Statistical Machine Translation
0.0015 Shift-Reduce Dependency DAG Parsing 0.0017
Knight, Kevin 0.0083 Nivre, Joakim 0.0120
Koehn, Philipp 0.0074 Johnson, Mark 0.0085
Ney, Hermann 0.0072 Nederhof, Mark-Jan 0.0064
RWTH Aachen University 0.0212 Vaxjo University 0.0113
Carnegie Mellon University 0.0183 Brown University 0.0107
University of Southern California 0.0177 University of Amsterdam 0.0094
Workshop on Statistical Machine Translation 0.0590 ACL 0.0512
EMNLP 0.0270 EMNLP 0.0259
COLING 0.0173 CoNLL 0.0223
Table 6: Examples of entities associated with selected topics.
8
determined by GCF. We then simply take random
walks from topics to entities and measure the pro-
portion at which the random walk arrives at each en-
tity of interest. These proportions become the entity-
topic association scores.
For example, if we wanted to find the authors
most associated with topic 12, we would take a num-
ber of random walks (say 50,000) starting at topic
12 and terminating as soon as the random walk first
reaches an author node. Measuring the proportion
at which random walks arrive at each allows us to
compute an association score between topic 12 and
each author.
A common problem in random walks on large
graphs is that the walk can easily get ?lost? between
two nodes that should be very near by taking a just
a few steps in the wrong direction. To keep the ran-
dom walks from taking these wrong steps, we adjust
the topology of the network using directed links to
keep the random walks moving in the ?right? direc-
tion. We design the graph such that if we desire a
random walk from nodes of type s to nodes of type t,
the random walk will never be able to follow an out-
going link that does not decrease its distance from
the nodes of t.
As shown in section 2.3, there are certain nodes at
which a random walk (like Pagerank) arrives at more
often than others simply because of their positions in
the graph. This suggests that there may be stationary
random walk distributions over entities, which we
would need to adjust for in order to find the most
significant entities for a topic.
Indeed this is what we do find. As an example, if
we sample topics uniformly and take random walks
to author nodes, by chance we end up at Jun?ichi
Tsujii on 0.3% of random walks, Eduard Hovy on
0.2% of walks, etc. These values are about 1000
times greater than would be expected at random.
To adjust for this effect, when we take a random
walk from a topic x to an entity type t, we subtract
out this stationary distribution for t, which corre-
sponds to the proportion of random walks that end
at any particular entity of type t by chance, and not
by virtue of the fact that the walk started at topic x.
The resulting distribution yields the entities of t that
are most significantly associated with topic x. Ta-
ble 6 gives examples of the most significant entities
for a couple of topics.
?200 ?150 ?100 ?50
RW-cooc
RW-sim
RTM
LDA
Coherence
Figure 2: Distribution of topic coherences for the
four topic models.
4.3 Topic Model Evaluation
We provide two separate evaluations in this section,
one of the topics alone, and one extrinstic evaluation
of the entire paper-topic model. The variants of ran-
dom walk topic models are compared against LDA
and the relational topic model (RTM), each with 100
topics (Chang and Blei, 2010). As RTM allows only
a single type of relationship between documents, we
use citations as the inter-document relationships.
4.3.1 Topic Coherence
The coherence of a topic is evaluated using the co-
herence metric introduced in (Mimno et al., 2011).
Given the top M words V (t) = (v(t)1 , ..., v(t)M ) for a
topic t, the coherence of that topic can be calculated
with the following formula:
C(t;V (t)) =
M?
m=2
m?1?
l=1
log
(
D(v(t)m , v(t)l ) + 1
D(v(t)l )
)
,
where D(v) is the number of documents contain-
ing v and D(v, v?) is the number of documents con-
taining both v and v?.
This measure of coherence is highly correlated
with manual annotations of topic quality, with a
higher coherence score corresponding to a more co-
herent, higher quality topic. After calculating the co-
herence for each of the 100 topics for RTM and the
random-walk topic model, the average coherence for
RTM topics was -135.2 and the average coherence
for word-similarity random walk topics was -122.2,
with statistical significance at p < 0.01. Figure 2
demonstrates this, showing that the word similarity-
based random walk method generates several highly
coherent topics. The average coherence for the LDA
and the co-occurence random walk model were sig-
nificantly lower.
9
4.3.2 Extrinsic Evaluation
One difficulty in evaluating this random-walk
topic model intrinsically against a statistical topic
model like RTM is that existing evaluation measures
assume certain statistical properties of the topic, for
example, that the topics are generated according to a
Dirichlet prior. Because of this, we choose instead to
evaluate this topic model extrinsically with a down-
stream application. We choose an information re-
trieval application, returning a ranked list of similar
documents, given a reference document.
We evaluate five different methods: citation-
RTM, LDA, the two versions of the random-walk
topic model, and a simple word vector similarity
baseline. Similarity between documents with the
topic models are determined by cosine similarity be-
tween the topic vectors of the two documents. Word
vector similarity determines the similarity between
documents by taking the cosine similarity of their
word vectors. From these similarity scores, a ranked
list is produced.
The document set for this task is the set of all pa-
pers appearing at ACL between 2000 and 2011. The
top 10 results returned by each method are pooled
and manually evaluated with a relevance score be-
tween 1 and 10. Thirty such result sets were manu-
ally annotated. We then evaluate each method ac-
cording to its discounted cumulative gain (DCG)
(Ja?rvelin and Keka?la?inen, 2000).
Performance of these methods is summarized in
Table 7. The co-occurence-based random walk topic
model performed comparably with the best per-
former at this task, LDA, and there was no signifi-
cant difference between the two at p < 0.05.
Going forward, an important problem is to rec-
oncile the co-occurence- and word-similarity-based
formulations of this topic model, as the two formu-
lations perform very differently in our two evalua-
tions. Heuristically, the co-occurence model seems
to create good human-readable topics, while the
word-similarity model creates topics that are more
mathematically-coherent, but less human-readable.
5 Related Work
Heterogeneous networks have been studied in a
number of different fields, such as biology (Sio-
son, 2005), transportation networks (Lozano and
Method DCG
Word vector 1.345 ? 0.007
LDA 3.302 ? 0.008
RTM 3.058 ? 0.011
Random-walk (cooc) 3.295 ? 0.006
Random-walk (sim) 2.761 ? 0.007
Table 7: DCG Performance of the various topic
models and baselines on the related document find-
ing task. A 95% confidence interval is provided.
Storchi, 2002), social networks (Lambiotte and Aus-
loos, 2006), and bibliographic networks (Sun et al.,
2011). These networks are also sometimes known
by the name complex networks or multimodal net-
works, but both these terms have other connotations.
We prefer ?heterogeneous networks? as used by Sun
et al. (2009).
There has also been some study of these networks
in general, in community detection (Murata, 2010),
clustering (Long et al., 2008; Sun et al., 2012), and
data mining (Muthukrishnan et al., 2010), but there
has not yet been any comprehensive study. Recently,
NLP has seen several uses of heterogeneous net-
works (though not by that name) for use with label
propagation algorithms (Das and Petrov, 2011; Spe-
riosu et al., 2011) and random walks (Toutanova et
al., 2004; Kok and Brockett, 2010).
Several authors have proposed the idea of using
network centrality measures to rank the impacts of
journals, authors, papers, etc. (Bollen et al., 2006;
Bergstrom et al., 2008; Chen et al., 2007; Liu et al.,
2005), and it has even been proposed that central-
ity can be applicable in bipartite networks (Zhou et
al., 2007). We propose that Pagerank on any gen-
eral heterogeneous network is appropriate for creat-
ing ranked lists for each type of entity. Most previ-
ous papers also lack a robust evaluation, demonstrat-
ing agreement with previous methods or with some
external awards or recognitions. We use a random
graph that replicates the properties of the real-world
network to show that Pagerank on the heterogeneous
network outperforms other methods.
Name disambiguation has been studied in a num-
ber of different settings, including graph-based set-
tings. It is common to use the coauthorship graph
(Kang et al., 2009; Fan et al., 2011), but authors
10
have also used lexical similarity graphs (On and Lee,
2007), citation graphs (McRae-Spencer and Shad-
bolt, 2006), or social networks (Malin, 2005). Al-
most all graph methods are unsupervised.
There have been some topic models developed
specifically for relational data (Wang et al., 2006;
Airoldi et al., 2008), but both of these models have
limitations in the types of relational data they are
able to model. The group topic model described in
(Wang et al., 2006) is able to create stronger topics
by considering associations between words, events,
and entities, but is very coarse in the way it han-
dles the behavior of entities, and does not generalize
to multiple different types of entities. The stochas-
tic blockmodel of (Airoldi et al., 2008) can create
blocks of similar entities in a graph and is general
in the types of graphs it can handle, but produces
less meaningful results on graphs that have specific
schemas.
6 Conclusion and Future Directions
In this paper, we present a heterogeneous net-
work treatment of the ACL Anthology Network and
demonstrate several applications of it. Using only
off-the-shelf graph algorithms with a single data rep-
resentation, the heterogeneous AAN, we are able to
very easily build a scientific impact measure that is
more accurate than existing measures, an author dis-
ambiguation system better than existing graph-based
author disambiguation systems, and a random-walk-
based topic model that is competitive with statistical
topic models.
While there are many other tasks, such as citation-
based summarization, that could likely be ap-
proached using this framework with the appropri-
ate addition of new types of nodes into the hetero-
geneous AAN network, there are even some poten-
tial synergies between the tasks described in this pa-
per that have yet to be explored. For example, we
may consider that the methods of the author disam-
biguation or topic modeling tasks could be to find
the highest-impact papers associated with a term (for
survey generation, perhaps) or high-impact authors
associated with a workshop?s topic (to select good
reviewers for it). We believe that heterogeneous
graphs are a flexible framework that will allow re-
searchers to find simple, flexible solutions for a va-
riety of problems.
Acknowledgments
This research is supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of
Interior National Business Center (DoI/NBC) contract
number D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions con-
tained herein are those of the authors and should not be
interpreted as necessarily representing the official poli-
cies or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
References
Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg,
and Eric P. Xing. 2008. Mixed membership stochastic
blockmodels. The Journal of Machine Learning Re-
search, 9:1981?2014.
Re?ka Albert and Albert-La?szlo? Baraba?si. 2002. Statisti-
cal mechanics of complex networks. Reviews of mod-
ern physics, 74(1):47.
A.L. Baraba?si and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512.
Carl T. Bergstrom, Jevin D. West, and Marc A. Wiseman.
2008. The eigenfactor metrics. The Journal of Neuro-
science, 28(45):11433?11434.
Steven Bird, Robert Dale, Bonnie J Dorr, Bryan Gib-
son, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir R Radev, and Yee Fan Tan. 2008.
The ACL anthology reference corpus: A reference
dataset for bibliographic research in computational lin-
guistics. In Proc. of the 6th International Conference
on Language Resources and Evaluation Conference
(LREC08), pages 1755?1759.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. the Journal of machine Learning
research, 3:993?1022.
Johan Bollen, Marko A. Rodriguez, and Herbert Van
de Sompel. 2006. Journal status. CoRR,
abs/cs/0601030.
Johan Bollen, Herbert Van de Sompel, Aric Hagberg, and
Ryan Chute. 2009. A principal component analysis of
39 scientific impact measures. PloS one, 4(6):e6022.
Tibor Braun, Wolfgang Gla?nzel, and Andra?s Schubert.
2006. A hirsch-type index for journals. Scientomet-
rics, 69(1):169?173.
Jonathan Chang and David M Blei. 2010. Hierarchical
relational models for document networks. The Annals
of Applied Statistics, 4(1):124?150.
11
Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner.
2007. Finding scientific gems with googles pagerank
algorithm. Journal of Informetrics, 1(1):8?15.
Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ
Newman. 2009. Power-law distributions in empirical
data. SIAM review, 51(4):661?703.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
Paul Erdo?s and Alfre?d Re?nyi. 1960. On the evolution of
random graphs. Magyar Tud. Akad. Mat. Kutato? Int.
Ko?zl, 5:17?61.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Xiaoming Fan, Jianyong Wang, Xu Pu, Lizhu Zhou, and
Bing Lv. 2011. On graph-based name disambigua-
tion. J. Data and Information Quality, 2(2):10:1?
10:23, February.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In Working Notes of the AAAI
Fall Symposium on Probabilistic Approaches to Natu-
ral Language, volume 54, page 60.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
363?371. ACL.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Extracting signed social networks from text.
TextGraphs-7, page 6.
Jorge E. Hirsch. 2005. An index to quantify an indi-
vidual?s scientific research output. Proceedings of the
National Academy of Sciences of the United states of
America, 102(46):16569.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 50?57. ACM.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2000. IR evalua-
tion methods for retrieving highly relevant documents.
In Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 41?48. ACM.
In-Su Kang, Seung-Hoon Na, Seungwoo Lee, Hanmin
Jung, Pyung Kim, Won-Kyung Sung, and Jong-Hyeok
Lee. 2009. On co-authorship for author disam-
biguation. Information Processing & Management,
45(1):84?97.
Brian Karrer and Mark EJ Newman. 2011. Stochas-
tic blockmodels and community structure in networks.
Physical Review E, 83(1):016107.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 145?153. ACL.
Oren Kurland and Lillian Lee. 2005. Pagerank without
hyperlinks: Structural reranking using links induced
by language models. In SIGIR ?05.
Renaud Lambiotte and Marcel Ausloos. 2006. Collabo-
rative tagging as a tripartite network. Computational
Science?ICCS 2006, pages 1114?1117.
David Liben-Nowell and Jon Kleinberg. 2007. The link-
prediction problem for social networks. Journal of the
American society for information science and technol-
ogy, 58(7):1019?1031.
R.N. Lichtenwalter, J.T. Lussier, and N.V. Chawla. 2010.
New perspectives and methods in link prediction. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 243?252. ACM.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. Infor-
mation processing & management, 41(6):1462?1480.
Bo Long, Zhongfei Zhang, and Tianbing Xu. 2008.
Clustering on complex graphs. In Proc. the 23rd Conf.
AAAI 2008.
Angelica Lozano and Giovanni Storchi. 2002. Shortest
viable hyperpath in multimodal networks. Transporta-
tion Research Part B: Methodological, 36(10):853?
874.
Bradley Malin. 2005. Unsupervised name disambigua-
tion via social network similarity. In Workshop on
Link Analysis, Counterterrorism, and Security, vol-
ume 1401, pages 93?102.
Sergei Maslov and Sidney Redner. 2008. Promise
and pitfalls of extending google?s pagerank algorithm
to citation networks. The Journal of Neuroscience,
28(44):11103?11105.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
Duncan M. McRae-Spencer and Nigel R. Shadbolt.
2006. Also by the same author: Aktiveauthor, a cita-
tion graph approach to name disambiguation. In Pro-
ceedings of the 6th ACM/IEEE-CS joint conference on
Digital libraries, pages 53?54. ACM.
12
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
HLT-EMNLP, pages 411?418. ACL.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 262?272. ACL.
Panchanan Mitra. 2006. Hirsch-type indices for rank-
ing institutions scientific research output. Current Sci-
ence, 91(11):1439.
Tsuyoshi Murata. 2010. Detecting communities from
tripartite networks. In Proceedings of the 19th inter-
national conference on World wide web, pages 1159?
1160. ACM.
Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu
Mei. 2010. Edge weight regularization over mul-
tiple graphs for similarity learning. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on,
pages 374?383. IEEE.
Mark E.J. Newman and Juyong Park. 2003. Why social
networks are different from other types of networks.
Physical Review E, 68(3):036122.
Byung-Won On and Dongwon Lee. 2007. Scalable name
disambiguation using multi-level graph partition. In
Proceedings of the 7th SIAM international conference
on data mining, pages 575?580.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web.
Romualdo Pastor-Satorras and Alessandro Vespignani.
2001. Epidemic spreading in scale-free networks.
Physical review letters, 86(14):3200?3203.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The ACL
anthology network corpus. Language Resources and
Evaluation, pages 1?26.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical association, 66(336):846?850.
S. Redner. 1998. How popular is your paper? an empir-
ical study of the citation distribution. The European
Physical Journal B-Condensed Matter and Complex
Systems, 4(2):131?134.
P. Sarkar, A.W. Moore, and A. Prakash. 2008. Fast incre-
mental proximity search in large graphs. In Proceed-
ings of the 25th international conference on Machine
learning, pages 896?903. ACM.
Allan A. Sioson. 2005. Multimodal networks in biology.
Ph.D. thesis, Virginia Polytechnic Institute and State
University.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 53?63, Edin-
burgh, Scotland, July. ACL.
Alexander Strehl and Joydeep Ghosh. 2003. Cluster
ensembles?a knowledge reuse framework for com-
bining multiple partitions. The Journal of Machine
Learning Research, 3:583?617.
Yizhou Sun, Jiawei Han, Peixiang Zhao, Zhijun Yin,
Hong Cheng, and Tianyi Wu. 2009. Rankclus: inte-
grating clustering with ranking for heterogeneous in-
formation network analysis. In Proceedings of the
12th International Conference on Extending Database
Technology: Advances in Database Technology, pages
565?576. ACM.
Yizhou Sun, Rick Barber, Manish Gupta, and Jiawei Han.
2011. Co-author relationship prediction in heteroge-
neous bibliographic networks.
Yizhou Sun, Charu C. Aggarwal, and Jiawei Han. 2012.
Relation strength-aware clustering of heterogeneous
information networks with incomplete attributes. Pro-
ceedings of the VLDB Endowment, 5(5):394?405.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Neural Information Processing Systems, volume 15.
Robert Tibshirani, Guenther Walther, and Trevor Hastie.
2001. Estimating the number of clusters in a data
set via the gap statistic. Journal of the Royal Sta-
tistical Society: Series B (Statistical Methodology),
63(2):411?423.
Kristina Toutanova, Christopher D Manning, and An-
drew Y Ng. 2004. Learning random walk models
for inducing word dependency distributions. In Pro-
ceedings of the twenty-first international conference
on Machine learning, page 103. ACM.
Xuerui Wang, Natasha Mohanty, and Andrew McCallum.
2006. Group and topic discovery from relations and
their attributes. Technical report, DTIC Document.
Kai Yu, Shipeng Yu, and Volker Tresp. 2006. Soft
clustering on graphs. Advances in Neural Information
Processing Systems, 18:1553.
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, and
C. Lee Giles. 2007. Co-ranking authors and docu-
ments in a heterogeneous network. In Data Mining,
2007. ICDM 2007. Seventh IEEE International Con-
ference on, pages 739?744. IEEE.
13
14
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?88,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Experimental Results on the Native Language Identification Shared Task
Amjad Abu-Jbara, Rahul Jha, Eric Morley, Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
[amjbara, rahuljha, eamorley, radev]@umich.edu
Abstract
We present a system for automatically iden-
tifying the native language of a writer. We
experiment with a large set of features and
train them on a corpus of 9,900 essays writ-
ten in English by speakers of 11 different lan-
guages. our system achieved an accuracy of
43% on the test data, improved to 63% with
improved feature normalization. In this paper,
we present the features used in our system, de-
scribe our experiments and provide an analysis
of our results.
1 Introduction
The task of Native Language Identification (NLI)
is the task of identifying the native language of a
writer or a speaker by analyzing their writing in
English. Previous work in this area shows that
there are several linguistic cues that can be used
to do such identification. Based on their native
language, different speakers tend to make different
kinds of errors pertaining to spelling, punctuation,
and grammar (Garfield, 1964; Wong and Dras, 2009;
Kochmar, 2011). We describe the complete set of
features we considered in Section 4. We evaluate
different combinations of these features, and differ-
ent ways of normalizing them in Section 5.
There are many possible applications for an NLI
system, as noted by Kochmar (2011): finding the
origins of anonymous text; error correction in var-
ious tasks including speech recognition, part-of-
speech tagging, and parsing; and in the field of sec-
ond language acquisition for identifying learner dif-
ficulties. We are most interested in statistical ap-
proaches to this problem because it may point to-
wards fruitful avenues of research in language and
sound transfer, which are how people apply knowl-
edge of their native language, and its phonology
and orthography, respectively, to a second language.
For example, Tsur and Rappoport (2007) found that
character bigrams are quite useful for NLI, which
led them to suggest that second language learners?
word choice may in part be driven by their native
languages. Analysis of such language and sound
translation patterns might be useful in understand-
ing the process of language acquisition in humans.
2 Previous Work
The work presented in this paper was done as part
of the NLI shared task (Tetreault et al, 2013), which
is the first time this problem has been the subject
of a shared task. However, several researchers have
investigated NLI and similar problems. Authorship
attribution, a related problem, has been well stud-
ied in the literature, starting from the seminal work
on disputed Federalist Papers by Mosteller and Wal-
lace (1964). The goal of authorship attribution is
to assign a text to one author from a candidate set
82
of authors. This technique has many applications,
and has recently been used to investigate terrorist
communication (Abbasi and Chen, 2005) and dig-
ital crime (Chaski, 2005). The goal of NLI some-
what similar to authorship attribution, in that NLI
attempts to distinguish between candidate commu-
nities of people who share a common cultural and
linguistic background, while authorship attribution
distinguishes between candidate individuals.
In the earliest treatment of this problem, Koppel
et al (2005) used stylistic text features to identify
the native language of an author. They used features
based on function words, character n-grams and er-
rors and idiosyncrasies such as spelling errors and
non-standard syntactic constructions. They exper-
imented on a dataset with essays written by non-
native English speakers from five countries, Russia,
Czech Republic, Bulgaria, France and Spain, with
258 instances from each dataset. They trained a
multi-class SVM model using the above features and
reported 10-fold cross validation accuracy of 80.2%.
Tsur and Rappoport (2007) studied the problem
of NLI with a focus on language transfer, i.e. how
a seaker?s native language affects the way in which
they acquire a second language, an important area in
Second Language Acquisition research. Their fea-
ture analysis showed that character bigrams alone
can lead to a classification accuracy of about 66%
in a 5-class task. They concluded that the choice of
words people make when writing in a second lan-
guage is highly influenced by the phonology of their
native language.
Wong and Dras (2009) studied syntactic errors de-
rived from contrastive analysis as features for NLI.
They used the five languages from Koppel et al
(2008) along with Chinese and Japanese, but did not
find an improvement in classification accuracy by
adding error features based on contrastive analysis.
Later, Wong and Dras (2011) studied a more gen-
eral set of syntactic features and showed that adding
these features improved the accuracy significantly.
They also investigated classification models based
on LDA (Wong et al, 2011), but did not find them
to be useful overall. They did, however, notice that
some of the topics were capturing information that
would be useful for identifying particular native lan-
guages. They also proposed the use of adaptor gram-
mars (Johnson et al, 2007), which are a generaliza-
tion of probabilistic context-free grammars, to cap-
ture collocational pairings. In a later paper, Wong
et al explored the use of adapter grammars in de-
tail (Wong et al, 2012) and showed that an exten-
sion of adaptor grammars to discover collocations
beyond lexical words can produce features useful for
the NLI task.
3 Dataset
The experiments for this paper were performed us-
ing the TOEFL11 dataset (Blanchard et al, 2013)
provided as part of the shared task. The dataset con-
tains essays written in English from native speakers
of 11 languages (Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish). The corpus contains 12,099 essays per
language sampled evenly from 8 prompts or topics.
This dataset was designed specifically to support the
task of NLI and addresses some of the shortcom-
ings of earlier datasets used for research in this area.
Specifically, the dataset has been carefully selected
in order to maintain consistency in topic distribu-
tions, character encodings and annotations across
the essays from different native languages. The data
was split into three data sets: a training set com-
prising 9,900 essays, a development set comprising
1,100 essays, and a test set comprising 1,100 essays.
4 Approach
We addressed the problem as a supervised, multi-
class classification task. We trained a Support Vector
Machine (SVM) classifier on a set of lexical, syntac-
tic and dependency features extracted from the train-
ing data. We computed the minimum and maximum
values for each of the features and normalized the
values by the range (max - min). Here we describe
the features in turn.
83
Character and Word N-grams Tsur and Rap-
poport (2007) found that character bigrams were
useful for NLI, and they suggested that this may be
due to the writer?s native language influencing their
choice of words. To reflect this, we compute features
using both characters and word N-grams. For char-
acters, we consider 2,3 and 4-grams, with padding
characters at the beginning and end of each sentence.
The features are generated over the entire training
data, i.e., every n-gram occurring in the training data
is used as a feature. Similarly, we create features
with 1,2 and 3-grams of words. Each word n-gram
is used as a separate feature. We explore both binary
features for each character or word n-gram, as well
as normalized count features.
Part-Of-Speech N-grams Several investigations,
for example those conducted by Kochmar (2011)
and Wong and Dras (2011), have found that part-of-
speech tags can be useful for NLI. Therefore we in-
clude part-of-speech (POS) n-grams as features. We
parse the sentences with the Stanford Parser (Klein
and Manning, 2003) and extract the POS tags. We
use binary features describing the presence or ab-
sence of POS bigrams in a document, as well as nu-
merical features describing their relative frequency
in a document.
Function Words Koppel et al (2005) found that
function words can help identify someone?s native
language. To this end, we include a categorical fea-
ture for the presence of function words that are in-
cluded in list of 321 function words.
Use of punctuation Based on our experience
with speakers of native languages, as well as
Kochmar?s (2011) observations of written English
produced by Germanic and Romance language
speakers, we suspect that speakers of different native
languages use punctuation in different ways, pre-
sumably based on the punctuation patterns in their
native language. For example, comma placement
differs between German and English, and neither
Chinese nor Japanese requires a full stop at the end
of every sentence. To capture these kinds of patterns,
we create two features for each essay: the number of
punctuation marks used per sentence, and the num-
ber of punctuation marks used per word.
Number of Unique Stems Speakers of different
native languages might differ in the amount of vo-
cabulary they use when communicating in English.
We capture this by counting the number of unique
stems in each essay and using this as an additional
feature. The hypothesis here is that depending on the
similarity of the native language with English, the
presence of common words, and other cultural cues,
people with different native language might have ac-
cess to different amounts of vocabulary.
Misuse of Articles We count instances in which
the number of an article is inconsistent with the as-
sociated noun. To do so, we fist identify all the det
dependency relations in the essay. We then com-
pute the ratio of det relations between ?a? or ?an?
and a plural noun (NNS), to all det relations. We
also count the ratio of det relations between ?a? or
?an? and an uncountable noun, to all det relations.
We do this using a list of 288 uncountable nouns.1
Capitalization The writing systems of some lan-
guages in the data set, for example Telugu, do not
include capitalization. Furthermore, other languages
may use capitalization quite differently from En-
glish, for example German, in which all nouns are
capitalized, and French, in which nationalities are
not. Character capitalization mistakes may be com-
mon in the text written by the speakers of such lan-
guages. We compute the ratio of words with at least
two letters that are written in all caps to identify ex-
cessive capitalization. We also count the relative fre-
quency of capitalized words that appear in the mid-
dle of a sentence that are not tagged as proper nouns
by the part of speech tagger.
Tense and Aspect Frequency Verbal tense and
aspect systems vary widely between languages. En-
glish has obligatory tense (past, present, future) and
1http://www.englishclub.com/vocabulary/nouns-
uncountable-list.htm
84
aspect (imperfect, perfect, progressive) marking on
verbs. Other languages, for example French, may
require verbs to be marked for tense, but not as-
pect. Still other languages, for example Chinese,
may use adverbials and temporal phrases to com-
municate temporal and aspectual information. To
attempt to capture some of the ways learners of En-
glish may be influenced by their native language?s
system of tense and aspect, we compute two fea-
tures. First, we compute the relative frequency of
each tense and aspect in the article from the counts
of each verb POS tags (ex. VB, VBD, VBG). We
also compute the percentage of sentences that con-
tain verbs of different tenses or aspect, again using
the verb POS tags.
Missing Punctuation We compute the relative
frequency of sentences that include an introductory
phrase (e.g. however, furthermore, moreover) that is
not followed by a comma. We also count the relative
frequency of sentences that start with a subordinat-
ing conjunction (e.g. sentences starting with if, after,
before, when, even though, etc.), but do not contain
a comma.
Average Number of Syllables We count the num-
ber of syllables per word and the ratio of words with
three or more syllables. To count the number of syl-
lables in a word, we used a perl module that esti-
mates the number of syllables by applying a set of
hand-crafted rules.2.
Arc Length We calculate several features pertain-
ing to dependency arc length and direction. We
parse each sentence separately, using the Stanford
Dependency Parser, and then compute a single value
for each of these features for each document. First,
we simply compute the percentage of arcs that point
left or right (PCTARCL, PCTARCR). We also com-
pute the minumum, maximum, and mean depen-
dency arc length, ignoring arc direction. We also
compute similar features for typed dependencies:
the minimum, maximum, and mean dependency arc
2http://search.cpan.org/dist/Lingua-EN-
Syllable/Syllable.pm
length for each typed dependency; and the percent-
age of arcs for each typed dependency that go to the
left or right.
Downtoners and Intensifiers We compute three
features to describe the use of downtoners, and three
for intensifiers in each document. First, we count the
number of downtoners or intensifiers in a given doc-
ument.3 We normalize this count by the number of
tokens, types, and sentences in the document to yield
the three features capturing the use of downtoners or
intensifiers.
Production Rules We compute a set of features to
describe the relative frequency of production rules
in a given document. First, we parse each sentence
using the Stanford Parser, using the default English
PCFG (Klein and Manning, 2003). We then count
all non-terminal production rules in a given docu-
ment, and report the relative frequency of each pro-
duction rule in that document.
Subject Agreement We count the number of sen-
tences in which there appears to be a mistake in sub-
ject agreement. To do this, we first identify nsubj
and nsubjpass dependency relationships. Of these
dependencies, we count ones meeting the following
criteria as mistakes: a third person singular present
tense verb with a nominal that is not third person
singular, and a third person singular subject with a
present tense verb not marked as third person sin-
gular. We then normalize the count of errors by the
total number of nsubj and nsubj pass dependencies
in the document, and the number of sentences in the
document to produce two features.
Words per Sentence We compute both the num-
ber of tokens per line and the number of types per
3The words we count as downtoners are: ?almost?, ?alot?,
?a lot?, ?barely?, ?a bit?, ?fairly?, ?hardly?, ?just?, ?kind of?,
?least?, ?less?, ?merely?, ?mildly?, ?nearly?, ?only?, ?partially?,
?partly?, ?practically?, ?rather?, ?scarcely?, ?sort of?, ?slightly?,
and ?somewhat?. The intensifiers are: ?a good deal?, ?a great
deal?, ?absolutely?, ?altogether?, ?completely?,?enormously?,
?entirely?, ?extremely?, ?fully?, ?greatly?, ?highly?, ?intensely?,
?more?, ?most?, ?perfectly?, ?quite?, ?really?, ?so?, ?strongly?,
?super?, ?thoroughly?, ?too?, ?totally?, ?utterly?, and ?very?.
85
line.
Topic Scores We construct an unsupervised topic
model for all of the documents using Mallet (Mc-
Callum, 2002) with 100 topics, dirichlet hyperpa-
rameter reestimation every 10 rounds, and all other
options set to default values. We then use the topic
weights as features.
Passive Constructions We count the number of
times an author uses passive constructions by count-
ing the number of nsubjpass dependencies in each
document. We normalize this count in two ways to
produce two different features: dividing by the num-
ber of sentences, and dividing by the total number of
nsubj and nsubjpass dependencies.
5 Experiments and Results
We used weka (Hall et al, 2009) and libsvm (Chang
and Lin, 2011) to run the experiments. The classi-
fication was done using an SVM classifier. We ex-
perimented with different SVM kernels and different
values for the cost parameter. The best performance
was achieved with a linear kernel and cost = 0.001.
We trained the model using the combination of the
training and the development sets. We submitted the
output of the system to the NLI shared task work-
shop. Our system achieved 43.3% accuracy. Table 1
shows the confusion matrix and the precision, recall,
and F-measure for each language. After the NLI
submission deadline, we noticed that we our system
was not handling the normalization of the features
properly which resulted in the poor performance.
After fixing the problem, our system achieved 63%
accuracy on both test data and 10-fold cross valida-
tion on the entire data.
6 Analysis
We did feature analysis on the training and devel-
opment data sets using the Chi-squared test. Our
feature analysis shows that the most important fea-
tures for the classifier were topic models, charac-
ter n-grams of all orders, word unigrams and bi-
grams, POS bigrams, capitalization features, func-
tion words, production rules, and arc length. These
results are consistent with those presented in previ-
ous work done on this task.
Looking at the confusion matrix in Figure 1, we
see that Korean and Japanese were the most com-
monly confused pair of languages. Hindi and Tel-
ugu, two languages from the Indian subcontinent,
were also often confused. To analyze this further,
we did another experiment by training just a binary
classifier on Korean and Japanese using the exact
same feature set as earlier. We achieved a 10-fold
cross validation accuracy of 83.3% on this classifi-
cation task. Thus, given just these two languages,
we were able to obtain high classification accuracy.
This suggests that a potentially fruitful strategy for
NLI systems might be to fuse often-confused pairs,
such as Korean/Japanese and Hindi/Telugu, into sin-
gleton classes for the initial run, and then run a sec-
ond classifier to do a more fine grained classification
within these higher level classes.
When doing feature analysis for these two lan-
guages, we found that the character bigrams rep-
resenting the country names were some of the top
features used for classification. For example ?Kor?
occurred as a trigram frequently in essays from na-
tive language speakers of Korean. Based on this, we
designed a small experiment where we created fea-
tures corresponding to the country name associated
with each native language, e.g., ?Korea?, ?China?,
?India?, ?France?, etc. For Arabic, we used a list of
22 countries where Arabic is spoken. Just using this
feature, we obtained a 10-fold cross validation accu-
racy of 21.3% on the development set. This suggests
that in certain genres, one may be able to leverage in-
formation conveying geographical and demographic
attributes for NLI.
7 Conclusion
In this paper, we presented a supervised system for
the task of Native Language Identification. We de-
scribe and motivate several features for this task
and report results of supervised classification using
these features on a test data set consisting of 11 lan-
86
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 41 7 8 3 6 2 3 5 10 7 8 44.6% 41.0% 42.7%
CHI 6 38 5 2 2 8 15 8 3 3 10 40.0% 38.0% 39.0%
FRE 8 6 43 8 1 14 2 4 6 1 7 39.1% 43.0% 41.0%
GER 3 3 10 49 4 9 1 7 6 0 8 54.4% 49.0% 51.6%
HIN 5 2 6 9 34 0 3 1 3 32 5 47.9% 34.0% 39.8%
ITA 5 3 10 5 1 52 2 1 17 0 4 46.0% 52.0% 48.8%
JPN 3 11 0 1 1 3 49 26 1 1 4 37.4% 49.0% 42.4%
KOR 2 6 6 1 1 2 35 40 1 1 5 38.1% 40.0% 39.0%
SPA 4 6 14 1 1 17 6 2 38 0 11 40.9% 38.0% 39.4%
TEL 9 7 3 4 18 2 2 2 2 48 3 51.1% 48.0% 49.5%
TUR 6 6 5 7 2 4 13 9 6 1 41 38.7% 41.0% 39.8%
Accuracy = 43.0%
Table 1: The results of our original submission to the NLI shared task on the test set. These results reflect the
performance of the system that does not normalize the features properly
guages provided as part of the NLI shared task. We
found that our classifier often confused two pairs
of languages that are spoken near one another, but
are linguistically unrelated: Hindi/Telugu and Ko-
rean/Japanese. We found that we could obtain high
classification accuracy on these two pairs of lan-
guages using a binary classifier trained on just these
pairs. During our feature analysis, we also found
that certain features that happened to convey geo-
graphical and demographic information were also
informative for this task.
References
Ahmed Abbasi and Hsinchun Chen. 2005. Apply-
ing authorship analysis to extremist-group web fo-
rum messages. IEEE Intelligent Systems, 20(5):67?75,
September.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Carole E. Chaski. 2005. Who?s at the keyboard: Au-
thorship attribution in digital evidence investigations.
International Journal of Digital Evidence, 4:2005.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
Advances in neural information processing systems,
19:641.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Ekaterina Kochmar. 2011. Identification of a Writer?s
Native Langauge by Error Analysis. Ph.D. thesis.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2008. Computational methods in authorship attribu-
tion. Journal of the American Society for information
Science and Technology, 60(1):9?26.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Frederick Mosteller and David L. Wallace. 1964. Infer-
ence and Disputed Authorship: The Federalist Papers.
Addison-Wesley, Reading, Mass.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
87
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, CACLA ?07, pages
9?16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
88
