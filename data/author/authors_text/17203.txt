First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385?393,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
Stanford, CA 94305, USA
danielcer@stanford.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
mdiab@ccls.columbia.edu
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
Abstract
Semantic Textual Similarity (STS) measures
the degree of semantic equivalence between
two texts. This paper presents the results of
the STS pilot task in Semeval. The training
data contained 2000 sentence pairs from pre-
viously existing paraphrase datasets and ma-
chine translation evaluation resources. The
test data also comprised 2000 sentences pairs
for those datasets, plus two surprise datasets
with 400 pairs from a different machine trans-
lation evaluation corpus and 750 pairs from a
lexical resource mapping exercise. The sim-
ilarity of pairs of sentences was rated on a
0-5 scale (low to high similarity) by human
judges using Amazon Mechanical Turk, with
high Pearson correlation scores, around 90%.
35 teams participated in the task, submitting
88 runs. The best results scored a Pearson
correlation>80%, well above a simple lexical
baseline that only scored a 31% correlation.
This pilot task opens an exciting way ahead,
although there are still open issues, specially
the evaluation metric.
1 Introduction
Semantic Textual Similarity (STS) measures the
degree of semantic equivalence between two sen-
tences. STS is related to both Textual Entailment
(TE) and Paraphrase (PARA). STS is more directly
applicable in a number of NLP tasks than TE and
PARA such as Machine Translation and evaluation,
Summarization, Machine Reading, Deep Question
Answering, etc. STS differs from TE in as much as
it assumes symmetric graded equivalence between
the pair of textual snippets. In the case of TE the
equivalence is directional, e.g. a car is a vehicle, but
a vehicle is not necessarily a car. Additionally, STS
differs from both TE and PARA in that, rather than
being a binary yes/no decision (e.g. a vehicle is not a
car), STS incorporates the notion of graded semantic
similarity (e.g. a vehicle and a car are more similar
than a wave and a car).
STS provides a unified framework that allows for
an extrinsic evaluation of multiple semantic compo-
nents that otherwise have tended to be evaluated in-
dependently and without broad characterization of
their impact on NLP applications. Such components
include word sense disambiguation and induction,
lexical substitution, semantic role labeling, multi-
word expression detection and handling, anaphora
and coreference resolution, time and date resolution,
named-entity handling, underspecification, hedging,
semantic scoping and discourse analysis. Though
not in the scope of the current pilot task, we plan to
explore building an open source toolkit for integrat-
ing and applying diverse linguistic analysis modules
to the STS task.
While the characterization of STS is still prelim-
inary, we observed that there was no comparable
existing dataset extensively annotated for pairwise
semantic sentence similarity. We approached the
construction of the first STS dataset with the fol-
lowing goals: (1) To set a definition of STS as a
graded notion which can be easily communicated to
non-expert annotators beyond the likert-scale; (2) To
gather a substantial amount of sentence pairs from
diverse datasets, and to annotate them with high
quality; (3) To explore evaluation measures for STS;
(4) To explore the relation of STS to PARA and Ma-
chine Translation Evaluation exercises.
385
In the next section we present the various sources
of the STS data and the annotation procedure used.
Section 4 investigates the evaluation of STS sys-
tems. Section 5 summarizes the resources and tools
used by participant systems. Finally, Section 6
draws the conclusions.
2 Source Datasets
Datasets for STS are scarce. Existing datasets in-
clude (Li et al, 2006) and (Lee et al, 2005). The
first dataset includes 65 sentence pairs which cor-
respond to the dictionary definitions for the 65
word pairs in Similarity(Rubenstein and Goode-
nough, 1965). The authors asked human informants
to assess the meaning of the sentence pairs on a
scale from 0.0 (minimum similarity) to 4.0 (maxi-
mum similarity). While the dataset is very relevant
to STS, it is too small to train, develop and test typ-
ical machine learning based systems. The second
dataset comprises 50 documents on news, ranging
from 51 to 126 words. Subjects were asked to judge
the similarity of document pairs on a five-point scale
(with 1.0 indicating ?highly unrelated? and 5.0 indi-
cating ?highly related?). This second dataset com-
prises a larger number of document pairs, but it goes
beyond sentence similarity into textual similarity.
When constructing our datasets, gathering natu-
rally occurring pairs of sentences with different de-
grees of semantic equivalence was a challenge in it-
self. If we took pairs of sentences at random, the
vast majority of them would be totally unrelated, and
only a very small fragment would show some sort of
semantic equivalence. Accordingly, we investigated
reusing a collection of existing datasets from tasks
that are related to STS.
We first studied the pairs of text from the Recog-
nizing TE challenge. The first editions of the chal-
lenge included pairs of sentences as the following:
T: The Christian Science Monitor named a US
journalist kidnapped in Iraq as freelancer Jill
Carroll.
H: Jill Carroll was abducted in Iraq.
The first sentence is the text, and the second is
the hypothesis. The organizers of the challenge an-
notated several pairs with a binary tag, indicating
whether the hypothesis could be entailed from the
text. Although these pairs of text are interesting we
decided to discard them from this pilot because the
length of the hypothesis was typically much shorter
than the text, and we did not want to bias the STS
task in this respect. We may, however, explore using
TE pairs for STS in the future.
Microsoft Research (MSR) has pioneered the ac-
quisition of paraphrases with two manually anno-
tated datasets. The first, called MSR Paraphrase
(MSRpar for short) has been widely used to evaluate
text similarity algorithms. It contains 5801 pairs of
sentences gleaned over a period of 18 months from
thousands of news sources on the web (Dolan et
al., 2004). 67% of the pairs were tagged as para-
phrases. The inter annotator agreement is between
82% and 84%. Complete meaning equivalence is
not required, and the annotation guidelines allowed
for some relaxation. The pairs which were anno-
tated as not being paraphrases ranged from com-
pletely unrelated semantically, to partially overlap-
ping, to those that were almost-but-not-quite seman-
tically equivalent. In this sense our graded annota-
tions enrich the dataset with more nuanced tags, as
we will see in the following section. We followed
the original split of 70% for training and 30% for
testing. A sample pair from the dataset follows:
The Senate Select Committee on Intelligence
is preparing a blistering report on prewar
intelligence on Iraq.
American intelligence leading up to the
war on Iraq will be criticized by a powerful
US Congressional committee due to report
soon, officials said today.
In order to construct a dataset which would reflect
a uniform distribution of similarity ranges, we sam-
pled the MSRpar dataset at certain ranks of string
similarity. We used the implementation readily ac-
cessible at CPAN1 of a well-known metric (Ukko-
nen, 1985). We sampled equal numbers of pairs
from five bands of similarity in the [0.4 .. 0.8] range
separately from the paraphrase and non-paraphrase
pairs. We sampled 1500 pairs overall, which we split
50% for training and 50% for testing.
The second dataset from MSR is the MSR Video
Paraphrase Corpus (MSRvid for short). The authors
showed brief video segments to Annotators from
Amazon Mechanical Turk (AMT) and were asked
1http://search.cpan.org/?mlehmann/
String-Similarity-1.04/Similarity.pm
386
Figure 1: Video and corresponding descriptions from
MSRvid
Figure 2: Definition and instructions for annotation
to provide a one-sentence description of the main ac-
tion or event in the video (Chen and Dolan, 2011).
Nearly 120 thousand sentences were collected for
2000 videos. The sentences can be taken to be
roughly parallel descriptions, and they included sen-
tences for many languages. Figure 1 shows a video
and corresponding descriptions.
The sampling procedure from this dataset is sim-
ilar to that for MSRpar. We construct two bags of
data to draw samples. The first includes all possible
pairs for the same video, and the second includes
pairs taken from different videos. Note that not all
sentences from the same video were equivalent, as
some descriptions were contradictory or unrelated.
Conversely, not all sentences coming from different
videos were necessarily unrelated, as many videos
were on similar topics. We took an equal number of
samples from each of these two sets, in an attempt to
provide a balanced dataset between equivalent and
non-equivalent pairs. The sampling was also done
according to string similarity, but in four bands in the
[0.5 .. 0.8] range, as sentences from the same video
had a usually higher string similarity than those in
the MSRpar dataset. We sampled 1500 pairs overall,
which we split 50% for training and 50% for testing.
Given the strong connection between STS sys-
tems and Machine Translation evaluation metrics,
we also sampled pairs of segments that had been
part of human evaluation exercises. Those pairs in-
cluded a reference translation and a automatic Ma-
chine Translation system submission, as follows:
The only instance in which no tax is levied is
when the supplier is in a non-EU country and
the recipient is in a Member State of the EU.
The only case for which no tax is still
perceived ?is an example of supply in the
European Community from a third country.
We selected pairs from the translation shared task
of the 2007 and 2008 ACL Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2007; Callison-Burch et al, 2008). For consistency,
we only used French to English system submissions.
The training data includes all of the Europarl human
ranked fr-en system submissions from WMT 2007,
with each machine translation being paired with the
correct reference translation. This resulted in 729
unique training pairs. The test data is comprised of
all Europarl human evaluated fr-en pairs from WMT
2008 that contain 16 white space delimited tokens or
less.
In addition, we selected two other datasets that
were used as out-of-domain testing. One of them
comprised of all the human ranked fr-en system
submissions from the WMT 2007 news conversa-
tion test set, resulting in 351 unique system refer-
ence pairs.2 The second set is radically different as
it comprised 750 pairs of glosses from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.1 (Fellbaum,
1998) senses. The mapping of the senses of both re-
sources comprised 110K sense pairs. The similarity
between the sense pairs was generated using simple
word overlap. 50% of the pairs were sampled from
senses which were deemed as equivalent senses, the
rest from senses which did not map to one another.
3 Annotation
In this first dataset we defined a straightforward lik-
ert scale ranging from 5 to 0, but we decided to pro-
vide definitions for each value in the scale (cf. Fig-
ure 2). We first did pilot annotations of 200 pairs se-
2At the time of the shared task, this data set contained dupli-
cates resulting in 399 sentence pairs.
387
lected at random from the three main datasets in the
training set. We did the annotation, and the pairwise
Pearson ranged from 84% to 87% among ourselves.
The agreement of each annotator with the average
scores of the other was between 87% and 89%.
In the future, we would like to explore whether
the definitions improve the consistency of the tag-
ging with respect to a likert scale without defini-
tions. Note also that in the assessment of the qual-
ity and evaluation of the systems performances, we
just took the resulting SS scores and their averages.
Using the qualitative descriptions for each score in
analysis and evaluation is left for future work.
Given the good results of the pilot we decided to
deploy the task in Amazon Mechanical Turk (AMT)
in order to crowd source the annotation task. The
turkers were required to have achieved a 95% of ap-
proval rating in their previous HITs, and had to pass
a qualification task which included 6 example pairs.
Each HIT included 5 pairs of sentences, and was
paid at 0.20$ each. We collected 5 annotations per
HIT. In the latest data collection, each HIT required
114.9 second for completion.
In order to ensure the quality, we also performed
post-hoc validation. Each HIT contained one pair
from our pilot. After the tagging was completed
we checked the correlation of each individual turker
with our scores, and removed annotations of turkers
which had low correlations (below 50%). Given the
high quality of the annotations among the turkers,
we could alternatively use the correlation between
the turkers itself to detect poor quality annotators.
4 Systems Evaluation
Given two sentences, s1 and s2, an STS system
would need to return a similarity score. Participants
can also provide a confidence score indicating their
confidence level for the result returned for each pair,
but this confidence is not used for the main results.
The output of the systems performance is evaluated
using the Pearson product-moment correlation co-
efficient between the system scores and the human
scores, as customary in text similarity (Rubenstein
and Goodenough, 1965). We calculated Pearson for
each evaluation dataset separately.
In order to have a single Pearson measure for each
system we concatenated the gold standard (and sys-
tem outputs) for all 5 datasets into a single gold stan-
dard file (and single system output). The first ver-
sion of the results were published using this method,
but the overall score did not correspond well to the
individual scores in the datasets, and participants
proposed two additional evaluation metrics, both of
them based on Pearson correlation. The organizers
of the task decided that it was more informative, and
on the benefit of the community, to also adopt those
evaluation metrics, and the idea of having a single
main evaluation metric was dropped. This decision
was not without controversy, but the organizers gave
more priority to openness and inclusiveness and to
the involvement of participants. The final result ta-
ble thus included three evaluation metrics. For the
future we plan to analyze the evaluation metrics, in-
cluding non-parametric metrics like Spearman.
4.1 Evaluation metrics
The first evaluation metric is the Pearson correla-
tion for the concatenation of all five datasets, as de-
scribed above. We will use overall Pearson or sim-
ply ALL to refer to this measure.
The second evaluation metric normalizes the out-
put for each dataset separately, using the linear least
squares method. We concatenated the system results
for five datasets and then computed a single Pear-
son correlation. Given Y = {yi} and X = {xi}
(the gold standard scores and the system scores,
respectively), we transform the system scores into
X ? = {x?i} in order to minimize the squared error?
i (yi ? x
?
i)
2. The linear transformation is given by
x?i = xi ? ?1 + ?2, where ?1 and ?2 are found an-
alytically. We refer to this measure as Normalized
Pearson or simply ALLnorm. This metric was sug-
gested by one of the participants, Sergio Jimenez.
The third evaluation metric is the weighted mean
of the Pearson correlations on individual datasets.
The Pearson returned for each dataset is weighted
according to the number of sentence pairs in that
dataset. Given ri the five Pearson scores for
each dataset, and ni the number of pairs in each
dataset, the weighted mean is given as
?
i=1..5(ri ?
ni)/
?
i=1..5 ni We refer to this measure as weighted
mean of Pearson or Mean for short.
4.2 Using confidence scores
Participants were allowed to include a confidence
score between 1 and 100 for each of their scores.
We used weighted Pearson to use those confidence
388
scores3. Table 2 includes the list of systems which
provided a non-uniform confidence. The results
show that some systems were able to improve their
correlation, showing promise for the usefulness of
confidence in applications.
4.3 The Baseline System
We produced scores using a simple word overlap
baseline system. We tokenized the input sentences
splitting at white spaces, and then represented each
sentence as a vector in the multidimensional to-
ken space. Each dimension had 1 if the token was
present in the sentence, 0 otherwise. Similarity of
vectors was computed using cosine similarity.
We also run a random baseline several times,
yielding close to 0 correlations in all datasets, as ex-
pected. We will refer to the random baseline again
in Section 4.5.
4.4 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 35
teams participated, submitting 88 system runs (cf.
first column of Table 1). Due to lack of space we
can?t detail the full names of authors and institutions
that participated. The interested reader can use the
name of the runs to find the relevant paper in these
proceedings.
There were several issues in the submissions. The
submission software did not ensure that the nam-
ing conventions were appropriately used, and this
caused some submissions to be missed, and in two
cases the results were wrongly assigned. Some par-
ticipants returned Not-a-Number as a score, and the
organizers had to request whether those where to be
taken as a 0 or as a 5.
Finally, one team submitted past the 120 hour
deadline and some teams sent missing files after the
deadline. All those are explicitly marked in Table 1.
The teams that included one of the organizers are
also explicitly marked. We want to stress that in
these teams the organizers did not allow the devel-
opers of the system to access any data or informa-
tion which was not available for the rest of partic-
ipants. One exception is weiwei, as they generated
3http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
the 110K OntoNotes-WordNet dataset from which
the other organizers sampled the surprise data set.
After the submission deadline expired, the orga-
nizers published the gold standard in the task web-
site, in order to ensure a transparent evaluation pro-
cess.
4.5 Results
Table 1 shows the results for each run in alphabetic
order. Each result is followed by the rank of the sys-
tem according to the given evaluation measure. To
the right, the Pearson score for each dataset is given.
In boldface, the three best results in each column.
First of all we want to stress that the large majority
of the systems are well above the simple baseline,
although the baseline would rank 70 on the Mean
measure, improving over 19 runs.
The correlation for the non-MT datasets were re-
ally high: the highest correlation was obtained was
for MSRvid (0.88 r), followed by MSRpar (0.73 r)
and On-WN (0.73 r). The results for the MT evalu-
ation data are lower, (0.57 r) for SMT-eur and (0.61
r) for SMT-News. The simple token overlap base-
line, on the contrary, obtained the highest results
for On-WN (0.59 r), with (0.43 r) on MSRpar and
(0.40 r) on MSRvid. The results for MT evaluation
data are also reversed, with (0.40 r) for SMT-eur and
(0.45 r) for SMT-News.
The ALLnorm measure yields the highest corre-
lations. This comes at no surprise, as it involves a
normalization which transforms the system outputs
using the gold standard. In fact, a random base-
line which gets Pearson correlations close to 0 in all
datasets would attain Pearson of 0.58914.
Although not included in the results table for lack
of space, we also performed an analysis of confi-
dence intervals. For instance, the best run according
to ALL (r = .8239) has a 95% confidence interval of
[.8123,.8349] and the second a confidence interval
of [.8016,.8254], meaning that the differences are
not statistically different.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, special emphasis on the tools and re-
sources that they used. Table 3 shows in a simpli-
4We run the random baseline 10 times. The mean is reported
here. The standard deviation is 0.0005
389
Run ALL Rank ALLnrm Rank Mean Rank MSRpar MSRvid SMT-eur On-WN SMT-news
00-baseline/task6-baseline .3110 87 .6732 85 .4356 70 .4334 .2996 .4542 .5864 .3908
aca08ls/task6-University Of Sheffield-Hybrid .6485 34 .8238 15 .6100 18 .5166 .8187 .4859 .6676 .4280
aca08ls/task6-University Of Sheffield-Machine Learning .7241 17 .8169 18 .5750 38 .5166 .8187 .4859 .6390 .2089
aca08ls/task6-University Of Sheffield-Vector Space .6054 48 .7946 44 .5943 27 .5460 .7241 .4858 .6676 .4280
acaputo/task6-UNIBA-DEPRI .6141 46 .8027 38 .5891 31 .4542 .7673 .5126 .6593 .4636
acaputo/task6-UNIBA-LSARI .6221 44 .8079 30 .5728 40 .3886 .7908 .4679 .6826 .4238
acaputo/task6-UNIBA-RI .6285 41 .7951 43 .5651 45 .4128 .7612 .4531 .6306 .4887
baer/task6-UKP-run1 .8117 4 .8559 4 .6708 4 .6821 .8708 .5118 .6649 .4672
baer/task6-UKP-run2 plus postprocessing smt twsi .8239 1 .8579 2 .6773 1 .6830 .8739 .5280 .6641 .4937
baer/task6-UKP-run3 plus random .7790 8 .8166 19 .4320 71 .6830 .8739 .5280 -.0620 -.0520
croce/task6-UNITOR-1 REGRESSION BEST FEATURES .7474 13 .8292 12 .6316 10 .5695 .8217 .5168 .6591 .4713
croce/task6-UNITOR-2 REGRESSION ALL FEATURES .7475 12 .8297 11 .6323 9 .5763 .8217 .5102 .6591 .4713
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS .6289 40 .8150 21 .5939 28 .4686 .8027 .4574 .6591 .4713
csjxu/task6-PolyUCOMP-RUN1 .6528 31 .7642 59 .5492 51 .4728 .6593 .4835 .6196 .4290
danielcer/stanford fsa? .6354 38 .7212 70 .4848 66 .3795 .5350 .4377 .6052 .4164
danielcer/stanford pdaAll? .4229 77 .7160 72 .5044 62 .4409 .4698 .4558 .6468 .4769
danielcer/stanford rte? .5589 55 .7807 55 .4674 67 .4374 .8037 .3533 .3077 .3235
davide buscaldi/task6-IRIT-pg1 .4280 76 .7379 65 .5009 63 .4295 .6125 .4952 .5387 .3614
davide buscaldi/task6-IRIT-pg3 .4813 68 .7569 61 .5202 58 .4171 .6728 .5179 .5526 .3693
davide buscaldi/task6-IRIT-wu .4064 81 .7287 69 .4898 65 .4326 .5833 .4856 .5317 .3480
demetrios glinos/task6-ATA-BASE .3454 83 .6990 81 .2772 87 .1684 .6256 .2244 .1648 .0988
demetrios glinos/task6-ATA-CHNK .4976 64 .7160 73 .3215 86 .2312 .6595 .1504 .2735 .1426
demetrios glinos/task6-ATA-STAT .4165 79 .7129 75 .3312 85 .1887 .6482 .2769 .2950 .1336
desouza/task6-FBK-run1 .5633 54 .7127 76 .3628 82 .2494 .6117 .1495 .4212 .2439
desouza/task6-FBK-run2 .6438 35 .8080 29 .5888 32 .5128 .7807 .3796 .6228 .5474
desouza/task6-FBK-run3 .6517 32 .8106 25 .6077 20 .5169 .7773 .4419 .6298 .6085
dvilarinoayala/task6-BUAP-RUN-1 .4997 63 .7568 62 .5260 56 .4037 .6532 .4521 .6050 .4537
dvilarinoayala/task6-BUAP-RUN-2 -.0260 89 .5933 89 .1016 89 .1109 .0057 .0348 .1788 .1964
dvilarinoayala/task6-BUAP-RUN-3 .6630 25 .7474 64 .5105 59 .4018 .6378 .4758 .5691 .4057
enrique/task6-UNED-H34measures .4381 75 .7518 63 .5577 48 .5328 .5788 .4785 .6692 .4465
enrique/task6-UNED-HallMeasures .2791 88 .6694 87 .4286 72 .3861 .2570 .4086 .6006 .5305
enrique/task6-UNED-SP INIST .4680 69 .7625 60 .5615 47 .5166 .6303 .4625 .6442 .4753
georgiana dinu/task6-SAARLAND-ALIGN VSSIM .4952 65 .7871 50 .5065 60 .4043 .7718 .2686 .5721 .3505
georgiana dinu/task6-SAARLAND-MIXT VSSIM .4548 71 .8258 13 .5662 43 .6310 .8312 .1391 .5966 .3806
jan snajder/task6-takelab-simple .8133 3 .8635 1 .6753 2 .7343 .8803 .4771 .6797 .3989
jan snajder/task6-takelab-syntax .8138 2 .8569 3 .6601 5 .6985 .8620 .3612 .7049 .4683
janardhan/task6-janardhan-UNL matching .3431 84 .6878 84 .3481 83 .1936 .5504 .3755 .2888 .3387
jhasneha/task6-Penn-ELReg .6622 27 .8048 34 .5654 44 .5480 .7844 .3513 .6040 .3607
jhasneha/task6-Penn-ERReg .6573 28 .8083 28 .5755 37 .5610 .7857 .3568 .6214 .3732
jhasneha/task6-Penn-LReg .6497 33 .8043 36 .5699 41 .5460 .7818 .3547 .5969 .4137
jotacastillo/task6-SAGAN-RUN1 .5522 57 .7904 47 .5906 29 .5659 .7113 .4739 .6542 .4253
jotacastillo/task6-SAGAN-RUN2 .6272 42 .8032 37 .5838 34 .5538 .7706 .4480 .6135 .3894
jotacastillo/task6-SAGAN-RUN3 .6311 39 .7943 45 .5649 46 .5394 .7560 .4181 .5904 .3746
Konstantin Z/task6-ABBYY-General .5636 53 .8052 33 .5759 36 .4797 .7821 .4576 .6488 .3682
M Rios/task6-UOW-LEX PARA .6397 36 .7187 71 .3825 80 .3628 .6426 .3074 .2806 .2082
M Rios/task6-UOW-LEX PARA SEM .5981 49 .6955 82 .3473 84 .3529 .5724 .3066 .2643 .1164
M Rios/task6-UOW-SEM .5361 59 .6287 88 .2567 88 .2995 .2910 .1611 .2571 .2212
mheilman/task6-ETS-PERP .7808 7 .8064 32 .6305 11 .6211 .7210 .4722 .7080 .5149
mheilman/task6-ETS-PERPphrases .7834 6 .8089 27 .6399 7 .6397 .7200 .4850 .7124 .5312
mheilman/task6-ETS-TERp .4477 73 .7291 68 .5253 57 .5049 .5217 .4748 .6169 .4566
nitish aggarwal/task6-aggarwal-run1? .5777 52 .8158 20 .5466 52 .3675 .8427 .3534 .6030 .4430
nitish aggarwal/task6-aggarwal-run2? .5833 51 .8183 17 .5683 42 .3720 .8330 .4238 .6513 .4499
nitish aggarwal/task6-aggarwal-run3 .4911 67 .7696 57 .5377 53 .5320 .6874 .4514 .5827 .2818
nmalandrakis/task6-DeepPurple-DeepPurple hierarchical .6228 43 .8100 26 .5979 23 .5984 .7717 .4292 .6480 .3702
nmalandrakis/task6-DeepPurple-DeepPurple sigmoid .5540 56 .7997 41 .5558 50 .5960 .7616 .2628 .6016 .3446
nmalandrakis/task6-DeepPurple-DeepPurple single .4918 66 .7646 58 .5061 61 .4989 .7092 .4437 .4879 .2441
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach? .3880 82 .6706 86 .4111 76 .3427 .3549 .4271 .5298 .4034
rada/task6-UNT-CombinedRegression .7418 14 .8406 7 .6159 14 .5032 .8695 .4797 .6715 .4033
rada/task6-UNT-IndividualDecTree .7677 9 .8389 9 .5947 25 .5693 .8688 .4203 .6491 .2256
rada/task6-UNT-IndividualRegression .7846 5 .8440 6 .6162 13 .5353 .8750 .4203 .6715 .4033
sbdlrhmn/task6-sbdlrhmn-Run1 .6663 23 .7842 53 .5376 54 .5440 .7335 .3830 .5860 .2445
sbdlrhmn/task6-sbdlrhmn-Run2 .4169 78 .7104 77 .4986 64 .4617 .4489 .4719 .6353 .4353
sgjimenezv/task6-SOFT-CARDINALITY .7331 15 .8526 5 .6708 3 .6405 .8562 .5152 .7109 .4833
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION .7107 19 .8397 8 .6486 6 .6316 .8237 .4320 .7109 .4833
siva/task6-DSS-alignheuristic .5253 60 .7962 42 .6030 21 .5735 .7123 .4781 .6984 .4177
siva/task6-DSS-average .5490 58 .8047 35 .5943 26 .5020 .7645 .4875 .6677 .4324
siva/task6-DSS-wordsim .5130 61 .7895 49 .5287 55 .3765 .7761 .4161 .5728 .3964
skamler /task6-EHU-RUN1v2?? .3129 86 .6935 83 .3889 79 .3605 .5187 .2259 .4098 .3465
sokolov/task6-LIMSI-cosprod .6392 37 .7344 67 .3940 78 .3948 .6597 .0143 .4157 .2889
sokolov/task6-LIMSI-gradtree .6789 22 .7377 66 .4118 75 .4848 .6636 .0934 .3706 .2455
sokolov/task6-LIMSI-sumdiff .6196 45 .7101 78 .4131 74 .4295 .5724 .2842 .3989 .2575
spirin2/task6-UIUC-MLNLP-Blend .4592 70 .7800 56 .5782 35 .6523 .6691 .3566 .6117 .4603
spirin2/task6-UIUC-MLNLP-CCM .7269 16 .8217 16 .6104 17 .5769 .8203 .4667 .5835 .4945
spirin2/task6-UIUC-MLNLP-Puzzle .3216 85 .7857 51 .4376 69 .5635 .8056 .0630 .2774 .2409
sranjans/task6-sranjans-1 .6529 30 .8018 39 .6249 12 .6124 .7240 .5581 .6703 .4533
sranjans/task6-sranjans-2 .6651 24 .8128 22 .6366 8 .6254 .7538 .5328 .6649 .5036
sranjans/task6-sranjans-3 .5045 62 .7846 52 .5905 30 .6167 .7061 .5666 .5664 .3968
tiantianzhu7/task6-tiantianzhu7-1 .4533 72 .7134 74 .4192 73 .4184 .5630 .2083 .4822 .2745
tiantianzhu7/task6-tiantianzhu7-2 .4157 80 .7099 79 .3960 77 .4260 .5628 .1546 .4552 .1923
tiantianzhu7/task6-tiantianzhu7-3 .4446 74 .7097 80 .3740 81 .3411 .5946 .1868 .4029 .1823
weiwei/task6-weiwei-run1?? .6946 20 .8303 10 .6081 19 .4106 .8351 .5128 .7273 .4383
yeh/task6-SRIUBC-SYSTEM1? .7513 11 .8017 40 .5997 22 .6084 .7458 .4688 .6315 .3994
yeh/task6-SRIUBC-SYSTEM2? .7562 10 .8111 24 .5858 33 .6050 .7939 .4294 .5871 .3366
yeh/task6-SRIUBC-SYSTEM3? .6876 21 .7812 54 .4668 68 .4791 .7901 .2159 .3843 .2801
ygutierrez/task6-UMCC DLSI-MultiLex .6630 26 .7922 46 .5560 49 .6022 .7709 .4435 .4327 .4264
ygutierrez/task6-UMCC DLSI-MultiSem .6529 29 .8115 23 .6116 16 .5269 .7756 .4688 .6539 .5470
ygutierrez/task6-UMCC DLSI-MultiSemLex .7213 18 .8239 14 .6158 15 .6205 .8104 .4325 .6256 .4340
yrkakde/task6-yrkakde-DiceWordnet .5977 50 .7902 48 .5742 39 .5294 .7470 .5531 .5698 .3659
yrkakde/task6-yrkakde-JaccNERPenalty .6067 47 .8078 31 .5955 24 .5757 .7765 .4989 .6257 .3468
Table 1: The first row corresponds to the baseline. ALL for overall Pearson, ALLnorm for Pearson after normaliza-
tion, and Mean for mean of Pearsons. We also show the ranks for each measure. Rightmost columns show Pearson for
each individual dataset. Note: ? system submitted past the 120 hour window, ? post-deadline fixes, ? team involving
one of the organizers.
390
Run ALL ALLw MSRpar MSRparw MSRvid MSRvidw SMT-eur SMT-eurw On-WN On-WNw SMT-news SMT-newsw
davide buscaldi/task6-IRIT-pg1 .4280 .4946 .4295 .4082 .6125 .6593 .4952 .5273 .5387 .5574 .3614 .4674
davide buscaldi/task6-IRIT-pg3 .4813 .5503 .4171 .4033 .6728 .7048 .5179 .5529 .5526 .5950 .3693 .4648
davide buscaldi/task6-IRIT-wu .4064 .4682 .4326 .4035 .5833 .6253 .4856 .5138 .5317 .5189 .3480 .4482
enrique/task6-UNED-H34measures .4381 .2615 .5328 .4494 .5788 .4913 .4785 .4660 .6692 .6440 .4465 .3632
enrique/task6-UNED-HallMeasures .2791 .2002 .3861 .3802 .2570 .2343 .4086 .4212 .6006 .5947 .5305 .4858
enrique/task6-UNED-SP INIST .4680 .3754 .5166 .5082 .6303 .5588 .4625 .4801 .6442 .5761 .4753 .4143
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach .3880 .3636 .3427 .3498 .3549 .3353 .4271 .3989 .5298 .4619 .4034 .3228
tiantianzhu7/task6-tiantianzhu7-1 .4533 .5442 .4184 .4241 .5630 .5630 .2083 .4220 .4822 .5031 .2745 .3536
tiantianzhu7/task6-tiantianzhu7-2 .4157 .5249 .4260 .4340 .5628 .5758 .1546 .4776 .4552 .4926 .1923 .3362
tiantianzhu7/task6-tiantianzhu7-3 .4446 .5229 .3411 .3611 .5946 .5899 .1868 .4769 .4029 .4365 .1823 .4014
Table 2: Results according to weighted correlation for the systems that provided non-uniform confidence alongside
their scores.
fied way the tools and resources used by those par-
ticipants that did submit a valid description file. In
the last row, the totals show that WordNet was the
most used resource, followed by monolingual cor-
pora and Wikipedia. Acronyms, dictionaries, mul-
tilingual corpora, stopword lists and tables of para-
phrases were also used.
Generic NLP tools like lemmatization and PoS
tagging were widely used, and to a lesser extent,
parsing, word sense disambiguation, semantic role
labeling and time and date resolution (in this or-
der). Knowledge-based and distributional methods
got used nearly equally, and to a lesser extent, align-
ment and/or statistical machine translation software,
lexical substitution, string similarity, textual entail-
ment and machine translation evaluation software.
Machine learning was widely used to combine and
tune components. Several less used tools were also
listed but were used by three or less systems.
The top scoring systems tended to use most of
the resources and tools listed (UKP, Takelab), with
some notable exceptions like Sgjimenez which was
based on string similarity. For a more detailed anal-
ysis, the reader is directed to the papers of the par-
ticipants in this volume.
6 Conclusions and Future Work
This paper presents the SemEval 2012 pilot eval-
uation exercise on Semantic Textual Similarity. A
simple definition of STS beyond the likert-scale was
set up, and a wealth of annotated data was pro-
duced. The similarity of pairs of sentences was
rated on a 0-5 scale (low to high similarity) by hu-
man judges using Amazon Mechanical Turk. The
dataset includes 1500 sentence pairs from MSRpar
and MSRvid (each), ca. 1500 pairs from WMT,
and 750 sentence pairs from a mapping between
OntoNotes and WordNet senses. The correlation be-
tween non-expert annotators and annotations from
the authors is very high, showing the high quality of
the dataset. The dataset was split 50% as train and
test, with the exception of the surprise test datasets:
a subset of WMT from a different domain and the
OntoNotes-WordNet mapping. All datasets are pub-
licly available.5
The exercise was very successful in participation
and results. 35 teams participated, submitting 88
runs. The best results scored a Pearson correlation
over 80%, well beyond a simple lexical baseline
with 31% of correlation. The metric for evaluation
was not completely satisfactory, and three evalua-
tion metrics were finally published. We discuss the
shortcomings of those measures.
There are several tasks ahead in order to make
STS a mature field. The first is to find a satisfac-
tory evaluation metric. The second is to analyze the
definition of the task itself, with a thorough analysis
of the definitions in the likert scale.
We would also like to analyze the relation be-
tween the STS scores and the paraphrase judgements
in MSR, as well as the human evaluations in WMT.
Finally, we would also like to set up an open frame-
work where NLP components and similarity algo-
rithms can be combined by the community. All in
all, we would like this dataset to be the focus of the
community working on algorithmic approaches for
semantic processing and inference at large.
Acknowledgements
We would like to thank all participants, specially (in al-
phabetic order) Yoan Gutierrez, Michael Heilman, Ser-
gio Jimenez, Nitin Madnami, Diana McCarthy and Shru-
tiranjan Satpathy for their contributions on evaluation
metrics. Eneko Agirre was partially funded by the
5http://www.cs.york.ac.uk/semeval-2012/
task6/
391
A
cr
on
ym
s
D
ic
ti
on
ar
ie
s
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
S
to
p
w
or
ds
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
or
dN
et
A
li
gn
m
en
t
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
M
ac
hi
ne
L
ea
rn
in
g
M
T
ev
al
ua
ti
on
M
W
E
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
M
T
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
O
th
er
aca08ls/task6-University Of Sheffield-Hybrid x x x x x x x
aca08ls/task6-University Of Sheffield-Machine Learning x x x x x x x
aca08ls/task6-University Of Sheffield-Vector Space x x x x x
baer/task6-UKP-run1 x x x x x x x x x x x x x x
baer/task6-UKP-run2 plus postprocessing smt twsi x x x x x x x x x x x x x x
baer/task6-UKP-run3 plus random x x x x x x x x x x x x x x
croce/task6-UNITOR-1 REGRESSION BEST FEATURES x x x x x x
croce/task6-UNITOR-2 REGRESSION ALL FEATURES x x x x x x
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS x x x x x x
csjxu/task6-PolyUCOMP-RUN x x x x
danielcer/stanford fsa x x x x x x x
danielcer/stanford pdaAll x x x x x x x
danielcer/stanford rte x x x x x x x x
davide buscaldi/task6-IRIT-pg1 x x x x x
davide buscaldi/task6-IRIT-pg3 x x x x x
davide buscaldi/task6-IRIT-wu x x x x x
demetrios glinos/task6-ATA-BASE x x x x x x x
demetrios glinos/task6-ATA-CHNK x x x x x x x
demetrios glinos/task6-ATA-STAT x x x x x x x
desouza/task6-FBK-run1 x x x x x x x x x x x x x
desouza/task6-FBK-run2 x x x x x x x x
desouza/task6-FBK-run3 x x x x x x
dvilarinoayala/task6-BUAP-RUN-1 x x
dvilarinoayala/task6-BUAP-RUN-2 x
dvilarinoayala/task6-BUAP-RUN-3 x x
jan snajder/task6-takelab-simple x x x x x x x x x x x x x
jan snajder/task6-takelab-syntax x x x x x x x x x
janardhan/task6-janardhan-UNL matching x x x x x x
jotacastillo/task6-SAGAN-RUN1 x x x x x x x x
jotacastillo/task6-SAGAN-RUN2 x x x x x x x x
jotacastillo/task6-SAGAN-RUN3 x x x x x x x x
Konstantin Z/task6-ABBYY-General
M Rios/task6-UOW-LEX PARA x x x x x x x x
M Rios/task6-UOW-LEX PARA SEM x x x x x x x x
M Rios/task6-UOW-SEM x x x x x x x
mheilman/task6-ETS-PERP x x x x x x x
mheilman/task6-ETS-PERPphrases x x x x x x x x x
mheilman/task6-ETS-TERp x x x x x x x
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach x x x x x x x x x x
rada/task6-UNT-CombinedRegression x x x x x x x x x
rada/task6-UNT-IndividualDecTree x x x x x x x x x
rada/task6-UNT-IndividualRegression x x x x x x x x x
sgjimenezv/task6-SOFT-CARDINALITY x x x
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION x x x
skamler /task6-EHU-RUN1v2 x x x x x
sokolov/task6-LIMSI-cosprod x x x x
sokolov/task6-LIMSI-gradtree x x x x
sokolov/task6-LIMSI-sumdiff x x x x
spirin2/task6-UIUC-MLNLP-Blend x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-CCM x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-Puzzle x x x x x x x x x x x
sranjans/task6-sranjans-1 x x x x x x x x
sranjans/task6-sranjans-2 x x x x x x x x x x x
sranjans/task6-sranjans-3 x x x x x x x x x x x
tiantianzhu7/task6-tiantianzhu7-1 x x x x
tiantianzhu7/task6-tiantianzhu7-2 x x x
tiantianzhu7/task6-tiantianzhu7-3 x x x x
weiwei/task6-weiwei-run1 x x x x x x
yeh/task6-SRIUBC-SYSTEM1 x x x x x x x
yeh/task6-SRIUBC-SYSTEM2 x x x x x x x
yeh/task6-SRIUBC-SYSTEM3 x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiLex x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSem x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSemLex x x x x x x x x
yrkakde/task6-yrkakde-DiceWordnet x x x
Total 8 6 10 33 5 5 9 20 47 7 31 37 49 13 13 4 7 12 43 9 4 13 17 10 5 15 25
Table 3: Resources and tools used by the systems that submitted a description file. Leftmost columns correspond to
the resources, and rightmost to tools, in alphabetic order.
392
European Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 270082
(PATHS project) and the Ministry of Economy under
grant TIN2009-14715-C04-01 (KNOW2 project). Daniel
Cer gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Language
Translation (BOLT) program through IBM. The STS an-
notations were funded by an extension to DARPA GALE
subcontract to IBM # W0853748 4911021461.0 to Mona
Diab. Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of the
author(s) and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 136?158.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, StatMT ?08, pages 70?106.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meetings of the Asso-
ciation for Computational Linguistics (ACL).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING
04: Proceedings of the 20th international conference
on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Michael D. Lee, Brandon Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text doc-
ument similarity. In Proceedings of the 27th Annual
Conference of the Cognitive Science Society, pages
1254?1259, Mahwah, NJ.
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150, August.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
E. Ukkonen. 1985. Algorithms for approximate string
matching. Information and Contro, 64:110?118.
393
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 32?43, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
*SEM 2013 shared task: Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
danielcer@stanford.edu
Mona Diab
George Washington University
mtdiab@gwu.edu
Aitor Gonzalez-Agirre
University of the Basque Country
agonzalez278@ikasle.ehu.es
Weiwei Guo
Columbia University
weiwei@cs.columbia.edu
Abstract
In Semantic Textual Similarity (STS), sys-
tems rate the degree of semantic equivalence,
on a graded scale from 0 to 5, with 5 be-
ing the most similar. This year we set up
two tasks: (i) a core task (CORE), and (ii)
a typed-similarity task (TYPED). CORE is
similar in set up to SemEval STS 2012 task
with pairs of sentences from sources related
to those of 2012, yet different in genre from
the 2012 set, namely, this year we included
newswire headlines, machine translation eval-
uation datasets and multiple lexical resource
glossed sets. TYPED, on the other hand, is
novel and tries to characterize why two items
are deemed similar, using cultural heritage
items which are described with metadata such
as title, author or description. Several types of
similarity have been defined, including simi-
lar author, similar time period or similar lo-
cation. The annotation for both tasks lever-
ages crowdsourcing, with relative high inter-
annotator correlation, ranging from 62% to
87%. The CORE task attracted 34 participants
with 89 runs, and the TYPED task attracted 6
teams with 14 runs.
1 Introduction
Given two snippets of text, Semantic Textual Simi-
larity (STS) captures the notion that some texts are
more similar than others, measuring the degree of
semantic equivalence. Textual similarity can range
from exact semantic equivalence to complete un-
relatedness, corresponding to quantified values be-
tween 5 and 0. The graded similarity intuitively cap-
tures the notion of intermediate shades of similarity
such as pairs of text differ only in some minor nu-
anced aspects of meaning only, to relatively impor-
tant differences in meaning, to sharing only some
details, or to simply being related to the same topic,
as shown in Figure 1.
One of the goals of the STS task is to create a
unified framework for combining several semantic
components that otherwise have historically tended
to be evaluated independently and without character-
ization of impact on NLP applications. By providing
such a framework, STS will allow for an extrinsic
evaluation for these modules. Moreover, this STS
framework itself could in turn be evaluated intrin-
sically and extrinsically as a grey/black box within
various NLP applications such as Machine Trans-
lation (MT), Summarization, Generation, Question
Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of NLP
tasks. STS is different from TE inasmuch as it as-
sumes bidirectional graded equivalence between the
pair of textual snippets. In the case of TE the equiv-
alence is directional, e.g. a car is a vehicle, but a ve-
hicle is not necessarily a car. STS also differs from
both TE and Paraphrasing (in as far as both tasks
have been defined to date in the literature) in that,
rather than being a binary yes/no decision (e.g. a ve-
hicle is not a car), we define STS to be a graded sim-
ilarity notion (e.g. a vehicle and a car are more sim-
ilar than a wave and a car). A quantifiable graded
bidirectional notion of textual similarity is useful for
a myriad of NLP tasks such as MT evaluation, infor-
mation extraction, question answering, summariza-
tion, etc.
32
? (5) The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
? (4) The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade Kabul.
The US army invaded Kabul on May 7th last year, 2010.
? (3) The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a suspect.
?He is not a suspect anymore.? John said.
? (2) The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
? (1) The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
? (0) The two sentences are on different topics.
John went horse back riding at dawn with a whole group of friends.
Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.
Figure 1: Annotation values with explanations and examples for the core STS task.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al, 2012). In addition, we held
a DARPA sponsored workshop at Columbia Uni-
versity1. In 2013, STS was selected as the official
Shared Task of the *SEM 2013 conference. Ac-
cordingly, in STS 2013, we set up two tasks: The
core task CORE, which is similar to the 2012 task;
and a pilot task on typed-similarity TYPED between
semi-structured records.
For CORE, we provided all the STS 2012 data
as training data, and the test data was drawn from
related but different datasets. This is in contrast
to the STS 2012 task where the train/test data
were drawn from the same datasets. The 2012
datasets comprised the following: pairs of sentences
from paraphrase datasets from news and video elic-
itation (MSRpar and MSRvid), machine transla-
tion evaluation data (SMTeuroparl, SMTnews) and
pairs of glosses (OnWN). The current STS 2013
dataset comprises the following: pairs of news head-
lines, SMT evaluation sentences (SMT) and pairs of
glosses (OnWN and FNWN).
The typed-similarity pilot task TYPED attempts
1http://www.cs.columbia.edu/?weiwei/
workshop/
to characterize, for the first time, the reason and/or
type of similarity. STS reduces the problem of judg-
ing similarity to a single number, but, in some appli-
cations, it is important to characterize why and how
two items are deemed similar, hence the added nu-
ance. The dataset comprises pairs of Cultural Her-
itage items from Europeana,2 a single access point
to millions of books, paintings, films, museum ob-
jects and archival records that have been digitized
throughout Europe. It is an authoritative source of
information coming from European cultural and sci-
entific institutions. Typically, the items comprise
meta-data describing a cultural heritage item and,
sometimes, a thumbnail of the item itself.
Participating systems in the TYPED task need to
compute the similarity between items, using the tex-
tual meta-data. In addition to general similarity, par-
ticipants need to score specific kinds of similarity,
like similar author, similar time period, etc. (cf. Fig-
ure 3).
The paper is structured as follows. Section 2 re-
ports the sources of the texts used in the two tasks.
Section 3 details the annotation procedure. Section
4 presents the evaluation of the systems, followed
by the results of CORE and TYPED tasks. Section 6
draws on some conclusions and forward projections.
2http://www.europeana.eu/
33
Figure 2: Annotation instructions for CORE task
year dataset pairs source
2012 MSRpar 1500 news
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 news
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2013 TYPED 1500 Cultural Heritage items
Table 1: Summary of STS 2012 and 2013 datasets.
2 Source Datasets
Table 1 summarizes the 2012 and 2013 datasets.
2.1 CORE task
The CORE dataset comprises pairs of news head-
lines (HDL), MT evaluation sentences (SMT) and
pairs of glosses (OnWN and FNWN).
For HDL, we used naturally occurring news head-
lines gathered by the Europe Media Monitor (EMM)
engine (Best et al, 2005) from several different news
sources. EMM clusters together related news. Our
goal was to generate a balanced data set across the
different similarity ranges, hence we built two sets
of headline pairs: (i) a set where the pairs come
from the same EMM cluster, (ii) and another set
where the headlines come from a different EMM
cluster, then we computed the string similarity be-
tween those pairs. Accordingly, we sampled 375
headline pairs of headlines that occur in the same
EMM cluster, aiming for pairs equally distributed
between minimal and maximal similarity using sim-
ple string similarity. We sample another 375 pairs
from the different EMM cluster in the same manner.
The SMT dataset comprises pairs of sentences
used in machine translation evaluation. We have two
different sets based on the evaluation metric used:
an HTER set, and a HYTER set. Both metrics use
the TER metric (Snover et al, 2006) to measure the
similarity of pairs. HTER typically relies on several
(1-4) reference translations. HYTER, on the other
hand, leverages millions of translations. The HTER
set comprises 150 pairs, where one sentence is ma-
chine translation output and the corresponding sen-
tence is a human post-edited translation. We sam-
ple the data from the dataset used in the DARPA
GALE project with an HTER score ranging from 0
to 120. The HYTER set has 600 pairs from 3 sub-
sets (each subset contains 200 pairs): a. reference
34
Figure 3: Annotation instructions for TYPED task
vs. machine translation. b. reference vs. Finite State
Transducer (FST) generated translation (Dreyer and
Marcu, 2012). c. machine translation vs. FST gen-
erated translation. The HYTER data set is used in
(Dreyer and Marcu, 2012).
The OnWN/FnWN dataset contains gloss pairs
from two sources: OntoNotes-WordNet (OnWN)
and FrameNet-WordNet (FnWN). These pairs are
sampled based on the string similarity ranging from
0.4 to 0.9. String similarity is used to measure the
similarity between a pair of glosses. The OnWN
subset comprises 561 gloss pairs from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.0 (Fellbaum,
1998). 370 out of the 561 pairs are sampled from the
110K sense-mapped pairs as made available from
the authors. The rest, 291 pairs, are sampled from
unmapped sense pairs with a string similarity rang-
ing from 0.5 to 0.9. The FnWN subset has 189
manually mapped pairs of senses from FrameNet 1.5
(Baker et al, 1998) to WordNet 3.1. They are ran-
domly selected from 426 mapped pairs. In combi-
nation, both datasets comprise 750 pairs of glosses.
2.2 Typed-similarity TYPED task
This task is devised in the context of the PATHS
project,3 which aims to assist users in accessing
digital libraries looking for items. The project
tests methods that offer suggestions about items that
might be useful to recommend, to assist in the inter-
pretation of the items, and to support the user in the
discovery and exploration of the collections. Hence
the task is about comparing pairs of items. The pairs
are generated in the Europeana project.
A study in the PATHS project suggested that users
would be interested in knowing why the system is
suggesting related items. The study suggested seven
similarity types: similar author or creator, similar
people involved, similar time period, similar loca-
3http://www.paths-project.eu
35
Figure 4: TYPED pair on our survey. Only general and author similarity types are shown.
tion, similar event or action, similar subject and sim-
ilar description. In addition, we also include general
similarity. Figure 3 shows the definition of each sim-
ilarity type as provided to the annotators.
The dataset is generated in semi-automatically.
First, members of the project manually select 25
pairs of items for each of the 7 similarity types (ex-
cluding general similarity), totalling 175 manually
selected pairs. After removing duplicates and clean-
ing the dataset, we got 163 pairs. Second, we use
these manually selected pairs as seeds to automat-
ically select new pairs as follows: Starting from
those seeds, we use the Europeana API to get similar
items, and we repeat this process 5 times in order to
diverge from the original items (we stored the vis-
ited items to avoid looping). Once removed from
the seed set, we select the new pairs following two
approaches:
? Distance 1: Current item and similar item.
? Distance 2: Current item and an item that is
similar to a similar item (twice removed dis-
tance wise)
This yields 892 pairs for Distance 1 and 445 of
Distance 2. We then divide the data into train and
test, preserving the ratios. The train data contains
82 manually selected pairs, 446 pairs with similarity
distance 1 and 222 pairs with similarity distance 2.
The test data follows a similar distribution.
Europeana items cannot be redistributed, so we
provide their urls and a script which uses the official
36
Europeana API to access and extract the correspond-
ing metadata in JSON format and a thumbnail. In
addition, the textual fields which are relevant for the
task are made accessible in text files, as follows:
? dcTitle: title of the item
? dcSubject: list of subject terms (from some vo-
cabulary)
? dcDescription: textual description of the item
? dcCreator: creator(s) of the item
? dcDate: date(s) of the item
? dcSource: source of the item
3 Annotation
3.1 CORE task
Figure 1 shows the explanations and values for
each score between 5 and 0. We use the Crowd-
Flower crowd-sourcing service to annotate the
CORE dataset. Annotators are presented with the
detailed instructions given in Figure 2 and are asked
to label each STS sentence pair on our 6 point scale
using a dropdown box. Five sentence pairs at a time
are presented to annotators. Annotators are paid
0.20 cents per set of 5 annotations and we collect
5 separate annotations per sentence pair. Annota-
tors are restricted to people from the following coun-
tries: Australia, Canada, India, New Zealand, UK,
and US.
To obtain high quality annotations, we create a
representative gold dataset of 105 pairs that are man-
ually annotated by the task organizers. During an-
notation, one gold pair is included in each set of 5
sentence pairs. Crowd annotators are required to
rate 4 of the gold pairs correct to qualify to work
on the task. Gold pairs are not distinguished in any
way from the non-gold pairs. If the gold pairs are
annotated incorrectly, annotators are told what the
correct annotation is and they are given an explana-
tion of why. CrowdFlower automatically stops low
performing annotators ? those with too many incor-
rectly labeled gold pairs ? from working on the task.
The distribution of scores in the headlines HDL
dataset is uniform, as in FNWN and OnWN, al-
though the scores are slightly lower in FNWN and
slightly higher in OnWN. The scores for SMT are
not uniform, with most of the scores uniformly dis-
tributed between 3.5 and 5, a few pairs between 2
and 3.5, and nearly no pairs with values below 2.
3.2 TYPED task
The dataset is annotated using crowdsourcing. The
survey contains the 1500 pairs of the dataset (750 for
train and 750 for test), plus 20 gold pairs for quality
control. Each participant is shown 4 training gold
questions at the beginning, and then one gold every
2 or 4 questions depending on the accuracy. If accu-
racy dropped to less than 66.7% percent the survey
is stopped and the answers from that particular an-
notator are discarded. Each annotator is allowed to
rate a maximum of 20 pairs to avoid getting answers
from people that are either tired or bored. To ensure
a good comprehension of the items, the task is re-
stricted to only accept annotators from some English
speaking countries: UK, USA, Australia, Canada
and New Zealand.
Participants are asked to rate the similarity be-
tween pairs of cultural heritage items from rang-
ing from 5 to 0, following the instructions shown
in Figure 3. We also add a ?Not Applicable? choice
for cases in which annotators are not sure or didn?t
know. For those cases, we calculate the similarity
score using the values of the rest of the annotators (if
none, we convert it to 0). The instructions given to
the annotators are the ones shown in Figure 3. Fig-
ure 4 shows a pair from the dataset, as presented to
annotators.
The similarity scores for the pairs follow a similar
distribution in all types. Most of the pairs have a
score between 4 and 5, which can amount to as much
as 50% of all pairs in some types.
3.3 Quality of annotation
In order to assess the annotation quality, we measure
the correlation of each annotator with the average of
the rest of the annotators. We then averaged all the
correlations. This method to estimate the quality is
identical to the method used for evaluation (see Sec-
tion 4.1) and it can be thus used as the upper bound
for the systems. The inter-tagger correlation in the
CORE dataset for each of dataset is as follows:
? HDL: 85.0%
? FNWN: 69.9%
? OnWN: 87.2%
? SMT: 65.8%
For the TYPED dataset, the inter-tagger correla-
tion values for each type of similarity is as follows:
? General: 77.0%
37
? Author: 73.1%
? People Involved: 62.5%
? Time period: 72.0%
? Location: 74.3%
? Event or Action: 63.9%
? Subject: 74.5%
? Description: 74.9%
In both datasets, the correlation figures are high,
confirming that the task is well designed. The weak-
est correlations in the CORE task are SMT and
FNWN. The first might reflect the fact that some
automatically produced translations are confusing
or difficult to understand, and the second could be
caused by the special style used to gloss FrameNet
concepts. In the TYPED task the weakest correla-
tions are for the People Involved and Event or Action
types, as they might be the most difficult to spot.
4 Systems Evaluation
4.1 Evaluation metrics
Evaluation of STS is still an open issue. STS ex-
periments have traditionally used Pearson product-
moment correlation, or, alternatively, Spearman
rank order correlation. In addition, we also need a
method to aggregate the results from each dataset
into an overall score. The analysis performed in
(Agirre and Amigo?, In prep) shows that Pearson and
averaging across datasets are the best suited com-
bination in general. In particular, Pearson is more
informative than Spearman, in that Spearman only
takes the rank differences into account, while Pear-
son does account for value differences as well. The
study also showed that other alternatives need to be
considered, depending on the requirements of the
target application.
We leave application-dependent evaluations for
future work, and focus on average weighted Pear-
son correlation. When averaging, we weight each
individual correlation by the size of the dataset.
In addition, participants in the CORE task are al-
lowed to provide a confidence score between 1 and
100 for each of their scores. The evaluation script
down-weights the pairs with low confidence, follow-
ing weighted Pearson.4 In order to compute sta-
tistical significance among system results, we use
4http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
a one-tailed parametric test based on Fisher?s z-
transformation (Press et al, 2002, equation 14.5.10).
4.2 The Baseline Systems
For the CORE dataset, we produce scores using a
simple word overlap baseline system. We tokenize
the input sentences splitting at white spaces, and
then represent each sentence as a vector in the mul-
tidimensional token space. Each dimension has 1
if the token is present in the sentence, 0 otherwise.
Vector similarity is computed using the cosine sim-
ilarity metric. We also run two freely available sys-
tems, DKPro (Bar et al, 2012) and TakeLab (S?aric? et
al., 2012) from STS 2012,5 and evaluate them on the
CORE dataset. They serve as two strong contenders
since they ranked 1st (DKPro) and 2nd (TakeLab) in
last year?s STS task.
For the TYPED dataset, we first produce XML
files for each of the items, using the fields as pro-
vided to participants. Then we run named entity
recognition and classification (NERC) and date de-
tection using Stanford CoreNLP. This is followed by
calculating the similarity score for each of the types
as follows.
? General: cosine similarity of TF-IDF vectors of
tokens from all fields.
? Author: cosine similarity of TF-IDF vectors for
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF-IDF vectors of loca-
tion/date/people recognized by NERC in all
fields.
? Events: cosine similarity of TF-IDF vectors of
verbs in all fields.
? Subject and description: cosine similarity of
TF-IDF vectors of respective fields.
IDF values are calculated from a subset of the
Europeana collection (Culture Grid collection). We
also run a random baseline several times, yielding
close to 0 correlations in all datasets, as expected.
4.3 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 34
teams participated in the CORE task, submitting 89
5Code is available at http://www-nlp.stanford.
edu/wiki/STS
38
Team and run Head. OnWN FNWN SMT Mean # Team and run Head. OnWN FNWN SMT Mean #
baseline-tokencos .5399 .2828 .2146 .2861 .3639 73 KnCe2013-all .3475 .3505 .1073 .1551 .2639 86
DKPro .7347 .7345 .3405 .3256 .5652 - KnCe2013-diff .4028 .3537 .1284 .1804 .2934 84
TakeLab-best .6559 .6334 .4052 .3389 .5221 - KnCe2013-set .0462 -.1526 .0376 -.0605 -.0397 90
TakeLab-sts12 .4858 .6334 .2693 .2787 .4340 - LCL Sapienza-ADW1 .6943 .4661 .3571 .3311 .4880 43
aolney-w3c3 .5248 .4701 .1777 .2744 .3986 67 LCL Sapienza-ADW2 .6520 .5280 .3598 .3681 .5019 32
BGU-1 .5075 .3252 .0768 .1843 .3181 81 LCL Sapienza-ADW3 .6205 .5108 .4462 .3838 .4996 34
BGU-2 .3608 .3777 -.0173 .0698 .2363 88 LIPN-tAll .7063 .6937 .4037 .3005 .5425 16
BGU-3 .3591 .3360 .0072 .2122 .2748 85 LIPN-tSp .5791 .7199 .3522 .3721 .5261 24
BUAP-RUN1 .5005 .2579 .1766 .2322 .3234 78 MayoClinicNLP-r1wtCDT .6584 .7775 .3735 .3605 .5649 6
BUAP-RUN2 .4860 .2872 .2082 .2117 .3216 79 MayoClinicNLP-r2CDT .6827 .6612 .3960 .3946 .5572 8
BUAP-RUN3 .4817 .2711 .2511 .1990 .3156 82 MayoClinicNLP-r3wtCD .6440 .8295 .3202 .3561 .5671 5
CFILT-1 .5336 .2381 .2261 .2906 .3531 75 NTNU-RUN1 .7279 .5952 .3215 .4015 .5519 9
CLaC-RUN1 .6774 .7667 .3793 .3068 .5511 10 NTNU-RUN2 .5909 .1634 .3650 .3786 .3946 68
CLaC-RUN2 .6921 .7366 .3793 .3375 .5587 7 NTNU-RUN3 .7274 .5882 .3115 .4035 .5498 12
CLaC-RUN3 .5276 .6495 .4158 .3082 .4755 47 PolyUCOMP-RUN1 .5176 .1517 .2496 .2914 .3284 77
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 SOFTCARDINALITY-run1 .6410 .7360 .3442 .3035 .5273 23
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 SOFTCARDINALITY-run2 .6713 .7412 .3838 .2981 .5402 18
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 SOFTCARDINALITY-run3 .6603 .7401 .3347 .2900 .5294 22
CPN-combined.RandSubSpace .6771 .5135 .3314 .3369 .4939 39 sriubc-System1? .6083 .2915 .2790 .3065 .4011 66
CPN-combined.SVM .6685 .5096 .3621 .3408 .4939 38 sriubc-System2? .6359 .3664 .2713 .3476 .4420 57
CPN-individual.RandSubSpace .6771 .5484 .3314 .2769 .4826 45 sriubc-System3? .5443 .2843 .2705 .3275 .3842 70
DeepPurple-length .6542 .5105 .2507 .2803 .4598 56 SXUCFN-run1 .6806 .5355 .3181 .3980 .5198 27
DeepPurple-linear .6878 .5105 .2693 .2787 .4721 50 SXUCFN-run2 .4881 .6146 .4237 .3844 .4797 46
DeepPurple-lineara .6227 .5105 .3265 .2952 .4607 55 SXUCFN-run3 .6761 .6481 .3025 .4003 .5458 14
deft-baseline .6532 .8431 .5083 .3265 .5795 3 SXULLL-1 .4840 .7146 .0415 .1543 .3944 69
deft-baseline2 .5706 .8111 .5503 .3325 .5495 13 UCam-A .5510 .3099 .2385 .1171 .3200 80
DLS@CU-char .3867 .2386 .3726 .3337 .3309 76 UCam-B .6399 .4440 .3995 .3400 .4709 53
DLS@CU-charSemantic .4669 .4165 .3859 .3411 .4056 64 UCam-C .4962 .5639 .1724 .3006 .4207 62
DLS@CU-charWordSemantic .4921 .3769 .4647 .3492 .4135 63 UCSP-NC? .1736 .0853 .1151 .1658 .1441 89
ECNUCS-Run1 .5656 .2083 .1725 .2949 .3533 74 UMBC EBIQUITY-galactus .7428 .7053 .5444 .3705 .5927 2
ECNUCS-Run2 .7120 .5388 .2013 .2504 .4720 51 UMBC EBIQUITY-ParingWords .7642 .7529 .5818 .3804 .6181 1
ECNUCS-Run3 .6799 .5284 .2203 .3595 .4967 35 UMBC EBIQUITY-saiyan .7838 .5593 .5815 .3563 .5683 4
HENRY-run1 .7601 .4631 .3516 .2801 .4917 41 UMCC DLSI-1 .5841 .4847 .2917 .2855 .4352 58
HENRY-run2 .7645 .4631 .3905 .3593 .5229 26 UMCC DLSI-2 .6168 .5557 .3045 .3407 .4833 44
HENRY-run3 .7103 .3934 .3364 .3308 .4734 48 UMCC DLSI-3 .3846 .1342 -.0065 .2736 .2523 87
IBM EG-run2 .7217 .6110 .3364 .3460 .5365 19 UNIBA-2STEPSML .4255 .4801 .1832 .2710 .3673 71
IBM EG-run5 .7410 .5987 .4133 .3426 .5452 15 UNIBA-DSM PERM .6319 .4910 .2717 .3155 .4610 54
IBM EG-run6 .7447 .6257 .4381 .3275 .5502 11 UNIBA-STACKING .6275 .4658 .2111 .2588 .4293 61
ikernels-sys1 .7352 .5432 .3842 .3180 .5188 28 Unimelb NLP-bahar .7119 .3490 .3813 .3507 .4733 49
ikernels-sys2 .7465 .5572 .3875 .3409 .5339 21 Unimelb NLP-concat .7085 .6790 .3374 .3230 .5415 17
ikernels-sys3 .7395 .4228 .3596 .3294 .4919 40 Unimelb NLP-stacking .7064 .6140 .1865 .3144 .5091 29
INAOE-UPV-run1 .6392 .3249 .2711 .3491 .4332 59 Unitor-SVRegressor run1 .6353 .5744 .3521 .3285 .4941 37
INAOE-UPV-run2 .6390 .3260 .2662 .3457 .4319 60 Unitor-SVRegressor run2 .6511 .5610 .3580 .3096 .4902 42
INAOE-UPV-run3 .6468 .6295 .4090 .3047 .5085 31 Unitor-SVRegressor run3 .6027 .5489 .3269 .3192 .4716 52
KLUE-approach 1 .6521 .6507 .3996 .3367 .5254 25 UPC-AE .6092 .5679 -.1268 .2090 .4037 65
KLUE-approach 2 .6510 .6869 .4189 .3360 .5355 20 UPC-AED .4136 .4770 -.0852 .1662 .3050 83
UPC-AED T .5119 .6386 -.0464 .1235 .3671 72
Table 2: Results on the CORE task. The first rows on the left correspond to the baseline and to two publicly available
systems, see text for details. Note: ? signals team involving one of the organizers, ? for systems submitting past the
120 hour window.
system runs. For the TYPED task, 6 teams partici-
pated, submitting 14 system runs.6
Some submissions had minor issues: one team
had a confidence score of 0 for all items (we re-
placed them by 100), and another team had a few
Not-a-Number scores for the SMT dataset, which
we replaced by 5. One team submitted the results
past the 120 hours. This team, and the teams that in-
6Due to lack of space we can?t detail the full names of au-
thors and institutions that participated.The interested reader can
use the name of the runs in Tables 2 and 3 to find the relevant
paper in these proceedings.
cluded one of the organizers, are explicitly marked.
We want to stress that in these teams the organizers
did not allow the developers of the system to access
any data or information which was not available for
the rest of participants. After the submission dead-
line expired, the organizers published the gold stan-
dard in the task website, in order to ensure a trans-
parent evaluation process.
4.4 CORE Task Results
Table 2 shows the results of the CORE task, with
runs listed in alphabetical order. The correlation in
39
Team and run General Author People involved Time Location Event Subject Description Mean #
baseline .6691 .4278 .4460 .5002 .4835 .3062 .5015 .5810 .4894 8
BUAP-RUN1 .6798 .6166 .0670 .2761 .0163 .1612 .5167 .5283 .3577 14
BUAP-RUN2 .6745 .6093 .1285 .3721 .0163 .1660 .5094 .5546 .3788 13
BUAP-RUN3 .6992 .6345 .1055 .1461 .0000 -.0668 .3729 .5120 .3004 15
BUT-1 .3686 .7468 .3920 .5725 .3604 .2906 .2270 .5882 .4433 9
ECNUCS-Run1 .6040 .7362 .3663 .4685 .3844 .4057 .5229 .6027 .5113 5
ECNUCS-Run2 .6064 .5684 .3663 .4685 .3844 .4057 .5563 .6027 .4948 7
PolyUCOMP-RUN1 .4888 .6940 .3223 .3820 .3621 .1625 .3962 .4816 .4112 12
PolyUCOMP-RUN2 .4893 .6940 .3253 .3777 .3628 .1968 .3962 .4816 .4155 11
PolyUCOMP-RUN3 .4915 .6940 .3254 .3737 .3667 .2207 .3962 .4816 .4187 10
UBC UOS-RUN1? .7256 .4568 .4467 .5762 .4858 .3090 .5015 .5810 .5103 6
UBC UOS-RUN2? .7457 .6618 .6518 .7466 .7244 .6533 .7404 .7751 .7124 4
UBC UOS-RUN3? .7461 .6656 .6544 .7411 .7257 .6545 .7417 .7763 .7132 3
Unitor-SVRegressor lin .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341 2
Unitor-SVRegressor rbf .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620 1
Table 3: Results on TYPED task. The first row corresponds to the baseline. Note: ? signals team involving one of the
organizers.
each dataset is given, followed by the mean cor-
relation (the official measure), and the rank of the
run. The baseline ranks 73. The highest correla-
tions are for OnWN (84%, by deft) and HDL (78%,
by UMBC), followed by FNWN (58%, by UMBC)
and SMT (40%, by NTNU). This fits nicely with the
inter-tagger correlations (respectively 87, 85, 70 and
65, cf. Section 3). It also shows that the systems get
close to the human correlations in the OnWN and
HDL dataset, with bigger differences for FNWN and
SMT.
The result of the best run (by UMBC) is signif-
icantly different (p-value < 0.05) than all runs ex-
cept the second best. The second best run is only
significantly different to the runs ranking 7th and
below, and the third best to the 14th run and be-
low. The difference between consecutive runs was
not significant. This indicates that many system runs
performed very close to each other.
Only 13 runs included non-uniform confidence
scores. In 10 cases the confidence value allowed
to improve performance, sometimes as much as .11
absolute points. For instance, SXUCFN-run3 im-
proves from .4773 to .5458. The most notable ex-
ception is MayoClinicNLP-r2CDT, which achieves
a mean correlation of .5879 instead of .5572 if they
provide uniform confidence values.
The Table also shows the results of TakeLab
and DKPro. We train the DKPro and TakeLab-
sts12 models on all the training and test STS 2012
data. We additionally train another variant sys-
tem of TakeLab, TakeLab-best, where we use tar-
geted training where the model yields the best per-
formance for each test subset as follows: (1) HDL
is trained on MSRpar 2012 data; (2) OnWN is
trained on all 2012 data; (3) FnWN is trained on
2012 OnWN data; (4) SMT is trained on 2012 SM-
Teuroparl data. Note that Takelab-best is an upper
bound, as the best combination is selected on the
test dataset. TakeLab-sts12, TakeLab-best, DKPro
rank as 58th, 27th and 6th in this year?s system sub-
missions, respectively. The different results yielded
from TakeLab depending on the training data sug-
gests that some STS systems are quite sensitive to
the source of the sentence pairs, indicating that do-
main adaptation techniques could have a role in this
task. On the other hand, DKPro performed ex-
tremely well when trained on all available training,
with no special tweaking for each dataset.
4.5 TYPED Task Results
Table 3 shows the results of TYPED task. The
columns show the correlation for each type of sim-
ilarity, followed by the mean correlation (the offi-
cial measure), and the rank of the run. The best sys-
tem (from Unitor) is best in all types. The baseline
ranked 8th, but the performance difference with the
best system is quite significant. The best result is
significantly different (p-value < 0.02) to all runs.
The second and third best runs are only significantly
different from the run ranking 5th and below. Note
that in this dataset the correlations of the best system
are higher than the inter-tagger correlations. This
might indicate that the task has been solved, in the
sense that the features used by the top systems are
enough to characterize the problem and reach hu-
man performance, although the correlations of some
40
A
cr
on
ym
s
D
is
tr
ib
ut
io
na
lm
em
or
y
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
O
pi
ni
on
an
d
S
en
ti
m
en
t
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
ik
ti
on
ar
y
W
or
d
em
be
dd
in
gs
W
or
dN
et
C
or
re
fe
re
nc
e
D
ep
en
de
nc
y
pa
rs
e
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
D
A
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
L
og
ic
al
in
fe
re
nc
e
M
et
ap
ho
r
or
M
et
on
ym
y
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
R
O
U
G
E
pa
ck
ag
e
S
co
pi
ng
S
ea
rc
h
en
gi
ne
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
aolney-w3c3 x x x
BGU-1 x x x x x x x
BGU-2 x x x x x x x
BGU-3 x x x x x x x
CFILT-APPROACH x x x x x
CLaC-Run1 x x x x x x x x
CLaC-Run2 x x x x x x x x
CLaC-Run3 x x x x x x x x
CNGL-LPSSVR x x x x x
CNGL-LPSSVRTL x x x x x
CNGL-LSSVR x x x x x
CPN-combined.RandSubSpace x x x x x x x x
CPN-combined.SVM x x x x x x x x
CPN-individual.RandSubSpace x x x x x x x x
DeepPurple-length x x x x x x x
DeepPurple-linear x x x x x x x
DeepPurple-lineara x x x x x x x
deft-baseline x x x x
deft-baseline x x x x x x
DLS@CU-charSemantic x x x x
DLS@CU-charWordSemantic x x x x x x
DLS@CU-charWordSemantic x x x
ECNUCS-Run1 x x x x x x x
ECNUCS-Run2 x x x x x x x
ECNUCS-Run3 x x x x x x x
HENRY-run1 x x x x x x x x x
HENRY-run2 x x x x x x x x
IBM EG-run2 x x x x x x
IBM EG-run5 x x x x x x
IBM EG-run6 x x x x x
ikernels-sys1 x x x x x x x x x x x
ikernels-sys2 x x x x x x x x x x x
ikernels-sys3 x x x x x x x x x x x
INAOE-UPV-run1 x x x x x x x
INAOE-UPV-run2 x x x x x x x
INAOE-UPV-run3 x x x x x x x
KLUE-approach 1 x x x x x x x
KLUE-approach 2 x x x x x x
KnCe2013-all x x x x x x x x
KnCe2013-div x x x x x x x x
KnCe2013-div x x x x x x x x
LCL Sapienza-ADW1 x x x
LCL Sapienza-ADW2 x x x
LCL Sapienza-ADW3 x x x
LIPN-tAll x x x x x x x x x x
LIPN-tSp x x x x x x x x x x
MayoClinicNLP-r1wtCDT x x x x x x x x x x x x
MayoClinicNLP-r2CDT x x x x x x x x x x x x
MayoClinicNLP-r3wtCD x x x x x x x x x x x x
NTNU-RUN1 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN2 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN3 x x x x x x x x x x x x x x x x x x x x x x x
PolyUCOMP-RUN1 x x x x
SOFTCARDINALITY-run1 x
SOFTCARDINALITY-run2 x x x
SOFTCARDINALITY-run3 x x x
SXUCFN-run1 x x x
SXUCFN-run2 x x x
SXUCFN-run3 x x x
SXULLL-1 x x
UCam-A x x x x
UCam-B x x x x
UCam-C x x x x
UCSP-NC x x x x x
UMBC EBIQUITY-galactus x x x x x x x
UMBC EBIQUITY-ParingWords x x x x x x
UMBC EBIQUITY-saiyan x x x x x x x
UMCC DLSI-1 x x x x x x x x x x
UMCC DLSI-2 x x x x x x x x x x
UMCC DLSI-3 x x x x x x x x x
UNIBA-2STEPSML x x x x x x x x x x x
UNIBA-DSM PERM x x x x x x
UNIBA-STACKING x x x x x x x x x x x
Unimelb NLP-bahar x x
Unimelb NLP-concat x x x x x x x x x x
Unimelb NLP-stacking x x x x x x x x x x
Unitor-SVRegressor run1 x x x x x x
Unitor-SVRegressor run2 x x x x x x
Unitor-SVRegressor run3 x x x x x x
Total 11 2 12 54 12 5 11 36 7 3 54 3 3 48 40 2 67 14 3 3 10 24 55 3 3 4 9 6 34 9 13 6 6
Table 4: CORE task: Resources and tools used by the systems that submitted a description file. Leftmost columns
correspond to the resources, and rightmost to tools, in alphabetic order.
41
types could be too low for practical use.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, making special emphasis on the tools
and resources that were used. Tables 4 and 5 show
schematically the tools and resources as reported by
some of the participants for the CORE and TYPED
tasks (respectively). In the last row, the totals show
that WordNet and monolingual corpora were the
most used resources for both tasks, followed by
Wikipedia and the use of acronyms (for CORE and
TYPED tasks respectively). Dictionaries, multilin-
gual corpora, opinion and sentiment analysis, and
lists and tables of paraphrases are also used.
For CORE, generic NLP tools such as lemmati-
zation and PoS tagging are widely used, and to a
lesser extent, distributional similarity, knowledge-
based similarity, syntactic analysis, named entity
recognition, lexical substitution and time and date
resolution (in this order). Other popular tools are
Semantic Role Labeling, Textual Entailment, String
Similarity, Tree Kernels and Word Sense Disam-
biguation. Machine learning is widely used to com-
bine and tune components (and so, it is not men-
tioned in the tables). Several less used tools are
also listed but are used by three or less systems.
The top scoring systems use most of the resources
and tools listed (UMBC EBIQUITY-ParingWords,
MayoClinicNLP-r3wtCD). Other well ranked sys-
tems like deft-baseline are only based on distribu-
tional similarity. Although not mentioned in the
descriptions files, some systems used the publicly
available DKPro and Takelab systems.
For the TYPED task, the most used tools are lem-
matizers, Named Entity Recognizers, and PoS tag-
gers. Distributional and Knowledge-base similarity
is also used, and at least four systems used syntactic
analysis and time and date resolution.7
6 Conclusions and Future Work
We presented the 2013 *SEM shared task on Seman-
tic Textual Similarity.8 Two tasks were defined: a
7For a more detailed analysis, the reader is directed to the
papers in this volume.
8All annotations, evaluation scripts and system outputs are
available in the website for the task9. In addition, a collabora-
tively maintained site10, open to the STS community, contains
A
cr
on
ym
s
M
on
ol
in
gu
al
co
rp
or
a
W
ik
ip
ed
ia
W
or
dN
et
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
yn
ta
x
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
BUT-1 x x x x x x x
PolyUCOMP-RUN2 x x x x
ECNUCS-Run1 x x x
ECNUCS-Run2 x x x x x x x
PolyUCOMP-RUN1 x x x x
PolyUCOMP-RUN3 x x x x
UBC UOS-RUN1 x x x x x x x x x x x
UBC UOS-RUN2 x x x x x x x x x x x x
UBC UOS-RUN3 x x x x x x x x x x x x
Unitor-SVRegressor lin x x x x x x x
Unitor-SVRegressor rbf x x x x x x x
Total 4 7 3 7 7 4 11 3 11 11 4 4 2
Table 5: TYPED task: Resources and tools used by
the systems that submitted a description file. Leftmost
columns correspond to the resources, and rightmost to
tools, in alphabetic order.
core task CORE similar to the STS 2012 task, and
a new pilot on typed-similarity TYPED. We had 34
teams participate in both tasks submitting 89 system
runs for CORE and 14 system runs for TYPED, in
total amounting to a 103 system evaluations. CORE
uses datasets which are related to but different from
those used in 2012: news headlines, MT evalua-
tion data, gloss pairs. The best systems attained
correlations close to the human inter tagger corre-
lations. The TYPED task characterizes, for the first
time, the reasons why two items are deemed simi-
lar. The results on TYPED show that the training
data provided allowed systems to yield high corre-
lation scores, demonstrating the practical viability
of this new task. In the future, we are planning on
adding more nuanced evaluation data sets that in-
clude modality (belief, negation, permission, etc.)
and sentiment. Also given the success rate of the
TYPED task, however, the data in this pilot is rel-
atively structured, hence in the future we are inter-
ested in investigating identifying reasons why two
pairs of unstructured texts as those present in CORE
are deemed similar.
Acknowledgements
We are grateful to the OntoNotes team for sharing OntoNotes
to WordNet mappings (Hovy et al 2006). We thank Lan-
guage Weaver, INC, DARPA and LDC for providing the SMT
data. This work is also partially funded by the Spanish Ministry
of Education, Culture and Sport (grant FPU12/06243). This
a comprehensive list of evaluation tasks, datasets, software and
papers related to STS.
42
work was partially funded by the DARPA BOLT and DEFT pro-
grams.
We want to thank Nikolaos Aletras, German Rigau and
Mark Stevenson for their help designing, annotating and col-
lecting the typed-similarity data. The development of the
typed-similarity dataset was supported by the PATHS project
(http://paths-project.eu) funded by the European Community?s
Seventh Framework Program (FP7/2007-2013) under grant
agreement no. 270082. The tasks were partially financed by
the READERS project under the CHIST-ERA framework (FP7
ERA-Net). We thank Europeana and all contributors to Euro-
peana for sharing their content through the API.
References
Eneko Agirre and Enrique Amigo?. In prep. Exploring
evaluation measures for semantic textual similarity. In
Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING ?98
Proceedings of the 17th international conference on
Computational linguistics - Volume 1.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, in conjunction with the
1st Joint Conference on Lexical and Computational
Semantics.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo Gar-
cia, and David Horby. 2005. Europe media monitor -
system description. In EUR Report 22173-En, Ispra,
Italy.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation evalua-
tion. In Human Language Technologies: Conference
of the North American Chapter of the Association of
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2002. Numerical Recipes: The Art of Sci-
entific Computing V 2.10 With Linux Or Single-Screen
License. Cambridge University Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
43
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 132?137, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UBC UOS-TYPED: Regression for Typed-similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Nikolaos Aletras
University of Sheffield
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
German Rigau
University of the Basque Country
Donostia, 20018, Basque Country
german.rigau@ehu.es
Mark Stevenson
University of Sheffield
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
We approach the typed-similarity task using
a range of heuristics that rely on information
from the appropriate metadata fields for each
type of similarity. In addition we train a linear
regressor for each type of similarity. The re-
sults indicate that the linear regression is key
for good performance. Our best system was
ranked third in the task.
1 Introduction
The typed-similarity dataset comprises pairs of Cul-
tural Heritage items from Europeana1, a single ac-
cess point to digitised versions of books, paintings,
films, museum objects and archival records from in-
stitutions throughout Europe. Typically, the items
comprise meta-data describing a cultural heritage
item and, sometimes, a thumbnail of the item itself.
Participating systems need to compute the similarity
between items using the textual meta-data. In addi-
tion to general similarity, the dataset includes spe-
cific kinds of similarity, like similar author, similar
time period, etc.
We approach the problem using a range of sim-
ilarity techniques for each similarity types, these
make use of information contained in the relevant
meta-data fields.In addition, we train a linear regres-
sor for each type of similarity, using the training data
provided by the organisers with the previously de-
fined similarity measures as features.
We begin by describing our basic system in Sec-
tion 2, followed by the machine learning system in
1http://www.europeana.eu/
Section 3. The submissions are explained in Section
4. Section 5 presents our results. Finally, we draw
our conclusions in Section 6.
2 Basic system
The items in this task are taken from Europeana.
They cannot be redistributed, so we used the urls
and scripts provided by the organizers to extract the
corresponding metadata. We analysed the text in the
metadata, performing lemmatization, PoS tagging,
named entity recognition and classification (NERC)
and date detection using Stanford CoreNLP (Finkel
et al, 2005; Toutanova et al, 2003). A preliminary
score for each similarity type was then calculated as
follows:
? General: cosine similarity of TF.IDF vectors of
tokens, taken from all fields.
? Author: cosine similarity of TF.IDF vectors of
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF.IDF vectors of loca-
tion/date/people entities recognized by NERC
in all fields.
? Events: cosine similarity of TF.IDF vectors of
event verbs and nouns. A list of verbs and
nouns possibly denoting events was derived us-
ing the WordNet Morphosemantic Database2.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields.
IDF values were calculated using a subset of Eu-
ropeana items (the Culture Grid collection), avail-
able internally. These preliminary scores were im-
2urlhttp://wordnetcode.princeton.edu/standoff-
files/morphosemantic-links.xls
132
proved using TF.IDF based on Wikipedia, UKB
(Agirre and Soroa, 2009) and a more informed time
similarity measure. We describe each of these pro-
cesses in turn.
2.1 TF.IDF
A common approach to computing document sim-
ilarity is to represent documents as Bag-Of-Words
(BOW). Each BOW is a vector consisting of the
words contained in the document, where each di-
mension corresponds to a word, and the weight is
the frequency in the corresponding document. The
similarity between two documents can be computed
as the cosine of the angle between their vectors. This
is the approached use above.
This approach can be improved giving more
weight to words which occur in only a few docu-
ments, and less weight to words occurring in many
documents (Baeza-Yates and Ribeiro-Neto, 1999).
In our system, we count document frequencies of
words using Wikipedia as a reference corpus since
the training data consists of only 750 items associ-
ated with short textual information and might not be
sufficient for reliable estimations. The TF.IDF sim-
ilarity between items a and b is defined as:
simtf.idf(a, b) =
?
w?a,b tfw,a ? tfw,b ? idf
2
w
??
w?a(tfw,a ? idfw)
2 ?
??
w?b(tfw,b ? idfw)
2
where tfw,x is the frequency of the term w in x ?
{a, b} and idfw is the inverted document frequency
of the word w measured in Wikipedia. We substi-
tuted the preliminary general similarity score by the
obtained using the TF.IDF presented in this section.
2.2 UKB
The semantic disambiguation UKB3 algorithm
(Agirre and Soroa, 2009) applies personalized
PageRank on a graph generated from the English
WordNet (Fellbaum, 1998), or alternatively, from
Wikipedia. This algorithm has proven to be very
competitive in word similarity tasks (Agirre et al,
2010).
To compute similarity using UKB we represent
WordNet as a graph G = (V,E) as follows: graph
nodes represent WordNet concepts (synsets) and
3http://ixa2.si.ehu.es/ukb/
dictionary words; relations among synsets are rep-
resented by undirected edges; and dictionary words
are linked to the synsets associated to them by di-
rected edges.
Our method is provided with a pair of vectors of
words and a graph-based representation of WordNet.
We first compute the personalized PageRank over
WordNet separately for each of the vector of words,
producing a probability distribution over WordNet
synsets. We then compute the similarity between
these two probability distributions by encoding them
as vectors and computing the cosine between the
vectors. We present each step in turn.
Once personalized PageRank is computed, it
returns a probability distribution over WordNet
synsets. The similarity between two vectors of
words can thus be implemented as the similarity be-
tween the probability distributions, as given by the
cosine between the vectors.
We used random walks to compute improved sim-
ilarity values for author, people involved, location
and event similarity:
? Author: UKB over Wikipedia using person en-
tities recognized by NERC in the dc:Creator
field.
? People involved and location: UKB over
Wikipedia using people/location entities recog-
nized by NERC in all fields.
? Events: UKB over WordNet using event nouns
and verbs recognized in all fields.
Results on the training data showed that perfor-
mance using this approach was quite low (with the
exception of events). This was caused by the large
number of cases where the Stanford parser did not
find entities which were in Wikipedia. With those
cases on mind, we combined the scores returned by
UKB with the similarity scores presented in Section
2 as follows: if UKB similarity returns a score, we
multiply both, otherwise we return the square of the
other similarity score. Using the multiplication of
the two scores, the results on the training data im-
proved.
2.3 Time similarity measure
In order to measure the time similarity between a
pair of items, we need to recognize time expres-
sions in both items. We assume that the year of
133
creation or the year denoting when the event took
place in an artefact are good indicators for time sim-
ilarity. Therefore, information about years is ex-
tracted from each item using the following pattern:
[1|2][0 ? 9]{3}. Using this approach, each item is
represented as a set of numbers denoting the years
mentioned in the meta-data.
Time similarity between two items is computed
based on the similarity between their associated
years. Similarity between two years is defined as:
simyear(y1, y2) = max{0, 1? |y1? y2| ? k}
k is a parameter to weight the difference between
two years, e.g. for k = 0.1 all items that have differ-
ence of 10 years or more assigned a score of 0. We
obtained best results for k = 0.1.
Finally, time similarity between items a and b is
computed as the maximum of the pairwise similarity
between their associated years:
simtime(a, b) = max?i?a
?j?b
{0, simyear(ai, bj)}
We substituted the preliminary time similarity
score by the measure obtained using the method pre-
sented in this section.
3 Applying Machine Learning
The above heuristics can be good indicators for the
respective kind of similarity, and can be thus applied
directly to the task. In this section, we take those
indicators as features, and use linear regression (as
made available by Weka (Hall et al, 2009)) to learn
models that fit the features to the training data.
We generated further similarity scores for gen-
eral similarity, including Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), UKB and Wikipedia Link
Vector Model (WLVM)(Milne, 2007) using infor-
mation taken from all fields, as explained below.
3.1 LDA
LDA (Blei et al, 2003) is a statistical method that
learns a set of latent variables called topics from a
training corpus. Given a topic model, documents
can be inferred as probability distributions over top-
ics, ?. The distribution for a document i is denoted
as ?i. An LDA model is trained using the train-
ing set consisting of 100 topics using the gensim
package4. The hyperparameters (?, ?) were set to
1
num of topics . Therefore, each item in the test set is
represented as a topic distribution.
The similarity between a pair of items is estimated
by comparing their topic distributions following the
method proposed in Aletras et al (2012; Aletras and
Stevenson (2012). This is achieved by considering
each distribution as a vector (consisting of the topics
corresponding to an item and its probability) then
computing the cosine of the angle between them, i.e.
simLDA(a, b) =
~?a ? ~?b
|~?a| ? | ~?b|
where ~?a is the vector created from the probability
distribution generated by LDA for item a.
3.2 Pairwise UKB
We run UKB (Section 2.2) to generate a probabil-
ity distribution over WordNet synsets for all of the
words of all items. Similarity between two words
is computed by creating vectors from these distri-
butions and comparing them using the cosine of the
angle between the two vectors. If a words does not
appear in WordNet its similarity value to every other
word is set to 0. We refer to that similarity metric as
UKB here.
Similarity between two items is computed by per-
forming pairwise comparison between their words,
for each, selecting the highest similarity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?b UKB(w1, w2)
|a|
+
?
w2?b
argmaxw1?a UKB(w2, w1)
|b|
)
where a and b are two items, |a| the number of
tokens in a and UKB(w1, w2) is the similarity be-
tween words w1 and w2.
3.3 WLVM
An algorithm described by Milne and Witten (2008)
associates Wikipedia articles which are likely to be
relevant to a given text snippet using machine learn-
ing techniques. We make use of that method to rep-
resent each item as a set of likely relevant Wikipedia
4http://pypi.python.org/pypi/gensim
134
articles. Then, similarity between Wikipedia arti-
cles is measured using the Wikipedia Link Vector
Model (WLVM) (Milne, 2007). WLVM uses both
the link structure and the article titles of Wikipedia
to measure similarity between two Wikipedia arti-
cles. Each link is weighted by the probability of it
occurring. Thus, the value of the weight w for a link
x? y between articles x and y is:
w(x? y) = |x? y| ? log
(
t?
z=1
t
z ? y
)
where t is the total number of articles in Wikipedia.
The similarity of articles is compared by forming
vectors of the articles which are linked from them
and computing the cosine of their angle. For exam-
ple the vectors of two articles x and y are:
x = (w(x? l1), w(x? l2), ..., w(x? ln))
y = (w(y ? l1), w(y ? l2), ..., w(y ? ln))
where x and y are two Wikipedia articles and x? li
is a link from article x to article li.
Since the items have been mapped to Wikipedia
articles, similarity between two items is computed
by performing pairwise comparison between articles
using WLVM, for each, selecting the highest simi-
larity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?bWLVM(w1, w2)
|a|
+
?
w2?b
argmaxw1?aWLVM(w2, w1)
|b|
)
where a and b are two items, |a| the number of
Wikipedia articles in a and WLVM(w1, w2) is the
similarity between concepts w1 and w2.
4 Submissions
We selected three systems for submission. The first
run uses the similarity scores of the basic system
(Section 2) for each similarity types as follows:
? General: cosine similarity of TF.IDF vectors,
IDF based on Wikipedia (as shown in Section
2.1).
? Author: product of the scores obtained ob-
tained using TF.IDF vectors and UKB (as
shown in Section 2.2) using only the data ex-
tracted from dc:Creator field.
? People involved and location: product of co-
sine similarity of TF.IDF vectors and UKB (as
shown in Section 2.2) using the data extracted
from all fields.
? Time period: time similarity measure (as
shown in Section 2.3).
? Events: product of cosine similarity of TF.IDF
vectors and UKB (as shown in Section 2.2) of
event nouns and verbs recognized in all fields.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields (as shown
in Section 2).
For the second run we trained a ML model for
each of the similarity types, using the following fea-
tures:
? Cosine similarity of TF.IDF vectors as shown
in Section 2 for the eight similarity types.
? Four new values for general similarity: TF.IDF
(Section 2.1), LDA (Section 3.1), UKB and
WLVM (Section 3.3).
? Time similarity as shown in Section 2.3.
? Events similarity computed using UKB initial-
ized with the event nouns and verbs in all fields.
We decided not to use the product of TF.IDF
and UKB presented in Section 2.2 in this system
because our intention was to measure the power of
the linear regression ML algorithm to learn on the
given raw data.
The third run is similar, but includes all available
features (21). In addition to the above, we included:
? Author, people involved and location similar-
ity computed using UKB initialized with peo-
ple/location recognized by NERC in dc:Creator
field for author, and in all fields for people in-
volved and location.
? Author, people involved, location and event
similarity scores computed by the product of
TF.IDF vectors and UKB values as shown in
Section 2.2.
5 Results
Evaluation was carried out using the official scorer
provided by the organizers, which computes the
Pearson Correlation score for each of the eight sim-
ilarity types plus an additional mean correlation.
135
Team and run General Author People involved Time Location Event Subject Description Mean
UBC UOS-RUN1 0.7269 0.4474 0.4648 0.5884 0.4801 0.2522 0.4976 0.5389 0.5033
UBC UOS-RUN2 0.7777 0.6680 0.6767 0.7609 0.7329 0.6412 0.7516 0.8024 0.7264
UBC UOS-RUN3 0.7866 0.6941 0.6965 0.7654 0.7492 0.6551 0.7586 0.8067 0.7390
Table 1: Results of our systems on the training data, using cross-validation when necessary.
Team and run General Author People involved Time Location Event Subject Description Mean Rank
UBC UOS-RUN1 0.7256 0.4568 0.4467 0.5762 0.4858 0.3090 0.5015 0.5810 0.5103 6
UBC UOS-RUN2 0.7457 0.6618 0.6518 0.7466 0.7244 0.6533 0.7404 0.7751 0.7124 4
UBC UOS-RUN3 0.7461 0.6656 0.6544 0.7411 0.7257 0.6545 0.7417 0.7763 0.7132 3
Table 2: Results of our submitted systems.
5.1 Development
The three runs mentioned above were developed us-
ing the training data made available by the organiz-
ers. In order to avoid overfitting we did not change
the default parameters of the linear regressor, and
10-fold cross-validation was used for evaluating the
models on the training data. The results of our sys-
tems on the training data are shown on Table 1. The
table shows that the heuristics (RUN1) obtain low
results, and that linear regression improves results
considerably in all types. Using the full set of fea-
tures, RUN3 improves slightly over RUN2, but the
improvement is consistent across all types.
5.2 Test
The test dataset was composed of 750 pairs of items.
Table 2 illustrates the results of our systems in the
test dataset. The results of the runs are very similar
to those obtained on the training data, but the dif-
ference between RUN2 and RUN3 is even smaller.
Our systems were ranked #3 (RUN 3), #4 (RUN
2) and #6 (RUN 1) among 14 systems submitted
by 6 teams. Our systems achieved good correlation
scores for almost all similarity types, with the excep-
tion of author similarity, which is the worst ranked
in comparison with the rest of the systems.
6 Conclusions and Future Work
In this paper, we presented the systems submitted
to the *SEM 2013 shared task on Semantic Tex-
tual Similarity. We combined some simple heuris-
tics for each type of similarity, based on the appro-
priate metadata fields. The use of lineal regression
improved the results considerably across all types.
Our system fared well in the competition. We sub-
mitted three systems and the highest-ranked of these
achieved the third best results overall.
Acknowledgements
This work is partially funded by the PATHS
project (http://paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082. Aitor Gonzalez-Agirre is supported by
a PhD grant from the Spanish Ministry of Education,
Culture and Sport (grant FPU12/06243).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for sim-
ilarity. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC10). European Language Resources Associa-
tion (ELRA). ISBN: 2-9517408-6-7. Pages 373?377.?.
Nikolaos Aletras and Mark Stevenson. 2012. Computing
similarity between cultural heritage items using multi-
modal features. In Proceedings of the 6th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 85?93, Avignon,
France.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a digi-
tal library of cultural heritage. J. Comput. Cult. Herit.,
5(4):16:1?16:19, December.
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
136
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
D. Milne and I. Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the ACM Con-
ference on Information and Knowledge Management
(CIKM?2008), Napa Valley, California.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia?s link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
137
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
