Proceedings of the ACL 2010 Conference Short Papers, pages 137?141,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Word Alignment with Synonym Regularization
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo,a.fujino}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We present a novel framework for word
alignment that incorporates synonym
knowledge collected from monolingual
linguistic resources in a bilingual proba-
bilistic model. Synonym information is
helpful for word alignment because we
can expect a synonym to correspond to
the same word in a different language.
We design a generative model for word
alignment that uses synonym information
as a regularization term. The experimental
results show that our proposed method
significantly improves word alignment
quality.
1 Introduction
Word alignment is an essential step in most phrase
and syntax based statistical machine translation
(SMT). It is an inference problem of word cor-
respondences between different languages given
parallel sentence pairs. Accurate word alignment
can induce high quality phrase detection and trans-
lation probability, which leads to a significant im-
provement in SMT performance. Many word
alignment approaches based on generative mod-
els have been proposed and they learn from bilin-
gual sentences in an unsupervised manner (Vo-
gel et al, 1996; Och and Ney, 2003; Fraser and
Marcu, 2007).
One way to improve word alignment quality
is to add linguistic knowledge derived from a
monolingual corpus. This monolingual knowl-
edge makes it easier to determine corresponding
words correctly. For instance, functional words
in one language tend to correspond to functional
words in another language (Deng and Gao, 2007),
and the syntactic dependency of words in each lan-
guage can help the alignment process (Ma et al,
2008). It has been shown that such grammatical
information works as a constraint in word align-
ment models and improves word alignment qual-
ity.
A large number of monolingual lexical seman-
tic resources such as WordNet (Miller, 1995) have
been constructed in more than fifty languages
(Sagot and Fiser, 2008). They include word-
level relations such as synonyms, hypernyms and
hyponyms. Synonym information is particularly
helpful for word alignment because we can ex-
pect a synonym to correspond to the same word
in a different language. In this paper, we explore a
method for using synonym information effectively
to improve word alignment quality.
In general, synonym relations are defined in
terms of word sense, not in terms of word form. In
other words, synonym relations are usually con-
text or domain dependent. For instance, ?head?
and ?chief? are synonyms in contexts referring to
working environment, while ?head? and ?forefront?
are synonyms in contexts referring to physical po-
sitions. It is difficult, however, to imagine a con-
text where ?chief? and ?forefront? are synonyms.
Therefore, it is easy to imagine that simply replac-
ing all occurrences of ?chief? and ?forefront? with
?head? do sometimes harm with word alignment
accuracy, and we have to model either the context
or senses of words.
We propose a novel method that incorporates
synonyms from monolingual resources in a bilin-
gual word alignment model. We formulate a syn-
onym pair generative model with a topic variable
and use this model as a regularization term with a
bilingual word alignment model. The topic vari-
able in our synonym model is helpful for disam-
biguating the meanings of synonyms. We extend
HM-BiTAM, which is a HMM-based word align-
ment model with a latent topic, with a novel syn-
onym pair generative model. We applied the pro-
posed method to an English-French word align-
ment task and successfully improved the word
137
Figure 1: Graphical model of HM-BiTAM
alignment quality.
2 Bilingual Word Alignment Model
In this section, we review a conventional gener-
ative word alignment model, HM-BiTAM (Zhao
and Xing, 2008).
HM-BiTAM is a bilingual generative model
with topic z, alignment a and topic weight vec-
tor ? as latent variables. Topic variables such
as ?science? or ?economy? assigned to individual
sentences help to disambiguate the meanings of
words. HM-BiTAM assumes that the nth bilin-
gual sentence pair, (En, Fn), is generated under a
given latent topic zn ? {1, . . . , k, . . . ,K}, where
K is the number of latent topics. Let N be the
number of sentence pairs, and In and Jn be the
lengths of En and Fn, respectively. In this frame-
work, all of the bilingual sentence pairs {E,F} =
{(En, Fn)}Nn=1 are generated as follows.
1. ? ? Dirichlet (?): sample topic-weight vector
2. For each sentence pair (En, Fn)
(a) zn ? Multinomial (?): sample the topic
(b) en,i:In |zn ? p (En |zn;? ): sample English
words from a monolingual unigram model given
topic zn
(c) For each position jn = 1, . . . , Jn
i. ajn ? p (ajn |ajn?1;T ): sample an align-
ment link ajn from a first order Markov pro-
cess
ii. fjn ? p (fjn |En, ajn , zn;B ): sample a
target word fjn given an aligned source
word and topic
where alignment ajn = i denotes source word ei
and target word fjn are aligned. ? is a parame-
ter over the topic weight vector ?, ? = {?k,e} is
the source word probability given the kth topic:
p (e |z = k ). B = {Bf,e,k} represents the word
translation probability from e to f under the kth
topic: p (f |e, z = k ). T =
{
Ti,i?
}
is a state tran-
sition probability of a first order Markov process.
Fig. 1 shows a graphical model of HM-BiTAM.
The total likelihood of bilingual sentence pairs
{E,F} can be obtained by marginalizing out la-
tent variables z, a and ?,
p (F,E; ?) =
?
z
?
a

p (F,E, z, a, ?; ?) d?, (1)
where ? = {?, ?, T,B} is a parameter set. In
this model, we can infer word alignment a by max-
imizing the likelihood above.
3 Proposed Method
3.1 Synonym Pair Generative Model
We design a generative model for synonym pairs
{f, f ?} in language F , which assumes that the
synonyms are collected from monolingual linguis-
tic resources. We assume that each synonym pair
(f, f ?) is generated independently given the same
?sense? s. Under this assumption, the probability
of synonym pair (f, f ?) can be formulated as,
p
(
f, f ?
)
?
?
s
p (f |s ) p
(
f ? |s
)
p (s) . (2)
We define a pair (e, k) as a representation of
the sense s, where e and k are a word in a dif-
ferent language E and a latent topic, respectively.
It has been shown that a word e in a different
language is an appropriate representation of s in
synonym modeling (Bannard and Callison-Burch,
2005). We assume that adding a latent topic k for
the sense is very useful for disambiguating word
meaning, and thus that (e, k) gives us a good ap-
proximation of s. Under this assumption, the syn-
onym pair generative model can be defined as fol-
lows.
p
(
{
f, f ?
}
; ??
)
?
?
(f,f ?)
?
e,k
p(f |e, k; ??)p(f ?|e, k; ??)p(e, k; ??),(3)
where ?? is the parameter set of our model.
3.2 Word Alignment with Synonym
Regularization
In this section, we extend the bilingual genera-
tive model (HM-BiTAM) with our synonym pair
model. Our expectation is that synonym pairs
138
Figure 2: Graphical model of synonym pair gen-
erative process
correspond to the same word in a different lan-
guage, thus they make it easy to infer accurate
word alignment. HM-BiTAM and the synonym
model share parameters in order to incorporate
monolingual synonym information into the bilin-
gual word alignment model. This can be achieved
via reparameterizing ?? in eq. 3 as,
p
(
f
?
?
?
e, k; ??
)
? p (f |e, k;B ) , (4)
p
(
e, k; ??
)
? p (e |k;? ) p (k;?) . (5)
Overall, we re-define the synonym pair model
with the HM-BiTAM parameter set ?,
p(
{
f, f ?
}
; ?)
? 1?
k? ?k?
?
(f,f ?)
?
k,e
?k?k,eBf,e,kBf ?,e,k. (6)
Fig. 2 shows a graphical model of the synonym
pair generative process. We estimate the param-
eter values to maximize the likelihood of HM-
BiTAM with respect to bilingual sentences and
that of the synonym model with respect to syn-
onym pairs collected from monolingual resources.
Namely, the parameter estimate, ??, is computed
as
?? = argmax
?
{
log p(F,E; ?) + ? log p(
{
f, f ?
}
; ?)
}
,
(7)
where ? is a regularization weight that should
be set for training. We can expect that the second
term of eq. 7 to constrain parameter set ? and
avoid overfitting for the bilingual word alignment
model. We resort to the variational EM approach
(Bernardo et al, 2003) to infer ?? following HM-
BiTAM. We omit the parameter update equation
due to lack of space.
4 Experiments
4.1 Experimental Setting
For an empirical evaluation of the proposed
method, we used a bilingual parallel corpus of
English-French Hansards (Mihalcea and Pedersen,
2003). The corpus consists of over 1 million sen-
tence pairs, which include 447 manually word-
aligned sentences. We selected 100 sentence pairs
randomly from the manually word-aligned sen-
tences as development data for tuning the regu-
larization weight ?, and used the 347 remaining
sentence pairs as evaluation data. We also ran-
domly selected 10k, 50k, and 100k sized sentence
pairs from the corpus as additional training data.
We ran the unsupervised training of our proposed
word alignment model on the additional training
data and the 347 sentence pairs of the evaluation
data. Note that manual word alignment of the
347 sentence pairs was not used for the unsuper-
vised training. After the unsupervised training, we
evaluated the word alignment performance of our
proposed method by comparing the manual word
alignment of the 347 sentence pairs with the pre-
diction provided by the trained model.
We collected English and French synonym pairs
from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4
(Sagot and Fiser, 2008), respectively. WOLF is a
semantic resource constructed from the Princeton
WordNet and various multilingual resources. We
selected synonym pairs where both words were in-
cluded in the bilingual training set.
We compared the word alignment performance
of our model with that of GIZA++ 1.03 1 (Vo-
gel et al, 1996; Och and Ney, 2003), and HM-
BiTAM (Zhao and Xing, 2008) implemented by
us. GIZA++ is an implementation of IBM-model
4 and HMM, and HM-BiTAM corresponds to ? =
0 in eq. 7. We adopted K = 3 topics, following
the setting in (Zhao and Xing, 2006).
We trained the word alignment in two direc-
tions: English to French, and French to English.
The alignment results for both directions were re-
fined with ?GROW? heuristics to yield high preci-
sion and high recall in accordance with previous
work (Och and Ney, 2003; Zhao and Xing, 2006).
We evaluated these results for precision, recall, F-
measure and alignment error rate (AER), which
are standard metrics for word alignment accuracy
(Och and Ney, 2000).
1http://fjoch.com/GIZA++.html
139
10k Precision Recall F-measure AER
GIZA++ standard 0.856 0.718 0.781 0.207
with SRH 0.874 0.720 0.789 0.198
HM-BiTAM standard 0.869 0.788 0.826 0.169
with SRH 0.884 0.790 0.834 0.160
Proposed 0.941 0.808 0.870 0.123
(a)
50k Precision Recall F-measure AER
GIZA++ standard 0.905 0.770 0.832 0.156
with SRH 0.903 0.759 0.825 0.164
HM-BiTAM standard 0.901 0.814 0.855 0.140
with SRH 0.899 0.808 0.853 0.145
Proposed 0.947 0.824 0.881 0.112
(b)
100k Precision Recall F-measure AER
GIZA++ standard 0.925 0.791 0.853 0.136
with SRH 0.934 0.803 0.864 0.126
HM-BiTAM standard 0.898 0.851 0.874 0.124
with SRH 0.909 0.860 0.879 0.114
Proposed 0.927 0.862 0.893 0.103
(c)
Table 1: Comparison of word alignment accuracy.
The best results are indicated in bold type. The
additional data set sizes are (a) 10k, (b) 50k, (c)
100k.
4.2 Results and Discussion
Table 1 shows the word alignment accuracy of the
three methods trained with 10k, 50k, and 100k ad-
ditional sentence pairs. For all settings, our pro-
posed method outperformed other conventional
methods. This result shows that synonym infor-
mation is effective for improving word alignment
quality as we expected.
As mentioned in Sections 1 and 3.1, the main
idea of our proposed method is to introduce la-
tent topics for modeling synonym pairs, and then
to utilize the synonym pair model for the regu-
larization of word alignment models. We expect
the latent topics to be useful for modeling poly-
semous words included in synonym pairs and to
enable us to incorporate synonym information ef-
fectively into word alignment models. To con-
firm the effect of the synonym pair model with
latent topics, we also tested GIZA++ and HM-
BiTAM with what we call Synonym Replacement
Heuristics (SRH), where all of the synonym pairs
in the bilingual training sentences were simply re-
placed with a representative word. For instance,
the words ?sick? and ?ill? in the bilingual sentences
# vocabularies 10k 50k 100k
English standard 8578 16924 22817
with SRH 5435 7235 13978
French standard 10791 21872 30294
with SRH 9737 20077 27970
Table 2: The number of vocabularies in the 10k,
50k and 100k data sets.
were replaced with the word ?sick?. As shown in
Table 2, the number of vocabularies in the English
and French data sets decreased as a result of em-
ploying the SRH.
We show the performance of GIZA++ and HM-
BiTAM with the SRH in the lines entitled ?with
SRH? in Table 1. The GIZA++ and HM-BiTAM
with the SRH slightly outperformed the standard
GIZA++ and HM-BiTAM for the 10k and 100k
data sets, but underperformed with the 50k data
set. We assume that the SRH mitigated the over-
fitting of these models into low-frequency word
pairs in bilingual sentences, and then improved the
word alignment performance. The SRH regards
all of the different words coupled with the same
word in the synonym pairs as synonyms. For in-
stance, the words ?head?, ?chief? and ?forefront? in
the bilingual sentences are replaced with ?chief?,
since (?head?, ?chief?) and (?head?, ?forefront?) are
synonyms. Obviously, (?chief?, ?forefront?) are
not synonyms, which is detrimented to word align-
ment.
The proposed method consistently outper-
formed GIZA++ and HM-BiTAM with the SRH
in 10k, 50k and 100k data sets in F-measure.
The synonym pair model in our proposed method
can automatically learn that (?head?, ?chief?) and
(?head?, ?forefront?) are individual synonyms with
different meanings by assigning these pairs to dif-
ferent topics. By sharing latent topics between
the synonym pair model and the word alignment
model, the synonym information incorporated in
the synonym pair model is used directly for train-
ing word alignment model. The experimental re-
sults show that our proposed method was effec-
tive in improving the performance of the word
alignment model by using synonym pairs includ-
ing such ambiguous synonym words.
Finally, we discuss the data set size used for un-
supervised training. As shown in Table 1, using
a large number of additional sentence pairs im-
proved the performance of all the models. In all
our experimental settings, all the additional sen-
140
tence pairs and the evaluation data were selected
from the Hansards data set. These experimental
results show that a larger number of sentence pairs
was more effective in improving word alignment
performance when the sentence pairs were col-
lected from a homogeneous data source. However,
in practice, it might be difficult to collect a large
number of such homogeneous sentence pairs for
a specific target domain and language pair. One
direction for future work is to confirm the effect
of the proposed method when training the word
alignment model by using a large number of sen-
tence pairs collected from various data sources in-
cluding many topics for a specific language pair.
5 Conclusions and Future Work
We proposed a novel framework that incorpo-
rates synonyms from monolingual linguistic re-
sources in a word alignment generative model.
This approach utilizes both bilingual and mono-
lingual synonym resources effectively for word
alignment. Our proposed method uses a latent
topic for bilingual sentences and monolingual syn-
onym pairs, which is helpful in terms of word
sense disambiguation. Our proposed method im-
proved word alignment quality with both small
and large data sets. Future work will involve ex-
amining the proposed method for different lan-
guage pairs such as English-Chinese and English-
Japanese and evaluating the impact of our pro-
posed method on SMT performance. We will also
apply our proposed method to a larger data sets
of multiple domains since we can expect a fur-
ther improvement in word alignment accuracy if
we use more bilingual sentences and more mono-
lingual knowledge.
References
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 597?604. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P.
Dawid, D. Heckerman, A. F. M. Smith, and M. West.
2003. The variational bayesian EM algorithm for in-
complete data: with application to scoring graphical
model structures. In Bayesian Statistics 7: Proceed-
ings of the 7th Valencia International Meeting, June
2-6, 2002, page 453. Oxford University Press, USA.
Y. Deng and Y. Gao. 2007. Guiding statistical word
alignment models with prior knowledge. In Pro-
ceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 1?8,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51?60, Prague, Czech Republic,
June. Association for Computational Linguistics.
Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008.
Improving word alignment using syntactic depen-
dencies. In Proceedings of the ACL-08: HLT Sec-
ond Workshop on Syntax and Structure in Statisti-
cal Translation (SSST-2), pages 69?77, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
HLT-NAACL 2003 Workshop on building and using
parallel texts: data driven machine translation and
beyond-Volume 3, page 10. Association for Compu-
tational Linguistics.
G. A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):41.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 440?447. Association for Computational
Linguistics Morristown, NJ, USA.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
B. Sagot and D. Fiser. 2008. Building a free French
wordnet from multilingual resources. In Proceed-
ings of Ontolex.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th Conference on Computa-
tional Linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main Conference
Poster Sessions, page 976. Association for Compu-
tational Linguistics.
B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
In Advances in Neural Information Processing Sys-
tems 20, pages 1689?1696, Cambridge, MA. MIT
Press.
141
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 206?211,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion Operator for Bayesian Tree Substitution Grammars
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
Abstract
We propose a model that incorporates an in-
sertion operator in Bayesian tree substitution
grammars (BTSG). Tree insertion is helpful
for modeling syntax patterns accurately with
fewer grammar rules than BTSG. The exper-
imental parsing results show that our model
outperforms a standard PCFG and BTSG for
a small dataset. For a large dataset, our model
obtains comparable results to BTSG, making
the number of grammar rules much smaller
than with BTSG.
1 Introduction
Tree substitution grammar (TSG) is a promising for-
malism for modeling language data. TSG general-
izes context free grammars (CFG) by allowing non-
terminal nodes to be replaced with subtrees of arbi-
trary size.
A natural extension of TSG involves adding an
insertion operator for combining subtrees as in
tree adjoining grammars (TAG) (Joshi, 1985) or
tree insertion grammars (TIG) (Schabes and Wa-
ters, 1995). An insertion operator is helpful for ex-
pressing various syntax patterns with fewer gram-
mar rules, thus we expect that adding an insertion
operator will improve parsing accuracy and realize a
compact grammar size.
One of the challenges of adding an insertion op-
erator is that the computational cost of grammar in-
duction is high since tree insertion significantly in-
creases the number of possible subtrees. Previous
work on TAG and TIG induction (Xia, 1999; Chi-
ang, 2003; Chen et al, 2006) has addressed the prob-
lem using language-specific heuristics and a maxi-
mum likelihood estimator, which leads to overfitting
the training data (Post and Gildea, 2009).
Instead, we incorporate an insertion operator in a
Bayesian TSG (BTSG) model (Cohn et al, 2011)
that learns grammar rules automatically without
heuristics. Our model uses a restricted variant of
subtrees for insertion to model the probability dis-
tribution simply and train the model efficiently. We
also present an inference technique for handling a
tree insertion that makes use of dynamic program-
ming.
2 Overview of BTSG Model
We briefly review the BTSG model described in
(Cohn et al, 2011). TSG uses a substitution operator
(shown in Fig. 1a) to combine subtrees. Subtrees for
substitution are referred to as initial trees, and leaf
nonterminals in initial trees are referred to as fron-
tier nodes. Their task is the unsupervised induction
of TSG derivations from parse trees. A derivation
is information about how subtrees are combined to
form parse trees.
The probability distribution over initial trees is de-
fined by using a Pitman-Yor process prior (Pitman
and Yor, 1997), that is,
e |X ? GX
GX |dX , ?X ? PYP (dX , ?X , P0 (? |X )) ,
where X is a nonterminal symbol, e is an initial tree
rooted with X , and P0 (? |X ) is a base distribution
over the infinite space of initial trees rooted with X .
dX and ?X are hyperparameters that are used to con-
trol the model?s behavior. Integrating out all possi-
ble values of GX , the resulting distribution is
206
p (ei |e?i, X, dX , ?X ) = ?ei,X + ?XP0 (ei, |X ) , (1)
where ?ei,X =
n?iei,X
?dX ?tei,X
?X+n
?i
?,X
and ?X =
?X+dX ?t?,X
?X+n
?i
?,X
. e?i = e1, . . . , ei?1 are previously gen-
erated initial trees, and n?iei,X is the number of times
ei has been used in e?i. tei,X is the number of ta-
bles labeled with ei. n
?i
?,X =
?
e n
?i
e,X and t?,X =?
e te,X are the total counts of initial trees and ta-
bles, respectively. The PYP prior produces ?rich get
richer? statistics: a few initial trees are often used
for derivation while many are rarely used, and this is
shown empirically to be well-suited for natural lan-
guage (Teh, 2006b; Johnson and Goldwater, 2009).
The base probability of an initial tree, P0 (e |X ),
is given as follows.
P0 (e |X ) =
?
r?CFG(e)
PMLE (r)?
?
A?LEAF(e)
sA
?
?
B?INTER(e)
(1? sB) , (2)
where CFG (e) is a set of decomposed CFG produc-
tions of e, PMLE (r) is a maximum likelihood esti-
mate (MLE) of r. LEAF (e) and INTER (e) are sets
of leaf and internal symbols of e, respectively. sX is
a stopping probability defined for each X .
3 Insertion Operator for BTSG
3.1 Tree Insertion Model
We propose a model that incorporates an insertion
operator in BTSG. Figure 1b shows an example of
an insertion operator. To distinguish them from ini-
tial trees, subtrees for insertion are referred to as
auxiliary trees. An auxiliary tree includes a special
nonterminal leaf node labeled with the same sym-
bol as the root node. This leaf node is referred to
as a foot node (marked with the subscript ?*?). The
definitions of substitution and insertion operators are
identical with those of TIG and TAG.
Since it is computationally expensive to allow any
auxiliary trees, we tackle the problem by introduc-
ing simple auxiliary trees, i.e., auxiliary trees whose
root node must generate a foot node as an immediate
child. For example, ?(N (JJ pretty) N*)? is a simple
auxiliary tree, but ?(S (NP ) (VP (V think) S*))? is
(a)
(b)
Figure 1: Example of (a) substitution and (b) inser-
tion (dotted line).
not. Note that we place no restriction on the initial
trees.
Our restricted formalism is a strict subset of TIG.
We briefly refer to some differences between TAG,
TIG and our insertion model. TAG generates tree
adjoining languages, a strict superset of context-
free languages, and the computational complexity
of parsing is O
(
n6
)
. TIG is a similar formalism
to TAG, but it does not allow wrapping adjunction
in TAG. Therefore, TIG generates context-free lan-
guages and the parsing complexity is O
(
n3
)
, which
is a strict subset of TAG. On the other hand, our
model prohibits neither wrapping adjunction in TAG
nor simultaneous adjunction in TIG, and allows only
simple auxiliary trees. The expressive power and
computational complexity of our formalism is iden-
tical to TIG, however, our model allows us to de-
fine the probability distribution over auxiliary trees
as having the same form as BTSG model. This en-
sures that we can make use of a dynamic program-
ming technique for training our model, which we de-
scribe the detail in the next subsection.
We define a probability distribution over simple
auxiliary trees as having the same form as eq. 1, that
is,
207
p (ei |e?i, X, d?X , ?
?
X ) = ?
?
ei,X + ?
?
XP
?
0 (ei, |X ) , (3)
where d?X and ?
?
X are hyperparameters of the in-
sertion model, and the definition of
(
??ei,X , ?
?
X
)
is
the same as that of (?ei,X , ?X) in eq. 1.
However, we need modify the base distribution
over simple auxiliary trees, P ?0 (e |X ), as follows,
so that all probabilities of the simple auxiliary trees
sum to one.
P ?0 (e |X ) = P
?
MLE (TOP (e))?
?
r?INTER_CFG(e)
PMLE (r)
?
?
A?LEAF(e)
sA ?
?
B?INTER(e)
(1? sB) , (4)
where TOP (e) is the CFG production that
starts with the root node of e. For example,
TOP (N (JJ pretty) (N*)) returns ?N ? JJ N*?.
INTER_CFG (e) is a set of CFG productions of e
excluding TOP (e). P ?MLE (r
?) is a modified MLE
for simple auxiliary trees, which is given by
{
C(r?)
C(X?X?Y )+C(X?Y X?) if r
?includes a foot node
0 else
where C (r?) is the frequency of r? in parse trees.
It is ensured that P ?0 (e |X ) generates a foot node as
an immediate child.
We define the probability distribution over both
initial trees and simple auxiliary trees with a PYP
prior. The base distribution over initial trees is de-
fined as P0 (e |X ), and the base distribution over
simple auxiliary trees is defined as P ?0 (e |X ). An
initial tree ei replaces a frontier node with prob-
ability p (ei |e?i, X, dX , ?X ). On the other hand,
a simple auxiliary tree e?i inserts an internal node
with probability aX?p?
(
e?i
?
?e??i, X, d
?
X , ?
?
X
)
, where
aX is an insertion probability defined for each X .
The stopping probabilities are common to both ini-
tial and auxiliary trees.
3.2 Grammar Decomposition
We develop a grammar decomposition technique,
which is an extension of work (Cohn and Blunsom,
2010) on BTSG model, to deal with an insertion
operator. The motivation behind grammar decom-
position is that it is hard to consider all possible
Figure 2: Derivation of Fig. 1b transformed by
grammar decomposition.
CFG rule probability
NP(NP (DT the) (N girl))?DT(DT the)Nins (N girl) (1? aDT)? aN
DT(DT the) ? the 1
Nins (N girl) ?Nins (N girl)(N (JJ pretty) N*) ?
?
(N (JJ pretty) N*),N
Nins (N girl)(N (JJ pretty) N*) ? JJ(JJ pretty)N(N girl) (1? aJJ)? 1
JJ(JJ pretty) ?pretty 1
N(N girl) ?girl 1
Table 1: The rules and probabilities of grammar de-
composition for Fig. 2.
derivations explicitly since the base distribution as-
signs non-zero probability to an infinite number of
initial and auxiliary trees. Alternatively, we trans-
form a derivation into CFG productions and assign
the probability for each CFG production so that its
assignment is consistent with the probability distri-
butions. We can efficiently calculate an inside prob-
ability (described in the next subsection) by employ-
ing grammar decomposition.
Here we provide an example of the derivation
shown in Fig. 1b. First, we can transform the deriva-
tion in Fig. 1b to another form as shown in Fig. 2.
In Fig. 2, all the derivation information is embed-
ded in each symbol. That is, NP(NP (DT the) (N girl)) is
a root symbol of the initial tree ?(NP (DT the) (N
girl))?, which generates two child nodes: DT(DT the)
and N(N girl). DT(DT the) generates the terminal node
?the?. On the other hand, Nins (N girl) denotes that
N(N girl) is inserted by some auxiliary tree, and
Nins (N girl)(N (JJ pretty) N*) denotes that the inserted simple aux-
iliary tree is ?(N (JJ pretty) (N*))?. The inserted
auxiliary tree, ?(N (JJ pretty) (N*))?, must generate
a foot node: ?(N girl)? as an immediate child.
208
Second, we decompose the transformed tree into
CFG productions and then assign the probability for
each CFG production as shown in Table 1, where
aDT, aN and aJJ are insertion probabilities for non-
terminal DT, N and JJ, respectively. Note that the
probability of a derivation according to Table 1 is
the same as the probability of a derivation obtained
from the distribution over the initial and auxiliary
trees (i.e. eq. 1 and eq. 3).
In Table 1, we assume that the auxiliary tree
?(N (JJ pretty) (N*))? is sampled from the first
term of eq. 3. When it is sampled from the sec-
ond term, we alternatively assign the probability
??(N (JJ pretty) N*), N.
3.3 Training
We use a blocked Metropolis-Hastings (MH) algo-
rithm (Cohn and Blunsom, 2010) to train our model.
The MH algorithm learns BTSG model parameters
efficiently, and it can be applied to our insertion
model. The MH algorithm consists of the following
three steps. For each sentence,
1. Calculate the inside probability (Lari and
Young, 1991) in a bottom-up manner using the
grammar decomposition.
2. Sample a derivation tree in a top-down manner.
3. Accept or reject the derivation sample by using
the MH test.
The MH algorithm is described in detail in (Cohn
and Blunsom, 2010). The hyperparameters of our
model are updated with the auxiliary variable tech-
nique (Teh, 2006a).
4 Experiments
We ran experiments on the British National Cor-
pus (BNC) Treebank 3 and the WSJ English Penn
Treebank. We did not use a development set since
our model automatically updates the hyperparame-
ters for every iteration. The treebank data was bina-
rized using the CENTER-HEAD method (Matsuzaki
et al, 2005). We replaced lexical words with counts
? 1 in the training set with one of three unknown
1Results from (Cohn and Blunsom, 2010).
2Results for length ? 40.
3http://nclt.computing.dcu.ie/~jfoster/resources/
corpus method F1
CFG 54.08
BNC BTSG 67.73
BTSG + insertion 69.06
CFG 64.99
BTSG 77.19
WSJ BTSG + insertion 78.54
(Petrov et al, 2006) 77.931
(Cohn and Blunsom, 2010) 78.40
Table 2: Small dataset experiments
# rules (# aux. trees) F1
CFG 35374 (-) 71.0
BTSG 80026 (0) 85.0
BTSG + insertion 65099 (25) 85.3
(Post and Gildea, 2009) - 82.62
(Cohn and Blunsom, 2010) - 85.3
Table 3: Full Penn Treebank dataset experiments
words using lexical features. We trained our model
using a training set, and then sampled 10k deriva-
tions for each sentence in a test set. Parsing results
were obtained with the MER algorithm (Cohn et al,
2011) using the 10k derivation samples. We show
the bracketing F1 score of predicted parse trees eval-
uated by EVALB4, averaged over three independent
runs.
In small dataset experiments, we used BNC (1k
sentences, 90% for training and 10% for testing) and
WSJ (section 2 for training and section 22 for test-
ing). This was a small-scale experiment, but large
enough to be relevant for low-resource languages.
We trained the model with an MH sampler for 1k
iterations. Table 2 shows the parsing results for
the test set. We compared our model with standard
PCFG and BTSG models implemented by us.
Our insertion model successfully outperformed
CFG and BTSG. This suggests that adding an inser-
tion operator is helpful for modeling syntax trees ac-
curately. The BTSG model described in (Cohn and
Blunsom, 2010) is similar to ours. They reported
an F1 score of 78.40 (the score of our BTSG model
was 77.19). We speculate that the performance gap
is due to data preprocessing such as the treatment of
rare words.
4http://nlp.cs.nyu.edu/evalb/
209
(N?P (N?P ) (: ?))
(N?P (N?P ) (ADVP (RB respectively)))
(P?P (P?P ) (, ,))
(V?P (V?P ) (RB then))
(Q?P (Q?P ) (IN of))
( ?SBAR ( ?SBAR ) (RB not))
(S? (S? ) (: ;))
Table 4: Examples of lexicalized auxiliary trees ob-
tained from our model in the full treebank dataset.
Nonterminal symbols created by binarization are
shown with an over-bar.
We also applied our model to the full WSJ Penn
Treebank setting (section 2-21 for training and sec-
tion 23 for testing). The parsing results are shown in
Table 3. We trained the model with an MH sampler
for 3.5k iterations.
For the full treebank dataset, our model obtained
nearly identical results to those obtained with BTSG
model, making the grammar size approximately
19% smaller than that of BTSG. We can see that only
a small number of auxiliary trees have a great impact
on reducing the grammar size. Surprisingly, there
are many fewer auxiliary trees than initial trees. We
believe this to be due to the tree binarization and our
restricted assumption of simple auxiliary trees.
Table 4 shows examples of lexicalized auxiliary
trees obtained with our model for the full treebank
data. We can see that punctuation (???, ?,?, and ?;?)
and adverb (RB) tend to be inserted in other trees.
Punctuation and adverb appear in various positions
in English sentences. Our results suggest that rather
than treat those words as substitutions, it is more rea-
sonable to consider them to be ?insertions?, which is
intuitively understandable.
5 Summary
We proposed a model that incorporates an inser-
tion operator in BTSG and developed an efficient
inference technique. Since it is computationally ex-
pensive to allow any auxiliary trees, we tackled the
problem by introducing a restricted variant of aux-
iliary trees. Our model outperformed the BTSG
model for a small dataset, and achieved compara-
ble parsing results for a large dataset, making the
number of grammars much smaller than the BTSG
model. We will extend our model to original TAG
and evaluate its impact on statistical parsing perfor-
mance.
References
J. Chen, S. Bangalore, and K. Vijay-Shanker. 2006.
Automated extraction of Tree-Adjoining Grammars
from treebanks. Natural Language Engineering,
12(03):251?299.
D. Chiang, 2003. Statistical Parsing with an Automati-
cally Extracted Tree Adjoining Grammar, chapter 16,
pages 299?316. CSLI Publications.
T. Cohn and P. Blunsom. 2010. Blocked inference in
Bayesian tree substitution grammars. In Proceedings
of the ACL 2010 Conference Short Papers, pages 225?
230, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Induc-
ing tree-substitution grammars. Journal of Machine
Learning Research. To Appear.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on unsu-
pervised word segmentation with adaptor grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 317?325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
A.K. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions? Natural Language Parsing:
Psychological, Computational, and Theoretical Per-
spectives, pages 206?250.
K. Lari and S.J. Young. 1991. Applications of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech & Language, 5(3):237?257.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 75?82. Association
for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), pages 433?440, Syd-
ney, Australia, July. Association for Computational
Linguistics.
210
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. The Annals of Probability, 25(2):855?900.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 45?48,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Y. Schabes and R.C. Waters. 1995. Tree insertion gram-
mar: a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees pro-
duced. Fuzzy Sets and Systems, 76(3):309?317.
Y. W. Teh. 2006a. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, School
of Computing, National University of Singapore.
Y. W. Teh. 2006b. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (ICCL-
ACL), pages 985?992.
F. Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS), pages 398?403.
211
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440?448,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bayesian Symbol-Refined Tree Substitution Grammars
for Syntactic Parsing
Hiroyuki Shindo? Yusuke Miyao? Akinori Fujino? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
?National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
yusuke@nii.ac.jp
Abstract
We propose Symbol-Refined Tree Substitu-
tion Grammars (SR-TSGs) for syntactic pars-
ing. An SR-TSG is an extension of the con-
ventional TSG model where each nonterminal
symbol can be refined (subcategorized) to fit
the training data. We aim to provide a unified
model where TSG rules and symbol refine-
ment are learned from training data in a fully
automatic and consistent fashion. We present
a novel probabilistic SR-TSG model based
on the hierarchical Pitman-Yor Process to en-
code backoff smoothing from a fine-grained
SR-TSG to simpler CFG rules, and develop
an efficient training method based on Markov
Chain Monte Carlo (MCMC) sampling. Our
SR-TSG parser achieves an F1 score of 92.4%
in the Wall Street Journal (WSJ) English Penn
Treebank parsing task, which is a 7.7 point im-
provement over a conventional Bayesian TSG
parser, and better than state-of-the-art discrim-
inative reranking parsers.
1 Introduction
Syntactic parsing has played a central role in natural
language processing. The resulting syntactic analy-
sis can be used for various applications such as ma-
chine translation (Galley et al, 2004; DeNeefe and
Knight, 2009), sentence compression (Cohn and La-
pata, 2009; Yamangil and Shieber, 2010), and ques-
tion answering (Wang et al, 2007). Probabilistic
context-free grammar (PCFG) underlies many sta-
tistical parsers, however, it is well known that the
PCFG rules extracted from treebank data via maxi-
mum likelihood estimation do not perform well due
to unrealistic context freedom assumptions (Klein
and Manning, 2003).
In recent years, there has been an increasing inter-
est in tree substitution grammar (TSG) as an alter-
native to CFG for modeling syntax trees (Post and
Gildea, 2009; Tenenbaum et al, 2009; Cohn et al,
2010). TSG is a natural extension of CFG in which
nonterminal symbols can be rewritten (substituted)
with arbitrarily large tree fragments. These tree frag-
ments have great advantages over tiny CFG rules
since they can capture non-local contexts explic-
itly such as predicate-argument structures, idioms
and grammatical agreements (Cohn et al, 2010).
Previous work on TSG parsing (Cohn et al, 2010;
Post and Gildea, 2009; Bansal and Klein, 2010) has
consistently shown that a probabilistic TSG (PTSG)
parser is significantly more accurate than a PCFG
parser, but is still inferior to state-of-the-art parsers
(e.g., the Berkeley parser (Petrov et al, 2006) and
the Charniak parser (Charniak and Johnson, 2005)).
One major drawback of TSG is that the context free-
dom assumptions still remain at substitution sites,
that is, TSG tree fragments are generated that are
conditionally independent of all others given root
nonterminal symbols. Furthermore, when a sentence
is unparsable with large tree fragments, the PTSG
parser usually uses naive CFG rules derived from
its backoff model, which diminishes the benefits ob-
tained from large tree fragments.
On the other hand, current state-of-the-art parsers
use symbol refinement techniques (Johnson, 1998;
Collins, 2003; Matsuzaki et al, 2005). Symbol
refinement is a successful approach for weaken-
ing context freedom assumptions by dividing coarse
treebank symbols (e.g. NP and VP) into sub-
categories, rather than extracting large tree frag-
ments. As shown in several studies on TSG pars-
ing (Zuidema, 2007; Bansal and Klein, 2010), large
440
tree fragments and symbol refinement work comple-
mentarily for syntactic parsing. For example, Bansal
and Klein (2010) have reported that deterministic
symbol refinement with heuristics helps improve the
accuracy of a TSG parser.
In this paper, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. SR-TSG is an extension of the conventional
TSG model where each nonterminal symbol can be
refined (subcategorized) to fit the training data. Our
work differs from previous studies in that we focus
on a unified model where TSG rules and symbol re-
finement are learned from training data in a fully au-
tomatic and consistent fashion. We also propose a
novel probabilistic SR-TSG model with the hierar-
chical Pitman-Yor Process (Pitman and Yor, 1997),
namely a sort of nonparametric Bayesian model, to
encode backoff smoothing from a fine-grained SR-
TSG to simpler CFG rules, and develop an efficient
training method based on blocked MCMC sampling.
Our SR-TSG parser achieves an F1 score of
92.4% in the WSJ English Penn Treebank pars-
ing task, which is a 7.7 point improvement over a
conventional Bayesian TSG parser, and superior to
state-of-the-art discriminative reranking parsers.
2 Background and Related Work
Our SR-TSG work is built upon recent work on
Bayesian TSG induction from parse trees (Post and
Gildea, 2009; Cohn et al, 2010). We firstly review
the Bayesian TSG model used in that work, and then
present related work on TSGs and symbol refine-
ment.
A TSG consists of a 4-tuple, G = (T,N, S,R),
where T is a set of terminal symbols, N is a set of
nonterminal symbols, S ? N is the distinguished
start nonterminal symbol and R is a set of produc-
tions (a.k.a. rules). The productions take the form
of elementary trees i.e., tree fragments of height
? 1. The root and internal nodes of the elemen-
tary trees are labeled with nonterminal symbols, and
leaf nodes are labeled with either terminal or nonter-
minal symbols. Nonterminal leaves are referred to
as frontier nonterminals, and form the substitution
sites to be combined with other elementary trees.
A derivation is a process of forming a parse tree.
It starts with a root symbol and rewrites (substi-
tutes) nonterminal symbols with elementary trees
until there are no remaining frontier nonterminals.
Figure 1a shows an example parse tree and Figure
1b shows its example TSG derivation. Since differ-
ent derivations may produce the same parse tree, re-
cent work on TSG induction (Post and Gildea, 2009;
Cohn et al, 2010) employs a probabilistic model of
a TSG and predicts derivations from observed parse
trees in an unsupervised way.
A Probabilistic Tree Substitution Grammar
(PTSG) assigns a probability to each rule in the
grammar. The probability of a derivation is defined
as the product of the probabilities of its component
elementary trees as follows.
p (e) =
?
x?e?e
p (e |x) ,
where e = (e1, e2, . . .) is a sequence of elemen-
tary trees used for the derivation, x = root (e) is the
root symbol of e, and p (e |x) is the probability of
generating e given its root symbol x. As in a PCFG,
e is generated conditionally independent of all oth-
ers given x.
The posterior distribution over elementary trees
given a parse tree t can be computed by using the
Bayes? rule:
p (e |t) ? p (t |e) p (e) .
where p (t |e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the task
of TSG induction from parse trees turns out to con-
sist of modeling the prior distribution p (e). Recent
work on TSG induction defines p (e) as a nonpara-
metric Bayesian model such as the Dirichlet Pro-
cess (Ferguson, 1973) or the Pitman-Yor Process to
encourage sparse and compact grammars.
Several studies have combined TSG induction and
symbol refinement. An adaptor grammar (Johnson
et al, 2007a) is a sort of nonparametric Bayesian
TSG model with symbol refinement, and is thus
closely related to our SR-TSG model. However,
an adaptor grammar differs from ours in that all its
rules are complete: all leaf nodes must be termi-
nal symbols, while our model permits nonterminal
symbols as leaf nodes. Furthermore, adaptor gram-
mars have largely been applied to the task of unsu-
pervised structural induction from raw texts such as
441
(a) (b) (c)
Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of
(a). The refinement annotation is hyphenated with a nonterminal symbol.
morphology analysis, word segmentation (Johnson
and Goldwater, 2009), and dependency grammar in-
duction (Cohen et al, 2010), rather than constituent
syntax parsing.
An all-fragments grammar (Bansal and Klein,
2010) is another variant of TSG that aims to uti-
lize all possible subtrees as rules. It maps a TSG
to an implicit representation to make the grammar
tractable and practical for large-scale parsing. The
manual symbol refinement described in (Klein and
Manning, 2003) was applied to an all-fragments
grammar and this improved accuracy in the English
WSJ parsing task. As mentioned in the introduc-
tion, our model focuses on the automatic learning of
a TSG and symbol refinement without heuristics.
3 Symbol-Refined Tree Substitution
Grammars
In this section, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. Our SR-TSG model is an extension of
the conventional TSG model where every symbol of
the elementary trees can be refined to fit the train-
ing data. Figure 1c shows an example of SR-TSG
derivation. As with previous work on TSG induc-
tion, our task is the induction of SR-TSG deriva-
tions from a corpus of parse trees in an unsupervised
fashion. That is, we wish to infer the symbol sub-
categories of every node and substitution site (i.e.,
nodes where substitution occurs) from parse trees.
Extracted rules and their probabilities can be used to
parse new raw sentences.
3.1 Probabilistic Model
We define a probabilistic model of an SR-TSG based
on the Pitman-Yor Process (PYP) (Pitman and Yor,
1997), namely a sort of nonparametric Bayesian
model. The PYP produces power-law distributions,
which have been shown to be well-suited for such
uses as language modeling (Teh, 2006b), and TSG
induction (Cohn et al, 2010). One major issue as
regards modeling an SR-TSG is that the space of the
grammar rules will be very sparse since SR-TSG al-
lows for arbitrarily large tree fragments and also an
arbitrarily large set of symbol subcategories. To ad-
dress the sparseness problem, we employ a hierar-
chical PYP to encode a backoff scheme from the SR-
TSG rules to simpler CFG rules, inspired by recent
work on dependency parsing (Blunsom and Cohn,
2010).
Our model consists of a three-level hierarchy. Ta-
ble 1 shows an example of the SR-TSG rule and its
backoff tree fragments as an illustration of this three-
level hierarchy. The topmost level of our model is a
distribution over the SR-TSG rules as follows.
e |xk ? Gxk
Gxk ? PYP
(
dxk , ?xk , P
sr-tsg (? |xk )
)
,
where xk is a refined root symbol of an elemen-
tary tree e, while x is a raw nonterminal symbol
in the corpus and k = 0, 1, . . . is an index of the
symbol subcategory. Suppose x is NP and its sym-
bol subcategory is 0, then xk is NP0. The PYP has
three parameters: (dxk , ?xk , P
sr-tsg). P sr-tsg (? |xk )
442
SR-TSG SR-CFG RU-CFG
Table 1: Example three-level backoff.
is a base distribution over infinite space of symbol-
refined elementary trees rooted with xk, which pro-
vides the backoff probability of e. The remaining
parameters dxk and ?xk control the strength of the
base distribution.
The backoff probability P sr-tsg (e |xk ) is given by
the product of symbol-refined CFG (SR-CFG) rules
that e contains as follows.
P sr-tsg (e |xk ) =
?
f?F (e)
scf ?
?
i?I(e)
(1? sci)
? H (cfg-rules (e |xk ))
? |xk ? Hxk
Hxk ? PYP
(
dx, ?x, P
sr-cfg (? |xk )
)
,
where F (e) is a set of frontier nonterminal nodes
and I (e) is a set of internal nodes in e. cf and ci
are nonterminal symbols of nodes f and i, respec-
tively. sc is the probability of stopping the expan-
sion of a node labeled with c. SR-CFG rules are
CFG rules where every symbol is refined, as shown
in Table 1. The function cfg-rules (e |xk ) returns
the SR-CFG rules that e contains, which take the
form of xk ? ?. Each SR-CFG rule ? rooted
with xk is drawn from the backoff distribution Hxk ,
and Hxk is produced by the PYP with parameters:(
dx, ?x, P sr-cfg
)
. This distribution over the SR-CFG
rules forms the second level hierarchy of our model.
The backoff probability of the SR-CFG rule,
P sr-cfg (? |xk ), is given by the root-unrefined CFG
(RU-CFG) rule as follows,
P sr-cfg (? |xk ) = I (root-unrefine (? |xk ))
? |x ? Ix
Ix ? PYP
(
d?x, ?
?
x, P
ru-cfg (? |x )
)
,
where the function root-unrefine (? |xk ) returns
the RU-CFG rule of ?, which takes the form of x?
?. The RU-CFG rule is a CFG rule where the root
symbol is unrefined and all leaf nonterminal sym-
bols are refined, as shown in Table 1. Each RU-CFG
rule ? rooted with x is drawn from the backoff distri-
bution Ix, and Ix is produced by a PYP. This distri-
bution over the RU-CFG rules forms the third level
hierarchy of our model. Finally, we set the back-
off probability of the RU-CFG rule, P ru-cfg (? |x),
so that it is uniform as follows.
P ru-cfg (? |x ) =
1
|x? ?|
.
where |x? ?| is the number of RU-CFG rules
rooted with x. Overall, our hierarchical model en-
codes backoff smoothing consistently from the SR-
TSG rules to the SR-CFG rules, and from the SR-
CFG rules to the RU-CFG rules. As shown in (Blun-
som and Cohn, 2010; Cohen et al, 2010), the pars-
ing accuracy of the TSG model is strongly affected
by its backoff model. The effects of our hierarchical
backoff model on parsing performance are evaluated
in Section 5.
4 Inference
We use Markov Chain Monte Carlo (MCMC) sam-
pling to infer the SR-TSG derivations from parse
trees. MCMC sampling is a widely used approach
for obtaining random samples from a probability
distribution. In our case, we wish to obtain deriva-
tion samples of an SR-TSG from the posterior dis-
tribution, p (e |t,d,?, s).
The inference of the SR-TSG derivations corre-
sponds to inferring two kinds of latent variables:
latent symbol subcategories and latent substitution
443
sites. We first infer latent symbol subcategories for
every symbol in the parse trees, and then infer latent
substitution sites stepwise. During the inference of
symbol subcategories, every internal node is fixed as
a substitution site. After that, we unfix that assump-
tion and infer latent substitution sites given symbol-
refined parse trees. This stepwise learning is simple
and efficient in practice, but we believe that the joint
learning of both latent variables is possible, and we
will deal with this in future work. Here we describe
each inference algorithm in detail.
4.1 Inference of Symbol Subcategories
For the inference of latent symbol subcategories, we
adopt split and merge training (Petrov et al, 2006)
as follows. In each split-merge step, each symbol
is split into at most two subcategories. For exam-
ple, every NP symbol in the training data is split into
either NP0 or NP1 to maximize the posterior prob-
ability. After convergence, we measure the loss of
each split symbol in terms of the likelihood incurred
when removing it, then the smallest 50% of the
newly split symbols as regards that loss are merged
to avoid overfitting. The split-merge algorithm ter-
minates when the total number of steps reaches the
user-specified value.
In each splitting step, we use two types of blocked
MCMC algorithm: the sentence-level blocked
Metroporil-Hastings (MH) sampler and the tree-
level blocked Gibbs sampler, while (Petrov et al,
2006) use a different MLE-based model and the EM
algorithm. Our sampler iterates sentence-level sam-
pling and tree-level sampling alternately.
The sentence-level MH sampler is a recently pro-
posed algorithm for grammar induction (Johnson et
al., 2007b; Cohn et al, 2010). In this work, we apply
it to the training of symbol splitting. The MH sam-
pler consists of the following three steps: for each
sentence, 1) calculate the inside probability (Lari
and Young, 1991) in a bottom-up manner, 2) sample
a derivation tree in a top-down manner, and 3) ac-
cept or reject the derivation sample by using the MH
test. See (Cohn et al, 2010) for details. This sampler
simultaneously updates blocks of latent variables as-
sociated with a sentence, thus it can find MAP solu-
tions efficiently.
The tree-level blocked Gibbs sampler focuses on
the type of SR-TSG rules and simultaneously up-
dates all root and child nodes that are annotated
with the same SR-TSG rule. For example, the
sampler collects all nodes that are annotated with
S0 ? NP1VP2, then updates those nodes to an-
other subcategory such as S0 ? NP2VP0 according
to the posterior distribution. This sampler is simi-
lar to table label resampling (Johnson and Goldwa-
ter, 2009), but differs in that our sampler can update
multiple table labels simultaneously when multiple
tables are labeled with the same elementary tree.
The tree-level sampler also simultaneously updates
blocks of latent variables associated with the type of
SR-TSG rules, thus it can find MAP solutions effi-
ciently.
4.2 Inference of Substitution Sites
After the inference of symbol subcategories, we
use Gibbs sampling to infer the substitution sites of
parse trees as described in (Cohn and Lapata, 2009;
Post and Gildea, 2009). We assign a binary variable
to each internal node in the training data, which in-
dicates whether that node is a substitution site or not.
For each iteration, the Gibbs sampler works by sam-
pling the value of each binary variable in random
order. See (Cohn et al, 2010) for details.
During the inference, our sampler ignores
the symbol subcategories of internal nodes of
elementary trees since they do not affect the
derivation of the SR-TSG. For example, the
elementary trees ?(S0 (NP0 NNP0) VP0)? and
?(S0 (NP1 NNP0) VP0)? are regarded as being the
same when we calculate the generation probabilities
according to our model. This heuristics is help-
ful for finding large tree fragments and learning
compact grammars.
4.3 Hyperparameter Estimation
We treat hyperparameters {d,?} as random vari-
ables and update their values for every MCMC it-
eration. We place a prior on the hyperparameters as
follows: d ? Beta (1, 1), ? ? Gamma (1, 1). The
values of d and ? are optimized with the auxiliary
variable technique (Teh, 2006a).
444
5 Experiment
5.1 Settings
5.1.1 Data Preparation
We ran experiments on the Wall Street Journal
(WSJ) portion of the English Penn Treebank data
set (Marcus et al, 1993), using a standard data
split (sections 2?21 for training, 22 for development
and 23 for testing). We also used section 2 as a
small training set for evaluating the performance of
our model under low-resource conditions. Hence-
forth, we distinguish the small training set (section
2) from the full training set (sections 2-21). The tree-
bank data is right-binarized (Matsuzaki et al, 2005)
to construct grammars with only unary and binary
productions. We replace lexical words with count
? 5 in the training data with one of 50 unknown
words using lexical features, following (Petrov et al,
2006). We also split off all the function tags and
eliminated empty nodes from the data set, follow-
ing (Johnson, 1998).
5.1.2 Training and Parsing
For the inference of symbol subcategories, we
trained our model with the MCMC sampler by us-
ing 6 split-merge steps for the full training set and 3
split-merge steps for the small training set. There-
fore, each symbol can be subdivided into a maxi-
mum of 26 = 64 and 23 = 8 subcategories, respec-
tively. In each split-merge step, we initialized the
sampler by randomly splitting every symbol in two
subcategories and ran the MCMC sampler for 1000
iterations. After that, to infer the substitution sites,
we initialized the model with the final sample from
a run on the small training set, and used the Gibbs
sampler for 2000 iterations. We estimated the opti-
mal values of the stopping probabilities s by using
the development set.
We obtained the parsing results with the MAX-
RULE-PRODUCT algorithm (Petrov et al, 2006) by
using the SR-TSG rules extracted from our model.
We evaluated the accuracy of our parser by brack-
eting F1 score of predicted parse trees. We used
EVALB1 to compute the F1 score. In all our exper-
iments, we conducted ten independent runs to train
our model, and selected the one that performed best
on the development set in terms of parsing accuracy.
1http://nlp.cs.nyu.edu/evalb/
Model F1 (small) F1 (full)
CFG 61.9 63.6
*TSG 77.1 85.0
SR-TSG (P sr-tsg) 73.0 86.4
SR-TSG (P sr-tsg, P sr-cfg) 79.4 89.7
SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg) 81.7 91.1
Table 2: Comparison of parsing accuracy with the
small and full training sets. *Our reimplementation
of (Cohn et al, 2010).
Figure 2: Histogram of SR-TSG and TSG rule sizes
on the small training set. The size is defined as the
number of CFG rules that the elementary tree con-
tains.
5.2 Results and Discussion
5.2.1 Comparison of SR-TSG with TSG
We compared the SR-TSG model with the CFG
and TSG models as regards parsing accuracy. We
also tested our model with three backoff hierarchy
settings to evaluate the effects of backoff smoothing
on parsing accuracy. Table 2 shows the F1 scores
of the CFG, TSG and SR-TSG parsers for small and
full training sets. In Table 2, SR-TSG (P sr-tsg) de-
notes that we used only the topmost level of the hi-
erarchy. Similary, SR-TSG (P sr-tsg, P sr-cfg) denotes
that we used only the P sr-tsg and P sr-cfg backoff mod-
els.
Our best model, SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg),
outperformed both the CFG and TSG models on
both the small and large training sets. This result
suggests that the conventional TSG model trained
from the vanilla treebank is insufficient to resolve
445
Model F1 (? 40) F1 (all)
TSG (no symbol refinement)
Post and Gildea (2009) 82.6 -
Cohn et al (2010) 85.4 84.7
TSG with Symbol Refinement
Zuidema (2007) - *83.8
Bansal et al (2010) 88.7 88.1
SR-TSG (single) 91.6 91.1
SR-TSG (multiple) 92.9 92.4
CFG with Symbol Refinement
Collins (1999) 88.6 88.2
Petrov and Klein (2007) 90.6 90.1
Petrov (2010) - 91.8
Discriminative
Carreras et al (2008) - 91.1
Charniak and Johnson (2005) 92.0 91.4
Huang (2008) 92.3 91.7
Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the
development set (? 100).
structural ambiguities caused by coarse symbol an-
notations in a training corpus. As we expected, sym-
bol refinement can be helpful with the TSG model
for further fitting the training set and improving the
parsing accuracy.
The performance of the SR-TSG parser was
strongly affected by its backoff models. For exam-
ple, the simplest model, P sr-tsg, performed poorly
compared with our best model. This result suggests
that the SR-TSG rules extracted from the training
set are very sparse and cannot cover the space of
unknown syntax patterns in the testing set. There-
fore, sophisticated backoff modeling is essential for
the SR-TSG parser. Our hierarchical PYP model-
ing technique is a successful way to achieve back-
off smoothing from sparse SR-TSG rules to simpler
CFG rules, and offers the advantage of automatically
estimating the optimal backoff probabilities from the
training set.
We compared the rule sizes and frequencies of
SR-TSG with those of TSG. The rule sizes of SR-
TSG and TSG are defined as the number of CFG
rules that the elementary tree contains. Figure 2
shows a histogram of the SR-TSG and TSG rule
sizes (by unrefined token) on the small training set.
For example, SR-TSG rules: S1 ? NP0VP1 and
S0 ? NP1VP2 were considered to be the same to-
ken. In Figure 2, we can see that there are almost
the same number of SR-TSG rules and TSG rules
with size = 1. However, there are more SR-TSG
rules than TSG rules with size ? 2. This shows
that an SR-TSG can use various large tree fragments
depending on the context, which is specified by the
symbol subcategories.
5.2.2 Comparison of SR-TSG with Other
Models
We compared the accuracy of the SR-TSG parser
with that of conventional high-performance parsers.
Table 3 shows the F1 scores of an SR-TSG and con-
ventional parsers with the full training set. In Ta-
ble 3, SR-TSG (single) is a standard SR-TSG parser,
446
and SR-TSG (multiple) is a combination of sixteen
independently trained SR-TSG models, following
the work of (Petrov, 2010).
Our SR-TSG (single) parser achieved an F1 score
of 91.1%, which is a 6.4 point improvement over
the conventional Bayesian TSG parser reported by
(Cohn et al, 2010). Our model can be viewed as
an extension of Cohn?s work by the incorporation
of symbol refinement. Therefore, this result con-
firms that a TSG and symbol refinement work com-
plementarily in improving parsing accuracy. Com-
pared with a symbol-refined CFG model such as the
Berkeley parser (Petrov et al, 2006), the SR-TSG
model can use large tree fragments, which strength-
ens the probability of frequent syntax patterns in
the training set. Indeed, the few very large rules of
our model memorized full parse trees of sentences,
which were repeated in the training set.
The SR-TSG (single) is a pure generative model
of syntax trees but it achieved results comparable to
those of discriminative parsers. It should be noted
that discriminative reranking parsers such as (Char-
niak and Johnson, 2005) and (Huang, 2008) are con-
structed on a generative parser. The reranking parser
takes the k-best lists of candidate trees or a packed
forest produced by a baseline parser (usually a gen-
erative model), and then reranks the candidates us-
ing arbitrary features. Hence, we can expect that
combining our SR-TSG model with a discriminative
reranking parser would provide better performance
than SR-TSG alone.
Recently, (Petrov, 2010) has reported that com-
bining multiple grammars trained independently
gives significantly improved performance over a sin-
gle grammar alone. We applied his method (referred
to as a TREE-LEVEL inference) to the SR-TSG
model as follows. We first trained sixteen SR-TSG
models independently and produced a 100-best list
of the derivations for each model. Then, we erased
the subcategory information of parse trees and se-
lected the best tree that achieved the highest likeli-
hood under the product of sixteen models. The com-
bination model, SR-TSG (multiple), achieved an F1
score of 92.4%, which is a state-of-the-art result for
the WSJ parsing task. Compared with discriminative
reranking parsers, combining multiple grammars by
using the product model provides the advantage that
it does not require any additional training. Several
studies (Fossum and Knight, 2009; Zhang et al,
2009) have proposed different approaches that in-
volve combining k-best lists of candidate trees. We
will deal with those methods in future work.
Let us note the relation between SR-CFG, TSG
and SR-TSG. TSG is weakly equivalent to CFG and
generates the same set of strings. For example, the
TSG rule ?S ? (NP NNP) VP? with probability p
can be converted to the equivalent CFG rules as fol-
lows: ?S ? NPNNP VP ? with probability p and
?NPNNP ? NNP? with probability 1. From this
viewpoint, TSG utilizes surrounding symbols (NNP
of NPNNP in the above example) as latent variables
with which to capture context information. The
search space of learning a TSG given a parse tree
is O (2n) where n is the number of internal nodes
of the parse tree. On the other hand, an SR-CFG
utilizes an arbitrary index such as 0, 1, . . . as latent
variables and the search space is larger than that of a
TSG when the symbol refinement model allows for
more than two subcategories for each symbol. Our
experimental results comfirm that jointly modeling
both latent variables using our SR-TSG assists accu-
rate parsing.
6 Conclusion
We have presented an SR-TSG, which is an exten-
sion of the conventional TSG model where each
symbol of tree fragments can be automatically sub-
categorized to address the problem of the condi-
tional independence assumptions of a TSG. We pro-
posed a novel backoff modeling of an SR-TSG
based on the hierarchical Pitman-Yor Process and
sentence-level and tree-level blocked MCMC sam-
pling for training our model. Our best model sig-
nificantly outperformed the conventional TSG and
achieved state-of-the-art result in a WSJ parsing
task. Future work will involve examining the SR-
TSG model for different languages and for unsuper-
vised grammar induction.
Acknowledgements
We would like to thank Liang Huang for helpful
comments and the three anonymous reviewers for
thoughtful suggestions. We would also like to thank
Slav Petrov and Hui Zhang for answering our ques-
tions about their parsers.
447
References
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In In Proc.
of ACL, pages 1098?1107.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP, pages 1204?1213.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL, 1:173?180.
Shay B Cohen, David M Blei, and Noah A Smith. 2010.
Variational Inference for Adaptor Grammars. In In
Proc. of HLT-NAACL, pages 564?572.
Trevor Cohn and Mirella Lapata. 2009. Sentence Com-
pression as Tree Transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing Tree-Substitution Grammars. Journal
of Machine Learning Research, 11:3053?3096.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29:589?637.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proc. of
EMNLP, page 727.
Thomas S Ferguson. 1973. A Bayesian Analysis of
Some Nonparametric Problems. Annals of Statistics,
1:209?230.
Victoria Fossum and Kevin Knight. 2009. Combining
Constituent Parsers. In Proc. of HLT-NAACL, pages
253?256.
Michel Galley, Mark Hopkins, Kevin Knight, Daniel
Marcu, Los Angeles, and Marina Del Rey. 2004.
What?s in a Translation Rule? Information Sciences,
pages 273?280.
Liang Huang. 2008. Forest Reranking : Discriminative
Parsing with Non-Local Features. In Proc. of ACL,
19104:0.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In In Proc. of HLT-NAACL, pages 317?325.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007a. Adaptor Grammars : A Frame-
work for Specifying Compositional Nonparametric
Bayesian Models. Advances in Neural Information
Processing Systems 19, 19:641?648.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-
ter. 2007b. Bayesian Inference for PCFGs via Markov
chain Monte Carlo. In In Proc. of HLT-NAACL, pages
139?146.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL, 1:423?430.
K Lari and S J Young. 1991. Applications of Stochas-
tic Context-Free Grammars Using the Inside?Outside
Algorithm. Computer Speech and Language, 5:237?
257.
Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, pages 75?82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of ACL, pages
433?440.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Proc. of HLT-NAACL, pages 19?27.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25:855?900.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In In Proc. of ACL-
IJCNLP, pages 45?48.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
YW Teh. 2006b. A Hierarchical Bayesian Language
Model based on Pitman-Yor Processes. In Proc. of
ACL, 44:985?992.
J Tenenbaum, TJ O?Donnell, and ND Goodman. 2009.
Fragment Grammars: Exploring Computation and
Reuse in Language. MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series.
Mengqiu Wang, Noah A Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model ? A Quasi-
Synchronous Grammar for QA. In Proc. of EMNLP-
CoNLL, pages 22?32.
Elif Yamangil and Stuart M Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In In
Proc. of ACL, pages 937?947.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li.
2009. K-Best Combination of Syntactic Parsers. In
Proc. of EMNLP, pages 1552?1560.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. of EMNLP-CoNLL, pages 551?560.
448
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 383?386,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MSS: Investigating the Effectiveness of Domain Combinations and
Topic Features for Word Sense Disambiguation
Sanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki Shindo
NTT Communication Science Laboratories
{sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jp
Abstract
We participated in the SemEval-2010
Japanese Word Sense Disambiguation
(WSD) task (Task 16) and focused on
the following: (1) investigating domain
differences, (2) incorporating topic fea-
tures, and (3) predicting new unknown
senses. We experimented with Support
Vector Machines (SVM) and Maximum
Entropy (MEM) classifiers. We achieved
80.1% accuracy in our experiments.
1 Introduction
We participated in the SemEval-2010 Japanese
Word Sense Disambiguation (WSD) task (Task 16
(Okumura et al, 2010)), which has two new char-
acteristics: (1) Both training and test data across
3 or 4 domains. The training data include books
or magazines (called PB), newspaper articles (PN),
and white papers (OW). The test data also include
documents from a Q&A site on the WWW (OC);
(2) Test data include new senses (called X) that are
not defined in dictionary.
There is much previous research on WSD. In
the case of Japanese, unsupervised approaches
such as extended Lesk have performed well (Bald-
win et al, 2010), although they are outperformed
by supervised approaches (Tanaka et al, 2007;
Murata et al, 2003). Therefore, we selected a su-
pervised approach and constructed Support Vector
Machines (SVM) and Maximum Entropy (MEM)
classifiers using common features and topic fea-
tures. We performed extensive experiments to in-
vestigate the best combinations of domains for
training.
We describe the data in Section 2, and our sys-
tem in Section 3. Then in Section 4, we show the
results and provide some discussion.
2 Data Description
2.1 Given Data
We show an example of Iwanami Kokugo Jiten
(Nishio et al, 1994), which is a dictionary used as
a sense inventory. As shown in Figure 1, each en-
try has POS information and definition sentences
including example sentences.
We show an example of the given training data
in (1). The given data are morphologically ana-
lyzed and partly tagged with Iwanami?s sense IDs,
such as '37713-0-0-1-1( in (1).
(1) <mor pos='??-?}( rd='??( bfm='?
?( sense= '37713-0-0-1-1( >1<</mor>
This task includes 50 target words that were
split into 219 senses in Iwanami; among them, 143
senses including two Xs that were not defined in
Iwanami, appear in the training data. In the test
data, 150 senses including eight Xs appear. The
training and test data share 135 senses including
two Xs; that is, 15 senses including six Xs in the
test data are unseen in the training data.
2.2 Data Pre-processing
We performed two preliminary pre-processing
steps. First, we restored the base forms because
the given training and test data have no informa-
tion about the base forms. (1) shows an example
of the original morphological data, and then we
added the base form (lemma), as shown in (2).
(2) <mor pos=' ? ?-? } ( rd=' ? ? (
bfm=' ? ? ( sense='37713-0-0-1-1(
lemma='1d(>1<</mor>
Secondly, we extracted example sentences from
Iwanami, which is used as a sense inventory. To
compensate for the lack of training data, we an-
alyzed examples with a morphological analyzer,
Mecab1 UniDic version, because the training and
test data were tagged with POS based on UniDic.
1http://mecab.sourceforge.net/
383
??
?
?
?
HEADWORD Ad91d[ddNd:take (?; Transitive Verb)
37713-0-0-1-0
[
<1> ??<8[GCBk3D?= to get something left into one?s hand
]
37713-0-0-1-1
[
<y> 3??=53k-<??(6
take and hold by hand. 'to lead someone by the hand(
]
?
?
?
?
?
Figure 1: Simplified Entry for Iwanami Kokugo Jiten: Ad take
For example, from the entry for Ad take, as
shown in Figure 1, we extracted an example sen-
tence and morphologically analyzed it, as shown
in (3)2, for the second sense, 37713-0-0-1-1. In
(3), the underlined part is the headword and is
tagged with 37713-0-0-1-1.
(3) 3
hand
k
ACC
1<
take
?
and
?(
lead
?(I) take someone?s hand and lead him/her?
3 System Description
3.1 Features
In this section, we describe the features we gener-
ated.
3.1.1 Baseline Features
For each target word w, we used the surface form,
the base form, the POS tag, and the top POS cat-
egories, such as nouns, verbs, and adjectives of
w. Here the target is the ith word, so we also
used the same information of i? 2, i? 1, i+ 1, and
i+2th words. We used bigrams, trigrams, and skip-
bigrams back and forth within three words. We re-
fer to the model that uses these baseline features
as bl.
3.1.2 Bag-of-Words
For each target word w, we got all base forms of
the content words within the same document or
within the same article for newspapers (PN). We
refer to the model that uses these baseline features
as bow.
3.1.3 Topic Features
In the SemEval-2007 English WSD tasks, a sys-
tem incorporating topic features achieved the
highest accuracy (Cai et al, 2007). Inspired by
(Cai et al, 2007), we also used topic features.
Their approach uses Bayesian topic models (La-
tent Dirichlet Allocation: LDA) to infer topics in
an unsupervised fashion. Then the inferred topics
2We use ACC as an abbreviation of accusative
postposition.
are added as features to reduce the sparsity prob-
lem with word-only features.
In our proposed approach, we use the inferred
topics to find 'related?( words and directly add
these word counts to the bag-of-words representa-
tion.
We applied gibbslda++3 to the training and test
data to obtain multiple topic classification per doc-
ument or article for newspapers (PN). We used the
document or article topics for newspapers (PN) in-
cluding the target word. We refer to the model
that uses these topic features as tpX, where X is
the number of topics and tpdistX with the topics
weighted by distributions. In particular, the topic
distribution of each document/article is inferred by
the LDA topic model using standard Gibbs sam-
pling.
We also add the most typical words in the topic
as a bag-of-words. For example, one topic might
include ? city, ?? Tokyo, ? train line, ? ward
and so on. A second topic might include ?? dis-
section, ? after, ?? medicine, U grave and so
on. If a document is inferred to contain the first
topic, then the words (? city, ?? Tokyo, ? train
line, ...) are added to the bag-of-words feature. We
refer to these features as twdY, including the most
typical Y words as bag-of-words.
3.2 Investigation between Domains
In preliminary experiments, we used both SVM4
and MEM (Nigam et al, 1999), with optimization
method L-BFGS (Liu and Nocedal, 1989) to train
the WSD model.
First, we investigated the effect between do-
mains (PN, PB, and OW). For training data, we se-
lected words that occur in more than 50 sentences,
separated the training data by domain, and tested
different domain combinations.
Table 1 shows the SVM results of the domain
combinations. For Table 1, we did a 5-fold cross
validation for the self domain and for comparison
3http://gibbslda.sourceforge.net/
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
384
Table 1: Investigation of Domain Combinations
on Training data (features: bl + bow, SVM)
Target Words 77, No. of Instances > 50
Domain Acc.(%) Diff. Comment
PN 78.7 - 63 words,
PN +OW 79.25 0.55 1094 instances
PN +PB 79.43 0.73
PN +ALL 79.34 0.64
PB 79.29 - 75 words,
PB +PN 78.85 -0.45 2463 instances
PB +OW 78.56 -0.73
PB +ALL 78.4 -0.89
OW 87.91 - 42 words,
OW +PN 89.05 1.14 703 instances
OW +PB 88.34 0.43
OW +ALL 89.05 1.14
with the results after adding the other domain data.
In Table 1, Diff. shows the differences to the self
domain.
As shown in Table 1, for PN and OW, using other
domains improved the results, but for PB, other do-
mains degraded the results. So we decided to se-
lect the domains for each target word.
In the formal run, for each pair of domain and
target words, we selected the combination of do-
main and dictionary examples that got the best
cross-validation result in the training data. Note
that in the case of no training data for the test data
domain, for example, since no OCs have training
data, we used all training data and dictionary ex-
amples.
We show the number of selected domain combi-
nations for each target domain in Table 2. Because
the distribution of target words is very unbalanced
in domains, not all types of target words appear in
every domain, as shown in Table 2.
3.3 Method for Predicting New Senses
We also tried to predict new senses (X) that didn?t
appear in the training data by calculating the en-
tropy for each target given in the MEM. We as-
sumed that high entropy (when the probabilities
of classes are uniformly dispersed) was indicative
of X; i.e., if [entropy > threshold] => predict X;
else => predict with MEM?s output sense tag.
Note that we used the words that were tagged
with Xs in the training data, except for the target
words. We compared the entropies of X and not
X of the words and heuristically tuned the thresh-
old based on the differences among entropies. Our
three official submissions correspond to different
thresholds.
Table 2: Used Domain Combinations
Used MEM SVM
Domain No. (%) No. (%)
Target: PB (48 types of target words)
ALL +EX 26 54.2 23 47.9
ALL 4 8.3 6 12.5
PB 11 22.9 8 16.7
PB +EX 1 2.1 1 2.1
PB +OW 1 2.1 3 6.3
PB +PN 5 10.4 7 14.6
Target: PN (46 types of target words)
ALL +EX 30 65.2 30 65.2
ALL 4 8.7 4 8.7
PN 4 8.7 1 2.2
PN +EX 0 0 1 2.2
PN +OW 2 4.3 2 4.3
PN +PB 6 13 8 17.4
Target: OW (16 types of target words)
ALL +EX 5 31.3 5 31.3
ALL 2 12.5 1 6.3
OW 6 37.5 3 18.8
OW +PB 3 18.8 3 18.8
OW +PN 0 0 4 25.0
Target: OC (46 types of target words)
ALL +EX 46 100 46 100
4 Results and Discussions
Our cross-validation experiments on the training
set showed that selecting data by domain combi-
nations works well, but unfortunately this failed
to achieve optimal results on the formal run. In
this section, we show the results using all of the
training data with no domain selections (also after
fixing some bugs).
Table 3 shows the results for the combination
of features on the test data. MEM greatly outper-
formed SVM. Its effective features are also quite
different. In the case of MEM, baseline features
(bl) almost gave the best result, and the topic fea-
tures improved the accuracy, especially when di-
vided into 200 topics. But for SVM, the topic
features are not so effective, and the bag-of-words
features improved accuracy.
For MEM with bl +tp200, which produced the
best result, the following are the best words: ?
outside (accuracy is 100%), C^ economy (98%),
?!d think (98%), d& big (98%), and %Z
culture (98%). On the other hand, the following
are the worst words: 1d take (36%), ? good
(48%), ?+d raise (48%), w2 put out (50%),
and ?= stand up (54%).
In Table 4, we show the results for each POS (bl
+tp200, MEM). The results for the verbs are com-
parably lower than the others. In future work, we
will consider adding syntactic features that may
improve the results.
385
Table 3: Comparisons among Features and Test data
TYPE Precision (%)
MEM SVM Explain
Base Line 68.96 68.96 Most Frequent Sense
bl 79.3 69.6 Base Line Features
bl +bow 77.0 70.8 + Bag-of-Words (BOW)
bl +bow +tp100 76.4 70.7 +BOW + Topics (100)
bl +bow +tp200 77.0 70.7 +BOW + Topics (200)
bl +bow +tp300 77.4 70.7 +BOW + Topics (300)
bl +bow +tp400 76.8 70.7 +BOW + Topics (400)
bl +bow +tpdist300 77.0 70.8 +BOW + Topics (300)*distribution
bl +bow +tp300 +twd100 76.2 70.8 + Topics (300) with 100 topic words
bl +bow +tp300 +twd200 76.0 70.8 + Topics (300) with 200 topic words
bl +bow +tp300 +twd300 75.9 70.8 + Topics (300) with 300 topic words
without bow
bl +tp100 79.3 69.6 + Topics (100)
bl +tp200 80.1 69.6 + Topics (200)
bl +tp300 79.6 69.6 + Topics (300)
bl +tp400 79.6 69.6 + Topics (400)
bl +tpdist100 79.3 69.6 + Topics (100)*distribution
bl +tpdist200 79.3 69.6 + Topics (200)*distribution
bl +tpdist300 79.3 69.6 + Topics (300)*distribution
bl +tp200 +twd100 74.6 69.6 + Topics (200) with 100 topic words
bl +tp300 +twd10 74.4 69.4 + Topics (300) with 10 topic words
bl +tp300 +twd20 75.2 69.3 + Topics (300) with 20 topic words
bl +tp300 +twd50 74.8 69.2 + Topics (300) with 50 topic words
bl +tp300 +twd200 74.6 69.6 + Topics (300) with 200 topic words
bl +tp300 +twd300 75.0 69.6 + Topics (300) with 300 topic words
bl +tp400 +twd100 74.1 69.6 + Topics (400) with 100 topic words
bl+tpdist100 +twd20 79.3 69.6 + Topics (100)*distribution with 20 topic words
bl+tpdist200 +twd20 79.3 69.6 + Topics (200)*distribution with 20 topic words
bl+tpdist400 +twd20 79.3 69.6 + Topics (400)*distribution with 20 topic words
Table 4: Results for each POS (bl +tp200, MEM)
POS No. of Types Acc. (%)
Nouns 22 85.5
Adjectives 5 79.2
Transitive Verbs 15 76.9
Intransitive Verbs 8 71.8
Total 50 80.1
In the formal run, we selected training data
for each pair of domain and target words and
used entropy to predict new unknown senses. Al-
though these two methods worked well in our
cross-validation experiments, they did not perform
well for the test data, probably due to domain mis-
match.
Finally, we also experimented with SVM and
MEM, and MEM gave better results.
References
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2010. A Re-
examination of MRD-based Word Sense Disambiguation.
Transactions on Asian Language Information Process, As-
sociation for Computing Machinery (ACM), 9(4):1?21.
Jun Fu Cai, Wee Sun Lee, and YW Teh. 2007. Improv-
ing Word Sense Disambiguation using Topic Features. In
Proceedings of EMNLP-CoNLL-2007, pp. 1015?1023.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited Mem-
ory BFGS Method for Large Scale Optimization. Math.
Programming, 45(3, (Ser. B)):503?528.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and Hitoshi Isahara. 2003. CRL at Japanese
dictionary-based task of SENSEVAL-2. Journal of Nat-
ural Language Processing, 10(3):115?143. (in Japanese).
Kamal Nigam, John Lafferty, and Andrew McCallum. 1999.
Using Maximum Entropy for Text Classification. In
IJCAI-99 Workshop on Machine Learning for Information
Filtering, pp. 61?67.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 Task: Japanese
WSD. In SemEval-2: Evaluation Exercises on Semantic
Evaluation.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fu-
jita, and Chikara Hashimoto. 2007. Word Sense Disam-
biguation Incorporating Lexical and Structural Semantic
Information. In Proceedings of EMNLP-CoNLL-2007, pp.
477?485.
386
