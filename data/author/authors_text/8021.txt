Classification of Multiple-Sentence Questions
Akihiro Tamura, Hiroya Takamura, and Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
aki@lr.pi.titech.ac.jp
{takamura, oku}@pi.titech.ac.jp
Abstract. Conventional QA systems cannot answer to the questions
composed of two or more sentences. Therefore, we aim to construct a
QA system that can answer such multiple-sentence questions. As the first
stage, we propose a method for classifying multiple-sentence questions
into question types. Specifically, we first extract the core sentence from
a given question text. We use the core sentence and its question focus in
question classification. The result of experiments shows that the proposed
method improves F-measure by 8.8% and accuracy by 4.4%.
1 Introduction
Question-Answering (QA) systems are useful in that QA systems return the
answer itself, while most information retrieval systems return documents that
may contain the answer.
QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC
(Question & Answering Challenge)2 in Japan. In these workshops, the inputs
to systems are only single-sentence questions, which are defined as the ques-
tions composed of one sentence. On the other hand, on the web there are a
lot of multiple-sentence questions (e.g., answer bank3, AskAnOwner4), which
are defined as the questions composed of two or more sentences: For example,
?My computer reboots as soon as it gets started. OS is Windows XP. Is there
any homepage that tells why it happens??. For conventional QA systems, these
questions are not expected and existing techniques are not applicable or work
poorly to these questions. Therefore, constructing QA systems that can handle
multiple-sentence questions is desirable.
An usual QA system is composed of three components: question process-
ing, document retrieval, and answer extraction. In question processing, a given
question is analyzed, and its question type is determined. This process is called
?question classification?. Depending on the question type, the process in the an-
swer extraction component usually changes. Consequently, the accuracy and the
efficiency of answer extraction depend on the accuracy of question classification.
1 http://trec.nist.gov/tracks.htm
2 http://www.nlp.is.ritsumei.ac.jp/qac/
3 http://www.theanswerbank.co.uk/
4 http://www.askanowner.com/
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 426?437, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Classification of Multiple-Sentence Questions 427
Therefore, as a first step towards developing a QA system that can han-
dle multiple-sentence questions, we propose a method for classifying multiple-
sentence questions. Specifically, in this work, we treat only questions which re-
quire one answer. For example, if the question ?The icon to return to desktop has
been deleted. Please tell me how to recover it.? is given, we would like ?WAY?
to be selected as the question type. We thus introduce core sentence extraction
component, which extracts the most important sentence for question classifica-
tion. This is because there are unnecessary sentences for question classification
in a multiple-sentence question, and we hope noisy features should be eliminated
before question classification with the component. If a multiple-sentence question
is given, we first extract the most important sentence for question classification
and then classify the question using the only information in the sentence.
In Section 2, we present the related work. In Section 3, we explain our pro-
posed method. In Section 4, we describe our experiments and results, where we
can confirm the effectiveness of the proposed method. Finally, in Section 5, we
describe the summary of this paper and the future work.
2 Related Work
This section presents some existing methods for question classification. The
methods are roughly divided into two groups: the ones based on hand-crafted
rules and the ones based on machine learning. The system ?SAIQA? [1], Xu et al
[2] used hand-crafted rules for question classification. However, methods based
on pattern matching have the following two drawbacks: high cost of making rules
or patterns by hand and low coverage.
Machine learning can be considered to solve these problems. Li et al [3] used
SNoW for question classification. The SNoW is a multi-class classifier that is
specifically tailored for learning in the presence of a very large number of fea-
tures. Zukerman et al [4] used decision tree. Ittycheriah et al [5] used maximum
entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared
question classification using machine learning methods (decision tree, maximum
entropy, SVM) with a rule-based method. The result showed that the accuracy
of question classification with SVM is the highest of all. According to Suzuki [6],
a lot of information is needed to improve the accuracy of question classification
and SVM is suitable for question classification, because SVM can classify ques-
tions with high accuracy even when the dimension of the feature space is large.
Moreover, Zhang et al [7] compared question classification with five machine
learning algorithms and showed that SVM outperforms the other four methods
as Suzuki [6] showed. Therefore, we also use SVM in classifying questions, as we
will explain later.
However, please note that we treat not only usual single-sentence questions,
but also multiple-sentence questions. Furthermore, our work differs from previous
work in that we treat real data on the web, not artificial data prepared for the
QA task. From these points, the results in this paper cannot be compared with
the ones in the previous work.
428 A.Tamura, H. Takamura, and M. Okumura
3 Two-Step Approach to Multiple-Sentence Question
Classification
This section describes our method for classifying multiple-sentence questions.
We first explain the entire flow of our question classification. Figure 1 shows the
proposed method.
question
preprocessing
core sentence 
extraction component 
question classification
component
a core sentence
single-sentence 
question
multiple-sentence 
question
question type
peculiar process to 
multiple-sentence questions
Fig. 1. The entire flow of question classification
An input question consisting of possibly multiple sentences is first prepro-
cessed. Parentheses parts are excluded in order to avoid errors in syntactic pars-
ing. The question is divided into sentences by punctuation marks.
The next process changes depending on whether the given question is a single-
sentence question or a multiple-sentence question. If the question consists of a
single sentence, the question is sent directly to question classification component.
If the question consists of multiple sentences, the question is sent to core sentence
extraction component. In the component, a core sentence, which is defined as
the most important sentence for question classification, is extracted. Then, the
core sentence is sent to the question classification component and the question is
classified using the information in the core sentence. In Figure 1, ?core sentence
extraction? is peculiar to multiple-sentence questions.
3.1 Core Sentence Extraction
When a multiple-sentence question is given, the core sentence of the question is
extracted. For example, if the question ?I have studied the US history. Therefore,
I am looking for the web page that tells me what day Independence Day is.? is
given, the sentence ?Therefore, I am looking for the web page that tells me what
day Independence Day is.? is extracted as the core sentence.
With the core sentence extraction, we can eliminate noisy information before
question classification. In the above example, the occurrence of the sentence
Classification of Multiple-Sentence Questions 429
?I have studied the US history.? would be a misleading information in terms of
question classification.
Here, we have based our work on the following assumption: a multiple-
sentence question can be classified using only the core sentence. Please note
that we treat only questions which require one answer.
We explain the method for extracting a core sentence. Suppose we have a
classifier, which returns Score(Si) for each sentence Si of Question. Question is
the set of sentences composing a given question. Score(Si) indicates the likeliness
of Si being the core sentence. The sentence with the largest value is selected as
the core sentence:
Core sentence = argmaxSi?QuestionScore(Si). (1)
We then extract features for constructing a classifier which returns Score(Si).
We use the information on the words as features. Only the features from the
target sentence would not be enough for accurate classification. This issue is
exemplified by the following questions (core sentences are underlined).
? Question 1:
Please advise a medication effective for hay fever. I want to relieve my
headache and stuffy nose. Especially my headache is severe.
? Question 2:
I want to relieve my headache and stuffy nose. Especially my head-
ache is severe.
While the sentence ?I want to relieve my headache and stuffy nose.? written in
bold-faced type is the core sentence in Question 2, the sentence is not suitable as
the core sentence in Question 1. These examples show that the target sentence
alone is sometimes not a sufficient evidence for core sentence extraction.
Thus, in classification of a sentence, we use its preceding and following sen-
tences. For that purpose, we introduce a notion of window size. ?Window size is
n? means ?the preceding n sentences and the following n sentences in addition to
the target sentence are used to make a feature vector?. For example, if window
size is 0, we use only the target sentence. If window size is ?, we use all the
sentences in the question.
We use SVM as a classifier. We regard the functional distance from the
separating hyperplane (i.e., the output of the separating function) as Score(Si).
Word unigrams and word bigrams of the target sentence and the sentences in
the window are used as features. A word in the target sentence and the same
word in the other sentences are regarded as two different features.
3.2 Question Classification
As discussed in Section 2, we use SVM in the classification of questions. We use
five sets of features: word unigrams, word bigrams, semantic categories of nouns,
question focuses, and semantic categories of question focuses. The semantic cat-
egories are obtained from a thesaurus (e.g., SHOP, STATION, CITY).
430 A.Tamura, H. Takamura, and M. Okumura
?Question focus? is the word that determines the answer class of the ques-
tion. The notion of question focus was described by Moldovan et al [8]. For
instance, in the question ?What country is ???, the question focus is ?coun-
try?. In many researches, question focuses are extracted with hand-crafted rules.
However, since we treat all kinds of questions including the questions which are
not in an interrogative form, such as ?Please teach me ?? and ?I don?t know ??,
it is difficult to manually create a comprehensive set of rules. Therefore, in this
paper, we automatically find the question focus in a core sentence according to
the following steps :
step 1 find the phrase5 including the last verb of the sentence or the phrase
with ??? at the end.
step 2 find the phrase that modifies the phrase found in step 1.
step 3 output the nouns and the unknown words in the phrase found in step 2.
The output of this procedure is regarded as a question focus. Although this
procedure itself is specific to Japanese, we suppose that we can extract question
focus for other languages with a similar simple procedure.
4 Experiments
We designed experiments to confirm the effectiveness of the proposed method.
In the experiments, we use data in Japanese. We use a package for SVM
computation, TinySVM 6, and a Japanese morphological analyzer, ChaSen 7 for
word segmentation of Japanese text. We use CaboCha 8 to obtain dependency
relations, when a question focus is extracted from a question. Semantic categories
are obtained from a thesaurus ?Goitaikei? [9].
4.1 Experimental Settings
We collect questions from two Japanese Q&A sites: hatena9 and
Yahoo!tiebukuro10. 2000 questions are extracted from each site and experimental
data consist of 4000 questions in total. A Q&A site is the site where a user puts a
question on the site and other users answer the question on the site. Such Q&A
sites include many multiple-sentence questions in various forms. Therefore, those
questions are appropriate for our experiments where non-artificial questions are
required.
Here, we manually exclude the following three kinds of questions from the
dataset: questions whose answers are only Yes or No, questions which require two
5 Phrase here is actually Japanese bunsetsu phrase, which is the smallest meaningful
sequence consisting of an independent word and accompanying words.
6 http://chasen.org/?taku/software/TinySVM/
7 http://chasen.naist.jp/hiki/ChaSen/
8 http://chasen.org/?taku/software/cabocha/
9 http://www.hatena.ne.jp/
10 http://knowledge.yahoo.co.jp/
Classification of Multiple-Sentence Questions 431
Table 1. The types and the distribution of 2376 questions
Nominal Answer Non-nominal Answer
Question Type Number Question Type Number
PERSON 64 REASON 132
PRODUCT 238 WAY 500
FACILITY 139 DEFINITION 73
LOCATION 393 DESCRIPTION 228
TIME 108 OPINION 173
NUMBER 53 OTHERS (TEXT) 131
OTHERS (NOUN) 144
1139 1237
TOTAL 2376
or more answers, and questions which are not actually questions. This deletion
left us 2376 questions. The question types that we used and their numbers are
shown in Table 111. Question types requiring nominal answers are determined
referring to the categories used by Sasaki et al [1].
Of the 2376 questions, 818 are single-sentence questions and 1558 are
multiple-sentence questions. The average number of sentences in a multiple-
sentence question is 3.49. Therefore, the task of core sentence extraction in our
setting is to decide a core sentence from 3.49 sentences on the average. As an eval-
uation measure for core sentence extraction, we use accuracy, which is defined
as the number of multiple-sentence questions whose core sentence is correctly
identified over the number of all the multiple-sentence questions. To calculate
the accuracy, correct core sentence of the 2376 questions is manually tagged in
the preparation of the experiments.
As an evaluation measure for question classification, we use F-measure, which
is defined as 2?Recall?Precision / (Recall+Precision). As another evaluation
measure for question classification, we use also accuracy, which is defined as the
number of questions whose type is correctly classified over the number of the
questions. All experimental results are obtained with two-fold cross-validation.
4.2 Core Sentence Extraction
We conduct experiments of core sentence extraction with four different window
sizes (0, 1, 2, and ?) and three different feature sets (unigram, bigram, and
unigram+bigram). Table 2 shows the result.
As this result shows, we obtained a high accuracy, more than 90% for this
task. The accuracy is so good that we can use this result for the succeeding task
of question classification, which is our main target. This result also shows that
large widow sizes are better for core sentence extraction. This shows that good
clues for core sentence extraction are scattered all over the question.
11 Although Sasaki et al [1] includes ORGANIZATION in question types, ORGA-
NIZATION is integrated into OTHERS (NOUN) in our work because the size of
ORGANIZATION is small.
432 A.Tamura, H. Takamura, and M. Okumura
Table 2. Accuracy of core sentence extraction with different window sizes and features
Window Size\ Features Unigram Bigram Unigram+Bigram
0 1350/1558= 0.866 1378/1558= 0.884 1385/1558= 0.889
1 1357/1558= 0.871 1386/1558= 0.890 1396/1558= 0.896
2 1364/1558= 0.875 1397/1558= 0.897 1405/1558= 0.902
? 1376/1558= 0.883 1407/1558= 0.903 1416/1558= 0.909
Table 3. Accuracy of core sentence extraction with simple methodologies
Methodology Accuracy
First Sentence 743/1558= 0.477
Last Sentence 471/1558= 0.302
Interrogative Sentence 1077/1558= 0.691
The result in Table 2 also shows that unigram+bigram features are most
effective for any window size in core sentence extraction.
To confirm the validity of our proposed method, we extract core sentences
with three simple methodologies, which respectively extract one of the following
sentences as the core sentence : (1) the first sentence, (2) the last sentence,
and (3) the last interrogative sentence (or the first sentence). Table 3 shows the
result. The result shows that such simple methodologies would not work in core
sentence extraction.
4.3 Question Classification: The Effectiveness of Core Sentence
Extraction
We conduct experiments to examine whether the core sentence extraction is
effective for question classification or not. For that purpose, we construct the
following three models:
Plain question. The given question is the input of question classification com-
ponent without core sentence extraction process.
Predicted core sentence. The core sentence extracted by the proposed
method in Section 3.1 is the input of question classification component. The
accuracy of core sentence extraction process is 90.9% as mentioned in Sec-
tion 4.2.
Correct core sentence. The correct core sentence tagged by hand is the input
of question classification component. This case corresponds to the case when
the accuracy of core sentence extraction process is 100%.
Word unigrams, word bigrams, and semantic categories of nouns are used as
features. The features concerning question focus cannot be used for the plain
question model, because the method for identifying the question focus requires
that the input be one sentence. Therefore, in order to clarify the effectiveness of
core sentence extraction itself, through fair comparison we do not use question
focus for each of the three models in these experiments.
Classification of Multiple-Sentence Questions 433
Table 4. F-measure and Accuracy of the three models for question classification
Model Plain Question Predicted Core Sentence Correct Core Sentence
Accuracy Of
Core Sentence Extraction ? 0.909 1.000
PERSON 0.462 0.434 0.505
PRODUCT 0.381 0.467 0.480
FACILITY 0.584 0.569 0.586
LOCATION 0.758 0.780 0.824
TIME 0.340 0.508 0.524
NUMBER 0.262 0.442 0.421
OTHERS (NOUN) 0.049 0.144 0.145
REASON 0.280 0.539 0.579
WAY 0.756 0.778 0.798
DEFINITION 0.643 0.624 0.656
DESCRIPTION 0.296 0.315 0.317
OPINION 0.591 0.675 0.659
OTHERS (TEXT) 0.090 0.179 0.186
Average 0.423 0.496 0.514
Accuracy 0.617 0.621 0.652
Table 4 shows the result. For most question types, the proposed method
with a predicted core sentence improves F-measure. This result shows that the
core sentence extraction is effective in question classification. We can still expect
some more improvement of performance, by boosting accuracy of core sentence
extraction.
In order to further clarify the importance of core sentence extraction, we
examine the accuracy for the questions whose core sentences are not correctly
extracted. Of 142 such questions, 54 questions are correctly classified. In short,
the accuracy is 38% and very low. Therefore, we can claim that without accurate
core sentence extraction, accurate question classification is quite hard.
4.4 Question Classification: More Detailed Investigation of Features
Here we investigate the effectiveness of each set of features and the influence
of the preceding and the following sentences of the core sentence. After that,
we conduct concluding experiments. In the first two experiments of this section,
we use only the correct core sentence tagged by hand as the input of question
classification.
The Effectiveness of Each Feature Set
First, to examine which feature set is effective in question classification, we
exclude a feature set one by one from the five feature sets described in Section
3.2 and conduct experiments of question classification. Please note that the five
feature sets can be used unlike the last experiment (Table 4), because the input
of question classification is one sentence.
434 A.Tamura, H. Takamura, and M. Okumura
Table 5. Experiments with each feature set being excluded. Here ?sem. noun? means
semantic categories of nouns. ?sem. qf? means semantic categories of question focuses.
Excluded Feature Set
All Unigram Bigram Sem. noun Qf Sem. Qf
PERSON 0.574 0.571 0.620 0.536 0.505 0.505
(-0.003) (+0.046) (-0.038) (-0.069) (-0.069)
PRODUCT 0.506 0.489 0.579 0.483 0.512 0.502
(-0.017) (+0.073) (-0.023) (+0.006) (-0.004)
FACILITY 0.612 0.599 0.642 0.549 0.615 0.576
(-0.013) (+0.03) (-0.063) (+0.003) (-0.036)
LOCATION 0.832 0.826 0.841 0.844 0.825 0.833
(-0.006) (+0.009) (+0.012) (-0.007) (+0.001)
TIME 0.475 0.506 0.548 0.420 0.502 0.517
(+0.031) (+0.073) (-0.055) (+0.027) (+0.042)
NUMBER 0.442 0.362 0.475 0.440 0.466 0.413
(-0.080) (+0.033) (-0.002) (+0.024) (-0.029)
OTHERS (NOUN) 0.210 0.182 0.267 0.204 0.198 0.156
(-0.028) (+0.057) (-0.006) (-0.012) (-0.054)
REASON 0.564 0.349 0.622 0.603 0.576 0.582
(-0.215) (+0.058) (+0.039) (+0.012) (+0.018)
WAY 0.817 0.803 0.787 0.820 0.817 0.807
(-0.014) (-0.030) (+0.003) (?0.000) (-0.010)
DEFINITION 0.652 0.659 0.603 0.640 0.647 0.633
(+0.007) (-0.049) (-0.012) (-0.005) (-0.019)
DESCRIPTION 0.355 0.308 0.355 0.363 0.357 0.334
(-0.047) (?0.000) (+0.008) (+0.002) (-0.021)
OPINION 0.696 0.670 0.650 0.703 0.676 0.685
(-0.026) (-0.046) (+0.007) (-0.020) (-0.011)
OTHERS (TEXT) 0.183 0.176 0.179 0.154 0.190 0.198
(-0.007) (-0.004) (-0.029) (+0.007) (+0.015)
Average 0.532 0.500 0.551 0.520 0.530 0.518
(-0.032) (+0.019) (-0.012) (-0.002) (-0.014)
Accuracy 0.674 0.632 0.638 0.668 0.661 0.661
Table 5 shows the result. The numbers in parentheses are differences of
F-measure compared with its original value. The decrease of F-measure suggests
the effectiveness of the excluded feature set.
We first discuss the difference of F-measure values in Table 5, by taking
PRODUCT and WAY as examples. The F-measure of PRODUCT is much
smaller than that of WAY. This difference is due to whether characteristic ex-
pressions are present in the type or not. In WAY, words and phrases such as
?method? and ?How do I - ?? are often used. Such words and phrases work as
good clues for classification. However, there is no such characteristic expressions
for PRODUCT. Although there is a frequently-used expression ?What is [noun] -
??, this expression is often used also in other types such as LOCATION and FA-
CILITY. We have to rely on currently-unavailable world knowledge of whether
the noun is a product name or not. This is the reason of the low F-measure for
PRODUCT.
We next discuss the difference of effective feature sets according to question
types. We again take PRODUCT and WAY as examples. The most effective
Classification of Multiple-Sentence Questions 435
Table 6. Experiments with different window sizes
Window Size
0 1 2 ?
PERSON 0.574 0.558 0.565 0.570
PRODUCT 0.506 0.449 0.441 0.419
FACILITY 0.612 0.607 0.596 0.578
LOCATION 0.832 0.827 0.817 0.815
TIME 0.475 0.312 0.288 0.302
NUMBER 0.442 0.322 0.296 0.311
OTHERS (NOUN) 0.210 0.123 0.120 0.050
REASON 0.564 0.486 0.472 0.439
WAY 0.817 0.808 0.809 0.792
DEFINITION 0.652 0.658 0.658 0.641
DESCRIPTION 0.355 0.358 0.357 0.340
OPINION 0.696 0.670 0.658 0.635
OTHERS (TEXT) 0.183 0.140 0.129 0.133
Average 0.532 0.486 0.477 0.463
Accuracy 0.674 0.656 0.658 0.653
feature set is semantic categories of nouns for ?PRODUCT? and bigrams for
?WAY?. Since whether a noun is a product name or not is important for PROD-
UCT as discussed before, semantic categories of nouns are crucial to PRODUCT.
On the other hand, important clues for WAY are phrases such as ?How do I?.
Therefore, bigrams are crucial to WAY.
Finally, we discuss the effectiveness of a question focus. The result in Table
5 shows that the F-measure does not change so much even if question focuses or
their semantic categories are excluded. This is because both question focuses and
their semantic categories are redundantly put in the feature sets. By comparing
Tables 4 and 5, we can confirm that question focuses improve question classifi-
cation performance (F-measure increases from 0.514 to 0.532). Please note again
that question focuses are not used in Table 4 for fair comparison.
The Influence of Window Size
Next, we clarify the influence of window size. As in core sentence extraction,
?Window size is n? means that ?the preceding n sentences and the following
n sentences in addition to the core sentence are used to make a feature vec-
tor?. We construct four models with different window sizes (0, 1, 2, and ?)
and compare their experimental results. In this experiment, we use five sets of
features and correct core sentence as the input of question classification like the
last experiment (Table 5).
Table 6 shows the result of the experiment. The result in Table 6 shows that
the model with the core sentence alone is best. Therefore, the sentences other
than the core sentence are considered to be noisy for classification and would
not contain effective information for question classification. This result suggests
that the assumption (a multiple-sentence question can be classified using only
the core sentence) described in Section 3.1 be correct.
436 A.Tamura, H. Takamura, and M. Okumura
Table 7. The result of concluding experiments
Plain Question The Proposed Method
core sentence extraction No Yes
feature sets unigram, bigram unigram,bigram,qf
sem. noun sem. noun,sem. qf
PERSON 0.462 0.492
PRODUCT 0.381 0.504
FACILITY 0.584 0.575
LOCATION 0.758 0.792
TIME 0.340 0.495
NUMBER 0.262 0.456
OTHERS (NOUN) 0.049 0.189
REASON 0.280 0.537
WAY 0.756 0.789
DEFINITION 0.643 0.626
DESCRIPTION 0.296 0.321
OPINION 0.591 0.677
OTHERS (TEXT) 0.090 0.189
Average 0.423 0.511
Accuracy 0.617 0.661
Concluding Experiments
We have so far shown that core sentence extraction and question focuses work
well for question classification. In this section, we conduct concluding experi-
ments which show that our method significantly improves the classification per-
formance. In the discussion on effective features, we used correct core sentences.
Here we use predicted core sentences.
The result is shown in Table 7. For comparison, we add to this table the
values of F-measure in Table 4, which correspond to plain question (i.e., without
core sentence extraction). The result shows that F-measure of most categories
increase, except for FACILITY and DEFINITION. From comparison of ?All?
in Table 5 with Table 7, the reason of decrease would be the low accuracies of
core sentence extraction for these categories. As shown in this table, in conclu-
sion, we obtained 8.8% increase of average F-measure of all and 4.4% increase of
accuracy, which is statistically significant in the sign-test with 1% significance-
level.
Someone may consider that the type of multiple-sentence questions can be
identified by ?one-step? approach without core sentence extraction. In a word,
the question type of each sentence in the given multiple-sentence question is
first identified by a classifier, and then the type of the sentence for which the
classifier outputs the largest score is selected as the type of the given question.
The classifier?s output indicates the likeliness of being the question type of a
given question. Therefore, we compared the proposed model with this model
in the preliminary experiment. The accuracy of question classification with the
proposed model is 66.1% (1570/2376), and that of the one-step approach is
61.7% (1467/2376). This result shows that our two-step approach is effective for
classification of multiple-sentence questions.
Classification of Multiple-Sentence Questions 437
5 Conclusions
In this paper, we proposed a method for identifying the types of multiple-
sentence questions. In our method, the core sentence is first extracted from a
given multiple-sentence question and then used for question classification.
We obtained accuracy of 90.9% in core sentence extraction and empirically
showed that larger window sizes are more effective in core sentence extraction.
We also showed that the extracted core sentences and the question focuses are
good for question classification. Core sentence extraction is quite important also
in the sense that question focuses could not be introduced without core sentences.
With the proposed method, we obtained the 8.8% increase of F-measure and
4.4% increase of accuracy.
Future work includes the following. The question focuses extracted in the
proposed method include nouns which might not be appropriate for question
classification. Therefore, we regard the improvement on the question focus detec-
tion as future work. To construct a QA system that can handle multiple-sentence
question, we are also planning to work on the other components: document re-
trieval, answer extraction.
References
1. Yutaka Sasaki, Hideki Isozaki, Tsutomu Hirao, Koji Kokuryou, and Eisaku Maeda:
NTT?s QA Systems for NTCIR QAC-1. Working Notes, NTCIR Workshop 3, Tokyo,
pp. 63?70, 2002.
2. Jinxi Xu, Ana Licuanan, and Ralph M.Weischedel: TREC 2003 QA at BBN: An-
swering Definitional Questions. TREC 2003, pp. 98?106, 2003.
3. Xin Li and Dan Roth: Learning Question Classifiers. COLING 2002, Taipei, Taiwan,
pp. 556?562, 2002.
4. Ingrid Zukerman and Eric Horvitz: Using Machine Learning Techniques to Interpret
WH-questions. ACL 2001, Toulouse, France, pp. 547?554, 2001.
5. Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and Adwait Ratnaparkhi: Ques-
tion Answering Using Maximum Entropy Components. NAACL 2001, pp. 33?39,
2001.
6. Jun Suzuki: Kernels for Structured Data in Natural Language Processing, Doctor
Thesis, Nara Institute of Science and Technology, 2005.
7. Dell Zhang and Wee Sun Lee: Question Classification using Support Vector Ma-
chines. SIGIR, Toronto, Canada, pp. 26?32, 2003.
8. Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum,
Roxana Girju, and Vasile Rus: Lasso: A Tool for Surfing the Answer Net. TREC-8,
pp. 175?184, 1999.
9. Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa,
Kentaro Ogura, Yoshifumi Oyama, and Yoshihiko Hayashi, editors: The Semantic
System, volume 1 of Goi-Taikei ? A Japanese Lexicon. Iwanami Shoten, 1997 (in
Japanese).
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 600?609, Prague, June 2007. c?2007 Association for Computational Linguistics
Japanese Dependency Analysis Using the Ancestor-Descendant Relation
Akihiro Tamura?? Hiroya Takamura?? Manabu Okumura??
? Common Platform Software Research Laboratories NEC Corporation
a-tamura@ah.jp.nec.com
?? Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan
{takamura,oku}@pi.titech.ac.jp
Abstract
We propose a novel method for Japanese de-
pendency analysis, which is usually reduced
to the construction of a dependency tree. In
deterministic approaches to this task, depen-
dency trees are constructed by series of ac-
tions of attaching a bunsetsu chunk to one of
the nodes in the tree being constructed. Con-
ventional techniques select the node based
on whether the new bunsetsu chunk and each
node in the trees are in a parent-child rela-
tion or not. However, tree structures include
relations between two nodes other than the
parent-child relation. Therefore, we use
ancestor-descendant relations in addition to
parent-child relations, so that the added re-
dundancy helps errors be corrected. Ex-
perimental results show that the proposed
method achieves higher accuracy.
1 Introduction
Japanese dependency analysis has been recognized
as one of the basic techniques in Japanese process-
ing. A number of techniques have been proposed
for years. Japanese dependency is usually repre-
sented by the relation between phrasal units called
?bunsetsu? chunks, which are the smallest meaning-
ful sequences consisting of an independent word and
accompanying words (e.g., a noun and a particle).
Hereafter, a ?chunk? means a bunsetsu chunk in this
paper. The relation between two chunks has a di-
?Akihiro Tamura belonged to Tokyo Institute of Technology
when this work was done.
Figure 1: Example of a dependency tree
rection from the modifier to the modifiee. All de-
pendencies in a sentence are represented by a de-
pendency tree, where a node indicates a chunk, and
node B is the parent of node A when chunk B is the
modifiee of chunk A. Figure 1 shows an example of
a dependency tree. The task of Japanese dependency
analysis is to find the modifiee for each chunk in a
sentence. The task is usually regarded as construc-
tion of a dependency tree.
In primitive approaches, the probabilities of de-
pendencies are given by manually constructed rules
and the modifiee of each chunk is determined. How-
ever, those rule-based approaches have problems in
coverage and consistency. Therefore, a number of
statistical techniques using machine learning algo-
rithms have recently been proposed. In most con-
ventional statistical techniques, the probabilities of
dependencies between two chunks are learned in the
learning phase, and then the modifiee of each chunk
is determined using the learned models in the anal-
ysis phase. In terms of dependency trees, the parent
node of each node is determined based on the likeli-
ness of parent-child relations between two nodes.
We here take notice of the characteristics of de-
pendencies which cannot be captured well only by
600
the parent-child relation. Consider, for example,
Figure 1. In Figure 1, ID 3(pizza-and) and ID
4(salad-accusative) are in a parallel structure. In the
structure, node 4 is a child of node 5(ate), but node
3 is not a child of 5, although 3 and 4 are both foods
and should share a tendency of being subcategorized
by the verb ?eat?. A number of conventional models
use the pair of 3(pizza-and) and 5(ate) as a nega-
tive instance because 3 does not modify 5. Conse-
quently, those models cannot learn and use the sub-
categorization preference of verbs well in the paral-
lel structures.
We focus on ancestor-descendant relations to
compensate for the weakness. Two nodes are in the
ancestor-descendant relation when one of the two
nodes is included in the path from the root node to
the other node. The upper node of the two nodes
is called an ?ancestor node? and the lower node a
?descendant node?. When the ancestor-descendant
relation is used, both of the above two instances
for nodes 3 and 4 can be considered as positive in-
stances. Therefore, it is expected that the ancestor-
descendant relation helps the algorithm capture the
characteristics that cannot be captured well by the
parent-child relation.
We aim to improve the performance of Japanese
dependency analysis by taking the ancestor-
descendant relation into account. In exploiting
ancestor-descendant information, it came to us that
redundant information is effectively utilized in a
coding problem in communications (Mackay, 2003).
Therefore, we propose a method in which the prob-
lem of determining the modifiee of a chunk is re-
garded as a kind of a coding problem: dependency is
expressed as a sequence of values, each of which de-
notes whether a parent-child relation or an ancestor-
descendant relation holds between two chunks.
In Section 2, we present the related work. In Sec-
tion 3, we explain our method. In Section 4, we de-
scribe our experiments and their results, where we
show the effectiveness of the proposed method. In
Section 5, we discuss the results of the experiments.
Finally, we describe the summary of this paper and
the future work in Section 6.
2 Conventional Statistical Methods for
Japanese Dependency Analysis
First, we describe general formulation of the
probability model for dependency analysis. We
denote a sequence of chunks, ?b1, b2, ..., bm?,
by B, and a sequence of dependency pat-
terns, ?Dep(1), Dep(2), ..., Dep(m)?, by D, where
Dep(i) = j means that bi modifies bj . Given the se-
quence B of chunks as an input, dependency analy-
sis is defined as the problem of finding the sequence
D of the dependency patterns that maximizes the
conditional probability P (D | B). A number of
the conventional methods assume that dependency
probabilities are independent of each other and ap-
proximate P (D | B) with
?m?1
i=1 P (Dep(i) | B).
P (Dep(i) | B) is estimated using machine learn-
ing algorithms. For example, Haruno et al (1999)
used Decision Trees, Sekine (2000) used Maximum
Entropy Models, Kudo and Matsumoto (2000) used
Support Vector Machines.
Another notable method is Cascaded Chunking
Model by Kudo and Matsumoto (2002). In their
model, a sentence is parsed by series of the fol-
lowing processes: whether or not the current chunk
modifies the following chunk is estimated, and if it
is so, the two chunks are merged together. Sassano
(2004) parsed a sentence efficiently using a stack.
The stack controls the modifier being analyzed.
These conventional methods determine the mod-
ifiee of each chunk based on the likeliness of de-
pendencies between two chunks (in terms of depen-
dency tree, the likeliness of parent-child relations
between two nodes). The difference between the
conventional methods and the proposed method is
that the proposed method determines the modifiees
based on the likeliness of ancestor-descendant re-
lations in addition to parent-child relations, while
the conventional methods tried to capture charac-
teristics that cannot be captured by parent-child re-
lations, by adding ad-hoc features such as features
of ?the chunk modified by the candidate modifiee?
to features of the candidate modifiee and the mod-
ifier. However, these methods do not deal with
ancestor-descendant relations between two chunks
directly, while our method uses that information di-
rectly. In Section 5, we empirically show that our
method uses the ancestor-descendant relation more
601
effectively than the conventional ones and explain
that our method is justifiable in terms of a coding
problem.
3 Proposed Method
The methods explained in this section construct a
dependency tree by series of actions of attaching
a node to one of the nodes in the trees being con-
structed. Hence, when the parent node of a certain
node is being determined, it is required that the par-
ent node should already be included in the tree being
constructed. To satisfy the requirement, we note the
characteristic of Japanese dependencies: dependen-
cies are directed from left to right. (i.e., the par-
ent node is closer to the end of a sentence than its
child node). Therefore, our methods analyze a sen-
tence backwards as in Sekine (2000) and Kudo and
Matsumoto (2000). Consider, for example, Figure
1. First, our methods determine the parent node of
ID 4(salad-accusative), and then that of ID 3(pizza-
and) is determined. Next, the parent node of ID 2(at
lunchtime), and finally, that of ID 1(he-nominative)
is determined and dependencies in a sentence are
identified. Please note that our methods are applica-
ble only to dependency structures of languages that
have a consistent head-direction like Japanese.
We explain three methods that are different in
the information used in determining the modifiee of
each chunk. In Section 3.1, we explain PARENT
METHOD and ANCESTOR METHOD, which de-
termine the modifiee of each chunk based on the
likeliness of only one type of the relation. PARENT
METHOD uses the parent-child relation, which is
used in conventional Japanese dependency analy-
sis. ANCESTOR METHOD is novel in that it
uses the ancestor-descendant relation which has not
been used in the existing methods. In Section
3.2, we explain our method, PARENT-ANCESTOR
METHOD, which determines the modifiees based
on the likeliness of both ancestor-descendant and
parent-child relations.
When the modifiee is determined using the
ancestor-descendant relation, it is necessary to take
into account the relations with every node in the tree.
Consider, for example, the case that the modifiee
of ID 1(he-nominative) is determined in Figure 1.
When using the parent-child relation, the modifiee
can be determined based only on the relation be-
tween ID 1 and 5. On the other hand, when using the
ancestor-descendant relation, the modifiee cannot be
determined based only on the relation between ID
1 and 5. This is because if one of ID 2, 3 and 4
is the modifiee of ID 1, the relation between ID 1
and 5 is ancestor-descendant. ID 5 is determined
as the modifiee of ID 1 only after the relations with
each node of ID 2, 3 and 4 are recognized not to
be ancestor-descendant. An elegant way to use the
ancestor-descendant relation, which we propose in
this paper, is to represent a dependency as a code-
word where each bit indicates the relation with a
node in the tree, and determine the modifiee based
on the relations with every node in the tree (for de-
tails to the next section).
3.1 Methods with a single relation: PARENT
METHOD and ANCESTOR METHOD
Figure 2 shows the pseudo code of the algo-
rithm to construct a dependency tree using PAR-
ENT METHOD or ANCESTOR METHOD. As
mentioned above, the two methods analyze a sen-
tence backwards. We should note that node1 to
noden in the algorithm respectively correspond to
the last chunk to the first chunk of a sentence.
MODEL PARENT(nodei,nodej) indicates the pre-
diction whether nodej is the parent of nodei or
not, which is the output of the learned model.
MODEL ANCESTOR(nodei,nodej) indicates the
prediction whether nodej is the ancestor of nodei or
not. String output indicates the sequence of the i?
1 predictions stored in step 3. The codeword denoted
by string[k] is the binary sequence given to the ac-
tion that nodei is attached to nodek. Parent[nodei]
indicates the node to which nodei is attached, and
Dis indicates a distance function. Thus, our method
predicts the correct actions by measuring the dis-
tance between the codeword string[k] and the pre-
dicted binary (later extended to real-valued) se-
quences string output. In other words, our method
selects the action that is the closest to the outputs of
the learned model.
Both models are learned from dependency trees
given as training data as shown in Figure 3. Each
relation is learned from ordered pairs of two nodes
in the trees. However, our algorithm in Figure 2
targets at dependencies directed from left to right.
602
1:for i = 1, 2, ..., n do
2: for j = 1, 2, ..., i ? 1 do
3: result parent[j]=MODEL PARENT(nodei,nodej)(in case of PARENT and PARENT-ANCESTOR METHOD)
3: result ancestor[j]=MODEL ANCESTOR(nodei,nodej)(in case of ANCESTOR and PARENT-ANCESTOR METHOD)
4: end
5: Parent[nodei]=argmink Dis(string[k], string output)
6:end
Figure 2: Pseudo code of PARENT, ANCESTOR,
and PARENT-ANCESTOR METHODS
Figure 3: Example of training instances
Therefore, the instances with a right-to-left depen-
dency are excluded from the training data. For ex-
ample, the instance with node4 being the candi-
date parent (or ancestor) of node1 is excluded in
Figure 3. MODEL PARENT uses ordered pairs
of a parent node and a child node as positive in-
stances and the other ordered pairs as negative in-
stances. MODEL ANCESTOR uses ordered pairs
of an ancestor node and a descendant node as
positive instances and the other ordered pairs as
negative instances. From the above description
and Figure 3, the number of training instances
used in learning MODEL PARENT is the same
as the number of training instances used in learn-
ing MODEL ANCESTOR. However, the number of
positive instances in learning MODEL ANCESTOR
is larger than in learning MODEL PARENT be-
cause the set of parent-child relations is a subset of
ancestor-descendant relations.
As mentioned above, the two methods analyze a
sentence backwards. We should note that node1 to
noden in the algorithm respectively correspond to
the last chunk to the first chunk of a sentence.
Next, we illustrate the process of determining the
parent node of a certain node nodem(with Figures 4
and 5). Hereafter, nodem is called a target node.
The parent node is determined based on the like-
liness of a relation; the parent-child and ancestor-
descendant relation are used in PARENT METHOD
and ANCESTOR METHOD respectively.
Our methods regard a dependency between the
target node and its parent node as a set of relations
between the target node and each node in the tree.
Each relation corresponds to one bit, which becomes
1 if the relation holds, ?1 otherwise. For example,
a sequence (?1,?1,?1, 1) represents that the par-
ent of node5 is node4 in PARENT METHOD (Fig-
ure 4), since the relation holds only between nodes
4 and 5.
First, the learned model judges whether the tar-
get node and each node in the current tree are in
a certain relation or not; PARENT METHOD uses
MODEL PARENT as the learned model and AN-
CESTOR METHOD uses MODEL ANCESTOR.
The sequence of the m?1 predictions by the learned
model is stored in string output.
The codeword string[k] is the binary (?1 or 1)
sequence that is to be output when the target node
is attached to the nodek. In Figures 4 and 5, the
set of string[k] (for node5) is in the dashed square.
For example, string[2] in ANCESTOR METHOD
(Figure 5) is (1, 1,?1,?1) since nodes 1 and 2 are
the ancestor of node5 if node5 is attached to node2.
Next, among the set of string[k], the codeword
that is the closest to the string output is selected.
The target node is then attached to the node cor-
responding to the selected codeword. In Figure 4,
the string[4], (?1,?1,?1, 1), is selected and then
node5 is attached to node4.
Japanese dependencies have the non-crossing
constraint: dependencies do not cross one another.
To satisfy the constraint, we remove the nodes that
will break the non-crossing constraint from the can-
didates of a parent node in step 5 of the algorithm.
PARENT METHOD differs from conventional
methods such as Sekine (2000) or Kudo and Mat-
sumoto (2000), in the process of determining the
parent node. These conventional methods select the
node given by argmaxjP (nodej | nodei) as the
parent node of nodei, setting the beam width to 1.
However, their processes are essentially the same as
the process in PARENT METHOD.
603
Figure 4: Analysis example using PARENT
METHOD
Figure 5: Analysis example using ANCESTOR
METHOD
3.2 Proposed method: PARENT-ANCESTOR
METHOD
The proposed method determines the parent node of
a target node based on the likeliness of ancestor-
descendant relations in addition to parent-child
relations. The use of ancestor-descendant rela-
tions makes it possible to capture the character-
istics which cannot be captured by parent-child
relations alone. The pseudo code of the pro-
posed method, PARENT-ANCESTOR METHOD,
is shown in Figure 2. MODEL PARENT and
MODEL ANCESTOR are learned as described in
Section 3.1. String output is the concatenation
of the predictions by both MODEL PARENT and
MODEL ANCESTOR. In addition, string[k] is
provided based not only on parent-child relations but
also on ancestor-descendant relations. An analysis
example using PARENT-ANCESTOR METHOD is
shown in Figure 6.
Figure 6: Analysis example using PARENT-
ANCESTOR METHOD
4 Experiment
4.1 Experimental settings
We used Kyoto University text corpus (Version
2.0) (Kurohashi and Nagao, 1997) for training and
test data. The articles on January 1st through 8th
(7,958 sentences) were used as training data, and the
articles on January 9th (1,246 sentences) as test data.
The dataset is the same as in leading works (Sekine,
2000; Kudo and Matsumoto, 2000; Kudo and Mat-
sumoto, 2002; Sassano, 2004).
We used SVMs as the algorithm of learning and
analyzing the relations between nodes. We used the
third degree polynomial kernel function and set the
soft margin parameter C to 1, which is exactly the
same setting as in Kudo and Matsumoto (2002). We
can obtain the real-valued score in step 3 of the al-
gorithm, which is the output of the separating func-
tion. The score can be regarded as likeliness of the
two nodes being in the parent-child (or the ancestor-
descendant). Therefore, we used the sequence of
the outputs of SVMs as string output, instead of
converting the scores into binary values indicating
whether a certain relation holds or not.
Two feature sets are used: static features and dy-
namic features. The static features used in the ex-
periments are shown in Table 1. The features are the
same as those used in Kudo and Matsumoto (2002).
In Table 1, HeadWord means the rightmost con-
tent word in the chunk whose part-of-speech is not
a functional category. FunctionalWord means the
604
Table 1: Static features used in experiments
Head Word (surface-form, POS, POS-subcategory,
inflection-type, inflection-form), Functional Word (
Modifier / surface-form, POS, POS-subcategory, inflection-type,
Modifiee inflection-form), brackets, quotation-marks,
punctuation-marks, position in sentence (beginning, end)
Between two distance (1,2-5,6-), case-particles, brackets,
chunks quotation-marks, punctuation-parks
Figure 7: Dynamic features
rightmost functional word or the inflectional form of
the rightmost predicate if there is no functional word
in the chunk.
Next, we explain the dynamic features used in
the experiments. Three types of dynamic features
were used in Kudo and Matsumoto (2002): (A)
the chunks modifying the current candidate modi-
fiee, (B) the chunk modified by the current candidate
modifiee, and (C) the chunks modifying the current
candidate modifier. The type C is not available in the
proposed method because the proposed method an-
alyzes a sentence backwards unlike Kudo and Mat-
sumoto (2002). Therefore, we did not use the type
C. We used the type A? and B? which are recursive
expansion of type A and B as the dynamic features
(Figure 7). The form of functional words or inflec-
tion was used as a type A? feature and POS and POS-
subcategory of HeadWord as a type B? feature.
4.2 Experimental results
In this section, we show the effectiveness of the pro-
posed method. First, we compare the three methods
described in Section 3: PARENT METHOD, AN-
CESTOR METHOD, and PARENT-ANCESTOR
METHOD. The results are shown in Table 2. Here,
dependency accuracy is the percentage of correct
dependencies (correct parent-child relations in trees
in test data), and sentence accuracy is the percent-
age of the sentences in which all the modifiees are
determined correctly (correctly constructed trees in
test data).
Table 2 shows that PARENT-ANCESTOR
METHOD is more accurate than the other two
Table 2: Result of dependency analysis using meth-
ods described in Section 3
Method Dependency SentenceAccuracy Accuracy
PARENT 88.95% 44.87%
ANCESTOR 87.64% 43.74%
PARENT-ANCESTOR 89.54% 47.38%
Table 3: Comparison to conventional methods
Feature Method Dependency SentenceAccuracy Accuracy
Only Proposed method 88.88% 46.33%
static Kudo and Matsumoto (2002) 88.71% 45.19%
Static + Proposed method 89.43% 47.94%
Dynamic A,B Kudo and Matsumoto (2002) 89.19% 46.64%
Original
Proposed method 89.54% 47.38%
Sekine (2000) 87.20% 40.76%
Kudo and Matsumoto (2000) 89.09% 46.17%
Kudo and Matsumoto (2002) 89.29% 47.53%
Sassano (2004) 89.56% 48.35%
w/o Rich Sassano (2004) 89.19% 47.05%w/o Conj 89.41% 47.86%
methods. In other words, the accuracy of depen-
dency analysis improves by utilizing the redundant
information. The improvement is statistically sig-
nificant in the sign-test with 1% significance-level.
Next, we compare the proposed method with
conventional methods. We compare the proposed
method particularly with Kudo and Matsumoto
(2002) with the same feature set. The reasons are
that Cascaded Chunking Model proposed in Kudo
and Matsumoto (2002) is used in a popular Japanese
dependency analyzer, CaboCha 1, and the compari-
son can highlight the effectiveness of our approach
because we can experiment under the same condi-
tions (e.g., dataset, feature set, learning algorithm).
A summary of the comparison is shown in Table 3.
Table 3 shows that the proposed method
outperforms conventional methods except Sas-
sano (2004)2, while Sassano (2004) used richer fea-
tures which are not used in the proposed method,
such as features for conjunctive structures based on
Kurohashi and Nagao (1994), features concerning
the leftmost content word in the candidate modi-
fiee. The comparison of the proposed method with
Sassano (2004)?s method without the features of
1http://chasen.org/?taku/software/
cabocha/
2We have not tested the improvement statistically because
we do not have access to the conventional methods.
605
Table 4: Accuracy of dependency analysis on paral-
lel structures
Parallel structures Other thanparallel structures
PARENT 74.18% 91.21%
ANCESTOR 73.24% 90.01%
PARENT-ANCESTOR 76.29% 91.63%
conjunctive structures (w/o Conj) and without the
richer features derived from the words in chunks
(w/o Rich) suggests that the proposed method is bet-
ter than or comparable to Sassano (2004)?s method.
5 Discussion
5.1 Performance on parallel structures
As mentioned in Section 1, the ancestor-descendant
relation is supposed to help to capture parallel struc-
tures. In this section, we discuss the performance of
dependency analysis on parallel structures. Parallel
structures such as those of nouns (e.g., Tom and Ken
eat hamburgers.) and those of verbs (e.g., Tom eats
hamburgers and drinks water.), are marked in Kyoto
University text corpus. We investigate the accuracy
of dependency analysis on parallel structures using
the information.
Table 4 shows that the accuracy on parallel struc-
tures improves by adding the ancestor-descendant
relation. The improvement is statistically significant
in the sign-test with 1% significance-level. Table 4
also shows that error reduction rate on parallel struc-
tures by adding the ancestor-descendant relation is
8.3% and the rate on the others is 4.7%. These show
that the ancestor-descendant relation work well es-
pecially for parallel structures.
In Table 4, the accuracy on parallel structures
using PARENT METHOD is slightly better than
that using ANCESTOR METHOD, while the dif-
ference is not statistically significant in the sign-
test. It shows that the parent-child relation is also
necessary for capturing the characteristics of paral-
lel structures. Consider the following two instances
in Figure 1 as an example: the ordered pair of ID
3(pizza-and) and ID 5(ate), and the ordered pair of
ID 4(salad-accusative) and ID 5. In ANCESTOR
METHOD, both instances are positive instances. On
the other hand, only the ordered pair of ID 4 and
ID 5 is a positive instance in PARENT METHOD.
Table 5: Comparison between usages of the
ancestor-descendant relation
Dependency Sentence
Accuracy Accuracy
Feature 88.57% 44.71%
Model 88.88% 46.33%
Hence, PARENT METHOD can learn appropriate
case-particles in a modifier of a verb. For exam-
ple, the particle which means ?and? does not mod-
ify verbs. However, it is difficult for ANCESTOR
METHOD to learn the characteristic. Therefore,
both parent-child and ancestor-descendant relations
are necessary for capturing parallel structures.
5.2 Discussion on usages of the
ancestor-descendant relation
In the proposed method, MODEL ANCESTOR,
which judges whether the relation between two
nodes is ancestor-descendant or not, is prepared,
and the information on the ancestor-descendant re-
lation is directly utilized. On the other hand,
conventional methods add the features regarding
the ancestor or descendant chunk to capture the
ancestor-descendant relation. In this section, we
empirically show that the proposed method utilizes
the information on the ancestor-descendant rela-
tion more effectively than conventional methods.
The results in the previous sections could not show
the effectiveness because MODEL PARENT and
MODEL ANCESTOR in the proposed method use
the features regarding the ancestor-descendant rela-
tion.
Table 5 shows the result of dependency analy-
sis using two types of usages of the information
on the ancestor-descendant relation. ?Feature? indi-
cates the conventional usage and ?Model? indicates
our usage. Please note that MODEL PARENT and
MODEL ANCESTOR used in ?Model? do not use
the features regarding the ancestor-descendant rela-
tion. Table 5 shows that our usage is more effec-
tive than the conventional usage. This is because
our usage takes advantage of redundancy in terms
of a coding problem as described in the next sec-
tion. Moreover, the learned features through the pro-
posed method would include more information than
606
ad-hoc features that were manually added.
5.3 Proposed method in terms of a coding
problem
In a coding problem, redundancy is effectively uti-
lized so that information can be transmitted more
properly (Mackay, 2003). This idea is the same as
the main point of the proposed method. In this sec-
tion, we discuss the proposed method in terms of a
coding problem.
In a coding problem, when encoding information,
the redundant bits are attached so that the added re-
dundancy helps errors be corrected. Moreover, the
following fact is known (Mackay, 2003):
the error-correcting ability is higher when the dis-
tances between the codewords are longer. (1)
For example, consider the following three types
of encodings: (A) two events are encoded respec-
tively into the codewords ?1 and 1 (the simplest
encoding), (B) into the codewords (?1,?1, 1) and
(1, 1, 1) (hamming distance:2), and (C) into the
codewords (?1,?1,?1) and (1, 1, 1) (hamming
distance:3). Please note that the hamming distance is
defined as the number of bits that differ between two
codewords. In (A), the correct information is not
transmitted if a one-bit error occurs. In (B), if an er-
ror occurs in the third bit, the error can be corrected
by assuming that the original codeword is closest
to the received codeword. In (C), any one-bit error
can be corrected. Thus, (B) has the higher error-
correcting ability than (A), and (C) has the higher
error-correcting ability than (B).
We explain the problem of determining the par-
ent node of a target node in the proposed method in
terms of the coding theory. A sequence of numbers
corresponds to a codeword. It is assumed that the
codeword which expresses the correct parent node
of the target node is transmitted. The codeword is
transmitted through the learned model through chan-
nels to the receiver. The receiver infers the parent
node from the received sequence (string output) in
consideration of the codewords that can be transmit-
ted (string[k]). Therefore, error-correcting ability,
the ability of correcting the errors in predictions in
step 3, is dependent on the distances between the
codewords (string[k]).
The codewords in PARENT-ANCESTOR
METHOD are the concatenation of the bits based on
both parent-child relations and ancestor-descendant
relations. Consequently, the distances between
codewords in PARENT-ANCESTOR METHOD are
longer than those in PARENT METHOD or AN-
CESTOR METHOD. From (1), the error-correcting
ability is expected to be higher. In terms of a coding
problem, the proposed method exploits the essence
of (1), and utilizes ancestor-descendant relations
effectively.
We assume that every bit added as redundancy is
correctly transmitted for the above-mentioned dis-
cussion. However, some of these added bits may be
transmitted wrongly in the proposed method. In that
case, the added redundancy may not help errors be
corrected than cause an error. In the experiments of
dependency analysis, the advantage prevails against
the disadvantage because accuracy of each bit of the
codeword is 94.5%, which is high value.
Discussion on applicability of existing codes
A number of approaches use Error Correcting
Output Coding (ECOC) (Dietterich and Bakiri,
1995; Ghani, 2000) for solving multiclass classifica-
tion problems as a coding problem. The approaches
assign a unique n-bit codeword to each class, and
then n classifiers are trained to predict each bit. The
predicted class is the one whose codeword is clos-
est to the codeword produced by the classifiers. The
codewords in these approaches are designed to be
well-separated from one another and have sufficient
error-correcting ability (e.g., BCH code).
However, these existing codewords are not ap-
plicable to the proposed method. In the proposed
method, we have two models respectively derived
from the parent-child and ancestor-descendant rela-
tion, which can be interpreted in terms of both lin-
guistic aspects and tree structures. If we use ECOC,
however, pairs of nodes are divided into positive and
negative instances arbitrarily. Since this division
lacks linguistic or structural meaning, training in-
stances will lose consistency and any proper model
will not be obtained. Moreover, we have to prepare
different models for each stage in tree construction,
because the length of the codewords vary according
to the number of nodes in the current tree.
607
Table 6: Result of dependency analysis using vari-
ous distance functions
Distance Method Dependency SentenceFunction Accuracy Accuracy
Hamming
PARENT(n) 85.05% 35.35%
PARENT(f) 85.48% 39.87%
ANCESTOR(n) 87.54% 43.42%
ANCESTOR(f) 86.97% 43.18%
Proposed method(n) 88.36% 43.74%
Proposed method(f) 88.45% 44.79%
PARENT 88.95% 44.87%
Cosine / ANCESTOR 87.64% 43.74%
Euclidean Proposed method 89.54% 47.38%
Manhattan
PARENT(n) 88.74% 44.63%
PARENT(f) 88.90% 44.79%
ANCESTOR 87.64% 43.74%
Proposed method 89.24% 46.89%
5.4 Influence of distance functions
In this section, we compare the performance of de-
pendency analysis with various distance functions:
hamming distance, euclidean distance, cosine dis-
tance, and manhattan distance. These distance func-
tions between sequences X=?x1 x2 ... xn? and
Y =?y1 y2 ... yn? are defined as follows:
? Ham(X,Y ) =
?n
i=1(1 ? ?(xi, yi)),
? Euc(X,Y ) =
??n
i=1(xi ? yi)2,
? Cos(X,Y ) = 1 ?
?n
i=1 xi?yi??n
i=1 x
2
i
??n
i=1 y
2
i
,
? Man(X,Y ) =
?n
i=1 | xi ? yi |.
In the hamming distance, string output is con-
verted to a binary sequence with their elements be-
ing of ?1 or 1. The cosine distance is equivalent to
the Euclidean distance under the condition that the
absolute value of every component of string[k] is
1.
The results of dependency analysis using these
distance functions are shown in Table 6. In Table
6, ?(n)? means that the nearest chunk in a sentence
is selected as the modifiee in order to break a tie,
which happens when the number of sequences satis-
fying the condition in step 5 is two or more, while
?(f)? means that the furthest chunk is selected. If the
results in case of (n) and (f) are the same, (n) and (f)
are omitted and only one result is shown.
Table 6 shows that the proposed method out-
performs PARENT METHOD and ANCESTOR
METHOD in any distance functions. It means that
the effectiveness of the proposed method does not
depend on distance functions. The result using the
hamming distance is much worse than using the
other distance functions. It means that using the
scores output by SVMs as the likeliness of a certain
relation improves the accuracy. The results of (n)
and (f) in the hamming distance are different. It is
because the hamming distances are always positive
integers and ties are more likely to happen. Table
6 also shows that the result of the cosine or the eu-
clidean distance is better than that of the manhattan
distance.
6 Conclusions
We proposed a novel method for Japanese depen-
dency analysis, which determines the modifiee of
each chunk based on the likeliness not only of
the parent-child relation but also of the ancestor-
descendant relation in a dependency tree. The
ancestor-descendant relation makes it possible to
capture the parallel structures in more depth. In
terms of a coding theory, the proposed method
boosts error-correcting ability by adding the redun-
dant bits based on ancestor-descendant relations and
increasing the distance between two codewords. Ex-
perimental results showed the effectiveness of the
proposed method. In addition, the results showed
that the proposed method outperforms conventional
methods.
Future work includes the following. In this pa-
per, we use the features proposed in Kudo and Mat-
sumoto (2002). By extracting new features that are
more suitable for the ancestor-descendant relation,
we can further improve our method. The features
used by Sassano (2004) are promising as well. We
are also planning to apply the proposed method to
other tasks which need to construct tree structures.
For example, (zero-) anaphora resolution is consid-
ered as a good candidate task for application.
References
Thomas G. Dietterich and Ghulum Bakiri. 1995. Solving
Multiclass Learning Problems via Error-Correcting
Output Codes. Journal of Artificial Intelligence Re-
search, 2:263?286.
Rayid Ghani. 2000. Using Error-Correcting Codes For
608
Text Classification. In Proc. of ICML-2000, pages
303?310.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1999. Using Decision Trees to Construct
a Practical Parser. Machine Learning, 34:131?149.
Taku Kudo and Yuji Matsumoto. 2000. Japanese Depen-
dency Analysis Based on Support Vector Machines. In
Proc. of EMNLP/VLC 2000, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc. of
CoNLL 2002, pages 63?69.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Uni-
versity text corpus project. In Proc. of ANLP, pages
115?118, Japan.
David J. C. Mackay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Manabu Sassano. 2004. Linear-Time Dependency Anal-
ysis for Japanese. In Proc. of COLING 2004, pages
8?14.
Satoshi Sekine. 2000. Japanese dependency analysis us-
ing a deterministic finite state transducer. In Proc. of
COLING 2000, pages 761?767.
609
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 24?36, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Bilingual Lexicon Extraction from Comparable Corpora Using Label
Propagation
Akihiro Tamura and Taro Watanabe and Eiichiro Sumita
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN
{akihiro.tamura,taro.watanabe,eiichiro.sumita}@nict.go.jp
Abstract
This paper proposes a novel method for lex-
icon extraction that extracts translation pairs
from comparable corpora by using graph-
based label propagation. In previous work,
it was established that performance drasti-
cally decreases when the coverage of a seed
lexicon is small. We resolve this problem
by utilizing indirect relations with the bilin-
gual seeds together with direct relations, in
which each word is represented by a distri-
bution of translated seeds. The seed distri-
butions are propagated over a graph repre-
senting relations among words, and transla-
tion pairs are extracted by identifying word
pairs with a high similarity in the seed dis-
tributions. We propose two types of the
graphs: a co-occurrence graph, representing
co-occurrence relations between words, and
a similarity graph, representing context sim-
ilarities between words. Evaluations using
English and Japanese patent comparable cor-
pora show that our proposed graph propaga-
tion method outperforms conventional meth-
ods. Further, the similarity graph achieved im-
proved performance by clustering synonyms
into the same translation.
1 Introduction
Bilingual lexicons are important resources for bilin-
gual tasks such as machine translation (MT) and
cross-language information retrieval (CLIR). There-
fore, the automatic building of bilingual lexicons
from corpora is one of the issues that have attracted
many researchers. As a solution, a number of pre-
vious works proposed extracting bilingual lexicons
from comparable corpora, in which documents were
not direct translations but shared a topic or domain1.
The use of comparable corpora is motivated by the
fact that large parallel corpora are only available for
a few language pairs and for limited domains.
Most of the previous methods are based on as-
sumption (I), that a word and its translation tend to
appear in similar contexts across languages (Rapp,
1999). Based on this assumption, many methods
calculate word similarity using context and then ex-
tract word translation pairs with a high-context sim-
ilarity. We call these methods context-similarity-
based methods. The context similarities are usu-
ally computed using a seed bilingual lexicon (e.g.
a general bilingual dictionary) by mapping contexts
expressed in two different languages into the same
space. In the mapping, information not represented
by the seed lexicon is discarded. Therefore, the
context-similarity-based methods could not find ac-
curate translation pairs if using a small seed lexicon.
Some of the previous methods tried to alleviate
the problem of the limited seed lexicon size (Koehn
and Knight, 2002; Morin and Prochasson, 2011;
Hazem et al2011), while others did not require any
seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et
al., 2008; Ismail and Manandhar, 2010; Daume? III
and Jagarlamudi, 2011). However, they suffer the
problems of high computational cost (Rapp, 1995),
sensitivity to parameters (Hazem et al2011),
low accuracy (Fung, 1995; Ismail and Manandhar,
2010), and ineffectiveness for language pairs with
1Although Vulic? et al2011) regarded document-aligned
texts such as texts on Wikipedia as comparable corpora, we do
not limit comparable corpora to these kinds of texts.
24
different types of characters (Koehn and Knight,
2002; Haghighi et al2008; Daume? III and Jagar-
lamudi, 2011).
In face of the above problems, we propose a novel
method that uses a graph-based label propagation
technique (Zhu and Ghahramani, 2002). The pro-
posed method is based on assumption (II), which is
derived by recursively applying assumption (I) to the
?contexts?: a word and its translation tend to have
similar co-occurrence (direct and indirect) relations
with all bilingual seeds across languages.
Based on assumption (II), we propose a three-
step approach: (1) constructing a graph for each
language with each edge indicating a direct co-
occurrence relation, (2) representing every word as a
seed translation distribution by iteratively propagat-
ing translated seeds in each graph, (3) finding two
words in different languages with a high similarity
with respect to the seed distributions. By propagat-
ing all the seeds on the graph, indirect co-occurrence
relations are also considered when computing bilin-
gual relations, which have been neglected in previ-
ous methods. In addition to the co-occurrence-based
graph construction, we propose a similarity graph,
which also takes into account context similarities be-
tween words.
The main contributions of this paper are as fol-
lows:
? We propose a bilingual lexicon extraction
method that captures co-occurrence relations
with all the seeds, including indirect rela-
tions, using graph-based label propagation.
In our experiments, we confirm that the
proposed method outperforms conventional
context-similarity-based methods (Rapp, 1999;
Andrade et al2010), and works well even if
the coverage of a seed lexicon is low.
? We propose a similarity graph which represents
context similarities between words. In our ex-
periments, we confirm that a similarity graph
is more effective than a co-occurrence-based
graph.
2 Context-Similarity-based Extraction
Method
The bilingual lexicon extraction from comparable
corpora was pioneered in (Rapp, 1995; Fung, 1995).
The popular similarity-based methods consist of the
following steps: modeling contexts, calculating con-
text similarities, and finding translation pairs.
Step 1. Modeling contexts: The context of each
word is generally modeled by a vector where each
dimension corresponds to a context word and each
dimension has a value indicating occurrence cor-
relation. Various definitions for the context have
been used: distance-based context (e.g. in a sen-
tence (Laroche and Langlais, 2010), in a para-
graph (Fung and McKeown, 1997), in a predefined
window (Rapp, 1999; Andrade et al2010)), and
syntactic-based context (e.g. predecessors and suc-
cessors in dependency trees (Garera et al2009),
certain dependency position (Otero and Campos,
2008)). Some treated context words equally re-
gardless of their positions (Fung and Yee, 1998),
while others treated the words separately for each
position (Rapp, 1999). Various correlation mea-
sures have been used: log-likelihood ratio (Rapp,
1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung
and Yee, 1998), pointwise mutual information
(PMI) (Andrade et al2010), context heterogene-
ity (Fung, 1995), etc.
Shao and Ng (2004) represented contexts using
language models. Andrade et al2010) used a
set of words with a positive association as a con-
text. Andrade et al2011a) used dependency re-
lations instead of context words. Ismail and Man-
andhar (2010) used only in-domain words in con-
texts. Pekar et al2006) constructed smoothed con-
text vectors for rare words. Laws et al2010) used
graphs in which vertices correspond to words and
edges indicate three types of syntactic relations such
as adjectival modification.
Step 2. Calculating context similarities: The con-
texts which are expressed in two different languages
are mapped into the same space. Previous methods
generally use a seed bilingual lexicon for this map-
ping. After that, similarities are calculated based
on the mapped context vectors using various mea-
sures: city-block metric (Rapp, 1999), cosine sim-
ilarity (Fung and Yee, 1998), weighted jaccard in-
dex (Hazem et al2011), Jensen-Shannon diver-
gence (Pekar et al2006), the number of overlap-
ping context words (Andrade et al2010), Sim-
Rank (Laws et al2010), euclidean distance (Fung,
1995), etc.
25
Japanese English
0.8
0.6
0.5
0.8
???? ? ????
(piranha)   (Amazon)???? ? ?????
(piranha)    (jungle)???? ? ??
(piranha)   (freshwater)?? ? ?
(freshwater)  (fish)
AssociationQuery ? Context Word
0.8
0.6
0.5
0.8
0.6
0.8
piranha   ? Amazon
piranha   ? jungle
piranha   ? freshwater
anaconda ? Amazon
anaconda ? jungle
freshwater ? fish
AssociationQuery ? Context Word
Seed Lexicon (Japanese ? English) :???? ? Amazon, ????? ? jungle, ? ? fish
Amazon   jungle???? ( 0.8 , 0.6 ) Amazon  junglepiranha    ( 0.8 , 0.6 )
anaconda ( 0.8 , 0.6 )
similarity???? ? piranha    1.0???? ? anaconda 1.0
Japanese English
0.5
0.8
0.6
0.8
0.8
0.6
0.8 0.6
0.5
0.8
?
(fish)
?????
(jungle)
????
(Amazon)
jungle
fish
???? ????? ?
0.5   ,    0.3   ,     0.2
Amazon  jungle  fish
0.55  ,   0.4   ,  0.05
Amazon jungle  fish
0.5 ,      0.3  ,   0.2
Proposed Method
Context-similarity-based Method
????
(piranha)
??
(freshwater)
freshwater
piranha
anaconda
Amazon
Figure 1: An Example of a Previous Method and our Pro-
posed Method
Andrade et al2011b) performed a linear trans-
formation of context vectors in accordance with the
notion that importance varies by context positions.
Gaussier et al2004) mapped context vectors via
latent classes to capture synonymy and polysemy in
a seed lexicon. Fis?er et al2011) and Kaji (2005)
calculated 2-way similarities.
Step 3. Finding translation pairs: A pair of words
is treated as a translation pair when their context
similarity is high. Various clues have been con-
sidered when computing the similarities: concept
class information obtained from a multilingual the-
saurus (De?jean et al2002), co-occurrence models
generated from aligned documents (Prochasson and
Fung, 2011), and transliteration information (Shao
and Ng, 2004).
2.1 Problems from Previous Works
Most of previous methods used a seed bilingual lex-
icon for mapping modeled contexts in two different
languages into the same space. The mapping heav-
ily relies on the entries in a given bilingual lexicon.
Therefore, if the coverage of the seed lexicon is low,
the context vectors become sparser and its discrim-
inative capability becomes lower, leading to extrac-
tion of incorrect translation equivalents.
Consider the example in Figure 1, where a
context-similarity-based method and our proposed
method find translation equivalents of the Japanese
word ????? (piranha)?. There are three con-
text words for the query. However, the informa-
tion on co-occurrence with ??? (freshwater)? dis-
appears after the context vector is mapped, because
the seed lexicon does not include ??? (freshwa-
ter)?. The same thing happens with the English word
?piranha?. As a result, the pair of ????? (pi-
ranha)? and ?anaconda? could be wrongly identified
as a translation pair.
Some previous work focused on the problem
of seed lexicon limitation. Morin and Prochas-
son (2011) complemented the seed lexicon with
bilingual lexicon extracted from parallel sentences.
Koehn and Knight (2002) used identically-spelled
words in two languages as a seed lexicon. However,
the method is not applicable for language pairs with
different types of characters such as English and
Japanese. Hazem et al2011) exploited k-nearest
words for a query, which is very sensitive to the pa-
rameter k.
Some previous work did not require any seed lex-
icon. Rapp (1995) proposed a computationally de-
manding matrix permutation method which maxi-
mizes a similarity between co-occurrence matrices
in two languages. Ismail and Manandhar (2010) in-
troduced a similarity measure between two words in
different languages without requiring any seed lex-
icon. Fung (1995) used context heterogeneity vec-
tors where each dimension is independent on lan-
guage types. However, their performances are worse
than those of conventional methods using a small
seed lexicon. Haghighi et al2008) and Daume?
III and Jagarlamudi (2011) proposed a generative
model based on probabilistic canonical correlation
analysis, where words are represented by context
features and orthographic features2. However, their
experiments showed that orthographic features to be
important for effectiveness, which means low per-
2In Haghighi et al2008) and Daume? III and Jagarla-
mudi (2011), indirect relations with seeds are considered topo-
logically, but our method utilizes degrees of indirect correla-
tions with seeds.
26
formance for language pairs with different character
types.
3 Lexicon Extraction Based on Label
Propagation
As described in Section 2, the performance of previ-
ous work is significantly degraded when used with a
small seed lexicon. This problem could be resolved
by incorporating indirect relations with all the seeds
when identifying translation pairs. For example, in
Figure 1, ????? (piranha)? has some degree of
association with the seed ?? - fish? through ???
(freshwater)? in both the Japanese side and the En-
glish side, although ????? (piranha)? and ??
(fish)? do not co-occur in the same contexts. More-
over, ?anaconda? has very little association with the
seed ?? - fish? in the English side. Therefore,
the indirect relation with the seed ?? - fish? helps
to discriminate from between ?piranha? and ?ana-
conda? and could be an important clue for identify-
ing a correct translation pair.
To utilize indirect relations, we introduce assump-
tion (II): a word and its translation tend to have simi-
lar co-occurrence (direct and indirect) relations with
all bilingual seeds across languages3. Based on as-
sumption (II), we propose to identify a word pair as
a translation pair when its co-occurrence (direct and
indirect) relations with all the seeds are similar.
To obtain co-occurrence relations with all the
seeds, including indirect relations, we focus on a
graph-based label propagation (LP) technique (Zhu
and Ghahramani, 2002). LP transfers labels from
labeled data points to unlabeled data points. In the
process, all vertices have soft labels that can be inter-
preted as label distributions. We apply LP to bilin-
gual lexicon extraction by representing each word as
a vertex in a graph with each edge encoding a direct
co-occurrence relation. Translated seeds are propa-
gated as labels, and seed distributions are obtained
for each word. From the seed distributions, we iden-
tify translation pairs.
In summary, our proposed method consists of
three steps (see Algorithm 1): (1) graph construc-
3Assumption (I) indicates direct co-occurrence relations be-
tween a word and its context words are preserved across differ-
ent languages. Therefore, assumption (II) is derived by recur-
sively applying assumption (I) to the ?context words?.
Algorithm 1 Bilingual Lexicon Extraction
Require: comparable corpora De and Df ,
a seed lexicon S consists of Se and Sf
Ensure: Output translation pairs T
1-1: Ge = {Ee, V e,W e} ? construct-graph(De)
1-2: Gf = {Ef , V f ,W f} ? construct-graph(Df )
2-1: G?e = {Ee, V e,W e, Qe} ? propagate-seed (Ge, Se)
2-2: G?f = {Ef , V f ,W f , Qf} ? propagate-seed (Gf , Sf )
3: T ? extract-translation (Qe, Qf , S)
tion for each language, (2) seed propagation in each
graph, (3) translation pair extraction.
3.1 Graph Construction
We construct a graph representing the association
between words for each language. Each graph is an
undirected graph because the association does not
have direction. The graphs are constructed as fol-
lows:
Step 1. Vertex assignment extracts words from
each corpus, and assigns a vertex to each of the ex-
tracted words. Let V = {v1, ? ? ? , vn} be a set of
vertices.
Step 2. Edge weight calculation calculates associ-
ation strength between two words as the weights of
edges. Let E and W be a set of edges and that of
the weights respectively, and eij ? E links vi and
vj , and wij ? W is the weight of eij . Note that
|E| = |W |.
Step 3. Edge pruning excludes edges whose
weights are lower than threshold, in order to reduce
the computational cost during seed propagations.
We propose two types of graphs that differ in the
association measure used in Step 2: a co-occurrence
graph and a similarity graph4.
3.1.1 Co-occurrence Graph
A co-occurrence graph directly encodes assump-
tion (II). Each edge in the graph indicates correlation
strength between occurrences of two linked words.
An example is shown in Figure 1.
In edge weight calculation, the co-occurrence
frequencies are first computed for each word pair in
the same context, and then the correlation strength is
estimated. There are various definitions of a context
or correlation measures that can be used (e.g. the
4We can combine the association measures used in a co-
occurrence graph and a similarity graph. We will leave this
combination approach for future work.
27
approaches used for modeling contexts in context-
similarity-based methods). In this paper, we use
words in a predefined window (window size is 10
in our experiments) as the context and PMI as the
correlation measure:
wij = PMI(vi, vj) = log
p(vi, vj)
p(vi) ? p(vj)
,
where p(vi) (or p(vj)) is the probability that vi (or
vj) occurs in a context, and p(vi, vj) is the probabil-
ity that vi and vj co-occur within the same context.
We estimate PMI(vi, vj) by the Bayesian method
proposed by Andrade et al010). Then, edges
with a negative association, PMI(vi, vj) ? 0, are
pruned in edge pruning.
3.1.2 Similarity Graph
Co-occurrence graphs are very sensitive to ac-
cidental relation caused by lower frequent co-
occurrence. Thus, we propose a similarity graph
where context similarities are employed as weights
of edges instead of simple co-occurrence-based cor-
relations. Since the context similarities are com-
puted by the global correlation among words which
co-occur, a similarity graph is less subject to acci-
dental co-occurrence. The use of a similarity graph
is inspired by assumption (III): a word and its trans-
lation tend to have similar context similarities with
all bilingual seeds across languages5.
In edge weight calculation, we first construct a
correlation vector representing co-occurrence rela-
tions for each word. The correlation vectors are con-
structed in the same way as the context vectors used
in context-similarity-based methods (see Section 2),
where context words are words in a predefined win-
dow (window size is 4 in our experiment), the as-
sociation measure is PMI, and context words are
treated separately for each position. A correlation
vector for each position is computed separately, then
concatenated into a single vector within the window.
Secondly, we calculate similarities between correla-
tion vectors. There are various similarity measures
that can be used, and cosine similarity is used in this
5This assumption is justified because context similarities are
based on co-occurrence relations that are preserved across dif-
ferent languages.
paper:
wij = Cos(f?i, f?j) =
f?i ? f?j
?f?i??f?j?
,
where f?i (or f?j) is the correlation vector of vi (or
vj). Then, in edge pruning, we preserve the edges
with top 100 weight for each vertex.
3.2 Seed Propagation
LP is a graph-based technique which transfers the
labels from labeled data to unlabeled data in or-
der to infer labels for unlabeled data. This is pri-
marily used when there is scarce labeled data but
abundant unlabeled data. LP has been success-
fully applied in common natural language process-
ing tasks such as word sense disambiguation (Niu
et al2005; Alexandrescu and Kirchhoff, 2007),
multi-class lexicon acquisition (Alexandrescu and
Kirchhoff, 2007), and part-of-speech tagging (Das
and Petrov, 2011). LP iteratively propagates la-
bel information from any vertex to nearby vertices
through weighted edges, and then a label distribu-
tion for each vertex is generated where the weights
of all labels add up to 1.
We adopt LP to obtain relations with all bilingual
seeds including indirect relations by treating each
seed as a label. First, each translated seed is assigned
to a label, and then the labels are propagated in the
graph described in Section 3.1.
The seed distribution for each word is initialized
as follows:
q0i (z) =
?
?
?
1 if vi ? Vs and z = vi
0 if vi ? Vs and z ?= vi
u(z) otherwise
,
where Vs is the set of vertices corresponding to
translated seeds, u is a uniform distribution, qki (i =
1 ? ? ? |V |) is the seed distribution for vi after k prop-
agation, and qki (z) is the weight of a label (i.e., a
translated seed) z in qki .
After initialization, we iteratively propagate the
seeds through weighted edges. In each propagation,
seeds are probabilistically propagated from linked
vertices under the condition that larger edge weights
allow seeds to travel through easier. Thus, the closer
vertices are, the more likely they have similar seed
distributions. In Figure 1, the balloons attached to
28
vertices in the graphs show examples of the seed dis-
tributions generated by propagations. For example,
the English word ?piranha? has the seed distribution
where the weights of the seeds ?Amazon?, ?jungle?,
and ?fish? are 0.5, 0.3, and 0.2, respectively. Specif-
ically, each of seed distributions is updated as fol-
lows:
qmi (z) =
?
?
?
q0i (z) if vi ? Vs
?
vj?N(vi) wij ? q
m?1
j (z)
?
vj?N(vi) wij
otherwise ,
where N(vi) is the set of vertices linking to vi. We
ran this procedure for 10 iterations in our experi-
ments.
3.3 Translation Pair Extraction
After label propagations, we treat a pair of words in
different languages with similar seed distributions as
a translation pair. Seed distribution can be regarded
as a vector where each dimension corresponds to
each translated seed and each dimension has up-
dated weight through label propagations. A sim-
ilarity between seed distributions can therefore be
calculated in the same way as a context-similarity-
based method. In this paper, we use the cosine sim-
ilarity defined by the following:
Cos(qfx , qey) =
?
si?S q
f
x(v
f
i ) ? qey(vei )
?
?
si?S(q
f
x(vfi ))2
?
?
si?S(q
e
y(vei ))2
,
where qfx (or qey) is the seed distribution for a word x
(or y) in the source language (or target language), S
is the seed lexicon whose i-th entry si is a pairing of
a translated seed in the source language vfi and one
in the target language vei .
4 Experiment
4.1 Experiment Data
We used English and Japanese patent documents
published between 1993 and 2005 by the US Patent
& Trademark Office and the Japanese Patent Of-
fice respectively, which were a part of the data used
in the NTCIR-8 patent translation task (Fujii et al
2010). Note that these documents are not aligned.
There are over three million English-Japanese
parallel sentences (e.g. training data, test data, and
Pair Japanese Word English Word
LexS 2,742 2,566 2,326
LexL 28,053 18,587 12,893
Table 1: Size of Seed Lexicons
development data used in the NTCIR-8 patent trans-
lation task, which is called NTCIR parallel data
hereafter) in the patent data. However, a preliminary
examination showed that the NTCIR parallel data
covers less than 3% of all words because there are
a number of technical terms and neologisms. There-
fore, the patent translation task is a task that requires
bilingual lexicon extraction from non-parallel data.
We selected documents belonging to the physics
domain from each monolingual corpus based on In-
ternational Patent Classification (IPC) code6, and
then used them as a comparable corpus in our ex-
periments. As a result, we used 1,479,831 Japanese
documents and 438,227 English documents. The
reason for selecting the physics domain is that this
domain contains the most documents of all the do-
mains.
The Japanese texts were segmented and part-of-
speech tagged by ChaSen7, and the English texts
were tokenized and part-of-speech tagged by Tree-
Tagger (Schmid, 1994). Next, function words were
removed since function words with little seman-
tic information spuriously co-occurred with many
words. As a result, the number of distinct words
in Japanese corpus and English corpus amounted to
1,111,302 and 4,099,8258, respectively.
We employed seed lexicons from two sources:
(1) EDR bilingual dictionary (EDR, 1990), (2)
automatic word alignments generated by running
GIZA++ (Och and Ney, 2003) with the NTCIR par-
allel data consisting of 3,190,654 parallel sentences.
From each source, we extracted pairs of nouns ap-
pearing in our corpus. From (2), we excluded word
pairs where the average of 2-way translation proba-
6SECTION G of IPC code indicates the physics domain.
7http://chasen-legacy.sourceforge.jp/
8The English words contain words in tables or mathematical
formula but the Japanese words do not because the data format
differs between English and Japanese. This is why the number
of English words is larger than that of Japanese words, even
though the number of English documents is smaller than that of
Japanese documents.
29
bilities was lower than 0.5. The pairs from (1) and
(2) amounted to 27,353 and 2,853 respectively, and
the two sets were not exclusive. In order to mea-
sure the impact of seed lexicon size, we prepared
two seed lexicons: LexL, a large seed lexicon that is
a union of all the extracted word pairs, and LexS , a
small seed lexicon that is a union of a random sam-
pling one-tenth of the pairs from (1) and one-tenth
of the pairs from (2). Table 1 shows the size of each
seed lexicon. Note that our seed lexicons include
one-to-many or many-to-one translation pairs.
We randomly selected 1,000 Japanese words as
our test data which were identified as either a noun
or an unknown by ChaSen and were not covered ei-
ther by the EDR bilingual dictionary or by the NT-
CIR parallel data. This is because the purpose of our
method is to complement existing bilingual dictio-
naries or parallel data. Note that the Japanese words
in our test data may not have translation equivalents
in the English side.
4.2 Competing Methods
We evaluated two types of our label propagation
based methods against two baselines. Cooc em-
ploys co-occurrence graphs and Sim uses similarity
graphs when constructing graphs for label propaga-
tion as described in Section 3.
Rapp is a typical context-similarity-based
method described in Section 2 (Rapp, 1999).
Context words are words in a window (window size
is 10) and are treated separately for each position.
Associations with context words are computed
using the log-likelihood ratio (Dunning, 1993). The
similarity measure between context vectors is the
city-block metric.
Andrade is a sophisticated method in context-
similarity-based methods (Andrade et al2010).
Context is a set of words with a positive association
in a window (window size is 10). The association
is calculated using the PMI estimated by a Bayesian
method, and a similarity between contexts is esti-
mated based on the number of overlapping words
(see the original paper for details).
4.3 Experiment Results
Table 2 shows the performance of each method us-
ing LexS or LexL. Hereafter, Method(L) (or
Method(S)) denotes the Method using LexL (or
LexS LexL
Acc1 Acc20 Acc1 Acc20
Rapp 1.5% 3.8% 4.8% 17.6%
Andrade 1.9% 4.2% 5.6% 17.6%
Cooc 3.2% 8.6% 9.2% 28.3%
Sim 4.1% 11.5% 10.8% 30.6%
Table 2: Performance on Bilingual Lexicon Extraction
LexS). We measure the performance on bilingual
lexicon extraction as Top N accuracy (AccN ), which
is the number of test words whose top N translation
candidates contain a correct translation equivalent
over the total number of test words (=1,000). Table
2 shows Top 1 and Top 20 accuracy. We manually9
evaluated whether translation candidates contained a
correct translation equivalent. We did not use recall
because we do not know if the translation equiva-
lents of a test word appear in the corpus.
Table 2 shows that the proposed methods outper-
form the baselines both when using LexS and using
LexL. The improvements are statistically significant
in the sign-test with 1% significance-level. The re-
sults show that capturing the relations with all the
seeds including indirect relations is effective.
The accuracies of the baselines in Table 2 are
worse than the previous reports: 14% Acc1 and 46%
Acc10 (Andrade et al2010), and 72% Acc1 (Rapp,
1999). This is because previous works evalu-
ated only the queries whose translation equivalents
existed in the experiment data, which is not al-
ways true in our experiments. Moreover, previous
works evaluated only high-frequency words: com-
mon nouns (Rapp, 1999) and words with a docu-
ment frequency of at least 50 (Andrade et al2010).
Our test data, on the other hand, includes many low-
frequency words. It is generally true that translation
of high-frequency words is much easier than that of
low frequency words. We discuss the impact of test
word frequencies in detail in Section 5.3.
Table 2 also shows that Sim outperforms Cooc
both when using LexS and using LexL. The im-
provements of Acc20 are statistically significant in
the sign-test with 5% significance-level.
9We could not evaluate using existing dictionaries because
most of the test data are technical terms and neologisms not
included in the dictionaries.
30
Sim(L) (2) Cooc(L) (5) Andrade(L) (181)
1 psychosis polynephropathy disease
2 manic-depression neuroleptic bowel
3 epilepsy iridocyclitis disorder
4 insomnia Tic symptom
5 dementia manic-depression sclerosis
Sim(S) (974) Cooc(S) (1652) Andrade(S) (1747)
1 ulceration dyslinesia bulimia
2 ulcer encephalomyelopathy spasticity
3 naphthol ganglionic Parkinson
4 dementia corticobasal Asymmetric
5 gastritis praecox anorexia
Table 3: Translation Candidates for ??? (manic-
depression)
???
Cooc(L) Andrade(L) Cooc(S) Andrade(S)
1 ??? (0.12) ??? (7.6) ?? (0.016) ?? (5.0)
narcotic narcotic dementia posteriori
2 ??? (0.11) ?? (6.3) ?? (0.014) ?? (3.7)
psychosis old alien,stepchild dementia
3 ??? (0.08) ??? (6.3) ?? (0.012) ?? (3.2)
neurosis psychosis posteriori ulcer
4 ???? (0.05) ???? (5.6) ?? (0.012) ???? (2.9)
hormone bronchitis electropositivity period
5 ??? (0.04) ?? (5.0) ?? (0.011) ?? (2.5)
insomnia posteriori ulcer seriousness
manic-depression
Cooc(L) Andrade(L) Cooc(S) Andrade(S)
1 illness illness ganja galop(0.15) (8.6) (0.012) (7.0)
2 neurosis psychotherapeutics carbanilide madness(0.11) (7.0) (0.011) (5.4)
3 seizure galop paludism libido(0.07) (7.0) (0.011) (5.2)
4 psychosis psychosis resignation vitiligo(0.06) (6.8) (0.010) (4.6)
5 insomnia somnambulism galop dementia(0.04) (6.7) (0.009) (4.3)
Table 4: Seeds with the Highest Weight
5 Discussion
5.1 Effect of Indirect Relations with Seeds
Table 3 shows a list of the top 5 translation can-
didates for the Japanese word ???? (manic-
depression)? for each method, where the ranks of the
correct translations are shown in parentheses next to
method names. Table 4 shows the top 5 translated
seeds which characterize the query, where the val-
ues in parentheses indicate weight. Table 3 shows
that Cooc(L) can find the correct translation equiv-
alent but Andrade(L) cannot. Table 4 shows that
Cooc(L) can utilize more seeds closely tied to the
query (e.g. ???? (neurosis)?, ???? (insom-
nia)?), which did not occur in the context of the
query in the experiment data. The result shows that
indirectly-related seeds are also important clues, and
our proposed method can utilize these.
5.2 Impact of Seed Lexicon Size
Table 2 shows that a reduction of seed lexicon size
degrades performance. This is natural for the base-
line methods because LexS cannot translate most of
context words, which are necessary for word charac-
terization. Consider Andrade(L) and Andrade(S)
in the example in Section 5.1. Table 4 shows that
Andrade(S) uses less relevant seeds with the query,
and has to express the query by seeds with less as-
sociation. For example, ???? (psychosis)? can-
not be used in Andrade(S) because LexS does not
have the seed. Therefore, it is more difficult for
Andrade(S) to find correct translation pairs.
The proposed methods also share the same ten-
dency, although each word is expressed by all the
seeds in the seed lexicon. Consider Cooc(L) and
Cooc(S) in the above example. Table 4 shows that
Cooc(S) expresses the query by a smooth seed dis-
tribution, which is difficult to discriminate from oth-
ers. This is because LexS does not have relevant
seeds for the query. This is why Cooc(S) cannot
find the correct translation equivalent. On the other
hand, Cooc(L) characterizes ????? and ?manic-
depression? by strongly relevant seeds (e.g. ???
? (psychosis)?,???? (neurosis)?), and then finds
the correct translation equivalent.
To examine the robustness-to-seed lexicon size,
we calculated the reduction rate of Acc20 with the
following expression: (Acc20 with LexL ? Acc20
with LexS) / Acc20 with LexL. The reduction rates
of Rapp, Andrade, Cooc, and Sim are 78.4%,
76.1%, 69.6%, and 62.4% respectively. Moreover,
the difference between degradation in Cooc and that
in Andrade is statistically significant in the sign-test
with 1% significance-level. These results indicate
that the proposed methods are more robust to seed
lexicon size than the baselines. This is because the
proposed methods can utilize seeds with indirect re-
lations while the baselines utilize only seeds in the
context.
To verify our claim, we examined the number
of test words which occurred with no seeds in the
context. There were 570 such words in Rapp(S),
387 in Rapp(L), 572 in Andrade(S), and 388 in
Andrade(L). The baselines cannot find their trans-
31
Low Freq. High Freq.
Acc1 Acc20 Acc1 Acc20
Rapp(L) 0.5% 2.4% 7.2% 25.6%
Andrade(L) 0.3% 1.8% 8.6% 26.3%
Cooc(L) 0.8% 4.3% 13.9% 40.7%
Sim(L) 2.2% 6.7% 15.0% 42.0%
Table 5: Comparison between Performance for High and
Low Frequency Words
lation equivalents. Words such as this occur even if
using LexL, and that number increases when LexS
is used. On the other hand, the proposed methods
are able to utilize all the seeds in order to find equiv-
alents for words such as these. Therefore, the pro-
posed methods work well even if the coverage of a
seed lexicon is low.
5.3 Impact of Word Frequencies
Our test data includes many low-frequency words
which are not covered by the EDR bilingual dic-
tionary or the NTCIR parallel data. 624 words ap-
pear in the corpus less than 50 times. Table 5 shows
AccN using LexL for 624 low-frequency words and
376 high-frequency words. Table 5 shows that per-
formance for low-frequency words is much worse
than that for high-frequency words. This is because
translation of high-frequency words utilizes abun-
dant and reliable context information, while the con-
text information for low-frequency words is statis-
tically unreliable. In the proposed methods, edges
linking rare words are sometimes generated based
on accidental co-occurrences, and then unrelated
seed information is transferred through the edges.
Therefore, even our label propagation based meth-
ods, especially for Cooc, could not identify the cor-
rect translation equivalents for rare words. Sim al-
leviated the problem by using a similarity graph in
which edges are generated based on global correla-
tion among words, as indicated by Table 5. Table
5 also suggests that top 20 translation candidates for
high-frequency words have potential to contribute to
bilingual tasks such as MT and CLIR although the
overall performance is still low.
5.4 Effect of Similarity Graphs
We examined AccN for synonyms of translated
seeds in Japanese. The Acc1 and Acc20 of Sim(L)
are 15.6% and 56.3%, respectively, and those of
Cooc(L) are 9.4% and 37.5%, respectively. The
results show that similarity graphs are effective for
clustering synonyms into the same translation equiv-
alents. For example, Sim(L) extracted the correct
translation pair of the English word ?iodine? and
the Japanese word ???????, a synonym of the
translated seed ???? (iodine)? in Japanese. This
is because synonyms tend to be linked in the similar-
ity graph and have similar seed distributions. On the
other hand, in the co-occurrence graph, synonyms
tend to be indirectly linked through mutual context
words, so the seed distributions of the two could be
far away from each other.
There are in particular many loanwords in patent
documents, which are spelled in different ways from
person to person. For example, the loan word for the
English word ?user? is often written as ?????,
but it is sometimes written as ??????, with an
additional prolonged sound mark. Therefore, Sim
is particularly effective for the experiment data.
5.5 Error Analysis
We discuss errors of the proposed methods except
the errors for low-frequency words (see Section
5.3). Our test data includes words whose transla-
tion equivalents inherently cannot be found. The
first of these types are words whose equivalent does
not exist in the English corpus. This is an unavoid-
able problem for methods based on comparable cor-
pora. The second one are words whose English
equivalents are compound words. The Japanese
morphological analyzer tends to group a compound
word into a single word, while the English text an-
alyzer does not perform a collocation of words di-
vided by the delimiter space. For example, the sin-
gle Japanese word ???? is equivalent to ?palm
pattern? or ?palm print?, which is composed of
two words. This case was counted as an error
even though the proposed methods found the word
?palm? as a equivalent of ????.
A main reason of errors other than those above
is word sense ambiguity, which is different in ev-
ery language. For example, the Japanese word ???
32
means ?right? and ?conservatism? in English. The
proposed methods merge different senses by prop-
agating seeds through these polysemous words in
only one language side. This is why translation pairs
could have wrong seed distributions and then the
proposed methods could not identify correct trans-
lation pairs. We will leave this word sense disam-
biguation problem for future work.
6 Related Work
Besides the comparable corpora approach discussed
in Section 2, many alternatives have been proposed
for bilingual lexicon extraction. The first is a method
that finds translation pairs in parallel corpora (Wu
and Xia, 1994; Fung and Church, 1994; Och and
Ney, 2003). However, large parallel corpora are only
available for a few language pairs and for limited
domains. Moreover, even the large parallel corpora
are relatively smaller than comparable corpora.
The second is a method that exploits the Web. Lu
et al2004) extracted translation pairs by mining
web anchor texts and link structures. As an alter-
native, mixed-language web pages are exploited by
first retrieving texts including both source and tar-
get languages from the web by using a search en-
gine or simple rules, and then extracting transla-
tion pairs from the mixed-language texts utilizing
various clues: Zhang and Vines (2004) used co-
occurrence statistics, Cheng et al2004) used co-
occurrences and context similarity information, and
Huang et al2005) used phonetic, semantic and
frequency-distance features. Lin et al2008) pro-
posed a method for extracting parenthetically trans-
lated terms, where a word alignment algorithm is
used for establishing the correspondences between
in-parenthesis and pre-parenthesis words. However,
those methods cannot find translation pairs when
they are not connected with each other through link
structures, or when they do not co-occur in the same
text.
Transliteration is a completely different way for
bilingual lexicon acquisition, in which a word in
one language is converted into another language us-
ing phonetic equivalence (Knight and Graehl, 1998;
Karimi et al2011). Although machine transliter-
ation works particularly well for proper names and
loan words, it cannot be employed for phonetically
dissimilar translations.
All the methods mentioned above may poten-
tially extract translation pairs more precisely than
our comparable corpora approach when their under-
lying assumptions are satisfied. We might improve
the performance of our method by augmenting a
seed lexicon with translation pairs extracted using
the above methods, as experimented with in Section
4, in which additional lexical entries are included
from parallel data.
7 Conclusion
We proposed a novel bilingual lexicon extraction
method using label propagation for alleviating the
limited seed lexicon size problem. The proposed
method captures relations with all the seeds in-
cluding indirect relations by propagating seed in-
formation. Moreover, we proposed using similar-
ity graphs in propagation process in addition to co-
occurrence graphs. Our experiments showed that the
proposed method outperforms conventional context-
similarity-based methods (Rapp, 1999; Andrade et
al., 2010), and the similarity graphs improve the
performance by clustering synonyms into the same
translation.
We are planning to investigate the following open
problems in future work: word sense disambigua-
tion and translation of compound words as described
in (Daille and Morin, 2005; Morin et al2007).
In addition, indirect relations have also been used
in other tasks, such as paraphrase acquisition from
bilingual parallel corpora (Kok and Brockett, 2010).
We will utilize their random walk approach or other
graph-based techniques such as modified adsorp-
tion (Talukdar and Crammer, 2009) for generating
seed distributions. We are also planning an end-to-
end evaluation, for instance, by employing the ex-
tracted bilingual lexicon into an MT system.
Acknowledgments
We thank anonymous reviewers of EMNLP-CoNLL
2012 for helpful suggestions and comments on a first
version of this paper. We also thank anonymous re-
viewers of First Workshop on Multilingual Model-
ing (MM-2012) for useful comments on this work.
33
References
Andrei Alexandrescu and Katrin Kirchhoff. 2007.
Data-Driven Graph Construction for Semi-Supervised
Graph-Based Learning in NLP. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 204?211.
Daniel Andrade, Tetsuya Nasukawa, and Junichi Tsu-
jii. 2010. Robust Measurement and Comparison
of Context Similarity for Finding Translation Pairs.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), pages
19?27.
Daniel Andrade, Takuya Matsuzaki, and Junichi Tsu-
jii. 2011a. Effective Use of Dependency Structure
for Bilingual Lexicon Creation. In Proceedings of
the 12th International Conference on Computational
Linguistics and Intelligent Text Processing (CICLing
2011) - Volume Part II, pages 80?92.
Daniel Andrade, Takuya Matsuzaki, and Junichi Tsujii.
2011b. Learning the Optimal Use of Dependency-
parsing Information for Finding Translations with
Comparable Corpora. In Proceedings of the 4th Work-
shop on Building and Using Comparable Corpora,
pages 10?18.
Pu-Jen Cheng, Jei-Wen Teng, Ruei-Cheng Chen, Jenq-
Haur Wang, Wen-Hsiang Lu, and Lee-Feng Chien.
2004. Translating Unknown Queries with Web Cor-
pora for Cross-Language Information Retrieval. In
Proceedings of the 27th Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 146?153.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002), pages 1?5.
Be?atrice Daille and Emmanuel Morin. 2005. French-
English Terminology Extraction from Comparable
Corpora. In Proceedings of 2nd International Joint
Conference on Natural Language Processing (IJCNLP
2005), pages 707?718.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
600?609.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main Adaptation for Machine Translation by Mining
Unseen Words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT2011),
pages 407?412.
Herve? De?jean, ?Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational linguistics (COLING 2002), pages 1?
7.
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. COMPUTATIONAL
LINGUISTICS, 19(1):61?74.
EDR. 1990. Bilingual Dictionary. In Technical Report
TR-029. Japan Electronic Dictionary Research Insti-
tute, Tokyo.
Darja Fis?er, Nikola Ljubes?ic?, ?Spela Vintar, and Senja Pol-
lak. 2011. Building and using comparable corpora for
domain-specific bilingual lexicon extraction. In Pro-
ceedings of the 4th Workshop on Building and Using
Comparable Corpora, pages 19?26.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of the 8th NTCIR Workshop, pages 371?376.
Pascale Fung and Kenneth Ward Church. 1994. K-
vec: A New Approach for Aligning Parallel Texts.
In Proceedings of the 15th International Conference
on Computational Linguistics (COLING 1994), pages
1096?1102.
Pascale Fung and Kathleen McKeown. 1997. Finding
Terminology Translations from Non-parallel Corpora.
In Proceedings of the 5th Annual Workshop on Very
Large Corpora, pages 192?202.
Pascale Fung and Lo Yuen Yee. 1998. An IR Approach
for Translating New Words from Nonparallel, Compa-
rable Texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics, Volume 1, pages 414?420.
Pascale Fung. 1995. Compiling Bilingual Lexicon
Entries from a Non-Parallel English-Chinese Corpus.
In Proceedings of the 3rd Annual Workshop on Very
Large Corpora, pages 173?183.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving Translation Lexicon In-
duction from Monolingual Corpora via Dependency
Contexts and Part-of-Speech Equivalences. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009), pages
129?137.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve De?jean. 2004. A Geomet-
34
ric View on Bilingual Lexicon Extraction from Com-
parable Corpora. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguistics
(ACL 2004), pages 526?533.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning Bilingual Lexicons
from Monolingual Corpora. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2008): the Human Language
Technology Conference (HLT), pages 771?779.
Amir Hazem, Emmanuel Morin, and Sebastian Pen?a Sal-
darriaga. 2011. Bilingual Lexicon Extraction from
Comparable Corpora as Metasearch. In Proceedings
of the 4th Workshop on Building and Using Compara-
ble Corpora, pages 35?43.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005. Min-
ing Key Phrase Translations from Web Corpora. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing (HLT-EMNLP 2005), pages
483?490.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora us-
ing in-domain terms. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 481?489.
Hiroyuki Kaji. 2005. Extracting Translation Equivalents
from Bilingual Comparable Corpora. IEICE - Trans.
Inf. Syst., E88-D:313?323.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin. 2011.
Machine Transliteration Survey. ACM Computing
Surveys, 43(3):1?46.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics, 24:599?
612.
Philipp Koehn and Kevin Knight. 2002. Learning a
Translation Lexicon from Monolingual Corpora. In
Proceedings of ACL Workshop on Unsupervised Lexi-
cal Acquisition, pages 9?16.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2010), pages
145?153.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), pages
617?625.
Florian Laws, Lukas Michelbacher, Beate Dorow, Chris-
tian Scheible, Ulrich Heid, and Hinrich Schu?tze. 2010.
A Linguistically Grounded Graph Model for Bilingual
Lexicon Extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 614?622.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining Parenthetical Transla-
tions from the Web by Word Alignment. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL 2008): the Human
Language Technology Conference (HLT), pages 994?
1002.
Wen-Hsiang Lu, Lee-Feng Chien, and Hsi-Jian Lee.
2004. Anchor Text Mining for Translation of Web
Queries: A Transitive Translation Approach. ACM
Transactions on Information Systems, 22(2):242?269.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual Lexicon Extraction from Comparable Cor-
pora Enhanced with Parallel Corpora. In Proceedings
of the 4th Workshop on Building and Using Compara-
ble Corpora, pages 27?34.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining -
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics (ACL 2007), pages
664?671.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005.
Word Sense Disambiguation Using Label Propagation
Based Semi-Supervised Learning. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL 2005), pages 395?402.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29:19?51.
Pablo Gamallo Otero and Jose? Ramom Pichel Campos.
2008. Learning Spanish-Galician Translation Equiva-
lents Using a Comparable Corpus and a Bilingual Dic-
tionary. In Proceedings of the 9th International Con-
ference on Computational Linguistics and Intelligent
Text Processing (CICLing 2008), pages 423?433.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding Translations for Low-
Frequency Words in Comparable Corpora. Machine
Translation, 20:247?266.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compara-
ble Documents. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT2011),
pages 1327?1335.
Reinhard Rapp. 1995. Identifying Word Translations in
Non-Parallel Texts. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL 1995), pages 320?322.
35
Reinhard Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German Cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL
1999), pages 519?526.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Li Shao and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING 2004), pages 618?624.
Partha Pratim Talukdar and Koby Crammer. 2009. New
Regularized Algorithms for Transductive Learning. In
Proceedings of the European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML-PKDD 2009), pages
442?457.
Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.
2011. Identifying Word Translations from Compara-
ble Corpora Using Latent Topic Models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2011), pages 479?484.
Dekai Wu and Xuanyin Xia. 1994. Learning an English-
Chinese Lexicon from a Parallel Corpus. In Proceed-
ings of the First Conference of the Association for Ma-
chine Translation in the Americas (AMTA 1994), pages
206?213.
Ying Zhang and Phil Vines. 2004. Using the Web for
Automated Translation Extraction in Cross-Language
Information Retrieval. In Proceedings of the 27th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 162?169.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from Labeled and Unlabeled Data with Label Propa-
gation. Technical report, CMU-CALD-02-107.
36
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155?165,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Distortion Model Considering Rich Context
for Statistical Machine Translation
Isao Goto?,? Masao Utiyama? Eiichiro Sumita?
Akihiro Tamura? Sadao Kurohashi?
?National Institute of Information and Communications Technology
?Kyoto University
goto.i-es@nhk.or.jp
{mutiyama, eiichiro.sumita, akihiro.tamura}@nict.go.jp
kuro@i.kyoto-u.ac.jp
Abstract
This paper proposes new distortion mod-
els for phrase-based SMT. In decoding, a
distortion model estimates the source word
position to be translated next (NP) given
the last translated source word position
(CP). We propose a distortion model that
can consider the word at the CP, a word
at an NP candidate, and the context of the
CP and the NP candidate simultaneously.
Moreover, we propose a further improved
model that considers richer context by dis-
criminating label sequences that specify
spans from the CP to NP candidates. It
enables our model to learn the effect of
relative word order among NP candidates
as well as to learn the effect of distances
from the training data. In our experiments,
our model improved 2.9 BLEU points for
Japanese-English and 2.6 BLEU points for
Chinese-English translation compared to
the lexical reordering models.
1 Introduction
Estimating appropriate word order in a target lan-
guage is one of the most difficult problems for
statistical machine translation (SMT). This is par-
ticularly true when translating between languages
with widely different word orders.
To address this problem, there has been a lot
of research done into word reordering: lexical
reordering model (Tillman, 2004), which is one
of the distortion models, reordering constraint
(Zens et al, 2004), pre-ordering (Xia and Mc-
Cord, 2004), hierarchical phrase-based SMT (Chi-
ang, 2007), and syntax-based SMT (Yamada and
Knight, 2001).
In general, source language syntax is useful for
handling long distance word reordering. However,
obtaining syntax requires a syntactic parser, which
is not available for many languages. Phrase-based
SMT (Koehn et al, 2007) is a widely used SMT
method that does not use a parser.
Phrase-based SMT mainly1 estimates word re-
ordering using distortion models2. Therefore, dis-
tortion models are one of the most important com-
ponents for phrase-based SMT. On the other hand,
there are methods other than distortion models for
improving word reordering for phrase-based SMT,
such as pre-ordering or reordering constraints.
However, these methods also use distortion mod-
els when translating by phrase-based SMT. There-
fore, distortion models do not compete against
these methods and are commonly used with them.
If there is a good distortion model, it will improve
the translation quality of phrase-based SMT and
benefit to the methods using distortion models.
In this paper, we propose two distortion mod-
els for phrase-based SMT. In decoding, a distor-
tion model estimates the source word position to
be translated next (NP) given the last translated
source word position (CP). The proposed models
are the pair model and the sequence model. The
pair model utilizes the word at the CP, a word at
an NP candidate site, and the words surrounding
the CP and the NP candidates (context) simultane-
ously. In addition, the sequence model, which is
the further improved model, considers richer con-
text by identifying the label sequence that spec-
ify the span from the CP to the NP. It enables
our model to learn the effect of relative word or-
der among NP candidates as well as to learn the
effect of distances from the training data. Our
model learns the preference relations among NP
1A language model also supports the estimation.
2In this paper, reordering models for phrase-based SMT,
which are intended to estimate the source word position to
be translated next in decoding, are called distortion models.
This estimation is used to produce a hypothesis in the target
language word order sequentially from left to right.
155
kinou  kare  wa  pari  de  hon  wo  katta
he   bought   books   in   Paris   yesterday
Source:
Target:
Figure 1: An example of left-to-right translation
for Japanese-English. Boxes represent phrases
and arrows indicate the translation order of the
phrases.
candidates. Our model consists of one probabilis-
tic model and does not require a parser. Exper-
iments confirmed the effectiveness of our method
for Japanese-English and Chinese-English transla-
tion, using NTCIR-9 Patent Machine Translation
Task data sets (Goto et al, 2011).
2 Distortion Model for Phrase-Based
SMT
A Moses-style phrase-based SMT generates target
hypotheses sequentially from left to right. There-
fore, the role of the distortion model is to esti-
mate the source phrase position to be translated
next whose target side phrase will be located im-
mediately to the right of the already generated hy-
potheses. An example is shown in Figure 1. In
Figure 1, we assume that only the kare wa (En-
glish side: ?he?) has been translated. The target
word to be generated next will be ?bought? and the
source word to be selected next will be its corre-
sponding Japanese word katta. Thus, a distortion
model should estimate phrases including katta as
a source phrase position to be translated next.
To explain the distortion model task in more de-
tail, we need to redefine more precisely two terms,
the current position (CP) and next position (NP) in
the source sentence. CP is the source sentence po-
sition corresponding to the rightmost aligned tar-
get word in the generated target word sequence.
NP is the source sentence position corresponding
to the leftmost aligned target word in the target
phrase to be generated next. The task of the distor-
tion model is to estimate the NP3 from NP candi-
dates (NPCs) for each CP in the source sentence.4
3NP is not always one position, because there may be mul-
tiple correct hypotheses.
4This definition is slightly different from that of existing
methods such as Moses and (Green et al, 2010). In existing
methods, CP is the rightmost position of the last translated
source phrase and NP is the leftmost position of the source
phrase to be translated next. Note that existing methods do
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 hon
6
 wo
7
 katta
8
he   bought   books   in   Paris   yesterday
(a)
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 ni
6
 satsu
7
 hon
8
 wo
9
 katta
10
he   bought   two   books   in   Paris   yesterday
(b)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 karita
6
 ga
7
 kanojo
8
 wa
9
 katta
10
he   borrowed   books  yesterday  but  she  bought
(c)
kinou
1
 kare
2
 wa
3
 kanojo
4
 ga
5
 katta
6
 hon
7
 wo
8
 karita
9
yesterday  he  borrowed  the  books  that  she  bought
(e)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 katta
6
 ga
7
 kanojo
8
 wa
9
 karita
10
he   bought   books   yesterday   but   she   borrowed
(d)
??
?~
??
??
?~
??
?~
???~
CP NP
Figure 2: Examples of CP and NP for Japanese-
English translation. The upper sentence is the
source sentence and the sentence underneath is a
target hypothesis for each example. The NP is in
bold, and the CP is in bold italics. The point of an
arrow with a ? mark indicates a wrong NP candi-
date.
Estimating NP is a difficult task. Figure 2 shows
some examples. The superscript numbers indicate
the word position in the source sentence.
In Figure 2 (a), the NP is 8. However, in Fig-
ure 2 (b), the word (kare) at the CP is the same as
(a), but the NP is different (the NP is 10). From
these examples, we see that distance is not the es-
sential factor in deciding an NP. And it also turns
out that the word at the CP alone is not enough to
estimate the NP. Thus, not only the word at the CP
but also the word at a NP candidate (NPC) should
be considered simultaneously.
In (c) and (d) in Figure 2, the word (kare) at the
CP is the same and karita (borrowed) and katta
(bought) are at the NPCs. Karita is the word at
the NP and katta is not the word at the NP for
(c), while katta is the word at the NP and karita
is not the word at the NP for (d). From these ex-
amples, considering what the word is at the NP
not consider word-level correspondences.
156
is not enough to estimate the NP. One of the rea-
sons for this difference is the relative word order
between words. Thus, considering relative word
order is important.
In (d) and (e) in Figure 2, the word (kare) at the
CP and the word order between katta and karita
are the same. However, the word at the NP for
(d) and the word at the NP for (e) are different.
From these examples, we can see that selecting
a nearby word is not always correct. The differ-
ence is caused by the words surrounding the NPCs
(context), the CP context, and the words between
the CP and the NPC. Thus, these should be con-
sidered when estimating the NP.
In summary, in order to estimate the NP, the fol-
lowing should be considered simultaneously: the
word at the NP, the word at the CP, the relative
word order among the NPCs, the words surround-
ing NP and CP (context), and the words between
the CP and the NPC.
There are distortion models that do not require
a parser for phrase-based SMT. The linear dis-
tortion cost model used in Moses (Koehn et al,
2007), whose costs are linearly proportional to
the reordering distance, always gives a high cost
to long distance reordering, even if the reorder-
ing is correct. The MSD lexical reordering model
(Tillman, 2004; Koehn et al, 2005; Galley and
Manning, 2008) only calculates probabilities for
the three kinds of phrase reorderings (monotone,
swap, and discontinuous), and does not consider
relative word order or words between the CP and
the NPC. Thus, these models are not sufficient for
long distance word reordering.
Al-Onaizan and Papineni (2006) proposed a
distortion model that used the word at the CP and
the word at an NPC. However, their model did not
use context, relative word order, or words between
the CP and the NPC.
Ni et al (2009) proposed a method that adjusts
the linear distortion cost using the word at the CP
and its context. Their model does not simultane-
ously consider both the word specified at the CP
and the word specified at the NPCs.
Green et al (2010) proposed distortion mod-
els that used context. Their model (the outbound
model) estimates how far the NP should be from
the CP using the word at the CP and its con-
text.5 Their model does not simultaneously con-
5They also proposed another model (the inbound model)
sider both the word specified at the CP and the
word specified at an NPC. For example, the out-
bound model considers the word specified at the
CP, but does not consider the word specified at an
NPC. Their models also do not consider relative
word order.
In contrast, our distortion model solves the
aforementioned problems. Our distortion models
utilize the word specified at the CP, the word spec-
ified at an NPC, and also the context of the CP
and the NPC simultaneously. Furthermore, our se-
quence model considers richer context including
the relative word order among NPCs and also in-
cluding all the words between the CP and the NPC.
In addition, unlike previous methods, our models
learn the preference relations among NPCs.
3 Proposed Method
In this section, we first define our distortion model
and explain our learning strategy. Then, we de-
scribe two proposed models: the pair model and
the sequence model that is the further improved
model.
3.1 Distortion Model and Learning Strategy
First, we define our distortion model. Let i be a
CP, j be an NPC, S be a source sentence, andX be
the random variable of the NP. In this paper, dis-
tortion probability is defined as P (X = j|i, S),
which is the probability of an NPC j being the NP.
Our distortion model is defined as the model cal-
culating the distortion probability.
Next, we explain the learning strategy for our
distortion model. We train this model as a dis-
criminative model that discriminates the NP from
NPCs. Let J be a set of word positions in S other
than i. We train the distortion model subject to
?
j?J
P (X = j|i, S) = 1.
The model parameters are learned to maximize the
distortion probability of the NP among all of the
NPCs J in each source sentence. This learning
strategy is a kind of preference relation learning
(Evgniou and Pontil, 2002). In this learning, the
that estimates reverse direction distance. Each NPC is re-
garded as an NP, and the inbound model estimates how far
the corresponding CP should be from the NP using the word
at the NP and its context.
157
distortion probability of the actual NP will be rel-
atively higher than those of all the other NPCs J .
This learning strategy is different from that of
(Al-Onaizan and Papineni, 2006; Green et al,
2010). For example, Green et al (2010) trained
their outbound model subject to ?c?C P (Y =
c|i, S) = 1, where C is the set of the nine distor-
tion classes6 and Y is the random variable of the
correct distortion class that the correct distortion is
classified into. Distortion is defined as j ? i ? 1.
Namely, the model probabilities that they learned
were the probabilities of distortion classes in all
of the training data, not the relative preferences
among the NPCs in each source sentence.
3.2 Pair Model
The pair model utilizes the word at the CP, the
word at an NPC, and the context of the CP and the
NPC simultaneously to estimate the NP. This can
be done by our distortion model definition and the
learning strategy described in the previous section.
In this work, we use the maximum entropy
method (Berger et al, 1996) as a discriminative
machine learning method. The reason for this
is that a model based on the maximum entropy
method can calculate probabilities. However, if
we use scores as an approximation of the distor-
tion probabilities, various discriminative machine
learning methods can be applied to build the dis-
tortion model.
Let s be a source word and sn1 = s1s2...sn be
a source sentence. We add a beginning of sen-
tence (BOS) marker to the head of the source sen-
tence and an end of sentence (EOS) marker to the
end, so the source sentence S is expressed as sn+10
(s0 = BOS, sn+1 = EOS). Our distortion model
calculates the distortion probability for an NPC
j ? {j|1 ? j ? n + 1 ? j ?= i} for each CP
i ? {i|0 ? i ? n}
P (X = j|i, S) = 1Zi
exp
(
wTf (i, j, S, o, d)
)
(1)
where
o =
{
0 (i < j)
1 (i > j) , d =
?
??
??
0 (|j ? i| = 1)
1 (2 ? |j ? i| ? 5)
2 (6 ? |j ? i|)
,
6(??,?8], [?7,?5], [?4,?3], ?2, 0, 1, [2, 3], [4, 6],
and [7,?). In (Green et al, 2010), ?1 was used as one of
distortion classes. However, ?1 represents the CP in our def-
inition, and CP is not an NPC. Thus, we shifted all of the
distortion classes for negative distortions by ?1.
Template
?o?, ?o, sp?1, ?o, ti?, ?o, tj?, ?o, d?, ?o, sp, sq?2,
?o, ti, tj?, ?o, ti?1, ti, tj?, ?o, ti, ti+1, tj?,
?o, ti, tj?1, tj?, ?o, ti, tj , tj+1?, ?o, si, ti, tj?,
?o, sj , ti, tj?
1 p ? {p|i? 2 ? p ? i + 2 ? j ? 2 ? p ? j + 2}
2 (p, q) ? {(p, q)|i ? 2 ? p ? i + 2 ? j ? 2 ? q ?
j + 2 ? (|p? i| ? 1 ? |q ? j| ? 1)}
Table 1: Feature templates. t is the part of speech
of s.
w is a weight parameter vector, each element
of f(?) is a binary feature function, and Zi =?
j?{j|1?j?n+1 ? j ?=i}(numerator of Equation 1)
is a normalization factor. o is an orientation of i to
j and d is a distance class.
The binary feature function that constitutes an
element of f(?) returns 1 when its feature is
matched and if else, returns 0. Table 1 shows the
feature templates used to produce the features. A
feature is an instance of a feature template.
In Equation 1, i, j, and S are used by the feature
functions. Thus, Equation 1 can utilize features
consisting of both si, which is the word specified
at i, and sj , which is the word specified at j, or
both the context of i and the context of j simulta-
neously. Distance is considered using the distance
class d. Distortion is represented by distance and
orientation. The pair model considers distortion
using six joint classes of d and o.
3.3 Sequence Model
The pair model does not consider relative word or-
der among NPCs or all the words between the CP
and an NPC. In this section, we propose a further
improved model, the sequence model, which con-
siders richer context including relative word order
among NPCs and also including all the words be-
tween the CP and an NPC.
In (c) and (d) in Figure 2, karita (borrowed) and
katta (bought) occur in the source sentences. The
pair model considers the effect of distances using
only the distance class d. If these positions are
in the same distance class, the pair model cannot
consider the differences in distances. In this case,
these are conflict instances during training and it
is difficult to distinguish the NP for translation.
Now to explain how to consider the relative
word order by the sequence model. The sequence
model considers the relative word order by dis-
criminating the label sequence corresponding to
the NP from the label sequences corresponding to
158
Label Description
C A position is the CP.
I A position is a position between the CP
and the NPC.
N A position is the NPC.
Table 2: The ?C, I, and N? label set.
La
be
ls
eq
ue
nc
e
ID
1 N C
3 C N
4 C I N
5 C I I N
6 C I I I N
7 C I I I I N
8 C I I I I I N
9 C I I I I I I N
10 C I I I I I I I N
11 C I I I I I I I I N
B
O
S0
ki
no
u1
ka
re
2
w
a3
ho
n4
w
o5
ka
ri
ta
6
ga
7
ka
no
jo
8
w
a9
ka
tta
10
EO
S1
1
(y
es
te
rd
ay
)
(h
e)
(b
oo
k)
(b
or
ro
w
ed
)
(s
he
)
(b
ou
gh
t)
Source sentence
Figure 3: Example of label sequences that specify
spans from the CP to each NPC for the case of
Figure 2 (c). The labels (C, I, and N) in the boxes
are the label sequences.
each NPC in each sentence. Each label sequence
corresponds to one NPC. Therefore, if we identify
the label sequence that corresponds to the NP, we
can obtain the NP. The label sequences specify the
spans from the CP to each NPC using three kinds
of labels indicating the type of word positions in
the spans. The three kinds of labels, ?C, I, and N,?
are shown in Table 2. Figure 3 shows examples
of the label sequences for the case of Figure 2 (c).
In Figure 3, the label sequences are represented by
boxes and the elements of the sequences are labels.
The NPC is used as the label sequence ID for each
label sequence.
The label sequence can treat relative word or-
der. For example, the label sequence ID of 10 in
Figure 3 knows that karita exists to the left of the
NPC of 10. This is because karita6 carries a la-
bel I while katta10 carries a label N, and a position
with label I is defined as relatively closer to the CP
than a position with label N. By utilizing the label
sequence and corresponding words, the model can
reflect the effect of karita existing between the CP
and the NPC of 10 on the probability.
For the sequence model, karita (borrowed) and
katta (bought) in (c) and (d) in Figure 2 are not
conflict instances in training, whereas they are
conflict instances in training for the pair model.
The reason is as follows. In order to make the
probability of the NPC of 10 smaller than the NPC
of 6, instead of making the weight parameters for
the features with respect to the word at the position
of 10 with label N smaller than the weight param-
eters for the features with respect to the word at
the position of 6 with label N, the sequence model
can give negative weight parameters for the fea-
tures with respect to the word at the position of 6
with label I.
We use a sequence discrimination technique
based on CRF (Lafferty et al, 2001) to identify the
label sequence that corresponds to the NP. There
are two differences between our task and the CRF
task. One difference is that CRF discriminates la-
bel sequences that consist of labels from all of the
label candidates, whereas we constrain the label
sequences to sequences where the label at the CP
is C, the label at an NPC is N, and the labels be-
tween the CP and the NPC are I. The other dif-
ference is that CRF is designed for discriminat-
ing label sequences corresponding to the same ob-
ject sequence, whereas we do not assign labels to
words outside the spans from the CP to each NPC.
However, when we assume that another label such
as E has been assigned to the words outside the
spans and there are no features involving label E,
CRF with our label constraints can be applied to
our task. In this paper, the method designed to
discriminate label sequences corresponding to the
different word sequence lengths is called partial
CRF.
The sequence model based on partial CRF is de-
rived by extending the pair model. We introduce
the label l and extend the pair model to discrimi-
nating the label sequences. There are two exten-
sions to the pair model. One extension uses la-
bels. We suppose that label sequences specify the
spans from the CP to each NPC. We conjoined all
the feature templates in Table 1 with an additional
feature template ?li, lj? to include the labels into
features where li is the label corresponding to the
position of i. The other extension uses sequence.
In the pair model, the position pair of (i, j) is used
to derive features. In contrast, to descriminate la-
bel sequences in the sequence model, the position
pairs of (i, k), k ? {k|i < k ? j ? j ? k < i}
159
and (k, j), k ? {k|i ? k < j ? j < k ? i}
are used to derive features. Note that in the feature
templates in Table 1, i and j are used to specify
two positions. When features are used for the se-
quence model, one of the positions is regarded as
k.
The distortion probability for an NPC j being
the NP given a CP i and a source sentence S is
calculated as:
P (X = j|i, S) =
1
Zi
exp
( ?
k?M?{j}
wTf (i, k, S, o, d, li, lk)
+
?
k?M?{i}
wTf (k, j, S, o, d, lk, lj)
)
(2)
where
M =
{
{m|i < m < j} (i < j)
{m|j < m < i} (i > j)
and Zi = ?j?{j|1?j?n+1 ? j ?=i}(numerator of
Equation 2) is a normalization factor. Since j is
used as the label sequence ID, discriminating j
also means discriminating label sequence IDs.
The first term in exp(?) in Equation 2 considers
all of the word pairs located at i and other posi-
tions in the sequence, and also their context. The
second term in exp(?) in Equation 2 considers all
of the word pairs located at j and other positions
in the sequence, and also their context.
By designing our model to discriminate among
different length label sequences, our model can
naturally handle the effect of distances. Many fea-
tures are derived from a long label sequence be-
cause it will contain many labels between the CP
and the NPC. On the other hand, fewer features
are derived from a short label sequence because a
short label sequence will contain fewer labels be-
tween the CP and the NPC. The bias from these
differences provides important clues for learning
the effect of distances.7
7Note that the sequence model does not only consider
larger context than the pair model, but that it also considers
labels. The pair model does not discriminate labels, whereas
the sequence model uses label N and label I for the positions
except for the CP, depending on each situation. For example,
in Figure 3, at position 6, label N is used in the label sequence
ID of 6, but label I is used in the label sequence IDs of 7 to
11. Namely, even if they are at the same position, the labels
in the label sequences are different. The sequence model dis-
criminates the label differences.
BOS  kare  wa  pari  de  hon  wo  katta  EOS
BOS  he  bought  books  in  Paris  EOS
Source:
Target:
training data
Figure 4: Examples of supervised training data.
The lines represent word alignments. The English
side arrows point to the nearest word aligned on
the right.
3.4 Training Data for Discriminative
Distortion Model
To train our discriminative distortion model, su-
pervised training data is needed. The training data
is built from a parallel corpus and word alignments
between corresponding source words and target
words. Figure 4 shows examples of training data.
We select the target words aligned to the source
words sequentially from left to right (target side
arrows). Then, the order of the source words in
the target word order is decided (source side ar-
rows). The source sentence and the source side
arrows are the training data.
4 Experiment
In order to confirm the effects of our distortion
model, we conducted a series of Japanese to En-
glish (JE) and Chinese to English (CE) translation
experiments.8
4.1 Common Settings
We used the patent data for the Japanese to En-
glish and Chinese to English translation subtasks
from the NTCIR-9 Patent Machine Translation
Task (Goto et al, 2011). There were 2,000 sen-
tences for the test data and 2,000 sentences for the
development data.
Mecab9 was used for the Japanese morpholog-
ical analysis. The Stanford segmenter10 and tag-
ger11 were used for Chinese segmentation and
POS tagging. The translation model was trained
using sentences of 40 words or less from the train-
ing data. So approximately 2.05 million sen-
tence pairs consisting of approximately 54 million
8We conducted JE and CE translation as examples of
language pairs with different word orders and of languages
where there is a great need for translation into English.
9http://mecab.sourceforge.net/
10http://nlp.stanford.edu/software/segmenter.shtml
11http://nlp.stanford.edu/software/tagger.shtml
160
Japanese tokens whose lexicon size was 134k and
50 million English tokens whose lexicon size was
213k were used for JE. And approximately 0.49
million sentence pairs consisting of 14.9 million
Chinese tokens whose lexicon size was 169k and
16.3 million English tokens whose lexicon size
was 240k were used for CE. GIZA++ and grow-
diag-final-and heuristics were used to obtain word
alignments. In order to reduce word alignment er-
rors, we removed articles {a, an, the} in English
and particles {ga, wo, wa} in Japanese before per-
forming word alignments because these function
words do not correspond to any words in the other
languages. After word alignment, we restored the
removed words and shifted the word alignment po-
sitions to the original word positions. We used 5-
gram language models that were trained using the
English side of each set of bilingual training data.
We used an in-house standard phrase-based
SMT system compatible with the Moses decoder
(Koehn et al, 2007). The SMT weighting param-
eters were tuned by MERT (Och, 2003) using the
development data. To stabilize the MERT results,
we tuned three times by MERT using the first half
of the development data and we selected the SMT
weighting parameter set that performed the best on
the second half of the development data based on
the BLEU scores from the three SMT weighting
parameter sets.
We compared systems that used a common
SMT feature set from standard SMT features and
different distortion model features. The com-
mon SMT feature set consists of: four translation
model features, phrase penalty, word penalty, and
a language model feature. The compared different
distortion model features are: the linear distortion
cost model feature (LINEAR), the linear distortion
cost model feature and the six MSD bidirectional
lexical distortion model (Koehn et al, 2005) fea-
tures (LINEAR+LEX), the outbound and inbound
distortion model features discriminating nine dis-
tortion classes (Green et al, 2010) (9-CLASS), the
proposed pair model feature (PAIR), and the pro-
posed sequence model feature (SEQUENCE).
4.2 Training for the Proposed Models
Our distortion model was trained as follows: We
used 0.2 million sentence pairs and their word
alignments from the data used to build the trans-
lation model as the training data for our distortion
models. The features that were selected and used
were the ones that had been counted12, using the
feature templates in Table 1, at least four times
for all of the (i, j) position pairs in the training
sentences. We conjoined the features with three
types of label pairs ?C, I?, ?I,N?, or ?C,N? as in-
stances of the feature template ?li, lj? to produce
features for SEQUENCE. The L-BFGS method
(Liu and Nocedal, 1989) was used to estimate the
weight parameters of maximum entropy models.
The Gaussian prior (Chen and Rosenfeld, 1999)
was used for smoothing.
4.3 Training for the Compared Models
For 9-CLASS, we used the same training data as
for our distortion models. Let ti be the part of
speech of si. We used the following feature tem-
plates to produce features for the outbound model:
?si?2?, ?si?1?, ?si?, ?si+1?, ?si+2?, ?ti?, ?ti?1, ti?,
?ti, ti+1?, and ?si, ti?. These feature templates corre-
spond to the components of the feature templates
of our distortion models. In addition to these fea-
tures, we used a feature consisting of the relative
source sentence position as the feature used by
(Green et al, 2010). The relative source sentence
position is discretized into five bins, one for each
quintile of the sentence. For the inbound model13,
i of the feature templates was changed to j. Fea-
tures occurring four or more times in the train-
ing sentences were used. The maximum entropy
method with Gaussian prior smoothing was used
to estimate the model parameters.
The MSD bidirectional lexical distortion model
was built using all of the data used to build the
translation model.
4.4 Results and Discussion
We evaluated translation quality based on the case-
insensitive automatic evaluation score BLEU-4
(Papineni et al, 2002). We used distortion lim-
its of 10, 20, 30, and unlimited (?), which limited
the number of words for word reordering to a max-
imum number. Table 3 presents our main results.
The proposed SEQUENCE outperformed the base-
lines for both Japanese to English and Chinese to
English translation. This demonstrates the effec-
tiveness of the proposed SEQUENCE. The scores
of the proposed SEQUENCE were higher than those
12When we counted features for selection, we only counted
features that were from the feature templates of ?si, sj?,
?ti, tj?, ?si, ti, tj?, and ?sj , ti, tj? in Table 1 when j was not
the NP, in order to avoid increasing the number of features.
13The inbound model is explained in footnote 5.
161
Japanese-English Chinese-English
Distortion limit 10 20 30 ? 10 20 30 ?
LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33
LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.13
9-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84
PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32
SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41
Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers
indicate no significant difference from the best result in each language pair using the bootstrap resampling
test at a significance level ? = 0.01 (Koehn, 2004).
Japanese-English Chinese-English
HIER 30.47 32.66
Table 4: Evaluation results for hierarchical phrase-
based SMT.
of the proposed PAIR. This confirms the effective-
ness for considering relative word order and words
between the CP and an NPC. The proposed PAIR
outperformed 9-CLASS, confirming that consider-
ing both the word specified at the CP and the word
specified at the NPC simultaneously was more ef-
fective than that of 9-CLASS.
For translating between languages with widely
different word orders such as Japanese and En-
glish, a small distortion limit is undesirable be-
cause there are cases where correct translations
cannot be produced with a small distortion limit,
since the distortion limit prunes the search space
that does not meet the constraint. Therefore,
a large distortion limit is required to translate
correctly. For JE translation, our SEQUENCE
achieved significantly better results at distortion
limits of 20 and 30 than that at a distortion limit
of 10, while the baseline systems of LINEAR,
LINEAR+LEX, and 9-CLASS did not achieve this.
This indicate that SEQUENCE could treat long
distance reordering candidates more appropriately
than the compared methods.
We also tested hierarchical phrase-based SMT
(Chiang, 2007) (HIER) using the Moses imple-
mentation. The common data was used to train
HIER. We used unlimited max-chart-span for the
system setting. Results are given in Table 4. Our
SEQUENCE outperformed HIER. The gain for JE
was large but the gain for CE was modest. Since
phrase-based SMT is generally faster in decod-
ing speed than hierarchical phrase-based SMT,
achieving better or comparable scores is worth-
Distortion
P
r
o
b
a
b
i
l
i
t
y
Figure 5: Average probabilities for large distortion
for Japanese-English translation.
while.
To investigate the tolerance for sparsity of the
training data, we reduced the training data for
the sequence model to 20,000 sentences for JE
translation.14 SEQUENCE using this model with
a distortion limit of 30 achieved a BLEU score
of 32.22.15 Although the score is lower than the
score of SEQUENCE with a distortion limit of 30
in Table 3, the score was still higher than those
of LINEAR, LINEAR+LEX, and 9-CLASS for JE
in Table 3. This indicates that the sequence model
also works even when the training data is not large.
This is because the sequence model considers not
only the word at the CP and the word at an NPC
but also rich context, and rich context would be ef-
fective even for a smaller set of training data.
14We did not conduct experiments using larger training
data because there would have been a very high computa-
tional cost to build models using the L-BFGS method.
15To avoid effects from differences in the SMT weighting
parameters, we used the same SMT weighting parameters for
SEQUENCE, with a distortion limit of 30, in Table 3.
162
To investigate how well SEQUENCE learns the
effect of distance, we checked the average distor-
tion probabilities for large distortions of j ? i? 1.
Figure 5 shows three kinds of probabilities for dis-
tortions from 3 to 20 for Japanese-English transla-
tion. One is the average distortion probabilities
in the Japanese test sentences for each distortion
for SEQUENCE, and another is this for PAIR. The
third (CORPUS) is the probabilities for the actual
distortions in the training data that were obtained
from the word alignments used to build the trans-
lation model. The probability for a distortion for
CORPUS was calculated by the number of the dis-
tortion divided by the total number of distortions
in the training data.
Figure 5 shows that when a distance class fea-
ture used in the model was the same (e.g., distor-
tions from 5 to 20 were the same distance class
feature), PAIR produced average distortion prob-
abilities that were almost the same. In contrast,
the average distortion probabilities for SEQUENCE
decreased when the lengths of the distortions in-
creased, even if the distance class feature was
the same, and this behavior was the same as that
of CORPUS. This confirms that the proposed
SEQUENCE could learn the effect of distances ap-
propriately from the training data.16
5 Related Works
We discuss related works other than discussed in
Section 2. Xiong et al (2012) proposed a model
predicting the orientation of an argument with re-
spect to its verb using a parser. Syntactic struc-
tures and predicate-argument structures are useful
for reordering. However, orientations do not han-
dle distances. Thus, our distortion model does not
compete against the methods predicting orienta-
tions using a parser and would assist them if used
16We also checked the average distortion probabilities for
the 9-CLASS outbound model in the Japanese test sentences
for Japanese-English translation. We averaged the average
probabilities for distortions in a distortion span of [4, 6] and
also averaged those in a distortion span of [7, 20], where the
distortions in each span are in the same distortion class. The
average probability for [4, 6] was 0.058 and that for [7, 20]
was 0.165. From CORPUS, the average probabilities in the
training data for each distortion in [4, 6] were higher than
those for each distortion in [7, 20]. However, the converse
was true for the comparison between the two average prob-
abilities for the outbound model. This is because the sum
of probabilities for distortions from 7 and above was larger
than the sum of probabilities for distortions from 4 to 6 in the
training data. This comparison indicates that the 9-CLASS
outbound model could not appropriately learn the effects of
large distances for JE translation.
together.
There are word reordering constraint methods
using ITG (Wu, 1997) for phrase-based SMT
(Zens et al, 2004; Yamamoto et al, 2008; Feng et
al., 2010). These methods consider sentence level
consistency with respect to ITG. The ITG con-
straint does not consider distances of reordering
and was used with other distortion models. Our
distortion model does not consider sentence level
consistency, so our distortion model and ITG con-
straint methods are thought to be complementary.
There are tree-based SMT methods (Chiang,
2007; Galley et al, 2004; Liu et al, 2006). In
many cases, tree-based SMT methods do not use
the distortion models that consider reordering dis-
tance apart from translation rules because it is not
trivial to use distortion scores considering the dis-
tances for decoders that do not generate hypothe-
ses from left to right. If it could be applied to these
methods, our distortion model might contribute to
tree-based SMT methods. Investigating the effects
will be for future work.
6 Conclusion
This paper described our distortion models for
phrase-based SMT. Our sequence model simply
consists of only one probabilistic model, but it can
consider rich context. Experiments indicate that
our models achieved better performance and the
sequence model could learn the effect of distances
appropriately. Since our models do not require a
parser, they can be applied to many languages. Fu-
ture work includes application to other language
pairs, incorporation into ITG constraint methods
and other reordering methods, and application to
tree-based SMT methods.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical report.
163
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Theodoros Evgniou and Massimiliano Pontil. 2002.
Learning preference relations from data. Neural
Nets Lecture Notes in Computer Science, 2486:23?
32.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010.
An efficient shift-reduce decoding algorithm for
phrased-based machine translation. In Coling 2010:
Posters, pages 285?293, Beijing, China, August.
Coling 2010 Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273?280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9, pages 559?578.
Spence Green, Michel Galley, and Christopher D.Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 867?875, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of 18th International Conference on Machine Learn-
ing, pages 282?289.
D.C. Liu and J. Nocedal. 1989. On the limited memory
method for large scale optimization. Mathematical
Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorder-
ings for machine translation. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
241?244, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101?
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508?
514, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
164
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France, July. Association for Computational Lin-
guistics.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing constraints from the source
tree on ITG constraints for SMT. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
1?9, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of Coling 2004, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
165
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841?851,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Part-of-Speech Induction in Dependency Trees for Statistical Machine
Translation
Akihiro Tamura?,?, Taro Watanabe?, Eiichiro Sumita?,
Hiroya Takamura?, Manabu Okumura?
? National Institute of Information and Communications Technology
{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp
? Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura, oku}@pi.titech.ac.jp
Abstract
This paper proposes a nonparametric
Bayesian method for inducing Part-of-
Speech (POS) tags in dependency trees
to improve the performance of statistical
machine translation (SMT). In particular,
we extend the monolingual infinite tree
model (Finkel et al, 2007) to a bilin-
gual scenario: each hidden state (POS tag)
of a source-side dependency tree emits a
source word together with its aligned tar-
get word, either jointly (joint model), or
independently (independent model). Eval-
uations of Japanese-to-English translation
on the NTCIR-9 data show that our in-
duced Japanese POS tags for dependency
trees improve the performance of a forest-
to-string SMT system. Our independent
model gains over 1 point in BLEU by re-
solving the sparseness problem introduced
in the joint model.
1 Introduction
In recent years, syntax-based SMT has made
promising progress by employing either depen-
dency parsing (Lin, 2004; Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008; Mi and Liu,
2010) or constituency parsing (Huang et al, 2006;
Liu et al, 2006; Galley et al, 2006; Mi and Huang,
2008; Zhang et al, 2008; Cohn and Blunsom,
2009; Liu et al, 2009; Mi and Liu, 2010; Zhang
et al, 2011) on the source side, the target side,
or both. However, dependency parsing, which
is a popular choice for Japanese, can incorporate
only shallow syntactic information, i.e., POS tags,
compared with the richer syntactic phrasal cate-
gories in constituency parsing. Moreover, exist-
ing POS tagsets might not be optimal for SMT
because they are constructed without considering
the language in the other side. Consider the ex-
amples in Figure 1. The Japanese noun ???? in
? ? ?? ?? ? ??
??? ? ??????? ? ???? ??
You can not use the Internet  .
I  pay  usage fees  .
noun particle particlenoun noun verb auxiliary verb
noun particle noun noun verbparticle
[Example 1]
[Example 2]
Japanese POS:
Japanese POS:
Figure 1: Examples of Existing Japanese POS
Tags and Dependency Structures
Example 1 corresponds to the English verb ?use?,
while that in Example 2 corresponds to the English
noun ?usage?. Thus, Japanese nouns act like verbs
in English in one situation, and nouns in English
in another. If we could discriminate POS tags for
two cases, we might improve the performance of a
Japanese-to-English SMT system.
In the face of the above situations, this pa-
per proposes an unsupervised method for inducing
POS tags for SMT, and aims to improve the perfor-
mance of syntax-based SMT by utilizing the in-
duced POS tagset. The proposed method is based
on the infinite tree model proposed by Finkel et
al. (2007), which is a nonparametric Bayesian
method for inducing POS tags from syntactic de-
pendency structures. In this model, hidden states
represent POS tags, the observations they generate
represent the words themselves, and tree structures
represent syntactic dependencies between pairs of
POS tags.
The proposed method builds on this model by
incorporating the aligned words in the other lan-
guage into the observations. We investigate two
types of models: (i) a joint model and (ii) an in-
dependent model. In the joint model, each hid-
den state jointly emits both a source word and its
aligned target word as an observation. The in-
dependent model separately emits words in two
languages from hidden states. By inferring POS
841
tags based on bilingual observations, both mod-
els can induce POS tags by incorporating infor-
mation from the other language. Consider, for ex-
ample, inducing a POS tag for the Japanese word ?
??? in Figure 1. Under a monolingual induction
method (e.g., the infinite tree model), the ????
in Example 1 and 2 would both be assigned the
same POS tag since they share the same observa-
tion. However, our models would assign separate
tags for the two different instances since the ??
?? in Example 1 and Example 2 could be disam-
biguated by encoding the target-side information,
either ?use? or ?usage?, in the observations.
Inference is efficiently carried out by beam sam-
pling (Gael et al, 2008), which combines slice
sampling and dynamic programming. Experi-
ments are carried out on the NTCIR-9 Japanese-
to-English task using a binarized forest-to-string
SMT system with dependency trees as its source
side. Our bilingually-induced tagset signifi-
cantly outperforms the original tagset and the
monolingually-induced tagset. Further, our inde-
pendent model achieves a more than 1 point gain
in BLEU, which resolves the sparseness problem
introduced by the bi-word observations.
2 Related Work
A number of unsupervised methods have been
proposed for inducing POS tags. Early methods
have the problem that the number of possible POS
tags must be provided preliminarily. This limita-
tion has been overcome by automatically adjust-
ing the number of possible POS tags using non-
parametric Bayesian methods (Finkel et al, 2007;
Gael et al, 2009; Blunsom and Cohn, 2011; Sirts
and Aluma?e, 2012). Gael et al (2009) applied
infinite HMM (iHMM) (Beal et al, 2001; Teh
et al, 2006), a nonparametric version of HMM,
to POS induction. Blunsom and Cohn (2011)
used a hierarchical Pitman-Yor process prior to the
transition and emission distribution for sophisti-
cated smoothing. Sirts and Aluma?e (2012) built a
model that combines POS induction and morpho-
logical segmentation into a single learning prob-
lem. Finkel et al (2007) proposed the infinite
tree model, which represents recursive branching
structures over infinite hidden states and induces
POS tags from syntactic dependency structures. In
the following, we overview the infinite tree model,
which is the basis of our proposed model. In par-
ticular, we will describe the independent children
H ?k
?k? z1
z2 z3
x1 x2 x3k=1,?,C
Hk
k
~
),...,(Dirichlet~|? ???pi
Figure 2: A Graphical Representation of the Finite
Tree Model
model (Finkel et al, 2007), where children are
dependent only on their parents, used in our pro-
posed model1.
2.1 Finite Tree Model
We first review the finite tree model, which can
be graphically represented in Figure 2. Let
Tt denote the tree whose root node is t. A
node t has a hidden state zt (the POS tag)
and an observation xt (the word). The prob-
ability of a tree Tt, pT (Tt), is recursively de-
fined: pT (Tt) = p(xt|zt)
?
t??c(t)
p(zt? |zt)pT (Tt?),
where c(t) is the set of the children of t.
Let each hidden state variable have C possible
values indexed by k. For each state k, there is
a parameter ?k which parameterizes the observa-
tion distribution for that state: xt|zt ? F (?zt). ?k
is distributed according to a prior distribution H:
?k ? H .
Transitions between states are governed by
Markov dynamics parameterized by pi, where
?ij = p(zc(t) = j|zt = i) and pik are the transition
probabilities from the parent?s state k. pik is dis-
tributed according to a Dirichlet distribution with
parameter ?: pik|? ? Dirichlet(?, . . . , ?). The
hidden state of each child zt? is distributed accord-
ing to a multinomial distributionpizt specific to the
parent?s state zt: zt? |zt ? Multinomial(pizt).
2.2 Infinite Tree Model
In the infinite tree model, the number of possible
hidden states is potentially infinite. The infinite
model is formed by extending the finite tree model
using a hierarchical Dirichlet process (HDP) (Teh
et al, 2006). The reason for using an HDP rather
1Finkel et al (2007) originally proposed three types of
models: besides the independent children model, the simul-
taneous children model and the markov children model. Al-
though we could apply the other two models, we leave this
for future work.
842
H ?k
?k?0 z1
z2 z3
x1 x2 x3?
? ?
Hk
k
~
),(DP~,|
)(GEM~|
00? ????pi
???
Figure 3: A Graphical Representation of the Infi-
nite Tree Model
than a simple Dirichlet process (DP)2 (Ferguson,
1973) is that we have to introduce coupling across
transitions from different parent?s states. A similar
measure was adopted in iHMM (Beal et al, 2001).
HDP is a set of DPs coupled through a shared
random base measure which is itself drawn from
a DP: each Gk ? DP(?0, G0) with a shared base
measure G0, and G0 ? DP(?,H) with a global
base measure H . From the viewpoint of the stick-
breaking construction3 (Sethuraman, 1994), the
HDP is interpreted as follows: G0 =
??
k?=1
?k???k?
and Gk =
??
k?=1
?kk???k? , where ? ? GEM(?),
pik ? DP(?0,?), and ?k? ? H .
We regard each Gk as two coindexed distribu-
tions: pik, a distribution over the transition prob-
abilities from the parent?s state k, and ?k? , an ob-
servation distribution for the state k?. Then, the
infinite tree model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt).
Figure 3 shows the graphical representation of the
infinite tree model. The primary difference be-
2DP is a measure on measures. It has two parameters, a
scaling parameter ? and a base measure H: DP (?,H).
3Sethuraman (1994) showed a definition of a measure
G ? DP(?0, G0). First, infinite sequences of i.i.d variables
(??k)?k=1 and (?k)?k=1 are generated: ??k|?0 ? Beta(1, ?0),
?k ? G0. Then, G is defined as: ?k = ??k
?k?1
l=1 (1 ? ??l),
G = ??k=1 ?k??k . If pi is defined by this process, then we
write pi ? GEM(?0).
H ?k
?k?0
?
? ? z1
z2 z3
z4 z5 z6
???
+pay? ??????+fees? ???+usage???+I? ???
Figure 4: An Example of the Joint Model
tween Figure 2 and Figure 3 is whether the number
of copies of the state is finite or not.
3 Bilingual Infinite Tree Model
We propose a bilingual variant of the infinite tree
model, the bilingual infinite tree model, which uti-
lizes information from the other language. Specifi-
cally, the proposed model introduces bilingual ob-
servations by embedding the aligned target words
in the source-side dependency trees. This paper
proposes two types of models that differ in their
processes for generating observations: the joint
model and the independent model.
3.1 Joint Model
The joint model is a simple application of the in-
finite tree model under a bilingual scenario. The
model is formally defined in the same way as in
Section 2.2 and is graphically represented simi-
larly to Figure 3. The only difference from the
infinite tree model is the instances of observations
(xt). Observations in the joint model are the com-
bination of source words and their aligned target
words4, while observations in the monolingual in-
finite tree model represent only source words. For
each source word, all the aligned target words are
copied and sorted in alphabetical order, and then
concatenated into a single observation. Therefore,
a single target word may be emitted multiple times
if the target word is aligned with multiple source
words. Likewise, there may be target words which
may not be emitted by our model, if the target
words are not aligned.
Figure 4 shows the process of generating Exam-
ple 2 in Figure 1 through the joint model, where
aligned words are jointly emitted as observations.
In Figure 4, the POS tag of ???? (z5) generates
4When no target words are aligned, we simply add a
NULL target word.
843
H ?k
?k?0
? ? z1
z2 z3
z4 z5
H? ?'k
?? pay
I?
???
??
? NONE NONEusage
fees z6
?
'~',~
),(DP~,|
)(GEM~|
00 HH kk
k ?? ????pi
???
Figure 5: A Graphical Representation of the Inde-
pendent Model
the string ???+usage? as the observation (x5).
Similarly, the POS tag of ???? in Example 1
would generate the string ???+use?. Hence, this
model can assign different POS tags to the two dif-
ferent instances of the word ????, based on the
different observation distributions in inference.
3.2 Independent Model
The joint model is prone to a data sparseness prob-
lem, since each observation is a combination of a
source word and its aligned target word. Thus, we
propose an independent model, where each hidden
state generates a source word and its aligned target
word separately. For the aligned target side, we in-
troduce an observation variable x?t for each zt and
a parameter ??k for each state k, which parame-
terizes a distinct distribution over the observations
x?t for that state. ??k is distributed according to a
prior distribution H ?. Specifically, the indepen-
dent model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H, ??k ? H ?,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt), x?t|zt ? F ?(??zt).
When multiple target words are aligned to a single
source word, each aligned word is generated sepa-
rately from observation distribution parameterized
by ??k.
Figure 5 graphs the process of generating Ex-
ample 2 in Figure 1 using the independent model.
x?t and ??k are introduced for aligned target words.
The state of ???? (z5) generates the Japanese
word ???? as x5 and the English word ?usage?
as x?5. Due to this factorization, the independent
model is less subject to the sparseness problem.
3.3 Introduction of Other Factors
We assumed the surface form of aligned target
words as additional observations in previous sec-
tions. Here, we introduce additional factors, i.e.,
the POS of aligned target words, in the observa-
tions. Note that POSs of target words are assigned
by a POS tagger in the target language and are not
inferred in the proposed model.
First, we can simply replace surface forms of
target words with their POSs to overcome the
sparseness problem. Second, we can incorporate
both information from the target language as ob-
servations. In the joint model, two pieces of in-
formation are concatenated into a single observa-
tion. In the independent model, we introduce ob-
servation variables (e.g., x?t and x??t ) and parame-
ters (e.g., ??k and ???k) for each information. Specif-
ically, x?t and ??k are introduced for the surface
form of aligned words, and x??t and ???k for the POS
of aligned words. Consider, for example, Example
1 in Figure 1. The POS tag of ???? generates the
string ???+use+verb? as the observation in the
joint model, while it generates ????, ?use?, and
?verb? independently in the independent model.
3.4 POS Refinement
We have assumed a completely unsupervised way
of inducing POS tags in dependency trees. An-
other realistic scenario is to refine the existing POS
tags (Finkel et al, 2007; Liang et al, 2007) so
that each refined sub-POS tag may reflect the in-
formation from the aligned words while preserv-
ing the handcrafted distinction from original POS
tagset. Major difference is that we introduce sep-
arate transition probabilities pisk and observation
distributions (?sk, ?
?s
k ) for each existing POS tag s.
Then, each node t is constrained to follow the dis-
tributions indicated by the initially assigned POS
tag st, and we use the pair (st, zt) as a state repre-
sentation.
3.5 Inference
In inference, we find the state set that maximizes
the posterior probability of state transitions given
observations (i.e., P (z1:n|x1:n)). However, we
cannot evaluate the probability for all possible
states because the number of states is infinite.
Finkel et al (2007) presented a sampling algo-
rithm for the infinite tree model, which is based on
the Gibbs sampling in the direct assignment rep-
resentation for iHMM (Teh et al, 2006). In the
844
Gibbs sampling, individual hidden state variables
are resampled conditioned on all other variables.
Unfortunately, its convergence is slow in HMM
settings because sequential data is likely to have
a strong correlation between hidden states (Gael
et al, 2008).
We present an inference procedure based on
beam sampling (Gael et al, 2008) for the joint
model and the independent model. Beam sam-
pling limits the number of possible state transi-
tions for each node to a finite number using slice
sampling (Neal, 2003), and then efficiently sam-
ples whole hidden state transitions using dynamic
programming. Beam sampling does not suffer
from slow convergence as in Gibbs sampling by
sampling the whole state variables at once. In ad-
dition, Gael et al (2008) showed that beam sam-
pling is more robust to initialization and hyperpa-
rameter choice than Gibbs sampling.
Specifically, we introduce an auxiliary variable
ut for each node in a dependency tree to limit
the number of possible transitions. Our procedure
alternates between sampling each of the follow-
ing variables: the auxiliary variables u, the state
assignments z, the transition probabilities pi, the
shared DP parameters ?, and the hyperparameters
?0 and ?. We can parallelize procedures in sam-
pling u and z because the slice sampling for u and
the dynamic programing for z are independent for
each sentence. See Gael el al. (2009) for details.
The only difference between inferences in the
joint model and the independent model is in com-
puting the posterior probability of state transi-
tions given observations (e.g., p(z1:n|x1:n) and
p(z1:n|x1:n, x?1:n)) in sampling z. In the follow-
ing, we describe each sampling stage. See Teh et
al., (2006) for details of sampling pi, ?, ?0 and ?.
Sampling u:
Each ut is sampled from the uniform distribu-
tion on [0, ?zd(t)zt ], where d(t) is the parent of
t: ut ? Uniform(0, ?zd(t)zt). Note that ut is a
positive number, since each transition probability
?zd(t)zt is larger than zero.
Sampling z:
Possible values k of zt are divided into the two
sets using ut: a finite set with ?zd(t)k > ut and
an infinite set with ?zd(t)k ? ut. The beam
sampling considers only the former set. Owing
to the truncation of the latter set, we can compute
the posterior probability of a state zt given ob-
servations for all t (t = 1, . . . , T ) using dynamic
programming as follows:
In the joint model, p(zt|x?(t), u?(t)) ?
p(xt|zt) ?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), u?(d(t))),
and in the independent model,
p(zt|x?(t), x??(t), u?(t)) ? p(xt|zt) ? p(x?t|zt)
?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), x??(d(t)), u?(d(t))),
where x?(t) (or u?(t)) denotes the set of xt (or ut)
on the path from the root node to the node t in a
tree.
In our experiments, we assume that F (?k)
is Multinomial(?k) and H is Dirichlet(?, . . . , ?),
which is the same in Finkel et al (2007). Un-
der this assumption, the posterior probability of an
observation is as follows: p(xt|zt) =
n?xtk + ?
n??k + N?
,
where n?xk is the number of observations x with
state k, n??k is the number of hidden states whose
values are k, and N is the total number of observa-
tions x. Similarly, p(x?t|zt) =
n?x?tk + ?
?
n??k + N ???
, where
N ? is the total number of observations x?.
When the posterior probability of a state zt
given observations for all t can be computed,
we first sample the state of each leaf node and
then perform backtrack sampling for every other
zt where the zt is sampled given the sample
for zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ?
p(zt|x?(t), u?(t))
?
t??c(t) p(zt? |zt, ut?).
Sampling pi:
We introduce a count variable nij ? n,
which is the number of observations with
state j whose parent?s state is i. Then,
we sample pi using the Dirichlet distri-
bution: (?k1, . . . , ?kK ,
??
k?=K+1 ?kk?) ?
Dirichlet(nk1 + ?0?1, . . . , nkK +
?0?K , ?0
??
k?=K+1 ?k?), where K is the
number of distinct states in z.
Sampling ?:
We introduce a set of auxiliary variables m, where
mij ? m is the number of elements of pij
corresponding to ?i. The conditional distribu-
tion of each variable is p(mij = m|z,?, ?0) ?
S(nij ,m)(?0?j)m, where S(n,m) are unsigned
Stirling numbers of the first kind5.
5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0,
S(n,m) = 0 for m > n, and S(n + 1,m) = S(n,m ?
1) + nS(n,m) for others.
845
The parameters ? are sampled using the Dirich-
let distribution: (?1, . . . , ?K ,
??
k?=K+1 ?k?) ?
Dirichlet(m?1, . . . ,m?K , ?), where m?k =?K
k?=1 mk?k.
Sampling ?0:
?0 is parameterized by a gamma hyperprior
with hyperparameters ?a and ?b. We introduce
two types of auxiliary variables for each state
(k = 1, . . . ,K), wk ? [0, 1] and vk ? {0, 1}.
The conditional distribution of each wk is
p(wk|?0) ? w?0k (1?wk)n?k?1 and that of each vk
is p(vk|?0) ? (
n?k
?0
)
vk
, where n?k =
?K
k?=1 nk?k.
The conditional distribution of ?0 given wk
and vk (k = 1, . . . ,K) is p(?0|w,v) ?
??a?1+m..?
?K
k=1 vk
0 e??0(?b?
?K
k=1 logwk), where
m?? =
?K
k?=1
?K
k??=1 mk?k?? .
Sampling ?:
? is parameterized by a gamma hyperprior with
hyperparameters ?a and ?b. We introduce an
auxiliary variable ?, whose conditional distribu-
tion is p(?|?) ? ??(1 ? ?)m???1. The con-
ditional distribution of ? given ? is p(?|?) ?
??a?1+Ke??(?b?log?).
4 Experiment
We tested our proposed models under the
NTCIR-9 Japanese-to-English patent translation
task (Goto et al, 2011), consisting of approxi-
mately 3.2 million bilingual sentences. Both the
development data and the test data consist of 2,000
sentences. We also used the NTCIR-7 develop-
ment data consisting of 2,741 sentences for devel-
opment testing purposes.
4.1 Experimental Setup
We evaluated our bilingual infinite tree model
for POS induction using an in-house developed
syntax-based forest-to-string SMT system. In
the training process, the following steps are per-
formed sequentially: preprocessing, inducing a
POS tagset for a source language, training a POS
tagger and a dependency parser, and training a
forest-to-string MT model.
Step 1. Preprocessing
We used the first 10,000 Japanese-English sen-
tence pairs in the NTCIR-9 training data for in-
ducing a POS tagset for Japanese6. The Japanese
sentences were segmented using MeCab7, and the
English sentences were tokenized and POS tagged
using TreeTagger (Schmid, 1994), where 43 and
58 types of POS tags are included in the Japanese
sentences and the English sentences, respectively.
The Japanese POS tags come from the second-
level POS tags in the IPA POS tagset (Asahara and
Matsumoto, 2003) and the English POS tags are
derived from the Penn Treebank. Note that the
Japanese POS tags are used for initialization of
hidden states and the English POS tags are used
as observations emitted by hidden states.
Word-by-word alignments for the sentence
pairs are produced by first running GIZA++ (Och
and Ney, 2003) in both directions and then com-
bining the alignments using the ?grow-diag-final-
and? heuristic (Koehn et al, 2003). Note that we
ran GIZA++ on all of the NTCIR-9 training data
in order to obtain better alignements.
The Japanese sentences are parsed using
CaboCha (Kudo and Matsumoto, 2002), which
generates dependency structures using a phrasal
unit called a bunsetsu8, rather than a word unit as
in English or Chinese dependency parsing. Since
we focus on the word-level POS induction, each
bunsetsu-based dependency tree is converted into
its corresponding word-based dependency tree us-
ing the following heuristic9: first, the last func-
tion word inside each bunsetsu is identified as
the head word10; then, the remaining words are
treated as dependents of the head word in the same
bunsetsu; finally, a bunsetsu-based dependency
structure is transformed to a word-based depen-
dency structure by preserving the head/modifier
relationships of the determined head words.
Step 2. POS Induction
A POS tag for each word in the Japanese sentences
is inferred by our bilingual infinite tree model, ei-
6Due to the high computational cost, we did not use all
the NTCIR-9 training data. We leave scaling up to a larger
dataset for future work.
7http://mecab.googlecode.com/svn/
trunk/mecab/doc/index.html
8A bunsetsu is the smallest meaningful sequence con-
sisting of a content word and accompanying function words
(e.g., a noun and a particle).
9We could use other word-based dependency trees such
as trees by the infinite PCFG model (Liang et al, 2007)
and syntactic-head or semantic-head dependency trees in
Nakazawa and Kurohashi (2012), although it is not our major
focus. We leave this for future work.
10If no function words exist in a bunsetsu, the last content
word is treated as the head word.
846
ther jointly (Joint) or independently (Ind). We
also performed monolingual induction of Finkel et
al. (2007) for comparison (Mono). In each model,
a sequence of sampling u, z, pi, ?, ?0, and ? is
repeated 10,000 times. In sampling ?0 and ?, hy-
perparameters ?a, ?b, ?a, and ?b are set to 2, 1,
1, and 1, respectively, which is the same setting in
Gael et al (2008). In sampling z, parameters ?, ??,
. . ., are set to 0.01. In the experiments, three types
of factors for the aligned English words are com-
pared: surface forms (?s?), POS tags (?P?), and the
combination of both (?s+P?). Further, two types of
inference frameworks are compared: induction
(IND) and refinement (REF ). In both frame-
works, each hidden state zt is first initialized to
the POS tags assigned by MeCab (the IPA POS
tagset), and then each state is updated through
the inference procedure described in Section 3.5.
Note that in REF , the sampling distribution over
zt is constrained to include only states that are a
refinement of the initially assigned POS tag.
Step 3. Training a POS Tagger and a
Dependency Parser
In this step, we train a Japanese dependency parser
from the 10,000 Japanese dependency trees with
the induced POS tags which are derived from Step
2. We employed a transition-based dependency
parser which can jointly learn POS tagging and
dependency parsing (Hatori et al, 2011) under an
incremental framework11. Note that the learned
parser can identify dependencies between words
and attach an induced POS tag for each word.
Step 4. Training a Forest-to-String MT
In this step, we train a forest-to-string MT model
based on the learned dependency parser in Step 3.
We use an in-house developed hypergraph-based
toolkit, cicada, for training and decoding with a
tree-to-string model, which has been successfully
employed in our previous work for system com-
bination (Watanabe and Sumita, 2011) and online
learning (Watanabe, 2012). All the Japanese and
English sentences in the NTCIR-9 training data
are segmented in the same way as in Step 1, and
then each Japanese sentence is parsed by the de-
pendency parser learned in Step 3, which simul-
taneously assigns induced POS tags and word de-
pendencies. Finally, a forest-to-string MT model
is learned with Zhang et al, (2011), which ex-
tracts translation rules by a forest-based variant of
11http://triplet.cc/software/corbit/
IND REF
BS 27.54
Mono 27.66 26.83
Joint[s] 28.00 28.00
Joint[P] 26.36 26.72
Joint[s+P] 27.99 27.82
Ind[s] 28.00 27.93
Ind[P] 28.11 28.63
Ind[s+P] 28.13 28.62
Table 1: Performance on Japanese-to-English
Translation Measured by BLEU (%)
the GHKM algorithm (Mi and Huang, 2008) af-
ter each parse tree is restructured into a binarized
packed forest. Parameters are tuned on the devel-
opment data using xBLEU (Rosti et al, 2011) as
an objective and L-BFGS (Liu and Nocedal, 1989)
as an optimization toolkit, since it is stable and less
prone to randomness, unlike MERT (Och, 2003)
or PRO (Hopkins and May, 2011). The develop-
ment test data is used to set up hyperparameters,
i.e., to terminate tuning iterations.
When translating Japanese sentences, a parse
tree for each sentence is constructed in the same
way as described earlier in this step, and then the
parse trees are translated into English sentences
using the learned forest-to-string MT model.
4.2 Experimental Results
Table 1 shows the performance for the test data
measured by case sensitive BLEU (Papineni et
al., 2002). We also present the performance of
our baseline forest-to-string MT system (BS) us-
ing the original IPA POS tags. In Table 1, num-
bers in bold indicate that the systems outperform
the baselines, BS and Mono. Under the Moses
phrase-based SMT system (Koehn et al, 2007)
with the default settings, we achieved a 26.80%
BLEU score.
Table 1 shows that the proposed systems outper-
form the baseline Mono. The differences between
the performance of Ind[s+P] and Mono are statis-
tically significant in the bootstrap method (Koehn,
2004), with a 1% significance level both in IND
and REF . The results indicate that integrating the
aligned target-side information in POS induction
makes inferred tagsets more suitable for SMT.
Table 1 also shows that the independent model
is more effective for SMT than the joint model.
This means that sparseness is a severe problem in
847
Model IND REF
Joint[s+P] 164 620
Ind[s+P] 102 517
IPA POS tags 42
Table 2: The Number of POS Tags
POS induction when jointly encoding bilingual in-
formation into observations. Additionally, all the
systems using the independent model outperform
BS. The improvements are statistically significant
in the bootstrap method (Koehn, 2004), with a 1%
significance level. The results show that the pro-
posed models can generate more favorable POS
tagsets for SMT than an existing POS tagset.
In Table 1, REF s are at least comparable to, or
better than, INDs except for Mono. This shows
that REF achieves better performance by preserv-
ing the clues from the original POS tagset. How-
ever, REF may suffer sever overfitting problem
for Mono since no bilingual information was in-
corporated. Further, when the full-level IPA POS
tags12 were used in BS, the system achieved a
27.49% BLEU score, which is worse than the re-
sult using the second-level IPA POS tags. This
means that manual refinement without bilingual
information may also cause an overfitting problem
in MT.
5 Discussion
5.1 Comparison to the IPA POS Tagset
Table 2 shows the number of the IPA POS tags
used in the experiments and the POS tags induced
by the proposed models. This table shows that
each induced tagset contains more POS tags than
the IPA POS tagset. In the experimental data,
some of Japanese verbs correspond to genuine En-
glish verbs, some are nominalized, and others cor-
respond to English past participle verbs or present
participle verbs which modify other words. Re-
spective examples are ?I use a card.?, ?Using the
index is faster.?, and ?I explain using an exam-
ple.?, where all the underlined words correspond
to the same Japanese word, ????, whose IPA
POS tag is a verb. Ind[s+P] in REF generated
the POS tagset where the three types are assigned
to separate POS groups.
The Japanese particle ??? is sometimes at-
tached to nouns to give them adverb roles. For
12377 types of full-level IPA POS tags were included in our
experimental data.
Tagging Dependency
IND REF IND REF
Original 90.37 93.62
Mono 90.75 88.04 91.77 91.51
Joint[s] 89.08 86.73 91.55 91.14
Joint[P] 80.54 79.98 91.06 91.29
Joint[s+P] 87.56 84.92 91.31 91.10
Ind[s] 87.62 84.33 92.06 92.58
Ind[P] 90.21 88.50 92.85 93.03
Ind[s+P] 89.57 86.12 92.96 92.78
Table 3: Tagging and Dependency Accuracy (%)
example, ??? (mutual) ??? is translated as
the adverb ?mutually? in English. Other times,
it is attached to words to make them the objects
of verbs. For example, ?? (he) ??????
(give)? is translated as ?give him?. The POS tags
by Ind[s+P] in REF discriminated the two types.
These examples show that the proposed mod-
els can disambiguate POS tags that have different
functions in English, whereas the IPA POS tagset
treats them jointly. Thus, such discrimination im-
proves the performance of a forest-to-string SMT.
5.2 Impact of Tagging and Dependency
Accuracy
The performance of our methods depends not only
on the quality of the induced tag sets but also on
the performance of the dependency parser learned
in Step 3 of Section 4.1. We cannot directly eval-
uate the tagging accuracy of the parser trained
through Step 3 because we do not have any data
with induced POS tags other than the 10,000-
sentence data gained through Step 2. Thus we split
the 10,000 data into the first 9,000 data for train-
ing and the remaining 1,000 for testing, and then
a dependency parser was learned in the same way
as in Step 3.
Table 3 shows the results. Original is the per-
formance of the parser learned from the training
data with the original POS tagset. Note that the de-
pendency accuracies are measured on the automat-
ically parsed dependency trees, not on the syntac-
tically correct gold standard trees. Thus Original
achieved the best dependency accuracy.
In Table 3, the performance for our bilingually-
induced POSs, Joint and Ind, are lower than
Original and Mono. It seems performing pars-
ing and tagging with the bilingually-induced POS
tagset is too difficult when only monolingual in-
848
formation is available to the parser. However, our
bilingually-induced POSs, except for Joint[P ],
with the lower accuracies are more effective for
SMT than the monolingually-induced POSs and
the original POSs, as indicated in Table 1. The
tagging accuracies for Joint[P ] both in IND and
REF are significantly lower than the others, while
the dependency accuracies do not differ signifi-
cantly. The lower tagging accuracies may directly
reflect the lower translation qualities for Joint[P ]
in Table 1.
6 Conclusion
We proposed a novel method for inducing POS
tags for SMT. The proposed method is a non-
parametric Bayesian method, which infers hidden
states (i.e., POS tags) based on observations repre-
senting not only source words themselves but also
aligned target words. Our experiments showed
that a more favorable POS tagset can be induced
by integrating aligned information, and further-
more, the POS tagset generated by the proposed
method is more effective for SMT than an existing
POS tagset (the IPA POS tagset).
Even though we employed word alignment
from GIZA++ with potential errors, large gains
were achieved using our proposed method. We
would like to investigate the influence of align-
ment errors in the future. In addition, we are plan-
ning to prove the effectiveness of our proposed
method for language pairs other than Japanese-to-
English. We are also planning to introduce our
proposed method to other syntax-based SMT, such
as a string-to-tree SMT and a tree-to-tree SMT.
Acknowledgments
We thank Isao Goto for helpful discussions and
anonymous reviewers for valuable comments. We
also thank Jun Hatori for helping us to apply his
software, Corbit, to our induced POS tagsets.
References
Masayuki Asahara and Yuji Matsumoto. 2003.
IPADIC User Manual. Technical report, Japan.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2001. The Infinite Hidden Markov Model.
In Advances in Neural Information Processing Sys-
tems, pages 577?584.
Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of
Speech Induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 865?874.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
Model of Syntax-Directed Tree to String Grammar
Induction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 352?361.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 541?548.
Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The Infinite Tree. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 272?279.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam Sampling for
the Infinite Hidden Markov Model. In Proceedings
of the 25th International Conference on Machine
Learning, pages 1088?1095.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsuper-
vised PoS tagging. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, pages 678?687.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961?968.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559?578.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental Joint POS Tag-
ging and Dependency Parsing in Chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-Directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop on
849
Computationally Hard Problemsand Joint Inference
in Speech and Language Processing, pages 1?8.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning, pages 63?69.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The Infinite PCFG using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 688?697.
Dekang Lin. 2004. A Path-based Transfer Model for
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 625?630.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improv-
ing Tree-to-Tree Translation with Packed Forests.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, pages 558?566.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 206?214.
Haitao Mi and Qun Liu. 2010. Constituency to De-
pendency Translation with Forests. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, pages 1433?1442.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by Bilingual Generation and Monolin-
gual Derivation. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1963?1978.
Radford M. Neal. 2003. Slice Sampling. Annals of
Statistics, 31:705?767.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Conference of the Association for Computa-
tional Linguistics, pages 271?279.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 159?165.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49.
Jayaram Sethuraman. 1994. A Constructive Definition
of Dirichlet Priors. Statistica Sinica, 4(2):639?650.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 577?
585.
Kairit Sirts and Tanel Aluma?e. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 407?416.
850
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
Processes. Journal of the American Statistical Asso-
ciation, 101(476):1566?1581.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
Translation System Combination by Confusion For-
est. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1249?1257.
Taro Watanabe. 2012. Optimized Online Rank Learn-
ing for Machine Translation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 253?262.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 559?
567.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized Forest to String Translation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 19?24.
851
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470?1480,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Recurrent Neural Networks for Word Alignment Model
Akihiro Tamura
?
, Taro Watanabe, Eiichiro Sumita
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
a-tamura@ah.jp.nec.com,
{taro.watanabe, eiichiro.sumita}@nict.go.jp
Abstract
This study proposes a word alignment
model based on a recurrent neural net-
work (RNN), in which an unlimited
alignment history is represented by re-
currently connected hidden layers. We
perform unsupervised learning using
noise-contrastive estimation (Gutmann
and Hyv?arinen, 2010; Mnih and Teh,
2012), which utilizes artificially generated
negative samples. Our alignment model is
directional, similar to the generative IBM
models (Brown et al, 1993). To overcome
this limitation, we encourage agreement
between the two directional models by
introducing a penalty function that en-
sures word embedding consistency across
two directional models during training.
The RNN-based model outperforms
the feed-forward neural network-based
model (Yang et al, 2013) as well as the
IBM Model 4 under Japanese-English
and French-English word alignment
tasks, and achieves comparable transla-
tion performance to those baselines for
Japanese-English and Chinese-English
translation tasks.
1 Introduction
Automatic word alignment is an important task for
statistical machine translation. The most classical
approaches are the probabilistic IBM models 1-5
(Brown et al, 1993) and the HMM model (Vogel
et al, 1996). Various studies have extended those
models. Yang et al (2013) adapted the Context-
Dependent Deep Neural Network for HMM (CD-
DNN-HMM) (Dahl et al, 2012), a type of feed-
forward neural network (FFNN)-based model, to
?
The first author is now affiliated with Knowledge
Discovery Research Laboratories, NEC Corporation, Nara,
Japan.
the HMM alignment model and achieved state-of-
the-art performance. However, the FFNN-based
model assumes a first-order Markov dependence
for alignments.
Recurrent neural network (RNN)-based models
have recently demonstrated state-of-the-art per-
formance that outperformed FFNN-based models
for various tasks (Mikolov et al, 2010; Mikolov
and Zweig, 2012; Auli et al, 2013; Kalchbrenner
and Blunsom, 2013; Sundermeyer et al, 2013).
An RNN has a hidden layer with recurrent con-
nections that propagates its own previous signals.
Through the recurrent architecture, RNN-based
models have the inherent property of modeling
long-span dependencies, e.g., long contexts, in in-
put data. We assume that this property would fit
with a word alignment task, and we propose an
RNN-based word alignment model. Our model
can maintain and arbitrarily integrate an alignment
history, e.g., bilingual context, which is longer
than the FFNN-based model.
The NN-based alignment models are super-
vised models. Unfortunately, it is usually dif-
ficult to prepare word-by-word aligned bilingual
data. Yang et al (2013) trained their model from
word alignments produced by traditional unsuper-
vised probabilistic models. However, with this
approach, errors induced by probabilistic mod-
els are learned as correct alignments; thus, gen-
eralization capabilities are limited. To solve this
problem, we apply noise-contrastive estimation
(NCE) (Gutmann and Hyv?arinen, 2010; Mnih
and Teh, 2012) for unsupervised training of our
RNN-based model without gold standard align-
ments or pseudo-oracle alignments. NCE artifi-
cially generates bilingual sentences through sam-
plings as pseudo-negative samples, and then trains
the model such that the scores of the original bilin-
gual sentences are higher than those of the sam-
pled bilingual sentences.
Our RNN-based alignment model has a direc-
1470
tion, such as other alignment models, i.e., from f
(source language) to e (target language) and from
e to f . It has been proven that the limitation may
be overcome by encouraging two directional mod-
els to agree by training them concurrently (Ma-
tusov et al, 2004; Liang et al, 2006; Grac?a et al,
2008; Ganchev et al, 2008). The motivation for
this stems from the fact that model and generaliza-
tion errors by the two models differ, and the mod-
els must complement each other. Based on this
motivation, our directional models are also simul-
taneously trained. Specifically, our training en-
courages word embeddings to be consistent across
alignment directions by introducing a penalty term
that expresses the difference between embedding
of words into an objective function. This con-
straint prevents each model from overfitting to a
particular direction and leads to global optimiza-
tion across alignment directions.
This paper presents evaluations of Japanese-
English and French-English word alignment tasks
and Japanese-to-English and Chinese-to-English
translation tasks. The results illustrate that our
RNN-based model outperforms the FFNN-based
model (up to +0.0792 F1-measure) and the IBM
Model 4 (up to +0.0703 F1-measure) for the word
alignment tasks. For the translation tasks, our
model achieves up to 0.74% gain in BLEU as com-
pared to the FFNN-based model, which matches
the translation qualities of the IBM Model 4.
2 Related Work
Various word alignment models have been pro-
posed. These models are roughly clustered into
two groups: generative models, such as those pro-
posed by Brown et al (1993), Vogel et al (1996),
and Och and Ney (2003), and discriminative mod-
els, such as those proposed by Taskar et al (2005),
Moore (2005), and Blunsom and Cohn (2006).
2.1 Generative Alignment Model
Given a source language sentence f
J
1
= f
1
, ..., f
J
and a target language sentence e
I
1
= e
1
, ..., e
I
,
f
J
1
is generated by e
I
1
via the alignment a
J
1
=
a
1
, ..., a
J
. Each a
j
is a hidden variable indicat-
ing that the source word f
j
is aligned to the target
word e
a
j
. Usually, a ?null? word e
0
is added to
the target language sentence and a
J
1
may contain
a
j
= 0, which indicates that f
j
is not aligned to
any target word. The probability of generating the
sentence f
J
1
from e
I
1
is defined as
p(f
J
1
|e
I
1
) =
?
a
J
1
p(f
J
1
, a
J
1
|e
I
1
). (1)
The IBM Models 1 and 2 and the HMM model
decompose it into an alignment probability p
a
and
a lexical translation probability p
t
as
p(f
J
1
, a
J
1
|e
I
1
) =
J
?
j=1
p
a
(a
j
|a
j?1
, j)p
t
(f
j
|e
a
j
). (2)
The three models differ in their definition of align-
ment probability. For example, the HMM model
uses an alignment probability with a first-order
Markov property: p
a
(a
j
|a
j
? a
j?1
). In addition,
the IBM models 3-5 are extensions of these, which
consider the fertility and distortion of each trans-
lated word.
These models are trained using the expectation-
maximization algorithm (Dempster et al, 1977)
from bilingual sentences without word-level align-
ments (unlabeled training data). Given a specific
model, the best alignment (Viterbi alignment) of
the sentence pair (f
J
1
, e
I
1
) can be found as
a?
J
1
= argmax
a
J
1
p(f
J
1
, a
J
1
|e
I
1
). (3)
For example, the HMM model identifies the
Viterbi alignment using the Viterbi algorithm.
2.2 FFNN-based Alignment Model
As an instance of discriminative models, we de-
scribe an FFNN-based word alignment model
(Yang et al, 2013), which is our baseline. An
FFNN learns a hierarchy of nonlinear features
that can automatically capture complex statisti-
cal patterns in input data. Recently, FFNNs have
been applied successfully to several tasks, such as
speech recognition (Dahl et al, 2012), statistical
machine translation (Le et al, 2012; Vaswani et
al., 2013), and other popular natural language pro-
cessing tasks (Collobert and Weston, 2008; Col-
lobert et al, 2011).
Yang et al (2013) have adapted a type of FFNN,
i.e., CD-DNN-HMM (Dahl et al, 2012), to the
HMM alignment model. Specifically, the lexical
translation and alignment probability in Eq. 2 are
computed using FFNNs as
s
NN
(a
J
1
|f
J
1
, e
I
1
) =
J
?
j=1
t
a
(a
j
? a
j?1
|c(e
a
j?1
))
?t
lex
(f
j
, e
a
j
|c(f
j
), c(e
a
j
)), (4)
1471
Lookup
Layer
Hidden Layer
Output Layer
Input fj-1 e
L L L L L L 
htanh(H? +BH)
O? +BO
aj-1
t   ( ,    |      ,      )fj eaj ef j-1j+1lex
z0
z1
fj fj+1 eaj eaj+1
z0
z1
aj-1
aj+1
Figure 1: FFNN-based model for computing a lex-
ical translation score of (f
j
, e
a
j
)
where t
a
and t
lex
are an alignment score and a lex-
ical translation score, respectively, s
NN
is a score
of alignments a
J
1
, and ?c(a word w)? denotes a
context of word w. Note that the model uses non-
probabilistic scores rather than probabilities be-
cause normalization over all words is computa-
tionally expensive. The model finds the Viterbi
alignment using the Viterbi algorithm, similar to
the classic HMM model. Note that alignments
in the FFNN-based model are also governed by
first-order Markov dynamics because an align-
ment score depends on the previous alignment
a
j?1
.
Figure 1 shows the network structure with one
hidden layer for computing a lexical translation
probability t
lex
(f
j
, e
a
j
|c(f
j
), c(e
a
j
)). The model
consists of a lookup layer, a hidden layer, and an
output layer, which have weight matrices. The
model receives a source and target word with their
contexts as inputs, which are words in a prede-
fined window (the window size is three in Fig-
ure 1). First, the lookup layer converts each in-
put word into its word embedding by looking up
its corresponding column in the embedding ma-
trix (L), and then concatenates them. Let V
f
(or
V
e
) be a set of source words (or target words) and
M be a predetermined embedding length. L is a
M ? (|V
f
|+ |V
e
|) matrix
1
. Word embeddings are
dense, low dimensional, and real-valued vectors
that can capture syntactic and semantic properties
of the words (Bengio et al, 2003). The concate-
nation (z
0
) is then fed to the hidden layer to cap-
ture nonlinear relations. Finally, the output layer
receives the output of the hidden layer (z
1
) and
computes a lexical translation score.
1
We add a special token ?unk? to handle unknown words
and ?null? to handle null alignments to V
f
and V
e
The computations in the hidden and output layer
are as follows
2
:
z
1
= f(H ? z
0
+ B
H
), (5)
t
lex
= O ? z
1
+ B
O
, (6)
where H , B
H
, O, and B
O
are |z
1
| ? |z
0
|, |z
1
| ? 1,
1?|z
1
|, and 1?1 matrices, respectively, and f(x)
is an activation function. Following Yang et al
(2013), a ?hard? version of the hyperbolic tangent,
htanh(x)
3
, is used as f(x) in this study.
The alignment model based on an FFNN is
formed in the same manner as the lexical trans-
lation model. Each model is optimized by mini-
mizing the following ranking loss with a margin
using stochastic gradient descent (SGD)
4
, where
gradients are computed by the back-propagation
algorithm (Rumelhart et al, 1986):
loss(?) =
?
(f ,e)?T
max{0, 1? s
?
(a
+
|f , e)
+s
?
(a
?
|f ,e)}, (7)
where ? denotes the weights of layers in the
model, T is a set of training data, a
+
is the gold
standard alignment, a
?
is the incorrect alignment
with the highest score under ?, and s
?
denotes the
score defined by Eq. 4 as computed by the model
under ?.
3 RNN-based Alignment Model
This section proposes an RNN-based alignment
model, which computes a score for alignments a
J
1
using an RNN:
s
NN
(a
J
1
|f
J
1
, e
I
1
) =
J
?
j=1
t
RNN
(a
j
|a
j?1
1
, f
j
, e
a
j
), (8)
where t
RNN
is the score of an alignment a
j
. The
prediction of the j-th alignment a
j
depends on all
preceding alignments a
j?1
1
. Note that the pro-
posed model also uses nonprobabilistic scores,
similar to the FFNN-based model.
The RNN-based model is illustrated in Figure
2. The model consists of a lookup layer, a hid-
den layer, and an output layer, which have weight
2
Consecutive l hidden layers can be used: z
l
= f(H
l
?
z
l?1
+ B
H
l
). For simplicity, this paper describes the model
with 1 hidden layer.
3
htanh(x) = ?1 for x < ?1, htanh(x) = 1 for x > 1,
and htanh(x) = x for others.
4
In our experiments, we used a mini-batch SGD instead
of a plain SGD.
1472
O? +BO
htanh(H? +R? +BH )
t    ( |    ,    ,    )
Lookup Layer
Hidden 
Layer
Output Layer
Input
L L
d
aj fjRNN eajj-1a1
fj eaj
yj
yj-1
yj
d
xj
xj dyj-1
Figure 2: RNN-based alignment model
matrices L, {H
d
, R
d
, B
d
H
}, and {O,B
O
}, respec-
tively. Each matrix in the hidden layer (H
d
, R
d
,
and B
d
H
) depends on alignment, where d denotes
the jump distance from a
j?1
to a
j
: d = a
j
?
a
j?1
. In our experiments, we merge distances
that are greater than 8 and less than -8 into the
special ??8? and ??-8? distances, respectively.
Specifically, the hidden layer has weight matrices
{H
??8
, H
?7
, ? ? ? , H
7
, H
?8
, R
??8
, R
?7
, ? ? ? ,
R
7
, R
?8
, B
??8
H
, B
?7
H
, ? ? ? , B
7
H
, B
?8
H
} and com-
putes y
j
using the corresponding matrices of the
jump distance d.
The Viterbi alignment is determined using the
Viterbi algorithm, similar to the FFNN-based
model, where the model is sequentially applied
from f
1
to f
J
5
. When computing the score of the
alignment between f
j
and e
a
j
, the two words are
input to the lookup layer. In the lookup layer, each
of these words is converted to its word embedding,
and then the concatenation of the two embeddings
(x
j
) is fed to the hidden layer in the same manner
as the FFNN-based model. Next, the hidden layer
receives the output of the lookup layer (x
j
) and
that of the previous hidden layer (y
j?1
). The hid-
den layer then computes and outputs the nonlinear
relations between them. Note that the weight ma-
trices used in this computation are embodied by
the specific jump distance d. The output of the hid-
den layer (y
j
) is copied and fed to the output layer
and the next hidden layer. Finally, the output layer
computes the score of a
j
(t
RNN
(a
j
|a
j?1
1
, f
j
, e
a
j
))
from the output of the hidden layer (y
j
). Note that
the FFNN-based model consists of two compo-
5
Strictly speaking, we cannot apply the dynamic pro-
gramming forward-backward algorithm (i.e., the Viterbi al-
gorithm) due to the long alignment history of y
i
. Thus, the
Viterbi alignment is computed approximately using heuristic
beam search.
nents: one is for lexical translation and the other
is for alignment. The proposed RNN produces a
single score that is constructed in the hidden layer
by employing the distance-dependent weight ma-
trices.
Specifically, the computations in the hidden and
output layer are as follows:
y
j
= f(H
d
? x
j
+ R
d
? y
j?1
+ B
d
H
), (9)
t
RNN
= O ? y
j
+ B
O
, (10)
where H
d
, R
d
, B
d
H
, O, and B
O
are |y
j
| ? |x
j
|,
|y
j
| ? |y
j?1
|, |y
j
| ? 1, 1 ? |y
j
|, and 1 ? 1 matri-
ces, respectively. Note that |y
j?1
| = |y
j
|. f(x) is
an activation function, which is a hard hyperbolic
tangent, i.e., htanh(x), in this study.
As described above, the RNN-based model has
a hidden layer with recurrent connections. Under
the recurrence, the proposed model compactly en-
codes the entire history of previous alignments in
the hidden layer configuration y
i
. Therefore, the
proposed model can find alignments by taking ad-
vantage of the long alignment history, while the
FFNN-based model considers only the last align-
ment.
4 Training
During training, we optimize the weight matrices
of each layer (i.e., L, H
d
, R
d
, B
d
H
, O, and B
O
)
following a given objective using a mini-batch
SGD with batch size D, which converges faster
than a plain SGD (D = 1). Gradients are com-
puted by the back-propagation through time algo-
rithm (Rumelhart et al, 1986), which unfolds the
network in time (j) and computes gradients over
time steps. In addition, an l2 regularization term
is added to the objective to prevent the model from
overfitting the training data.
The RNN-based model can be trained by a
supervised approach, similar to the FFNN-based
model, where training proceeds based on the rank-
ing loss defined by Eq. 7 (Section 2.2). However,
this approach requires gold standard alignments.
To overcome this drawback, we propose an un-
supervised method using NCE, which learns from
unlabeled training data.
4.1 Unsupervised Learning
Dyer et al (2011) presented an unsupervised
alignment model based on contrastive estimation
(CE) (Smith and Eisner, 2005). CE seeks to dis-
criminate observed data from its neighborhood,
1473
which can be viewed as pseudo-negative samples.
Dyer et al (2011) regarded all possible align-
ments of the bilingual sentences, which are given
as training data (T ), and those of the full transla-
tion search space (?) as the observed data and its
neighborhood, respectively.
We introduce this idea to a ranking loss with
margin as
loss(?) = max
{
0, 1?
?
(f+,e+)?T
E
?
[s
?
(a|f
+
, e
+
)]
+
?
(f+,e?)??
E
?
[s
?
(a|f
+
, e
?
)]
}
, (11)
where ? is a set of all possible alignments given
(f , e), E
?
[s
?
] is the expected value of the scores
s
?
on ?, e
+
denotes a target language sentence in
the training data, and e
?
denotes a pseudo-target
language sentence. The first expectation term is
for the observed data, and the second is for the
neighborhood.
However, the computation for ? is prohibitively
expensive. To reduce computation, we employ
NCE, which uses randomly sampled sentences
from all target language sentences in ? as e
?
, and
calculate the expected values by a beam search
with beam width W to truncate alignments with
low scores. In our experiments, we set W to 100.
In addition, the above criterion is converted to an
online fashion as
loss(?) =
?
f+?T
max
{
0, 1? E
GEN
[s
?
(a|f
+
, e
+
)]
+
1
N
?
e?
E
GEN
[s
?
(a|f
+
, e
?
)]
}
, (12)
where e
+
is a target language sentence aligned to
f
+
in the training data, i.e., (f
+
, e
+
) ? T , e
?
is
a randomly sampled pseudo-target language sen-
tence with length |e
+
|, and N denotes the num-
ber of pseudo-target language sentences per source
sentence f
+
. Note that |e
+
| = |e
?
|. GEN is a
subset of all possible word alignments ?, which is
generated by beam search.
In a simple implementation, each e
?
is gener-
ated by repeating a random sampling from a set of
target words (V
e
) |e
+
| times and lining them up
sequentially. To employ more discriminative neg-
ative samples, our implementation samples each
word of e
?
from a set of the target words that co-
occur with f
i
? f
+
whose probability is above a
threshold C under the IBM Model 1 incorporating
l
0
prior (Vaswani et al, 2012). The IBM Model
1 with l
0
prior is convenient for reducing transla-
tion candidates because it generates more sparse
alignments than the standard IBM Model 1.
4.2 Agreement Constraints
Both of the FFNN-based and RNN-based models
are based on the HMM alignment model, and they
are therefore asymmetric, i.e., they can represent
one-to-many relations from the target side. Asym-
metric models are usually trained in each align-
ment direction. The model proposed by Yang et
al. (2013) is no exception. However, it has been
demonstrated that encouraging directional mod-
els to agree improves alignment performance (Ma-
tusov et al, 2004; Liang et al, 2006; Grac?a et al,
2008; Ganchev et al, 2008).
Inspired by their work, we introduce an agree-
ment constraint to our learning. The constraint
concretely enforces agreement in word embed-
dings of both directions. The proposed method
trains two directional models concurrently based
on the following objective by incorporating a
penalty term that expresses the difference between
word embeddings:
argmin
?
FE
{
loss(?
FE
) + ???
L
EF
? ?
L
FE
?
}
, (13)
argmin
?
EF
{
loss(?
EF
) + ???
L
FE
? ?
L
EF
?
}
, (14)
where ?
FE
(or ?
EF
) denotes the weights of lay-
ers in a source-to-target (or target-to-source) align-
ment model, ?
L
denotes weights of a lookup layer,
i.e., word embeddings, and ? is a parameter that
controls the strength of the agreement constraint.
??? indicates the norm of ?. 2-norm is used in our
experiments. Equations 13 and 14 can be applied
to both supervised and unsupervised approaches.
Equations 7 and 12 are substituted into loss(?)
in supervised and unsupervised learning, respec-
tively. The proposed constraint penalizes overfit-
ting to a particular direction and enables two di-
rectional models to optimize across alignment di-
rections globally.
Our unsupervised learning procedure is summa-
rized in Algorithm 1. In Algorithm 1, line 2 ran-
domly samples D bilingual sentences (f
+
, e
+
)
D
from training data T . Lines 3-1 and 3-2 gener-
ate N pseudo-negative samples for each f
+
and
e
+
based on the translation candidates of f
+
and
e
+
found by the IBM Model 1 with l
0
prior,
1474
Algorithm 1 Training Algorithm
Input: ?
1
FE
, ?
1
EF
, training data T , MaxIter,
batch size D, N , C, IBM1, W , ?
1: for all t such that 1 ? t ?MaxIter do
2: {(f
+
, e
+
)
D
}?sample(D,T )
3-1: {(f
+
, {e
?
}
N
)
D
}?neg
e
({(f
+
, e
+
)
D
}, N,C, IBM1)
3-2: {(e
+
, {f
?
}
N
)
D
}?neg
f
({(f
+
, e
+
)
D
}, N,C, IBM1)
4-1: ?
t+1
FE
?update((f
+
, e
+
, {e
?
}
N
)
D
, ?
t
FE
, ?
t
EF
,W, ?)
4-2: ?
t+1
EF
?update((e
+
, f
+
, {f
?
}
N
)
D
, ?
t
EF
, ?
t
FE
,W, ?)
5: end for
Output: ?
MaxIter+1
EF
, ?
MaxIter+1
FE
Train Dev Test
BTEC 9 K 0 960
Hansards 1.1 M 37 447
FBIS
NIST03
240 K 878
919
NIST04 1,597
IWSLT 40 K 2,501 489
NTCIR 3.2 M 2,000 2,000
Table 1: Size of experimental datasets
IBM1 (Section 4.1). Lines 4-1 and 4-2 update the
weights in each layer following a given objective
(Sections 4.1 and 4.2). Note that ?
t
FE
and ?
t
EF
are
concurrently updated in each iteration, and ?
t
EF
(or ?
t
FE
) is employed to enforce agreement be-
tween word embeddings when updating ?
t
FE
(or
?
t
EF
).
5 Experiment
5.1 Experimental Data
We evaluated the alignment performance of the
proposed models with two tasks: Japanese-
English word alignment with the Basic Travel
Expression Corpus (BTEC) (Takezawa et al,
2002) and French-English word alignment with
the Hansard dataset (Hansards) from the 2003
NAACL shared task (Mihalcea and Pedersen,
2003). In addition, we evaluated the end-to-end
translation performance of three tasks: a Chinese-
to-English translation task with the FBIS corpus
(FBIS), the IWSLT 2007 Japanese-to-English
translation task (IWSLT ) (Fordyce, 2007), and
the NTCIR-9 Japanese-to-English patent transla-
tion task (NTCIR) (Goto et al, 2011)
6
.
Table 1 shows the sizes of our experimental
datasets. Note that the development data was
not used in the alignment tasks, i.e., BTEC
6
We did not evaluate the translation performance on the
Hansards data because the development data is very small
and performance is unreliable.
and Hansards, because the hyperparameters of
the alignment models were set by preliminary
small-scale experiments. The BTEC data is
the first 9,960 sentence pairs in the training data
for IWSLT , which were annotated with word
alignment (Goh et al, 2010). We split these
pairs into the first 9,000 for training data and
the remaining 960 as test data. All the data in
BTEC is word-aligned, and the training data in
Hansards is unlabeled data. In FBIS, we used
the NIST02 evaluation data as the development
data, and the NIST03 and 04 evaluation data as
test data (NIST03 and NIST04).
5.2 Comparing Methods
We evaluated the proposed RNN-based alignment
models against two baselines: the IBM Model
4 and the FFNN-based model with one hidden
layer. The IBM Model 4 was trained by pre-
viously presented model sequence schemes (Och
and Ney, 2003): 1
5
H
5
3
5
4
5
, i.e., five iterations of
the IBM Model 1 followed by five iterations of the
HMM Model, etc., which is the default setting for
GIZA++ (IBM4). For the FFNN-based model,
we set the word embedding length M to 30, the
number of units of a hidden layer |z
1
| to 100, and
the window size of contexts to 5. Hence, |z
0
| is
300 (30?5?2). Following Yang et al (2013), the
FFNN-based model was trained by the supervised
approach described in Section 2.2 (FFNN
s
).
For the RNN-based models, we set M to 30
and the number of units of each recurrent hid-
den layer |y
j
| to 100. Thus, |x
j
| is 60 (30 ? 2).
The number of units of each layer of the FFNN-
based and RNN-based models and M were set
through preliminary experiments. To demonstrate
the effectiveness of the proposed learning meth-
ods, we evaluated four types of RNN-based mod-
els: RNN
s
, RNN
s+c
, RNN
u
, and RNN
u+c
,
where ?s/u? denotes a supervised/unsupervised
model and ?+c? indicates that the agreement con-
straint was used.
In training all the models except IBM4, the
weights of each layer were initialized first. For
the weights of a lookup layer L, we preliminarily
trained word embeddings for the source and target
language from each side of the training data. We
then set the word embeddings to L to avoid falling
into local minima. Other weights were randomly
initialized to [?0.1, 0.1]. For the pretraining, we
1475
Alignment BTEC Hansards
IBM4 0.4859 0.9029
FFNN
s
(I) 0.4770 0.9020
RNN
s
(I) 0.5053
+
0.9068
RNN
s+c
(I) 0.5174
+
0.9202
+
RNN
u
0.5307
+
0.9037
RNN
u+c
0.5562
+
0.9275
+
FFNN
s
(R) 0.8224 -
RNN
s
(R) 0.8798
+
-
RNN
s+c
(R) 0.8921
+
-
Table 2: Word alignment performance (F1-
measure)
used the RNNLM Toolkit
7
(Mikolov et al, 2010)
with the default options. We mapped all words
that occurred less than five times to the special to-
ken ?unk?. Next, each weight was optimized us-
ing the mini-batch SGD, where batch size D was
100, learning rate was 0.01, and an l
2
regulariza-
tion parameter was 0.1. The training stopped after
50 epochs. The other parameters were set as fol-
lows: W , N and C in the unsupervised learning
were 100, 50, and 0.001, respectively, and ? for
the agreement constraint was 0.1.
In the translation tasks, we used the Moses
phrase-based SMT systems (Koehn et al, 2007).
All Japanese and Chinese sentences were seg-
mented by ChaSen
8
and the Stanford Chinese seg-
menter
9
, respectively. In the training, long sen-
tences with over 40 words were filtered out. Using
the SRILM Toolkits (Stolcke, 2002) with modified
Kneser-Ney smoothing, we trained a 5-gram lan-
guage model on the English side of each training
data for IWSLT and NTCIR, and a 5-gram lan-
guage model on the Xinhua portion of the English
Gigaword corpus for FBIS. The SMT weighting
parameters were tuned by MERT (Och, 2003) in
the development data.
5.3 Word Alignment Results
Table 2 shows the alignment performance by
the F1-measure. Hereafter, MODEL(R) and
MODEL(I) denote the MODEL trained from
gold standard alignments and word alignments
found by the IBM Model 4, respectively. In
Hansards, all models were trained from ran-
7
http://www.fit.vutbr.cz/
?
imikolov/
rnnlm/
8
http://chasen-legacy.sourceforge.jp/
9
http://nlp.stanford.edu/software/
segmenter.shtml
domly sampled 100 K data
10
. We evaluated
the word alignments produced by first applying
each model in both directions and then combin-
ing the alignments using the ?grow-diag-final-
and? heuristic (Koehn et al, 2003). The signif-
icance test on word alignment performance was
performed by the sign test with a 5% significance
level. ?+? in Table 2 indicates that the compar-
isons are significant over corresponding baselines,
IBM4 and FFNN
s
(R/I).
In Table 2, RNN
u+c
, which includes all our
proposals, i.e., the RNN-based model, the unsu-
pervised learning, and the agreement constraint,
achieves the best performance for both BTEC
and Hansards. The differences from the base-
lines are statistically significant.
Table 2 shows that RNN
s
(R/I) outperforms
FFNN
s
(R/I), which is statistically significant
in BTEC. These results demonstrate that captur-
ing the long alignment history in the RNN-based
model improves the alignment performance. We
discuss the difference of the RNN-based model?s
effectiveness between language pairs in Section
6.1. Table 2 also shows that RNN
s+c
(R/I) and
RNN
u+c
achieve significantly better performance
than RNN
s
(R/I) and RNN
u
in both tasks, re-
spectively. This indicates that the proposed agree-
ment constraint is effective in training better mod-
els in both the supervised and unsupervised ap-
proaches.
In BTEC, RNN
u
and RNN
u+c
significantly
outperform RNN
s
(I) and RNN
s+c
(I), respec-
tively. The performance of these models is com-
parable with Hansards. This indicates that our
unsupervised learning benefits our models because
the supervised models are adversely affected by
errors in the automatically generated training data.
This is especially true when the quality of training
data, i.e., the performance of IBM4, is low.
5.4 Machine Translation Results
Table 3 shows the translation performance by the
case sensitive BLEU4 metric
11
(Papineni et al,
2002). Table 3 presents the average BLEU of three
different MERT runs. In NTCIR and FBIS,
each alignment model was trained from the ran-
10
Due to high computational cost, we did not use all the
training data. Scaling up to larger datasets will be addressed
in future work.
11
We used mteval-v13a.pl as the evaluation tool
(http://www.itl.nist.gov/iad/mig/tests/
mt/2009/).
1476
Alignment IWSLT NTCIR
FBIS
NIST03 NIST04
IBM4
all
46.47
27.91 25.90 28.34
IBM4 27.25 25.41 27.65
FFNN
s
(I) 46.38 27.05 25.45 27.61
RNN
s
(I) 46.43 27.24 25.47 27.56
RNN
s+c
(I) 46.51 27.12 25.55 27.73
RNN
u
47.05
?
27.79
?
25.76
?
27.91
?
RNN
u+c
46.97
?
27.76
?
25.84
?
28.20
?
Table 3: Translation performance (BLEU4(%))
domly sampled 100 K data, and then a translation
model was trained from all the training data that
was word-aligned by the alignment model. In ad-
dition, for a detailed comparison, we evaluated the
SMT system where the IBM Model 4 was trained
from all the training data (IBM4
all
). The sig-
nificance test on translation performance was per-
formed by the bootstrap method (Koehn, 2004)
with a 5% significance level. ?*? in Table 3 in-
dicates that the comparisons are significant over
both baselines, i.e., IBM4 and FFNN
s
(I).
Table 3 also shows that better word align-
ment does not always result in better translation,
which has been discussed previously (Yang et al,
2013). However, RNN
u
and RNN
u+c
outper-
form FFNN
s
(I) and IBM4 in all tasks. These
results indicate that our proposals contribute to im-
proving translation performance
12
. In addition,
Table 3 shows that these proposed models are
comparable to IBM4
all
in NTCIR and FBIS
even though the proposed models are trained from
only a small part of the training data.
6 Discussion
6.1 Effectiveness of RNN-based Alignment
Model
Figure 3 shows word alignment examples from
FFNN
s
and RNN
s
, where solid squares indi-
cate the gold standard alignments. Figure 3 (a)
shows that RRN
s
adequately identifies compli-
cated alignments with long distances compared
to FFNN
s
(e.g., jaggy alignments of ?have you
been learning? in Fig 3 (a)) because RNN
s
cap-
tures alignment paths based on long alignment his-
tory, which can be viewed as phrase-level align-
ments, while FFNN
s
employs only the last align-
ment.
In French-English word alignment, the most
12
We also confirmed the effectiveness of our models on the
NIST05 and NTCIR-10 evaluation data.
How
long
have
you
been
learning
English
?
??
?
? ??? ????? ?? ?????? ?? ?? ? ?
?
?
? ?
? ?
?
?
?
?
(a) Japanese-English Alignment
they
also
have
a
role
to
play
in
food
chain
.
the
eux aus
si
ont un r?le ? jou
erdan
s
la cha
?ne
alim
ent
aire
.
(b) French-English Alignment
?
?
?
?
?
?
?
?
?
?
?
?
? : FFNN  (R)s
: RNN  (R)s
? : FFNN  (I)s
: RNN  (I)s
Figure 3: Word alignment examples
Alignment 40 K 9 K 1 K
IBM4 0.5467 0.4859 0.4128
RNN
u+c
0.6004 0.5562 0.4842
RNN
s+c
(R) - 0.8921 0.6063
Table 4: Word alignment performance on BTEC
with various sized training data
valuable clues are located locally because English
and French have similar word orders and their
alignment has more one-to-one mappings than
Japanese-English word alignment (Figure 3). Fig-
ure 3 (b) shows that both RRN
s
and FFNN
s
work for such simpler alignments. Therefore,
the RNN-based model has less effect on French-
English word alignment than Japanese-English
word alignment, as indicated in Table 2.
6.2 Impact of Training Data Size
Table 4 shows the alignment performance on
BTEC with various training data sizes, i.e., train-
ing data for IWSLT (40 K), training data for
BTEC (9 K), and the randomly sampled 1 K
data from the BTEC training data. Note that
RNN
s+c
(R) cannot be trained from the 40 K data
because the 40 K data does not have gold standard
1477
Alignment BTEC Hansards
FFNN
s
(I) 0.4770 0.9020
FFNN
s+c
(I) 0.4854
+
0.9085
+
FFNN
u
0.5105
+
0.9026
FFNN
u+c
0.5313
+
0.9144
+
FFNN
s
(R) 0.8224 -
FFNN
s+c
(R) 0.8367
+
-
Table 5: Word alignment performance of various
FFNN-based models (F1-measure)
word alignments.
Table 4 demonstrates that the proposed RNN-
based model outperforms IBM4 trained from the
unlabeled 40 K data by employing either the 1
K labeled data or the 9 K unlabeled data, which
is less than 25% of the training data for IBM4.
Consequently, the SMT system using RNN
u+c
trained from a small part of training data can
achieve comparable performance to that using
IBM4 trained from all training data, which is
shown in Table 3.
6.3 Effectiveness of Unsupervised
Learning/Agreement Constraints
The proposed unsupervised learning and agree-
ment constraints can be applied to any NN-based
alignment model. Table 5 shows the alignment
performance of the FFNN-based models trained
by our supervised/unsupervised approaches (s/u)
with and without our agreement constraints. In
Table 5, ?+c? denotes that the agreement con-
straint was used, and ?+? indicates that the
comparison with its corresponding baseline, i.e.,
FFNN
s
(I/R), is significant in the sign test with a
5% significance level.
Table 5 shows that FFNN
s+c
(R/I) and
FFNN
u+c
achieve significantly better perfor-
mance than FFNN
s
(R/I) and FFNN
u
, respec-
tively, in both BTEC and Hansards. In addi-
tion, FFNN
u
and FFNN
u+c
significantly out-
perform FFNN
s
(I) and FFNN
s+c
(I), respec-
tively, in BTEC. The performance of these mod-
els is comparable in Hansards. These results
indicate that the proposed unsupervised learning
and agreement constraint benefit the FFNN-based
model, similar to the RNN-based model.
7 Conclusion
We have proposed a word alignment model based
on an RNN, which captures long alignment his-
tory through recurrent architectures. Furthermore,
we proposed an unsupervised method for training
our model using NCE and introduced an agree-
ment constraint that encourages word embeddings
to be consistent across alignment directions. Our
experiments have shown that the proposed model
outperforms the FFNN-based model (Yang et al,
2013) for word alignment and machine translation,
and that the agreement constraint improves align-
ment performance.
In future, we plan to employ contexts composed
of surrounding words (e.g., c(f
j
) or c(e
a
j
) in the
FFNN-based model) in our model, even though
our model implicitly encodes such contexts in the
alignment history. We also plan to enrich each
hidden layer in our model with multiple layers
following the success of Yang et al (2013), in
which multiple hidden layers improved the perfor-
mance of the FFNN-based model. In addition, we
would like to prove the effectiveness of the pro-
posed method for other datasets.
Acknowledgments
We thank the anonymous reviewers for their help-
ful suggestions and valuable comments on the first
version of this paper.
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 65?72.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning. In
1478
Proceedings of the 25th International Conference on
Machine Learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-Dependent Pre-trained Deep Neu-
ral Networks for Large Vocabulary Speech Recog-
nition. Audio, Speech, and Language Processing,
IEEE Transactions on, 20(1):30?42.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1?38.
Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised Word Alignment with
Arbitrary Features. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, pages 409?419.
Cameron S. Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the
4th International Workshop on Spoken Languaeg
Translation, pages 1?12.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better Alignments = Better Translations? In
Proceedings of the 46th Annual Conference of the
Association for Computational Linguistics: Human
Language Technologies, pages 986?993.
Chooi-Ling Goh, TaroWatanabe, Hirofumi Yamamoto,
and Eiichiro Sumita. 2010. Constraining a Gen-
erative Word Alignment Model with Discriminative
Output. IEICE Transactions, 93-D(7):1976?1983.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559?578.
Jo?ao V. Grac?a, Kuzman Ganchev, and Ben Taskar.
2008. Expectation Maximization and Posterior
Constraints. In Advances in Neural Information
Processing Systems 20, pages 569?576.
Michael Gutmann and Aapo Hyv?arinen. 2010. Noise-
Contrastive Estimation: A New Estimation Principle
for Unnormalized Statistical Models. In Proceed-
ings of the 13st International Conference on Artifi-
cial Intelligence and Statistics, pages 297?304.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700?1709.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388?395.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 39?48.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of the Main
Conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric Word Alignments for Statistical
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 219?225.
Rada Mihalcea and Ted Pedersen. 2003. An Evalua-
tion Exercise for Word Alignment. In Proceedings
of the HLT-NAACL 2003 Workshop on Building and
Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 1?10.
Tomas Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Lan-
guage Model. In Proceedings of the 4th IEEE Work-
shop on Spoken Language Technology, pages 234?
239.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proceedings of 11th Annual Conference of the Inter-
national Speech Communication Association, pages
1045?1048.
Andriy Mnih and Yee Whye Teh. 2012. A Fast and
Simple Algorithm for Training Neural Probabilistic
Language Models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751?1758.
1479
Robert C. Moore. 2005. A Discriminative Framework
for Bilingual Word Alignment. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 81?88.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Learning Internal Representations by Error
Propagation. In D. E. Rumelhart and J. L. McClel-
land, editors, Parallel Distributed Processing, pages
318?362. MIT Press.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 354?362.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, pages 8430?8434.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-
aya, Hirofumi Yamamoto, and Seiichi Yamamoto.
2002. Toward a Broad-coverage Bilingual Corpus
for Speech Translation of Travel Conversations in
the Real World. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Eval-
uation, pages 147?152.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach to
Word Alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?80.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller Alignment Models for Better Trans-
lations: Unsupervised Word Alignment with the l
0
-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 311?319.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-Scale
Neural Language Models Improves Translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1387?1392.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 836?841.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word Alignment Modeling with Con-
text Dependent Deep Neural Network. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 166?175.
1480
