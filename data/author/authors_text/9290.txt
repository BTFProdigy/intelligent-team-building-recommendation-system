Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 288?295,
New York, June 2006. c?2006 Association for Computational Linguistics
Exploring Syntactic Features for Relation Extraction using 
a Convolution Tree Kernel 
 
 
 
Min ZHANG         Jie ZHANG        Jian SU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, zhangjie, sujian}@i2r.a-star.edu.sg 
 
 
Abstract 
This paper proposes to use a convolution 
kernel over parse trees to model syntactic 
structure information for relation extrac-
tion. Our study reveals that the syntactic 
structure features embedded in a parse 
tree are very effective for relation extrac-
tion and these features can be well cap-
tured by the convolution tree kernel. 
Evaluation on the ACE 2003 corpus 
shows that the convolution kernel over 
parse trees can achieve comparable per-
formance with the previous best-reported 
feature-based methods on the 24 ACE re-
lation subtypes. It also shows that our 
method significantly outperforms the pre-
vious two dependency tree kernels on the 
5 ACE relation major types. 
1 Introduction 
Relation extraction is a subtask of information ex-
traction that finds various predefined semantic re-
lations, such as location, affiliation, rival, etc., 
between pairs of entities in text. For example, the 
sentence ?George Bush is the president of the 
United States.? conveys the semantic relation 
?President? between the entities ?George Bush? 
(PER) and ?the United States? (GPE: a Geo-Political 
Entity --- an entity with land and a government (ACE, 2004)). 
Prior feature-based methods for this task 
(Kambhatla 2004; Zhou et al, 2005) employed a 
large amount of diverse linguistic features, varying 
from lexical knowledge, entity mention informa-
tion to syntactic parse trees, dependency trees and 
semantic features. Since a parse tree contains rich 
syntactic structure information, in principle, the 
features extracted from a parse tree should contrib-
ute much more to performance improvement for 
relation extraction. However it is reported (Zhou et 
al., 2005; Kambhatla, 2004) that hierarchical struc-
tured syntactic features contributes less to per-
formance improvement. This may be mainly due to 
the fact that the syntactic structure information in a 
parse tree is hard to explicitly describe by a vector 
of linear features. As an alternative, kernel meth-
ods (Collins and Duffy, 2001) provide an elegant 
solution to implicitly explore tree structure features 
by directly computing the similarity between two 
trees. But to our surprise, the sole two-reported 
dependency tree kernels for relation extraction on 
the ACE corpus (Bunescu and Mooney, 2005; Cu-
lotta and Sorensen, 2004) showed much lower per-
formance than the feature-based methods. One 
may ask: are the syntactic tree features very useful 
for relation extraction? Can tree kernel methods 
effectively capture the syntactic tree features and 
other various features that have been proven useful 
in the feature-based methods? 
In this paper, we demonstrate the effectiveness 
of the syntactic tree features for relation extraction 
and study how to capture such features via a con-
volution tree kernel. We also study how to select 
the optimal feature space (e.g. the set of sub-trees 
to represent relation instances) to optimize the sys-
tem performance. The experimental results show 
that the convolution tree kernel plus entity features 
achieves slightly better performance than the pre-
vious best-reported feature-based methods. It also 
shows that our method significantly outperforms 
the two dependency tree kernels (Bunescu and 
Mooney, 2005; Culotta and Sorensen, 2004) on the 
5 ACE relation types. 
The rest of the paper is organized as follows. In 
Section 2, we review the previous work. Section 3 
discusses our tree kernel based learning algorithm. 
288
Section 4 shows the experimental results and com-
pares our work with the related work. We conclude 
our work in Section 5.   
2 Related Work 
The task of relation extraction was introduced as a 
part of the Template Element task in MUC6 and 
formulated as the Template Relation task in MUC7 
(MUC, 1987-1998). 
Miller et al (2000) address the task of relation 
extraction from the statistical parsing viewpoint. 
They integrate various tasks such as POS tagging, 
NE tagging, template extraction and relation ex-
traction into a generative model. Their results es-
sentially depend on the entire full parse tree. 
 Kambhatla (2004) employs Maximum Entropy 
models to combine diverse lexical, syntactic and 
semantic features derived from the text for relation 
extraction. Zhou et al (2005) explore various fea-
tures in relation extraction using SVM. They con-
duct exhaustive experiments to investigate the 
incorporation and the individual contribution of 
diverse features. They report that chunking infor-
mation contributes to most of the performance im-
provement from the syntactic aspect.  
The features used in Kambhatla (2004) and 
Zhou et al (2005) have to be selected and carefully 
calibrated manually. Kambhatla (2004) use the 
path of non-terminals connecting two mentions in 
a parse tree as the parse tree features. Besides, 
Zhou et al (2005) introduce additional chunking 
features to enhance the parse tree features. How-
ever, the hierarchical structured information in the 
parse trees is not well preserved in their parse tree-
related features.  
As an alternative to the feature-based methods, 
kernel methods (Haussler, 1999) have been pro-
posed to implicitly explore features in a high di-
mensional space by employing a kernel function to 
calculate the similarity between two objects di-
rectly. In particular, the kernel methods could be 
very effective at reducing the burden of feature 
engineering for structured objects in NLP research 
(Culotta and Sorensen, 2004). This is because a 
kernel can measure the similarity between two dis-
crete structured objects directly using the original 
representation of the objects instead of explicitly 
enumerating their features. 
Zelenko et al (2003) develop a tree kernel for 
relation extraction. Their tree kernel is recursively 
defined in a top-down manner, matching nodes 
from roots to leaf nodes. For each pair of matching 
nodes, a subsequence kernel on their child nodes is 
invoked, which matches either contiguous or 
sparse subsequences of node. Culotta and Sorensen 
(2004) generalize this kernel to estimate similarity 
between dependency trees. One may note that their 
tree kernel requires the matchable nodes must be at 
the same depth counting from the root node. This 
is a strong constraint on the matching of syntax so 
it is not surprising that the model has good preci-
sion but very low recall on the ACE corpus (Zhao 
and Grishman, 2005). In addition, according to the 
top-down node matching mechanism of the kernel, 
once a node is not matchable with any node in the 
same layer in another tree, all the sub-trees below 
this node are discarded even if some of them are 
matchable to their counterparts in another tree. 
Bunescu and Mooney (2005) propose a shortest 
path dependency kernel for relation extraction. 
They argue that the information to model a rela-
tionship between entities is typically captured by 
the shortest path between the two entities in the 
dependency graph. Their kernel is very straight-
forward. It just sums up the number of common 
word classes at each position in the two paths. We 
notice that one issue of this kernel is that they limit 
the two paths must have the same length, otherwise 
the kernel similarity score is zero. Therefore, al-
though this kernel shows non-trivial performance 
improvement than that of Culotta and Sorensen 
(2004), the constraint makes the two dependency 
kernels share the similar behavior: good precision 
but much lower recall on the ACE corpus. 
Zhao and Grishman (2005) define a feature-
based composite kernel to integrate diverse fea-
tures. Their kernel displays very good performance 
on the 2004 version of ACE corpus. Since this is a 
feature-based kernel, all the features used in the 
kernel have to be explicitly enumerated. Similar 
with the feature-based method, they also represent 
the tree feature as a link path between two entities. 
Therefore, we wonder whether their performance 
improvement is mainly due to the explicitly incor-
poration of diverse linguistic features instead of the 
kernel method itself. 
The above discussion suggests that the syntactic 
features in a parse tree may not be fully utilized in 
the previous work, whether feature-based or ker-
nel-based. We believe that the syntactic tree fea-
tures could play a more important role than that 
289
reported in the previous work. Since convolution 
kernels aim to capture structural information in 
terms of sub-structures, which providing a viable 
alternative to flat features, in this paper, we pro-
pose to use a convolution tree kernel to explore 
syntactic features for relation extraction. To our 
knowledge, convolution kernels have not been ex-
plored for relation extraction1.  
3 Tree Kernels for Relation Extraction  
In this section, we discuss the convolution tree 
kernel associated with different relation feature 
spaces. In Subsection 3.1, we define seven differ-
ent relation feature spaces over parse trees. In Sub-
section 3.2, we introduce a convolution tree kernel 
for relation extraction. Finally we compare our 
method with the previous work in Subsection 3.3. 
3.1 Relation Feature Spaces 
In order to study which relation feature spaces (i.e., 
which portion of parse trees) are optimal for rela-
tion extraction, we define seven different relation 
feature spaces as follows (as shown in Figure 1): 
 
(1) Minimum Complete Tree (MCT):  
It is the complete sub-tree rooted by the node of 
the nearest common ancestor of the two entities 
under consideration.  
 
(2) Path-enclosed Tree (PT): 
It is the smallest common sub-tree including the 
two entities. In other words, the sub-tree is en-
closed by the shortest path linking the two entities 
in the parse tree (this path is also typically used as 
the path tree features in the feature-based meth-
ods). 
 
(3) Chunking Tree (CT): 
It is the base phrase list extracted from the PT. 
We prune out all the internal structures of the PT 
and only keep the root node and the base phrase 
list for generating the chunking tree. 
                                                          
1 Convolution kernels were proposed as a concept of kernels 
for a discrete structure by Haussler (1999) in machine learning 
study. This framework defines a kernel between input objects 
by applying convolution ?sub-kernels? that are the kernels for 
the decompositions (parts) of the objects. Convolution kernels 
are abstract concepts, and the instances of them are deter-
mined by the definition of ?sub-kernels?. The Tree Kernel 
(Collins and Duffy, 2001), String Subsequence Kernel (SSK) 
(Lodhi et al, 2002) and Graph Kernel (HDAG Kernel) (Su-
zuki et al, 2003) are examples of convolution kernels in-
stances in the NLP field.  
(4) Context-Sensitive Path Tree (CPT): 
It is the PT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the 
node of entity 2. If the sibling is unavailable, then 
we move to the parent of current node and repeat 
the same process until the sibling is available or 
the root is reached. 
(5) Context-Sensitive Chunking Tree (CCT): 
It is the CT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the 
node of entity 2. If the sibling is unavailable, the 
same process as generating the CPT is applied. 
Then we do a further pruning process to guarantee 
that the context structures of the CCT is still a list 
of base phrases.  
(6) Flattened  PT (FPT): 
We define two criteria to flatten the PT in order 
to generate the Flattened Parse tree: if the in and 
out arcs of a non-terminal node (except POS node) 
are both single, the node is to be removed; if a 
node has the same phrase type with its father node, 
the node is also to be removed. 
(7) Flattened CPT (FCPT): 
We use the above two criteria to flatten the CPT 
tree to generate the Flattened CPT.  
Figure 1 in the next page illustrates the different 
sub-tree structures for a relation instance in sen-
tence ?Akyetsu testified he was powerless to stop 
the merger of an estimated 2000 ethnic Tutsi's in 
the district of Tawba.?. The relation instance is an 
example excerpted from the ACE corpus, where an 
ACE-defined relation ?AT.LOCATED? exists be-
tween the entities ?Tutsi's? (PER) and ?district? 
(GPE).  
We use Charniak?s parser (Charniak, 2001) to 
parse the example sentence. Due to space limita-
tion, we do not show the whole parse tree of the 
entire sentence here. Tree T1 in Figure 1 is the 
MCT of the relation instance example, where the 
sub-structure circled by a dashed line is the PT. 
For clarity, we re-draw the PT as in T2. The only 
difference between the MCT and the PT lies in 
that the MCT does not allow the partial production 
rules. For instance, the most-left two-layer sub-tree 
[NP [DT ? E1-O-PER]] in T1 is broken apart in 
T2. By comparing the performance of T1 and T2, we 
can test whether the sub-structures with partial 
production rules as in T2 will decrease perform-
ance. T3 is the CT. By comparing the performance 
of T2 and T3, we want to study whether the chunk-
ing information or the parse tree is more effective 
290
for relation extraction. T4 is the CPT, where the 
two structures circled by dashed lines are the so-
called context structures. T5 is the CCT, where the 
additional context structures are also circled by 
dashed lines. We want to study if the limited con-
text information in the CPT and the CCT can help 
boost performance. Moreover, we illustrate the 
other two flattened trees in T6 and T7. The two cir-
cled nodes in T2 are removed in the flattened trees. 
We want to study if the eliminated small structures 
are noisy features for relation extraction.  
3.2 The Convolution Tree Kernel 
Given the relation instances defined in the previous 
section, we use the same convolution tree kernel as 
the parse tree kernel (Collins and Duffy, 2001) and 
the semantic kernel (Moschitti, 2004). Generally, 
we can represent a parse tree T by a vector of inte-
ger counts of each sub-tree type (regardless of its 
ancestors): 
 
( )T? = (# of sub-trees of type 1, ?, # of sub-
trees of type i, ?, # of sub-trees of type n) 
 
This results in a very high dimensionality since the 
number of different sub-trees is exponential in its 
size. Thus it is computational infeasible to directly 
use the feature vector ( )T? . To solve the compu-
 
T1): MCT 
T2): PT
T3): CT T4):CPT 
T5):CCT 
T6):FPT 
T7):FCPT
Figure 1. Relation Feature Spaces of the Example Sentence ??? to stop the merger of an estimated 
2000 ethnic Tutsi's in the district of Tawba.?, where the phrase type ?E1-O-PER? denotes 
that the current phrase is the 1st entity, its entity type is ?PERSON? and its mention level is 
?NOMIAL?, and likewise for the other two phrase types ?E2-O-GPE? and ?E-N-GPE?. 
291
tational issue, we introduce the tree kernel function 
which is able to calculate the dot product between 
the above high dimensional vectors efficiently. The 
kernel function is defined as follows: 
 
1 1 2 2
1 2 1 2 1 2
1 2
( , ) ( ), ( ) ( )[ ], ( )[ ]
( ) ( )
i
i in N n N i
K T T T T T i T i
I n I n
? ? ? ?
? ?
=< >=
= ?
?
? ? ?
 
where N1 and N2 are the sets of all nodes in trees T1 
and T2, respectively, and Ii(n) is the indicator func-
tion that is 1 iff a sub-tree of type i occurs with 
root at node n and zero otherwise. Collins and 
Duffy (2002) show that 1 2( , )K T T  is an instance of 
convolution kernels over tree structures, and which 
can be computed in 1 2(| | | |)O N N?  by the follow-
ing recursive definitions (Let 1 2( , )n n? =  
1 2( ) ( )i ii I n I n?? ):  
(1) if 1n  and 2n  do not have the same syntactic tag 
or their children are different then 1 2( , ) 0n n? = ; 
(2) else if their children are leaves (POS tags), then 
1 2( , ) 1n n ?? = ? ; 
(3) else 
1( )
1 2 1 2
1
( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j?
=
? = +?? , 
where 1( )nc n is the number of the children of 1n , 
( , )ch n j  is the jth child of node n  and 
? ( 0 1?< < ) is the decay factor in order to make 
the kernel value less variable with respect to the 
tree sizes. 
3.3 Comparison with Previous Work 
It would be interesting to review the differences 
between our method and the feature-based meth-
ods. The basic difference between them lies in the 
relation instance representation and the similarity 
calculation mechanism. A relation instance in our 
method is represented as a parse tree while it is 
represented as a vector of features in the feature-
based methods. Our method estimates the similar-
ity between two relation instances by only count-
ing the number of sub-structures that are in 
common while the feature methods calculate the 
dot-product between the feature vectors directly. 
The main difference between them is the different 
feature spaces. By the kernel method, we implicitly 
represent a parse tree by a vector of integer counts 
of each sub-structure type. That is to say, we con-
sider the entire sub-structure types and their occur-
ring frequencies. In this way, on the one hand, the 
parse tree-related features in the flat feature set2 
are embedded in the feature space of our method: 
?Base Phrase Chunking? and ?Parse Tree? fea-
tures explicitly appear as substructures of a parse 
tree. A few of entity-related features in the flat fea-
ture set are also captured by our feature space: ?en-
tity type? and ?mention level? explicitly appear as 
phrase types in a parse tree. On the other hand, the 
other features in the flat feature set, such as ?word 
features?, ?bigram word features?, ?overlap? and 
?dependency tree? are not contained in our feature 
space. From the syntactic viewpoint, the tree repre-
sentation in our feature space is more robust than 
?Parse Tree Path? feature in the flat feature set 
since the path feature is very sensitive to the small 
changes of parse trees (Moschitti, 2004) and it also 
does not maintain the hierarchical information of a 
parse tree. Due to the extensive exploration of syn-
tactic features by kernel, our method is expected to 
show better performance than the previous feature-
based methods. 
It is also worth comparing our method with the 
previous relation kernels. Since our method only 
counts the occurrence of each sub-tree without 
considering its ancestors, our method is not limited 
by the constraints in Culotta and Sorensen (2004) 
and that in Bunescu and Mooney (2005) as dis-
cussed in Section 2. Compared with Zhao and 
Grishman?s kernel, our method directly uses the 
original representation of a parse tree while they 
flatten a parse tree into a link and a path. Given the 
above improvements, our method is expected to 
outperform the previous relation kernels.  
4 Experiments 
The aim of our experiment is to verify the effec-
tiveness of using richer syntactic structures and the 
convolution tree kernel for relation extraction. 
4.1 Experimental Setting 
Corpus: we use the official ACE corpus for 2003 
evaluation from LDC as our test corpus. The ACE 
corpus is gathered from various newspaper, news-
wire and broadcasts. The same as previous work 
                                                          
2 For the convenience of discussion, without losing generality, 
we call the features used in Zhou et al (2005) and Kambhatla 
(2004) flat feature set. 
292
(Zhou et al, 2005), our experiments are carried out 
on explicit relations due to the poor inter-annotator 
agreement in annotation of implicit relations and 
their limited numbers. The training set consists of 
674 annotated text documents and 9683 relation 
instances. The test set consists of 97 documents 
and 1386 relation instances. The 2003 evaluation 
defined 5 types of entities: Persons, Organizations, 
Locations, Facilities and GPE. Each mention of an 
entity is associated with a mention type: proper 
name, nominal or pronoun. They further defined 5 
major relation types and 24 subtypes: AT (Base-In, 
Located?), NEAR (Relative-Location), PART 
(Part-of, Subsidiary ?), ROLE (Member, Owner 
?) and SOCIAL (Associate, Parent?). As previ-
ous work, we explicitly model the argument order 
of the two mentions involved. We thus model rela-
tion extraction as a multi-class classification prob-
lem with 10 classes on the major types (2 for each 
relation major type and a ?NONE? class for non-
relation (except 1 symmetric type)) and 43 classes 
on the subtypes (2 for each relation subtype and a 
?NONE? class for non-relation (except 6 symmet-
ric subtypes)). In this paper, we only measure the 
performance of relation extraction models on 
?true? mentions with ?true? chaining of corefer-
ence (i.e. as annotated by LDC annotators).  
  
Classifier: we select SVM as the classifier used in 
this paper since SVM can naturally work with ker-
nel methods and it also represents the state-of-the-
art machine learning algorithm. We adopt the one 
vs. others strategy and select the one with largest 
margin as the final answer. The training parameters 
are chosen using cross-validation (C=2.4 (SVM); 
? =0.4(tree kernel)). In our implementation, we 
use the binary SVMLight developed by Joachims 
(1998) and Tree Kernel Toolkits developed by 
Moschitti (2004). 
 
Kernel Normalization: since the size of a parse 
tree is not constant, we normalize 1 2( , )K T T by divid-
ing it by 1 1 2 2( , ) ( , )K T T K T T? .  
 
Evaluation Method: we parse the sentence using 
Charniak parser and iterate over all pair of men-
tions occurring in the same sentence to generate 
potential instances. We find the negative samples 
are 10 times more than the positive samples. Thus 
data imbalance and sparseness are potential prob-
lems. Recall (R), Precision (P) and F-measure (F) 
are adopted as the performance measure. 
4.2 Experimental Results  
In order to study the impact of the sole syntactic 
structure information embedded in parse trees on 
relation extraction, we remove the entity informa-
tion from parse trees by replacing the entity-related 
phrase type (?E1-O-PER?, etc., in Figure 1) with 
?NP?. Then we carry out a couple of preliminary 
experiments on the test set using parse trees re-
gardless of entity information.  
 
Feature Spaces P R F 
Minimum Complete Tree 77.45 38.39 51.34 
Path-enclosed Tree (PT) 72.77 53.80 61.87 
Chunking Tree (CT) 75.18 44.75 56.11 
Context-Sensitive PT(CPT) 77.87 42.80 55.23 
Context-Sensitive CT 78.33 40.84 53.69 
Flattened PT 76.86 45.69 57.31 
Flattened CPT 80.60 41.20 54.53 
 
Table 1. Performance of seven relation feature 
spaces over the 5 ACE major types using parse 
tree information only 
 
Table 1 reports the performance of our defined 
seven relation feature spaces over the 5 ACE major 
types using parse tree information regardless of 
any entity information. This preliminary experi-
ments show that:  
 
 
? Overall the tree kernel over different relation 
feature spaces is effective for relation extraction 
since we use the parse tree information only. We 
will report the detailed performance comparison 
results between our method and previous work 
later in this section. 
? Using the PTs achieves the best performance. 
This means the portion of a parse tree enclosed 
by the shortest path between entities can model 
relations better than other sub-trees. 
? Using the MCTs get the worst performance. 
This is because the MCTs introduce too much 
left and right context information, which may be 
noisy features, as shown in Figure 1. It suggests 
that only allowing complete (not partial) produc-
tion rules in the MCTs does harm performance. 
? The performance of using CTs drops by 5 in F-
measure compared with that of using the PTs. 
This suggests that the middle and high-level 
structures beyond chunking is also very useful 
for relation extraction. 
293
? The context-sensitive trees show lower perform-
ance than the corresponding original PTs and 
CTs. In some cases (e.g. in sentence ?the merge 
of company A and company B?.?, ?merge? is 
the context word), the context information is 
helpful. However the effective scope of context 
is hard to determine. 
? The two flattened trees perform worse than the 
original trees, but better than the corresponding 
context-sensitive trees. This suggests that the 
removed structures by the flattened trees con-
tribute non-trivial performance improvement.  
 
In the above experiments, the path-enclosed tree 
displays the best performance among the seven 
feature spaces when using the parse tree structural 
information only. In the following incremental ex-
periments, we incorporate more features into the 
path-enclosed parse trees and it shows significant 
performance improvement. 
 
Path-enclosed Tree (PT) P R F 
Parse tree structure in-
formation only 
72.77 53.80 61.87 
+Entity information  76.14 62.85 68.86 
+Semantic features 76.32 62.99 69.02 
 
Table 2. Performance of Path-enclosed Trees 
with different setups over the 5 ACE major types 
 
Table 2 reports the performance over the 5 ACE 
major types using Path-enclosed trees enhanced 
with more features in nodes. The 1st row is the 
baseline performance using structural information 
only. We then integrate entity information, includ-
ing Entity type and Mention level features, into the 
corresponding nodes as shown in Figure 1. The 2nd 
row in Table 2 reports the performance of this 
setup. Besides the entity information, we further 
incorporate the semantic features used in Zhou et 
al. (2005) into the corresponding leaf nodes. The 
3rd row in Table 2 reports the performance of this 
setup. Please note that in the 2nd and 3rd setups, we 
still use the same tree kernel function with slight 
modification on the rule (2) in calculating 
1 2( , )n n?  (see subsection 3.2) to make it consider 
more features associated with each individual 
node: 1 2( , )  n n feature weight ?? = ? . From Table 
2, we can see that the basic feature of entity infor-
mation is quite useful, which largely boosts per-
formance by 7 in F-measure. The final 
performance of our tree kernel method for relation 
extraction is 76.32/62.99/69.02 in preci-
sion/recall/F-measure over the 5 ACE major types.   
 
Methods P R F 
Ours: convolution kernel 
over parse trees 
76.32 
(64.6) 
62.99 
(50.76)
69.02 
(56.83)
Kambhatla (2004):  
feature-based ME 
- 
(63.5) 
- 
(45.2) 
- 
(52.8) 
Zhou et al (2005):  
feature-based SVM 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5)
Culotta and Sorensen 
(2004): dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
Bunescu and Mooney 
(2005): shortest path de-
pendency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
 
Table 3. Performance comparison, the numbers in 
parentheses report the performance over the 24 
ACE subtypes while the numbers outside paren-
theses is for the 5 ACE major types 
 
Table 3 compares the performance of different 
methods on the ACE corpus3. It shows that our 
method achieves the best-reported performance on 
both the 24 ACE subtypes and the 5 ACE major 
types. It also shows that our tree kernel method 
significantly outperform the previous two depend-
ency kernel algorithms by 16 in F-measure on the 
5 ACE relation types4. This may be due to two rea-
sons: one reason is that the dependency tree lacks 
the hierarchical syntactic information, and another 
reason is due to the two constraints of the two de-
pendency kernels as discussed in Section 2 and 
Subsection 3.3. The performance improvement by 
our method suggests that the convolution tree ker-
nel can explore the syntactic features (e.g. parse 
tree structures and entity information) very effec-
tively and the syntactic features are also particu-
                                                          
3 Zhao and Grishman (2005) also evaluated their algorithm on 
the ACE corpus and got good performance. But their experi-
mental data is for 2004 evaluation, which defined 7 entity 
types with 44 entity subtypes, and 7 relation major types with 
27 subtypes, so we are not ready to compare with each other. 
4 Bunescu and Mooney (2005) used the ACE 2002 corpus, 
including 422 documents, which is known to have many in-
consistencies than the 2003 version. Culotta and Sorensen 
(2004) used an ACE corpus including about 800 documents, 
and they did not specify the corpus version. Since the testing 
corpora are in different sizes and versions, strictly speaking, it 
is not ready to compare these methods exactly and fairly. Thus 
Table 3 is only for reference purpose. We just hope that we 
can get a few clues from this table. 
294
larly effective for the task of relation extraction. In 
addition, we observe from Table 1 that the feature 
space selection (the effective portion of a parse 
tree) is also critical to relation extraction. 
  
Error Type # of error instance 
False Negative 414 
False Positive 173 
Cross Type 97 
 
Table 4. Error Distribution 
 
Finally, Table 4 reports the error distribution in 
the case of the 3rd experiment in Table 2. It shows 
that 85.9% (587/684) of the errors result from rela-
tion detection and only 14.1% (97/684) of the er-
rors result from relation characterization. This is 
mainly due to the imbalance of the posi-
tive/negative instances and the sparseness of some 
relation types on the ACE corpus. 
5 Conclusion and Future Work 
In this paper, we explore the syntactic features us-
ing convolution tree kernels for relation extraction. 
We conclude that: 1) the relations between entities 
can be well represented by parse trees with care-
fully calibrating effective portions of parse trees; 
2) the syntactic features embedded in a parse tree 
are particularly effective for relation extraction; 3) 
the convolution tree kernel can effectively capture 
the syntactic features for relation extraction. 
The most immediate extension of our work is to 
improve the accuracy of relation detection. We 
may adopt a two-step method (Culotta and Soren-
sen, 2004) to separately model the relation detec-
tion and characterization issues. We may integrate 
more features (such as head words or WordNet 
semantics) into nodes of parse trees. We can also 
benefit from the learning algorithm to study how to 
solve the data imbalance and sparseness issues 
from the learning algorithm viewpoint. In the fu-
ture, we would like to test our algorithm on the 
other version of the ACE corpus and to develop 
fast algorithm (Vishwanathan and Smola, 2002) to 
speed up the training and testing process of convo-
lution kernels.  
 
Acknowledgements: We would like to thank Dr. 
Alessandro Moschitti for his great help in using his 
Tree Kernel Toolkits and fine-tuning the system. 
We also would like to thank the three anonymous 
reviewers for their invaluable suggestions. 
References  
ACE. 2004. The Automatic Content Extraction (ACE) 
Projects. http://www.ldc.upenn.edu/Projects/ACE/ 
Bunescu R. C. and Mooney R. J. 2005. A Shortest Path 
Dependency Kernel for Relation Extraction. 
EMNLP-2005 
Charniak E. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
Collins M. and Duffy N. 2001. Convolution Kernels for 
Natural Language. NIPS-2001 
Culotta A. and Sorensen J. 2004. Dependency Tree Ker-
nel for Relation Extraction. ACL-2004 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
Joachims T. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kambhatla Nanda. 2004. Combining lexical, syntactic 
and semantic features with Maximum Entropy mod-
els for extracting relations. ACL-2004 (poster) 
Lodhi H., Saunders C., Shawe-Taylor J., Cristianini N. 
and Watkins C. 2002. Text classification using string 
kernel. Journal of Machine Learning Research, 
2002(2):419-444 
Miller S., Fox H., Ramshaw L. and Weischedel R. 2000. 
A novel use of statistical parsing to extract informa-
tion from text. NAACL-2000 
Moschitti Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. ACL-2004 
MUC. 1987-1998. The nist MUC website: http: 
//www.itl.nist.gov/iaui/894.02/related_projects/muc/ 
Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003. Hi-
erarchical Directed Acyclic Graph Kernel: Methods 
for Structured Natural Language Data. ACL-2003 
Vishwanathan S.V.N. and Smola A.J. 2002. Fast ker-
nels for String and Tree Matching. NIPS-2002 
Zelenko D., Aone C. and Richardella A. 2003. Kernel 
Methods for Relation Extraction. Journal of Machine 
Learning Research. 2003(2):1083-1106 
Zhao Shubin and Grishman Ralph. 2005. Extracting 
Relations with Integrated Information Using Kernel 
Methods. ACL-2005 
Zhou Guodong, Su Jian, Zhang Jie and Zhang Min. 
2005. Exploring Various Knowledge in Relation Ex-
traction. ACL-2005 
295
Proceedings of the 43rd Annual Meeting of the ACL, pages 427?434,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring Various Knowledge in Relation Extraction 
 
ZHOU GuoDong   SU Jian  ZHANG Jie  ZHANG Min  
Institute for Infocomm research  
21 Heng Mui Keng Terrace, Singapore 119613  
Email: {zhougd, sujian, zhangjie, mzhang}@i2r.a-star.edu.sg  
  
Abstract 
Extracting semantic relationships between en-
tities is challenging. This paper investigates 
the incorporation of diverse lexical, syntactic 
and semantic knowledge in feature-based rela-
tion extraction using SVM. Our study illus-
trates that the base phrase chunking 
information is very effective for relation ex-
traction and contributes to most of the per-
formance improvement from syntactic aspect 
while additional information from full parsing 
gives limited further enhancement. This sug-
gests that most of useful information in full 
parse trees for relation extraction is shallow 
and can be captured by chunking. We also 
demonstrate how semantic information such as 
WordNet and Name List, can be used in fea-
ture-based relation extraction to further im-
prove the performance. Evaluation on the 
ACE corpus shows that effective incorporation 
of diverse features enables our system outper-
form previously best-reported systems on the 
24 ACE relation subtypes and significantly 
outperforms tree kernel-based systems by over 
20 in F-measure on the 5 ACE relation types. 
1 Introduction 
With the dramatic increase in the amount of textual 
information available in digital archives and the 
WWW, there has been growing interest in tech-
niques for automatically extracting information 
from text. Information Extraction (IE) systems are 
expected to identify relevant information (usually 
of pre-defined types) from text documents in a cer-
tain domain and put them in a structured format.  
According to the scope of the NIST Automatic 
Content Extraction (ACE) program, current 
research in IE has three main objectives: Entity 
Detection and Tracking (EDT), Relation Detection 
and Characterization (RDC), and Event Detection 
and Characterization (EDC). The EDT task entails 
the detection of entity mentions and chaining them 
together by identifying their coreference. In ACE 
vocabulary, entities are objects, mentions are 
references to them, and relations are semantic 
relationships between entities. Entities can be of 
five types: persons, organizations, locations, 
facilities and geo-political entities (GPE: 
geographically defined regions that indicate a 
political boundary, e.g. countries, states, cities, 
etc.). Mentions have three levels: names, nomial 
expressions or pronouns. The RDC task detects 
and classifies implicit and explicit relations1 
between entities identified by the EDT task. For 
example, we want to determine whether a person is 
at a location, based on the evidence in the context. 
Extraction of semantic relationships between 
entities can be very useful for applications such as 
question answering, e.g. to answer the query ?Who 
is the president of the United States??.  
This paper focuses on the ACE RDC task and 
employs diverse lexical, syntactic and semantic 
knowledge in feature-based relation extraction 
using Support Vector Machines (SVMs). Our 
study illustrates that the base phrase chunking 
information contributes to most of the performance 
inprovement from syntactic aspect while additional 
full parsing information does not contribute much, 
largely due to the fact that most of relations 
defined in ACE corpus are within a very short 
distance. We also demonstrate how semantic in-
formation such as WordNet (Miller 1990) and 
Name List can be used in the feature-based frame-
work. Evaluation shows that the incorporation of 
diverse features enables our system achieve best 
reported performance. It also shows that our fea-
                                                          
1 In ACE (http://www.ldc.upenn.edu/Projects/ACE), 
explicit relations occur in text with explicit evidence 
suggesting the relationships. Implicit relations need not 
have explicit supporting evidence in text, though they 
should be evident from a reading of the document.  
427
ture-based approach outperforms tree kernel-based 
approaches by 11 F-measure in relation detection 
and more than 20 F-measure in relation detection 
and classification on the 5 ACE relation types.  
The rest of this paper is organized as follows. 
Section 2 presents related work. Section 3 and 
Section 4 describe our approach and various 
features employed respectively. Finally, we present 
experimental setting and  results in Section 5 and 
conclude with some general observations in 
relation extraction in Section 6. 
2 Related Work 
The relation extraction task was formulated at the 
7th Message Understanding Conference (MUC-7 
1998) and is starting to be addressed more and 
more within the natural language processing and 
machine learning communities.  
Miller et al(2000) augmented syntactic full 
parse trees with semantic information correspond-
ing to entities and relations, and built generative 
models for the augmented trees. Zelenko et al
(2003) proposed extracting relations by computing 
kernel functions between parse trees. Culotta et al
(2004) extended this work to estimate kernel func-
tions between augmented dependency trees and 
achieved 63.2 F-measure in relation detection and 
45.8 F-measure in relation detection and classifica-
tion on the 5 ACE relation types. Kambhatla 
(2004) employed Maximum Entropy models for 
relation extraction with features derived from 
word, entity type, mention level, overlap, depend-
ency tree and parse tree. It achieves 52.8 F-
measure on the 24 ACE relation subtypes. Zhang 
(2004) approached relation classification by com-
bining various lexical and syntactic features with 
bootstrapping on top of Support Vector Machines. 
Tree kernel-based approaches proposed by Ze-
lenko et al(2003) and Culotta et al(2004) are able 
to explore the implicit feature space without much 
feature engineering. Yet further research work is 
still expected to make it effective with complicated 
relation extraction tasks such as the one defined in 
ACE. Complicated relation extraction tasks may 
also impose a big challenge to the modeling ap-
proach used by Miller et al(2000) which integrates 
various tasks such as part-of-speech tagging, 
named entity recognition, template element extrac-
tion and relation extraction, in a single model.   
This paper will further explore the feature-based 
approach with a systematic study on the extensive 
incorporation of diverse lexical, syntactic and se-
mantic information. Compared with Kambhatla 
(2004), we separately incorporate the base phrase 
chunking information, which contributes to most 
of the performance improvement from syntactic 
aspect. We also show how semantic information 
like WordNet and Name List can be equipped to 
further improve the performance. Evaluation on 
the ACE corpus shows that our system outper-
forms Kambhatla (2004) by about 3 F-measure on 
extracting 24 ACE relation subtypes. It also shows 
that our system outperforms tree kernel-based sys-
tems (Culotta et al2004) by over 20 F-measure on 
extracting 5 ACE relation types. 
3 Support Vector Machines 
Support Vector Machines (SVMs) are a supervised 
machine learning technique motivated by the sta-
tistical learning theory (Vapnik 1998). Based on 
the structural risk minimization of the statistical 
learning theory, SVMs seek an optimal separating 
hyper-plane to divide the training examples into 
two classes and make decisions based on support 
vectors which are selected as the only effective 
instances in the training set. 
Basically, SVMs are binary classifiers. 
Therefore, we must extend SVMs to multi-class 
(e.g. K) such as the ACE RDC task. For efficiency, 
we apply the one vs. others strategy, which builds 
K classifiers so as to separate one class from all 
others, instead of the pairwise strategy, which 
builds K*(K-1)/2 classifiers considering all pairs of 
classes. The final decision of an instance in the 
multiple binary classification is determined by the 
class which has the maximal SVM output. 
Moreover, we only apply the simple linear kernel, 
although other kernels can peform better.  
The reason why we choose SVMs for this 
purpose is that SVMs represent the state-of?the-art 
in  the machine learning research community, and 
there are good implementations of the algorithm 
available. In this paper, we use the binary-class 
SVMLight2 deleveloped by Joachims (1998). 
                                                          
2 Joachims has just released a new version of SVMLight 
for multi-class classification. However, this paper only 
uses the binary-class version. For details about 
SVMLight, please see http://svmlight.joachims.org/ 
428
4 Features 
The semantic relation is determined between two 
mentions. In addition, we distinguish the argument 
order of the two mentions (M1 for the first mention 
and M2 for the second mention), e.g. M1-Parent-
Of-M2 vs. M2-Parent-Of-M1. For each pair of 
mentions3, we compute various lexical, syntactic 
and semantic features. 
4.1 Words 
According to their positions, four categories of 
words are considered: 1) the words of both the 
mentions, 2) the words between the two mentions, 
3) the words before M1, and 4) the words after M2. 
For the words of both the mentions, we also differ-
entiate the head word4 of a mention from other 
words since the head word is generally much more 
important. The words between the two mentions 
are classified into three bins: the first word in be-
tween, the last word in between and other words in 
between. Both the words before M1 and after M2 
are classified into two bins: the first word next to 
the mention and the second word next to the men-
tion. Since a pronominal mention (especially neu-
tral pronoun such as ?it? and ?its?) contains little 
information about the sense of the mention, the co-
reference chain is used to decide its sense. This is 
done by replacing the pronominal mention with the 
most recent non-pronominal antecedent when de-
termining the word features, which include: 
? WM1: bag-of-words in M1 
? HM1: head word of M1 
                                                          
3 In ACE, each mention has a head annotation and an 
extent annotation. In all our experimentation, we only 
consider the word string between the beginning point of 
the extent annotation and the end point of the head an-
notation. This has an effect of choosing the base phrase 
contained in the extent annotation. In addition, this also 
can reduce noises without losing much of information in 
the mention. For example, in the case where the noun 
phrase ?the former CEO of McDonald? has the head 
annotation of ?CEO? and the extent annotation of ?the 
former CEO of McDonald?, we only consider ?the for-
mer CEO? in this paper. 
4 In this paper, the head word of a mention is normally 
set as the last word of the mention. However, when a 
preposition exists in the mention, its head word is set as 
the last word before the preposition. For example, the 
head word of the name mention ?University of Michi-
gan? is ?University?. 
? WM2: bag-of-words in M2 
? HM2: head word of M2 
? HM12: combination of HM1 and HM2 
? WBNULL: when no word in between 
? WBFL: the only word in between when only 
one word in between 
? WBF: first word in between when at least two 
words in between 
? WBL: last word in between when at least two 
words in between 
? WBO: other words in between except first and 
last words when at least three words in between 
? BM1F: first word before M1 
? BM1L: second word before M1 
? AM2F: first word after M2 
? AM2L: second word after M2 
4.2 Entity Type 
This feature concerns about the entity type of both 
the mentions, which can be PERSON, 
ORGANIZATION, FACILITY, LOCATION and 
Geo-Political Entity or GPE: 
? ET12: combination of mention entity types 
4.3 Mention Level 
This feature considers the entity level of both the 
mentions, which can be NAME, NOMIAL and 
PRONOUN: 
? ML12: combination of mention levels 
4.4 Overlap 
This category of features includes: 
? #MB: number of other mentions in between 
? #WB: number of words in between 
? M1>M2 or M1<M2: flag indicating whether 
M2/M1is included in M1/M2.  
Normally, the above overlap features are too 
general to be effective alone. Therefore, they are 
also combined with other features: 1) 
ET12+M1>M2; 2) ET12+M1<M2; 3) 
HM12+M1>M2; 4) HM12+M1<M2. 
4.5 Base Phrase Chunking 
It is well known that chunking plays a critical role 
in the Template Relation task of the 7th Message 
Understanding Conference (MUC-7 1998). The 
related work mentioned in Section 2 extended to 
explore the information embedded in the full parse 
trees. In this paper, we separate the features of base 
429
phrase chunking from those of full parsing. In this 
way, we can separately evaluate the contributions 
of base phrase chunking and full parsing. Here, the 
base phrase chunks are derived from full parse 
trees using the Perl script5 written by Sabine 
Buchholz from Tilburg University and the Collins? 
parser (Collins 1999) is employed for full parsing. 
Most of the chunking features concern about the 
head words of the phrases between the two men-
tions. Similar to word features, three categories of 
phrase heads are considered: 1) the phrase heads in 
between are also classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between; 2) the 
phrase heads before M1 are classified into two 
bins: the first phrase head before and the second 
phrase head before; 3) the phrase heads after M2 
are classified into two bins: the first phrase head 
after and the second phrase head after. Moreover, 
we also consider the phrase path in between. 
? CPHBNULL when no phrase in between 
? CPHBFL: the only phrase head when only one 
phrase in between 
? CPHBF: first phrase head in between when at 
least two phrases in between 
? CPHBL: last phrase head in between when at 
least two phrase heads in between 
? CPHBO: other phrase heads in between except 
first and last phrase heads when at least three 
phrases in between 
? CPHBM1F: first phrase head before M1 
? CPHBM1L: second phrase head before M1 
? CPHAM2F: first phrase head after M2 
? CPHAM2F: second phrase head after M2 
? CPP: path of phrase labels connecting the two 
mentions in the chunking  
? CPPH: path of phrase labels connecting the two 
mentions in the chunking augmented with head 
words, if at most two phrases in between 
4.6 Dependency Tree 
This category of features includes information 
about the words, part-of-speeches and phrase la-
bels of the words on which the mentions are de-
pendent in the dependency tree derived from the 
syntactic full parse tree. The dependency tree is 
built by using the phrase head information returned 
by the Collins? parser and linking all the other 
                                                          
5 http://ilk.kub.nl/~sabine/chunklink/ 
fragments in a phrase to its head. It also includes 
flags indicating whether the two mentions are in 
the same NP/PP/VP. 
? ET1DW1: combination of the entity type and 
the dependent word for M1 
? H1DW1: combination of the head word and the 
dependent word for M1 
? ET2DW2: combination of the entity type and 
the dependent word for M2 
? H2DW2: combination of the head word and the 
dependent word for M2 
? ET12SameNP: combination of ET12 and 
whether M1 and M2 included in the same NP 
? ET12SamePP: combination of ET12 and 
whether M1 and M2 exist in the same PP 
? ET12SameVP: combination of ET12 and 
whether M1 and M2 included in the same VP 
4.7 Parse Tree 
This category of features concerns about the in-
formation inherent only in the full parse tree.  
? PTP: path of phrase labels (removing dupli-
cates) connecting M1 and M2 in the parse tree  
? PTPH: path of phrase labels (removing dupli-
cates) connecting M1 and M2 in the parse tree 
augmented with the head word of the top phrase 
in the path.  
4.8 Semantic Resources 
Semantic information from various resources, such 
as WordNet, is used to classify important words 
into different semantic lists according to their indi-
cating relationships. 
Country Name List 
This is to differentiate the relation subtype 
?ROLE.Citizen-Of?, which defines the relationship 
between a person and the country of the person?s 
citizenship, from other subtypes, especially 
?ROLE.Residence?, where defines the relationship 
between a person and the location in which the 
person lives. Two features are defined to include 
this information: 
? ET1Country: the entity type of M1 when M2 is 
a country name 
? CountryET2: the entity type of M2 when M1 is 
a country name 
 
 
430
Personal Relative Trigger Word List 
This is used to differentiate the six personal social 
relation subtypes in ACE: Parent, Grandparent, 
Spouse, Sibling, Other-Relative and Other-
Personal. This trigger word list is first gathered 
from WordNet by checking whether a word has the 
semantic class ?person|?|relative?. Then, all the 
trigger words are semi-automatically6 classified 
into different categories according to their related 
personal social relation subtypes. We also extend 
the list by collecting the trigger words from the 
head words of the mentions in the training data 
according to their indicating relationships. Two 
features are defined to include this information: 
? ET1SC2: combination of the entity type of M1 
and the semantic class of M2 when M2 triggers 
a personal social subtype. 
? SC1ET2: combination of the entity type of M2 
and the semantic class of M1 when the first 
mention triggers a personal social subtype. 
5 Experimentation 
This paper uses the ACE corpus provided by LDC 
to train and evaluate our feature-based relation ex-
traction system. The ACE corpus is gathered from 
various newspapers, newswire and broadcasts. In 
this paper, we only model explicit relations be-
cause of poor inter-annotator agreement in the an-
notation of implicit relations and their limited 
number. 
5.1 Experimental Setting 
We use the official ACE corpus from LDC. The 
training set consists of 674 annotated text docu-
ments (~300k words) and 9683 instances of rela-
tions. During development, 155 of 674 documents 
in the training set are set aside for fine-tuning the 
system. The testing set is held out only for final 
evaluation. It consists of 97 documents (~50k 
words) and 1386 instances of relations. Table 1 
lists the types and subtypes of relations for the 
ACE Relation Detection and Characterization 
(RDC) task, along with their frequency of occur-
rence in the ACE training set. It shows that the 
                                                          
6 Those words that have the semantic classes ?Parent?, 
?GrandParent?, ?Spouse? and ?Sibling? are automati-
cally set with the same classes without change. How-
ever, The remaining words that do not have above four 
classes are manually classified. 
ACE corpus suffers from a small amount of anno-
tated data for a few subtypes such as the subtype 
?Founder? under the type ?ROLE?. It also shows 
that the ACE RDC task defines some difficult sub-
types such as the subtypes ?Based-In?, ?Located? 
and ?Residence? under the type ?AT?, which are 
difficult even for human experts to differentiate.  
Type Subtype Freq 
AT(2781) Based-In 347 
 Located 2126 
 Residence 308 
NEAR(201) Relative-Location 201 
PART(1298) Part-Of 947 
 Subsidiary 355 
 Other 6 
ROLE(4756) Affiliate-Partner 204 
 Citizen-Of 328 
 Client 144 
 Founder 26 
 General-Staff 1331 
 Management 1242 
 Member 1091 
 Owner 232 
 Other 158 
SOCIAL(827) Associate 91 
 Grandparent 12 
 Other-Personal 85 
 Other-Professional 339 
 Other-Relative 78 
 Parent 127 
 Sibling 18 
 Spouse 77 
Table 1: Relation types and subtypes in the ACE 
training data 
In this paper, we explicitly model the argument 
order of the two mentions involved. For example, 
when comparing mentions m1 and m2, we distin-
guish between m1-ROLE.Citizen-Of-m2 and m2-
ROLE.Citizen-Of-m1. Note that only 6 of these 24 
relation subtypes are symmetric: ?Relative-
Location?, ?Associate?, ?Other-Relative?, ?Other-
Professional?, ?Sibling?, and ?Spouse?. In this 
way, we model relation extraction as a multi-class 
classification problem with 43 classes, two for 
each relation subtype (except the above 6 symmet-
ric subtypes) and a ?NONE? class for the case 
where the two mentions are not related. 
5.2 Experimental Results 
In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of coreference (i.e. as annotated by the 
corpus annotators) in the ACE corpus. Table 2 
measures the performance of our relation extrac-
431
tion system over the 43 ACE relation subtypes on 
the testing set. It shows that our system achieves 
best performance of 63.1%/49.5%/ 55.5 in preci-
sion/recall/F-measure when combining diverse 
lexical, syntactic and semantic features. Table 2 
also measures the contributions of different fea-
tures by gradually increasing the feature set. It 
shows that: 
Features P R F 
Words 69.2 23.7 35.3 
+Entity Type 67.1 32.1 43.4 
+Mention Level 67.1 33.0 44.2 
+Overlap 57.4 40.9 47.8 
+Chunking 61.5 46.5 53.0 
+Dependency Tree 62.1 47.2 53.6 
+Parse Tree 62.3 47.6 54.0 
+Semantic Resources 63.1 49.5 55.5 
Table 2: Contribution of different features over 43 
relation subtypes in the test data 
? Using word features only achieves the perform-
ance of 69.2%/23.7%/35.3 in precision/recall/F-
measure.  
? Entity type features are very useful and improve 
the F-measure by 8.1 largely due to the recall 
increase. 
? The usefulness of mention level features is quite 
limited. It only improves the F-measure by 0.8 
due to the recall increase. 
? Incorporating the overlap features gives some 
balance between precision and recall. It in-
creases the F-measure by 3.6 with a big preci-
sion decrease and a big recall increase. 
? Chunking features are very useful. It increases 
the precision/recall/F-measure by 4.1%/5.6%/ 
5.2 respectively. 
? To our surprise, incorporating the dependency 
tree and parse tree features only improve the F-
measure by 0.6 and 0.4 respectively. This may 
be due to the fact that most of relations in the 
ACE corpus are quite local. Table 3 shows that 
about 70% of relations exist where two men-
tions are embedded in each other or separated 
by at most one word. While short-distance rela-
tions dominate and can be resolved by above 
simple features, the dependency tree and parse 
tree features can only take effect in the remain-
ing much less long-distance relations. However, 
full parsing is always prone to long distance er-
rors although the Collins? parser used in our 
system represents the state-of-the-art in full 
parsing. 
? Incorporating semantic resources such as the 
country name list and the personal relative trig-
ger word list further increases the F-measure by 
1.5 largely due to the differentiation of the rela-
tion subtype ?ROLE.Citizen-Of? from ?ROLE. 
Residence? by distinguishing country GPEs 
from other GPEs. The effect of personal relative 
trigger words is very limited due to the limited 
number of testing instances over personal social 
relation subtypes. 
Table 4 separately measures the performance of 
different relation types and major subtypes. It also 
indicates the number of testing instances, the num-
ber of correctly classified instances and the number 
of wrongly classified instances for each type or 
subtype. It is not surprising that the performance 
on the relation type ?NEAR? is low because it oc-
curs rarely in both the training and testing data. 
Others like ?PART.Subsidary? and ?SOCIAL. 
Other-Professional? also suffer from their low oc-
currences. It also shows that our system performs 
best on the subtype ?SOCIAL.Parent? and ?ROLE. 
Citizen-Of?. This is largely due to incorporation of 
two semantic resources, i.e. the country name list 
and the personal relative trigger word list. Table 4 
also indicates the low performance on the relation 
type ?AT? although it frequently occurs in both the 
training and testing data. This suggests the diffi-
culty of detecting and classifying the relation type 
?AT? and its subtypes. 
Table 5 separates the performance of relation 
detection from overall performance on the testing 
set. It shows that our system achieves the perform-
ance of 84.8%/66.7%/74.7 in precision/recall/F-
measure on relation detection. It also shows that 
our system achieves overall performance of 
77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in preci-
sion/recall/F-measure on the 5 ACE relation types 
and the best-reported systems on the ACE corpus. 
It shows that our system achieves better perform-
ance by ~3 F-measure largely due to its gain in 
recall. It also shows that feature-based methods 
dramatically outperform kernel methods. This sug-
gests that feature-based methods can effectively 
combine different features from a variety of 
sources (e.g. WordNet and gazetteers) that can be 
brought to bear on relation extraction. The tree 
kernels developed in Culotta et al(2004) are yet to 
be effective on the ACE RDC task. 
Finally, Table 6 shows the distributions of er-
rors. It shows that 73% (627/864) of errors results 
432
from relation detection and 27% (237/864) of er-
rors results from relation characterization, among 
which 17.8% (154/864) of errors are from misclas-
sification across relation types and 9.6% (83/864) 
of errors are from misclassification of relation sub-
types inside the same relation types. This suggests 
that relation detection is critical for relation extrac-
tion. 
# of other mentions in between # of relations 
0 1 2 3 >=4 Overall 
0 3991 161 11 0 0 4163 
1 2350 315 26 2 0 2693 
2 465 95 7 2 0 569 
3 311 234 14 0 0 559 
4 204 225 29 2 3 463 
5 111 113 38 2 1 265 
>=6 262 297 277 148 134 1118 
#  
of  
the words 
 in  
between 
Overall 7694 1440 402 156 138 9830 
Table 3: Distribution of relations over #words and #other mentions in between in the training data 
Type Subtype #Testing Instances #Correct #Error P R F 
AT  392 224 105 68.1 57.1 62.1 
 Based-In 85 39 10 79.6 45.9 58.2 
 Located 241 132 120 52.4 54.8 53.5 
 Residence 66 19 9 67.9 28.8 40.4 
NEAR  35 8 1 88.9 22.9 36.4 
 Relative-Location 35 8 1 88.9 22.9 36.4 
PART  164 106 39 73.1 64.6 68.6 
 Part-Of 136 76 32 70.4 55.9 62.3 
 Subsidiary 27 14 23 37.8 51.9 43.8 
ROLE  699 443 82 84.4 63.4 72.4 
 Citizen-Of 36 25 8 75.8 69.4 72.6 
 General-Staff 201 108 46 71.1 53.7 62.3 
 Management 165 106 72 59.6 64.2 61.8 
 Member 224 104 36 74.3 46.4 57.1 
SOCIAL  95 60 21 74.1 63.2 68.5 
 Other-Professional 29 16 32 33.3 55.2 41.6 
 Parent 25 17 0 100 68.0 81.0 
Table 4: Performance of different relation types and major subtypes in the test data 
Relation Detection RDC on Types RDC on Subtypes System 
P R F P R F P R F 
Ours: feature-based 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5 
Kambhatla (2004):feature-based - - - - - - 63.5 45.2 52.8 
Culotta et al(2004):tree kernel 81.2 51.8 63.2 67.1 35.0 45.8 - - - 
Table 5: Comparison of our system with other best-reported systems on the ACE corpus 
Error Type #Errors 
False Negative 462 Detection Error 
False Positive 165 
Cross Type Error 154 Characterization  
Error Inside Type Error 83 
Table 6: Distribution of errors 
6 Discussion and Conclusion 
In this paper, we have presented a feature-based 
approach for relation extraction where diverse 
lexical, syntactic and semantic knowledge are em-
ployed. Instead of exploring the full parse tree in-
formation directly as previous related work, we 
incorporate the base phrase chunking information 
first. Evaluation on the ACE corpus shows that 
base phrase chunking contributes to most of the 
performance improvement from syntactic aspect 
while further incorporation of the parse tree and 
dependence tree information only slightly im-
proves the performance. This may be due to three 
reasons: First, most of relations defined in ACE 
have two mentions being close to each other. 
While short-distance relations dominate and can be 
resolved by simple features such as word and 
chunking features, the further dependency tree and 
parse tree features can only take effect in the re-
maining much less and more difficult long-distance 
relations. Second, it is well known that full parsing 
433
is always prone to long-distance parsing errors al-
though the Collins? parser used in our system 
achieves the state-of-the-art performance. There-
fore, the state-of-art full parsing still needs to be 
further enhanced to provide accurate enough in-
formation, especially PP (Preposition Phrase) at-
tachment. Last, effective ways need to be explored 
to incorporate information embedded in the full 
parse trees. Besides, we also demonstrate how se-
mantic information such as WordNet and Name 
List, can be used in feature-based relation extrac-
tion to further improve the performance. 
The effective incorporation of diverse features 
enables our system outperform previously best-
reported systems on the ACE corpus. Although 
tree kernel-based approaches facilitate the explora-
tion of the implicit feature space with the parse tree 
structure, yet the current technologies are expected 
to be further advanced to be effective for relatively 
complicated relation extraction tasks such as the 
one defined in ACE where 5 types and 24 subtypes 
need to be extracted. Evaluation on the ACE RDC 
task shows that our approach of combining various 
kinds of evidence can scale better to problems, 
where we have a lot of relation types with a rela-
tively small amount of annotated data. The ex-
periment result also shows that our feature-based 
approach outperforms the tree kernel-based ap-
proaches by more than 20 F-measure on the extrac-
tion of 5 ACE relation types.  
In the future work, we will focus on exploring 
more semantic knowledge in relation extraction, 
which has not been covered by current research. 
Moreover, our current work is done when the En-
tity Detection and Tracking (EDT) has been per-
fectly done. Therefore, it would be interesting to 
see how imperfect EDT affects the performance in 
relation extraction. 
References  
Agichtein E. and Gravano L. (2000). Snowball: Extract-
ing relations from large plain text collections. In Pro-
ceedings of 5th ACM International Conference on 
Digital Libraries. 4-7 June 2000. San Antonio, TX. 
Brin S. (1998). Extracting patterns and relations from 
the World Wide Web. In Proceedings of WebDB 
workshop at 6th International Conference on Extend-
ing DataBase Technology (EDBT?1998).23-27 
March 1998, Valencia, Spain 
Collins M. (1999).  Head-driven statistical models for 
natural language parsing. Ph.D. Dissertation, Univer-
sity of Pennsylvania. 
Collins M. and Duffy N. (2002). Covolution kernels for 
natural language. In Dietterich T.G., Becker S. and 
Ghahramani Z. editors. Advances in Neural Informa-
tion Processing Systems 14. Cambridge, MA.  
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. In Proceedings of 42th 
Annual Meeting of the Association for Computational 
Linguistics. 21-26 July 2004. Barcelona, Spain 
Cumby C.M. and Roth D. (2003). On kernel methods 
for relation learning. In Fawcett T. and Mishra N. 
editors. In Proceedings of 20th International Confer-
ence on Machine Learning (ICML?2003). 21-24 Aug 
2003. Washington D.C. USA. AAAI Press. 
Haussler D. (1999). Covention kernels on discrete struc-
tures. Technical Report UCS-CRL-99-10. University 
of California, Santa Cruz. 
Joachims T. (1998). Text categorization with Support 
Vector Machines: Learning with many relevant fea-
tures. In Proceedings of European Conference on 
Machine Learning(ECML?1998).  21-23 April 1998. 
Chemnitz, Germany 
Miller G.A. (1990). WordNet: An online lexical data-
base. International Journal of Lexicography. 
3(4):235-312. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to extract 
information from text. In Proceedings of 6th Applied 
Natural Language Processing Conference. 29 April  
- 4 May 2000, Seattle, USA 
MUC-7. (1998). Proceedings of the 7th Message Under-
standing Conference (MUC-7). Morgan Kaufmann, 
San Mateo, CA. 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. In Proceedings of 42th Annual 
Meeting of the Association for Computational Lin-
guistics. 21-26 July 2004. Barcelona, Spain. 
Roth D. and Yih W.T. (2002). Probabilistic reasoning 
for entities and relation recognition. In Proceedings 
of 19th International Conference on Computational 
Linguistics(CoLING?2002). Taiwan. 
Vapnik V. (1998). Statistical Learning Theory. Whiley, 
Chichester, GB. 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. pp1083-1106. 
Zhang Z. (2004). Weekly-supervised relation classifica-
tion for Information Extraction. In Proceedings of 
ACM 13th Conference on Information and Knowl-
edge Management (CIKM?2004). 8-13 Nov 2004. 
Washington D.C., USA. 
434
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825?832,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Composite Kernel to Extract Relations between Entities with 
both Flat and Structured Features 
Min Zhang         Jie Zhang       Jian Su      Guodong Zhou 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, zhangjie, sujian, zhougd}@i2r.a-star.edu.sg 
 
Abstract 
This paper proposes a novel composite ker-
nel for relation extraction. The composite 
kernel consists of two individual kernels: an 
entity kernel that allows for entity-related 
features and a convolution parse tree kernel 
that models syntactic information of relation 
examples. The motivation of our method is 
to fully utilize the nice properties of kernel 
methods to explore diverse knowledge for 
relation extraction. Our study illustrates that 
the composite kernel can effectively capture 
both flat and structured features without the 
need for extensive feature engineering, and 
can also easily scale to include more fea-
tures. Evaluation on the ACE corpus shows 
that our method outperforms the previous 
best-reported methods and significantly out-
performs previous two dependency tree ker-
nels for relation extraction. 
1 Introduction 
The goal of relation extraction is to find various 
predefined semantic relations between pairs of 
entities in text. The research on relation extrac-
tion has been promoted by the Message Under-
standing Conferences (MUCs) (MUC, 1987-
1998) and Automatic Content Extraction (ACE) 
program (ACE, 2002-2005). According to the 
ACE Program, an entity is an object or set of ob-
jects in the world and a relation is an explicitly 
or implicitly stated relationship among entities. 
For example, the sentence ?Bill Gates is chair-
man and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities 
?Bill Gates? (PERSON.Name) and ?Microsoft 
Corporation? (ORGANIZATION. Commercial).  
In this paper, we address the problem of rela-
tion extraction using kernel methods (Sch?lkopf 
and Smola, 2001). Many feature-based learning 
algorithms involve only the dot-product between 
feature vectors. Kernel methods can be regarded 
as a generalization of the feature-based methods 
by replacing the dot-product with a kernel func-
tion between two vectors, or even between two 
objects. A kernel function is a similarity function 
satisfying the properties of being symmetric and 
positive-definite. Recently, kernel methods are 
attracting more interests in the NLP study due to 
their ability of implicitly exploring huge amounts 
of structured features using the original represen-
tation of objects. For example, the kernels for 
structured natural language data, such as parse 
tree kernel (Collins and Duffy, 2001), string ker-
nel (Lodhi et al, 2002) and graph kernel (Suzuki 
et al, 2003) are example instances of the well-
known convolution kernels1 in NLP. In relation 
extraction, typical work on kernel methods in-
cludes: Zelenko et al (2003), Culotta and Soren-
sen (2004) and Bunescu and Mooney (2005). 
This paper presents a novel composite kernel 
to explore diverse knowledge for relation extrac-
tion. The composite kernel consists of an entity 
kernel and a convolution parse tree kernel. Our 
study demonstrates that the composite kernel is 
very effective for relation extraction. It also 
shows without the need for extensive feature en-
gineering the composite kernel can not only cap-
ture most of the flat features used in the previous 
work but also exploit the useful syntactic struc-
ture features effectively. An advantage of our 
method is that the composite kernel can easily 
cover more knowledge by introducing more ker-
nels. Evaluation on the ACE corpus shows that 
our method outperforms the previous best-
reported methods and significantly outperforms 
the previous kernel methods due to its effective 
exploration of various syntactic features. 
The rest of the paper is organized as follows. 
In Section 2, we review the previous work. Sec-
tion 3 discusses our composite kernel. Section 4 
reports the experimental results and our observa-
tions. Section 5 compares our method with the 
                                                 
1 Convolution kernels were proposed for a discrete structure 
by Haussler (1999) in the machine learning field. This 
framework defines a kernel between input objects by apply-
ing convolution ?sub-kernels? that are the kernels for the 
decompositions (parts) of the objects.  
825
previous work from the viewpoint of feature ex-
ploration. We conclude our work and indicate the 
future work in Section 6. 
2 Related Work 
Many techniques on relation extraction, such as 
rule-based (MUC, 1987-1998; Miller et al, 
2000), feature-based (Kambhatla 2004; Zhou et 
al., 2005) and kernel-based (Zelenko et al, 2003; 
Culotta and Sorensen, 2004; Bunescu and 
Mooney, 2005), have been proposed in the litera-
ture. 
Rule-based methods for this task employ a 
number of linguistic rules to capture various rela-
tion patterns. Miller et al (2000) addressed the 
task from the syntactic parsing viewpoint and 
integrated various tasks such as POS tagging, NE 
tagging, syntactic parsing, template extraction 
and relation extraction using a generative model. 
Feature-based methods (Kambhatla, 2004; 
Zhou et al, 2005; Zhao and Grishman, 20052) 
for this task employ a large amount of diverse 
linguistic features, such as lexical, syntactic and 
semantic features. These methods are very effec-
tive for relation extraction and show the best-
reported performance on the ACE corpus. How-
ever, the problems are that these diverse features 
have to be manually calibrated and the hierarchi-
cal structured information in a parse tree is not 
well preserved in their parse tree-related features, 
which only represent simple flat path informa-
tion connecting two entities in the parse tree 
through a path of non-terminals and a list of base 
phrase chunks. 
Prior kernel-based methods for this task focus 
on using individual tree kernels to exploit tree 
structure-related features. Zelenko et al (2003) 
developed a kernel over parse trees for relation 
extraction. The kernel matches nodes from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Culotta and Sorensen (2004) gen-
eralized it to estimate similarity between depend-
ency trees. Their tree kernels require the match-
able nodes to be at the same layer counting from 
the root and to have an identical path of ascend-
ing nodes from the roots to the current nodes. 
The two constraints make their kernel high preci-
sion but very low recall on the ACE 2003 corpus. 
Bunescu and Mooney (2005) proposed another 
dependency tree kernel for relation extraction. 
                                                 
2 We classify the feature-based kernel defined in (Zhao and 
Grishman, 2005) into the feature-based methods since their 
kernels can be easily represented by the dot-products be-
tween explicit feature vectors. 
Their kernel simply counts the number of com-
mon word classes at each position in the shortest 
paths between two entities in dependency trees. 
The kernel requires the two paths to have the 
same length; otherwise the kernel value is zero. 
Therefore, although this kernel shows perform-
ance improvement over the previous one (Culotta 
and Sorensen, 2004), the constraint makes the 
two dependency kernels share the similar behav-
ior: good precision but much lower recall on the 
ACE corpus. 
The above discussion shows that, although 
kernel methods can explore the huge amounts of 
implicit (structured) features, until now the fea-
ture-based methods enjoy more success. One 
may ask: how can we make full use of the nice 
properties of kernel methods and define an effec-
tive kernel for relation extraction? 
In this paper, we study how relation extraction 
can benefit from the elegant properties of kernel 
methods: 1) implicitly exploring (structured) fea-
tures in a high dimensional space; and 2) the nice 
mathematical properties, for example, the sum, 
product, normalization and polynomial expan-
sion of existing kernels is a valid kernel 
(Sch?lkopf and Smola, 2001). We also demon-
strate how our composite kernel effectively cap-
tures the diverse knowledge for relation extrac-
tion.  
3 Composite Kernel for Relation Ex-
traction  
In this section, we define the composite kernel 
and study the effective representation of a rela-
tion instance. 
3.1 Composite Kernel 
Our composite kernel consists of an entity kernel 
and a convolution parse tree kernel. To our 
knowledge, convolution kernels have not been 
explored for relation extraction. 
 
(1) Entity Kernel: The ACE 2003 data defines 
four entity features: entity headword, entity type 
and subtype (only for GPE), and mention type 
while the ACE 2004 data makes some modifica-
tions and introduces a new feature ?LDC men-
tion type?. Our statistics on the ACE data reveals 
that the entity features impose a strong constraint 
on relation types. Therefore, we design a linear 
kernel to explicitly capture such features: 
1 2 1 21,2
( , ) ( . , . )L E i iiK R R K R E R E== ?  (1) 
where 1R and 2R stands for two relation instances, 
Ei means the ith entity of a relation instance, and 
826
( , )EK ? ?  is a simple kernel function over the fea-
tures of entities: 
1 2 1 2( , ) ( . , . )E i iiK E E C E f E f=? (2) 
where if represents the i
th entity feature, and the 
function ( , )C ? ?  returns 1 if the two feature val-
ues are identical and 0 otherwise. ( , )EK ? ?  re-
turns the number of feature values in common of 
two entities. 
 
(2) Convolution Parse Tree Kernel: A convo-
lution kernel aims to capture structured informa-
tion in terms of substructures. Here we use the 
same convolution parse tree kernel as described 
in Collins and Duffy (2001) for syntactic parsing 
and Moschitti (2004) for semantic role labeling. 
Generally, we can represent a parse tree T  by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
 
( )T? = (# subtree1(T), ?, # subtreei(T), ?,  # 
subtreen(T) ) 
 
where # subtreei(T) is the occurrence number of 
the ith sub-tree type (subtreei) in T. Since the 
number of different sub-trees is exponential with 
the parse tree size, it is computationally infeasi-
ble to directly use the feature vector ( )T? . To 
solve this computational issue, Collins and Duffy 
(2001) proposed the following parse tree kernel 
to calculate the dot product between the above 
high dimensional vectors implicitly. 
 
 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
1 2
( , ) ( ), ( )
   # ( ) # ( )
   ( ) ( )
   ( , )
( ) ( )
i i
i ii
subtree subtreei n N n N
n N n N
K T T T T
subtree T subtree T
I n I n
n n
? ?
? ?
? ?
=< >
=
=
= ?
?
?
?
? ? ?
? ?
(3) 
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )
isubtree
I n  is a function 
that is 1 iff the subtreei occurs with root at node n 
and zero otherwise, and 1 2( , )n n?  is the number of 
the common subtrees rooted at n1 and n2, i.e. 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
 
1 2( , )n n? can be computed by the following recur-
sive rules:  
(1) if the productions (CFP rules) at 1n  and 2n  
are different, 1 2( , ) 0n n? = ; 
(2) else if both 1n  and 2n  are pre-terminals (POS 
tags), 1 2( , ) 1n n ?? = ? ; 
(3) else, 1( )1 2 1 21( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j? =? = +?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and? (0<? <1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes. In ad-
dition, the recursive rule (3) holds because given 
two nodes with the same children, one can con-
struct common sub-trees using these children and 
common sub-trees of further offspring.  
The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two relation instances. The 
time complexity for computing this kernel 
is 1 2(| | | |)O N N? . 
In this paper, two composite kernels are de-
fined by combing the above two individual ker-
nels in the following ways: 
 
1) Linear combination: 
 
1 1 2 1 2 1 2
? ?( , ) ( , ) (1 ) ( , )LK R R K R R K T T? ?? ?= + ? (4) 
 
Here, ? ( , )K ? ?  is the normalized3 ( , )K ? ? and ?  
is the coefficient. Evaluation on the development 
set shows that this composite kernel yields the 
best performance when ? is set to 0.4. 
 
2) Polynomial expansion: 
 
2 1 2 1 2 1 2
? ?( , ) ( , ) (1 ) ( , )PLK R R K R R K T T? ?? ?= + ? (5) 
 
Here, ? ( , )K ? ?  is the normalized ( , )K ? ? , ( , )pK ? ?  
is the polynomial expansion of ( , )K ? ?  with de-
gree d=2, i.e. 2( , ) ( ( , ) 1)pK K? ? ? ?= + , and ?  is the 
coefficient. Evaluation on the development set 
shows that this composite kernel yields the best 
performance when ? is set to 0.23. 
The polynomial expansion aims to explore the 
entity bi-gram features, esp. the combined fea-
tures from the first and second entities, respec-
tively. In addition, due to the different scales of 
the values of the two individual kernels, they are 
normalized before combination. This can avoid 
one kernel value being overwhelmed by that of 
another one.  
The entity kernel formulated by eqn. (1) is a 
proper kernel since it simply calculates the dot 
product of the entity feature vectors. The tree 
kernel formulated by eqn. (3) is proven to be a 
proper kernel (Collins and Duffy, 2001). Since 
kernel function set is closed under normalization, 
polynomial expansion and linear combination 
(Sch?lkopf and Smola, 2001), the two composite 
kernels are also proper kernels. 
                                                 
3  A kernel ( , )K x y  can be normalized by dividing it by 
( , ) ( , )K x x K y y? .  
827
3.2 Relation Instance Spaces 
A relation instance is encapsulated by a parse 
tree. Thus, it is critical to understand which por-
tion of a parse tree is important in the kernel cal-
culation. We study five cases as shown in Fig.1. 
 
(1) Minimum Complete Tree (MCT): the com-
plete sub-tree rooted by the nearest common an-
cestor of the two entities under consideration. 
 
(2) Path-enclosed Tree (PT): the smallest com-
mon sub-tree including the two entities. In other 
words, the sub-tree is enclosed by the shortest 
path linking the two entities in the parse tree (this 
path is also commonly-used as the path tree fea-
ture in the feature-based methods). 
 
(3) Context-Sensitive Path Tree (CPT): the PT 
extended with the 1st left word of entity 1 and the 
1st right word of entity 2. 
 
(4) Flattened Path-enclosed Tree (FPT): the 
PT with the single in and out arcs of non-
terminal nodes (except POS nodes) removed. 
 
(5) Flattened CPT (FCPT): the CPT with the 
single in and out arcs of non-terminal nodes (ex-
cept POS nodes) removed.  
 
Fig. 1 illustrates different representations of an 
example relation instance. T1 is MCT for the 
relation instance, where the sub-tree circled by a 
dashed line is PT, which is also shown in T2 for 
clarity. The only difference between MCT and 
PT lies in that MCT does not allow partial pro-
duction rules (for example, NP?PP is a partial 
production rule while NP?NP+PP is an entire 
production rule in the top of T2). For instance, 
only the most-right child in the most-left sub-tree 
[NP [CD 200] [JJ domestic] [E1-PER ?]] of T1 
is kept in T2. By comparing the performance of 
T1 and T2, we can evaluate the effect of sub-trees 
with partial production rules as shown in T2 and 
the necessity of keeping the whole left and right 
context sub-trees as shown in T1 in relation ex-
traction. T3 is CPT, where the two sub-trees cir-
cled by dashed lines are included as the context 
to T2 and make T3 context-sensitive. This is to 
evaluate whether the limited context information 
in CPT can boost performance. FPT in T4 is 
formed by removing the two circled nodes in T2. 
This is to study whether and how the elimination 
of single non-terminal nodes affects the perform-
ance of relation extraction.  
T1): MCT T2): PT 
T3):CPT T4): FPT 
Figure 1. Different representations of a relation instance in the example sentence ??provide bene-
fits to 200 domestic partners of their own workers in New York?, where the phrase type 
?E1-PER? denotes that the current node is the 1st entity with type ?PERSON?, and like-
wise for the others. The relation instance is excerpted from the ACE 2003 corpus, where 
a relation ?SOCIAL.Other-Personal? exists between entities ?partners? (PER) and 
?workers? (PER). We use Charniak?s parser (Charniak, 2001) to parse the example sen-
tence. To save space, the FCPT is not shown here. 828
4 Experiments 
4.1 Experimental Setting 
Data: We use the English portion of both the 
ACE 2003 and 2004 corpora from LDC in our 
experiments. In the ACE 2003 data, the training 
set consists of 674 documents and 9683 relation 
instances while the test set consists of 97 docu-
ments and 1386 relation instances. The ACE 
2003 data defines 5 entity types, 5 major relation 
types and 24 relation subtypes. The ACE 2004 
data contains 451 documents and 5702 relation 
instances. It redefines 7 entity types, 7 major re-
lation types and 23 subtypes. Since Zhao and 
Grishman (2005) use a 5-fold cross-validation on 
a subset of the 2004 data (newswire and broad-
cast news domains, containing 348 documents 
and 4400 relation instances), for comparison, we 
use the same setting (5-fold cross-validation on 
the same subset of the 2004 data, but the 5 parti-
tions may not be the same) for the ACE 2004 
data. Both corpora are parsed using Charniak?s 
parser (Charniak, 2001). We iterate over all pairs 
of entity mentions occurring in the same sen-
tence to generate potential relation instances. In 
this paper, we only measure the performance of 
relation extraction models on ?true? mentions 
with ?true? chaining of coreference (i.e. as anno-
tated by LDC annotators). 
 
Implementation: We formalize relation extrac-
tion as a multi-class classification problem. SVM 
is selected as our classifier. We adopt the one vs. 
others strategy and select the one with the largest 
margin as the final answer. The training parame-
ters are chosen using cross-validation (C=2.4 
(SVM); ? =0.4(tree kernel)). In our implementa-
tion, we use the binary SVMLight (Joachims, 
1998) and Tree Kernel Tools (Moschitti, 2004). 
Precision (P), Recall (R) and F-measure (F) are 
adopted to measure the performance. 
4.2 Experimental Results 
In this subsection, we report the experiments of 
different kernel setups for different purposes. 
 
(1) Tree Kernel only over Different Relation 
Instance Spaces: In order to better study the im-
pact of the syntactic structure information in a 
parse tree on relation extraction, we remove the 
entity-related information from parse trees by 
replacing the entity-related phrase types (?E1-
PER? and so on as shown in Fig. 1) with ?NP?. 
Table 1 compares the performance of 5 tree ker-
nel setups on the ACE 2003 data using the tree 
structure information only. It shows that:  
 
? Overall the five different relation instance 
spaces are all somewhat effective for relation 
extraction. This suggests that structured syntactic 
information has good predication power for rela-
tion extraction and the structured syntactic in-
formation can be well captured by the tree kernel.  
? MCT performs much worse than the others. 
The reasons may be that MCT includes too 
much left and right context information, which 
may introduce many noisy features and cause 
over-fitting (high precision and very low recall 
as shown in Table 1). This suggests that only 
keeping the complete (not partial) production 
rules in MCT does harm performance. 
? PT achieves the best performance. This means 
that only keeping the portion of a parse tree en-
closed by the shortest path between entities can 
model relations better than all others. This may 
be due to that most significant information is 
with PT and including context information may 
introduce too much noise. Although context 
may include some useful information, it is still a 
problem to correctly utilize such useful informa-
tion in the tree kernel for relation extraction. 
? CPT performs a bit worse than PT. In some 
cases (e.g. in sentence ?the merge of company A 
and company B?.?, ?merge? is a critical con-
text word), the context information is helpful. 
However, the effective scope of context is hard 
to determine given the complexity and variabil-
ity of natural languages. 
? The two flattened trees perform worse than the 
original trees. This suggests that the single non-
terminal nodes are useful for relation extraction.  
Evaluation on the ACE 2004 data also shows 
that PT achieves the best performance (72.5/56.7 
/63.6 in P/R/F). More evaluations with the entity 
type and order information incorporated into tree 
nodes (?E1-PER?, ?E2-PER? and ?E-GPE? as 
shown in Fig. 1) also show that PT performs best 
with 76.1/62.6/68.7 in P/R/F on the 2003 data 
and 74.1/62.4/67.7 in P/R/F on the 2004 data. 
 
Instance Spaces P(%) R(%) F 
Minimum Complete Tree 
(MCT) 77.5 38.4 51.3 
Path-enclosed Tree (PT) 72.8 53.8 61.9 
Context-Sensitive PT(CPT) 75.9 48.6 59.2 
Flattened PT 72.7 51.7 60.4 
Flattened CPT 76.1 47.2 58.2 
 
Table 1. five different tree kernel setups on the 
ACE 2003 five major types using the parse 
tree structure information only (regardless of 
any entity-related information) 
829
PTs (with Tree Struc-
ture Information only) 
P(%) R(%) F 
Entity kernel only 75.1 
(79.5) 
42.7 
(34.6)
54.4 
(48.2) 
Tree kernel only 72.5 
(72.8) 
56.7 
(53.8)
63.6 
(61.9) 
Composite kernel 1 
(linear combination) 
73.5 
(76.3) 
67.0 
(63.0)
70.1 
(69.1) 
Composite kernel 2 
(polynomial expansion)
76.1 
(77.3) 
68.4 
(65.6)
72.1 
(70.9) 
 
Table 2. Performance comparison of different 
kernel setups over the ACE major types of 
both the 2003 data (the numbers in parenthe-
ses) and the 2004 data (the numbers outside 
parentheses) 
 
(2) Composite Kernels: Table 2 compares the 
performance of different kernel setups on the 
ACE major types. It clearly shows that:  
? The composite kernels achieve significant per-
formance improvement over the two individual 
kernels. This indicates that the flat and the struc-
tured features are complementary and the com-
posite kernels can well integrate them: 1) the 
flat entity information captured by the entity 
kernel; 2) the structured syntactic connection 
information between the two entities captured 
by the tree kernel. 
 
? The composite kernel via the polynomial ex-
pansion outperforms the one via the linear com-
bination by ~2 in F-measure. It suggests that the 
bi-gram entity features are very useful.  
 
? The entity features are quite useful, which can 
achieve F-measures of 54.4/48.2 alone and can 
boost the performance largely by ~7 (70.1-
63.2/69.1-61.9) in F-measure when combining 
with the tree kernel.  
 
? It is interesting that the ACE 2004 data shows 
consistent better performance on all setups than 
the 2003 data although the ACE 2003 data is 
two times larger than the ACE 2004 data. This 
may be due to two reasons: 1) The ACE 2004 
data defines two new entity types and re-defines 
the relation types and subtypes in order to re-
duce the inconsistency between LDC annota-
tors. 2) More importantly, the ACE 2004 data 
defines 43 entity subtypes while there are only 3 
subtypes in the 2003 data. The detailed classifi-
cation in the 2004 data leads to significant per-
formance improvement of 6.2 (54.4-48.2) in F-
measure over that on the 2003 data. 
Our composite kernel can achieve 
77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over 
the ACE 2003/2004 major types, respectively. 
Methods (2002/2003 data) P(%) R(%) F 
Ours: composite kernel 2 
(polynomial expansion) 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Zhou et al (2005):  
feature-based SVM 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based ME 
 (-) 
(63.5) 
 (-) 
(45.2) 
 (-) 
(52.8) 
Ours: tree kernel with en-
tity information at node 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu and Mooney 
(2005): shortest path de-
pendency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta and Sorensen 
(2004): dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
 
Table 3. Performance comparison on the ACE 
2003/2003 data over both 5 major types (the 
numbers outside parentheses) and 24 subtypes 
(the numbers in parentheses)  
 
Methods (2004 data) P(%) R(%) F 
Ours: composite kernel 2 
(polynomial expansion) 
76.1 
 (68.6) 
68.4 
(59.3)
72.1 
 (63.6)
Zhao and Grishman (2005): 
feature-based kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
 
Table 4. Performance comparison on the ACE 
2004 data over both 7 major types (the numbers 
outside parentheses) and 23 subtypes (the num-
bers in parentheses) 
 
(3) Performance Comparison: Tables 3 and 4 
compare our method with previous work on the 
ACE 2002/2003/2004 data, respectively. They 
show that our method outperforms the previous 
methods and significantly outperforms the previ-
ous two dependency kernels4. This may be due to 
two reasons: 1) the dependency tree (Culotta and 
Sorensen, 2004) and the shortest path (Bunescu 
and Mooney, 2005) lack the internal hierarchical 
phrase structure information, so their correspond-
ing kernels can only carry out node-matching 
directly over the nodes with word tokens; 2) the 
parse tree kernel has less constraints. That is, it is 
                                                 
4 Bunescu and Mooney (2005) used the ACE 2002 corpus, 
including 422 documents, which is known to have many 
inconsistencies than the 2003 version. Culotta and Sorensen 
(2004) used a generic ACE corpus including about 800 
documents (no corpus version is specified). Since the testing 
corpora are in different sizes and versions, strictly speaking, 
it is not ready to compare these methods exactly and fairly. 
Therefore Table 3 is only for reference purpose. We just 
hope that we can get a few clues from this table. 
830
not restricted by the two constraints of the two 
dependency kernels (identical layer and ances-
tors for the matchable nodes and identical length 
of two shortest paths, as discussed in Section 2).  
 
The above experiments verify the effective-
ness of our composite kernels for relation extrac-
tion. They suggest that the parse tree kernel can 
effectively explore the syntactic features which 
are critical for relation extraction.  
 
# of error instances Error Type 
  2004 data 2003 data 
False Negative 198  416 
False Positive 115 171 
Cross Type 62 96 
 
Table 5. Error distribution of major types on 
both the 2003 and 2004 data for the compos-
ite kernel by polynomial expansion 
 
(4) Error Analysis: Table 5 reports the error 
distribution of the polynomial composite kernel 
over the major types on the ACE data. It shows 
that 83.5%(198+115/198+115+62) / 85.8%(416 
+171/416+171+96) of the errors result from rela-
tion detection and only 16.5%/14.2% of the er-
rors result from relation characterization. This 
may be due to data imbalance and sparseness 
issues since we find that the negative samples are 
8 times more than the positive samples in the 
training set. Nevertheless, it clearly directs our 
future work. 
5 Discussion 
In this section, we compare our method with the 
previous work from the feature engineering 
viewpoint and report some other observations 
and issues in our experiments. 
5.1 Comparison with Previous Work 
This is to explain more about why our method 
performs better and significantly outperforms the 
previous two dependency tree kernels from the 
theoretical viewpoint. 
(1) Compared with Feature-based Methods: 
The basic difference lies in the relation instance 
representation (parse tree vs. feature vector) and 
the similarity calculation mechanism (kernel 
function vs. dot-product). The main difference is 
the different feature spaces. Regarding the parse 
tree features, our method implicitly represents a 
parse tree by a vector of integer counts of each 
sub-tree type, i.e., we consider the entire sub-tree 
types and their occurring frequencies. In this way, 
the parse tree-related features (the path features 
and the chunking features) used in the feature-
based methods are embedded (as a subset) in our 
feature space. Moreover, the in-between word 
features and the entity-related features used in 
the feature-based methods are also captured by 
the tree kernel and the entity kernel, respectively. 
Therefore our method has the potential of effec-
tively capturing not only most of the previous 
flat features but also the useful syntactic struc-
ture features. 
 
(2) Compared with Previous Kernels: Since 
our method only counts the occurrence of each 
sub-tree without considering the layer and the 
ancestors of the root node of the sub-tree, our 
method is not limited by the constraints (identi-
cal layer and ancestors for the matchable nodes, 
as discussed in Section 2) in Culotta and Soren-
sen (2004). Moreover, the difference between 
our method and Bunescu and Mooney (2005) is 
that their kernel is defined on the shortest path 
between two entities instead of the entire sub-
trees. However, the path does not maintain the 
tree structure information. In addition, their ker-
nel requires the two paths to have the same 
length. Such constraint is too strict. 
5.2 Other Issues 
(1) Speed Issue: The recursively-defined convo-
lution kernel is much slower compared to fea-
ture-based classifiers. In this paper, the speed 
issue is solved in three ways. First, the inclusion 
of the entity kernel makes the composite kernel 
converge fast. Furthermore, we find that the 
small portion (PT) of a full parse tree can effec-
tively represent a relation instance. This signifi-
cantly improves the speed. Finally, the parse tree 
kernel requires exact match between two sub-
trees, which normally does not occur very fre-
quently. Collins and Duffy (2001) report that in 
practice, running time for the parse tree kernel is 
more close to linear (O(|N1|+|N2|), rather than 
O(|N1|*|N2| ). As a result, using the PC with Intel 
P4 3.0G CPU and 2G RAM, our system only 
takes about 110 minutes and 30 minutes to do 
training on the ACE 2003 (~77k training in-
stances) and 2004 (~33k training instances) data, 
respectively.  
(2) Further Improvement: One of the potential 
problems in the parse tree kernel is that it carries 
out exact matches between sub-trees, so that this 
kernel fails to handle sparse phrases (i.e. ?a car? 
vs. ?a red car?) and near-synonymic grammar 
tags (for example, the variations of a verb (i.e. 
go, went, gone)). To some degree, it could possi-
bly lead to over-fitting and compromise the per-
831
formance. However, the above issues can be 
handled by allowing grammar-driven partial rule 
matching and other approximate matching 
mechanisms in the parse tree kernel calculation. 
Finally, it is worth noting that by introducing 
more individual kernels our method can easily 
scale to cover more features from a multitude of 
sources (e.g. Wordnet, gazetteers, etc) that can 
be brought to bear on the task of relation extrac-
tion. In addition, we can also easily implement 
the feature weighting scheme by adjusting the 
eqn.(2) and the rule (2) in calculating 1 2( , )n n?  
(see subsection 3.1). 
6 Conclusion and Future Work 
Kernel functions have nice properties. In this 
paper, we have designed a composite kernel for 
relation extraction. Benefiting from the nice 
properties of the kernel methods, the composite 
kernel could well explore and combine the flat 
entity features and the structured syntactic fea-
tures, and therefore outperforms previous best-
reported feature-based methods on the ACE cor-
pus. To our knowledge, this is the first research 
to demonstrate that, without the need for exten-
sive feature engineering, an individual tree ker-
nel achieves comparable performance with the 
feature-based methods. This shows that the syn-
tactic features embedded in a parse tree are par-
ticularly useful for relation extraction and which 
can be well captured by the parse tree kernel. In 
addition, we find that the relation instance repre-
sentation (selecting effective portions of parse 
trees for kernel calculations) is very important 
for relation extraction. 
The most immediate extension of our work is 
to improve the accuracy of relation detection. 
This can be done by capturing more features by 
including more individual kernels, such as the 
WordNet-based semantic kernel (Basili et al, 
2005) and other feature-based kernels. We can 
also benefit from machine learning algorithms to 
study how to solve the data imbalance and 
sparseness issues from the learning algorithm 
viewpoint. In the future work, we will design a 
more flexible tree kernel for more accurate simi-
larity measure.  
 
Acknowledgements: We would like to thank 
Dr. Alessandro Moschitti for his great help in 
using his Tree Kernel Toolkits and fine-tuning 
the system. We also would like to thank the three 
anonymous reviewers for their invaluable sug-
gestions. 
References 
ACE. 2002-2005. The Automatic Content Extraction 
Projects. http://www.ldc.upenn.edu/Projects /ACE/ 
Basili R., Cammisa M. and Moschitti A. 2005. A Se-
mantic Kernel to classify text with very few train-
ing examples. ICML-2005 
Bunescu R. C. and Mooney R. J. 2005. A Shortest 
Path Dependency Kernel for Relation Extraction. 
EMNLP-2005 
Charniak E. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
Collins M. and Duffy N. 2001. Convolution Kernels 
for Natural Language. NIPS-2001 
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. ACL-2004 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kambhatla N. 2004. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. ACL-2004 (poster) 
Lodhi H., Saunders C., Shawe-Taylor J., Cristianini 
N. and Watkins C. 2002. Text classification using 
string kernel. Journal of Machine Learning Re-
search, 2002(2):419-444 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
2000. A novel use of statistical parsing to extract 
information from text. NAACL-2000 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. ACL-2004 
MUC. 1987-1998. http://www.itl.nist.gov/iaui/894.02/ 
related_projects/muc/ 
Sch?lkopf B. and Smola A. J. 2001. Learning with 
Kernels: SVM, Regularization, Optimization and 
Beyond. MIT Press, Cambridge, MA 407-423 
Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003. 
Hierarchical Directed Acyclic Graph Kernel: 
Methods for Structured Natural Language Data. 
ACL-2003 
Zelenko D., Aone C. and Richardella A. 2003. Kernel 
Methods for Relation Extraction. Journal of Ma-
chine Learning Research. 2003(2):1083-1106 
Zhao S.B. and Grishman R. 2005. Extracting Rela-
tions with Integrated Information Using Kernel 
Methods. ACL-2005 
Zhou G.D., Su J, Zhang J. and Zhang M. 2005. Ex-
ploring Various Knowledge in Relation Extraction. 
ACL-2005 
832
Multi-Criteria-based Active Learning for Named Entity Recognition 
Dan Shen??1 Jie Zhang?? Jian Su? Guodong Zhou? Chew-Lim Tan? 
? Institute for Infocomm Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg 
{shendan,zhangjie,tancl}@comp.nus.edu.sg 
                                                                 
1 Current address of the first author: Universit?t des Saarlandes, Computational Linguistics Dept., 66041 Saarbr?cken, Germany 
dshen@coli.uni-sb.de 
 
 
 
 
Abstract 
In this paper, we propose a multi-criteria -
based active learning approach and effec-
tively apply it to named entity recognition. 
Active learning targets to minimize the 
human annotation efforts by selecting ex-
amples for labeling.  To maximize the con-
tribution of the selected examples, we 
consider the multiple criteria: informative-
ness, representativeness and diversity  and 
propose measures to quantify them.  More 
comprehensively, we incorporate all the 
criteria using two selection strategies, both 
of which result in less labeling cost than 
single-criterion-based method.  The results 
of the named entity recognition in both 
MUC-6 and GENIA show that the labeling 
cost can be reduced by at least 80% with-
out degrading the performance. 
1 Introduction 
In the machine learning approaches of natural lan-
guage processing (NLP), models are generally 
trained on large annotated corpus.  However, anno-
tating such corpus is expensive and time-
consuming, which makes it difficult to adapt an 
existing model to a new domain.  In order to over-
come this difficulty, active learning (sample selec-
tion) has been studied in more and more NLP 
applications such as POS tagging (Engelson and 
Dagan 1999), information extraction (Thompson et 
al. 1999), text classif ication (Lewis and Catlett 
1994; McCallum and Nigam 1998; Schohn and 
Cohn 2000; Tong and Koller 2000; Brinker 2003), 
statistical parsing (Thompson et al 1999; Tang et 
al. 2002; Steedman et al 2003), noun phrase 
chunking (Ngai and Yarowsky 2000), etc. 
Active learning is based on the assumption that 
a small number of annotated examples and a large 
number of unannotated examples are available.  
This assumption is valid in most NLP tasks.  Dif-
ferent from supervised learning in which the entire 
corpus are labeled manually, active learning is to 
select the most useful example for labeling and add 
the labeled example  to training set to retrain model.  
This procedure is repeated until the model achieves 
a certain level of performance.  Practically, a batch 
of examples are selected at a time, called batched-
based sample selection (Lewis and Catlett 1994) 
since it is time consuming to retrain the model if 
only one new example is added to the training set.  
Many existing work in the area focus on two ap-
proaches: certainty-based methods (Thompson et 
al. 1999; Tang et al 2002; Schohn and Cohn 2000; 
Tong and Koller 2000; Brinker 2003) and commit-
tee-based methods (McCallum and Nigam 1998; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000) to select the most informative examples for 
which the current model are most uncertain. 
Being the first piece of work on active learning 
for name entity recognition (NER) task, we target 
to minimize the human annotation efforts yet still 
reaching the same level of performance as a super-
vised learning approach.  For this purpose, we 
make a more comprehensive consideration on the 
contribution of individual examples, and more im-
portantly maximizing the contribution of a batch 
based on three criteria : informativeness, represen-
tativeness and diversity. 
First, we propose three scoring functions to 
quantify the informativeness of an example , which 
can be used to select the most uncertain examples.  
Second, the representativeness measure is further 
proposed to choose the examples representing the 
majority.  Third, we propose two diversity consid-
erations (global and local) to avoid repetition 
among the examples of a batch.  Finally, two com-
bination strategies with the above three criteria are 
proposed to reach the maximum effectiveness on 
active learning for NER. 
We build our NER model using Support Vec-
tor Machines (SVM).  The experiment shows that 
our active learning methods achieve a promising 
result in this NER task.  The results in both MUC-
6 and GENIA show that the amount of the labeled 
training data can be reduced by at least 80% with-
out degrading the quality of the named entity rec-
ognizer.  The contributions not only come from the 
above measures, but also the two sample selection 
strategies which effectively incorporate informa-
tiveness, representativeness and diversity criteria.  
To our knowledge, it is the first work on consider-
ing the three criteria all together for active learning.  
Furthermore, such measures and strategies can be 
easily adapted to other active learning tasks as well.  
 
2 Multi-criteria for NER Active Learning 
Support Vector Machines (SVM) is a powerful 
machine learning method, which has been applied 
successfully in NER tasks, such as (Kazama et al 
2002; Lee et al 2003).  In this paper, we apply ac-
tive learning methods to a simple  and effective 
SVM model to recognize one class of names at a 
time, such as protein names, person names, etc.  In 
NER, SVM is to classify a word into positive class 
?1? indicating that the word is a part of an entity, 
or negative class ?-1? indicating that the word is 
not a part of an entity.  Each word in SVM is rep-
resented as a high-dimensional feature vector in-
cluding surface word information, orthographic 
features, POS feature and semantic trigger features 
(Shen et al 2003).  The semantic trigger features 
consist of some special head nouns for an entity 
class which is supplied by users.  Furthermore, a 
window (size = 7), which represents the local con-
text of the target word w, is also used to classify w.   
However, for active learning in NER, it is not 
reasonable to select a single word without context 
for human to label.  Even if we require human to 
label a single word, he has to make an addition 
effort to refer to the context of the word.  In our 
active learning process, we select a word sequence 
which consists of a machine-annotated named en-
tity and its context rather than a single word.  
Therefore, all of the measures we propose for ac-
tive learning should be applied to the machine-
annotated named entities and we have to further 
study how to extend the measures for words to 
named entities.  Thus, the active learning in SVM-
based NER will be more complex than that in sim-
ple classification tasks, such as text classif ication 
on which most SVM active learning works are 
conducted (Schohn and Cohn 2000; Tong and 
Koller 2000; Brinker 2003).  In the next part, we 
will introduce informativeness, representativeness 
and diversity measures for the SVM-based NER. 
2.1 Informativeness 
The basic idea of informativeness criterion is simi-
lar to certainty-based sample selection methods, 
which have been used in many previous works.  In 
our task, we use a distance-based measure to 
evaluate the informativeness of a word and extend 
it to the measure of an entity using three scoring 
functions.  We prefer the examples with high in-
formative degree for which the current model are 
most uncertain. 
2.1.1 Informativeness Measure for Word 
In the simplest linear form, training SVM is to find 
a hyperplane that can separate the posit ive and 
negative examples in training set with maximum 
margin.  The margin is defined by the distance of 
the hyperplane to the nearest of the positive and 
negative examples.  The training examples which 
are closest to the hyperplane are called support 
vectors.  In SVM, only the support vectors are use-
ful for the classification, which is different from 
statistical models.  SVM training is to get these 
support vectors and their weights from training set 
by solving quadratic programming problem.  The 
support vectors can later be used to classify the test 
data. 
Intuitively, we consider the informativeness of 
an example  as how it can make effect on the sup-
port vectors by adding it to training set.  An exam-
ple may be informative for the learner if the 
distance of its feature vector to the hyperplane is 
less than that of the support vectors to the hyper-
plane (equal to 1).  This intuition is also justified 
by (Schohn and Cohn 2000; Tong and Koller 2000) 
based on a version space analysis.  They state that 
labeling an example that lies on or close to the hy-
perplane is guaranteed to have an effect on the so-
lution.  In our task, we use the distance to measure 
the informativeness of an example. 
The distance of a word?s feature vector to the 
hyperplane is computed as follows: 
1
( ) ( , )
N
i i i
i
Dist y k ba
=
= +?w s w  
where w is the feature vector of the word, ai, yi, si 
corresponds to the weight, the class and the feature 
vector of the ith support vector respectively.  N is 
the number of the support vectors in current model. 
We select the example with minimal Dist, 
which indicates that it comes closest to the hyper-
plane in feature space.  This example is considered 
most informative for current model. 
2.1.2 Informativeness Measure for Named 
Entity 
Based on the above informativeness measure for a 
word, we compute the overall informativeness de-
gree of a named entity NE.  In this paper, we pro-
pose three scoring functions as follows. Let NE = 
w1?wN in which wi is the feature vector of the ith 
word of NE. 
? Info_Avg: The informativeness of NE is 
scored by the average distance of the words in 
NE to the hyperplane.  
 ( ) 1 ( )
i
i
N E
Info NE Dist
?
= - ?
w
w  
 where, wi is the feature vector of the ith word in 
NE. 
? Info_Min: The informativeness of NE is 
scored by the minimal distance of the words in 
NE. 
 ( ) 1 { ( )}
i
iNE
Info NE Min Dist
?
= -
w
w  
? Info_S/N: If the distance of a word to the hy-
perplane is less than a threshold a (= 1 in our 
task), the word is considered with short dis-
tance.  Then, we compute the proportion of the 
number of words with short distance to the to-
tal number of words in the named entity and 
use this proportion to quantify the informa-
tiveness of the named entity.  
 
( ( ) )
( ) i
i
N E
NUM Dist
Info NE
N
a
?
<
= w
w
 
In Section 4.3, we will evaluate the effective-
ness of these scoring functions. 
2.2 Representativeness 
In addition to the most informative example, we 
also prefer the most representative example.  The 
representativeness of an example can be evaluated 
based on how many examples there are similar or 
near to it.  So, the examples with high representa-
tive degree are less likely to be an outlier.  Adding 
them to the training set will have effect on a large 
number of unlabeled examples.  There are only a 
few works considering this selection criterion 
(McCallum and Nigam 1998; Tang et al 2002) and 
both of them are specific to their tasks, viz. text 
classification and statistical parsing.  In this section, 
we compute the simila rity between words using a 
general vector-based measure, extend this measure 
to named entity level using dynamic time warping 
algorithm and quantify the representativeness of a 
named entity by its density. 
2.2.1 Similarity Measure  between Words 
In general vector space model, the similarity be-
tween two vectors may be measured by computing 
the cosine value of the angle between them.  The 
smaller the angle is, the more similar between the 
vectors are.  This measure, called cosine-similarity 
measure, has been widely used in information re-
trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).    
In our task, we also use it to quantify the similarity 
between two words.  Particularly, the calculation in 
SVM need be projected to a higher dimensional 
space by using a certain kernel function ( , )i jK w w .  
Therefore, we adapt the cosine-similarity measure 
to SVM as follows: 
( , )
( , )
( , ) ( , )
i j
i j
i i j j
k
Sim
k k
=
w w
w w
w w w w
 
where, wi and wj are the feature vectors of the 
words i and j.  This calculation is also supported by 
(Brinker 2003)?s work.  Furthermore, if we use the 
linear kernel ( , )i j i jk = ?w w w w , the measure is 
the same as the traditional cosine similarity meas-
ure cos i j
i j
q
?
=
?
w w
w w
 and may be regarded as a 
general vector-based similarity measure. 
2.2.2 Similarity Meas ure between Named En-
tities 
In this part, we compute the similarity between two 
machine-annotated named entities given the simi-
larities between words.  Regarding an entity as a 
word sequence, this work is analogous to the 
alignment of two sequences.  We employ the dy-
namic time warping (DTW) algorithm (Rabiner et 
al. 1978) to find an optimal alignment between the 
words in the sequences which maximize the accu-
mulated similarity degree between the sequences.  
Here, we adapt it to our task.  A sketch of the 
modified algorithm is as follows. 
Let NE1 = w11w12?w1n?w1N, (n = 1,?, N) and 
NE2 = w21w22?w2m?w2M, (m = 1,?, M) denote two 
word sequences to be matched.  NE1 and NE2 con-
sist of M and N words respectively.  NE1(n) = w1n 
and NE2(m) = w2m.  A similarity value Sim(w1n ,w2m) 
has been known for every pair of words (w1n,w2m) 
within NE1 and NE2.  The goal of DTW is to find a 
path, m = map(n), which map n onto the corre-
sponding m such that the accumulated similarity 
Sim* along the path is maximized. 
1 2
{ ( )} 1
* { ( ( ), ( ( ))}
N
m a p n n
Sim M a x Sim N E n N E m a p n
=
= ?  
A dynamic programming method is used to deter-
mine the optimum path map(n).  The accumulated 
similarity SimA to any grid point (n, m) can be re-
cursively calculated as 
1 2( , ) ( , ) ( 1, )A n m Aq mSim n m Sim w w M a x S i m n q?= + -
Finally, * ( , )ASim Sim N M=  
Certainly, the overall similarity measure Sim* 
has to be normalized as longer sequences normally 
give higher similarity value.  So, the similarity be-
tween two sequences NE1 and NE2 is calculated as 
1 2
*( , )
( , )
SimSim NE NE
Max N M
=  
2.2.3 Representativeness Measure for Named 
Entity 
Given a set of machine-annotated named entities 
NESet = {NE1, ? , NEN}, the representativeness of 
a named entity NEi in NESet is quantified by its 
density.  The density of NEi is defined as the aver-
age similarity between NEi and all the other enti-
ties NEj in NESet as follows. 
( , )
( )
1
i j
j i
i
Sim NE NE
Density N E
N
?=
-
?
 
If NEi has the largest density among all the entities 
in NESet, it can be regarded as the centroid of NE-
Set and also the most representative examples in 
NESet. 
2.3 Diversity 
Diversity criterion is to maximize the training util-
ity of a batch.  We prefer the batch in which the 
examples have high variance to each other.  For 
example, given the batch size 5, we try not to se-
lect five repetitious examples at a time.  To our 
knowledge, there is only one work (Brinker 2003) 
exploring this criterion.  In our task, we propose 
two methods: local and global, to make the exam-
ples diverse enough in a batch.   
2.3.1 Global Consideration 
For a global consideration, we cluster all named 
entities in NESet based on the similarity measure 
proposed in Section 2.2.2.  The named entities in 
the same cluster may be considered similar to each 
other, so we will select the named entities from 
different clusters at one time.  We employ a K-
means clustering algorithm (Jelinek 1997), which 
is shown in Figure 1. 
Given: 
NESet = {NE1, ? , NEN} 
Suppose: 
The number of clusters is K 
Initialization: 
Randomly equally partition {NE1, ? , NEN} into K 
initial clusters Cj (j = 1, ? , K). 
Loop until the number of changes for the centroids of 
all clusters is less than a threshold 
? Find the centroid of each cluster Cj (j = 1, ? , K). 
 arg ( ( , ))
j i j
j i
NE C NE C
NECent max Sim NE NE
? ?
= ?  
? Repartition {NE1, ? , NEN} into K clusters.  NEi 
will be assigned to Cluster Cj if 
 
( , ) ( , ),i j i wSim NE NECent Sim NE NECent w j? ?  
Figure 1: Global Consideration for Diversity: K-
Means Clustering algorithm 
In each round, we need to compute the pair-
wise similarities within each cluster to get the cen-
troid of the cluster.  And then, we need to compute 
the similarities between each example and all cen-
troids to repartition the examples.  So, the algo-
rithm is time-consuming.  Based on the assumption 
that N examples are uniformly distributed between 
the K clusters, the time complexity of the algo-
rithm is about O(N2/K+NK) (Tang et al 2002).  In 
one of our experiments, the size of the NESet (N) is 
around 17000 and K is equal to 50, so the time 
complexity is about O(106).  For efficiency, we 
may filter the entities in NESet before clustering 
them, which will be further discussed in Section 3.  
2.3.2 Local Consideration 
When selecting a machine-annotated named entity, 
we compare it with all previously selected named 
entities in the current batch.  If the similarity be-
tween them is above a threshold ?, this example 
cannot be allowed to add into the batch.  The order 
of selecting examples is based on some measure, 
such as informativeness measure, representative-
ness measure or their combination.  This local se-
lection method is shown in Figure 2.  In this way, 
we avoid selecting too similar examples (similarity 
value ?  ?) in a batch.  The threshold ? may be the 
average similarity between the examples in NESet. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = empty 
Loop until BatchSet is full 
? Select NEi based on some measure from NESet. 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 2: Local Consideration for Diversity 
 
This consideration only requires O(NK+K2) 
computational time.  In one of our experiments (N 
 ?17000 and K = 50), the time complexity is about 
O(105).  It is more efficient than clustering algo-
rithm described in Section 2.3.1.  
 
3 Sample Selection strategies 
In this section, we will study how to combine and 
strike a proper balance between these criteria, viz. 
informativeness, representativeness and diversity, 
to reach the maximum effectiveness on NER active 
learning.  We build two strategies to combine the 
measures proposed above.  These strategies are 
based on the varying priorities of the criteria and 
the varying degrees to satisfy the criteria. 
? Strategy 1: We first consider the informative-
ness criterion.  We choose m examples with the 
most informativeness score from NESet to an in-
termediate set called INTERSet.  By this pre-
selecting, we make the selection process faster in 
the later steps since the size of INTERSet is much 
smaller than that of NESet.  Then we cluster the 
examples in INTERSet and choose the centroid of 
each cluster into a batch called BatchSet.  The cen-
troid of a cluster is the most representative exam-
ple in that cluster since it has the largest density.  
Furthermore, the examples in different clusters 
may be considered diverse to each other.  By this 
means, we consider representativeness and diver-
sity criteria at the same time.  This strategy is 
shown in Figure 3.  One limitation of this strategy 
is that clustering result may not reflect the distribu-
tion of whole sample space since we only cluster 
on INTERSet for efficiency.  The other is that since 
the representativeness of an example is only evalu-
ated on a cluster.  If the cluster size is too small, 
the most representative example in this cluster may 
not be representative in the whole sample space. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
INTERSet with the maximal size M 
Steps :  
? BatchSet  = ?  
? INTERSet = ?  
? Select M entities with most Info score from NESet 
to INTERSet. 
? Cluster the entities in INTERSet into K clusters 
? Add the centroid entity of each cluster to BatchSet 
Figure 3: Sample Selection Strategy 1 
 
? Strategy 2: (Figure 4) We combine the infor-
mativeness and representativeness criteria  using 
the functio ( ) (1 ) ( )i iInfo NE Density NEl l+ - , in 
which the Info and Density  value of NEi are nor-
malized first.  The individual importance of each 
criterion in this function is adjusted by the trade-
off parameter l ( 0 1l? ? ) (set to 0.6 in our 
experiment).  First, we select a candidate example 
NEi with the maximum value of this function from 
NESet.  Second, we consider diversity criterion 
using the local method in Section 3.3.2.  We add 
the candidate example NEi to a batch only if NEi is 
different enough from any previously selected ex-
ample in the batch.  The threshold ? is set to the 
average pair-wise similarity of the entities in NE-
Set. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = ?  
Loop until BatchSet is full 
? Select NEi which have the maximum value for the 
combination function between Info score and Den-
sity socre from NESet. 
arg ( ( ) (1 ) ( ))
i
i i i
N E NESet
N E Max Info NE Density NEl l
?
= + -
 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 4: Sample Selection Strategy 2 
 
4 Experimental Results and Analysis 
4.1 Experiment Settings  
In order to evaluate the effectiveness of our selec-
tion strategies, we apply them to recognize protein 
(PRT) names in biomedical domain using GENIA 
corpus V1.1 (Ohta et al 2002) and person (PER), 
location (LOC), organization (ORG) names in 
newswire domain using MUC-6 corpus.  First, we 
randomly split the whole corpus into three parts: an 
initial training set to build an in itial model, a test 
set to evaluate the performance of the model and 
an unlabeled set to select examples.  The size of 
each data set is shown in Table 1.  Then, iteratively, 
we select a batch of examples following the selec-
tion strategies proposed, require human experts to 
label them and add them into the training set.  The 
batch size K = 50 in GENIA and 10 in MUC-6.  
Each example is defined as a machine-recognized 
named entity and its context words (previous 3 
words and next 3 words). 
Domain Class Corpus Initial Training Set Test Set Unlabeled Set 
Biomedical PRT GENIA1.1 10 sent. (277 words) 900 sent. (26K words) 8004 sent. (223K words) 
PER 5 sent. (131 words) 7809 sent. (157K words) 
LOC 5 sent. (130 words) 7809 sent. (157K words) 
 
Newswire 
ORG 
 
MUC-6 
 5 sent. (113 words) 
 
602 sent. (14K words) 
 7809 sent. (157K words) 
Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG) 
The goal of our work is to minimize the human 
annotation effort to learn a named entity recognizer 
with the same performance level as supervised 
learning.  The performance of our model is evalu-
ated using ?precision/recall/F-measure?. 
4.2 Overall Result in GENIA and MUC-6 
In this section, we evaluate our selection strategies 
by comparing them with a random selection 
method, in which a batch of examples is randomly 
selected iteratively, on GENIA and MUC-6 corpus.  
Table 2 shows the amount of training data needed 
to achieve the performance of supervised learning 
using various selection methods, viz. Random, 
Strategy1 and Strategy2.  In GENIA, we find: 
? The model achieves 63.3 F-measure using 223K  
words in the supervised learning. 
? The best performer is Strategy2 (31K words), 
requiring less than 40% of the training data that 
Random (83K words) does and 14% of the train-
ing data that the supervised learning does. 
? Strategy1 (40K words) performs slightly worse 
than Strategy2, requiring 9K more words.  It is 
probably because Strategy1 cannot avoid select-
ing outliers if a cluster is too small. 
? Random (83K words) requires about 37% of the 
training data that the supervised learning does.  It 
indicates that only the words in and around a 
named entity are useful for classification and the 
words far from the named entity may not be 
helpful. 
 
Class Supervised Random Strategy1 Strategy2 
PRT 223K (F=63.3) 83K 40K 31K 
PER 157K (F=90.4) 11.5K 4.2K 3.5K 
LOC 157K (F=73.5) 13.6K 3.5K 2.1K 
ORG 157K (F=86.0) 20.2K 9.5K 7.8K 
Table 2: Overall Result in GENIA and MUC-6 
Furthermore, when we apply our model to news-
wire domain (MUC-6) to recognize person, loca-
tion and organization names, Strategy1 and 
Strategy2 show a more promising result by com-
paring with the supervised learning and Random, 
as shown in Table 2.  On average, about 95% of 
the data can be reduced to achieve the same per-
formance with the supervised learning in MUC-6.  
It is probably because NER in the newswire do-
main is much simpler than that in the biomedical 
domain (Shen et al 2003) and named entities are 
less and distributed much sparser in the newswire 
texts than in the biomedical texts. 
 
4.3 Effectiveness of Informativeness-based 
Selection Method 
In this section, we investigate the effectiveness of 
informativeness criterion in NER task.  Figure 5 
shows a plot of training data size versus F-measure 
achieved by the informativeness-based measures in 
Section 3.1.2: Info_Avg, Info_Min  and Info_S/N as 
well as Random.  We make the comparisons in 
GENIA corpus.  In Figure 5, the horizontal line is 
the performance level (63.3 F-measure) achieved 
by supervised learning (223K words).  We find 
that the three informativeness-based measures per-
form similarly and each of them outperforms Ran-
dom.  Table 3 highlights the various data sizes to 
achieve the peak performance using these selection 
methods.  We find that Random (83K words) on 
average requires over 1.5 times as much as data to 
achieve the same performance as the informative-
ness-based selection methods (52K words). 
 
0.5
0.55
0.6
0.65
0 20 40 60 80K words
F
Supervised
Random
Info_Min
Info_S/N
Info_Avg
 
Figure 5: Active learning curves: effectiveness of the three in-
formativeness-criterion-based selections comparing with the 
Random selection. 
Supervised Random Info_Avg Info_Min Info_ S/N 
223K 83K 52.0K 51.9K 52.3K 
Table 3: Training data sizes for various selection methods to 
achieve the same performance level as the supervised learning 
 
4.4 Effectiveness of Two Sample Selection 
Strategies 
In addition to the informativeness criterion, we 
further incorporate representativeness and diversity 
criteria into active learning using two strategies 
described in Section 3.  Comparing the two strate-
gies with the best result of the single-criterion-
based selection methods Info_Min , we are to jus-
tify that representativeness and diversity are also 
important factors for active learning.  Figure 6 
shows the learning curves for the various methods: 
Strategy1, Strategy2 and Info_Min.  In the begin-
ning iterations (F-measure < 60), the three methods 
performed similarly.  But with the larger training 
set, the efficiencies of Stratety1 and Strategy2 be-
gin to be evident.  Table 4 highlights the final re-
sult of the three methods.  In order to reach the 
performance of supervised learning, Strategy1 
(40K words) and Strategyy2 (31K words) require 
about 80% and 60% of the data that Info_Min 
(51.9K) does.  So we believe the effective combi-
nations of informativeness, representativeness and 
diversity will help to learn the NER model more 
quickly and cost less in annotation. 
0.5
0.55
0.6
0.65
0 20 40 60 K words
F
Supervised
Info_Min
Strategy1
Strategy2
 
Figure 6: Active learning curves: effectiveness of the two 
multi-criteria-based selection strategies comparing with the 
informativeness-criterion-based selection (Info_Min). 
Info_Min Strategy1 Strategy2 
51.9K 40K 31K 
Table 4: Comparisons of training data sizes for the multi-
criteria-based selection strategies and the informativeness-
criterion-based selection (Info_Min) to achieve the same per-
formance level as the supervised learning. 
 
5 Related Work 
Since there is no study on active learning for NER 
task previously, we only introduce general active 
learning methods here.  Many existing active learn-
ing methods are to select the most uncertain exam-
ples using various measures (Thompson et al 1999; 
Schohn and Cohn 2000; Tong and Koller 2000; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000).  Our informativeness-based measure is 
similar to these works.  However these works just 
follow a single criterion.  (McCallum and Nigam 
1998; Tang et al 2002) are the only two works 
considering the representativeness criterion in ac-
tive learning.  (Tang et al 2002) use the density 
information to weight the selected examples while 
we use it to select examples.  Moreover, the repre-
sentativeness measure we use is relatively general 
and easy to adapt to other tasks, in which the ex-
ample selected is a sequence of words, such as text 
chunking, POS tagging, etc.  On the other hand, 
(Brinker 2003) first incorporate diversity in active 
learning for text classification.  Their work is simi-
lar to our local consideration in Section 2.3.2.  
However, he didn?t further explore how to avoid 
selecting outliers to a batch.  So far, we haven?t 
found any previous work integrating the informa-
tiveness, representativeness and diversity all to-
gether. 
 
6 Conclusion and Future Work 
In this paper, we study the active learning in a 
more complex NLP task, named entity recognition.  
We propose a multi-criteria -based approach to se-
lect examples based on their informativeness, rep-
resentativeness and diversity, which are 
incorporated all together by two strategies (local 
and global).  Experiments show that, in both MUC-
6 and GENIA, both of the two strategies combin-
ing the three criteria outperform the single criterion 
(informativeness).  The labeling cost can be sig-
nificantly reduced by at least 80% comparing with 
the supervised learning.  To our best knowledge, 
this is not only the first work to report the empiri-
cal results of active learning for NER, but also the 
first work to incorporate the three criteria all to-
gether for selecting examples. 
Although the current experiment results are 
very promising, some parameters in our experi-
ment, such as the batch size K and the l in the 
function of strategy 2, are decided by our experi-
ence in the domain.  In practical application, the 
optimal value of these parameters should be de-
cided automatically based on the training process.  
Furthermore, we will study how to overcome the 
limitation of the strategy 1 discussed in Section 3 
by using more effective clustering algorithm.  An-
other interesting work is to study when to stop ac-
tive learning.  
 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern Information Retrieval. ISBN 0-201-39829-X. 
K. Brinker. 2003. Incorporating Diversity in Ac-
tive Learning with Support Vector Machines. In 
Proceedings of ICML, 2003. 
S. A. Engelson and I. Dagan. 1999. Committee-
Based Sample Selection for Probabilistic Classi-
fiers. Journal of Artifical Intelligence Research. 
F. Jelinek. 1997. Statistical Methods for Speech 
Recognition. MIT Press. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedi-
cal Named Entity Recognition. In Proceedings 
of the ACL2002 Workshop on NLP in Biomedi-
cine. 
K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs.  In Proceedings of the ACL2003 Work-
shop on NLP in Biomedicine. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning. 
In Proceedings of ICML, 1994. 
A. McCallum and K. Nigam. 1998. Employing EM 
in Pool-Based Active Learning for Text Classi-
fication. In Proceedings of ICML, 1998. 
G. Ngai and D. Yarowsky. 2000. Rule Writing or 
Annotation: Cost-efficient Resource Usage for 
Base Noun Phrase Chunking. In Proceedings of 
ACL, 2000. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii. 
2002. The GENIA corpus: An annotated re-
search abstract corpus in molecular biology do-
main. In Proceedings of HLT 2002. 
L. R. Rabiner, A. E. Rosenberg and S. E. Levinson. 
1978. Considerations in Dynamic Time Warping 
Algorithms for Discrete Word Recognition.  In 
Proceedings of IEEE Transactions on acoustics, 
speech and signal processing. Vol. ASSP-26, 
NO.6. 
D. Schohn and D. Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In 
Proceedings of the 17th International Confer-
ence on Machine Learning. 
D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan. 
2003. Effective Adaptation of a Hidden Markov 
Model-based Named Entity Recognizer for Bio-
medical Domain. In Proceedings of the 
ACL2003 Workshop on NLP in Biomedicine. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. 
Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and 
J. Crim. 2003. Example Selection for Bootstrap-
ping Statistical Parsers. In Proceedings of HLT-
NAACL, 2003. 
M. Tang, X. Luo and S. Roukos. 2002. Active 
Learning for Statistical Natural Language Pars-
ing. In Proceedings of the ACL 2002. 
C. A. Thompson, M. E. Califf and R. J. Mooney. 
1999. Active Learning for Natural Language 
Parsing and Information Extraction. In Proceed-
ings of ICML 1999. 
S. Tong and D. Koller. 2000. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search. 
V. Vapnik. 1998. Statistical learning theory. 
N.Y.:John Wiley. 
 
Effective Adaptation of a Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain 
Dan Shen?? Jie Zhang?? Guodong Zhou? Jian Su? Chew-Lim Tan?
? Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,zhougd,sujian}@i2r.a-star.edu.sg 
{tancl}@comp.nus.edu.sg 
 
 
Abstract 
In this paper, we explore how to adapt a 
general Hidden Markov Model-based 
named entity recognizer effectively to 
biomedical domain.  We integrate various 
features, including simple deterministic 
features, morphological features, POS 
features and semantic trigger features, to 
capture various evidences especially for 
biomedical named entity and evaluate 
their contributions.  We also present a 
simple algorithm to solve the abbreviation 
problem and a rule-based method to deal 
with the cascaded phenomena in biomedi-
cal domain.  Our experiments on GENIA 
V3.0 and GENIA V1.1 achieve the 66.1 
and 62.5 F-measure respectively, which 
outperform the previous best published 
results by 8.1 F-measure when using the 
same training and testing data.  
1 Introduction 
As the research in biomedical domain has grown 
rapidly in recent years, a huge amount of nature 
language resources have been developed and be-
come a rich knowledge base.  The technique of 
named entity (NE) recognition (NER) is strongly 
demanded to be applied in biomedical domain.  
Since in previous work, many NER systems have 
been applied successfully in newswire domain 
(Zhou and Su 2002; Bikel et al 1999; Borthwich et 
al. 1999), more and more explorations have been 
done to port existing NER system into biomedical 
domain (Kazama et al 2002; Takeuchi et al 2002; 
Nobata et al 1999 and 2000; Collier et al 2000; 
Gaizauskas et al 2000; Fukuda et al 1998; Proux 
et al 1998).  However, compared with those in 
newswire domain, these systems haven?t got high 
performance.  It is probably because of the follow-
ing factors of biomedical NE (Zhang et al 2003): 
1. Some modifiers are often before basic NEs, 
e.g. activated B cell lines, and sometimes biomedi-
cal NEs are very long, e.g. 47 kDa sterol regula-
tory element binding factor.  This kind of factor 
highlights the difficulty for identifying the bound-
ary of NE. 
2. Two or more NEs share one head noun by 
using conjunction or disjunction construction, e.g. 
91 and 84 kDa proteins.  It is hard to identify these 
NEs respectively. 
3. An entity may be found with various spelling 
forms, e.g. N-acetylcysteine, N-acetyl-cysteine, 
NAcetylCysteine, etc.  Since the use of capitaliza-
tion is casual, the capitalization information may 
not be so evidential in this domain. 
4. NE may be cascaded.  One NE may be em-
bedded in another NE, e.g. <PROTEIN><DNA> 
kappa 3</DNA> binding factor </PROTEIN>.  
More effort must be made to identify this kind of 
NE. 
5. Abbreviations are frequently used in bio-
medical domain, e.g. TCEd, IFN, TPA, etc.  Since 
abbreviations don?t have many evidences for cer-
tain NE class, it is difficult to classify them cor-
rectly. 
These factors above make NER in biomedical 
domain difficult.  Therefore, it is necessary to ex-
plore more evidential features and more effective 
methods to cope with such difficulties. 
In this paper, we will study how to adapt a gen-
eral Hidden Markov Model (HMM)-based NE rec-
ognizer (Zhou and Su 2002) to biomedical domain.  
We specially explore various evidences for bio-
medical NE and propose methods to cope with ab-
breviations and cascaded phenomena.  As a result, 
features (simple deterministic features, morpho-
logical features, part-of-speech features and head 
noun trigger features) and methods (abbreviation 
recognition algorithm and rule-based cascaded 
phenomena resolution) are integrated in our system.  
The experiment shows that system outperforms the 
best published system by 8.1 F-measure. 
In Section 2, we will introduce the HMM-
based NE recognizer briefly.  In Section 3, we will 
focus on the features that we have used.  The 
methods and the adaptations of different features 
will be discussed in detail.  In Section 5 and 6, we 
will present the solutions of abbreviation and cas-
caded phenomena. Finally, our experiment results 
will be presented and the contributions of different 
features will be analyzed in Section 7. 
2 
3 
3.1 
HMM-based Named Entity Recognizer 
Our system is adapted from a HMM-based NE 
recognizer, which has been proved very effective 
in MUC (Zhou and Su 2002). 
The purpose of HMM is to find the most likely 
tag sequence T for a given sequence 
of tokens G  that maxi-
mizes . 
n
n ttt ???= 211
n gg ?= 211
)1
nG
ng??
|( 1
nTP
In token sequence G , the token g  is defined 
as , where w is the word and is 
the feature set related with the word . 
n
1 i
i
>=< iii wfg , i if
w
In tag sequenceT , each tag consists of three 
parts: 1. Boundary category, which denotes the 
position of the current word in NE.  2. Entity cate-
gory, which indicates the NE class.  3. Feature set, 
which will be discussed in Section 3. 
n
1 it
When we incorporate a plentiful feature set in 
HMM, we will encounter data sparseness problem.  
An alternative back-off modeling approach by 
means of constraint relaxation is applied in our 
model (Zhou and Su 2002).  It enables the decod-
ing process effectively find a near optimal fre-
quently occurred pattern entry in determining the 
NE tag probability distribution of current word. 
Finally, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence 
in the state space of the possible tag distribution 
based on the state transition probabilities.  Fur-
thermore, some constraints on the boundary cate-
gory and entity category between two consecutive 
tags are applied to filter the invalid NE tags (Zhou 
and Su 2002). 
Feature Set 
Simple Deterministic Features (Fsd) 
The purpose of simple deterministic features is to 
capture the capitalization, digitalization and word 
formation information.  This kind of features have 
been widely used in both newswire NER system, 
such as (Zhou and Su 2002), and biomedical NER 
system, such as (Nobata et al 1999; Gaizauskas et 
al. 2000; Collier et al 2000; Takeuchi and Collier 
2002; Kazama et al 2002).  Based on the charac-
teristics of biomedical NEs, we designed simple 
deterministic features manually.  Table 1 shows the 
simple deterministic features with descending or-
der of priority. 
 
Fsd Name Example 
Comma , 
Dot . 
LRB ( 
RRB ) 
LSB [ 
RSB ] 
RomanDigit II 
GreekLetter Beta 
StopWord in, at 
ATCGsequence AACAAAG 
OneDigit 5 
AllDigits 60 
DigitCommaDigit 1,25 
DigitDotDigit 0.5 
OneCap T 
AllCaps CSF 
CapLowAlpha All 
CapMixAlpha IgM 
LowMixAlpha kDa 
AlphaDigitAlpha H2A 
AlphaDigit T4 
DigitAlphaDigit 6C2 
DigitAlpha 19D 
Table 1: Simple deterministic features 
From Table 1, we can find that: 
1. Features such as comma, dot, StopWord, etc. 
are designed intuitively to provide information to 
detect the boundary of NE. 
2. Features Parenthesis is often used to indicate 
the definition of abbreviation in biomedical docu-
ments. 
3. Features GreekLetter and RomanDigit are 
specially designed to capture the symbols 
frequently occurred in biomedical NE. 
4. Feature ATCG sequence identify the similar-
ity of words according to their word formations, 
e.g. AACAAAG, CTCAGGA, etc. 
5. Features dealing with mixed alphabets and 
digits such as AlphaDigitAlpha, CapMixAlpha, etc. 
are beneficial for biomedical abbreviations. 
Furthermore, we evaluate these features and 
compare with those used in MUC (Zhou and Su, 
2002).  The reported result of the simple determi-
nistic features used in MUC can achieve F-
measure of 74.1 (Zhou and Su 2002), but when 
they are used in biomedical domain, they only get 
F-measure of 24.3.  By contrast, using the simple 
deterministic features we designed for biomedical 
NER, the system achieves F-measure of 29.4.  Ac-
cording to the comparison, some findings may be 
concluded as follows: 
1) Simple deterministic features are domain de-
pendent, which suggests that it is necessary to de-
sign special features for biomedical NER. 
2) Simple deterministic features have weaker 
predictive power for NE classes in biomedical do-
main than in newswire domain. 
3.2 Morphological Feature (Fm) 
Morphological information, such as prefix/suffix, 
is considered as an important cue for terminology 
identification.  In our system, we get most frequent 
100 prefixes and suffixes from training data as 
candidates.  Then, each of these candidates is 
evaluated according to formula f1.  ( )
i
ii
i N
OUTIN
Wt
## ?=   (f1) 
in which, #INi is the number that prefix/suffix i 
occurs within NEs; #OUTi is the number that pre-
fix/suffix i occurs out of NEs; Ni is the total num-
ber of prefix/suffix i. 
The formula assumes that the particular pre-
fix/suffix, which is most likely inside NEs and 
least likely outside NEs, may be thought as a good 
evidence for distinguishing the NEs.  The candi-
dates with Wt above a certain threshold (0.7 in ex-
periment) are chosen.  Then, we calculated the 
frequency of each prefix/suffix in each NE class 
and group the prefixes/suffixes with the similar 
distribution among NE classes into one feature.  
This is because prefixes/suffixes with the similar 
distribution have the similar contribution, and it 
will avoid suffering from the data sparseness prob-
lem.  Some of morphological features were listed 
in Table 2. 
 
Fm Name Prefix/Suffix Example 
sOOC ~cin actinomycin 
 ~mide Cycloheximide 
 ~zole Sulphamethoxazole 
sLPD ~lipid Phospholipids 
 ~rogen Estrogen 
 ~vitamin dihydroxyvitamin 
sCTP ~blast erythroblast 
 ~cyte thymocyte 
 ~phil eosinophil 
sPEPT ~peptide neuropeptide 
sMA ~ma hybridoma 
sVIR ~virus cytomegalovirus 
Table 2: Examples of morphological features 
 
From Table 2, the suffixes ~cin, ~mide, ~zole 
have been grouped into one feature sOOC because 
they all have the high frequency in the NE class 
OtherOrganicCompound and relatively low fre-
quencies in the other NE classes.   In our system, 
totally 37 prefixes and suffixes were selected and 
grouped to 23 features. 
3.3 Part-of-Speech Features (Fpos) 
In the previous NER research in newswire domain, 
part-of-speech (POS) features were stated not use-
ful, as POS features may affect the use of some 
important capitalization information (Zhou and Su 
2002).  However, since more and more words with 
lower case are included in NEs, capitalization in-
formation in biomedical domain is not as eviden-
tial as it in newswire domain (Zhang et al 2003).  
Moreover, since many biomedical NEs are descrip-
tive and long, identifying NE boundary is not a 
trivial task.  POS tagging can provide the evidence 
of noun phrase region based on word syntactic in-
formation and the noun phrases are most likely to 
be NE.  Therefore, we reconsidered the POS tag-
ging.   
In previous research, (Kazama et al 2002) 
make use of POS information and conclude that it 
only slightly improves performance.  Moreover, 
(Collier et al 2000; Nobata et al 2000; Takeuchi 
and Collier. 2002) don?t incorporate POS informa-
tion in their systems.  The probable reason ex-
plained by them is that since POS tagger they used 
is trained on newswire articles, the assigned POS 
tags are often incorrect in biomedical documents.  
On the whole, it can be concluded that POS infor-
mation hasn?t been well used in previous work. 
In our experiment, a POS tagger was trained us-
ing 80% of GENIA V2.1 corpus (536 abstracts, 
123K words) and evaluated on the rest 20% (134 
abstracts, 29K words).  We use GENIA corpus to 
train the POS tagger in order to let it be adapted for 
biomedical domain.  As for comparison, we also 
trained the POS tagger on Wall Street Journal arti-
cles (2500 articles, 756K words) and tested on the 
20% of GENIA corpus.  The results are shown in 
Table 3. 
 
Training set Testing set Precision 
2500 WSJ articles 84.31 
536 GENIA abstracts 
134 GENIA 
abstracts 97.37 
Table 3: Comparison of POS tagger using dif-
ferent training data  
 
From Table 3, it can be found that POS tagger 
trained on the biomedical documents performs 
much better on the biomedical testing documents 
than that trained on WSJ articles.  This is consis-
tent with earlier explanation for why POS features 
are not so useful in biomedical NER (Nobata et al 
2000; Takeuchi and Collier 2002).   
3.4 Semantic Trigger Features 
Semantic trigger features are collected to capture 
the evidence of certain NE class based on the se-
mantic information of some key words.  Initially, 
we design two types of semantic triggers: head 
noun triggers and special verb triggers. 
3.4.1 Head Noun Triggers (Fhnt) 
Head noun means the main noun or noun phrase of 
some compound words and describes the function 
or the property, e.g. ?B cells? is the head noun for 
the NE ?activated human B cells?.  Compared with 
the other words in NE, head noun is a much more 
decisive factor for distinguishing NE classes.  For 
instance, 
<OtherName>IFN-gamma treatment</OtherName> 
<DNA>IFN-gamma activation sequence</DNA> 
In our work, we extract uni-gram and bi-grams 
of head nouns automatically from training data, 
and rank them by frequency.  According to the ex-
periment, we selected 60% top ranked head nouns 
as trigger features for each NE class.  Some exam-
ples are shown in Table 4. 
In the future application, we may also extract 
the head nouns from some public resources to en-
hance the triggers. 
 
1-gram 2-grams 
PROTEIN 
interleukin activator protein 
interferon binding protein 
kinase cell receptor 
ligand gene product 
CELL TYPE 
lymphocyte blast cell 
astrocyte blood lymphocyte 
eosinophil killer cell 
fibroblast peripheral monocyte 
DNA 
DNA X chromosome 
breakpoint alpha promoter 
cDNA binding motif 
chromosome promoter element 
Table 4: Examples of head noun triggers 
3.4.2 Special Verb Triggers (Fsvt) 
Besides collecting the triggers, such as head noun 
triggers, from the NEs themselves, we also extract 
the triggers from the local contexts of the NEs.  
Recently, some frequently occurred verbs in bio-
medical document have been proved useful for 
extracting the interaction between entities (Thomas 
et al 2000; Sekimizu et al 1998).  In biomedical 
NER, we have the intuition that particular verbs 
may also provide the evidence for boundary and 
NE class.  For instance, the verb bind is often used 
to indicate the interaction between proteins. 
In our system, we selected 20 most frequent 
verbs which occur adjacent to NE from training 
data automatically as the verb trigger features, 
which is shown in Table 5.   
 
 
Special Verb Triggers 
activate express 
bind induce 
inhibit interact 
regulate stimulate 
Table 5: Examples of special verb triggers 
4 Method for Abbreviation Recognition 
Abbreviations are widely used in biomedical do-
main.  Identifying the class of them constitutes an 
important and difficult problem (Zhang et al 2003). 
In our current system, we incorporate a method 
to classify abbreviation by mapping the abbrevia-
tion to its full form. This approach is based on the 
assumption that it is easier to classify the full form 
than abbreviation.  In most cases, this assumption 
is valid because the full form has more evidences 
than its abbreviation to capture the NE class.  
Moreover, if we can map the abbreviation to its 
full form in the current document, the recognized 
abbreviation is still helpful for classifying the same 
forthcoming abbreviations in the same document, 
as in (Zhou and Su 2002). 
In practice, abbreviation and its full form often 
occur simultaneously with parenthesis when first 
appear in biomedical documents.  There are two 
cases: 
1. full form (abbreviation) 
2. abbreviation (full form) 
Most patterns conform to the first case and if 
the content inside the parenthesis includes more 
than two words, the second case is assumed 
(Schwartz and Hearst 2003).   
In these two cases, the use of parenthesis is 
both evidential and confusing.  On one hand, it is 
evidential because it can provide the indication to 
map the abbreviation to its full form.  On the other 
hand, it is confusing because it makes the annota-
tion of NE more complicated.  Sometimes, the ab-
breviation and its full form are annotated 
separately, such as  
<CellType>human mononulear leuko-
cytes</CellType>(<CellType>hMNL</CellType>), 
and sometimes, they are all embedded in the whole 
entity, such as 
<OtherName>leukotriene B4 (LTB4) genera-
tion</OtherName>.   
Therefore, parenthesis needs to be treated specially.  
We develop an abbreviation recognition algorithm 
described in Figure 1. 
In preprocessing stage, we remove the abbre-
viations and parentheses from the sentence, when 
the abbreviation is first defined.  This measure will 
make the annotation simpler and the NE recognizer 
more effective.  The main work in this stage is to 
judge which case the current pattern belongs to and 
record the original positions of the abbreviation 
and parenthesis. 
After applying the HMM-based NE recognizer 
to the sentence, we restore the abbreviation and 
parenthesis to the original position in the sentence.  
Next, the abbreviation is classified.  There are two 
priorities of the class (from high to low): the class 
of its full form identified by the recognizer, and the 
class of the abbreviation itself identified by the 
recognizer.  At last, the same abbreviation occur-
ring in the rest sentences of the current document 
are assigned the same NE class.   
 
for each sentence Si in the document{ 
if exist parenthesis{ 
judge the case of { 
?full form (abbr.)?; 
?abbr. (full form)?; 
} 
store the abbr. A and position Pa  to a list; 
record the parenthesis position Pp; 
remove A and parenthesis from sentence; 
apply HMM-based NE recognizer to Si; 
restore A and parenthesis into Pa, Pp; 
if Pp within an identified NE E with the class CE 
parenthesis is included in E; 
else{ 
parenthesis is not included; 
   classify A to CE; 
   classify A in the rest part of document to CE; 
} 
} 
else apply HMM-based NE recognizer to Si; 
} 
Figure 1: Abbreviation recognition algorithm 
5 Solution of Cascaded Phenomena 
In (Zhang et al 2003), they state that 16.57% of 
NEs in GENIA V3.0 have cascaded annotations, 
such as  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Currently, we only consider the longest NE and 
ignore the embedded NEs.   
Based on the features described in section 3, 
our system counters some problems when dealing 
with cascaded NEs.  The probable reason is that 
the features we used are not so effective for this 
kind of NEs.   
For instance, POS is based on the assumption 
that NE is most likely to be a noun phrase.  For 
cascaded NE, this assumption may not always be 
valid because one NE may consist of two or more 
noun phrases connected by some special words, 
such as TSH receptor specific T cell lines. 
Moreover, in section 3.4.1, we have shown that 
head noun is the significant clue for distinguishing 
NE classes.  Even for cascaded NEs, head noun 
features are still effective to some extent, such as 
IL-2 mRNA.  However, cascaded NEs sometimes 
contain two or more head nouns, which belong to 
different NE classes.  For example, <DNA>IgG Fc 
receptor type IC gene</DNA>, in which receptor 
is the head noun of protein and gene is the head 
noun of DNA.  In general, the latter head noun will 
be more important.  Unfortunately, it seems that 
sometimes the shorter NE is more possible to be 
identified, such as <protein>IgG Fc recep-
tor</protein> type IC gene.   
On the whole, we have to explore an additional 
method to cope with the cascaded phenomena 
separately.  In our experiment, we attempt to solve 
this problem based on some rules. 
In GENIA corpus, we find that there are four 
basic types of cascaded NEs: 
1. < <NE> head noun >  
2. < modifier <NE> > 
3. < <NE1> <NE2> > 
4. < <NE1> word <NE2> > 
Moreover, these cascaded NEs may be generated 
iteratively.  For instance, 
5. < modifier <NE> head noun > 
6. < <NE1> <NE2> head noun > 
The rules are constructed automatically from 
the cascaded NEs in training data.  Corresponding 
to the four basic types of cascaded NEs mentioned 
before, we propose four patterns and apply them 
iteratively in each sentence: 
1. <entity1> head noun ? <entity2>  
e.g. <Protein> binding motif ? <DNA> 
2. <entity1> <entity2> ? <entity3> 
e.g. <Lipid> <Protein> ? <Protein> 
3. modifier <entity1> ? <entity2> 
e.g. anti <Protein> ? <Protein> 
4. <entity1> word <entity2> ? <entity3> 
e.g. <Virus> infected <Multicell> ? <Multicell> 
In our system, 102 rules are incorporated to 
classify the cascaded NEs. 
6 
6.1 
6.2 
Experiments 
GENIA Corpus 
GENIA corpus is the largest annotated corpus in 
molecular biology domain available to public 
(Ohta et al 2002).  In our experiment, three ver-
sions are used: 
? GENIA Version 1.1 (V1.1) -- It contains 670 
MEDLINE abstracts.  Since a lot of previous re-
lated work used this version, we use it to compare 
our result with others?. 
? GENIA Version 2.1 (V2.1) -- It contains the 
same 670 abstracts as V1.1 and POS tagging.  We 
use it to train and evaluate our POS tagger. 
? GENIA Version 3.0 (V3.0) -- It contains 2000 
abstracts, which is the superset of V1.1.  We use it 
to get the latest result and find out the effect of 
training data size. 
The annotation of NE is based on the GENIA 
ontology.  In our task, we use 23 distinct NE 
classes.  As for the conjunctive and disjunctive 
NEs, we ignore such cases and take the whole con-
struction as one entity.  In addition, for the cas-
caded annotations in V3.0, currently, we only 
consider the longest one level of the annotations. 
Experimental Results 
The system is evaluated using standard ?preci-
sion/recall/F-measure?, in which ?F-measure? is 
defined as F-measure = (2PR) / (P+R). 
We evaluate our NER system on both V3.0 and 
V1.1, each of which has been split into a training 
set and a testing set.  As for V1.1, we divide the 
corpus into 590 abstracts (136K words) as training 
set and the rest 80 abstracts (17K words) as testing 
set.  As for V3.0, we use the same testing set as 
V1.1 and the rest 1920 abstracts (447K words) as 
training set. 
 
Corpus P R F 
Our system on V3.0 66.5 65.7 66.1 
Our system on V1.1 63.8 61.3 62.5 
Kazama?s on V1.1 56.2 52.8 54.4 
Table 6: Comparison of overall performance 
 
Table 6 shows the overall performance of our 
system on V3.0 and V1.1, and the best reported 
system on V1.1 described in (Kazama et al 2002).  
On V1.1, we use the same training and testing data 
and capture the same NE classes as (Kazama et al 
2002).  Our system (62.5 F-measure) outperforms 
Kazama?s (54.4 F-measure) by 8.1 F-measure.  
This probably benefits from the various evidential 
features and the effective methods we proposed.  
Furthermore, as our expectation, the performance 
achieved on V3.0 (66.1 F-measure) is better than 
that on V1.1 (62.5 F-measure), which indicate that 
our system still has some room for improvement 
with the larger training data set. 
 
 
Figure 2: Performance of each NE class 
 
In addition, Figure 2 shows the detailed per-
formance chart of each NE class on V3.0.  In the 
figure, the numbers in the parenthesis are the num-
ber that NEs of that class occur in training/testing 
data.  It can be found that the performances vary a 
lot among the NE classes.  Some NE classes that 
have very few training data, such as Carbohydrate 
and Organism, get extremely low performance.  
In order to evaluate the contributions of differ-
ent features, we evaluate our system using different 
combinations of features (Table 7). 
From Table 7, several findings are concluded:  
1) With only Fsd, our system achieves a basic 
level F-measure of 29.4. 
2) Fm shows the positive effect with 2.4 F-
measure improvement based on the basic level.  
However, it only can slightly improve the perform-
ance (+1.2 F-measure) based on Fsd, Fpos and Fhnt.  
The probable reason is that the evidences included 
in Fm have already been captured by Fhnt.  More-
over, the evidences captured by Fhnt are more accu-
rate than that captured by Fm.  The contribution 
made by Fm may come from where there is no indi-
cation of Fhnt. 
 
Fsd Fm Fpos Fhnt Fsvt P R F 
?     42.4 22.5 29.4 
? ?    44.8 24.6 31.8 
? ? ?   58.3 50.9 54.3 
?  ? ?  62.0 61.6 61.8 
? ? ? ?  64.4 61.7 63.0 
? ? ? ? ? 60.6 59.3 60.0 
Table 7: Effects of different features on V3.0 
 
3) Fpos is proved very beneficial as it makes 
great increase on F-measure (+22.5) based on Fsd 
and Fm.   
4) Fhnt leads to an improvement of 8.7 F-
measure based on Fsd, Fm and Fpos. 
5) Out of our expectation, the use of Fsvt de-
creases both precision and recall, which may be 
explained as the present and past participles of 
some special verbs often play the adjective-like 
roles inside biomedical NEs, such as IL10-
inhibited lymphocytes.  
 
 P R F 
Fsd+Fm+Fpos+Fhnt 64.4 61.7 63.0 
+abbr. recog. algorithm 64.6 62.5 63.5 
+rule-based casc. method 66.2 65.8 66.0 
+both 66.5 65.7 66.1 
Table 8: Effects of solution for abbr. and casc. 
 
From Table 8, it can be found that the abbrevia-
tion recognition method slightly improves the per-
formance by 0.5 F-measure.  The probable reason 
is that the recognition of abbreviation relies too 
much on the recognition of its full form.  Once the 
full form is wrongly classified, the abbreviation 
and the forthcoming ones throughout the document 
are wrong altogether.  In the near future, the pre-
defined abbreviation dictionary may be incorpo-
rated to enhance the decision of NE class. 
Moreover, it can be found that the rule-based 
method effectively solves the problem of cascaded 
phenomena and shows prominent improvement 
(+3.0 F-measure) based on the performance of 
?Fsd+Fm+Fpos+Fhnt?. 
7 Conclusion 
In the paper, we describe our exploration on how 
to adapt a general HMM-based named entity rec-
ognizer to biomedical domain.  We integrate vari-
ous evidences for biomedical NER, including 
lexical, morphological, syntactic and semantic in-
formation.  Furthermore, we present a simple algo-
rithm to solve the abbreviation problem and a rule-
based method to deal with the cascaded phenom-
ena. Based on such evidences and methods, our 
system is successfully adapted to biomedical do-
main and achieves significantly better performance 
than the best published system.  In the near future, 
more effective abbreviation recognition algorithm 
and some pre-defined NE lists for some classes 
may be incorporated to enhance our system. 
Acknowledgements 
We would like to thank Mr. Tan Soon Heng for his 
support of biomedical knowledge.   
References 
M. Bikel Danie, R.Schwartz and M. Weischedel Ralph. 
1999.  An Algorithm that Learns What's in a Name. 
In Proc. of Machine Learning (Special Issue on NLP). 
A. Borthwick.  1999.  A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis. New 
York University. 
N. Collier, C. Nobata, and J. Tsujii.  2000.  Extracting 
the names of genes and gene products with a hidden 
Markov model.  In Proc. of COLING 2000, pages 
201-207. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.  
1998.  Toward information extraction: identifying 
protein names from biological papers.  In Proc. of the 
Pacific Symposium on Biocomputing?98 (PSB?98), 
pages 707-718, January. 
R. Gaizauskas, G. Demetriou and K. Humphreys.  Term 
Recognition and Classification in Biological Science 
Journal Articles.  2000.  In Proc. of the Computional 
Terminology for Medical and Biological Applications 
Workshop of the 2nd International Conference on 
NLP, pages 37-44. 
J. Kazama, T. Makino, Y.Ohta, and J. Tsujii.  2002.  
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition.  In Proc. of the Work-
shop on Natural Language Processing in the Bio-
medical Domain (at ACL?2002), pages 1-8. 
C. Nobata, N. Collier, and J. Tsujii.  1999.  Automatic 
term identification and classification in biology texts.  
In Proc. of the 5th NLPRS, pages 369-374. 
C. Nobata, N. Collier, and J. Tsujii.  2000.  Comparison 
between tagged corpora for the named entity task.  In 
Proc. of the Workshop on Comparing Corpora (at 
ACL?2000), pages 20-27. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.  
2002.  The GENIA corpus: An annotated research 
abstract corpus in molecular biology domain.  In 
Proc. of HLT 2002. 
D. Proux, F. Rechenmann, L. Julliard, V. Pillet and B. 
Jacq.  1998.  Detecting Gene Symbols and Names in 
Biological Texts: A First Step toward Pertinent In-
formation Extraction.  In Proc. of Genome Inform 
Ser Workshop Genome Inform, pages 72-80. 
A.S. Schwartz and M.A. Hearst.  2003.  A Simple Algo-
rithm for Identifying Abbreviation Definitions in 
Biomedical Text.  In Proc. of the Pacific Symposium 
on Biocomputing (PSB 2003) Kauai. 
T. Sekimizu, H. Park, and J. Tsujii.  1998.  Identifying 
the interaction between genes and gene products 
based on frequently seen verbs in medline abstracts.  
In Proc. of Genome Informatics, Universal Academy 
Press, Inc.  
K. Takeuchi and N. Collier.  2002.  Use of Support Vec-
tor Machines in Extended Named Entity Recognition.  
In Proc. of the Sixth Conference on Natural Lan-
guage Learning (CONLL 2002), pages 119-125. 
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and M. 
Carroll.  2000.  Automatic extraction of protein inter-
actions from scientific abstracts.  In Proc. of the Pa-
cific Symposium on Biocomputing?2000 (PSB?2000), 
pages 541-551, Hawaii, January. 
A. J. Viterbi.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding algo-
rithm.  In Proc. of IEEE Transactions on Information 
Theory, pages 260-269. 
J. Zhang, D. Shen, G. Zhou, J. Su and C. Tan. 2003.  
Exploring Various Evidences for Recognition of 
Named Entities in Biomedical Domain.  Submitted to 
EMNLP 2003. 
G. Zhou and J. Su.  2002.  Named Entity Recognition 
using an HMM-based Chunk Tagger.  In Proc. of the 
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 473-480. 
Coling 2010: Poster Volume, pages 766?774,
Beijing, August 2010
A Review Selection Approach for Accurate Feature Rating Estimation
Chong Long? Jie Zhang? Xiaoyan Zhu??
? State Key Laboratory on Intelligent Technology and Systems,
Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science, Tsinghua University
?School of Computer Engineering, Nanyang Technological University
?{Corresponding Author: zxy-dcs@tsinghua.edu.cn}
Abstract
In this paper, we propose a review se-
lection approach towards accurate esti-
mation of feature ratings for services on
participatory websites where users write
textual reviews for these services. Our
approach selects reviews that compre-
hensively talk about a feature of a service
by using information distance of the re-
views on the feature. The rating estima-
tion of the feature for these selected re-
views using machine learning techniques
provides more accurate results than that
for other reviews. The average of these
estimated feature ratings also better rep-
resents an accurate overall rating for the
feature of the service, which provides
useful feedback for other users to choose
their satisfactory services.
1 Introduction
Most of participatory websites such as Amazon
(amazon.com) do not collect from users feature1
ratings for services, simply because it may cost
users too much effort to provide detailed feature
ratings. Even for a very few websites that do col-
lect feature ratings such as a popular travel web-
site TripAdvisor (tripadvisor.com), a big portion
(approximately 43%) of users may still not pro-
vide them. However, feature ratings are useful
for users to make informed consumption deci-
sions especially in the case where users may be
interested more in some particular features of the
services. Machine learning techniques have been
proposed for sentiment classification (Pang et al,
2002; Mullen and Collier, 2004) based on anno-
tated samples from experts, but they have limited
1A feature broadly means an attribute or a function of a
service.
performance especially when estimating ratings
of a multi-point scale (Pang and Lee, 2005).
In this paper, we propose a novel review se-
lection approach for accurate feature rating es-
timation. More specifically, our approach se-
lects reviews written by the users who compre-
hensively talk about a certain feature of a ser-
vice - that are comprehensive on this feature, us-
ing information distance of reviews on the fea-
ture based on Kolmogorov complexity (Li and
Vita?nyi, 1997). This feature is obviously impor-
tant to the users. People tend to be more knowl-
edgable in the aspects they consider important.
These users therefore represent a subset of ex-
perts. Statistical analysis reveals that these ex-
pert users are more likely to agree on a common
rating for the feature of the service. The rating
estimation of the feature for these selected re-
views based on annotated samples from experts
using machine learning techniques is thus able to
provide more accurate results than that for other
reviews. This statistical evidence also allows us
to use the average of the estimated feature rat-
ings to better represent an overall opinion of ex-
perts for the feature of the service, which will
be particularly useful for assisting other users to
correctly make their consumption decisions.
We verify our approach and arguments based
on real data collected from the TripAdvisor web-
site. First, our approach is shown to be able to
effectively select reviews that comprehensively
talk about features of a service. We then adopt
the machine learning method proposed in (Pang
and Lee, 2005) and the Bayesian Network clas-
sifier (Russell and Norvig, 2002) for feature rat-
ing estimation. Our experimental results show
that the accuracy of estimating feature ratings
for these selected reviews is higher than that
for other reviews, for both the machine learning
766
methods. And, the average of these estimated
ratings is testified to closely represent the over-
all feature rating of the service. Our approach is
therefore verified to be a successful step towards
accurate feature rating estimation.
2 Related Work
Our work aims at estimating feature ratings of a
service based on its textual reviews. It is related
to sentiment classification. The task of sentiment
classification is to determine the semantic orien-
tations of words, sentences or documents. (Pang
et al, 2002) is the earliest work of automatic
sentiment classification at document level, using
several machine learning approaches with com-
mon textual features to classify movie reviews.
Mullen and Collier (Mullen and Collier, 2004)
integrated PMI values, Osgood semantic factors
and some syntactic relations into the features of
SVM. Pang and Lee (Pang and Lee, 2004) pro-
posed another machine learning method based
on subjectivity detection and minimum-cut in
graph. However, these approaches focus only on
binary classification of reviews.
In 2005, Pang and Lee extended their ear-
lier work in (Pang and Lee, 2004) to determine
a reviewer?s evaluation with respect to multi-
scales (Pang and Lee, 2005). The rating esti-
mation is viewed as multi-class sentiment cate-
gorization on documents. They used SVM re-
gression as the multi-class classifier, and also ap-
plied a meta-algorithm based on a metric label-
ing formulation of the problem, which alters a
given n-ary classifier?s output in an explicit at-
tempt to ensure that similar items receive sim-
ilar labels. They collected movie reviews from
a website named IMDB and tested the perfor-
mance of their classifier under both four-class
and five-class categorization. The five-class sen-
timent classification is adopted in the evaluation
of our method (see Section 5). The performance
of their approach is limited. One important rea-
son is that their method considers every review
when estimating a feature rating of a movie.
However, some reviews do not contain much of
the users? opinions about a certain feature sim-
ply because the users do not care much or are
not knowledgable about the feature. In our work,
we study the characteristics of reviews? feature
ratings. We investigate which reviews are more
useful for us to estimate feature ratings. From
some observations stated in the next section, we
will see that reviews written by different users
reflect their own preferred features of a service.
3 Accurate Feature Rating Estimation
Participatory websites allow users to write tex-
tual reviews to discuss features of services that
they have consumed. These reviews usually con-
tain words that strongly express the users? opin-
ions about the corresponding features. These
words contain important information for estimat-
ing a numerical rating for the feature. The es-
timated ratings can be used for assisting other
users when they need to choose which services
to consume. Machine learning techniques are
often used for training a learner based on an-
notated samples from experts and estimating a
rating for a feature discussed in a review. How-
ever, for a review that does not mention a fea-
ture or discusses it only in a limited sense, the
estimation accuracy is expected to be very low.
Besides, the opinion expressed by the user who
writes this kind of review is not representative
because this user obviously does not care much
about the feature. We believe that if we carefully
select reviews for estimating feature ratings, the
accuracy will be increased and the estimated rat-
ings will be more representative.
We then statistically analyze real data col-
lected from the TripAdvisor website. The results
reveal that users who comprehensively discuss
a feature of a service in their reviews are more
likely to agree on a common rating for this fea-
ture of the service. This phenomenon can also
be intuitively explained as follows. For the users
who comprehensively discuss about a feature,
the feature is obviously more important to them.
People tend to be more knowledgable in the as-
pects they consider important. These users there-
fore represent a subset of experts. Experts likely
provide more objective and representative feed-
back about the feature, and therefore the ratings
from them for the feature contain less noise and
767
are more similar.
Based on the above discussion that experts
tend to have similar opinions on a feature of a
service, a learner trained by a machine learning
technique based on annotated samples from ex-
perts should then be able to more accurately esti-
mate the feature ratings from reviews written by
other experts. Since the opinions of experts con-
verge, the average of the estimated feature rat-
ings also better represents an overall rating for
the feature of the service.
We propose a review selection approach us-
ing information distance of reviews on the fea-
ture based on Kolmogorov complexity, to select
reviews that comprehensively discuss a feature
of a service. We rank the reviews based on the
comprehensiveness on the feature. The top re-
views will be selected for the estimation of fea-
ture ratings. Also, the average of these estimated
feature ratings will be used for representing the
overall rating for the feature. Next, we will first
describe in detail how our approach selects com-
prehensive reviews on a given feature.
4 Our Review Selection Approach
Our review selection approach selects reviews
that comprehensively talk about a feature. Ac-
cording to this definition, a review?s comprehen-
siveness depends on the amount of information
discussed on a feature. We use Kolmogorov
complexity and information distance to measure
the amount of information. Kolmogorov com-
plexity was introduced almost half a century ago
by R. Solomonoff, A.N. Kolmogorov and G.
Chaitin, see (Li and Vita?nyi, 1997). It is now
widely accepted as an information theory for in-
dividual objects parallel to that of Shannon?s in-
formation theory which is defined on an ensem-
ble of objects.
4.1 Theory
Fix a universal Turing machine U . The Kol-
mogorov complexity (Li and Vita?nyi, 1997) of a
binary string x condition to another binary string
y, KU (x|y), is the length of the shortest (prefix-
free) program for U that outputs x with input y.
It can be shown that for different universal Tur-
ing machine U ?, for all x, y
KU (x|y) = KU ?(x|y) + C,
where the constant C depends only on U ?. Thus
KU (x|y) can be simply written as K(x|y). They
write K(x|?), where ? is the empty string, as
K(x). It has also been defined in (Bennett et
al., 1998) that the energy to convert between x
and y to be the smallest number of bits needed to
convert from x to y and vice versa. That is, with
respect to a universal Turing machine U , the cost
of conversion between x and y is:
E(x, y)=min{|p|: U(x, p)=y, U(y, p)=x}
(1)
It is clear that E(x, y) ? K(x|y) + K(y|x).
From this observation, the following theorem has
been proved in (Bennett et al, 1998):
Theorem 1 E(x, y) = max{K(x|y),K(y|x)}.
Thus, the max distance was defined in (Ben-
nett et al, 1998):
Dmax(x, y) = max{K(x|y),K(y|x)}. (2)
This distance is shown to satisfy the basic
distance requirements such as positivity, sym-
metricity, triangle inequality and is admissible.
Here for an object x, we can measure its in-
formation by Kolmogorov complexity K(x); for
two objects x and y, their shared information can
be measured by information distance D(x, y).
In (Long et al, 2008), the authors generalize
the theory of information distance to more than
two objects. Similar to Equation 1, given strings
x1, . . . , xn, they define the minimum amount of
thermodynamic energy needed to convert from
any xi to any xj as:
Em(x1, .., xn)=min{|p|:U(xi, p, j)=xj for all i,j}
Then it is proved in (Long et al, 2008) that:
Theorem 2 Modulo to an O(logn) additive fac-
tor,
min
i
K(x1 . . . xn|xi) ? Em(x1, . . . , xn)
Given n objects, the left-hand side of Equa-
tion 3 may be interpreted as the most compre-
hensive object that contains the most information
about all of the others.
768
4.2 Review Selection Method
Our review selection method is based on the in-
formation distance discussed in the previous sec-
tion. However, our problem is that neither the
Kolmogorov complexity K(?, ?) nor Dmax(?, ?)
is computable. Therefore, we find a way to ?ap-
proximate? these two measures. The most use-
ful information in a review article is the English
words that are related to the features. If we can
extract all of these related words from the review
articles, the size of the word set can be regarded
as a rough estimation of information content (or
Kolmogorov complexity) of the review articles.
In Section 5 we will see that this gives very good
practical results.
4.2.1 Outline
Our method is outlined in the following. First,
for each type of product or service (such as a ho-
tel), a small set of core feature words (such as
price and room) is generated through statistics.
Then, these feature words are used to generate
the expanded words. Third, a parser is used to
find the dependent words associated with the oc-
currence of the core feature words and expanded
words in a review. For each review-feature pair,
the union of the core feature words, expanded
words and dependent words in the review defines
the related word set of the review on the feature.
Lastly, information distance is used to select the
most comprehensive reviews on a feature.
4.2.2 Word Extraction
Feature words are the most direct and frequent
words describing a feature, for example, price,
room or service of a hotel. Given a feature, the
core feature words are the very few most com-
mon English words that are used to refer to that
feature. For example, both ?value? and ?price?
are used to refer to the same feature of a ho-
tel. In (Hu and Liu, 2004), the authors indicate
that when customers comment on product fea-
tures, the words they use converge. If we re-
move the feature words with frequency lower
than 1% of the total frequency of all feature
words, the remaining words, which are just core
feature words, can still cover more than 90%
occurrences. So firstly we extract those words
through statistics; then some of those with the
same meaning (such as ?value? and ?price?) are
grouped into one feature. They are just ?core fea-
ture words?.
Apart from core feature words, many other
less-frequently used words that are connected
to the feature also contribute to the information
content of the feature. For example, ?price? is
an important feature of a hotel, but the word
?price? is usually dropped from a sentence. In-
stead, words such as ?$?, ?dollars?, ?USD?, and
?CAD? are used. We use information distance
d(., .) based on Google to expand words (Cili-
brasi and Vita?nyi, 2007). Let ? be a feature and
A be the set of its core feature words. The dis-
tance between a word w and the feature ? is then
defined to be
d(w,?) = min
v?A
d(w, v)
A distance threshold is then used to determine
which words should be in the set of expanded
words for a given feature.
If a core feature word or an expanded word is
found in a sentence, the words which have gram-
matical dependent relationship with it are called
the dependent words (de Marneffe et al, 2006).
For example, in sentence ?It has a small, but
beautiful room?, the words ?small? and ?beauti-
ful? are both dependent words of the core feature
word ?room?. All these words also contribute to
the reviews and are important to determine the
reviewer?s attitude towards a feature.
The Stanford Parser (de Marneffe et al, 2006)
is used to parse each review. For review i and
feature j, the core feature words and expanded
words in the review are first computed. Then the
parsing result is examined to find all the depen-
dent words for the core feature words and ex-
panded words, all of which are called ?related
words?.
4.2.3 Computing Information Distance
If there are m reviews x1, x2, . . . , xm, n fea-
tures u1, u2, . . . , un, and the related word set Si
is defined to be the union of all the related words
that occur in the review xi. From the left-hand
side of Equation 3, the most comprehensive xi
769
on feature uk is such that
i = argmin
i
K(S1 . . . Sn|Si, uk). (3)
Let Si and Sj be two sets of words,
K(SiSj |uk) = K(Si ? Sj |uk),
K(Si|Sj , uk) = K(Si \ Sj |uk),
and
K(Si|uk)=
?
w
K(w|uk)?
?
w
(K(w, uk)?K(uk))
where w ? Si and w is in xi?s related word set on
feature uk. For each word w in a set S, the Kol-
mogorov complexity can be estimated through
coding theorem (Li and Vita?nyi, 1997):
K(w, uk)=? logP (w, uk), K(uk)=? logP (uk)
where P (w, uk) can be estimated by df(w, uk),
which is the document frequency of word w and
feature uk co-exist on the whole corpus. Sim-
ilarly, P (uk) can be estimated by feature uk?s
document frequency on the corpus. In the next
section, Equation 3 will be used to select reviews
that comprehensively talk about a feature.
5 Experimental Verification
In this section, we present a set of experimen-
tal results to support our work. Our experiments
are carried out using real data collected from the
travel website TripAdvisor. This website indexes
hotels from cities across the world. It collects
feedback from travelers. Feedback of each trav-
eler consists of a textual review written by the
traveler and numerical ratings (from 1, lowest,
to 5, highest) for different features of hotels (e.g.,
value, service, rooms).
Table 1: Summary of the Data Set
Location# Hotels# Feedback# Feedback with
feature rating
Boston 57 3949 2096
Sydney 47 1370 879
Vegas 40 5588 3144
We crawled this website to collect travelers?
feedback for hotels in three cities: Boston, Syd-
ney and Las Vegas. Note that during this crawl-
ing process, we carefully removed information
about travelers and hotels to protect their privacy.
For users? feedback, we recorded only the tex-
tual reviews and the numerical ratings on four
features: Value(V), Rooms(R), Service(S) and
Cleanliness(C). These features are rated by a sig-
nificant number of users. Table 1 summarizes
our data set. For each one of the cities, this table
contains information about the number of hotels,
the total amount of feedback and the amount of
feedback with feature ratings. In general, each
hotel has sufficient amount of feedback with fea-
ture ratings for us to evaluate our work.
Table 2: Comprehensive Reviews on Each Fea-
ture (Boston)
Top # V R S C
1 Y Y Y Y
2 Y Y Y Y
3 N Y Y N
4 Y Y Y N
5 Y Y Y Y
6 Y Y N Y
... ... ... ... ...
5.1 Evaluation of Review Selection
We first evaluate the performance of our re-
view selection approach using manually anno-
tated data. More specifically, in our data set,
for one city, 40 reviews (120 reviews in total)
are selected for manual annotation. The annota-
tor looks over each review and decides whether
the review is comprehensive on a given feature.
Comprehensive reviews on the feature are anno-
tated as ?Y?, and the reviews that are not com-
prehensive on this feature are annotated as ?N?.
For the review set of each city, the number of re-
views annotated as comprehensive is equal to or
less than 20% of the total number of the selected
reviews for this city (eight in this experiment).
Note that it is possible that one review can be
comprehensive on more than one features.
We then use our review selection approach
770
discussed in Section 4 to rank the reviews for ho-
tels in each city, according to their comprehen-
siveness on each feature. For example, the most
comprehensive review on the feature ?Value?,
which has the minimal information distance to
this feature (see Equation 3), is ranked No.1. Ta-
ble 2 shows the annotated reviews for Boston ho-
tels that are ranked on top six on each feature. It
can be obviously seen from the table that most
of these top reviews are labeled as comprehen-
sive reviews on respective features. Our com-
prehensive review selection approach generally
performs well.
Table 3: Performance of Comprehensive Review
Selection
City Feature Precision Recall F-Score
Boston V 0.833 0.714 0.769
R 1.000 0.875 0.933
S 0.857 1.000 0.923
C 0.833 1.000 0.909
Sydney V 0.667 1.000 0.800
R 0.600 0.857 0.706
S 0.667 0.857 0.750
C 0.750 1.000 0.857
Vegas V 0.778 1.000 0.875
R 0.727 1.000 0.842
S 0.714 0.714 0.714
C 0.667 0.800 0.727
To clearly present the performance of our
comprehensive review selection approach, we
use the measures of precision, recall and f-score.
The measure f-score is a single value that can
represent the result of our evaluation. It is the
harmonic mean of precision and recall. Suppose
there are n reviews in total. Let pjk (1 ? k ? n)
be the review ranked the kth comprehensive on
feature j. Define
zjk =
{
1 if pjk is labelled comprehensive on j;
0 otherwise.
The precision P , recall R, and f-score F of top k
comprehensive reviews on feature j are formal-
ized as follows
Pjk =
?k
l=1 zjl
k ,Rjk =
?k
l=1 zjl?N
l=1 zjl
,
Fjk =
2PjkRjk
Pjk +Rjk
For each ranked review set on feature j, the
maximum Fjk and its associated Pjk and Rjk are
listed in Table 3. From this table, it can be seen
that for the best f-scores, the precision and recall
values are mostly larger than 70%, that is, a great
part of reviews that are labeled as comprehensive
receive top rankings from our comprehensive re-
view selection approach. Our approach is thus
carefully verified to be able to accurately select
comprehensive reviews on any given feature.
5.2 Statistical Analysis
A group of users who comprehensively discuss
a certain feature are more likely to agree on a
common rating for that feature. In this experi-
ment, we use our review selection approach to
verify this argument.
Table 4: Deviation of Feature Ratings
City Feature 20% 50% All
V 0.884 (0.0003) 1.030 1.136
Boston R 0.940 (0.2248) 1.037 1.013
S 1.026 (0.0443) 1.130 1.144
C 0.798 (0.0093) 0.892 0.949
V 0.862 (0.0266) 1.009 1.054
Sydney R 0.788 (0.0497) 0.932 0.945
S 0.941 (0.0766) 1.162 1.116
C 0.651 (0.0037) 0.905 0.907
V 0.845 (0.0002) 1.236 1.291
Vegas R 1.105 (0.2111) 1.148 1.175
S 1.112 (0.0574) 1.286 1.269
C 0.936 (0.0264) 1.096 1.158
More specifically, for each city, hotels that re-
ceive no less than 10 reviews with feature ratings
are selected. We use our comprehensive review
selection approach to select top 20% and 50%
comprehensive reviews on each feature for ho-
tels in each city. We calculate the standard devi-
ation of their feature ratings, as well as that of all
feature ratings, for each hotel in a city. We then
average these standard deviations over the hotels
in the same city. The average values are listed
in Table 4. The feature ratings of comprehensive
reviews on the feature have smaller average stan-
771
dard deviations. Standard T-test is used to mea-
sure the significance of the results between top
20% comprehensive reviews and all reviews, city
by city and feature by feature. Their p-values are
shown in the braces, and they are significant at
the standard 0.05 significance threshold. It can
be seen from the table that although for some
items there does not seem to be a significant dif-
ference, the results are significant for the entire
data set.
Therefore, when these travelers write reviews
that are comprehensive on one feature, their rat-
ings for this feature tend to converge. This evi-
dence indicates that the estimation of ratings for
the feature from these comprehensive reviews
can provide better results, which will be con-
firmed in Section 5.3. These estimated feature
ratings can also be averaged to represent a spe-
cific opinion of these travelers on the feature,
which will be verified in Section 5.4.
5.3 Feature Rating Estimation
In this section, we carry out experiments to tes-
tify that the estimation of feature ratings for com-
prehensive reviews using our review selection
approach provides better performance than that
for all reviews. We adopt the approach of Pang
and Lee (Pang and Lee, 2005) described in Sec-
tion 2 for feature rating estimation. In short, they
applied a meta-algorithm, based on a metric la-
beling formulation of the problem to alter a given
n-ary SVM?s output in an explicit attempt. We
also adopt a Baysian Network classifier for fea-
ture rating estimation.
Similar to the method of Pang and Lee, we
build up a feature rating classification system to
estimate reviews? feature ratings. However, the
method of Pang and Lee focuses only on sin-
gle rating classification for a review and assumes
that every word of the review can contribute to
this single rating. While it comes to feature rat-
ing classification, the system has to decide which
terms or phrases in the review are talking about
this feature. We train a Naive Bayes classifier
to retrieve all the sentences related to a feature.
Then all the core feature words, expanded words
and dependent words are extracted to train a
SVM classifier and the Bayesian Network clas-
sifier for five-class classification (1 to 5). The
eight-fold cross-validation is used to train and
test the performance of feature rating estimation
on all the reviews and the top 20% comprehen-
sive reviews, respectively.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
Value Rooms Service Clean Average
Av
era
ge
 Di
ffe
ren
ce
Comprehensive ReviewsAll Reviews
Figure 1: Average Error of Feature Rating Esti-
mation for the Adopted Method of Pang and Lee
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
Value Rooms Service Clean Average
Av
era
ge
 Di
ffe
ren
ce
Comprehensive ReviewsAll Reviews
Figure 2: Average Error of Feature Rating Esti-
mation for the Bayesian Network classifier
We formalize a performance measure as fol-
lows. Suppose there are n reviews in total. For a
test review i (1 ? i ? n), its real feature rating
(given by the review writer) is fi, and its predi-
cated feature rating (predicted by our classifica-
tion system) is gi. Both fi and gi are integers
between 1 and 5. The performance of the classi-
fication on all n reviews can be measured by the
average of the absolute difference (d) between
each fi and gi pair,
d =
?n
i=1 |fi ? gi|
n . (4)
The lower d is, the better performance the clas-
sifier can provide.
Figures 1 and 2 show the results for the perfor-
mance of feature rating estimation on all reviews
versus that on selected comprehensive reviews,
772
for the adopted approach of Pang and Lee and
the Baysian Network classifier respectively. It
can be seen that the average difference between
real feature ratings and estimated feature ratings
on each feature when using selected comprehen-
sive reviews is significantly lower than that when
using all reviews, for both the approaches. On
average, the performance of feature rating esti-
mation is improved by more than 12.5% using
our review selection approach. And, our review
selection approach is generally applicable to dif-
ferent classifiers.
5.4 Estimating Overall Feature Rating
Supported by the statistical evidence verified in
Section 5.2 that the users who write compre-
hensive reviews on one feature will more likely
agree on a common rating for this feature, we
can then use an average of the feature ratings for
top 20% comprehensive reviews to reflect a gen-
eral opinion of knowledgable/expert users. In
this section, we show directly the performance
of estimating an overall feature rating for a ho-
tel using ratings for the selected comprehensive
reviews, and compare it with that for all reviews.
Table 5: Performance of Estimating Overall Fea-
ture Rating for Comprehensive Reviews
City V R S C AVG
Boston 0.637 0.426 0.570 0.660 0.573
Sydney 0.273 0.729 0.567 0.680 0.562
Vegas 0.485 0.502 0.277 0.613 0.469
Average 0.465 0.552 0.471 0.651 0.535
Table 6: Performance of Estimating Overall Fea-
ture Rating for All reviews
City V R S C AVG
Boston 0.809 0.791 0.681 0.642 0.731
Sydney 0.433 0.886 0.588 0.593 0.625
Vegas 0.652 0.733 0.502 0.942 0.707
Average 0.631 0.803 0.590 0.726 0.688
Suppose there are m hotels. For each hotel
j, we first select the top 20% comprehensive re-
views on each feature using our review selection
approach. We average the real ratings of one fea-
ture provide by travelers for these reviews, de-
noted as f?j . We then estimate the feature rat-
ings for these comprehensive reviews using the
adopted machine learning method of Pang and
Lee. The average of these estimated ratings is
denoted as g?j . Similar to Equation 4, the av-
erage difference between all f?j and g?j pairs on
each feature for hotels in each city are calculated
and listed in Table 5. From this table, we can
see that the average difference between the es-
timated average feature rating and real average
feature rating is only about 0.53. Our review
selection approach produces fairly good perfor-
mance for estimating an overall feature rating for
a hotel. We then also calculate the average dif-
ference for all reviews. The results are listed in
Table 6. We can see that the average difference
is larger (about 0.69) in this case. The perfor-
mance of estimating an overall feature rating is
increased by nearly 23.2% through our review
selection approach.
6 Conclusion
In this paper, we presented a novel review selec-
tion approach to improve the accuracy of feature
rating estimation. We select reviews that com-
prehensively talk about a feature of one service,
using information distance of reviews on the fea-
ture based on Kolmogorov complexity. As eval-
uated using real data, the rating estimation for
the feature from these reviews provides more ac-
curate results than that for other reviews, inde-
pendent of which classifiers are used. The aver-
age of these estimated feature ratings also better
represents an accurate overall rating for the fea-
ture of the service.
In future work, we will further improve the ac-
curacy of estimating a general rating for a feature
of a service based on the selected comprehensive
reviews on this feature using our review selec-
tion approach. Comprehensive reviews may con-
tribute differently to the estimation of an overall
feature rating. In our next step, a more sophisti-
cated model will be developed to assign different
weights to these different reviews.
773
References
Bennett, C.H., P Gacs, M Li, P.M.B. Vita?nyi, and
W.H. Zurek. 1998. Information distance. IEEE
Transactions on Information Theory, 44(4):1407?
1423, July.
Cilibrasi, Rudi L. and Paul M.B. Vita?nyi. 2007.
The google similarity distance. IEEE Transactions
on Knowledge and Data Engineering, 19(3):370?
383, March.
de Marneffe, Marie Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In The fifth international conference on Language
Resources and Evaluation (LREC), May.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In 10th ACM Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 168?177.
Li, M. and P. Vita?nyi. 1997. An Introduction
to Kolmogorov Complexity and its Applications.
Springer-Verlag.
Long, Chong, Xiaoyan Zhu, Ming Li, and Bin Ma.
2008. Information shared by many objects. In
ACM 17th Conference on Information and Knowl-
edge Management.
Mullen, Tony and Nigel Collier. 2004. Sentiment
analysis using support vector machines with di-
verse information sources. In Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 412?418, July.
Pang, Bo and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 271?278, July.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment catego-
rization with respect to rating scales. In Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 115?124, June.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? sentiment
classification using machine learning techniques.
In Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86,
July.
Russell, S. and P. Norvig. 2002. Artificial Intel-
ligence: A Modern Approach. Second Edition,
Prentice Hall, Englewood Cliffs, New Jersey.
774
