Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), page 2,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modeling word learning as communicative inference
Michael C. Frank
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139
mcfrank@mit.edu
Abstract
How do children learn their first words? I
describe a model that makes joint inferences
about what speakers are trying to talk about
and the meanings of the words they use. This
model provides a principled framework for in-
tegrating a wide variety of non-linguistic in-
formation sources into the process of word
learning.
Talk Pre?cis
How do children learn their first words? Much
work in this field has focused on the social as-
pects of word learning: that children make use of
speakers? intentions?as signaled by a wide range
of non-linguistic cues such as their eye-gaze, what
they are pointing at, or even what referents are new
to them?to infer the meanings of words (Bloom,
2002). However, recent evidence has suggested that
adults and children are able to learn words simply
from the consistent co-occurrence of words and their
referents, even across otherwise ambiguous situa-
tions and without explicit social cues as to which ref-
erent is being talked about (Yu & Smith 2007; Smith
& Yu, 2008).
In this talk I describe work aiming to combine
these two sets of evidence within a single probab-
listic framework (Frank, Goodman, & Tenenbaum,
2009). We propose a model in which learners at-
tempt to infer speakers? moment-to-moment com-
municative intentions jointly with the meanings of
the words they have used to express these intentions.
This process of joint inference allows our model to
explain away two major sources of noise in sim-
pler statistical word learning proposals: the fact that
speakers do not talk about every referent and that not
all words that speakers utter are referential.
We find that our model outperforms associative
models in learning words accurately from natural
corpus data and is able to fit children?s behavior in
a number of experimental results from developmen-
tal psychology. In addition, we have used this basic
framework to begin investigating how learners use
the rich variety of non-linguistic information signal-
ing speakers? intentions in service of word learning.
As an example of this work, I will describe an ex-
tension of the model to use discourse continuity as a
cue for speakers? intentions.
Acknowledgments
This work supported by a Jacob Javits Graduate Fel-
lowship and NSF Doctoral Dissertation Research
Improvement Grant #0746251.
References
Paul Bloom. 2002. How Children Learn the Meanings of
Words. Cambridge, MA: MIT Press.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Using speakers? referential inten-
tions to model early cross-situational word learning.
Psychological Science.
Linda Smith and Chen Yu. 2008. Infants rapidly learn
word-referent mappings via cross-situational statistics.
Cognition, 106, 1558-1568.
Chen Yu and Linda Smith. 2007. Rapid word learning
under uncertainty via cross-situational statistics. Psy-
chological Science, 18, 414-420.
2
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501?509,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Words and Their Meanings from Unsegmented Child-directed
Speech
Bevan K. Jones & Mark Johnson
Dept of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912, USA
{Bevan Jones,Mark Johnson}@Brown.edu
Michael C. Frank
Dept of Brain and Cognitive Science
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
mcfrank@mit.edu
Abstract
Most work on language acquisition treats
word segmentation?the identification of lin-
guistic segments from continuous speech?
and word learning?the mapping of those seg-
ments to meanings?as separate problems.
These two abilities develop in parallel, how-
ever, raising the question of whether they
might interact. To explore the question, we
present a new Bayesian segmentation model
that incorporates aspects of word learning and
compare it to a model that ignores word mean-
ings. The model that learns word meanings
proposes more adult-like segmentations for
the meaning-bearing words. This result sug-
gests that the non-linguistic context may sup-
ply important information for learning word
segmentations as well as word meanings.
1 Introduction
Acquiring a language entails mastering many learn-
ing tasks simultaneously, including identifying
where words begin and end in continuous speech
and learning meanings for those words. It is com-
mon to treat these tasks as separate, sequential pro-
cesses, where segmentation is a prerequisite to word
learning but otherwise there are few if any depen-
dencies. The earliest evidence of segmentation,
however, is for words bordering a child?s own name
(Bortfeld et al, 2005). In addition, infants begin
learning their first words before they achieve adult-
level competence in segmentation. These two pieces
of evidence raise the question of whether the tasks of
meaning learning and segmentation might mutually
inform one another.
To explore this question we present a joint model
that simultaneously identifies word boundaries and
attempts to associate meanings with words. In do-
ing so we make two contributions. First, by model-
ing the two levels of structure in parallel we simu-
late a more realistic situation. Second, a joint model
allows us to explore possible synergies and interac-
tions. We find evidence that our joint model per-
forms better on a segmentation task than an alterna-
tive model that does not learn word meanings.
The picture in Figure 1 depicts a language learn-
ing situation from our corpus (originally from Fer-
nald and Morikawa, 1993; recoded in Frank et al,
2009) where a mother talks while playing with var-
ious toys. Setting down the dog and picking up the
hand puppet of a pig, she asks, ?Is that the pig??
Starting out, a young learner not only does not know
that the word ?pig? refers to the puppet but does not
even know that ?pig? is a word at all. Our model
simulates the learning task, taking as input the un-
segmented phonemic representation of the speech
along with the set of objects in the non-linguistic
context as shown in Figure 1 (a), and infers both a
segmentation and a word-object mapping as in Fig-
ure 1 (b).
One can formulate the word learning task as
that of finding a reasonably small set of reusable
word-meaning pairs consistent with the underlying
communicative intent. Infant directed speech often
refers to objects in the immediate environment, and
early word learning seems to involve associating fre-
quently co-occurring word-object pairs (Akhtar and
Montague, 1999; Markman, 1990). Several compu-
tational models are based on this idea that a word
501
Figure 1: (a) The input to our system for the utterance
?Is that the pig?? consists of an unsegmented sequence
of phonemes and the set of objects representing the non-
linguistic context. These objects were manually iden-
tified by inspecting the associated video, a frame from
which is shown above. (b) The gold-standard segmenta-
tion and word-object assignments of the same utterance,
against which the output of our system is evaluated (all
words except ?pIg? are mapped to a special ?null? object,
as explained in the text).
that frequently occurs in the presence of an object
and not so frequently in its absence is likely to re-
fer to that object (Frank et al, 2009a; Siskind, 1996;
Yu and Ballard, 2007). Importantly, all these models
assume words are pre-segmented in the input.
While the word segmentation task relates less
clearly to the communicative content, it can be for-
mulated according to a similar objective, that of at-
tempting to explain the sound sequences in the input
in terms of some reasonably small set of reusable
units, or words. Computational models have suc-
cessfully addressed the problem in much this way
(Johnson and Goldwater, 2009; Goldwater et al,
2009; Brent, 1999), and the general approach is con-
sistent with experimental observations that humans
are sensitive to statistics of sound sequences (Saffran
et al, 1996; Frank et al, 2007).
The two tasks can be integrated in a relatively
seamless way, since, as we have just formulated
them, they have a common objective, that of finding
a minimal, consistent set of reusable units. However,
the two deal with different types of information with
different dependencies. The basic idea is that learn-
ing a vocabulary that both meets the constraints of
the word-learning task and is consistent with the ob-
jective of the segmentation task can yield a better
segmentation. That is, we hope to find a synergy in
the joint inference of meaning and segmentation.
Note that to the best of our knowledge there is
very little computational work that combines word
form and word meaning learning (Frank et al 2006
takes a first step but their model is applicable only
to small artificial languages). Frank et al (2009a)
and Regier (2003) review pure word learning mod-
els and, in addition to the papers we have already
cited, Brent (1999) presents a fairly comprehensive
review of previous pure segmentation models. How-
ever, none of the models reviewed make any attempt
to jointly address the two problems. Similarly, in the
behavioral literature on development, we are aware
of only one segmentation study (Graf-Estes et al,
2007) that involves non-linguistic context, though
this study treats the two tasks sequentially rather
than jointly.
We now describe our model and inference proce-
dure and follow with evaluation and discussion.
2 Model Definition
Cross-situational meaning learning in our joint word
learning and segmenting model is inspired by the
model of Frank et al (2009a). Our model can
be viewed as a variant of the Latent Dirichlet Al-
location (LDA) topic model of Blei et al (2003),
where topics are drawn from the objects in the non-
linguistic context. The model associates each utter-
ance with a single referent object, the topic, and ev-
ery word in the utterance is either generated from a
distribution over words associated with that object
or else from a distribution associated with a special
?null? object shared by all utterances. Note that in
this paper we use ?topic? to denote the referent ob-
ject of an utterance, otherwise we depart from topic
modeling convention and use the term ?object? in-
stead.
Segmentation is based on the unigram model pro-
posed by Brent (1999) and reformulated by Goldwa-
ter et al (2009) in terms of a Dirichlet process. Since
both LDA and the unigram segmenter are based on
unigram distributions it is relatively straightforward
502
Figure 2: Topical Unigram Model: Oj is the set of objects
in the non-linguistic context of the jth utterance, zj is the
utterance topic, wji is the ith word of the utterance, xji is
the category of the word (referring or non-referring), and
the other variables are distribution parameters.
to integrate the two to simultaneously infer word
boundaries and word-object associations.
Figure 2 illustrates a slightly simplified form of
the model, and the the relevant distributions are as
follows:
z|O ? Uniform(O)
Gz|z, ?0, ?1, P0 ?
{
DP(?1, P0) if z 6= 0
DP(?0, P0) otherwise
pi ? Beta(1, 1)
x|pi ? Bernoulli(pi)
w|G, z, x ?
{
Gz if x = 1
G0 if x = 0
Note that Uniform(O) denotes a discrete uniform
distribution over the elements of the set O. P0 is
described later.
Briefly, each utterance has a single topic zj , drawn
from the objects in the non-linguistic context Oj ,
and then for each word wji we first flip a coin xji
to determine if it refers to the topic or not. Then, de-
pending on xji the word is either drawn from a dis-
tribution specific to the topic (xji = 1) or from a dis-
tribution associated with the ?null? object (xji = 0).
In slightly greater detail but still glossing over the
details of how the multinomial parameters are gen-
erated, the generative story proceeds as follows:
1. For each utterance, indexed by j
2. (a) Pick a single topic zj uniformly from the set
of objects in the environment Oj
(b) For each word wji of the utterance
(c) i. Determine if it refers to zj or not by set-
ting xji to 1 (referring) with probability pi,
and to 0 (non-referring) otherwise.
ii. if xji is 1, draw wji from the topic specific
distribution over words Gzj .
iii. otherwise, draw wji from G0, the distribu-
tion over words associated with the ?null?
object.
This generative story is a simplification since it
does not describe how we model utterance bound-
aries. It is important for segmentation purposes
to explicitly model utterance boundaries since, un-
like utterance-internal word boundaries, we as-
sume utterance boundaries are observed. Thus,
the story is complicated by the fact that there is
a chance each time we generate a word that we
also generate an utterance boundary. The choice of
whether to terminate the utterance or not is captured
by a Bernoulli(?) random variable $ji indicating
whether the ith word was the last word of the jth
utterance.
? ? Beta(1, 1)
$|? ? Bernoulli(?)
The Gz multinomial parameters are generated
from a Dirichlet process with base distribution over
words, P0, which describes how new word types
are generated from their constituent phonemes.
Phonemes are generated sequentially, i.i.d. uni-
formly from m phonemic types. In addition, there
is a probability p# of generating a word boundary.
P0(w) = (1? p#)|w|?1p#
1
m|w|
The concentration parameters ?0 and ?1 also play
a critical role in the generation of words and word
types. Any given word has a certain probability
of either being produced from the set of previously
seen word types, or from an entirely new one. The
503
greater the concentration parameter, the more likely
the model is to appeal to the base distribution P0 to
introduce a new word type.
Like Frank et al (2009a), we distinguish between
two coarse grammatical categories, referring and
non-referring. Referring words are generated by the
topic, while non-referring words are drawn from G0,
a distribution associated with the ?null? object. The
distinction ensures sparse word-object maps that
obey the principle of mutual exclusion. Otherwise
all words in the utterance would be associated with
the topic object, resulting in a very large set of words
for each object that is very likely to overlap with the
words for other objects. As a further bias toward
a small lexicon, we employ different concentration
parameters (?0 and ?1) for the non-referring and re-
ferring words, using a much smaller value for the
referring words. Intuitively, there should be a rela-
tively small prior probability of introducing a new
word-object pair, corresponding to a small ?1 value.
On the other hand, most other words don?t refer to
the topic object (or any other object for that matter),
corresponding to a much larger ?0 value.
Note that this topical unigram model is a straight-
forward generalization of the unigram segmentation
model (Goldwater et al, 2009) to the case of multi-
ple topics. In fact, if all words were assumed to refer
to the same object (or to no object at all) the models
would be identical.
Unlike LDA, each ?document? has only one topic,
which is necessitated by the fact that in our model
documents correspond single utterances. The ut-
terances in our corpus of child directed speech are
often only four or five words long, whereas the
general LDA model assumes documents are much
larger. Thus, there may not be enough words to in-
fer a useful utterance specific distribution over top-
ics. Consequently, rather than inferring a separate
topic distribution for each utterance, we simply as-
sume a uniform distribution over objects in the non-
linguistic context. In effect, we rely entirely on the
non-linguistic context and word-object associations
to infer topics. Though necessitated by data sparsity
issues, we also note that it is very rare in our cor-
pus for utterances to refer to more than one object in
the non-linguistic context, so the choice of a single
topic may also be a more accurate model. In fact,
even with multi-sentence documents, LDA may per-
form better if only one topic is assumed per sentence
(Gruber et al, 2007).
3 Inference
We use a collapsed Gibbs sampling procedure, in-
tegrating over all possible Gz , pi, and ? values and
then iteratively sample values for each variable con-
ditioned on the current state of all other variables.
We visit each utterance once per iteration, sample a
topic, and then visit each possible word boundary lo-
cation to sample the boundary and word categories
simultaneously according to their joint probability.
A single topic is sampled for each utterance, con-
ditioned on the words and their current determina-
tions as referring or non-referring. Since zj is drawn
from a uniform distribution, this probability is sim-
ply proportionate to the conditional probability of
the words given zj and the xji variables.
P (zj |wj,xj,h?j) ?
?(?Wjw n
(h?)
w,zj + ?1P0(w))
?(?Wjw n
(h)
w,zj + ?1P0(w))
?
Wj
?
w
?(n(h)w,zj + ?1P0(w))
?(n(h?)w,zj + ?1P0(w))
Here, P (zj |wj,xj,h?j) is the probability of topic
zj given the current hypothesis h for all variables ex-
cluding those for the current utterance. Also, n(h
?j)
w,zj
is the count of occurrences of word type w that refer
to topic zj among the current variable assignments,
and Wj is the set of word types appearing in utter-
ance j. The vectors of word and category variables
in utterance j are represented as wj and xj, respec-
tively. Note that only referring words have any bear-
ing on the appropriate selection of zj and so all fac-
tors involving only non-referring words are absorbed
by the constant of proportionality.
The word categories can be sampled conditioned
on the current word boundary states according to the
following conditional probability, where n(h
?ji)
xji is
the number of words categorized according to label
504
xji over the entire corpus excluding word wji.
P (xji|wji, zj ,h?ji) ? P (wji|zj , xji,h?ji)
?P (xji|h?ji)
=
n(h
?ji)
wji,xjizj + ?xjiP0(wji)
n(h
?ji)
?,xjizj + ?xji
? n
(h?ji)
xji + 1
n(h
?ji)
? + 2
(1)
In practice, however, we actually sample the word
category variables jointly with the boundary states,
using a scheme similar to that outlined in Gold-
water et al (2009). We visit each possible word
boundary location (any point between two consec-
utive phonemes) and compute probabilities for the
hypotheses for which the phonemic environment
makes up either one word or two. As illustrated be-
low there are two sets of cases: those where we treat
the segment as a single word, and those where we
treat it as two words.
x1 x2 x3
. . .#w1#. . . vs. . . .#w2#w3#. . .
? ?
The probabilities of the hypotheses can be derived
by application of equation 1. Since the x variables
can each describe two possible events, there are a to-
tal of six different cases to consider for each bound-
ary assignment: two cases without and four with a
word boundary.
The probability of each of the two cases without
a word boundary can be computed as follows:
P (w1, x1|z,h?) =
n(h
?)
w1,x1z + ?x1P0(w1)
n(h
?)
?,x1z + ?x1
?n
(h?)
x1 + 1
n(h
?)
? + 2
?
n(h
?)
$1 + 1
n(h
?)
? + 2
Here h? signifies the current hypothesis for all
variables excluding those for the current segment
and n(h
?)
$1 is the count for h
? of either utterance fi-
nal words if w1 is utterance final or non-utterance
final words if w1 is also not utterance final.
In the four cases with a word boundary, we have
two words and two categories to sample.
P (w2, x2, w3, x3|z,h?) =
n(h
?)
w2,x2z + ?x2P0(w2)
n(h
?)
?,x2z + ?x2
?n
(h?)
x2 + 1
n(h
?)
? + 2
?
n(h
?)
$2=0 + 1
n(h
?)
? + 2
?n
(h?)
w3,x3z + ?x2(x3)?w2(w3) + ?x3P0(w3)
n(h
?)
?,x3z + ?x2(x3) + ?x3
?n
(h?)
x3 + ?x2(x3) + 1
n(h
?)
? + 3
?
n(h
?)
$3 + ?$2($3) + 1
n(h
?)
? + 3
Here ?x(y) is 1 if x = y and 0 otherwise.
4 Results & Model Comparisons
4.1 Corpus
Our training corpus (Fernald and Morikawa, 1993;
Frank et al, 2009b) consists of about 22,000 words
and 5,600 utterances. Video recordings consisting
of mother-child play over pairs of toys were ortho-
graphically transcribed, and each utterance was an-
notated with the set of objects present in the non-
linguistic context. The object referred to by the ut-
terance, if any, was noted, as described in Frank et al
(2009b). We used the VoxForge dictionary to map
orthographic words to phoneme sequences in a pro-
cess similar to that described in Brent (1999).
Figure 1 (a) presents an example of the coding
of phonemic transcription and non-linguistic context
for a single utterance. The input to the system con-
sists solely of the phonemic transcription and the ob-
jects in the non-linguistic context.
4.2 Evaluation
We ran the sampler ten times for 100,000 iterations
with parameter settings of ?1 = 0.01, ?0 = 20, and
p# = 0.5, keeping only the final sample for evalu-
ation. We defined the word-object pairs for a sam-
ple as the words in the referring category that were
paired at least once with a particular topic. These
pairs were then compared against a gold standard
set of word-object pairs, while segmentation perfor-
mance was evaluated by comparing the final bound-
ary assignments against the gold standard segmenta-
tion.
505
4.2.1 Word Learning
To explore the contribution of word boundaries
to the joint word learning and segmenting task, we
compare our full joint model against a variant that
only infers topics, using the gold standard segmen-
tation as input. In this way we also reproduce the
usual assumption of a sequential relationship be-
tween segmentation and word learning and test the
necessity of the simplifying assumption. The re-
sults are shown in Table 2. We compare them with
three different metric types: topic accuracy; preci-
sion, recall, and F-score of the word-object pairs;
and Kullback-Liebler (KL) divergence.
First, treating utterances with no referring words
as though they have no topic, we compute the ac-
curacy of the inferred topics. Note that we don?t
report accuracy for the the variant with no non-
linguistic context, since in this case the objects are
interchangeable, and we have a problem identifying
which cluster corresponds to which topic. Table 2
shows that the joint segmentation and word learning
model gets the topic right for 81% of the utterances.
The variant that assumes pre-segmented input does
comparably well with an accuracy of 79%. Surpris-
ingly, it seems that knowing the gold segmentation
doesn?t add very much, at least for the topic infer-
ence task.
To evaluate how well we discovered the word-
object map, we manually compiled a list of all the
nouns in the corpus that named one of the 30 ob-
jects. We used this set of nouns, cross-referenced
with their topic objects, as a gold standard set of
word-object pairs. By counting the co-occurrences,
we also compute a gold standard probability distri-
bution for the words given the topic, P (w|z, x = 1).
Precision, recall and F-score are computed as per
Frank et al (2009a). In particular, precision is the
fraction of gold pairs among the sampled set and re-
call is the fraction of sampled pairs among the gold
standard pairs.
p = |Sampled ? Gold||Sampled| , r =
|Sampled ? Gold|
|Gold|
KL divergence is a way of measuring the differ-
ence between distributions. Small numbers gener-
ally indicate a close match and is zero only when
the two are equal. Using the empirical distribution
Object Words
BOX thebox box
BRUSH brush
BUNNY rabbit Rosie
BUS bus
CAR car thecar
CHEESE cheese
DOG thedoggy doggy
DOLL doll thedoll yeah benice
DOUGH dough
ERNIE Ernie
Table 1: Subset of an inferred word-object mapping. For
clarity, the proposed words have been converted to stan-
dard English orthography.
p r f KL acc
Joint 0.21 0.45 0.28 2.78 0.81
Gold Seg 0.21 0.60 0.31 1.82 0.79
Table 2: Word Learning Performance. Comparing
precision, recall, and F-score of word-object pairs,
DKL(P (w, z)||Q(w, z)), and accuracy of utterance top-
ics for the full joint model and a variant that only infers
meanings given a gold standard segmentation.
over gold topics P (z), we use the standard formula
for KL divergence to compare the gold standard dis-
tribution P against the inferred distribution Q. I.e.,
we compute DKL(P (w, z)||Q(w, z)).
The model learns fairly meaningful word-object
associations; results are shown in Table 2. As in the
case of topic accuracy, the joint and word learning
only variants perform similarly, this time with some-
what better performance for the easier task with an
F-score and KL divergence of 0.31 and 1.82 vs. 0.28
and 2.78 for the joint task.
Table 1 illustrates the sort of word-object pairs
the model discovers. As can be seen, many of the
errors are due to the segmentation, usually under-
segmentation errors where it segments two words as
one. This is a general problem with the unigram seg-
menter on which our model is based (Goldwater et
al., 2009). Yet, even though these segmentation er-
rors are also counted as word learning errors, they
are often still meaningful in the sense that the true
referring word is a subsequence.
So, word segmentation has an impact on word
learning. Yet, the joint model still tends to uncover
reasonable meanings. The next question is whether
these meanings have an impact on the segmentation.
506
NoCon Random Joint
Referring Nouns 0.36 0.35 0.50
Neighbors 0.33 0.33 0.37
Utt Final Nouns 0.36 0.36 0.52
Entire Corpus 0.53 0.53 0.54
Table 3: Segmentation performance. F-score for three
subsets and the full corpus for three variants: the model
without non-linguistic context, the model with random
topics, and the full joint model.
4.2.2 Word Segmentation
To measure the impact of word learning on seg-
mentation, we again compare the model on the full
joint task against two other variants: one where top-
ics are randomly selected, and one that ignores the
non-linguistic context. For the random topics vari-
ant, we choose each topic during initialization ac-
cording to the empirical distribution over gold topics
and treat these topic assignments as observed vari-
ables for subsequent iterations. The variant that ig-
nores non-linguistic context draws topics uniformly
from the entire set of objects ever discussed in the
corpus, another test of the contribution of the non-
linguistic context to segmentation. We report token
F-score, computed as per Goldwater et al (2009),
where any segment proposed by the model is a true
positive only if it matches the gold segmentation and
is a false positive otherwise. Any segment in the
gold data not found by the model is a false negative.
Table 3 shows the segmentation performance for
various subsets as well as for the entire corpus. Be-
cause we are primarily interested in synergies be-
tween word learning and segmentation, we focus on
the words most directly impacted by the meanings:
gold standard referring nouns and their neighboring
words.
The model behaves the same with randomized
topics as without context; it learns none of the gold
standard pairs (no matter how we identify clusters
with topics for the contextless case). On all subsets,
the full joint model outperforms the other two vari-
ants. In particular, the greatest gain is for the refer-
ring nouns, with a 21% reduction in error. Also, sim-
ilar to the findings of Bortfeld et al (2005) regarding
6 month olds? abilities to segment words adjoining a
familiar name, we also find that neighboring words
benefit from sharing a word boundary with a learned
word.
The model performs exceptionally well on utter-
ance final referring nouns, with a 24% reduction
in error. This may explain certain psycholinguistic
observations. Frank et al (2006) performed an ar-
tificial language experiment with humans subjects
demonstrating that adults were able to learn words
at the same time as they learned to segment the lan-
guage. However, subjects did much better on a word
learning task when the meaning bearing words were
consistently placed at the end of utterances. There
are several possible reasons why this might have
been the case. For instance, it is common in English
for the object noun to occur at the end of the sen-
tence, and since the subjects were all English speak-
ers, they may have found it easier to learn an artifi-
cial language with a similar pattern. However, our
model predicts another simple possibility: the seg-
mentation task is easier at the end because one of
the two word boundaries is already known (the ut-
terance boundary itself).
4.3 Discussion
The word learning model generally prefers a very
sparse word-to-object map. This is enforced by us-
ing a concentration parameter ?1 that is quite small
relative to the ?0 parameter, and it biases the model
so that the distributions over referring words are
very different from that over non-referring words. A
small concentration parameter biases the estimator
to prefer a small set of word types. In contrast, the
relatively large concentration parameter for the non-
referring words tends to result in most of the words
receiving highest probability as non-referring words.
The model thus categorizes words accordingly. It is
in part due to this tendency towards sparse word-
object maps that the model enforces mutual exclu-
sivity, a phenomenon well documented among natu-
ral word learners (Markman, 1990).
Aside from contributing to mutual exclusivity
and specialization among the topical word distri-
butions, the small concentration parameter also has
important implications for the segmentation task.
A very small value for ?1 discourages the learner
from acquiring more word types for each mean-
ing than absolutely necessary, thereby forcing the
segmenter to use fewer types to explain the se-
quence of phonemes. A model without any notion
507
of meaning cannot maintain separate distributions
for different topics, and must in some sense treat all
words as non-referring. A segmenting model with-
out meanings cannot share the word learner?s reluc-
tance to propose new meaning-bearing word types
and might propose three separate types for ?your
book?, ?a book?, and ?the book?. However, with
a small enough prior on new referring word types,
the word learner that discovers a common refer-
ent for all three sequences and, preferring fewer re-
ferring word types, is more likely to discover the
common subsequence ?book?. With a single word-
object pair (?book?, BOOK), the word learner could
explain reference for all three sequences instead of
using the three separate pairs (?yourbook?, BOOK),
(?abook?, BOOK), and (?thebook?, BOOK).
While relying on non-linguistic context helps seg-
ment the meaning-bearing words, the overall im-
provement is small in our current corpus. One rea-
son for this small improvement was that only 9%
of the tokens in the corpus were referring words.
In corpora containing a larger variety of objects ?
and in cases where sub- and super-ordinate labels
like ?eyes? and ?ears? are coded ? this percentage is
likely to be much higher, leading to a greater boost
in overall segmentation performance.
We should acknowledge that the decisions en-
tailed in enriching the annotations are neither triv-
ial nor without theoretic implication, however. It is
not immediately obvious how to represent the non-
linguistic correlates of verbs, for instance. Devel-
opmentally, verbs are typically acquired much later
than nouns, and it has been argued that this may be
due to the difficulty of producing a cognitive rep-
resentation of the associated meaning (Gentner and
Boroditsky, 2001). Even among concrete nouns, not
all are equal. Children tend to have a bias toward
whole objects when mapping novel words to their
non-linguistic counterparts (Markman, 1990). De-
cisions about more sophisticated encoding of non-
linguistic information may thus require more knowl-
edge about children?s representations of the world
around them
5 Conclusion and Future Work
We find (1) that it is possible to jointly infer both
meanings and a segmentation in a fully unsupervised
way and (2) that doing so improves the segmenta-
tion performance of our model. In particular, we
found that although the word learning side suffered
from segmentation errors, and performed worse than
a model that learned from a gold standard segmen-
tation, the loss was only slight. On the other hand,
segmentation performance for the meaning bearing
words improved a great deal. The first result sug-
gests that is not necessary to assume fully segmented
input in order to learn word meanings, and that the
segmentation and word learning tasks can be effec-
tively modeled in parallel, allowing us to explore po-
tential developmental interactions. The second re-
sult suggests that synergies do actually exist and ar-
gue not only that we can model the two as parallel
processes, but that doing so could prove fruitful.
Our model is relatively simple both in terms of
word learning and in terms of word segmentation.
For instance, social cues and shared attention, or dis-
course effects, might all play a role (Frank et al,
2009b). Shared features or other relationships can
also potentially impact how quickly one might gen-
eralize a label to multiple instances (Tenenbaum and
Xu, 2000). There are many ways to elaborate on the
word learning task, with additional potential syner-
gistic implications.
We might also elaborate the linguistic structures
we incorporate into the word learning model. For
instance, Johnson (2008) explores synergies in syl-
lable and morphological structures in word segmen-
tation. Aspects of linguistic structure, such as mor-
phology, may contribute to word meaning learning
beyond its contribution to word segmentation per-
formance.
Acknowledgments
This research was funded by NSF awards 0544127
and 0631667 to Mark Johnson and by NSF DDRIG
0746251 to Michael C. Frank. We would also like
to thank Anne Fernald for providing the corpus and
Maeve Cullinane for help in coding it.
References
Nameera Akhtar and Lisa Montague. 1999. Early lexi-
cal acquisition: The role of cross-situational learning.
First Language, 19(57 Pt 3):347?358.
508
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Heather Bortfeld, James L. Morgan, Roberta Michnick
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speechstream segmentation. Psychological Science,
16(4):298?304.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Anne Fernald and Hiromi Morikawa. 1993. Common
themes and cultural variations in japanese and ameri-
can mothers? speech to infants. In Child Development,
number 3, pages 637?656, June.
Michael C. Frank, Vikash Mansinghka, Edward Gibson,
and Joshua B. Tenenbaum. 2006. Word segmentation
as word learning: Integrating stress and meaning with
distributional cues. In Proceedings of the 31st Annual
Boston University Conference on Language Develop-
ment.
Michael C. Frank, Sharon Goldwater, Vikash Mans-
inghka, Tom Griffiths, and Joshua Tenenbaum. 2007.
Modeling human performance in statistical word seg-
mentation. Proceedings of the 29th Annual Meeting of
the Cognitive Science Society, pages 281?286.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009a. Using speakers? referential inten-
tions to model early cross-situational word learning.
Psychological Science, 5:578?585.
Michael C. Frank, Noah D. Goodman, Joshua B. Tenen-
baum, and Anne Fernald. 2009b. Continuity of dis-
course provides information for word learning.
Dedre Gentner and Lera Boroditsky. 2001. Individua-
tion, relativity, and early word learning. Language,
culture, & cognition, 3:215?56.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali,
and Jenny R. Saffran. 2007. Can infants map meaning
to newly segmented words? statistical segmentation
and word learning. Psychological Science, 18(3):254?
260.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Artificial Intelligence
and Statistics (AISTATS), March.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Ellen M. Markman. 1990. Constraints children place on
word learning. Cognitive Science, 14:57?77.
Terry Regier. 2003. Emergent constraints on word-
learning: A computational review. Trends in Cognitive
Sciences, 7:263?268.
Jenny R. Saffran, Elissa L. Newport, and Richard N.
Aslin. 1996. Word segmentation: The role of dis-
tributional cues. Journal of memory and Language,
35:606?621.
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39?91.
Joshua B. Tenenbaum and Fei Xu. 2000. Word learn-
ing as bayesian inference. In Proceedings of the 22nd
Annual Conference of the Cognitive Science Society,
pages 517?522.
Chen Yu and Dana H. Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149?2165.
509
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883?891,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Social Information in Grounded Language Learning via
Grammatical Reductions
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
Mark.Johnson@MQ.edu.au
Katherine Demuth
Department of Linguistics
Macquarie University
Sydney, Australia
Katherine.Demuth@MQ.edu.au
Michael Frank
Department of Psychology
Stanford University
Stanford, California
mcfrank@Stanford.edu
Abstract
This paper uses an unsupervised model of
grounded language acquisition to study the
role that social cues play in language acqui-
sition. The input to the model consists of (or-
thographically transcribed) child-directed ut-
terances accompanied by the set of objects
present in the non-linguistic context. Each
object is annotated by social cues, indicating
e.g., whether the caregiver is looking at or
touching the object. We show how to model
the task of inferring which objects are be-
ing talked about (and which words refer to
which objects) as standard grammatical in-
ference, and describe PCFG-based unigram
models and adaptor grammar-based colloca-
tion models for the task. Exploiting social
cues improves the performance of all mod-
els. Our models learn the relative importance
of each social cue jointly with word-object
mappings and collocation structure, consis-
tent with the idea that children could discover
the importance of particular social informa-
tion sources during word learning.
1 Introduction
From learning sounds to learning the meanings of
words, social interactions are extremely important
for children?s early language acquisition (Baldwin,
1993; Kuhl et al, 2003). For example, children who
engage in more joint attention (e.g. looking at par-
ticular objects together) with caregivers tend to learn
words faster (Carpenter et al, 1998). Yet compu-
tational or formal models of social interaction are
rare, and those that exist have rarely gone beyond
the stage of cue-weighting models. In order to study
the role that social cues play in language acquisition,
this paper presents a structured statistical model of
grounded learning that learns a mapping between
words and objects from a corpus of child-directed
utterances in a completely unsupervised fashion. It
exploits five different social cues, which indicate
which object (if any) the child is looking at, which
object the child is touching, etc. Our models learn
the salience of each social cue in establishing refer-
ence, relative to their co-occurrence with objects that
are not being referred to. Thus, this work is consis-
tent with a view of language acquisition in which
children learn to learn, discovering organizing prin-
ciples for how language is organized and used so-
cially (Baldwin, 1993; Hollich et al, 2000; Smith et
al., 2002).
We reduce the grounded learning task to a gram-
matical inference problem (Johnson et al, 2010;
Bo?rschinger et al, 2011). The strings presented to
our grammatical learner contain a prefix which en-
codes the objects and their social cues for each ut-
terance, and the rules of the grammar encode rela-
tionships between these objects and specific words.
These rules permit every object to map to every
word (including function words; i.e., there is no
?stop word? list), and the learning process decides
which of these rules will have a non-trivial proba-
bility (these encode the object-word mappings the
system has learned).
This reduction of grounded learning to grammat-
ical inference allows us to use standard grammati-
cal inference procedures to learn our models. Here
we use the adaptor grammar package described in
Johnson et al (2007) and Johnson and Goldwater
(2009) with ?out of the box? default settings; no
parameter tuning whatsoever was done. Adaptor
grammars are a framework for specifying hierarchi-
cal non-parametric models that has been previously
used to model language acquisition (Johnson, 2008).
883
Social cue Value
child.eyes objects child is looking at
child.hands objects child is touching
mom.eyes objects care-giver is looking at
mom.hands objects care-giver is touching
mom.point objects care-giver is pointing to
Figure 1: The 5 social cues in the Frank et al (to appear)
corpus. The value of a social cue for an utterance is a
subset of the available topics (i.e., the objects in the non-
linguistic context) of that utterance.
A semanticist might argue that our view of refer-
ential mapping is flawed: full noun phrases (e.g., the
dog), rather than nouns, refer to specific objects, and
nouns denote properties (e.g., dog denotes the prop-
erty of being a dog). Learning that a noun, e.g., dog,
is part of a phrase used to refer to a specific dog (say,
Fido) does not suffice to determine the noun?s mean-
ing: the noun could denote a specific breed of dog,
or animals in general. But learning word-object rela-
tionships is a plausible first step for any learner: it is
often only the contrast between learned relationships
and novel relationships that allows children to in-
duce super- or sub-ordinate mappings (Clark, 1987).
Nevertheless, in deference to such objections, we
call the object that a phrase containing a given noun
refers to the topic of that noun. (This is also appro-
priate, given that our models are specialisations of
topic models).
Our models are intended as an ?ideal learner? ap-
proach to early social language learning, attempt-
ing to weight the importance of social and structural
factors in the acquisition of word-object correspon-
dences. From this perspective, the primary goal is
to investigate the relationships between acquisition
tasks (Johnson, 2008; Johnson et al, 2010), looking
for synergies (areas of acquisition where attempting
two learning tasks jointly can provide gains in both)
as well as areas where information overlaps.
1.1 A training corpus for social cues
Our work here uses a corpus of child-directed
speech annotated with social cues, described in
Frank et al (to appear). The corpus consists
of 4,763 orthographically-transcribed utterances of
caregivers to their pre-linguistic children (ages 6, 12,
and 18 months) during home visits where children
played with a consistent set of toys. The sessions
were video-taped, and each utterance was annotated
with the five social cues described in Figure 1.
Each utterance in the corpus contains the follow-
ing information:
? the sequence of orthographic words uttered by
the care-giver,
? a set of available topics (i.e., objects in the non-
linguistic objects),
? the values of the social cues, and
? a set of intended topics, which the care-giver
refers to.
Figure 2 presents this information for an example ut-
terance. All of these but the intended topics are pro-
vided to our learning algorithms; the intended top-
ics are used to evaluate the output produced by our
learners.
Generally the intended topics consist of zero or
one elements from the available topics, but not al-
ways: it is possible for the caregiver to refer to two
objects in a single utterance, or to refer to an object
not in the current non-linguistic context (e.g., to a
toy that has been put away). There is a considerable
amount of anaphora in this corpus, which our mod-
els currently ignore.
Frank et al (to appear) give extensive details on
the corpus, including inter-annotator reliability in-
formation for all annotations, and provide detailed
statistical analyses of the relationships between the
various social cues, the available topics and the in-
tended topics. That paper also gives instructions on
obtaining the corpus.
1.2 Previous work
There is a growing body of work on the role of social
cues in language acquisition. The language acqui-
sition research community has long recognized the
importance of social cues for child language acqui-
sition (Baldwin, 1991; Carpenter et al, 1998; Kuhl
et al, 2003).
Siskind (1996) describes one of the first exam-
ples of a model that learns the relationship between
words and topics, albeit in a non-statistical frame-
work. Yu and Ballard (2007) describe an associative
learner that associates words with topics and that
exploits prosodic as well as social cues. The rela-
tive importance of the various social cues are spec-
ified a priori in their model (rather than learned, as
they are here), and unfortunately their training cor-
pus is not available. Frank et al (2008) describes a
Bayesian model that learns the relationship between
words and topics, but the version of their model that
included social cues presented a number of chal-
lenges for inference. The unigram model we de-
scribe below corresponds most closely to the Frank
884
.dog # .pig child.eyes mom.eyes mom.hands # ## wheres the piggie
Figure 2: The photograph indicates non-linguistic context containing a (toy) pig and dog for the utterance Where?s the
piggie?. Below that, we show the representation of this utterance that serves as the input to our models. The prefix (the
portion of the string before the ?##?) lists the available topics (i.e., the objects in the non-linguistic context) and their
associated social cues (the cues for the pig are child.eyes, mom.eyes and mom.hands, while the dog is not associated
with any social cues). The intended topic is the pig. The learner?s goals are to identify the utterance?s intended topic,
and which words in the utterance are associated with which topic.
Sentence
Topic.pig
T.None
.dog
NotTopical.child.eyes
NotTopical.child.hands
NotTopical.mom.eyes
NotTopical.mom.hands
NotTopical.mom.point
#
Topic.pig
T.pig
.pig
Topical.child.eyes
child.eyes
Topical.child.hands
Topical.mom.eyes
Topical.mom.hands
mom.hands
Topical.mom.point
#
Topic.None
##
Words.pig
Word.None
wheres
Words.pig
Word.None
the
Words.pig
Word.pig
piggie
Figure 3: Sample parse generated by the Unigram PCFG. Nodes coloured red show how the ?pig? topic is propagated
from the prefix (before the ?##? separator) into the utterance. The social cues associated with each object are generated
either from a ?Topical? or a ?NotTopical? nonterminal, depending on whether the corresponding object is topical or
not.
885
et al model. Johnson et al (2010) reduces grounded
learning to grammatical inference for adaptor gram-
mars and shows how it can be used to perform word
segmentation as well as learning word-topic rela-
tionships, but their model does not take social cues
into account.
2 Reducing grounded learning with social
cues to grammatical inference
This section explains how we reduce ground learn-
ing problems with social cues to grammatical in-
ference problems, which lets us apply a wide vari-
ety of grammatical inference algorithms to grounded
learning problems. An advantage of reducing
grounded learning to grammatical inference is that
it suggests new ways to generalise grounded learn-
ing models; we explore three such generalisations
here. The main challenge in this reduction is finding
a way of expressing the non-linguistic information
as part of the strings that serve as the grammatical in-
ference procedure?s input. Here we encode the non-
linguistic information in a ?prefix? to each utterance
as shown in Figure 2, and devise a grammar such
that inference for the grammar corresponds to learn-
ing the word-topic relationships and the salience of
the social cues for grounded learning.
All our models associate each utterance with zero
or one topics (this means we cannot correctly anal-
yse utterances with more than one intended topic).
We analyse an utterance associated with zero topics
as having the special topic None, so we can assume
that every utterance has exactly one topic. All our
grammars generate strings of the form shown in Fig-
ure 2, and they do so by parsing the prefix and the
words of the utterance separately; the top-level rules
of the grammar force the same topic to be associated
with both the prefix and the words of the utterance
(see Figure 3).
2.1 Topic models and the unigram PCFG
As Johnson et al (2010) observe, this kind of
grounded learning can be viewed as a specialised
kind of topic inference in a topic model, where the
utterance topic is constrained by the available ob-
jects (possible topics). We exploit this observation
here using a reduction based on the reduction of
LDA topic models to PCFGs proposed by Johnson
(2010). This leads to our first model, the unigram
grammar, which is a PCFG.1
1In fact, the unigram grammar is equivalent to a HMM,
but the PCFG parameterisation makes clear the relationship
Sentence? Topict Wordst ?t ? T
?
TopicNone ? ##
Topict ? Tt TopicNone ?t ? T
?
Topict ? TNone Topict ?t ? T
Tt ? t Topicalc1 ?t ? T
Topicalci ? (ci) Topicalci+1 i = 1, . . . , `? 1
Topicalc` ? (c`) #
TNone ? t NotTopicalc1 ?t ? T
NotTopicalci ? (ci) NotTopicalci+1 i = 1, . . . , `? 1
NotTopicalc` ? (c`) #
Wordst ?WordNone (Wordst) ?t ? T ?
Wordst ?Wordt (Wordst) ?t ? T
Wordt ? w ?t ? T ?, w ?W
Figure 4: The rule schema that generate the unigram
PCFG. Here (c1, . . . , c`) is an ordered list of the so-
cial cues, T is the set of all non-None available topics,
T ? = T ? {None}, and W is the set of words appearing
in the utterances. Parentheses indicate optionality.
Figure 4 presents the rules of the unigram gram-
mar. This grammar has two major parts. The rules
expanding the Topict nonterminals ensure that the
social cues for the available topic t are parsed un-
der the Topical nonterminals. All other available
topics are parsed under TNone nonterminals, so their
social cues are parsed under NotTopical nontermi-
nals. The rules expanding these non-terminals are
specifically designed so that the generation of the so-
cial cues corresponds to a series of binary decisions
about each social cue. For example, the probability
of the rule
Topicalchild.eyes ? .child.eyes Topicalchild.hands
is the probability of an object that is an utterance
topic occuring with the child.eyes social cue. By es-
timating the probabilities of these rules, the model
effectively learns the probability of each social cue
being associated with a Topical or a NotTopical
available topic, respectively.
The nonterminals Wordst expand to a sequence
of Wordt and WordNone nonterminals, each of
which can expand to any word whatsoever. In prac-
tice Wordt will expand to those words most strongly
associated with topic t, while WordNone will expand
to those words not associated with any topic.
between grounded learning and estimation of grammar rule
weights.
886
Sentence? Topict Collocst ?t ? T
?
Collocst ? Colloct (Collocst) ?t ? T ?
Collocst ? CollocNone (Collocst) ?t ? T
Colloct ?Wordst ?t ? T ?
Wordst ?Wordt (Wordst) ?t ? T ?
Wordst ?WordNone (Wordst) ?t ? T
Wordt ?Word ?t ? T ?
Word? w ?w ?W
Figure 5: The rule schema that generate the collocation
adaptor grammar. Adapted nonterminals are indicated via
underlining. Here T is the set of all non-None available
topics, T ? = T ? {None}, and W is the set of words ap-
pearing in the utterances. The rules expanding the Topict
nonterminals are exactly as in unigram PCFG.
2.2 Adaptor grammars
Our other grounded learning models are based on
reductions of grounded learning to adaptor gram-
mar inference problems. Adaptor grammars are a
framework for stating a variety of Bayesian non-
parametric models defined in terms of a hierarchy of
Pitman-Yor Processes: see Johnson et al (2007) for
a formal description. Informally, an adaptor gram-
mar is specified by a set of rules just as in a PCFG,
plus a set of adapted nonterminals. The set of
trees generated by an adaptor grammar is the same
as the set of trees generated by a PCFG with the
same rules, but the generative process differs. Non-
adapted nonterminals in an adaptor grammar expand
just as they do in a PCFG: the probability of choos-
ing a rule is specified by its probability. However,
the expansion of an adapted nonterminal depends on
how it expanded in previous derivations. An adapted
nonterminal can directly expand to a subtree with
probability proportional to the number of times that
subtree has been previously generated; it can also
?back off? to expand using a grammar rule, just as
in a PCFG, with probability proportional to a con-
stant.2
Thus an adaptor grammar can be viewed as
caching each tree generated by each adapted non-
terminal, and regenerating it with probability pro-
portional to the number of times it was previously
generated (with some probability mass reserved to
generate ?new? trees). This enables adaptor gram-
2This is a description of Chinese Restaurant Processes,
which are the predictive distributions for Dirichlet Processes.
Our adaptor grammars are actually based on the more general
Pitman-Yor Processes, as described in Johnson and Goldwater
(2009).
Sentence
Topic.pig
...
Collocs.pig
Colloc.None
Words.None
Word.None
Word
wheres
Collocs.pig
Colloc.pig
Words.pig
Word.None
Word
the
Words.pig
Word.pig
Word
piggie
Figure 6: Sample parse generated by the collocation
adaptor grammar. The adapted nonterminals Colloct and
Wordt are shown underlined; the subtrees they dominate
are ?cached? by the adaptor grammar. The prefix (not
shown here) is parsed exactly as in the Unigram PCFG.
mars to generalise over subtrees of arbitrary size.
Generic software is available for adaptor grammar
inference, based either on Variational Bayes (Cohen
et al, 2010) or Markov Chain Monte Carlo (Johnson
and Goldwater, 2009). We used the latter software
because it is capable of performing hyper-parameter
inference for the PCFG rule probabilities and the
Pitman-Yor Process parameters. We used the ?out-
of-the-box? settings for this software, i.e., uniform
priors on all PCFG rule parameters, a Beta(2, 1)
prior on the Pitman-Yor a parameters and a ?vague?
Gamma(100, 0.01) prior on the Pitman-Yor b pa-
rameters. (Presumably performance could be im-
proved if the priors were tuned, but we did not ex-
plore this here).
Here we explore a simple ?collocation? extension
to the unigram PCFG which associates multiword
collocations, rather than individual words, with top-
ics. Hardisty et al (2010) showed that this signifi-
cantly improved performance in a sentiment analy-
sis task.
The collocation adaptor grammar in Figure 5 gen-
erates the words of the utterance as a sequence of
collocations, each of which is a sequence of words.
Each collocation is either associated with the sen-
tence topic or with the None topic, just like words in
the unigram model. Figure 6 shows a sample parse
generated by the collocation adaptor grammar.
We also experimented with a variant of the uni-
gram and collocation grammars in which the topic-
specific word distributions Wordt for each t ? T
887
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.7381
unigram? none 0.3261 0.3767 0.3054 0.4914 0.1893 0.1131 0.5811 0.1167 0.06583 0.5122
unigram? all 0.5117 0.6106 0.4986 0.7875 0.2846 0.1693 0.891 0.1684 0.09402 0.8049
colloc? none 0.5238 0.3419 0.3844 0.3078 0.2551 0.1732 0.4843 0.2162 0.1495 0.3902
colloc? all 0.6492 0.6034 0.6664 0.5514 0.3981 0.2613 0.8354 0.3375 0.2269 0.6585
Figure 7: Utterance topic, word topic and lexicon results for all models, on data with and without social cues. The
results for the variant models, in which Wordt nonterminals expand via WordNone, are shown under unigram? and
colloc?. Utterance topic shows how well the model discovered the intended topics at the utterance level, word topic
shows how well the model associates word tokens with topics, and lexicon shows how well the topic most frequently
associated with a word type matches an external word-topic dictionary. In this figure and below, ?colloc? abbreviates
?collocation?, ?acc.? abbreviates ?accuracy?, ?prec.? abbreviates ?precision? and ?rec.? abbreviates ?recall?.
(the set of non-None available topics) expand via
WordNone non-terminals. That is, in the variant
grammars topical words are generated with the fol-
lowing rule schema:
Wordt ?WordNone ?t ? T
WordNone ?Word
Word? w ?w ?W
In these variant grammars, the WordNone nontermi-
nal generates all the words of the language, so it de-
fines a generic ?background? distribution over all the
words, rather than just the nontopical words. An ef-
fect of this is that the variant grammars tend to iden-
tify fewer words as topical.
3 Experimental evaluation
We performed grammatical inference using the
adaptor grammar software described in Johnson and
Goldwater (2009).3 All experiments involved 4 runs
of 5,000 samples each, of which the first 2,500 were
discarded for ?burn-in?.4 From these samples we
extracted the modal (i.e., most frequent) analysis,
3Because adaptor grammars are a generalisation of PCFGs,
we could use the adaptor grammar software to estimate the un-
igram model.
4We made no effort to optimise the computation, but it
seems the samplers actually stabilised after around a hundred
iterations, so it was probably not necessary to sample so exten-
sively. We estimated the error in our results by running our most
complex model (the colloc? model with all social cues) 20 times
(i.e., 20?8 chains for 5,000 iterations) so we could compute the
variance of each of the evaluation scores (it is reasonable to as-
sume that the simpler models will have smaller variance). The
standard deviation of all utterance topic and word topic mea-
sures is between 0.005 and 0.01; the standard deviation for lex-
icon f-score is 0.02, lexicon precision is 0.01 and lexicon recall
is 0.03. The adaptor grammar software uses a sentence-wise
which we evaluated as described below. The results
of evaluating each model on the corpus with social
cues, and on another corpus identical except that the
social cues have been removed, are presented in Fig-
ure 7.
Each model was evaluated on each corpus as fol-
lows. First, we extracted the utterance?s topic from
the modal parse (this can be read off the Topict
nodes), and compared this to the intended topics an-
notated in the corpus. The frequency with which
the models? predicted topics exactly matches the
intended topics is given under ?utterance topic ac-
curacy?; the f-score, precision and recall of each
model?s topic predictions are also given in the table.
Because our models all associate word tokens
with topics, we can also evaluate the accuracy with
which word tokens are associated with topics. We
constructed a small dictionary which identifies the
words that can be used as the head of a phrase to
refer to the topical objects (e.g., the dictionary in-
dicates that dog, doggie and puppy name the topi-
cal object DOG). Our dictionary is relatively conser-
vative; between one and eight words are associated
with each topic. We scored the topic label on each
word token in our corpus as follows. A topic label is
scored as correct if it is given in our dictionary and
the topic is one of the intended topics for the utter-
ance. The ?word topic? entries in Figure 7 give the
results of this evaluation.
blocked sampler, so it requires fewer iterations than a point-
wise sampler. We used 5,000 iterations because this is the soft-
ware?s default setting; evaluating the trace output suggests it
only takes several hundred iterations to ?burn in?. However, we
ran 8 chains for 25,000 iterations of the colloc? model; as ex-
pected the results of this run are within two standard deviations
of the results reported above.
888
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram +child.eyes 0.4573 0.5725 0.4559 0.7694 0.2891 0.1724 0.8951 0.1362 0.07415 0.8333
unigram +child.hands 0.3399 0.4011 0.3246 0.5247 0.2008 0.121 0.5892 0.09705 0.05324 0.5476
unigram +mom.eyes 0.338 0.4023 0.3234 0.5322 0.1992 0.1198 0.5908 0.09664 0.053 0.5476
unigram +mom.hands 0.3563 0.4279 0.3437 0.5667 0.1984 0.1191 0.5948 0.09959 0.05455 0.5714
unigram +mom.point 0.3063 0.3548 0.285 0.4698 0.1806 0.1086 0.5359 0.09224 0.05057 0.5238
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc +child.eyes 0.5159 0.5006 0.4652 0.542 0.351 0.2309 0.7312 0.1432 0.07989 0.6905
colloc +child.hands 0.4827 0.4275 0.3999 0.4592 0.2897 0.1913 0.5964 0.1192 0.06686 0.5476
colloc +mom.eyes 0.4697 0.4171 0.3869 0.4525 0.2708 0.1781 0.5642 0.1013 0.05666 0.4762
colloc +mom.hands 0.4747 0.4251 0.3942 0.4612 0.274 0.1806 0.5666 0.09548 0.05337 0.4524
colloc +mom.point 0.4228 0.3378 0.3151 0.3639 0.2575 0.1716 0.5157 0.09278 0.05202 0.4286
Figure 8: Effect of using just one social cue on the experimental results for the unigram and collocation models. The
?importance? of a social cue can be quantified by the degree to which the model?s evaluation score improves when
using a corpus containing that social cue relative to its evaluation score when using a corpus without any social cues.
The most important social cue is the one which causes performance to improve the most.
Finally, we extracted a lexicon from the parsed
corpus produced by each model. We counted how
often each word type was associated with each topic
in our sampler?s output (including the None topic),
and assigned the word to its most frequent topic.
The ?lexicon? entries in Figure 7 show how well
the entries in these lexicons match the entries in the
manually-constructed dictionary discussed above.
There are 10 different evaluation scores, and no
model dominates in all of them. However, the top-
scoring result in every evaluation is always for a
model trained using social cues, demonstrating the
importance of these social cues. The variant colloca-
tion model (trained on data with social cues) was the
top-scoring model on four evaluation scores, which
is more than any other model.
One striking thing about this evaluation is that the
recall scores are all much higher than the precision
scores, for each evaluation. This indicates that all
of the models, especially the unigram model, are la-
belling too many words as topical. This is perhaps
not too surprising: because our models completely
lack any notion of syntactic structure and simply
model the association between words and topics,
they label many non-nouns with topics (e.g., woof
is typically labelled with the topic DOG).
3.1 Evaluating the importance of social cues
It is scientifically interesting to be able to evalu-
ate the importance of each of the social cues to
grounded learning. One way to do this is to study
the effect of adding or removing social cues from
the corpus on the ability of our models to perform
grounded learning. An important social cue should
have a large impact on our models? performance; an
unimportant cue should have little or no impact.
Figure 8 compares the performance of the uni-
gram and collocation models on corpora containing
a single social cue to their performance on the cor-
pus without any social cues, while Figure 9 com-
pares the performance of these models on corpora
containing all but one social cue to the corpus con-
taining all of the social cues. In both of these evalua-
tions, with respect to all 10 evaluation measures, the
child.eyes social cue had the most impact on model
performance.
Why would the child?s own gaze be more impor-
tant than the caregiver?s? Perhaps caregivers are fol-
lowing in, i.e., talking about objects that their chil-
dren are interested in (Baldwin, 1991). However, an-
other possible explanation is that this result is due to
the general continuity of conversational topics over
time. Frank et al (to appear) show that for the cur-
rent corpus, the topic of the preceding utterance is
very likely to be the topic of the current one also.
Thus, the child?s eyes might be a good predictor be-
cause they reflect the fact that the child?s attention
has been drawn to an object by previous utterances.
Notice that these two possible explanations of the
importance of the child.eyes cue are diametrically
opposed; the first explanation claims that the cue is
important because the child is driving the discourse,
while the second explanation claims that the cue is
important because the child?s gaze follows the topic
of the caregiver?s previous utterance. This sort of
question about causal relationships in conversations
may be very difficult to answer using standard de-
scriptive techniques, but it may be an interesting av-
889
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
unigram ?child.eyes 0.3836 0.4659 0.3738 0.6184 0.2149 0.1286 0.6546 0.1111 0.06089 0.6341
unigram ?child.hands 0.4907 0.6063 0.4863 0.8051 0.296 0.1769 0.9056 0.1525 0.08353 0.878
unigram ?mom.eyes 0.4799 0.5974 0.4768 0.7996 0.2898 0.1727 0.9007 0.1551 0.08486 0.9024
unigram ?mom.hands 0.4871 0.5996 0.4815 0.7945 0.2925 0.1746 0.8991 0.1561 0.08545 0.9024
unigram ?mom.point 0.4875 0.6033 0.4841 0.8004 0.2934 0.1752 0.9007 0.1558 0.08525 0.9024
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.738
colloc ?child.eyes 0.5604 0.5746 0.529 0.6286 0.39 0.2561 0.8176 0.1534 0.08642 0.6829
colloc ?child.hands 0.5849 0.6 0.5609 0.6451 0.4145 0.273 0.8612 0.1662 0.09375 0.7317
colloc ?mom.eyes 0.5709 0.5829 0.5457 0.6255 0.4036 0.2655 0.8418 0.1662 0.09375 0.7317
colloc ?mom.hands 0.5795 0.5935 0.5571 0.6349 0.4038 0.2653 0.8442 0.1788 0.1009 0.7805
colloc ?mom.point 0.5851 0.6006 0.5607 0.6467 0.4097 0.2685 0.8644 0.1742 0.09841 0.7561
Figure 9: Effect of using all but one social cue on the experimental results for the unigram and collocation models.
The ?importance? of a social cue can be quantified by the degree to which the model?s evaluation score degrades when
that just social cue is removed from the corpus, relative to its evaluation score when using a corpus without all social
cues. The most important social cue is the one which causes performance to degrade the most.
enue for future investigation using more structured
models such as those proposed here.5
4 Conclusion and future work
This paper presented four different grounded learn-
ing models that exploit social cues. These models
are all expressed via reductions to grammatical in-
ference problems, so standard ?off the shelf? gram-
matical inference tools can be used to learn them.
Here we used the same adaptor grammar software
tools to learn all these models, so we can be rel-
atively certain that any differences we observe are
due to differences in the models, rather than quirks
in the software.
Because the adaptor grammar software performs
full Bayesian inference, including for model param-
eters, an unusual feature of our models is that we
did not need to perform any parameter tuning what-
soever. This feature is particularly interesting with
respect to the parameters on social cues. Psycholog-
ical proposals have suggested that children may dis-
cover that particular social cues help in establishing
reference (Baldwin, 1993; Hollich et al, 2000), but
prior modeling work has often assumed that cues,
cue weights, or both are prespecified. In contrast, the
models described here could in principle discover a
wide range of different social conventions.
5A reviewer suggested that we can test whether child.eyes
effectively provides the same information as the previous topic
by adding the previous topic as a (pseudo-) social cue. We tried
this, and child.eyes and previous.topic do in fact seem to convey
very similar information: e.g., the model with previous.topic
and without child.eyes scores essentially the same as the model
with all social cues.
Our work instantiates the strategy of investigating
the structure of children?s learning environment us-
ing ?ideal learner? models. We used our models to
investigate scientific questions about the role of so-
cial cues in grounded language learning. Because
the performance of all four models studied in this
paper improve dramatically when provided with so-
cial cues in all ten evaluation metrics, this paper pro-
vides strong support for the view that social cues are
a crucial information source for grounded language
learning.
We also showed that the importance of the differ-
ent social cues in grounded language learning can
be evaluated using ?add one cue? and ?subtract one
cue? methodologies. According to both of these, the
child.eyes cue is the most important of the five so-
cial cues studied here. There are at least two pos-
sible reasons for this: the caregiver?s topic could
be determined by the child?s gaze, or the child.eyes
cue could be providing our models with information
about the topic of the previous utterance.
Incorporating topic continuity and anaphoric de-
pendencies into our models would be likely to im-
prove performance. This improvement might also
help us distinguish the two hypotheses about the
child.eyes cue. If the child.eyes cue is just provid-
ing indirect information about topic continuity, then
the importance of the child.eyes cue should decrease
when we incorporate topic continuity into our mod-
els. But if the child?s gaze is in fact determining the
care-giver?s topic, then child.eyes should remain a
strong cue even when anaphoric dependencies and
topic continuity are incorporated into our models.
890
Acknowledgements
This research was supported under the Australian
Research Council?s Discovery Projects funding
scheme (project number DP110102506).
References
Dare A. Baldwin. 1991. Infants? contribution to the
achievement of joint reference. Child Development,
62(5):874?890.
Dare A. Baldwin. 1993. Infants? ability to consult the
speaker for clues to word reference. Journal of Child
Language, 20:395?395.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
M. Carpenter, K. Nagell, M. Tomasello, G. Butterworth,
and C. Moore. 1998. Social cognition, joint attention,
and communicative competence from 9 to 15 months
of age. Monographs of the society for research in child
development.
E.V. Clark. 1987. The principle of contrast: A constraint
on language acquisition. Mechanisms of language ac-
quisition, 1:33.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564?
572, Los Angeles, California, June. Association for
Computational Linguistics.
Michael Frank, Noah Goodman, and Joshua Tenenbaum.
2008. A Bayesian framework for cross-situational
word-learning. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 457?464, Cambridge,
MA. MIT Press.
Michael C. Frank, Joshua Tenenbaum, and Anne Fernald.
to appear. Social and discourse contributions to the
determination of reference in cross-situational word
learning. Language, Learning, and Development.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 284?
292, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
G.J. Hollich, K. Hirsh-Pasek, and R. Golinkoff. 2000.
Breaking the language barrier: An emergentist coali-
tion model for the origins of word learning. Mono-
graphs of the Society for Research in Child Develop-
ment.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018?1026.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, pages 398?406, Columbus, Ohio. Association for
Computational Linguistics.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu.
2003. Foreign-language experience in infancy: Effects
of short-term exposure and social interaction on pho-
netic learning. Proceedings of the National Academy
of Sciences USA, 100(15):9096?9101.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61(1-2):39?91.
L.B. Smith, S.S. Jones, B. Landau, L. Gershkoff-Stowe,
and L. Samuelson. 2002. Object name learning pro-
vides on-the-job training for attention. Psychological
Science, 13(1):13.
Chen Yu and Dana H Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149?2165.
891
Parsing entire discourses as very long strings: Capturing topic continuity in
grounded language learning
Minh-Thang Luong
Department of Computer Science
Stanford University
Stanford, California
lmthang@stanford.edu
Michael C. Frank
Department of Psychology
Stanford University
Stanford, California
mcfrank@stanford.edu
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
Mark.Johnson@MQ.edu.au
Abstract
Grounded language learning, the task of map-
ping from natural language to a representation
of meaning, has attracted more and more in-
terest in recent years. In most work on this
topic, however, utterances in a conversation
are treated independently and discourse struc-
ture information is largely ignored. In the
context of language acquisition, this indepen-
dence assumption discards cues that are im-
portant to the learner, e.g., the fact that con-
secutive utterances are likely to share the same
referent (Frank et al, 2013). The current pa-
per describes an approach to the problem of
simultaneously modeling grounded language
at the sentence and discourse levels. We com-
bine ideas from parsing and grammar induc-
tion to produce a parser that can handle long
input strings with thousands of tokens, creat-
ing parse trees that represent full discourses.
By casting grounded language learning as a
grammatical inference task, we use our parser
to extend the work of Johnson et al (2012),
investigating the importance of discourse con-
tinuity in children?s language acquisition and
its interaction with social cues. Our model
boosts performance in a language acquisition
task and yields good discourse segmentations
compared with human annotators.
1 Introduction
Learning mappings between natural language (NL)
and meaning representations (MR) is an important
goal for both computational linguistics and cognitive
science. Accurately learning novel mappings is cru-
cial in grounded language understanding tasks and
such systems can suggest insights into the nature of
children language learning.
Two influential examples of grounded language
learning tasks are the sportscasting task, RoboCup,
where the NL is the set of running commentary and
the MR is the set of logical forms representing ac-
tions like kicking or passing (Chen and Mooney,
2008), and the cross-situational word-learning task,
where the NL is the caregiver?s utterances and the
MR is the set of objects present in the context
(Siskind, 1996; Yu and Ballard, 2007). Work
in these domains suggests that, based on the co-
occurrence between words and their referents in
context, it is possible to learn mappings between NL
and MR even under substantial ambiguity.
Nevertheless, contexts like RoboCup?where ev-
ery single utterance is grounded?are extremely
rare. Much more common are cases where a sin-
gle topic is introduced and then discussed at length
throughout a discourse. In a television news show,
for example, a topic might be introduced by present-
ing a relevant picture or video clip. Once the topic
is introduced, the anchors can discuss it by name
or even using a pronoun without showing a picture.
The discourse is grounded without having to ground
every utterance.
Moreover, although previous work has largely
treated utterance order as independent, the order of
utterances is critical in grounded discourse contexts:
if the order is scrambled, it can become impossible
to recover the topic. Supporting this idea, Frank et
al. (2013) found that topic continuity?the tendency
to talk about the same topic in multiple utterances
that are contiguous in time?is both prevalent and
informative for word learning. This paper examines
the importance of topic continuity through a gram-
matical inference problem. We build on Johnson et
al. (2012)?s work that used grammatical inference to
315
Transactions of the Association for Computational Linguistics, 1 (2013) 315?326. Action Editor: Mark Steedman.
Submitted 2/2013; Revised 6/2013; Published 7/2013. c?2013 Association for Computational Linguistics.

	
 









  	
 	   	 	 
	

	





	
	

	
	

	


	


	

	

	
	

	
	

	


	


	

	
	