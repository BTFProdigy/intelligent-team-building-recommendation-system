Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 421?432, Dublin, Ireland, August 23-29 2014.
Investigating the Usefulness of Generalized Word Representations in SMT
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Helmut Schmid Alexander Fraser
Ludwig Maximilian University Munich
fraser,schmid@cis.uni-muenchen.de
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Abstract
We investigate the use of generalized representations (POS, morphological analysis and word
clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our
integration enables these models to learn richer lexical and reordering patterns, consider wider
contextual information and generalize better in sparse data conditions. When interpolating gen-
eralized OSM models on the standard IWSLT and WMT tasks we observed improvements of up
to +1.35 on the English-to-German task and +0.63 for the German-to-English task. Using auto-
matically generated word classes in standard phrase-based models and the OSM models yields
an average improvement of +0.80 across 8 language pairs on the IWSLT shared task.
1 Introduction
The increasing availability of digital text has galvanized the use of empirical methods in many fields
including Machine Translation. Given bilingual text, it is now possible to automatically learn translation
rules that required years of effort previously. Bilingual data, however, is abundantly available for only a
handful of language pairs. The problem of reliably estimating statistical models for translation becomes
more of a challenge under sparse data conditions especially when translating into morphologically rich
or syntactically divergent languages. The former becomes challenging due to lexical sparsity and the
latter suffers from sparsity in learning underlying reordering patterns. The last decade of research in
Statistical Machine Translation has witnessed many attempts to integrate linguistic analysis into SMT
models, to address the challenges of (i) translating into morphologically rich language languages, (ii)
modeling syntactic divergence across languages for better generalization in sparse data conditions.
The integration of the Operation Sequence Model into phrase-based paradigm (Durrani et al., 2013a;
Durrani et al., 2013b) improved the reordering capability and addressed the problem of the phrasal inde-
pendence assumption in the phrase-based models. The OSM model integrates translation and reordering
into a single generative story. By jointly considering translation and reordering context across phrasal
boundaries, the OSM model considers much richer conditioning than phrasal translation and lexicalized
reordering models. However, due to data sparsity the model often falls back to very small context sizes.
We address this problem by learning operation sequences over generalized representations such as POS
and Morph tags. This enables us to learn richer translation and reordering patterns that can general-
ize better in sparse data conditions. The model benefits from wider contextual information as we show
empirically in our results.
We investigate two methods to combine generalized OSM models with the lexically driven OSM
model and experimented on German-English translation tasks. Our best system that uses a linear combi-
nation of different OSM models gives significant improvements over a competitive baseline system. An
improvement of up to +1.35 was observed on the English-to-German and up to +0.63 BLEU points on
the German-to-English task over a factored augmented baseline system (Koehn and Hoang, 2007).
POS taggers and morphological analyzers, however, are not available for many resource poor lan-
guages. In the second half of the paper we investigate whether annotating the data with automatic word
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
421
clusters helps improve the performance. Word clustering is similar to POS-tagging/Morphological anno-
tation except that it also captures interesting syntactic and lexical semantics, for example countries and
languages are grouped in separate clusters, animate objects are differentiated from inanimate objects,
colors are grouped in a separate cluster etc. Word clusters, however, deterministically map each word
type to a unique
1
cluster, unlike POS/Morph tagging, and therefore might be less useful for disambigua-
tion. We use the mkcls utility in GIZA (Och and Ney, 2003) to cluster source and target vocabularies
into classes and will therefore refer to automatic classes as Och clusters/classes in this paper.
We first use Och classes as an additional factor in phrase-based translation model, along with a target
LM model over cluster-ids to improve the baseline system. We then additionally use the OSM model
over cluster-ids. Our experiments include translation from English to Dutch, French, Italian, Polish,
Portuguese, Russian, Spanish, Slovenian and Turkish on IWSLT shared task data. Our results show an
average improvement of +0.80, ranging from +0.41 to +2.02. Compared to the improved baseline system
obtained by using Och classes as a factor in phrase-based translation models, adding an OSM model over
cluster-ids improved performance in four (French, Spanish, Dutch and Slovenian) out of eight cases. In
other cases performance stayed constant or dropped slightly. We also used POS annotations for three
tasks, namely translating from English into French, Spanish and Dutch to compare the performance of
the two different kinds of generalizations. Surprisingly, using Och classes always performed better than
using POS annotations. The rest of the paper is organized as follows. Section 2 gives an account on
related work. Section 3 discusses the factor-based OSM model. Section 4 presents the experimental
setup and the results. Section 5 concludes the paper.
2 Related Work
Previous work on integrating linguistic knowledge into SMT models can be broken into two groups. The
first group focuses on using linguistic knowledge to improve reordering between syntactically different
languages. A second group focuses on translating into morphologically rich languages.
Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the target
order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source
sentences. Collins et al. (2005) and Popovi?c and Ney (2006) proposed methods for reordering the source
using a small set of handcrafted rules. Crego and Mari?no (2007) use syntactic trees to derive rewrite
rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create
longer phrase translation. A whole new paradigm of using syntactic annotation to address long range
reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007)
etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS
tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except
that OSM model is substantially different from the TSM model as it integrates both the translation and
reordering mechanisms into a combined model. Therefore both translation and reordering decisions can
benefit from richer generalized representations.
A second group of work addresses the problem of translating into morphologically richer languages.
The idea of translating to stems and then inflecting the stems in a separate step has been studied by
Toutanova et al. (2008), de Gispert and Mari?no (2008), Fraser et al. (2012), Chahuneau et al. (2013) and
others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors
into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as
additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeN-
ero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement
errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find
out which features are best handled by modeling them as a part of translation, and which ones are better
predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use
word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker
and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother
1
We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible
classes when calculating the n-gram probabilities.
422
Figure 1: Operation Sequence Model ? Training Sentence with Generation and Test Sentences
distributions and better generalizations has been a widely known and applied technique in natural lan-
guage processing. Training based on word classes has been previously explored by various researchers.
Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based
on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013),
Chahuneau et al. (2013) and Bisazza and Monz (2014).
More recent research has started to set apart from the conventional maximum likelihood estimates
toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al.,
2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improve-
ments, traditional models continue to dominate the field due to their simplicity and low computational
complexity. How much of the improvement will be retained when scaling these models to all available
data instead of a limited amount will be interesting.
3 Operation Sequence Model
The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework
(Casacuberta and Vidal, 2004; Mari?no et al., 2006). It represents the translation process through a
sequence of operations. An operation can be to simultaneously generate source or target words or to
perform reordering. Reordering is carried out through jump and gap operations. The model is different
from its ancestors in that it strongly integrates translation and reordering into a single generative story in
which translation decisions can influence and get impacted by the reordering decisions and vice versa.
Given a bilingual sentence pair < F,E > and its alignment A, a sequence of operations o
1
, o
2
. . . , o
J
is generated deterministically through a conversion algorithm. The model is learned by learning Markov
chains over these sequences and is formally defined as:
p
osm
(F,E,A) =
J
?
j=1
p(o
j
|o
j?n+1
, ..., o
j?1
)
Figure 1 shows an example of an aligned bilingual sentence pair and the corresponding operation se-
quence used to generate it. There is a 1-1 correspondence between a sentence pair and its operation
sequence. We thus get a unique sequence for every bilingual sentence pair given the alignment.
3.1 Motivation
Due to data sparsity it is impossible to observe all possible reordering patterns with all possible lexical
choices in translation operations. The lexically driven OSM model therefore often backs off to very
small context sizes. Coming back to the training example in Figure 1. The useful reordering pattern
423
learned through this example is:
Ich kann umstellen? I can rearrange
which is memorized through the operation sequence:
Generate(Ich, I) ? Generate(kann, can) ? Insert Gap ? Generate(umstellen, rearrange)
It can generalize to the test sentence shown in Figure 1(a). However, it fails to generalize to the sentences
in Figure 1(b) and (c) although the underlying reordering pattern is the same. The second part of the
German verb complex usually appears at the end of a clause or a sentence and needs to be moved in order
to produce the correct English word order. However, due to data sparsity such a combination of lexical
decisions and reordering decisions may not be observed during training. The model would therefore fail
to generalize in such circumstances. This problem can be addressed by learning a generalized form of
the same reordering rule. By annotating the corpus with word classes such as POS tags, we obtain the
reordering pattern:
PPER VMFIN VVINF? PP MD VB
memorized through the operation sequence:
Generate (PPER,PP) ? Generate (VMFIN,MD) ? Insert Gap ? Generate (VVINF,VB)
This rule generalizes to all the test sentences in Figure 1. Since the OSM model strongly couples
translation and reordering, the probability of each translation or reordering operation depends on the
n previous translation/reordering decisions. The generalization of the model by replacing words with
POS tags allows the model to consider a wider syntactic context, thus improving lexical decisions and
the reordering capability of the model. Using different kinds of word classes, we can also control the
type of abstraction. Using lemmas for example, we can map different forms of the verb ?k?onnen ? can?
(kann, kannst, konnte) to a single class. Och clusters can provide different levels of granularity.
3.2 Models
Given that we can learn OSM models over different word representations, the question then is how
to combine the lexically driven OSM model with an OSM model based on a generalized word repre-
sentation. The simplest approach is to treat each OSM model as a separate feature in the log-linear
framework, thus summing up the weighted log probabilities. The effect of this is similar to an And
operation. A translation is considered good if both, the word-based OSM and the POS-based OSM
models indicate that it is a good translation. However, an Or operation might be more desirable in
some scenarios. The operation Generate (trotz, in spite of) should be ranked high although the POS-
based operation Generate(APPR, IN IN IN) is improbable. Similarly, the generalized operation sequence:
Insert Gap ? Generate (ADJ, JJ) ? Jump Back ? Generate (NOM, NN)
that captures the swapping of noun and adjective in French-English, should be ranked higher
even though noir (black) never appeared after cheval (horse) during training and the sequence:
Insert Gap ? Generate (noir, black) ? Jump Back ? Generate (cheval , horse)
is never observed. Instead of using both the models, a single model that could switch between
different generalized OSMs during translation and choose the one which gives the best prediction
in each situation, can be used. In order to achieve this effect, we formulated a second model that
interpolates the lexically driven OSM model with its generalized variants. However, we can only
424
interpolate two models that predict the same representation. The lexically driven OSM predicts the
surface forms whereas the POS-based OSM predicts POS translations. To make the two comparable,
we multiply the POS-based OSM probability with the probability of the lexical operation given the POS
operation. More specifically the probability of the generalized model gm can be defined as:
p
gm
(o
j
|o
j?1
j?n+1
) = p
osm
pos
(o
?
j
|o
?
j?1
j?n+1
) p(o
j
|o
?
j
) (1)
where p
osm
pos
is the operation sequence model learned over POS tags and p(o
j
|o
?
j
) is the probability of
the lexical operation given the POS-based operation. It is 1 for all reordering operations. We assume here
that for each lexical operation o
j
a corresponding POS-based operation o
?
j
is uniquely determined. With
p
osm
sur
= p
osm
sur
(o
j
|o
j?1
j?n+1
) (lexically driven OSM model) and p
gm
= p
gm
(o
j
|o
j?1
j?n+1
) (generalized
OSM model as described above), the overall probability of the new model p
osm
is defined as:
p
osm
= ?p
osm
sur
+ (1? ?)p
gm
(2)
Such an interpolation is expensive in the discriminative training. It would require a sub-tuning routine
inside of tuning, a main loop to train all the features including the OSM model and an inner loop to
distribute the weight assigned to OSM model among lexically driven and POS-based OSM models. We
therefore just take the larger one of the two model values and add a POS-based translation penalty ?. The
value of this penalty is the number of times that the POS-based operation was chosen when translating
a sentence. This penalty acts similarly as the prior ? above. Using this formulation, the model could
therefore be redefined as:
p
osm
=
{
p
osm
sur
if p
osm
sur
? e
?
p
gm
e
?
p
gm
otherwise
(3)
where ? is the weight for the POS driven translation penalty ?. This allows the optimizer to control
whether it prefers the lexically driven or the POS-driven OSM model. By setting a very low weight ?
the optimizer can force the translator to always choose lexically driven OSM. This formulation can be
extended to multiple generalized OSM models based on e.g. POS tags, morphological tags, or word
clusters. Equation 2 can be rewritten as follows:
p
osm
= ?
1
p
osm
sur
+
n
?
i=2
?
i
p
gm
i
(4)
with
?
n
i=1
?
i
= 1 and p
gm
i
defined analogous to Equation 1.
Setting p
gm
1
= p
osm
sur
and ?
1
= 0, we can again simplify Equation 4 by taking the maximum to:
p
osm
=
n
max
i=1
e
?
i
p
gm
i
(5)
We use a translation penalty ?
i
for each generalized model and tune its weight ?
i
along with the weights
of other features. We will refer to this model as Model
or
in this paper and the commonly used log-
linear interpolation of the features as Model
and
. The intuition behind Model
or
is that we back-off
to generalized representations only when the lexically driven model doesn?t provide enough contextual
evidence. The downside of this approach, however, is that unlike Model
and
, it cannot distribute weights
over multiple features and solely relies on a single model.
4 Evaluation
Data: We ran experiments with data made available for the translation task of the IWSLT-13 (Cettolo et
al., 2013): International Workshop on Spoken Language Translation
2
and WMT-13 (Bojar et al., 2013):
Eighth Workshop on Statistical Machine Translation.
3
The sizes of bitext used for the estimation of
translation and monolingual language models are reported in Table 1.
We used LoPar (Schmid, 2000) to obtain morphological analysis and POS annotation of German and
MXPOST (Ratnaparkhi, 1998), a maximum entropy model for English POS tags. For other language
pairs we used TreeTagger (Schmid, 1994).
2
http://www.iwslt2013.org/
3
http://www.statmt.org/wmt13/
425
Pair Parallel Monolingual Pair Parallel Monolingual Pair Parallel Monolingual
de?en ?4.6 M ?287.3 M en?de ?4.6 M ?59.5 M en?fr ?5.5 M ?69 M
en?es ?4.1 M ?59.6 M en?nl ?2.1 M ?21.7 M en?ru ?1.15 M ?21 M
en?pt ?1.0 M ?2.3 M en?pl ?0.77 M ?0.8 M en?sl ?0.63 M ?0.65 M
en?tr ?0.13 M ?0.14 M
Table 1: Number of Sentences (in Millions) used for Training
Model iwslt
10
wmt
13
iwslt
10
wmt
13
English-to-German German-to-English
Baseline 23.56 20.38 31.46 27.27
M
and
(pos,pos)
23.93?+0.37 20.61 ?+0.23 31.91?+0.45 27.55 ?+0.28
M
and
(pos,morph)
24.62?+1.06 20.88?+0.50 32.09?+0.63 27.62?+0.35
M
and
(all)
24.91?+1.35 20.93?+0.55 32.00?+0.54 27.71?+0.44
M
or
(pos,pos)
23.61 ?+0.05 20.24 ?-0.14 31.55 ?+0.09 27.32 ?+0.05
M
or
(pos,morph)
23.83 ?+0.27 20.44 ?+0.08 31.58 ?+0.12 27.20 ?-0.07
M
or
(all)
23.88 ?+0.32 20.55 ?+0.17 31.40 ?-0.06 27.15 ?-0.12
Table 2: Evaluating Generalized OSM Models for German-English pairs ? Bold: Statistically Significant
(Koehn, 2004) w.r.t Baseline
Baseline System: We trained a Moses system (Koehn et al., 2007), replicating the settings described
in (Birch et al., 2013) developed for the 2013 Workshop on Spoken Language Translation. The features
included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011)
used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4
additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty,
lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6,
100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Prun-
ing (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the
no-reordering-over-punctuation heuristic. We used the compact phrase table representation by Junczys-
Dowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and
Knight, 2003). German-to-English and English-to-German baseline systems also used POS and mor-
phological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney
smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang,
2007). We used an unsupervised transliteration model (Durrani et al., 2014) to transliterate OOV words
when translating into Russian.
Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013
datasets made available for the IWSLT-13 workshop. We performed a secondary set of experiments
for German-English pairs using tuning and test sets made available for the WMT-13 workshop. We
concatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences. Evaluation
was performed on the news-test set 2013 which contains 3000 sentences. Tuning was performed using
the k-best batch MIRA algorithm (Cherry and Foster, 2012) with at most 25 iterations. We use BLEU
(Papineni et al., 2002) as a metric to evaluate our results.
Results I ? Using Linguistic Annotation: We trained 5-gram OSM models over different representa-
tions and added these to the baseline system. First we evaluated Model
and
(M
and
) which uses a MIRA
tuned linear combination of different OSM models versus Model
or
(M
or
) which computes only one
OSM model but allows the generator to switch between different OSM models built on various gener-
alized forms. Table 2 shows results from running experiments on German-English pairs. We found that
the simpler model Model
and
outperforms Model
or
in all the experiments. Model
or
does not give
significant improvements over the baseline system and shows an occasional drop. This result is contrary
to the expectation formulated in Section 3.2. We speculate that the optimizer faces problems to train this
kind of model, because it cannot take into account that the selected OSM model can change when the
weight parameter is modified. It assumes that the feature stays constant. In our formulation the same
426
derivation can occur with different feature scores in different decoding runs and the optimizer is unable
to handle this. Our speculation is based on the observation of ?
?
, the weight of feature ? which allows
the translator to switch between different OSM models. The value of ?
?
was not stable across different
iterations and different experiments.
Model
and
consistently improves the baseline. Adding an OSM model over [pos, morph] (source:pos,
target:morph) combination gave the best results, giving a statistically significant gain of +1.06 on the
iwslt
10
test-set and +0.50 on the wmt
13
test-set. Using an OSM model over a [pos,pos] combination
also showed improvements, however, not as much as using morphological tags. Morphological tags pro-
vide richer information for disambiguation when translating into German. Note that the baseline system
also used a target sequence model over morphological tags. Nevertheless using an OSM [pos,morph]
model still gives significant improvements which shows that learning a joint model over source and tar-
get units is more fruitful than only considering target-side information. Using both the models together
gave best results for English-to-German giving a further improvement of +0.29 on the iwslt
10
task but
no real gain on the wmt
13
task. Using morphological tags also produced the best results for the German-
to-English pair, giving a statistically significant gain of +0.63 on iwslt
10
and +0.35 on wmt
13
. Using
both the models together did not give any further significant improvements. The results changed by
+0.10 and -0.09 on the wmt
13
and iwslt
10
test-sets respectively.
Results-II ? Using Och Classes: In our secondary experiments we tested the effect of using Och
clusters. The overall goal was to study whether using unsupervised word classes can serve the same
purpose as POS tags and to compare the two methods of annotating the data. We obtained Och clusters
using the mkcls utility (Och, 1999) in GIZA++ (Och and Ney, 2003). This is generally run during
the alignment process where data is divided into 50 classes to estimate IBM Model-4. Chahuneau et
al. (2013) found mapping data to 600 Och clusters useful, so we used this as well. We additionally
experimented with using 200 and 1000 classes. We integrated Och clusters as additional factors
4
when
training the phrase-translation models and used a monolingual n-gram model over cluster-ids built on the
target-side of the in-domain corpus. Then we added a 5-gram OSM model over cluster-ids. We replace
surface forms with their cluster-ids in source and target corpus and convert it to operation sequences,
that jointly generate source and target cluster-ids. We only used Model
and
for these experiments when
adding an OSM model over cluster-ids.
B
0
50 200 600 1000 POS 50 200 600 1000 POS
Target Sequence Model over Word Clusters Operation Sequence Model over Word Clusters
en? fr 33.17 33.30 33.40 33.05 33.05 33.14 33.76 33.74 33.58 33.75 33.03
en? es 34.14 34.33 34.58 34.46 33.96 33.91 34.73 34.62 34.60 34.55 34.35
en? nl 26.51 26.67 26.15 26.31 26.47 26.55 26.91 26.52 26.61 26.49 26.62
en? ru 13.12 13.34 13.51 13.53 13.97 ? 13.61 13.66 13.80 13.63 ?
en? sl 17.98 18.67 18.55 17.67 17.97 ? 18.64 18.91 18.17 17.98 ?
en? pt 30.80 31.62 32.21 32.40 32.44 ? 31.77 32.44 32.34 31.90 ?
en? pl 9.74 9.90 10.11 10.05 10.43 ? 10.06 10.19 10.24 10.14 ?
en? tr 7.18 7.43 7.45 7.50 7.50 ? 7.26 7.28 7.51 7.54 ?
Table 3: Evaluating Phrase-based and N-gram-based Translation Models over Och Clusters
Table 3 shows results from using models based on cluster-ids. The left side of the table evaluate the
use of adding a target sequence model over cluster-ids using a factored-based translation model. Results
improved consistently in all resource poor languages (pt, pl, tr) giving significant improvements in most
of the cases. Mixed results were obtained for the pairs with a reasonable amount of parallel data (fr,
es, nl), showing an occasional drop in performance. However, improvements can be found for all the
language pairs.
4
Note that adding cluster-ids in factored models alone has no impact in this scenario, as we are using hard clustering (each
word deterministically maps onto a unique cluster-id). In a joint source-target factored model which is what we are using, it
will result in an identical distribution as the baseline system.
427
In the right half of the table we tested whether additionally using an OSM model built over cluster-ids,
on top of a phrase-based system that uses cluster-ids as factor and target language model, improves the
performance any further. Consistent improvements were seen in Spanish and French. Better systems
were produced in the case of French, Spanish, Dutch and Slovenian. No improvements were observed
for Turkish and Portuguese whereas the performance got worse in Polish and Russian.
Using 50 classes consistently improved the baseline. Different numbers of clusters provide different
levels of abstraction and granularity. We also tried using OSM models over different numbers of clus-
ters simultaneously for English-to-Spanish, English-to-French and English-to-Dutch pairs in an effort to
explore whether using different numbers of clusters to classify data provides different information. A
slight gain was observed for EN-ES as the best system improved from 34.73 to 34.95. No further gains
were observed for the other two pairs.
We also used POS annotation as a factor instead of Och clusters in French, Spanish and Dutch. See
the POS columns of Table 3. Using POS as an additional factor, did not improve over the baseline
performance. A significant drop was seen in the case of English-to-Spanish. Using a POS-based OSM
on top of the POS-based phrase-model did not help either except for Spanish where results got improved
by +0.44 over its phrase-based variant that used a POS factor. However, using Och clusters produced
better results in all three cases. We speculate that the reason for this result is that Och clusters are
more evenly distributed as compared to POS tags where the distribution is biased toward noun class
and secondly Och clusters are optimized for language modeling. Also each word is deterministically
mapped to a single class but can have multiple POS tags. The latter thus causes a sparser translation
model. Finally Table 4 shows the comparison of results on iwslt
11?13
by running baseline B
0
and best
systems B
x
in Tables 3.
iwslt
11
iwslt
12
iwslt
13
Avg
B
0
B
x
B
0
B
x
B
0
B
x
B
0
B
x
?
en? fr 39.84 40.63 40.50 41.24 ? ? 40.24 40.94 +0.70
en? es 32.89 33.24 26.45 26.81 34.01 34.73 31.12 31.60 +0.48
en? nl 30.01 30.31 26.40 26.72 24.96 25.57 27.12 27.53 +0.41
en? ru 14.93 15.91 13.01 13.53 15.65 16.4 14.53 15.28 +0.75
en? sl ? ? 11.34 12.40 12.85 13.73 12.09 13.10 +1.01
en? pt 31.61 33.62 33.24 34.91 30.83 33.24 31.89 33.92 +2.02
en? pl 12.73 13.13 9.52 10.50 11.30 11.54 11.18 11.72 +0.53
en? tr 7.01 7.42 6.99 7.43 6.21 6.84 6.74 7.23 +0.49
Avg 24.15 24.89 20.93 21.69 19.40 20.29 21.49 22.29 +0.80
Table 4: Evaluating on Test Sets iwslt
11?13
? B
0
= Baseline System, B
x
= Best Systems in Tables 2
Analysis: In a post-evaluation analysis we confirmed whether using generalized OSM models actually
consider a wider contextual window than its lexically driven variant. The graph shown in Figure 2 shows
average context size considered (on top of each set of bars) and percentages of 1-5 gram matches by
different OSM models. The results show that the probability of an operation is conditioned on less than a
trigram in the OSM model over surface forms. In comparison OSM models over POS, morph or cluster-
ids consider a window of roughly 4 previous operations thus considering more contextual information.
The percentage of 5-gram matches increases from 15.5% to 59.2% using POS-based OSM model and
up to 45.6% in morph-based OSM model, the number of unigram matches are decreased from 8.30% to
less than 1% in both the models. Similar observation is made for the OSM models over clusters where
5-gram matches improve from 12% to 30% on average, showing the ability of the generalized models to
use richer conditioning thus improving the translation quality.
We also analyzed what kind of words are clustered together using Och classes and found that clusters
capture both syntax and lexical semantics. Figure 2 (b) shows several useful clusters to exhibit this. We
also saw negative examples where words from different classes are clustered together. ?Boy?, ?Girl? and
?Man? for example were clustered into a single class but ?Woman? in another. Similarly ?Grey? and
?Orange? were grouped together with animated objects.
428
Figure 2: (a) Average Size of N-grams Used in Different OSM Models and Percentages of 1-5 Gram
Matches in Three Language Pairs (b) Different Word Clusters using 50 Classes
5 Conclusion
In this paper we investigated the usefulness of integrating word classes in phrase-based models and
Operation Sequence N-gram models. We explored two models of interpolating generalized OSM models
and tested variations on the standard IWSLT and WMT tasks. Our results showed that the simpler more
commonly used method of integrating the models in the log-linear framework worked best. We showed
that by learning OSM models over generalized POS and morphological representations, we were able
to build richer models that outperformed state-of-the-art baseline systems. Statistically significant gains
of up to +1.35 and +0.63 were observed in English-to-German and German-to-English tasks. We also
made use of Och classes as additional factors in phrase translation and language models. These were
tested translating from English to 8 different languages which includes a mixture of morphologically
rich (French, Spanish and Russian, Dutch, and Turkish) and sparse data (Portuguese, Polish, Slovenian
and Turkish) languages. Our results show that using clusters was helpful in all of the cases. Using
the OSM model over word-clusters additionally improved the performance further. Our results show an
average improvement of +0.80, ranging from +0.41 to +2.02. Our EN-FR systems were ranked third (on
tst2013) and second (on tst2011-tst2012) in IWSLT-13 translation task following EU-Bridge (Freitag et
al., 2013) which used our output for system combination. The code to train class-based models has been
made available to the research community via the Moses toolkit. See Advanced Features
5
in the Moses
Decoder for details.
Acknowledgements
We would like to thank the anonymous reviewers for their helpful feedback and suggestions. The re-
search leading to these results has received funding from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreements n
?
287658 (EU-Bridge) and n
?
287688 (MateCat).
Alexander Fraser was funded by Deutsche Forschungsgemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid was supported by Deutsche Forschungsgemeinschaft
grant SFB 732. This publication only reflects the authors? views.
References
Alexandra Birch, Nadir Durrani, and Philipp Koehn. 2013. Edinburgh SLT and MT System Description for the
IWSLT 2013 Evaluation. In Proceedings of the 10th International Workshop on Spoken Language Translation,
5
http://www.statmt.org/moses/?n=Moses.AdvancedFeatures
429
pages 40?48, Heidelberg, Germany, December.
Arianna Bisazza and Christof Monz. 2014. Class-Based Language Modeling for Translating into Morphologically
Rich Languages. In Proceedings of the 25th Annual Conference on Computational Linguistics (COLING),
Dublin, Ireland, August.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Francisco Casacuberta and Enrique Vidal. 2004. Machine Translation with Inferred Stochastic Finite-State Trans-
ducers. Computational Linguistics, 30:205?225.
Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th
IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,
Heidelberg, Germany.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into Morphologically Rich
Languages with Synthetic Phrases. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing.
Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 427?436, Montr?eal, Canada, June. Association for Computational Lin-
guistics.
Colin Cherry. 2013. Improved Reordering for Phrase-Based Translation using Sparse Features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31, Atlanta, Georgia, June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Trans-
lation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),
pages 531?540, Ann Arbor, MI.
Josep M. Crego and Jos?e B. Mari?no. 2007. Syntax-Enhanced N-gram-Based SMT. In Proceedings of the 11th
Machine Translation Summit, MT Summit XI, pages 111?118.
Josep M. Crego and Franc?ois Yvon. 2010. Improving Reordering with Linguistically Informed Bilingual N-
Grams. In Coling 2010: Posters, pages 197?205, Beijing, China, August. Coling 2010 Organizing Committee.
Adri`a de Gispert and Jos?e B. Mari?no. 2008. On the Impact of Morphology in English to Spanish statistical MT.
Speech Communication, 50(11-12):1034?1046.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A Joint Sequence Translation Model with Integrated
Reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013a. Model With Minimal Translation Units, But Decode
With Phrases. In The 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta, Georgia, USA, June. Association for Computational Lin-
guistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013b. Can Markov Models
Over Minimal Translation Units Help Phrase-Based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, Sofia, Bulgaria, August. Association for Computational Linguistics.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn. 2014. Integrating an Unsupervised Transliteration
Model into Statistical Machine Translation. In Proceedings of the 15th Conference of the European Chapter of
the ACL (EACL 2014), Gothenburg, Sweden, April. Association for Computational Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Translation. In
Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1012?
1020, Columbus, OH, USA. The Association for Computer Linguistics.
Ahmed El Kholy and Nizar Habash. 2012. Translate, Predict or Generate: Modeling Rich Morphology in Statis-
tical Machine Translation. volume 12.
430
Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pages 664?674, Avignon, France, April. Association for Computational Linguistics.
Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Nadir Durrani, Matthias Huck, Philipp Koehn,
Thanh-Le Ha, Jan Niehues, Mohammed Mediani, Teresa Herrmann, Alex Waibel, Nicola Bertoldi, Mauro Cet-
tolo, and Marcello Federico. 2013. EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In
International Workshop on Spoken Language Translation, Heidelberg, Germany, December.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of
COLING-ACL, pages 961?968, Sydney, Australia. Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Scott Wen-tau Yih, and Li Deng. 2014. Learning Continuous Phrase Representations
for Translation Modeling. In Proceedings of the Association for Computational Linguistics, Baltimore, MD,
USA, June.
Spence Green and John DeNero. 2012. A Class-Based Agreement Model for Generating Accurately Inflected
Translations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 146?155, Jeju Island, Korea, July. Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello Federico. 2010. FBK at WMT 2010: Word Lattices for Mor-
phological Reduction and Chunk-Based Reordering. In Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 88?92, Uppsala, Sweden, July. Association for Computational
Linguistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse Lexicalised features and Topic Adaptation for
SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages
268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages 187?197, Edinburgh, Scotland, United Kingdom, July.
Hieu Hoang and Philipp Koehn. 2009. Improving Mid-Range Re-Ordering Using Templates of Factors. In
Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 372?379, Athens,
Greece, March. Association for Computational Linguistics.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum Translation Modeling with Recurrent
Neural Networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Com-
putational Linguistics, pages 20?29, Gothenburg, Sweden, April. Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Computational Linguistics.
Marcin Junczys-Dowmunt. 2012. Phrasal Rank-Encoding: Exploiting Phrase Redundancy and Translational
Relations for Phrase Table Compression. The Prague Bulletin of Mathematical Linguistics, 98:63?74.
Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the 10th
Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 187?193,
Morristown, NJ.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demonstrations,
Prague, Czech Republic.
Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, pages 169?176.
431
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon. 2012. Continuous Space Translation Models with Neural
Networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pages 39?48, Montr?eal, Canada, June. Association for
Computational Linguistics.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-Based Machine Translation. Computational Linguistics, 32(4):527?549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
pages 198?206, Edinburgh, Scotland, July. Association for Computational Linguistics.
Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In Processings of EACL, pages
71?76, Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, ACL ?02, pages 311?318, Morristown, NJ, USA.
Maja Popovi?c and Hermann Ney. 2006. POS-based Word Reorderings for Statistical Machine Translation. In
International Conference on Language Resources and Evaluation, pages 1278?1283, Genoa, Italy.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference
on New Methods in Language Processing, pages 44?49, Manchester, UK.
Helmut Schmid. 2000. Lopar: Design and implementation. Bericht des sonderforschungsbereiches ?sprachtheo-
retische grundlagen fr die computerlinguistik?, Institute for Computational Linguistics, University of Stuttgart.
Holger Schwenk. 2012. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.
In Proceedings of COLING 2012: Posters, pages 1071?1080, Mumbai, India, December. The COLING 2012
Organizing Committee.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-08: HLT, pages 514?522, Columbus, Ohio, June. Association for
Computational Linguistics.
Joern Wuebker and Hermann Ney. 2012. Phrase Model Training for Statistical Machine Translation with Word
Lattices of Preprocessing Alternatives. In NAACL 2012 Seventh Workshop on Statistical Machine Translation,
pages 450?459, Montreal, Canada, June. Association for Computational Linguistics.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving Statistical Machine Translation
with Word Class Models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1377?1381, Seattle, Washington, USA, October. Association for Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of Coling 2004, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical
Machine Translation from English to Turkish. In Proceedings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 454?464, Uppsala, Sweden, July. Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing.
In Proceedings on the Workshop on Statistical Machine Translation, pages 138?141, New York City, June.
Association for Computational Linguistics.
432
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 148?153,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Integrating an Unsupervised Transliteration Model into
Statistical Machine Translation
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Hieu Hoang Philipp Koehn
University of Edinburgh
hieu.hoang,pkoehn@inf.ed.ac.uk
Hassan Sajjad
Qatar Computing Research Institute
hsajjad@@qf.org.qa
Abstract
We investigate three methods for integrat-
ing an unsupervised transliteration model
into an end-to-end SMT system. We in-
duce a transliteration model from parallel
data and use it to translate OOV words.
Our approach is fully unsupervised and
language independent. In the methods
to integrate transliterations, we observed
improvements from 0.23-0.75 (? 0.41)
BLEU points across 7 language pairs. We
also show that our mined transliteration
corpora provide better rule coverage and
translation quality compared to the gold
standard transliteration corpora.
1 Introduction
All machine translation (MT) systems suffer from
the existence of out-of-vocabulary (OOV) words,
irrespective of the amount of data available for
training. OOV words are mostly named entities,
technical terms or foreign words that can be trans-
lated to the target language using transliteration.
Much work (Al-Onaizan and Knight, 2002;
Zhao et al., 2007; Kashani et al., 2007; Habash,
2009) has been done on transliterating named enti-
ties and OOVs, and transliteration has been shown
to improve MT quality. Transliteration has also
shown to be useful for translating closely related
language pairs (Durrani et al., 2010; Nakov and
Tiedemann, 2012), and for disambiguation (Her-
mjakob et al., 2008; Azab et al., 2013). How-
ever, despite its utility, a transliteration module
does not exist in the commonly used MT toolk-
its, such as Moses (Koehn et al., 2007). One of the
main reasons is that the training data, a corpus of
transliteration pairs, required to build a translitera-
tion system, is not readily available for many lan-
guage pairs. Even if such a training data is avail-
able, mechanisms to integrate transliterated words
into MT pipelines are unavailable in these toolkits.
Generally, a supervised transliteration system is
trained separately outside of an MT pipeline, and
a na??ve approach, to replace OOV words with their
1-best transliterations in the post/pre-processing
step of decoding is commonly used.
In this work i) we use an unsupervised model
based on Expectation Maximization (EM) to in-
duce transliteration corpus from word aligned par-
allel data, which is then used to train a translitera-
tion model, ii) we investigate three different meth-
ods for integrating transliteration during decoding,
that we implemented within the Moses toolkit. To
the best of our knowledge, our work is the fore-
most attempt to integrate unsupervised translitera-
tion model into SMT.
This paper is organized as follows. Section 2
describes the unsupervised transliteration mining
system, which automatically mines transliteration
pairs from the same word-aligned parallel corpus
as used for training the MT system. Section 3 de-
scribes the transliteration model that is trained us-
ing the automatically extracted pairs. Section 4
presents three methods for incorporating translit-
eration into the MT pipeline, namely: i) replac-
ing OOVs with the 1-best transliteration in a post-
decoding step, ii) selecting the best translitera-
tion from the list of n-best transliterations using
transliteration and language model features in a
post-decoding step, iii) providing a transliteration
phrase-table to the decoder on the fly where it
can consider all features to select the best translit-
eration of OOV words. Section 5 presents re-
sults. Our integrations achieved an average im-
provement of 0.41 BLEU points over a competi-
tive baseline across 7 language pairs (Arabic, Ben-
gali, Farsi, Hindi, Russian, Telugu and Urdu-into-
English). An additional experiment showed that
our system provides better rule coverage as op-
posed to another built from gold standard translit-
eration corpus and produces better translations.
148
2 Transliteration Mining
The main bottleneck in building a transliteration
system is the lack of availability of translitera-
tion training pairs. It is, however, fair to assume
that any parallel data would contain a reasonable
number of transliterated word pairs. Transliter-
ation mining can be used to extract such word
pairs from the parallel corpus. Most previous
techniques on transliteration mining generally use
supervised and semi-supervised methods (Sherif
and Kondrak, 2007; Jiampojamarn et al., 2010;
Darwish, 2010; Kahki et al., 2012). This con-
strains the mining solution to language pairs for
which training data (seed data) is available. A few
researchers proposed unsupervised approaches to
mine transliterations (Lee and Choi, 1998; Sajjad
et al., 2011; Lin et al., 2011). We adapted the work
of Sajjad et al. (2012) as summarized below.
Model: The transliteration mining model is a
mixture of two sub-models, namely: a translit-
eration and a non-transliteration sub-model. The
idea is that the transliteration model would as-
sign higher probabilities to transliteration pairs
compared to the probabilities assigned by a non-
transliteration model to the same pairs. Consider a
word pair (e, f), the transliteration model prob-
ability for the word pair is defined as follows:
p
tr
(e, f) =
?
a?Align(e,f)
|a|
?
j=1
p(q
j
)
where Align(e, f) is the set of all possible se-
quences of character alignments, a is one align-
ment sequence and q
j
is a character alignment.
The non-transliteration model deals with the
word pairs that have no character relationship be-
tween them. It is modeled by multiplying source
and target character unigram models:
p
ntr
(e, f) =
|e|
?
i=1
p
E
(e
i
)
|f |
?
i=1
p
F
(f
i
)
The transliteration mining model is defined
as an interpolation of the transliteration sub-model
and the non-transliteration sub-model:
p(e, f) = (1? ?)p
tr
(e, f) + ?p
ntr
(e, f)
? is the prior probability of non-transliteration.
The non-transliteration model does not change
during training. We compute it in a pre-processing
step. The transliteration model learns character
alignment using expectation maximization (EM).
See Sajjad et al. (2012) for more details.
3 Transliteration Model
Now that we have transliteration word pairs, we
can learn a transliteration model. We segment the
training corpus into characters and learn a phrase-
based system over character pairs. The translitera-
tion model assumes that source and target charac-
ters are generated monotonically.
1
Therefore we
do not use any reordering models. We use 4 basic
phrase-translation features (direct, inverse phrase-
translation, and lexical weighting features), lan-
guage model feature (built from the target-side of
mined transliteration corpus), and word and phrase
penalties. The feature weights are tuned
2
on a dev-
set of 1000 transliteration pairs.
4 Integration to Machine Translation
We experimented with three methods for integrat-
ing transliterations, described below:
Method 1: involves replacing OOVs in the out-
put with the 1-best transliteration. The success of
Method 1 is solely contingent on the accuracy of
the transliteration model. Also, it ignores con-
text which may lead to incorrect transliteration.
For example, the Arabic word transliterates
to ?Bill? when followed by ?Clinton? and ?Bell?
if preceded by ?Alexander Graham?.
Method 2: provides n-best transliterations to
a monotonic decoder that uses a monolingual
language model and a transliteration phrase-
translation table to rescore transliterations. We
carry forward the 4 translation model features used
in the transliteration system to build a transliter-
ation phrase-table. We additionally use an LM-
OOV feature which counts the number of words
in a hypothesis that are unknown to the lan-
guage model. Smoothing methods such as Kneser-
Ney assign significant probability mass to unseen
events, which may cause the decoder to make in-
correct transliteration selection. The LM-OOV
feature acts as a prior to penalize such hypotheses.
Method 3: Method 2 can not benefit from all in-
decoding features and phenomenon like reorder-
ing. It transliterates Urdu compound
(Arabian Sea) to ?Sea Arabian?, if is an un-
known word. In method 3, we feed the translitera-
tion phrase-table directly into the first-pass decod-
ing which allows reordering of UNK words. We
1
Mining algorithm also makes this assumption.
2
Tuning data is subtracted from the training corpus while
tuning to avoid over-fitting. After the weights are tuned, we
add it back, retrain GIZA, and estimate new models.
149
use the decoding-graph-backoff option in Moses,
that allows multiple translation phrase tables and
back-off models. As in method 2, we also use the
LM-OOV feature in method 3.
3
5 Evaluation
Data: We experimented with 7 language pairs,
namely: Arabic, Bengali, Farsi, Hindi, Russian,
Telugu and Urdu-into-English. For Arabic
4
and
Farsi, we used the TED talks data (Cettolo et al.,
2012) made available for IWSLT-13, and we used
the dev2010 set for tuning and the test2011 and
test2012 sets for evaluation. For Indian languages
we used the Indic multi-parallel corpus (Post et
al., 2012), and we used the dev and test sets pro-
vided with the parallel corpus. For Russian, we
used WMT-13 data (Bojar et al., 2013), and we
used half of the news-test2012 for tuning and other
half for testing. We also evaluated on the news-
test2013 set. For all, we trained the language
model using the monolingual WMT-13 data. See
Table 1 for data statistics.
Lang Train
tm
Train
tr
Dev Test
1
Test
2
AR 152K 6795 887 1434 1704
BN 24K 1916 775 1000
FA 79K 4039 852 1185 1116
HI 39K 4719 1000 1000
RU 2M 302K 1501 1502 3000
TE 45K 4924 1000 1000
UR 87K 9131 980 883
Table 1: No. of sentences in Training Data and
Mined Transliteration Corpus (Types) (Train
tr
)
Baseline Settings: We trained a Moses system
replicating the settings used in competition-grade
systems (Durrani et al., 2013b; Birch et al., 2013):
a maximum sentence length of 80, GDFA sym-
metrization of GIZA++ alignments (Och and Ney,
2003), an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a 5-gram OSM (Dur-
rani et al., 2013a), msd-bidirectional-fe lexical-
3
Method 3 is desirable in cases where the decoder can
translate or transliterate a word. For example Hindi word
can be translated to ?Border? and also transliterated
to name ?Seema?. Identifying such candidates that can be
translated or transliterated is a challenge. Machine learning
techniques (Goldwasser and Roth, 2008; Kirschenbaum and
Wintner, 2009) and named entity recognizers (Klementiev
and Roth, 2006; Hermjakob et al., 2008) have been used for
this purpose. Though, we only focus on OOV words, method
3 can be used if such a classifier/NE tagger is available.
4
Arabic and Urdu are segmented using MADA (Habash
and Sadat, 2006) and UWS (Durrani and Hussain, 2010).
ized reordering, sparse lexical and domain fea-
tures (Hasler et al., 2012), a distortion limit of
6, 100-best translation options, MBR decoding
(Kumar and Byrne, 2004), Cube Pruning (Huang
and Chiang, 2007), and the no-reordering-over-
punctuation heuristic. We tuned with the k-best
batch MIRA (Cherry and Foster, 2012).
5
Transliteration Miner: The miner extracts
transliterations from a word-aligned parallel cor-
pus. We only used word pairs with 1-to-1 align-
ments.
6
Before feeding the list into the miner, we
cleaned it by removing digits, symbols, word pairs
where source or target is composed from less than
3 characters, and words containing foreign char-
acters that do not belong to this scripts. We ran
the miner with 10 iterations of EM. The number
of transliteration pairs (types) extracted for each
language pair is shown in Table 1 (Train
tr
).
Transliteration System: Before evaluating our
integrations into the SMT system, we performed
an intrinsic evaluation of the transliteration system
that we built from the mined pairs. We formed
test data for Arabic?English (1799 pairs), Hindi?
English (2394 pairs) and Russian?English (1859
pairs) by concatenating the seed data and gold
standard transliteration pairs both provided for the
Shared Task on Transliteration mining (Kumaran
et al., 2010). Table 2 shows precision and recall of
the mined transliteration system (MTS).
AR HI RU
Precision (1-best Accuracy) 20.0% 25.3% 46.1%
Recall (100-best Accuracy) 80.2% 79.3% 87.5%
Table 2: Precision and Recall of MTS
The precision (1-best accuracy) of the translit-
eration model is quite low. This is because the
transliteration corpus is noisy and contains imper-
fect transliteration pairs. For example, the miner
extracted the pair ( , Australasia), while
the correct transliteration is ?Australia?. We can
improve the precision by tightening the mining
threshold probability. However, our end goal is to
improve end-to-end MT and not the transliteration
system. We observed that recall is more important
than precision for overall MT quality. We provide
an empirical justification for this when discussing
the final experiments.
5
Retuning the transliteration features was not helpful, de-
fault weights are used.
6
M-N/1-N alignments are less likely to be transliterations.
150
MT Experiments: Table 3 gives a comprehen-
sive evaluation of the three methods of integra-
tion discussed in Section 4 along with the num-
ber
7
of OOV words (types) in different tests. We
report BLEU gains (Papineni et al., 2002) obtained
by each method. Method 1 (M
1
), that replaces
OOV words with 1-best transliteration gave an av-
erage improvement of +0.13. This result can be at-
tributed to the low precision of the transliteration
system (Table 2). Method 2 (M
2
), that translit-
erates OOVs in second pass monotonic decoding,
gave an average improvement of +0.39. Slightly
higher gains were obtained using Method 3 (M
3
),
that integrates transliteration phrase-table inside
decoder on the fly. However, the efficacy of M
3
in
comparison to M
2
is not as apparent, as M
2
pro-
duced better results than M
3
in half of the cases.
Lang Test B
0
M
1
M
2
M
3
OOV
AR iwslt
11
26.75 +0.12 +0.36 +0.25 587
iwslt
12
29.03 +0.10 +0.30 +0.27 682
BN jhu
12
16.29 +0.12 +0.42 +0.46 1239
FA iwslt
11
20.85 +0.10 +0.40 +0.31 559
iwslt
12
16.26 +0.04 +0.20 +0.26 400
HI jhu
12
15.64 +0.21 +0.35 +0.47 1629
RU wmt
12
33.95 +0.24 +0.55 +0.49 434
wmt
13
25.98 +0.25 +0.40 +0.23 799
TE jhu
12
11.04 -0.09 +0.40 +0.75 2343
UR jhu
12
23.25 +0.24 +0.54 +0.60 827
Avg 21.9 +0.13 +0.39 +0.41 950
Table 3: End-to-End MT Evaluation ? B
0
=
Baseline, M
1
= Method
1
, M
2
= Method
2
, M
3
=
Method
3
, BLEU gains shown for each method
In an effort to test whether improving translit-
eration precision would improve end-to-end SMT
results, we carried out another experiment. Instead
of building a transliteration system from mined
corpus, we built it using the gold standard corpus
(for Arabic, Hindi and Russian), that we also used
previously to do an intrinsic evaluation. We then
replaced our mined transliteration systems with
the gold standard transliteration systems, in the
best performing SMT systems for these languages.
Table 4 shows a comparison of performances. Al-
though the differences are small, systems using
mined transliteration system (MTS) outperformed
its counterpart that uses gold standard translitera-
tion system (GTS), except in Hindi?English where
7
Note that not all OOVs can be transliterated. This num-
ber is therefore an upper bound what can be transliterated.
both systems were equal.
AR HI RU
iwslt
11
iwslt
12
jhu
12
wmt
12
iwslt
13
MTS 27.11 29.33 16.11 34.50 26.38
GST 26.99 29.20 16.11 34.33 26.22
Table 4: Comparing Gold Standard Transliteration
(GST) and Mined Transliteration Systems
In the error analysis we found that the GST
system suffered from sparsity and did not pro-
vide enough coverage of rules to produce right
transliterations. For example, Arabic drops the
determiner (al), but such additions were not
observed in gold transliteration pairs. Arabic
word (Gigapixel) is therefore translit-
erated to ?algegabksl?. Similarly the GST system
learned no transliteration pairs to account for the
rule ?b ? p? and therefore erroneously translit-
erated (Spurlock) to ?Sbrlok?. Similar
observations were true for the case of Russian?
English. The rules ?a? u? and ?y? ? were not
observed in the gold set, and hence
(hurricane) was transliterated to ?herricane? and
(Talbot) to ?Talboty?. This shows that
better recall obtained from the mined pairs led to
overall improvement.
6 Conclusion
We incorporated unsupervised transliteration min-
ing model into standard MT pipeline to automati-
cally transliterate OOV words without needing ad-
ditional resources. We evaluated three methods
for integrating transliterations on 7 language pairs
and showed improvements ranging from 0.23-0.75
(? 0.41) BLEU points. We also showed that our
mined transliteration corpus provide better recall
and overall translation quality compared to the
gold standard transliteration corpus. The unsu-
pervised transliteration miner and its integration
to SMT has been made available to the research
community via the Moses toolkit.
Acknowledgments
We wish to thank the anonymous reviewers and
Kareem Darwish for their valuable feedback on
an earlier draft of this paper. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n
?
287658. This publication only reflects the au-
thors? views.
151
References
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing Named Entities Using Monolingual and Bilin-
gual Resources. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics.
Mahmoud Azab, Houda Bouamor, Behrang Mohit, and
Kemal Oflazer. 2013. Dudley North visits North
London: Learning When to Transliterate to Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 439?444, Atlanta, Georgia, June. Association
for Computational Linguistics.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Eighth Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT
3
: Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16
th
Conference of the European Association for Ma-
chine Translation (EAMT), pages 261?268, Trento,
Italy, May.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436, Montr?eal, Canada, June. Associa-
tion for Computational Linguistics.
Kareem Darwish. 2010. Transliteration Mining with
Phonetic Conflation and Iterative Training. In Pro-
ceedings of the 2010 Named Entities Workshop, Up-
psala, Sweden.
Nadir Durrani and Sarmad Hussain. 2010. Urdu Word
Segmentation. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 528?536, Los Angeles, California,
June. Association for Computational Linguistics.
Nadir Durrani, Hassan Sajjad, Alexander Fraser, and
Helmut Schmid. 2010. Hindi-to-Urdu Machine
Translation through Transliteration. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, Uppsala, Sweden.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Dan Goldwasser and Dan Roth. 2008. Active Sam-
ple Selection for Named Entity Transliteration. In
Proceedings of ACL-08: HLT, Short Papers, pages
53?56, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49?52, New York City,
USA, June. Association for Computational Linguis-
tics.
Nizar Habash. 2009. REMOOV: A Tool for Online
Handling of Out-of-Vocabulary Words in Machine
Translation. In Proceedings of the Second Interna-
tional Conference on Arabic Language Resources
and Tools, Cairo, Egypt, April. The MEDAR Con-
sortium.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Ulf Hermjakob, Kevin Knight, and Hal Daum?e III.
2008. Name Translation in Statistical Machine
Translation - Learning When to Transliterate. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Columbus, Ohio.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young
Kim, and Grzegorz Kondrak. 2010. Transliteration
152
Generation and Mining with Limited Training Re-
sources. In Proceedings of the 2010 Named Entities
Workshop, Uppsala, Sweden.
Ali El Kahki, Kareem Darwish, Ahmed Saad El Din,
and Mohamed Abd El-Wahab. 2012. Transliter-
ation Mining Using Large Training and Test Sets.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12.
Mehdi M. Kashani, Eric Joanis, Roland Kuhn, George
Foster, and Fred Popowich. 2007. Integration of
an Arabic Transliteration Module into a Statistical
Machine Translation System. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.
Amit Kirschenbaum and Shuly Wintner. 2009. Lightly
Supervised Transliteration for Machine Translation.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 433?
441, Athens, Greece, March. Association for Com-
putational Linguistics.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 82?88, New York
City, USA, June. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics, Demon-
stration Program, Prague, Czech Republic.
Shankar Kumar and William J. Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In HLT-NAACL, pages 169?176.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010.
Whitepaper of news 2010 shared task on transliter-
ation mining. In Proceedings of the 2010 Named
Entities Workshop, pages 29?38, Uppsala, Sweden,
July. Association for Computational Linguistics.
Jae-Sung Lee and Key-Sun Choi. 1998. English
to Korean Statistical Transliteration for Information
Retrieval. Computer Processing of Oriental Lan-
guages, 12(1):17?37.
Wen-Pin Lin, Matthew Snover, and Heng Ji. 2011.
Unsupervised Language-Independent Name Trans-
lation Mining from Wikipedia Infoboxes. In Pro-
ceedings of the First workshop on Unsupervised
Learning in NLP, pages 43?52, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Preslav Nakov and J?org Tiedemann. 2012. Com-
bining Word-Level and Character-Level Models for
Machine Translation Between Closely-Related Lan-
guages. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 301?305, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, ACL ?02, pages 311?318, Mor-
ristown, NJ, USA.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing Parallel Corpora for Six Indian
Languages via Crowdsourcing. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 401?409, Montr?eal, Canada, June. As-
sociation for Computational Linguistics.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An Algorithm for Unsupervised Translitera-
tion Mining with an Application to Word Alignment.
In Proceedings of the 49th Annual Conference of
the Association for Computational Linguistics, Port-
land, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A Statistical Model for Unsupervised and
Semi-supervised Transliteration Mining. In Pro-
ceedings of the 50th Annual Conference of the Asso-
ciation for Computational Linguistics, Jeju, Korea.
Tarek Sherif and Grzegorz Kondrak. 2007. Bootstrap-
ping a Stochastic Transducer for Arabic-English
Transliteration Extraction. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, Prague, Czech Republic.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A Log-Linear Block Transliteration
Model based on Bi-Stream HMMs. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, Rochester, New York.
153
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 528?536,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Urdu Word Segmentation 
 
Nadir Durrani Sarmad Hussain 
Institute for NLP Center for Research in Urdu Language Processing 
Universit?t Stuttgart National University of Computer and Emerging Sciences 
durrani@ims.uni-stuttgart.de sarmad.hussain@nu.edu.pk 
 
 
  
Abstract 
Word Segmentation is the foremost obligatory task in 
almost all the NLP applications where the initial phase 
requires tokenization of input into words. Urdu is 
amongst the Asian languages that face word segmenta-
tion challenge. However, unlike other Asian languages, 
word segmentation in Urdu not only has space omission 
errors but also space insertion errors. This paper dis-
cusses how orthographic and linguistic features in Urdu 
trigger these two problems. It also discusses the work 
that has been done to tokenize input text. We employ a 
hybrid solution that performs an n-gram ranking on top 
of rule based maximum matching heuristic. Our best 
technique gives an error detection of 85.8% and over-
all accuracy of 95.8%.  Further issues and possible fu-
ture directions are also discussed. 
1 Introduction 
All language processing applications require input 
text to be tokenized into words for further 
processing.  Languages like English normally use 
white spaces or punctuation marks to identify word 
boundaries, though with some complications, e.g. 
the word ?e.g.? uses a period in between and thus 
the period does not indicate a word boundary. 
However, many Asian languages like Thai, Khmer, 
Lao and Dzongkha do not have word boundaries 
and thus do not use white space to consistently 
mark word endings.  This makes the process of 
tokenization of input into words for such languages 
very challenging. 
 
Urdu is spoken by more than 100 million people, 
mostly in Pakistan and India1.  It is an Indo-Aryan 
language, written using Arabic script from right to 
left, and Nastalique writing style (Hussain, 2003).  
                                                          
1
 Ethnologue.com 
http://www.ethnologue.com/14/show_language.asp?code=UR
D 
Nastalique is a cursive writing system, which also 
does not have a concept of space.  Thus, though 
space is used in typing the language, it serves other 
purposes, as discussed later in this paper.  This en-
tails that space cannot be used as a reliable delimi-
ter for words.  Therefore, Urdu shares the word 
segmentation challenge for language processing, 
like other Asian languages. 
 
This paper explains the problem of word segmen-
tation in Urdu.  It gives details of work done to 
investigate linguistic typology of words and moti-
vation of using space in Urdu.  The paper then 
presents an algorithm developed to automatically 
process the input to produce consistent word seg-
mentation, and finally discusses the results and 
future directions. 
2 Urdu Writing System 
Urdu is written in cursive Arabic script. Characters 
in general join with the neighbors within a word 
and in doing so acquire different shapes. Logically, 
a character can acquire up to four shapes, i.e. ini-
tial, medial, final position in a connected sequence 
or an isolated form.  The characters having this 
four-way shaping are known as joiners. However, 
another set of characters only join with characters 
before them but do not join with character after 
them, and are termed as non-joiners.  The non-
joiners only have final and isolated forms.  For 
example Arabic Letter Farsi Yeh ? is a joiner and 
has four shapes ? ,? ,? and ? respectively and 
Arabic letter Dal ? is a non-joiner and has two 
forms ? and ? only. The shape that these characters 
acquire depends upon the context. 
 
Table 1 lists the orthographic rules that Urdu cha-
racters follow. For example, the table shows that in 
the middle of a word, if the character is a non-
joiner, it acquires final shape when following a 
528
joiner and isolated shape when following a non-
joiner.  This joining behavior results in formation 
of multiple connected portions within a word, each 
called a ligature.   
 
 
Table 1: Orthographic Rules for Urdu 
 
The concept of space as a word boundary marker is 
not present in Urdu writing. As an Urdu learner, a 
person is not taught to leave a space between 
words, but only to generate correct shaping while 
writing. Thus, the concept of space is only learnt 
later on when the person learns how to use a com-
puter.  However, space is introduced as a tool to 
control the correct letter shaping and not to consis-
tently separate words.  For example, the native 
speaker learns to insert a space within the word 
????? ??? (?needy?) to generate the correct shape 
of ?.  Without space it appears as ???????? which 
is visually incorrect. On contrary, the user finds it 
unnecessary to insert a space between the two 
words ???????? (?Urdu Center?), because the cor-
rect shaping is produced automatically as the first 
word ends with a non-joiner. Therefore ???????? 
and ???? ???? look identical. 
 
Though space character is not present in Urdu, 
with increasing usage of computer it is now being 
used, both to generate correct shaping (as dis-
cussed above) and also to separate words (a habit 
being carried over to Urdu from English literate 
computer users).  This makes space an unreliable 
cue for word boundary. The problem is further ob-
fuscated by the lack of a clear definition of a work 
in Urdu in some contexts.  The next section dis-
cusses these issues. 
3 Segmentation Issues in Urdu Text 
The segmentation challenges can be divided into 
two categories, challenges caused due to joiner and 
non-joiner characters.   
3.1 Space Omission 
As discussed, for words ending with non-joiners 
correct shaping is generated even when space is 
not typed and thus, many times a user omits the 
space.  Though there is no visible implication, 
from the perspective of computational processing 
not typing a space merges current word with the 
next word.  Figure 1 below illustrates an example, 
where the phrase has eight words (or tokens) each 
ending with a non-joiner and thus the whole 
phrase can be written without a space and is still 
visibly same and equally readable. 
 
????? ?? ??? ???? ??? ???? ?? ??? 
(a) 
?????????????????????????? 
(b) 
Figure 1: All Words Ending with Non-Joiners (a) 
with Spaces, (b) without Spaces between Words 
(?Troop Leader Ahmed Sher Dogar Said?) 
 
Another frequent set of space omissions are caused 
due to variation in the definition of a word in Urdu.  
There are certain function words in Urdu which 
may be combined with other function words and 
content words by some writers but may be written 
separately by others.  Shape variation may also 
occur in some of these cases, but is overlooked by 
the writers.  Table 2 gives some examples of such 
cases.  Though the merged form is not considered 
correct diction, it is still frequently used and thus 
has to be handled.  It is not considered spelling 
error but a writing variation. 
 
POS Combined Separated Translation 
Pn+CM ?? ?? ???? Yours 
D+ NN ?? ??? ????? at that time 
CM+ NN ?? ??? ????? Towards 
V+TA ??? ?? ????? will do 
CM + P ?? ??? ????? For 
Pn  = Pronoun, D = Demonstrative, NN = Noun, CM 
= Case Marker, V=Verb, P = Particle 
 
Table 2: Multiple Words Written in Connected 
Form Causing Shaping Changes 
 
Due to reasonable frequency of such cases, these 
may be considered as acceptable alternatives, and 
thus Urdu word segmentation system would need 
to deal with both forms and consider them equiva-
lent.  This process is productively applicable and 
Word J-Shape Example NJ-Shape Example 
Start I ???? Is ???? 
 
Middle 
M after J ???? F after J ???? 
I after NJ ???? Is after J ???? 
 
End 
F after J ??? F after J ??? 
Is after NJ ??? Is after NJ ?? 
J = Joiners, NJ = Non-Joiners 
I = Initial, Is = Isolated, M = Medial, F = Final 
Underlined = Shape in Consideration 
529
not limited to a few pre-determined cases. Addi-
tional complication in the process arises from the 
fact that in some cases (last two cases in Table 2) 
the spellings also change when two words are writ-
ten in combined form, due to the way these charac-
ters are encoded.  Urdu considers ? and ? both 
logically same characters at a certain level, though 
with different shapes to indicated different vowels 
(Hussain, 2004).  In combined form they render the 
same shape.  However, Unicode terms ? as a non-
joiner with no medial shape.  Thus, the Urdu writ-
ers use ? to generate the medial position of ? in 
combined form. 
3.2 Space Insertion 
When multiple morphemes are juxtaposed within a 
word, many of them tend to retain their shaping as 
separate ligatures.  If ending characters are joiners, 
space is usually inserted by writers to prevent them 
from joining and thus to retain the separate ligature 
identity.  This causes an extra space within a word.  
Though this creates the visually acceptable form, it 
creates two tokens from a single word in the con-
text of its processing.  If the writers do not type a 
space between these two morphemes within a word 
they would join and create a visually incorrect 
shape. Such examples are common in Urdu2.  Few 
of these cases are given in Table 3.   
 
Class A B Translation 
i ??????? ???? ??? Married 
ii ?????? ??? ??? Candle 
iii ????????? ???? ????? Unnecessarily 
iv ??????? ???? ??? Telephone 
v ??????? ?? ??? ?? PhD 
i= Affixation, ii = Compounding ,  
iii = Reduplication, iv = Foreign Word,  
v = Abbreviations 
 
Table 3: (A) Separated Form (Correct Shaping, but 
Two Tokens), (B) Combined Form (Erroneous 
Shaping, with one Token) 
 
As categorized in Table 3, the space insertion 
problem is caused due to multiple reasons.  Data 
analyzed shows that space is inserted (i) to keep 
affixes separate from the stem, (ii) in some cases, 
                                                          
2
 Though Unicode recommends using Zero Width Non-Joiner 
character in these context, this is not generally known by Urdu 
typists and thus not practiced; Further, this character is not 
available on most Urdu keyboards. 
to keep two words compounded together from vi-
sually merging, (iii) to keep reduplicated words 
from combining, (iv) to enhance readability of 
some foreign words written in Urdu, and (v) to 
keep English letters separate and readable when 
English abbreviations are transliterated. 
3.3 Extent of Segmentation Issues in Urdu 
In an earlier work on Urdu spell checking (Naseem 
and Hussain, 2007) report that a significant number 
of spelling errors3 are due to irregular use of space, 
as discussed above.  The study does a spelling 
check of an Urdu corpus.  The errors reported by 
the spelling checker are manually analyzed.  A to-
tal of 975 errors are found and of which 736 errors 
were due to irregular use of space (75.5%) and 239 
errors are non-space-related errors (24.5%). Of the 
space related errors, majority of errors (672 or 70% 
of total errors) are due to space omission and 53 
errors (5%) were due to space insertion. Thus irre-
gular use of space causes an extremely high per-
centage of all errors and has to be addressed for all 
language processing applications for Urdu.   
 
A study of Urdu words was also conducted as part 
of the current work. Text was used from popular 
Urdu online news sites (www.bbc.co.uk/urdu and 
http://search.jang.com.pk/). A data of 5,000 words 
from both corpora was observed and space inser-
tion and omission cases were counted. These 
counts are given in Table 4.  Counts for Space In-
sertion are sub-divided into the four categories 
identified earlier.   
 
Problem BBC Jang Total 
Space Omission 373 563 936 
Space Insertion  
 Affixation 298 467 765 
 Reduplication 52 76 128 
 Compounding 133 218 351 
 Abbreviation 263 199 462 
Total 1119 1523 2642 
 
Table 4: Space Omission and Insertion Counts 
from Online BBC and Jang Urdu News Websites 
 
The data shows that a significantly high percentage 
of errors related to space, with significant errors 
                                                          
3
 Errors based on tokenization on space and punctuation mark-
ers 
530
related to both omission and insertion.  Within in-
sertion errors, affixation, compounding and ab-
breviation related errors are more significant 
(because reduplication is a less frequent phenome-
non).  
 
In summary, the space related errors are significant 
and must be addressed as a precursor to any signif-
icant language and speech processing of the lan-
guage 
3.4 Ambiguity in Defining Urdu Word 
Another confounding factor in this context it that 
there is no clear agreement on word boundaries of 
Urdu in some cases.  
 
Compound words are hard to categorize as single 
or multiple words.  Urdu forms compounds in 
three ways: (i) by placing two words together, e.g. 
??? ??? (?parents?, literally ?father mother?), (ii) 
by putting a combining mark between them4, e.g. 
???? ???? (?prime minister?), and (iii) by putting 
the conjunction ? between two words, e.g.  ??? ?
??? (?Discipline?).  
 
Similarly certain cases of reduplication are also 
considered a single word by a native speaker, e.g. 
???? (?fluently?) and ????? (?equal?), while others 
are not, e.g. ????? ????? (?slowly?).  There are also 
cases which are ambiguous, as there is no agree-
ment even within native speakers.   
 
Moreover, certain function words, normally case 
markers, postpositions and auxiliaries, may be 
written joined with other words in context or sepa-
rately.  The words like ??? ??  may also be written 
in joined form ?????, and the different forms may 
be perceived as multiple or single words respec-
tively. 
 
This is demonstrated by the results of a study done 
with 30 native speakers of Urdu (including univer-
sity students, language researchers and language 
teachers).  The subjects were asked to mark wheth-
er they considered some text a single word or a 
sequence of two words.   Some relevant results are 
given in Table 5.  The table indicates that for the 
types of phenomena in Table 4, the native speakers 
                                                          
4
 The diacritics (called zer-e-izafat or hamza-e-izafat) are op-
tional, and are not written in the example given. 
do not always agree on the word boundary, that 
certain cases are very ambiguous, and that writing 
with or without space also changes the perception 
of where the word boundary should lie.   
 
Word(s) # of Words Category 
1 2 
6 24 ????????? Compounding with 
conjunctive diacritic 
13 17 ?????? ??????? -do- 
2 28 ????? ??? -do- 
2 28 ??????? -do- 
5 25 ??? ????? Compounding with 
conjunctive character ? 
1 29 ??? ???? -do- 
0 30 ????? ???? Suffixation 
8 22 ????? ???? -do- 
27 3 ??? ??? Reduplication 
27 3 ???? ???? -do- 
15 15 ???? Space omission between 
two auxiliaries 
12 18 ?????? Space omission between 
verb and auxiliary 
25 5 ???? ?? Same as above but 
without space omission 
 
Table 5: Survey on Word Definition 
 
As the word boundary is ambiguously perceived, it 
is not always clear when to mark it.  To develop a 
more consistent solution, the current work tags the 
different levels of boundaries, and it is left up to 
the application provider using the output to decide 
which tags to translate to word level boundaries. 
Free morphemes are placed and identified at first 
level.  At second level we identify strings that are 
clearly lexicalized as a single word.  Compounds, 
reduplication and abbreviations are dealt at third 
level. 
4 Summary of Existing Techniques 
Rule based techniques have been extensively used 
for word segmentation.  Techniques including 
longest matching (Poowarawan, 1986; Rarunrom, 
1991) try to match longest possible dictionary 
look-up. If a match is found at nth letter next look-
up is performed starting from n+1 index. Longest 
matching with word binding force is used for Chi-
nese word segmentation (Wong and Chang, 1997). 
However, the problem with this technique is that it 
consistently segments a letter sequence the same 
way, and does not take the context into account.  
531
Thus, shorter word sequences are never generated, 
even where they are intended. 
Maximum matching is another rule based tech-
nique that was proposed to solve the shortcomings 
of longest matching. It generates all possible seg-
mentations out of a given sequence of characters 
using dynamic programming. It then selects the 
best segmentation based on some heuristics.  Most 
popularly used heuristic selects the segmentation 
with minimum number of words. This heuristic 
fails when alternatives have same number of 
words. Some additional heuristics are then often 
applied, including longest match (Sornlertlamva-
nich, 1995). Many variants of maximum matching 
have been applied (Liang, 1986; Li et al, 1991; Gu 
and Mao, 1994; Nie et al, 1994). 
There is a third category of rule based techniques, 
which also use additional linguistic information for 
generating intermediate solutions which are then 
eventually mapped onto words.  For example, rule 
based techniques have also been applied to lan-
guages like Thai and Lao to determine syllables, 
before syllables are eventually mapped onto words, 
e.g. see (Phissamy et al, 2007).   
 
There has been an increasing application of statis-
tical methods, including n-grams, to solve word 
segmentation.  These techniques are based at let-
ters, syllables and words, and use contextual in-
formation to resolve segmentation ambiguities, e.g.  
(Aroonmanakul, 2002; Krawtrakul et al, 1997).   
The limitation of statistical methods is that they 
only use immediate context and long distance de-
pendencies cannot be directly handled. Also the 
performance is based on training corpus. Neverthe-
less, statistical methods are considered to be very 
effective to solve segmentation ambiguities.  
 
Finally, another class of segmentation techniques 
applies several types of features, e.g. Winnow and 
RIPPER algorithms (Meknavin et al, 1997; Blum 
1997). The idea is to learn several sources of fea-
tures that characterize the context in which each 
word tends to occur. Then these features are com-
bined to remove the segmentation ambiguities 
(Charoenpornsawat and Kijsirikul 1998). 
 
 
 
5 Segmentation Model for Urdu 
Although many other languages share the same 
problem of word boundary identification for lan-
guage processing, Urdu problem is unique due to 
its cursive script and its irregular use of space to 
create proper shaping.  Though other languages 
only have space omission challenge, Urdu has both 
omission and insertion problems further confound-
ing the issue.   
 
We employ a combination of techniques to inves-
tigate an effective algorithm to achieve Urdu seg-
mentation.  These techniques are incorporated 
based on knowledge of Urdu linguistic and writing 
system specific information for effective segmen-
tation.  For space omission problem a rule based 
maximum matching technique is used to generate 
all the possible segmentations. The resulting possi-
bilities are ranked using three different heuristics, 
namely min-word, unigram and bigram techniques.   
 
For space insertion, we first sub-classify the prob-
lem based on linguistic information, and then use 
different techniques for the different cases. Space 
insertion between affixes is done by extracting all 
possible affixes from Urdu corpus. Some affixes in 
Urdu are also free morphemes so it is important to 
identify in segmentation process whether or not 
they are part of preceding or following word. For 
example ??? is also a free morpheme (?nose?) and 
a suffix that makes adjective from noun, e.g. in 
word ??? ??? (?dangerous?).   This is done based 
on the part of speech information of the words in 
the context. 
 
Reduplication is handled using edit distance algo-
rithm. In Urdu the reduplicated morpheme is either 
the same or a single edit-distance from the base 
morpheme, e.g. ???? has same string repeated, ????? 
has one insertion, and ???? ???? has one substitu-
tion.  Thus, if a string is less than two edits from its 
neighbor it is an instance of reduplication5. As the 
examples suggest, the reduplication may not only 
be limited to word initial position and may also 
occur word medially.  However, if the length of 
base word is less than four, it is further to avoid 
function words (case markers, postpositions, aux-
                                                          
5
 Insertion, deletion and substitution are all considered contri-
buting a single edit distance here.  
532
iliaries, etc.) from being mistakenly identified as a 
case of reduplication, e.g. ??? ??? (?was done?) has 
two words with a single edit distance but is not a 
reduplicated sequence.  
 
Urdu does not abbreviate strings, but abbreviations 
from English are frequently transliterated into Ur-
du. This sequence can be effectively recognized by 
developing a simple finite automaton. The automa-
ton treats marks all such co-occurring morphemes 
because they are likely to be an English abbrevia-
tion transliterated into Urdu, e.g. ?? ??? ?? 
(?PhD?). If such morphemes are preceding proper 
names then these are not combined as they are 
more likely to be the initials of an abbreviated 
name, e.g. ?? ???? ???  (?N. D. Shakir?).  This ap-
proach confuses the morpheme ?? (genitive case 
marker) of Urdu with the transliteration of English 
letter ?k?.  If we write ?? ?? ??? ?? ???  (?after 
PhD?), it is interpreted as ?P H D K after?.  This 
has to be explicitly handled. 
 
As classification of compounds into one or two 
word sequences is unclear, unambiguous cases are 
explicitly handled via lexical look-up.  An initial 
lexicon of 1850 compound words has been devel-
oped for the system based on a corpus of Urdu. 
Common foreign words are also included in this 
list.   
5.1 Algorithm 
The segmentation process starts with pre-
processing, which involves removing diacritics (as 
they are optionally used in Urdu and not consi-
dered in the current algorithm because they are 
frequently incorrectly marked by users6) and nor-
malizing the input text to remove encoding ambi-
guities7.  Input is then tokenized based on space 
and punctuation characters in the input stream. As 
has been discussed, space does not necessarily in-
dicate word boundary.  However presence of space 
does imply word or morpheme boundary in many 
                                                          
6
 The word  ????? is written with the super-script Alef placed 
on Lam and Yay characters.  The latter variation is correct but 
the former incorrect variation is also common in the corpus.   
7
 Unicode provides multiple codes for a few letters, and both 
composed and decomposed forms for others.  These have to be 
mapped onto same underlying encoding sequence for further 
processing.  See 
http://www.crulp.org/software/langproc/urdunormalization.ht
m for details.   
cases, which can still be useful. The tokenization 
process gives what we call an Orthographic Word 
(OW).  OW is used instead of ?word? because one 
OW may eventually give multiple words and mul-
tiple OWs may combine to give a single word.  
Keeping space related information also keeps the 
extent of problem to be solved within a reasonable 
computational complexity.  For example input 
string ???? ??? ????? (the name of the first author) 
with spaces giving three OWs, creates 2 x 1 x 7 = 
14 possible segmentations when sent separately to 
the maximum matching module (space omission 
error removal - see Figure 2). However, if we re-
move the spaces from the input and send input as a 
single OW ???????????? to maximum matching 
process, we get 77 possible segmentations. This 
number grows exponentially with the length of 
input sentence. Throwing away space character 
means we are losing important information so we 
keep that intact to our use. 
 
After pre-processing a series of modules further 
process the input string and convert the OWs into a 
sequence of words.  This is summarized in Figure 
2 and explained below. 
 
Each OW is sent to a module which deals with 
space omission errors. This module extracts all 
possible morpheme segmentations out of an OW. 
Ten best segmentations of these are selected based 
on minimum-word heuristic.  This heuristic prefers 
segmentations with minimum number of mor-
phemes. Such a heuristic is important to prevent 
the search space to explode. We observed that us-
ing 10-best segmentations proved to be sufficient 
in most cases as OW normally encapsulates two or 
three Urdu words but as a heuristic we also added a 
feature which increases this number of 10-best 
segmentations to 15, 20, 25-best and so on depend-
ing upon number of characters in an OW. Ten best 
segmentations for each OW are merged with the 
extracted segmentations of other OWs. Up till here 
we have successfully resolved all space omission 
errors and the input sentence has been segmented 
into morphemes. The 10n (where ?n? is No. of 
OWs) segmentations are then passed on to space 
insertion error removal module. This module has 
several sub-modules that handle different linguistic 
phenomena like reduplication, affixation, abbrevia-
tions and compounding. 
 
533
The reduplication identification module employs 
single edit distance algorithm to see if adjacent 
morphemes are at single edit-distance of each oth-
er. If the edit distance is less than two, then the 
reduplication is identified and marked. 
 
 
Diacritic Removal / Tokenization 
 
Space Omission Error Removal 
 
Check for Reduplication within an OW 
 
Lexical Look-ups for Spelling Variations 
 
Maximum Matching Module 
 
Ranking-based on Min-Word Heuristic 
 
 
Space Insertion Error Removal 
 
Reduplication Handling 
 
English Abbreviation Handling 
 
Affixation Handling 
 
Compound Word Tagging 
 
 
N-Gram Based Ranking 
 
Figure 2: Urdu Word Segmentation Process 
 
 For example the module will correctly recognize 
consecutively occurring OWs ????? and ????? as a 
case of reduplication.  Reduplication is also ap-
plied earlier in space omission error module as 
there may also be a case of reduplication within a 
single OW. This module handles such cases, by 
dividing words in halves and identifying possible 
reduplications.  Thus, if the words are written 
without space, e.g. ?????????? (innocent) they are 
still identified and tagged as reduplicated words 
????? and ?????. 
 
This list of words is then fed into an automaton 
which recognizes the sequence of abbreviations 
generated by transliterating English letters.   
 
A complete affix list is compiled, and in the next 
stage the short listed word sequences are processed 
through a process which looks through this list to 
determine if any of the OWs may be combined.  
Part of speech information of stem is also used to 
confirm if OWs can be merged. 
Urdu compounds are finally identified.  This is 
done by using a compound list generated through 
the corpus.  As compounding is arbitrary, where 
speakers are not certain in many cases that a se-
quence of morphemes form a single compound or 
not, the segmentation process leaves this level to 
the discretion of the user.  Whichever compounds 
are listed in a compound lexicon are treated as a 
single compound word.  Those not listed are not 
tagged as compounds.  User may enhance this list 
arbitrarily.  The lexicon is initialized with a list of 
non-controversial compound, as verified from pub-
lished dictionaries.   
 
Eventually, all the segmentations are re-ranked. 
We used three different re-ranking methods name-
ly minimum-word heuristic, unigram and bi-gram 
based sequence probabilities, comparative analysis. 
 
Based on the segmentation process, the output se-
quence contains the following tagging.  As dis-
cussed earlier, the word segmentation may be 
defined based on this tagging by the individual 
application using this process. 
 
Phenomenon Tags Examples 
Word <W></W> <W>?????</W> 
Root <R></R> <W><R>?????</R> 
<S>???</S></W> 
Suffix <S></S> <W><R>????</R> 
<S>?????</S></W> 
Prefix <P></P> <W><P>??</P> 
<R>??????</R></W> 
XY Com-
pounds 
<C1></C1> <C1><W>?????</W> 
<W>????</W></C1> 
X-e-Y Com-
pounds 
<C2></C2> <C2><W>????</W> 
<W> ???? </W></C2> 
X-o-Y Com-
pounds 
<C3></C3> <C3><W>???</W> 
<W>?</W> 
<W>????</W></C3> 
Reduplication <Rd></Rd> <Rd><W>????</W> 
<W>????</W></Rd> 
Abbreviations <A></A> <A><W>??</W> 
<W>??</W> </A> 
Figure 3: Urdu Word Segmentation Tag Set 
 
A regular word is tagged using <w> ?</w> pair.  
Roots, suffixes and prefixes are also tagged within 
a word. Reduplication, compounding and abbrevia-
tions are all considered to be multi-word tags and 
relevant words are grouped within these tags. 
Three different kind of compounding is separately 
tagged. 
534
6 Results 
The algorithm was tested on a very small, manual-
ly segmented corpus of 2367 words. The corpus 
we selected contained 404 segmentation errors 
with 221 cases of space omissions and 183 cases of 
space insertions. In space insertion category there 
were 66 cases of affixation, 63 cases of compound-
ing, 32 cases of reduplication and 22 cases of ab-
breviations. The results for all three techniques are 
shown below: 
 
 Categories Errors %ages 
 Affixation 59/66 89.39 
 Reduplication 27/32 84.37 
Abbreviations 19/22 86.36 
Compounds 28/63 44.44 
Total 133/183 72.67 
 
Table 6: Percentages of Number of Errors Detected 
in Different Categories of Space Insertion Error 
There were 221 cases of space omission errors 
where multiple words were written in a continuum. 
Given below is a table that shows how many of 
these were correctly identified by each of the used 
techniques. Clearly, statistical techniques outper-
form a simple minimum number of words heuris-
tic. Bigrams are likely to produce better results if 
the training corpus is improved. Our training cor-
pus contained manually segmented 70K words. 
The bigram probabilities are obtained using 
SRILM-Toolkit (Stolcke, 2002). 
 Categories Errors %ages 
Maximum Matching 186/221 84.16 
Unigram 214/221 96.83 
Bigram 209/221 94.5 
 
Table 7: %age of No. of Errors Detected in Space 
Omission with Different Ranking Techniques 
Following table gives cumulative results for cor-
rectly identified space omission and insertion er-
rors.  
 
Categories Errors %ages 
Maximum Matching 323/404 79.95 
Unigram 347/404 85.8 
Bigram 339/404 83.9 
 
Table 8: %age of No. of Errors Detected Cumula-
tively 
 
Final table counts total number of words (redupli-
cation, compounds and abbreviations cases are in-
clusive) in test corpus and total number of 
correctly identified words after running the entire 
segmentation process. 
 
 
Categories Detected %ages 
Maximum Matching 2209/2367 93.3 
Unigram 2269/2367 95.8 
Bigram 2266/2367 95.7 
 
Table 9: Percentage of Correctly Detected Words 
7 Future Work  
This work presents a preliminary effort on word 
segmentation problem in Urdu. It is a multi-
dimensional problem. Each dimension requires a 
deeper study and analysis. Each sub-problem has 
been touched in this work and a basic solution for 
all has been devised. However to improve on re-
sults each of these modules require a separate 
analysis and study. Statistics is only used in rank-
ing of segmentations. In future work bi-gram sta-
tistics can be used to merge morphemes. More data 
can be tagged to find out joining probabilities for 
the affixes that occur as free morpheme. Such 
analysis will reveal whether an affix is more in-
clined towards joining or occurs freely more fre-
quently. Similarly a corpus can be tagged on 
compounds. For each morpheme its probability to 
occur in compound can be calculated. If two or 
more morphemes with higher compounding proba-
bilities co-occur they can be joined together. Simi-
larly corpus can be tagged for abbreviations.  
 
Ranking of segmentations and affix merging can 
be improved if POS tags are also involved with 
bigram probabilities. Use of POS tags with n-gram 
technique is proven to be very helpful in solving 
unknown word problems. Our model does not ex-
plicitly handle unknown words. Currently the max-
imum matching module splits an unknown OW 
into smaller Urdu morphemes. For example 
?????????? (Kolesnikov) is erroneously split into 
?????????????. More serious problems occur in 
cases when OW is a mixture of known and un-
known words. For example in case ????????????? 
(?Fraser must go?). All these are to be addressed in 
future work. 
 
535
References 
Andreas, S. 2002. SRILM - an extensible language 
modeling toolkit. In Intl. Conf. Spoken Language 
Processing, Denver, Colorado.  
 
Aroonmanakul, W. 2002. Collocation and Thai 
Word Segmentation. In proceeding of SNLPOrien-
tal COCOSDA. 
 
Blum, A. 1997. Empirical Support for Winnow and 
Weighted-Majority Algorithm: Results on a Ca-
lendar Scheduling Domain, Machine Learning, 
26:5-23. 
 
Charoenpornsawat, P., Kijsirikul, B. 1998. Fea-
ture-Based Thai Unknown Word Boundary Identi-
fication Using Winnow. In Proceedings of the 
1998 IEEE Asia-Pacific Conference on Circuits 
and Systems (APCCAS?98). 
 
Gu, P. and Mao, Y. 1994. The adjacent matching 
algorithm of Chinese automatic word segmentation 
and its implementation in the QHFY Chinese-
English system. In International Conference on 
Chinese Computing, Singapore. 
 
Hussain, S. 2003. www. LICT4D . asia / Fonts / 
Nafees_Nastalique.  In the Proceedings of 12th 
AMIC Annual Conference on E-Worlds: Govern-
ments, Business and Civil Society, Asian Media 
Information Center, Singapore.  Also available at 
http://www.crulp.org/Publication/papers/2003/ww
w.LICT4D.asia.pdf.   
 
Hussain, S. 2004. Letter to Sound Rules for Urdu 
Text to Speech System.  In the Proceedings of 
Workshop on Computational Approaches to Arabic 
Script-based Languages, COLING 2004, Geneva, 
Switzerland, 2004. 
 
 
Krawtrakul, A., Thumkanon. C., Poovorawan. Y. 
and Suktarachan. M. 1997. Automatic Thai Un-
known Word Recognition. In Proceedings of the 
natural language Processing Pacific Rim Sympo-
sium. 
Li, B.Y., S. Lin, C.F. Sun, and M.S. Sun. 1991. A 
maximum-matching word segmentation algorithm 
using corpus tags for disambiguation. In 
ROCLING IV, pages: 135-146, Taipei. ROCLING 
 
Liang, N. 1986. A written Chinese automatic seg-
mentation system-CDWS. In Journal of Chinese 
Information Processing, 1(1):44-52. 
 
Meknavin. S., Charenpornsawat. P. and Kijsirikul. 
B. 1997. Feature-based Thai Words Segmentation. 
NLPRS, Incorporating SNLP. 
 
Naseem, T., Hussain, S. 2007.  Spelling Error 
Trends in Urdu.  In the Proceedings of Conference 
on Language Technology (CLT07), University of 
Peshawar, Pakistan. 
 
Nie, J., Jin W., and Hannan, M. 1994. A hybrid 
approach to unknown word detection and segmen-
tation of Chinese. In International Conference on 
Chinese Computing, Singapore. 
 
Phissamay, P., Dalolay,V., Chanhsililath, C., Sili-
masak, O. Hussain, S., and Durrani, N.  2007.  Syl-
labification of Lao Script for Line Breaking.  In 
PAN Localization Working Papers 2004-2007.  . 
 
Poowarawan, Y., 1986. Dictionary-based Thai Syl-
lable Separation. In Proceedings of the Ninth Elec-
tronics Engineering Conference 
 
Rarunrom, S., 1991. Dictionary-based Thai Word 
Separation. Senior Project Report. 
 
Sornlertlamvanich, V. 1995. Word Segmentation 
for Thai in a Machine Translation System (in 
Thai), Papers on Natural Language Processing, 
NECTEC, Thailand 
 
Wong, P., Chan, C. 1996. Chinese Word Segmen-
tation based on Maximum Matching and Word 
Binding Force. In Proceedings of COLING 96, pp. 
200-203. 
 
536
Proceedings of NAACL-HLT 2013, pages 1?11,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Model With Minimal Translation Units, But Decode With Phrases
Nadir Durrani?
University of Edinburgh
dnadir@inf.ed.ac.uk
Alexander Fraser Helmut Schmid
University of Stuttgart
fraser,schmid@ims.uni-stuttgart.de
Abstract
N-gram-based models co-exist with their
phrase-based counterparts as an alternative
SMT framework. Both techniques have pros
and cons. While the N-gram-based frame-
work provides a better model that captures
both source and target contexts and avoids
spurious phrasal segmentation, the ability to
memorize and produce larger translation units
gives an edge to the phrase-based systems dur-
ing decoding, in terms of better search per-
formance and superior selection of transla-
tion units. In this paper we combine N-gram-
based modeling with phrase-based decoding,
and obtain the benefits of both approaches.
Our experiments show that using this combi-
nation not only improves the search accuracy
of the N-gram model but that it also improves
the BLEU scores. Our system outperforms
state-of-the-art phrase-based systems (Moses
and Phrasal) and N-gram-based systems by
a significant margin on German, French and
Spanish to English translation tasks.
1 Introduction
Statistical Machine Translation advanced from
word-based models (Brown et al, 1993) towards
more sophisticated models that take contextual in-
formation into account. Phrase-based (Och and
Ney, 2004; Koehn et al, 2003) and N-gram-based
(Marin?o et al, 2006) models are two instances of
such frameworks. While the two models have some
common properties, they are substantially different.
?Much of the work presented here was carried out while the
first author was at the University of Stuttgart.
Phrase-based systems employ a simple and effec-
tive machinery by learning larger chunks of trans-
lation called phrases1. Memorizing larger units en-
ables the phrase-based model to learn local depen-
dencies such as short reorderings, idioms, insertions
and deletions, etc. The model however, has the fol-
lowing drawbacks: i) it makes independence as-
sumptions over phrases ignoring the contextual in-
formation outside of phrases ii) it has issues han-
dling long-distance reordering iii) it has the spurious
phrasal segmentation problem which allows multi-
ple derivations of a bilingual sentence pair having
different model scores for each segmentation.
Modeling with minimal translation units helps ad-
dress some of these issues. The N-gram-based SMT
framework is based on tuples. Tuples are mini-
mal translation units composed of source and target
cepts2. N-gram-based models are Markov models
over sequences of tuples (Marin?o et al, 2006; Crego
and Marin?o, 2006) or operations encapsulating tu-
ples (Durrani et al, 2011). This mechanism has sev-
eral useful properties. Firstly, no phrasal indepen-
dence assumption is made. The model has access
to both source and target context outside of phrases.
Secondly the model learns a unique derivation of a
bilingual sentence given its alignment, thus avoiding
the spurious segmentation problem.
Using minimal translation units, however, results
in a higher number of search errors because of i)
1A phrase-pair in PBSMT is a sequence of source and target
words that is translation of each other, and is not necessarily a
linguistic constituent. Phrases are built by combining minimal
translation units and ordering information.
2A cept is a group of words in one language that is translated
as a minimal unit in one specific context (Brown et al, 1993).
1
poor translation selection, ii) inaccurate future-cost
estimates and iii) incorrect early pruning of hypothe-
ses that would produce better model scores if al-
lowed to continue. In order to deal with these
problems, search is carried out only on a graph
of pre-calculated orderings, and ad-hoc reordering
limits are imposed to constrain the search space
(Crego et al, 2005; Crego and Marin?o, 2006), or
a higher beam size is used in decoding (Durrani
et al, 2011). The ability to memorize and pro-
duce larger translation chunks during decoding, on
the other hand, gives a distinct advantage to the
phrase-based system during search. Phrase-based
systems i) have access to uncommon translations,
ii) do not require higher beam sizes, iii) have more
accurate future-cost estimates because of the avail-
ability of phrase-internal language model context
before search is started. To illustrate this consider
the German-English phrase-pair ?scho? ein Tor ?
scored a goal?, composed from the tuples (cept-
pairs) ?scho? ? scored?, ?ein ? a? and ?Tor ? goal?.
It is likely that the N-gram system does not have
the tuple ?scho? ? scored? in its n-best translation
options because ?scored? is an uncommon transla-
tion for ?scho?? outside the sports domain. Even if
?scho? ? scored? is hypothesized, it will be ranked
quite low in the stack until ?ein? and ?Tor? are gen-
erated in the next steps. A higher beam is required
to prevent it from getting pruned. Phrase-based sys-
tems, on the other hand, are likely to have access to
the phrasal unit ?scho? ein Tor ? scored a goal? and
can generate it in a single step. Moreover, a more ac-
curate future-cost estimate can be computed because
of the available context internal to the phrase.
In this work, we extend the N-gram model, based
on operation sequences (Durrani et al, 2011), to
use phrases during decoding. The main idea is to
study whether a combination of modeling with min-
imal translation units and using phrasal information
during decoding helps to solve the above-mentioned
problems.
The remainder of this paper is organized as fol-
lows. The next two sections review phrase-based
and N-gram-based SMT. Section 2 provides a com-
parison of phrase-based and N-gram-based SMT.
Section 3 summarizes the operation sequence model
(OSM), the main baseline for this work. Section
4 analyzes the search problem when decoding with
Figure 1: Different Segmentations of a Bilingual Sen-
tence Pair
minimal units. Section 5 discusses how information
available in phrases can be used to improve search
performance. Section 6 presents the results of this
work. We conducted experiments on the German-to-
English and French-to-English translation tasks and
found that using phrases in decoding improves both
search accuracy and BLEU scores. Finally we com-
pare our system with two state-of-the-art phrase-
based systems (Moses and Phrasal) and two state-
of-the-art N-gram-based systems (Ncode and OSM)
on standard translation tasks.
2 Previous Work
Phrase-based and N-gram-based SMT are alter-
native frameworks for string-to-string translation.
Phrase-based SMT segments a bilingual sentence
pair into phrases that are continuous sequences of
words (Och and Ney, 2004; Koehn et al, 2003)
or discontinuous sequences of words (Galley and
Manning, 2010). These phrases are then reordered
through a lexicalized reordering model that takes
into account the orientation of a phrase with respect
to its previous phrase (Tillmann and Zhang, 2005)
or block of phrases (Galley and Manning, 2008).
There are several drawbacks of the phrase-based
model. Firstly it makes an independence assump-
tion over phrases, according to which phrases are
translated independently of each other, thus ignor-
ing the contextual information outside of the phrasal
boundary. This problem is corrected by the monolin-
gual language model that takes context into account.
But often the language model cannot compensate for
the dispreference of the translation model for non-
local dependencies. The second problem is that the
model is unaware of the actual phrasal segmentation
of a sentence during training. It therefore learns all
possible ways of segmenting a bilingual sentence.
Different segmentations of a bilingual sentence re-
2
sult in different probability scores for the translation
and reordering models, causing spurious ambiguity
in the model. See Figure 1. In the first segmentation,
the model learns the lexical and reordering proba-
bilities of the phrases ?sie wu?rden ? they would?
and ?gegen ihre kampagne abstimmen ? vote against
your campaign?. In the second segmentation, the
model learns the lexical and reordering probabilities
of the phrases ?sie ? they? ?wu?rden ? would?, ?ab-
stimmen ? vote?, ?gegen ihre kampagne ? against
your campaign?. Both segmentations result in dif-
ferent translation and reordering scores. This kind
of ambiguity in the model subsequently results in
the presence of many different equivalent segmen-
tations in the search space. Also note that the two
segmentations contain different information. From
the first segmentation the model learns the depen-
dency between the verb ?abstimmen ? vote? and the
phrase ?gegen ihre kampagne ? against your cam-
paign?. The second segmentation allows the model
to capture the reordering of the complex verb pred-
icate ?wu?rden ? would? and ?abstimmen ? vote? by
learning that the verb ?abstimmen ? vote? is discon-
tinuous with respect to the auxiliary. This informa-
tion cannot be captured in the first segmentation be-
cause of the phrasal independence assumption and
stiff phrasal boundaries. The model loses one of the
dependencies depending upon which segmentation
it chooses during decoding.
N-gram-based SMT is an instance of a joint
model that generates source and target strings to-
gether in bilingual translation units called tuples.
Tuples are essentially phrases but they are atomic
units that cannot be decomposed any further. This
condition of atomicity results in a unique segmen-
tation of the bilingual sentence pair given its align-
ments. The model does not make any phrasal inde-
pendence assumption and generates a tuple by look-
ing at a context of n ? 1 previous tuples (or opera-
tions). This allows the N-gram model to model all
the dependencies through a single derivation.
The main drawback of N-gram-based SMT is its
poor search mechanism which is inherent from us-
ing minimal translation units during search. Decod-
ing with tuples has problems with a high number
of search errors caused by lower translation cover-
age, inaccurate future-cost estimation and pruning
of correct hypotheses (see Section 4.2 for details).
Crego and Marin?o (2006) proposed a way to couple
reordering and search through POS-based rewrite
rules. These rules are learned during training when
units with crossing alignments are unfolded through
source linearization to form minimal tuples. For ex-
ample, in Figure 1, the N-gram-based MT will lin-
earize the word sequence ?gegen ihre kampagne ab-
stimmen? to ?abstimmen gegen ihre kampagne?, so
that it is in the same order as the English words.
It also learns a POS-rule ?IN PRP NN VB ? VB
IN PRP NN?. The POS-based rewrite rules serve
to precompute the orderings that are hypothesized
during decoding. Coupling reordering and search
allows the N-gram model to arrange hypotheses in
2m stacks (for an m word source sentence), each
containing hypotheses that cover exactly the same
foreign words. This removes the need for future-
cost estimation3. Secondly, memorizing POS-based
rules enables phrase-based like reordering, however
without lexical selection. There are three drawbacks
of this approach. Firstly, lexical generation and re-
ordering are decoupled. Search is only performed on
a small number of reorderings, pre-calculated using
the source side and completely ignoring the target-
side. And lastly, the POS-based rules face data spar-
sity problems especially in the case of long distance
reorderings.
Durrani et al (2011) recently addressed these
problems by proposing an operation sequence N-
gram model which strongly couples translation and
reordering, hypothesizes all possible reorderings
and does not require POS-based rules. Represent-
ing bilingual sentences as a sequence of operations
enables them to memorize phrases and lexical re-
ordering triggers like PBSMT. However, using min-
imal units during decoding and searching over all
possible reorderings means that hypotheses can no
longer be arranged in 2m stacks. The problem of
inaccurate future-cost estimates resurfaces resulting
in more search errors. A higher beam size of 500 is
therefore used to produce translation units in com-
parison to phrase-based systems. This, however,
still does not eliminate all search errors. This pa-
per shows that using phrases instead of cepts in de-
3Using m stacks with future-cost estimation is a more effi-
cient solution but is not used ?due to the complexity of accu-
rately computing these estimations in the N-gram architecture?
(Crego et al, 2011).
3
coding improves the search accuracy and translation
quality. It also shows that using some phrasal in-
formation in cept-based decoding captures some of
these improvements.
3 Operation Sequence Model
The N-gram model with integrated reordering mod-
els a sequence of operations obtained through the
transformation of a bilingual sentence pair. An op-
eration can either be to i) generate a sequence of
source and target words, ii) to insert a gap as a place-
holder for skipped words, iii) or to jump forward and
backward in a sentence to translate words discon-
tinuously. The translate operation Generate(X,Y)
encapsulates the translation tuple (X,Y). It gener-
ates source and target translations simultaneously4.
This is similar to N-gram-based SMT except that
the tuples in the N-gram-based model are generated
monotonically, whereas in this case lexical genera-
tion and reordering information is strongly coupled
in an operation sequence.
Consider the phrase pair:
The model memorizes it
through the sequence:
Generate(Wie, What is)? Gap? Generate (Sie,
your)? Jump Back (1)?Generate (heissen, name)
Let O = o1, . . . , oj?1 be a sequence of opera-
tions as hypothesized by the translator to generate
the bilingual sentence pair ?F,E? with an alignment
function A. The translation model is defined as:
p(F,E,A) = p(oJ1 ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
where n indicates the amount of context used. The
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stol-
cke, 2002) with Kneser-Ney smoothing. A 9-gram
model is used. Several count-based features such as
gap and open gap penalties and distance-based fea-
tures such as gap-width and reordering distance are
added to the model, along with the lexical weighting
and length penalty features in a standard log-linear
framework (Durrani et al, 2011).
4The generation is carried out in the order of the target lan-
guage E.
4 Search
4.1 Overview of Decoding Framework
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004a).
The decoder uses beam search to build up the trans-
lation from left to right. The hypotheses are ar-
ranged in m stacks such that stack i maintains hy-
potheses that have already translated i many foreign
words. The ultimate goal is to find the best scor-
ing hypothesis, that has translated all the words in
the foreign sentence. The overall process can be
roughly divided into the following steps: i) extrac-
tion of translation units ii) future-cost estimation, iii)
hypothesis extension iv) recombination and pruning.
During the hypothesis extension each extracted
phrase is translated into a sequence of operations.
The reordering operations (gaps and jumps) are gen-
erated by looking at the position of the translator,
the last foreign word generated etc. (Refer to Algo-
rithm 1 in Durrani et al (2011)). The probability of
an operation depends on the n ? 1 previous opera-
tions. The model backs-off to the smaller n-grams
of operations if the full history is unknown. We use
Kneser-Ney smoothing to handle back-off5.
4.2 Drawbacks of Cept-based Decoding
One of the main drawbacks of the operation se-
quence model is that it has a more difficult search
problem than the phrase-based model. The opera-
tion model, although based on minimal translation
units, can learn larger translation chunks by mem-
orizing a sequence of operations. However, using
cepts during decoding has the following drawbacks:
i) the cept-based decoder does not have access to
all the translation units that a phrase-based decoder
uses as part of a larger phrase. ii) it requires a higher
beam size to prevent early pruning of better hypothe-
ses that lead toward higher model scores when al-
lowed to continue and iii) it uses worse future-cost
estimates than the phrase-based decoder.
Recall the example from the last section. For
the cept-based decoder to generate the same phrasal
translation, it requires three separate tuple transla-
tions ?Wie ? what is?, ?Sie ? your? and ?hei?en ?
name?. Here we are faced with three challenges.
5We also tried Witten-Bell and Good Turing methods of dis-
counting and found Kneser-Ney smoothing to produce the best
results.
4
Translation Coverage: The first problem is that
the N-gram model does not have the same cov-
erage of translation options. The English cepts
?what is?, ?your? and ?name? are not good candi-
date translations for the German cepts ?Wie?, ?Sie?
and ?hei?en?, respectively. When extracting tuple
translations for these cepts from the Europarl data
for our system, the tuple ?Wie ? what is? is ranked
124th, ?hei?en ? name? is ranked 56th, and ?Sie ?
your? is ranked 9th in the list of n-best translation
candidates. Typically only the 20 best translation
options are used, to reduce the decoding time, and
such phrasal units with less frequent cept transla-
tions are never hypothesized in the N-gram-based
systems. The phrase-based system on the other hand
can extract the phrase ?Wie hei?en Sie ? what is
your name? even if it is observed only once dur-
ing training. A similar problem is also reported in
Costa-jussa` et al (2007). When trying to repro-
duce the sentences in the n-best translation output
of the phrase-based system, the N-gram-based sys-
tem was only able to produce 37.5% of the sen-
tences in the Spanish-to-English and 37.2% in the
English-to-Spanish translation tasks. In compar-
ison the phrase-based system was able to repro-
duce 57.5% and 48.6% of the sentences in the n-
best translation output of the Spanish-to-English and
English-to-Spanish N-gram-based systems.
Larger Beam Size: A related problem is that a
higher beam size is required in cept-based decod-
ing to prevent uncommon translations from getting
pruned. The phrase-based system can generate the
phrase-pair ?Wie hei?en Sie ? what is your name?
in a single step placing it directly into the stack three
words to the right. The cept-based decoder generates
this phrase in three stacks with the tuple translations
?Wie ? What is?, ?Sie ? your? and ?hei?en ? name?.
A very large stack size is required during decoding
to prevent the pruning of ?Wie ? What is? which is
ranked quite low in the stack until the tuple ?Sie ?
your? is hypothesized in the next stack. Costa-jussa`
et al (2007) reports a significant drop in the perfor-
mance of N-gram-based SMT when a beam size of
10 is used instead of 50 in their experiments. For the
(cept-based) operation sequence model, Durrani et
al. (2011) required a stack size of 500. In compari-
son, the translation quality achieved by phrase-based
SMT remains the same when varying the beam size
between 5 and 50.
Future-Cost Estimation: A third problem is
caused by inaccurate future-cost estimation. Using
phrases helps phrase-based SMT to better estimate
the future language model cost because of the larger
context available, and allows the decoder to capture
local (phrase-internal) reorderings in the future cost.
In comparison the future cost for tuples is mostly un-
igram probabilities. The future-cost estimate for the
phrase pair ?Wie hei?en Sie ? What is your name?
is estimated by calculating the cost of each feature.
The language model cost, for example, is estimated
in the phrase-based system as follows:
plm = p(What)? p(is|What)? p(your|What is)
? p(name|What is your)
The cost of the direct phrase translation probabil-
ity, one of the features used in the phrase translation
model, is estimated as:
ptm = p(What is your name|Wie hei?en Sie)
Phrase-based SMT is aware during the prepro-
cessing step that the words ?Wie hei?en Sie? may
be translated as a phrase. This is helpful for estimat-
ing a more accurate future cost because the phrase-
internal context is already available. The same is not
true for the operation sequence model, to which only
minimal units are available. The operation model
does not have the information that ?Wie hei?en Sie?
may be translated as a phrase during decoding. The
future-cost estimate available to the operation model
for the span covering ?Wie hei?en Sie? will have un-
igram probabilities for both the translation and lan-
guage model:
plm = p(What)? p(is|What)? p(your)? p(name)
ptm = p(Generate(Wie, What is))? p(Generate
(hei?en,name))? p(Generate(Sie, your))
Thus the future-cost estimate in the operation
model is much worse than that of the phrase-based
model. The poor future-cost estimation leads to
search errors, causing a drop in the translation qual-
ity. A more accurate future-cost estimate for the
translation model cost would be:
5
ptm = p(Generate(Wie,What is))? p(Insert Gap|C)
? p(Generate(Sie,your)|C)? p(Jump Back(1)|C)
p(Generate(hei?en,name)|C)
where C is the context, i.e., the n?1 previously gen-
erated operations. The future-cost estimates com-
puted in this manner are much more accurate be-
cause they not only consider context, but also take
the reordering operations into account.
5 N-gram Model with Phrase-based
Decoding
In the last section we discussed the disadvantages of
using cepts during search in a left-to-right decoding
framework. We now define a method to empirically
study the mentioned drawbacks and whether using
information available in phrase-pairs during decod-
ing can help improve search accuracy and translation
quality.
5.1 Training
We extended the training steps in Durrani et al
(2011) to extract a phrase lexicon from the paral-
lel data. We extract all phrase pairs of length 6 and
below, that are consistent (Och et al, 1999) with
the word alignments. Only continuous phrases as
used in a traditional phrase-based system are ex-
tracted thus allowing only inside-out (Wu, 1997)
type of alignments. The future cost of each fea-
ture component used in the log-linear model is cal-
culated. The operation sequence required to hypoth-
esize each phrase is generated and its future cost is
calculated. The future costs of other features such
as language models, lexicalized probability features,
etc. are also estimated. The estimates of the count-
based reordering penalties (gap penalty and open
gap penalty) and the distance-based features (gap-
width and reordering distance) could not be esti-
mated previously with cepts but are available when
using phrases.
5.2 Decoding
We extended the decoder developed by Durrani et al
(2011) and tried three ideas. In our primary experi-
ments we enabled the decoder to use phrases instead
of cepts. This allows the decoder to i) use phrase-
internal context when computing the future-cost es-
timates, ii) hypothesize translation options not avail-
able to the cept-based decoder iii) cover multiple
source words in a single step subsequently improv-
ing translation coverage and search. Note that us-
ing phrases instead of cepts during decoding, does
not reintroduce the spurious phrasal segmentation
problem as is present in the phrase-based system,
because the model is built on minimal units which
avoids segmentation ambiguity. Different compo-
sitions of the same phrasal unit lead to exactly the
same model score. We therefore do not create any
alternative compositions of the same phrasal unit
during decoding. This option is not available in
phrase-based decoding, because an alternative com-
position may lead towards a better model score.
In our secondary set of experiments, we used
cept-based decoding but modified the decoder to
use information available from the phrases extracted
for the test sentences. Firstly, we used future-cost
estimates from the extracted phrases (see system
cept.500.fc in Table1). This however, leads to in-
consistency in the cases where the future cost is es-
timated from some phrasal unit that cannot be gen-
erated through the available cept translations. For
example, say the best cost to cover the sequence
?Wie hei?en Sie? is given by the phrase ?What is
your name?. The 20-best translation options in cept-
based system, however, do not have tuples ?Wie ?
What? and ?hei?en ? name?. To remove this dis-
crepancy, we add all such tuples that are used in
the extracted phrases, to the list of extracted cepts
(system cept.500.fc.t). We also studied how much
gain we obtain by only adding tuples from phrases
and using cept-based future-cost estimates (system
cept.500.t).
5.3 Evaluation Method
To evaluate our modifications we apply a simple
strategy. We hold the model constant and change
the search to use the baseline decoder, which uses
minimal translation units, or the modified decoders
that use phrasal information during decoding. The
model parameters are optimized by running MERT
(minimum error rate training) for the baseline de-
coder on the dev set. After we have the optimized
weights, we run the baseline decoder and our mod-
ifications on the test. Note that because all the de-
coding runs use the same feature vector, the model
6
stays constant, only search changes. This allows us
to compare different decoding runs, obtained using
the same parameters, but different search strategies,
in terms of model scores. We compute a search ac-
curacy and translation quality for each run.
Search accuracy is computed by comparing trans-
lation hypotheses from the different decoding runs.
We form a collection of the best scoring hypotheses
by traversing through all the runs and selecting the
sentences with highest model score. For each input
sentence we select a single best scoring hypothesis.
The best scoring hypothesis can be contributed from
several runs. In this case all these runs will be given
a credit for that particular sentence when computing
the search accuracy. The search accuracy of a decod-
ing run is defined as the percentage of hypotheses
that were contributed from this run, when forming a
list of best scoring hypotheses. For example, for a
test set of 1000 sentences, the accuracy of a decod-
ing run would be 30% if it was able to produce the
best scoring hypothesis for 300 sentences in the test
set. Translation quality is measured through BLEU
(Papineni et al, 2002).
6 Experimental Setup
We initially experimented with two language pairs:
German-to-English (G-E) and French-to-English (F-
E). We trained our system and the baseline sys-
tems on most of the data6 made available for the
translation task of the Fourth Workshop on Statis-
tical Machine Translation.7 We used 1M bilin-
gual sentences, for the estimation of the transla-
tion model and 2M sentences from the monolingual
corpus (news commentary) which also contains the
English part of the bilingual corpus. Word align-
ments are obtained by running GIZA++ (Och and
Ney, 2003) with the grow-diag-final-and (Koehn et
al., 2005) symmetrization heuristic. We follow the
training steps described in Durrani et al (2011), con-
sisting of i) post-processing the alignments to re-
move discontinuous and unaligned target cepts, ii)
conversion of bilingual alignments into operation
sequences, iii) estimation of the n-gram language
models.
6We did not use all the available data due to scalability is-
sues. The scores reported are therefore well below those ob-
tained by the systems submitted to the WMT evaluation series.
7http://www.statmt.org/wmt09/translation-task.html
6.1 Search Accuracy Results
We divided our evaluation into two halves. In
the first half we carried out experiments to mea-
sure search accuracy and translation quality of
our decoders against the baseline cept-based OSM
(cept.500) that uses minimal translation units with a
stack size of 500. We used the version of the cept-
based OSM system that does not allow discontinu-
ous8 source cepts. To increase the speed of the sys-
tem we used a hard reordering limit of 159, in the
baseline decoder and our modifications, disallowing
jumps that are beyond 15 words from the first open
gap. For each extracted cept or phrase 10-best trans-
lation options are extracted.
Using phrases in search reduces the decoding
speed. In order to make a fair comparison, both the
phrase-based and the baseline cept-based decoders
should be allowed to run for the same amount of
time. We therefore reduced the stack size in the
phrase-based decoder so that it runs in the same
amount of time as the cept-based decoder. We found
that using a stack size of 20010 for the phrase-based
decoder was comparable in speed to using a stack-
size of 500 in the cept-based decoding.
We first tuned the baseline on dev11 to obtain an
optimized weight vector. We then ran the baseline
and our decoders as discussed in Section 5.2 on the
dev-test. Then we repeated this experiment by tun-
ing the weights with our phrase-based decoder (us-
ing a stack size of 100) and ran all the decoders again
using the new weights.
Table 1 shows the average search accuracies and
BLEU scores of the two experiments. Using phrases
during decoding in the G-E experiments resulted
in a statistically significant12 0.69 BLEU points
gain comparing our best system phrase.200 with the
baseline system cept.500. We mark a result as sig-
8Discontinuous source-side units did not lead to any im-
provements in (Durrani et al, 2011) and increased the decoding
times by multiple folds. We also found these to be less useful.
9Imposing a hard reordering limit significantly reduced the
decoding time and also slightly increased the BLEU scores.
10Higher stack sizes leads to improvement in model scores
for both German-English and French-English and slight im-
provement of BLEU in the case of the former.
11We used news-dev2009a as dev and news-dev2009b as dev-
test and tuned the weights with Z-MERT (Zaidan, 2009).
12We use bootstrap resampling (Koehn, 2004b) to test our
results against the baseline result.
7
System German French
Acc. BLEU Acc. BLEU
Baseline System cept.stack-size
cept.50 25.95% 19.50 42.10% 21.44
cept.100 30.04% 19.79 47.32% 21.70
cept.200 35.17% 19.98 51.47% 21.82
cept.500 41.56% 20.14 54.93% 21.87
Our Cept-based Decoders
cept.500.fc 48.44% 20.52* 54.73% 21.86
cept.500.t 52.24% 20.34 67.95% 22.00
cept.500.fc.t 61.81% 20.53* 67.76% 21.96
Our Phrase-based Decoders
phrase.50 58.88% 20.58* 80.83% 22.04
phrase.100 69.85% 20.73* 88.34% 22.13
phrase.200 79.71% 20.83* 92.93% 22.17*
Table 1: Search Accuracies (Acc.) and BLEU scores of
the Baseline and Our Decoders with different Stack Sizes
(fc = Future Cost Estimated from Phrases, t = Cept Trans-
lation Options enriched from Phrases)
nificant if the improvement shown by our decoder
over the baseline decoder (cept.500) is significant at
the p ? 0.05 level, in both the runs. All the out-
puts that show statistically significant improvements
over the baseline decoder (cept.500) in Table 1 are
marked with an asterisk.
The search accuracy of our best system
(phrase.200), in G-E experiments is roughly
80%, which means that 80% of the times the
phrase-based decoder (using stack size 200) was
able to produce the same model score or a better
model score than the cept-based decoders (using
a stack size of 500). Our F-E experiments also
showed improvements in BLEU and model scores.
The search accuracy of our best system phrase.200
is roughly 93% as compared with 55% in the
baseline decoder (cept.500) giving a BLEU point
gain of +0.30 over the baseline.
Our modifications to the cept-based decoder also
showed improvements. We found that extending
the cept translation table (cept.500.t) using phrases
helps both in G-E and F-E experiments by extend-
ing the list of n-best translation options by 18% and
18.30% respectively. Using future costs estimated
from phrases (cept.500.fc) improved both search ac-
curacy and BLEU scores in G-E experiments, but
does not lead to any improvements in the F-E ex-
periments, as both BLEU and model scores drop
slightly. We looked at a few examples where the
model score dropped and found that in these cases,
the best scoring hypotheses are ranked very low ear-
lier in the decoding and make their way to the top
gradually in subsequent steps. A slight difference in
the future-cost estimate prunes these hypotheses in
one or the other decoder. We found future cost to
be more critical in G-E than F-E experiments. This
can be explained by the fact that more reordering is
required in G-E and it is necessary to account for the
reordering operations and jump-based features (gap-
based penalties, reordering distance and gap-width)
in the future-cost estimation. F-E on the other hand
is largely monotonic except for a few short distance
reorderings such as flipping noun and adjective.
6.2 Comparison with other Baseline Systems
In the second half of our evaluation we compared
our best system phrase.200 with the baseline sys-
tem cept.500, and other state-of-the-art phrase-based
and N-gram-based systems on German-to-English,
French-to-English, and Spanish-to-English tasks13.
We used the official evaluation data (news-test sets)
from the Statistical Machine Translation Workshops
2009-2011 for all three language pairs (German,
Spanish and French). The feature weights for all the
systems are tuned using the dev set news-dev2009a.
We separately tune the baseline system (cept.500)
and the phrase-based system (phrase.200) and do not
hold the lambda vector constant like before.
Baseline Systems: We also compared our system
with i) Moses (Koehn et al, 2007), ii) Phrasal14 (Cer
et al, 2010), and iii) Ncode (Crego et al, 2011).
We used the default stack sizes of 100 for
Moses15, 200 for Phrasal, 25 for Ncode (with 2m
stacks). A 5-gram English language model is used.
Both phrase-based systems use 20-best phrases for
translation, Ncode uses 25-best tuple translations.
The training and test data for Ncode was tagged us-
ing TreeTagger (Schmid, 1994). All the baseline
systems used lexicalized reordering model. A hard
reordering limit16 of 6 words is used as a default in
13We did not include the results of Spanish in the previous
section due to space limitations but these are similar to those of
the French-to-English translation task.
14Phrasal provides two extensions to Moses: i) hierarchical
reordering model (Galley and Manning, 2008) and ii) discon-
tinuous phrases (Galley and Manning, 2010).
15Using stacks sizes from 200?1000 did not improve results.
16We tried to increase the distortion limit in the baseline sys-
8
both the baseline phrase-based systems. Amongst
the other defaults we retained the hard source gap
penalty of 15 and a target gap penalty of 7 in Phrasal.
We provide Moses and Ncode with the same post-
edited alignments17 from which we removed target-
side discontinuities. We feed the original alignments
to Phrasal because of its ability to learn discontinu-
ous source and target phrases. All the systems use
MERT for the optimization of the weight vector.
Ms Pd Nc C500 P200
German-to-English
MT09 18.73* 19.00* 18.37* 19.06* 19.66
MT10 18.58* 18.96* 18.64* 19.12* 19.70
MT11 17.38* 17.58* 17.49* 17.87* 18.19
French-to-English
MT09 24.61* 24.73* 24.28* 24.94* 25.27
MT10 23.69* 23.09* 23.96 23.90* 24.25
MT11 25.17* 25.55* 24.92* 25.40* 25.92
Spanish-to-English
MT09 24.38* 24.63 24.72 24.48* 24.72
MT10 25.55* 25.66* 25.87 25.68* 26.10
MT11 25.72* 26.17* 26.36* 26.48 26.67
Table 2: Comparison on 3-Test Sets ? Ms = Moses, Pd
= Phrasal (Discontinuous Phrases), Nc = Ncode, C500 =
Cept.500, P200 = Phrase.200
Table 2 compares the performance of our phrase-
based decoder against the baselines. Our system
shows an improvement over all the baseline systems
for the G-E pair, in 11 out of 12 cases in the F-E
pair and in 8 out of 12 cases in the S-E language
pair. We mark a baseline with ?*? to indicate that
our decoder shows an improvement over this base-
line result which is significant at the p ? 0.05 level.
7 Conclusion and Future Work
We proposed a combination of using a model based
on minimal units and decoding with phrases. Mod-
eling with minimal units enables us to learn local
and non-local dependencies in a unified manner and
avoid spurious segmentation ambiguities. However,
using minimal units also in the search presents a
significant challenge because of the poor transla-
tion coverage, inaccurate future-cost estimates and
tems to 15 (in G-E experiments) as used in our systems but the
results dropped significantly in case of Moses and slightly for
Phrasal so we used the default limits for both decoders.
17Using post-processed alignments gave slightly better re-
sults than the original alignments for these baseline systems.
Details are omitted due to space limitation.
the pruning of the correct hypotheses. Phrase-based
SMT on the other hand overcomes these drawbacks
by using larger translation chunks during search.
However, the drawback of the phrase-based model is
the phrasal independence assumption, spurious am-
biguity in segmentation and a weak mechanism to
handle non-local reorderings. We showed that com-
bining a model based on minimal units with phrase-
based decoding can improve both search accuracy
and translation quality. We also showed that the
phrasal information can be indirectly used in cept-
based decoding with improved results. We tested
our system against the state-of-the-art phrase-based
and N-gram-based systems, for German-to-English,
French-to-English, and Spanish-to-English for three
standard test sets. Our system showed statistically
significant improvements over all the baseline sys-
tems in most of the cases. We have shown the bene-
fits of using phrase-based search with a model based
on minimal units. In future work, we would like to
study whether a phrase-based system like Moses or
Phrasal can profit from an OSM-style or N-gram-
style feature. Feng et al (2010) previously showed
that adding a linearized source-side language model
in a phrase-based system helped. It would also
be interesting to study whether the insight of us-
ing minimal units for modeling and phrase-based
search would hold for hierarchical SMT. Vaswani et
al. (2011) recently showed that a Markov model over
the derivation history of minimal rules can obtain the
same translation quality as using grammars formed
with composed rules.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. Nadir
Durrani and Alexander Fraser were funded by
Deutsche Forschungsgemeinschaft grant Models of
Morphosyntax for Statistical Machine Translation.
Nadir Durrani was partially funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n ? 287658. Helmut
Schmid was supported by Deutsche Forschungsge-
meinschaft grant SFB 732. This work was sup-
ported in part by the IST Programme of the Eu-
ropean Community, under the PASCAL2 Network
of Excellence, IST-2007-216886. This publication
only reflects the authors? views.
9
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Daniel Cer, Michel Galley, Daniel Jurafsky, and Christo-
pher D. Manning. 2010. Phrasal: A Statistical Ma-
chine Translation Toolkit for Exploring New model
Features. In Proceedings of the NAACL HLT 2010
Demonstration Session, pages 9?12, Los Angeles,
California, June.
Marta R. Costa-jussa`, Josep M. Crego, David Vilar,
Jose? A.R. Fonollosa, Jose? B. Marin?o, and Hermann
Ney. 2007. Analysis and System Combination of
Phrase- and N-Gram-Based Statistical Machine Trans-
lation Systems. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Companion Volume, Short Papers, pages 137?140,
Rochester, New York, April.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
Statistical MT by Coupling Reordering and Decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. Reordered Search and Unfolding Tuples for N-
Gram-Based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283?
289, Phuket, Thailand.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. Ncode: an Open Source Bilingual N-gram SMT
Toolkit. The Prague Bulletin of Mathematical Lin-
guistics, (96):49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with Inte-
grated Reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statisti-
cal Machine Translation. In Conference of the Associ-
ation for Machine Translation in the Americas 2010,
Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL, pages 127?133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL 2007
Demonstrations, Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical Significance Tests
for Machine Translation Evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A. R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N-gram-Based Machine
Translation. Computational Linguistics, 32(4):527?
549.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(1):417?449.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 20?28, University of Maryland,
College Park, MD.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
10
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillmann and Tong Zhang. 2005. A Local-
ized Prediction Model for Statistical Machine Transla-
tion. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 557?564, Ann Arbor, Michigan, June.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-to-
String Translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Omar F. Zaidan. 2009. Z-MERT: A Fully Configurable
Open Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague Bulletin
of Mathematical Linguistics, 91:79?88.
11
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 465?474,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hindi-to-Urdu Machine Translation Through Transliteration
Nadir Durrani Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{durrani,sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We present a novel approach to integrate
transliteration into Hindi-to-Urdu statisti-
cal machine translation. We propose two
probabilistic models, based on conditional
and joint probability formulations, that are
novel solutions to the problem. Our mod-
els consider both transliteration and trans-
lation when translating a particular Hindi
word given the context whereas in pre-
vious work transliteration is only used
for translating OOV (out-of-vocabulary)
words. We use transliteration as a tool
for disambiguation of Hindi homonyms
which can be both translated or translit-
erated or transliterated differently based
on different contexts. We obtain final
BLEU scores of 19.35 (conditional prob-
ability model) and 19.00 (joint probability
model) as compared to 14.30 for a base-
line phrase-based system and 16.25 for a
system which transliterates OOV words in
the baseline system. This indicates that
transliteration is useful for more than only
translating OOV words for language pairs
like Hindi-Urdu.
1 Introduction
Hindi is an official language of India and is writ-
ten in Devanagari script. Urdu is the national lan-
guage of Pakistan, and also one of the state lan-
guages in India, and is written in Perso-Arabic
script. Hindi inherits its vocabulary from Sanskrit
while Urdu descends from several languages in-
cluding Arabic, Farsi (Persian), Turkish and San-
skrit. Hindi and Urdu share grammatical structure
and a large proportion of vocabulary that they both
inherited from Sanskrit. Most of the verbs and
closed-class words (pronouns, auxiliaries, case-
markers, etc) are the same. Because both lan-
guages have lived together for centuries, some
Urdu words which originally came from Arabic
and Farsi have also mixed into Hindi and are now
part of the Hindi vocabulary. The spoken form of
the two languages is very similar.
The extent of overlap between Hindi and Urdu
vocabulary depends upon the domain of the text.
Text coming from the literary domain like novels
or history tend to have more Sanskrit (for Hindi)
and Persian/Arabic (for Urdu) vocabulary. How-
ever, news wire that contains text related to me-
dia, sports and politics, etc., is more likely to have
common vocabulary.
In an initial study on a small news corpus of
5000 words, randomly selected from BBC1 News,
we found that approximately 62% of the Hindi
types are also part of Urdu vocabulary and thus
can be transliterated while only 38% have to be
translated. This provides a strong motivation to
implement an end-to-end translation system which
strongly relies on high quality transliteration from
Hindi to Urdu.
Hindi and Urdu have similar sound systems but
transliteration from Hindi to Urdu is still very hard
because some phonemes in Hindi have several or-
thographic equivalents in Urdu. For example the
?z? sound2 can only be written as whenever it
occurs in a Hindi word but can be written as ,
, and in an Urdu word. Transliteration
becomes non-trivial in cases where the multiple
orthographic equivalents for a Hindi word are all
valid Urdu words. Context is required to resolve
ambiguity in such cases. Our transliterator (de-
scribed in sections 3.1.2 and 4.1.3) gives an accu-
racy of 81.6% and a 25-best accuracy of 92.3%.
Transliteration has been previously used only as
a back-off measure to translate NEs (Name Enti-
ties) and OOV words in a pre- or post-processing
step. The problem we are solving is more difficult
than techniques aimed at handling OOV words,
1http://www.bbc.co.uk/hindi/index.shtml
2All sounds are represented using SAMPA notation.
465
Hindi Urdu SAMPA Gloss
/ Am Mango/Ordinary
/ d ZAli Fake/Net
/ Ser Lion/Verse
Table 1: Hindi Words That Can Be Transliterated
Differently in Different Contexts
Hindi Urdu SAMPA Gloss
/ simA Border/Seema
/ Amb@r Sky/Ambar
/ vId Ze Victory/Vijay
Table 2: Hindi Words That Can Be Translated or
Transliterated in Different Contexts
which focus primarily on name transliteration, be-
cause we need different transliterations in differ-
ent contexts; in their case context is irrelevant. For
example: consider the problem of transliterating
the English word ?read? to a phoneme represen-
tation in the context ?I will read? versus the con-
text ?I have read?. An example of this for Hindi
to Urdu transliteration: the two Urdu words
(face/condition) and (chapter of the Koran)
are both written as (sur@t d) in Hindi. The
two are pronounced identically in Urdu but writ-
ten differently. In such cases we hope to choose
the correct transliteration by using context. Some
other examples are shown in Table 1.
Sometimes there is also an ambiguity of
whether to translate or transliterate a particular
word. The Hindi word , for example, will
be translated to (peace, s@kun) when it is a
common noun but transliterated to (Shanti,
SAnt di) when it is a proper name. We try to
model whether to translate or transliterate in a
given situation. Some other examples are shown
in Table 2.
The remainder of this paper is organized as fol-
lows. Section 2 provides a review of previous
work. Section 3 introduces two probabilistic mod-
els for integrating translations and transliterations
into a translation model which are based on condi-
tional and joint probability distributions. Section 4
discusses the training data, parameter optimization
and the initial set of experiments that compare our
two models with a baseline Hindi-Urdu phrase-
based system and with two transliteration-aided
phrase-based systems in terms of BLEU scores
(Papineni et al, 2001). Section 5 performs an er-
ror analysis showing interesting weaknesses in the
initial formulations. We remedy the problems by
adding some heuristics and modifications to our
models which show improvements in the results as
discussed in section 6. Section 7 gives two exam-
ples illustrating how our model decides whether
to translate or transliterate and how it is able to
choose among different valid transliterations given
the context. Section 8 concludes the paper.
2 Previous Work
There has been a significant amount of work on
transliteration. We can break down previous work
into three groups. The first group is generic
transliteration work, which is evaluated outside of
the context of translation. This work uses either
grapheme or phoneme based models to translit-
erate words lists (Knight and Graehl, 1998; Li
et al, 2004; Ekbal et al, 2006; Malik et al,
2008). The work by Malik et al addresses Hindi to
Urdu transliteration using hand-crafted rules and
a phonemic representation; it ignores translation
context.
A second group deals with out-of-vocabulary
words for SMT systems built on large parallel cor-
pora, and therefore focuses on name translitera-
tion, which is largely independent of context. Al-
Onaizan and Knight (2002) transliterate Arabic
NEs into English and score them against their re-
spective translations using a modified IBM Model
1. The options are further re-ranked based on dif-
ferent measures such as web counts and using co-
reference to resolve ambiguity. These re-ranking
methodologies can not be performed in SMT at
the decoding time. An efficient way to compute
and re-rank the transliterations of NEs and inte-
grate them on the fly might be possible. However,
this is not practical in our case as our model con-
siders transliterations of all input words and not
just NEs. A log-linear block transliteration model
is applied to OOV NEs in Arabic to English SMT
by Zhao et al (2007). This work is also translit-
erating only NEs and not doing any disambigua-
tion. The best method proposed by Kashani et
al. (2007) integrates translations provided by ex-
ternal sources such as transliteration or rule-base
translation of numbers and dates, for an arbitrary
number of entries within the input text. Our work
is different from Kashani et al (2007) in that our
model compares transliterations with translations
466
on the fly whereas transliterations in Kashani et al
do not compete with internal phrase tables. They
only compete amongst themselves during a sec-
ond pass of decoding. Hermjakob et al (2008) use
a tagger to identify good candidates for translit-
eration (which are mostly NEs) in input text and
add transliterations to the SMT phrase table dy-
namically such that they can directly compete with
translations during decoding. This is closer to
our approach except that we use transliteration as
an alternative to translation for all Hindi words.
Our focus is disambiguation of Hindi homonyms
whereas they are concentrating only on translit-
erating NE?s. Moreover, they are working with
a large bitext so they can rely on their transla-
tion model and only need to transliterate NEs and
OOVs. Our translation model is based on data
which is both sparse and noisy. Therefore we pit
transliterations against translations for every input
word. Sinha (2009) presents a rule-based MT sys-
tem that uses Hindi as a pivot to translate from En-
glish to Urdu. This work also uses transliteration
only for the translation of unknown words. Their
work can not be used for direct translation from
Hindi to Urdu (independently of English) ?due to
various ambiguous mappings that have to be re-
solved?.
The third group uses transliteration models in-
side of a cross-lingual IR system (AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003; Pirkola
et al, 2003). Picking a single best transliteration
or translation in context is not important in an IR
system. Instead, all the options are used by giv-
ing them weights and context is typically not taken
into account.
3 Our Approach
Both of our models combine a character-based
transliteration model with a word-based transla-
tion model. Our models look for the most probable
Urdu token sequence un1 for a given Hindi token
sequence hn1 . We assume that each Hindi token is
mapped to exactly one Urdu token and that there is
no reordering. The assumption of no reordering is
reasonable given the fact that Hindi and Urdu have
identical grammar structure and the same word or-
der. An Urdu token might consist of more than one
Urdu word3. The following sections give a math-
3This occurs frequently in case markers with nouns,
derivational affixes and compounds etc. These are written
as single words in Hindi as opposed to Urdu where they are
ematical formulation of our two models, Model-1
and Model-2.
3.1 Model-1 : Conditional Probability Model
Applying a noisy channel model to compute the
most probable translation u?n1 , we get:
argmax
un1
p(un1 |h
n
1 ) = argmax
un1
p(un1 )p(h
n
1 |u
n
1 )
(1)
3.1.1 Language Model
The language model (LM) p(un1 ) is implemented
as an n-gram model using the SRILM-Toolkit
(Stolcke, 2002) with Kneser-Ney smoothing. The
parameters of the language model are learned from
a monolingual Urdu corpus. The language model
is defined as:
p(un1 ) =
n?
i=1
pLM (ui|u
i?1
i?k) (2)
where k is a parameter indicating the amount of
context used (e.g., k = 4 means 5-gram model).
ui can be a single or a multi-word token. A
multi-word token consists of two or more Urdu
words. For a multi-word ui we do multiple lan-
guage model look-ups, one for each uix in ui =
ui1 , . . . , uim and take their product to obtain the
value pLM (ui|u
i?1
i?k).
Language Model for Unknown Words: Our
model generates transliterations that can be known
or unknown to the language model and the trans-
lation model. We refer to the words known to
the language model and to the translation model
as LM-known and TM-known words respectively
and to words that are unknown as LM-unknown
and TM-unknown respectively.
We assign a special value ? to the LM-unknown
words. If one or more uix in a multi-word ui are
LM-unknown we assign a language model score
pLM (ui|u
i?1
i?k) = ? for the entire ui, meaning
that we consider partially known transliterations
to be as bad as fully unknown transliterations. The
parameter ? controls the trade-off between LM-
known and LM-unknown transliterations. It does
not influence translation options because they are
always LM-known in our case. This is because our
monolingual corpus also contains the Urdu part of
translation corpus. The optimization of ? is de-
scribed in section 4.2.1.
written as two words. For example (beautiful ; xub-
sur@t d) and (your?s ; ApkA) are written as
and respectively in Urdu.
467
3.1.2 Translation Model
The translation model (TM) p(hn1 |u
n
1 ) is approx-
imated with a context-independent model:
p(hn1 |u
n
1 ) =
n?
i=1
p(hi|ui) (3)
where hi and ui are Hindi and Urdu tokens re-
spectively. Our model estimates the conditional
probability p(hi|ui) by interpolating a word-
based model and a character-based (translitera-
tion) model.
p(hi|ui) = ?pw(hi|ui) + (1? ?)pc(hi|ui) (4)
The parameters of the word-based translation
model pw(h|u) are estimated from the word align-
ments of a small parallel corpus. We only retain
1-1/1-N (1 Hindi word, 1 or more Urdu words)
alignments and throw away N-1 and M-N align-
ments for our models. This is further discussed in
section 4.1.1.
The character-based transliteration model
pc(h|u) is computed in terms of pc(h, u), a joint
character model, which is also used for Chinese-
English back-transliteration (Li et al, 2004) and
Bengali-English name transliteration (Ekbal et al,
2006). The character-based transliteration proba-
bility is defined as follows:
pc(h, u) =
?
an1?align(h,u)
p(an1 )
=
?
an1?align(h,u)
n?
i=1
p(ai|a
i?1
i?k) (5)
where ai is a pair consisting of the i-th Hindi char-
acter hi and the sequence of 0 or more Urdu char-
acters that it is aligned with. A sample alignment
is shown in Table 3(b) in section 4.1.3. Our best
results are obtained with a 5-gram model. The
parameters p(ai|a
i?1
i?k) are estimated from a small
transliteration corpus which we automatically ex-
tracted from the translation corpus. The extrac-
tion details are also discussed in section 4.1.3. Be-
cause our overall model is a conditional probabil-
ity model, joint-probabilities are marginalized us-
ing character-based prior probabilities:
pc(h|u) =
pc(h, u)
pc(u)
(6)
The prior probability pc(u) of the character se-
quence u = cm1 is defined with a character-based
language model:
pc(u) =
m?
i=1
p(ci|c
i?1
i?k) (7)
The parameters p(ci|c
i?1
i?k) are estimated from
the Urdu part of the character-aligned translitera-
tion corpus. Replacing (6) in (4) we get:
p(hi|ui) = ?pw(hi|ui) + (1? ?)
pc(hi, ui)
pc(ui)
(8)
Having all the components of our model defined
we insert (8) and (2) in (1) to obtain the final equa-
tion:
u?n1 = argmax
un1
n?
i=1
pLM (ui|u
i?1
i?k)[?pw(hi|ui)
+ (1? ?)
pc(hi, ui)
pc(ui)
] (9)
The optimization of the interpolating factor ? is
discussed in section 4.2.1.
3.2 Model-2 : Joint Probability Model
This section briefly defines a variant of our model
where we interpolate joint probabilities instead of
conditional probabilities. Again, the translation
model p(hn1 |u
n
1 ) is approximated with a context-
independent model:
p(hn1 |u
n
1 ) =
n?
i=1
p(hi|ui) =
n?
i=1
p(hi, ui)
p(ui)
(10)
The joint probability p(hi, ui) of a Hindi and an
Urdu word is estimated by interpolating a word-
based model and a character-based model.
p(hi, ui) = ?pw(hi, ui)+(1??)pc(hi, ui) (11)
and the prior probability p(ui) is estimated as:
p(ui) = ?pw(ui) + (1? ?)pc(ui) (12)
The parameters of the translation model pw(hi, ui)
and the word-based prior probabilities pw(ui) are
estimated from the 1-1/1-N word-aligned corpus
(the one that we also used to estimate translation
probabilities pw(hi|ui) previously).
The character-based transliteration probability
pc(hi, ui) and the character-based prior probabil-
ity pc(ui) are defined by (5) and (7) respectively in
468
the previous section. Putting (11) and (12) in (10)
we get
p(hn1 |u
n
1 ) =
n?
i=1
?pw(hi, ui) + (1? ?)pc(hi, ui)
?pw(ui) + (1? ?)pc(ui)
(13)
The idea is to interpolate joint probabilities and di-
vide them by the interpolated marginals. The final
equation for Model-2 is given as:
u?n1 = argmax
un1
n?
i=1
pLM (ui|u
i?1
i?k)?
?pw(hi, ui) + (1? ?)pc(hi, ui)
?pw(ui) + (1? ?)pc(ui)
(14)
3.3 Search
The decoder performs a stack-based search using
a beam-search algorithm similar to the one used
in Pharoah (Koehn, 2004a). It searches for an
Urdu string that maximizes the product of trans-
lation probability and the language model proba-
bility (equation 1) by translating one Hindi word
at a time. It is implemented as a two-level pro-
cess. At the lower level, it computes n-best
transliterations for each Hindi word hi accord-
ing to pc(h, u). The joint probabilities given by
pc(h, u) are marginalized for each Urdu transliter-
ation to give pc(h|u). At the higher level, translit-
eration probabilities are interpolated with pw(h|u)
and then multiplied with language model probabil-
ities to give the probability of a hypothesis. We use
20-best translations and 25-best transliterations for
pw(h|u) and pc(h|u) respectively and a 5-gram
language model.
To keep the search space manageable and time
complexity polynomial we apply pruning and re-
combination. Since our model uses monotonic de-
coding we only need to recombine hypotheses that
have the same context (last n-1 words). Next we
do histogram-based pruning, maintaining the 100-
best hypotheses for each stack.
4 Evaluation
4.1 Training
This section discusses the training of the different
model components.
4.1.1 Translation Corpus
We used the freely available EMILLE Corpus
as our bilingual resource which contains roughly
13,000 Urdu and 12,300 Hindi sentences. From
these we were able to sentence-align 7000 sen-
tence pairs using the sentence alignment algorithm
given by Moore (2002).
The word alignments for this task were ex-
tracted by using GIZA++ (Och and Ney, 2003) in
both directions. We extracted a total of 107323
alignment pairs (5743 N-1 alignments, 8404 M-
N alignments and 93176 1-1/1-N alignments). Of
these alignments M-N and N-1 alignment pairs
were ignored. We manually inspected a sample of
1000 instances of M-N/N-1 alignments and found
that more than 70% of these were (totally or par-
tially) wrong. Of the 30% correct alignments,
roughly one-third constitute N-1 alignments. Most
of these are cases where the Urdu part of the align-
ment actually consists of two (or three) words
but was written without space because of lack of
standard writing convention in Urdu. For exam-
ple (can go ; d ZA s@kt de) is alterna-
tively written as (can go ; d ZAs@kt de)
i.e. without space. We learned that these N-1
translations could be safely dropped because we
can generate a separate Urdu word for each Hindi
word. For valid M-N alignments we observed that
these could be broken into 1-1/1-N alignments in
most of the cases. We also observed that we usu-
ally have coverage of the resulting 1-1 and 1-N
alignments in our translation corpus. Looking at
the noise in the incorrect alignments we decided
to drop N-1 and M-N cases. We do not model
deletions and insertions so we ignored null align-
ments. Also 1-N alignments with gaps were ig-
nored. Only the alignments with contiguous words
were kept.
4.1.2 Monolingual Corpus
Our monolingual Urdu corpus consists of roughly
114K sentences. This comprises 108K sentences
from the data made available by the University of
Leipzig4 + 5600 sentences from the training data
of each fold during cross validation.
4.1.3 Transliteration Corpus
The training corpus for transliteration is extracted
from the 1-1/1-N word-alignments of the EMILLE
corpus discussed in section 4.1.1. We use an edit
distance algorithm to align this training corpus at
the character level and we eliminate translation
pairs with high edit distance which are unlikely to
be transliterations.
4http://corpora.informatik.uni-leipzig.de/
469
We used our knowledge of the Hindi and Urdu
scripts to define the initial character mapping. The
mapping was further extended by looking into
available Hindi-Urdu transliteration systems[5,6]
and other resources (Gupta, 2004; Malik et al,
2008; Jawaid and Ahmed, 2009). Each pair in the
character map is assigned a cost. A Hindi charac-
ter that always map to only one Urdu character is
assigned a cost of 0 whereas the Hindi characters
that map to different Urdu characters are assigned
a cost of 0.2. The edit distance metric allows
insert, delete and replace operations. The hand-
crafted pairs define the cost of replace operations.
We set a cost of 0.6 for deletions and insertions.
These costs were optimized on held out data. The
details of optimization are not mentioned due to
limited space. Using this metric we filter out the
word pairs with high edit-distance to extract our
transliteration corpus. We were able to extract
roughly 2100 unique pairs along with their align-
ments. The resulting alignments are modified by
merging unaligned ? ? 1 (no character on source
side, 1 character on target side) or ? ? N align-
ments with the preceding alignment pair. If there
is no preceding alignment pair then it is merged
with the following pair. Table 3 gives an example
showing initial alignment (a) and the final align-
ment (b) after applying the merge operation. Our
model retains 1 ? ? and N ? ? alignments as
deletion operations.
a) Hindi ? b c ? e f
Urdu A XY C D ? F
b) Hindi b c e f
Urdu AXY CD ? F
Table 3: Alignment (a) Before (b) After Merge
The parameters pc(h, u) and pc(u) are trained
on the aligned corpus using the SRILM toolkit.
We use Add-1 smoothing for unigrams and
Kneser-Ney smoothing for higher n-grams.
4.1.4 Diacritic Removal and Normalization
In Urdu, short vowels are represented with diacrit-
ics but these are rarely written in practice. In or-
der to keep the data consistent, all diacritics are
removed. This loss of information is not harm-
ful when transliterating/translating from Hindi to
Urdu because undiacritized text is equally read-
5CRULP: http://www.crulp.org/software/langproc.htm
6Malerkotla.org: http://translate.malerkotla.co.in
able to native speakers as its diacritized counter
part. However leaving occasional diacritics in the
corpus can worsen the problem of data sparsity by
creating spurious ambiguity7.
There are a few Urdu characters that have mul-
tiple equivalent Unicodes. All such forms are nor-
malized to have only one representation8.
4.2 Experimental Setup
We perform a 5-fold cross validation taking 4/5 of
the data as training and 1/5 as test data. Each fold
comprises roughly 1400 test sentences and 5600
training sentences.
4.2.1 Parameter Optimization
Our model contains two parameters ? (the inter-
polating factor between translation and transliter-
ation modules) and ? (the factor that controls the
trade-off between LM-known and LM-unknown
transliterations). The interpolating factor ? is ini-
tialized, inspired by Written-Bell smoothing, with
a value of NN+B
9. We chose a very low value
1e?40 for the factor ? initially, favoring LM-
known transliterations very strongly. Both of these
parameters are optimized as described below.
Because our training data is very sparse we do
not use held-out data for parameter optimization.
Instead we optimize these parameters by perform-
ing a 2-fold optimization for each of the 5 folds.
Each fold is divided into two halves. The param-
eters ? and ? are optimized on the first half and
the other half is used for testing, then optimiza-
tion is done on the second half and the first half is
used for testing. The optimal value for parameter
? occurs between 0.7-0.84 and for the parameter
? between 1e?5 and 1e?10.
4.2.2 Results
Baseline Pb0: We ran Moses (Koehn et al, 2007)
using Koehn?s training scripts10, doing a 5-fold
cross validation with no reordering11. For the
other parameters we use the default values i.e.
5-gram language model and maximum phrase-
length= 6. Again, the language model is imple-
7It should be noted though that diacritics play a very im-
portant role when transliterating in the reverse direction be-
cause these are virtually always written in Hindi as dependent
vowels.
8www.crulp.org/software/langproc/urdunormalization.htm
9N is the number of aligned word pairs (tokens) and B is
the number of different aligned word pairs (types).
10http://statmt.org/wmt08/baseline.html
11Results are worse with reordering enabled.
470
M Pb0 Pb1 Pb2 M1 M2
BLEU 14.3 16.25 16.13 18.6 17.05
Table 4: Comparing Model-1 and Model-2 with
Phrase-based Systems
mented as an n-gram model using the SRILM-
Toolkit with Kneser-Ney smoothing. Each fold
comprises roughly 1400 test sentences, 5000 in
training and 600 in dev12. We also used two meth-
ods to incorporate transliterations in the phrase-
based system:
Post-process Pb1: All the OOV words in the
phrase-based output are replaced with their top-
candidate transliteration as given by our translit-
eration system.
Pre-process Pb2: Instead of adding translit-
erations as a post process we do a second pass
by adding the unknown words with their top-
candidate transliteration to the training corpus and
rerun Koehn?s training script with the new training
corpus. Table 4 shows results (taking arithmetic
average over 5 folds) from Model-1 and Model-
2 in comparison with three baselines discussed
above.
Both our systems (Model-1 and Model-2) beat
the baseline phrase-based system with a BLEU
point difference of 4.30 and 2.75 respectively. The
transliteration aided phrase-based systems Pb1
and Pb2 are closer to our Model-2 results but are
way below Model-1 results. The difference of
2.35 BLEU points between M1 and Pb1 indicates
that transliteration is useful for more than only
translating OOV words for language pairs like
Hindi-Urdu. Our models choose between trans-
lations and transliterations based on context un-
like the phrase-based systems Pb1 and Pb2 which
use transliteration only as a tool to translate OOV
words.
5 Error Analysis
Based on preliminary experiments we found three
major flaws in our initial formulations. This sec-
tion discusses each one of them and provides some
heuristics and modifications that we employ to try
to correct deficiencies we found in the two models
described in section 3.1 and 3.2.
12After having the MERT parameters, we add the 600 dev
sentences back into the training corpus, retrain GIZA, and
then estimate a new phrase table on all 5600 sentences. We
then use the MERT parameters obtained before together with
the newer (larger) phrase-table set.
5.1 Heuristic-1
A lot of errors occur because our translation model
is built on very sparse and noisy data. The moti-
vation for this heuristic is to counter wrong align-
ments at least in the case of verbs and functional
words (which are often transliterations). This
heuristic favors translations that also appear in the
n-best transliteration list over only-translation and
only-transliteration options. We modify the trans-
lation model for both the conditional and the joint
model by adding another factor which strongly
weighs translation+transliteration options by tak-
ing the square-root of the product of the translation
and transliteration probabilities. Thus modifying
equations (8) and (11) in Model-1 and Model-2
we obtain equations (15) and (16) respectively:
p(hi|ui) = ?1pw(hi|ui) + ?2
pc(hi, ui)
pc(ui)
+ ?3
?
pw(hi|ui)
pc(hi, ui)
pc(ui)
(15)
p(hi, ui) = ?1pw(hi, ui) + ?2pc(hi, ui)
+ ?3
?
pw(hi, ui)pc(hi, ui) (16)
For the optimization of lambda parameters we
hold the value of the translation coefficient ?113
and the transliteration coefficient ?2 constant (us-
ing the optimized values as discussed in section
4.2.1) and optimize ?3 again using 2-fold opti-
mization on all the folds as described above14.
5.2 Heuristic-2
When an unknown Hindi word occurs for which
all transliteration options are LM-unknown then
the best transliteration should be selected. The
problem in our original models is that a fixed LM
probability ? is used for LM-unknown transliter-
ations. Hence our model selects the translitera-
tion that has the best pc(hi,ui)pc(ui) score i.e. we max-
imize pc(hi|ui) instead of pc(ui|hi) (or equiva-
lently pc(hi, ui)). The reason is an inconsistency
in our models. The language model probabil-
ity of unknown words is uniform (and equal to
?) whereas the translation model uses the non-
uniform prior probability pc(ui) for these words.
There is another reason why we can not use the
13The translation coefficient ?1 is same as ? used in previ-
ous models and the transliteration coefficient ?2 = 1? ?
14After optimization we normalize the lambdas to make
their sum equal to 1.
471
value ? in this case. Our transliterator model also
produces space inserted words. The value of ? is
very small because of which transliterations that
are actually LM-unknown, but are mistakenly bro-
ken into constituents that are LM-known, will al-
ways be preferred over their counter parts. An ex-
ample of this is (America) for which two
possible transliterations as given by our model are
(AmerIkA, without space) and (AmerI
kA, with space). The latter version is LM-known
as its constituents are LM-known. Our models al-
ways favor the latter version. Space insertion is an
important feature of our transliteration model. We
want our transliterator to tackle compound words,
derivational affixes, case-markers with nouns that
are written as one word in Hindi but as two or more
words in Urdu. Examples were already shown in
section 3?s footnote.
We eliminate the inconsistency by using pc(ui)
as the 0-gram back-off probability distribution in
the language model. For an LM-unknown translit-
erations we now get in Model-1:
p(ui|u
i?1
i?k)[?pw(hi|ui) + (1? ?)
pc(hi, ui)
pc(ui)
]
= p(ui|u
i?1
i?k)[(1? ?)
pc(hi, ui)
pc(ui)
]
=
k?
j=0
?(ui?1i?j )pc(ui)[(1? ?)
pc(hi, ui)
pc(ui)
]
=
k?
j=0
?(ui?1i?j )[(1? ?)pc(hi, ui)]
where
?k
j=0 ?(u
i?1
i?j ) is just the constant that
SRILM returns for unknown words. The last
line of the calculation shows that we simply drop
pc(ui) if ui is LM-unknown and use the constant?k
j=0 ?(u
i?1
i?j ) instead of ?. A similar calculation
for Model-2 gives
?k
j=0 ?(u
i?1
i?j )pc(hi, ui).
5.3 Heuristic-3
This heuristic discusses a flaw in Model-2. For
transliteration options that are TM-unknown, the
pw(h, u) and pw(u) factors becomes zero and the
translation model probability as given by equation
(13) becomes:
(1? ?)pc(hi, ui)
(1? ?)pc(ui)
=
pc(hi, ui)
pc(ui)
In such cases the ? factor cancels out and no
weighting of word translation vs. transliteration
H1 H2 H12
M1 18.86 18.97 19.35
M2 17.56 17.85 18.34
Table 5: Applying Heuristics 1 and 2 and their
Combinations to Model-1 and Model-2
H3 H13 H23 H123
M2 18.52 18.93 18.55 19.00
Table 6: Applying Heuristic 3 and its Combina-
tions with other Heuristics to Model-2
occurs anymore. As a result of this, translitera-
tions are sometimes incorrectly favored over their
translation alternatives.
In order to remedy this problem we assign a
minimal probability ? to the word-based prior
pw(ui) in case of TM-unknown transliterations,
which prevents it from ever being zero. Because
of this addition the translation model probability
for LM-unknown words becomes:
(1? ?)pc(hi, ui)
?? + (1? ?)pc(ui)
where ? =
1
Urdu Types in TM
6 Final Results
This section shows the improvement in BLEU
score by applying heuristics and combinations of
heuristics in both the models. Tables 5 and 6 show
the improvements achieved by using the differ-
ent heuristics and modifications discussed in sec-
tion 5. We refer to the results as MxHy where x
denotes the model number, 1 for the conditional
probability model and 2 for the joint probability
model and y denotes a heuristic or a combination
of heuristics applied to that model15.
Both heuristics (H1 and H2) show improve-
ments over their base models M1 and M2.
Heuristic-1 shows notable improvement for both
models in parts of test data which has high num-
ber of common vocabulary words. Using heuris-
tic 2 we were able to properly score LM-unknown
transliterations against each other. Using these
heuristics together we obtain a gain of 0.75 over
M-1 and a gain of 1.29 over M-2.
Heuristic-3 remedies the flaw in M2 by assign-
ing a special value to the word-based prior pw(ui)
for TM-unknown words which prevents the can-
celation of interpolating parameter ?. M2 com-
bined with heuristic 3 (M2H3) results in a 1.47
15For example M1H1 refers to the results when heuristic-
1 is applied to model-1 whereas M2H12 refers to the results
when heuristics 1 and 2 are together applied to model 2.
472
BLEU point improvement and combined with all
the heuristics (M2H123) gives an overall gain of
1.95 BLEU points and is close to our best results
(M1H12). We also performed significance test
by concatenating all the fold results. Both our best
systems M1H12 and M2H123 are statistically sig-
nificant (p < 0.05)16 over all the baselines dis-
cussed in section 4.2.2.
One important issue that has not been investi-
gated yet is that BLEU has not yet been shown
to have good performance in morphologically rich
target languages like Urdu, but there is no metric
known to work better. We observed that some-
times on data where the translators preferred to
translate rather than doing transliteration our sys-
tem is penalized by BLEU even though our out-
put string is a valid translation. For other parts of
the data where the translators have heavily used
transliteration, the system may receive a higher
BLEU score. We feel that this is an interesting
area of research for automatic metric developers,
and that a large scale task of translation to Urdu
which would involve a human evaluation cam-
paign would be very interesting.
7 Sample Output
This section gives two examples showing how our
model (M1H2) performs disambiguation. Given
below are some test sentences that have Hindi
homonyms (underlined in the examples) along
with Urdu output given by our system. In the first
example (given in Figure 1) Hindi word can be
transliterated to ( Lion) or (Verse) depend-
ing upon the context. Our model correctly identi-
fies which transliteration to choose given the con-
text.
In the second example (shown in Figure 2)
Hindi word can be translated to (peace,
s@kun) when it is a common noun but transliter-
ated to (Shanti, SAnt di) when it is a proper
name. Our model successfully decides whether to
translate or transliterate given the context.
8 Conclusion
We have presented a novel way to integrate
transliterations into machine translation. In
closely related language pairs such as Hindi-Urdu
with a significant amount of vocabulary overlap,
16We used Kevin Gimpel?s tester
(http://www.ark.cs.cmu.edu/MT/) which uses bootstrap
resampling (Koehn, 2004b), with 1000 samples.
Ser d Z@ngl kA rAd ZA he
?Lion is the king of jungle?
AIqbAl kA Aek xub sur@t d Ser he
?There is a beautiful verse from Iqbal?
Figure 1: Different Transliterations in Different
Contexts
p hIr b hi vh s@kun se n@her?h s@kt dA
?Even then he can?t live peacefully?
Aom SAnt di Aom frhA xAn ki d dusri fIl@m he
?Om Shanti Om is Farah Khan?s second film?
Figure 2: Translation or Transliteration
transliteration can be very effective in machine
translation for more than just translating OOV
words. We have addressed two problems. First,
transliteration helps overcome the problem of data
sparsity and noisy alignments. We are able to gen-
erate word translations that are unseen in the trans-
lation corpus but known to the language model.
Additionally, we can generate novel translitera-
tions (that are LM-Unknown). Second, generat-
ing multiple transliterations for homograph Hindi
words and using language model context helps us
solve the problem of disambiguation. We found
that the joint probability model performs almost as
well as the conditional probability model but that
it was more complex to make it work well.
Acknowledgments
The first two authors were funded by the Higher
Education Commission (HEC) of Pakistan. The
third author was funded by Deutsche Forschungs-
gemeinschaft grants SFB 732 and MorphoSynt.
The fourth author was funded by Deutsche
Forschungsgemeinschaft grant SFB 732.
473
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In CIKM 03: Proceed-
ings of the twelfth international conference on In-
formation and knowledge management, pages 139?
146.
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual
resources. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 400?408.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proceedings of the
COLING/ACL poster sessions, pages 191?198, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Swati Gupta. 2004. Aligning Hindi and Urdu bilin-
gual corpora for robust projection. Masters project
dissertation, Department of Computer Science, Uni-
versity of Sheffield.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III.
2008. Name translation in statistical machine trans-
lation - learning when to transliterate. In Proceed-
ings of ACL-08: HLT, pages 389?397, Columbus,
Ohio. Association for Computational Linguistics.
Bushra Jawaid and Tafseer Ahmed. 2009. Hindi to
Urdu conversion: beyond simple transliteration. In
Conference on Language and Technology 2009, La-
hore, Pakistan.
Mehdi M. Kashani, Eric Joanis, Roland Kuhn, George
Foster, and Fred Popowich. 2007. Integration of an
Arabic transliteration module into a statistical ma-
chine translation system. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 17?24, Prague, Czech Republic. Association
for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In ACL ?04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics,
pages 159?166, Barcelona, Spain. Association for
Computational Linguistics.
M G Abbas Malik, Christian Boitet, and Pushpak Bhat-
tacharyya. 2008. Hindi Urdu machine translitera-
tion using finite-state transducers. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, Manchester, UK.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY.
Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, Kari
Visala, and Kalervo Ja?rvelin. 2003. Fuzzy trans-
lation of cross-lingual spelling variants. In SIGIR
?03: Proceedings of the 26th annual international
ACM SIGIR conference on Research and develop-
ment in informaion retrieval, pages 345?352, New
York, NY, USA. ACM.
R. Mahesh K. Sinha. 2009. Developing English-Urdu
machine translation via Hindi. In Third Workshop
on Computational Approaches to Arabic Script-
based Languages (CAASL3), MT Summit XII, Ot-
tawa, Canada.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the ACL 2003 workshop
on Multilingual and mixed-language named entity
recognition, pages 57?64, Morristown, NJ, USA.
Association for Computational Linguistics.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 364?371, Rochester, New York. Associ-
ation for Computational Linguistics.
474
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1045?1054,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Joint Sequence Translation Model with Integrated Reordering
Nadir Durrani Helmut Schmid Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
{durrani,schmid,fraser}@ims.uni-stuttgart.de
Abstract
We present a novel machine translation model
which models translation by a linear sequence
of operations. In contrast to the ?N-gram?
model, this sequence includes not only trans-
lation but also reordering operations. Key
ideas of our model are (i) a new reordering
approach which better restricts the position to
which a word or phrase can be moved, and
is able to handle short and long distance re-
orderings in a unified way, and (ii) a joint
sequence model for the translation and re-
ordering probabilities which is more flexi-
ble than standard phrase-based MT. We ob-
serve statistically significant improvements in
BLEU over Moses for German-to-English and
Spanish-to-English tasks, and comparable re-
sults for a French-to-English task.
1 Introduction
We present a novel generative model that explains
the translation process as a linear sequence of oper-
ations which generate a source and target sentence
in parallel. Possible operations are (i) generation of
a sequence of source and target words (ii) insertion
of gaps as explicit target positions for reordering op-
erations, and (iii) forward and backward jump oper-
ations which do the actual reordering. The probabil-
ity of a sequence of operations is defined according
to an N-gram model, i.e., the probability of an op-
eration depends on the n ? 1 preceding operations.
Since the translation (generation) and reordering op-
erations are coupled in a single generative story,
the reordering decisions may depend on preceding
translation decisions and translation decisions may
depend on preceding reordering decisions. This pro-
vides a natural reordering mechanism which is able
to deal with local and long-distance reorderings in a
consistent way. Our approach can be viewed as an
extension of the N-gram SMT approach (Marin?o et
al., 2006) but our model does reordering as an inte-
gral part of a generative model.
The paper is organized as follows. Section 2 dis-
cusses the relation of our work to phrase-based and
the N-gram SMT. Section 3 describes our genera-
tive story. Section 4 defines the probability model,
which is first presented as a generative model, and
then shifted to a discriminative framework. Section
5 provides details on the search strategy. Section 6
explains the training process. Section 7 describes
the experimental setup and results. Section 8 gives
a few examples illustrating different aspects of our
model and Section 9 concludes the paper.
2 Motivation and Previous Work
2.1 Relation of our work to PBSMT
Phrase-based SMT provides a powerful translation
mechanism which learns local reorderings, transla-
tion of short idioms, and the insertion and deletion of
words sensitive to local context. However, PBSMT
also has some drawbacks. (i) Dependencies across
phrases are not directly represented in the translation
model. (ii) Discontinuous phrases cannot be used.
(iii) The presence of many different equivalent seg-
mentations increases the search space.
Phrase-based SMT models dependencies between
words and their translations inside of a phrase well.
However, dependencies across phrase boundaries
are largely ignored due to the strong phrasal inde-
1045
German English
hat er ein buch gelesen he read a book
hat eine pizza gegessen has eaten a pizza
er he
hat has
ein a
eine a
menge lot of
butterkekse butter cookies
gegessen eaten
buch book
zeitung newspaper
dann then
Table 1: Sample Phrase Table
pendence assumption. A phrase-based system us-
ing the phrase table1 shown in Table 1, for exam-
ple, correctly translates the German sentence ?er
hat eine pizza gegessen? to ?he has eaten a pizza?,
but fails while translating ?er hat eine menge but-
terkekse gegessen? (see Table 1 for a gloss) which
is translated as ?he has a lot of butter cookies eaten?
unless the language model provides strong enough
evidence for a different ordering. The generation of
this sentence in our model starts with generating ?er
? he?, ?hat ? has?. Then a gap is inserted on the Ger-
man side, followed by the generation of ?gegessen ?
eaten?. At this point, the (partial) German and En-
glish sentences look as follows:
er hat gegessen
he has eaten
We jump back to the gap on the German side
and fill it by generating ?eine ? a? and ?pizza ?
pizza?, for the first example and generating ?eine ?
a?, ?menge ? lot of?, ?butterkekse ? butter cookies?
for the second example, thus handling both short
and long distance reordering in a unified manner.
Learning the pattern ?hat gegessen ? has eaten?
helps us to generalize to the second example with
unseen context. Notice how the reordering deci-
sion is triggered by the translation decision in our
model. The probability of a gap insertion operation
after the generation of the auxiliaries ?hat ? has? will
be high because reordering is necessary in order to
move the second part of the German verb complex
(?gegessen?) to its correct position at the end of the
clause. This mechanism better restricts reordering
1The examples given in this section are not taken from the
real data/system, but made-up for the sake of argument.
Figure 1: (a) Known Context (b) Unknown Context
than traditional PBSMT and is able to deal with local
and long-distance reorderings in a consistent way.
Another weakness of the traditional phrase-based
system is that it can only capitalize on continuous
phrases. Given the phrase inventory in Table 1,
phrasal MT is able to generate example in Figure
1(a). The information ?hat...gelesen ? read? is inter-
nal to the phrase pair ?hat er ein buch gelesen ? he
read a book?, and is therefore handled conveniently.
On the other hand, the phrase table does not have
the entry ?hat er eine zeitung gelesen ? he read a
newspaper? (Figure 1(b)). Hence, there is no option
but to translate ?hat...gelesen? separately, translat-
ing ?hat? to ?has? which is a common translation for
?hat? but wrong in the given context. Context-free
hierarchical models (Chiang, 2007; Melamed, 2004)
have rules like ?hat er X gelesen ? he read X? to han-
dle such cases. Galley and Manning (2010) recently
solved this problem for phrasal MT by extracting
phrase pairs with source and target-side gaps. Our
model can also use tuples with source-side discon-
tinuities. The above sentence would be generated
by the following sequence of operations: (i) gener-
ate ?dann ? then? (ii) insert a gap (iii) generate ?er
? he? (iv) backward jump to the gap (v) generate
?hat...[gelesen] ? read? (only ?hat? and ?read? are
added to the sentences yet) (vi) jump forward to the
right-most source word so far generated (vii) insert
a gap (viii) continue the source cept (?gelesen? is in-
serted now) (ix) backward jump to the gap (x) gen-
erate ?ein ? a? (xi) generate ?buch ? book?.
Figure 2: Pattern
From this operation se-
quence, the model learns a
pattern (Figure 2) which al-
lows it to generalize to the
example in Figure 1(b). The open gap represented
by serves a similar purpose as the non-terminal
categories in a hierarchical phrase-based system
such as Hiero. Thus it generalizes to translate ?eine
zeitung? in exactly the same way as ?ein buch?.
1046
Another problem of phrasal MT is spurious
phrasal segmentation. Given a sentence pair and
a corresponding word alignment, phrasal MT can
learn an arbitrary number of source segmentations.
This is problematic during decoding because differ-
ent compositions of the same minimal phrasal units
are allowed to compete with each other.
2.2 Relation of our work to N-gram SMT
N-gram based SMT is an alternative to hierarchi-
cal and non-hierarchical phrase-based systems. The
main difference between phrase-based and N-gram
SMT is the extraction procedure of translation units
and the statistical modeling of translation context
(Crego et al, 2005a). The tuples used in N-gram
systems are much smaller translation units than
phrases and are extracted in such a way that a unique
segmentation of each bilingual sentence pair is pro-
duced. This helps N-gram systems to avoid the
spurious phrasal segmentation problem. Reorder-
ing works by linearization of the source side and tu-
ple unfolding (Crego et al, 2005b). The decoder
uses word lattices which are built with linguistically
motivated re-write rules. This mechanism is further
enhanced with an N-gram model of bilingual units
built using POS tags (Crego and Yvon, 2010). A
drawback of their reordering approach is that search
is only performed on a small number of reorderings
that are pre-calculated on the source side indepen-
dently of the target side. Often, the evidence for
the correct ordering is provided by the target-side
language model (LM). In the N-gram approach, the
LM only plays a role in selecting between the pre-
calculated orderings.
Our model is based on the N-gram SMT model,
but differs from previous N-gram systems in some
important aspects. It uses operation n-grams rather
than tuple n-grams. The reordering approach is en-
tirely different and considers all possible orderings
instead of a small set of pre-calculated orderings.
The standard N-gram model heavily relies on POS
tags for reordering and is unable to use lexical trig-
gers whereas our model exclusively uses lexical trig-
gers and no POS information. Linearization and un-
folding of the source sentence according to the target
sentence enables N-gram systems to handle source-
side gaps. We deal with this phenomenon more di-
rectly by means of tuples with source-side discon-
tinuities. The most notable feature of our work is
that it has a complete generative story of transla-
tion which combines translation and reordering op-
erations into a single operation sequence model.
Like the N-gram model2, our model cannot deal
with target-side discontinuities. These are elimi-
nated from the training data by a post-editing pro-
cess on the alignments (see Section 6). Galley and
Manning (2010) found that target-side gaps were not
useful in their system and not useful in the hierarchi-
cal phrase-based system Joshua (Li et al, 2009).
3 Generative Story
Our generative story is motivated by the complex re-
orderings in the German-to-English translation task.
The German and English sentences are jointly gen-
erated through a sequence of operations. The En-
glish words are generated in linear order3 while
the German words are generated in parallel with
their English translations. Occasionally the trans-
lator jumps back on the German side to insert some
material at an earlier position. After this is done, it
jumps forward again and continues the translation.
The backward jumps always end at designated land-
ing sites (gaps) which were explicitly inserted be-
fore. We use 4 translation and 3 reordering opera-
tions. Each is briefly discussed below.
Generate (X,Y): X and Y are German and English
cepts4 respectively, each with one or more words.
Words in X (German) may be consecutive or discon-
tinuous, but the words in Y (English) must be con-
secutive. This operation causes the words in Y and
the first word in X to be added to the English and
German strings respectively, that were generated so
far. Subsequent words in X are added to a queue to
be generated later. All the English words in Y are
generated immediately because English is generated
in linear order. The generation of the second (and
subsequent) German word in a multi-word cept can
be delayed by gaps, jumps and the Generate Source
Only operation defined below.
Continue Source Cept: The German words added
2However, Crego and Yvon (2009), in their N-gram system,
use split rules to handle target-side gaps and show a slight im-
provement on a Chinese-English translation task.
3Generating the English words in order is also what the de-
coder does when translating from German to English.
4A cept is a group of words in one language translated as a
minimal unit in one specific context (Brown et al, 1993).
1047
to the queue by the Generate (X,Y) operation are
generated by the Continue Source Cept operation.
Each Continue Source Cept operation removes one
German word from the queue and copies it to the
German string. If X contains more than one German
word, say n many, then it requires n translation op-
erations, an initial Generate (X1...Xn, Y ) operation
and n ? 1 Continue Source Cept operations. For
example ?hat...gelesen ? read? is generated by the
operation Generate (hat gelesen, read), which adds
?hat? and ?read? to the German and English strings
and ?gelesen? to a queue. A Continue Source Cept
operation later removes ?gelesen? from the queue
and adds it to the German string.
Generate Source Only (X): The string X is added
at the current position in the German string. This op-
eration is used to generate a German word X with no
corresponding English word. It is performed imme-
diately after its preceding German word is covered.
This is because there is no evidence on the English-
side which indicates when to generate X. Generate
Source Only (X) helps us learn a source word dele-
tion model. It is used during decoding, where a Ger-
man word (X) is either translated to some English
word(s) by a Generate (X,Y) operation or deleted
with a Generate Source Only (X) operation.
Generate Identical: The same word is added at
the current position in both the German and En-
glish strings. The Generate Identical operation is
used during decoding for the translation of unknown
words. The probability of this operation is estimated
from singleton German words that are translated to
an identical string. For example, for a tuple ?Port-
land ? Portland?, where German ?Portland? was ob-
served exactly once during training, we use a Gen-
erate Identical operation rather than Generate (Port-
land, Portland).
We now discuss the set of reordering operations
used by the generative story. Reordering has to be
performed whenever the German word to be gen-
erated next does not immediately follow the previ-
ously generated German word. During the genera-
tion process, the translator maintains an index which
specifies the position after the previously covered
German word (j), an index (Z) which specifies the
index after the right-most German word covered so
far, and an index of the next German word to be cov-
ered (j?). The set of reordering operations used in
Table 2: Step-wise Generation of Example 1(a). The ar-
row indicates position j.
generation depends upon these indexes.
Insert Gap: This operation inserts a gap which acts
as a place-holder for the skipped words. There can
be more than one open gap at a time.
Jump Back (W): This operation lets the translator
jump back to an open gap. It takes a parameter W
specifying which gap to jump to. Jump Back (1)
jumps to the closest gap to Z, Jump Back (2) jumps
to the second closest gap to Z, etc. After the back-
ward jump the target gap is closed.
Jump Forward: This operation makes the transla-
tor jump to Z. It is performed if some already gen-
erated German word is between the previously gen-
erated word and the word to be generated next. A
Jump Back (W) operation is only allowed at position
Z. Therefore, if j 6= Z, a Jump Forward operation
has to be performed prior to a Jump Back operation.
Table 2 shows step by step the generation of a
German/English sentence pair, the corresponding
translation operations, and the respective values of
the index variables. A formal algorithm for convert-
ing a word-aligned bilingual corpus into an opera-
tion sequence is presented in Algorithm 1.
4 Model
Our translation model p(F,E) is based on opera-
tion N-gram model which integrates translation and
reordering operations. Given a source string F , a
sequence of tuples T = (t1, . . . , tn) as hypothe-
sized by the decoder to generate a target string E,
the translation model estimates the probability of a
1048
Algorithm 1 Corpus Conversion Algorithm
i Position of current English cept
j Position of current German word
j? Position of next German word
N Total number of English cepts
fj German word at position j
Ei English cept at position i
Fi Sequence of German words linked to Ei
Li Number of German words linked with Ei
k Number of already generated German words for Ei
aik Position of kth German translation of Ei
Z Position after right-most generated German word
S Position of the first word of a target gap
i := 0; j := 0; k := 0
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
Z := j
while i < N do
j? := aik
if j < j? then
if fj was not generated yet then
Insert Gap
if j = Z then
j := j?
else
Jump Forward
if j? < j then
if j < Z and fj was not generated yet then
Insert Gap
W := relative position of target gap
Jump Back (W)
j := S
if j < j? then
Insert Gap
j := j?
if k = 0 then
Generate (Fi, Ei) {or Generate Identical}
else
Continue Source Cept
j := j + 1; k := k + 1
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
if Z < j then
Z := j
if k = Li then
i := i + 1; k := 0
Remarks:
We use cept positions for English (not word positions) because
English cepts are composed of consecutive words. German po-
sitions are word-based.
The relative position of the target gap is 1 if it is closest to Z, 2
if it is the second closest gap etc.
The operation Generate Identical is chosen if Fi = Ei and the
overall frequency of the German cept Fi is 1.
generated operation sequence O = (o1, . . . , oJ) as:
p(F,E) ?
J?
j=1
p(oj |oj?m+1...oj?1)
where m indicates the amount of context used. Our
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stolcke,
2002) with Kneser-Ney smoothing. We use a 9-gram
model (m = 8).
Integrating the language model the search is de-
fined as:
E? = argmax
E
pLM (E)p(F,E)
where pLM (E) is the monolingual language model
and p(F,E) is the translation model. But our trans-
lation model is a joint probability model, because of
which E is generated twice in the numerator. We
add a factor, prior probability ppr(E), in the denom-
inator, to negate this effect. It is used to marginalize
the joint-probability model p(F,E). The search is
then redefined as:
E? = argmax
E
pLM (E)
p(F,E)
ppr(E)
Both, the monolingual language and the prior
probability model are implemented as standard
word-based n-gram models:
px(E) ?
J?
j=1
p(wj |wj?m+1, . . . , wj?1)
where m = 4 (5-gram model) for the standard
monolingual model (x = LM ) and m = 8 (same
as the operation model5) for the prior probability
model (x = pr).
In order to improve end-to-end accuracy, we in-
troduce new features for our model and shift from
the generative6 model to the standard log-linear ap-
proach (Och and Ney, 2004) to tune7 them. We
search for a target stringE which maximizes a linear
combination of feature functions:
5In decoding, the amount of context used for the prior prob-
ability is synchronized with the position of back-off in the op-
eration model.
6Our generative model is about 3 BLEU points worse than
the best discriminative results.
7We tune the operation, monolingual and prior probability
models as separate features. We expect the prior probability
model to get a negative weight but we do not force MERT to
assign a negative weight to this feature.
1049
E? = argmax
E
?
?
?
J?
j=1
?jhj(F,E)
?
?
?
where ?j is the weight associated with the feature
hj(F,E). Other than the 3 features discussed above
(log probabilities of the operation model, monolin-
gual language model and prior probability model),
we train 8 additional features discussed below:
Length Bonus The length bonus feature counts the
length of the target sentence in words.
Deletion Penalty Another feature for avoiding too
short translations is the deletion penalty. Deleting a
source word (Generate Source Only (X)) is a com-
mon operation in the generative story. Because there
is no corresponding target-side word, the monolin-
gual language model score tends to favor this op-
eration. The deletion penalty counts the number of
deleted source words.
Gap Bonus and Open Gap Penalty These features
are introduced to guide the reordering decisions. We
observe a large amount of reordering in the automat-
ically word aligned training text. However, given
only the source sentence (and little world knowl-
edge), it is not realistic to try to model the reasons
for all of this reordering. Therefore we can use a
more robust model that reorders less than humans.
The gap bonus feature sums to the total number of
gaps inserted to produce a target sentence. The open
gap penalty feature is a penalty (paid once for each
translation operation performed) whose value is the
number of open gaps. This penalty controls how
quickly gaps are closed.
Distortion and Gap Distance Penalty We have
two additional features to control the reordering de-
cisions. One of them is similar8 to the distance-
based reordering model used by phrasal MT. The
other feature is the gap distance penalty which calcu-
lates the distance between the first word of a source
ceptX and the start of the left-most gap. This cost is
paid once for each Generate, Generate Identical and
Generate Source Only. For a source cept coverd by
indexes X1, . . . , Xn, we get the feature value gj =
X1?S, where S is the index of the left-most source
word where a gap starts.
8Let X1, . . . , Xn and Y1, . . . , Ym represent indexes of the
source words covered by the tuples tj and tj?1 respectively.
The distance between tj and tj?1 is given as dj = min(|Xk ?
Yl| ? 1) ?Xk ? {X1, . . . , Xn} and ? Yl ? {Y1, . . . , Ym}
Lexical Features We also use source-to-target
p(e|f) and target-to-source p(f |e) lexical transla-
tion probabilities. Our lexical features are standard
(Koehn et al, 2003). The estimation is motivated by
IBM Model-1. Given a tuple ti with source words
f = f1, f2, . . . , fn, target words e = e1, e2, . . . , em
and an alignment a between the source word posi-
tions x = 1, . . . , n and the target word positions
y = 1, . . . ,m, the lexical feature pw(f |e) is com-
puted as follows:
pw(f |e, a) =
n?
x=1
1
|{y : (x, y) ? a}|
?
?(x,y)?a
w(fx|ey)
pw(e|f, a) is computed in the same way.
5 Decoding
Our decoder for the new model performs a stack-
based search with a beam-search algorithm similar
to that used in Pharoah (Koehn, 2004a). Given an
input sentence F , it first extracts a set of match-
ing source-side cepts along with their n-best trans-
lations to form a tuple inventory. During hypoth-
esis expansion, the decoder picks a tuple from the
inventory and generates the sequence of operations
required for the translation with this tuple in light
of the previous hypothesis.9 The sequence of op-
erations may include translation (generate, continue
source cept etc.) and reordering (gap insertions,
jumps) operations. The decoder also calculates the
overall cost of the new hypothesis. Recombination
is performed on hypotheses having the same cov-
erage vector, monolingual language model context,
and operation model context. We do histogram-
based pruning, maintaining the 500 best hypotheses
for each stack.10
9A hypothesis maintains the index of the last source word
covered (j), the position of the right-most source word covered
so far (Z), the number of open gaps, the number of gaps so
far inserted, the previously generated operations, the generated
target string, and the accumulated values of all the features dis-
cussed in Section 4.
10We need a higher beam size to produce translation units
similar to the phrase-based systems. For example, the phrase-
based system can learn the phrase pair ?zum Beispiel ? for ex-
ample? and generate it in a single step placing it directly into the
stack two words to the right. Our system generates this example
with two separate tuple translations ?zum ? for? and ?Beispiel
? example? in two adjacent stacks. Because ?zum ? for? is not
a frequent translation unit, it will be ranked quite low in the first
stack until the tuple ?Beispiel ? example? appears in the second
stack. Koehn and his colleagues have repeatedly shown that in-
1050
Figure 3: Post-editing of Alignments (a) Initial (b) No
Target-Discontinuities (c) Final Alignments
6 Training
Training includes: (i) post-editing of the alignments,
(ii) generation of the operation sequence (iii) estima-
tion of the n-gram language models.
Our generative story does not handle target-side
discontinuities and unaligned target words. There-
fore we eliminate them from the training corpus in a
3-step process: If a source word is aligned with mul-
tiple target words which are not consecutive, first
the link to the least frequent target word is iden-
tified, and the group of links containing this word
is retained while the others are deleted. The in-
tuition here is to keep the alignments containing
content words (which are less frequent than func-
tional words). The new alignment has no target-
side discontinuities anymore, but might still contain
unaligned target words. For each unaligned target
word, we determine the (left or right) neighbour that
it appears more frequently with and align it with the
same source word as the neighbour. The result is
an alignment without target-side discontinuities and
unaligned target words. Figure 3 shows an illustra-
tive example of the process. The tuples in Figure 3c
are ?A ? U V?, ?B ? W X Y?, ?C ? NULL?, ?D ? Z?.
We apply Algorithm 1 to convert the preprocessed
aligned corpus into a sequence of translation opera-
tions. The resulting operation corpus contains one
sequence of operations per sentence pair.
In the final training step, the three language mod-
els are trained using the SRILM Toolkit. The oper-
ation model is estimated from the operation corpus.
The prior probability model is estimated from the
target side part of the bilingual corpus. The mono-
lingual language model is estimated from the target
side of the bilingual corpus and additional monolin-
gual data.
creasing the Moses stack size from 200 to 1000 does not have
a significant effect on translation into English, see (Koehn and
Haddow, 2009) and other shared task papers.
7 Experimental Setup
7.1 Data
We evaluated the system on three data sets with
German-to-English, Spanish-to-English and French-
to-English news translations, respectively. We used
data from the 4th version of the Europarl Corpus
and the News Commentary which was made avail-
able for the translation task of the Fourth Workshop
on Statistical Machine Translation.11 We use 200K
bilingual sentences, composed by concatenating the
entire news commentary (? 74K sentences) and Eu-
roparl (? 126K sentence), for the estimation of the
translation model. Word alignments were generated
with GIZA++ (Och and Ney, 2003), using the grow-
diag-final-and heuristic (Koehn et al, 2005). In or-
der to obtain the best alignment quality, the align-
ment task is performed on the entire parallel data and
not just on the training data we use. All data is low-
ercased, and we use the Moses tokenizer and recap-
italizer. Our monolingual language model is trained
on 500K sentences. These comprise 300K sentences
from the monolingual corpus (news commentary)
and 200K sentences from the target-side part of the
bilingual corpus. The latter part is also used to train
the prior probability model. The dev and test sets
are news-dev2009a and news-dev2009b which con-
tain 1025 and 1026 parallel sentences. The feature
weights are tuned with Z-MERT (Zaidan, 2009).
7.2 Results
Baseline: We compare our model to a recent ver-
sion of Moses (Koehn et al, 2007) using Koehn?s
training scripts and evaluate with BLEU (Papineni
et al, 2002). We provide Moses with the same ini-
tial alignments as we are using to train our system.12
We use the default parameters for Moses, and a 5-
gram English language model (the same as in our
system).
We compare two variants of our system. The first
system (Twno?rl) applies no hard reordering limit
and uses the distortion and gap distance penalty fea-
tures as soft constraints, allowing all possible re-
orderings. The second system (Twrl?6) uses no dis-
tortion and gap distance features, but applies a hard
constraint which limits reordering to no more than 6
11http://www.statmt.org/wmt09/translation-task.html
12We tried applying our post-processing to the alignments
provided to Moses and found that this made little difference.
1051
Source German Spanish French
Blno?rl 17.41 19.85 19.39
Blrl?6 18.57 21.67 20.84
Twno?rl 18.97 22.17 20.94
Twrl?6 19.03 21.88 20.72
Table 3: This Work(Tw) vs Moses (Bl), no-rl = No Re-
ordering Limit, rl-6 = Reordering limit 6
positions. Specifically, we do not extend hypotheses
that are more than 6 words apart from the first word
of the left-most gap during decoding. In this exper-
iment, we disallowed tuples which were discontin-
uous on the source side. We compare our systems
with two Moses systems as baseline, one using no
reordering limit (Blno?rl) and one using the default
distortion limit of 6 (Blrl?6).
Both of our systems (see Table 3) outperform
Moses on the German-to-English and Spanish-to-
English tasks and get comparable results for French-
to-English. Our best system (Twno?rl), which uses
no hard reordering limit, gives statistically signifi-
cant (p < 0.05)13 improvements over Moses (both
baselines) for the German-to-English and Spanish-
to-English translation task. The results for Moses
drop by more than a BLEU point without the re-
ordering limit (see Blno?rl in Table 3). All our
results are statistically significant over the baseline
Blno?rl for all the language pairs.
In another experiment, we tested our system also
with tuples which were discontinuous on the source
side. These gappy translation units neither improved
the performance of the system with hard reordering
limit (Twrl?6?asg) nor that of the system without
reordering limit (Twno?rl?asg) as Table 4 shows.
In an analysis of the output we found two reasons
for this result: (i) Using tuples with source gaps in-
creases the list of extracted n-best translation tuples
exponentially which makes the search problem even
more difficult. Table 5 shows the number of tuples
(with and without gaps) extracted when decoding
the test file with 10-best translations. (ii) The fu-
ture cost14 is poorly estimated in case of tuples with
gappy source cepts, causing search errors.
In an experiment, we deleted gappy tuples with
13We used Kevin Gimpel?s implementation of pairwise boot-
strap resampling (Koehn, 2004b), 1000 samples.
14The dynamic programming approach of calculating future
cost for bigger spans gives erroneous results when gappy cepts
can interleave. Details omitted due to space limitations.
Source German Spanish French
Twno?rl?asg 18.61 21.60 20.59
Twrl?6?asg 18.65 21.40 20.47
Twno?rl?hsg 18.91 21.93 20.87
Twrl?6?hsg 19.23 21.79 20.85
Table 4: Our Systems with Gappy Units, asg = All Gappy
Units, hsg = Heuristic for pruning Gappy Units
Source German Spanish French
Gaps 965515 1705156 1473798
No-Gaps 256992 313690 343220
Heuristic (hsg) 281618 346993 385869
Table 5: 10-best Translation Options With & Without
Gaps and using our Heuristic
a score (future cost estimate) lower than the sum of
the best scores of the parts. This heuristic removes
many useless discontinuous tuples. We found that
results improved (Twno?rl?hsg and Twrl?6?hsg in
Table 4) compared to the version using all gaps
(Twno?rl?asg, Twrl?6?asg), and are closer to the
results without discontinuous tuples (Twno?rl and
Twrl?6 in Table 3).
8 Sample Output
In this section we compare the output of our sys-
tems and Moses. Example 1 in Figure 4 shows
the powerful reordering mechanism of our model
which moves the English verb phrase ?do not want
to negotiate? to its correct position between the sub-
ject ?they? and the prepositional phrase ?about con-
crete figures?. Moses failed to produce the correct
word order in this example. Notice that although
our model is using smaller translation units ?nicht
? do not?, ?verhandlen ? negotiate? and ?wollen ?
want to?, it is able to memorize the phrase transla-
tion ?nicht verhandlen wollen ? do not want to ne-
gotiate? as a sequence of translation and reordering
operations. It learns the reordering of ?verhandlen ?
negotiate? and ?wollen ? want to? and also captures
dependencies across phrase boundaries.
Example 2 shows how our system without a re-
ordering limit moves the English translation ?vote?
of the German clause-final verb ?stimmen? across
about 20 English tokens to its correct position be-
hind the auxiliary ?would?.
Example 3 shows how the system with gappy tu-
ples translates a German sentence with the particle
verb ?kehrten...zuru?ck? using a single tuple (dashed
lines). Handling phenomena like particle verbs
1052
Figure 4: Sample Output Sentences
strongly motivates our treatment of source side gaps.
The system without gappy units happens to pro-
duce the same translation by translating ?kehrten? to
?returned? and deleting the particle ?zuru?ck? (solid
lines). This is surprising because the operation for
translating ?kehrten? to ?returned? and for deleting
the particle are too far apart to influence each other
in an n-gram model. Moses run on the same exam-
ple deletes the main verb (?kehrten?), an error that
we frequently observed in the output of Moses.
Our last example (Figure 5) shows that our model
learns idioms like ?meiner Meinung nach ? In my
opinion ,? and short phrases like ?gibt es ? there
are? showing its ability to memorize these ?phrasal?
translations, just like Moses.
9 Conclusion
We have presented a new model for statistical MT
which can be used as an alternative to phrase-
based translation. Similar to N-gram based MT,
it addresses three drawbacks of traditional phrasal
MT by better handling dependencies across phrase
boundaries, using source-side gaps, and solving the
phrasal segmentation problem. In contrast to N-
gram based MT, our model has a generative story
which tightly couples translation and reordering.
Furthermore it considers all possible reorderings un-
like N-gram systems that perform search only on
Figure 5: Learning Idioms
a limited number of pre-calculated orderings. Our
model is able to correctly reorder words across
large distances, and it memorizes frequent phrasal
translations including their reordering as probable
operations sequences. Our system outperformed
Moses on standard Spanish-to-English and German-
to-English tasks and achieved comparable results for
French-to-English. A binary version of the corpus
conversion algorithm and the decoder is available.15
Acknowledgments
The authors thank Fabienne Braune and the re-
viewers for their comments. Nadir Durrani was
funded by the Higher Education Commission (HEC)
of Pakistan. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732.
15http://www.ims.uni-stuttgart.de/?durrani/resources.html
1053
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franois Yvon. 2009. Gappy
translation units under left-to-right smt decoding. In
Proceedings of the meeting of the European Associa-
tion for Machine Translation (EAMT), pages 66?73,
Barcelona, Spain.
Josep Maria Crego and Franc?ois Yvon. 2010. Improv-
ing reordering with linguistically informed bilingual
n-grams. In Coling 2010: Posters, pages 197?205,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Josep M. Crego, Marta R. Costa-juss, Jos B. Mario,
and Jos A. R. Fonollosa. 2005a. Ngram-based ver-
sus phrasebased statistical machine translation. In In
Proceedings of the International Workshop on Spoken
Language Technology (IWSLT05, pages 177?184.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005b. Reordered search and unfolding tuples for
ngram-based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283?
289, Phuket, Thailand.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT 2009 shared task
with reordering and speed improvements to Moses.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 160?164, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description for
the 2005 iwslt speech translation evaluation. In Inter-
national Workshop on Spoken Language Translation
2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Demonstration Program,
Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests
for machine translation evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Zhifei Li, Chris Callison-burch, Chris Dyer, Juri Ganitke-
vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.
Thornton, Jonathan Weese, and Omar F. Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4):527?549.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
1054
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399?405,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Can Markov Models Over Minimal Translation Units Help Phrase-Based
SMT?
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Hieu Hoang Philipp Koehn
University of Edinburgh
hieu.hoang,pkoehn@inf.ed.ac.uk
Alexander Fraser Helmut Schmid
Ludwig Maximilian University Munich
fraser,schmid@cis.uni-muenchen.de
Abstract
The phrase-based and N-gram-based
SMT frameworks complement each other.
While the former is better able to memo-
rize, the latter provides a more principled
model that captures dependencies across
phrasal boundaries. Some work has been
done to combine insights from these two
frameworks. A recent successful attempt
showed the advantage of using phrase-
based search on top of an N-gram-based
model. We probe this question in the
reverse direction by investigating whether
integrating N-gram-based translation and
reordering models into a phrase-based
decoder helps overcome the problematic
phrasal independence assumption. A large
scale evaluation over 8 language pairs
shows that performance does significantly
improve.
1 Introduction
Phrase-based models (Koehn et al, 2003; Och
and Ney, 2004) learn local dependencies such as
reorderings, idiomatic collocations, deletions and
insertions by memorization. A fundamental draw-
back is that phrases are translated and reordered
independently of each other and contextual infor-
mation outside of phrasal boundaries is ignored.
The monolingual language model somewhat re-
duces this problem. However i) often the language
model cannot overcome the dispreference of the
translation model for nonlocal dependencies, ii)
source-side contextual dependencies are still ig-
nored and iii) generation of lexical translations and
reordering is separated.
The N-gram-based SMT framework addresses
these problems by learning Markov chains over se-
quences of minimal translation units (MTUs) also
known as tuples (Marin?o et al, 2006) or over op-
erations coupling lexical generation and reorder-
ing (Durrani et al, 2011). Because the mod-
els condition the MTU probabilities on the previ-
ous MTUs, they capture non-local dependencies
and both source and target contextual information
across phrasal boundaries.
In this paper we study the effect of integrating
tuple-based N-gram models (TSM) and operation-
based N-gram models (OSM) into the phrase-
based model in Moses, a state-of-the-art phrase-
based system. Rather than using POS-based
rewrite rules (Crego and Marin?o, 2006) to form
a search graph, we use the ability of the phrase-
based system to memorize larger translation units
to replicate the effect of source linearization as
done in the TSM model.
We also show that using phrase-based search
with MTU N-gram translation models helps to ad-
dress some of the search problems that are non-
trivial to handle when decoding with minimal
translation units. An important limitation of the
OSM N-gram model is that it does not handle un-
aligned or discontinuous target MTUs and requires
post-processing of the alignment to remove these.
Using phrases during search enabled us to make
novel changes to the OSM generative story (also
applicable to the TSM model) to handle unaligned
target words and to use target linearization to deal
with discontinuous target MTUs.
We performed an extensive evaluation, carrying
out translation experiments from French, Spanish,
Czech and Russian to English and in the opposite
direction. Our integration of the OSM model into
Moses and our modification of the OSM model to
deal with unaligned and discontinuous target to-
kens consistently improves BLEU scores over the
399
baseline system, and shows statistically significant
improvements in seven out of eight cases.
2 Previous Work
Several researchers have tried to combine the ideas
of phrase-based and N-gram-based SMT. Costa-
jussa` et al (2007) proposed a method for combin-
ing the two approaches by applying sentence level
reranking. Feng et al (2010) added a linearized
source-side language model in a phrase-based sys-
tem. Crego and Yvon (2010) modified the phrase-
based lexical reordering model of Tillman (2004)
for an N-gram-based system. Niehues et al (2011)
integrated a bilingual language model based on
surface word forms and POS tags into a phrase-
based system. Zhang et al (2013) explored multi-
ple decomposition structures for generating MTUs
in the task of lexical selection, and to rerank the
N-best candidate translations in the output of a
phrase-based. A drawback of the TSM model is
the assumption that source and target information
is generated monotonically. The process of re-
ordering is disconnected from lexical generation
which restricts the search to a small set of precom-
puted reorderings. Durrani et al (2011) addressed
this problem by coupling lexical generation and
reordering information into a single generative
process and enriching the N-gram models to learn
lexical reordering triggers. Durrani et al (2013)
showed that using larger phrasal units during de-
coding is superior to MTU-based decoding in an
N-gram-based system. However, they do not use
phrase-based models in their work, relying only
on the OSM model. This paper combines insights
from these recent pieces of work and show that
phrase-based search combined with N-gram-based
and phrase-based models in decoding is the over-
all best way to go. We integrate the two N-gram-
based models, TSM and OSM, into phrase-based
Moses and show that the translation quality is im-
proved by taking both translation and reordering
context into account. Other approaches that ex-
plored such models in syntax-based systems used
MTUs for sentence level reranking (Khalilov and
Fonollosa, 2009), in dependency translation mod-
els (Quirk and Menezes, 2006) and in target lan-
guage syntax systems (Vaswani et al, 2011).
3 Integration of N-gram Models
We now describe our integration of TSM and
OSM N-gram models into the phrase-based sys-
Figure 1: Example (a) Word Alignments (b) Un-
folded MTU Sequence (c) Operation Sequence (d)
Step-wise Generation
tem. Given a bilingual sentence pair (F,E) and
its alignment (A), we first identify minimal trans-
lation units (MTUs) from it. An MTU is defined
as a translation rule that cannot be broken down
any further. The MTUs extracted from Figure 1(a)
are A ? a,B ? b, C . . .H ? c1 and D ? d.
These units are then generated left-to-right in two
different ways, as we will describe next.
3.1 Tuple Sequence Model (TSM)
The TSM translation model assumes that MTUs
are generated monotonically. To achieve this ef-
fect, we enumerate the MTUs in the target left-
to-right order. This process is also called source
linearization or tuple unfolding. The resulting se-
quence of monotonic MTUs is shown in Figure
1(b). We then define a TSM model over this se-
quence (t1, t2, . . . , tJ ) as:
ptsm(F,E,A) =
J?
j=1
p(tj |tj?n+1, ..., tj?1)
where n indicates the amount of context used. A
4-gram Kneser-Ney smoothed language model is
trained with SRILM (Stolcke, 2002).
Search: In previous work, the search graph in
TSM N-gram SMT was not built dynamically
like in the phrase-based system, but instead con-
structed as a preprocessing step using POS-based
rewrite rules (learned when linearizing the source
side). We do not adopt this framework. We use
1We use . . . to denote discontinuous MTUs.
400
phrase-based search which builds up the decoding
graph dynamically and searches through all pos-
sible reorderings within a fixed window. During
decoding we use the phrase-internal alignments to
perform source linearization. For example, if dur-
ing decoding we would like to apply the phrase
pair ?C D H ? d c?, a combination of t3 and t4 in
Figure 1(b), then we extract the MTUs from this
phrase-pair and linearize the source to be in the
order of the target. We then compute the TSM
probability given the n ? 1 previous MTUs (in-
cluding MTUs occurring in the previous source
phrases). The idea is to replicate rewrite rules
with phrase-pairs to linearize the source. Previ-
ous work on N-gram-based models restricted the
length of the rewrite rules to be 7 or less POS tags.
We use phrases of length 6 and less.
3.2 Operation Sequence Model (OSM)
The OSM model represents a bilingual sentence
pair and its alignment through a sequence of oper-
ations that generate the aligned sentence pair. An
operation either generates source and target words
or it performs reordering by inserting gaps and
jumping forward and backward. The MTUs are
generated in the target left-to-right order just as in
the TSM model. However rather than linearizing
the source-side, reordering operations (gaps and
jumps) are used to handle crossing alignments.
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the sequence of operations shown in Fig-
ure 1(c). A step-wise generation of MTUs along
with reordering operations is shown in Figure 1(d).
We learn a Markov model over a sequence of oper-
ations (o1, o2, . . . , oJ ) that encapsulate MTUs and
reordering information which is defined as fol-
lows:
posm(F,E,A) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
A 9-gram Kneser-Ney smoothed language model
is trained with SRILM.3 By coupling reorder-
ing with lexical generation, each (translation or
reordering) decision conditions on n ? 1 previ-
ous (translation and reordering) decisions span-
ning across phrasal boundaries. The reordering
decisions therefore influence lexical selection and
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
3We also tried a 5-gram model, the performance de-
creased slightly in some cases.
vice versa. A heterogeneous mixture of translation
and reordering operations enables the OSM model
to memorize reordering patterns and lexicalized
triggers unlike the TSM model where translation
and reordering are modeled separately.
Search: We integrated the generative story of
the OSM model into the hypothesis extension pro-
cess of the phrase-based decoder. Each hypothesis
maintains the position of the source word covered
by the last generated MTU, the right-most source
word generated so far, the number of open gaps
and their relative indexes, etc. This information
is required to generate the operation sequence for
the MTUs in the hypothesized phrase-pair. After
the operation sequence is generated, we compute
its probability given the previous operations. We
define the main OSM feature, and borrow 4 sup-
portive features, the Gap, Open Gap, Gap-width
and Deletion penalties (Durrani et al, 2011).
3.3 Problem: Target Discontinuity and
Unaligned Words
Two issues that we have ignored so far are the han-
dling of MTUs which have discontinuous targets,
and the handling of unaligned target words. Both
TSM and OSM N-gram models generate MTUs
linearly in left-to-right order. This assumption be-
comes problematic in the cases of MTUs that have
target-side discontinuities (See Figure 2(a)). The
MTU A? g . . . a can not be generated because of
the intervening MTUs B ? b, C . . .H ? c and
D ? d. In the original TSM model, such cases are
dealt with by merging all the intervening MTUs
to form a bigger unit t?1 in Figure 2(c). A solu-
tion that uses split-rules is proposed by Crego and
Yvon (2009) but has not been adopted in Ncode
(Crego et al, 2011), the state-of-the-art TSM N-
gram system. Durrani et al (2011) dealt with
this problem by applying a post-processing (PP)
heuristic that modifies the alignments to remove
such cases. When a source word is aligned to a
discontinuous target-cept, first the link to the least
frequent target word is identified, and the group
of links containing this word is retained while the
others are deleted. The alignment in Figure 2(a),
for example, is transformed to that in Figure 2(b).
This allows OSM to extract the intervening MTUs
t2 . . . t5 (Figure 2(c)). Note that this problem does
not exist when dealing with source-side disconti-
nuities: the TSM model linearizes discontinuous
source-side MTUs such as C . . .H ? c. The
401
Figure 2: Example (a) Original Alignments (b)
Post-Processed Alignments (c) Extracted MTUs ?
t?1 . . . t?3 (from (a)) and t1 . . . t7 (from (b))
OSM model deals with such cases through Insert
Gap and Continue Cept operations.
The second problem is the unaligned target-side
MTUs such as ? ? f in Figure 2(a). Inserting
target-side words ?spuriously? during decoding is
a non-trival problem because there is no evidence
of when to hypothesize such words. These cases
are dealt with in N-gram-based SMT by merging
such MTUs to the MTU on the left or right based
on attachment counts (Durrani et al, 2011), lexical
probabilities obtained from IBM Model 1 (Marin?o
et al, 2006), or POS entropy (Gispert and Marin?o,
2006). Notice how ?? f (Figure 2(a)) is merged
with the neighboring MTU E ? e to form a new
MTU E ? ef (Figure 2 (c)). We initially used the
post-editing heuristic (PP) as defined by Durrani et
al. (2011) for both TSM and OSM N-gram mod-
els, but found that it lowers the translation quality
(See Row 2 in Table 2) in some language pairs.
3.4 Solution: Insertion and Linearization
To deal with these problems, we made novel modi-
fications to the generative story of the OSM model.
Rather than merging the unaligned target MTU
such as ? ? f , to its right or left MTU, we gen-
erate it through a new Generate Target Only (f)
operation. Orthogonal to its counterpart Generate
Source Only (I) operation (as used for MTU t7 in
Figure 2 (c)), this operation is generated as soon
as the MTU containing its previous target word
is generated. In Figure 2(a), ? ? f is generated
immediately after MTU E ? e is generated. In
a sequence of unaligned source and target MTUs,
unaligned source MTUs are generated before the
unaligned target MTUs. We do not modify the de-
coder to arbitrarily generate unaligned MTUs but
hypothesize these only when they appear within
an extracted phrase-pair. The constraint provided
by the phrase-based search makes the Generate
Target Only operation tractable. Using phrase-
based search therefore helps addressing some of
the problems that exist in the decoding framework
of N-gram SMT.
The remaining problem is the discontinuous tar-
get MTUs such as A? g . . . a in Figure 2(a). We
handle this with target linearization similar to the
TSM source linearization. We collapse the target
words g and a in the MTU A ? g . . . a to occur
consecutively when generating the operation se-
quence. The conversion algorithm that generates
the operations thinks that g and a occurred adja-
cently. During decoding we use the phrasal align-
ments to linearize such MTUs within a phrasal
unit. This linearization is done only to compute
the OSM feature. Other features in the phrase-
based system (e.g., language model) work with the
target string in its original order. Notice again how
memorizing larger translation units using phrases
helps us reproduce such patterns. This is achieved
in the tuple N-gram model by using POS-based
split and rewrite rules.
4 Evaluation
Corpus: We ran experiments with data made
available for the translation task of the Eighth
Workshop on Statistical Machine Translation. The
sizes of bitext used for the estimation of translation
and monolingual language models are reported in
Table 1. All data is true-cased.
Pair Parallel Monolingual Lang
fr?en ?39 M ?91 M fr
cs?en ?15.6 M ?43.4 M cs
es?en ?15.2 M ?65.7 M es
ru?en ?2 M ?21.7 M ru
?287.3 M en
Table 1: Number of Sentences (in Millions) used
for Training
We follow the approach of Schwenk and Koehn
(2008) and trained domain-specific language mod-
els separately and then linearly interpolated them
using SRILM with weights optimized on the held-
out dev-set. We concatenated the news-test sets
from four years (2008-2011) to obtain a large dev-
setin order to obtain more stable weights (Koehn
and Haddow, 2012). For Russian-English and
English-Russian language pairs, we divided the
tuning-set news-test 2012 into two halves and used
402
No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru
1. Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88
2. 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05
3. 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96
4. 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22
5. 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25
Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline
the first half for tuning and second for test. We test
our systems on news-test 2012. We tune with the
k-best batch MIRA algorithm (Cherry and Foster,
2012).
Moses Baseline: We trained a Moses system
(Koehn et al, 2007) with the following settings:
maximum sentence length 80, grow-diag-final-
and symmetrization of GIZA++ alignments, an
interpolated Kneser-Ney smoothed 5-gram lan-
guage model with KenLM (Heafield, 2011) used at
runtime, msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al, 2012), distortion limit of 6, 100-best
translation options, minimum bayes-risk decoding
(Kumar and Byrne, 2004), cube-pruning (Huang
and Chiang, 2007) and the no-reordering-over-
punctuation heuristic.
Results: Table 2 shows uncased BLEU scores
(Papineni et al, 2002) on the test set. Row 2 (+pp)
shows that the post-editing of alignments to re-
move unaligned and discontinuous target MTUs
decreases the performance in the case of ru-en, cs-
en and en-fr. Row 3 (+pp+tsm) shows that our in-
tegration of the TSM model slightly improves the
BLEU scores for en-fr, and es-en. Results drop
in ru-en and en-ru. Row 4 (+pp+osm) shows that
the OSM model consistently improves the BLEU
scores over the Baseline systems (Row 1) giving
significant improvements in half the cases. The
only result that is lower than the baseline system
is that of the ru-en experiment, because OSM is
built with PP alignments which particularly hurt
the performance for ru-en. Finally Row 5 (+osm*)
shows that our modifications to the OSM model
(Section 3.4) give the best result ranging from
[0.24?0.65] with statistically significant improve-
ments in seven out of eight cases. It also shows im-
provements over Row 4 (+pp+osm) even in some
cases where the PP heuristic doesn?t hurt. The
largest gains are obtained in the ru-en translation
task (where the PP heuristic inflicted maximum
damage).
5 Conclusion and Future Work
We have addressed the problem of the indepen-
dence assumption in PBSMT by integrating N-
gram-based models inside a phrase-based system
using a log-linear framework. We try to replicate
the effect of rewrite and split rules as used in the
TSM model through phrasal alignments. We pre-
sented a novel extension of the OSM model to
handle unaligned and discontinuous target MTUs
in the OSM model. Phrase-based search helps us
to address these problems that are non-trivial to
handle in the decoding frameworks of the N-gram-
based models. We tested our extentions and modi-
fications by evaluating against a competitive base-
line system over 8 language pairs. Our integra-
tion of TSM shows small improvements in a few
cases. The OSM model which takes both reorder-
ing and lexical context into consideration consis-
tently improves the performance of the baseline
system. Our modification to the OSM model pro-
duces the best results giving significant improve-
ments in most cases. Although our modifications
to the OSM model enables discontinuous MTUs,
we did not fully utilize these during decoding, as
Moses only uses continous phrases. The discon-
tinuous MTUs that span beyond a phrasal length
of 6 words are therefore never hypothesized. We
would like to explore this further by extending the
search to use discontinuous phrases (Galley and
Manning, 2010).
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment n ? 287658. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732. This
publication only reflects the authors views.
403
References
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Marta R. Costa-jussa`, Josep M. Crego, David Vilar,
Jose? A.R. Fonollosa, Jose? B. Marin?o, and Her-
mann Ney. 2007. Analysis and System Combina-
tion of Phrase- and N-Gram-Based Statistical Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 137?140, Rochester, New York, April.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
Statistical MT by Coupling Reordering and Decod-
ing. Machine Translation, 20(3):199?215.
Josep M. Crego and Franc?ois Yvon. 2009. Gappy
Translation Units under Left-to-Right SMT Decod-
ing. In Proceedings of the Meeting of the European
Association for Machine Translation (EAMT), pages
66?73, Barcelona, Spain.
Josep M. Crego and Franc?ois Yvon. 2010. Improv-
ing Reordering with Linguistically Informed Bilin-
gual N-Grams. In Coling 2010: Posters, pages 197?
205, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. Ncode: an Open Source Bilingual N-gram
SMT Toolkit. The Prague Bulletin of Mathematical
Linguistics, 96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statis-
tical Machine Translation. In Conference of the As-
sociation for Machine Translation in the Americas
2010, Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2010.
Accurate Non-Hierarchical Phrase-Based Transla-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 966?974, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Adria` Gispert and Jose? B. Marin?o. 2006. Linguis-
tic Tuple Segmentation in N-Gram-Based Statistical
Machine Translation. In INTERSPEECH.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Maxim Khalilov and Jose? A. R. Fonollosa. 2009. N-
Gram-Based Statistical Machine Translation Versus
Syntax Augmented Machine Translation: Compar-
ison and System Combination. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 424?432, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007 Demonstrations, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In HLT-NAACL, pages 169?176.
404
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 198?206, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Morristown, NJ, USA.
Christopher Quirk and Arul Menezes. 2006. Do We
Need Phrases? Challenging the Conventional Wis-
dom in Statistical Machine Translation. In HLT-
NAACL.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillman. 2004. A Unigram Orienta-
tion Model for Statistical Machine Translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-
to-String Translation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 856?864, Portland, Oregon, USA, June.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond Left-to-Right: Multi-
ple Decomposition Structures for SMT. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Atlanta, Georgia,
USA, June. Association for Computational Linguis-
tics.
405
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114?121,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Edinburgh?s Machine Translation Systems for European Language Pairs
Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn
School of Informatics
University of Edinburgh
Scotland, United Kingdom
{dnadir,bhaddow,kheafiel,pkoehn}@inf.ed.ac.uk
Abstract
We validated various novel and recently
proposed methods for statistical machine
translation on 10 language pairs, using
large data resources. We saw gains
from optimizing parameters, training with
sparse features, the operation sequence
model, and domain adaptation techniques.
We also report on utilizing a huge lan-
guage model trained on 126 billion tokens.
The annual machine translation evaluation cam-
paign for European languages organized around
the ACL Workshop on Statistical Machine Trans-
lation offers the opportunity to test recent advance-
ments in machine translation in large data condi-
tion across several diverse language pairs.
Building on our own developments and external
contributions to the Moses open source toolkit, we
carried out extensive experiments that, by early in-
dications, led to a strong showing in the evaluation
campaign.
We would like to stress especially two contri-
butions: the use of the new operation sequence
model (Section 3) within Moses, and ? in a sepa-
rate unconstraint track submission ? the use of a
huge language model trained on 126 billion tokens
with a new training tool (Section 4).
1 Initial System Development
We start with systems (Haddow and Koehn, 2012)
that we developed for the 2012 Workshop on
Statistical Machine Translation (Callison-Burch
et al, 2012). The notable features of these systems
are:
? Moses phrase-based models with mostly de-
fault settings
? training on all available parallel data, includ-
ing the large UN parallel data, the French-
English 109 parallel data and the LDC Giga-
word data
? very large tuning set consisting of the test sets
from 2008-2010, with a total of 7,567 sen-
tences per language
? German?English with syntactic pre-
reordering (Collins et al, 2005), compound
splitting (Koehn and Knight, 2003) and use
of factored representation for a POS target
sequence model (Koehn and Hoang, 2007)
? English?German with morphological target
sequence model
Note that while our final 2012 systems in-
cluded subsampling of training data with modified
Moore-Lewis filtering (Axelrod et al, 2011), we
did not use such filtering at the starting point of
our development. We will report on such filtering
in Section 2.
Moreover, our system development initially
used the WMT 2012 data condition, since it took
place throughout 2012, and we switched to WMT
2013 training data at a later stage. In this sec-
tion, we report cased BLEU scores (Papineni et al,
2001) on newstest2011.
1.1 Factored Backoff (German?English)
We have consistently used factored models in past
WMT systems for the German?English language
pairs to include POS and morphological target se-
quence models. But we did not use the factored
decomposition of translation options into multi-
ple mapping steps, since this usually lead to much
slower systems with usually worse results.
A good place, however, for factored decompo-
sition is the handling of rare and unknown source
words which have more frequent morphological
variants (Koehn and Haddow, 2012a). Here, we
used only factored backoff for unknown words,
giving gains in BLEU of +.12 for German?English.
1.2 Tuning with k-best MIRA
In preparation for training with sparse features, we
moved away from MERT which is known to fall
114
apart with many more than a couple of dozen fea-
tures. Instead, we used k-best MIRA (Cherry and
Foster, 2012). For the different language pairs, we
saw improvements in BLEU of -.05 to +.39, with an
average of +.09. There was only a minimal change
in the length ratio (Table 1)
MERT k-best MIRA ?
de-en 22.11 (1.010) 22.10 (1.008) ?.01 (+.002)
fr-en 30.00 (1.023) 30.11 (1.026) +.11 (?.003)
es-en 30.42 (1.021) 30.63 (1.020) +.21 (?.001)
cs-en 25.54 (1.022) 25.49 (1.024) ?.05 (?.002)
en-de 16.08 (0.995) 16.04 (1.001) ?.04 (?.006)
en-fr 29.26 (0.980) 29.65 (0.982) +.39 (?.002)
en-es 31.92 (0.985) 31.95 (0.985) +.03 (?.000)
en-cs 17.38 (0.967) 17.42 (0.974) +.04 (?.007)
avg ? ? +.09
Table 1: Tuning with k-best MIRA instead of MERT
(cased BLEU scores with length ratio)
1.3 Translation Table Smoothing with
Kneser-Ney Discounting
Previously, we smoothed counts for the phrasal
conditional probability distributions in the trans-
lation model with Good Turing discounting. We
explored the use of Kneser-Ney discounting, but
results are mixed (no difference on average, see
Table 2), so we did not pursue this further.
Good Turing Kneser Ney ?
de-en 22.10 22.15 +.05
fr-en 30.11 30.13 +.02
es-en 30.63 30.64 +.01
cs-en 25.49 25.56 +.07
en-de 16.04 15.93 ?.11
en-fr 29.65 29.75 +.10
en-es 31.95 31.98 +.03
en-cs 17.42 17.26 ?.16
avg ? ? ?.00
Table 2: Translation model smoothing with Kneser-Ney
1.4 Sparse Features
A significant extension of the Moses system over
the last couple of years was the support for large
numbers of sparse features. This year, we tested
this capability on our big WMT systems. First, we
used features proposed by Chiang et al (2009):
? phrase pair count bin features (bins 1, 2, 3,
4?5, 6?9, 10+)
? target word insertion features
? source word deletion features
? word translation features
? phrase length feature (source, target, both)
The lexical features were restricted to the 50 most
frequent words. All these features together only
gave minor improvements (Table 3).
baseline sparse ?
de-en 22.10 22.02 ?.08
fr-en 30.11 30.24 +.13
es-en 30.63 30.61 ?.02
cs-en 25.49 25.49 ?.00
en-de 16.04 15.93 ?.09
en-fr 29.65 29.81 +.16
en-es 31.95 32.02 +.07
en-cs 17.42 17.28 ?.14
avg ? ? +.04
Table 3: Sparse features
We also explored domain features in the sparse
feature framework, in three different variations.
Assume that we have three domains, and a phrase
pair occurs in domain A 15 times, in domain B 5
times, and in domain C never.
We compute three types of domain features:
? binary indicator, if phrase-pairs occurs in do-
main (example: indA = 1, indB = 1, indC = 0)
? ratio how frequent the phrase pairs occurs in
domain (example: ratioA = 1515+5 = .75, ratioB =
5
15+5 = .25, ratioC = 0)
? subset of domains in which phrase pair oc-
curs (example: subsetAB = 1, other subsets 0)
We tested all three feature types, and found
the biggest gain with the domain indicator feature
(+.11, Table 4). Note that we define as domain the
different corpora (Europarl, etc.). The number of
domains ranges from 2 to 9 (see column #d).1
#d base. indicator ratio subset
de-en 2 22.10 22.14 +.04 22.07 ?.03 22.12 +.02
fr-en 4 30.11 30.34 +.23 30.29 +.18 30.15 +.04
es-en 3 30.63 30.88 +.25 30.64 +.01 30.82 +.19
cs-en 9 25.49 25.58 +.09 25.58 +.09 25.46 ?.03
en-de 2 16.122 16.14 +.02 15.96 ?.16 16.01 ?.11
en-fr 4 29.65 29.75 +.10 29.71 +.05 29.70 +.05
en-es 3 31.95 32.06 +.11 32.13 +.18 32.02 +.07
en-cs 9 17.42 17.45 +.03 17.35 ?.07 17.44 +.02
avg. - ? +.11 +.03 +.03
Table 4: Sparse domain features
When combining the domain features and the
other sparse features, we see roughly additive
gains (Table 5). We use the domain indicator fea-
ture and the other sparse features in subsequent ex-
periments.
1In the final experiments on the 2013 data condition, one
domain (commoncrawl) was added for all language pairs.
115
baseline indicator ratio subset
de-en 22.10 22.18 +.08 22.10 ?.00 22.16 +.06
fr-en 30.11 30.41 +.30 30.49 +.38 30.36 +.25
es-en 30.63 30.75 +.12 30.56 ?.07 30.85 +.22
cs-en 25.49 25.56 +.07 25.63 +.14 25.43 ?.06
en-de 16.12 15.95 ?.17 15.96 ?.16 16.05 ?.07
en-fr 29.65 29.96 +.31 29.88 +.23 29.92 +.27
en-es 31.95 32.12 +.17 32.16 +.21 32.08 +.23
en-cs 17.42 17.38 ?.04 17.35 ?.07 17.40 ?.02
avg. ? +.11 +.09 +.11
Table 5: Combining domain and other sparse features
1.5 Tuning Settings
Given the opportunity to explore the parameter
tuning of models with sparse features across many
language pairs, we investigated a number of set-
tings. We expect tuning to work better with more
iterations, longer n-best lists and bigger cube prun-
ing pop limits. Our baseline settings are 10 itera-
tions with 100-best lists (accumulating) and a pop
limit of 1000 for tuning and 5000 for testing.
base 25 it. 25it+1k-best 25it+pop5k
de-en 22.18 22.16 ?.02 22.14 ?.04 22.17 ?.01
fr-en 30.41 30.40 ?.01 30.44 +.03 30.49 +.08
es-en 30.75 30.91 +.16 30.86 +.11 30.81 +.06
cs-en 25.56 25.60 +.04 25.64 +.08 25.56 ?.00
en-de 15.96 15.99 +.03 16.05 +.09 15.96 ?.00
en-fr 29.96 29.90 ?.06 29.95 ?.01 29.92 ?.04
en-es 32.12 32.17 +.05 32.11 ?.01 32.19 +.07
en-cs 17.38 17.43 +.05 17.50 +.12 17.38 ?.00
avg ? +.03 +.05 +.02
Table 6: Tuning settings (number of iterations, size of n-best
list, and cube pruning pop limit)
Results support running tuning for 25 iterations
but we see no gains for 5000 pops. There is ev-
idence that an n-best list size of 1000 is better in
tuning but we did not adopt this since these large
lists take up a lot of disk space and slow down the
MIRA optimization step (Table 6).
1.6 Smaller Phrases
Given the very large corpus sizes (up to a billion
words of parallel data for French?English), the
size of translation model and lexicalized reorder-
ing model becomes a challenge. Hence, we want
to examine if restriction to smaller phrases is fea-
sible without loss in translation quality. Results
in Table 7 suggest that a maximum phrase length
of 5 gives almost identical results, and only with
a phrase length limit of 4 significant losses occur.
We adopted the limit of 5.
max 7 max 6 max 5 max 4
de-en 22.16 22.03 ?.13 22.05 ?.11 22.17 +.01
fr-en 30.40 30.30 ?.10 30.39 ?.01 30.23 ?.17
es-en 30.91 30.80 ?.09 30.86 ?.05 30.81 ?.10
cs-en 25.60 25.55 ?.05 25.53 ?.07 25.48 ?.12
en-de 15.99 15.94 ?.05 15.97 ?.02 16.03 +.04
en-fr 29.90 29.97 +.07 29.89 ?.01 29.77 ?.13
en-es 32.17 32.13 ?.04 32.27 +.10 31.93 ?.24
en-cs 17.43 17.46 +.03 17.41 ?.02 17.41 ?.02
avg ? ?.05 ?.03 ?.09
Table 7: Maximum phrase length, reduced from baseline
1.7 Unpruned Language Models
Previously, we trained 5-gram language models
using the default settings of the SRILM toolkit in
terms of singleton pruning. Thus, training throws
out all singletons n-grams of order 3 and higher.
We explored whether unpruned language models
could give better performance, even if we are only
able to train 4-gram models due to memory con-
straints. At the time, we were not able to build un-
pruned 4-gram language models for English, but
for the other language pairs we did see improve-
ments of -.07 to +.13 (Table 8). We adopted such
models for these language pairs.
5g pruned 4g unpruned ?
en-fr 29.89 29.83 ?.07
en-es 32.27 32.34 +.07
en-cs 17.41 17.54 +.13
Table 8: Language models without singleton pruning
1.8 Translations per Input Phrase
Finally, we explored one more parameter: the limit
on how many translation options are considered
per input phrase. The default for this setting is 20.
However, our experiments (Table 9) show that we
can get better results with a translation table limit
of 100, so we adopted this.
ttl 20 ttl 30 ttl 50 ttl 100
de-en 21.05 +.06 +.09 +.01
fr-en 30.39 ?.02 +.05 +.07
es-en 30.86 ?.00 ?.03 ?.07
cs-en 25.53 +.24 +.13 +.20
en-de 15.97 +.03 +.07 +.11
en-fr 29.83 +.14 +.19 +.13
en-es 32.34 +.08 +.10 +.07
en-cs 17.54 ?.05 ?.02 +.01
avg ? +.06 +.07 +.07
Table 9: Maximal number translations per input phrase
1.9 Other Experiments
We explored a number of other settings and fea-
tures, but did not observe any gains.
116
? Using HMM alignment instead of IBM
Model 4 leads to losses of ?.01 to ?.27.
? An earlier check of modified Moore?Lewis
filtering (see also below in Section 3) gave
very inconsistent results.
? Filtering the phrase table with significance
filtering (Johnson et al, 2007) leads to losses
of ?.19 to ?.63.
? Throwing out phrase pairs with direct transla-
tion probability ?(e?|f?) of less than 10?5 has
almost no effect.
? Double-checking the contribution of the
sparse lexical features in the final setup, we
observe an average losses of ?.07 when drop-
ping these features.
? For the German?English language pairs we
saw some benefits to using sparse lexical fea-
tures over POS tags instead of words, so we
used this in the final system.
1.10 Summary
We adopted a number of changes that improved
our baseline system by an average of +.30, see Ta-
ble 10 for a breakdown.
avg. method
+.01 factored backoff
+.09 kbest MIRA
+.11 sparse features and domain indicator
+.03 tuning with 25 iterations
?.03 maximum phrase length 5
+.02 unpruned 4-gram LM
+.07 translation table limit 100
+.30 total
Table 10: Summary of impact of changes
Minor improvements that we did not adopt was
avoiding reducing maximum phrase length to 5
(average +.03) and tuning with 1000-best lists
(+.02).
The improvements differed significantly by lan-
guage pair, as detailed in Table 11, with the
biggest gains for English?French (+.70), no gain
for English?German and no gain for English?
German.
1.11 New Data
The final experiment of the initial system devel-
opment phase was to train the systems on the new
data, adding newstest2011 to the tuning set (now
10,068 sentences). Table 12 reports the gains on
newstest2012 due to added data, indicating very
clearly that valuable new data resources became
available this year.
baseline improved ?
de-en 21.99 22.09 +.10
fr-en 30.00 30.46 +.46
es-en 30.42 30.79 +.37
cs-en 25.54 25.73 +.19
en-de 16.08 16.08 ?.00
en-fr 29.26 29.96 +.70
en-es 31.92 32.41 +.49
en-cs 17.38 17.55 +.17
Table 11: Overall improvements per language pair
WMT 2012 WMT 2013 ?
de-en 23.11 24.01 +0.90
fr-en 29.25 30.77 +1.52
es-en 32.80 33.99 +1.19
cs-en 22.53 22.86 +0.33
ru-en ? 31.67 ?
en-de 16.78 17.95 +1.17
en-fr 27.92 28.76 +0.84
en-es 33.41 34.00 +0.59
en-cs 15.51 15.78 +0.27
en-ru ? 23.78 ?
Table 12: Training with new data (newstest2012 scores)
2 Domain Adaptation Techniques
We explored two additional domain adaptation
techniques: phrase table interpolation and modi-
fied Moore-Lewis filtering.
2.1 Phrase Table Interpolation
We experimented with phrase-table interpolation
using perplexity minimisation (Foster et al, 2010;
Sennrich, 2012). In particular, we used the im-
plementation released with Sennrich (2012) and
available in Moses, comparing both the naive and
modified interpolation methods from that paper.
For each language pair, we took the alignments
created from all the data concatenated, built sepa-
rate phrase tables from each of the individual cor-
pora, and interpolated using each method. The re-
sults are shown in Table 13
baseline naive modified
fr-en 30.77 30.63 ?.14 ?
es-en? 33.98 33.83 ?.15 34.03 +.05
cs-en? 23.19 22.77 ?.42 23.03 ?.17
ru-en 31.67 31.42 ?.25 31.59 ?.08
en-fr 28.76 28.88 +.12 ?
en-es 34.00 34.07 +.07 34.31 +.31
en-cs 15.78 15.88 +.10 15.87 +.09
en-ru 23.78 23.84 +.06 23.68 ?.10
Table 13: Comparison of phrase-table interpolation (two
methods) with baseline (on newstest2012). The baselines are
as Table 12 except for the starred rows where tuning with
PRO was found to be better. The modified interpolation was
not possible in fr?en as it uses to much RAM.
The results from the phrase-table interpolation
are quite mixed, and we only used the technique
117
for the final system in en-es. An interpolation
based on PRO has recently been shown (Haddow,
2013) to improve on perplexity minimisation is
some cases, but the current implementation of this
method is limited to 2 phrase-tables, so we did not
use it in this evaluation.
2.2 Modified Moore-Lewis Filtering
In last year?s evaluation (Koehn and Haddow,
2012b) we had some success with modified
Moore-Lewis filtering (Moore and Lewis, 2010;
Axelrod et al, 2011) of the training data. This
year we conducted experiments in most of the lan-
guage pairs using MML filtering, and also exper-
imented using instance weighting (Mansour and
Ney, 2012) using the (exponential of) the MML
weights. The results are show in Table 14
base MML Inst. Wt Inst. Wt
line 20% (scale)
fr-en 30.77 ? ? ?
es-en? 33.98 34.26 +.28 33.85 ?.13 33.98 ?.00
cs-en? 23.19 22.62 ?.57 23.17 ?.02 23.13 ?.06
ru-en 31.67 31.58 ?.09 31.57 ?.10 31.62 ?.05
en-fr 28.67 28.74 +.07 28.81 +.17 28.63 ?.04
en-es 34.00 34.07 +.07 34.27 +.27 34.03 +.03
en-cs 15.78 15.37 ?.41 15.87 +.09 15.89 +.11
en-ru 23.78 22.90 ?.88 23.82 +.05 23.72 ?.06
Table 14: Comparison of MML filtering and weighting with
baseline. The MML uses monolingual news as in-domain,
and selects from all training data after alignment.The weight-
ing uses the MML weights, optionally downscaled by 10,
then exponentiated. Baselines are as Table 13.
As with phrase-table interpolation, MML filter-
ing and weighting shows a very mixed picture, and
not the consistent improvements these techniques
offer on IWSLT data. In the final systems, we used
MML filtering only for es-en.
3 Operation Sequence Model (OSM)
We enhanced the phrase segmentation and re-
ordering mechanism by integrating OSM: an op-
eration sequence N-gram-based translation and re-
ordering model (Durrani et al, 2011) into the
Moses phrase-based decoder. The model is based
on minimal translation units (MTUs) and Markov
chains over sequences of operations. An opera-
tion can be (a) to jointly generate a bi-language
MTU, composed from source and target words, or
(b) to perform reordering by inserting gaps and do-
ing jumps.
Model: Given a bilingual sentence pair <
F,E > and its alignment A, we transform it to
Figure 1: Bilingual Sentence with Alignments
sequence of operations (o1, o2, . . . , oJ ) and learn
a Markov model over this sequence as:
posm(F,E,A) = p(oJ1 ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision condi-
tions on n ? 1 previous (translation and reorder-
ing) decisions spanning across phrasal boundaries
thus overcoming the problematic phrasal indepen-
dence assumption in the phrase-based model. In
the OSM model, the reordering decisions influ-
ence lexical selection and vice versa. Lexical gen-
eration is strongly coupled with reordering thus
improving the overall reordering mechanism.
We used the modified version of the OSM
model (Durrani et al, 2013b) that addition-
ally handles discontinuous and unaligned target
MTUs3. We borrow 4 count-based supportive fea-
tures, the Gap, Open Gap, Gap-width and Dele-
tion penalties from Durrani et al (2011).
Training: During training, each bilingual sen-
tence pair is deterministically converted to a
unique sequence of operations. Please refer to
Durrani et al (2011) for a list of operations and
the conversion algorithm and see Figure 1 and Ta-
ble 15 for a sample bilingual sentence pair and
its step-wise conversion into a sequence of oper-
ation. A 9-gram Kneser-Ney smoothed operation
sequence model is trained with SRILM.
Search: Although the OSM model is based on
minimal units, phrase-based search on top of OSM
model was found to be superior to the MTU-based
decoding in Durrani et al (2013a). Following this
framework allows us to use OSM model in tandem
with phrase-based models. We integrated the gen-
erative story of the OSM model into the hypothe-
sis extension of the phrase-based Moses decoder.
Please refer to (Durrani et al, 2013b) for details.
Results: Table 16 shows case-sensitive BLEU
scores on newstest2012 and newstest2013 for fi-
3In the original OSM model these are removed from the
alignments through a post-processing heuristic which hurts in
some language pairs. See Durrani et al (2013b) for detailed
experiments.
118
Operation Sequence Generation
Generate(Ich, I) Ich ?
I
Generate Target Only (do) Ich ?
I do
Insert Gap Ich nicht ?
Generate (nicht, not) I do not
Jump Back (1) Ich gehe ? nicht
Generate (gehe, go) I do not go
Generate Source Only (ja) Ich gehe ja ? nicht
I do not go
Jump Forward Ich gehe ja nicht ?
I do not go
Generate (zum, to the) . . . gehe ja nicht zum ?
. . . not go to the
Generate (haus, house) . . . ja nicht zum haus ?
. . . go to the house
Table 15: Step-wise Generation of Figure 1
LP Baseline +OSM
newstest 2012 2013 2012 2013
de-en 23.85 26.54 24.11 +.26 26.83 +.29
fr-en 30.77 31.09 30.96 +.19 31.46 +.37
es-en 34.02 30.04 34.51 +.49 30.94 +.90
cs-en 22.70 25.70 23.03 +.33 25.79 +.09
ru-en 31.87 24.00 32.33 +.46 24.33 +.33
en-de 17.95 20.06 18.02 +.07 20.26 +.20
en-fr 28.76 30.03 29.36 +.60 30.39 +.36
en-es 33.87 29.66 34.44 +.57 30.10 +.44
en-cs 15.81 18.35 16.16 +.35 18.62 +.27
en-ru 23.75 18.44 24.05 +.30 18.84 +.40
Table 16: Results using the OSM Feature
nal systems from Section 1 and these systems aug-
mented with the operation sequence model. The
model gives gains for all language pairs (BLEU
+.09 to +.90, average +.37, on newstest2013).
4 Huge Language Models
To overcome the memory limitations of SRILM,
we implemented modified Kneser-Ney (Kneser
and Ney, 1995; Chen and Goodman, 1998)
smoothing from scratch using disk-based stream-
ing algorithms. This open-source4 tool is de-
scribed fully by Heafield et al (2013). We used it
to estimate an unpruned 5?gram language model
on web pages from ClueWeb09.5 The corpus was
preprocessed by removing spam (Cormack et al,
2011), selecting English documents, splitting sen-
tences, deduplicating, tokenizing, and truecasing.
Estimation on the remaining 126 billion tokens
took 2.8 days on a single machine with 140 GB
RAM (of which 123 GB was used at peak) and six
hard drives in a RAID5 configuration. Statistics
about the resulting model are shown in Table 17.
4http://kheafield.com/code/
5http://lemurproject.org/clueweb09/
1 2 3 4 5
393m 3,775m 17,629m 39,919m 59,794m
Table 17: Counts of unique n-grams (m for millions) for the
5 orders in the unconstrained language model
The large language model was then quantized
to 10 bits and compressed to 643 GB with KenLM
(Heafield, 2011), loaded onto a machine with 1
TB RAM, and used as an additional feature in
unconstrained French?English, Spanish?English,
and Czech?English submissions. This additional
language model is the only difference between our
final constrained and unconstrained submissions;
no additional parallel data was used. Results are
shown in Table 18. Improvement from large lan-
guage models is not a new result (Brants et al,
2007); the primary contribution is estimating on a
single machine.
Constrained Unconstrained ?
fr-en 31.46 32.24 +.78
es-en 30.59 31.37 +.78
cs-en 27.38 28.16 +.78
ru-en 24.33 25.14 +.81
Table 18: Gain on newstest2013 from the unconstrained lan-
guage model. Our time on shared machines with 1 TB is
limited so Russian?English was run after the deadline and
German?English was not ready in time.
5 Summary
Table 19 breaks down the gains over the final sys-
tem from Section 1 from using the operation se-
quence models (OSM), modified Moore-Lewis fil-
tering (MML), fixing a bug with the sparse lex-
ical features (Sparse-Lex Bugfix), and instance
weighting (Instance Wt.), translation model com-
bination (TM-Combine), and use of the huge lan-
guage model (ClueWeb09 LM).
Acknowledgments
Thanks to Miles Osborne for preprocessing the ClueWeb09
corpus. The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE) and grant agreement
288487(MosesCore).This work made use of the resources
provided by the Edinburgh Compute and Data Facility6.
The ECDF is partially supported by the eDIKT initia-
tive7. This work also used the Extreme Science and
Engineering Discovery Environment (XSEDE), which is
supported by National Science Foundation grant number
OCI-1053575. Specifically, Stampede was used under
allocation TG-CCR110017.
6http://www.ecdf.ed.ac.uk/
7http://www.edikt.org.uk/
119
System 2012 2013
Spanish-English
1. Baseline 34.02 30.04
2. 1+OSM 34.51 +.49 30.94 +.90
3. 1+MML (20%) 34.38 +.36 30.38 +.34
4. 1+Sparse-Lex Bugfix 34.17 +.15 30.33 +.29
5. 1+2+3: OSM+MML 34.65 +.63 30.51 +.47
6. 1+2+3+4 34.68 +.66 30.59 +.55
7. 6+ClueWeb09 LM 31.37 +1.33
English-Spanish
1. Baseline 33.87 29.66
2. 1+OSM 34.44 +.57 30.10 +.44
3. 1+TM-Combine 34.31 +44 29.76 +.10
4. 1+Instance Wt. 34.27 +.40 29.63 ?.03
5. 1+Sparse-Lex Bugfix 34.20 +.33 29.86 +.20
6. 1+2+3: OSM+TM-Cmb. 34.63 +.76 30.21 +.55
7. 1+2+4: OSM+Inst. Wt. 34.58 +.71 30.11 +.45
8. 1+2+3+5 34.78 +.91 30.43 +.77
Czech-English
1. Baseline 22.70 25.70
2. 1+OSM 23.03 +.33 25.79 +.09
3. 1+with PRO 23.19 +.49 26.08 +.38
4. 1+Sparse-Lex Bugfix 22.86 +.16 25.74 +.04
5. 1+OSM+PRO 23.42 +.72 26.23 +.53
6. 1+2+3+4 23.16 +.46 25.94 +.24
7. 5+ClueWeb09 LM 27.06 +.36
English-Czech
1. Baseline 15.85 18.35
2. 1+OSM 16.16 +.31 18.62 +.27
French-English
1. Baseline 30.77 31.09
2. 1+OSM 30.96 +.19 31.46 +.37
3. 2+ClueWeb09 LM 32.24 +1.15
English-French
1. Baseline 28.76 30.03
2. 1+OSM 29.36 +.60 30.39 +.36
3. 1+Sparse-Lex Bugfix 28.97 +.21 30.08 +.05
4. 1+2+3 29.37 +.61 30.58 +.55
German-English
1. Baseline 23.85 26.54
2. 1+OSM 24.11 +.26 26.83 +.29
English-German
1. Baseline 17.95 20.06
2. 1+OSM 18.02 +.07 20.26 +.20
Russian-English
1. Baseline 31.87 24.00
2. 1+OSM 32.33 +.46 24.33 +.33
English-Russian
1. Baseline 23.75 18.44
2. 1+OSM 24.05 +.40 18.84 +.40
Table 19: Summary of methods with BLEU scores on news-
test2012 and newstest2013. Bold systems were submitted,
with the ClueWeb09 LM systems submitted in the uncon-
straint track. The German?English and English?German
OSM systems did not complete in time for the official sub-
mission.
References
Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation
via pseudo in-domain data selection. In Proceedings of the
2011 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 355?362, Edinburgh, Scotland,
UK. Association for Computational Linguistics.
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.
(2007). Large language models in machine translation.
In Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-CoNLL),
pages 858?867.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut,
R., and Specia, L. (2012). Findings of the 2012 work-
shop on statistical machine translation. In Proceedings of
the Seventh Workshop on Statistical Machine Translation,
pages 10?48, Montreal, Canada. Association for Compu-
tational Linguistics.
Chen, S. and Goodman, J. (1998). An empirical study of
smoothing techniques for language modeling. Technical
Report TR-10-98, Harvard University.
Cherry, C. and Foster, G. (2012). Batch tuning strategies for
statistical machine translation. In Proceedings of the 2012
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language
Technologies, pages 427?436, Montre?al, Canada. Associ-
ation for Computational Linguistics.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001 new
features for statistical machine translation. In Proceedings
of Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 218?226, Boulder,
Colorado. Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause
restructuring for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 531?540,
Ann Arbor, Michigan. Association for Computational Lin-
guistics.
Cormack, G. V., Smucker, M. D., and Clarke, C. L. (2011).
Efficient and effective spam filtering and re-ranking for
large web datasets. Information retrieval, 14(5):441?465.
Durrani, N., Fraser, A., and Schmid, H. (2013a). Model With
Minimal Translation Units, But Decode With Phrases. In
The 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Atlanta, Georgia, USA. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Hoang, H., and Koehn,
P. (2013b). Can Markov Models Over Minimal Transla-
tion Units Help Phrase-Based SMT? In Proceedings of
the 51st Annual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Schmid, H., and Fraser, A. (2011). A Joint Se-
quence Translation Model with Integrated Reordering. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 1045?1054, Portland, Oregon, USA.
Foster, G., Goutte, C., and Kuhn, R. (2010). Discriminative
instance weighting for domain adaptation in statistical ma-
chine translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing,
pages 451?459, Cambridge, MA. Association for Compu-
tational Linguistics.
120
Haddow, B. (2013). Applying pairwise ranked optimisation
to improve the interpolation of translation models. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 342?347, Atlanta,
Georgia. Association for Computational Linguistics.
Haddow, B. and Koehn, P. (2012). Analysing the effect of
out-of-domain data on smt systems. In Proceedings of
the Seventh Workshop on Statistical Machine Translation,
pages 175?185, Montreal, Canada. Association for Com-
putational Linguistics.
Heafield, K. (2011). KenLM: Faster and smaller language
model queries. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 187?197, Edin-
burgh, Scotland. Association for Computational Linguis-
tics.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.
(2013). Scalable modified Kneser-Ney language model
estimation. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia, Bul-
garia.
Johnson, H., Martin, J., Foster, G., and Kuhn, R. (2007).
Improving translation quality by discarding most of the
phrasetable. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 967?975.
Kneser, R. and Ney, H. (1995). Improved backing-off for
m-gram language modeling. In Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal
Processing, pages 181?184.
Koehn, P. and Haddow, B. (2012a). Interpolated backoff for
factored translation models. In Proceedings of the Tenth
Conference of the Association for Machine Translation in
the Americas (AMTA).
Koehn, P. and Haddow, B. (2012b). Towards Effective Use of
Training Data in Statistical Machine Translation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine
Translation, pages 317?321, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored Translation
Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical methods for com-
pound splitting. In Proceedings of Meeting of the Euro-
pean Chapter of the Association of Computational Lin-
guistics (EACL).
Mansour, S. and Ney, H. (2012). A Simple and Effec-
tive Weighted Phrase Extraction for Machine Translation
Adaptation. In Proceedings of IWSLT.
Moore, R. C. and Lewis, W. (2010). Intelligent selection of
language model training data. In Proceedings of the ACL
2010 Conference Short Papers, pages 220?224, Uppsala,
Sweden. Association for Computational Linguistics.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001).
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176(W0109-022),
IBM Research Report.
Sennrich, R. (2012). Perplexity minimization for translation
model domain adaptation in statistical machine transla-
tion. In Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics, pages 539?549, Avignon, France. Association for
Computational Linguistics.
121
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 122?127,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions of OSM Systems at WMT13
Nadir Durrani1, Helmut Schmid2, Alexander Fraser2,
Hassan Sajjad3, Richa?rd Farkas4
1University of Edinburgh ? dnadir@inf.ed.ac.uk
2Ludwig Maximilian University Munich ? schmid,fraser@cis.uni-muenchen.de
3Qatar Computing Research Institute ? hsajjad@qf.org.qa
4University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
This paper describes Munich-Edinburgh-
Stuttgart?s submissions to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results of the translation
tasks from German, Spanish, Czech and
Russian into English and from English to
German, Spanish, Czech, French and Rus-
sian. The systems described in this paper
use OSM (Operation Sequence Model).
We explain different pre-/post-processing
steps that we carried out for different
language pairs. For German-English we
used constituent parsing for reordering
and compound splitting as preprocessing
steps. For Russian-English we transliter-
ated the unknown words. The translitera-
tion system is learned with the help of an
unsupervised transliteration mining algo-
rithm.
1 Introduction
In this paper we describe Munich-Edinburgh-
Stuttgart?s1 joint submissions to the Eighth Work-
shop on Statistical Machine Translation. We use
our in-house OSM decoder which is based on
the operation sequence N-gram model (Durrani
et al, 2011). The N-gram-based SMT frame-
work (Marin?o et al, 2006) memorizes Markov
chains over sequences of minimal translation units
(MTUs or tuples) composed of bilingual transla-
tion units. The OSM model integrates reordering
operations within the tuple sequences to form a
heterogeneous mixture of lexical translation and
1Qatar Computing Research Institute and University of
Szeged were partnered for RU-EN and DE-EN language pairs
respectively.
reordering operations and learns a Markov model
over a sequence of operations.
Our decoder uses the beam search algorithm in
a stack-based decoder like most sequence-based
SMT frameworks. Although the model is based
on minimal translation units, we use phrases dur-
ing search because they improve the search accu-
racy of our system. The earlier decoder (Durrani
et al, 2011) was based on minimal units. But we
recently showed that using phrases during search
gives better coverage of translation, better future
cost estimation and lesser search errors (Durrani
et al, 2013a) than MTU-based decoding. We have
therefore shifted to phrase-based search on top of
the OSM model.
This paper is organized as follows. Section 2
gives a short description of the model and search
as used in the OSM decoder. In Section 3 we
give a description of the POS-based operation se-
quence model that we test for our German-English
and English-German experiments. Section 4 de-
scribes our processing of the German and English
data for German-English and English-German ex-
periments. In Section 5 we describe the unsuper-
vised transliteration mining that has been done for
the Russian-English and English-Russian experi-
ments. In Section 6 we describe the sub-sampling
technique that we have used for several language
pairs. In Section 7 we describe the experimental
setup followed by the results. Finally we summa-
rize the paper in Section 8.
2 System Description
2.1 Model
Our systems are based on the OSM (Operation Se-
quence Model) that simultaneously learns trans-
lation and reordering by representing a bilingual
122
Figure 1: Bilingual Sentence with Alignments
sentence pair and its alignments as a unique se-
quence of operations. An operation either jointly
generates source and target words, or it performs
reordering by inserting gaps or jumping to gaps.
We then learn a Markov model over a sequence of
operations o1, o2, . . . , oJ that encapsulate MTUs
and reordering information as:
posm(o1, ..., oJ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision depends
on n? 1 previous (translation and reordering) de-
cisions spanning across phrasal boundaries. The
reordering decisions therefore influence lexical se-
lection and vice versa. A heterogeneous mixture
of translation and reordering operations enables us
to memorize reordering patterns and lexicalized
triggers unlike the classic N-gram model where
translation and reordering are modeled separately.
2.2 Training
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the following sequence of operations:
Generate(Beide, Both)? Generate(La?nder, coun-
tries)? Generate(haben, have)? Insert Gap?
Generate(investiert, invested)
At this point, the (partial) German and English
sentences look as follows:
Beide La?nder haben investiert
Both countries have invested
The translator then jumps back and covers the
skipped German words through the following se-
quence of operations:
Jump Back(1)?Generate(Millionen, millions)?
Generate(von, of)? Generate(Dollar, dollars)
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
The generative story of the OSM model also
supports discontinuous source-side cepts and
source-word deletion. However, it doesn?t provide
a mechanism to deal with unaligned and discon-
tinuous target cepts. These are handled through
a 3-step process3 in which we modify the align-
ments to remove discontinuous and unaligned tar-
get MTUs. Please see Durrani et al (2011) for
details. After modifying the alignments, we con-
vert each bilingual sentence pair and its align-
ments into a sequence of operations as described
above and learn an OSM model. To this end,
a Kneser-Ney (Kneser and Ney, 1995) smoothed
9-gram model is trained with SRILM (Stolcke,
2002) while KenLM (Heafield, 2011) is used at
runtime.
2.3 Feature Functions
We use additional features for our model and em-
ploy the standard log-linear approach (Och and
Ney, 2004) to combine and tune them. We search
for a target string E which maximizes a linear
combination of feature functions:
E? = argmax
E
?
?
?
J?
j=1
?jhj(o1, ..., oJ)
?
?
?
where ?j is the weight associated with the fea-
ture hj(o1, ..., oj). Apart from the main OSM
feature we train 9 additional features: A target-
language model (see Section 7 for details), 2 lex-
ical weighting features, gap and open gap penalty
features, two distance-based distortion models and
2 length-based penalty features. Please refer to
Durrani et al (2011) for details.
2.4 Phrase Extraction
Phrases are extracted in the following way: The
aligned training corpus is first converted to an op-
eration sequence. Each subsequence of operations
that starts and ends with a translation operation, is
considered a ?phrase?. The translation operations
include Generate Source Only (X) operation which
deletes unaligned source word. Such phrases may
be discontinuous if they include reordering opera-
tions. We replace each subsequence of reordering
operations by a discontinuity marker.
3Durrani et al (2013b) recently showed that our post-
processing of alignments hurt the performance of the Moses
Phrase-based system in several language pairs. The solu-
tion they proposed has not been incorporated into the current
OSM decoder yet.
123
During decoding, we match the source tokens
of the phrase with the input. Whenever there is
a discontinuity in the phrase, the next source to-
ken can be matched at any position of the input
string. If there is no discontinuity marker, the next
source token in the phrase must be to the right of
the previous one. Finally we compute the number
of uncovered input tokens within the source span
of the hypothesized phrase and reject the phrase
if the number is above a threshold. We use a
threshold value of 2 which had worked well in
initial experiments. Once the positions of all the
source words of a phrase are known, we can com-
pute the necessary reordering operations (which
may be different from the ones that appeared in
the training corpus). This usage of phrases al-
lows the decoder to generalize from a seen trans-
lation ?scored a goal ? ein Tor schoss? (where
scored/a/goal and schoss/ein/Tor are aligned, re-
spectively) to ?scored a goal ? schoss ein Tor?.
The phrase can even be used to translate ?er schoss
heute ein Tor ? he scored a goal today? although
?heute? appears within the source span of the
phrase ?ein Tor schoss?. Without phrase-based
decoding, the unusual word translations ?schoss?
scored? and ?Tor?goal? (at least outside of the soc-
cer literature) are likely to be pruned.
The phrase tables are further filtered with
threshold pruning. The translation options with
a frequency less than x times the frequency of
the most frequent translation are deleted. We use
x = 0.02. We use additional settings to increase
this threshold for longer phrases. The phrase fil-
tering heuristic was used to speed up decoding. It
did not lower the BLEU score in our small scale
experiments (Durrani et al, 2013a), however we
could not test whether this result holds in a large
scale evaluation.
2.5 Decoder
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004).
The decoder uses beam search to build up the
translation from left to right. The hypotheses are
arranged in m stacks such that stack i maintains
hypotheses that have already translated imany for-
eign words. The ultimate goal is to find the best
scoring hypothesis, that translates all the words
in the foreign sentence. During the hypothesis
extension each extracted phrase is translated into
a sequence of operations. The reordering opera-
tions (gaps and jumps) are generated by looking at
the position of the translator, the last foreign word
generated etc. (Please refer to Algorithm 1 in Dur-
rani et al (2011)). The probability of an opera-
tion depends on the n?1 previous operations. The
model is smoothed with Kneser-Ney smoothing.
3 POS-based OSM Model
Part-of-speech information is often relevant for
translation. The word ?stores? e.g. should be
translated to ?La?den? if it is a noun and to ?spei-
chert? when it is a verb. The sentence ?The small
child cries? might be incorrectly translated to ?Die
kleinen Kind weint? where the first three words
lack number, gender and case agreement.
In order to better learn such constraints which
are best expressed in terms of part of speech, we
add another OSM model as a new feature to the
log-linear model of our decoder, which is identi-
cal to the regular OSM except that all the words
have been replaced by their POS tags. The input
of the decoder consists of the input sentence with
automatically assigned part-of-speech tags. The
source and target part of the training data are also
automatically tagged and phrases with words and
POS tags on both sides are extracted. The POS-
based OSM model is only used in the German-to-
English and English-to-German experiments.4 So
far, we only used coarse POS tags without gender
and case information.
4 Constituent Parse Reordering
Our German-to-English system used constituent
parses for pre-ordering of the input. We parsed all
of the parallel German to English data available,
and the tuning, test and blind-test sets. We then
applied reordering rules to these parses. We used
the rules for reordering German constituent parses
of Collins et al (2005) together with the additional
rules described by Fraser (2009). These are ap-
plied as a preprocess to all German data (training,
tuning and test data). To produce the parses, we
started with the generative BitPar parser trained on
the Tiger treebank with optimizations of the gram-
mar, as described by (Fraser et al, 2013). We then
performed self-training using the high quality Eu-
roparl corpus - we parsed it, and then retrained the
parser on the output.
4This work is ongoing and we will present detailed exper-
iments in the future.
124
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics (Koehn and Knight,
2003). We also split portmanteaus like German
?zum? formed from ?zu dem? meaning ?to the?.
Due to time constraints, we did not address Ger-
man inflection. See Weller et al (2013) for further
details of the linguistic processing involved in our
German-to-English system.
5 Transliteration Mining/Handling
OOVs
The machine translation system fails to translate
out-of-vocabulary words (OOVs) as they are un-
known to the training data. Most of the OOVs
are named entities and simply passing them to
the output often produces correct translations if
source and target language use the same script.
If the scripts are different transliterating them to
the target language script could solve this prob-
lem. However, building a transliteration system
requires a list of transliteration pairs for training.
We do not have such a list and making one is a
cumbersome process. Instead, we use the unsu-
pervised transliteration mining system of Sajjad et
al. (2012) that takes a list of word pairs for train-
ing and extracts transliteration pairs that can be
used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows:
We word-align the parallel corpus using
GIZA++ in both direction and symmetrize the
alignments using the grow-diag-final-and heuris-
tic. We extract all word pairs which occur as 1-
to-1 alignments (like Sajjad et al (2011)) and later
refer to them as the list of word pairs. We train the
unsupervised transliteration mining system on the
list of word pairs and extract transliteration pairs.
We use these mined pairs to build a transliteration
system using the Moses toolkit. The translitera-
tion system is applied in a post-processing step
to transliterate OOVs. Please refer to Sajjad et
al. (2013) for further details on our transliteration
work.
6 Sub-sampling
Because of scalability problems we were not able
to use the entire data made available for build-
ing the translation model in some cases. We used
modified Moore-Lewis sampling (Axelrod et al,
2011) for the language pairs es-en, en-es, en-fr,
and en-cs. In each case we included the News-
Commentary and Europarl corpora in their en-
tirety, and scored the sentences in the remaining
corpora (the selection corpus) using a filtering cri-
terion, adding 10% of the selection corpus to
the training data. We can not say with certainty
whether using the entire data will produce better
results with the OSM decoder. However, we know
that the same data used with the state-of-the-art
Moses produced worse results in some cases. The
experiments in Durrani et al (2013c) showed that
MML filtering decreases the BLEU scores in es-
en (news-test13: Table 19) and en-cs (news-test12:
Table 14). We can therefore speculate that being
able to use all of the data may improve our results
somewhat.
7 Experiments
Parallel Corpus: The amount of bitext used for
the estimation of the translation models is: de?en
? 4.5M and ru?en ? 2M parallel sentences. We
were able to use all the available data for cs-to-en
(? 15.6M sentences). However, sub-sampled data
was used for en-to-cs (? 3M sentences), en-to-fr
(? 7.8M sentences) and es?en (? 3M sentences).
Monolingual Language Model: We used all
the available training data (including LDC Giga-
word data) for the estimation of monolingual lan-
guage models: en? 287.3M sentences, fr? 91M,
es ? 65.7M, cs ? 43.4M and ru ? 21.7M sen-
tences. All data except for ru-en and en-ru was
true-cased. We followed the approach of Schwenk
and Koehn (2008) by training language models
from each sub-corpus separately and then linearly
interpolated them using SRILM with weights op-
timized on the held-out dev-set. We concatenated
the news-test sets from four years (2008-2011) to
obtain a large dev-set5 in order to obtain more sta-
ble weights (Koehn and Haddow, 2012).
Decoder Settings: For each extracted input
phrase only 15-best translation options were used
during decoding.6 We used a hard reordering limit
5For Russian-English and English-Russian language
pairs, we divided the tuning-set news-test 2012 into two
halves and used the first half for tuning and second for test.
6We could not experiment with higher n-best translation
options due to a bug that was not fixed in time and hindered
us from scaling.
125
of 16 words which disallows a jump beyond 16
source words. A stack size of 100 was used during
tuning and 200 for decoding the test set.
Results: Table 1 shows the uncased BLEU
scores along with the rank obtained on the sub-
mission matrix.7 We also show the results from
human evaluation.
Lang Evaluation
Automatic Human
BLEU Rank Win Ratio Rank
de-en 27.6 9/31 0.562 6-8
es-en 30.4 6/12 0.569 3-5
cs-en 26.4 3/11 0.581 2-3
ru-en 24.5 8/22 0.534 7-9
en-de 20.0 6/18
en-es 29.5 3/13 0.544 5-6
en-cs 17.6 14/22 0.517 4-6
en-ru 18.1 6/15 0.456 9-10
en-fr 30.0 7/26 0.541 5-9
Table 1: Translating into and from English
8 Conclusion
In this paper, we described our submissions to
WMT 13 in all the shared-task language pairs
(except for fr-en). We used an OSM-decoder,
which implements a model on n-gram of opera-
tions encapsulating lexical generation and reorder-
ing. For German-to-English we used constituent
parsing and applied linguistically motivated rules
to these parses, followed by compound splitting.
We additionally used a POS-based OSM model for
German-to-English and English-to-German exper-
iments. For Russian-English language pairs we
used unsupervised transliteration mining. Because
of scalability issues we could not use the entire
data in some language pairs and used only sub-
sampled data. Our Czech-to-English system that
was built from the entire data did better in both
automatic and human evaluation compared to the
systems that used sub-sampled data.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
7http://matrix.statmt.org/
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander
Fraser was funded by Deutsche Forschungsge-
meinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid
was supported by Deutsche Forschungsgemein-
schaft grant SFB 732. Richa?rd Farkas was
partially funded by the Hungarian National Ex-
cellence Program (TA?MOP 4.2.4.A/2-11-1-2012-
0001). This publication only reflects the authors?
views.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In ACL05, pages 531?540, Ann Arbor,
MI.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013b. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013c. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
126
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the EACL 2009 Fourth
Workshop on Statistical Machine Translation, pages
115?119, Athens, Greece, March.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the ACL 2010
Fifth Workshop on Statistical Machine Translation,
Uppsala, Sweden.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume I, pages 181?184, Detroit, Michigan, May.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 187?193, Morristown, NJ.
Philipp Koehn. 2004. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC).
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart Submissions at WMT13: Mor-
phological and Syntactic Processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
127
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219?224,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
QCRI-MES Submission at WMT13: Using Transliteration Mining to
Improve Statistical Machine Translation
Hassan Sajjad1, Svetlana Smekalova2, Nadir Durrani3,
Alexander Fraser4, Helmut Schmid4
1Qatar Computing Research Institute ? hsajjad@qf.org.qa
2University of Stuttgart ? smekalsa@ims.uni-stuttgart.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Ludwig-Maximilians University Munich ? (fraser|schmid)@cis.uni-muenchen.de
Abstract
This paper describes QCRI-MES?s sub-
mission on the English-Russian dataset to
the Eighth Workshop on Statistical Ma-
chine Translation. We generate improved
word alignment of the training data by
incorporating an unsupervised translitera-
tion mining module to GIZA++ and build
a phrase-based machine translation sys-
tem. For tuning, we use a variation of PRO
which provides better weights by optimiz-
ing BLEU+1 at corpus-level. We translit-
erate out-of-vocabulary words in a post-
processing step by using a transliteration
system built on the transliteration pairs
extracted using an unsupervised translit-
eration mining system. For the Russian
to English translation direction, we apply
linguistically motivated pre-processing on
the Russian side of the data.
1 Introduction
We describe the QCRI-Munich-Edinburgh-
Stuttgart (QCRI-MES) English to Russian and
Russian to English systems submitted to the
Eighth Workshop on Statistical Machine Trans-
lation. We experimented using the standard
Phrase-based Statistical Machine Translation
System (PSMT) as implemented in the Moses
toolkit (Koehn et al, 2007). The typical pipeline
for translation involves word alignment using
GIZA++ (Och and Ney, 2003), phrase extraction,
tuning and phrase-based decoding. Our system is
different from standard PSMT in three ways:
? We integrate an unsupervised transliteration
mining system (Sajjad et al, 2012) into the
GIZA++ word aligner (Sajjad et al, 2011).
So, the selection of a word pair as a correct
alignment is decided using both translation
probabilities and transliteration probabilities.
? The MT system fails when translating out-of-
vocabulary (OOV) words. We build a statis-
tical transliteration system on the translitera-
tion pairs mined by the unsupervised translit-
eration mining system and transliterate them
in a post-processing step.
? We use a variation of Pairwise Ranking Op-
timization (PRO) for tuning. It optimizes
BLEU at corpus-level and provides better
feature weights that leads to an improvement
in translation quality (Nakov et al, 2012).
We participate in English to Russian and Rus-
sian to English translation tasks. For the Rus-
sian/English system, we present experiments with
two variations of the parallel corpus. One set of
experiments are conducted using the standard par-
allel corpus provided by the workshop. In the sec-
ond set of experiments, we morphologically re-
duce Russian words based on their fine-grained
POS tags and map them to their root form. We
do this on the Russian side of the parallel corpus,
tuning set, development set and test set. This im-
proves word alignment and learns better transla-
tion probabilities by reducing the vocabulary size.
The paper is organized as follows. Section
2 talks about unsupervised transliteration mining
and its incorporation to the GIZA++ word aligner.
In Section 3, we describe the transliteration sys-
tem. Section 4 describes the extension of PRO
that optimizes BLEU+1 at corpus level. Section
5 and Section 6 present English/Russian and Rus-
sian/English machine translation experiments re-
spectively. Section 7 concludes.
219
2 Transliteration Mining
Consider a list of word pairs that consists of either
transliteration pairs or non-transliteration pairs.
A non-transliteration pair is defined as a word
pair where words are not transliteration of each
other. They can be translation, misalignment,
etc. Transliteration mining extracts transliteration
pairs from the list of word pairs. Sajjad et al
(2012) presented an unsupervised transliteration
mining system that trains on the list of word pairs
and filters transliteration pairs from that. It models
the training data as the combination of a translit-
eration sub-model and a non-transliteration sub-
model. The transliteration model is a joint source
channel model. The non-transliteration model as-
sumes no correlation between source and target
word characters, and independently generates a
source and a target word using two fixed uni-
gram character models. The transliteration mining
model is defined as an interpolation of the translit-
eration model and the non-transliteration model.
We apply transliteration mining to the list of
word pairs extracted from English/Russian paral-
lel corpus and mine transliteration pairs. We use
the mined pairs for the training of the translitera-
tion system.
2.1 Transliteration Augmented-GIZA++
GIZA++ aligns parallel sentences at word level. It
applies the IBM models (Brown et al, 1993) and
the HMM model (Vogel et al, 1996) in both direc-
tions i.e. source to target and target to source. It
generates a list of translation pairs with translation
probabilities, which is called the t-table. Sajjad
et al (2011) used a heuristic-based transliteration
mining system and integrated it into the GIZA++
word aligner. We follow a similar procedure but
use the unsupervised transliteration mining system
of Sajjad et al (2012).
We define a transliteration sub-model and train
it on the transliteration pairs mined by the unsuper-
vised transliteration mining system. We integrate
it into the GIZA++ word aligner. The probabil-
ity of a word pair is calculated as an interpolation
of the transliteration probability and the transla-
tion probability stored in the t-table of the differ-
ent alignment models used by the GIZA++ aligner.
This interpolation is done for all iterations of all
alignment models.
2.1.1 Estimating Transliteration Probabilities
We use the algorithm for the estimation of translit-
eration probabilities of Sajjad et al (2011). We
modify it to improve efficiency. In step 6 of Al-
gorithm 1 instead of taking all f that coocur with
e, we take only those that have a word length ra-
tio in range of 0.8-1.2.1 This reduces cooc(e) by
more than half and speeds up step 9 of Algorithm
1. The word pairs that are filtered out from cooc(e)
won?t have transliteration probability pti(f |e). We
do not interpolate in these cases and use the trans-
lation probability as it is.
Algorithm 1 Estimation of transliteration proba-
bilities, e-to-f direction
1: unfiltered data? list of word pairs
2: filtered data?transliteration pairs extracted using unsu-
pervised transliteration mining system
3: Train a transliteration system on the filtered data
4: for all e do
5: nbestTI(e) ? 10 best transliterations for e accord-
ing to the transliteration system
6: cooc(e)? set of all f that cooccur with e in a parallel
sentence with a word length in ratio of 0.8-1.2
7: candidateTI(e)? cooc(e) ? nbestTI(e)
8: for all f do
9: pmoses(f, e) ? joint transliteration probability of e
and f according to the transliterator
10: Calculate conditional transliteration probability
pti(f |e)? pmoses(f,e)?
f??CandidateTI(e) pmoses(f ?,e)
2.1.2 Modified EM Training
Sajjad et al (2011) modified the EM training of
the word alignment models. They combined the
translation probabilities of the IBM models and
the HMM model with the transliteration proba-
bilities. Consider pta(f |e) = fta(f, e)/fta(e) is
the translation probability of the word alignment
models. The interpolated probability is calcu-
lated by adding the smoothed alignment frequency
fta(f, e) to the transliteration probability weight
by the factor ?. The modified translation probabil-
ities is given by:
p?(f |e) = fta(f, e) + ?pti(f |e)fta(e) + ?
(1)
where fta(f, e) = pta(f |e)fta(e). pta(f |e) is ob-
tained from the original t-table of the alignment
model. fta(e) is the total corpus frequency of e.
? is the transliteration weight which is defined as
the number of counts the transliteration model gets
versus the translation model. The model is not
1We assume that the words with very different character
counts are less likely to be transliterations.
220
very sensitive to the value of ?. We use ? = 50
for our experiments. The procedure we described
of estimation of transliteration probabilities and
modification of EM is also followed in the oppo-
site direction f-to-e.
3 Transliteration System
The unsupervised transliteration mining system
(as described in Section 2) outputs a list of translit-
eration pairs. We consider transliteration word
pairs as parallel sentences by putting a space af-
ter every character of the words and train a PSMT
system for transliteration. We apply the transliter-
ation system to OOVs in a post-processing step on
the output of the machine translation system.
Russian is a morphologically rich language.
Different cases of a word are generally represented
by adding suffixes to the root form. For OOVs
that are named entities, transliterating the inflected
forms generates wrong English transliterations as
inflectional suffixes get transliterated too. To han-
dle this, first we need to identify OOV named en-
tities (as there can be other OOVs that are not
named entities) and then transliterate them cor-
rectly. We tackle the first issue as follows: If
an OOV word is starting with an upper case let-
ter, we identify it as a named entity. To correctly
transliterate it to English, we stem the named en-
tity based on a list of suffixes ( , , , , , )
and transliterate the stemmed form. For morpho-
logically reduced Russian (see Section 6.1), we
follow the same procedure as OOVs are unknown
to the POS tagger too and are (incorrectly) not re-
duced to their root forms. For OOVs that are not
identified as named entities, we transliterate them
without any pre-processing.
4 PRO: Corpus-level BLEU
Pairwise Ranking Optimization (PRO) (Hopkins
and May, 2011) is an extension of MERT (Och,
2003) that can scale to thousands of parameters.
It optimizes sentence-level BLEU+1 which is an
add-one smoothed version of BLEU (Lin and Och,
2004). The sentence-level BLEU+1 has a bias
towards producing short translations as add-one
smoothing improves precision but does not change
the brevity penalty. Nakov et al (2012) fixed this
by using several heuristics on brevity penalty, ref-
erence length and grounding the precision length.
In our experiments, we use the improved version
of PRO as provided by Nakov et al (2012). We
call it PROv1 later on.
5 English/Russian Experiments
5.1 Dataset
The amount of bitext used for the estimation of the
translation model is ? 2M parallel sentences. We
use newstest2012a for tuning and newstest2012b
(tst2012) as development set.
The language model is estimated using large
monolingual corpus of Russian ? 21.7M sen-
tences. We follow the approach of Schwenk and
Koehn (2008) by training domain-specific lan-
guage models separately and then linearly inter-
polate them using SRILM with weights optimized
on the held-out development set. We divide the
tuning set newstest2012a into two halves and use
the first half for tuning and second for test in or-
der to obtain stable weights (Koehn and Haddow,
2012).
5.2 Baseline Settings
We word-aligned the parallel corpus using
GIZA++ (Och and Ney, 2003) with 5 iterations
of Model1, 4 iterations of HMM and 4 iterations
of Model4, and symmetrized the alignments us-
ing the grow-diag-final-and heuristic (Koehn et al,
2003). We built a phrase-based machine transla-
tion system using the Moses toolkit. Minimum er-
ror rate training (MERT), margin infused relaxed
algorithm (MIRA) and PRO are used to optimize
the parameters.
5.3 Main System Settings
Our main system involves a pre-processing step
? unsupervised transliteration mining, and a post-
processing step ? transliteration of OOVs. For the
training of the unsupervised transliteration min-
ing system, we take the word alignments from
our baseline settings and extract all word pairs
which occur as 1-to-1 alignments (like Sajjad et
al. (2011)) and later refer to them as a list of
word pairs. The unsupervised transliteration min-
ing system trains on the list of word pairs and
mines transliteration pairs. We use the mined pairs
to build a transliteration system using the Moses
toolkit. The transliteration system is used in Algo-
rithm 1 to generate transliteration probabilities of
candidate word pairs and is also used in the post-
processing step to transliterate OOVs.
We run GIZA++ with identical settings as de-
scribed in Section 5.2. We interpolate for ev-
221
GIZA++ TA-GIZA++ OOV-TI
MERT 23.41 23.51 23.60
MIRA 23.60 23.73 23.85
PRO 23.57 23.68 23.70
PROv1 23.65 23.76 23.87
Table 1: BLEU scores of English to Russian ma-
chine translation system evaluated on tst2012 us-
ing baseline GIZA++ alignment and translitera-
tion augmented-GIZA++. OOV-TI presents the
score of the system trained using TA-GIZA++ af-
ter transliterating OOVs
ery iteration of the IBM Model1 and the HMM
model. We had problem in applying smoothing
for Model4 and did not interpolate transliteration
probabilities for Model4. The alignments are re-
fined using the grow-diag-final-and heuristic. We
build a phrase-based system on the aligned pairs
and tune the parameters using PROv1. OOVs are
transliterated in the post-processing step.
5.4 Results
Table 1 summarizes English/Russian results on
tst2012. Improved word alignment gives up to
0.13 BLEU points improvement. PROv1 improves
translation quality and shows 0.08 BLEU point
increase in BLEU in comparison to the parame-
ters tuned using PRO. The transliteration of OOVs
consistently improve translation quality by at least
0.1 BLEU point for all systems.2 This adds to a
cumulative gain of up to 0.2 BLEU points.
We summarize results of our systems trained on
GIZA++ and transliteration augmented-GIZA++
(TA-GIZA++) and tested on tst2012 and tst2013
in Table 2. Both systems use PROv1 for tuning
and transliteration of OOVs in the post-processing
step. The system trained on TA-GIZA++ per-
formed better than the system trained on the base-
line aligner GIZA++.
6 Russian/English Experiments
In this section, we present translation experiments
in Russian to English direction. We morphologi-
cally reduce the Russian side of the parallel data in
a pre-processing step and train the translation sys-
tem on that. We compare its result with the Rus-
sian to English system trained on the un-processed
parallel data.
2We see similar gain in BLEU when using operation se-
quence model (Durrani et al, 2011) for decoding and translit-
erating OOVs in a post-processing step (Durrani et al, 2013).
SYS tst2012 tst2013
GIZA++ 23.76 18.4
TA-GIZA++ 23.87 18.5*
Table 2: BLEU scores of English to Russian ma-
chine translation system evaluated on tst2012 and
tst2013 using baseline GIZA++ alignment and
transliteration augmented-GIZA++ alignment and
post-processed the output by transliterating OOVs.
Human evaluation in WMT13 is performed on
TA-GIZA++ tested on tst2013 (marked with *)
6.1 Morphological Processing
The linguistic processing of Russian involves POS
tagging and morphological reduction. We first tag
the Russian data using a fine grained tagset. The
tagger identifies lemmas and the set of morpholog-
ical attributes attached to each word. We reduce
the number of these attributes by deleting some
of them, that are not relevant for English (for ex-
ample, gender agreement of verbs). This gener-
ates a morphologically reduced Russian which is
used in parallel with English for the training of
the machine translation system. Further details on
the morphological processing of Russian are de-
scribed in Weller et al (2013).
6.1.1 POS Tagging
We use RFTagger (Schmid and Laws, 2008) for
POS tagging. Despite the good quality of tagging
provided by RFTagger, some errors seem to be un-
avoidable due to the ambiguity of certain gram-
matical forms in Russian. A good example of
this is neuter nouns that have the same form in
all cases, or feminine nouns, which have identi-
cal forms in singular genitive and plural nomina-
tive (Sharoff et al, 2008). Since Russian sentences
have free word order, and the case of nouns can-
not be determined on that basis, this imperfection
can not be corrected during tagging or by post-
processing the tagger output.
6.1.2 Morphological Reduction
English in comparison to Slavic group of lan-
guages is morphologically poor. For example, En-
glish has no morphological attributes for nouns
and adjectives to express gender or case; verbs in
English have no gender either. Russian, on the
contrary, has rich morphology. It suffices to say
that the Russian has 6 cases and 3 grammatical
genders, which manifest themselves in different
222
suffixes for nouns, pronouns, adjectives and some
verb forms.
When translating from Russian into English, a
lot of these attributes become meaningless and ex-
cessive. It makes sense to reduce the number of
morphological attributes before the text is sup-
plied for the training of the MT system. We ap-
ply morphological reduction to nouns, pronouns,
verbs, adjectives, prepositions and conjunctions.
The rest of the POS (adverbs, particles, interjec-
tions and abbreviations) have no morphological at-
tributes and are left unchanged.
We apply morphological reduction to train,
tune, development and test data. We refer to this
data set as morph-reduced later on.
6.2 Dataset
We use two variations of the parallel corpus to
build and test the Russian to English system. One
system is built on the data provided by the work-
shop. For the second system, we preprocess the
Russian side of the data as described in Section
6.1. Both the provided parallel corpus and the
morph-reduced parallel corpus consist of 2M par-
allel sentences each. We use them for the estima-
tion of the translation model. We use large train-
ing data for the estimation of monolingual lan-
guage model ? en? 287.3M sentences. We follow
the identical procedure of interpolated language
model as described in Section 5.1. We use new-
stest2012a for tuning and newstest2012b (tst2012)
for development.
6.3 System Settings
We use identical system settings to those described
in Section 5.3. We trained the systems sepa-
rately on GIZA++ and transliteration augmented-
GIZA++ to compare their results. All systems are
tuned using PROv1. The translation output is post-
processed to transliterate OOVs.
6.4 Results
Table 3 summarizes results of Russian to English
machine translation systems trained on the orig-
inal parallel corpus and on the morph-reduced
corpus and using GIZA++ and transliteration
augmented-GIZA++ for word alignment. The sys-
tem using TA-GIZA++ for alignment shows the
best results for both tst2012 and tst2013. The im-
proved alignment gives a BLEU improvement of
up to 0.4 points.
Original corpus
SYS tst2012 tst2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS tst2012 tst2013
GIZA++ 31.22 24.30
TA-GIZA++ 31.40 24.45
Table 3: Russian to English machine translation
system evaluated on tst2012 and tst2013. Human
evaluation in WMT13 is performed on the system
trained using the original corpus with TA-GIZA++
for alignment (marked with *)
The system built on the morph-reduced data
shows degradation in results by 1.29 BLEU points.
However, the percentage of OOVs reduces for
both test sets when using the morph-reduced data
set compared to the original parallel corpus. We
analyze the output of the system and find that the
morph-reduced system makes mistakes in choos-
ing the right tense of the verb. This might be one
reason for poor performance. This implies that the
morphological reduction is slightly damaging the
data, perhaps for specific parts of speech. In the
future, we would like to investigate this issue in
detail.
7 Conclusion
In this paper, we described the QCRI-Munich-
Edinburgh-Stuttgart machine translation systems
submitted to the Eighth Workshop on Statistical
Machine Translation. We aligned the parallel cor-
pus using transliteration augmented-GIZA++ to
improve the word alignments. We built a phrase-
based system using the Moses toolkit. For tun-
ing the feature weights, we used an improvement
of PRO that optimizes for corpus-level BLEU. We
post-processed the output of the machine transla-
tion system to transliterate OOV words.
For the Russian to English system, we mor-
phologically reduced the Russian data in a pre-
processing step. This reduced the vocabulary size
and helped to generate better word alignments.
However, the performance of the SMT system
dropped by 1.29 BLEU points in decoding. We
will investigate this issue further in the future.
223
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander Fraser
was funded by Deutsche Forschungsgemeinschaft
grant Models of Morphosyntax for Statistical Ma-
chine Translation. Helmut Schmid was supported
by Deutsche Forschungsgemeinschaft grant SFB
732. This publication only reflects the authors
views.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2).
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, Sofia, Bulgaria.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Edinburgh, United Kingdom.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montre?al,
Canada.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic.
Chin-Yew Lin and Franz Josef Och. 2004. OR-
ANGE: a method for evaluating automatic evalua-
tion metrics for machine translation. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, Geneva, Switzerland.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised translitera-
tion mining with an application to word alignment.
In Proceedings of the 49th Annual Conference of
the Association for Computational Linguistics, Port-
land, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Conference of the Associa-
tion for Computational Linguistics, Jeju, Korea.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, Manchester,
United Kingdom.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, Hyderabad, India.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing
and evaluating a russian tagset. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In 16th International Conference on
Computational Linguistics, Copenhagen, Denmark.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria.
224
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232?239,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions at WMT13:
Morphological and Syntactic Processing for SMT
Marion Weller1, Max Kisselew1, Svetlana Smekalova1, Alexander Fraser2,
Helmut Schmid2, Nadir Durrani3, Hassan Sajjad4, Richa?rd Farkas5
1University of Stuttgart ? (wellermn|kisselmx|smekalsa)@ims.uni-stuttgart.de
2Ludwig-Maximilian University of Munich ? (schmid|fraser)@cis.uni-muenchen.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Qatar Computing Research Institute ? hsajjad@qf.org.qa
5University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
We present 5 systems of the Munich-
Edinburgh-Stuttgart1 joint submissions to
the 2013 SMT Shared Task: FR-EN, EN-
FR, RU-EN, DE-EN and EN-DE. The
first three systems employ inflectional gen-
eralization, while the latter two employ
parser-based reordering, and DE-EN per-
forms compound splitting. For our ex-
periments, we use standard phrase-based
Moses systems and operation sequence
models (OSM).
1 Introduction
Morphologically complex languages often lead to
data sparsity problems in statistical machine trans-
lation. For translation pairs with morphologically
rich source languages and English as target lan-
guage, we focus on simplifying the input language
in order to reduce the complexity of the translation
model. The pre-processing of the source-language
is language-specific, requiring morphological anal-
ysis (FR, RU) as well as sentence reordering (DE)
and dealing with compounds (DE). Due to time
constraints we did not deal with inflection for DE-
EN and EN-DE.
The morphological simplification process con-
sists in lemmatizing inflected word forms and deal-
ing with word formation (splitting portmanteau
prepositions or compounds). This needs to take
into account translation-relevant features (e.g. num-
ber) which vary across the different language pairs:
while French only has the features number and
gender, a wider array of features needs to be con-
sidered when modelling Russian (cf. table 6). In
addition to morphological reduction, we also apply
transliteration models learned from automatically
1The language pairs DE-EN and RU-EN were developed
in collaboration with the Qatar Computing Research Institute
and the University of Szeged.
mined transliterations to handle out-of-vocabulary
words (OOVs) when translating from Russian.
Replacing inflected word forms with simpler
variants (lemmas or the components of split com-
pounds) aims not only at reducing the general com-
plexity of the translation model, but also at decreas-
ing the amount of out-of-vocabulary words in the
input data. This is particularly the case with Ger-
man compounds, which are very productive and
thus often lack coverage in the parallel training
data, whereas the individual components can be
translated. Similarly, inflected word forms (e.g. ad-
jectives) benefit from the reduction to lemmas if
the full inflection paradigm does not occur in the
parallel training data.
For EN-FR, a translation pair with a morpho-
logically complex target language, we describe a
two-step translation system built on non-inflected
word stems with a post-processing component for
predicting morphological features and the genera-
tion of inflected forms. In addition to the advantage
of a more general translation model, this method
also allows the generation of inflected word forms
which do not occur in the training data.
2 Experimental setup
The translation experiments in this paper are car-
ried out with either a standard phrase-based Moses
system (DE-EN, EN-DE, EN-FR and FR-EN) or
with an operation sequence model (RU-EN, DE-
EN), cf. Durrani et al (2013b) for more details.
An operation sequence model (OSM) is a state-
of-the-art SMT-system that learns translation and
reordering patterns by representing a sentence pair
and its word alignment as a unique sequence of
operations (see e.g. Durrani et al (2011), Durrani
et al (2013a) for more details). For the Moses sys-
tems we used the old train-model perl scripts rather
than the EMS, so we did not perform Good-Turing
smoothing; parameter tuning was carried out with
batch-mira (Cherry and Foster, 2012).
232
1 Removal of empty lines
2 Conversion of HTML special characters like
&quot; to the corresponding characters
3 Unification of words that were written both
with an ? or with an oe to only one spelling
4 Punctuation normalization and tokenization
5 Putting together clitics and apostrophes like
l ? or d ? to l? and d?
Table 1: Text normalization for FR-EN.
Definite determiners la / l? / les ? le
Indefinite determiners un / une ? un
Adjectives Infl. form ? lemma
Portmanteaus e. g. au ? a` le
Verb participles Reduced to
inflected for gender non-inflected
and number verb participle form
ending in e?e/e?s/e?es ending in e?
Clitics and apostroph- d? ? de,
ized words are converted qu? ? que,
to their lemmas n? ? ne, ...
Table 2: Rules for morphological simplification.
The development data consists of the concate-
nated news-data sets from the years 2008-2011.
Unless otherwise stated, we use all constrained data
(parallel and monolingual). For the target-side lan-
guage models, we follow the approach of Schwenk
and Koehn (2008) and train a separate language
model for each corpus and then interpolate them
using weights optimized on development data.
3 French to English
French has a much richer morphology than English;
for example, adjectives in French are inflected with
respect to gender and number whereas adjectives
in English are not inflected at all. This causes data
sparsity in coverage of French inflected forms. We
try to overcome this problem by simplifying French
inflected forms in a pre-processing step in order to
adapt the French input better to the English output.
Processing of the training and test data The
pre-processing of the French input consists of two
steps: (1) normalizing not well-formed data (cf.
table 1) and (2) morphological simplification.
In the second step, the normalized training data
is annotated with Part-of-Speech tags (PoS-tags)
and word lemmas using RFTagger (Schmid and
Laws, 2008) which was trained on the French tree-
bank (Abeille? et al, 2003). French forms are then
simplified according to the rules given in table 2.
Data and experiments We trained a French to
English Moses system on the preprocessed and
System BLEU (cs) BLEU (ci)
Baseline 29.90 31.02
Simplified French* 29.70 30.83
Table 3: Results of the French to English system
(WMT-2012). The marked system (*) corresponds
to the system submitted for manual evaluation. (cs:
case-sensitive, ci: case-insensitive)
simplified constrained parallel data.
Due to tractability problems with word align-
ment, the 109 French-English corpus and the UN
corpus were filtered to a more manageable size.
The filtering criteria are sentence length (between
15 and 25 words), as well as strings indicating that
a sentence is neither French nor English, or other-
wise not well-formed, aiming to obtain a subset of
good-quality sentences. In total, we use 9M par-
allel sentences. For the English language model
we use large training data with 287.3M true-cased
sentences (including the LDC Giga-word data).
We compare two systems: a baseline with reg-
ular French text, and a system with the described
morphological simplifications. Results for the
WMT-2012 test set are shown in table 3. Even
though the baseline is better than the simplified
system in terms of BLEU, we assume that the trans-
lation model of the simplified system benefits from
the overall generalization ? thus, human annotators
might prefer the output of the simplified system.
For the WMT-2013 set, we obtain BLEU scores
of 29,97 (cs) and 31,05 (ci) with the system built
on simplified French (mes-simplifiedfrench).
4 English to French
Translating into a morphologically rich language
faces two problems: that of asymmetry of mor-
phological information contained in the source and
target language and that of data sparsity.
In this section we describe a two-step system de-
signed to overcome these types of problems: first,
the French data is reduced to non-inflected forms
(stems) with translation-relevant morphological fea-
tures, which is used to built the translation model.
The second step consists of predicting all neces-
sary morphological features for the translation out-
put, which are then used to generate fully inflected
forms. This two-step setup decreases the complex-
ity of the translation task by removing language-
specific features from the translation model. Fur-
thermore, generating inflected forms based on word
stems and morphological features allows to gener-
233
ate forms which do not occur in the parallel training
data ? this is not possible in a standard SMT setup.
The idea of separating the translation into two
steps to deal with complex morphology was in-
troduced by Toutanova et al (2008). Fraser et
al. (2012) applied this method to the language
pair English-German with an additional special
focus on word formation issues such as the split-
ting and merging of portmanteau prepositions and
compounds. The presented inflection prediction
systems focuses on nominal inflection; verbal in-
flection is not addressed.
Morphological analysis and resources The
morphological analysis of the French training data
is obtained using RFTagger, which is designed
for annotating fine-grained morphological tags
(Schmid and Laws, 2008). For generating inflected
forms based on stems and morphological features,
we use an extended version of the finite-state mor-
phology FRMOR (Zhou, 2007). Additionally, we
use a manually compiled list of abbreviations and
named entities (names of countries) and their re-
spective grammatical gender.
Stemming For building the SMT system, the
French data (parallel and monolingual) is trans-
formed into a stemmed representation. Nouns,
i.e. the heads of NPs or PPs, are marked with
inflection-relevant features: gender is considered
as part of the stem, whereas number is determined
by the source-side input: for example, we expect
source-language words in plural to be translated by
translated by stems with plural markup. This stem-
markup is necessary in order to guarantee that the
number information is not lost during translation.
For a better generalization, portmanteaus are split
into separate parts: au? a`+le (meaning, ?to the?).
Predicting morphological features For predict-
ing the morphological features of the SMT output
(number and gender), we use a linear chain CRF
(Lavergne et al, 2010) trained on data annotated
with these features using n-grams of stems and part-
of-speech tags within a window of 4 positions to
each side of the current word. Through the CRF,
the values specified in the stem-markup (number
and gender on nouns) are propagated over the rest
of the linguistic phrase, as shown in column 2 of
table 4. Based on the stems and the morphological
features, inflected forms can be generated using
FRMOR (column 3).
Post-processing As the French data has been
normalized, a post-processing step is needed in or-
der to generate correct French surface forms: split
portmanteaus are merged into their regular forms
based on a simple rule set. Furthermore, apostro-
phes are reintroduced for words like le, la, ne, ... if
they are followed by a vowel. Column 4 in table 4
shows post-processing including portmanteau for-
mation. Since we work on lowercased data, an
additional recasing step is required.
Experiments and evaluation We use the same
set of reduced parallel data as the FR-EN system;
the language model is built on 32M French sen-
tences. Results for the WMT-2012 test set are given
in table 5. Variant 1 shows the results for a small
system trained only on a part of the training data
(Europarl+News Commentary), whereas variant 2
corresponds to the submitted system. A small-scale
analysis indicated that the inflection prediction sys-
tem tends to have problems with subject-verb agree-
ment. We trained a factored system using addi-
tional PoS-tags with number information which
lead to a small improvement on both variants.
While the small model is significantly better than
the baseline2 as it benefits more from the general-
ization, the result for the full system is worse than
the baseline3. Here, given the large amount of
data, the generalization effect has less influence.
However, we assume that the more general model
from the inflection prediction system produces bet-
ter translations than a regular model containing a
large amount of irrelevant inflectional information,
particularly when considering that it can produce
well-formed inflected sequences that are inaccessi-
ble to the baseline. Even though this is not reflected
in terms of BLEU, humans might prefer the inflec-
tion prediction system.
For the WMT-2013 set, we obtain BLEU scores
of 29.6 (ci) and 28.30 (cs) with the inflection pre-
diction system mes-inflection (marked in table 5).
5 Russian-English
The preparation of the Russian data includes the
following stages: (1) tokenization and tagging and
(2) morphological reduction.
Tagging and tagging errors For tagging, we use
a version of RFTagger (Schmid and Laws, 2008)
2Pairwise bootstrap resampling with 1000 samples.
3However, the large inflection-prediction system has a
slightly better NIST score than the baseline (7.63 vs. 7.61).
234
SMT-output predicted generated after post- gloss
with stem-markup in bold print features forms processing
avertissement<Masc><Pl>[N] Masc.Pl avertissements avertissements warnings
sinistre[ADJ] Masc.Pl sinistres sinistres dire
de[P] ? de du from
le[ART] Masc.Sg le the
pentagone<Masc><Sg>[N] Masc.Sg pentagone pentagone pentagon
sur[P] ? sur sur over
de[P] ? de d? of
e?ventuel[ADJ] Fem.Pl e?ventuelles e?ventuelles potential
re?duction<Fem><Pl>[N] Fem.Pl re?ductions re?ductions reductions
de[P] ? de du of
le[ART] Masc.Sg le the
budget<Masc><Sg>[N] Masc.Sg budget budget budget
de[P] ? de de of
le[ART] Fem.Sg la la the
de?fense<Fem><Sg>[N] Fem.Sg de?fense de?fense de?fense
Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.
that has been developed based on data tagged with
TreeTagger (Schmid, 1994) using a model from
Sharoff et al (2008). The data processed by Tree-
Tagger contained errors such as wrong definition
of PoS for adverbs, wrong selection of gender for
adjectives in plural and missing features for pro-
nouns and adverbs. In order to train RFTagger, the
output of TreeTagger was corrected with a set of
empirical rules. In particular, the morphological
features of nominal phrases were made consistent
to train RFTagger: in contrast to TreeTagger, where
morphological features are regarded as part of the
PoS-tag, RFTagger allows for a separate handling
of morphological features and POS tags.
Despite a generally good tagging quality, some
errors seem to be unavoidable due to the ambiguity
of certain grammatical forms in Russian. A good
example of this are neuter nouns that have the same
form in all cases, or feminine nouns, which have
identical forms in singular genitive and plural nom-
inative (Sharoff et al, 2008). Since Russian has no
binding word order, and the case of nouns cannot
be determined on that basis, such errors cannot be
corrected with empirical rules implemented as post-
System BLEU (ci) BLEU (cs)
1 Baseline 24.91 23.40
InflPred 25.31 23.81
InflPred-factored 25.53 24.04
2 Baseline 29.32 27.65
InflPred* 29.07 27.40
InflPred-factored 29.17 27.46
Table 5: Results for French inflection prediction
on the WMT-2012 test set. The marked system (*)
corresponds to the system submitted for manual
evaluation.
processing. Similar errors occur when specifying
the case of adjectives, since the suffixes of adjec-
tives are even less varied as compared to the nouns.
In our application, we hope that this type of error
does not affect the result due to the following sup-
pression of a number of morphological attributes
including the case of adjectives.
Morphological reduction In comparison to
Slavic languages, English is morphologically poor.
For example, English has no morphological at-
tributes for nouns and adjectives to express gender
or case; verbs have no gender either. In contrast,
Russian is morphologically very rich ? there are
e.g. 6 cases and 3 grammatical genders, which
manifest themselves in different suffixes for nouns,
pronouns, adjectives and some verb forms. When
translating from Russian into English, many of
these attributes are (hopefully) redundant and are
therefore deleted from the training data. The mor-
phological reduction in our system was applied to
nouns, pronouns, verbs, adjectives, prepositions
and conjunctions. The rest of the POS (adverbs,
particles, interjections and abbreviations) have no
morphological attributes. The list of the original
and the reduced attributes is given in Table 6.
Transliteration mining to handle OOVs The
machine translation system fails to translate out-of-
vocabulary words (OOVs) as they are unknown to
the training data. Most of the OOVs are named en-
tities and transliterating them to the target language
script could solve this problem. The transliteration
system requires a list of transliteration pairs for
training. As we do not have such a list, we use
the unsupervised transliteration mining system of
Sajjad et al (2012) that takes a list of word pairs for
235
Part of Attributes Reduced
Speech RFTagger attributes
Noun Type Type
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep gen,notgen
Animate
Case 2
Pronoun Person Person
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep nom,notnom
Syntactic type
Animated
Verb Type Type
VForm VForm
Tense Tense
Person Person
Number Number
Gender
Voice Voice
Definiteness
Aspect Aspect
Case
Adjec- Type Type
tive Degree Degree
Gender
Number
Case
Definiteness
Prep- Type
osition Formation
Case
Conjunc- Type Type
tion Formation Formation
Table 6: Rules for simplifying the morphological
complexity for RU.
training and extracts transliteration pairs that can
be used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows: We
word-align the parallel corpus using GIZA++ and
symmetrize the alignments using the grow-diag-
final-and heuristic. We extract all word pairs which
occur as 1-to-1 alignments (Sajjad et al, 2011) and
later refer to them as a list of word pairs. We train
the unsupervised transliteration mining system on
the list of word pairs and extract transliteration
pairs. We use these mined pairs to build a transliter-
ation system using the Moses toolkit. The translit-
eration system is applied as a post-processing step
to transliterate OOVs.
The morphological reduction of Russian (cf. sec-
tion 5) does not process most of the OOVs as they
are also unknown to the POS tagger. So OOVs that
we get are in their original form. When translit-
Original corpus
SYS WMT-2012 WMT-2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS WMT-2012 WMT-2013
GIZA++ 31.22 24.3
TA-GIZA++ 31.40 24.45
Table 7: Russian to English machine translation
system evaluated on WMT-2012 and WMT-2013.
Human evaluation in WMT13 is performed on the
system trained using the original corpus with TA-
GIZA++ for alignment (marked with *).
erating them, the inflected forms generate wrong
English transliterations as inflectional suffixes get
transliterated too, specially OOV named entities.
We solved this problem by stemming the OOVs
based on a list of suffixes ( , , , , , ) and
transliterating the stemmed forms.
Experiments and results We trained the sys-
tems separately on GIZA++ and transliteration
augmented-GIZA++ (TA-GIZA++) to compare
their results; for more details see Sajjad et al
(2013). All systems are tuned using PROv1 (Nakov
et al, 2012). The translation output is post-
processed to transliterate OOVs.
Table 7 summarizes the results of RU-EN trans-
lation systems trained on the original corpus and
on the morph-reduced corpus. Using TA-GIZA++
alignment gives the best results for both WMT-
2012 and WMT-2013, leading to an improvement
of 0.4 BLEU points.
The system built on the morph-reduced data
leads to decreased BLEU results. However, the per-
centage of OOVs is reduced for both test sets when
using the morph-reduced data set compared to the
original data. An analysis of the output showed
that the morph-reduced system makes mistakes in
choosing the right tense of the verb, which might
be one reason for this outcome. In the future, we
would like to investigate this issue in detail.
6 German to English and English to
German
We submitted systems for DE-EN and EN-DE
which used constituent parses for pre-reordering.
For DE-EN we also deal with word formation is-
sues such as compound splitting. We did not per-
form inflectional normalization or generation for
German due to time constraints, instead focusing
236
our efforts on these issues for French and Russian
as previously described.
German to English German has a wider diver-
sity of clausal orderings than English, all of which
need to be mapped to the English SVO order. This
is a difficult problem to solve during inference, as
shown for hierarchical SMT by Fabienne Braune
and Fraser (2012) and for phrase-based SMT by
Bisazza and Federico (2012).
We syntactically parsed all of the source side
sentences of the parallel German to English data
available, and the tuning, test and blindtest sets.
We then applied reordering rules to these parses.
We use the rules for reordering German constituent
parses of Collins et al (2005) together with the
additional rules described by Fraser (2009). These
are applied as a preprocess to all German data.
For parsing the German sentences, we used the
generative phrase-structure parser BitPar with opti-
mizations of the grammar, as described by Fraser
et al (2013). The parser was trained on the Tiger
Treebank (Brants et al, 2002) along with utilizing
the Europarl corpus as unlabeled data. At the train-
ing of Bitpar, we followed the targeted self-training
approach (Katz-Brown et al, 2011) as follows. We
parsed the whole Europarl corpus using a grammar
trained on the Tiger corpus and extracted the 100-
best parse trees for each sentence. We selected the
parse tree among the 100 candidates which got the
highest usefulness scores for the reordering task.
Then we trained a new grammar on the concatena-
tion of the Tiger corpus and the automatic parses
from Europarl.
The usefulness score estimates the value of a
parse tree for the reordering task. We calculated
this score as the similarity between the word order
achieved by applying the parse tree-based reorder-
ing rules of Fraser (2009) and the word order indi-
cated by the automatic word alignment between
the German and English sentences in Europarl.
We used the Kendall?s Tau Distance as the simi-
larity metric of two word orderings (as suggested
by Birch and Osborne (2010)).
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics. We also split German
portmanteaus like zum? zu dem (meaning to the).
system BLEU BLEU system name
(ci) (cs)
DE-EN (OSM) 27.60 26.12 MES
DE-EN (OSM) 27.48 25.99 not submitted
BitPar not self-trained
DE-EN (Moses) 27.14 25.65 MES-Szeged-
reorder-split
DE-EN (Moses) 26.82 25.36 not submitted
BitPar not self-trained
EN-DE (Moses) 19.68 18.97 MES-reorder
Table 8: Results on WMT-2013 (blindtest)
English to German The task of mapping En-
glish SVO order to the different clausal orders in
German is difficult. For our English to German
systems, we solved this by parsing the English and
applying the system of Gojun and Fraser (2012) to
reorder English into the correct German clausal or-
der (depending on the clause type which is detected
using the English parse, see (Gojun and Fraser,
2012) for further details).
We primarily used the Charniak-Johnson gener-
ative parser (Charniak and Johnson, 2005) to parse
the English Europarl data and the test data. How-
ever, due to time constraints we additionally used
Berkeley parses of about 400K Europarl sentences
and the other English parallel training data. We
also left a small amount of the English parallel
training data unparsed, which means that it was
not reordered. For tune, test and blindtest (WMT-
2013), we used the Charniak-Johnson generative
parser.
Experiments and results We used all available
training data for constrained systems; results for
the WMT-2013 set are given in table 8. For the
contrastive BitPar results, we reparsed WMT-2013.
7 Conclusion
We presented 5 systems dealing with complex mor-
phology. For two language pairs with a morpho-
logically rich source language (FR and RU), the
input was reduced to a simplified representation
containing only translation-relevant morphologi-
cal information (e.g. number on nouns). We also
used reordering techniques for DE-EN and EN-DE.
For translating into a language with rich morphol-
ogy (EN-FR), we applied a two-step method that
first translates into a stemmed representation of
the target language and then generates inflected
forms based on morphological features predicted
on monolingual data.
237
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions, Daniel
Quernheim for providing Berkeley parses of some
of the English data, Stefan Ru?d for help with the
manual evalution, and Philipp Koehn and Barry
Haddow for providing data and alignments.
Nadir Durrani was funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n. 287658. Alexan-
der Fraser was funded by Deutsche Forschungs-
gemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation and from the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Marion Weller was funded from the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Svetlana Smekalova was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Trans-
lation. Helmut Schmid and Max Kisselew were
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Richa?rd Farkas was supported by
the European Union and the European Social Fund
through project FuturICT.hu (grant n. TA?MOP-
4.2.2.C-11/1/KONV-2012-0013). This publication
only reflects the authors? views.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Build-
ing a treebank for french. In A. Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of ACL WMT and MetricsMATR, Upp-
sala, Sweden.
Arianna Bisazza and Marcello Federico. 2012. Mod-
ified distortion matrices for phrase-based statistical
machine translation. In ACL, pages 478?487.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL, pages 173?180, Ann Arbor, MI,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Porceedings of ACL 2005.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL-HLT
2011, Portland, Oregon, USA.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In Proceedings of NAACL
2013, Atlanta, Georgia, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013b. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Anita Gojun Fabienne Braune and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
EAMT 2012.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of EACL 2012,
Avignon, France.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In EACL WMT.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to avoid burning ducks: Combining linguistic analy-
sis and corpus statistics for German compound pro-
cessing. In ACL WMT and Metrics MATR.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL 2010, pages 504?513.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. Mumbai, India.
238
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of ACL 2011, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
ACL 2012, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008, Stroudsburg, PA, USA.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: a German Computational Morphology Cov-
ering Derivation, Composition, and Inflection. In
Proceedings of LREC 2004.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In Proceedings of IJCNLP 2008.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating russian tagsets. In Proceedings of LREC
2008.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-HLT
2008.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diploma Thesis, Insti-
tute for Natural Language Processing, University of
Stuttgart.
239
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97?104,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Phrase-based Machine Translation Systems for WMT-14
Nadir Durrani Barry Haddow Philipp Koehn
School of Informatics
University of Edinburgh
{dnadir,bhaddow,pkoehn}@inf.ed.ac.uk
Kenneth Heafield
Computer Science Department
Stanford University
heafield@cs.stanford.edu
Abstract
This paper describes the University of Ed-
inburgh?s (UEDIN) phrase-based submis-
sions to the translation and medical trans-
lation shared tasks of the 2014 Work-
shop on Statistical Machine Translation
(WMT). We participated in all language
pairs. We have improved upon our 2013
system by i) using generalized represen-
tations, specifically automatic word clus-
ters for translations out of English, ii) us-
ing unsupervised character-based models
to translate unknown words in Russian-
English and Hindi-English pairs, iii) syn-
thesizing Hindi data from closely-related
Urdu data, and iv) building huge language
on the common crawl corpus.
1 Translation Task
Our baseline systems are based on the setup de-
scribed in (Durrani et al., 2013b) that we used
for the Eighth Workshop on Statistical Machine
Translation (Bojar et al., 2013). The notable fea-
tures of these systems are described in the follow-
ing section. The experiments that we carried out
for this year?s translation task are described in the
following sections.
1.1 Baseline
We trained our systems with the following set-
tings: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, hierarchical lexicalized re-
ordering (Galley and Manning, 2008), a lexically-
driven 5-gram operation sequence model (OSM)
(Durrani et al., 2013a) with 4 count-based sup-
portive features, sparse domain indicator, phrase
length, and count bin features (Blunsom and Os-
borne, 2008; Chiang et al., 2009), a distortion limit
of 6, maximum phrase-length of 5, 100-best trans-
lation options, Minimum Bayes Risk decoding
(Kumar and Byrne, 2004), Cube Pruning (Huang
and Chiang, 2007), with a stack-size of 1000
during tuning and 5000 during test and the no-
reordering-over-punctuation heuristic (Koehn and
Haddow, 2009). We used POS and morphologi-
cal tags as additional factors in phrase translation
models (Koehn and Hoang, 2007) for German-
English language pairs. We also trained target se-
quence models on the in-domain subset of the par-
allel corpus using Kneser-Ney smoothed 7-gram
models. We used syntactic-preordering (Collins
et al., 2005) and compound splitting (Koehn and
Knight, 2003) for German-to-English systems.
We used trivia tokenizer for tokenizing Hindi.
The systems were tuned on a very large tun-
ing set consisting of the test sets from 2008-2012,
with a total of 13,071 sentences. We used news-
test 2013 for the dev experiments. For Russian-
English pairs news-test 2012 was used for tuning
and for Hindi-English pairs, we divided the news-
dev 2014 into two halves, used the first half for
tuning and second for dev experiments.
1.2 Using Generalized Word Representations
We explored the use of automatic word clusters
in phrase-based models (Durrani et al., 2014a).
We computed the clusters with GIZA++?s mkcls
(Och, 1999) on the source and target side of the
parallel training corpus. Clusters are word classes
that are optimized to reduce n-gram perplexity.
By generating a cluster identifier for each out-
put word, we are able to add an n-gram model
97
over these identifiers as an additional scoring func-
tion. The inclusion of such an additional factor
is trivial given the factored model implementation
(Koehn and Hoang, 2007) of Moses (Koehn et al.,
2007). The n-gram model is trained in the similar
way as the regular language model. We trained
domain-specific language models separately and
then linearly interpolated them using SRILM with
weights optimized on the tuning set (Schwenk and
Koehn, 2008).
We also trained OSM models over cluster-ids
(?). The lexically driven OSM model falls back to
very small context sizes of two to three operations
due to data sparsity. Learning operation sequences
over cluster-ids enables us to learn richer trans-
lation and reordering patterns that can generalize
better in sparse data conditions. Table 1 shows
gains from adding target LM and OSM models
over cluster-ids. Using word clusters was found
more useful translating from English-to-*.
from English into English
Lang B
0
+Cid ? B
0
+Cid ?
de 20.60 20.85 +0.25 27.44 27.34 -0.10
cs 18.84 19.39 +0.55 26.42 26.42 ?0.00
fr 30.73 30.82 +0.09 31.64 31.76 +0.12
ru 18.78 19.67 +0.89 24.45 24.63 +0.18
hi 10.39 10.52 +0.13 15.48 15.26 -0.22
Table 1: Using Word Clusters in Phrase-based and
OSM models ? B
0
= System without Clusters,
+Cid = with Cluster
We also trained OSM models over POS and
morph tags. For the English-to-German sys-
tem we added an OSM model over [pos, morph]
(source:pos, target:morph) and for the German-
to-English system we added an OSM model over
[morph,pos] (source:morph, target:pos), a config-
uration that was found to work best in our previous
experiments (Birch et al., 2013). Table 2 shows
gains from additionally using OSM models over
POS/morph tags.
Lang B
0
+OSM
p,m
?
en-de 20.44 20.60 +0.16
de-en 27.24 27.44 +0.20
Table 2: Using POS and Morph Tags in
OSM models ? B
0
= Baseline, +OSM
p,m
=
POS/Morph-based OSM
1.3 Unsupervised Transliteration Model
Last year, our Russian-English systems performed
badly on the human evaluation. In comparison
other participants that used transliteration did well.
We could not train a transliteration system due
to unavailability of a transliteration training data.
This year we used an EM-based method to in-
duce unsupervised transliteration models (Durrani
et al., 2014b). We extracted transliteration pairs
automatically from the word-aligned parallel data
and used it to learn a transliteration system. We
then built transliteration phrase-tables for trans-
lating OOV words and used the post-decoding
method (Method 2 as described in the paper) to
translate these.
Pair Training OOV B
0
+T
r
?
ru-en 232K 1356 24.63 25.06 +0.41
en-ru 232K 681 19.67 19.91 +0.24
hi-en 38K 503 14.67 15.48 +0.81
en-hi 38K 394 11.76 12.83 +1.07
Table 3: Using Unsupervised Transliteration
Model ? Training = Extracted Transliteration Cor-
pus (types), OOV = Out-of-vocabulary words (to-
kens) B
0
= System without Transliteration, +T
r
= Transliterating OOVs
Table 3 shows the number (types) of translit-
eration pairs extracted using unsupervised min-
ing, number of OOV words (tokens) in each pair
and the gains achieved by transliterating unknown
words.
1.4 Synthesizing Hindi Data from Urdu
Hindi and Urdu are closely related language pairs
that share grammatical structure and have a large
overlap in vocabulary. This provides a strong
motivation to transform any Urdu-English paral-
lel data into Hindi-English by translating the Urdu
part into Hindi. We made use of the Urdu-English
segment of the Indic multi-parallel corpus (Post
et al., 2012) which contains roughly 87K sentence
pairs. The Hindi-English segment of this corpus
is a subset of parallel data made available for the
translation task but is completely disjoint from the
Urdu-English segment.
We initially trained a Urdu-to-Hindi SMT sys-
tem using a very tiny EMILLE
1
corpus (Baker
1
EMILLE corpus contains roughly 12000 sentences of
Hindi and Urdu comparable data. From these we were able
to sentence align 7000 sentences to build an Urdu-to-Hindi
system.
98
et al., 2002). But we found this system to be use-
less for translating the Urdu part of Indic data due
to domain mismatch and huge number of OOV
words (approximately 310K tokens). To reduce
sparsity we synthesized additional phrase-tables
using interpolation and transliteration.
Interpolation: We trained two phrase transla-
tion tables p(u?
i
|e?
i
) and p(e?
i
|
?
h
i
), from Urdu-
English (Indic corpus) and Hindi-English (Hin-
dEnCorp (Bojar et al., 2014)) bilingual cor-
pora. Given the phrase-table for Urdu-English
p(u?
i
|e?
i
) and the phrase-table for English-Hindi
p(e?
i
|
?
h
i
), we estimated a Urdu-Hindi phrase-table
p(u?
i
|
?
h
i
) using the well-known convolution model
(Utiyama and Isahara, 2007; Wu and Wang, 2007):
p(u?
i
|
?
h
i
) =
?
e?
i
p(u?
i
|e?
i
)p(e?
i
|
?
h
i
)
The number of entries in the baseline Urdu-to-
Hindi phrase-table were approximately 254K. Us-
ing interpolation we were able to build a phrase-
table containing roughly 10M phrases. This re-
duced the number of OOV tokens from 310K to
approximately 50K.
Transliteration: Urdu and Hindi are written in
different scripts (Arabic and Devanagri respec-
tively). We added a transliteration component
to our Urdu-to-Hindi system. An unsupervised
transliteration model is learned from the word-
alignments of Urdu-Hindi parallel data. We were
able to extract around 2800 transliteration pairs.
To learn a richer transliteration model, we addi-
tionally fed the interpolated phrase-table, as de-
scribed above, to the transliteration miner. We
were able to mine additional 21000 translitera-
tion pairs and built a Urdu-Hindi character-based
model from it. The transliteration module can
be used to translate the 50K OOV words but
previous research (Durrani et al., 2010; Nakov
and Tiedemann, 2012) has shown that translit-
eration is useful for more than just translating
OOV words when translating closely related lan-
guage pairs. To fully capitalize on the large over-
lap in Hindi?Urdu vocabulary, we transliterated
each word in the Urdu test-data into Hindi and
produced a phrase-table with 100-best transliter-
ations. The two synthesized (triangulated and
transliterated) phrase-tables are then used along
with the baseline Urdu-to-Hindi phrase-table in
a log-linear model. Detailed results on Urdu-to-
Hindi baseline and improvements obtained from
using transliteration and triangulated phrase-tables
are presented in Durrani and Koehn (2014). Using
our best Urdu-to-Hindi system, we translated the
Urdu part of the multi-indic corpus to form Hindi-
English parallel data. Table 4 shows results from
using the synthesized Hindi-English corpus in iso-
lation (Syn) and on top of the baseline system
(B
0
+ Syn).
Pair B
0
Syn ? B
0
+ Syn ?
hi-en 14.28 10.49 -3.79 14.72 +0.44
en-hi 10.59 9.01 -1.58 11.76 +1.17
Table 4: Evaluating Synthesized (Syn) Hindi-
English Parallel Data, B
0
= System without Syn-
thesized Data
1.5 Huge Language Models
Our unconstrained submissions use an additional
language model trained on web pages from the
2012, 2013, and winter 2013 CommonCrawl.
2
The additional language model is the only differ-
ence between the constrained and unconstrained
submissions; we did not use additional parallel
data. These language models were trained on text
provided by the CommonCrawl foundation, which
they converted to UTF-8 after stripping HTML.
Languages were detected using the Compact Lan-
guage Detection 2
3
and, except for Hindi where
we lack tools, sentences were split with the Eu-
roparl sentence splitter (Koehn, 2005). All text
was then deduplicated, minimizing the impact of
boilerplate, such as social media sharing buttons.
We then tokenized and truecased the text as usual.
Statistics are shown in Table 5. A full description
of the pipeline, including a public data release, ap-
pears in Buck et al. (2014).
Lang Lines (B) Tokens (B) Bytes
en 59.13 975.63 5.14 TiB
de 3.87 51.93 317.46 GiB
fr 3.04 49.31 273.96 GiB
ru 1.79 21.41 220.62 GiB
cs 0.47 5.79 34.67 GiB
hi 0.01 0.28 3.39 GiB
Table 5: Size of huge language model training data
We built unpruned modified Kneser-Ney lan-
guage models using lmplz (Heafield et al., 2013).
2
http://commoncrawl.org
3
https://code.google.com/p/cld2/
99
Pair B
0
+L
newstest 2013 2014 2013 2014
en-de 20.85 20.10 ? 20.61 +0.51
en-cs 19.39 21.00 20.03 +0.64 21.60 +0.60
en-ru 19.90 28.70 20.80 +0.90 29.90 +1.20
en-hi 11.43 11.10 12.83 +1.40 12.50 +1.40
hi-en 15.48 13.90 ? 14.80 +0.90
Table 6: Gains obtained by using huge language
models ? B
0
= Baseline, +L = Adding Huge LM
While the Hindi and Czech models are small
enough to run directly, models for other languages
are quite large.We therefore created a filter that op-
erates directly on files in KenLM trie binary for-
mat, preserving only n-grams whose words all ap-
pear in the target side vocabulary of at least one
source sentence. For example, an English lan-
guage model trained on just the 2012 and 2013
crawls takes 3.5 TB without any quantization. Af-
ter filtering to the Hindi-English tuning set, the
model fit in 908 GB, again without quantization.
We were then able to tune the system on a machine
with 1 TB RAM. Results are shown in Table 6; we
did not submit to English-French because the sys-
tem takes too long to tune.
1.6 Miscellaneous
Hindi-English: 1) A large number of Hindi sen-
tences in the Hindi-English parallel corpus were
ending with a full-stop ?.?, although the end-of-
the-sentence marker in Hindi is ?Danda? (|). Re-
placing full-stops with Danda gave improvement
of +0.20 for hi-en and +0.40 in en-hi. 2) Using
Wiki subtitles did not give any improvement in
BLEU and were in fact harmful for the en-hi di-
rection.
Russian-English: We tried to improve word-
alignments by integrating a transliteration sub-
model into GIZA++ word aligner. The probabil-
ity of a word pair is calculated as an interpola-
tion of the transliteration probability and transla-
tion probability stored in the t-table of the differ-
ent alignment models used by the GIZA++ aligner.
This interpolation is done for all iterations of all
alignment models (See Sajjad et al. (2013) for de-
tails). Due to shortage of time we could only run it
for Russian-to-English. The improved alignments
gave a gain of +0.21 on news-test 2013 and +0.40
on news-test 2014.
Pair GIZA++ Fast Align ?
de-en 24.02 23.89 ?.13
fr-en 30.78 30.66 ?.12
es-en 34.07 34.24 +.17
cs-en 22.63 22.44 ?.19
ru-en 31.68 32.03 +.35
en-de 18.04 17.88 ?.16
en-fr 28.96 28.83 ?.13
en-es 34.15 34.32 +.17
en-cs 15.70 16.02 +.32
avg +.03
Table 7: Comparison of fast word alignment
method (Dyer et al., 2013) against GIZA++
(WMT 2013 data condition, test on new-
stest2012). The method was not used in the official
submission.
Pair Baseline MSD Hier. MSD Hier. MSLR
de-en 27.04 27.10 +.06 27.17 +.13
fr-en 31.63 - 31.65 +.02
es-en 31.20 31.14 ?.06 31.25 +.05
cs-en 26.11 26.32 +.21 26.26 +.15
ru-en 24.09 24.01 ?.08 24.19 +.11
en-de 20.43 20.34 ?.09 20.32 -.11
en-fr 30.54 - 30.52 ?.02
en-es 30.36 30.44 +.08 30.51 +.15
en-cs 18.53 18.59 +.06 18.66 +.13
en-ru 18.37 18.47 +.10 18.19 ?.18
avg + .035 +.045
Table 8: Hierarchical lexicalized reordering model
(Galley and Manning, 2008).
Fast align: In preliminary experiments, we
compared the fast word alignment method by
Dyer et al. (2013) against our traditional use of
GIZA++. Results are quite mixed (Table 7), rang-
ing from a gain of +.35 for Russian-English to a
loss of ?.19 for Czech-English. We stayed with
GIZA++ for all of our other experiments.
Hierarchical lexicalized reordering model:
We explored the use of the hierarchical lexicalized
reordering model (Galley and Manning, 2008)
in two variants: using the same orientations as
our traditional model (monotone, discontinuous,
swap), and one that distinguishes the discontin-
uous orientations to the left and right. Table 8
shows slight improvements with these models, so
we used them in our baseline.
Threshold filtering of phrase table: We exper-
imented with discarding some phrase table entry
due to their low probability. We found that phrase
translations with the phrase translation probability
100
?(f |e)<10
?4
can be safely discarded with almost
no change in translations. However, discarding
phrase translations with the inverse phrase transla-
tion probability ?(e|f)<10
?4
is more risky, espe-
cially with morphologically rich target languages,
so we kept those.
1.7 Summary
Table 9 shows cumulative gains obtained from us-
ing word classes, transliteration and big language
models
4
over the baseline system. Our German-
English constrained systems were used for EU-
Bridge system combination, a collaborative effort
to improve the state-of-the-art in machine transla-
tion (See Freitag et al. (2014) for details).
from English into English
Lang B
0
B
1
? B
0
B
1
?
de 20.44 20.85 +0.41 27.24 27.44 +0.20
cs 18.84 20.03 +1.19 26.42 26.42 ?0.00
fr 30.73 30.82 +0.09 31.64 31.76 +0.12
ru 18.78 20.81 +2.03 24.45 25.21 +0.76
hi 9.27 12.83 +3.56 14.08 15.48 +1.40
Table 9: Cumulative gains obtained for each lan-
guage ? B
0
= Baseline, B
1
= Best System
2 Medical Translation Task
For the medical translation task, the organisers
supplied several medical domain corpora (detailed
on the task website), as well some out-of-domain
patent data, and also all the data available for the
constrained track of the news translation task was
permitted. In general, we attempted to use all of
this data, except for the LDC Gigaword language
model data (for reasons of time) and we divided
the data into ?in-domain? and ?out-of-domain?
corpora. The data sets are summarised in Tables
10 and 11.
In order to create systems for the medical trans-
lation tasks, we used phrase-based Moses with ex-
actly the same settings as for the news translation
task, including the OSM (Durrani et al., 2011),
and compound splitting Koehn and Knight (2003)
for German source. We did not use word clusters
(Section 1.2), as they did not give good results on
this task, but we have yet to find a reason for this.
For language model training, we decided not to
build separate models on each corpus as there was
4
Cumulative gains do not include gains obtain from big
language models for hi-en and en-de.
Data Set cs-en de-en fr-en
coppa-in n n y
PatTR-in-claims n y y
PatTR-in-abstract n y y
PatTR-in-titles n y y
UMLS y y y
MuchMore n y n
EMEA y y y
WikiTitles y y y
PatTR-out n y y
coppa-out n n y
MultiUN n n y
czeng y n n
europarl y y y
news-comm y y y
commoncrawl y y y
FrEnGiga n n y
Table 10: Parallel data sets used in the medical
translation task. The sets above the line were clas-
sified as ?in-domain? and those below as ?out-of-
domain?.
Data Set cs de en fr
PIL n n y n
DrugBank n n y n
WikiArticles y y y y
PatTR-in-description n y y y
GENIA n n y n
FMA n n y n
AACT n n y n
PatTR-out-description n y y y
Table 11: Additional monolingual data used in
the medical translation task. Those above the line
were classified as ?in-domain? and the one below
as ?out-of-domain?. We also used the target sides
of all the parallel corpora for language modelling.
a large variation in corpus sizes. Instead we con-
catenated the in-domain target sides with the in-
domain extra monolingual data to create training
data for an in-domain language model, and simi-
larly for the out-of-domain data. The two language
models were interpolated using SRILM, minimis-
ing perplexity on the Khresmoi summary develop-
ment data.
During system development, we only had 500
sentences of development data (SUMMARY-DEV)
from the Khresmoi project, so we decided to se-
lect further development and devtest data from the
EMEA corpus, reasoning that it was fairly close
in domain to SUMMARY-DEV. We selected a tun-
ing set (5000 sentence pairs, which were added to
SUMMARY-DEV) and a devtest set (3000 sentence
pairs) from EMEA after first de-duplicating it, and
ignoring sentence pairs which were too short, or
101
contained too many capital letters or numbers. The
EMEA contains many duplicated sentences, and
we removed all sentence pairs where either side
was a duplicate, reducing the size of the corpus
to about 25% of the original. We also removed
EMEA from Czeng, since otherwise it would over-
lap with our selected development sets.
We also experimented with modified Moore-
Lewis (Moore and Lewis, 2010; Axelrod et al.,
2011) data selection, using the EMEA corpus as
the in-domain corpus (for the language model re-
quired in MML) and selecting from all the out-of-
domain data.
When running on the final test set (SUMMARY-
TEST) we found that it was better to tune just on
SUMMARY-DEV, even though it was much smaller
than the EMEA dev set we had selected. All but
two (cs-en, de-en) of our submitted systems used
the MML selection, because it worked better on
our EMEA devtest set. However, as can be seen
from Table 12, systems built with all the data gen-
erally perform better. We concluded that EMEA
was not a good representative of the Khresmoi
data, perhaps because of domain differences, or
perhaps just because of the alignment noise that
appears (from informal inspection) to be present
in EMEA.
from English into English
in in+20 in+out in in+20 in+out
de 18.59 20.88 ? 36.17 ? 38.57
cs 18.78 23.45 23.77 30.12 ? 36.32
fr 35.24 40.74 41.04 45.15 46.44 46.58
Table 12: Results (cased BLEU) on the khresmoi
summary test set. The ?in? systems include all
in-domain data, the ?in+20? systems also include
20% of the out-of-domain data and the ?out? sys-
tems include all data. The submitted systems are
shown in italics, except for de-en and cs-en where
we submitted a ?in+out? systems. For de-en, this
was tuned on SUMMARY-DEV plus the EMEA dev
set and scored 37.31, whilst for cs-en we included
LDC Giga in the LM, and scored 36.65.
For translating the Khresmoi queries, we used
the same systems as for the summaries, except that
generally we did not retune on the SUMMARY-DEV
data. We added a post-processing script to strip
out extraneous stop words, which improved BLEU,
but we would not expect it to matter in a real CLIR
system as it would do its own stop-word removal.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements n
?
287658 (EU-BRIDGE),
n
?
287688 (MateCat) and n
?
288769 (ACCEPT).
Huge language model experiments made use of
the Stampede supercomputer provided by the
Texas Advanced Computing Center (TACC) at
The University of Texas at Austin under NSF
XSEDE allocation TG-CCR140009. We also ac-
knowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Op-
erational Language Translation (BOLT) program
through IBM. This publication only reflects the
authors? views.
References
Axelrod, A., He, X., and Gao, J. (2011). Domain
adaptation via pseudo in-domain data selection.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK.
Association for Computational Linguistics.
Baker, P., Hardie, A., McEnery, T., Cunningham,
H., and Gaizauskas, R. J. (2002). EMILLE,
a 67-million word corpus of indic languages:
Data collection, mark-up and harmonisation. In
LREC.
Birch, A., Durrani, N., and Koehn, P. (2013). Ed-
inburgh SLT and MT system description for the
IWSLT 2013 evaluation. In Proceedings of the
10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg,
Germany.
Blunsom, P. and Osborne, M. (2008). Probabilis-
tic inference for machine translation. In Pro-
ceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 215?223, Honolulu, Hawaii. Association
for Computational Linguistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 workshop on statistical
machine translation. In Eighth Workshop on
Statistical Machine Translation, WMT-2013,
pages 1?44, Sofia, Bulgaria.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
102
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth In-
ternational Language Resources and Evalua-
tion Conference (LREC?14), Reykjavik, Ice-
land. ELRA, European Language Resources
Association. in prep.
Buck, C., Heafield, K., and van Ooyen, B. (2014).
N-gram counts and language models from the
common crawl. In Proceedings of the Language
Resources and Evaluation Conference, Reyk-
jav??k, Iceland.
Chiang, D., Knight, K., and Wang, W. (2009).
11,001 New Features for Statistical Machine
Translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics, pages
218?226, Boulder, Colorado. Association for
Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Durrani, N., Fraser, A., Schmid, H., Hoang, H.,
and Koehn, P. (2013a). Can markov mod-
els over minimal translation units help phrase-
based SMT? In Proceedings of the 51st An-
nual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria. Association
for Computational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Durrani, N. and Koehn, P. (2014). Improving ma-
chine translation via triangulation and transliter-
ation. In Proceedings of the 17th Annual Con-
ference of the European Association for Ma-
chine Translation (EAMT), Dubrovnik, Croatia.
Durrani, N., Koehn, P., Schmid, H., and Fraser,
A. (2014a). Investigating the usefulness of
generalized word representations in SMT. In
Proceedings of the 25th Annual Conference on
Computational Linguistics (COLING), Dublin,
Ireland. To Appear.
Durrani, N., Sajjad, H., Fraser, A., and Schmid,
H. (2010). Hindi-to-urdu machine translation
through transliteration. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, pages 465?474, Up-
psala, Sweden. Association for Computational
Linguistics.
Durrani, N., Sajjad, H., Hoang, H., and Koehn, P.
(2014b). Integrating an unsupervised translit-
eration model into statistical machine transla-
tion. In Proceedings of the 15th Conference of
the European Chapter of the ACL (EACL 2014),
Gothenburg, Sweden. Association for Compu-
tational Linguistics.
Durrani, N., Schmid, H., and Fraser, A. (2011).
A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Tech-
nologies, pages 1045?1054, Portland, Oregon,
USA.
Dyer, C., Chahuneau, V., and Smith, N. A. (2013).
A simple, fast, and effective reparameterization
of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, pages 644?
648, Atlanta, Georgia. Association for Compu-
tational Linguistics.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). EU-BRIDGE MT:
combined machine translation. In Proceedings
of the ACL 2014 Ninth Workshop on Statistical
Machine Translation, Baltimore, MD, USA.
Galley, M. and Manning, C. D. (2008). A sim-
ple and effective hierarchical phrase reorder-
ing model. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 848?856, Honolulu,
Hawaii.
Heafield, K. (2011). Kenlm: Faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Trans-
lation, pages 187?197, Edinburgh, Scotland,
United Kingdom.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and
Koehn, P. (2013). Scalable modified Kneser-
Ney language model estimation. In Proceedings
103
of the 51st Annual Meeting of the Association
for Computational Linguistics, Sofia, Bulgaria.
Huang, L. and Chiang, D. (2007). Forest rescor-
ing: Faster decoding with integrated language
models. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 144?151, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings
of MT Summit.
Koehn, P. and Haddow, B. (2009). Edinburgh?s
Submission to all Tracks of the WMT 2009
Shared Task with Reordering and Speed Im-
provements to Moses. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation, pages 160?164, Athens, Greece. Associ-
ation for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868?876, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In ACL 2007 Demonstrations,
Prague, Czech Republic.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Kumar, S. and Byrne, W. J. (2004). Mini-
mum bayes-risk decoding for statistical ma-
chine translation. In HLT-NAACL, pages 169?
176.
Moore, R. C. and Lewis, W. (2010). Intelligent
selection of language model training data. In
Proceedings of the ACL 2010 Conference Short
Papers, pages 220?224, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Nakov, P. and Tiedemann, J. (2012). Combining
word-level and character-level models for ma-
chine translation between closely-related lan-
guages. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages
301?305, Jeju Island, Korea. Association for
Computational Linguistics.
Och, F. J. (1999). An efficient method for deter-
mining bilingual word classes. In Ninth Confer-
ence the European Chapter of the Association
for Computational Linguistics (EACL), pages
71?76.
Post, M., Callison-Burch, C., and Osborne, M.
(2012). Constructing parallel corpora for six in-
dian languages via crowdsourcing. In Proceed-
ings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 401?409, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at wmt13: Using transliteration mining to
improve statistical machine translation. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Schwenk, H. and Koehn, P. (2008). Large and di-
verse language models for statistical machine
translation. In International Joint Conference
on Natural Language Processing, pages 661?
666.
Utiyama, M. and Isahara, H. (2007). A compar-
ison of pivot methods for phrase-based statis-
tical machine translation. In 2007 Meeting of
the North American Chapter of the Association
for Computational Linguistics (NAACL), pages
484?491.
Wu, H. and Wang, H. (2007). Pivot language
approach for phrase-based statistical machine
translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 856?863, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
104
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
