Creating a Finite-State Parser with Application Semantics
Owen Rambow
University of Pennsylvania
Philadelphia, PA 19104
USA
Srinivas Bangalore
AT&T Labs ? Research
Florham Park, NJ 07932
USA
Tahir Butt
Johns Hopkins University
Baltimore, MD 21218
USA
Alexis Nasr
Universite? Paris 7
75005 Paris
France
Richard Sproat
AT&T Labs ? Research
Florham Park, NJ 07932
USA
rambow@unagi.cis.upenn.edu
Abstract
Parsli is a finite-state (FS) parser which can be
tailored to the lexicon, syntax, and semantics
of a particular application using a hand-editable
declarative lexicon. The lexicon is defined in
terms of a lexicalized Tree Adjoining Grammar,
which is subsequently mapped to a FS represen-
tation. This approach gives the application de-
signer better and easier control over the natural
language understanding component than using
an off-the-shelf parser. We present results using
Parsli on an application that creates 3D-images
from typed input.
1 Parsing and Application-Specific
Semantics
One type of Natural Language Understanding
(NLU) application is exemplified by the database
access problem: the user may type in free source
language text, but the NLU component must
map this text to a fixed set of actions dictated
by the underlying application program. We
will call such NLU applications ?application-
semantic NLU?. Other examples of application-
semantic NLU include interfaces to command-
based applications (such as airline reservation
systems), often in the guise of dialog systems.
Several general-purpose off-the-shelf (OTS)
parsers have become widely available (Lin,
1994; Collins, 1997). For application-semantic
NLU, it is possible to use such an OTS parser in
conjunction with a post-processor which trans-
fers the output of the parser (be it phrase struc-
ture or dependency) to the domain semantics. In
addition to mapping the parser output to appli-
cation semantics, the post-processor often must
also ?correct? the output of the parser: the parser
may be tailored for a particular domain (such as
Wall Street Journal (WSJ) text), but the new do-
main presents linguistic constructions not found
in the original domain (such as questions). It
may also be the case that the OTS parser consis-
tently misanalyzes certain lexemes because they
do not occur in the OTS corpus, or occur there
with different syntactic properties. While many
of the parsers can be retrained, often an anno-
tated corpus is not available in the application
domain (since, for example, the application it-
self is still under development and there is not
yet a user community). The process of retraining
may also be quite complex in practice. A further
disadvantage of this approach is that the post-
processor must typically be written by hand, as
procedural code. In addition, the application-
semantic NLU may not even exploit the strengths
of the OTS parser, because the NLU required
for the application is not only different (ques-
tions), but generally simpler (the WSJ contains
very long and syntactically complex sentences
which are not likely to be found as input in in-
teractive systems, including dialog systems).
This discussion suggests that we (i) need an
easy way to specify application semantics for a
parser and (ii) that we do not usually need the full
power of a full recursive parser. In this paper, we
suggest that application-semantic NLP may be
better served by a lexicalized finite-state (FS)
parser. We present PARSLI, a FS parser which
can be tailored to the application semantics us-
ing a hand-editable declarative lexicon. This ap-
proach gives the application designer better and
easier control over the NLU component. Further-
more, while the finite-state approach may not be
sufficient for WSJ text (given its syntactic com-
plexity), it is sufficient for most interactive sys-
tems, and the advantage in speed offered by FS
approaches in more crucial in interactive appli-
cations. Finally, in speech-based systems, the
lattice that is output from the speech recognition
component can easily used as input to a FS-based
parser.
2 Sample Application: WORDSEYE
WORDSEYE (Coyne and Sproat, 2001) is a
system for converting English text into three-
dimensional graphical scenes that represent that
text. WORDSEYE performs syntactic and se-
mantic analysis on the input text, producing a
description of the arrangement of objects in a
scene. An image is then generated from this
scene description. At the core of WORDSEYE
is the notion of a ?pose?, which can be loosely
defined as a figure (e.g. a human figure) in a con-
figuration suggestive of a particular action.
For WORDSEYE, the NLP task is thus to
map from an input sentence to a representation
that the graphics engine can directly interpret in
terms of poses. The graphical component can
render a fixed set of situations (as determined by
its designer); each situation has several actors in
situation-specific poses, and each situation can
be described linguistically using a given set of
verbs. For example, the graphical component
may have a way of depicting a commercial trans-
action, with two humans in particular poses (the
buyer and the seller), the goods being purchased,
and the payment amount. In English, we have
different verbs that can be used to describe this
situation (buy, sell, cost, and so on). These verbs
have different mappings of their syntactic argu-
ments to the components in the graphical repre-
sentation. We assume a mapping from syntax to
domain semantics, leaving to lexical semantics
the question of how such a mapping is devised
and derived. (For many applications, such map-
pings can be derived by hand, with the seman-
tic representation an ad-hoc notation.) We show
a sample of such mapping in Figure 1. Here,
we assume that the graphics engine of WORD-
SEYE knows how to depict a TRANSACTION
when some of the semantic arguments of a trans-
action (such as CUSTOMER, ITEM, AMOUNT)
are specified.
We show some sample transductions in Fig-
ure 2. In the output, syntactic constituents are
bracketed. Following each argument is informa-
tion about its grammatical function (?GF=0? for
example) and about its semantic role (ITEM for
example). If a lexical item has a semantics of
its own, the semantics replaces the lexical item
(this is the case for verbs), otherwise the lexical
item remains in place. In the case of the transi-
tive cost, the verbal semantics in Figure 1 spec-
ifies an implicit CUSTOMER argument. This is
generated when cost is used transitively, as can
be seen in Figure 2.
3 Mapping Tree Adjoining Grammar
to Finite State Machines
What is crucial for being able to define a map-
ping from words to application semantics is a
very abstract notion of grammatical function: in
devising such a mapping, we are not interested
in how English realizes certain syntactic argu-
ments, i.e., in the phrase structure of the verbal
projection. Instead, we just want to be able to re-
fer to syntactic functions, such as subject or indi-
rect object. Tree Adjoining Grammar (TAG) rep-
resents the entire syntactic projection from a lex-
eme in its elementary structures in an elementary
tree; because of this, each elementary tree can
be associated with a lexical item (lexicalization,
(Joshi and Schabes, 1991)). Each lexical item
can be associated with one or more trees which
represent the lexeme?s valency; these trees are
referred to as its supertags. In a derivation, sub-
stituting or adjoining the tree of one lexeme into
that of another creates a direct dependency be-
tween them. The syntactic functions are labeled
with integers starting with zero (to avoid discus-
sions about names), and are retained across op-
erations such as topicalization, dative shift and
passivization.
A TAG consists of a set of elementary trees of
two types, initial trees and auxiliary trees. These
trees are then combined using two operations,
substitution and adjunction. In substitution, an
initial tree is appended to a specially marked
node with the same label as the initial tree?s root
node. In adjunction, a non-substitution node is
rewritten by an auxiliary tree, which has a spe-
cially marked frontier node called the footnode.
The effect is to insert the auxiliary tree into the
middle of the other tree.
We distinguish two types of auxiliary trees.
Adjunct auxiliary trees are used for adjuncts;
they have the property that the footnode is al-
Verb Supertag Verb semantics Argument semantics
paid A nx0Vnx1 transaction 0=Customer 1=Amount
cost A nx0Vnx1 transaction 0=Item 1=Amount Implicit=Customer
cost A nx0Vnx2nx1 transaction 0=Item 1=Amount 2=Customer
bought, purchased A nx0Vnx1 transaction 0=Customer 1=Item
socks A NXN none none
Figure 1: Sample entries for a commercial transaction situation
In: I bought socks
Out: ( ( I ) GF=0 AS=CUSTOMER TRANSACTION ( socks ) GF=1 AS=ITEM )
In:the pajamas cost my mother-in-law 12 dollars
Out: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION ( ( my ) mother-in-law ) GF=2 AS=CUSTOMER ( (
12 ) dollars ) GF=1 AS=AMOUNT )
In: the pajamas cost 12 dollars
Out: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION IMP:CUSTOMER ( ( 12 ) dollars ) GF=1
AS=AMOUNT )
Figure 2: Sample transductions generated by Parsli (?GF? for grammatical function, ?AS? for argu-
ment semantics, ?Imp? for implicit argument)
ways a daughter node of the root node, and the
label on these nodes is not, linguistically speak-
ing, part of the projection of the lexical item of
that tree. For example, an adjective will project
to AdjP, but the root- and footnode of its tree will
be labeled NP, since an adjective adjoins to NP.
We will refer to the root- and footnode of an ad-
junct auxiliary tree as its passive valency struc-
ture. Note that the tree for an adjective also spec-
ifies whether it adjoins from the left (footnode
on right) or right (footnode on left). Predicative
auxiliary trees are projected from verbs which
subcategorize for clauses. Since a verb projects
to a clausal category, and has a node labeled with
a clausal category on its frontier (for the argu-
ment), the resulting tree can be interpreted as an
auxiliary tree, which is useful in analyzing long-
distance wh-movement (Frank, 2001).
To derive a finite-state transducer (FST) from
a TAG, we do a depth-first traversal of each ele-
mentary tree (but excluding the passive valency
structure, if present) to obtain a sequence of non-
terminal nodes. For predicative auxiliary trees,
we stop at the footnode. Each node becomes two
states of the FST, one state representing the node
on the downward traversal on the left side, the
other representing the state on the upward traver-
sal, on the right side. For leaf nodes, the two
states are juxtaposed. The states are linearly con-
nected with   -transitions, with the left node state
of the root node the start state, and its right node
state the final state (except for predicative auxil-
iary trees ? see above). To each non-leaf state,
we add one self loop transition for each tree in
the grammar that can adjoin at that state from
the specified direction (i.e., for a state represent-
ing a node on the downward traversal, the auxil-
iary tree must adjoin from the left), labeled with
the tree name. For each pair of adjacent states
representing a substitution node, we add transi-
tions between them labeled with the names of
the trees that can substitute there. We output the
number of the grammatical function, and the ar-
gument semantics, if any is specified. For the
lexical head, we transition on the head, and out-
put the semantics if defined, or simply the lex-
eme otherwise. There are no other types of leaf
nodes since we do not traverse the passive va-
lency structure of adjunct auxiliary tees. At the
beginning of each FST, an   -transition outputs an
open-bracket, and at the end, an   -transition out-
puts a close-bracket. The result of this phase of
the conversion is a set of FSTs, one per elemen-
tary tree of the grammar. We will refer to them
as ?elementary FSTs?.
0 1<epsilon>:( 2A_NXG:GF=0
A_NXN:GF=0
3<epsilon>:FE=Customer 4ordered:transaction 5A_NXG:GF=1
A_NXN:GF=1
6<epsilon>:FE=Item 7<epsilon>:)
Figure 4: FST corresponding to TAG tree in Figure 3
S
NP

Arg0
VP
V
ordered
NP

Arg1
Figure 3: TAG tree for word ordered; the dow-
narrow indicates a substitution node for the nom-
inal argument
4 Constructing the Parser
In our approach, each elementary FST describes
the syntactic potential of a set of (syntactically
similar) words (as explained in Section 3). There
are several ways of associating words with FSTs.
Since FSTs correspond directly to supertags (i.e.,
trees in a TAG grammar), the basic way to
achieve such a mapping is to list words paired
with supertags, along with the desired seman-
tic associated with each argument position (see
Figure 1). The parser can also be divided into
a lexical machine which transduces words to
classes, and a syntactic machine, which trans-
duces classes to semantics. This approach has
the advantage of reducing the size of the over-
all machine since the syntax is factored from the
lexicon.
The lexical machine transduces input words to
classes. To determine the mapping from word to
supertag, we use the lexical probability  	

where 	 is the word and  the class. These
are derived by maximum likelihood estimation
from a corpus. Once we have determined for all
words which classes we want to pair them with,
we create a disjunctive FST for all words associ-
ated with a given supertag machine, which trans-
duces the words to the class name. We replaces
the class?s FST (as determined by its associated
supertag(s)) with the disjunctive head FST. The
weights on the lexical transitions are the nega-
tive logarithm of the emit probability 	 
 (ob-
tained in the same manner as are the lexical prob-
abilities).
For the syntactic machine, we take each ele-
mentary tree machine which corresponds to an
initial tree (i.e., a tree which need not be ad-
joined) and form their union. We then perform
a series of iterative replacements; in each iter-
ation, we replace each arc labeled by the name
of an elementary tree machine by the lexicalized
version of that tree machine. Of course, in each
iteration, there are many more replacements than
in the previous iteration. We use 5 rounds of iter-
ation; obviously, the number of iterations restrict
the syntactic complexity (but not the length) of
recognized input. However, because we output
brackets in the FSTs, we obtain a parse with
full syntactic/lexical semantic (i.e., dependency)
structure, not a ?shallow parse?.
This construction is in many ways similar to
similar constructions proposed for CFGs, in par-
ticular that of (Nederhof, 2000). One difference
is that, since we start from TAG, recursion is al-
ready factored, and we need not find cycles in the
rules of the grammar.
5 Experimental Results
We present results in which our classes are de-
fined entirely with respect to syntactic behav-
ior. This is because we do not have available
an important corpus annotated with semantics.
We train on the Wall Street Journal (WSJ) cor-
pus. We evaluate by taking a list of 205 sen-
tences which are chosen at random from entries
to WORDSEYE made by the developers (who
were testing the graphical component using a dif-
ferent parser). Their average length is 6.3 words.
We annotated the sentences by hand for the de-
sired dependency structure, and then compared
the structural output of PARSLI to the gold stan-
dard (we disregarded the functional and seman-
tic annotations produced by PARSLI). We eval-
uate performance using accuracy, the ration of
n Correctness Accuracy Nb
2 1.00 1.00 12
4 0.83 0.84 30
6 0.70 0.82 121
8 0.62 0.80 178
12 0.59 0.79 202
16 0.58 0.79 204
20 0.58 0.78 205
Figure 5: Results for sentences with  or fewer
words; Nb refers to the number of sentences in
this category
n Correctness Accuracy
1 0.58 0.78
2 0.60 0.79
4 0.62 0.81
8 0.69 0.85
12 0.68 0.86
20 0.70 0.87
30 0.73 0.89
Figure 6: Results for  -best analyses
the number of dependency arcs which are cor-
rectly found (same head and daughter nodes) in
the best parse for each sentence to the number
of arcs in the entire test corpus. We also report
the percentage of sentences for which we find the
correct dependency tree (correctness). For our
test corpus, we obtain an accuracy of 0.78 and
a correctness of 0.58. The average transduction
time per sentence (including initialization of the
parser) is 0.29 s. Figure 5 shows the dependence
of the scores on sentence length. As expected,
the longer the sentence, the worse the score.
We can obtain the n-best paths through the
FST; the scores for n-best paths are summarized
in Figure 6. Since the scores keep increasing, we
believe that we can further improve our 1-best
results by better choosing the correct path. We
intend to adapt the FSTs to use probabilities of
attaching particular supertags to other supertags
(rather than uniform weights for all attachments)
in order to better model the probability of differ-
ent analyses. Another option, of course, is bilex-
ical probabilities.
6 Discussion and Outlook
We have presented PARSLI, a system that takes
a high-level specification of domain lexical se-
mantics and generates a finite-state parser that
transduces input to the specified semantics.
PARSLI uses Tree Adjoining Grammar as an in-
terface between syntax and lexical semantics.
Initial evaluation results are encouraging, and we
expect to greatly improve on current 1-best re-
sults by using probabilities of syntactic combi-
nation. While we have argued that many appli-
cations do not need a fully recursive parser, the
same approach to using TAG as an intermediate
between application semantics and syntax can be
used in a chart parser; for a chart parser using the
FS machines discussed in this paper, see (Nasr et
al., 2002).
References
Michael Collins. 1997. Three generative, lex-
icalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of
the Association for Computational Linguis-
tics, Madrid, Spain, July.
Bob Coyne and Richard Sproat. 2001. Word-
sEye: An automatic text-to-scene conversion
system. In SIGGRAPH 2001, Los Angeles,
CA.
Robert Frank. 2001. Phrase Structure Composi-
tion and Syntactic Dependencies. MIT Press,
Cambridge, Mass.
Aravind K. Joshi and Yves Schabes. 1991. Tree-
adjoining grammars and lexicalized gram-
mars. In Maurice Nivat and Andreas Podel-
ski, editors, Definability and Recognizability
of Sets of Trees. Elsevier.
Dekang Lin. 1994. PRINCIPAR?an efficient,
broad-coverage, principle-based parser. In
coling94, pages 482?488, Kyoto, Japan.
Alexis Nasr, Owen Rambow, John Chen, and
Srinivas Bangalore. 2002. Context-free pars-
ing of a tree adjoining grammar using finite-
state machines. In Proceedings of the Sixth
International Workshop on tree Adjoining
Grammar and related Formalisms (TAG+6),
Venice, Italy.
Mark-Jan Nederhof. 2000. Practical experi-
ments with regular approximation of context-
free languages. Computational Linguistics,
26(1):17?44.
Tagging with Hidden Markov Models Using Ambiguous Tags
Alexis Nasr
LaTTice - Universite? Paris 7
anasr@linguist.jussieu.fr
Fre?de?ric Be?chet
Laboratoire d?Informatique
d?Avignon
frederic.bechet@lia.univ-avignon.fr
Alexandra Volanschi
LaTTice - Universite? Paris 7
avolansk@linguist.jussieu.fr
Abstract
Part of speech taggers based on Hidden
Markov Models rely on a series of hypothe-
ses which make certain errors inevitable.
The idea developed in this paper consists
in allowing a limited, controlled ambiguity
in the output of the tagger in order to avoid
a number of errors. The ambiguity takes
the form of ambiguous tags which denote
subsets of the tagset. These tags are used
when the tagger hesitates between the dif-
ferent components of the ambiguous tags.
They are introduced in an existing lexicon
and 3-gram database. Their lexical and
syntactic counts are computed on the basis
of the lexical and syntactic counts of their
constituents, using impurity functions. The
tagging process itself, based on the Viterbi
algorithm, is unchanged. Experiments con-
ducted on the Brown corpus show a recall of
0.982, for an ambiguity rate of 1.233 which
is to be compared with a baseline recall of
0.978 for an ambiguity rate of 1.414 using
the same ambiguous tags and with a recall
of 0.955 corresponding to the one best solu-
tion of standard tagging (without ambigu-
ous tags).
1 Introduction
Taggers are commonly used as pre-processors
for more sophisticated treatments like full syn-
tactic parsing or chunking. Although taggers
achieve high accuracy, they still make some
mistakes that quite often impede the following
stages. There are at least two solutions to this
problem. The first consists in devising more so-
phisticated taggers either by providing the tag-
ger with more linguistic knowldge or by refining
the tagging process, through better probability
estimation, for example. The second strategy
consists in allowing some ambiguity in the out-
put of the tagger. It is the second solution that
was chosen in this paper. We believe that this
is an instance of a more general problem in se-
quential natural language processing chains, in
which a module takes as input the output of
the preceding module. Since we cannot, in most
cases, expect a module to produce only correct
solutions, modules should be able to deal with
ambiguous input and ambiguous output. In our
case, the input is non ambiguous while the out-
put is ambiguous. From this perspective, the
quality of the tagger is evaluated by the trade-
off it achieves between accuracy and ambiguity.
The introduction of ambiguous tags in the
tagger output raises the question of the process-
ing of these ambiguous tags in the post-tagging
stages of the application. Leaving some ambigu-
ity in the output of the tagger only makes sense
if these other processes can handle it. In the
case of a chunker, ambiguous tags can be taken
into account through the use of weighted finite
state machines, as proposed in (Nasr and Volan-
schi, 2004). In the case of a syntactic parser,
such a device can usually deal with some ambi-
guity and discard the incorrect elements of an
ambiguous tag when they do not lead to a com-
plete analysis of the sentence. The parser itself
acts, in a sense, as a tagger since, while pars-
ing the sentence, it chooses the right tag among
a set of possible tags for each word. The rea-
son why we still need a tagger and don?t let the
parser do the job is time and space complexity.
Parsers are usually more time and space con-
suming than taggers and highly ambiguous tags
assignments can lead to prohibitive processing
time and memory requirements.
The tagger described in this paper is based
on the standard Hidden Markov Model archi-
tecture (Charniak et al, 1993; Brants, 2000).
Such taggers assign to a sequence of words
W = w1 . . . wn , the part of speech tag sequence
T? = t?1 . . . t?n which maximizes the joint prob-
ability P (T,W ) where T ranges over all possi-
ble tag sequences of length n. The probability
P (T,W ) is itself decomposed into a product of
2n probabilities, n lexical probabilities P (wi|ti)
(emission probabilities of the HMM) and n syn-
tactic probabilites (transition probabilities of the
HMM). Syntactic probabilities model the prob-
ability of the occurrence of tag ti given a history
which is the knowledge of the h preceding tags
(ti?1 . . . ti?h). Increasing the length of the his-
tory increases the predictive power of the tag-
ger but also the number of parameters to esti-
mate and therefore the amount of training data
needed. Histories of length 2 constitute a com-
mon trade-off for part of speech tagging.
We define an ambiguous tag as a tag that de-
notes a subset of the original tagset. In the re-
mainder of the paper, tags will be represented
as subscripted capitals T : T1, T2 . . .. Ambigu-
ous tags will be noted with multiple subscripts.
T1,3,5 for example, denotes the set {T1, T3, T5}.
We define the ambiguity of an ambiguous tag as
the cardinality of the set it denotes. This notion
is extended to non ambiguous tags, which can
be seen as singletons, their ambiguity is there-
fore equal to 1.
Ambiguous tags are actually new tags whose
lexical and syntactic probability distributions
are computed on the basis of lexical and syn-
tactic distributions of their constituents. The
lexical and syntactic probability distributions of
Ti1,...,in should be computed in such a way that,
when a word in certain context can be tagged
as Ti1 , . . . , Tin with probabilities that are close
enough, the tagger should choose the ambiguous
tag Ti1,...,in .
The idea of changing the tagset in order to im-
prove tagging accuracy has already been tested
by several researchers. (Tufis? et al, 2000) re-
ports experiments of POS tagging of Hungarian
with a large tagset (about one thousand differ-
ent tags). In order to reduce data sparseness
problems, they devise a reduced tagset which is
used for tagging. The same kind of idea is de-
veloped in (Brants, 1995). The major difference
between these approaches and ours, is that they
devise the reduced tagset in such a way that, af-
ter tagging, a unique tag of the extended tagset
can be recovered for each word. Our perspective
is significantly different since we allow unrecov-
erable ambiguity in the output of the tagger and
leave to the other processing stages the task of
reducing it. In the HMM based taggers frame-
work, our work bears a certain resemblance with
(Brants, 2000) who distinguishes between reli-
able and unreliable tag assignments using prob-
abilities computed by the tagger. Unreliable
tag assignments are those for which the prob-
ability is below a given threshold. He shows
that taking into account only reliable assign-
ments can significantly improve the accuracy,
from 96.6% to 99.4%. In the latter case, only
64.5% of the words are reliably tagged. For the
remaining 35.5%, the accuracy is 91.6%. These
figures show that taking into account probabil-
ities computed by the tagger discriminates well
these two situations. The main difference be-
tween his work and ours is that he does not
propose a way to deal with unreliable assign-
ments, which we treat using ambiguous tags.
The paper is structured as follows: section 2
describes how the probability distributions of
the ambiguous tags are estimated. Section 3
presents an iterative method to automatically
discover good ambiguous tags as well as an ex-
periment on the Brown corpus. Section 4 con-
cludes the paper.
2 Computing probability
distributions for ambiguous tags
Probabilistic models for part of speech taggers
are built in two stages. In a first stage, counts
are collected from a tagged training corpus
while in the second, probabilities are computed
on the basis of these counts. Two type of counts
are collected: lexical counts, noted Cl(w, T )
indicating how many times word w has been
tagged T in the training corpus and syntactic
counts Cs(T1, T2, T3) indicating how many
times the tag sequence T1, T2, T3 occurred in
the training corpus. Lexical counts are stored
in a lexicon and syntactic counts in a 3-gram
database.
These real counts will be used to compute
fictitious counts for ambiguous tags on the basis
of which probability distributions will be esti-
mated. The rationale behind the computation
of the counts (lexical as well as syntactic) of an
ambiguous tag T1...j is that they must reflect
the homogeneity of the counts of {T1 . . . Tj}. If
they are all equal, the count of T1...j should be
maximal.
Impurity functions (Breiman et al, 1984) per-
fectly model this behavior1: an impurity func-
tion ? is a function defined on the set of all N-
tuples of numbers (p1, . . . , pN ) satisfying ?j ?
[1, . . . , N ], pj ? 0 and
?N
j=1 pj = 1 with the fol-
lowing properties:
1Entropy would be another candidate for such compu-
tation. The same experiments have also been conducted
using entropy and lead to almost the same results.
? ? reaches its maximum at the point
( 1N , . . . , 1N )
? ? achieves its minimum at the points
(1, 0, . . . , 0), (0, 1, . . . , 0), . . . (0, 0, . . . , 1)
Given an impurity function ?, we define the
impurity measure of a N-tuple of counts C =
(c1, . . . , cN ) as follows :
I(c1, . . . , cN ) = ?(f1, . . . , fN ) (1)
where fi is the relative frequency of ci in C:
fi =
ci
?N
k=1 ck
The impurity function we have used is the
Gini impurity criteria:
?(f1, . . . , fN ) =
?
i6=j
fifj
whose maximal value is equal to N?1N .
The impurity measure will be used to com-
pute both lexical and syntactic fictitious counts
as described in the two following sections.
2.1 Lexical counts
Lexical counts for an ambiguous tag T1,...,n are
computed using lexical impurity Il(w, T1,...,n)
which measures the impurity of the n-tuple
(Cl(w, T1), . . . , Cl(w, Tn)):
Il(w, T1,...,n) = I(Cl(w, T1), . . . , Cl(w, Tn))
A high lexical impurity Il(w, T1,...,n) means
that w is ambiguous with respect to the differ-
ent classes T1, . . . , Tn. It reaches its maximum
when w has the same probability to belong to
any of them. The lexical count Cl(w, T1,...,n) is
computed using the following formula:
Cl(w, T1,...,n) = Il(w, T1,...,n)
n
?
i=1
Cl(w, Ti)
This formula is used to update a lexicon, for
each lexical entry, the counts of the ambiguous
tags are computed and added to the entry. The
two entries daily and deals whose original counts
are represented below2:
daily RB 32 JJ 41
deals NNS 1 VBZ 13
2RB, JJ, NNS and VBZ stand respectively for adverb,
adjective, plural noun and verb (3rd person singular,
present).
are updated to3:
daily RB 32 JJ 41 JJ_RB 36
deals NNS 1 VBZ 13 NNS_VBZ 2
2.2 Syntactic counts
Syntactic counts of the form Cs(X,Y, T1,...,n)
are computed using syntactic impurity
Is(X,Y, T1,...,n) which measures the impurity of
the n-tuple I(Cs(X,Y, T1), . . . , Cs(X,Y, Tn)) :
Is(X, Y, T1,...,n) = I(Cs(X, Y, T1), . . . , Cs(X, Y, Tn))
A maximum syntactic impurity means that
all the tags T1, . . . , Tn have the same probabil-
ity of occurrence after the tag sequence X Y .
If any of them has a probability of occurrence
equal to zero after such a tag sequence, the im-
purity is also equal to zero. The syntactic count
Cs(X,Y, T1,...,n) is computed using the following
formula:
Cs(X, Y, T1,...,n) = Is(X, Y, T1,...,n)
n
?
i=1
Cs(X, Y, Ti)
Such a formula is used to update the 3-
gram database in three steps. First, syntac-
tic counts of the form Cs(X,Y, T1,...,n) (with
X and Y unambiguous) are computed, then
syntactic counts of the form Cs(X,T1,...,n, Y )
(with X unambiguous and Y possibly ambigu-
ous) and eventually, syntactic counts of the form
Cs(T1,...,n, X, Y ) (for X and Y possibly ambigu-
ous). The following four real 3-grams:
A A A 100 A A B 100
A B A 10 A B B 1000
will give rise to following five fictitious ones:
A A A_B 100 A A_B A 18
A A_B A_B 31 A A_B B 181
A B A_B 19
which will be added to the 3-gram database.
Note that the real 3-grams are not modified dur-
ing this operation.
Once the lexicon and the 3-gram database
have been updated, both real and fictitious
counts are used to estimate lexical and syntactic
probability distribution. These probability dis-
tributions constitute the model. The tagging
process itself, based on the Viterbi search algo-
rithm, is unchanged.
3The fictitious counts were rounded to the nearest
integer.
2.3 Data sparseness
The introduction of new tags in the tagset in-
creases the number of states in the HMM and
therefore the number of parameters to be esti-
mated. It is important to notice that even if the
number of parameters increases, the model does
not become more sensitive to data sparseness
problems than the original model was. The rea-
son is that fictitious counts are computed based
on actual counts. The occurrence, in the train-
ing corpus, of an event (as the occurrence of
a sequence of tags or the occurrence of a word
with a given tag) is used for estimating both the
probability of the event associated to the sim-
ple tag and the probabilities of the events asso-
ciated with the ambiguous tags which contain
the simple tag. For example, the occurrence of
the word w with tag T , in the training corpus,
will be used to estimate the lexical probabil-
ity P (w|T ) as well as the lexical probabilities
P (w|T ?) for every ambiguous tag T ? of which T
may be a component.
3 Learning ambiguous tags from
errors
Since ambiguous tags are not given a priori,
candidates can be selected based on the errors
made by the tagger. The idea developed in this
section consists in learning iteratively ambigu-
ous tags on the basis of the errors made by a
tagger. When a word w tagged T1 in a refer-
ence corpus has been wrongly tagged T2 by the
tagger, that means that T1 and T2 are lexically
and syntactically ambiguous, with respect to w
and a given context. Consequently, T1,2 is a po-
tential candidate for an ambiguous tag.
The process of discovering ambiguous tags
starts with a tagged training corpus whose
tagset is called T0. A standard tagger, M0,
is trained on this corpus. M0 is used to tag
the training corpus. A confusion matrix is then
computed and the most frequent error is se-
lected to form an ambiguous tag which is added
to T0 to constitute T1. M0 is then updated
with the new ambiguous tag to constitue M1,
as described in section 2. The process is iter-
ated : the training corpus is tagged with Mi,
the most frequent error is used to constitue Ti+1
and a new tagger Mi+1 is built, based on Mi.
The process continues until the result of the tag-
ging on the development corpus converges or the
number of iterations has reached a given thresh-
old.
3.1 Experiments
The model described in section 2 has been
tested on the Brown corpus (Francis and
Kuc?era, 1982), tagged with the 45 tags of the
Penn treebank tagset (Marcus et al, 1993),
which constitute the initial tagset T0. The cor-
pus has been divided in a training corpus of
961, 3 K words, a development corpus of 118, 6
K words and a test corpus of 115, 6 K words.
The development corpus was used to detect the
convergence and the final model was evaluated
on the test corpus. The iterative tag learning
algorithm converged after 50 iterations.
A standard trigram model (without ambigu-
ous tags) M0 was trained on the training cor-
pus using the CMU-Cambridge statistical lan-
guage modeling toolkit (Clarkson and Rosen-
feld, 1997). Smoothing was done through back-
off on bigrams and unigrams using linear dis-
counting (Ney et al, 1994).
The lexical probabilities were estimated on
the training corpus. Unknown words (words of
the development and test corpus not present in
the lexicon) were taken into account by a sim-
ple technique: the words of the development
corpus not present in the training corpus were
used to estimate the lexical counts of unknown
words Cl(UNK, t). During tagging, if a word is
unknown, the probability distribution of word
UNK is used. The development corpus contains
4097 unknown words (3.4% of the corpus) and
the test corpus 3991 (3.3%).
3.1.1 Evaluation measures
The result of the tagging process consists in a
sequence of ambiguous and non ambiguous tags.
This result can no longer be evaluated using ac-
curacy alone (or word error rate), as it is usu-
ally the case in part of speech tagging, since the
introduction of ambiguous tags allows the tag-
ger to assign multiple tags to a word. This is
why two measures have been used to evaluate
the output of the tagger with respect to a gold
standard: the recall and the ambiguity rate.
Given an output of the tagger T = t1 . . . tn,
where ti is the tag associated to word i by the
tagger, and a gold reference R = r1 . . . rn where
r1 is the correct tag for word wi, the recall of T
is computed as follows :
REC(T ) =
?n
i=1 ?(ri ? ti)
n
where ?(p) equals to 1 if predicate p is true
and 0 otherwise. A recall of 1 means that for
every word occurrence, the correct tag is an el-
ement of the tag given by the tagger.
The ambiguity rate of T is computed as fol-
lows :
AMB(T ) =
?n
i=1 AMB(ti)
n
where AMB(ti) is the ambiguity of tag ti. An
ambiguity rate of 1 means that no ambiguous
tag has been introduced. The maximum ambi-
guity rate for the development corpus (when all
the possible tags of a word are kept) is equal to
2.4.
3.1.2 Baseline models
The successive modelsMi are based on the dif-
ferent tagsets Ti. Their output is evaluated with
the two measures described above. But these
figures by themselves are difficult to interpret if
we cannot compare them with the output of an-
other tagging process based on the same tagset.
The only point of comparision at hand is model
M0 but it is based on tagset T0, which does not
contain ambiguous tags. In order to create such
a point of comparison, a baseline model Bi is
built at every iteration. The general idea is to
replace in the training corpus, all occurrences of
tags that appear as an element of an ambigu-
ous tag of Ti by the ambiguous tag itself. After
the replacement stage, a model Bi is computed
and used to tag the development corpus. The
output of the tagging is evaluated using recall
and ambiguity rate and can be compared to the
output of model Mi.
The replacement stage described above is ac-
tually too simplistic and gives rise to very poor
baseline models. There are two problems with
this approach. The first is that a tag Ti can ap-
pear as a member of several ambiguous tags and
we must therefore decide which one to choose.
The second, is that a word tagged Ti in the ref-
erence corpus might be unambiguous, it would
therefore be ?unfair? to associate to it an am-
biguous tag. This is the reason why the replace-
ment step is more elaborate. At iteration i, for
each couple (wj , Tj) of the training corpus, a
lookup is done in the lexicon, which gives access
to all the possible non ambiguous tags word wj
can have. If there is an ambiguous tag T in
Ti such that all its elements are possible tags of
wj then, couple (wj , Tj) is replaced with (wj , T )
in the corpus. If several ambiguous tags fulfill
this condition, the ambiguous tag which has the
highest lexical count for wj is chosen.
Another simple way to build a baseline would
be to produce the n best solutions of the tag-
ger, then take for each word of the input the
tags associated to it in the different solutions
and make an ambiguous tag out of these tags.
This solution was not adopted for two reasons.
The first is that this method mixes tags from
different solutions of the tagger and can lead
to completely incoherent tags sequences. It is
difficult to measure the influence of this inco-
herence on the post-tagging stages of the ap-
plication and we didn?t try to measure it em-
pirically. But the idea of potentially producing
solutions which are given very poor probabili-
ties by the model is unappealing. The second
reason is that we cannot control anymore which
ambiguous tags will be created (although this
feature might be desirable in some cases). It
will be therefore difficult to compare the result
with our models (the tagsets will be different).4
3.1.3 Results
The results of the successive models have been
plotted in figure 1 and summarized in table 1,
which also shows the results on the test corpus.
For each iteration i, recall and ambiguity rates
of modelsMi and Bi on the development corpus
were computed. The results show, as expected,
that recall and ambiguity rate increase with the
increase of the number of ambiguous tags added
to the tagset. This is true for both models Mi
and Bi. The figure also shows that recall of Bi,
for a given i, is generally a bit lower than Mi
while its ambiguity is higher. Figure 2 shows
that for the same recall Bi introduces more am-
biguous tags than Mi.
The list of the 20 first ambiguous tags created
during the process is represented below :
1 IN_RB 11 IN_WDT_WP
2 DT_IN_WDT_WP 12 VBD_VBN
3 JJ_VBN 13 JJ_NN_NNP_NNS_RB_VBG
4 NN_VB 14 JJ_NN_NNP
5 JJ_NN 15 JJ_NN_NNP_NNS_RB
6 IN_RB_RP 16 JJR_RBR
4As a point of comparison we will nevertheless give a
few figures here. For low values of n, the n best solutions
have better recall for a given value of the ambiguity rate.
For instance, the 4 best tagger output yields a recall of
0.9767 for an ambiguity rate of 1.12, while, for the same
ambiguity rate, the iterative method obtains a 0.9604 re-
call. However, the 0.982 recall value which we attained
at the end of the iterative ambiguous tag learning pro-
cedure, corresponding to an ambiguity rate of 1.23, was
also reached by keeping the 7 best solutions of the tag-
ger, with an ambiguity rate of 1.20 (only slightly better
than ours).
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0  5  10  15  20  25  30  35  40  45  50
 1
 1.05
 1.1
 1.15
 1.2
 1.25
 1.3
 1.35
 1.4
 1.45
 1.5
re
ca
ll
am
bi
gu
ity
iterations
recall
ambiguity
recall (baseline)
ambiguity (baseline)
Figure 1: Recall and ambiguity rate of the suc-
cessive models on development corpus
 1
 1.05
 1.1
 1.15
 1.2
 1.25
 1.3
 1.35
 0.955  0.96  0.965  0.97  0.975
am
bi
gu
ity
recall
Model Mi
Baseline
Figure 2: Comparing ambiguity rates for a fixed
value of recall
7 NNPS_NNS 17 NN_VBG
8 VB_VBP 18 CD_NN
9 JJ_RB 19 WDT_WP
10 DT_RB 20 JJ_NN_NNP_NNS
Model DEV TEST
REC AMB REC AMB
M0 = B0 0.955 1 0.955 1
B40 0.978 1.414 0.979 1.418
M40 0.980 1.232 0.982 1.232
Table 1: Results on development and test cor-
pus
3.1.4 Model efficiency
The original idea of our method consists in cor-
recting errors that were made by M0, through
the introduction of ambiguous tags. Ideally, we
would like models Mi with i > 0 to introduce
an ambiguous tag only where M0 made a mis-
take. Unfortunately, it is not always the case.
We have classified the use of ambiguous tags
into four situations function of their influence
on both recall and ambiguity rate as indicated
in table 2, where G stands for the gold standard.
In situations 1 and 2 model M0 made a mis-
take. In situation 1, the mistake was corrected
by the introduction of the ambiguous tag while
in situation 2 it was not. In situations 3 and 4,
modelM0 did not make a mistake. In situation
3 the introduction of the ambiguous tag did not
create a mistake while it did in situation 4.
Situation G M0 Mi REC AMB
1 T1 T2 T1,2 + +
2 T3 T4 T1,2 0 +
3 T1 T1 T1,2 0 +
4 T3 T3 T1,2 ? +
Table 2: Influence of the introduction of an am-
biguous tag on recall and ambiguity rates
The frequency of each situation for some of
the 20 first ambiguous tags has been reported
in table 3. The last column of the table indicates
the frequency of the ambiguous tag (number of
occurrences of this tag divided by the sum of
occurrences of all ambiguous tags). The figures
show that ambiguous tags are not very efficient:
only a moderate proportion of their occurrences
(24% on average) actually corrected an error.
While we are very rarely confronted with sit-
uation 4 which decreases recall and increases
ambiguity (0.5% on average), in the vast ma-
jority of cases ambiguous tags simply increase
the ambiguity without correcting any mistakes.
Ambiguous tags behave quite differently with
respect to the four situations described above.
In the best cases (tag 6), 46% of the occurrences
corrected an error, and the tag is used one out of
ten times the tagger selects an ambiguous tag,
as opposed to tag 19 , which corrected errors in
48% of the cases but is not frequently used. The
worst configuration is tag 9, which, although
not chosen very often, corrects an error in 13%
of the occurrences and increases the ambiguity
in 85% of its occurrences.
A more detailed evaluation of the basic tag-
ging mistakes has suggested a better adapted
and more subtle method of using the ambiguous
tags which may at the same time constitute a di-
rection for future work. While the vast majority
of mistakes are due to mixing up word classes,
such as the -ing forms used as adjectives, as
nouns or as verbs, about one third of the mis-
takes concern only 25 common words such as
that, out, there, on, off, etc. Using the ambigu-
Tag 1 2 3 4 freq
1 0.220 0.026 0.746 0.006 0.126
5 0.129 0.014 0.852 0.002 0.165
6 0.461 0.000 0.538 0.000 0.107
9 0.133 0.012 0.850 0.003 0.082
19 0.483 0.064 0.419 0.032 0.012
AVG 0.241 0.029 0.722 0.005
Table 3: Error analysis of some ambiguous tags
ous tags for these words alone has yielded a re-
call of 0.965 on the test corpus (25% errors less
than model M0) while keeping the ambiguity
rate very low (1.04). With this procedure, 35%
of the ambiguous tags occurrences corrected an
error made byM0 and 59% increased the ambi-
guity. The result can be improved by designing
two sets of ambiguous tags: one to be used for
this set of words, and one for the word-classes
most often mistaken.
4 Conclusions and Future Work
We have presented a method for computing the
probability distributions associated to ambigu-
ous tags, denoting subsets of the tagset, in an
HMM based part of speech tagger. An iterative
method for discovering ambiguous tags, based
on the mistakes made by the tagger allowed to
reach a recall of 0.982 for an ambiguity rate of
1.232. These figures can be compared to the
baseline model which achieves a recall of 0.979
and an ambiguity rate of 1.418 using the same
ambiguous tags. An analysis of ambiguous tags
showed that they do not always behave in the
way expected; some of them introduce a lot of
ambiguity without correcting many mistakes.
This work will be developed in two directions.
The first one concerns the study of the differ-
ent behaviour of ambiguous tags which could
be influenced by computing differently the ficti-
tious counts of each ambiguous tag, based on its
behaviour on a development corpus in order to
force or prevent its introduction during tagging.
The second direction concerns experiments on
supertagging (Bangalore and Joshi, 1999) fol-
lowed by a parsing stage the tagging stage asso-
ciates to each word a supertag. The supertags
are then combined by the parser to yield a parse
of the sentence. Errors of the supertagger (al-
most one out of 5 words is attributed the wrong
supertag) often impede the parsing stage. The
idea is therefore to allow some ambiguity during
the supertagging stage, leaving to the parser the
task of selecting the right supertag using syntac-
tic constraints that are not available to the tag-
ger. Such experiments will constitute one way
of testing the viability of our approach.
References
Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An approach to almost pars-
ing. Computational Linguistics, 25(2):237?
265.
Thorsten Brants. 1995. Tagset reduction with-
out information loss. In ACL?95, Cambridge,
USA.
Thorsten Brants. 2000. Tnt - a statistical
part-of-speech tagger. In Sixth Applied Natu-
ral Language Processing Conference, Seattle,
USA.
L. Breiman, J. H. Friedman, R. A. Olshen, and
C. J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth & Brooks, Pacific
Grove, California.
Eugene Charniak, Curtis Hendrickson, Neil Ja-
cobson, and Mike Perkowitz. 1993. Equa-
tions for part-of-speech tagging. In 11th Na-
tional Conference on Artificial Intelligence,
pages 784?789.
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the cmu-
cambridge toolkit. In Eurospeech.
Nelson Francis and Henry Kuc?era. 1982. Fre-
quency Analysis of English Usage: Lexicon
and Grammar. Houghton Mifflin, Boston.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The
penn treebank. Computational Linguistics,
9(2):313?330, june. Special Issue on Using
Large Corpora.
Alexis Nasr and Alexandra Volanschi. 2004.
Couplage d?un e?tiqueteur morpho-syntaxique
et d?un analyseur partiel repre?sente?s sous
la forme d?automates finis ponde?re?s. In
TALN?2004, pages 329?338, Fez, Morocco.
H. Ney, U. Essen, and R. Kneser. 1994.
On structuring probabilistic dependencies in
stochastic language modelling. Computer
Speech and Language, 8:1?38.
Dan Tufis?, Pe?ter Dienes, Csaba Oravecz, and
Tama?s Va?radi. 2000. Principled hidden
tagset design for tiered tagging of hungarian.
In LREC, Athens, Greece.
A Simple String-Rewriting Formalism for Dependency Grammar
Alexis NASR
Lattice, UFRL
Universite? Paris 7
F-75005 Paris
France
alexis.nasr@linguist.jussieu.fr
Owen RAMBOW
Columbia University
Department of Computer Science
1214 Amsterdam Avenue
New York, NY 10027-7003, USA
rambow@cs.columbia.edu
Abstract
Recently, dependency grammar has gained renewed
attention as empirical methods in parsing have
emphasized the importance of relations between
words, which is what dependency grammars model
explicitly, but context-free phrase-structure gram-
mars do not. While there has been much work on
formalizing dependency grammar and on parsing
algorithms for dependency grammars in the past,
there is not a complete generative formalization of
dependency grammar based on string-rewriting in
which the derivation structure is the desired depen-
dency structure. Such a system allows for the defi-
nition of a compact parse forest in a straightforward
manner. In this paper, we present a simple gen-
erative formalism for dependency grammars based
on Extended Context-Free Grammar, along with
a parser; the formalism captures the intuitions of
previous formalizations while deviating minimally
from the much-used Context-Free Grammar.
1 Introduction
Dependency grammar has a long tradition in syn-
tactic theory, dating back to at least Tesnie`re?s work
from the thirties. Recently, it has gained renewed
attention as empirical methods in parsing have em-
phasized the importance of relations between words
(see, e.g., (Collins, 1997)), which is what depen-
dency grammars model explicitly, but context-free
phrase-structure grammars do not. In this paper, we
address an important issue in using grammar for-
malisms: the compact representation of the parse
forest. Why is this an important issue? It is well
known that for non-toy grammars and non-toy ex-
amples, a sentence can have a staggeringly large
number of analyses (for example, using a context-
free grammar (CFG) extracted from the Penn Tree-
bank, a sentence of 25 words may easily have
1,000,000 or more analyses). By way of an exam-
ple of an ambiguous sentence (though with only two
readings), the two dependency representations for
the ambiguous sentence (1) are given in Figure 1.
(1) Pilar saw a man with a telescope
It is clear that if we want to evaluate each possi-
ble analysis (be it using a probabilistic model or a
different method, for example a semantic checker),
we cannot efficiently do so if we enumerate all
cases.1 We have two options: we can either use a
greedy heuristic method for checking which does
not examine all possible solutions, which entails
we may miss the optimal solution, or we perform
our checking operation on a representation which
encodes all options in a compact representation.
This is possible because the exponential number of
possible analyses (exponential in the length of the
input sentence) share subanalyses, thus making a
polynomial-size representation possible. This rep-
resentation is called the shared parse forest and it
has been extensively studied for CFGs (see, for ex-
ample, (Lang, 1991)). To our knowledge, there has
been no description of the notion of shared parse
forest for dependency trees to date. In this paper, we
propose a formalization which is very closely based
on the shared parse forest for CFG. We achieve this
by defining a generative string-rewriting formal-
ism whose derivation trees are dependency trees.
The formalism, and the corresponding shared parse
forests, are used in a probabilistic chart parser for
dependency grammar, which is described in (Nasr
and Rambow, 2004b).
While there has been much work on formalizing
dependency grammar and on parsing algorithms for
dependency grammars in the past, we observe that
there is not, to our knowledge, a complete gener-
ative formalization2 of dependency grammar based
on string-rewriting in which the derivation structure
is exactly the desired dependency structure. The
most salient reason for the lack of such a gener-
ative dependency grammar is the absence of non-
1We would like to thank two anonymous reviewers for use-
ful comments.
2While the notions are related historically and conceptually,
we refer to a type of mathematical formalization, not to the
school of linguistics known as ?Generative Grammar?.
terminal symbols in a dependency tree, which pre-
vents us from interpreting it as a derivation struc-
ture in a system that distinguishes between termi-
nal and nonterminal symbols. The standard solu-
tion to this problem, proposed by Gaifman (1965),
is to introduce nonterminal symbols denoting lex-
ical categories, as depicted in figure 2 (called the
?labelled phrase-structure trees induced by a depen-
dency tree? by Gaifman (1965)). Clearly, the ?pure?
dependency tree can be derived in a straightforward
manner. The string rewriting system described in
(Gaifman, 1965) generates as derivation structures
this kind of trees.
There is however a deeper problem when con-
sidering dependency trees as derivation structures,
following from the fact that in a dependency tree,
modifiers3 are direct dependents of the head they
modify, and (in certain syntactic contexts) the num-
ber of modifiers is unbounded. Thus, if we wish to
obtain a tree as shown in Figure 2, we need to have
productions whose right-hand side is of unbounded
size, which is not possible in a context-free gram-
mar. Indeed, the formalization of dependency gram-
mar proposed by Gaifman (1965) is unsatisfactory
in that it does not allow for an unbounded number
of modifiers!
In this paper, we follow a suggestion made by
Abney (1996) and worked out in some detail in
(Lombardo, 1996)4 to extend Gaifman?s notation
with regular expressions, similar to the approach
used in extended context-free grammars. The re-
sult is a simple generative formalism which has
the property that the derivation structures are de-
pendency trees, except for the introduction of pre-
terminal nodes as shown in Figure 2. We do not
mean to imply that our formalism is substantially
different from previous formalizations of depen-
dency grammar; the goal of this paper is to present
a clean and easy-to-use generative formalism with
a straightforward notion of parse forest. In partic-
ular, our formalism, Generative Dependency Gram-
mar, allows for an unbounded number of daughter
nodes in the derivation tree through the use of reg-
ular expressions in its rules. The parser uses the
3We use the term modifier in its linguistic sense as a type of
syntactic dependency (another type being argument). We use
head (or mother) and dependent (or daughter) to refer to nodes
in a tree. Sometimes, in the formal and parsing literature, mod-
ifier is used to designate any dependent node, but we consider
that usage confusing because of the related but different mean-
ing of the term modifier that is well-established in the linguistic
literature.
4In fact, much of our formalism is very similar to (Lom-
bardo, 1996), who however does not discuss parsing (only
recognition), nor the representation of the parse forest.
corresponding finite-state machines which straight-
forwardly allows for a binary-branching representa-
tion of the derivation structure for the purpose of
parsing, and thus for a compact (polynomial and
not exponential) representation of the parse forest.
This formalism is based on previous work presented
in (Kahane et al, 1998), which has been substan-
tially reformulated in order to simplify it.5 In par-
ticular, we do not address non-projectivity here, but
acknowledge that for certain languages it is a cru-
cial issue. We will extend our basic approach in the
spirit of (Kahane et al, 1998) in future work.
The paper is structured as follows. We start
out by surveying previous formalizations of depen-
dency grammar in Section 2. In Section 3, we intro-
duce several formalisms, including Generative De-
pendency Grammar. We present a parsing algorithm
in Section 4, and mention empirical results in Sec-
tion 5. We then conclude.
2 Previous Formalizations of Dependency
Grammar
We start out by observing that ?dependency gram-
mar? should be contrasted with ?phrase structure
grammar?, not ?CFG?, which is a particular formal-
ization of phrase structure grammar. Thus, just as it
only makes sense to study the formal properties of
a particular formalization of phrase structure gram-
mar, the question about the formal properties of de-
pendency grammar in general is not well defined,
nor the question of a comparison of a dependency
formalism with dependency grammar.
There have been (at least) four types of formal-
izations of dependency grammars in the past.6 None
of these approaches, to our knowledge, discuss the
notion of shared parse forest. The first approach
(for example, (Lombardo and Lesmo, 2000)) fol-
lows Gaifman (1965) in proposing traditional string
rewriting rules, which however do not allow for an
unbounded number of adjuncts.
In the second approach, the dependency structure
is constructed in reference to a parallel (?deeper?)
structure (Sgall et al, 1986; Mel?c?uk, 1988). Be-
cause the rules make reference to other struc-
5Kahane et al (1998) present three different types of rules,
for subcategorization, modification, and linear precedence. In
the formalism presented in this paper, they have been collapsed
into one.
6We leave aside here work on tree rewriting systems such
as Tree Adjoining Grammar, which, when lexicalized, have
derivation structures which are very similar to dependency
trees. See (Rambow and Joshi, 1997) for a discussion related
to TAG, and see (Rambow et al, 2001) for the definition of a
tree-rewriting system which can be used to develop grammars
whose derivations faithfully mirror syntactic dependency.
saw
 HH
Pilar man
 HH
a with
telescope
a
saw


HH
HH
Pilar man
a
with
telescope
a
Figure 1: Two dependency trees
Pilar
N
a
D man
a
VV
saw
D man
a
sawN
P
with
Pilar
N
N
a
D telesope
N P
with N
telesopeD
Figure 2: Two dependency trees with lexical categories
tures, these approaches cannot be formalized in
a straightforward manner as context-free rewriting
formalisms.
In the third approach, which includes formaliza-
tions of dependency structure such as Dependency
Tree Grammar of Modina (see (Dikovsky and Mod-
ina, 2000) for an overview), Link Grammar (Sleator
and Temperley, 1993) or the tree-composition ap-
proach of Nasr (1996), rules construct the depen-
dency tree incrementally; in these approaches, the
grammar licenses dependency relations which, in a
derivation, are added to the tree one by one, or in
groups. In contrast, we are interested in a string-
rewriting system; in such a system, we cannot add
dependency relations incrementally: all daughters
of a node must be added at once to represent a sin-
gle rewrite step.
In the fourth approach, the dependency grammar
is converted into a headed context-free grammar
(Abney, 1996; Holan et al, 1998), also the Basic
Dependency Grammar of Beletskij (1967) as cited
in (Dikovsky and Modina, 2000). This approach al-
lows for the recovery of the dependency structure
both from the derivation tree and from a parse for-
est represented in polynomial space. (In fact, our
parsing algorithm draws on this work.) However,
the approach of course requires the introduction of
additional nonterminal nodes. Finally, we observe
that Recursive Transition Networks (Woods, 1970)
can be used to encode a grammar whose deriva-
tion trees are dependency trees. However, they are
more a general framework for encoding grammars
than a specific type of grammar (for example, we
can also use them to encode CFGs). In a some-
what related manner, Alshawi et al (2000) use cas-
caded head automata to derive dependency trees, but
leave the nature of the cascading under-formalized.
Eisner (2000) provides a formalization of a system
that uses two different automata to generate left and
right children of a head. His formalism is very close
to the one we present, but it is not a string-rewriting
formalism (and not really generative at all). We
are looking for a precise formulation of a genera-
tive dependency grammar, and the question has re-
mained open whether there is an alternate formal-
ism which allows for an unbounded number of ad-
juncts, introduces all daughter nodes at once in a
string-rewriting step, and avoids the introduction of
additional nonterminal nodes.
3 Formalism
In this section we first review the definition of Ex-
tended Context-Free Grammar and then show how
we use it to model dependency derivations. An Ex-
tended Context-Free Grammar (or ECFG for short)
is like a context-free grammar (CFG), except that
the right-hand side is a regular expression over the
terminal and nonterminal symbols of the grammar.
At each step in a derivation, we first choose a rewrite
rule (as we do in CFG), and then we choose a string
which is in the language denoted by the regular ex-
pression associated with the rule. This string is then
treated like the right-hand side of a CFG rewrite
rule.
In the following, if G is a grammar and R a regu-
lar expression, then L(G) denotes the language gen-
erated by the grammar and L(R) the language de-
noted by the regular expression. If F is a class of
grammars (such as CFG), then L(F) denote the class
of languages generated by the grammars in F. We
now give a formal definition, which closely follows
that given by Albert et al (1999).7
A Extended Context-Free Grammar is a 4-
tuple (VN, VT, P, S), where:
? VN is a finite set of nonterminal symbols,
? VT is a finite set of terminal symbols (disjoint
from VN),
? P is a finite set of rules, which are ordered
pairs consisting of an element of VN and a reg-
ular expression over VN ? VT,
? S, a subset of VN, contains the possible start
symbols.
We will use the traditional arrow notation (??)
to write rules.
For A ? VN and u, v ? (VN ? VT)? we say that
uAv yields uwv (written uAv =? uwv) if A ??
R is in P and w is in L(R). The transitive closure
of the yield relation (denoted ?=? ) is defined in the
usual manner.
The language generated by a Extended Context-
Free Grammar is the set {w ? V ?T | A
?=? w,A ?
S}.
We now define a restricted version of ECFG
which we will use for defining dependency gram-
mars. The only new formal requirement is that
the rules be lexicalized in the sense of (Joshi and
Schabes, 1991). For our formalism, this means
that the regular expression in a production is such
that each string in its denoted language contains at
least one terminal symbol. Linguistically speaking,
this means that each rule is associated with exactly
7ECFG has been around informally since the sixties (e.g.,
the Backus-Naur form); for a slightly different formalization,
see (Madsen and Kristensen, 1976), whose definition allows
for an infinite rule set.
one lexical item (which may be multi-word). We
will call this particular type of Extended Context-
Free Grammar a lexicalized Extended Context-
Free Grammar or, for obvious reasons, a Genera-
tive Dependency Grammar (GDG for short). When
we use a GDG for linguistic description, its left-
hand side nonterminal will be interpreted as the lex-
ical category of the lexical item and will represent
its maximal projection.8
A Generative Dependency Grammar is a lexi-
calized ECFG.
It is sometimes useful to have dependency repre-
sentations with labeled arcs (typically labeled with
syntactic functions such as SUBJ for subject or ADJ
for adjunct). There are different ways of achieving
this goal; here, we discuss the use of feature struc-
tures in conjunction with the nonterminal symbols,
for example N[gf:subj] instead of just N. Fea-
ture structures are of course useful for other reasons
as well, such as expressing morphological features.
In terms of our formalism, the use of bounded fea-
ture structures can be seen as a shorthand notation
for an increased set of nonterminal symbols. The
use of feature structures (rather than simple nonter-
minal symbols) allows for a more perspicuous rep-
resentation of linguistic relations through the use of
underspecification. Note that the use of underspec-
ified feature structures in rules can potentially lead
to an exponential increase (exponential in the size
of the set of feature values) of the size of the set of
rules if rules contain underspecified feature struc-
tures on the right-hand side. However, we note that
the feature representing, grammatical function will
presumably always be fully specified on the right-
hand side of a rule (the head determines the func-
tion of its dependents). Underspecification in the
left-hand side of a rule only leads to linear compact-
ification of the rule set.
We now give a toy linguistic example. We let
GLing be (VN, VT, P, S) as follows:
? VN = {V,N,D,A,Adv}
? VT = { Pilar, saw, man, a, telescope, with, tall,
very }
? P consists of the following rules:
p1 : V ?? N saw N P ?
p2 : N ??
(Pilar | D (A) (man | telescope) P ?)
8For practical purposes, we can separate the lexicon (which
assigns lexical categories to lexemes) from the syntactic rules
(which hold for all lexemes of a class), as does Gaifman (1965),
resulting in a straightforward notational extension to our for-
malism.
V
p1=? N saw N P
p2p2p3=? Pilar saw D man with N
p6p2=? Pilar saw a man with D telescope
p6=? Pilar saw a man with a telescope
Figure 3: A sample GDG derivation
p3 : P ?? with N
p4 : A ?? Adv? tall
p5 : Adv ?? very
p6 : D ?? a
? S = {V }
A derivation is shown in Figure 3; the corre-
sponding derivation tree is shown in the right part
of Figure 2. As can be seen, the derivation structure
is a dependency tree, except for the use of pretermi-
nals, as we desired.
The first part of the following theorem follows
from the existence of a Greibach Normal Form for
ECFG (Albert et al, 1999). The second part follows
immediately from the closure of CFG under regular
substitution.
L(GDG) = L(ECFG) = L(CFG).
Of course, ECFG, GDG and CFG are not strongly
equivalent in the standard sense for string rewrit-
ing systems of having the same sets of derivation
trees. Clearly, ECFG can generate all sets of deriva-
tion trees that GDG can, while CFG cannot (because
of the unbounded branching factor of ECFG and of
GDG); ECFG can also generate all sets of deriva-
tion trees that CFG can, while GDG cannot (because
of the lexicalization requirement). ECFG thus has
a greater strong generative capacity than CFG and
GDG, while those of GDG and CFG are incompa-
rable.
It is interesting to notice the difference between
the rewriting operation of a nonterminal symbol as
defined for a ECFG or a GDG and the equivalent
rewriting steps with a weakly equivalent CFG. A
GDG rewriting operation of a symbol X using a
rule r is decomposed in two stages, the first stage
consists in choosing a string w which belongs to the
set denoted by the right-hand side of r. During the
second stage, X is replaced by w. These two stages
are of a different nature, the first concerns the gener-
ation of CFG rules (and hence a CFG) using a GDG
while the second concerns the generation of a string
using the generated CFG. The equivalent rewriting
operation (X ? w) with a CFG does not distin-
guish the same two stages, both the selection of w
and the rewriting of X as w are done at the same
time.
man
(m2,4)
(m2,3)
(m3,3)
(m3,2) (m2,4)
(m2,3) telescopesaw
(m2,4)
(m2,4)
Pilar a
(D,2)
with
a
(D,2)
(m1,4) (m1,4)
(m1,4)
(m1,3)
(m1,2)
Figure 4: A packed parse forest
4 Parsing Algorithm
The parsing algorithm given here is a simple exten-
sion of the CKY algorithm. The difference is in the
use of finite state machines in the items in the chart
to represent the right-hand sides of the rules of the
ECFG.9 A rule with category C as its left-hand side
will give rise to a finite state machine which we call
a C-rule FSM; its final states mark the completed
recognition of a constituent of label C .
CKY-Style parsing algorithm for Extended
Context-Free Grammars.
Input. A ECFG G and an input string W =
w1 ? ? ?wn.
Output. The parse table T for W such that ti,j
contains (M, q) iff M is a C-rule-FSM, q is one
of the final states of M , and we have a derivation
C +=?wi ? ? ?wj . If i = j, ti,j also contains the in-
put symbol wi.
Method.
? Initialization: For each i, 1 ? i ? n, add wi
to ti,i.
? Completion: If ti,j contains either the input
symbol w or an item (M, q) such that q is
a final state of M , and M is a C-rule-FSM,
then add to ti,j all (M ?, q?) such that M ? is a
rule-FSM which transitions from a start state
to state q? on input w or C . Add a single back-
pointer from (M ?, q?) in ti,j to (M, q) or w in
ti,j .
9Recent work in the context of using ECFG for pars-
ing SGML and XML proposes an LL-type parser for ECFG
(Bru?ggemann-Klein and Wood, 2003); their approach also ex-
ploits the automaton representation of the right-hand side of
rules, as is natural for an algorithm dealing with ECFG.
1 2 3
N saw N
4 Pm1
1 3 4
Pilar
A manD
2 Ptelescope
telescope
man
m2
with N
2 31m3
Figure 5: Three rules FSM m1, m2 and m3. m1 is a V-rule-FSM corresponding to rule p1, m2 is an
N-rule-FSM which corresponds to rule p2 and m3 is a P-rule-FSM which corresponds to rule p3
P(3)
P(2) N(4)
N(3) telescopewith
a
D(2)
V(5)
V(5)
V(4)
sawV(3)
N(4)
Pilar
man
N(4)
N(3)
a
D(2)
V(5)
P(3)
P(2) N(4)
N(3) telescopewith
a
D(2)
N(4)
man
N(4)
N(3)
a
D(2)
V(4)
sawV(3)
N(4)
Pilar
Figure 6: A parse forest
? Scanning: If (M1, q1) is in ti,k, and tk+1,j
contains either the input symbol w or the item
(M2, q2) where q2 is a final state and M2 is a
C-rule-FSM, then add (M1, q) to ti,j (if not al-
ready present) if M1 transitions from q1 to q on
either w or C . Add a double backpointer from
(M1, q) in ti,j to (M1, q1) in ti,k (left back-
pointer) and to either w or (M2, q2) in tk+1,j
(right backpointer).
At the end of the parsing process, a packed parse
forest has been built. The packed forest correspond-
ing to the parse of sentence Pilar saw a man with a
telescope, using the grammar of Section 3 is repre-
sented in Figure 4. The nonterminal nodes are la-
beled with pairs (M, q) where M is an rule-FSM
and q a state of this FSM. Three rule-FSMs corre-
sponding to rules p1, p2 and p3 have been repre-
sented in Figure 5.
Obtaining the dependency trees from the packed
parse forest is performed in two stages. In a first
stage, a forest of binary syntagmatic trees is ob-
tained from the packed forest and in a second stage,
each syntagmatic tree is transformed into a depen-
dency tree. We shall not give the details of these
processes. The two trees resulting from de-packing
of Figure 4 are represented in Figure 6. The dif-
ferent nodes of the syntagmatic tree that will be
grouped in a single node of the dependency trees
have been circled.
5 Empirical Results
While the presentation of empirical results is not the
object of this paper, we give an overview of some
empirical work using ECFG for natural language
processing in this section. For full details, we refer
to (Nasr and Rambow, 2004a; Nasr and Rambow,
2004b; Nasr, 2004).
The parser presented in Section 4 above has been
implemented. We have investigated the use the
parser in a two-step probabilistic framework. In a
first step, we determine which rules of the ECFG
should be used for each word in the input sentence.
(Recall that a grammar rule encodes the active and
passive valency, as well as how any arguments are
realized, for example, fronted or in canonical posi-
tion.) This step is called supertagging and has been
suggested and studied in the context of Tree Adjoin-
ing Grammar by Bangalore and Joshi (1999). In
a second step, we use a probabilistic ECFG where
the probabilities are non-lexical and are based en-
tirely on the grammar rules. We extract the most
probable derivation from the compact parse forest
using dynamic programming in the usual manner.
This non-lexical probability model is used because
the supertagging step already takes the words in the
sentence into account. The probabilities can be en-
coded directly as weights on the transitions in the
rule-FSMs used by the parser.
The ECFG grammar we use has been automati-
cally extracted from the Penn Treebank (PTB). In
fact, we first extract a Tree Insertion Grammar fol-
lowing the work of (Xia et al, 2000; Chen, 2001;
Chiang, 2000), and then directly convert the trees
of the obtained TAG into automata for the parser.
It is clear that one could also derive an explicit
ECFG in the same manner. The extracted gram-
mar has about 4.800 rules. The probabilities are
estimated from the corpus during extraction. Note
that there are many different ways of extracting an
ECFG from the PTB, corresponding to different the-
ories of syntactic dependency. We have chosen to
directly model predicate-argument relations rather
than more surface-oriented syntactic relations such
as agreement, so that all function words (determin-
ers, auxiliaries, and so on) depend on the lexical
word. Strongly governed prepositions are treated as
part of a lexeme rather than as full prepositions.
We have investigated several different ways of
modeling the probability of attaching a sequence of
modifiers at a certain point in the derivation (con-
ditioning on the position of the modifier in the se-
quence or conditioning on the previous modifier
used). We found that using position or context im-
proves on using neither.
We have performed two types of experiments: us-
ing the correct ECFG rule for each word, and as-
signing ECFG rules automatically using supertag-
ging. In the case of using the correct supertag, we
obtain unlabeled dependency accuracies of about
98% (i.e., in about 2% of cases a word is assigned
a wrong governor). Automatic supertagging (us-
ing standard n-gram tagging methods) for a gram-
mar our size has an accuracy of about 80%. This
is also approximately the dependency accuracy ob-
tained when parsing the output of a supertagger. We
conclude from this performance that if we can in-
crease the performance of the supertagger, we can
also directly increase the performance of the parser.
Current work includes examining which grammati-
cal distinctions the grammar should make in order to
optimize both supertagging and parsing (Toussenel,
2004).
6 Conclusion
We have presented a generative string-rewriting
system, Extended Context-Free Grammar, whose
derivation trees are dependency trees with un-
bounded branching factor. We have shown how we
can reuse the representation of shared parse forests
well-known from CFGs for Extended Context-Free
Grammar. The question arises whether we can rep-
resent the shared parse forest in a manner more di-
rectly in the spirit of dependency. This question was
investigated by (Nasr, 2003). He shows that the fac-
toring realized in the shared forest presented here
and which is the key to the polynomial represen-
tation of a potentially exponential set, can be done
directly on the dependency trees by introducing the
notion of dependency sets.
References
Abney, Steven (1996). A grammar of projections.
Unpublished manuscript, Universita?t Tu?bingen.
Albert, Ju?rgen; Giammarresi, Dora; and Wood, Der-
ick (1999). Extended context-free grammars and
normal form algorithms. In Champarnaud, Jean-
Marc; Maurel, Denis; and Ziadi, Djelloul, ed-
itors, Automata Implementations: Third Inter-
national Workshop on Implementing Automata
(WIA?98), volume 1660 of LNCS, pages 1?12.
Springer Verlag.
Alshawi, Hiyan; Bangalore, Srinivas; and Douglas,
Shona (2000). Learning dependency translation
models as collections of finite-state head trans-
ducers. cl, 26(1):45?60.
Bangalore, Srinivas and Joshi, Aravind (1999).
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237?266.
Bru?ggemann-Klein, Anne and Wood, Derick
(2003). The parsing of extended context-free
grammars. Unpublished manuscript, Technische
Universita?t Mu?nchen and Hong Kong University
of Science & Technology.
Chen, John (2001). Towards Efficient Statistical
Parsing Using Lexicalized Grammatical Infor-
mation. PhD thesis, University of Delaware.
Chiang, David (2000). Statistical parsing with an
automatically-extracted tree adjoining grammar.
In 38th Meeting of the Association for Compu-
tational Linguistics (ACL?00), pages 456?463,
Hong Kong, China.
Collins, Michael (1997). Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics, Madrid,
Spain.
Dikovsky, Alexander and Modina, Larissa (2000).
Dependencies on the other side of the curtain.
Traitement Automatique des Langues, 41(1):79?
111.
Eisner, Jason (2000). Bilexical grammars and their
cubic-time parsing algorithms. In Bunt, Harry C.
and Nijholt, Anton, editors, New Developments
in Natural Language Parsing. Kluwer Academic
Publishers.
Gaifman, Haim (1965). Dependency systems and
phrase-structure systems. Information and Con-
trol, 8:304?337.
Holan, Toma?s?; Kubon?, Vladislav; Oliva, Karel; and
Pla?tek, Martin (1998). Two useful measures of
word order complexity. In Kahane, Sylvain and
Polgue`re, Alain, editors, Processing of Depen-
dency Grammars: Proceeding of the Workshop,
pages 21?28, Montre?al, Canada. ACL/COLING.
Joshi, Aravind K. and Schabes, Yves (1991). Tree-
adjoining grammars and lexicalized grammars.
In Nivat, Maurice and Podelski, Andreas, editors,
Definability and Recognizability of Sets of Trees.
Elsevier.
Kahane, Sylvain; Nasr, Alexis; and Rambow, Owen
(1998). Pseudo-projectivity: A polynomially
parsable non-projective dependency grammar. In
36th Meeting of the Association for Computa-
tional Linguistics and 17th International Con-
ference on Computational Linguistics (COLING-
ACL?98), pages 646?652, Montre?al, Canada.
Lang, Bernard (1991). Towards a uniform formal
framework for parsing. In Tomita, M., editor,
Current Issues in Parsing technology, chapter 11,
pages 153?171. Kluwer Academic Publishers.
Lombardo, Vincenzo (1996). An Earley-style
parser for dependency grammars. In Proceedings
of the 16th International Conference on Compu-
tational Linguistics (COLING?96), Copenhagen.
Lombardo, Vincenzo and Lesmo, Leonardo (2000).
A formal theory of dependency syntax with
empty units. Traitement automatque des langues,
41(1):179?209.
Madsen, O.L. and Kristensen, B.B. (1976). LR-
parsing of extended context-free grammars. Acta
Informatica, 7:61?73.
Mel?c?uk, Igor A. (1988). Dependency Syntax: The-
ory and Practice. State University of New York
Press, New York.
Nasr, Alexis (1996). Un syste`me de reformula-
tion automatique de phrases fonde? sur la The?orie
Sens-Texte : application aux langues contro?le?es.
PhD thesis, Universite? Paris 7.
Nasr, Alexis (2003). Factoring sufrace syntac-
tic structures. In First International Conference
on Meaning-Text Theory, pages 249?258, Paris,
France.
Nasr, Alexis (2004). Grammaires de de?pendances
ge?ne?ratives: expe?riences sur le Corpus Paris 7.
Unpublished manuscript, Universite? Paris 7.
Nasr, Alexis and Rambow, Owen (2004a). Depen-
dency parsing based on n-best-path supertagging.
Unpublished manuscript, Universite? Paris 7 and
COlumbia University.
Nasr, Alexis and Rambow, Owen (2004b). Su-
pertagging and full parsing. In Proceedings of
the Workshop on Tree Adjoining Grammar and
Related Formalisms (TAG+7), Vancouver, BC,
Canada.
Rambow, Owen and Joshi, Aravind (1997). A for-
mal look at dependency grammars and phrase-
structure grammars, with special consideration
of word-order phenomena. In Wanner, Leo,
editor, Recent Trends in Meaning-Text Theory,
pages 167?190. John Benjamins, Amsterdam and
Philadelphia.
Rambow, Owen; Vijay-Shanker, K.; and Weir,
David (2001). D-Tree Substitution Grammars.
Computational Linguistics, 27(1).
Sgall, P.; Hajic?ova?, E.; and Panevova?, J. (1986).
The meaning of the sentence and its semantic and
pragmatic aspects. Reidel, Dordrecht.
Sleator, Daniel and Temperley, Davy (1993). Pars-
ing english with a link grammar,. In Proceedings
of the Third International Workshop on Parsing
Technologies IIWPT?93).
Toussenel, Franc?ois (2004). Why supertagging is
hard. In Proceedings of the Workshop on Tree
Adjoining Grammar and Related Formalisms
(TAG+7), Vancouver, BC, Canada.
Woods, William A. (1970). Transition network
grammars for natural language analysis. Com-
mun. ACM, 3(10):591?606.
Xia, Fei; Palmer, Martha; and Joshi, Aravind
(2000). A uniform method of grammar extrac-
tion and its applications. In Proc. of the EMNLP
2000, Hong Kong.
Proceedings of NAACL HLT 2009: Short Papers, pages 185?188,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
MICA: A Probabilistic Dependency Parser
Based on Tree Insertion Grammars
Application Note
Srinivas Bangalore Pierre Boulllier
AT&T Labs ? Research INRIA
Florham Park, NJ, USA Rocquencourt, France
srini@research.att.com Pierre.Boullier@inria.fr
Alexis Nasr Owen Rambow Beno??t Sagot
Aix-Marseille Universite? CCLS, Columbia Univserity INRIA
Marseille, France New York, NY, USA Rocquencourt, France
alexis.nasr@lif.univ-mrs.fr rambow@ccls.columbia.edu benoit.sagot@inria.fr
Abstract
MICA is a dependency parser which returns
deep dependency representations, is fast, has
state-of-the-art performance, and is freely
available.
1 Overview
This application note presents a freely avail-
able parser, MICA (Marseille-INRIA-Columbia-
AT&T).1 MICA has several key characteristics that
make it appealing to researchers in NLP who need
an off-the-shelf parser.
? MICA returns a deep dependency parse, in
which dependency is defined in terms of lex-
ical predicate-argument structure, not in terms
of surface-syntactic features such as subject-verb
agreement. Function words such as auxiliaries
and determiners depend on their lexical head, and
strongly governed prepositions (such as to for give)
are treated as co-heads rather than as syntactic heads
in their own right. For example, John is giving books
to Mary gets the following analysis (the arc label is
on the terminal).
giving
John
arc=0
is
arc=adj
books
arc=1
to
arc=co-head
Mary
arc=2
The arc labels for the three arguments John,
books, and Mary do not change when the sentence
is passivized or Mary undergoes dative shift.
1We would like to thank Ryan Roth for contributing the
MALT data.
? MICA is based on an explicit phrase-structure
tree grammar extracted from the Penn Treebank.
Therefore, MICA can associate dependency parses
with rich linguistic information such as voice, the
presence of empty subjects (PRO), wh-movement,
and whether a verb heads a relative clause.
?MICA is fast (450 words per second plus 6 sec-
onds initialization on a standard high-end machine
on sentences with fewer than 200 words) and has
state-of-the-art performance (87.6% unlabeled de-
pendency accuracy, see Section 5).
? MICA consists of two processes: the supertag-
ger, which associates tags representing rich syntac-
tic information with the input word sequence, and
the actual parser, which derives the syntactic struc-
ture from the n-best chosen supertags. Only the su-
pertagger uses lexical information, the parser only
sees the supertag hypotheses.
? MICA returns n-best parses for arbitrary n;
parse trees are associated with probabilities. A
packed forest can also be returned.
? MICA is freely available2, easy to install under
Linux, and easy to use. (Input is one sentence per
line with no special tokenization required.)
There is an enormous amount of related work,
and we can mention only the most salient, given
space constraints. Our parser is very similar to the
work of (Shen and Joshi, 2005). They do not em-
ploy a supertagging step, and we do not restrict our
trees to spinal projections. Other parsers using su-
pertagging include the LDA of Bangalore and Joshi
(1999), the CCG-based parser of Clark and Curran
(2004), and the constraint-based approach of Wang
2http://www1.ccls.columbia.edu/?rambow/mica.html
185
and Harper (2004). Widely used dependency parsers
which generate deep dependency representations in-
clude Minipar (Lin, 1994), which uses a declarative
grammar, and the Stanford parser (Levy and Man-
ning, 2004), which performs a conversion from a
standard phrase-structure parse. All of these systems
generate dependency structures which are slightly
different from MICA?s, so that direct comparison
is difficult. For comparison purposes, we therefore
use the MALT parser generator (Nivre et al, 2004),
which allows us to train a dependency parser on our
own dependency structures. MALT has been among
the top performers in the CoNLL dependency pars-
ing competitions.
2 Supertags and Supertagging
Supertags are elementary trees of a lexicalized
tree grammar such as a Tree-Adjoining Gram-
mar (TAG) (Joshi, 1987). Unlike context-free gram-
mar rules which are single level trees, supertags are
multi-level trees which encapsulate both predicate-
argument structure of the anchor lexeme (by includ-
ing nodes at which its arguments must substitute)
and morpho-syntactic constraints such as subject-
verb agreement within the supertag associated with
the anchor. There are a number of supertags for each
lexeme to account for the different syntactic trans-
formations (relative clause, wh-question, passiviza-
tion etc.). For example, the verb give will be associ-
ated with at least these two trees, which we will call
tdi and tdi-dat. (There are also many other trees.)
tdi tdi-dat
S
NP0 ? VP
V? NP1 ? PP
P
to
NP2 ?
S
NP0 ? VP
V? NP2 ?NP1 ?
Supertagging is the task of disambiguating among
the set of supertags associated with each word in
a sentence, given the context of the sentence. In
order to arrive at a complete parse, the only step
remaining after supertagging is establishing the at-
tachments among the supertags. Hence the result of
supertagging is termed as an ?almost parse? (Banga-
lore and Joshi, 1999).
The set of supertags is derived from the Penn
Treebank using the approach of Chen (2001). This
extraction procedure results in a supertag set of
4,727 supertags and about one million words of su-
pertag annotated corpus. We use 950,028 annotated
words for training (Sections 02-21) and 46,451 (Sec-
tion 00) annotated words for testing in our exper-
iments. We estimate the probability of a tag se-
quence directly as in discriminative classification
approaches. In such approaches, the context of the
word being supertagged is encoded as features for
the classifier. Given the large scale multiclass la-
beling nature of the supertagging task, we train su-
pertagging models as one-vs-rest binary classifica-
tion problems. Detailed supertagging experiment re-
sults are reported in (Bangalore et al, 2005) which
we summarize here. We use the lexical, part-of-
speech attributes from the left and right context
in a 6-word window and the lexical, orthographic
(e.g. capitalization, prefix, suffix, digit) and part-
of-speech attributes of the word being supertagged.
Crucially, this set does not use the supertags for the
words in the history. Thus during decoding the su-
pertag assignment is done locally and does not need
a dynamic programming search. We trained a Max-
ent model with such features using the labeled data
set mentioned above and achieve an error rate of
11.48% on the test set.
3 Grammars and Models
MICA grammars are extracted in a three steps pro-
cess. In a first step, a Tree Insertion Grammar (TIG)
(Schabes and Waters, 1995) is extracted from the
treebank, along with a table of counts. This is the
grammar that is used for supertagging, as described
in Section 2. In a second step, the TIG and the count
table are used to build a PCFG. During the last step,
the PCFG is ?specialized? in order to model more
finely some lexico-syntactic phenomena. The sec-
ond and third steps are discussed in this section.
The extracted TIG is transformed into a PCFG
which generates strings of supertags as follows. Ini-
tial elementary trees (which are substituted) yield
rules whose left hand side is the root category of
the elementary tree. Left (respectively right) aux-
iliary trees (the trees for which the foot node is the
186
left (resp. right) daughter of the root) give birth to
rules whose left-hand side is of the form Xl (resp.
Xr), where X is the root category of the elementary
tree. The right hand side of each rule is built during
a top down traversal of the corresponding elemen-
tary tree. For every node of the tree visited, a new
symbol is added to the right hand side of rule, from
left to right, as follows:
? The anchor of the elementary tree adds the su-
pertag (i.e., the name of the tree), which is a terminal
symbol, to the context-free rule.
? A substitution node in the elementary tree adds
its nonterminal symbol to the context-free rule.
? A interior node in the elementary tree at which
adjunction may occur adds to the context-free rule
the nonterminal symbol X ?r or X ?l , where X is the
node?s nonterminal symbol, and l (resp. r) indicates
whether it is a left (resp. right) adjunction. Each
interior node is visited twice, the first time from the
left, and then from the right. A set of non-lexicalized
rules (i.e., rules that do not generate a terminal sym-
bol) allow us to generate zero or more trees anchored
by Xl from the symbol X ?l . No adjunction, the first
adjunction, and the second adjunction are modeled
explicitly in the grammar and the associated prob-
abilistic model, while the third and all subsequent
adjunctions are modeled together.
This conversion method is basically the same as
that presented in (Schabes and Waters, 1995), ex-
cept that our PCFG models multiple adjunctions at
the same node by positions (a concern Schabes and
Waters (1995) do not share, of course). Our PCFG
construction differs from that of Hwa (2001) in that
she does not allow multiple adjunction at one node
(Schabes and Shieber, 1994) (which we do since we
are interested in the derivation structure as a repre-
sentation of linguistic dependency). For more in-
formation about the positional model of adjunction
and a discussion of an alternate model, the ?bigram
model?, see (Nasr and Rambow, 2006).
Tree tdi from Section 2 gives rise to the following
rule (where tdi and tCO are terminal symbols and
the rest are nonterminals): S ? S?l NP VP?l V?l tdiV?r NP PP?l P?l tCO P?r NP PP?r VP?r S?r
The probabilities of the PCFG rules are estimated
using maximum likelihood. The probabilistic model
refers only to supertag names, not to words. In the
basic model, the probability of the adjunction or sub-
stitution of an elementary tree (the daughter) in an-
other elementary tree (the mother) only depends on
the nonterminal, and does not depend on the mother
nor on the node on which the attachment is per-
formed in the mother elementary tree. It is well
known that such a dependency is important for an
adequate probabilistic modelling of syntax. In order
to introduce such a dependency, we condition an at-
tachment on the mother and on the node on which
the attachment is performed, an operation that we
call mother specialization. Mother specialization is
performed by adding to all nonterminals the name of
the mother and the address of a node. The special-
ization of a grammar increase vastly the number of
symbols and rules and provoke severe data sparse-
ness problems, this is why only a subset of the sym-
bols are specialized.
4 Parser
SYNTAX (Boullier and Deschamp, 1988) is a sys-
tem used to generate lexical and syntactic analyzers
(parsers) (both deterministic and non-deterministic)
for all kind of context-free grammars (CFGs) as
well as some classes of contextual grammars. It
has been under development at INRIA for several
decades. SYNTAX handles most classes of determin-
istic (unambiguous) grammars (LR, LALR, RLR)
as well as general context-free grammars. The
non-deterministic features include, among others,
an Earley-like parser generator used for natural lan-
guage processing (Boullier, 2003).
Like most SYNTAX Earley-like parsers, the archi-
tecture of MICA?s PCFG-based parser is the follow-
ing:
? The Earley-like parser proper computes a shared
parse forest that represents in a factorized (polyno-
mial) way all possible parse trees according to the
underlying (non-probabilistic) CFG that represents
the TIG;
? Filtering and/or decoration modules are applied
on the shared parse forest; in MICA?s case, an n-
best module is applied, followed by a dependency
extractor that relies on the TIG structure of the CFG.
The Earley-like parser relies on Earley?s algo-
rithm (Earley, 1970). However, several optimiza-
tions have been applied, including guiding tech-
niques (Boullier, 2003), extensive static (offline)
187
computations over the grammar, and efficient data
structures. Moreover, Earley?s algorithm has been
extended so as to handle input DAGs (and not only
sequences of forms). A particular effort has been
made to handle huge grammars (over 1 million
symbol occurrences in the grammar), thanks to ad-
vanced dynamic lexicalization techniques (Boullier
and Sagot, 2007). The resulting efficiency is satisfy-
ing: with standard ambiguous NLP grammars, huge
shared parse forest (over 1010 trees) are often gener-
ated in a few dozens of milliseconds.
Within MICA, the first module that is applied on
top of the shared parse forest is SYNTAX?s n-best
module. This module adapts and implements the al-
gorithm of (Huang and Chiang, 2005) for efficient
n-best trees extraction from a shared parse forest. In
practice, and within the current version of MICA,
this module is usually used with n = 1, which iden-
tifies the optimal tree w.r.t. the probabilistic model
embedded in the original PCFG; other values can
also be used. Once the n-best trees have been ex-
tracted, the dependency extractor module transforms
each of these trees into a dependency tree, by ex-
ploiting the fact that the CFG used for parsing has
been built from a TIG.
5 Evaluation
We compare MICA to the MALT parser. Both
parsers are trained on sections 02-21 of our de-
pendency version of the WSJ PennTreebank, and
tested on Section 00, not counting true punctuation.
?Predicted? refers to tags (PTB-tagset POS and su-
pertags) predicted by our taggers; ?Gold? refers to
the gold POS and supertags. We tested MALT using
only POS tags (MALT-POS), and POS tags as well
as 1-best supertags (MALT-all). We provide unla-
beled (?Un?) and labeled (?Lb?) dependency accu-
racy (%). As we can see, the predicted supertags do
not help MALT. MALT is significantly slower than
MICA, running at about 30 words a second (MICA:
450 words a second).
MICA MALT-POS MALT-all
Pred Gold Pred Gold Pred Gold
Lb 85.8 97.3 86.9 87.4 86.8 96.9
Un 87.6 97.6 88.9 89.3 88.5 97.2
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?266.
Srinivas Bangalore, Patrick Haffner, and Gae?l Emami.
2005. Factoring global inference by enriching local rep-
resentations. Technical report, AT&T Labs ? Reserach.
Pierre Boullier and Philippe Deschamp.
1988. Le syste`me SYNTAXTM ? manuel
d?utilisation et de mise en ?uvre sous UNIXTM.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Beno??t Sagot. 2007. Are very large
grammars computationnaly tractable? In Proceedings of
IWPT?07, Prague, Czech Republic.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 7th International Workshop on =20 Pars-
ing Technologies, pages 43?54, Nancy, France.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D. the-
sis, University of Delaware.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL?04.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, Vancouver, Canada.
Rebecca Hwa. 2001. Learning Probabilistic Lexicalized
Grammars for Natural Language Processing. Ph.D. the-
sis, Harvard University.
Aravind K. Joshi. 1987. An introduction to Tree Ad-
joining Grammars. In A. Manaster-Ramer, editor, Math-
ematics of Language. John Benjamins, Amsterdam.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Correct-
ing the surface dependency approximation. In ACL?04.
Dekang Lin. 1994. PRINCIPAR?an efficient, broad-
coverage, principle-based parser. In Coling?94.
Alexis Nasr and Owen Rambow. 2006. Parsing with
lexicalized probabilistic recursive transition networks. In
Finite-State Methods and Natural Language Processing,
Springer Verlag Lecture Notes in Commputer Science.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL-2004.
Yves Schabes and Stuart Shieber. 1994. An alternative
conception of tree-adjoining derivation. Computational
Linguistics, 1(20):91?124.
Yves Schabes and Richard C. Waters. 1995. Tree Inser-
tion Grammar. Computational Linguistics, 21(4).
Libin Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP?05.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proceed-
ings of the ACL Workshop on Incremental Parsing.
188
 	


Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 117?128,
Paris, October 2009. c?2009 Association for Computational Linguistics
Constructing parse forests that include exactly the n-best PCFG trees
Pierre Boullier1, Alexis Nasr2 and Beno??t Sagot1
1. Alpage, INRIA Paris-Rocquencourt & Universite? Paris 7
Domaine de Voluceau ? Rocquencourt, BP 105 ? 78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
2. LIF, Univ. de la Me?diterranne?e
163, avenue de Luminy - Case 901 ? 13288 Marseille Cedex 9, France
Alexis.Nasr@lif.univ-mrs.fr
Abstract
This paper describes and compares two al-
gorithms that take as input a shared PCFG
parse forest and produce shared forests
that contain exactly the n most likely trees
of the initial forest. Such forests are
suitable for subsequent processing, such
as (some types of) reranking or LFG f-
structure computation, that can be per-
formed ontop of a shared forest, but that
may have a high (e.g., exponential) com-
plexity w.r.t. the number of trees contained
in the forest. We evaluate the perfor-
mances of both algorithms on real-scale
NLP forests generated with a PCFG ex-
tracted from the Penn Treebank.
1 Introduction
The output of a CFG parser based on dynamic
programming, such as an Earley parser (Earley,
1970), is a compact representation of all syntac-
tic parses of the parsed sentence, called a shared
parse forest (Lang, 1974; Lang, 1994). It can rep-
resent an exponential number of parses (with re-
spect to the length of the sentence) in a cubic size
structure. This forest can be used for further pro-
cessing, as reranking (Huang, 2008) or machine
translation (Mi et al, 2008).
When a CFG is associated with probabilistic in-
formation, as in a Probabilistic CFG (PCFG), it
can be interesting to process only the n most likely
trees of the forest. Standard state-of-the-art algo-
rithms that extract the n best parses (Huang and
Chiang, 2005) produce a collection of trees, los-
ing the factorization that has been achieved by the
parser, and reproduce some identical sub-trees in
several parses.
This situation is not satisfactory since post-
parsing processes, such as reranking algorithms
or attribute computation, cannot take advantage
of this lost factorization and may reproduce some
identical work on common sub-trees, with a com-
putational cost that can be exponentally high.
One way to solve the problem is to prune the
forest by eliminating sub-forests that do not con-
tribute to any of the n most likely trees. But this
over-generates: the pruned forest contains more
than the n most likely trees. This is particularly
costly for post-parsing processes that may require
in the worst cases an exponential execution time
w.r.t. the number of trees in the forest, such as
LFG f-structures construction or some advanced
reranking techniques. The experiments detailed
in the last part of this paper show that the over-
generation factor of pruned sub-forest is more or
less constant (see 6): after pruning the forest so as
to keep the n best trees, the resulting forest con-
tains approximately 103n trees. At least for some
post-parsing processes, this overhead is highly
problematic. For example, although LFG parsing
can be achieved by computing LFG f-structures
on top of a c-structure parse forest with a reason-
able efficiency (Boullier and Sagot, 2005), it is
clear that a 103 factor drastically affects the overall
speed of the LFG parser.
Therefore, simply pruning the forest is not an
adequate solution. However, it will prove useful
for comparison purposes.
The new direction that we explore in this pa-
per is the production of shared forests that con-
tain exactly the n most likely trees, avoiding both
the explicit construction of n different trees and
the over-generation of pruning techniques. This
can be seen as a transduction which is applied on
a forest and produces another forest. The trans-
duction applies some local transformations on the
structure of the forest, developing some parts of
the forest when necessary.
The structure of this paper is the following. Sec-
tion 2 defines the basic objects we will be dealing
with. Section 3 describes how to prune a shared
117
forest, and introduces two approaches for build-
ing shared forests that contain exactly the n most
likely parses. Section 4 describes experiments that
were carried out on the Penn Treebank and sec-
tion 5 concludes the paper.
2 Preliminaries
2.1 Instantiated grammars
Let G = ?N ,T ,P, S? be a context-free grammar
(CFG), defined in the usual way (Aho and Ullman,
1972). Throughout this paper, we suppose that we
manipulate only non-cyclic CFGs,1 but they may
(and usually do) include ?-productions. Given a
production p ? P, we note lhs(p) its left-hand
side, rhs(p) its right-hand side and |p| the length
of rhs(p). Moreover, we note rhsk(p), with 1 ?
k ? |p|, the kth symbol of rhs(p). We call A-
production any production p ? P of G such that
lhs(p) = A.
A complete derivation of a sentence w =
t1 . . . t|w| (?i ? |w|, ti ? T ) w.r.t. G is of the form
S ??
G,w
?A? ?
G,w
?X1X2 . . . Xr? ??
G,w
w. By def-
inition, A ? X1X2 . . . Xr is a production of G.
Each of A, X1, X2, . . . , Xr spans a unique oc-
currence of a substring ti+1 . . . tj of w, that can
be identified by the corresponding range, noted
i..j. A complete derivation represents a parse tree
whose yield is w, in which each symbol X of
range i..j roots a subtree whose yield is ti+1 . . . tj
(i.e., a derivation of the form X ??
G,w
ti+1 . . . tj).
Let us define the w-instantiation operation (or
instantiation). It can be applied to symbols and
productions of G, and to G itself, w.r.t. a string
w. It corresponds to the well-known intersection
of G with the linear automaton that corresponds
to the string w. We shall go into further detail for
terminology, notation and illustration purposes.
An instantiated non terminal symbol is a triple
noted Ai..j where A ? N and 0 ? i ? j ? |w|.
Similarly, an instantiated terminal symbol is a
triple noted Ti..j where T ? T and 0 ? i ? j =
i + 1 ? |w|. An instantiated symbol, terminal or
non terminal, is noted Xi..j . For any instantiated
symbol Xi..j , i (resp. j) is called its lower bound
1Actually, cyclic CFG can be treated as well, but not
cyclic parse forests. Therefore, if using a cyclic CFG which,
on a particular sentence, builds a cyclic parse forest, cycles
have to be removed before the algorithms descibed in the next
sections are applied. This is the case in the SYNTAX system
(see below).
(resp. upper bound), and can be extracted by the
operator lb() (resp. ub()).
An instantiated production (or instantiated
rule) is a context-free production Ai..j ?
X1i1..j1X2i2..j2 . . . Xrir ..jr whose left-hand side is an
instantiated non terminal symbol and whose right-
hand side is a (possibly empty) sequence of in-
stantiated (terminal or non terminal) symbols, pro-
vided the followings conditions hold:
1. the indexes involved are such that i = i1, j =
jr , and ?l such that 1 ? l < r, jl = il+1;
2. the corresponding non-instantiated produc-
tion A ? X1X2 . . . Xr is a production of
G.
If lhs(p) = Ai..j , we set lb(p) = i and ub(p) = j.
In a complete derivation S ??
G,w
?A? ?
G,w
?X1X2 . . . Xr? ??
G,w
w, any symbol X that spans
the range i..j can be replaced by the instantiated
symbols Xi..j . For example, the axiom S can be
replaced by the instantiated axiom S0..|w| in the
head of the derivation. If applied to the whole
derivation, this operation creates an instantiated
derivation, whose rewriting operations define a
particular set of instantiated productions. Given
G and w, the set of all instantiated productions in-
volved in at least one complete derivation of w is
unique, and noted Pw. An instantiated derivation
represents an instantiated parse tree, i.e., a parse
tree whose node labels are instantiated symbols.
In an instantiated parse tree, each node label is
unique, and therefore we shall not distinguish be-
tween a node in an instantiated parse tree and its
label (i.e., an instantiated symbol).
Then, the w-instantiated grammar Gw for G
and w is a CFG ?Nw,Tw,Pw, S0..|w|? such that:
1. Pw is defined as explained above;
2. Nw is a set of instantiated non terminal sym-
bols;
3. Tw is a set of instantiated terminal symbols.
It follows from the definition of Pw that (instan-
tiated) symbols of Gw have the following prop-
erties: Ai..j ? Nw ? A ??G,w ti+1 . . . tj , and
Ti..j ? Tw ? T = tj .
The w-instantiated CFG Gw represents all parse
trees for w in a shared (factorized) way. It is the
grammar representation of the parse forest of w
118
w.r.t. G.2 In fact, L(Gw) = {w} and the set
of parses of w with respect to Gw is isomorphic
to the set of parses of w with respect to G, the
isomorphism being the w-instantiation operation.
The size of a forest is defined as the size of the
grammar that represents it, i.e., as the number of
symbol occurrences in this grammar, which is de-
fined as the number of productions plus the sum of
the lengths of all right-hand sides.
Example 1: First running example.
Let us illustrate these definitions by an example.
Given the sentence w = the boy saw a man with a
telescope and the grammar G (that the reader has
in mind), the instantiated productions of Gw are:
Det0..1 ? the0..1 N1..2 ? boy1..2
NP0..2 ? Det0..1 N1..2 V2..3 ? saw2..3
Det3..4 ? a3..4 N4..5 ? man4..5
NP3..5 ? Det3..4 N4..5 Prep5..6 ? with5..6
Det6..7 ? a6..7 N7..8 ? telescope7..8
NP6..8 ? Det6..7 N7..8 PP5..8 ? Prep5..6 NP6..8
NP3..8 ? NP3..5 PP5..8 VP2..8 ? V2..3 NP3..8
VP2..5 ? V2..3 NP3..5 VP2..8 ? VP2..5 PP5..8
S0..8 ? NP0..2 VP2..8
They represent the parse forest of w according to
G. This parse forest contains two trees, since there
is one ambiguity: VP2..8 can be rewritten in two
different ways.
The instantiated grammar Gw can be repre-
sented as an hypergraph (as in (Klein and Man-
ning, 2001) or (Huang and Chiang, 2005)) where
the instantiated symbols of Gw correspond to the
vertices of the hypergraph and the instantiated pro-
ductions to the hyperarcs.
We define the extension of an instantiated sym-
bol Xi..j , noted E(Xi..j), as the set of instantiated
parse trees that have Xi..j as a root. The set of all
parse trees of w w.r.t. G is therefore E(S0..|w|). In
the same way, we define the extension of an in-
stantiated production Xi..j ? ? to be the subset
of E(Xi..j) that corresponds to derivations of the
form Xi..j ?G,w ?
??
G,w
ti+1 . . . tj (i.e., trees rooted
in Xi..j and where the daughters of the node Xi..j
are the symbols of ?).
2.2 Forest traversals
Let us suppose that we deal with non-cyclic
forests, i.e., we only consider forests that are rep-
2In particular, if G is a binary grammar, its w-instantation
(i.e., the parse forest of w) has a size O(|w|3), whereas it rep-
resents a potentially exponential number of parse trees w.r.t
|w| since we manipulate only non-cyclic grammars.
resented by a non-recursive instantiated CFG. In
this case, we can define two different kinds of for-
est traversals.
A bottom-up traversal of a forest is a traversal
with the following constraint: an Ai..j-production
is visited if and only if all its instantiated right-
hand side symbols have already been visited; the
instantiated symbol Ai..j is visited once all Ai..j-
productions have been visited. The bottom-up
visit starts by visiting all instantiated productions
with right-hand sides that are empty or contain
only (instantiated) terminal symbols.
A top-down traversal of a forest is a traversal
with the following constraint: a node Ai..j is vis-
ited if and only if all the instantiated productions
in which it occurs in right-hand side have already
been visited; once an instantiated production Ai..j
has been visited, all its Ai..j-productions are vis-
ited as well. Of course the top-down visit starts by
the visit of the axiom S0..|w|.
2.3 Ranked instantiated grammar
When an instantiated grammar Gw =
?Nw,Tw,Pw, S0..|w|? is built on a PCFG, ev-
ery parse tree in E(S0..|w|) has a probability that
is computed in the usual way (Booth, 1969). We
might be interested in extracting the kth most
likely tree of the forest represented by Gw,3 with-
out unfolding the forest, i.e., without enumerating
trees. In order to do so, we need to add some
extra structure to the instantiated grammar. The
augmented instantiated grammar will be called a
ranked instantiated grammar.
This extra structure takes the form of n-best ta-
bles that are associated with each instantiated non
terminal symbol (Huang and Chiang, 2005), thus
leading to ranked instantiated non terminal sym-
bols, or simply instantiated symbols when the con-
text is non ambiguous. A ranked instantiated non
terminal symbol is written ?Ai..j,T (Ai..j)?, where
T (Ai..j) is the n-best table associated with the in-
stantiated symbol Ai..j .
T (Ai..j) is a table of at most n entries. The
k-th entry of the table, noted e, describes how to
build the k-th most likely tree of E(Ai..j). This
tree will be called the k-th extention of Ai..j , noted
Ek(Ai..j). More precisely, e indicates the instanti-
ated Ai..j-production p such that Ek(Ai..j) ? E(p).
It indicates furthermore which trees of the exten-
3In this paper, we shall use the kth most likely tree and the
tree of rank k as synonyms.
119
sions of p?s right-hand side symbols must be com-
bined together in order to build Ek(Ai..j).
We also define the m,n-extension of Ai..j as
follows: Em,n(Ai..j) = ?m?k?nEk(Ai..j).
Example 2: n-best tables for the first running
example.
Let us illustrate this idea on our first running ex-
ample. Recall that in Example 1, the symbol VP2..8
can be rewritten using the two following produc-
tions :
VP2..8 ? V2..3 NP3..8
VP2..8 ? VP2..5 PP5..8
T (VP2..8) has the following form:
1 P1 VP2..8 ? V2..3 NP3..8 ?1, 1? 1
2 P2 VP2..8 ? VP2..5 PP5..8 ?1, 1? 1
This table indicates that the most likely tree
associated with VP2..8 (line one) has probability
P1 and is built using the production VP2..8 ?
V2..3 NP3..8 by combining the most likely tree of
E(V2..3) (indicated by the first 1 in ?1, 1?) with the
most likely tree of E(NP3..8) (indicated by the sec-
ond 1 in ?1, 1?). It also indicates that the most
likely tree of E(VP2..8) is the most likely tree of
E(VP2..8 ? V2..3 NP3..8) (indicated by the pres-
ence of 1 in the last column of entry 1) and the
second most likely tree of E(VP2..8) is the most
likely tree of E(VP2..8 ? VP2..5 PP5..8). This last
integer is called the local rank of the entry.
More formally, the entry T (Ai..j)[k] is defined
as a 4-tuple ?Pk, pk, ~vk, lk? where k is the rank
of the entry, Pk is the probability of the tree
Ek(Ai..j), pk is the instantiated production such
that Ek(Ai..j) ? E(pk), ~vk is a tuple of |rhs(pk)|
integers and lk is the local rank.
The tree Ek(Ai..j) is rooted by Ai..j , and its
daughters root N = |rhs(pk)| subtrees that are
E ~vk[1](rhs1(pk)), . . . , E ~vk [N ](rhsN (pk)).
Given an instantiated symbol Ai..j and an in-
stantitated production p ? P (Ai..j), we define
the n-best table of p to be the table composed
of the entries ?Pk, pk, ~vk, lk? of T (Ai..j) such that
pk = p.
Example 3: Second running example.
The following is a standard PCFG (probabili-
ties are shown next to the corresponding clauses).
S ? A B 1
A ? A1 0.7 A1 ? a 1
A ? A2 0.3 A2 ? a 1
B ? B1 0.6 B1 ? b 1
B ? B2 0.4 B2 ? b 1
The instantiation of the underlying (non-
probabilistic) CFG grammar by the input text
w = a b is the following.
S1..3 ? A1..2 B2..3
A1..2 ? A11..2 A11..2 ? a1..2
A1..2 ? A21..2 A21..2 ? a1..2
B2..3 ? B12..3 B12..3 ? b2..3
B2..3 ? B22..3 B22..3 ? b2..3
This grammar represents a parse forest that con-
tains four different trees, since on the one hand one
can reach (parse) the instantiated terminal symbol
a1..2 through A1 or A2, and on the other hand one
can reach (parse) the instantiated terminal sym-
bol b1..2 through B1 or B2. Therefore, when dis-
cussing this example in the remainder of the paper,
each of these four trees will be named accordingly:
the tree obtained by reaching a through Ai and b
through Bj (i and j are 1 or 2) shall be called
Ti,j .
The corresponding n-best tables are trivial
(only one line) for all instantiated symbols but
A1..2, B2..3 and S1..3. That of A1..2 is the follow-
ing 2-line table.
1 0.7 A ? A1 ?1? 1
2 0.3 A ? A2 ?1? 1
The n-best table for B2..3 is similar. The n-best
table for S1..3 is:
1 0.42 S1..3 ? A1..2 B2..3 ?1, 1? 1
2 0.28 S1..3 ? A1..2 B2..3 ?1, 2? 2
3 0.18 S1..3 ? A1..2 B2..3 ?2, 1? 3
4 0.12 S1..3 ? A1..2 B2..3 ?2, 2? 4
Thanks to the algorithm sketched in section 2.4,
these tables allow to compute the following obvi-
ous result: the best tree is T1,1, the second-best
tree is T1,2, the third-best tree is T2,1 and the worst
tree is T2,2.
If n = 3, the pruned forest over-generates: all
instantiated productions take part in at least one
of the three best trees, and therefore the pruned
forest is the full forest itself, which contains four
trees.
We shall use this example later on so as to il-
lustrate both methods we introduce for building
forests that contain exactly the n best trees, with-
out overgenerating.
2.4 Extracting the kth-best tree
An efficient algorithm for the extraction of the n-
best trees is introduced in (Huang and Chiang,
2005), namely the authors? algorithm 3, which
120
is a re-formulation of a procedure originally pro-
posed by (Jime?nez and Marzal, 2000). Contrar-
ily to (Huang and Chiang, 2005), we shall sketch
this algorithm with the terminology introduced
above (whereas the authors use the notion of hy-
pergraph). The algorithm relies on the n-best ta-
bles described above: extracting the kth-best tree
consists in extending the n-best tables as much as
necessary by computing all lines in each n-best ta-
ble up to those that concern the kth-best tree.4
The algorithm can be divided in two sub-
algorithms: (1) a bottom-up traversal of the for-
est for extracting the best tree; (2) a top-down
traversal for extracting the kth-best tree provided
the (k ? 1)th-best has been already extracted.
The extraction of the best tree can be seen as a
bottom-up traversal that initializes the n-best ta-
bles: when visiting a node Ai..j , the best probabil-
ity of each Ai..j-production is computed by using
the tables associated with each of their right-hand
side symbols. The best of these probabilities gives
the first line of the n-best table for Ai..j (the result
for other productions are stored for possible later
use). Once the traversal is completed (the instanti-
ated axiom has been reached), the best tree can be
easily output by following recursively where the
first line of the axiom?s n-best table leads to.
Let us now assume we have extracted all k?-best
trees, 1 ? k? < k, for a given k ? n. We want
to extract the kth-best tree. We achieve this recur-
sively by a top-down traversal of the forest. In or-
der to start the construction of the kth-best tree, we
need to know the following:
? which instantiated production p must be used
for rewriting the instantiated axiom,
? for each of p?s right-hand side symbols Ai..j ,
which subtree rooted in Ai..j must be used;
this subtree is identified by its local rank
kAi..j , i.e., the rank of its probability among
all subtrees rooted in Ai..j.
This information is given by the kth line of the n-
best table associated with the instantiated axiom.
If this kth line has not been filled yet, it is com-
puted recursively.5 Once the kth line of the n-best
4In the remainder of this paper, we shall use ?extracting
the kth-best tree? as a shortcut for ?extending the n-best ta-
bles up to what is necessary to extract the kth-best tree? (i.e.,
we do not necessarily really build or print the kth-best tree).
5Because the k ? 1th-best tree has been computed, this n-
best table is filled exactly up to line k?1. The kth line is then
table is known, i.e., p and all kAi..j ?s are known,
the rank k is added to p?s so-called rankset, noted
?(p). Then, the top-down traversal extracts recur-
sively for each Ai..j the appropriate subtree as de-
fined by kAi..j . After having extracted the n-th
best tree, we know that a given production p is in-
cluded in the kth-best tree, 1 ? k ? n, if and only
if k ? ?(p).
3 Computing sub-forests that only
contain the n best trees
Given a ranked instantiated grammar Gw, we are
interested in building a new instantiated grammar
which contains exactly the n most likely trees of
E(Gw). In this section, we introduce two algo-
rithms that compute such a grammar (or forest).
Both methods rely on the construction of new
symbols, obtained by decorating instantiated sym-
bols of Gw.
An empirical comparison of the two methods is
described in section 4. In order to evaluate the
size of the new constructed grammars (forests),
we consider as a lower bound the so-called pruned
forest, which is the smallest sub-grammar of the
initial instantiated grammar that includes the n
best trees. It is built simply by pruning produc-
tions with an empty rankset: no new symbols
are created, original instantiated symbols are kept.
Therefore, it is a lower bound in terms of size.
However, the pruned forest usually overgenerates,
as illustrated by Example 3.
3.1 The ranksets method
The algorithm described in this section builds an
instantiated grammar Gnw by decorating the sym-
bols of Gw. The new (decorated) symbols have
the form A?i..j where ? is a set of integers called
a rankset. An integer r is a rank iff we have
1 ? r ? n.
The starting point of this algorithm is set of n-
best tables, built as explained in section 2.4, with-
out explicitely unfolding the forest.
computed as follows: while constructing the k?th-best trees
for each k? between 1 and k?1, we have identified many pos-
sible rewritings of the instantiated axiom, i.e., many (produc-
tion, right-hand side local ranks) pairs; we know the proba-
bility of all these rewritings, although only some of them con-
situte a line of the instantiated axiom?s n-best table; we now
identify new rewritings, starting from known rewritings and
incrementing only one of their local ranks; we compute (re-
cursively) the probability of these newly identified rewritings;
the rewriting that has the best probability among all those that
are not yet a line of the n-best table is then added: it is its kth
line.
121
A preliminary top-down step uses these n-best
tables for building a parse forest whose non-
terminal symbols (apart from the axiom) have the
form A?i..j where ? is a singleton {r}: the sub-
forest rooted in A{r}i..j contains only one tree, that
of local rank r. Only the axiom is not decorated,
and remains unique. Terminal symbols are not af-
fected either.
At this point, the purpose of the algorithm is to
merge productions with identical right-hand sides,
whenever possible. This is achieved in a bottom-
up fashion as follows. Consider two symbols A?1i..j
and A?2i..j , which differ only by their underlying
ranksets. These symbols correspond to two dif-
ferent production sets, namely the set of all A?1i..j-
productions (resp. A?2i..j-productions). Each of
these production sets define a set of right-hand
sides. If these two right-hand side sets are iden-
tical we say that A?1i..j and A
?2
i..j are equivalent. In
that case introduce the rankset ? = ?1 ? ?2 and
create a new non-terminal symbol A?i..j . We now
simply replace all occurrences of A?1i..j and A
?2
i..j
in left- and right-hand sides by A?i..j . Of course
(newly) identical productions are erased. After
such a transformation, the newly created symbol
may appear in the right-hand side of productions
that now only differ by their left-hand sides; the
factorization spreads to this symbol in a bottom-
up way. Therefore, we perform this transforma-
tion until no new pair of equivalent symbols is
found, starting from terminal leaves and percolat-
ing bottom-up as far as possible.
Example 4: Applying the ranksets method to
the second running example.
Let us come back to the grammar of Example 3,
and the same input text w = a b as before. As
in Example 3, we consider the case when we are
interested in the n = 3 best trees.
Starting from the instantiated grammar and the
n-best tables given in Example 3, the preliminary
top-down step builds the following forest (for clar-
ity, ranksets have not been shown on symbols that
root sub-forests containing only one tree):
S1..3 ? A{1}1..2 B
{1}
2..3
S1..3 ? A{1}1..2 B
{2}
2..3
S1..3 ? A{2}1..2 B
{1}
2..3
A{1}1..2 ? A11..2 A11..2 ? a1..2
A{2}1..2 ? A21..2 A21..2 ? a1..2
B{1}2..3 ? B12..3 B12..3 ? b2..3
B{2}2..3 ? B22..3 B22..3 ? b2..3
In this example, the bottom-up step doesn?t fac-
torize out any other symbols, and this is therefore
the final output of the ranksets method. It con-
tains 2 more productions and 3 more symbols than
the pruned forest (which is the same as the origi-
nal forest), but it contains exactly the 3 best trees,
contrarily to the pruned forest.
3.2 The rectangles method
In this section only, we assume that the grammar
G is binary (and therefore the forest, i.e., the gram-
mar Gw, is binary). Standard binarization algo-
rithms can be found in the litterature (Aho and Ull-
man, 1972).
The algorithm described in this section per-
forms, as the preceding one, a decoration of the
symbols of Gw. The new (decorated) symbols
have the form Ax,yi..j , where x and y denote ranks
such that 1 ? x ? y ? n. The semantics of the
decoration is closely related to the x, y extention
of Ai..j , introduced in 2.3:
E(Ax,yi..j) = Ex,y(Ai..j)
It corresponds to ranksets (in the sense of the
previous section) that are intervals: Ax,yi..j is equiv-
alent to the previous section?s A{x,x+1,...,y?1,y}i..j . In
other words, the sub-forest rooted with Ax,yi..j con-
tains exactly the trees of the initial forest, rooted
with Ai..j , which rank range from x to y.
The algorithm performs a top-down traversal of
the initial instantiated grammar Gw. This traver-
sal also takes as input two parameters x and y. It
starts with the symbol S0..|w| and parameters 1 and
n. At the end of the traversal, a new decorated for-
est is built which contains exactly n most likely
the parses. During the traversal, every instantiated
symbol Ai..j will give birth to decorated instanti-
ated symbols of the form Ax,yi..j where x and y are
determined during the traversal. Two different ac-
tions are performed depending on whether we are
122
visiting an instantiated symbol or an instantiated
production.
3.2.1 Visiting an instantiated symbol
When visiting an instantiated symbol Ai..j with
parameters x and y, a new decorated instan-
tiated symbol Ax,yi,j is created and the traver-
sal continues on the instantiated productions of
P (Ai..j) with parameters that have to be com-
puted. These parameters depend on how the el-
ements of Ex,y(Ai..j) are ?distributed? among the
sets E(p) with p ? P (Ai..j). In other words, we
need to determine xk?s and yk?s such that:
Ex,y(Ai..j) =
?
pk?P (Ai..j)
Exk,yk(pk)
The idea can be easily illustrated on an exam-
ple. Suppose we are visiting the instantiated sym-
bol Ai..j with parameters 5 and 10. Suppose also
that Ai..j can be rewritten using the two instanti-
ated productions p1 and p2. Suppose finally that
the 5 to 10 entries of T (Ai..j) are as follows6:
5 p1 4
6 p2 2
7 p2 3
8 p1 5
9 p2 4
10 p1 6
This table says that E5(Ai..j) = E4(p1) i.e. the
5th most likely analysis of E(Ai..j) is the 4th most
likely analysis of E(p1) and E6(Ai..j) = E2(p2)
and so on. From this table we can deduce that:
E5,10(Ai..j) = E4,6(p1) ? E2,4(p2)
The traversal therefore continues on p1 and p2
with parameters 4, 6 and 2, 4.
3.2.2 Visiting an instantiated production
When visiting an instantiated production p of the
form Ai..j ? Bi..l Cl..j with parameters x and y,
a collection of q instantiated productions pr of the
form Ax,yi..j ? B
x1r,x2r
i..l C
y1r ,y2r
l..j , with 1 ? r ? q,
are built, where the parameters x1r, x2r , y1r , y2r and
q have to be computed.
Once the parameters q and x1r, x2r , y1r , y2r with
1 ? r ? q, have been computed, the traversal
continues independently on Bi..l with parameters
x1r and x2r and on Cl..j with parameters y1r and y2r .
6Only the relevant part of the table have been kept in the
figure.
The computation of the parameters x1r, x2r , y1r
and y2r for 1 ? r ? q, is the most complex part of
the algorithm, it relies on the three notions of rect-
angles, q-partitions and n-best matrices, which are
defined below.
Given a 4-tuple of parameters x1r , x2r, y1r , y2r ,
a rectangle is simply a pairing of the form
??x1r , x2r?, ?y1r , y2r ??. A rectangle can be interpreted
as a couple of rank ranges : ?x1r , y1r ? and ?x2r , y2r?.
It denotes the cartesian product
[
x1r, x2r
]?[y1r , y2r
]
.
Let ??x11, x21?, ?y11 , y21??, . . . , ??x1q , x2q?, ?y1q , y2q ??
be a collection of q rectangles. It will be called a
q-partition of the instantiated production p iff the
following is true:
Ex,y(p) =
?
1?r?q
E(Ax,yi..j ? Bx
1
r,x2r
i..l C
y1r ,y2r
l..j )
To put it differently, this definition means that
??x11, x21?, ?y11 , y21??, . . . , ??x1q , x2q?, ?y1q , y2q?? is a q
partition of p if any tree of E(Bx1r,x2ri..l ) combined
with any tree of E(Cy1r ,y2rl..j ) is a tree of Ex,y(p) and,
conversely, any tree of Ex,y(p) is the combination
of a tree of E(Bx1r,x2ri..l ) and a tree of E(Cy
1
r ,y2r
l..j ).
The n-best matrix associated with an instanti-
ated production p, introduced in (Huang and Chi-
ang, 2005), is merely a two dimensional represen-
tation of the n-best table of p. Such a matrix, rep-
resents how the n most likely trees of E(p) are
built. An example of an n-best matrix is repre-
sented in figure 1. This matrix says that the first
most likely tree of p is built by combining the
tree E1(Bi..l) with the tree E1(Cl..j) (there is a 1
in the cell of coordinate ?1, 1?). The second most
likely tree is built by combining the tree E1(Bi..l)
and E2(Cl..j) (there is a 2 in the cell of coordinate
?1, 2?) and so on.
1 2
3
4
6
7
8
9
10
11
12
13
14 15
16
17
18
20 21
23
5
24
26
2 3 5 6
2
3
4
5
6
19
41
1
22 2725
28
29
30
31 32 34
33
35
36
Cl..j
Bi..l
Figure 1: n-best matrix
An n-best matrix M has, by construction, the
remarkable following properties:
123
M(i, y) < M(x, y) ?i 1 ? i < x
M(x, j) < M(x, y) ?j 1 ? j < y
Given an n-best matrix M of dimensions d =
X ? Y and two integers x and y such that 1 ? x <
y ? d, M can be decomposed into three regions:
? the lower region, composed of the cells
which contain ranks i with 1 ? i < x
? the intermediate region, composed of the
cells which contain ranks i with x ? i ? y
? the upper region, composed of the cells
which contain ranks i such that y < i ? d.
The three regions of the matrix of figure 1, for
x = 4 and y = 27 have been delimited with bold
lines in figure 2.
1 2
3
4
6
7
8
9
10
11
12
13
14 15
16
17
18
20 21
23
5
24
26
2 3 5 6
2
3
4
5
6
19
41
1
22 2725
28
29
30
31 32 34
33
35
36
Bi..l
Cl..j
Figure 2: Decomposition of an n-best matrix into
a lower, an intermediate and an upper region with
parameters 4 and 27.
It can be seen that a rectangle, as introduced
earlier, defines a sub-matrix of the n-best matrix.
For example the rectangle ??2, 5?, ?2, 5?? defines
the sub-matrix which north west corner is M(2, 2)
and south east corner is M(5, 5), as represented in
figure 3.
When visiting an instantiated production p, hav-
ing M as an n-best matrix, with the two parame-
ters x and y, the intermediate region of M , with
respect to x and y, contains, by definition, all the
ranks that we are interested in (the ranks rang-
ing from x to y). This region can be partitioned
into a collection of disjoint rectangular regions.
Each such partition therefore defines a collection
of rectangles or a q-partition.
The computation of the parameters x1r, y1r , x2r
and y2r for an instantiated production p therefore
boils down to the computation of a partition of the
intermediate region of the n-best matrix of p.
9
10
11
12
13
17
18
20 21
5
24
26
2 5
2
5 19 22 2725
Cl..j
Bi..l
Figure 3: The sub-matrix corresponding to the
rectangle ??2, 5?, ?2, 5??
We have represented schematically, in figure 4,
two 4-partitions and a 3-partition of the interme-
diate region of the matrix of figure 2. The left-
most (resp. rightmost) partition will be called the
vertical (resp. horizontal) partition. The middle
partition will be called an optimal partition, it de-
composes the intermediate region into a minimal
number of sub-matrices.
   
   
   



III
IV
I
II
   
   
   



I
III
II
   
   
   



II
I 
III
IV
Figure 4: Three partitions of an n-best matrix
The three partitions of figure 4 will give birth to
the following instantiated productions:
? Vertical partition
A4,27i..j ? B3,6i..l C
1,1
l..j A
4,27
i..j ? B2,5i..l C
2,2
l..j
A4,27i..j ? B1,5i..l C
3,5
l..j A
4,27
i..j ? B1,1i..l C
6,6
l..j
? Optimal partition
A4,27i..j ? B1,1i..l C
3,6
l..j A
4,27
i..j ? B2,5i..l C
2,5
l..j
A4,27i..j ? B3,6i..l C
1,1
l..j
? Horizontal partition
A4,27i..j ? B1,1i..l C
3,6
l..j A
4,27
i..j ? B2,2i..l C
2,5
l..j
A4,27i..j ? B3,5i..l C
1,5
l..j A
4,27
i..j ? B6,6i..l C
1,1
l..j
Vertical and horizontal partition of the interme-
diate region of a n-best matrix can easily be com-
puted. We are not aware of an efficient method that
computes an optimal partition. In the implemen-
tation used for experiments described in section 4,
124
a simple heuristic has been used which computes
horizontal and vertical partitions and keeps the
partition with the lower number of parts.
The size of the new forest is clearly linked to
the partitions that are computed: a partition with
a lower number of parts will give birth to a lower
number of decorated instantiated productions and
therefore a smaller forest. But this optimization
is local, it does not take into account the fact that
an instantiated symbol may be shared in the initial
forest. During the computation of the new forest,
an instantiated production p can therefore be vis-
ited several times, with different parameters. Sev-
eral partitions of p will therefore be computed. If
a rectangle is shared by several partitions, this will
tend to decrease the size of the new forest. The
global optimal must therefore take into account all
the partitions of an instantiated production that are
computed during the construction of the new for-
est.
Example 5: Applying the rectangles method to
the second running example.
We now illustrate more concretely the rectan-
gles method on our second running example intro-
duced in Example 3. Let us recall that we are in-
terested in the n = 3 best trees, the original forest
containing 4 trees.
As said above, this method starts on the instan-
tiated axiom S1..3. Since it is the left-hand side
of only one production, this production is visited
with parameters 1, 3. Moreover, its n-best table is
the same as that of S1..3, given in Example 3. We
show here the corresponding n-best matrix, with
the empty lower region, the intermediate region
(cells corresponding to ranks 1 to 3) and the upper
region:
4
1 2
3
2
2
1
1A1..2
B2..3
As can be seen on that matrix, there are two op-
timal 2-partitions, namely the horizontal and the
vertical partitions, illustrated as follows:
II
I
II I
Let us arbitrarily chose the vertical partition. It
gives birth to two S 1..3-productions, namely:
S 1,31..3 ? A1,21..2 B1,12..3
S 1,31..3 ? A1,11..2 B2,22..3
Since this is the only non-trivial step while apply-
ing the rectangles algorithm to this example, we
can now give its final result, in which the axiom?s
(unnecessary) decorations have been removed:
S1..3 ? A1,21..2 B
{1,1}
2..3
S1..3 ? A1,11..2 B
{2,2}
2..3
A1,21..2 ? A11..2 A11..2 ? a1..2
A1,21..2 ? A21..2 A21..2 ? a1..2
B1,22..3 ? B12..3 B12..3 ? b2..3
B2,22..3 ? B22..3 B22..3 ? b2..3
Compared to the forest built by the ranksets algo-
rithm, this forest has one less production and one
less non-terminal symbol. It has only one more
production than the over-generating pruned for-
est.
4 Experiments on the Penn Treebank
The methods described in section 3 have been
tested on a PCFG G extracted from the Penn Tree-
bank (Marcus et al, 1993). G has been extracted
naively: the trees have been decomposed into bi-
nary context free rules, and the probability of ev-
ery rule has been estimated by its relative fre-
quency (number of occurrences of the rule divided
by the number of occurrences of its left hand side).
Rules occurring less than 3 times and rules with
probabilities lower than 3? 10?4 have been elim-
inated. The grammar produced contains 932 non
terminals and 3, 439 rules.7
The parsing has been realized using the SYN-
TAX system which implements, and optimizes, the
Earley algorithm (Boullier, 2003).
The evaluation has been conducted on the 1, 845
sentences of section 1, which constitute our test
set. For every sentence and for increasing values
of n, an n-best sub-forest has been built using the
rankset and the rectangles method.
The performances of the algorithms have been
measured by the average compression rate they
7We used this test set only to generate practical NLP
forests, with a real NLP grammar, and evaluate the perfor-
mances of our algorithms for constucting sub-forests that
contain only the n-best trees, both in terms of compression
rate and execution time. Therefore, the evaluation carried out
here has nothing to do with the usual evaluation of the pre-
cision and recall of parsers based on the Penn Treebank. In
particular, we are not interested here in the accuracy of such
a grammar, its only purpose is to generate parse forests from
which n-best sub-forests will be built.
125
0e+00
1e+05
2e+05
3e+05
4e+05
5e+05
6e+05
7e+05
8e+05
9e+05
 0  100  200  300  400  500  600  700  800  900 1000
av
g.
 n
b 
of
 t
re
es
 i
n 
th
e 
pr
un
ed
 f
or
es
t
n
Figure 5: Overgeneration of the pruned n-best forest
 1
 10
 100
 1000
 1  10  100  1000
co
mp
re
ss
io
n 
ra
te
n
pruned forest
rectangles
ranksets
Figure 6: Average compression rates
achieve for different values of n. The compres-
sion rate is obtained by dividing the size of the
n-best sub-forest of a sentence, as defined in sec-
tion 2, by the size of the (unfolded) n-best forest.
The latter is the sum of the sizes of all trees in the
forest, where every tree is seen as an instantiated
grammar, its size is therefore the size of the corre-
sponding instantiated grammar.
The size of the n-best forest constitutes a natu-
ral upper bound for the representation of the n-best
trees. Unfortunately, we have no natural lower
bound for the size of such an object. Neverthe-
less, we have computed the compression rates of
the pruned n-best forest and used it as an imperfect
lower bound. As already mentioned, its imper-
fection comes from the fact that a pruned n-best
forest contains more trees than the n best ones.
This overgeneration appears clearly in Figure 5
which shows, for increasing values of n, the av-
erage number of trees in the n-best pruned forest
for all sentences in our test set.
Figure 6 shows the average compression rates
achieved by the three methods (forest pruning,
rectangles and ranksets) on the test set for increas-
ing values of n. As predicted, the performances lie
between 1 (no compression) and the compression
of the n-best pruned forest. The rectangle method
outperforms the ranksets algorithm for every value
of n.
The time needed to build an 100-best forest with
the rectangle and the ranksets algorithms is shown
in Figure 7. This figure shows the average parsing
126
 0
 200
 400
 600
 800
 1000
 1200
 5  10  15  20  25  30  35  40  45
ti
me
 i
n 
mi
ll
is
ec
on
ds
sentence length
parsing
ranksets
rectangles
Figure 7: Processing time
time for sentences of a given length, as well as the
average time necessary for building the 100-best
forest using the two aforementioned algorithms.
This time includes the parsing time i.e. it is the
time necessary for parsing a sentence and build-
ing the 100-best forest. As shown by the figure,
the time complexities of the two methods are very
close.
5 Conclusion and perspectives
This work presented two methods to build n-
best sub-forests. The so called rectangle meth-
ods showed to be the most promising, for it al-
lows to build efficient sub-forests with little time
overhead. Future work will focus on computing
optimized partitions of the n-best matrices, a cru-
cial part of the rectangle method, and adapting the
method to arbitrary (non binary) CFG. Another
line of research will concentrate on performing
re-ranking of the n-best trees directly on the sub-
forest.
Acknowledgments
This research is supported by the French National
Research Agency (ANR) in the context of the
SEQUOIA project (ANR-08-EMER-013).
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Taylor L. Booth. 1969. Probabilistic representation of
formal languages. In Tenth Annual Symposium on
Switching and Automata Theory, pages 74?81.
Pierre Boullier and Philippe Deschamp. 1988.
Le syste`me SYNTAXTM - manuel d?utilisation.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Benot Sagot. 2005. Efficient and
robust LFG parsing: SXLFG. In Proceedings of
IWPT?05, Vancouver, Canada.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of IWPT?03, pages 43?54.
Jay Earley. 1970. An efficient context-free parsing
algorithm. Communication of the ACM, 13(2):94?
102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL?08, pages 586?594.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, United Kingdom. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT?01.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In J. Loeckx, ed-
itor, Proceedings of the Second Colloquium on Au-
tomata, Languages and Programming, volume 14 of
Lecture Notes in Computer Science, pages 255?269.
Springer-Verlag.
127
Bernard Lang. 1994. Recognition can be harder then
parsing. Computational Intelligence, 10:486?494.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
128
Proceedings of NAACL-HLT 2013, pages 239?247,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Enforcing Subcategorization Constraints in a Parser Using Sub-parses
Recombining
Seyed Abolghasem Mirroshandel?,? Alexis Nasr? Beno??t Sagot
?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universite? Aix-Marseille, Marseille, France
Alpage, INRIA & Universite? Paris-Diderot, Paris, France
?Computer Engineering Department, Faculty of Engineering,
University of Guilan, Rasht, Iran
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
benoit.sagot@inria.fr)
Abstract
Treebanks are not large enough to adequately
model subcategorization frames of predica-
tive lexemes, which is an important source of
lexico-syntactic constraints for parsing. As
a consequence, parsers trained on such tree-
banks usually make mistakes when selecting
the arguments of predicative lexemes. In this
paper, we propose an original way to correct
subcategorization errors by combining sub-
parses of a sentence S that appear in the list
of the n-best parses of S. The subcatego-
rization information comes from three differ-
ent resources, the first one is extracted from
a treebank, the second one is computed on a
large corpora and the third one is an existing
syntactic lexicon. Experiments on the French
Treebank showed a 15.24% reduction of er-
roneous subcategorization frames (SF) selec-
tions for verbs as well as a relative decrease of
the error rate of 4% Labeled Accuracy Score
on the state of the art parser on this treebank.
1 Introduction
Automatic syntactic parsing of natural languages
has witnessed many important changes in the last
fifteen years. Among these changes, two have mod-
ified the nature of the task itself. The first one is
the availability of treebanks such as the Penn Tree-
bank (Marcus et al, 1993) or the French Treebank
(Abeille? et al, 2003), which have been used in the
parsing community to train stochastic parsers, such
as (Collins, 1997; Petrov and Klein, 2008). Such
work remained rooted in the classical language the-
oretic tradition of parsing, generally based on vari-
ants of generative context free grammars. The sec-
ond change occurred with the use of discriminative
machine learning techniques, first to rerank the out-
put of a stochastic parser (Collins, 2000; Charniak
and Johnson, 2005) and then in the parser itself (Rat-
naparkhi, 1999; Nivre et al, 2007; McDonald et al,
2005a). Such parsers clearly depart from classical
parsers in the sense that they do not rely anymore on
a generative grammar: given a sentence S, all pos-
sible parses for S1 are considered as possible parses
of S. A parse tree is seen as a set of lexico-syntactic
features which are associated to weights. The score
of a parse is computed as the sum of the weights of
its features.
This new generation of parsers allows to reach
high accuracy but possess their own limitations. We
will focus in this paper on one kind of weakness
of such parser which is their inability to properly
take into account subcategorization frames (SF) of
predicative lexemes2, an important source of lexico-
syntactic constraints. The proper treatment of SF is
actually confronted to two kinds of problems: (1)
the acquisition of correct SF for verbs and (2) the
integration of such constraints in the parser.
The first problem is a consequence of the use of
treebanks for training parsers. Such treebanks are
composed of a few thousands sentences and only a
small subpart of acceptable SF for a verb actually
1Another important aspect of the new parsing paradigm is
the use of dependency trees as a means to represent syntactic
structure. In dependency syntax, the number of possible syn-
tactic trees associated to a sentence is bounded, and only de-
pends on the length of the sentence, which is not the case with
syntagmatic derivation trees.
2We will concentrate in this paper on verbal SF.
239
occur in the treebank.
The second problem is a consequence of the pars-
ing models. For algorithmic complexity as well as
data sparseness reasons, the parser only considers
lexico-syntactic configurations of limited domain of
locality (in the parser used in the current work, this
domain of locality is limited to configurations made
of one or two dependencies). As described in more
details in section 2, SF often exceed in scope such
domains of locality and are therefore not easy to in-
tegrate in the parser. A popular method for intro-
ducing higher order constraints in a parser consist in
reranking the n best output of a parser as in (Collins,
2000; Charniak and Johnson, 2005). The reranker
search space is restricted by the output of the parser
and high order features can be used. One draw-
back of the reranking approach is that correct SF for
the predicates of a sentence can actually appear in
different parse trees. Selecting complete trees can
therefore lead to sub-optimal solutions. The method
proposed in this paper merges parts of different trees
that appear in an n best list in order to build a new
parse.
Taking into account SF in a parser has been a ma-
jor issue in the design of syntactic formalisms in the
eighties and nineties. Unification grammars, such
as Lexical Functional Grammars (Bresnan, 1982),
Generalized Phrase Structure Grammars (Gazdar et
al., 1985) and Head-driven Phrase Structure Gram-
mars (Pollard and Sag, 1994), made SF part of the
grammar. Tree Adjoining Grammars (Joshi et al,
1975) proposed to extend the domain of locality of
Context Free Grammars partly in order to be able
to represent SF in a generative grammar. More
recently, (Collins, 1997) proposed a way to intro-
duce SF in a probabilistic context free grammar and
(Arun and Keller, 2005) used the same technique
for French. (Carroll et al, 1998), used subcate-
gorization probabilities for ranking trees generated
by unification-based phrasal grammar and (Zeman,
2002) showed that using frame frequency in a de-
pendency parser can lead to a significant improve-
ment of the performance of the parser.
The main novelties of the work presented here is
(1) the way a new parse is built by combining sub-
parses that appear in the n best parse list and (2)
the use of three very different resources that list the
possible SF for verbs.
The organization of the paper is the following: in
section 2, we will briefly describe the parsing model
that we will be using for this work and give accuracy
results on a French corpus. Section 3 will describe
three different resources that we have been using to
correct SF errors made by the parser and give cov-
erage results for these resources on a development
corpus. Section 4 will propose three different ways
to take into account, in the parser, the resources de-
scribed in section 3 and give accuracy results. Sec-
tion 5 concludes the paper.
2 The Parser
The parser used in this work is the second order
graph based parser (McDonald et al, 2005b) imple-
mentation of (Bohnet, 2010). The parser was trained
on the French Treebank (Abeille? et al, 2003) which
was transformed into dependency trees by (Candito
et al, 2009). The size of the treebank and its de-
composition into train, development and test sets are
represented in table 1.
nb of sentences nb of tokens
TRAIN 9 881 278 083
DEV 1 239 36 508
TEST 1 235 36 340
Table 1: Size and decomposition of the French Treebank
The parser gave state of the art results for parsing
of French, reported in table 2. Table 2 reports the
standard Labeled Accuracy Score (LAS) and Unla-
beled Accuracy Score (UAS) which is the ratio of
correct labeled (for LAS) or unlabeled (for UAS) de-
pendencies in a sentence. We also defined a more
specific measure: the SF Accuracy Score (SAS)
which is the ratio of verb occurrences that have been
paired with the correct SF by the parser. We have
introduced this quantity in order to measure more
accurately the impact of the methods described in
this paper on the selection of a SF for the verbs of a
sentence.
TEST DEV
SAS 80.84 79.88
LAS 88.88 88.53
UAS 90.71 90.37
Table 2: Subcategorization Frame Accuracy, Labeled and
Unlabeled Accuracy Score on TEST and DEV.
240
We have chosen a second order graph parser in
this work for two reasons. The first is that it is the
parsing model that obtained the best results on the
French Treebank. The second is that it allows us
to impose structural constraints in the solution of
the parser, as described in (Mirroshandel and Nasr,
2011), a feature that will reveal itself precious when
enforcing SF in the parser output.
3 The Resources
Three resources have been used in this work in order
to correct SF errors. The first one has been extracted
from a treebank, the second has been extracted from
an automatically parsed corpus that is several order
of magnitude bigger than the treebank. The third one
has been extracted from an existing lexico-syntactic
resource. The three resources are respectively de-
scribed in sections 3.2, 3.3 and 3.4. Before describ-
ing the resources, we describe in details, in section
3.1 our definition of SF. In section 3.5, we evalu-
ate the coverage of these resources on the DEV cor-
pus. Coverage is an important characteristic of a re-
source: in case of an SF error made by the parser, if
the correct SF that should be associated to a verb, in
a sentence, does not appear in the resource, it will be
impossible to correct the error.
3.1 Subcat Frames Description
In this work, a SF is defined as a couple (G,L)
where G is the part of speech tag of the element that
licenses the SF. This part of speech tag can either
be a verb in infinitive form (VINF), a past participle
(VPP), a finite tense verb (V) or a present participle
(VPR). L is a set of couples (f, c) where f is a syn-
tactic function tag chosen among a set F and c is
a part of speech tag chosen among the set C. Cou-
ple (f, c) indicates that function f can be realized as
part of speech tag c. Sets F and C are respectively
displayed in top and bottom tables of figure 1. An
anchored SF (ASF) is a couple (v, S) where v is a
verb lemma and S is a SF, as described above.
A resource is defined as a collection of ASF
(v, S), each associated to a count c, to represent the
fact that verb v has been seen with SF S c times. In
the case of the resource extracted form an existing
lexicon (section 3.4), the notion of count is not ap-
plicable and we will consider that it is always equal
SUJ subject
OBJ object
A OBJ indirect object introduced by the preposition a`
DE OBJ indirect object introduced by the preposition de
P OBJ indirect object introduced by another preposition
ATS attribute of the subject
ATO attribute of the direct object
ADJ adjective
CS subordinating conjunction
N noun
V verb finite tense
VINF verb infinitive form
VPP verb past participle
VPR verb present participle
Figure 1: Syntactic functions of the arguments of the SF
(top table). Part of speech tags of the arguments of the SF
(bottom table)
to one.
Below is an example of three ASF for the french
verb donner (to give). The first one is a transitive SF
where both the subject and the object are realized as
nouns as in Jean donne un livre (Jean gives a book.).
The second one is ditransitive, it has both a direct
object and an indirect one introduced by the prepo-
sition a` as in Jean donne un livre a` Marie. (Jean
gives a book to Marie). The third one corresponds
to a passive form as in le livre est donne? a` Marie par
Jean (The book is given to Marie by Jean).
(donner,(V,(suj,N),(obj,N)))
(donner,(V,(suj,N),(obj,N),(a_obj,N)))
(donner,(VPP,(suj,N),(aux_pass,V),
(a_obj,N),(p_obj,N)))
One can note that when an argument corresponds
to an indirect dependent of the verb (introduced ei-
ther by a preposition or a subordinating conjunc-
tion), we do not represent in the SF, the category
of the element that introduces the argument, but the
category of the argument itself, a noun or a verb.
Two important choices have to be made when
defining SF. The first one concerns the dependents
of the predicative element that are in the SF (argu-
ment/adjunct distinction) and the second is the level
of abstraction at which SF are defined.
In our case, the first choice is constrained by the
treebank annotation guidelines. The FTB distin-
guishes seven syntactic functions which can be con-
sidered as arguments of a verb. They are listed in
the top table of figure 1. Most of them are straight-
241
forward and do not deserve an explanation. Some-
thing has to be said though on the syntactic function
P OBJ which is used to model arguments of the verb
introduced by a preposition that is neither a` nor de,
such as the agent in passive form, which is intro-
duced by the preposition par.
We have added in the SF two elements that do not
correspond to arguments of the verb: the reflexive
pronoun, and the passive auxiliary. The reason for
adding these elements to the SF is that their pres-
ence influences the presence or absence of some ar-
guments of the verb, and therefore the SF.
The second important choice that must be made
when defining SF is the level of abstraction, or, in
other words, how much the SF abstracts away from
its realization in the sentence. In our case, we have
used two ways to abstract away from the surface re-
alization of the SF. The first one is factoring sev-
eral part of speech tags. We have factored pronouns,
common nouns and proper nouns into a single cat-
egory N. We have not gathered verbs in different
modes into one category since the mode of the verb
influences its syntactic behavior and hence its SF.
The second means of abstraction we have used is
the absence of linear order between the arguments.
Taking into account argument order increases the
number of SF and, hence, data sparseness, without
adding much information for selecting the correct
SF, this is why we have decided to to ignore it. In
our second example above, each of the three argu-
ments can be realized as one out of eight parts of
speech that correspond to the part of speech tag N
and the 24 possible orderings are represented as one
canonical ordering. This SF therefore corresponds
to 12 288 possible realizations.
3.2 Treebank Extracted Subcat Frames
This resource has been extracted from the TRAIN
corpus. At a first glance, it may seen strange to ex-
tract data from the corpus that have been used for
training our parser. The reason is that, as seen in
section 1, SF are not directly modeled by the parser,
which only takes into account subtrees made of, at
most, two dependencies.
The extraction procedure of SF from the treebank
is straightforward : the tree of every sentence is vis-
ited and, for every verb of the sentence, its daughters
are visited, and, depending whether they are consid-
ered as arguments of the verb (with respect to the
conventions or section 3.1), they are added to the SF.
The number of different verbs extracted, as well as
the number of different SF and the average number
of SF per verb are displayed in table 3. Column T
(for Train) is the one that we are interested in here.
T L A0 A5 A10
nb of verbs 2058 7824 23915 4871 3923
nb of diff SF 666 1469 12122 2064 1355
avg. nb of SF 4.83 52.09 14.26 16.16 13.45
Table 3: Resources statistics
The extracted resource can directly be compared
with the TREELEX resource (Kupsc and Abeille?,
2008), which has been extracted from the same tree-
bank. The result that we obtain is different, due to
the fact that (Kupsc and Abeille?, 2008) have a more
abstract definition of SF. As a consequence, they de-
fine a smaller number of SF: 58 instead of 666 in
our case. The smaller number of SF yields a smaller
average number of SF per verb: 1.72 instead of 4.83
in our case.
3.3 Automatically computed Subcat Frames
The extraction procedure described above has been
used to extract ASF from an automatically parsed
corpus. The corpus is actually a collection of three
corpora of slightly different genres. The first one
is a collection of news reports of the French press
agency Agence France Presse, the second is a col-
lection of newspaper articles from a local French
newspaper : l?Est Re?publicain. The third one is
a collection of articles from the French Wikipedia.
The size of the different corpora are detailed in ta-
ble 4.
The corpus was first POS tagged with the MELT
tagger (Denis and Sagot, 2010), lemmatized with the
MACAON tool suite (Nasr et al, 2011) and parsed
in order to get the best parse for every sentence.
Then the ASF have been extracted.
The number of verbs, number of SF and average
number of SF per verb are represented in table 3,
in column A0 (A stands for Automatic). As one
can see, the number of verbs and SF are unrealis-
tic. This is due to the fact that the data that we ex-
tract SF from is noisy: it consists of automatically
produced syntactic trees which contain errors (recall
242
CORPUS Sent. nb. Tokens nb.
AFP 2 041 146 59 914 238
EST REP 2 998 261 53 913 288
WIKI 1 592 035 33 821 460
TOTAL 5 198 642 147 648 986
Table 4: sizes of the corpora used to collect SF
that the LAS on the DEV corpus is 88, 02%). There
are two main sources of errors in the parsed data: the
pre-processing chain (tokenization, part of speech
tagging and lemmatization) which can consider as
a verb a word that is not, and, of course, parsing
errors, which tend to create crazy SF. In order to
fight against noise, we have used a simple thresh-
olding: we only collect ASF that occur more than a
threshold i. The result of the thresholding appears
in columns A5 and A10 , where the subscript is the
value of the threshold. As expected both the number
of verbs and SF decrease sharply when increasing
the value of the threshold.
Extracting SF for verbs from raw data has been
an active direction of research for a long time, dat-
ing back at least to the work of (Brent, 1991) and
(Manning, 1993). More recently (Messiant et al,
2008) proposed such a system for French verbs. The
method we use for extracting SF is not novel with
respect to such work. Our aim was not to devise
new extraction techniques but merely to evaluate the
resource produced by such techniques for statistical
parsing.
3.4 Using an existing resource
The third resource that we have used is the Lefff
(Lexique des formes fle?chies du franc?ais ? Lexicon
of French inflected form), a large-coverage syntac-
tic lexicon for French (Sagot, 2010). The Lefff was
developed in a semi-automatic way: automatic tools
were used together with manual work. The latest
version of the Lefff contains 10,618 verbal entries
for 7,835 distinct verbal lemmas (the Lefff covers all
categories, but only verbal entries are used in this
work).
A sub-categorization frame consists in a list of
syntactic functions, using an inventory slightly more
fine-grained than in the French Treebank, and for
each of them a list of possible realizations (e.g.,
noun phrase, infinitive clause, or null-realization if
the syntactic function is optional).
For each verbal lemma, we extracted all sub-
categorization frames for each of the four verbal
part-of-speech tags (V, VINF, VPR, VPP), thus cre-
ating an inventory of SFs in the same sense and for-
mat as described in Section 3.1. Note that such SFs
do not contain alternatives concerning the way each
syntactic argument is realized or not: this extraction
process includes a de-factorization step. Its output,
hereafter L, contains 801,246 distinct (lemma, SF)
pairs.
3.5 Coverage
In order to be able to correct SF errors, the three
resources described above must possess two impor-
tant characteristics: high coverage and high accu-
racy. Coverage measures the presence, in the re-
source, of the correct SF of a verb, in a given sen-
tence. Accuracy measures the ability of a resource
to select the correct SF for a verb in a given context
when several ones are possible.
We will give in this section coverage result, com-
puted on the DEV corpus. Accuracy will be de-
scribed and computed in section 4. The reason why
the two measures are not described together is due
to the fact that coverage can be computed on a ref-
erence corpus while accuracy must be computed on
the output of a parser, since it is the parser that will
propose different SF for a verb in a given context.
Given a reference corpus C and a resource R,
two coverage measures have been computed, lexi-
cal coverage, which measures the ratio of verbs of C
that appear in R and syntactic coverage, which mea-
sures the ratio of ASF of C that appear in R. Two
variants of each measures are computed: on types
and on occurrences. The values of these measures
computed on the DEV corpus are summarized in ta-
ble 5.
T L A0 A5 A10
Lex. types 89.56 99.52 99.52 98.56 98.08
cov. occ 96.98 99.85 99.85 99.62 99.50
Synt. types 62.24 78.15 95.78 91.08 88.84
cov. occ 73.54 80.35 97.13 93.96 92.39
Table 5: Lexical and syntactic coverage of the three re-
sources on DEV
The figures of table 5 show that lexical cover-
age of the three resources is quite high, ranging
243
from 89.56 to 99.52 when computed on types and
from 96.98 to 99.85 when computed on occurrences.
The lowest coverage is obtained by the T resource,
which does not come as a surprise since it is com-
puted on a rather small number of sentences. It
is also interesting to note that lexical coverage of
A does not decrease much when augmenting the
threshold, while the size of the resource decreases
dramatically (as shown in table 3). This validates
the hypothesis that the resource is very noisy and
that a simple threshold on the occurrences of ASF is
a reasonable means to fight against noise.
Syntactic coverage is, as expected, lower than lex-
ical coverage. The best results are obtained by A0:
95.78 on types and 97.13 on occurrences. Thresh-
olding on the occurrences of anchored SF has a big-
ger impact on syntactic coverage than it had on lexi-
cal coverage. A threshold of 10 yields a coverage of
88.84 on types and 92.39 on occurrences.
4 Integrating Subcat Frames in the Parser
As already mentioned in section 1, SF usually ex-
ceed the domain of locality of the structures that are
directly modeled by the parser. It is therefore dif-
ficult to integrate directly SF in the model of the
parser. In order to circumvent the problem, we have
decided to work on the n-best output of the parser:
we consider that a verb v, in a given sentence S,
can be associated to any of the SF that v licenses in
one of the n-best trees. The main weakness of this
method is that an SF error can be corrected only if
the right SF appears at least in one of the n-best parse
trees.
In order to estimate an upper bound of the SAS
that such methods can reach (how many SF errors
can actually be corrected), we have computed the
oracle SAS on the 100 best trees of the DEV corpus
DEV (for how many verbs the correct SF appears
in at least one of the n-best parse trees). The oracle
score is equal to 95.16, which means that for 95.16%
of the verb occurrences of the DEV, the correct SF
appears somewhere in the 100-best trees. 95.16 is
therefore the best SAS that we can reach. Recall
that the baseline SAS is equal to 79.88% the room
for progress is therefore equal to 15.28% absolute.
Three experiments are described below. In the
first one, section 4.1, a simple technique, called Post
Processing is used. Section 4.2 describes a second
technique, called Double Parsing, which is a is a
refinement of Post Processing. Both sections 4.1
and 4.2 are based on single resources. Section 4.3
proposes a simple way to combine the different re-
sources.
4.1 Post Processing
The post processing method (PP) is the simplest one
that we have tested. It takes as input the different
ASF that occur in the n-best output of the parser as
well as a resource R. Given a sentence, let?s note
T1 . . . Tn the trees that appear in the n-best output
of the parser, in decreasing order of their score. For
every verb v of the sentence, we note S(v) the set
of all the SF associated to v that appear in the trees
T1 . . . Tn.
Given a verb v and a SF s, we define the following
functions:
C(v, s) is the number of occurrences of the ASF
(v, s) in the trees T1 . . . Tn.
F(v) is the SF associated to v in T1
CR(v, s) the number of occurrences of the ASF
(v, s) in the resource R.
We define a selection function as a function that
selects a SF for a given verb in a given sentence.
A selection function has to take into account the in-
formation given by the resource (whether an SF is
acceptable/frequent for a given verb) as well as the
information given by the parser (whether the parser
has a strong preference to associate a given SF to a
given verb).
In our experiments, we have tested two simple
selection functions. ?R which selects the first SF
s ? S(v), such that CR(v, s) > 0 when traversing
the trees T1 . . . Tn in the decreasing order of score
(best tree first).
The second function, ?R(v) compares the most
frequent SF for v in the resourceRwith the SF of the
first parse. If the ratio of the number of occurrences
in the n-best of the former and the latter is above a
threshold ?, the former is selected. More formally:
?R(v) =
?
????
????
s? = argmaxs?S(v) CR(v, s)
if C(v,s?)C(v,F(v)) > ?
F(v)
otherwise
244
The coefficient? has been optimized on DEV cor-
pus. Its value is equal to 2.5 for the Automatic re-
source, 2 for the Train resource and 1.5 for the Lefff.
The construction of the new solution proceeds as
follows: for every verb v of the sentence, a SF is se-
lected with the selection function. It is important to
note, at this point, that the SF selected for different
verbs of the sentence can pertain to different parse
trees. The new solution is built based on tree T1. For
every verb v, its arguments are potentially modified
in agreement with the SF selected by the selection
function. There is no guarantee at this point that the
solution is well formed. We will return to this prob-
lem in section 4.2.
We have evaluated the PP method with different
selection functions on the TEST corpus. The results
of applying function ?R were more successful. As
a result we just report the results of this function in
table 6. Different levels of thresholding for resource
A gave almost the same results, we therefore used
A10 which is the smallest one.
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.14 89.03 89.03
UAS 90.71 90.91 90.81 90.82
Table 6: LAS and UAS on TEST using PP
The results of table 6 show two interesting facts.
First, the SAS is improved, it jumps from 80.84 to
83.11. PP therefore corrects some SF errors made
by the parser. It must be noted however that this im-
provement is much lower than the oracle score. The
second interesting fact is the very moderate increase
of both LAS and UAS. This is due to the fact that
the number of dependencies modified is small with
respect to the total number of dependencies. The
impact on LAS and UAS is therefore weak.
The best results are obtained with resource T . Al-
though the coverage of T is low, the resource is very
close to the train data, this fact probably explains the
good results obtained with this resource.
It is interesting, at this point, to compare our
method with a reranking approach. In order to do so,
we have compared the upper bound of the number of
SF errors that can be corrected when using rerank-
ing and our approach. The results of the comparison
computed on a list of 100 best trees is reported in
table 7 which shows the ratio of subcat frame errors
that could be corrected with a reranking approach
and the ratio of errors sub-parse recombining could
reach.
DEV TEST
reranking 53.9% 58.5%
sub-parse recombining 75.5% 76%
Table 7: Correction rate for subcat frames errors with dif-
ferent methods
Table 7 shows that combining sub-parses can, in
theory, correct a much larger number of wrong SF
assignments than reranking.
4.2 Double Parsing
The post processing method shows some improve-
ment over the baseline. But it has an important draw-
back: it can create inconsistent parses. Recall that
the parser we are using is based on a second order
model. In other words, the score of a dependency
depends on some neighboring dependencies. When
building a new solution, the post processing method
modifies some dependencies independently of their
context, which may give birth to very unlikely con-
figurations.
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies. The new solution
is therefore the optimal solution that preserves the
dependencies modified by the PP method.
The double parsing (DP) method is therefore a
three stage method. First, sentence S is parsed, pro-
ducing the n-best parses. Then, the post processing
method is used, modifying the first best parse. Let?s
note D the set of dependencies that were changed in
this process. In the last stage, a new parse is pro-
duced, that preserves D.
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.30 89.25 89.31
UAS 90.71 91.07 91.05 91.08
Table 8: LAS and UAS on TEST using DP
245
The results of DP on TEST are reported in table
8. SAS did not change with respect to PP, because
DP keeps the SF selected by PP. As expected DP
does increase LAS and UAS. Recomputing an op-
timal solution therefore increases the quality of the
parses. Table 8 also shows that the three resources
get alost the same LAS and UAS although SAS is
better for resource T.
4.3 Combining Resources
Due to the different generation techniques of our
three resources, another direction of research is
combining them. We did different experiments con-
cerning all possible combination of resources: A and
L (AL), T and L (TL), T and A (TA), and all tree
(TAL) resources. The results of these combinations
for PP and DP methods are shown in tables 9 and
10, respectively.
The resource are combined in a back-off schema:
we search for a candidate ASF in a first resource. If
it is found, the search stops. Otherwise, the next re-
source(s) are probed. One question that arises is:
which sequence is the optimal one for combining
the resources. To answer this question, we did sev-
eral experiments on DEV set. Our experiments have
shown that it is better to search T resource, then
A, and, eventually, L. The results of this combining
method, using PP are reported in table 9. The best
results are obtained for the TL combination. The
SAS jumps from 83.11 to 83.76. As it was the case
with single resources, the LAS and UAS increase is
moderate.
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.03 89.22 89.19 89.19
UAS 90.71 90.79 90.98 90.95 90.95
Table 9: LAS and UAS on TEST using PP with resource
combination
With DP (table 9), the order of resource combina-
tion is exactly the same as with PP. As was the case
with single resources, DP has a positive, but moder-
ate, impact on LAS and UAS.
The results of tables 9 and 10 do not show con-
siderable improvement over single resources. This
might be due to the large intersection between our
resources. In other words, they do not have comple-
mentary information, and their combination will not
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.22 89.31 89.34 89.34
UAS 90.71 91.02 91.05 91.08 91.09
Table 10: LAS and UAS on TEST using DP with resource
combination
introduce much information. Another possible rea-
son for this result is the combination technique used.
More sophisticated techniques might yield better re-
sults.
5 Conclusions
Subcategorization frames for verbs constitute a rich
source of lexico-syntactic information which is hard
to integrate in graph based parsers. In this paper, we
have used three different resources for subcatego-
rization frames. These resources are from different
origins with various characteristics. We have pro-
posed two different methods to introduce the useful
information from these resources in a second order
model parser. We have conducted different exper-
iments on French Treebank that showed a 15.24%
reduction of erroneous SF selections for verbs. Al-
though encouraging, there is still plenty of room
for better results since the oracle score for 100 best
parses is equal to 95.16% SAS and we reached
83.76%. Future work will concentrate on more elab-
orate selection functions as well as more sophisti-
cated ways to combine the different resources.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
EDYLEX (ANR-08-CORD-009).
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of french. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 306?313.
Association for Computational Linguistics.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89?97.
246
Michael Brent. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceedings
of ACL.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press.
M. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants aux
de?pendances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
J. Carroll, G. Minnen, and T. Briscoe. 1998. Can sub-
categorisation probabilities help a statistical parser?
Arxiv preprint cmp-lg/9806013.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
P. Denis and B. Sagot. 2010. Exploitation d?une
ressource lexicale pour la construction d?un e?tiqueteur
morphosyntaxique e?tat-de-l?art du franc?ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan Sag. 1985. Generalized Phrase Structure Gram-
mar. Harvard University Press.
Aravind Joshi, Leon Levy, and M Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10:136?163.
Anna Kupsc and Anne Abeille?. 2008. Treelex: A subcat-
egorisation lexicon for french verbs. In Proceedings of
the First International Conference on Global Interop-
erability for Language Resources.
Christopher Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of ACL.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
C. Messiant, A. Korhonen, T. Poibeau, et al 2008.
Lexschem: A large subcategorization lexicon for
french verbs. In Proceedings of the Language Re-
sources and Evaluation Conference.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
A. Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Slav Petrov and Dan Klein. 2008. Discriminative Log-
Linear Grammars with Latent Variables. In J.C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 20 (NIPS),
pages 1153?1160, Cambridge, MA. MIT Press.
Carl Pollard and Ivan Sag. 1994. Head-driven Phrase
Structure Grammmar. CSLI Series. University of
Chicago Press.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1):151?175.
Beno??t Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for french. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), pages 2744?2751, Valletta, Malta.
D. Zeman. 2002. Can subcategorization help a statistical
dependency parser? In Proceedings of the 19th in-
ternational conference on Computational linguistics-
Volume 1, pages 1?7. Association for Computational
Linguistics.
247
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86?91,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
MACAON
An NLP Tool Suite for Processing Word Lattices
Alexis Nasr Fre?de?ric Be?chet Jean-Franc?ois Rey Beno??t Favre Joseph Le Roux?
Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 6166
Universite? Aix-Marseille
(alexis.nasr,frederic.bechet,jean-francois.rey,benoit.favre,joseph.le.roux)
@lif.univ-mrs.fr
Abstract
MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been de-
signed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several na-
tive modules for common tasks such as a tok-
enization, a part-of-speech tagging or syntac-
tic parsing, all communicating with each other
through XML files . In addition, exchange pro-
tocols with external tools are easily definable.
MACAON is a fast, modular and open tool, dis-
tributed under GNU Public License.
1 Introduction
The automatic processing of textual data generated
by NLP software, resulting from Machine Transla-
tion, Automatic Speech Recognition or Automatic
Text Summarization, raises new challenges for lan-
guage processing tools. Unlike native texts (texts
produced by humans), this new kind of texts is the
result of imperfect processors and they are made
of several hypotheses, usually weighted with con-
fidence measures. Automatic text production sys-
tems can produce these weighted hypotheses as n-
best lists, word lattices, or confusion networks. It is
crucial for this space of ambiguous solutions to be
kept for later processing since the ambiguities of the
lower levels can sometimes be resolved during high-
level processing stages. It is therefore important to
be able to represent this ambiguity.
?This work has been funded by the French Agence Nationale
pour la Recherche, through the projects SEQUOIA (ANR-08-
EMER-013) and DECODA (2009-CORD-005-01)
MACAON is a suite of tools developped to pro-
cess ambiguous input and extend inference of in-
put modules within a global scope. It con-
sists in several modules that perform classical
NLP tasks (tokenization, word recognition, part-of-
speech tagging, lemmatization, morphological anal-
ysis, partial or full parsing) on either native text
or word lattices. MACAON is distributed under
GNU public licence and can be downloaded from
http://www.macaon.lif.univ-mrs.fr/.
From a general point of view, a MACAON module
can be seen as an annotation device1 which adds a
new level of annotation to its input that generally de-
pends on annotations from preceding modules. The
modules communicate through XML files that allow
the representation different layers of annotation as
well as ambiguities at each layer. Moreover, the ini-
tial XML structuring of the processed files (logical
structuring of a document, information from the Au-
tomatic Speech Recognition module . . . ) remains
untouched by the processing stages.
As already mentioned, one of the main charac-
teristics of MACAON is the ability for each module
to accept ambiguous inputs and produce ambiguous
outputs, in such a way that ambiguities can be re-
solved at a later stage of processing. The compact
representation of ambiguous structures is at the heart
of the MACAON exchange format, described in sec-
tion 2. Furthermore every module can weight the
solutions it produces. such weights can be used to
rank solutions or limit their number for later stages
1Annotation must be taken here in a general sense which in-
cludes tagging, segmentation or the construction of more com-
plex objets as syntagmatic or dependencies trees.
86
of processing.
Several processing tools suites alread exist for
French among which SXPIPE (Sagot and Boullier,
2008), OUTILEX (Blanc et al, 2006), NOOJ2 or UNI-
TEX3. A general comparison of MACAON with these
tools is beyond the scope of this paper. Let us just
mention that MACAON shares with most of them the
use of finite state machines as core data represen-
tation. Some modules are implemented as standard
operations on finite state machines.
MACAON can also be compared to the numerous
development frameworks for developping process-
ing tools, such as GATE4, FREELING5, ELLOGON6
or LINGPIPE7 that are usually limited to the process-
ing of native texts.
The MACAON exchange format shares a cer-
tain number of features with linguistic annotation
scheme standards such as the Text Encoding Initia-
tive8, XCES9, or EAGLES10. They all aim at defining
standards for various types of corpus annotations.
The main difference between MACAON and these
approaches is that MACAON defines an exchange for-
mat between NLP modules and not an annotation
format. More precisely, this format is dedicated to
the compact representation of ambiguity: some in-
formation represented in the exchange format are
to be interpreted by MACAON modules and would
not be part of an annotation format. Moreover,
the MACAON exchange format was defined from the
bottom up, originating from the authors? need to use
several existing tools and adapt their input/output
formats in order for them to be compatible. This is in
contrast with a top down approach which is usually
chosen when specifying a standard. Still, MACAON
shares several characteristics with the LAF (Ide and
Romary, 2004) which aims at defining high level
standards for exchanging linguistic data.
2www.nooj4nlp.net/pages/nooj.html
3www-igm.univ-mlv.fr/?unitex
4gate.ac.uk
5garraf.epsevg.upc.es/freeling
6www.ellogon.org
7alias-i.com/lingpipe
8www.tei-c.org/P5
9www.xml-ces.org
10www.ilc.cnr.it/eagles/home.html
2 The MACAON exchange format
The MACAON exchange format is based on four con-
cepts: segment, attribute, annotation level and seg-
mentation.
A segment refers to a segment of the text or
speech signal that is to be processed, as a sentence,
a clause, a syntactic constituent, a lexical unit, a
named entity . . . A segment can be equipped with at-
tributes that describe some of its aspects. A syntac-
tic constituent, for example, will define the attribute
type which specifies its syntactic type (Noun Phrase,
Verb Phrase . . . ). A segment is made of one or more
smaller segments.
A sequence of segments covering a whole sen-
tence for written text, or a spoken utterance for oral
data, is called a segmentation. Such a sequence can
be weighted.
An annotation level groups together segments of
a same type, as well as segmentations defined on
these segments. Four levels are currently defined:
pre-lexical, lexical, morpho-syntactic and syntactic.
Two relations are defined on segments: the prece-
dence relation that organises linearly segments of a
given level into segmentations and the dominance
relation that describes how a segment is decomposed
in smaller segments either of the same level or of a
lower level.
We have represented in figure 2, a schematic rep-
resentation of the analysis of the reconstructed out-
put a speech recognizer would produce on the in-
put time flies like an arrow11. Three annotation lev-
els have been represented, lexical, morpho-syntactic
and syntactic. Each level is represented by a finite-
state automaton which models the precedence rela-
tion defined over the segments of this level. Seg-
ment time, for example, precedes segment flies. The
segments are implicitly represented by the labels of
the automaton?s arcs. This label should be seen as
a reference to a more complex objet, the actual seg-
ment. The dominance relations are represented with
dashed lines that link segments of different levels.
Segment time, for example, is dominated by seg-
ment NN of the morpho-syntactic level.
This example illustrates the different ambiguity
cases and the way they are represented.
11For readability reasons, we have used an English example,
MACAON, as mentioned above, currently exists for French.
87
thyme
time
flies like
liken
an arrow
a row
JJ IN
VB
DT NN
DT NN
VB
NN
NN
VBZ
VB VB
VP
VP
NP
NP
VP
NP
VP
VP
PP
NP
NP
Figure 1: Three annotation levels for a sample sentence.
Plain lines represent annotation hypotheses within a level
while dashed lines represent links between levels. Trian-
gles with the tip up are ?and? nodes and triangles with
the tip down are ?or? nodes. For instance, in the part-of-
speech layer, The first NN can either refer to ?time? or
?thyme?. In the chunking layer, segments that span mul-
tiple part-of-speech tags are linked to them through ?and?
nodes.
The most immediate ambiguity phenomenon is
the segmentation ambiguity: several segmentations
are possible at every level. This ambiguity is rep-
resented in a compact way through the factoring of
segments that participate in different segmentations,
by way of a finite state automaton.
The second ambiguity phenomenon is the dom-
inance ambiguity, where a segment can be decom-
posed in several ways into lower level segments.
Such a case appears in the preceding example, where
the NN segment appearing in one of the outgoing
transition of the initial state of the morpho-syntactic
level dominates both thyme and time segments of the
lexical level. The triangle with the tip down is an
?or? node, modeling the fact that NN corresponds to
time or thyme.
Triangles with the tip up are ?and? nodes. They
model the fact that the PP segment of the syntac-
tic level dominates segments IN, DT and NN of the
morpho-syntactic level.
2.1 XML representation
The MACAON exchange format is implemented in
XML. A segment is represented with the XML tag
<segment> which has four mandatory attributes:
? type indicates the type of the segment, four dif-
ferent types are currently defined: atome (pre-
lexical unit usually referred to as token in en-
glish), ulex (lexical unit), cat (part of speech)
and chunk (a non recursive syntactic unit).
? id associates to a segment a unique identifier in
the document, in order to be able to reference
it.
? start and end define the span of the segment.
These two attributes are numerical and repre-
sent either the index of the first and last char-
acter of the segment in the text string or the
beginning and ending time of the segment in
a speech signal.
A segment can define other attributes that can be
useful for a given description level. We often find
the stype attribute that defines subtypes of a given
type.
The dominance relation is represented through the
use of the <sequence> tag. The domination of the
three segments IN, DT and NN by a PP segment,
mentionned above is represented below, where p1,
p2 and p3 are respectively the ids of segments IN,
DT and NN.
<segment type="chunk" stype="PP" id="c1">
<sequence>
<elt segref="p1"/>
<elt segref="p2"/>
<elt segref="p3"/>
</sequence>
</segment>
The ambiguous case, described above where seg-
ment NN dominates segments time or thyme is rep-
resented below as a disjunction of sequences inside
a segment. The disjunction itself is not represented
as an XML tag. l1 and l2 are respectively the ids
of segments time and thyme.
<segment type="cat" stype="NN" id="c1">
<sequence>
<elt segref="l1" w="-3.37"/>
</sequence>
<sequence>
<elt segref="l2" w="-4.53"/>
</sequence>
</segment>
88
The dominance relation can be weighted, by way
of the attribute w. Such a weight represents in the
preceding example the conditional log-probability
of a lexical unit given a part of speech, as in a hidden
Markov model.
The precedence relation (i.e. the organization
of segments in segmentations), is represented as a
weighted finite state automaton. Automata are rep-
resented as a start state, accept states and a list of
transitions between states, as in the following exam-
ple that corresponds to the lexical level of our exam-
ple.
<fsm n="9">
<start n="0"/>
<accept n="6"/>
<ltrans>
<trans o="0" d="1" i="l1" w="-7.23"/>
<trans o="0" d="1" i="l2" w="-9.00"/>
<trans o="1" d="2" i="l3" w="-3.78"/>
<trans o="2" d="3" i="l4" w="-7.37"/>
<trans o="3" d="4" i="l5" w="-3.73"/>
<trans o="2" d="4" i="l6" w="-6.67"/>
<trans o="4" d="5" i="l7" w="-4.56"/>
<trans o="5" d="6" i="l8" w="-2.63"/>
<trans o="4" d="6" i="l9" w="-7.63"/>
</ltrans>
</fsm>
The <trans/> tag represents a transition, its
o,d,i and w features are respectively the origin, and
destination states, its label (the id of a segment) and
a weight.
An annotation level is represented by the
<section> tag which regroups two tags, the
<segments> tag that contains the different segment
tags defined at this annotation level and the <fsm>
tag that represents all the segmentations of this level.
3 The MACAON architecture
Three aspects have guided the architecture of
MACAON: openness, modularity, and speed. Open-
ness has been achieved by the definition of an ex-
change format which has been made as general as
possible, in such a way that mapping can be de-
fined from and to third party modules as ASR, MT
systems or parsers. Modularity has been achieved
by the definition of independent modules that com-
municate with each other through XML files using
standard UNIX pipes. A module can therefore be re-
placed easily. Speed has been obtained using effi-
cient algorithms and a representation especially de-
signed to load linguistic data and models in a fast
way.
MACAON is composed of libraries and compo-
nents. Libraries contain either linguistic data, mod-
els or API functions. Two kinds of components are
presented, the MACAON core components and third
party components for which mappings to and from
the MACAON exchange format have been defined.
3.1 Libraries
The main MACAON library is macaon common.
It defines a simple interface to the MACAON ex-
change format and functions to load XML MACAON
files into memory using efficient data structures.
Other libraries macaon lex, macaon code and
macaon tagger lib represent the lexicon, the
morphological data base and the tagger models in
memory.
MACAON only relies on two third-party libraries,
which are gfsm12, a finite state machine library and
libxml, an XML library13.
3.2 The MACAON core components
A brief description of several standard components
developed in the MACAON framework is given be-
low. They all comply with the exchange format de-
scribed above and add a <macaon stamp> to the
XML file that indicates the name of the component,
the date and the component version number, and rec-
ognizes a set of standard options.
maca select is a pre-processing component: it adds
a macaon tag under the target tags specified by
the user to the input XML file. The follow-
ing components will only process the document
parts enclosed in macaon tags.
maca segmenter segments a text into sentences by
examining the context of punctuation with a
regular grammar given as a finite state automa-
ton. It is disabled for automatic speech tran-
scriptions which do not typically include punc-
tuation signs and come with their own segmen-
tation.
12ling.uni-potsdam.de/?moocow/projects/
gfsm/
13xmlsoft.org
89
maca tokenizer tokenizes a sentence into pre-
lexical units. It is also based on regular gram-
mars that recognize simple tokens as well as a
predefined set of special tokens, such as time
expressions, numerical expressions, urls. . . .
maca lexer allows to regroup pre-lexical units into
lexical units. It is based on the lefff French lex-
icon (Sagot et al, 2006) which contains around
500,000 forms. It implements a dynamic pro-
gramming algorithm that builds all the possible
grouping of pre-lexical units into lexical units.
maca tagger associates to every lexical unit one or
more part-of-speech labels. It is based on a
trigram Hidden Markov Model trained on the
French Treebank (Abeille? et al, 2003). The es-
timation of the HMM parameters has been re-
alized by the SRILM toolkit (Stolcke, 2002).
maca anamorph produces the morphological anal-
ysis of lexical units associated to a part of
speech. The morphological information come
from the lefff lexicon.
maca chunker gathers sequences of part-of-speech
tags in non recursive syntactic units. This com-
ponent implements a cascade of finite state
transducers, as proposed by Abney (1996). It
adds some features to the initial Abney pro-
posal, like the possibility to define the head of
a chunk.
maca conv is a set of converters from and to the
MACAON exchange format. htk2macaon
and fsm2macaon convert word lattices from
the HTK format (Young, 1994) and ATT
FSM format (Mohri et al, 2000) to the
MACAON exchange format. macaon2txt and
txt2macaon convert from and to plain text
files. macaon2lorg and lorg2macaon
convert to and from the format of the LORG
parser (see section 3.3).
maca view is a graphical interface that allows to in-
spect MACAON XML files and run the compo-
nents.
3.3 Third party components
MACAON is an open architecture and provides a rich
exchange format which makes possible the repre-
sentation of many NLP tools input and output in the
MACAON format. MACAON has been interfaced with
the SPEERAL Automatic Speech Recognition Sys-
tem (Nocera et al, 2006). The word lattices pro-
duced by SPEERAL can be converted to pre-lexical
MACAON automata.
MACAON does not provide any native module for
parsing yet but it can be interfaced with any already
existing parser. For the purpose of this demonstra-
tion we have chosen the LORG parser developed at
NCLT, Dublin14. This parser is based on PCFGs
with latent annotations (Petrov et al, 2006), a for-
malism that showed state-of-the-art parsing accu-
racy for a wide range of languages. In addition it of-
fers a sophisticated handling of unknown words re-
lying on automatically learned morphological clues,
especially for French (Attia et al, 2010). Moreover,
this parser accepts input that can be tokenized, pos-
tagged or pre-bracketed. This possibility allows for
different settings when interfacing it with MACAON.
4 Applications
MACAON has been used in several projects, two of
which are briefly described here, the DEFINIENS
project and the LUNA project.
DEFINIENS (Barque et al, 2010) is a project that
aims at structuring the definitions of a large coverage
French lexicon, the Tre?sor de la langue franc?aise.
The lexicographic definitions have been processed
by MACAON in order to decompose the definitions
into complex semantico-syntactic units. The data
processed is therefore native text that possesses a
rich XML structure that has to be preserved during
processing.
LUNA15 is a European project that aims at extract-
ing information from oral data about hotel booking.
The word lattices produced by an ASR system have
been processed by MACAON up to a partial syntactic
level from which frames are built. More details can
be found in (Be?chet and Nasr, 2009). The key aspect
of the use of MACAON for the LUNA project is the
ability to perform the linguistic analyses on the mul-
tiple hypotheses produced by the ASR system. It is
therefore possible, for a given syntactic analysis, to
14www.computing.dcu.ie/?lorg. This software
should be freely available for academic research by the time
of the conference.
15www.ist-luna.eu
90
Figure 2: Screenshot of the MACAON visualization inter-
face (for French models). It allows to input a text and see
the n-best results of the annotation.
find all the word sequences that are compatible with
this analysis.
Figure 2 shows the interface that can be used to
see the output of the pipeline.
5 Conclusion
In this paper we have presented MACAON, an NLP
tool suite which allows to process native text as well
as several hypotheses automatically produced by an
ASR or an MT system. Several evolutions are cur-
rently under development, such as a named entity
recognizer component and an interface with a de-
pendency parser.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
Prague, Czech Republic, pages 8?15.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Lucie Barque, Alexis Nasr, and Alain Polgue`re. 2010.
From the definitions of the tre?sor de la langue franc?aise
to a semantic database of the french language. In EU-
RALEX 2010, Leeuwarden, Pays Bas.
Fre?de?ric Be?chet and Alexis Nasr. 2009. Robust depen-
dency parsing for spoken language understanding of
spontaneous speech. In Interspeech, Brighton, United
Kingdom.
Olivier Blanc, Matthieu Constant, and Eric Laporte.
2006. Outilex, plate-forme logicielle de traitement de
textes e?crits. In TALN 2006, Leuven.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural language engineering, 10(3-4):211?225.
M. Mohri, F. Pereira, and M. Riley. 2000. The design
principles of a weighted finite-state transducer library.
Theoretical Computer Science, 231(1):17?32.
P. Nocera, G. Linares, D. Massonie?, and L. Lefort. 2006.
Phoneme lattice based A* search algorithm for speech
recognition. In Text, Speech and Dialogue, pages 83?
111. Springer.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Beno??t Sagot and Pierre Boullier. 2008. Sxpipe 2:
architecture pour le traitement pre?syntaxique de cor-
pus bruts. Traitement Automatique des Langues,
49(2):155?188.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The lefff 2 Syn-
tactic Lexicon for French: Architecture, Acquisition,
Use. In International Conference on Language Re-
sources and Evaluation, Genoa.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado.
S.J. Young. 1994. The HTK Hidden Markov Model
Toolkit: Design and Philosophy. Entropic Cambridge
Research Laboratory, Ltd, 2:2?44.
91
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777?785,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Semi-supervised Dependency Parsing using Lexical Affinities
Seyed Abolghasem Mirroshandel?,? Alexis Nasr? Joseph Le Roux
?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universite? Aix-Marseille, Marseille, France
LIPN, Universite? Paris Nord & CNRS,Villetaneuse, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
leroux@univ-paris13.fr)
Abstract
Treebanks are not large enough to reliably
model precise lexical phenomena. This de-
ficiency provokes attachment errors in the
parsers trained on such data. We propose
in this paper to compute lexical affinities,
on large corpora, for specific lexico-syntactic
configurations that are hard to disambiguate
and introduce the new information in a parser.
Experiments on the French Treebank showed
a relative decrease of the error rate of 7.1% La-
beled Accuracy Score yielding the best pars-
ing results on this treebank.
1 Introduction
Probabilistic parsers are usually trained on treebanks
composed of few thousands sentences. While this
amount of data seems reasonable for learning syn-
tactic phenomena and, to some extent, very frequent
lexical phenomena involving closed parts of speech
(POS), it proves inadequate when modeling lexical
dependencies between open POS, such as nouns,
verbs and adjectives. This fact was first recognized
by (Bikel, 2004) who showed that bilexical depen-
dencies were barely used in Michael Collins? parser.
The work reported in this paper aims at a better
modeling of such phenomena by using a raw corpus
that is several orders of magnitude larger than the
treebank used for training the parser. The raw cor-
pus is first parsed and the computed lexical affinities
between lemmas, in specific lexico-syntactic config-
urations, are then injected back in the parser. Two
outcomes are expected from this procedure, the first
is, as mentioned above, a better modeling of bilexi-
cal dependencies and the second is a method to adapt
a parser to new domains.
The paper is organized as follows. Section 2 re-
views some work on the same topic and highlights
their differences with ours. In section 3, we describe
the parser that we use in our experiments and give
a detailed description of the frequent attachment er-
rors. Section 4 describes how lexical affinities be-
tween lemmas are calculated and their impact is then
evaluated with respect to the attachment errors made
by the parser. Section 5 describes three ways to in-
tegrate the lexical affinities in the parser and reports
the results obtained with the three methods.
2 Previous Work
Coping with lexical sparsity of treebanks using raw
corpora has been an active direction of research for
many years.
One simple and effective way to tackle this prob-
lem is to put together words that share, in a large
raw corpus, similar linear contexts, into word clus-
ters. The word occurrences of the training treebank
are then replaced by their cluster identifier and a new
parser is trained on the transformed treebank. Us-
ing such techniques (Koo et al, 2008) report signi-
ficative improvement on the Penn Treebank (Marcus
et al, 1993) and so do (Candito and Seddah, 2010;
Candito and Crabbe?, 2009) on the French Treebank
(Abeille? et al, 2003).
Another series of papers (Volk, 2001; Nakov
and Hearst, 2005; Pitler et al, 2010; Zhou et al,
2011) directly model word co-occurrences. Co-
occurrences of pairs of words are first collected in a
777
raw corpus or internet n-grams. Based on the counts
produced, lexical affinity scores are computed. The
detection of pairs of words co-occurrences is gen-
erally very simple, it is either based on the direct
adjacency of the words in the string or their co-
occurrence in a window of a few words. (Bansal
and Klein, 2011; Nakov and Hearst, 2005) rely on
the same sort of techniques but use more sophisti-
cated patterns, based on simple paraphrase rules, for
identifying co-occurrences.
Our work departs from those approaches by the
fact that we do not extract the lexical information
directly on a raw corpus, but we first parse it and
then extract the co-occurrences on the parse trees,
based on some predetermined lexico-syntactic pat-
terns. The first reason for this choice is that the lin-
guistic phenomena that we are interested in, such as
as PP attachment, coordination, verb subject and ob-
ject can range over long distances, beyond what is
generally taken into account when working on lim-
ited windows. The second reason for this choice was
to show that the performances that the NLP commu-
nity has reached on parsing, combined with the use
of confidence measures allow to use parsers to ex-
tract accurate lexico-syntactic information, beyond
what can be found in limited annotated corpora.
Our work can also be compared with self train-
ing approaches to parsing (McClosky et al, 2006;
Suzuki et al, 2009; Steedman et al, 2003; Sagae
and Tsujii, 2007) where a parser is first trained on
a treebank and then used to parse a large raw cor-
pus. The parses produced are then added to the ini-
tial treebank and a new parser is trained. The main
difference between these approaches and ours is that
we do not directly add the output of the parser to the
training corpus, but extract precise lexical informa-
tion that is then re-injected in the parser. In the self
training approach, (Chen et al, 2009) is quite close
to our work: instead of adding new parses to the tree-
bank, the occurrence of simple interesting subtrees
are detected in the parses and introduced as new fea-
tures in the parser.
The way we introduce lexical affinity measures in
the parser, in 5.1, shares some ideas with (Anguiano
and Candito, 2011), who modify some attachments
in the parser output, based on lexical information.
The main difference is that we only take attachments
that appear in an n-best parse list into account, while
they consider the first best parse and compute all po-
tential alternative attachments, that may not actually
occur in the n-best forests.
3 The Parser
The parser used in this work is the second order
graph based parser (McDonald et al, 2005; Ku?bler
et al, 2009) implementation of (Bohnet, 2010). The
parser was trained on the French Treebank (Abeille?
et al, 2003) which was transformed into dependency
trees by (Candito et al, 2009). The size of the tree-
bank and its decomposition into train, development
and test sets is represented in table 1.
nb of sentences nb of words
FTB TRAIN 9 881 278 083
FTB DEV 1 239 36 508
FTB TEST 1 235 36 340
Table 1: Size and decomposition of the French Treebank
The part of speech tagging was performed with
the MELT tagger (Denis and Sagot, 2010) and lem-
matized with the MACAON tool suite (Nasr et al,
2011). The parser gave state of the art results for
parsing of French, reported in table 2.
pred. POS tags gold POS tags
punct no punct punct no punct
LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
Table 2: Labeled and unlabeled accuracy score for auto-
matically predicted and gold POS tags with and without
taking into account punctuation on FTB TEST.
Figure 1 shows the distribution of the 100 most
common error types made by the parser. In this
figure, x axis shows the error types and y axis
shows the error ratio of the related error type
( number of errors of the specific typetotal number of errors ). We define an error
type by the POS tag of the governor and the POS
tag of the dependent. The figure presents a typical
Zipfian distribution with a low number of frequent
error types and a large number of unfrequent error
types. The shape of the curve shows that concen-
trating on some specific frequent errors in order to
increase the parser accuracy is a good strategy.
778
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 10  20  30  40  50  60  70  80  90  100
er
ro
r 
ra
ti
o
Error Type
Figure 1: Distribution of the types of errors
Table 3 gives a finer description of the most com-
mon types of error made by the parser. Here we
define more precise patterns for errors, where some
lexical values are specified (for prepositions) and, in
some cases, the nature of the dependency is taken
into account. Every line of the table corresponds to
one type of error. The first column describes the
error type. The second column indicates the fre-
quency of this type of dependency in the corpus. The
third one displays the accuracy for this type of de-
pendency (the number of dependencies of this type
correctly analyzed by the parser divided by the to-
tal number of dependencies of this type). The fourth
column shows the contribution of the errors made on
this type of dependency to the global error rate. The
last column associates a name with some of the error
types that will prove useful in the remainder of the
paper to refer to the error type.
Table 3 shows two different kinds of errors that
impact the global error rate. The first one concerns
very common dependencies that have a high accu-
racy but, due to their frequency, hurt the global er-
ror rate of the parser. The second one concerns low
frequency, low accuracy dependency types. Lines 2
and 3, respectively attachment of the preposition a` to
a verb and the subject dependency illustrate such a
contrast. They both impact the total error rate in the
same way (2.53% of the errors). But the first one
is a low frequency low accuracy type (respectively
0.88% and 69.11%) while the second is a high fre-
quency high accuracy type (respectively 3.43% and
93.03%). We will see in 4.2.2 that our method be-
haves quite differently on these two types of error.
dependency freq. acc. contrib. name
N?N 1.50 72.23 2.91
V? a` 0.88 69.11 2.53 VaN
V?suj? N 3.43 93.03 2.53 SBJ
N? CC 0.77 69.78 2.05 NcN
N? de 3.70 92.07 2.05 NdeN
V? de 0.66 74.68 1.62 VdeN
V?obj? N 2.74 90.43 1.60 OBJ
V? en 0.66 81.20 1.24
V? pour 0.46 67.78 1.10
N? ADJ 6.18 96.60 0.96 ADJ
N? a` 0.29 70.64 0.72 NaN
N? pour 0.12 38.64 0.67
N? en 0.15 47.69 0.57
Table 3: The 13 most common error types
4 Creating the Lexical Resource
The lexical resource is a collection of tuples
?C, g, d, s? where C is a lexico-syntactic configu-
ration, g is a lemma, called the governor of the
configuration, d is another lemma called the depen-
dent and s is a numerical value between 0 and 1,
called the lexical affinity score, which accounts for
the strength of the association between g and d in
the context C. For example the tuple ?(V, g)
obj
?
(N, d), eat , oyster , 0.23? defines a simple configu-
ration (V, g)
obj
? (N, d) that is an object depen-
dency between verb g and noun d. When replac-
ing variables g and d in C respectively with eat
and oyster , we obtain the fully specified lexico syn-
tactic pattern(V, eat)
obj
? (N, oyster), that we call
an instantiated configuration. The numerical value
0.23 accounts for how much eat and oyster like
to co-occur in the verb-object configuration. Con-
figurations can be of arbitrary complexity but they
have to be generic enough in order to occur fre-
quently in a corpus yet be specific enough to model
a precise lexico syntactic phenomenon. The context
(?, g)
?
? (?, d), for example is very generic but does
not model a precise linguistic phenomenon, as selec-
tional preferences of a verb, for example. Moreover,
configurations need to be error-prone. In the per-
spective of increasing a parser performances, there
is no point in computing lexical affinity scores be-
tween words that appear in a configuration for which
779
the parser never makes mistakes.
The creation of the lexical resource is a three stage
process. The first step is the definition of configura-
tions, the second one is the collection of raw counts
from the machine parsed corpora and the third one is
the computation of lexical affinities based on the raw
counts. The three steps are described in the follow-
ing subsection while the evaluation of the created
resource is reported in subsection 4.2.
4.1 Computing Lexical Affinities
A set of 9 configurations have been defined. Their
selection is a manual process based on the analysis
of the errors made by the parser, described in sec-
tion 3, as well as on the linguistic phenomena they
model. The list of the 9 configurations is described
in Table 4. As one can see on this table, configu-
rations are usually simple, made up of one or two
dependencies. Linguistically, configurations OBJ
and SBJ concern subject and object attachments,
configuration ADJ is related to attachments of ad-
jectives to nouns and configurations NdeN, VdeN,
VaN, and NaN indicate prepositional attachments.
We have restricted ourselves here to two common
French prepositions a` and de. Configurations NcN
and VcV deal respectively with noun and verb coor-
dination.
Name Description
OBJ (V, g)
obj
? (N, d)
SBJ (V, g)
subj
? (N, d)
ADJ (N, g) ? ADJ
NdeN (N, g) ? (P, de)? (N, d)
VdeN (V, g) ? (P, de)? (N, d)
NaN (N, g) ? (P, a`)? (N, d)
VaN (V, g) ? (P, a`)? (N, d)
NcN (N, g) ? (CC, ?)? (N, d)
VcV (V, g) ? (CC, ?)? (V, d)
Table 4: List of the 9 configurations.
The computation of the number of occurrences of
an instantiated configuration in the corpus is quite
straightforward, it consists in traversing the depen-
dency trees produced by the parser and detect the
occurrences of this configuration.
At the end of the counts collection, we have gath-
CORPUS Sent. nb. Tokens nb.
AFP 1 024 797 31 486 618
EST REP 1 103 630 19 635 985
WIKI 1 592 035 33 821 460
TOTAL 3 720 462 84 944 063
Table 5: sizes of the corpora used to gather lexical counts
ered for every lemma l its number of occurrences as
governor (resp. dependent) of configurationC in the
corpus, noted C(C, l, ?) (resp. C(C, ?, l)), as well as
the number of occurrences of configuration C with
lemma lg as a governor and lemma ld as a depen-
dent, noted C(C, lg, ld). We are now in a position
to compute the score s(C, lg, ld). This score should
reflect the tendency of lg and ld to appear together
in configuration C. It should be maximal if when-
ever lg occurs as the governor of configuration C,
the dependent position is occupied by ld and, sym-
metrically, if whenever ld occurs as the dependent of
configuration C, the governor position is occupied
by lg. A function that conforms such a behavior is
the following:
s(C, lg, ld) =
1
2
(
C(C, lg, ld)
C(C, lg, ?)
+
C(C, lg, ld)
C(C, ?, ld)
)
it takes its values between 0 (lg and ld never
co-occur) and 1 (g and d always co-occur). This
function is close to pointwise mutual information
(Church and Hanks, 1990) but takes its values be-
tween 0 and 1.
4.2 Evaluation
Lexical affinities were computed on three corpora of
slightly different genres. The first one, is a collection
of news report of the French press agency Agence
France Presse, the second is a collection of news-
paper articles from a local French newspaper : l?Est
Re?publicain. The third one is a collection of articles
from the French Wikipedia. The size of the different
corpora are detailed in table 5. The corpus was first
POS tagged, lemmatized and parsed in order to get
the 50 best parses for every sentence. Then the lexi-
cal resource was built, based on the 9 configurations
described in table 4.
The lexical resource has been evaluated on
FTB DEV with respect to two measures: coverage
780
and correction rate, described in the next two sec-
tions.
4.2.1 Coverage
Coverage measures the instantiated configura-
tions present in the evaluation corpus that are in the
resource. The results are presented in table 6. Every
line represents a configuration, the second column
indicates the number of different instantiations of
this configuration in the evaluation corpus, the third
one indicates the number of instantiated configura-
tions that were actually found in the lexical resource
and the fourth column shows the coverage for this
configuration, which is the ratio third column over
the second. Last column represents the coverage of
the training corpus (the lexical resource is extracted
on the training corpus) and the last line represents
the same quantities computed on all configurations.
Table 6 shows two interesting results: firstly the
high variability of coverage with respect to configu-
rations, and secondly the low coverage when the lex-
ical resource is computed on the training corpus, this
fact being consistent with the conclusions of (Bikel,
2004). A parser trained on a treebank cannot be ex-
pected to reliably select the correct governor in lex-
ically sensitive cases.
Conf. occ. pres. cov. T cov.
OBJ 1017 709 0.70 0.21
SBJ 1210 825 0.68 0.24
ADJ 1791 1239 0.69 0.33
NdeN 1909 1287 0.67 0.31
VdeN 189 107 0.57 0.16
NaN 123 61 0.50 0.20
VaN 422 273 0.65 0.23
NcN 220 55 0.25 0.10
VcV 165 93 0.56 0.04
? 7046 4649 0.66 0.27
Table 6: Coverage of the lexical resource over FTB DEV.
4.2.2 Correction Rate
While coverage measures how many instantiated
configurations that occur in the treebank are actu-
ally present in the lexical resource, it does not mea-
sure if the information present in the lexical resource
can actually help correcting the errors made by the
parser.
We define Correction Rate (CR) as a way to ap-
proximate the usefulness of the data. Given a word
d present in a sentence S and a configuration C, the
set of all potential governors of d in configuration
C, in all the n-best parses produced by the parser is
computed. This set is noted G = {g1, . . . , gj}. Let
us note GL the element of G that maximizes the lex-
ical affinity score. When the lexical resource gives
no score to any of the elements of G, GL is left un-
specified.
Ideally, G should not be the set of governors in
the n-best parses but the set of all possible governors
for d in sentence S. Since we have no simple way
to compute the latter, we will content ourselves with
the former as an approximation of the latter.
Let us note GH the governor of d in the (first)
best parse produced and GR the governor of d in the
correct parse. CR measures the effect of replacing
GH with GL.
We have represented in table 7 the different sce-
narios that can happen when comparing GH , GR
and GL.
GL = GR or GL unspec. CC
GH = GR GL 6= GR CE
GL = GR EC
GH 6= GR GL 6= GR or GL unspec. EE
GR /? G NA
Table 7: Five possible scenarios when comparing the
governor of a word produced by the parser (GH ), in
the reference parse (GR) and according to the lexical re-
source (GL).
In scenarios CC and CE, the parser did not make
a mistake (the first letter, C, stands for correct). In
scenario CC, the lexical affinity score was compat-
ible with the choice of the parser or the lexical re-
source did not select any candidate. In scenario CE,
the lexical resource introduced an error. In scenar-
ios EC and EE, the parser made an error. In EC,
the error was corrected by the lexical resource while
in EE, it wasn?t. Either because the lexical resource
candidate was not the correct governor or it was un-
specified. The last case, NA, indicates that the cor-
rect governor does not appear in any of the n-best
parses. Technically this case could be integrated in
EE (an error made by the parser was not corrected
by the lexical resource) but we chose to keep it apart
781
since it represents a case where the right solution
could not be found in the n-best parse list (the cor-
rect governor is not a member of set G).
Let?s note nS the number of occurrences of sce-
nario S for a given configuration. We compute CR
for this configuration in the following way:
CR =
old error number - new error number
old error number
=
nEC ? nCE
nEE + nEC + nNA
When CR is equal to 0, the correction did not have
any impact on the error rate. When CR> 0, the error
rate is reduced and if CR < 0 it is increased1.
CR for each configuration is reported in table 8.
The counts of the different scenarios have also been
reported.
Conf. nCC nCE nEC nEE nNA CR
OBJ 992 30 51 5 17 0.29
SBJ 1131 35 61 16 34 0.23
ADJ 2220 42 16 20 6 -0.62
NdeN 2083 93 42 44 21 -0.48
VdeN 150 2 49 1 13 0.75
NaN 89 5 21 10 2 0.48
VaN 273 19 132 8 11 0.75
NcN 165 17 12 31 12 -0.09
VcN 120 21 14 11 5 -0.23
? 7223 264 398 146 121 0.20
Table 8: Correction Rate of the lexical resource with re-
spect to FTB DEV.
Table 8 shows very different results among con-
figurations. Results for PP attachments VdeN, VaN
and NaN are quite good (a CR of 75% for a given
configuration, as VdeN indicates that the number of
errors on such a configuration is decreased by 25%).
It is interesting to note that the parser behaves quite
badly on these attachments: their accuracy (as re-
ported in table 3) is, respectively 74.68, 69.1 and
70.64. Lexical affinity helps in such cases. On
the other hand, some attachments like configuration
ADJ and NdeN, for which the parser showed very
good accuracy (96.6 and 92.2) show very poor per-
formances. In such cases, taking into account lexical
affinity creates new errors.
1One can note, that contrary to coverage, CR does not mea-
sure a characteristic of the lexical resource alone, but the lexical
resource combined with a parser.
On average, using the lexical resource with this
simple strategy of systematically replacing GH with
GL allows to decrease by 20% the errors made on
our 9 configurations and by 2.5% the global error
rate of the parser.
4.3 Filtering Data with Ambiguity Threshold
The data used to extract counts is noisy: it con-
tains errors made by the parser. Ideally, we would
like to take into account only non ambiguous sen-
tences, for which the parser outputs a single parse
hypothesis, hopefully the good one. Such an ap-
proach is obviously doomed to fail since almost ev-
ery sentence will be associated to several parses.
Another solution would be to select sentences for
which the parser has a high confidence, using confi-
dence measures as proposed in (Sa?nchez-Sa?ez et al,
2009; Hwa, 2004). But since we are only interested
in some parts of sentences (usually one attachment),
we don?t need high confidence for the whole sen-
tence. We have instead used a parameter, defined on
single dependencies, called the ambiguity measure.
Given the n best parses of a sentence and a depen-
dency ?, present in at least one of the n best parses,
let us note C(?) the number of occurrences of ? in
the n best parse set. We note AM(?) the ambiguity
measure associated to ?. It is computed as follows:
AM(?) = 1?
C(?)
n
An ambiguity measure of 0 indicates that ? is non
ambiguous in the set of the n best parses (the word
that constitutes the dependent in ? is attached to the
word that constitutes the governor in ? in all the n-
best analyses). When n gets large enough this mea-
sure approximates the non ambiguity of a depen-
dency in a given sentence.
Ambiguity measure is used to filter the data when
counting the number of occurrences of a configura-
tion: only occurrences that are made of dependen-
cies ? such that AM(?) ? ? are taken into account.
? is called the ambiguity threshold.
The results of coverage and CR given above were
computed for ? equal to 1, which means that, when
collecting counts, all the dependencies are taken into
account whatever their ambiguity is. Table 9 shows
coverage and CR for different values of ? . As ex-
pected, coverage decreases with ? . But, interest-
782
ingly, decreasing ? , from 1 down to 0.2 has a posi-
tive influence on CR. Ambiguity threshold plays the
role we expected: it allows to reduce noise in the
data, and corrects more errors.
? = 1.0 ? = 0.4 ? = 0.2 ? = 0.0
cov/CR cov/CR cov/CR cov/CR
OBJ 0.70/0.29 0.58/0.36 0.52/0.36 0.35/0.38
SBJ 0.68/0.23 0.64/0.23 0.62/0.23 0.52/0.23
ADJ 0.69/-0.62 0.61/-0.52 0.56/-0.52 0.43/-0.38
NdeN 0.67/-0.48 0.58/-0.53 0.52/-0.52 0.38/-0.41
VdeN 0.57/0.75 0.44/0.73 0.36/0.73 0.20/0.30
NaN 0.50/0.48 0.34/0.42 0.28/0.45 0.15/0.48
VaN 0.65/0.75 0.50/0.8 0.41/0.80 0.26/0.48
NcN 0.25/-0.09 0.19/0 0.16/0.02 0.07/0.13
VcV 0.56/-0.23 0.42/-0.07 0.28/0.03 0.08/0.07
Avg 0.66/0.2 0.57/0.23 0.51/0.24 0.38/0.17
Table 9: Coverage and Correction Rate on FTB DEV for
several values of ambiguity threshold.
5 Integrating Lexical Affinity in the Parser
We have devised three methods for taking into ac-
count lexical affinity scores in the parser. The first
two are post-processing methods, that take as input
the n-best parses produced by the parser and mod-
ify some attachments with respect to the information
given by the lexical resource. The third method in-
troduces the lexical affinity scores as new features in
the parsing model. The three methods are described
in 5.1, 5.2 and 5.3. They are evaluated in 5.4.
5.1 Post Processing Method
The post processing method is quite simple. It is
very close to the method that was used to compute
the Correction Rate of the lexical resource, in 4.2.2:
it takes as input the n-best parses produced by the
parser and, for every configuration occurrence C
found in the first best parse, the set (G) of all po-
tential governors of C, in the n-best parses, is com-
puted and among them, the word that maximizes the
lexical affinity score (GL) is identified.
Once GL is identified, one can replace the choice
of the parser (GH ) with GL. This method is quite
crude since it does not take into account the confi-
dence the parser has in the solution proposed. We
observed, in 4.2.2 that CR was very low for configu-
rations for which the parser achieves good accuracy.
In order to introduce the parser confidence in the fi-
nal choice of a governor, we compute C(GH) and
C(GL) which respectively represent the number of
times GH and GL appear as the governor of config-
uration C. The choice of the final governor, noted
G?, depends on the ratio of C(GH) and C(GL). The
complete selection strategy is the following:
1. if GH = GL or GL is unspecified, G? = GH .
2. if GH 6= GL, G? is determined as follows:
G? =
{
GH if
C(GH)
C(GL)
> ?
GL otherwise
where ? is a coefficient that is optimized on the
development data set.
We have reported, in table 10 the values of CR,
for the 9 different features, using this strategy, for
? = 1. We do not report the values of CR for other
values of ? since they are very close to each other.
The table shows several noticeable facts. First, the
new strategy performs much better than the former
one (crudely replacing GH by GL), the value of CR
increased from 0.2 to 0.4, which means that the er-
rors made on the nine configurations are now de-
creased by 40%. Second, CR is now positive for ev-
ery configuration: the number of errors is decreased
for every configuration.
Conf. OBJ SUJ ADJ NdeN VdeN
CR 0.45 0.46 0.14 0.05 0.73
Conf. NaN VaN NcN VcV ?
CR 0.12 0.8 0.12 0.1 0.4
Table 10: Correction Rate on FTB DEV when taking into
account parser confidence.
5.2 Double Parsing Method
The post processing method performs better than the
naive strategy that was used in 4.2.2. But it has an
important drawback: it creates inconsistent parses.
Recall that the parser we are using is based on a sec-
ond order model, which means that the score of a de-
pendency depends on some neighboring ones. Since
with the post processing method only a subset of the
dependencies are modified, the resulting parse is in-
consistent: the score of some dependencies is com-
puted on the basis of other dependencies that have
been modified.
783
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies.
The double parsing method is therefore a three
stage method. First, sentence S is parsed, producing
the n-best parses. Then, the post processing method
is used, modifying the first best parse. Let?s note
D the set of dependencies that were changed in this
process. In the last stage, a new parse is produced,
that preserves D.
5.3 Feature Based Method
In the feature based method, new features are
added to the parser that rely on lexical affinity
scores. These features are of the following form:
?C, lg, ld, ?C(s)?, where C is a configuration num-
ber, s is the lexical affinity score (s = s(C, lg, ld))
and ?c(?) is a discretization function.
Discretization of the lexical affinity scores is nec-
essary in order to fight against data sparseness. In
this work, we have used Weka software (Hall et al,
2009) to discretize the scores with unsupervised bin-
ning. Binning is a simple process which divides
the range of possible values a parameter can take
into subranges called bins. Two methods are im-
plemented in Weka to find the optimal number of
bins: equal-frequency and equal-width. In equal-
frequency binning, the range of possible values are
divided into k bins, each of which holds the same
number of instances. In equal-width binning, which
is the method we have used, the range are divided
into k subranges of the same size. The optimal num-
ber of bins is the one that minimizes the entropy of
the data. Weka computes different number of bins
for different configurations, ranging from 4 to 10.
The number of new features added to the parser is
equal to
?
C B(C) where C is a configuration and
B(C) is the number of bins for configuration C.
5.4 Evaluation
The three methods described above have been evalu-
ated on FTB TEST. Results are reported in table 11.
The three methods outperformed the baseline (the
state of the art parser for French which is a second
order graph based method) (Bohnet, 2010). The best
performances were obtained by the Double Parsing
method that achieved a labeled relative error reduc-
tion of 7, 1% on predicted POS tags, yielding the
best parsing results on the French Treebank. It per-
forms better than the Post Processing method, which
means that the second parsing stage corrects some
inconsistencies introduced in the Post Processing
method. The performances of the Feature Based
method are disappointing, it achieves an error reduc-
tion of 1.4%. This result is not easy to interpret. It
is probably due to the limited number of new fea-
tures introduced in the parser. These new features
probably have a hard time competing with the large
number of other features in the training process.
pred. POS tags gold POS tags
punct no punct punct no punct
BL LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
PP LAS 88.45 90.73 89.46 91.78
UAS 90.61 93.20 91.44 93.86
DP LAS 88.87 91.10 89.72 91.90
UAS 90.84 93.30 91.58 93.99
FB LAS 88.19 90.33 89.29 91.43
UAS 90.22 92.62 91.09 93.46
Table 11: Parser accuracy on FTB TEST using the
standard parser (BL) the post processing method (PP),
the double parsing method (DP) and the feature based
method.
6 Conclusion
Computing lexical affinities, on large corpora, for
specific lexico-syntactic configurations that are hard
to disambiguate has shown to be an effective way
to increase the performances of a parser. We have
proposed in this paper one method to compute lexi-
cal affinity scores as well as three ways to introduce
this new information in a parser. Experiments on a
French corpus showed a relative decrease of the er-
ror rate of 7.1% Labeled Accuracy Score.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the projects
SEQUOIA (ANR-08-EMER-013) and EDYLEX
(ANR-08-CORD-009).
784
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
E.H. Anguiano and M. Candito. 2011. Parse correction
with specialized models for difficult attachment types.
In Proceedings of EMNLP.
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of ACL, pages 693?
702.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89?97.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 138?141.
M. Candito and D. Seddah. 2010. Parsing word clusters.
In Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 76?84.
M. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants aux
de?pendances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
W. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570?579.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
P. Denis and B. Sagot. 2010. Exploitation d?une
ressource lexicale pour la construction d?un e?tiqueteur
morphosyntaxique e?tat-de-l?art du franc?ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
ACL HLT, pages 595?603.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 1(1):1?127.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT NAACL, pages 152?159.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
P. Nakov and M. Hearst. 2005. Using the web as an
implicit training set: application to structural ambigu-
ity resolution. In Proceedings of HLT-EMNLP, pages
835?842.
A. Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886?
894.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL shared task session
of EMNLP-CoNLL, volume 7, pages 1044?1050.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Statistical confidence measures for probabilistic pars-
ing. In Proceedings of RANLP, pages 388?392.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL, pages 331?338.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551?560.
M. Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting
web-derived selectional preference to improve statisti-
cal dependency parsing. In Proceedings of HLT-ACL,
pages 1556?1565.
785
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 161?169,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Extracting a Semantic Lexicon of French Adjectives from a Large
Lexicographic Dictionary
Selja Seppa?la? and Alexis Nasr
Laboratoire d?Informatique Fondamentale
Aix Marseille Universite?
163, avenue de Luminy
F-13288 Marseille Cedex 9
alexis.nasr@lif.univ-mrs.fr
selja.seppala@lif.univ-mrs.fr
Lucie Barque
Lexiques Dictionnaires Informatique
Universite? Paris 13
99, avenue Jean-Baptiste Cle?ment
F-93430 Villetaneuse
lucie.barque@univ-paris13.fr
Abstract
We present a rule-based method to automati-
cally create a large-coverage semantic lexicon
of French adjectives by extracting paradig-
matic relations from lexicographic definitions.
Formalized adjectival resources are, indeed,
scarce for French and they mostly focus on
morphological and syntactic information. Our
objective is, therefore, to contribute enriching
the available set of resources by taking advan-
tage of reliable lexicographic data and formal-
izing it with the well-established lexical func-
tions formalism. The resulting semantic lexi-
con of French adjectives can be used in NLP
tasks such as word sense disambiguation or
machine translation. After presenting related
work, we describe the extraction method and
the formalization procedure of the data. Our
method is then quantitatively and qualitatively
evaluated. We discuss the results of the evalu-
ation and conclude on some perspectives.
1 Introduction
Formalized semantic resources are highly valuable
in areas such as NLP, linguistic analysis or lan-
guage acquisition. However, creating such resources
from scratch is time-consuming and generally yields
limited-size lexicons. Existing lexicographic dictio-
naries do have a large coverage and present a reli-
able content. They lack nevertheless the sufficient
formalization. In this paper, we present a rule-based
method to automatically create a large-coverage se-
mantic lexicon of French adjectives by extracting
paradigmatic relations from lexicographic defini-
tions using lexico-syntactic patterns. Formalized ad-
jectival resources are, indeed, scarce for French and
they mostly focus on morphological and syntactic
information. Our goal is, therefore, to contribute en-
riching the available set of resources by taking ad-
vantage of reliable lexicographic data and formal-
izing it with the well-established lexical functions
formalism of the Meaning-Text theory (Mel?c?uk,
1996). The resulting semantic lexicon of French
adjectives can be used in NLP tasks such as word
sense disambiguation or machine translation1. In
section 2, we present related work. In section 3, we
expose the method used to build the lexicon, i.e. the
extraction method and the formalization procedure
of the data, and outline the main results. Finally, in
section 4, we present a quantitative evaluation of our
method and a qualitative evaluation of our data, and
discuss their results. We conclude on some perspec-
tives for future work.
2 Related Work
It is well established that there are different types
of adjectives distinguished by properties, such as
gradation and markedness, and by their seman-
tic and syntactic behaviors (antonymy, selectional
preferences) (Fellbaum et al, 1993; Raskin and
Nirenburg, 1996). WordNet, for example, distin-
guishes different types of adjectives according to
their semantic and syntactic behaviors: descriptive,
reference-modifying, color and relational adjectives
(Fellbaum et al, 1993). However, it mainly accounts
for the first and the last types of adjectives. Descrip-
1For other possible NLP applications of lexicons encoded
with the lexical function formalism, see Schwab and Lafour-
cade (2007).
161
tive adjectives are organized in adjectival synsets
that are mostly related through antonymy (heavy?
light); synsets of relational adjectives are linked to a
related noun by a pointer (fraternal?brother). Fell-
baum et al (1993:36) acknowledge the existence of
more diverse relations to nominal synsets, but, to our
knowledge, these are not accounted for in WordNet.
This limitation is also present in the open access
French version of the Princeton WordNet, WOLF
(Sagot and Fis?er, 2012). This limitation has led
projects extending WordNet to other languages, like
EuroWordNet, ItalWordNet or WordNet.PT, to add
a few more relations to account for this diversity
(Alonge et al, 2000; Marrafa and Mendes, 2006;
Vossen, 2002). The number of new relations is how-
ever limited. As can be seen, WordNet-type ap-
proaches focus on relating adjectival synsets using
a few semantic relations, mostly antonymy and plain
related to relations.
Our goal is to achieve a finer, and thus richer, se-
mantic characterization of the relations holding be-
tween French adjectives and other words from all
syntactic categories using the formalism of lexical
functions. We assume that the type of the adjective is
reflected in the structure of its lexicographic defini-
tion. Thus, to extract semantically relevant informa-
tion from adjectival definitions, we propose to create
different types of rules accounting for this diversity
of defining structures.
Formalized French lexicons contain rather limited
adjectival data. One can cite the morphological lex-
icon that links French denominal adjectives to the
nouns they are derived from (Strnadova` and Sagot,
2011) or the syntactic characterization of French ad-
jectives based on an automatic extraction of subcat-
egorization frames proposed in Kups?c? (2008). Our
method is meant to complete this set of resources
with an adjectival lexicon that is not limited to cer-
tain types of adjectives (like descriptive or denom-
inal) nor to morphologically related adjectives, and
which provides semantic information.
3 Method and Results
The method we use to extract formalized semantic
information from unformalized lexicographic defi-
nitions follows two steps : extracting relations be-
tween defined adjectives and elements of their def-
initions using lexico-syntactic rules (section 3.1)
and mapping these relations to regular relations that
can be expressed in terms of lexical functions (sec-
tion 3.2).
3.1 Extracting Paradigmatic Relations from
Lexicographic Definitions
The dictionary used in this project is the Tre?sor de la
langue franc?aise informatise?2 (TLFi). It is the elec-
tronic version of a 100,000 word lexicographic dic-
tionary of 19th and 20th century French, the Tre?sor
de la langue franc?aise (Dendien and Pierrel, 2003).
The TLFi contains a total of 13,513 adjectival
entries, among which 6,425 entries correspond to
mere adjectives and 7,088 to adjectives and other
parts of speech (generally nouns)3. Each of these
entries includes one or more definitions, which add
up to 44,410 definitions, among which 32,475 are
estimated to be adjectival. This approximation is
obtained after filtering out 11,935 non-adjectival
definitions from the mixed entries using a lexico-
syntactic definition parsing program aimed at detect-
ing nominal definitions. The remaining definitions
are mostly adjectival, with exceptions due to more
complex definition structures that are not accounted
for by the filtering method. Table 1 sums up the main
figures.
Adjectival entries 6,425
Not only adjectival entries 7,088
Estimated adjectival definitions 32,475
Table 1: Adjectives in the TLFi
To extract semantically relevant information from
adjectival definitions, we use a lexico-syntactic
adjectival definition parsing program which uses
lexico-syntactic rules that are linearly matched to
syntactically annotated adjectival definitions4. The
extraction method consists of the following steps:
1. First, tagging and lemmatizing the definition so
2TLFi, http://atilf.atilf.fr/tlf.htm.
3It is difficult to determine exactly how many adjectives are
defined in the TLFi since the dictionary often joins together
words that can be both used as a noun or an adjective (for ex-
ample JEUNE-young).
4The definitions are syntactically annotated with the Macaon
tool suite (Nasr et al, 2010) that was adapted to the special
sublanguage of lexicographic definitions.
162
that each word is related to a part of speech tag
(POS).
(1) RETENU = Qui fait preuve de mode?ration.
(restrained = Who shows moderation.)
Qui/prorel fait/v preuve/nc de/prep
mode?ration/nc ./poncts
2. Second, running the adjectival definition pars-
ing program to obtain a triplet composed of the
defined adjective (<adj>), a relation (<rel>)
and an argument (<arg>), i.e. a word or group
of words that is linked by the extracted relation
to the defined adjective.
(2) <adj>retenu</adj>
<rel>fait preuve de</rel>
<arg>mode?ration</arg>
A lexico-syntactic rule extracts from a definition
the <rel> and <arg> elements. As can be seen
in figure 1, each lexico-syntactic rule is composed
of a left-hand side (LHS) containing either a lexi-
cal unit (lex), such as qui, or a POS tag (cat) like
v (verb), both of which can be optional (op="y"),
and a right-hand side (RHS) specifying which ele-
ments of the LHS are to be extracted as semanti-
cally relevant: a relation (REL) and/or an argument
(ARG)5.
In figure 1, the denominal rule 2.2 identifies
adjectival definitions corresponding to the lexico-
syntactic pattern stated by the LHS of the rule, such
as that of the adjective RETENU in example 2 above6.
The LHS contains nine elements, where the first two
correspond to lexical items and the remaining ones
to POS tags. Five elements are marked as optional,
since a definition may for example start by the for-
mula Qui est (Which/Who is) followed by some verb,
or it may directly begin with a verb. This verb has to
be followed by a noun (nc) and a preposition (prep),
which may be followed by a determinant and/or an
adjective, but which has to be followed by a noun,
etc. The RHS of the rule states that the relation to
be extracted corresponds to elements 3, 4 and 5 of
5For definitions by synonymy, only the argument is speci-
fied, the default semantic relation being synonymy.
6Note that the adjective RETENU (retained) is, morpholog-
ically speaking, not a denominal. However, the rule extracts
a noun to which this adjective is related in its definition, i.e.
MODE?RATION (moderation). It is, therefore, the rule that is con-
sidered denominal.
<regle num="2.2" rel="denominal">
<lhs>
<elt lex="qui" op="y" />
<elt lex="est" op="y" />
<elt cat="v" />
<elt cat="nc" />
<elt cat="prep" />
<elt cat="det" op="y" />
<elt cat="adj" op="y" />
<elt cat="nc" />
<elt cat="adj" op="y" />
</lhs>
<rhs>
<rel>
<elt num="3" />
<elt num="4" />
<elt num="5" />
</rel>
<arg>
<elt num="7" />
<elt num="8" />
<elt num="9" />
</arg>
</rhs>
</regle>
Figure 1: Example of Lexico-Syntactic Rule
the LHS, and that the argument is composed of ele-
ments 7, 8 and 97.
The relation extraction program reads the dictio-
nary definition from the beginning of the sentence
checking whether it contains the elements specified
in the LHS of the rule. In case the rule matches
the lexico-syntactic elements composing the defini-
tion, it outputs the lexical elements of the defini-
tion corresponding to the lexical or syntactic infor-
mation specified in the RHS of the rule in the form
REL(ARG)=ADJ, where ADJ stands for the adjec-
tive of the dictionary entry. For instance, applying
the rule from figure 1 to the definition of the adjec-
tive RETENU returns the relation fait preuve de and
the argument mode?ration (example 2).
A total of 109 lexico-syntactic rules have been de-
signed. These rules cover 76.1 % of the adjectival
definitions (24,716/32,475 definitions). The rules
can broadly be grouped into four categories corre-
sponding to different adjectival definition structures.
This categorization is done according to the type of
defining information matched by the rules:
7In the RHS, the number assigned as a value to the num
attribute corresponds to the line number of the elt in the LHS.
163
1. The adjective is defined by one or more syn-
onyms.
? REL = synonymy; ARG = adjective
(3) DIAGONAL = Transversal, oblique. (diago-
nal = Transversal, oblique.)
? syn(transversal) = DIAGO-
NAL; syn(oblique) = DIAGONAL
(syn(transversal) = diagonal; syn(oblique)
= diagonal)
2. The adjective is defined by another adjective
modified by an adverb.
? REL = adverb; ARG = adjective
(4) KILOME?TRIQUE = Qui est tre`s long, qui
n?en finit pas. (kilometric = Which is very
long, never-ending.)
? tre`s(long) = KILOME?TRIQUE
(very(long) = kilometric)
3. The adjective is defined by a relation to a prop-
erty of the thing denoted by the modified noun.
The argument of this complex REL consists of
a noun phrase (NP), a verbal phrase (VP) or an
adjective (ADJ).
? REL = relation + property; ARG =
NP/VP/ADJ
(5) AGRE?GATIF = Qui a la faculte? d?agre?ger.
(aggregative = Which has the power to ag-
gregate.)
? a la faculte? de(agre?ger) = AGRE?GATIF
(has power to(aggregate) = aggregative)
VERSICOLORE = Dont la couleur est
changeante. (versicolor = Which color is
changing.)
? dont la couleur est(changeante) = VER-
SICOLORE (which color is(changing) =
versicolor)
4. The adjective is defined by a relation having as
argument a noun phrase, a verbal phrase or an
adjective.
? REL = relation; ARG = NP/VP/ADJ
(6) ACADE?MIQUE = Qui manque d?originalite?,
de force; conventionnel. (academic =
Which lacks originality, strength; conven-
tional.)
? manque de(originalite?) =
ACADE?MIQUE (lacks(originality) =
academic)
INANALYSABLE = Qui ne peut e?tre analyse?,
qui ne peut e?tre de?compose? en ses e?le?ments
distinctifs. (unanalyzable = Which cannot
be analyzed, which cannot be decompozed
in its distinctive elements.)
? ne peut e?tre(analyse?) = IN-
ANALYSABLE (cannot be(analyzed) =
unanalyzable)
The rules extract a total of 5,284 different rela-
tion types in the form (REL, ARG), where REL is
a lexicalized expression and ARG a phrasal type, as
illustrated in example (7).
(7)
(capable de, VPinf) (capable of, VPinf )
(constitue? de, NP) (constituted by, NP)
(couvert de, NP) (covered with, NP)
(fonde? sur, NP) (founded on, NP)
(peu, ADJ) (not very, ADJ)
(propre a`, NP) (particular to, NP)
(propre a`, VPinf) (capable of, VPinf )
(relatif a`, NP) (relating to, NP)
One can note that the lexicalized relation is some-
times followed by different phrasal types, as can be
seen for propre a` in example (7). In those cases,
each (REL, ARG) pair is considered as a distinct re-
lation type.
3.2 Formalizing Paradigmatic Relations with
Lexical Functions
Lexical functions (LF) are a formal tool designed
to describe all types of genuine lexical relations
(paradigmatic and syntactic ones) between lexical
units of any language (Mel?c?uk, 1996). Some of the
standard lexical functions that often return adjectival
values are briefly presented below:
? A0 ? This paradigmatic lexical function returns the
adjective that semantically corresponds to the argu-
ment. E.g. A0(CHAT) = FE?LIN (A0(cat) = feline);
A0(CRIME) = CRIMINEL (A0(crime) = criminal)
? A1/A2 ? These paradigmatic lexical functions re-
turn the adjectives that typically characterize, re-
spectively, the first and second argument of the
predicate given as argument to the functions. This
predicate can be nominal, adjectival or verbal.
For example, given that the nominal predicate
DE?CEPTION (disappointment) has two arguments,
the person that is disappointed and the reason of the
disappointment, function A1 applied to DE?CEPTION
returns the adjective DE?C?U (disappointed), while
function A2 returns DE?CEVANT (disappointing).
E.g. A1(DE?CEPTION) = DE?C?U (A2(disappointment)
= disappointed); A2(DE?CEPTION) = DE?CEVANT
(A2(disappointment) = disappointing)
164
? Able1/Able2 ? Closely related to A1 and A2, these
functions return the adjective that means that the
first (Able1) or the second (Able2) argument of the
predicate P ?might P or is likely to P? (whereas
A1 just means ?arg1 that P? and A2 ?arg2 that
is P-ed?). E.g. Able1(CRAINDRE) = PEUREUX
(Able1(to fear) = coward); Able2(CRAINDRE) =
EFFRAYANT (Able2(to fear) = frightening)
? Magn ? This function returns an intensificator of
the predicate. This intensificator can modify the
argument, as in heavy rain (Magn expresses then
a syntagmatic relation), or can be another adjec-
tive that intensifies the meaning of the argument
(Magn expresses then a paradigmatic relation). E.g.
Magn(MAUVAIS) = AFFREUX (Magn(bad) = awful)
? Anti ? This function returns the argument?s
antonym(s). E.g. Anti(ABSENT) = PRE?SENT
(Anti(absent) = present)
? AntiA1 ? This complex lexical function returns
the adjective that means that the first argument of
the predicate P ?is not P (anymore)?. E.g. An-
tiA1(FAIM) = REPU (AntiA1(hunger) = full)
We use this formalism to describe the paradig-
matic relations between adjectives and the argu-
ments extracted in the previous step. These rela-
tions are formulated in a non-systematic way in the
TLFi?s definitions. Definitions in traditional dictio-
naries are written in natural language and, thus, are
not formal enough to be used as such, for example,
in NLP tasks. In order to formalize the lexicon, a
mapping is done between lexical functions describ-
ing paradigmatic relations and the different ways of
expressing these relations in the TLFi?s definitions
(see relation types in example 7), as illustrated in ta-
ble 2.
This REL-LF mapping covers 67.3 % of the ex-
tracted relations (16,646/24,716 extracted relations).
Table 3 shows the complete list of lexical functions
used in our lexicon and their distribution: the three
lexical functions A0, A1 and QSyn represent around
90 % of the relations.
4 Evaluation
The method and the data have been evaluated in
two ways. The method has first been evaluated by
comparing our data to an external resource, the Dic-
tionnaire de combinatoire8 (DiCo), a French lex-
8The electronic version of the DiCo can be accessed here:
http://olst.ling.umontreal.ca/dicouebe/index.php.
A0 (qui) est relatif a`, est propre a` + N, se rapporte
a`, . . . (who/that is related to, particular to . . . )
A1 (qui) a la forme de, est atteint de, . . .
(who/that has the shape of, suffers from . . . )
A2 (qui) produit, provoque, a rec?u, . . .
(who/that causes, has obtained . . . )
Able1 qui peut, est propre a` + V, susceptible de, . . .
(who/that can, is likely to . . . )
Able2 que l?on peut, . . .
(who/that can be . . . )
Anti qui n?est pas, qui s?oppose a`, . . .
(that is not, that is opposed to . . . )
AntiA1 (qui) n?a pas de, est de?pourvu de, manque de, . . .
(who/that has no, is un-sthg, lacks sthg . . . )
Table 2: LFs and Their Glosses in the TLFi Definitions
A0 A1 A2 Able1 Able2
28.8 % 27.71 % 4.38 % 6.65 % 0.37 %
Anti AntiA1 AntiA2 AntiAble1 AntiAble2
1.64 % 3.49 % 0.21 % 1.24 % 1.04 %
QSyn Magn Ver AntiMagn AntiVer
21.73 % 1.60 % 0.62 % 0.35 % 0.20 %
Table 3: LF?s Distribution in the French Adjectival Lexi-
con
icographic dictionary describing words with their
paradigmatic and syntagmatic relations expressed in
the LF formalism. In this first evaluation, we de-
termine the performance of the method by quan-
tifying the number of reference elements from the
DiCo that can be extracted from the TLFi with our
rules (section 4.1). Since relations involving adjec-
tives are scarce in the DiCo, our data has then been
qualitatively evaluated by an expert familiar with the
formalism of lexical functions9 (section 4.2). The
expert evaluates the relevance of the argument and
the adequacy of the proposed lexical function to de-
scribe the relation between the defined adjective and
the argument.
4.1 Comparison With the DiCo Data
The first evaluation procedure is meant to measure
the performance of the extraction program against
an existing resource. The reference is constituted
by selecting 240 triplets in the form LF(ARG)=ADJ
from the DiCo. An automatic evaluation script com-
pares these reference triplets with the hypothesized
triplets extracted from the TLFi. The system catego-
9The expert is not an author of this paper.
165
rizes the reference triplets in one of three large cat-
egories explained below: ?Impossible?, ?Yes? and
?No?, the latter ones indicating whether the method
allows to extract the reference triplets from the TLFi
or not. In the ?No? cases, the evaluation system
subcategorizes the reference triplet according to a
possible explanation of the failure of the extraction
method.
1. IMPOSSIBLE (42.9 %, 103/240 triplets)
Cases where the reference triplets cannot be
used as an evaluation reference because either
the adjective of the reference is absent from
the TLFi dictionary (5 %, 12/240 triplets, ex-
ample 8) or the reference argument is absent
from the definition(s) of the corresponding ad-
jective in the TLFi (37.9 %, 91/240 triplets, ex-
ample 9).
(8) DiCo-reference
QSyn(humain) = philanthrope
(QSyn(human) = philanthropic)
TLFi-hypothesis
?(?) = ?
The adjective philanthrope (philanthropic)
does not have an entry in the TLFi.
(9) DiCo-reference
A1(richesse) = riche
(A1(wealth) = rich)
TLFi-hypothesis
A1Perf(fortune) = riche
(A1Perf(fortune) = rich)
In this example, the argument richesse
(wealth) does not exist in any of the 15 def-
initions of riche (rich) in the TLFi.
2. YES (20.4 %, 49/240 triplets)
(a) Total matches: these cases correspond to
the intersection of the two resources, i.e.
cases where the triplets are identical on
both sides (16.3 %, 39/240 triplets).
(10) DiCo-reference
A1(faute) = fautif
TLFi-hypothesis
A1(faute) = fautif
(A1(fault) = guilty)
(b) Partial matches: cases where the adjec-
tives and LFs are identical on both sides
and where the reference argument is in-
cluded in the hypothesis argument (4.2 %,
10/240 triplets).
(11) DiCo-reference
A1(de?faite) = vaincu
(A1(defeat) = vanquished)
TLFi-hypothesis
A1(de?faite militaire) = vaincu
(A1(military defeat) = vanquished)
3. NO (36.7 %, 88/240 triplets) Four types of
cases can be distinguished:
(a) Cases where the reference adjective is in
the TLFi but absent from the set of hy-
pothesis adjectives. These cases can be
explained by the fact that the extraction
rules did not match a definition in the
TLFi or by the fact that no LF has been
mapped to the lexical relation that was ex-
tracted from the TLFi definitions (13.8 %,
33/240 triplets).
(12) DiCo-reference
A0(lait) = lactique
(A0(milk) = lactic)
TLFi-hypothesis
?(?) = ?
(b) Cases where the adjective and the argu-
ment of the reference and of the hypoth-
esis are identical or where the arguments
match partially, but the LFs are differ-
ent (11.3 %, 27/240 triplets, example 13).
This divergence might indicate an erro-
neous mapping between the extracted lex-
icalized relation and the LF. It could also
be explained by the possibility of describ-
ing the same pair of ADJ-ARG with two
different LFs.
(13) DiCo-reference
Able1(haine) = haineux
TLFi-hypothesis
A1(haine) = haineux
(A1(hate) = hateful)
(c) Cases where the extraction rule outputs
an ill-formed hypothesis argument result-
ing from some problem in the extraction
rule (example 14), or where the hypoth-
esis triplet is not erroneous as such but
corresponds to a new triplet-variant for a
particular adjective (example 15) (11.7 %,
28/240 triplets).
(14) DiCo-reference A0(sucre) = sucrier
(A0(sugar) = sugar (nominal adjec-
tive))
166
TLFi-hypothesis
A0(production) = sucrier
(A0(production) = sugar)
TLFi-definition
SUCRIER = Qui est relatif a` la
production, a` la fabrication du sucre.
(sugar (adj.) = Related to the produc-
tion, the manufacture of sugar.)
In example 14, the TLFi definition for su-
crier contains the reference argument su-
cre, but the extraction rule did not match
the right string, resulting in an ill-formed
hypothesis argument.
(15) DiCo-reference A1(enthousiasme) =
enthousiaste
(A1(enthusiasm) = enthusiastic)
TLFi-hypothesis
A1(admiration passionne?e) = enthou-
siaste
(A1(passionate admiration) = enthu-
siastic)
In example 15, the hypothesis argument
extracted by the rule is well-formed but
does not correspond to the reference argu-
ment. The hypothesis triplet can thus be
considered as a new variant for the adjec-
tive enthousiaste (enthusiastic).
The most significant results of the first evalua-
tion are synthesized in table 4. Note that the ref-
erence does not cover every relation type that has
been taken into account in our lexicon: among the
15 relation types listed in table 3 above, only ten are
present in the DiCo resource and six illustrated in
table 4.
Eval. % A1 A0 QSyn Able2 A2 Able1
Imp. 42.9 33 10 31 7 12 6
Yes 20.4 18 24 0 1 2 2
No 36.7 29 16 10 14 6 8
Total 100 80 50 41 22 20 16
Table 4: Results of the First Evaluation Against the DiCo
If the reference triplets marked ?Impossible?
(Imp.) are excluded, this evaluation shows that the
simple rule-based system proposed to extract seman-
tically relevant information from lexicographic def-
initions of adjectives covers 35.8 % of the 137 ref-
erence triplets that can be used for the evaluation.
The analysis of the 88 ?No? cases shows that most
of the problems are due to insufficient rule-coverage
and/or REL-LF mapping (37.5 %, 33/88). This fig-
ure could be reduced by further analyzing the def-
initions that are not accounted for by the rules in
order to add more rules, and by mapping more lex-
icalized relations to LFs. The latter solution might,
however, prove difficult due to the high frequency
of reduced- or single-occurrence relations extracted.
30.7 % (27/88) of the ?No? cases correspond to a
difference in LFs and 31.8 % (28/88) to either ill-
formed arguments or to new variant-triplets. A man-
ual check of the 53 hypothesis triplets extracted for
the 28 adjectives of the latter types of cases shows
that in only 12 cases the hypothesis arguments are
ill-formed (corresponding to 6/28 reference triplets);
the rest corresponds to, a priori, acceptable argu-
ments, i.e. to new triplet variants (41/53 cases), al-
though a few of them are technically speaking ill-
formed. Therefore, most of the remaining 55.7 %
(49/88) ?No? cases should be qualitatively evalu-
ated.
These mitigated quantitative results have to be put
in perspective. The first evaluation was meant to test
the performance of the extraction rules against data
from an existing resource, but, as the figures show,
the vast majority of the reference triplets cannot be
tested. This quantitative evaluation thus highlights
the difficulty of using existing resources for this
kind of task (particularly when such resources are
scarce). Moreover, it proves insufficient to measure
the actual performance of the rules. Two types of
cases are indeed unaccounted for: first, there might
be many correct hypothesis triplets that are not in
the reference, since there is a huge discrepancy in
the number of triplets between the reference and the
hypotheses; second, the hypothesis triplets that don?t
match to the reference might still be correct. There-
fore, other qualitative evaluation methods have to be
used.
4.2 Evaluation by an LF Expert
An expert of the LF formalism has evaluated
the quality of 150 triplets taken from the 16,646
LF(ARG)=ADJ triplets of the lexicon. First, he eval-
uated the argument (0 for a wrong argument, 1 for a
valid argument) and, when he judged that the argu-
ment was correct, he evaluated the LF: 2 for a good
167
LF, 1 for a partially satisfying LF and 0 for an in-
valid LF. To sum up, four configurations are possi-
ble:
? Case 1 ? ARG:0
E.g. A2(converti-converted) = AGATISE?-agatized
The expert considers that the argument is invalid.
Indeed, AGATISE? means converted into an agate but
the program extracted converted as an argument in-
stead of agate.
? Case 2 ? ARG:1 LF:0
E.g. Able1(admiration) = ADMIRABLE
The expert considers that the argument is valid but
the LF is not the right one: the adjective AD-
MIRABLE characterizes the second argument of ad-
miration and not the first one. The correct LF
should therefore be Able2.
? Case 3 ? ARG:1 LF:1
A1(trouble-confusion) = AHURI-dazed
The expert considers that the argument is valid but
the LF is incomplete: it is true that the adjective
AHURI qualifies the first argument of confusion but,
more precisely, it conveys information on the man-
ifestation of the emotion. So a more precise LF
should be A1-Manif.
? Case 4 ? ARG:1 LF:2
Magn(agite?-upset) = AFFOLE?-distraught
The expert considers that the argument and the LF
are valid since AFFOLE? indeed means very upset.
Table 5 shows the results of the qualitative evalu-
ation of lexical functions. Cases 3 and 4 are consid-
ered to be accurate.
Case 1 Case 2 Case 3 Case 4 Total Accuracy
11 34 32 73 150 70.5 %
Table 5: Evaluation by the Expert
When confronted to cases 2 and 3, the expert was
invited to give the correct LF. This information will
be processed in order to improve the matching be-
tween relations extracted from the TLFi and appro-
priate lexical functions.
5 Conclusion
In this article, we presented a rule-based method
to automatically extract paradigmatic relations from
lexicographic definitions of adjectives using lexico-
syntactic patterns. This method was completed with
a manual mapping of the most frequently extracted
lexicalized relations (which are quite heterogenous)
to formal lexical functions. Our goal is to automati-
cally create a formalized semantic lexicon of French
adjectives that would be complementary to the few
existing adjectival resources that can be used, for
instance, in NLP tasks. The adjectival lexicon, in
which each adjective is related by a lexical func-
tion to an NP/VP/adjectival/adverbial argument, was
quantitatively and qualitatively evaluated.
The first evaluation, entirely automatic, was
aimed at testing the performance of the method. It
yielded rather inconclusive results mainly due to the
scarcity of the external data available for the task. A
thorough analysis of the different types of ?errors?
showed that the number of ?technical problems?
can be reduced by refining the extraction rules, by
adding more of them, and by completing the map-
ping of extracted relations to LFs. It also highlighted
the necessity to evaluate the method qualitatively.
The second evaluation was, thus, aimed at rating the
acceptability of the extracted relations. It was real-
ized by an expert of the lexical functions formalism
and gave good results, with a precision of around
70 %.
The automatically created adjectival lexicon pre-
sented in this paper can be easily extended by
a straightforward inversion of the LF(ARG)=ADJ
triplets. The resulting triplets would either complete
existing lexical entries if integrated into a similarly
encoded nominal and verbal lexicon, or constitute
new entries in the adjectival lexicon, thus extend-
ing the syntactic categories represented in the lexi-
con. The LF formalism could also be used to further
enrich adjectival entries by making automatic infer-
ences between adjective-argument pairs and their re-
spective synonyms. E.g. infer A0(kitty)=feline from
A0(cat)=feline and syn(cat)=kitty. Finally, mapping
LFs with the existing relations in WordNet could al-
low to integrate this adjectival lexicon to the French
WOLF.
Acknowledgements
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
EDYLEX (ANR-08-CORD-009).
168
References
Alonge A., Bertagna F., Calzolari N., Roventini A., and
Zampolli A. 2000. Encoding information on adjec-
tives in a lexical-semantic net for computational ap-
plications. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, NAACL 2000, pages 42?49, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Dendien J. and Pierrel J.-M. 2003. Le Tre?sor
de la Langue Franc?aise informatise? : un exem-
ple d?informatisation d?un dictionnaire de langue de
re?fe?rence. Traitement Automatique des Langues.
44(2):11-37.
Fellbaum C., Gross D. and Miller K. J. 1993. Adjec-
tives in WordNet. Technical report, Cognitive Science
Laboratory, Princeton University, 26?39.
Kups?c? A. 2008. Adjectives in TreeLex. In M. Klopotek,
A. Przepio?rkowski, S. Wierzchon? et K. Trojanowski
(eds.), 16th International Conference Intelligent Infor-
mation Systems. Zakopane, Poland, 16-18 juin, Aca-
demic Publishing House EXIT, 287?296.
Marrafa, P. and Mendes, S. 2006. Modeling adjectives
in computational relational lexica. In Proceedings of
the COLING/ACL on Main conference poster sessions,
COLING-ACL ?06, pages 555?562, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mel?c?uk I. 1996. Lexical Functions: A Tool for
the Description of Lexical Relations in the Lexicon.
In: L. Wanner (ed.). Lexical Functions in Lexicog-
raphy and Natural Language Processing. Amster-
dam/Philadelphia: Benjamins, 37-102.
Nasr A., Be?chet F., Rey J.-F., Favre B. and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. The 49th Annual Meeting of the Associ-
ation for Computational Linguistics.
Raskin V. and Nirenburg S. 1996. Adjectival Modifi-
cation in Text Meaning Represention. Proceedings of
COLING ?96.
Sagot B. and Fis?er D. 2012. Automatic extension of
WOLF. 6th International Global Wordnet Conference
(GWC2012). Matsue, Japan.
Schwab D. and Lafourcade M. 2011. Modelling, Detec-
tion and Exploitation of Lexical Functions for Analy-
sis. ECTI Journal. Vol.2. 97-108.
Strnadova? J. and Sagot B. 2011. Construction d?un lex-
ique des adjectifs de?nominaux. Actes de TALN 2011.
Vol.2. 69-74. Montpellier, France.
Vossen, P. 2002. WordNet, EuroWordNet and Global
WordNet. Revue franc?aise de linguistique applique?e,
7(1):27?38.
169
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89?99,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Generative Constituent Parsing and Discriminative Dependency Reranking:
Experiments on English and French
Joseph Le Roux Beno?t Favre? Alexis Nasr? Seyed Abolghasem Mirroshandel?,?
LIPN, Universit? Paris Nord ? CNRS UMR 7030, Villetaneuse, France
?LIF, Universit? Aix-Marseille ? CNRS UMR 7279, Marseille, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
leroux@univ-paris13.fr, benoit.favre@lif.univ-mrs.fr,
alexis.nasr@lif.univ-mrs.fr, ghasem.mirroshandel@lif.univ-mrs.fr
Abstract
We present an architecture for parsing in two
steps. A phrase-structure parser builds for
each sentence an n-best list of analyses which
are converted to dependency trees. These de-
pendency structures are then rescored by a dis-
criminative reranker. Our method is language
agnostic and enables the incorporation of ad-
ditional information which are useful for the
choice of the best parse candidate. We test
our approach on the the Penn Treebank and
the French Treebank. Evaluation shows a sig-
nificative improvement on different parse met-
rics.
1 Introduction
Two competing approaches exist for parsing natural
language. The first one, called generative, is based
on the theory of formal languages and rewriting sys-
tems. Parsing is defined here as a process that trans-
forms a string into a tree or a tree forest. It is of-
ten grounded on phrase-based grammars ? although
there are generative dependency parsers ? in partic-
ular context-free grammars or one of their numer-
ous variants, that can be parsed in polynomial time.
However, the independence hypothesis that under-
lies this kind of formal system does not allow for
precise analyses of some linguistic phenomena, such
as long distance and lexical dependencies.
In the second approach, known as discriminative,
the grammar is viewed as a system of constraints
over the correct syntactic structures, the words of the
sentence themselves being seen as constraints over
the position they occupy in the sentence. Parsing
boils down to finding a solution that is compatible
with the different constraints. The major problem of
this approach lies in its complexity. The constraints
can, theoretically, range over any aspect of the final
structures, which prevents from using efficient dy-
namic programming techniques when searching for
a global solution. In the worst case, final structures
must be enumerated in order to be evaluated. There-
fore, only a subset of constraints is used in imple-
mentations for complexity reasons. This approach
can itself be divided into formalisms relying on logic
to describe constraints, as the model-theoretic syn-
tax (Pullum and Scholz, 2001), or numerical for-
malisms that associate weights to lexico-syntactic
substructures. The latter has been the object of some
recent work thanks to progresses achieved in the
field of Machine Learning. A parse tree is repre-
sented as a vector of features and its accuracy is
measured as the distance between this vector and the
reference.
One way to take advantage of both approaches
is to combine them sequentially, as initially pro-
posed by Collins (2000). A generative parser pro-
duces a set of candidates structures that constitute
the input of a second, discriminative module, whose
search space is limited to this set of candidates.
Such an approach, parsing followed by reranking,
is used in the Brown parser (Charniak and Johnson,
2005). The approach can be extended in order to
feed the reranker with the output of different parsers,
as shown by (Johnson and Ural, 2010; Zhang et al,
2009).
In this paper we are interested in applying rerank-
ing to dependency structures. The main reason is
that many linguistic constraints are straightforward
to implement on dependency structures, as, for ex-
ample, subcategorization frames or selectional con-
straints that are closely linked to the notion of de-
89
pendents of a predicate. On the other hand, depen-
dencies extracted from constituent parses are known
to be more accurate than dependencies obtained
from dependency parsers. Therefore the solution we
choose is an indirect one: we use a phrase-based
parser to generate n-best lists and convert them to
lists of dependency structures that are reranked. This
approach can be seen as trade-off between phrase-
based reranking experiments (Collins, 2000) and the
approach of Carreras et al (2008) where a discrimi-
native model is used to score lexical features repre-
senting unlabelled dependencies in the Tree Adjoin-
ing Grammar formalism.
Our architecture, illustrated in Figure 1, is based
on two steps. During the first step, a syntagmatic
parser processes the input sentence and produces n-
best parses as well as their probabilities. They are
annotated with a functional tagger which tags syn-
tagms with standard syntactic functions subject, ob-
ject, indirect object . . . and converted to dependency
structures by application of percolation rules. In the
second step, we extract a set of features from the
dependency parses and the associated probabilities.
These features are used to reorder the n-best list
and select a potentially more accurate parse. Syn-
tagmatic parses are produced by the implementation
of a PCFG-LA parser of (Attia et al, 2010), simi-
lar to (Petrov et al, 2006), a functional tagger and
dependency converter for the target language. The
reranking model is a linear model trained with an
implementation of the MIRA algorithm (Crammer et
al., 2006)1.
Charniak and Johnson (2005) and Collins (2000)
rerank phrase-structure parses and they also include
head-dependent information, in other words unla-
belled dependencies. In our approach we take into
account grammatical functions or labelled depen-
dencies.
It should be noted that the features we use are very
generic and do not depend on the linguistic knowl-
edge of the authors. We applied our method to En-
glish, the de facto standard for testing parsing tech-
nologies, and French which exhibits many aspects of
a morphologically rich language. But our approach
could be applied to other languages, provided that
1This implementation is available at https://github.
com/jihelhere/adMIRAble.
the resources ? treebanks and conversion tools ? ex-
ist.
(1) PCFG-LA n-best constituency parses
(2) Function annotation
(3) Conversion to dependency parses
(4) Feature extraction
(5) MIRA reranking
w
Final constituency & dependency parse
Input text
Figure 1: The parsing architecture: production of the n-
best syntagmatic trees (1) tagged with functional labels
(2), conversion to a dependency structure (3) and feature
extraction (4), scoring with a linear model (5). The parse
with the best score is considered as final.
The structure of the paper is the following: in
Section 2 we describe the details of our generative
parser and in Section 3 our reranking model together
with the features templates. Section 4 reports the re-
sults of the experiments conducted on the Penn Tree-
bank (Marcus et al, 1994) as well as on the Paris 7
Treebank (Abeill? et al, 2003) and Section 5 con-
cludes the paper.
2 Generative Model
The first part of our system, the syntactic analysis
itself, generates surface dependency structures in a
sequential fashion (Candito et al, 2010b; Candito
et al, 2010a). A phrase structure parser based on
Latent Variable PCFGs (PCFG-LAs) produces tree
structures that are enriched with functions and then
converted to labelled dependency structures, which
will be processed by the parse reranker.
90
2.1 PCFG-LAs
Probabilistic Context Free Grammars with Latent
Annotations, introduced in (Matsuzaki et al, 2005)
can be seen as automatically specialised PCFGs
learnt from treebanks. Each symbol of the gram-
mar is enriched with annotation symbols behaving
as subclasses of this symbol. More formally, the
probability of an unannotated tree is the sum of the
probabilities of its annotated counterparts. For a
PCFG-LA G, R is the set of annotated rules, D(t)
is the set of (annotated) derivations of an unanno-
tated tree t, and R(d) is the set of rules used in a
derivation d. Then the probability assigned by G to
t is:
PG(t) =
?
d?D(t)
PG(d) =
?
d?D(t)
?
r?R(d)
PG(r) (1)
Because of this alternation of sums and products
that cannot be optimally factorised, there is no ex-
act polynomial dynamic programming algorithm for
parsing. Matsuzaki et al (2005) and Petrov and
Klein (2007) discuss approximations of the decod-
ing step based on a Bayesian variational approach.
This enables cubic time decoding that can be fur-
ther enhanced with coarse-to-fine methods (Char-
niak and Johnson, 2005).
This type of grammars has already been tested
on a variety of languages, in particular English
and French, giving state-of-the-art results. Let us
stress that this phrase-structure formalism is not lex-
icalised as opposed to grammars previously used in
reranking experiments (Collins, 2000; Charniak and
Johnson, 2005). The notion of lexical head is there-
fore absent at parsing time and will become avail-
able only at the reranking step.
2.2 Dependency Structures
A syntactic theory can either be expressed with
phrase structures or dependencies, as advocated for
in (Rambow, 2010). However, some information
may be simpler to describe in one of the representa-
tions. This equivalence between the modes of repre-
sentations only stands if the informational contents
are the same. Unfortunately, this is not the case
here because the phrase structures that we use do
not contain functional annotations and lexical heads,
whereas labelled dependencies do.
This implies that, in order to be converted
into labelled dependency structures, phrase struc-
ture parses must first be annotated with functions.
Previous experiments for English and French as
well (Candito et al, 2010b) showed that a sequential
approach is better than an integrated one for context-
free grammars, because the strong independence hy-
pothesis of this formalism implies a restricted do-
main of locality which cannot express the context
needed to properly assign functions. Most func-
tional taggers, such as the ones used in the following
experiments, rely on classifiers whose feature sets
can describe the whole context of a node in order to
make a decision.
3 Discriminative model
Our discriminative model is a linear model
trained with the Margin-Infused Relaxed Algorithm
(MIRA) (Crammer et al, 2006). This model com-
putes the score of a parse tree as the inner product
of a feature vector and a weight vector represent-
ing model parameters. The training procedure of
MIRA is very close to that of a perceptron (Rosen-
blatt, 1958), benefiting from its speed and relatively
low requirements while achieving better accuracy.
Recall that parsing under this model consists in
(1) generating a n-best list of constituency parses
using the generative model, (2) annotating each of
them with function tags, (3) converting them to de-
pendency parses, (4) extracting features, (5) scoring
each feature vector against the model, (6) selecting
the highest scoring parse as output.
For training, we collect the output of feature ex-
traction (4) for a large set of training sentences and
associate each parse tree with a loss function that de-
notes the number of erroneous dependencies com-
pared to the reference parse tree. Then, model
weights are adjusted using MIRA training so that the
parse with the lowest loss gets the highest score. Ex-
amples are processed in sequence, and for each of
them, we compute the score of each parse according
to the current model and find an updated weight vec-
tor that assigns the first rank to the best parse (called
oracle). Details of the algorithm are given in the fol-
lowing sections.
91
3.1 Definitions
Let us consider a vector space of dimensionmwhere
each component corresponds to a feature: a parse
tree p is represented as a sparse vector ?(p). The
model is a weight vector w in the same space where
each weight corresponds to the importance of the
features for characterizing good (or bad) parse trees.
The score s(p) of a parse tree p is the scalar product
of its feature vector ?(p) and the weight vector w.
s(p) =
m?
i=1
wi?i(p) (2)
Let L be the n-best list of parses produced by the
generative parser for a given sentence. The highest
scoring parse p? is selected as output of the reranker:
p? = argmax
p?L
s(p) (3)
MIRA learning consists in using training sen-
tences and their reference parses to determine the
weight vector w. It starts with w = 0 and modifies
it incrementally so that parses closest to the refer-
ence get higher scores. Let l(p), loss of parse p,
be the number of erroneous dependencies (governor,
dependent, label) compared to the reference parse.
We define o, the oracle parse, as the parse with the
lowest loss in L.
Training examples are processed in sequence as
an instance of online learning. For each sentence,
we compute the score of each parse in the n-best
list. If the highest scoring parse differs from the or-
acle (p? 6= o), the weight vector can be improved.
In this case, we seek a modification of w ensuring
that o gets a better score than p? with a difference
at least proportional to the difference between their
loss. This way, very bad parses get pushed deeper
than average parses. Finding such weight vector
can be formulated as the following constrained opti-
mization problem:
minimize: ||w||2 (4)
subject to: s(o)? s(p?) ? l(o)? l(p?) (5)
Since there is an infinity of weight vectors that
satisfy constraint 5, we settle on the one with the
smallest magnitude. Classical constrained quadratic
optimization methods can be applied to solve this
problem: first, Lagrange multipliers are used to in-
troduce the constraint in the objective function, then
the Hildreth algorithm yields the following analytic
solution to the non-constrained problem:
w? = w + ? (?(o)? ?(p?)) (6)
? = max
[
0,
l(o)? l(p?)? [s(o)? s(p?)]
||?(o)? ?(p?)||2
]
(7)
Here, w? is the new weight vector, ? is an up-
date magnitude and [?(o)? ?(p?)] is the difference
between the feature vector of the oracle and that of
the highest scoring parse. This update, similar to
the perceptron update, draws the weight vector to-
wards o while pushing it away from p?. Usual tricks
that apply to the perceptron also apply here: (a) per-
forming multiple passes on the training data, and (b)
averaging the weight vector over each update2. Al-
gorithm 1 details the instructions for MIRA training.
Algorithm 1 MIRA training
for i = 1 to t do
for all sentences in training set do
Generate n-best list L from generative parser
for all p ? L do
Extract feature vector ?(p)
Compute score s(p) (eq. 2)
end for
Get oracle o = argminp l(p)
Get best parse p? = argmaxp s(p)
if p? 6= o then
Compute ? (eq. 7)
Update weight vector (eq. 6)
end if
end for
end for
Return average weight vector over updates.
3.2 Features
The quality of the reranker depends on the learning
algorithm as much as on the feature set. These fea-
tures can span over any subset of a parse tree, up to
the whole tree. Therefore, there are a very large set
of possible features to choose from. Relevant fea-
tures must be general enough to appear in as many
2This can be implemented efficiently using two weight vec-
tors as for the averaged perceptron.
92
parses as possible, but specific enough to character-
ize good and bad configurations in the parse tree.
We extended the feature set from (McDonald,
2006) which showed to be effective for a range of
languages. Our feature templates can be categorized
in 5 classes according to their domain of locality.
In the following, we describe and exemplify these
templates on the following sentence from the Penn
treebank, in which we target the PMOD dependency
between ?at? and ?watch.?
Probability Three features are derived from the
PCFG-LA parser, namely the posterior proba-
bility of the parse (eq. 1), its normalized prob-
ability relative to the 1-best, and its rank in the
n-best list.
Unigram Unigram features are the most simple as
they only involve one word. Given a depen-
dency between position i and position j of type
l, governed by xi, denoted xi
l
? xj , two fea-
tures are created: one for the governor xi and
one for the dependent xj . They are described
as 6-tuples (word, lemma, pos-tag, is-governor,
direction, type of dependency). Variants with
wildcards at each subset of tuple slots are also
generated in order to handle sparsity.
In our example, the dependency between
?looked? and ?at? generates two features:
[at, at, IN, G, R, PMOD] and
[looked, look, NN, D, L, PMOD]
And also wildcard features such as:
[-, at, IN, G, R, PMOD], [at,
-, IN, G, R, PMOD] ...
[at, -, -, -, -, PMOD]
This wildcard feature generation is applied to
all types of features. We will omit it in the re-
mainder of the description.
Bigram Unlike the previous template, bigram fea-
tures model the conjunction of the governor
and the dependent of a dependency relation,
like bilexical dependencies in (Collins, 1997).
Given dependency xi
l
? xj , the feature cre-
ated is (word xi, lemma xi, pos-tag xi, word
xj , lemma xj , pos-tag xj , distance3 from i to
j, direction, type).
The previous example generates the following
feature:
[at, at, IN, watch, watch, NN,
2, R, PMOD]
Where 2 is the distance between ?at? and
?watch?.
Linear context This feature models the linear con-
text between the governor and the dependent
of a relation by looking at the words between
them. Given dependency xi
l
? xj , for each
word from i + 1 to j ? 1, a feature is created
with the pos-tags of xi and xj , and the pos tag
of the word between them (no feature is create
if j = i + 1). An additional feature is created
with pos-tags at positions i? 1, i, i+ 1, j ? 1,
j, j +1. Our example yields the following fea-
tures:
[IN, PRP$, NN], and [VBD, IN,
PRP$, PRP$, NN, .].
Syntactic context: siblings This template and the
next one look at two dependencies in two con-
figurations. Given two dependencies xi
l
? xj
and xi
m
? xk, we create the feature (word,
lemma, pos-tag for xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, I, I, PRP,
at, at, IN, 1, 1, L, SBJ, R,
ADV]
Syntactic context: chains Given two dependencies
xi
l
? xj
m
? xk, we create the feature (word,
lemma, pos-tag of xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, at, at, IN,
watch, watch, NN, 1, 2, R, ADV,
3In every template, distance features are quantified in 7
classes: 1, 2, 3, 4, 5, 5 to 10, more.
93
R, PMOD]
It is worth noting that our feature templates only
rely on information available in the training set, and
do not use any external linguistic knowledge.
4 Experiments
In this section, we evaluate our architecture on
two corpora, namely the Penn Treebank (Marcus et
al., 1994) and the French Treebank (Abeill? et al,
2003). We first present the corpora and the tools
used for annotating and converting structures, then
the performances of the phrase structure parser alone
and with the discriminative reranker.
4.1 Treebanks and Tools
For English, we use the Wall Street Journal sections
of the Penn Treebank. We learn the PCFG-LA from
sections 02-214. We then use FUNTAG (Chrupa?a
et al, 2007) to add functions back to the PCFG-LA
analyses. For the conversion to dependency struc-
tures we use the LTH tool (Johansson and Nugues,
2007). In order to get the gold dependencies, we run
LTH directly on the gold parse trees. We use sec-
tion 22 for development and section 23 for the final
evaluation.
For French, we use the Paris 7 Treebank (or
French Treebank, FTB). As in several previous ex-
periments we decided to divide the 12,350 phrase
structure trees in three sets: train (80%), develop-
ment (10%) and test (10%). The syntactic tag set for
French is not fixed and we decided to use the one
described in (Candito and Crabb?, 2009) to be able
to compare this system with recent parsing results
on French. As for English, we learn the PCFG-LA
without functional annotations which are added af-
terwards. We use the dependency structures devel-
oped in (Candito et al, 2010b) and the conversion
toolkit BONSA?. Furthermore, to test our approach
against state of the art parsing results for French
we use word clusters in the phrase-based parser as
in (Candito and Crabb?, 2009).
For both languages we constructed 10-fold train-
ing data from train sets in order to avoid overfitting
the training data. The trees from training sets were
divided into 10 subsets and the parses for each sub-
set were generated by a parser trained on the other
4Functions are omitted.
9 subsets. Development and test parses are given by
a parser using the whole training set. Development
sets were used to choose the best reranking model.
For lemmatisation, we use the MATE lemmatiser
for English and a home-made lemmatiser for French
based on the lefff lexicon (Sagot, 2010).
4.2 Generative Model
The performances of our parser are summarised in
Figure 2, (a) and (b), where F-score denotes the Par-
seval F-score5, and LAS and UAS are respectively
the Labelled and Unlabelled Attachment Score of
the converted dependency structures6. We give or-
acle scores (the score that our system would get if
it selected the best parse from the n-best lists) when
the parser generates n-best lists of depth 10, 20, 50
and 100 in order to get an idea of the effectiveness
of the reranking process.
One of the issues we face with this approach is
the use of an imperfect functional annotator. For
French we evaluate the loss of accuracy on the re-
sulting dependency structure from the gold develop-
ment set where functions have been omitted. The
UAS is 100% but the LAS is 96.04%. For English
the LAS from section 22 where functions are omit-
ted is 95.35%.
From the results presented in this section we can
make two observations. First, the results of our
parser are at the state of the art on English (90.7%
F-score) and on French (85.7% F-score). So the
reranker will be confronted with the difficult task of
improving on these scores. Second, the progression
margin is sensible with a potential LAS error reduc-
tion of 41% for English and 40.2% for French.
4.3 Adding the Reranker
4.3.1 Learning Feature Weights
The discriminative model, i.e. template instances
and their weights, is learnt on the training set parses
obtained via 10-fold cross-validation. The genera-
tive parser generates 100-best lists that are used as
learning example for the MIRA algorithm. Feature
extraction produces an enormous number of fea-
tures: about 571 millions for English and 179 mil-
5We use a modified version of evalb that gives the ora-
cle score when the parser outputs a list of candidates for each
sentence.
6All scores are measured without punctuation.
94
(a) Oracle Scores on PTB dev set (b) Oracle Scores on FTB dev set
8990
9192
9394
9596
97
1 10 20 50 100Size of n-best list
UASLASF-score
Oracle s
core
86
88
90
92
94
86
88
90
92
94
1 10 20 50 100Size of n?best list
Oracle s
core
UASLASF-score
(c) Reranker scores on PTB dev set (d) Reranker scores on FTB dev set
899
091
929
394
1 10 20 50 100Size of n-best list
UASLASF-score
Reranke
d score
868
788
899
091
1 10 20 50 100Size of n?best list
UASLASF-score
Rerank
er score
Figure 2: Oracle and reranker scores on PTB and FTB data on the dev. set, according to the depth of the n-best.
lions for French. Let us remark that this large set of
features is not an issue because our discriminative
learning algorithm is online, that is to say it consid-
ers only one example at a time, and it only gives
non-null weights to useful features.
4.3.2 Evaluation
In order to test our system we first tried to eval-
uate the impact of the length of the n-best list over
the reranking predictions7. The results are shown in
Figure 2, parts (c) and (d).
For French, we can see that even though the LAS
and UAS are consistently improving with the num-
ber of candidates, the F-score is maximal with 50
candidates. However the difference between 50 can-
didates and 100 candidates is not statistically signifi-
cant. For English, the situation is simpler and scores
improve continuously on the three metrics.
Finally we run our system on the test sets for both
treebanks. Results are shown8 in Table 1 for En-
glish, and Table 2 for French. For English the im-
provement is 0.9% LAS, 0.7% Parseval F-score and
7The model is always trained with 100 candidates.
8F < 40 is the parseval F-score for sentences with less than
40 words.
0.8% UAS.
Baseline Reranker
F 90.4 91.1
F < 40 91.0 91.7
LAS 88.9 89.8
UAS 93.1 93.9
Table 1: System results on PTB Test set
For French we have improvements of 0.3/0.7/0.9.
If we add a template feature indicating the agree-
ment between part-of-speech provided by the PCFG-
LA parser and a part-of-speech tagger (Denis
and Sagot, 2009), we obtain better improvements:
0.5/0.8/1.1.
Baseline Reranker Rerank + MElt
F 86.6 87.3 87.4
F < 40 88.7 89.0 89.2
LAS 87.9 89.0 89.2
UAS 91.0 91.9 92.1
Table 2: System results on FTB Test set
95
4.3.3 Comparison with Related Work
We compare our results with related parsing re-
sults on English and French.
For English, the main results are shown in Ta-
ble 3. From the presented data, we can see that
indirect reranking on LAS may not seem as good
as direct reranking on phrase-structures compared to
F-scores obtained in (Charniak and Johnson, 2005)
and (Huang, 2008) with one parser or (Zhang et
al., 2009) with several parsers. However, our sys-
tem does not rely on any language specific feature
and can be applied to other languages/treebanks. It
is difficult to compare our system for LAS because
most systems evaluate on gold data (part-of-speech,
lemmas and morphological information) like Bohnet
(2010).
Our system also compares favourably with the
system of Carreras et al (2008) that relies on a more
complex generative model, namely Tree Adjoining
Grammars, and the system of Suzuki et al (2009)
that makes use of external data (unannotated text).
F LAS UAS
Huang, 2008 91.7 ? ?
Bohnet, 2010 ? 90.3 ?
Zhang et al 2008 91.4 ? 93.2
Huang and Sagae, 2010 ? ? 92.1
Charniak et al 2005 91.5 90.0 94.0
Carreras et al 2008 ? ? 93.5
Suzuki et al 2009 ? ? 93.8
This work 91.1 89.8 93.9
Table 3: Comparison on PTB Test set
For French, see Table 4, we compare our system
with the MATE parser (Bohnet, 2010), an improve-
ment over the MST parser (McDonald et al, 2005)
with hash kernels, using the MELT part-of-speech
tagger (Denis and Sagot, 2009) and our own lemma-
tiser.
We also compare the French system with results
drawn from the benchmark performed by Candito et
al. (2010a). The first system (BKY-FR) is close to
ours without the reranking module, using the Berke-
ley parser adapted to French. The second (MST-
FR) is based on MSTParser (McDonald et al, 2005).
These two system use word clusters as well.
The next section takes a close look at the models
of the reranker and its impact on performance.
F < 40 LAS UAS
This work 89.2 89.2 92.1
MATE + MELT ? 89.2 91.8
BKY-FR 88.2 86.8 91.0
MST-FR ? 88.2 90.9
Table 4: Comparison on FTB Test set
4.3.4 Model Analysis
It is interesting to note that in the test sets, the
one-best of the syntagmatic parser is selected 52.0%
of the time by the reranker for English and 34.3% of
the time for French. This can be explained by the
difference in the quantity of training data in the two
treebanks (four times more parses are available for
English) resulting in an improvement of the quality
of the probabilistic grammar.
We also looked at the reranking models, specifi-
cally at the weight given to each of the features. It
shows that 19.8% of the 571 million features have
a non-zero weight for English as well as 25.7% of
the 179 million features for French. This can be ex-
plained by the fact that for a given sentence, features
that are common to all the candidates in the n-best
list are not discriminative to select one of these can-
didates (they add the same constant weight to the
score of all candidates), and therefore ignored by the
model. It also shows the importance of feature engi-
neering: designing relevant features is an art (Char-
niak and Johnson, 2005).
We took a closer look at the 1,000 features of
highest weight and the 1,000 features of lowest
weight (negative) for both languages that represent
the most important features for discriminating be-
tween correct and incorrect parses. For English,
62.0% of the positive features are backoff features
which involve at least one wildcard while they are
85.9% for French. Interestingly, similar results hold
for negative features. The difference between the
two languages is hard to interpret and might be due
in part to lexical properties and to the fact that these
features may play a balancing role against towards
non-backoff features that promote overfitting.
Expectedly, posterior probability features have
the highest weight and the n-best rank feature has the
highest negative weight. As evidenced by Table 5,
96
en (+) en (-) fr (+) fr (-)
Linear 30.4 36.1 44.8 44.0
Unigram 20.7 16.3 9.7 8.2
Bigram 27.4 29.1 20.8 24.4
Chain 15.4 15.3 13.7 19.4
Siblings 5.8 3.0 10.8 3.6
Table 5: Repartition of weight (in percentage) in the
1,000 highest (+) and lowest (-) weighted features for En-
glish and French.
among the other feature templates, linear context oc-
cupies most of the weight mass of the 1,000 highest
weighted features. It is interesting to note that the
unigram and bigram templates are less present for
French than for English while the converse seems to
be true for the linear template. Sibling features are
consistently less relevant.
In terms of LAS performance, on the PTB test
set the reranked output is better than the baseline
on 22.4% of the sentences while the opposite is true
for 10.4% of the sentences. In 67.0% of the sen-
tences, they have the same LAS (but not necessar-
ily the same errors). This emphasises the difficulty
of reranking an already good system and also ex-
plains why oracle performance is not reached. Both
the baseline and reranker output are completely cor-
rect on 21.3% of the sentences, while PCFG-LA cor-
rectly parses 23% of the sentences and the MIRA
brings that number to 26%.
Figures 3 and 4 show hand-picked sentences for
which the reranker selected the correct parse. The
French sentence is a typical difficult example for
PCFGs because it involves a complex rewriting rule
which might not be well covered in the training
data (SENT ? NP VP PP PONCT PP PONCT PP
PONCT). The English example is tied to a wrong
attachment of the prepositional phrase to the verb
instead of the date, which lexicalized features of the
reranker handle easily.
5 Conclusion
We showed that using a discriminative reranker, on
top of a phrase structure parser, based on converted
dependency structures could lead to significant im-
provements over dependency and phrase structure
parse results. We experimented on two treebanks
for two languages, English and French and we mea-
sured the improvement of parse quality on three dif-
ferent metrics: Parseval F-score, LAS and UAS,
with the biggest error reduction on the latter. How-
ever the gain is not as high as expected by looking
at oracle scores, and we can suggest several possible
improvements on this method.
First, the sequential approach is vulnerable to cas-
cading errors. Whereas the generative parser pro-
duces several candidates, this is not the case of the
functional annotators: these errors are not amend-
able. It should be possible to have a functional tag-
ger with ambiguous output upon which the reranker
could discriminate. It remains an open question as
how to integrate ambiguous output from the parser
and from the functional tagger. The combination
of n-best lists would not scale up and working on
the ambiguous structure itself, the packed forest as
in (Huang, 2008), might be necessary. Another pos-
sibility for future work is to let the phrase-based
parser itself perform function annotation, but some
preliminary tests on French showed disappointing
results.
Second, designing good features, sufficiently gen-
eral but precise enough, is, as already coined
by Charniak and Johnson (2005), an art. More for-
mally, we can see several alternatives. Dependency
structures could be exploited more thoroughly using,
for example, tree kernels. The restricted number of
candidates enables the use of more global features.
Also, we haven?t used any language-specific syntac-
tic features. This could be another way to improve
this system, relying on external linguistic knowledge
(lexical preferences, subcategorisation frames, cop-
ula verbs, coordination symmetry . . . ). Integrating
features from the phrase-structure trees is also an op-
tion that needs to be explored.
Third this architecture enables the integration of
several systems. We experimented on French using a
part-of-speech tagger but we could also use another
parser and either use the methodology of (Johnson
and Ural, 2010) or (Zhang et al, 2009) which fu-
sion n-best lists form different parsers, or use stack-
ing methods where an additional parser is used as
a guide for the main parser (Nivre and McDonald,
2008).
Finally it should be noted that this system does not
rely on any language specific feature, and thus can
be applied to languages other that French or English
97
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
INof DTthis NNyear
NP
PP
NP
PP
VP
..
S
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
PP
INof DTthis NNyear
NP
PP
VP
..
S
depen
dency parse
s
synta
gmati
c
parse
s
Before reranking After reranking
Figure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the first
candidate of the n-best list suffered from an incorrect attachment.
SENT
NP VN PP
PONCT
NP
PONCTNPP NPP V VPP P AP DET ADJ PONCT P ADJADJ
SENT
NP VN PP
PONCT NP PONCTNPP NPP V VPP P AP P NC PONCT P ADJADJ NP
PP PP
NPP NPP V VPP P ADJ PONCT DET ADJ PONCT P ADJ PONCT NPP NPP V VPP P ADJ PONCT P NC PONCT P ADJ PONCTdepen
dency parses
syntag
matic parses
Before reranking After reranking
Figure 4: Sentence from the FTB for which the best parse according to baseline was incorrect, probably due to the
tendency of the PCFG-LA model to prefer rules with more support. The reranker selected the correct parse.
without re-engineering new reranking features. This
makes this architecture suitable for morphologically
rich languages.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
SEQUOIA (ANR-08-EMER-013).
References
Anne Abeill?, Lionel Cl?ment, and Toussenel Fran?ois,
2003. Treebanks, chapter Building a treebank for
French. Kluwer, Dordrecht.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceedings
of COLING.
M.-H. Candito and B. Crabb?. 2009. Improving Gen-
erative Statistical Parsing with Semi-Supervised Word
Clustering. In Proceedings of IWPT 2009.
M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010a. Benchmarking of Statistical Depen-
dency Parsers for French. In Proceedings of COL-
ING?2010.
Marie Candito, Beno?t Crabb?, and Pascal Denis. 2010b.
Statistical French Dependency Parsing : Treebank
Conversion and First Results. In Proceedings of
LREC2010.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming and the Perceptron for
Efficient, Feature-rich Parsing. In CONLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
98
Grzegorz Chrupa?a, Nicolas Stroppa, Josef van Genabith,
and Georgiana Dinu. 2007. Better training for func-
tion labeling. In Proceedings of RANLP, Borovets,
Bulgaria.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
ShalevShwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithm. Journal of Machine
Learning Research.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Pro-
ceedings PACLIC 23, Hong Kong, China.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings of
ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the ARPA Speech and Natural Lan-
guage Workshop.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsujii.
2005. Probabilistic CFG with Latent Annotations. In
Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Association for Computational Linguistics
(ACL).
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950?958.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In HLT-NAACL, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Geoffrey K. Pullum and Barbara C. Scholz. 2001. On the
distinction between model-theoretic and generative-
enumerative syntactic frameworks. In Logical Aspects
of Computational Linguistics.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In NAACL HLT.
Frank Rosenblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review.
Beno?t Sagot. 2010. The lefff, a freely available and
large-coverage lexicon for french. In Proceedings of
LREC 2010, La Valette, Malta.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 551?560. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of EMNLP.
99
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 95?102,
Dublin, Ireland, August 23 2014.
Automatically building a Tunisian Lexicon for Deverbal Nouns
Ahmed Hamdi N?ria Gala Alexis Nasr
Laboratoire d?Informatique Fondamentale de Marseille, Aix-Marseille Universit?
{ahmed.hamdi,nuria.gala,alexis.nasr}@lif.univ-mrs.fr
Abstract
The sociolinguistic situation in Arabic countries is characterized by diglossia (Ferguson, 1959) :
whereas one variant Modern Standard Arabic (MSA) is highly codified and mainly used for writ-
ten communication, other variants coexist in regular everyday?s situations (dialects). Similarly,
while a number of resources and tools exist for MSA (lexica, annotated corpora, taggers, parsers
. . . ), very few are available for the development of dialectal Natural Language Processing tools.
Taking advantage of the closeness of MSA and its dialects, one way to solve the problem of the
lack of resources for dialects consists in exploiting available MSA resources and NLP tools in
order to adapt them to process dialects. This paper adopts this general framework: we propose a
method to build a lexicon of deverbal nouns for Tunisian (TUN) using MSA tools and resources
as starting material.
1 Introduction
The Arabic language presents both a standard written form and a number of spoken variants (dialects).
While dialects differ from one country to another, sometimes even within the same country, the written
variety (Modern Standard Arabic, MSA), is the same for all the Arabic countries. Similarly, MSA is
highly codified, and used mainly for written communication and formal spoken situations (news, political
debates). Spoken varieties are used in informal daily discussions and in informal written communication
on the web (social networks, blogs and forums). Such unstandardized varieties differ from MSA with
respect to phonology, morphology, syntax and the lexicon. Linguistic resources (lexica, corpora) and
natural language processing (NLP) tools for such dialects (parsers) are very rare.
Different approaches are discussed in the litterature to cope with Arabic dialects processing. A gen-
eral solution is to build specific resources and tools. For example, (Maamouri et al., 2004) created a
Levantine annotated corpus (oral transcriptions) for speech recognition research. (Habash et al., 2005;
Habash and Rambow, 2006) proposed a system including a morphological analyzer and a generator for
Arabic dialects (MAGEAD) used for MSA and Levantine Arabic. (Habash et al., 2012) also built a
morphological analyzer for Egyptian Arabic that extends an existing resource, the Egyptian Colloquial
Arabic Lexicon. Other approaches take advantage of the special relation (closeness) that exists betweeen
MSA and dialects in order to adapt MSA resources and tools to dialects. To name a few, (Chiang et
al., 2006) used MSA treebanks to parse Levantine Arabic. (Sawaf, 2010) presented a translation system
for handling dialectal Arabic, using an algorithm to normalize spontaneous and dialectal Arabic into
MSA. (Salloum and Habash, 2013) developped a translation system pivoting through MSA from some
Arabic dialects (Levantine, Egyptian, Iraqi, and Gulf Arabic) to English. (Hamdi et al., 2013) proposed
a translation system between Tunisian (TUN) and MSA verbs using an analyser and a generator for both
variants.
Yet if the first kind of approach is more linguistically accurate because it takes into account specificities
of each dialect, building resources from scratch is costly and extremely time consuming. In this paper
we will thus adopt the second approach: we will present a method to automatically build a lexicon for
Tunisian deverbal nouns by exploiting available MSA resources as well as an existing MSA-TUN lexicon
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
95
for verbs (Boujelbane et al., 2013). We will use a root lexicon to generate possible deverbal nouns which
will be later filtered through a large MSA lexicon.
This work is part of a larger project that aims at ?translating? TUN to an approximative form of MSA
in order to use MSA NLP tools on the output of this translation process. The final lexicon for TUN
deverbal nouns will be integrated into a morphological and syntactic parser for TUN.
The paper is organized as follows: in section 2 we describe and compare some morphological aspects
of MSA and TUN, focusing on derivation. We then discuss in section 3 our approach to build a TUN lex-
icon for deverbal nouns from an existing MSA-TUN resource for verbs. Section 4 presents an evaluation
of the results obtained and section 5 proposes some solutions to increase the coverage of the lexicon.
2 Arabic Morphology
Arabic words are built following two kinds of morphological operations: templatic and affixational.
Functionally, both operations are used inflectionally or derivationally (Habash, 2010). In templatic mor-
phology, a root and a pattern combine to form a word stem. A root is a sequence of three, four or five
letters that defines an abstract notion while a pattern is a vocalized template which marks where the root
radicals are inserted. To give an example, by combining the root h

H
	
? f t H
1
with the verbal patterns
1a2a3 and ta1a22a3, two verbs are generated : (1) i

J
	
? fataH ?to open? and (2) i

J
	
?

K tafattaH ?to bloom?.
Derivation consists in replacing each digit of the pattern by the corresponding letter in the root.
Arabic verbs have ten basic triliteral patterns, which are conventionally noted with the Latin numbers
I, . . . , X. and two basic quadriliteral patterns (XI, XII) (Habash, 2010). A verb is the combination of a
root and a pattern.
Many deverbal nouns can be derived from verbs. Nine kind of deverbal nouns (1, 2, 3 ... 9) are defined
in Arabic (Al-Ghulayaini, 2010), each of them corresponds to a semantic relationship between the verb
and the deverbal noun (see table 1).
1 ??A
	
?? @ ??? @ active participle
2 ???
	
??
?
@ ??? @ passive participle
3 PY?? infinitive form
4
	
?A??
?
@ ??? @ noun of place
5
	
?A?
	
Q? @ ??? @ noun of time
6

??

B@ ??? @ noun of instrument
7

??D
.

??

?
	
?? analogous adjective
8 ?J


	
?
	
?

J? @ ??? @ comparative adjective
9

?
	
??AJ
.
?
?
@

?
	
?J


? exaggerate form
Table 1: Arabic deverbal nouns
From the verbs (1) i

J
	
? fataH ?to open? and (2) i

J
	
?

K tafattaH ?to bloom?, many deverbal nouns can be
derived, such as, l
