Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1037?1046, Dublin, Ireland, August 23-29 2014.
Automatic Discovery of Adposition Typology
Rishiraj Saha Roy
IIT Kharagpur
Kharagpur, India ? 721302.
rishiraj@cse.iitkgp.ernet.in
Rahul Katare
?
IIT Kharagpur
Kharagpur, India ? 721302.
rah.ykg@gmail.com
Niloy Ganguly
IIT Kharagpur
Kharagpur, India ? 721302.
niloy@cse.iitkgp.ernet.in
Monojit Choudhury
Microsoft Research India
Bangalore, India ? 560001.
monojitc@microsoft.com
Abstract
Natural languages (NL) can be classified as prepositional or postpositional based on the order of
the noun phrase and the adposition. Categorizing a language by its adposition typology helps in
addressing several challenges in linguistics and natural language processing (NLP). Understand-
ing the adposition typologies for less-studied languages by manual analysis of large text corpora
can be quite expensive, yet automatic discovery of the same has received very little attention till
date. This research presents a simple unsupervised technique to automatically predict the adpo-
sition typology for a language. Most of the function words of a language are adpositions, and we
show that function words can be effectively separated from content words by leveraging differ-
ences in their distributional properties in a corpus. Using this principle, we show that languages
can be classified as prepositional or postpositional based on the rank correlations derived from
entropies of word co-occurrence distributions. Our claims are substantiated through experiments
on 23 languages from ten diverse families, 19 of which are correctly classified by our technique.
1 Introduction
Adpositions form a subcategory of function words that combine with noun phrases to denote their se-
mantic or grammatical relationships with verbs, and sometimes other noun phrases. NLs can be neatly
divided into a few basic typologies based on the order of the noun phrase and its adposition. If the ad-
position is placed before the noun phrase, it is called a preposition. Postpositions and inpositions, on the
other hand, are adpositions that are placed after and inside noun phrases respectively. If prepositions are
predominantly used in the language, for example in English, Bulgarian and Russian, then the language
is said to be prepositional. Similarly, Japanese, Hindi and Turkish are some examples of postpositional
languages, which predominantly use postpositions. These two are the most commonly found adposition
typologies across the globe. Out of 1185 languages analyzed on the World Atlas of Language Structures
(WALS)
1
(Dryer and Haspelmath, 2011), there are 577 postpositional, 512 prepositional and only 8 in-
positional languages. There are a few (30 and 58 respectively) languages which use no or both kinds of
adpositions. The order of adpositions is strongly correlated with many other word order typologies. For
instance, postpositional languages usually have Object-Verb ordering, whereas prepositional languages
have Verb-Object ordering (Greenberg, 1963). Daum?e and Campbell (2007) present a statistical model
for automatically discovering such implications from a large typological database and discuss many other
typological implications involving adpositions.
Motivation. Knowledge of the typological characteristics of languages is not only of interest to lin-
guists, but also very useful in NLP for two main reasons. First, typological information, if appropriately
exploited while designing computational methods, can lead to very promising results in tasks like co-
reference resolution and machine translation (Haghighi and Klein, 2007; Moore and Quirk, 2007). Sec-
ond, as Bender and Langendoen (2010) have pointed out, in order to claim that a computational technique
?
This work was done during the author?s internship at Microsoft Research India.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://wals.info/
1037
is truly language independent, one must show its usefulness for languages having diverse typological fea-
tures. However, there is very little work on the automatic discovery of typological characteristics, primar-
ily because it is assumed that such information is readily available. However, Hammarstr?om et al. (2008)
argue that documenting a language and its typological features is a time consuming process for the lin-
guists and therefore, automatic methods for bootstrapping language description is a worthwhile effort
towards language preservation. Lewis and Xia (2008) mine inter-linearized data from the Web and infer
typological features for ?low-density? languages, i.e. languages represented in scarce quantities on the
Web. We argue that apart from documenting and understanding the typology of ?low-density? languages,
unsupervised discovery of adposition typology is also useful for analyzing undeciphered languages and
scripts, such as the Indus valley script (Rao et al., 2009) and the Cypro-Minoan syllabary (Palaima,
1989), as well as newly emerging languages, such as the language of Web search queries (Saha Roy et
al., 2012) or the Nicaraguan sign language (Meir et al., 2010). While the former cases are interesting
from historical and language change perspectives, the latter cases are useful for more practical reasons
(for example, improvement in query understanding leading to better Web search, and development of
interactive systems for deaf children).
Approach. In this work, we show that some simple word co-occurrence statistics, that can easily be
computed from any medium-sized text corpus, can be used as reliable predictors of adposition typology
of a language. These statistics have been arrived at based on two fundamental assumptions: (a) adposi-
tions constitute a large fraction of function words; and (b) the strict ordering between the adposition and
the noun phrase leads to differential co-occurrence characteristics on the left and right sides of the adpo-
sition. Therefore, if the function words of a language are automatically detected and the co-occurrence
statistics on the left and right of those words are appropriately analyzed, then it should be possible to
tell the prepositional languages apart from the postpositional ones. Specifically, we measure counts and
entropies of left, right and total (either side) co-occurrence distributions for each word. We show that left
co-occurrence statistics are better indicators of function words for prepositional languages, while right
co-occurrence statistics perform better for postpositional languages. Interestingly, the performance of
total co-occurrence statistics lie in between the two for both types of languages. Thus, the nature of the
difference in performances of left (or right) and total co-occurrences is likely to be indicative of the ad-
position typology of the language. We formalize this intuition to devise our test for adposition typology.
We demonstrate our technique on 23 languages from ten language families, of which 14 are prepositional
and 9 are postpositional. Our technique is able to consistently predict the correct adposition typology for
19 of these languages. The remaining four languages are highly inflectional and agglutinating in nature,
and hence not amenable to the present technique.
Organization. The rest of this paper is organized as follows. In Sec. 2, we present our method for
function word detection using word co-occurrence statistics, along with results showing the effectiveness
of such an approach. In Sec. 3, we propose our test for discovering the adposition typology of a language
based on correlations inferred from different co-occurrence statistics. Sec. 4 discusses experiments con-
ducted on diverse languages and inferences drawn from the observations. Finally, Sec. 5 summarizes our
contribution and indicates possible directions for future work.
2 Function Word Detection
Our method for the prediction of the adposition typology of a language relies on the facts that most
adpositions are function words, and the distributional properties of function words are very different
from those of content words. We exploit this difference to first formulate a method for extracting the
function words of a language from a corpus. We then proceed to use the same underlying principle
to automatically discover the adposition typology for languages, where we do not assume that the true
function word lists are available.
By function words, we refer to all the closed-class lexical items in a language, e.g., pronouns, de-
terminers, prepositions, conjunctions, interjections and other particles (as opposed to open-class items,
e.g., nouns, verbs, adjectives and adverbs). For the function word detection experiments, we shall look
at four languages from different families: English, Italian, Hindi and Bangla. English is a Germanic
1038
Language Corpus source S N V Function word list source F
English Leipzig Corpora
a
1M 19.8M 342157 Sequence Publishing
b
229
Italian -do- 1M 20M 434680 -do- 257
Hindi -do- 0.3M 5.5M 127428 Manually constructed by linguists and 481
augmented by extracting pronouns,
determiners, prepositions, conjunctions
and interjections from POS-tagged
corpora available at LDC
c
Bangla Crawl of Anandabazar Patrika
d
0.05M 16.2M 411878 -do- 510
a
http://corpora.informatik.uni-leipzig.de/download.html
b
http://www.sequencepublishing.com/academic.html\#function-words
c
http://www.ldc.upenn.edu (Catalog Nos. LDC2010T24 and LDC2010T16 for Hindi and Bangla respectively)
d
http://www.anandabazar.com/
Table 1: Details of NL corpora for function word detection experiments.
language, Italian is a Romanic language, and Hindi and Bangla belong to the Indo-Aryan family. English
and Italian are prepositional languages with subject-verb-object word order, while Hindi and Bangla are
postpositional, relatively free word order with preference for subject-object-verb. Therefore, any func-
tion word characterization strategy that works across these languages is expected to work for a large
variety of languages.
The details of the corpora used for these four languages are summarized in Table 1. M in the value
columns denotes million. S, N , V and F denote the numbers of all sentences, all words, unique words
(vocabulary size) and function words, respectively. We note that the Indian languages have almost twice
as many function words as compared to the European ones. This is due to morphological richness and
the existence of large numbers of modal and vector verbs.
Frequency is often used as an indicator for detecting function words, but the following factors affect
its robustness. If the corpus size is not large, many function words will not occur a sufficient num-
ber of times. For example, even though the and in will be very frequent in most English corpora,
meanwhile and off may not be so. As a result, if frequency is used as a function word detector with
small datasets, we will have a problem of low recall. In our experiments, we measure corpus size, N ,
as the total number of words present. If our language corpus is restricted, or sampled only from specific
domains, words specific to those domains will have high frequencies and will get detected as function
words. For example, the word government will be much more frequent in political news corpora than
although. The number of unique words in a corpus, or the vocabulary size, V , is a good indicator of
its diversity. For restricted domain corpora, V grows much more slowly withN than in a general domain
corpus.
We now introduce other properties of function words that may help in more robust detection. We
observe the following interesting characteristics about the syntactic distributions of function and content
words in NL, which can be summarized by the following two postulates.
Postulate I. Function words, in general, tend to co-occur with a larger number of distinct words than
content words. What can occur to the immediate left or right of a content word is much more restricted
than that in the case of function words. We hypothesize that even if a content word, e.g., government,
might have high frequency owing to the nature of the domain, there will only be a relatively fewer number
of words that can co-occur immediately after or before it. Therefore, the co-occurrence count may be a
more robust indicator of function words.
Postulate II. The co-occurrence patterns of function words are less likely to show bias towards spe-
cific words than those for content words. For example, and will occur beside several other words like
school, elephant and pipe with more or less equally distributed co-occurrence counts with all of
these words. In contrast, the co-occurrence distribution of school will be skewed, with more bias to-
wards to, high and bus than over, through and coast, with the list of words occurring beside
school also being much smaller than that for and.
In order to test Postulate I, we measure the number of distinct words that occur to the immediate left,
1039
right and either side of each unique word in the sub-sampled corpora. We shall refer to these statistics
as left, right and total co-occurrence counts (LCC, RCC and TCC) respectively. To test Postulate II, we
compute the entropy of the co-occurrence distributions of the words occurring to the left, right and either
side (i.e., total) contexts of a word w:
Entropy(w) = ?
?
t
i
? context(w)
p
t
i
|w
log
2
(p
t
i
|w
) (1)
where, context(w) is the set of all words co-occurring with w either in the left, the right or the total
contexts, and p(t
i
|w) is the probability of observing word t
i
in that specific context.
Context. In this paper, the left, right and total contexts of a word w respectively denote the imme-
diately preceding (one) word, immediately succeeding (one) word and both the immediately preceding
and the immediately succeeding words for w respectively, in sentences of the corpus. The definition of
context (i.e., whether it includes the preceding or the succeeding one or two or three words) will change
the absolute values of our results, but all the trends are expected to remain the same.
We shall refer to the co-occurrence entropies as left, right and total Co-occurrence Entropies (LCE,
RCE and TCE respectively). Due to their pivotal role in syntactically connecting the different words or
parts of a sentence to each other, we would expect LCC, RCC or TCC of function words to be higher
than that of content words due to Postulate I; similarly, due to Postulate II we can expect the LCE, RCE
or TCE to be higher for function words than for content words. If the LCE or LCC of a word w is high,
it means that a large number of distinct words can precede w in the language (additionally, almost with
equal probabilities for high LCE). Thus, predicting the previous word of w is difficult. Similarly, if RCE
or RCC of w is high, it means that a large number of words can follow w in the language (additionally,
almost with equal probabilities for high RCE). Thus, predicting the next word of w is difficult. A high
TCE for a word implies that the word can be preceded and followed by a large number of words, making
the prediction of either the next or the previous word (or both) for w difficult.
2.1 Experiments and Results
In our approach, the output is a ranked list of words sorted in descending order of the corresponding
property. Here we adopt a popular metric, Average Precision (AP), used in Information Retrieval (IR)
for the evaluation of ranked lists. More specifically, let w
1
, w
2
, . . . , w
n
be a ranked list of words sorted
according to some corpus statistic, say, frequency. Thus, if i < j, then frequency of w
i
is greater than
the frequency of w
j
. Precision at rank k, denoted by P@k, is defined as
P@k =
1
k
k
?
i=1
f(w
i
) (2)
where, f(w
i
) is one if w
i
is a function word, and is zero otherwise. This function can be computed
based on the gold standard lists of function words. Subsequently, average precision at rank n, denoted
by AP@n, is defined as
AP@n =
1
n
n
?
k=1
P@k (3)
AP@n is a better metric than P@k because P@k is insensitive to the rank at which function words
occur in the list. In our experiments, we compute AP@n averaged overN corpus sub-samples, which is
given by
1
N
?
N
r=1
(AP@n)
r
where (AP@n)
r
is the AP@n for the r
th
sub-sample. We note that there are
other metrics popularly used in IR, e.g. the Normalized Discounted Cumulative Gain (nDCG). However,
these are more sensitive to the correctness of the top few items in the list and hence, are not suitable for
us. Knowing that the number of function words in a popular NL is at least 200 (Table 1), we compute
AP@200 with respect to the gold standard lists of function words for all our experiments.
1040
Language Typology Fr LCC LCE TCC TCE RCC RCE
English Prepositional 0.663 0.702
?
0.729
?
0.684
?
0.679
?
0.637 0.527
Italian Prepositional 0.611 0.639
?
0.645
?
0.636
?
0.620 0.606 0.601
Hindi Postpositional 0.682 0.614 0.510 0.698
?
0.694
?
0.716
?
0.713
?
Bangla Postpositional 0.648 0.684
?
0.691
?
0.730
?
0.763
?
0.741
?
0.757
?
The four highest values in a row are marked in boldface. Statistically significant improvement over frequency is marked by
?
.
The paired t-test was performed and the null hypothesis was rejected if p-value < 0.05.
Table 2: AP@200 for all indicators, averaged over 200 (N , V ) pairs for each language.
(a) (b)
Figure 1: (Colour online) Performance of co-occurrence statistics for (a) English, and (b) Hindi, with
respect to frequency for AP@200 with variation in N.
We now sort the list of all words in descending order of each of the seven indicators. We then compute
AP@200 for these seven lists. To bring out the performance difference of each of the six co-occurrence
features with respect to frequency, we plot (in Figs. 1 and 2) the following measure against N :
Value plotted =
Metric for indicator ? Metric for Fr
Metric for Fr
(4)
The x-axis can now be thought of as representing the performance of frequency. In Fig. 1, for a
particular N , the data points were averaged over all (N , V ) pairs (we had 20 (N , V ) pairs for each N ).
For Fig. 2, V was binned into five zones, and for each zone, the AP was averaged over all corresponding
(N , V ) pairs. The observations (both N and V variation) for French and Italian were similar to that of
English, while those for Hindi and Bangla were similar to each other. Table 2 reports AP values for all
statistics for the four languages. From Table 2, we see that for all the languages, AP for some of the
co-occurrence statistics are higher than AP obtained using frequency.
Regular improvements over frequency. From the plots and Table 2, it is evident that some of the
co-occurrence statistics consistently beat frequency as indicators. In fact, as evident from Figs. 1 and
2, use of co-occurrence statistics results in systematic improvement over frequency with variations in N
and V , and hence, are very robust indicators. Among the co-occurrence statistics, both entropies and
counts are observed to have comparable performance.
3 Detection of Adposition Typology
From the results presented above, we observe that the best function word indicator depends upon lan-
guage typology. Interestingly, while LCE and LCC are the best indicators of function words for the two
prepositional languages of English and Italian, RCE and RCC perform better for Hindi and Bangla, the
postpositional languages. This observation can be explained as follows. For a prepositional language,
the function words, which are often the adpositions, precede the content word it is linked to. Therefore,
the words following an adposition (or a function word) mark the beginnings of syntactic units such as
1041
(a) (b)
Figure 2: (Colour online) Performance of co-occurrence statistics for (a) Italian, and (b) Bangla, with
respect to frequency for AP@200 with variation in V.
noun phrases and are typically restricted to certain syntactic categories. However, the words that precede
the adpositions have no or much weaker syntactic restrictions. Hence, the LCE and LCC are higher and
consequently better and more robust indicators of function words for prepositional languages. For very
similar reasons, the RCE and RCC are better indicators of function words for postpositional languages.
Importantly, we observe that TCE and TCC seem to be reasonably good predictors of function words
irrespective of the typology, with performances lying in between the poorest indicators (RCE and RCC
for prepositional languages and LCE and LCC for postpositional languages) and the best indicators (LCE
and LCC for prepositional languages and RCE and RCC for postpositional languages) for all the four
languages. This makes them safe indicators to rely on when not much is known about the language
syntax. In fact, the philosophy of this research is to be of assistance in these less-known cases. Thus,
co-occurrence statistics have potential in predicting the adposition typology of a new language, which
we leverage in this research.
We now describe our intuition and method behind our tests for automatically detecting the adposition
typology of a language. In this context, we do not know the actual function words or adpositions of the
language under consideration. Let us take the three lists of the top 200 words from a language corpus,
sorted according to the statistics TCE, LCE and RCE. For a prepositional language, we can expect to see
the highest number of function words towards the top of the list when sorted according to LCE, followed
by the number of function words towards the top of the TCE list. The RCE list would be expected to be
the poorest in this regard. Thus, we expect a higher overlap between the top 200 word lists for TCE and
LCE, than for TCE and RCE. The reverse is expected to be true for postpositional languages. Similar
arguments can be presented for LCC, RCC and TCC as well. We quantify this correlation between
the lists using two different statistics ? the Pearson?s correlation coefficient (r) and Spearman?s Rank
Correlation Coefficient (?).
For computing Pearson?s coefficients, we use the actual values of the distributional statistics, while for
Spearman?s rank coeffcients, we use the ranks of the words. Let r(TL) and ?(TL) respectively denote
the Pearson?s and Spearman?s Rank correlation coefficients of the lists sorted by TCE and LCE (or TCC
and LCC), and similarly, let r(TR) and ?(TR) denote the respective coefficients for the lists sorted by
TCE and RCE (or TCC and RCC).
Postulate. For a prepositional language, the top-200 words by LCE will have a higher correlation with
the top-200 words by TCE than the corresponding correlation of RCE with TCE. For a postpositional
language, the top-200 words by RCE will have a higher correlation with the top-200 words by TCE.
Formally, for prepositional languages, r(TL) > r(TR), and ?(TL) > ?(TR), while for postpositional
languages r(TL) < r(TR) and ?(TL) < ?(TR).
1042
Language Family ?(TL) ?(TR) ?(Diff .) Predicted True
Bulgarian Slavic (Indo-European) 0.726 0.518 0.208 Pre- Pre- (Scatton, 1984)
Danish Germanic (Indo-European) 0.621 0.495 0.126 Pre- Pre- (Allan et al., 1995)
Dutch Germanic (Indo-European) 0.662 0.204 0.458 Pre- Pre- (Shetter, 1958)
English Germanic (Indo-European) 0.461 0.436 0.025 Pre- Pre- (Selkirk, 1996)
German Germanic (Indo-European) 0.563 0.517 0.046 Pre- Pre- (Lederer, 1969)
Italian Romance (Indo-European) 0.730 0.456 0.274 Pre- Pre- (Sauer, 1891)
Macedonian Slavic (Indo-European) 0.692 0.488 0.205 Pre- Pre- (Friedman, 1993)
Norwegian Germanic (Indo-European) 0.619 0.600 0.019 Pre- Pre- (Olson, 1901)
Polish Slavic (Indo-European) 0.798 0.554 0.243 Pre- Pre- (Bielec, 1998)
Russian Slavic (Indo-European) 0.743 0.652 0.091 Pre- Pre- (Borras and Christian, 1959)
Slovenian Slavic (Indo-European) 0.701 0.668 0.032 Pre- Pre- (Priestly, 1993)
Swedish Germanic (Indo-European) 0.663 0.525 0.138 Pre- Pre- (Holmes and Hinchliffe, 1994)
Ukrainian Slavic (Indo-European) 0.785 0.714 0.070 Pre- Pre- (Stechishin, 1958)
Gujarati Indic (Indo-European) 0.540 0.581 ?0.041 Post- Post- (Cardona, 1965)
Hindi Indic (Indo-European) 0.529 0.731 ?0.202 Post- Post- (McGregor, 1977)
Japanese Japanese (Japanese) 0.429 0.626 ?0.197 Post- Post- (Hinds, 1986)
Nepali Indic (Indo-European) 0.495 0.719 ?0.224 Post- Post- (Bandhu, 1973)
Tamil Southern Dravidian (Dravidian) 0.748 0.805 ?0.057 Post- Post- (Asher, 1982)
Turkish Turkic (Altaic) 0.531 0.769 ?0.238 Post- Post- (Underhill, 1976)
Estonian Finnic(Uralic) 0.790 0.733 0.057 Pre- Post- (Tauli, 1983)
Finnish Finnic (Uralic) 0.671 0.656 0.015 Pre- Post- (Sulkala and Karjalainen, 1992)
Hungarian Ugric (Uralic) 0.457 0.329 0.128 Pre- Post- (Kenesei et al., 1998)
Lithuanian Baltic (Indo-European) 0.715 0.724 ?0.009 Post- Pre- (Dambriunas et al., 1966)
Misclassified languages are marked in gray.
Table 3: Detecting adposition typology using Spearman?s rank correlation coefficients on entropy lists.
4 Experimental Results and Observations
In this section, we first present our datasets, followed by detailed experiments on adposition typology
detection and inferences drawn from the observations.
4.1 Datasets
For all our typology detection experiments, we use datasets from the publicly available Leipzig Corpora
2
.
We selected 23 languages from various families that are typologically diverse. A (300, 000)-sentence
corpora was used for all the languages so as to ensure similar-sized corpora for all the languages (many
languages do not have a larger corpus). All languages examined have been listed in Table 3, along with
their families and true adposition typologies (accompanied by appropriate references).
4.2 Experiments and Results
We extracted the top-200 words by TCE, LCE and RCE, and TCC, LCC and RCC from the 300k-
sentence corpora. We then computed r(TL), ?(TL), r(TR) and ?(TR), for both entropies and counts.
As per our postulate, if ?(TL)? ?(TR) (= ?(Difference)) is positive, the language is prepositional; if it
is negative, the language is postpositional. The same can be expected for r(Difference).
The performance of ? as a predictor was found to be better than r. Results when the entropy lists are
used are presented in Table 3. For only 4 out of 23 languages, the typology predictions are incorrect. We
observe that three of these misclassified languages are from the Uralic family that are synthetic in nature
characterized by extensive regular agglutination of modifiers to verbs, nouns, adjectives and numerals.
2
http://corpora.informatik.uni-leipzig.de/download.html
1043
Corpus Size Entropy lists (r) Entropy lists (?) Count lists (r) Count lists (?)
10k 17/23 21/23 17/23 13/23
100k 17/23 19/23 18/23 13/23
300k 16/23 19/23 16/23 13/23
The highest value in a row is marked in boldface.
Table 4: Correct predictions by strategy with varying factors.
The average number of characters in words of these languages were found to be in the relatively higher
range of nine to eleven. Thus, function words, especially the adpositions, seldom occur as free words in
these languages and hence our method cannot capture the distributional characteristics of the adpositions.
It is worthwhile to note that the method can predict the correct typology for other languages that employ
agglutination to a lesser degree (Bulgarian, Dutch, German, Tamil and Turkish). Lithuanian, though not
synthetic, is a highly inflectional language and therefore, instead of adpositions it makes extensive use
of case-markers. With ?(Difference) very close to zero, our prediction for Lithuanian is inconclusive.
A note on synthetic languages: For synthetic languages, the difference between the two rank corre-
lation coefficients are close to zero, which provides us with a direct way to identify them. One could
also employ unsupervised morphological analysis to automatically identify and segment affixes, which
will provide deeper insight into the morpho-syntactic properties of the language. Nevertheless, affixes
(like infixes in Arabic or case-marking suffixes in Bangla) are technically not considered as adposi-
tions, and therefore, they do not really determine the adposition typology. Languages are divided into
four classes according to their adposition usage: prepositional, postpositional, ambi-positional (use both
types) and adposition-less (use none). Thus, as far as adposition typology is concerned, it suffices to
identify whether a language is primarily adposition-less, which our technique is potentially capable of
doing (we demonstrate it for four languages, but we believe more experimentation is needed to establish
this claim). Note that a language may use case-marking affixes along with adpositions. In such cases our
method is able to correctly determine the typology, as demonstrated for Bangla.
4.3 Experimental variations
We repeated the above experiments with lists of TCC, LCC and RCC instead of the co-occurrence en-
tropies. The performance was found to be poorer than the entropy lists, with nine classification errors
instead of the earlier four. Performance of these lists by co-occurrence counts was found to be poorer in
other cases as well (Table 4). We systematically experimented with r instead of ?. To test the perfor-
mance of our method with even smaller corpora, we sub-sampled 3 and 30 corpora containing 100k and
10k sentences respectively from the 300k corpus. We computed the correlation between the original top
200 words obtained using TCE (or TCC) from the 300k corpus and the corresponding LCE and RCE (or
LCC and RCC) lists obtained from the smaller corpora. For a given language, the mean of ?(Difference)
and r(Difference) were used to predict the typology (observed standard deviations were very low, of the
order of 10
?3
). The results of these experiments are summarized in Table 4. Out of 23 languages, 21 and
19 were correctly classified by ? for corpora of 10k and 100k sentences. The corresponding number for
r are 18 for both 10k and 100k, and 17 for 300k corpora. Thus, the sensitivity of the method improves
with slightly smaller corpora, provided that the TCE list, which is being used as a proxy for the gold
standard function word list, is computed from a slightly larger corpus. Finally, we note that using Spear-
man?s rank correlation coefficient with lists constructed by co-occurrence entropy consistently produces
the best results.
5 Conclusions and Future Work
Knowing the adposition typology of a natural language can be useful in several NLP tasks, and can be
especially useful in understanding new or undeciphered languages. In this research, we have taken one
1044
of the first steps towards automatic discovery of adposition typology. First, we have shown, through ex-
periments on two prepositional and two postpositional languages, that function words can be effectively
extracted from medium-sized corpora using word co-occurrence statistics, and such measures usually
outperform simple frequency when used for the same task. Next, difference in behavior of various co-
occurrence statistics for prepositional and postpositional languages has been exploited to devise a simple
strategy for predicting the adposition typology of a language. Simple differences of rank correlation
coefficients among total, left and right word co-occurrence entropies have been shown to be potent sig-
nals towards automatic discovery of adposition and noun phrase typology in a language. Results show
sufficient promise through an extensive evaluation over 23 languages.
We ventured into this study while solving a very practical and important problem: query understanding
through analysis of the structure of Web search queries. While queries seem to have an emergent syntax,
it is unclear whether they have function words, and if so what role they play in determining the query
grammar. To this end, we conducted the current study. Thus, we envisage that this technique will be
applicable for any such emergent linguistic system, such as pidgins, creoles, and computer mediated
communications (CMCs) like SMS and chats, where there is a large amount of text data available but
the grammar is emerging or yet to be analyzed. Other examples are that of undeciphered languages,
e.g., Indus valley language or script. In fact, our method can be applied to any system of symbols, be it
linguistic or non-linguistic, such as musical note sequences.
As future work, it is important to improve our prediction accuracy further, while including more
languages in the experimental setup. Combining clues from other sources to resolve uncertain cases
and devising better ways of choosing corpus size and significance thresholds are some of the avenues in
which effort may be channelized. Extending our approach to a morpheme-level analysis would also be
beneficial in dealing with highly agglutinative and inflectional languages.
Acknowledgements
The first author was supported by Microsoft Corporation and Microsoft Research India under the Mi-
crosoft Research India PhD Fellowship Award. We would like to thank Amritayan Nayak, Walmart
eCommerce (who was then a student of IIT Kharagpur working as an intern at Microsoft Research India)
for contributing to some of the early experiments related to this study.
References
Robin Allan, Philip Holmes, and Tom Lundsk?r-Nielsen. 1995. Danish: A Comprehensive Grammar. Routledge,
London.
R. E. Asher. 1982. Tamil, volume 7 of Lingua Descriptive Studies. North-Holland, Amsterdam.
Churamani Bandhu. 1973. Clause patterns in nepali. In Austin Hale, editor, Clause, sentence, and discourse
patterns in selected languages of Nepal 2, volume 40.2 of Summer Institute of Linguistics Publications in
Linguistics and Related Fields, pages 1?79. Summer Institute of Linguistics of the University of Oklahoma,
Norman.
Emily M. Bender and D Terence Langendoen. 2010. Computational linguistics in support of linguistic theory.
Linguistic Issues in Language Technology, 3(1).
Dana Bielec. 1998. Polish: An Essential Grammar. Routledge, London.
F. M. Borras and R. F. Christian. 1959. Russian Syntax: Aspects of Modern Russian Syntax and Vocabulary.
Clarendon Press, Oxford.
George Cardona. 1965. A Gujarati Reference Grammar. The University of Pennsylvania Press, Philadelphia.
Leonardas Dambriunas, Antanas Klimas, and William R. Schmalstieg. 1966. Introduction to Modern Lithuanian.
Franciscan Fathers Press, Brooklyn.
Hal Daum?e and Lyle Campbell. 2007. A bayesian model for discovering typological implications. In Annual
Meeting of the Association for Computational Linguistics, pages 65?72.
1045
Matthew S. Dryer and Martin Haspelmath, editors. 2011. The World Atlas of Language Structures Online. Max
Planck Digital Library, Munich, 2011 edition.
Victor A. Friedman. 1993. Macedonian. In Bernard Comrie and Greville G. Corbett, editors, The Slavonic
Languages, pages 249?305. Routledge, London / New York.
Joseph H. Greenberg. 1963. Some universals of grammar with particular reference to the order of meaningful
elements. Universals of language, 2:73?113.
Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In
Annual meeting-Association for Computational Linguistics, pages 848?855.
Harald Hammarstr?om, Christina Thornell, Malin Petzell, and Torbj?orn Westerlund. 2008. Bootstrapping language
description: The case of mpiemo (bantu a, central african republic). In Proceedings of the Sixth international
conference on Language Resources and Evaluation, LREC ?08.
John Hinds. 1986. Japanese, volume 4 of Croom Helm Descriptive Grammars. Croom Helm, Routledge, London.
Philip Holmes and Ian Hinchliffe. 1994. Swedish: A Comprehensive Grammar. Routledge, London.
Istvn Kenesei, Robert M. Vago, and Anna Fenyvesi. 1998. Hungarian. Descriptive Grammars. Routledge, London
/ New York.
Herbert Lederer. 1969. Reference Grammar of the German Language. Charles Scribner?s Sons, New York. Based
on Grammatik der Deutschen Sprache, by Doras Schulz and Heinz Griesbach.
W. Lewis and F. Xia. 2008. Automatically identifying computationally relevant typological features. In Proc. of
the Third International Joint Conference on Natural Language Processing (IJCNLP-2008).
R. S. McGregor. 1977. Outline of Hindi Grammar. Oxford University Press, Delhi. 2nd edition.
Irit Meir, Wendy Sandler, Carol Padden, and Mark Aronoff. 2010. Emerging sign languages. Oxford handbook of
deaf studies, language, and education, 2:267?280.
R. C. Moore and C. Quirk. 2007. An iteratively-trained segmentation-free phrase translation model for statistical
machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 112?
119. Association for Computational Linguistics.
Julius E. Olson. 1901. Norwegian Grammar and Reader. Scott, Foresman and Co, Chicago.
Thomas G. Palaima. 1989. Cypro-minoan scripts: Problems of historical context in problems in decipherment.
Biblioth`eque des Cahiers de l?Institut de Linguistique de Louvain, 49:121?187.
T. M. S. Priestly. 1993. Slovene. In Bernard Comrie and Greville G. Corbett, editors, The Slavonic Languages,
pages 388?451. Routledge, London.
R.P.N. Rao, N. Yadav, M.N. Vahia, H. Joglekar, R. Adhikari, and I. Mahadevan. 2009. Entropic evidence for
linguistic structure in the Indus script. Science, 324(5931):1165?1165.
Rishiraj Saha Roy, Monojit Choudhury, and Kalika Bali. 2012. Are web search queries an evolving protolan-
guage? In Proceedings of the 9th International Conference on the Evolution of Language, Evolang 9, pages
304?311, Singapore. World Scientific Publishing Co.
Charles Marquard Sauer. 1891. Italian Conversational Grammar. Julius Gross, Heidelberg.
Ernest A. Scatton. 1984. A Reference Grammar of Modern Bulgarian. Slavica Publishers, Columbus, Ohio.
Elizabeth Selkirk. 1996. The Prosodic Structure of Function Words. In James L. Morgan and Katherine Demuth,
editors, Signal to Syntax: Bootstrapping from Speech to Grammar in Early Acquisition. Routledge.
William Z. Shetter. 1958. Introduction to Dutch. Martinus Nijhoff, The Hague.
J. W. Stechishin. 1958. Ukrainian Grammar. Trident Press, Winnipeg.
Helena Sulkala and Merja Karjalainen. 1992. Finnish. Descriptive Grammar Series. Routledge, London.
Valter Tauli. 1983. Standard Estonian Grammar. Volume 2: Syntax, volume 14 of Studia Uralica et Altaica
Upsaliensia. Almqvist and Wiksell, Uppsala.
Robert Underhill. 1976. Turkish Grammar. Massachusetts Institute of Technology (MIT) Press, Cambridge.
1046
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1713?1722,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing
Reveals Turker Biases in Query Segmentation
Rohan Ramanath?
R. V. College of Engineering
Bangalore, India
ronramanath@gmail.com
Monojit Choudhury
Microsoft Research Lab India
Bangalore, India
monojitc@microsoft.com
Kalika Bali
Microsoft Research Lab India
Bangalore, India
kalikab@microsoft.com
Rishiraj Saha Roy?
Indian Institute of Technology Kharagpur
Kharagpur, India
rishiraj@cse.iitkgp.ernet.in
Abstract
Query segmentation, like text chunking,
is the first step towards query understand-
ing. In this study, we explore the effec-
tiveness of crowdsourcing for this task.
Through carefully designed control ex-
periments and Inter Annotator Agreement
metrics for analysis of experimental data,
we show that crowdsourcing may not be a
suitable approach for query segmentation
because the crowd seems to have a very
strong bias towards dividing the query into
roughly equal (often only two) parts. Sim-
ilarly, in the case of hierarchical or nested
segmentation, turkers have a strong prefer-
ence towards balanced binary trees.
1 Introduction
Text chunking of Natural Language (NL) sentences
is a well studied problem that is an essential pre-
processing step for many NLP applications (Ab-
ney, 1991; Abney, 1995). In the context of Web
search queries, query segmentation is similarly the
first step towards analysis and understanding of
queries (Hagen et al, 2011). The task in both the
cases is to divide the sentence or the query into
contiguous segments or chunks of words such that
the words from a segment are related to each other
more strongly than words from different segments
(Bendersky et al, 2009). It is typically assumed
that the segments are structurally and semantically
coherent and, therefore, the information contained
in them can be processed holistically.
?The work was done during author?s internship at Mi-
crosoft Research Lab India.
? This author was supported by Microsoft Corporation
and Microsoft Research India under the Microsoft Research
India PhD Fellowship Award.
f Pipe representation Boundary var.
4 apply | first aid course | on line 1 0 0 1 0
3 apply first aid course | on line 0 0 0 1 0
2 apply first aid | course on line 0 0 1 0 0
1 apply | first aid | course | on line 1 0 1 1 0
Table 1: Example of flat segmentation by Turkers.
f is the frequency of annotations; segment bound-
aries are represented by |.
f Bracket representation Boundary var.
4 ((apply first) ((aid course) (on line))) 0 2 0 1 0
2 (((apply (first aid)) course) (on line)) 1 0 2 3 0
2 ((apply ((first aid) course)) (on line)) 2 0 1 3 0
1 (apply (((first aid) course) (on line))) 3 0 1 2 0
1 ((apply (first aid)) (course (on line))) 1 0 2 1 0
Table 2: Example of nested segmentation by Turk-
ers. f is the frequency of annotations.
A majority of work on query segmentation re-
lies on manually segmented queries by human ex-
perts for training and evaluation of segmentation
algorithms. These are typically small datasets and
even with detailed annotation guidelines and/or
close supervision, low Inter Annotator Agreement
(IAA) remains an issue. For instance, Table 1 il-
lustrates the variation in flat segmentation by 10
annotators. This confusion is mainly because the
definition of a segment in a query is ambiguous
and of an unspecified granularity. This is fur-
ther compounded by the fact that other than eas-
ily recognizable and agreed upon segments such as
Named Entities or Multi-Word Expressions, there
is no established notion of linguistic grouping such
as phrases and clauses in a query.
Although there is little work on the use of
crowdsourcing for query segmentation (Hagen et
al., 2011; Hagen et al, 2012), the idea that the
1713
crowd could be a potential (and cheaper) source
for reliable segmentation seems a reasonable as-
sumption. The need for larger datasets makes this
an attractive proposition. Also, a larger number
of annotations could be appropriately distilled to
obtain better quality segmentations.
In this paper we explore crowdsourcing as an
option for query segmentation through experi-
ments designed using Amazon Mechanical Turk
(AMT)1. We compare the results against gold
datasets created by trained annotators. We ad-
dress the issues pertaining to disagreements due to
both ambiguity and granularity and attempt to ob-
jectively quantify their role in IAA. To this end,
we also conduct similar annotation experiments
for NL sentences and randomly generated queries.
While queries are not as structured as NL sen-
tences they are not simply a set of random words.
Thus, it is necessary to compare query segmenta-
tion to the u?ber-structure of NL sentences as well
as the unter-structure of random n-grams. This has
important implications for understanding any in-
herent biases annotators may have as a result of
the apparent lack of structure of the queries.
To quantify the effect of granularity on segmen-
tation, we also ask annotators to provide hierar-
chical or nested segmentations for real and ran-
dom queries, as well as sentences. Following
Abney?s (1992) proposal for hierarchical chunk-
ing of NL, we ask the annotators to group ex-
actly two words or segments at a time to recur-
sively form bigger segments. The concept is illus-
trated in Fig. 1. Table 2 shows annotations from
10 Turkers. It is important to constrain the join-
ing of exactly two segments or words at a time
to avoid the issue of fuzziness in granularity. We
shall refer to this style of annotation as Nested
segmentation, whereas the non-hierarchical non-
constrained chunking will be referred to as Flat
segmentation.
Through statistical analysis of the experimen-
tal data we show that crowdsourcing may not be
the best practice for query segmentation, not only
because of ambiguity and granularity issues, but
because there exist very strong biases amongst an-
notators to divide a query into two roughly equal
parts that result in misleadingly high agreements.
As a part of our analysis framework, we introduce
a new IAA metric for comparison across flat and
nested segmentations. This versatile metric can be
1https://www.mturk.com/mturk/welcome
3
2
1
apply 0
first aid
course
0
on line
Figure 1: Nested Segmentation: Illustration.
readily adapted for measuring IAA for other lin-
guistic annotation tasks, especially when done us-
ing crowdsourcing.
The rest of the paper is organized as follows.
Sec 2 provides a brief overview of related work.
Sec 3 describes the experiment design and proce-
dure. In Sec 4, we introduce a new metric for IAA,
that could be uniformly applied across flat and
nested segmentations. Results of the annotation
experiments are reported in Sec 5. In Sec 6, we an-
alyze the possible statistical and linguistic biases
in annotation. Sec 7 concludes the paper by sum-
marizing the work and discussing future research
directions. All the annotated datasets used in this
research are freely available for non-commercial
research purposes2.
2 Related Work
Query segmentation was introduced by Risvik et.
al. (2003) as a possible means to improve Informa-
tion Retrieval. Since then there has been a signif-
icant amount of research exploring various algo-
rithms for this task and its use in IR (see Hagen et.
al. (2011) for a survey). Most of the research and
evaluation considers query segmentation as a pro-
cess analogous to identification of phrases within
a query which when put within double-quotes (im-
plying exact matching of the quoted phrase in the
document) leads to better IR performance. How-
ever, this is a very restricted view of the process
and does not take into account the full potential of
query segmentation.
A more generic notion of segments leads to di-
verse and ambiguous definitions, making its eval-
uation a hard problem (see Saha Roy et. al. (2012)
for a discussion on issues with evaluation). Most
automatic segmentation techniques (Bergsma and
Wang, 2007; Tan and Peng, 2008; Zhang et al,
2Related datasets and supplementary material can be ac-
cessed from http://bit.ly/161Gkk9 or can be ob-
tained by directly emailing the authors.
1714
2009; Brenes et al, 2010; Hagen et al, 2011; Li et
al., 2011) have so far been evaluated only against
a small set of human-annotated queries (Bergsma
and Wang, 2007). The reported low IAA for such
datasets casts serious doubts on the reliability of
annotation and the performance of the algorithms
evaluated on them (Hagen et al, 2011; Saha Roy
et al, 2012).
To address the problem of data scarcity, Ha-
gen et. al. (2011) have created larger annotated
datasets through crowdsourcing3. However, in
their approach the crowd is provided with a few
(four) possible segmentations of a query to choose
from (known through a personal communication
with a authors). Thus, it presupposes an automatic
process that can generate the correct segmentation
of a query within top few options. It is far from
obvious how to generate these initial segmenta-
tions in a reliable manner. This may also result
in an over-optimistic IAA. An ideal segmentation
should be based on the annotators? own interpreta-
tion of the query. Nevertheless, if large scale data
has to be procured, crowdsourcing seems to be the
only efficient and effective model for this task, and
has been proven to be so for other IR and linguistic
annotations; see Carvalho et al (2011) for exam-
ples of crowdsourcing for IR resources and (Snow
et al, 2008; Callison-Burch, 2009) for language
resources.
In the context of NL text, segmentation has
been traditionally referred to as chunking and is
a well-studied problem. Abney (1991; 1992;
1995) defines a chunk as a sub-tree within a
syntactic phrase structure tree corresponding to
Noun, Prepositional, Adjectival, Adverbial and
Verb Phrases. Similarly, Bharati et al(1995) de-
fines it as Noun Group and Verb Group based only
on local surface information. However, cognitive
and annotation experiments for chunking of En-
glish (Abney, 1992) and other language text (Bali
et al, 2009) have shown that native speakers agree
on major clause and phrase boundaries, but may
not do so on more fine-grained chunks. One im-
portant implication of this is that annotators are
expected to agree more on the higher level bound-
aries for nested segmentation than the lower ones.
We note that hierarchical query segmentation was
proposed for the first time by Huang et al (2010),
where the authors recursively split a query (or its
fragment) into exactly two parts and evaluate the
3http://www.webis.de/research/corpora
final output against human annotations.
3 Experiments
The annotation experiments have been designed to
systematically study the various aspects of query
segmentation. In order to verify the effective-
ness and reliability of crowdsourcing, we designed
an AMT experiment for flat segmentation of Web
search queries. As a baseline, we would like to
compare these annotations with those from hu-
man experts trained for the task. We shall refer
to this baseline as the Gold annotation set. Since
we believe that the issue of granularity could be
the prime reason for previously reported low IAA
for segmentation, we also designed AMT-based
nested segmentation experiments for the same set
of queries, and obtained the corresponding gold
annotations.
Finally, to estimate the role of ambiguity inher-
ent in the structure of Web search queries on IAA,
we conducted two more control experiments, both
through crowdsourcing. First, flat and nested seg-
mentation of well-formed English, i.e., NL sen-
tences of similar length distribution; and second,
flat and nested segmentation of randomly gener-
ated queries. Higher IAA for NL sentences would
lead us to conclude that ambiguity and lack of
structure in queries is the main reason for low
agreements. On the other hand high or comparable
IAA for random queries would mean that annota-
tions have strong biases.
Thus, we have the following four pairs of anno-
tation experiments: flat and nested segmentation
of queries from crowdsourcing, corresponding flat
and nested gold annotations, flat and nested seg-
mentation of English sentences from crowdsourc-
ing, and flat and nested segmentations for ran-
domly generated queries through crowdsourcing.
3.1 Dataset
For our experiments, we need a set of Web search
queries and well-formed English sentences. Fur-
thermore, for generating the random queries, we
will use search query logs to learn n-gram mod-
els. In particular, we use the following datasets:
Q500, QG500: Saha Roy et al (2012) re-
leased a dataset of 500 queries, 5 to 8 words long,
for evaluation of various segmentation algorithms.
This dataset has flat segmentations from three an-
notators obtained under controlled experimental
settings, and can be considered as Gold annota-
1715
Figure 2: Length distribution of datasets.
tions. Hence, we select this set for our experiments
as well. We procured the corresponding nested
segmentation for these queries from two human
experts, who are regular search engine users, be-
tween 20 and 30 years old, and familiar with var-
ious linguistic annotation tasks. They annotated
the data under supervision. They were trained and
paid for the task. We shall refer to the set of flat
and nested gold annotations as QG500, whereas
Q500 will be reserved for AMT experiments.
Q700: Since 500 queries may not be enough
for reliable conclusion and since the queries may
not have been chosen specifically for the purpose
of annotation experiments, we expanded the set
with another 700 queries sampled from a slice of
the query logs of Bing Australia4 containing 16.7
million queries issued over a period of one month
(May 2010). We picked, uniformly at random,
queries that are 4 to 8 words long, have only En-
glish letters and numerals, and a high click entropy
because ?a query with a larger click entropy value
is more likely to be an informational or ambiguous
query? (Dou et al, 2008). Q500 consists of tail-
ish queries with frequency between 5 and 15 that
have at least one multiword named entity; but un-
like the case of Q700, click-entropy was not con-
sidered during sampling. As we shall see, this dif-
ference is clearly reflected in the results.
S300: We randomly selected 300 English sen-
tences from a collection of full texts of public do-
main books5 that were 5 to 15 words long, and
checked them for well-formedness. This set will
be referred to as S300.
QRand: Instead of generating search queries
by throwing in words randomly, we thought it
will be more interesting to explore annotation of
4http://www.bing.com/?cc=au
5http://www.gutenberg.org
Parameter Flat Details Nested Details
Time needed: actual (allotted) 49 sec (10 min) 1 min 52 sec (15 min)
Reward per HIT $0.02 $0.06
Instruction video duration 26 sec 1 min 40 sec
Turker qualification Completion rate >100 tasks
Turker approval rate Acceptance rate >60 %
Turker location United States of America
Table 3: Specifics of the HITs for AMT.
queries generated using n-gram models for n =
1, 2, 3. We estimated the models from the Bing
Australia log of 16.7 million queries. We gener-
ated 250 queries each of desired length distribu-
tion using the 1, 2 and 3-gram models. We shall
refer to these as U250, B250, T250 (for Uni, Bi
and Trigram) respectively, and the whole dataset
as QRand. Fig. 2 shows the query and sentence
length distribution for the various sets.
3.2 Crowdsourcing Experiments
We used AMT to get our annotations through
crowdsourcing. Pilot experiments were carried out
to test the instruction set and examples presented.
Based on the feedback, the precise instructions for
the final experiments were designed.
Two separate AMT Human Intelligence Tasks
(HITs) were designed for flat and nested query
segmentation. Also, the experiments for queries
(Q500+Q700) were conducted separately from
S300 and QRand. Thus, we had six HITs in
all. The concept of flat and nested segmentation
was introduced to the Turkers with the help of ex-
amples presented in two short videos6. When in
doubt regarding the meaning of a query, the Turk-
ers were advised to issue the query on a search
engine of their choice and find out its possible
interpretation(s). Note that we intentionally kept
definitions of flat and nested segmentation fuzzy
because (a) it would require very long instruction
manuals to cover all possible cases and (b) Turkers
do not tend to read verbose and complex instruc-
tions. Table 3 summarizes other specifics of HITs.
Honey pots or trap questions whose answers are
known a priori are often included in a HIT to iden-
tify turkers who are unable to solve the task ap-
propriately leading to incorrect annotations. How-
ever, this trick cannot be employed in our case be-
cause there is no notion of an absolutely correct
segmentation. We observe that even with unam-
biguous queries, even expert annotators may dis-
6Flat: http://youtu.be/eMeLjJIvIh0, Nested:
http://youtu.be/xE3rwANbFvU
1716
agree on some of the segment boundaries. Hence,
we decided to include annotations from all the
turkers, except for those that were syntactically ill-
formed (e.g., non-binary nested segmentation).
4 Inter Annotator Agreement
Inter Annotator Agreement is the only way to
judge the reliability of annotated data in absence
of an end application. Therefore, before we can
venture into analysis of the experimental data, we
need to formalize the notion of IAA for flat and
nested queries. The task is non-trivial for two
reasons. First, traditional IAA measures are de-
fined for a fixed set of annotators. However, for
crowdsourcing based annotations, different anno-
tators might have annotated different parts of the
dataset. For instance, we observed that a total
of 128 turkers have provided the flat annotations
for Q700, when we had only asked for 10 anno-
tations per query. Thus, on average, a turker has
annotated only 7.81% of the 700 queries. In fact,
we found that 31 turkers had annotated less than
5 queries. Hence, measures such as Cohen?s ?
(1960) cannot be directly applied in this context
because for crowdsourced annotations, we cannot
meaningfully compute annotator-specific distribu-
tion of the labels and biases.
Second, most of the standard annotation metrics
do not generalize for flat segmentation and trees.
Artstein and Poesio (2008) provides a comprehen-
sive survey of the IAA metrics and their usage in
NLP. They note that all the metrics assume that
a fixed set of labels are used for items. There-
fore, it is far from obvious how to compare chunk-
ing or segmentation that covers the whole text or
that might have overlapping units as in the case of
nested segmentation. Furthermore, we would like
to compare the reliability of flat and nested seg-
mentation, and therefore, ideally we would like to
have an IAA metric that can be meaningfully ap-
plied to both of these cases.
After considering various measures, we decided
to appropriately generalize one of the most versa-
tile and effective IAA metrics proposed till date,
the Kripendorff?s ? (2004). To be consistent with
prior work, we will stick to the notation used
in Artstein and Poesio (2008) and redefine the
? in the context of flat and nested segmentation.
Note that though the notations introduced here will
be from the perspective of queries, it is equally
applicable to sentences and the generalization is
straightforward.
4.1 Notations and Definitions
Let Q be the set of all queries with cardinality q.
A query q ? Q can be represented as a sequence of
|q| words: w1w2 . . . w|q|. We introduce |q?1| ran-
dom variables, b1, b2, . . . b|q|?1, such that bi rep-
resents the boundary between the words wi and
wi+1. A flat or nested segmentation of q, repre-
sented by qj , j varying from 1 to total number of
annotations c, is a particular instantiation of these
boundary variables as described below.
Definition. A flat segmentation, qj can be
uniquely defined by a binary assignment of the
boundary variables bj,i, where bj,i = 1 iff wi and
wi+1 belong to two different flat segments. Oth-
erwise, bj,i = 0. Thus, q has 2|q|?1 possible flat
segmentations.
Definition. A nested segmentation qj can also
be uniquely defined by assigning non-negative in-
tegers to the boundary variables such that bj,i = 0
iff words wi and wi+1 form an atomic segment
(i.e., they are grouped together), else bj,i = 1 +
max(lefti, righti), where lefti and righti are
the heights of the largest subtrees ending at wi and
beginning at wi+1 respectively.
This numbering scheme for nested segmenta-
tion can be understood through Fig. 1. Every in-
ternal node of the binary tree corresponding to the
nested segmentation is numbered according to its
height. The lowest internal nodes, both of whose
children are query words, are assigned a value of
0. Other internal nodes get a value of one greater
than the height of its higher child. Since every in-
ternal node corresponds to a boundary, we assign
the height of the node to the corresponding bound-
aries. The number of unique nested segmentations
of a query of length |q| is its corresponding Cata-
lan number7.
Boundary variables for flat and nested segmen-
tation are illustrated with an example of each kind
in Tables 1 and 2 (last column).
4.2 Krippendorff ?s ? for Segmentation
Krippendorff ?s ? (Krippendorff, 2004) is an ex-
tremely versatile agreement coefficient, which is
based on the assumption that the expected agree-
ment is calculated by looking at the overall distri-
bution of judgments without regard to which anno-
tator produced them (Artstein and Poesio, 2008).
7http://goo.gl/vKQvK
1717
Hence, it is appropriate for crowdsourced annota-
tion, where the judgments come from a large num-
ber of unrelated annotators. Moreover, it allows
for different magnitudes of disagreement, which
is a useful feature as we might want to differen-
tially penalize disagreements at various levels of
the tree for nested segmentation.
? is defined as
? = 1? DoDe
= 1? s
2
within
s2total
(1)
where Do and De are, respectively, the observed
and expected disagreements that are measured by
s2within ? the variance within the annotation of an
item and s2total ? variance across annotations of
all items. We adapt the equations presented in
pp.565-566 of Artstein and Poesio (2008) for mea-
suring these quantities for queries:
s2within =
1
2qc(c? 1)
?
q?Q
c?
m=1
c?
n=1
d(qm, qn)
(2)
s2total =
1
2qc(qc? 1)
?
q?Q
c?
m=1
?
q??Q
c?
n=1
d(qm, q?n)
(3)
where, d(qm, q?n) is a distance metric for the agree-
ment between annotations qm and q?n.
We define two different distance metrics d1 and
d2 that are applicable to flat and nested segmenta-
tion. We shall first define these metrics for com-
paring queries with equal length (i.e., |q| = |q?|):
d1(qm, q?n) =
1
|q| ? 1
|q|?1?
i=1
|bm,i ? b?n,i| (4)
d2(qm, q?n) =
1
|q| ? 1
|q|?1?
i=1
|b2m,i ? (b?n,i)2| (5)
While d1 penalizes all disagreements equally, d2
penalizes disagreements higher up the tree more.
d2 might be a desirable metric for nested seg-
mentation, because research on sentence chunk-
ing shows that annotators agree more on clause or
major phrase boundaries, even though they may
not always agree on intra-clausal or intra-phrasal
boundaries (Bali et al, 2009). Note that for flat
segmentation, d1 and d2 are identical, and hence
we will denote them as d.
We propose the following extension to these
metrics for queries of unequal lengths. Without
loss of generality, let us assume that |q| < |q?|. k
is 1 or 2; r = |q?| ? |q|+ 1.
dk(qm, q?n) =
1
r(|q| ? 1)
r?1?
a=0
|q|?1?
i=1
|bkm,i ? (b?n,i+a)k| (6)
4.3 IAA under Random Bias Assumption
Krippendorff?s ? uses the cross-item variance as
an estimate of chance agreement, which is reli-
able in general. However, this might result in mis-
leadingly low values of IAA, especially when the
items in the set are indeed expected to have sim-
ilar annotations. To resolve this, we also com-
pute the chance agreement under a random bias
model. The random model assumes that all the
structural annotations of q are equiprobable. For
flat segmentation, it boils down to the fact that
all the 2|q|?1 annotations are equally likely, which
is equivalent to the assumption that any boundary
variable bi has 0.5 probability of being 0 and 0.5
for 1.
Analytical computation of the expected proba-
bility distributions of d1(qm, qn) and d2(qm, qn)
is harder for nested segmentation. Therefore, we
programmatically generate all possible trees for q,
which is again dependent only on |q| and com-
pute d1 and d2 between all pairs of trees, from
which the expected distributions can be readily
estimated. Let us denote this expected cumula-
tive probability distribution for flat segmentation
as Pd(x; |q|) = the probability that for a pair
of randomly chosen flat segmentations of q, qm
and qn, d(qm, qn) ? x. Likewise, let Pd1(x; |q|)
and Pd2(x; |q|) be the respective probabilities that
for any two nested segmentations qm and qn of
q, the following holds: d1(qm, qn) ? x and
d2(qm, qn) ? x.
We define the IAA under random bias model as
(k is 1, 2 or null):
S = 1qc2
?
q?Q
c?
m=1
c?
n=1
Pdk(dk(qm, qn); |q|) (7)
Thus, S is the expected probability of observing a
similar or worse agreement by random chance, av-
eraged over all pairs of annotations for all queries,
and not a chance corrected IAA metric such as
?. Thus, S = 1 implies that the observed agree-
ment is almost always better than that by random
chance and S = 0.5 and 0 respectively imply that
the observed agreement is as good as and almost
always worse than that by random chance. We
1718
Dataset Flat Nested
d1 d1 d2
Q700 0.21(0.59) 0.21(0.89) 0.16(0.68)
Q500 0.22(0.62) 0.15(0.70) 0.15(0.44)
QG500 0.61(0.88) 0.66(0.88) 0.67(0.80)
S300 0.27(0.74) 0.18(0.94) 0.14(0.75)
U250 0.23(0.89) 0.42(0.90) 0.30(0.78)
B250 0.22(0.86) 0.34(0.88) 0.22(0.71)
T250 0.20(0.86) 0.44(0.89) 0.34(0.76)
Table 4: Agreement Statistics: ?(S).
also note that a high value of S and low value
of ? indicate that though the annotators agree on
the judgment of individual items, they also tend to
agree on judgments of two different items, which
in turn, could be due to strong annotator biases or
due to lack of variability of the dataset.
In the supplementary material, computations of
? and S have been explained in further details
through worked out examples. Tables for the ex-
pected distributions of d, d1 and d2 under the ran-
dom annotation assumption are also available.
5 Results
Table 4 reports the values of ? and S for flat
and nested segmentation on the various datasets.
For nested segmentation, the values were com-
puted for two different distance metrics d1 and
d2. As expected, the highest value of ? for both
flat and nested segmentation is observed for gold
annotations. An ? > 0.6 indicates quite good
IAA, and thus, reliable annotations. Higher ? for
nested segmentation QG500 than flat further vali-
dates our initial postulate that nested segmentation
may reduce disagreement from granularity issues
inherent in the definition of flat segmentation.
Opposite trends are observed for Q700, Q500
and S300, where ? for flat is the highest, followed
by that for nested using d1, and then d2. More-
over, except for flat segmentation of sentences, ?
lies between 0.14 and 0.22, which is quite low.
This clearly shows that segmentation, either flat
or nested, cannot be reliably procured through
crowdsourcing. Lower ? for d2 than d1 further
indicates that annotators disagree more for higher
levels of the trees, contrary to what we had ex-
pected. However, nearly equal IAA for sentences
and queries implies that low agreement may not be
an outcome of inherent ambiguity in the structure
of queries. Slightly higher ? for flat segmentation
and a much higher ? for nested segmentation of
QRand reinforce the fact that low IAA is not due
to a lack of structure in queries.
It is interesting to note that ? for nested segmen-
tation of S300 and all segmentations of QRand
are low or medium despite the fact that S is very
high in all these cases. Thus, it is clear that an-
notators have a strong bias towards certain struc-
tures across queries. In the next section, we will
analyze some of these biases. We also computed
the IAA between QG500 and Q500, and found
? = 0.27. This is much lower than ? for QG500,
though slightly higher than that for Q500. We did
not observe any significant variation in agreement
with respect to the length of the queries.
6 Biases in Annotation
The IAA statistics clearly show that there are cer-
tain strong biases in both flat and nested query
segmentation, especially those obtained through
crowdsourcing. To identify these biases, we went
through the annotations and came up with possi-
ble hypotheses, which we tried to verify through
statistical analysis of the data. Here, we report the
most prominent biases that were thus discovered.
Bias 1: During flat segmentation, annotators pre-
fer dividing the query into two segments of roughly
equal length.
As discussed earlier, one of the major problems
of flat segmentation is the fuzziness in granularity.
In our experiments, we intentionally left the de-
cision of whether to go for fine or coarse-grained
segmentation to the annotator. However, it is sur-
prising to observe that annotators typically divide
the query into two segments (see Fig. 3, plots A1
and A2), and at times three, but hardly ever more
than three. This bias is observed across queries,
sentences and random queries, where the percent-
age of annotations with 2 or 3 segments are greater
than 83%, 91% and 96% respectively. This bias
is most strongly visible for QRand because the
lack of syntactic or semantic cohesion between the
words provides no clue for segmentation.
Furthermore, we observe that typically seg-
ments tend to be of equal length. For this, we com-
puted standard deviations (sd) of segment lengths
for all annotations having 2 or 3 segments; the dis-
tribution of sd is shown in Fig. 3, plots B1 and B2.
We observe that for all datasets, sd lies mainly be-
tween 0.5 and 1 (for perspective, consider a query
1719
Figure 3: Analysis of annotation biases: A1, A2 ? number of segments per flat segmentation vs. length;
B1, B2 ? standard deviation of segment length for flat segmentation; C1, C2 ? distribution of the tree
heights in nested segmentation.
Length Expected Q500 QG500 Q700 S300 QRand
5 2.57 2.00 2.02 2.08 2.02 2.01
6 3.24 2.26 2.23 2.23 2.24 2.02
7 3.88 2.70 2.71 2.67 2.55 2.62
8 4.47 2.89 2.68 2.72 2.72 2.35
Table 5: Average height for nested segmentation.
with 7 words; with two segments of length 3 and
4 the sd is 0.5, and for 2 and 5, the sd is 1.5), im-
plying that segments are roughly of equal length.
It is likely that due to this bias, the S or observed
agreement is moderately high for queries and very
high for sentences, but then it also leads to high
agreement across different queries and sentences
(i.e., high s2total) especially when they are of equal
length, which in turn brings down the value of ? ?
the true agreement after bias correction.
Bias 2: During nested segmentation, annotators
prefer balanced binary trees.
Quite analogous to bias 1, for nested segmen-
tation we observe that annotators tend to prefer
more balanced binary trees. Fig. 3 plots C1 and C2
show the distribution of the tree heights for various
cases and Table 5 reports the corresponding aver-
age height of the trees for queries and sentences
of various lengths and the the expected value of
the height if all trees were equally likely. The ob-
served heights are much lower than the expected
values clearly implying the preference of the an-
notators for more balanced trees.
Thus, the crowd seems to choose the middle
path, avoiding extremes and hence may not be a
reliable source of annotation for query segmen-
tation. It can be argued that similar biases are
also observed for gold annotations, and therefore,
probably it is the inherent structure of the queries
and sentences that lead to such biased distribution
of segmentation patterns. However, note that ? for
QG500 is much higher than all other cases, which
shows that the true agreement between gold anno-
tators is immune to such biases or skewed distri-
butions in the datasets. Furthermore, high values
of ? for QRand despite the very strong biases in
annotation shows that there perhaps is very little
choice that the annotators have while segmenting
randomly generated queries. On the other hand,
the textual coherence of the real queries and sen-
tences provide many different choices for segmen-
tation and the Turker typically gets carried away
by these biases, leading to low ?.
Bias 3: Phrase structure drives segmentation only
when reconcilable with Bias 1. Whenever the sen-
tence or query has a verb phrase (VP) spanning
roughly half of it, annotators seem to chunk be-
fore the VP as one would expect, quite as of-
ten as just after the verb, which is quite unex-
pected. For instance, the sentence A gentle
sarcasm ruffled her anger. gathers as
many as eight flat annotations with a boundary be-
tween sarcasm and ruffled, and four with
a boundary between ruffled and her. How-
ever, if the VP is very short consisting of a single
1720
Position Q500 QG500 Q700 S300 QRand
Both 2.24 0.37 2.78 2.08 0.63
None 50.34 56.85 35.74 35.84 39.81
Right 23.86 21.50 19.02 12.52 15.23
Left 18.08 15.97 40.59 45.96 21.21
Table 6: Percentages of positions of segment
boundaries with respect to prepositions. Prepo-
sitions occurring in the beginning or end of a
query/sentence have been excluded from the anal-
ysis; hence, numbers in a column do not total 100.
verb, as in A fleeting and furtive air
of triumph erupted., annotators seem to
attempt for a balanced annotation due to Bias 1.
As a clear middle boundary is not present in such
sentences, the annotations show a lot more varia-
tion and disagreement. For instance, only 1 out of
10 annotations had a boundary before erupted
in the above example. In fact, at least one anno-
tation had a boundary after each word in the sen-
tence, with no clear majority.
Bias 4: Prepositions influence segment bound-
aries differently for queries and sentences. We
automatically labeled all the prepositions in the
flat annotations and classified them according to
the criterion of whether a boundary was placed
immediately before or after it, or on both sides
or neither side. The statistics, reported in Ta-
ble 6, show that for NL sentences a majority
of the boundaries are present before the prepo-
sition, marking the beginning of a prepositional
phrase. However, for queries, a much richer pat-
tern emerges depending on the specific preposi-
tion. For instance, to, of and for are often
chunked with the previous word (e.g., how to |
choose a bike size, birthday party
ideas for | one year old). We believe
that this difference is because in sentences due
to the presence of a verb, the PP has a well-
defined head, lack of which leads to preposition
in queries getting chunked with words that form
more commonly seen patterns (e.g., flights
to and tickets for).
Bias 3 and 4 present the complex interpretation
of the structure of queries by the annotators which
could be due to some emerging cognitive model of
queries among the search engine users. This is a
fascinating and unexplored aspect of query struc-
tures that demands deeper investigation through
cognitive and psycholinguistic experiments.
7 Conclusion
We have studied various aspects of query segmen-
tation through crowdsourcing by designing and
conducting suitable experiments. Analysis of ex-
perimental data leads us to conclude the follow-
ing: (a) crowdsoucing may not be a very effective
way to collect judgments for query segmentation;
(b) addressing fuzziness of granularity for flat seg-
mentation by introducing strict binary nested seg-
ments does not lead to better agreement in crowd-
sourced annotations, though it definitely improves
the IAA for gold standard segmentations, imply-
ing that low IAA in flat segmentation among ex-
perts is primarily an effect of unspecified granular-
ity of segments; (c) low IAA is not due to the in-
herent structural ambiguity in queries as this holds
true for sentences as well; (d) there are strong bi-
ases in crowdsourced annotations, mostly because
turkers prefer more balanced segment structures;
and (e) while annotators are by and large guided
by linguistic principles, application of these prin-
ciples differ between query and NL sentences and
also closely interact with other biases.
One of the important contributions of this work
is the formulation of a new IAA metric for com-
paring across flat and nested segmentations, espe-
cially for crowdsourcing based annotations. Since
trees are commonly used across various linguistic
annotations, this metric can have wide applicabil-
ity. The metric, moreover, can be easily adapted
to other annotation schemes as well by defining an
appropriate distance metric between annotations.
Since large scale data for query segmentation is
very useful, it would be interesting to see if the
problem can be rephrased to the Turkers in a way
so as to obtain more reliable judgments. Yet a
deeper question is regarding the theoretical status
of query structure, which though in an emergent
state is definitely an operating model for the anno-
tators. Our future work in this area would specifi-
cally target understanding and formalization of the
theoretical model underpinning a query.
Acknowledgments
We thank Ed Cutrell and Andrew Cross, Microsoft
Research Lab India, for their help in setting up the
AMT experiments. We would also like to thank
Anusha Suresh, IIT Kharagpur, India, for helping
us with data preparation.
1721
References
Steven P. Abney. 1991. Parsing By Chunks. Kluwer
Academic Publishers.
Steven P. Abney. 1992. Prosodic Structure, Perfor-
mance Structure And Phrase Structure. In Proceed-
ings 5th DARPA Workshop on Speech and Natural
Language, pages 425?428. Morgan Kaufmann.
Steven P. Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
Computational Linguistics and the Foundations of
Linguistic Theory, pages 145?164.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,
Sankalan Prasad, and Arpit Maheswari. 2009. Cor-
relates between Performance, Prosodic and Phrase
Structures in Bangla and Hindi: Insights from a Psy-
cholinguistic Experiment. In Proceedings of Inter-
national Conference on Natural Language Process-
ing, pages 101 ? 110.
Michael Bendersky, W. B. Croft, and David A. Smith.
2009. Two-stage query segmentation for informa-
tion retrieval. In Proceedings of the 32nd interna-
tional ACM Special Interest Group on Information
Retrieval (SIGIR) Conference on Research and De-
velopment in Information Retrieval, pages 810?811.
ACM.
Shane Bergsma and Qin Iris Wang. 2007. Learning
Noun Phrase Query Segmentation. In Proceedings
of Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 819?826.
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and
KV Ramakrishnamacharyulu. 1995. Natural lan-
guage processing: a Paninian perspective. Prentice-
Hall of India New Delhi.
David J. Brenes, Daniel Gayo-Avello, and Rodrigo
Garcia. 2010. On the fly query segmentation using
snippets. In CERI ?10, pages 259?266.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?09, pages 286?295. Associa-
tion for Computational Linguistics.
Vitor R Carvalho, Matthew Lease, and Emine Yilmaz.
2011. Crowdsourcing for search evaluation. ACM
Sigir forum, 44(2):17?22.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-
Rong Wen. 2008. Are Click-through Data Adequate
for Learning Web Search Rankings? In Proceed-
ings of the 17th ACM Conference on Information
and Knowledge Management, pages 73?82. ACM.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query Segmentation
Revisited. In Proceedings of the 20th Interna-
tional Conference on World Wide Web, pages 97?
106. ACM.
Matthias Hagen, Martin Potthast, Anna Beyer, and
Benno Stein. 2012. Towards Optimum Query Seg-
mentation: In Doubt Without. In Proceedings of the
Conference on Information and Knowledge Man-
agement, pages 1015?1024.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong
Li, Kuansan Wang, Fritz Behr, and C. Lee Giles.
2010. Exploring web scale language models for
search query processing. In Proceedings of the 19th
international conference on World wide web, WWW
?10, pages 451?460, New York, NY, USA. ACM.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to its Methodology. Sage,Thousand
Oaks, CA.
Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and
Kuansan Wang. 2011. Unsupervised query segmen-
tation using clickthrough for information retrieval.
In SIGIR ?11, pages 285?294. ACM.
Knut Magne Risvik, Tomasz Mikolajewski, and Peter
Boros. 2003. Query segmentation for web search.
In WWW (Posters).
Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-
hury, and Srivatsan Laxman. 2012. An IR-based
Evaluation Framework for Web Search Query Seg-
mentation. In Proceedings of the International ACM
Special Interest Group on Information Retrieval (SI-
GIR) Conference on Research and Development in
Information Retrieval, pages 881?890. ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 254?263, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bin Tan and Fuchun Peng. 2008. Unsupervised Query
Segmentation Using Generative Language Models
and Wikipedia. In Proceedings of the 17th Inter-
national Conference on World Wide Web (WWW),
pages 347?356. ACM.
Chao Zhang, Nan Sun, Xia Hu, Tingzhu Huang, and
Tat-Seng Chua. 2009. Query segmentation based on
eigenspace similarity. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 185?188, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1722
