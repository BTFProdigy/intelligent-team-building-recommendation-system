Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232?239,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Fully Unsupervised Discovery of Concept-Specific Relationships
by Web Mining
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem 91904, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem 91904, Israel
www.cs.huji.ac.il/?arir
Moshe Koppel
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
koppel@cs.biu.ac.il
Abstract
We present a web mining method for discov-
ering and enhancing relationships in which a
specified concept (word class) participates.
We discover a whole range of relationships
focused on the given concept, rather than
generic known relationships as in most pre-
vious work. Our method is based on cluster-
ing patterns that contain concept words and
other words related to them. We evaluate the
method on three different rich concepts and
find that in each case the method generates a
broad variety of relationships with good pre-
cision.
1 Introduction
The huge amount of information available on the
web has led to a flurry of research on methods for
automatic creation of structured information from
large unstructured text corpora. The challenge is to
create as much information as possible while pro-
viding as little input as possible.
A lot of this research is based on the initial insight
(Hearst, 1992) that certain lexical patterns (?X is a
country?) can be exploited to automatically gener-
ate hyponyms of a specified word. Subsequent work
(to be discussed in detail below) extended this initial
idea along two dimensions.
One objective was to require as small a user-
provided initial seed as possible. Thus, it was ob-
served that given one or more such lexical patterns,
a corpus could be used to generate examples of hy-
ponyms that could then, in turn, be exploited to gen-
erate more lexical patterns. The larger and more reli-
able sets of patterns thus generated resulted in larger
and more precise sets of hyponyms and vice versa.
The initial step of the resulting alternating bootstrap
process ? the user-provided input ? could just as well
consist of examples of hyponyms as of lexical pat-
terns.
A second objective was to extend the information
that could be learned from the process beyond hy-
ponyms of a given word. Thus, the approach was
extended to finding lexical patterns that could pro-
duce synonyms and other standard lexical relations.
These relations comprise all those words that stand
in some known binary relation with a specified word.
In this paper, we introduce a novel extension of
this problem: given a particular concept (initially
represented by two seed words), discover relations
in which it participates, without specifying their
types in advance. We will generate a concept class
and a variety of natural binary relations involving
that class.
An advantage of our method is that it is particu-
larly suitable for web mining, even given the restric-
tions on query amounts that exist in some of today?s
leading search engines.
The outline of the paper is as follows. In the next
section we will define more precisely the problem
we intend to solve. In section 3, we will consider re-
lated work. In section 4 we will provide an overview
of our solution and in section 5 we will consider the
details of the method. In section 6 we will illustrate
and evaluate the results obtained by our method. Fi-
nally, in section 7 we will offer some conclusions
and considerations for further work.
232
2 Problem Definition
In several studies (e.g., Widdows and Dorow, 2002;
Pantel et al 2004; Davidov and Rappoport, 2006)
it has been shown that relatively unsupervised and
language-independent methods could be used to
generate many thousands of sets of words whose
semantics is similar in some sense. Although ex-
amination of any such set invariably makes it clear
why these words have been grouped together into
a single concept, it is important to emphasize that
the method itself provides no explicit concept defi-
nition; in some sense, the implied class is in the eye
of the beholder. Nevertheless, both human judgment
and comparison with standard lists indicate that the
generated sets correspond to concepts with high pre-
cision.
We wish now to build on that result in the fol-
lowing way. Given a large corpus (such as the web)
and two or more examples of some concept X , au-
tomatically generate examples of one or more rela-
tions R ? X ? Y , where Y is some concept and R
is some binary relationship between elements of X
and elements of Y .
We can think of the relations we wish to gener-
ate as bipartite graphs. Unlike most earlier work,
the bipartite graphs we wish to generate might be
one-to-one (for example, countries and their capi-
tals), many-to-one (for example, countries and the
regions they are in) or many-to-many (for example,
countries and the products they manufacture). For a
given class X , we would like to generate not one but
possibly many different such relations.
The only input we require, aside from a corpus,
is a small set of examples of some class. However,
since such sets can be generated in entirely unsuper-
vised fashion, our challenge is effectively to gener-
ate relations directly from a corpus given no addi-
tional information of any kind. The key point is that
we do not in any manner specify in advance what
types of relations we wish to find.
3 Related Work
As far as we know, no previous work has directly
addressed the discovery of generic binary relations
in an unrestricted domain without (at least implic-
itly) pre-specifying relationship types. Most related
work deals with discovery of hypernymy (Hearst,
1992; Pantel et al 2004), synonymy (Roark and
Charniak, 1998; Widdows and Dorow, 2002; Davi-
dov and Rappoport, 2006) and meronymy (Berland
and Charniak, 1999).
In addition to these basic types, several stud-
ies deal with the discovery and labeling of more
specific relation sub-types, including inter-verb re-
lations (Chklovski and Pantel, 2004) and noun-
compound relationships (Moldovan et al 2004).
Studying relationships between tagged named en-
tities, (Hasegawa et al 2004; Hassan et al 2006)
proposed unsupervised clustering methods that as-
sign given (or semi-automatically extracted) sets of
pairs into several clusters, where each cluster corre-
sponds to one of a known relationship type. These
studies, however, focused on the classification of
pairs that were either given or extracted using some
supervision, rather than on discovery and definition
of which relationships are actually in the corpus.
Several papers report on methods for using the
web to discover instances of binary relations. How-
ever, each of these assumes that the relations them-
selves are known in advance (implicitly or explic-
itly) so that the method can be provided with seed
patterns (Agichtein and Gravano, 2000; Pantel et al
2004), pattern-based rules (Etzioni et al 2004), rela-
tion keywords (Sekine, 2006), or word pairs exem-
plifying relation instances (Pasca et al 2006; Alfon-
seca et al 2006; Rosenfeld and Feldman, 2006).
In some recent work (Strube and Ponzetto, 2006),
it has been shown that related pairs can be gener-
ated without pre-specifying the nature of the rela-
tion sought. However, this work does not focus on
differentiating among different relations, so that the
generated relations might conflate a number of dis-
tinct ones.
It should be noted that some of these papers utilize
language and domain-dependent preprocessing in-
cluding syntactic parsing (Suchanek et al 2006) and
named entity tagging (Hasegawa et al 2004), while
others take advantage of handcrafted databases such
as WordNet (Moldovan et al 2004; Costello et al
2006) and Wikipedia (Strube and Ponzetto, 2006).
Finally, (Turney, 2006) provided a pattern dis-
tance measure which allows a fully unsupervised
measurement of relational similarity between two
pairs of words; however, relationship types were not
discovered explicitly.
233
4 Outline of the Method
We will use two concept words contained in a con-
cept class C to generate a collection of distinct re-
lations in which C participates. In this section we
offer a brief overview of our method.
Step 1: Use a seed consisting of two (or more) ex-
ample words to automatically obtain other examples
that belong to the same class. Call these concept
words. (For instance, if our example words were
France and Angola, we would generate more coun-
try names.)
Step 2: For each concept word, collect instances
of contexts in which the word appears together with
one other content word. Call this other word a tar-
get word for that concept word. (For example, for
France we might find ?Paris is the capital of France?.
Paris would be a target word for France.)
Step 3: For each concept word, group the contexts
in which it appears according to the target word that
appears in the context. (Thus ?X is the capital of Y ?
would likely be grouped with ?Y ?s capital is X?.)
Step 4: Identify similar context groups that ap-
pear across many different concept words. Merge
these into a single concept-word-independent clus-
ter. (The group including the two contexts above
would appear, with some variation, for other coun-
tries as well, and all these would be merged into
a single cluster representing the relation capital-
of(X,Y).)
Step 5: For each cluster, output the relation con-
sisting of all <concept word, target word> pairs that
appear together in a context included in the cluster.
(The cluster considered above would result in a set
of pairs consisting of a country and its capital. Other
clusters generated by the same seed might include
countries and their languages, countries and the re-
gions in which they are located, and so forth.)
5 Details of the Method
In this section we consider the details of each of
the above-enumerated steps. It should be noted
that each step can be performed using standard web
searches; no special pre-processed corpus is re-
quired.
5.1 Generalizing the seed
The first step is to take the seed, which might con-
sist of as few as two concept words, and generate
many (ideally, all, when the concept is a closed set
of words) members of the class to which they be-
long. We do this as follows, essentially implement-
ing a simplified version of the method of Davidov
and Rappoport (2006). For any pair of seed words
Si and Sj , search the corpus for word patterns of the
form SiHSj , where H is a high-frequency word in
the corpus (we used the 100 most frequent words
in the corpus). Of these, we keep all those pat-
terns, which we call symmetric patterns, for which
SjHSi is also found in the corpus. Repeat this pro-
cess to find symmetric patterns with any of the struc-
tures HSHS, SHSH or SHHS. It was shown in
(Davidov and Rappoport, 2006) that pairs of words
that often appear together in such symmetric pat-
terns tend to belong to the same class (that is, they
share some notable aspect of their semantics). Other
words in the class can thus be generated by search-
ing a sub-corpus of documents including at least two
concept words for those words X that appear in a
sufficient number of instances of both the patterns
SiHX and XHSi, where Si is a word in the class.
The same can be done for the other three pattern
structures. The process can be bootstrapped as more
words are added to the class.
Note that our method differs from that of Davidov
and Rappoport (2006) in that here we provide an ini-
tial seed pair, representing our target concept, while
there the goal is grouping of as many words as pos-
sible into concept classes. The focus of our paper is
on relations involving a specific concept.
5.2 Collecting contexts
For each concept word S, we search the corpus for
distinct contexts in which S appears. (For our pur-
poses, a context is a window with exactly five words
or punctuation marks before or after the concept
word; we choose 10,000 of these, if available.) We
call the aggregate text found in all these context win-
dows the S-corpus.
From among these contexts, we choose all pat-
terns of the form H1SH2XH3 or H1XH2SH3,
where:
234
? X is a word that appears with frequency below
f1 in the S-corpus and that has sufficiently high
pointwise mutual information with S. We use
these two criteria to ensure that X is a content
word and that it is related to S. The lower the
threshold f1, the less noise we allow in, though
possibly at the expense of recall. We used f1 =
1, 000 occurrences per million words.
? H2 is a string of words each of which occurs
with frequency above f2 in the S-corpus. We
want H2 to consist mainly of words common
in the context of S in order to restrict patterns
to those that are somewhat generic. Thus, in
the context of countries we would like to retain
words like capital while eliminating more spe-
cific words that are unlikely to express generic
patterns. We used f2 = 100 occurrences per
million words (there is room here for automatic
optimization, of course).
? H1 and H3 are either punctuation or words that
occur with frequency above f3 in the S-corpus.
This is mainly to ensure that X and S aren?t
fragments of multi-word expressions. We used
f3 = 100 occurrences per million words.
? We call these patterns, S-patterns and we call
X the target of the S-pattern. The idea is that S
and X very likely stand in some fixed relation
to each other where that relation is captured by
the S-pattern.
5.3 Grouping S-patterns
If S is in fact related to X in some way, there might
be a number of S-patterns that capture this relation-
ship. For each X , we group all the S-patterns that
have X as a target. (Note that two S-patterns with
two different targets might be otherwise identical,
so that essentially the same pattern might appear in
two different groups.) We now merge groups with
large (more than 2/3) overlap. We call the resulting
groups, S-groups.
5.4 Identifying pattern clusters
If the S-patterns in a given S-group actually capture
some relationship between S and the target, then
one would expect that similar groups would appear
for a multiplicity of concept words S. Suppose that
we have S-groups for three different concept words
S such that the pairwise overlap among the three
groups is more than 2/3 (where for this purpose two
patterns are deemed identical if they differ only at S
and X). Then the set of patterns that appear in two or
three of these S-groups is called a cluster core. We
now group all patterns in other S-groups that have an
overlap of more than 2/3 with the cluster core into a
candidate pattern pool P . The set of all patterns in
P that appear in at least two S-groups (among those
that formed P ) pattern cluster. A pattern cluster that
has patterns instantiated by at least half of the con-
cept words is said to represent a relation.
5.5 Refining relations
A relation consists of pairs (S, X) where S is a con-
cept word and X is the target of some S-pattern in a
given pattern cluster. Note that for a given S, there
might be one or many values of X satisfying the re-
lation. As a final refinement, for each given S, we
rank all such X according to pointwise mutual in-
formation with S and retain only the highest 2/3. If
most values of S have only a single corresponding X
satisfying the relation and the rest have none, we try
to automatically fill in the missing values by search-
ing the corpus for relevant S-patterns for the missing
values of S. (In our case the corpus is the web, so
we perform additional clarifying queries.)
Finally, we delete all relations in which all con-
cept words are related to most target words and all
relations in which the concept words and the target
words are identical. Such relations can certainly be
of interest (see Section 7), but are not our focus in
this paper.
5.6 Notes on required Web resources
In our implementation we use the Google search
engine. Google restricts individual users to 1,000
queries per day and 1,000 pages per query. In each
stage we conducted queries iteratively, each time
downloading all 1,000 documents for the query.
In the first stage our goal was to discover sym-
metric relationships from the web and consequently
discover additional concept words. For queries in
this stage of our algorithm we invoked two require-
ments.
First, the query should contain at least two con-
cept words. This proved very effective in reduc-
235
ing ambiguity. Thus of 1,000 documents for the
query bass, 760 deal with music, while if we add to
the query a second word from the intended concept
(e.g., barracuda), then none of the 1,000 documents
deal with music and the vast majority deal with fish,
as intended.
Second, we avoid doing overlapping queries. To
do this we used Google?s ability to exclude from
search results those pages containing a given term
(in our case, one of the concept words).
We performed up to 300 different queries for in-
dividual concepts in the first stage of our algorithm.
In the second stage, we used web queries to as-
semble S-corpora. On average, about 1/3 of the con-
cept words initially lacked sufficient data and we
performed up to twenty additional queries for each
rare concept word to fill its corpus.
In the last stage, when clusters are constructed,
we used web queries for filling missing pairs of one-
to-one or several-to-several relationships. The to-
tal number of filling queries for a specific concept
was below 1,000, and we needed only the first re-
sults of these queries. Empirically, it took between
0.5 to 6 day limits (i.e., 500?6,000 queries) to ex-
tract relationships for a concept, depending on its
size (the number of documents used for each query
was at most 100). Obviously this strategy can be
improved by focused crawling from primary Google
hits, which can drastically reduce the required num-
ber of queries.
6 Evaluation
In this section we wish to consider the variety of re-
lations that can be generated by our method from a
given seed and to measure the quality of these rela-
tions in terms of their precision and recall.
With regard to precision, two claims are being
made. One is that the generated relations correspond
to identifiable relations. The other claim is that to
the extent that a generated relation can be reason-
ably identified, the generated pairs do indeed belong
to the identified relation. (There is a small degree of
circularity in this characterization but this is proba-
bly the best we can hope for.)
As a practical matter, it is extremely difficult to
measure precision and recall for relations that have
not been pre-determined in any way. For each gen-
erated relation, authoritative resources must be mar-
shaled as a gold standard. For purposes of evalu-
ation, we ran our algorithm on three representative
domains ? countries, fish species and star constel-
lations ? and tracked down gold standard resources
(encyclopedias, academic texts, informative web-
sites, etc) for the bulk of the relations generated in
each domain.
This choice of domains allowed us to explore
different aspects of algorithmic behavior. Country
and constellation domains are both well defined and
closed domains. However they are substantially dif-
ferent.
Country names is a relatively large domain which
has very low lexical ambiguity, and a large number
of potentially useful relations. The main challenge
in this domain was to capture it well.
Constellation names, in contrast, are a relatively
small but highly ambiguous domain. They are used
in proper names, mythology, names of entertainment
facilities etc. Our evaluation examined how well the
algorithm can deal with such ambiguity.
The fish domain contains a very high number of
members. Unlike countries, it is a semi-open non-
homogenous domain with a very large number of
subclasses and groups. Also, unlike countries, it
does not contain many proper nouns, which are em-
pirically generally easier to identify in patterns. So
the main challenge in this domain is to extract un-
blurred relationships and not to diverge from the do-
main during the concept acquisition phase.
We do not show here all-to-all relationships such
as fish parts (common to all or almost all fish), be-
cause we focus on relationships that separate be-
tween members of the concept class, which are
harder to acquire and evaluate.
6.1 Countries
Our seed consisted of two country names. The in-
tended result for the first stage of the algorithm
was a list of countries. There are 193 countries in
the world (www.countrywatch.com) some of which
have multiple names so that the total number of
commonly used country names is 243. Of these,
223 names (comprising 180 countries) are charac-
ter strings with no white space. Since we consider
only single word names, these 223 are the names we
hope to capture in this stage.
236
Using the seed words France and Angola, we
obtained 202 country names (comprising 167 dis-
tinct countries) as well as 32 other names (consisting
mostly of names of other geopolitical entities). Us-
ing the list of 223 single word countries as our gold
standard, this gives precision of 0.90 and recall of
0.86. (Ten other seed pairs gave results ranging in
precision: 0.86-0.93 and recall: 0.79-0.90.)
The second part of the algorithm generated a set
of 31 binary relations. Of these, 25 were clearly
identifiable relations many of which are shown in
Table 1. Note that for three of these there are stan-
dard exhaustive lists against which we could mea-
sure both precision and recall; for the others shown,
sources were available for measuring precision but
no exhaustive list was available from which to mea-
sure recall, so we measured coverage (the number
of countries for which at least one target concept is
found as related).
Another eleven meaningful relations were gener-
ated for which we did not compute precision num-
bers. These include celebrity-from, animal-of, lake-
in, borders-on and enemy-of. (The set of relations
generated by other seed pairs differed only slightly
from those shown here for France and Angola.)
6.2 Fish species
In our second experiment, our seed consisted of two
fish species, barracuda and bluefish. There are 770
species listed in WordNet of which 447 names are
character strings with no white space. The first stage
of the algorithm returned 305 of the species listed
in Wordnet, another 37 species not listed in Word-
net, as well as 48 other names (consisting mostly
of other sea creatures). The second part of the al-
gorithm generated a set of 15 binary relations all of
which are meaningful. Those for which we could
find some gold standard are listed in Table 2.
Other relations generated include served-with,
bait-for, food-type, spot-type, and gill-type.
6.3 Constellations
Our seed consisted of two constellation names,
Orion and Cassiopeia. There are 88 standard
constellations (www.astro.wisc.edu) some of which
have multiple names so that the total number of com-
monly used constellations is 98. Of these, 87 names
(77 constellations) are strings with no white space.
Relationship Prec. Rec/Cov
Sample pattern
(Sample pair)
capital-of 0.92 R=0.79
in (x), capital of (y),
(Luanda, Angola)
language-spoken-in 0.92 R=0.60
to (x) or other (y) speaking
(Spain, Spanish)
in-region 0.73 R=0.71
throughout (x), from (y) to
(America, Canada)
city-in 0.82 C=0.95
west (x) ? forecast for (y).
(England, London)
river-in 0.92 C=0.68
central (x), on the (y) river
(China, Haine)
mountain-range-in 0.77 C=0.69
the (x) mountains in (y) ,
(Chella, Angola)
sub-region-of 0.81 C=0.81
the (y) region of (x),
(Veneto, Italy)
industry-of 0.70 C=0.90
the (x) industry in (y) ,
(Oil, Russia)
island-in 0.98 C=0.55
, (x) island , (y) ,
(Bathurst, Canada)
president-of 0.86 C=0.51
president (x) of (y) has
(Bush, USA)
political-position-in 0.81 C=0.75
former (x) of (y) face
(President, Ecuador)
political-party-of 0.91 C=0.53
the (x) party of (y) ,
(Labour, England)
festival-of 0.90 C=0.78
the (x) festival, (y) ,
(Tanabata, Japan)
religious-denomination-of 0.80 C=0.62
the (x) church in (y) ,
(Christian, Rome)
Table 1: Results on seed { France, Angola }.
237
Relationship Prec. Cov
Sample pattern
(Sample pair)
region-found-in 0.83 0.80
best (x) fishing in (y) .
(Walleye, Canada)
sea-found-in 0.82 0.64
of (x) catches in the (y) sea
(Shark, Adriatic)
lake-found-in 0.79 0.51
lake (y) is famous for (x) ,
(Marion, Catfish)
habitat-of 0.78 0.92
, (x) and other (y) fish
(Menhaden, Saltwater)
also-called 0.91 0.58
. (y) , also called (x) ,
(Lemonfish, Ling)
eats 0.90 0.85
the (x) eats the (y) and
(Perch, Minnow)
color-of 0.95 0.85
the (x) was (y) color
(Shark, Gray)
used-for-food 0.80 0.53
catch (x) ? best for (y) or
(Bluefish, Sashimi)
in-family 0.95 0.60
the (x) family , includes (y) ,
(Salmonid, Trout)
Table 2: Results on seed { barracud, bluefish }.
The first stage of the algorithm returned 81 constel-
lation names (77 distinct constellations) as well as
38 other names (consisting mostly of names of indi-
vidual stars). Using the list of 87 single word con-
stellation names as our gold standard, this gives pre-
cision of 0.68 and recall of 0.93.
The second part of the algorithm generated a set
of ten binary relations. Of these, one concerned
travel and entertainment (constellations are quite
popular as names of hotels and lounges) and another
three were not interesting. Apparently, the require-
ment that half the constellations appear in a relation
limited the number of viable relations since many
constellations are quite obscure. The six interesting
relations are shown in Table 3 along with precision
and coverage.
7 Discussion
In this paper we have addressed a novel type of prob-
lem: given a specific concept, discover in fully un-
supervised fashion, a range of relations in which it
participates. This can be extremely useful for study-
ing and researching a particular concept or field of
study.
As others have shown as well, two concept words
can be sufficient to generate almost the entire class
to which the words belong when the class is well-
defined. With the method presented in this paper,
using no further user-provided information, we can,
for a given concept, automatically generate a diverse
collection of binary relations on this concept. These
relations need not be pre-specified in any way. Re-
sults on the three domains we considered indicate
that, taken as an aggregate, the relations that are gen-
erated for a given domain paint a rather clear picture
of the range of information pertinent to that domain.
Moreover, all this was done using standard search
engine methods on the web. No language-dependent
tools were used (not even stemming); in fact, we re-
produced many of our results using Google in Rus-
sian.
The method depends on a number of numerical
parameters that control the subtle tradeoff between
quantity and quality of generated relations. There is
certainly much room for tuning of these parameters.
The concept and target words used in this paper
are single words. Extending this to multiple-word
expressions would substantially contribute to the ap-
plicability of our results.
In this research we effectively disregard many re-
lationships of an all-to-all nature. However, such
relationships can often be very useful for ontology
construction, since in many cases they introduce
strong connections between two different concepts.
Thus, for fish we discovered that one of the all-to-
all relationships captures a precise set of fish body
parts, and another captures swimming verbs. Such
relations introduce strong and distinct connections
between the concept of fish and the concepts of fish-
body-parts and swimming. Such connections may
be extremely useful for ontology construction.
238
Relationship Prec. Cov
Sample pattern
(Sample pair)
nearby-constellation 0.87 0.70
constellation (x), near (y),
(Auriga, Taurus)
star-in 0.82 0.76
star (x) in (y) is
(Antares , Scorpius)
shape-of 0.90 0.55
, (x) is depicted as (y).
(Lacerta, Lizard)
abbreviated-as 0.93 0.90
. (x) abbr (y),
(Hidra, Hya)
cluster-types-in 0.92 1.00
famous (x) cluster in (y),
(Praesepe, Cancer)
location 0.82 0.70
, (x) is a (y) constellation
(Draco, Circumpolar)
Table 3: Results on seed { Orion, Cassiopeia }.
References
Agichtein, E., Gravano, L., 2000. Snowball: Extracting
relations from large plain-text collections. Proceedings
of the 5th ACM International Conference on Digital
Libraries.
Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,
P., 2006. Towards large-scale non-taxonomic relation
extraction: estimating the precision of rote extractors.
Workshop on Ontology Learning and Population at
COLING-ACL ?06.
Berland, M., Charniak, E., 1999. Finding parts in very
large corpora. ACL ?99.
Chklovski T., Pantel P., 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
?04.
Costello, F., Veale, T., Dunne, S., 2006. Using Word-
Net to automatically deduce relations between words
in noun-noun compounds, COLING-ACL ?06.
Davidov, D., Rappoport, A., 2006. Efficient unsupervised
discovery of word categories using symmetric patterns
and high frequency words. COLING-ACL ?06.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,
Shaked, T., Soderland, S., Weld, D., Yates, A., 2004.
Methods for domain-independent information extrac-
tion from the web: an experimental comparison. AAAI
?04.
Hasegawa, T., Sekine, S., Grishman, R., 2004. Discover-
ing relations among named entities from large corpora.
ACL ?04.
Hassan, H., Hassan, A., Emam, O., 2006. unsupervised
information extraction approach using graph mutual
reinforcement. EMNLP ?06.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,
Girju, R., 2004. Models for the semantic classifica-
tion of noun phrases. Workshop on Comput. Lexical
Semantics at HLT-NAACL ?04.
Pantel, P., Ravichandran, D., Hovy, E., 2004. Towards
terascale knowledge acquisition. COLING ?04.
Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A., 2006.
Names and similarities on the web: fact extraction in
the fast lane. COLING-ACL ?06.
Roark, B., Charniak, E., 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lex-
icon construction. ACL ?98.
Rosenfeld B., Feldman, R.: URES : an unsupervised
web relation extraction system. Proceedings, ACL ?06
Poster Sessions.
Sekine, S., 2006 On-demand information extraction.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Suchanek F. M., G. Ifrim, G. Weikum. 2006. LEILA:
learning to extract information by linguistic analysis.
Workshop on Ontology Learning and Population at
COLING-ACL ?06.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Widdows, D., Dorow, B., 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ?02.
239
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1449?1454,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatically Identifying Pseudepigraphic Texts 
Moshe Koppel 
Bar Ilan University 
Ramat-Gan, 52900, Israel  
moishk@gmail.com 
Shachar Seidman 
Bar Ilan University 
Ramat-Gan, 52900, Israel 
shachar9@gmail.com 
 
 
Abstract 
The identification of pseudepigraphic texts ? 
texts not written by the authors to which they 
are attributed ? has important historical, fo-
rensic and commercial applications. We in-
troduce an unsupervised technique for identi-
fying pseudepigrapha. The idea is to identify 
textual outliers in a corpus based on the pair-
wise similarities of all documents in the cor-
pus. The crucial point is that document simi-
larity not be measured in any of the standard 
ways but rather be based on the output of a re-
cently introduced algorithm for authorship ve-
rification. The proposed method strongly 
outperforms existing techniques in systematic 
experiments on a blog corpus. 
1 Introduction 
The Shakespeare attribution problem is centuries 
old and shows no signs of abating. Some scholars 
argue that some, or even all, of Shakespeare?s 
works were not actually written by him. The most 
mainstream theory ? and the one that interests us 
here ? is that most of the works were written by 
Shakespeare, but that several of them were not. 
Could modern methods of computational author-
ship attribution be used to detect which, if any, of 
the works attributed to Shakespeare were not writ-
ten by him? 
More generally, this paper deals with the unsu-
pervised problem of detecting pseudepigrapha: 
documents in a supposedly single-author corpus 
that were not actually written by the corpus?s pre-
sumed author. Studies as early as Mendenhall 
(1887), have observed that texts by a single author 
tend to be somewhat homogeneous in style. If this 
is indeed the case, we would expect that pseudepi-
grapha would be detectable as outliers.  
Identifying such outlier texts is, of course, a 
special case of general outlier identification, one of 
the central tasks of statistics. We will thus consider 
the pseudepigrapha problem in the context of the 
more general outlier detection problem. 
Typically, research on textual outliers assumes 
that we have a corpus of known authentic docu-
ments and are asked to decide if a specified other 
document is authentic or not (Juola and Stamata-
tos, 2013). One crucial aspect of our problem is 
that we do not assume that any specific text in a 
corpus is known a priori to be authentic or pseude-
pigraphic; we can assume only that most of the 
documents in the corpus are authentic. 
The method we introduce in this paper builds on 
the approach of Koppel and Winter (2013) for de-
termining if two documents are by the same au-
thor. We apply that method to every pair of 
documents in a corpus and use properties of the 
resulting adjacency graph to identify outliers. In 
the following section, we briefly outline previous 
work. In Section 3 we provide a framework for 
outlier detection and in Section 4 we describe our 
method. In Section 5 we describe the experimental 
setting and give results and in Section 6 we present 
results for the plays of Shakespeare. 
2 Related Work 
Identifying outlier texts consists of two main stag-
es: first, representing each text as a numerical vec-
tor representing relevant linguistic features of the 
text and second, using generic methods to identify 
outlier vectors.  
There is a vast literature on generic methods for 
outlier detection, summarized in Hodge & Austin 
(2004) and Chandola et al (2009). Since our prob-
1449
lem setup does not entail obtaining any labeled 
examples of authentic or outlier documents, super-
vised and semi-supervised methods are inapplica-
ble. The available methods are unsupervised, 
principally probabilistic or proximity-based me-
thods. A classical variant of such methods for un-
ivariate normally distributed data uses the the z-
score (Grubbs, 1969). Such simple univariate out-
lier detectors are, however, inappropriate for iden-
tifying outliers in a high-dimensional textual 
corpus. Subsequent work, such as the Stahel-
Donoho Estimator (Stahel, 1981; Donoho, 1982), 
PCout (Filzmoser et al, 2008), LOF (Breunig and 
Kriegel, 2000) and ABOD (Kriegel et al, 2008) 
have generalized univariate methods to high-
dimensional data points. 
In his comprehensive review of outlier detection 
methods in textual data, Guthrie (2008) compares a 
variety of vectorization methods along with a va-
riety of generic outlier methods. The vectorization 
methods employ a variety of lexical and syntactic 
stylistic features, while the outlier detection me-
thods use a variety of similarity/distance measures 
such as cosine and Euclidean distance. Similar me-
thods have also been used in the field of intrinsic 
plagiarism detection, which involves segmenting a 
text and then identifying outlier segments (Stama-
tatos, 2009; Stein et al, 2010). 
3 Proximity Methods 
Formally, the problem we wish to solve is defined 
as follows: Given a set of documents D = 
{d1,?,dn}, all or most of which were written by 
author A, which, if any, documents in D were not 
written by A? 
We begin by considering the kinds of proximity 
methods for textual outlier detection considered by 
Guthrie (2008) and in the work on intrinsic plagiar-
ism detection; these will serve as baseline methods 
for our approach. The idea is simple: mark as an 
outlier any document that is too far from the rest of 
the documents in the corpus.  
We briefly sketch the key steps: 
1. Represent a document as a numerical vector.  
The kinds of measurable features that can be 
used to represent a document include frequen-
cies of word unigrams, function words, parts-
of-speech and character n-grams, as well as 
complexity measures such as type/token ratio, 
sentence and word length and so on. 
2. Measure the similarity of two document vec-
tors. 
We can use either inverses of distance meas-
ures such as Euclidean distance or Manhattan 
distance, or else direct similarity measures 
such as cosine or min-max. 
3. Use an aggregation method to measure the 
similarity of a document to a set of documents. 
One approach is to simply measure the dis-
tance from a document to the centroid of all 
the other documents (centroid method). 
Another approach is to first measure the simi-
larity of a document to each other document 
and then to aggregate the results by averaging 
all the obtained values (mean method): 
 
Alternatively, we can average the values only 
for the k nearest neighbors (k-NN method): 
 
(where Dk = k nearest neighbors of di). 
Yet another method is to use median distance 
(median method).  
 
We note that the centroid method and mean 
method suffer from the masking effect (Bendre 
and Kale, 1987; Rousseeuw and Leroy, 2003): 
the presence of some outliers in the data can 
greatly distort the estimator's results regarding 
the presence of other outliers. The k-NN me-
thod and the median method are both much 
more robust. 
4. Choose some threshold beyond which a docu-
ment is marked as an outlier.  
Choosing the threshold is one of the central is-
sues in statistical approaches. For our purpos-
es, however, the choice of threshold is simply 
a parameter trading off recall and precision. 
4 Second-Order Similarity 
Our approach is to use an entirely different kind of 
similarity measure in Step 2. Rather than use a 
first-order similarity measure, as is customary, we 
employ a second-order similarity measure that is 
the output of an algorithm used for the authorship 
verification problem (Koppel et al 2011), in which 
we need to determine if two, possibly short, docu-
ments were written by the same author.  
That algorithm, known as the ?impostors me-
thod? (IM), works as follows. Given two docu-
1450
ments, d1 and d2, generate an appropriate set of 
impostor documents, p1,?,pm and represent each 
of the documents in terms of some large feature set 
(for example, the frequencies of various words or 
character n-grams in the document). For some ran-
dom subset of the feature set, measure the similari-
ty of d1 to d2 as well as to each of the documents 
p1,?,pm and note if d1 is closer to d2 than to any of 
the impostors. Repeat this k times, choosing a dif-
ferent random subset of the features in each itera-
tion. If d1 is closer to d2 than to any of the 
impostors (and likewise switching the roles of d1 
and d2) for at least ?% of iterations, then output 
that d2 and d1 are the same author. (The parameter 
? is used to trade-off recall and precision.)  
Adapting that method for our purposes, we use 
the proportion of iterations for which d1 is closer to 
d2 than to any of the impostors as our similarity 
measure (adding a small twist to make the measure 
symmetric over d1 and d2, as can be seen in line 
2.2.2 of the algorithm). More precisely, we do the 
following:  
 
Given: Corpus D={d1,?,dn} 
1. Choose a feature set FS for representing documents, a 
first-order similarity measure sim, and an impostor set 
{p1,?,pm}. 
2. For each pair of documents <di, dj> in set D: 
2.1. Let sim2(di, dj) := 0 
2.2. Iterate K times: 
2.2.1. Randomly choose 40% of features in FS 
2.2.2. If sim(di, dj)
 2  >   
maxu?{1,..,m}sim(di,  pu)*maxu?{1,..,m}sim(dj, pu), 
then sim2(di, dj) ? sim2(di, dj) + 1/K 
3. For each document di in set D: 
3.1. Compute sim2(di, D) = agg w?{1,..,n}[sim2(di, dw)] 
where agg is some aggregation function  
3.2. If sim2(di, D) < ? (where ? is a parameter),  
then mark di as outlier. 
 
The method for choosing the impostor set is 
corpus-dependent, but quite straightforward: we 
simply choose random impostors from the same 
genre and language as the documents in question. 
The choice of feature set FS, first-order similarity 
measure sim, and aggregation function agg can be 
varied. For FS, we simply use bag-of-words 
(BOW). As for sim and agg, we show below re-
sults of experiments comparing the effectiveness of 
various choices for these parameters.  
Using second-order similarity has several sur-
face advantages over standard first-order measures. 
First, it is decisive: for most pairs, second-order 
similarity will be close to 0 or close to 1. Second, it 
is self-normalizing: scaling doesn?t depend on the 
size of the underlying feature sets or the lengths of 
the documents. As we will see, it is also simply 
much more effective for identifying outliers. 
5 Experiments 
We begin by assembling a corpus consisting of 
3540 blog posts written by 156 different bloggers. 
The blogs are taken from the blog corpus assem-
bled by Schler et al (2006) for use in authorship 
attribution tasks. Each of the blogs was written in 
English by a single author in 2004 and each post 
consists of 1000 words (excess is truncated). 
For our initial experiments, each trial consists of 
10 blog posts, all but p of which are by a single 
blogger. The number of pseudepigraphic docu-
ments, p, is chosen from a uniform distribution 
over the set {0,1,2,3}. Our task is to identify 
which, if any, documents in the set are not by the 
main author of the set. The pseudepigraphic docu-
ments might be written by a single author or by 
multiple authors. 
To measure the performance of a given similari-
ty measure sim, we do the following in each trial: 
1. Represent each document in the trial set D 
in terms of BOW. 
2. Measure the similarity of each pair of doc-
uments in the trial set using the similarity 
measure sim. 
3. Using some aggregation function agg, 
compute for each document di:   
sim(di, D) = agg w?{1,..,n}[sim(di, dw)]. 
4. If sim (di, D) < ?, mark di as an outlier 
(where ? is a parameter ). 
 
Our objective is to show that results using 
second-order similarity are stronger than those us-
ing first-order similarity. Before we do this, we 
need to determine the best aggregation function to 
use in our experiments. In Figure 1, we show re-
call-precision breakeven values (for the outlier 
class) over 250 independent trials, for each of our 
four first-order similarity measures (inverse Eucli-
dean, inverse Manhattan, cosine, min-max) used in 
conjunction with each of four aggregation func-
tions (centroid, mean, k-NN mean, median). As is 
evident, k-NN is the best aggregation function in 
each case. We will give these baseline methods an 
advantage by using k-NN as our aggregation func-
tion in all our subsequent experiments. 
1451
 
Figure 1. Breakeven values on first-order similarity 
measures with various aggregation functions. 
 
 We are now ready to perform our main expe-
riment. We use BOW as our feature set and k-NN 
as our aggregation function. We use 500 random 
blog posts as our impostor set. In Figure 2, we 
show recall-precision curves for outlier documents 
over 250 independent trials, as just described, us-
ing four first-order similarity measures as well our 
second-order similarity measure using each of the 
four as a base measure. As can be seen, even the 
worst second-order similarity measure significantly 
outperforms all the standard first-order measures. 
In Figure 3, we show the breakeven values for each 
measure, pairing each first-order measure with the 
second-order measure that uses it as a base. Clear-
ly, the mere use of a second-order method im-
proves results, regardless of the base measure. 
 
 
Figure 2. Recall-precision curves for four first-order 
similarity measures and four second-order similarity 
measures, based on 250 trials of 10 documents each. 
 
 
Figure 3. Breakeven values for first-order measures and 
corresponding second-order measures. 
 
 Thus far we have considered authorial corpora 
consisting of only ten documents. In Figures 4 and 
5, we repeat the experiment described in Figures 2 
and 3 above, but with each trial consisting of 50 
documents including any number of pseudepi-
graphic documents in the range 0 to 15. The same 
phenomenon is apparent: second-order similarity 
strongly improves results over the corresponding 
first-order base similarity measure.  
 
 
Figure 4. Recall-precision curves for four first-order 
similarity measures and four second-order similarity 
measures, based on 250 trials of 50 documents each. 
 
1452
 
Figure 5. Breakeven values for first-order measures and 
corresponding second-order measures 
6 Results on Shakespeare 
We applied our methods to the texts of 42 plays by 
Shakespeare (taken from Project Gutenberg). We 
included two plays by Thomas Kyd as sanity 
checks. In addition, we included three plays occa-
sionally attributed to Shakespeare, but generally 
regarded by authorities as pseudepigrapha (A York-
shire Tragedy, The Life of Sir John Oldcastle and 
Pericles Prince of Tyre). We also included King 
Edward III and King Henry VI (Part 1), both of 
which are subjects of dispute among Shakespeare 
scholars. As impostors we used 39 works by con-
temporaries of Shakespeare, including Christopher 
Marlowe, Ben Jonson and John Fletcher.  
We found that the two plays by Thomas Kyd 
and the three pseudepigraphic plays were all 
among the seven furthest outliers, as one would 
expect. In addition, King Edward III was 9th fur-
thest. King Henry VI (Part 1) was not found to be 
an outlier at all. Curiously, however, three undis-
puted plays by Shakespeare were found to be 
greater outliers than King Edward III. These are 
The Merry Wives of Windsor, The Comedy of Er-
rors and The Tragedy of Julius Caesar. The Merry 
Wives of Windsor is a particularly distant outlier, 
even further out than Oldcastle and Pericles. We 
leave it to Shakespeare scholars to explain the rea-
sons for these anomalies. 
7 Conclusion 
In this paper we defined the problem of unsuper-
vised outlier detection in the authorship verifica-
tion domain. Our method combines standard 
outlier detection methods with a novel inter-
document similarity measure. This similarity 
measure is the output of the impostors method re-
cently developed for solving the authorship verifi-
cation problem. We have found that use of the 
kNN method for outlier detection in conjunction 
with this second-order similarity measure strongly 
outperforms methods based on any outlier detec-
tion method used in conjunction with any standard 
first-order similarity measures. This improvement 
proves to be robust, holding for various corpus siz-
es and various underlying base similarity measures 
used in the second-order similarity measure. 
The method can be used to resolve historical 
conundrums regarding the authenticity of works in 
questioned corpora, such as the Shakespeare cor-
pus briefly considered here. This is currently the 
subject of our ongoing research. 
 
References 
S. M. Bendre and B. K. Kale. 1987. Masking effect 
on tests for outliers in normal samples, Biome-
trika, 74(4):891-896.  
Markus M. Breunig,  Hans-Peter Kriegel, 
Raymond T. Ng and J?rg Sander. 2000. LOF: 
Identifying Density-Based Local Outliers, ACM 
SIGMOD Conference Proceedings. 
Varun Chandola, Arindam Banerjee and Vipin 
Kumar. 2009. Anomaly detection: a survey. 
ACM Computing Surveys 41, 3, Article 15.  
David L. Donoho. 1982. Breakdown properties of 
multivariate location estimators. Ph.D. 
qualifying paper, Harvard University. 
Peter Filzmoser, Ricardo Maronna and Mark 
Werner. 2008. Outlier identification in high 
dimensions. Computational Statistics and Data 
Analysis, 52:1694-1711. 
David Guthrie. 2008. Unsupervised Detection of 
Anomalous Text. PhD Thesis, University of 
Sheffield.  
Frank E. Grubbs. 1969. Procedures for detecting 
outlying observations in samples, 
Technometrics. 
V.J. Hodge and J. Austin. 2004. A survey of outlier 
detection methodologies. Artificial. Intelligence 
Review, 22 (2). pp. 85-126. 
1453
Patrick Juola and Efstathios Stamatatos. 2013. 
Overview of the Author Identification Task at 
PAN 2013. P. Forner, R. Navigli, and D. Tufis 
(eds) CLEF 2013 Evaluation Labs and 
Workshop ?Working Notes Papers. 
Moshe Koppel and Jonathan Schler 2004. 
Authorship verification as a one-class 
classification problem. In ICML ?04: Twenty-
first International Conference on Machine 
Learning, New York, NY, USA. 
Moshe Koppel, Jonathan Schler, and Shlomo 
Argamon. 2011. Authorship attribution in the 
wild. Language Resources and Evaluation, 
45(1): 83?94. 
Moshe Koppel M. and Yaron Winter. 2013. 
Determining If Two Documents Are by the Same 
Author. J. Am. Soc. Inf. Sci. Technol. 
Frederick Mosteller and David L. Wallace. 1964. 
Inference and Disputed Authorship: The 
Federalist. Reading, Mass. Addison Wesley. 
Hans-Peter Kriegel, Matthias S. Schubert and 
Arthur Zimek. 2008. Angle-based outlier 
detection in high dimensional data. Proc. KDD. 
Thomas C. Mendenhall. 1887. The characteristic 
curves of composition, Science 9, 237-259. 
Sridhar Ramaswamy, Rajeev Rastogi and Kyuseok 
Shim. 2000. Efficient Algorithms for Mining 
Outliers from Large Data Sets. Proc. ACM 
SIDMOD Int. Conf. on Management of Data. 
Peter J. Rousseeuw. 1984. Least median of squares 
regression. Journal of the American Statistical 
Association, 79(388):87-880. 
Peter J. Rousseeuw and Annick M. Leroy. 2003. 
Robust Regression and Outlier Detection. John 
Wiley & Sons. 
J. Schler, M. Koppel, S. Argamon and J. 
Pennebaker. 2006. Effects of Age and Gender on 
Blogging. in Proceedings of 2006 AAAI Spring 
Symposium on Computational Approaches for 
Analyzing Weblogs. 
Werner A. Stahel. 1981. Breakdown of covariance 
estimators. Research Report 31, Fachgruppe f?ur 
Statistik, Swiss Federal Institute of Technology 
(ETH), Zurich. 
Efstathios Stamatatos. 2009. Intrinsic plagiarism 
detection using character n-gram profiles. 
Proceedings of the SEPLN?09 Workshop on 
Uncovering Plagiarism, Authorship and Social 
Software Misuse. pp. 38?46. 
Benno Stein B, Nedim Lipka and Peter 
Prettenhofer. 2010. Intrinsic Plagiarism 
Analysis. Language Resources and Evaluation, 
1?20. 2010. 
Benno Stein B, Nedim Lipka and Peter 
Prettenhofer. 2010. Intrinsic Plagiarism 
Analysis. Language Resources and Evaluation, 
1?20. 2010. 
 
1454
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880?1891,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Authorship Attribution of Micro-Messages
Roy Schwartz Oren Tsur Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
{roys02|oren|arir}@cs.huji.ac.il
Moshe Koppel
Department of Computer Science
Bar Ilan University
koppel@macs.biu.ac.il
Abstract
Work on authorship attribution has tradition-
ally focused on long texts. In this work, we
tackle the question of whether the author of
a very short text can be successfully iden-
tified. We use Twitter as an experimental
testbed. We introduce the concept of an au-
thor?s unique ?signature?, and show that such
signatures are typical of many authors when
writing very short texts. We also present a new
authorship attribution feature (?flexible pat-
terns?) and demonstrate a significant improve-
ment over our baselines. Our results show that
the author of a single tweet can be identified
with good accuracy in an array of flavors of
the authorship attribution task.
1 Introduction
Research in authorship attribution has developed
substantially over the last decade (Stamatatos,
2009). The vast majority of such research has been
dedicated towards finding the author of long texts,
ranging from single passages to book chapters. In
recent years, the growing popularity of social me-
dia has created special interest, both theoretical and
computational, in short texts. This has led to many
recent authorship attribution projects that experi-
mented with web data such as emails (Abbasi and
Chen, 2008), web forum messages (Solorio et al,
2011) and blogs (Koppel et al, 2011b). This paper
addresses the question to what extent the authors of
very short texts can be identified. To answer this
question, we experiment with Twitter tweets.
Twitter messages (tweets) are limited to 140 char-
acters. This restriction imposes major difficulties on
authorship attribution systems, since authorship at-
tribution methods that work well on long texts are
often not as useful when applied to short texts (Bur-
rows, 2002; Sanderson and Guenter, 2006).
Nonetheless, tweets are relatively self-contained
and have smaller sentence length variance com-
pared to excerpts from longer texts (see Section 3).
These characteristics make Twitter data appealing as
a testbed when focusing on short texts. Moreover,
an authorship attribution system of tweets may have
various applications. Specifically, a range of cyber-
crimes can be addressed using such a system, includ-
ing identity fraud and phishing.
In this paper, we introduce the concept of k-
signatures. We denote the k-signatures of an author
a as the features that appear in at least k% of a?s
training samples, while not appearing in the training
set of any other author. When k is large, such signa-
tures capture a unique style used by a. An analysis
of our training set reveals that unique k-signatures
are typical of many authors. Moreover, a substantial
portion of the tweets in our training set contain at
least one such signature. These findings suggest that
a single tweet, although short and sparse, often con-
tains sufficient information for identifying its author.
Our results show that this is indeed the case.
We train an SVM classifier with a set of features
that include character n-grams and word n-grams.
We use a rigorous experimental setup, with varying
number of authors (values between 50-1,000) and
various sizes of the training set, ranging from 50 to
1,000 tweets per author. In all our experiments, a
single tweet is used as test document. We also use
a setting in which the system is allowed to respond
don?t know in cases of uncertainty. Applying this
option results in higher precision, at the expense of
1880
lower recall.
Our results show that the author of a tweet can be
successfully identified. For example, when using a
dataset of as many as 1,000 authors with 200 train-
ing tweets per author, we are able to obtain 30.3%
accuracy (as opposed to a random baseline of only
0.1%). Using a dataset of 50 authors with as few
as 50 training tweets per author, we obtain 50.7%
accuracy. Using a dataset of 50 authors with 1,000
training tweets per author, our results reach as high
as 71.2% in the standard classification setting, and
exceed 91% accuracy with 60% recall in the don?t
know setting.
We also apply a new set of features, never previ-
ously used for this task ? flexible patterns. Flexi-
ble patterns essentially capture the context in which
function words are used. The effectiveness of func-
tion words as authorship attribution features (Koppel
et al, 2009) suggests using flexible pattern features.
The fact that flexible patterns are learned from plain
text in a fully unsupervised manner makes them
domain and language independent. We demon-
strate that using flexible patterns gives significant
improvement over our baseline system. Further-
more, using flexible patterns, our system obtains a
6.1% improvement over current state-of-the-art re-
sults in authorship attribution on Twitter.
To summarize, the contribution of this paper is
threefold.
? We provide the most extensive research to date
on authorship attribution of micro-messages,
and show that authors of very short texts can
be successfully identified.
? We introduce the concept of an author?s unique
k-signature, and demonstrate that such signa-
tures are used by many authors in their writing
of micro-messages.
? We present a new feature for authorship attri-
bution ? flexible patterns ? and show its sig-
nificant added value over other methods. Us-
ing this feature, our system obtains a 6.1% im-
provement over the current state-of-the-art.
The rest of the paper is organized as follows. Sec-
tions 2 and 3 describe our methods and our experi-
mental testbed (Twitter). Section 4 presents the con-
cept of k-signatures. Sections 5 and 6 present our
experiments and results. Flexible patterns are pre-
sented in Section 7 and related work is presented in
Section 8.
2 Methodology
In the following we briefly describe the main fea-
tures employed by our system. The features below
are binary features.
Character n-grams. Character n-gram features
are especially useful for authorship attribution on
micro-messages since they are relatively tolerant
to typos and non-standard use of punctuation (Sta-
matatos, 2009). These are common in the non-
formal style generally applied in social media ser-
vices. Consider the example of misspelling ?Brit-
ney? as ?Brittney?. The misspelled name shares the
4-grams ?Brit? and ?tney? with the correct name. As
a result, these features provide information about the
author?s style (or at least her topic of interest), which
is not available through lexical features.
Following standard practice, we use 4-grams
(Sanderson and Guenter, 2006; Layton et al, 2010;
Koppel et al, 2011b). White spaces are considered
characters (i.e., a character n-gram may be com-
posed of letters from two different words). A sin-
gle white-space is appended to the beginning and
the end of each tweet. For efficiency, we consider
only character n-gram features that appear at least
tcng times in the training set of at least one author
(see Section 5).
Word n-grams. We hypothesize that word n-gram
features would be useful for authorship attribution
on micro-messages. We assume that under a strict
length restriction, many authors would prefer using
short, repeating phrases (word n-grams).
In our experiments, we consider 2 ? n ? 5.1
We regard sequences of punctuation marks as words.
Two special words are added to each tweet to indi-
cate the beginning and the end of the tweet. For effi-
ciency, we consider only word n-gram features that
appear at least twng times in the training set of at
least one author (see Section 5).
Model. We use libsvm?s Matlab implementation
of a multi-class SVM classifier with a linear kernel
1We skip unigrams as they are generally captured by the
character n-gram features.
1881
(Chang and Lin, 2011). We use ten-fold cross vali-
dation on the training set to select the best regular-
ization factor between 0.5 and 0.005.2
3 Experimental Testbed
Our main research question in this paper is to deter-
mine the extent to which authors of very short texts
can be identified. A major issue in working with
short texts is selecting the right dataset. One ap-
proach is breaking longer texts into shorter chunks
(Sanderson and Guenter, 2006). We take a differ-
ent approach and experiment with micro-messages
(specifically, tweets).
Tweets have several properties making them an
ideal testbed for authorship attribution of short texts.
First, tweets are posted as single units and do not
necessarily refer to each other. As a result, they tend
to be self contained. Second, tweets have more stan-
dardized length distribution compared to other types
of web data. We compared the mean and standard
deviation of sentence length in our Twitter dataset
and in a corpus of English web data (Ferraresi et al,
2008).3 We found that (a) tweets are shorter than
standard web data (14.2 words compared to 20.9),
and (b) the standard deviation of the length of tweets
is much smaller (6.4 vs. 21.4).
Pre-Processing. We use a Twitter corpus that in-
cludes approximately 5 ? 108 tweets.4 All non-
English tweets and tweets that contain fewer than
3 words are removed from the dataset. We also re-
move tweets marked as retweets (using the RT sign,
a standard Twitter symbol to indicate that this tweet
was written by a different user). As some users
retweet without using the RT sign, we also remove
tweets that are an exact copy of an existing tweet
posted in the previous seven days.
Apart from plain text, some tweets contain ref-
erences to other Twitter users (in the format of
@<user>). Since using reference information
makes this task substantially easier (Layton et al,
2010), we replace each user reference with the spe-
cial meta tag REF. For sparsity reasons, we also re-
place web addresses with the meta tag URL, num-
2In practice, 0.05 or 0.1 are selected in almost all cases.
3http://wacky.sslmit.unibo.it
4These comprise ?15% of all public tweets created from
May 2009 to March 2010.
0 5 10 15 20 25 30 35 40 45 >50
0
10
20
30
40
50
60
70
80
90
Number of k?signatures per user
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 1: Number of users with at least x k-signatures
(100 authors, 180 training tweets per author).
bers with the meta tag NUM, time of day with the
meta tag TIME and dates with the meta tag DATE.
4 k-Signatures
In this section, we show that many authors adopt
a unique style when writing micro-messages. This
style can be detected by a strong classification algo-
rithm (such as SVM), and be sufficient to correctly
identify the author of a single tweet.
We define the concept of the k-signature of an au-
thor a to be a feature that appears in at least k% of
a?s training set, while not appearing in the training
set of any other user. Such signatures can be useful
for identifying future (unlabeled) tweets written by
a.
To validate our hypothesis, we use a dataset of
100 authors with 180 tweets per author. We com-
pute the number of k-signatures used by each of
the authors in our dataset. Figure 1 shows our re-
sults for a range of k values (2%, 5%, 10%, 20%
and 50%). Results demonstrate that 81 users use
at least one 2%-signature, 43 users use at least one
5%-signature, and 17 users use at least one 10%-
signature. These results indicate that a large portion
of the users adopt a unique signature (or set of sig-
natures) when writing short texts. Table 1 provides
examples of 10%-signatures.
1882
Signature Type 10%-signature Examples
Character n-grams
? ? ??
REF oh ok ? ? Glad you found it!
Hope everyone is having a good afternoon ? ?
REF Smirnoff lol keeping the goose in the freezer ? ?
?yew ?
gurl yew serving me tea nooch
REF about wen yew and ronnie see each other
REF lol so yew goin to check out tini?s tonight huh???
Word n-grams
.. lal
REF aww those are cool where u get those.. how do ppl react.. lal
Ludas album is gone be hott.. lal
Dayum refs don?t get injury timeouts.. lal.. get him off the field..
smoochies , e3
I?m just back after takin? a very long, icy cold
shower........Shivering smoochies,E3 http://bit.ly/4CzzP9
A blue stout or two would be nice as well, Purr!Blue smooth
smoochies,E3 http://bit.ly/75D4fO
That is sooooooooooooooooooo unfair!Double smoochies,E3
http://bit.ly/07sXRGX
Table 1: Examples of 10%-signatures.
Results also show that seven users use one or
more 20%-signatures, and five users even use one
or more 50%-signatures. Looking carefully at these
users, we find that they write very structured mes-
sages, and are probably bots, such as news feeds,
bidding systems, etc. Table 2 provides examples of
tweets posted by such users.5
Another interesting question is how many tweets
contain at least one k-signature. Figure 2 shows
for each user the number of tweets in her training
set for which at least one k-signature is found. Re-
sults demonstrate that a total of 18.6% of the train-
ing tweets contain at least one 2%-signature, 10.3%
the training tweets contain at least one 5%-signature
and 6.5% of the training tweets contain at least one
10%-signature. These findings validate our assump-
tion that many users use k-signatures in short texts.
These findings also have direct implications on
authorship attribution of micro-messages, since k-
signatures are reliable classification features. As
a result, texts written by authors that tend to use
k-signatures are likely to be easily identified by a
reasonable classification algorithm. Consequently,
k-signatures provide a possible explanation for the
high quality results presented in this paper.
In the broader context, the presence (and contri-
5Our k-signature method can actually be useful for automat-
ically identifying such users. We defer this to future work.
0 20 40 60 80 100 120 140 160 180
0
10
20
30
40
50
60
70
80
90
Number of Tweets with at least one k?Signature
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 2: Number of users with at least x training tweets
that contain at least one k-signature (100 authors, 180
training tweets per author).
bution) of k-signatures is in line with the hypothesis
proposed by (Davidov et al, 2010a): while still us-
ing an informal and unstructured (grammatical) lan-
guage, authors tend to use typical and unique struc-
tures in order to allow a short message to stand alone
without a clear conversational context.
1883
User 20%-signature Examples
1 I?m listening to :
I?m listening to: Sigur R?s ? Intro:
http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb
I?m listening to: Tina Arena ? In Command:
http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25
I?m listening to: Midnight Oil ? Under the Overpass:
http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg
2 news now ( str )
#Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks of
the HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging
#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-
resentatives press to reneg... http://bit.ly/bHPn2L
#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-
try?s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging
3
( NUM bids )
end date :
NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +
CASE: &#163;66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..
http://bit.ly/7uPt6V
Microsoft Xbox 360 Game System - Console Only - Working: US $51.99
(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTv
Microsoft Sony Playstation 3 (80 GB) Console 6 Months Old:
&#163;190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..
http://bit.ly/7kwtDS
Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-
signatures.
5 Experiments
We report of three different experimental configu-
rations. In the experiments described below, each
dataset is divided into training and test sets using
ten-fold cross validation. On the test phase, each
document contains a single tweet.
Experimenting with varying Training Set Sizes.
In order to test the affect of the training set size,
we experiment with an increasingly larger number
of tweets per author. Experimenting with a range of
training set sizes serves two purposes: (a) to check
whether the author of a tweet can be identified us-
ing a very small number of (short) training samples,
and (b) check howmuch our system can benefit from
training on a larger corpus.
In our experiments we only consider users who
posted between 1,000?2,000 tweets6 (a total of
6This range is selected since on one hand we want at least
1,000 tweets per author for our experiments, and on the other
hand we noticed that users with a larger number of tweets in
corpus tend to be spammers or bots that are very easy to identify,
so we limit this number to 2,000.
10,183 users), and randomly select 1,000 tweets per
user. From these users, we select 10 groups of 50
users each.7 We perform a set of classification ex-
periments, selecting for each author an increasingly
larger subset of her 1,000 tweets as training set. Sub-
set sizes are (50, 100, 200, 500, 1,000). Thresh-
old values for our features in each setting (see Sec-
tion 2) are (2, 2, 4, 10, 20) for tcng and (2, 2, 2, 3, 5)
for twng, respectively.
Experimenting with varying Numbers of Au-
thors. In a second set of experiments, we use an
increasingly larger number of authors (values be-
tween 100-1,000), in order to check whether the au-
thor of a very short text can be identified in a ?needle
in a haystack? type of setting.
Due to complexity issues, we only experiment
with 200 tweets per author as training set. We se-
lect groups of size 100, 200, 500 and 1,000 users
(one group per size). We use the same threshold val-
ues as the 200 tweets per author setting previously
described (tcng = 4, twng = 2).
7An eleventh group is selected as development set.
1884
0 100 200 300 400 500 600 700 800 900 1000
45
50
55
60
65
70
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 3: Authorship attribution accuracy for 50 authors
with various training set sizes. The values are averaged
over 10 groups. The random baseline is 2%.
Recall-Precision Tradeoff. Another aspect of our
research question is the level of certainty our system
has when suggesting an author for a given tweet.
In cases of uncertainty, many real life applications
would prefer not to get any response instead of get-
ting a response with low certainty. Moreover, in real
life applications we are often not even sure that the
real author is part of our training set. Consequently,
we allow our system to respond ?don?t know? in
cases of low confidence (Koppel et al, 2006; Kop-
pel et al, 2011b). This allows our system to obtain
higher precision, at the expense of lower recall.
To implement this feature, we use SVM?s proba-
bility estimates, as implemented in libsvm. These
estimates give a score to each potential author.
These scores reflect the probability that this author
is the correct author, as decided by the prediction
model. The selected author is always the one with
the highest probability estimate.
As selection criterion, we use a set of increasingly
larger thresholds (0.05-0.9) for the probability of the
selected author. This means that we do not select test
samples for which the selected author has a proba-
bility estimate value lower than the threshold.
0 100 200 300 400 500 600 700 800 900 1000
25
30
35
40
45
50
55
60
Number of Candidate Authors
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 4: Authorship attribution accuracy with varying
number of candidate authors, using 200 training tweets
per author. The random baselines for 509, 100, 200, 500
and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,
respectively.
6 Basic Results
Experimenting with varying Training Set Sizes.
Figure 3 shows results for our experiments with
50 authors and various training set sizes. Results
demonstrate that authors of very short texts can be
successfully identified, even with as few as 50 tweets
per author (49.5%). When given more training sam-
ples, authors are identified much more accurately
(up to 69.7%). Results also show that, according to
our hypothesis, word n-gram features substantially
improve over character n-grams features only (3%
averaged improvement over all settings).
Experimenting with varying Numbers of Au-
thors. Figure 4 shows our results for various num-
bers of authors, using 200 tweets per author as train-
ing set. Results demonstrate that authors of an
unknown tweet can be identified to a large extent
even when there are as many as 1,000 candidate au-
thors (30.3%, as opposed to a random baseline of
only 0.1%). Results further validate that word n-
gram features substantially improve over character
9Results for 50 authors with 200 tweets per author are taken
from Figure 3.
1885
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
1,000 tweets/author
500 tweets/author
200 tweets/author
100 tweets/author
50 tweets/author
Figure 5: Recall-precision curves for 50 authors with
varying training set sizes.
n-grams features (2.6% averaged improvement).
Recall-Precision Tradeoff. Figure 5 shows the
recall-precision curves for our experiments with 50
authors and varying training set sizes. Results
demonstrate that we are able to obtain very high pre-
cision (over 90%) while still maintaining a relatively
high recall (from ?35% recall for 50 tweets per au-
thor up to> 60% recall for 1,000 tweets per author).
Figure 6 shows the recall-precision curves for our
experiments with varying number of authors. Re-
sults demonstrate that even in the 1,000 authors set-
ting, we are able to obtain high precision values
(90% and 70%) with reasonable recall values (18%
and ?30%, respectively).
7 Flexible Patterns
In previous sections we provided strong evidence
that authors of micro-messages can be successfully
identified using standard methods. In this section we
present a new feature, never previously used for this
task ? flexible patterns. We show that flexible pat-
terns can be used to improve classification results.
Flexible patterns are a generalization of word n-
grams, in the sense that they capture potentially un-
seen word n-grams. As a result, flexible patterns
can pick up fine-grained differences between au-
thors? styles. Unlike other types of pattern features,
0 10 20 30 40 50 60 70 80 90 100
30
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
50 authors
100 authors
200 authors
500 authors
1,000 authors
Figure 6: Recall-precision curves for varying number of
authors.
flexible patterns are computed automatically from
plain text. As such, they can be applied to various
tasks, independently of domain and language. We
describe them in detail.
Word Frequency. Flexible patterns are composed
of high frequency words (HFW) and content words
(CW). Every word in the corpus is defined as either
HFW or CW. This clustering is performed by count-
ing the number of times each word appears in the
corpus of size s. A word that appears more than
10?4?s times in a corpus is considered HFW. A
word that appears less than 10?3?s times in a cor-
pus is considered CW. Some words may serve both
as HFWs and CWs (see Davidov and Rappoport
(2008b) for discussion).
Structure of a Flexible Pattern. Flexible patterns
start and end with an HFW. A sequence of zero or
more CWs separates consecutive HFWs. At least
one CW must appear in every pattern.10 For effi-
ciency, at most six HFWs (and as a result, five CW
sequences) may appear in a flexible pattern. Exam-
ples of flexible patterns include
1. ?theHFW CW ofHFW theHFW?
10Omitting this treats word n-grams as flexible patterns.
1886
Flexible Pattern Features. Flexible patterns can
serve as binary classification features; a tweet
matches a given flexible pattern if it contains the
flexible pattern sequence. For example, (1) is
matched by (2).
2. ?Go to theHFW houseCW ofHFW theHFW rising sun?
Partial Flexible Patterns. A flexible pattern may
appear in a given tweet with additional words not
originally found in the flexible pattern, and/or with
only a subset of the HFWs (Davidov et al, 2010a).
For example, (3) is a partial match of (1), since the
word ?great? is not part of the original flexible pat-
tern. Similarly, (4) is another partial match of (1),
since (a) the word ?good? is not part of the original
flexible pattern and (b) the second occurrence of the
word ?the? does not appear in (4) (missing word is
marked by ).
3. ?TheHFW greatHFW kingCW ofHFW theHFW ring?
4. ?TheHFW goodHFW kingCW ofHFW Spain?
We use such cases as features with lower weight,
proportional to the number of found HFWs in the
tweet (w = 0.5?nfoundnexpected ). For example, (1) receives a
weight of 1 (complete match) against (2). Against
(3), it receives a weight of 0.5 (= 0.5?33 , partial
match with no missing HFWs). Against (4) it re-
ceives a weight of 1/3 (= 0.5?23 , partial match with
only 2/3 HFWs found).
Experimenting with Flexible Pattern Features.
We repeat our experiments with varying training set
sizes (see Section 5) with two more systems: one
that uses character n-grams and flexible pattern fea-
tures, and another that uses character n-grams, word
n-grams and flexible patterns. High frequency word
counts are computed separately for each author us-
ing her training set. We only consider flexible pat-
tern features that appear at least tfp times in the
training set of at least one author. Values of tfp for
training set sizes (50, 100, 200, 500, 1,000) are (2,
3, 7, 7, 8), respectively.
Results. Figure 7 shows our results. Results
demonstrate that flexible pattern features have an
added value over both character n-grams alone (av-
eraged 2.9% improvement) and over character n-
grams and word n-grams together (averaged 1.5%
0 100 200 300 400 500 600 700 800 900 1000
35
40
45
50
55
60
65
70
75
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams, Word N?grams &
Flex. Patt. Feats.
Char. N?grams + Flex. Patt. Feats.
Char. N?grams + Word N?grams
Char. N?grams
SCAP
Naive Bayes
Figure 7: Authorship attribution accuracy for 50 authors
with various training set sizes and various feature sets.
The values are averaged over 10 groups. The random
baseline is 2%.
Comparison to previous work: SCAP ? SCAP algo-
rithm results, as reported by (Layton et al, 2010), Naive
Bayes ? Naive Bayes algorithm results, as reported by
(Boutwell, 2011).
improvement). We perform t-tests on each of our
training set sizes to check whether the latter im-
provement is significant. Results demonstrate that
it is highly significant in all settings, with p-values
smaller than values between 10?3 (for 50 tweets per
author) and 10?8 (1,000 tweets per author).
Comparison to Previous Works. Figure 7 also
shows results for the only two works that experi-
mented in some of the settings we experimented in:
Layton et al (2010) and Boutwell (2011) (see Sec-
tion 8). Our system substantially outperforms these
two systems, by margins of 5.9% to 19%. These
margins are explained by the choice of algorithm
(SVM and not SCAP/naive Bayes) and our set of
features (character n-grams + word n-grams + flex-
ible patterns compared to character n-grams only).
In order to rule out the possibility that these mar-
gins stem from using different datasets, we tested
our system on the dataset used in (Layton et al,
2010). Our system obtains even higher results on
this dataset than on our datasets (61.6%, a total im-
1887
provement of 6.1% over (Layton et al, 2010)).
Discussion. To illustrate the additional contribu-
tion of flexible patterns over word n-grams, consider
the following tweets, written by the same author.
5. ?. . . theHFW wayCW IHFW treatedCW herHFW?
6. ?. . . half of theHFW thingsCW IHFW have seen?
7. ?. . . theHFW friendsCW IHFW have had for years?
8. ?. . . in theHFW neighborhoodCW IHFW grew up in?
Consider a case where (5) is part of the test set,
while (6-8) appear in the training set. As (5) shares
no sequence of words with (6-8), no word n-gram
feature is able to identify the author?s style in (5).
However, this style can be successfully identified us-
ing the flexible pattern (9), shared by (5-8).
9. theHFW CW IHFW
This demonstrates the added value flexible pattern
features have over word n-gram features.
8 Related Work
Authorship attribution dates back to the end of 19th
century, when (Mendenhall, 1887) applied sentence
length and word length features to plays of Shake-
speare. Ever since, many methods have been devel-
oped for this task. For recent surveys, see (Koppel
et al, 2009; Stamatatos, 2009; Juola, 2012).
Authorship attribution methods can be generally
divided into two categories (Stamatatos, 2009). In
similarity-based methods, an anonymous text is at-
tributed to some author whose writing style is most
similar (by some distance metric). In machine learn-
ing methods, which we follow in this paper, anony-
mous texts are classified, using machine learning al-
gorithms, into different categories (in this case, dif-
ferent authors).
Machine learning papers differ from each other by
the features and machine learning algorithm. Exam-
ples of features include HFWs (Mosteller and Wal-
lace, 1964; Argamon et al, 2007), character n-gram
(Kjell, 1994; Hoorn et al, 1999; Stamatatos, 2008),
word n-grams (Peng et al, 2004), part-of-speech
n-grams (Koppel and Schler, 2003; Koppel et al,
2005) and vocabulary richness (Abbasi and Chen,
2005).
The various machine learning algorithms used in-
clude naive Bayes (Mosteller and Wallace, 1964;
Kjell, 1994), neural networks (Matthews and Mer-
riam, 1993; Kjell, 1994), K-nearest neighbors (Kjell
et al, 1995; Hoorn et al, 1999) and SVM (De Vel et
al., 2001; Diederich et al, 2003; Koppel and Schler,
2003).
Traditionally, authorship attribution systems have
mainly been evaluated against long texts such as
theater plays (Mendenhall, 1887), essays (Yule,
1939; Mosteller and Wallace, 1964), biblical books
(Mealand, 1995; Koppel et al, 2011a) and book
chapters (Argamon et al, 2007; Koppel et al, 2007).
In recent year, many works focused on web data
such as emails (De Vel et al, 2001; Koppel and
Schler, 2003; Abbasi and Chen, 2008), web forum
messages (Abbasi and Chen, 2005; Solorio et al,
2011), blogs (Koppel et al, 2006; Koppel et al,
2011b) and chat messages (Abbasi and Chen, 2008).
Some works focused on SMS messages (Mohan et
al., 2010; Ishihara, 2011).
Authorship Attribution on Twitter. The perfor-
mance of authorship attribution systems on short
texts is affected by several factors (Stamatatos,
2009). These factors include the number of candi-
date authors, the training set size and the size of the
test document.
Very few authorship attribution works experi-
mented with Twitter. Unlike our work, all used a
single group of authors (group sizes varied between
3-50). Layton et al (2010) used the SCAP method-
ology (Frantzeskou et al, 2007) with character n-
gram features. They experimented with 50 authors
and compared different numbers of tweets per au-
thor (values between 20-200). Surprisingly, they
showed that their system does not improve when
given more training tweets. In our work, we no-
ticed a different trend, and showed that more data
can be extremely valuable for authorship attribution
systems on micro-messages (see Section 6). Silva
et al (2011) trained an SVM classifier with various
features (e.g., punctuation and vocabulary features)
on a small dataset of three authors only, with vary-
ing training set size. Although their work used a
set of Twitter-specific features that we do not explic-
itly use, our features implicitly cover a large portion
of their features (such as punctuation and emoticon
1888
features, which are largely covered by character n-
grams).
Boutwell (2011) used a naive Bayes classifier
with character n-gram features. She experimented
with 50 authors and two training size values (120
and 230). She also provided a set of experiments that
studied the effect of joining several tweets into a sin-
gle document. Mikros and Perifanos (2013) trained
an SVM classifier with character n-gram and word
n-grams. They experimented with 10 authors of
Greek text, and also joined several tweets into a sin-
gle document. Joining several tweets into a longer
document is appealing since it can lead to substantial
improvement of the classification results, as demon-
strated by the works above. However, this approach
requires the test data to contain several tweets that
are known a-priori to be written by the same author.
This assumption is not always realistic. In our paper,
we intentionally focus on a single tweet as document
size.
Flexible Patterns. Patterns were introduced by
(Hearst, 1992), who used hand crafted patterns
to discover hyponyms. Hard coded patterns
were used for many tasks, such as discovering
meronymy (Berland and Charniak, 1999), noun cat-
egories (Widdows and Dorow, 2002), verb relations
(Chklovski and Pantel, 2004) and semantic class
learning (Kozareva et al, 2008).
Patterns were first extracted in a fully unsuper-
vised manner (?flexible patterns?) by (Davidov and
Rappoport, 2006), who used flexible patterns in or-
der to establish noun categories, and (Bicic?i and
Yuret, 2006) who used them for analogy question
answering. Ever since, flexible patterns were used
as features for various tasks such as extraction of
semantic relationships (Davidov et al, 2007; Tur-
ney, 2008b; Bollegala et al, 2009), detection of
synonyms (Turney, 2008a), disambiguation of nom-
inal compound relations (Davidov and Rappoport,
2008a), sentiment analysis (Davidov et al, 2010b)
and detection of sarcasm (Tsur et al, 2010).
9 Conclusion
The main goal of this paper is to measure to what
extent authors of micro-messages can be identified.
We have shown that authors of very short texts
can be successfully identified in an array of au-
thorship attribution settings reported for long doc-
uments. This is the first work on micro-messages
to address some of these settings. We introduced
the concept of k-signature. Using this concept, we
proposed an interpretation of our results. Last, we
presented the first authorship attribution system that
uses flexible patterns, and demonstrated that using
these features significantly improves over other sys-
tems. Our system obtains 6.1% improvement over
the current state-of-the-art.
Acknowledgments
We would like to thank Elad Eban and Susan Good-
man for their helpful advice, as well as Robert Lay-
ton for providing us with his dataset. This research
was funded (in part) by the Harry and Sylvia Hoff-
man leadership and responsibility program (for the
first author) and the Intel Collaborative Research In-
stitute for Computational Intelligence (ICRI-CI).
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-
thorship analysis to extremist-group web forum mes-
sages. IEEE Intelligent Systems, 20:67?75.
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:
A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Transac-
tions on Information Systems, 26(2):7:1?7:29.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features: Research articles. J. Am. Soc. Inf. Sci.
Technol., 58(6):802?822.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proc. of ACL, pages
57?64, College Park, Maryland, USA.
Ergun Bicic?i and Deniz Yuret. 2006. Clustering word
pairs to answer analogy questions. In Proc. of TAINN,
pages 1?8.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2009. Measuring the similarity between
implicit semantic relations from the web. In Proc. of
WWW, New York, New York, USA. ACM Press.
Sarah R. Boutwell. 2011. Authorship Attribution of
Short Messages Using Multimodal Features. Master?s
thesis, Naval Postgraduate School.
John Burrows. 2002. ?Delta?: a Measure of Stylistic
Difference and a Guide to Likely Authorship. Literary
and Linguistic Computing, 17(3):267?287.
1889
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Dekang Lin and Dekai Wu, editors, Proc.
of EMNLP, pages 33?40, Barcelona, Spain.
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proc.
of ACL-Coling, pages 297?304, Sydney, Australia.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of ACL-08: HLT,
pages 227?235, Columbus, Ohio, June. Association
for Computational Linguistics.
Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
SAT analogy questions. In Proc. of ACL-HLT, pages
692?700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of ACL,
pages 232?239, Prague, Czech Republic.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proc. of CoNLL, pages 107?
116, Uppsala, Sweden.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proc. of Coling, pages 241?249, Bei-
jing, China.
Olivier De Vel, Alison Anderson, Malcolm Corney, and
George Mohay. 2001. Mining e-mail content for au-
thor identification forensics. ACM Sigmod Record,
30(4):55?64.
JoachimDiederich, Jo?rg Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied intelligence, 19(1-
2):109?123.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english. In
Proc. of the 4th Web as Corpus Workshop, WAC-4.
Georgia Frantzeskou, Efstathios Stamatatos, Stefanos
Gritzalis, and Carole E Chaski. 2007. Identifying au-
thorship by byte-level n-grams: The source code au-
thor profile (scap) method. Int Journal of Digital Evi-
dence, 6(1):1?18.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of Coling
? Volume 2, pages 539?545, Stroudsburg, PA, USA.
Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and
Floor van der Ham. 1999. Neural network identifi-
cation of poets using letter sequences. Literary and
Linguistic Computing, 14(3):311?338.
Shunichi Ishihara. 2011. A forensic authorship clas-
sification in sms messages: A likelihood ratio based
approach using n-gram. In Proc. of the Australasian
Language Technology Association Workshop 2011,
pages 47?56, Canberra, Australia.
Patrick Juola. 2012. Large-scale experiments in author-
ship attribution. English Studies, 93(3):275?283.
Bradley Kjell, W Addison Woods, and Ophir Frieder.
1995. Information retrieval using letter tuples with
neural network and nearest neighbor classifiers. In
IEEE International Conference on Systems, Man and
Cybernetics, volume 2, pages 1222?1226. IEEE.
Bradley Kjell. 1994. Authorship determination using let-
ter pair frequency features with neural network classi-
fiers. Literary and Linguistic Computing, 9(2):119?
124.
Moshe Koppel and Jonathan Schler. 2003. Exploiting
stylistic idiosyncrasies for authorship attribution. In
Proc. of IJCAI?03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis, volume 69,
page 72.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, KDD ?05, pages 624?628, New York,
NY, USA.
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and
EranMesseri. 2006. Authorship attribution with thou-
sands of candidate authors. In SIGIR, pages 659?660.
Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-
Dokow. 2007. Measuring differentiability: Unmask-
ing pseudonymous authors. JMLR, 8:1261?1276.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9?26.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011a. Unsupervised decom-
position of a document into authorial components. In
Proc. of ACL-HLT, pages 1356?1364, Portland, Ore-
gon, USA.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011b. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
1890
pattern linkage graphs. In Proc. of ACL-HLT, pages
1048?1056, Columbus, Ohio.
Robert Layton, PaulWatters, and Richard Dazeley. 2010.
Authorship attribution for twitter in 140 characters or
less. In Proc. of the 2010 Second Cybercrime and
Trustworthy Computing Workshop, CTC ?10, pages 1?
8, Washington, DC, USA. IEEE Computer Society.
Robert AJ Matthews and Thomas VN Merriam. 1993.
Neural computation in stylometry i: An application to
the works of shakespeare and fletcher. Literary and
Linguistic Computing, 8(4):203?209.
DL Mealand. 1995. Correspondence analysis of luke.
Literary and linguistic computing, 10(3):171?182.
Thomas Corwin Mendenhall. 1887. The characteristic
curves of composition. Science, ns-9(214S):237?246.
George K Mikros and Kostas Perifanos. 2013. Author-
ship attribution in greek tweets using authors multi-
level n-gram profiles. In 2013 AAAI Spring Sympo-
sium Series.
Ashwin Mohan, Ibrahim M Baggili, and Marcus K
Rogers. 2010. Authorship attribution of sms mes-
sages using an n-grams approach. Technical report,
CERIAS Tech Report 2011.
Frederick Mosteller and David Lee Wallace. 1964.
Inference and disputed authorship: The Federalist.
Addison-Wesley.
Fuchun Peng, Dale Schuurmans, and Shaojun Wang.
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Information Retrieval, 7(3-
4):317?345.
Conrad Sanderson and Simon Guenter. 2006. Short text
authorship attribution via sequence kernels, markov
chains and author unmasking: An investigation. In
Proc. of EMNLP, pages 482?491, Sydney, Australia.
Rui Sousa Silva, Gustavo Laboreiro, Lu??s Sarmento, Tim
Grant, Euge?nio Oliveira, and Belinda Maia. 2011.
?twazn me!!! ;(? automatic authorship analysis of
micro-blogging messages. In Proc. of the 16th inter-
national conference on Natural language processing
and information systems, NLDB?11, pages 161?168,
Berlin, Heidelberg. Springer-Verlag.
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-Gomez. 2011. Modality specific
meta features for authorship attribution in web forum
posts. In Proc. of IJCNLP, pages 156?164, Chiang
Mai, Thailand, November.
Efstathios Stamatatos. 2008. Author identification: Us-
ing text sampling to handle the class imbalance prob-
lem. Inf. Process. Manage., 44(2):790?799.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm?a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product reviews.
In Proc. of ICWSM.
Peter Turney. 2008a. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proc. of
Coling, pages 905?912,Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Peter D. Turney. 2008b. The latent relation mapping en-
gine: Algorithm and experiments. Journal of Artificial
Intelligence Research, 33:615?655.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of Coling, pages 1?7, Stroudsburg, PA, USA.
George Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: with application
to two cases of disputed authorship. Biometrika, 30(3-
4):363?390.
1891
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1318?1326,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Translationese and Its Dialects 
Moshe Koppel Noam Ordan 
Department of Computer Science Department of Computer Science 
Bar Ilan University University of Haifa 
Ramat-Gan, Israel 52900 Haifa, Israel 31905 
moishk@gmail.com noam.ordan@gmail.com 
 
 
 
Abstract 
While it is has often been observed that the 
product of translation is somehow different 
than non-translated text, scholars have empha-
sized two distinct bases for such differences. 
Some have noted interference from the source 
language spilling over into translation in a 
source-language-specific way, while others 
have noted general effects of the process of 
translation that are independent of source lan-
guage. Using a series of text categorization 
experiments, we show that both these effects 
exist and that, moreover, there is a continuum 
between them. There are many effects of 
translation that are consistent among texts 
translated from a given source language, some 
of which are consistent even among texts 
translated from families of source languages. 
Significantly, we find that even for widely 
unrelated source languages and multiple ge-
nres, differences between translated texts and 
non-translated texts are sufficient for a learned 
classifier to accurately determine if a given 
text is translated or original. 
1 Introduction 
The products of translation (written or oral) are 
generally assumed to be ontologically different 
from non-translated texts. Researchers have em-
phasized two aspects of this difference. Some 
(Baker 1993) have emphasized general effects of 
the process of translation that are independent of 
source language and regard the collective product 
of this process in a given target language as an ?in-
terlanguage? (Selinker, 1972), ?third code? (Fraw-
ley, 1984) or ?translationese? (Gellerstam, 1986). 
Others (Toury, 1995) have emphasized the effects 
of interference, the process by which a specific 
source language leaves distinct marks or finger-
prints in the target language, so that translations 
from different source languages into the same tar-
get language may be regarded as distinct dialects 
of translationese.  
We wish to use text categorization methods to 
set both of these claims on a firm empirical foun-
dation. We will begin by bringing evidence for two 
claims: 
(1) Translations from different source languages 
into the same target language are sufficiently dif-
ferent from each other for a learned classifier to 
accurately identify the source language of a given 
translated text;  
(2) Translations from a mix of source languages 
are sufficiently distinct from texts originally writ-
ten in the target language for a learned classifier to 
accurately determine if a given text is translated or 
original. 
Each of these claims has been made before, but 
our results will strengthen them in a number of 
ways. Furthermore, we will show that the degree of 
difference between translations from two source 
languages reflects the degree of difference between 
the source languages themselves. Translations 
from cognate languages differ from non-translated 
texts in similar ways, while translations from unre-
lated languages differ from non-translated texts in 
distinct ways. The same result holds for families of 
languages. 
The outline of the paper is as follows. In the fol-
lowing section, we show that translations from dif-
ferent source languages can be distinguished from 
each other and that closely related source languag-
es manifest similar forms of interference. In sec-
tion 3, we show that, in a corpus involving five 
European languages, we can distinguish translatio-
nese from non-translated text and we consider 
some salient markers of translationese. In section 
1318
4, we consider the extent to which markers of 
translationese cross over into non-European lan-
guages as well as into different genres. Finally, we 
consider possible applications and implications for 
future studies.  
2 Interference Effects in Translationese 
In this section, we perform several text categoriza-
tion experiments designed to show the extent to 
which interference affects (both positively and ne-
gatively) our ability to classify documents. 
2.1 The Europarl Corpus 
The main corpus we will use throughout this paper 
is Europarl (Koehn, 2005), which consists of tran-
scripts of addresses given in the European Parlia-
ment. The full corpus consists of texts translated 
into English from 11 different languages (and vice 
versa), as well as texts originally produced in Eng-
lish. For our purposes, it will be sufficient to use 
translations from five languages (Finnish, French, 
German, Italian and Spanish), as well as original 
English. We note that this corpus constitutes a 
comparable corpus (Laviosa, 1997), since it con-
tains (1) texts written originally in a certain lan-
guage (English), as well as (2) texts translated into 
that same language, matched for genre, domain, 
publication timeframe, etc. Each of the five trans-
lated components is a text file containing just un-
der 500,000 words; the original English component 
is a file of the same size as the aggregate of the 
other five. 
The five source languages we use were selected 
by first eliminating several source languages for 
which the available text was limited and then 
choosing from among the remaining languages, 
those of varying degrees of pairwise similarity. 
Thus, we select three cognate (Romance) languag-
es (French, Italian and Spanish), a fourth less re-
lated language (German), and a fifth even further 
removed (Finnish). As will become clear, the mo-
tivation is to see whether the distance between the 
languages impacts the distinctiveness of the trans-
lation product. 
We divide each of the translated corpora into 
250 equal chunks, paying no attention to natural 
units within the corpus. Similarly, we divide the 
original English corpus into 1250 equal chunks. 
We set aside 50 chunks from each of the translated 
corpora and 250 chunks from the original English 
corpus for development purposes (as will be ex-
plained below). The experiments described below 
use the remaining 1000 translated chunks and 1000 
original English chunks.   
2.2 Identifying source language 
Our objective in this section is to measure the ex-
tent to which translations are affected by source 
language. Our first experiment will be to use text 
categorization methods to learn a classifier that 
categorizes translations according to source lan-
guage. We will check the accuracy of such clas-
sifiers on out-of-sample texts. High accuracy 
would reflect that there are exploitable differences 
among translations of otherwise comparable texts 
that differ only in terms of source language. 
The details of the experiment are as follows. We 
use the 200 chunks from each translated corpus, as 
described above. We use as our feature set a list of 
300 function words taken from LIWC (Pennebak-
er, 2001) and represent each chunk as a vector of 
size 300 in which each entry represents the fre-
quency of the corresponding feature in the chunk. 
The restriction to function words is crucial; we 
wish to rely only on stylistic differences rather than 
content differences that might be artifacts of the 
corpus. 
We use Bayesian logistic regression (Madigan, 
2005) as our learning method in order to learn a 
classifier that classifies a given text into one of five 
classes representing the different source languages. 
We use 10-fold cross-validation as our testing me-
thod.  
We find that 92.7% of documents are correctly 
classified.  
In Table 1 we show the confusion matrix for the 
five languages. As can be seen, there are more mis-
takes across the three cognate languages than be-
tween those three languages and German and still 
fewer mistakes involving the more distant Finnish 
language. 
 
 It Fr Es De Fi 
It 169 19 8 4 0 
Fr 18 161 12 8 1 
Es 3 11 172 11 3 
De 4 12 3 178 3 
Fi 0 1 2 5 192 
Table 1: Confusion matrix for 10-fold cross validation 
experiment to determine source language of texts trans-
lated into English 
1319
 
This result strengthens that of van Halteren 
(2008) in a similar experiment. Van Halteren, also 
using Europarl (but with Dutch as the fifth source 
language, rather than Finnish), obtained accuracy 
of 87.2%-96.7% for a two-way decision on source 
language, and 81.5%-87.4% for a six-way decision 
(including the original which has no source lan-
guage). Significantly, though, van Halteren?s fea-
ture set included content words and he notes that 
many of the most salient differences reflected dif-
ferences in thematic emphasis. By restricting our 
feature set to function words, we neutralize such 
effects. 
In Table 2, we show the two words most over-
represented and the two words most under-
represented in translations from each source lan-
guage (ranked according to an unpaired T-test). 
For each of these, the difference between frequen-
cy of use in the indicated language and frequency 
of use in the other languages in aggregate is signif-
icant at p<0.01. 
 
 over-represented under-represented 
Fr of, finally here, also 
It upon, moreover also, here 
Es with, therefore too, then 
De here, then of, moreover 
Fi be, example me, which 
Table 2: Most salient markers of translations from each 
source language. 
 
The two most underrepresented words for 
French and Italian, respectively, are in fact identic-
al. Furthermore, the word too which is underrepre-
sented for Spanish is a near synonym of also which 
appears in both French and Spanish. This suggests 
the possibility that interference effects in cognate 
languages such as French, Italian and Spanish 
might be similar. We will see presently that this is 
in fact the case.  
When a less related language is involved we see 
the opposite picture. For German, both underrepre-
sented items appear as overrepresented in the 
Romance languages, and, conversely, underrepre-
sented items in the Romance languages appear as 
overrepresented items for German. This may cast 
doubt on the idea that all translations share univer-
sal properties and that at best we may claim that 
particular properties are shared by closely related 
languages but not others. In the experiments pre-
sented in the next subsection, we?ll find that trans-
lationese is gradable: closely related languages 
share more features, yet even further removed lan-
guages share enough properties to hold the general 
translationese hypothesis as valid.  
2.3 Identifying translationese per source lan-
guage  
We now wish to measure in a subtler manner the 
extent to which interference affects translation. In 
this experiment, the challenge is to learn a classifi-
er that classifies a text as belonging to one of only 
two classes: original English (O) or translated-into-
English (T). The catch is that all our training texts 
for the class T will be translations from some fixed 
source language, while all our test documents in T 
will be translations from a different source lan-
guage. What accuracy can be achieved in such an 
experiment? The answer to this question will tell 
us a great deal about how much of translationese is 
general and how much of it is language dependent. 
If accuracy is close to 100%, translationese is pure-
ly general (Baker, 1993). (We already know from 
the previous experiment that that's not the case.). If 
accuracy is near 50%, there are no general effects, 
just language-dependent ones. Note that, whereas 
in our first experiment above pair-specific interfe-
rence facilitated good classification, in this expe-
riment pair-specific interference is an impediment 
to good classification. 
The details of the experiment are as follows. We 
create, for example, a ?French? corpus consisting 
of the 200 chunks of text translated from French 
and 200 original English texts. We similarly create 
a corpus for each of the other source languages, 
taking care that each of the 1000 original English 
texts appears in exactly one of the corpora. As 
above, we represent each chunk in terms of fre-
quencies of function words. Now, using Bayesian 
logistic regression, we learn a classifier that distin-
guishes T from O in the French corpus. We then 
apply this learned classifier to the texts in, for ex-
ample, the equivalent ?Italian? corpus to see if we 
can classify them as translated or original. We re-
peat this for each of the 25 ?train_corpus, 
test_corpus? pairs. 
In Table 3, we show the accuracy obtained for 
each such pair. (For the case where the training 
corpus and testing corpus are identical ? the di-
1320
agonal of the matrix ? we show results for ten-fold 
cross-validation.)  
We note several interesting facts. First, results of 
cross-validation within each corpus are very 
strong. For any given source language, it is quite 
easy to distinguish translations from original Eng-
lish. This corroborates results obtained by Baroni 
and Bernardini (2006), Ilisei et al (2010), Kuro-
kawa et al (2009) and van Halteren (2008), which 
we will discuss below.  
We note further, that for the cases where we 
train on one source language and test on another, 
results are far worse. This clearly indicates that 
interference effects from one source language 
might be misleading when used to identify transla-
tions from a different language. Thus, for example, 
in the Finnish corpus, the word me is a strong indi-
cator of original English (constituting 0.0003 of 
tokens in texts translated from Finnish as opposed 
to 0.0015 of tokens in original English texts), but 
in the German corpus, me is an indicator of trans-
lated text (constituting 0.0020 of tokens in text 
translated from German). 
The most interesting result that can be seen in 
this table is that the accuracy obtained when train-
ing using language x and testing using language y 
depends precisely on the degree of similarity be-
tween x and y. Thus, for training and testing within 
the three cognate languages, results are fairly 
strong, ranging between 84.5% and 91.5%. For 
training/testing on German and testing/training on 
one of the other European languages, results are 
worse, ranging from 68.5% to 83.3%. Finally, for 
training/testing on Finnish and testing/training on 
any of the European languages, results are still 
worse, hovering near 60% (with the single unex-
plained outlier for training on German and testing 
on Finnish).  
Finally, we note that even in the case of training 
or testing on Finnish, results are considerably bet-
ter than random, suggesting that despite the con-
founding effects of interference, some general 
properties of translationese are being picked up in 
each case. We explore these in the following sec-
tion. 
 
3 General Properties of Translationese  
Having established that there are source-language-
dependent effects on translations, let?s now con-
sider source-language-independent effects on 
translation. 
3.1 Identifying translationese 
In order to identify general effects on translation, 
we now consider the same two-class classification 
problem as above, distinguishing T from O, except 
that now the translated texts in both our train and 
test data will be drawn from multiple source lan-
guages. If we succeed at this task, it must be be-
cause of features of translationese that cross 
source-languages.  
The details of our experiment are as follows. We 
use as our translated corpus, the 1000 translated 
chunks (200 from each of five source languages) 
and as our original English corpus all 1000 original 
English chunks. As above, we represent each 
chunk in terms of function words frequencies. We 
use Bayesian logistic regression to learn a two-
class classifier and test its accuracy using ten-fold 
cross-validation.  
Remarkably, we obtain accuracy of 96.7%.  
This result extends and strengthens results re-
ported in some earlier studies. Ilisei et al (2010), 
Kurokawa (2009) and van Halteren (2008) each 
obtained above 90% accuracy in distinguishing 
translation from original. However, in each case 
the translations were from a single source lan-
guage. (Van Halteren considered multiple source 
languages, but each learned classifier used only 
one of them.) Thus, those results do not prove that 
translationese has distinctive source-language-
independent features. To our knowledge, the only 
earlier work that used a learned classifier to identi-
fy translations in which both test and train sets in-
volved multiple source languages is Baroni and 
Bernardini (2006), in which the target language 
was Italian and the source languages were known 
to be varied. The actual distribution of source lan-
guages was, however, not known to the research-
ers. They obtained accuracy of 86.7%. Their result 
was obtained using combinations of lexical and 
syntactic features. 
 
   Train     
 It Fr Es De Fi 
It 98.3 91.5 86.5 71.3 61.5 
Fr 91 97 86.5 68.5 60.8 
Es 84.5 88.3 95.8 76.3 59.5 
De 82 83.3 78.5 95 80.8 
Fi 56 60.3 56 62.3 97.3 
Table 3: Results of learning a T vs. O classifier us-
ing one source language and testing it using another 
source language 
1321
3.2 Some distinguishing features 
Let us now consider some of the most salient func-
tion words for which frequency of usage in T dif-
fers significantly from that in O. While there are 
many such features, we focus on two categories of 
words that are most prominent among those with 
the most significant differences.  
   First, we consider animate pronouns. In Table 4, 
we show the frequencies of animate pronouns in O 
and T, respectively (the possessive pronouns, mine, 
yours and hers, not shown, are extremely rare in 
the corpus). As can be seen, all pronouns are un-
der-represented in T; for most (bolded), the differ-
ence is significant at p<0.01.  
By contrast, the word the is significantly overre-
presented in T (15.32% in T vs. 13.73% in O; sig-
nificant at p<0.01).  
 
 
word freq O freq T 
I 2.552% 2.148% 
we 2.713% 2.344% 
you 0.479% 0.470% 
he 0.286% 0.115% 
she 0.081% 0.039% 
me 0.148% 0.141% 
us 0.415% 0.320% 
him 0.066% 0.033% 
her 0.091% 0.056% 
my 0.462% 0.345% 
our 0.696% 0.632% 
your 0.119% 0.109% 
his 0.218% 0.123% 
Table 4: Frequency of pronouns  in O and T in the Eu-
roparl corpus. Bold indicates significance at p<0.01. 
 
In Table 5, we consider cohesive markers, 
tagged as adverbs (Schmid, 2004). (These are ad-
verbs that can appear at the beginning of a sen-
tence followed immediately by a comma.)  
 
word freq O freq T 
therefore 0.153% 0.287% 
thus 0.015% 0.041% 
consequently 0.006% 0.014% 
hence 0.007% 0.013% 
accordingly 0.006% 0.011% 
however 0.216% 0.241% 
nevertheless 0.019% 0.045% 
also 0.460% 0.657% 
furthermore 0.012% 0.048% 
moreover 0.008% 0.036% 
indeed 0.098% 0.053% 
actually 0.065% 0.042% 
Table 5: Frequency of cohesive adverbs  in O and T in 
the Europarl corpus. Bold indicates significance at 
p<0.01. 
 
We note that the preponderance of such cohesive 
markers are significantly more frequent in transla-
tions. In fact, we also find that a variety of phrases 
that serve the same purpose as cohesive adverbs, 
such as in fact and as a result are significantly 
more frequent in translationese. 
The general principle underlying these pheno-
mena is subject to speculation. Previous research-
ers have noted the phenomenon of explicitation, 
according to which translators tend to render im-
plicit utterances in the source text into explicit ut-
terances in the target text (Blum-Kulka, 1986, 
Laviosa-Braithwaite, 1998), for example by filling 
out elliptical expressions or adding connectives to 
increase cohesion of the text (Laviosa-Braithwaite, 
1998). It is plausible that the use of cohesive ad-
verbs is an instantiation of this phenomenon. 
With regard to the under-representation of pro-
nouns and the over-representation of the, there are 
a number of possible interpretations. It may be that 
this too is the result of explicitation, in which ana-
phora is resolved by replacing pronouns with noun 
phrases (e.g., the man instead of he). But it also 
might be that this is an example of simplification 
(Laviosa- Braithwaite 1998, Laviosa 2002), ac-
cording to which the translator simplifies the mes-
sage, the language, or both. Related results 
confirming the simplification hypothesis were 
found by Ilisei et al (2010) on Spanish texts. In 
particular, they found that type-to-token ratio (lexi-
cal variety/richness), mean sentence length and 
proportion of grammatical words (lexical densi-
ty/readability) are all smaller in translated texts.  
We note that Van Halteren (2008) and Kurokawa 
et al (2009), who considered lexical features, 
found cultural differences, like over-representation 
of ladies and gentlemen in translated speeches. 
Such differences, while of general interest, are or-
thogonal to our purposes in this paper.  
1322
3.3 Overriding language-specific effects 
We found in Section 2.3 that when we trained in 
one language and tested in another, classification 
succeeded to the extent that the source languages 
used in training and testing, respectively, are re-
lated to each other. In effect, general differences 
between translationese and original English were 
partially overwhelmed by language-specific differ-
ences that held for the training language but not the 
test language. We thus now revisit that earlier ex-
periment, but restrict ourselves to features that dis-
tinguish translationese from original English 
generally.  
To do this, we use the small development corpus 
described in Section 2.1.  We use Bayesian logistic 
regression to learn a classifier to distinguish be-
tween translationese and original English. We se-
lect the 10 highest-weighted function-word 
markers for T and the 10 highest-weighted func-
tion-word markers for O in the development cor-
pus. We then rerun our train-on-source-language-x, 
test-on-source-language-y experiment using this 
restricted set as our feature set. We now find that 
even in the difficult case where we train on Finnish 
and test on another language (or vice versa), we 
succeed at distinguishing translationese from orig-
inal English with accuracy above 80%. This consi-
derably improves the earlier results shown in Table 
3. Thus, a bit of feature engineering facilitates 
learning a good classifier for T vs. O even across 
source languages. 
4 Other Genres and Language Families  
We have found both general and language-specific 
differences between translationese and original 
English in one large corpus. It might be wondered 
whether the phenomena we have found hold in 
other genres and for a completely different set of 
source languages. To test this, we consider a 
second corpus. 
4.1 The IHT corpus  
Our second corpus includes three translated corpo-
ra, each of which is an on-line local supplement to 
the International Herald Tribune (IHT): Kathime-
rini (translated from Greek), Ha?aretz (translated 
from Hebrew), and the JoongAng Daily (translated 
from Korean). In addition, the corpus includes 
original English articles from the IHT. Each of the 
four components contains four different domains 
balanced roughly equally: news (80,000 words), 
arts and leisure (50,000), business and finance 
(50,000), and opinion (50,000) and each covers the 
period from April-September 2004. Each compo-
nent consists of about 230,000 tokens. (Unlike for 
our Europarl corpus, the amount of English text 
available is not equal to the aggregate of the trans-
lated corpora, but rather equal to each of the indi-
vidual corpora.) 
It should be noted that the IHT corpus belongs 
to the writing modality while the Europarl corpus 
belongs to the speaking modality (although possi-
bly post-edited). Furthermore, the source languag-
es (Hebrew, Greek and Korean) in the IHT corpus 
are more disparate than those in the Europarl cor-
pus.  
Our first objective is to confirm that the results 
we obtained earlier on the Europarl corpus hold for 
the IHT corpus as well.  
Perhaps more interestingly, our second objective 
is to see if the gradability phenomenon observed 
earlier (Table 3) generalizes to families of lan-
guages. Our first hypothesis is that a classifier for 
identifying translationese that is trained on Euro-
parl will succeed only weakly to identify transla-
tionese in IHT. But our second hypothesis is that 
there are sufficient general properties of translatio-
nese that cross language families and genres that a 
learned classifier can accurately identify transla-
tionese even on a test corpus that includes both 
corpora, spanning eight disparate languages across 
two distinct genres. 
4.2 Results on IHT corpus 
Running essentially the same experiments as de-
scribed for the Europarl corpus, we obtain the fol-
lowing results.  
First of all, we can determine source language 
with accuracy of 86.5%. This is a somewhat weak-
er result than the 92.7% result obtained on Euro-
parl, especially considering that there are only 
three classes instead of five. The difference is most 
likely due to the fact that the IHT corpus is about 
half the size of the Europarl corpus. Nevertheless, 
it is clear that source language strongly affects 
translationese in this corpus. 
Second, as can be seen in Table 6, we find that 
the gradability phenomenon occurs in this corpus 
as well. Results are strongest when the train and 
1323
test corpora involve the same source language and 
trials involving Korean, the most distant language, 
are somewhat weaker than those across Greek and 
Hebrew. 
 
                  Train 
 Gr He Ko 
Gr 89.8 73.4 64.8 
He 82.0 86.3 65.5 
Ko 73.0 72.5 85.0 
Table 6: Results of learning a T vs. O classifier using 
one source language and testing it using another source 
language 
 
Third, we find in ten-fold cross-validation expe-
riments that we can distinguish translationese from 
original English in the IHT corpus with accuracy 
of 86.3%. Thus, despite the great distance between 
the three source languages in this corpus, general 
differences between translationese and original 
English are sufficient to facilitate reasonably accu-
rate identification of translationese.  
 
4.3 Combining the corpora 
First, we consider whether a classifier learned on 
the Europarl corpus can be used to identify trans-
lationese in the IHT corpus, and vice versa. It 
would be consistent with our findings in Section 
2.3, that we would achieve better than random 
results but not high accuracy, since there are no 
doubt features common to translations from the 
five European languages of Europarl that are dis-
tinct from those of translations from the very dif-
ferent languages in IHT.  
   In fact, we find that training on Europarl and 
testing on IHT yields accuracy of 64.8%, while 
training on IHT and testing on Europarl yields 
accuracy of 58.8%. The weak results reflect both 
differences between the families of source lan-
guages involved in the respective corpora, as well 
as genre differences. Thus, for example, we find 
that of the pronouns shown in Table 4 above, only 
he and his are significantly under-represented in 
translationese in the IHT corpus. Thus, that effect 
is specific either to the genre of Europarl or to the 
European languages considered there.  
   Now, we combine the two corpora and check if 
we can identify translationese across two genres 
and eight languages.  We run the same experiments 
as described above, using 200 texts from each of 
the eight source languages and 1600 non-translated 
English texts, 1000 from Europarl and 600 from 
IHT.  
   In 10-fold cross-validation, we find that we can 
distinguish translationese from non-translated Eng-
lish with accuracy of 90.5%. 
   This shows that there are features of translatio-
nese that cross genres and widely disparate lan-
guages. Thus, for one prominent example, we find 
that, as in Europarl, the word the is over-
represented in translationese in IHT (15.36% in T 
vs. 13.31% in O; significant at p<0.01). In fact, the 
frequencies across corpora are astonishingly con-
sistent. 
   To further appreciate this point, let?s look at the 
frequencies of cohesive adverbs in the IHT corpus. 
    We find essentially, the same pattern in IHT as 
we did in Europarl. The preponderance of cohesive 
adverbs are over-represented in translationese, 
most of them with differences significant at 
p<0.01. Curiously, the word actually is a counter-
example in both corpora. 
5 Conclusions 
We have found that we can learn classifiers that 
determine source language given a translated text, 
as well as classifiers that distinguish translated text 
from non-translated text in the source language. 
These text categorization experiments suggest that 
both source language and the mere fact of being 
word freq O freq T 
therefore 0.011% 0.031% 
thus 0.011% 0.027% 
consequently 0.000% 0.004% 
hence 0.003% 0.007% 
accordingly 0.003% 0.003% 
however 0.078% 0.129% 
nevertheless 0.008% 0.018% 
also 0.305% 0.453% 
furthermore 0.003% 0.011% 
moreover 0.009% 0.008% 
indeed 0.018% 0.024% 
actually 0.032% 0.018% 
Table 7: Frequency of cohesive adverbs in O and T 
in the IHT corpus. Bold indicates significance at 
p<0.01.  
1324
translated play a crucial role in the makeup of a 
translated text.  
    It is important to note that our learned classifiers 
are based solely on function words, so that, unlike 
earlier studies, the differences we find are unlikely 
to include cultural or thematic differences that 
might be artifacts of corpus construction. 
In addition, we find that the exploitability of dif-
ferences between translated texts and non-
translated texts are related to the difference be-
tween source languages: translations from similar 
source languages are different from non-translated 
texts in similar ways. 
Linguists use a variety of methods to quantify 
the extent of differences and similarities between 
languages. For example, Fusco (1990) studies 
translations between Spanish and Italian and con-
siders the impact of structural differences between 
the two languages on translation quality. Studying 
the differences and distance between languages by 
comparing translations into the same language may 
serve as another way to deepen our typological 
knowledge. As we have seen, training on source 
language x and testing on source language y pro-
vides us with a good estimation of the distance be-
tween languages, in accordance with what we find 
in standard works on typology (cf. Katzner, 2002).   
In addition to its intrinsic interest, the finding 
that the distance between languages is directly cor-
related with our ability to distinguish translations 
from a given source language from non-translated 
text is of great importance for several computa-
tional tasks. First, translations can be studied in 
order to shed new light on the differences between 
languages and can bear on attested techniques for 
using cognates to improve machine translation 
(Kondrak & Sherif, 2006). Additionally, given the 
results of our experiments, it stands to reason that 
using translated texts, especially from related 
source languages, will prove beneficial for con-
structing language models and will outperform 
results obtained from non-translated texts. This, 
too, bears on the quality of machine translation. 
Finally, we find that there are general properties 
of translationese sufficiently strong that we can 
identify translationese even in a combined corpus 
that is comprised of eight very disparate languages 
across two distinct genres, one spoken and the oth-
er written. Prominent among these properties is the 
word the, as well as a number of cohesive adverbs, 
each of which is significantly over-represented in 
translated texts.  
References  
Mona Baker. 1993. Corpus linguistics and translation 
studies: Implications and applications. In Gill Francis 
Mona Baker and Elena Tognini Bonelli, editors, Text 
and technology: in honour of John Sinclair, pages 
233-252. John Benjamins, Amsterdam. 
Marco Baroni and Silvia Bernardini. 2006. A new ap-
proach to the study of Translationese: Machine-
learning the difference between original and trans-
lated text. Literary and Linguistic Computing, 
21(3):259-274. 
Shoshan Blum-Kulka. Shifts of cohesion and coherence 
in translation. 1986. In Juliane House and Shoshana 
Blum-Kulka (Eds), Interlingual and Intercultural 
Communication (17-35). T?bingen: G?nter Narr Ver-
lag.  
William Frawley. 1984. Prolegomenon to a theory of 
translation. In William Frawley (ed), Translation. Li-
terary, Linguistic and Philosophical Perspectives 
(179-175). Newark: University of Delaware Press. 
Maria Antonietta Fusco. 1990. Quality in conference 
interpreting between cognate languages: A prelimi-
nary approach to the Spanish-Italian case. The Inter-
preters? Newsletter, 3, 93-97.  
Martin Gellerstam. 1986. Translationese in Swedish 
novels translated from English, in Lars Wollin & 
Hans Lindquist (eds.), Translation Studies in Scandi-
navia (88-95). Lund: CWK Gleerup. 
Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and 
Ruslan Mitkov. Identification of translationese: A 
machine learning approach. In Alexander F. Gel-
bukh, editor, Proceedings of CICLing-2010: Compu-
tational Linguistics and Intelligent Text Processing, 
11th International, volume 6008 of Lecture Notes in 
Computer Science, pages 503-511. Springer, 2010. 
Kenneth Katzner. 2002. The Languages of the World. 
Routledge. 
Grzegorz Kondrak and Tarek Sherif. 2006. Evaluation 
of several phonetic similarity algorithms on the task 
of cognate identification. In Proceedings of the 
Workshop on Linguistic Distances (LD '06). 43-50.  
David Kurokawa, Cyril Goutte, and Pierre Isabelle. 
2009. Automatic detection of translated text and its 
impact on machine translation. In Proceedings of 
MT-Summit XII.  
Sara Laviosa: 1997. How Comparable can 'Comparable 
Corpora' Be?. Target, 9 (2), pp. 289-319.  
1325
Sara Laviosa-Braithwaite. 1998. In Mona Baker (ed.) 
Routledge Encyclopedia of Translation Studies. Lon-
don/New York: Routledge, pp.288-291. 
Sara Laviosa. 2002. Corpus-based Translation Studies. 
Theory, Findings, Applications. Amsterdam/New 
York: Rodopi. 
David Madigan, Alexander Genkin, David D. Lewis and 
Dmitriy Fradkin 2005. Bayesian Multinomial Logis-
tic Regression for Author Identification, In Maxent 
Conference, 509-516. 
James W. Pennebaker, Martha E. Francis, and Roger J. 
Booth. 2001. Linguistic Inquiry and Word Count 
(LIWC): LIWC2001 Manual. Erlbaum Publishers, 
 Mahwah, NJ, USA. 
Helmut Schmid. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. 2004. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.  
Larry Selinker.1972. Interlanguage. International Re-
view of Applied Linguistics. 10, 209-241. 
Gideon Toury. 1995. Descriptive Translation Studies 
and beyond. John Benjamins, Amsterdam / Philadel-
phia. 
Hans van Halteren. 2008. Source language markers in 
EUROPARL translations. In COLING '08: Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 937-944. 
1326
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1356?1364,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Decomposition of a Document into Authorial Components 
 
 
Moshe Koppel      Navot Akiva Idan Dershowitz Nachum Dershowitz 
Dept. of Computer Science  Dept. of Bible School of Computer Science 
Bar-Ilan University Hebrew University Tel Aviv University 
Ramat Gan, Israel Jerusalem, Israel Ramat Aviv, Israel 
{moishk,navot.akiva}@gmail.com dershowitz@gmail.com nachumd@tau.ac.il 
 
 
 
 
Abstract 
We propose a novel unsupervised method 
for separating out distinct authorial compo-
nents of a document. In particular, we show 
that, given a book artificially ?munged? 
from two thematically similar biblical 
books, we can separate out the two consti-
tuent books almost perfectly. This allows 
us to automatically recapitulate many con-
clusions reached by Bible scholars over 
centuries of research. One of the key ele-
ments of our method is exploitation of dif-
ferences in synonym choice by different 
authors. 
1 Introduction  
We propose a novel unsupervised method for 
separating out distinct authorial components of a 
document.  
There are many instances in which one is faced 
with a multi-author document and wishes to deli-
neate the contributions of each author. Perhaps the 
most salient example is that of documents of his-
torical significance that appear to be composites of 
multiple earlier texts. The challenge for literary 
scholars is to tease apart the document?s various 
components. More contemporary examples include 
analysis of collaborative online works in which 
one might wish to identify the contribution of a 
particular author for commercial or forensic pur-
poses.  
We treat two versions of the problem. In the 
first, easier, version, the document to be decom-
posed is given to us segmented into units, each of 
which is the work of a single author. The challenge 
is only to cluster the units according to author. In 
the second version, we are given an unsegmented 
document and the challenge includes segmenting 
the document as well as clustering the resulting 
units. 
We assume here that no information about the 
authors of the document is available and that in 
particular we are not supplied with any identified 
samples of any author?s writing. Thus, our me-
thods must be entirely unsupervised.  
There is surprisingly little literature on this 
problem, despite its importance. Some work in this 
direction has been done on intrinsic plagiarism de-
tection (e.g., Meyer zu Eisen 2006) and document 
outlier detection (e.g., Guthrie et al 2008), but this 
work makes the simplifying assumption that there 
is a single dominant author, so that outlier units 
can be identified as those that deviate from the 
document as a whole. We don?t make this simpli-
fying assumption. Some work on a problem that is 
more similar to ours was done by Graham et al 
(2005). However, they assume that examples of 
pairs of paragraphs labeled as same-
author/different-author are available for use as the 
basis of supervised learning. We make no such 
assumption. 
The obvious approach to our unsupervised ver-
sion of the problem would be to segment the text 
(if necessary), represent each of the resulting units 
of text as a bag-of-words, and then use clustering 
algorithms to find natural clusters. We will see, 
however, that this na?ve method is quite inade-
quate. Instead, we exploit a method favored by the 
literary scholar, namely, the use of synonym 
choice. Synonym choice proves to be far more use-
ful for authorial decomposition than ordinary lexi-
cal features. However, synonyms are relatively 
1356
sparse and hence, though reliable, they are not 
comprehensive; that is, they are useful for separat-
ing out some units but not all. Thus, we use a two-
stage process: first find a reliable partial clustering 
based on synonym usage and then use these as the 
basis for supervised learning using a different fea-
ture set, such as bag-of-words. 
We use biblical books as our testbed. We do 
this for two reasons. First, this testbed is well mo-
tivated, since scholars have been doing authorial 
analysis of biblical literature for centuries. Second, 
precisely because it is of great interest, the Bible 
has been manually tagged in a variety of ways that 
are extremely useful for our method. 
Our main result is that given artificial books 
constructed by randomly ?munging? together ac-
tual biblical books, we are able to separate out au-
thorial components with extremely high accuracy, 
even when the components are thematically simi-
lar. Moreover, our automated methods recapitulate 
many of the results of extensive manual research in 
authorial analysis of biblical literature. 
The structure of the paper is as follows. In the 
next section, we briefly review essential informa-
tion regarding our biblical testbed. In Section 3, we 
introduce a na?ve method for separating compo-
nents and demonstrate its inadequacy. In Section 4, 
we introduce the synonym method, in Section 5 we 
extend it to the two-stage method, and in Section 6, 
we offer systematic empirical results to validate 
the method. In Section 7, we extend our method to 
handle documents that have not been pre-
segmented and present more empirical results. In 
Section 8, we suggest conclusions, including some 
implications for Bible scholarship. 
2 The Bible as Testbed 
While the biblical canon differs across religions 
and denominations, the common denominator con-
sists of twenty-odd books and several shorter 
works, ranging in length from tens to thousands of 
verses. These works vary significantly in genre, 
and include historical narrative, law, prophecy, and 
wisdom literature. Some of these books are re-
garded by scholars as largely the product of a sin-
gle author?s work, while others are thought to be 
composites in which multiple authors are well-
represented ? authors who in some cases lived in 
widely disparate periods. In this paper, we will 
focus exclusively on the Hebrew books of the Bi-
ble, and we will work with the original untran-
slated texts. 
The first five books of the Bible, collectively 
known as the Pentateuch, are the subject of much 
controversy. According to the predominant Jewish 
and Christian traditions, the five books were writ-
ten by a single author ? Moses. Nevertheless, scho-
lars have found in the Pentateuch what they believe 
are distinct narrative and stylistic threads corres-
ponding to multiple authors.  
Until now, the work of analyzing composite 
texts has been done in mostly impressionistic fa-
shion, whereby each scholar attempts to detect the 
telltale signs of multiple authorship and compila-
tion. Some work on biblical authorship problems 
within a computational framework has been at-
tempted, but does not handle our problem. Much 
earlier work (for example, Radday 1970; Bee 
1971; Holmes 1994) uses multivariate analysis to 
test whether the clusters in a given clustering of 
some biblical text are sufficiently distinct to be 
regarded as probably a composite text. By contrast, 
our aim is to find the optimal clustering of a docu-
ment, given that it is composite. Crucially, unlike 
that earlier work, we empirically prove the efficacy 
of our methods by testing it against known ground 
truth. Other computational work on biblical au-
thorship problems (Mealand 1995; Berryman et al 
2003) involves supervised learning problems 
where some disputed text is to be attributed to one 
of a set of known authors. The supervised author-
ship attribution problem has been well-researched 
(for surveys, see Juola (2008), Koppel et al (2009) 
and Stamatatos (2009)), but it is quite distinct from 
the unsupervised problem we consider here.  
Since our problem has been dealt with almost 
exclusively using heuristic methods, the subjective 
nature of such research has left much room for de-
bate. We propose to set this work on a firm algo-
rithmic basis by identifying an optimal stylistic 
subdivision of the text. We do not concern our-
selves with how or why such distinct threads exist. 
Those for whom it is a matter of faith that the Pen-
tateuch is not a composition of multiple writers can 
view the distinction investigated here as that of 
multiple styles. 
3 A Na?ve Algorithm 
For expository purposes, we will use a canoni-
cal example to motivate and illustrate each of a 
1357
sequence of increasingly sophisticated algorithms 
for solving the decomposition problem. Jeremiah 
and Ezekiel are two roughly contemporaneous 
books belonging to the same biblical sub-genre 
(prophetic works), and each is widely thought to 
consist primarily of the work of a single distinct 
author. Jeremiah consists of 52 chapters and Eze-
kiel consists of 48 chapters. For our first challenge, 
we are given all 100 unlabeled chapters and our 
task is to separate them out into the two constituent 
books. (For simplicity, let?s assume that it is 
known that there are exactly two natural clusters.) 
Note that this is a pre-segmented version of the 
problem since we know that each chapter belongs 
to only one of the books. 
As a first try, the basics of which will serve as a 
foundation for more sophisticated attempts, we do 
the following: 
1. Represent each chapter as a bag-of-words (us-
ing all words that appear at least k times in the 
corpus). 
2. Compute the similarity of every pair of chapters 
in the corpus. 
3. Use a clustering algorithm to cluster the chap-
ters into two clusters. 
We use k=2, cosine similarity and ncut cluster-
ing (Dhillon et al 2004). Comparing the Jeremiah-
Ezekiel split to the clusters thus obtained, we have 
the following matrix: 
Book Cluster I Cluster II 
Jer 
Eze 
29 
28 
23 
20 
 
As can be seen, the clusters are essentially or-
thogonal to the Jeremiah-Ezekiel split. Ideally, 
100% of the chapters would lie on the majority 
diagonal, but in fact only 51% do. Formally, our 
measure of correspondence between the desired 
clustering and the actual one is computed by first 
normalizing rows and then computing the weight 
of the majority diagonal relative to the whole. This 
measure, which we call normalized majority di-
agonal (NMD), runs from 50% (when the clusters 
are completely orthogonal to the desired split) to 
100% (where the clusters are identical with the 
desired split). NMD is equivalent to maximal ma-
cro-averaged recall where the maximum is taken 
over the (two) possible assignments of books to 
clusters. In this case, we obtain an NMD of 51.5%, 
barely above the theoretical minimum. 
This negative result is not especially surprising 
since there are many ways for the chapters to split 
(e.g., according to thematic elements, sub-genre, 
etc.) and we can?t expect an unsupervised method 
to read our minds. Thus, to guide the method in the 
direction of stylistic elements that might distin-
guish between Jeremiah and Ezekiel, we define a 
class of generic biblical words consisting of all 223 
words that appear at least five times in each of ten 
different books of the Bible. 
Repeating our experiment of above, though li-
miting our feature set to generic biblical words, we 
obtain the following matrix: 
Book Cluster I Cluster II 
Jer 
Eze 
32 
28 
20 
20 
 
As can be seen, using generic words yields 
NMD of 51.3%, which does not improve matters at 
all. Thus, we need to try a different approach. 
4 Exploiting Synonym Usage 
One of the key features used by Bible scholars 
to classify different components of biblical litera-
ture is synonym choice. The underlying hypothesis 
is that different authorial components are likely to 
differ in the proportions with which alternative 
words from a set of synonyms (synset) are used. 
This hypothesis played a part in the pioneering 
work of Astruc (1753) on the book of Genesis ?
using a single synset: divine names ? and has been 
refined by many others using broader feature sets, 
such as that of Carpenter and Hartford-Battersby 
(1900). More recently, the synonym hypothesis has 
been used in computational work on authorship 
attribution of English texts in the work of Clark 
and Hannon (2007) and Koppel et al (2006). 
This approach presents several technical chal-
lenges. First, ideally ? in the absence of a suffi-
ciently comprehensive thesaurus ? we would wish 
to identify synonyms in an automated fashion. 
Second, we need to adapt our similarity measure 
for reasons that will be made clear below. 
4.1 (Almost) Automatic Synset Identification 
One of the advantages of using biblical litera-
ture is the availability of a great deal of manual 
annotation. In particular, we are able to identify 
synsets by exploiting the availability of the stan-
dard King James translation of the Bible into Eng-
1358
lish (KJV). Conveniently, and unlike most modern 
translations, KJV almost invariably translates syn-
onyms identically. Thus, we can generally identify 
synonyms by considering the translated version of 
the text. There are two points we need to be precise 
about. First, it is not actually words that we regard 
as synonymous, but rather word roots. Second, to 
be even more precise, it is not quite roots that are 
synonymous, but rather senses of roots. Conve-
niently, Strong?s (1890 [2010]) Concordance lists 
every occurrence of each sense of each root that 
appears in the Bible separately (where senses are 
distinguished in accordance with the KJV transla-
tion). Thus, we can exploit KJV and the concor-
dance to automatically identify synsets as well as 
occurrences of the respective synonyms in a syn-
set.1 (The above notwithstanding, there is still a 
need for a bit of manual intervention: due to poly-
semy in English, false synsets are occasionally 
created when two non-synonymous Hebrew words 
are translated into two senses of the same English 
word. Although this could probably be handled 
automatically, we found it more convenient to do a 
manual pass over the raw synsets and eliminate the 
problems.)  
The above procedure yields a set of 529 synsets 
including a total of 1595 individual synonyms. 
Most synsets consist of only two synonyms, but 
some include many more. For example, there are 7 
Hebrew synonyms corresponding to ?fear?. 
4.2 Adapting the Similarity Measure 
Let?s now represent a unit of text as a vector in 
the following way. Each entry represents a syn-
onym in one of the synsets. If none of the syn-
onyms in a synset appear in the unit, all their cor-
responding entries are 0. If j different synonyms in 
a synset appear in the unit, then each correspond-
ing entry is 1/j and the rest are 0. Thus, in the typi-
cal case where exactly one of the synonyms in a 
synset appears, its corresponding entry in the vec-
tor is 1 and the rest are 0. 
Now we wish to measure the similarity of two 
such vectors. The usual cosine measure doesn?t 
capture what we want for the following reason. If 
the two units use different members of a synset, 
cosine is diminished; if they use the same members 
of a synset, cosine is increased. So far, so good. 
But suppose one unit uses a particular synonym 
                                                           
1
 Thanks to Avi Shmidman for his assistance with this. 
and the other doesn?t use any member of that syn-
set. This should teach us nothing about the similar-
ity of the two units, since it reflects only on the 
relevance of the synset to the content of that unit; it 
says nothing about which synonym is chosen when 
the synset is relevant. Nevertheless, in this case, 
cosine would be diminished. 
The required adaptation is as follows: we first 
eliminate from the representation any synsets that 
do not appear in both units (where a synset is said 
to appear in a unit if any of its constituent syn-
onyms appear in the unit). We then compute cosine 
of the truncated vectors. Formally, for a unit x 
represented in terms of synonyms, our new similar-
ity measure is cos'(x,y) = cos(x|S(x ?y),y|S(x ?y)), 
where x|S(x ?y) is the projection of x onto the syn-
sets that appear in both x and y.  
4.3  Clustering Jeremiah-Ezekiel Using Syn-
onyms 
We now apply ncut clustering to the similarity 
matrix computed as described above. We obtain 
the following split: 
Book Cluster I Cluster II 
Jer 
Eze 
48 
5 
4 
43 
 
Clearly, this is quite a bit better than results ob-
tained using simple lexical features as described 
above. Intuition for why this works can be pur-
chased by considering concrete examples. There 
are two Hebrew synonyms ? p???h and miq??a? 
corresponding to the word ?corner?, two (min??h 
and t?r?m?h) corresponding to the word ?obla-
tion?, and two (n??a? and ???al) corresponding to 
the word ?planted?. We find that p???h, min??h 
and n??a? tend to be located in the same units and, 
concomitantly, miq??a?, t?r?m?h and ???al are lo-
cated in the same units. Conveniently, the former 
are all Jeremiah and the latter are all Ezekiel.  
While the above result is far better than those 
obtained using more na?ve feature sets, it is, never-
theless, far from perfect. We have, however, one 
more trick at our disposal that will improve these 
results further. 
5 Combining Partial Clustering and Su-
pervised Learning 
Analysis of the above clustering results leads to 
two observations. First, some of the units belong 
1359
firmly to one cluster or the other. The rest have to 
be assigned to one cluster or the other because 
that?s the nature of the clustering algorithm, but in 
fact are not part of what we might think of as the 
core of either cluster. Informally, we say that a unit 
is in the core of its cluster if it is sufficiently simi-
lar to the centroid of its cluster and it is sufficiently 
more similar to the centroid of its cluster than to 
any other centroid. Formally, let S be a set of syn-
sets, let B be a set of units, and let C be a cluster-
ing of B where the units in B are represented in 
terms of the synsets in S. For a unit x in cluster 
C(x) with centroid c(x), we say that x is in the core 
of C(x) if cos'(x,c(x))>?1 and cos'(x,c(x))-cos'(x,c)>?2 
for every centroid c?c(x). In our experiments be-
low, we use ?1=1/?2 (corresponding to an angle of 
less than 45 degrees between x and the centroid of 
its cluster) and ?2=0.1. 
Second, the clusters that we obtain are based on 
a subset of the full collection of synsets that does 
the heavy lifting. Formally, we say that a synonym 
n in synset s is over-represented in cluster C if 
p(x?C|n?x) > p(x?C|s?x) and p(x?C|n?x) > p(x?C). 
That is, n is over-represented in C if knowing that 
n appears in a unit increases the likelihood that the 
unit is in C, relative to knowing only that some 
member of n?s synset appears in the unit and rela-
tive to knowing nothing. We say that a synset s is a 
separating synset for a clustering {C1,C2} if some 
synonym in s is over-represented in C1 and a dif-
ferent synonym in s is over-represented in C2. 
5.1 Defining the Core of a Cluster 
We leverage these two observations to formally 
define the cores of the respective clusters using the 
following iterative algorithm. 
1. Initially, let S be the collection of all synsets, let 
B be the set of all units in the corpus 
represented in terms of S, and let {C1,C2} be 
an initial clustering of the units in B. 
2. Reduce B to the cores of C1 and C2. 
3. Reduce S to the separating synsets for {C1,C2}. 
4. Redefine C1 and C2 to be the clusters obtained 
from clustering the units in the reduced B 
represented in terms of the synsets in reduced S. 
5. Repeat Steps 2-4 until convergence (no further 
changes to the retained units and synsets). 
At the end of this process, we are left with two 
well-separated cluster cores and a set of separating 
synsets. When we compute cores of clusters in our 
Jeremiah-Ezekiel experiment, 26 of the initial 100 
units are eliminated. Of the 154 synsets that appear 
in the Jeremiah-Ezekiel corpus, 118 are separating 
synsets for the resulting clustering. The resulting 
cluster cores split with Jeremiah and Ezekiel as 
follows:  
Book Cluster I Cluster II 
Jer 
Eze 
36 
2 
0 
36 
 
We find that all but two of the misplaced units 
are not part of the core. Thus, we have a better 
clustering but it is only a partial one. 
5.2 Using Cores for Supervised Learning 
Now that we have what we believe are strong 
representatives of each cluster, we can use them in 
a supervised way to classify the remaining unclus-
tered units. The interesting question is which fea-
ture set we should use. Using synonyms would just 
get us back to where we began. Instead we use the 
set of generic Bible words introduced earlier. The 
point to recall is that while this feature set proved 
inadequate in an unsupervised setting, this does not 
mean that it is inadequate for separating Jeremiah 
and Ezekiel, given a few good training examples. 
Thus, we use a bag-of-words representation re-
stricted to generic Bible words for the 74 units in 
our cluster cores and label them according to the 
cluster to which they were assigned. We now apply 
SVM to learn a classifier for the two clusters. We 
assign each unit, including those in the training set, 
to the class assigned to it by the SVM classifier. 
The resulting split is as follows: 
Book Cluster I Cluster II 
Jer 
Eze 
51 
0 
1 
48 
 
Remarkably, even the two Ezekiel chapters that 
were in the Jeremiah cluster (and hence were es-
sentially misleading training examples) end up on 
the Ezekiel side of the SVM boundary.  
It should be noted that our two-stage approach 
to clustering is a generic method not specific to our 
particular application. The point is that there are 
some feature sets that are very well suited to a par-
ticular unsupervised problem but are sparse, so 
they give only a partial clustering. At the same 
time, there are other feature sets that are denser 
and, possibly for that reason, adequate for super-
1360
vised separation of the intended classes but inade-
quate for unsupervised separation of the intended 
classes. This suggests an obvious two-stage me-
thod for clustering, which we use here to good ad-
vantage. 
This method is somewhat reminiscent of semi-
supervised methods sometimes used in text catego-
rization where few training examples are available 
(Nigam et al 2000). However, those methods typi-
cally begin with some information, either in the 
form of a small number of labeled documents or in 
the form of keywords, while we are not supplied 
with these. Furthermore, the semi-supervised work 
bootstraps iteratively, at each stage using features 
drawn from within the same feature set, while we 
use exactly two stages, the second of which uses a 
different type of feature set than the first.  
For the reader?s convenience, we summarize the 
entire two-stage method: 
1. Represent units in terms of synonyms. 
2. Compute similarities of pairs of units using 
cos'. 
3. Use ncut to obtain an initial clustering. 
4. Use the iterative method to find cluster cores. 
5. Represent units in cluster cores in terms of ge-
neric words. 
6. Use units in cluster cores as training for learn-
ing an SVM classifier. 
7. Classify all units according to the learned SVM 
classifier. 
6 Empirical Results 
We now test our method on other pairs of bibli-
cal books to see if we obtain comparable results to 
those seen above. We need, therefore, to identify a 
set of biblical books such that (i) each book is suf-
ficiently long (say, at least 20 chapters), (ii) each is 
written by one primary author, and (iii) the authors 
are distinct. Since we wish to use these books as a 
gold standard, it is important that there be a broad 
consensus regarding the latter two, potentially con-
troversial, criteria. Our choice is thus limited to the 
following five books that belong to two biblical 
sub-genres: Isaiah, Jeremiah, Ezekiel (prophetic 
literature), Job and Proverbs (wisdom literature). 
(Due to controversies regarding authorship (Pope 
1952, 1965), we include only Chapters 1-33 of 
Isaiah and only Chapters 3-41 of Job.) 
Recall that our experiment is as follows: For 
each pair of books, we are given all the chapters in 
the union of the two books and are given no infor-
mation regarding labels. The object is to sort out 
the chapters belonging to the respective two books. 
(The fact that there are precisely two constituent 
books is given.) 
We will use the three algorithms seen above: 
1. generic biblical words representation and ncut 
clustering; 
2. synonym representation and ncut clustering; 
3. our two-stage algorithm. 
We display the results in two separate figures. 
In Figure 1, we see results for the six pairs of 
books that belong to different sub-genres. In Figure 
2, we see results for the four pairs of books that are 
in the same genre. (For completeness, we include 
Jeremiah-Ezekiel, although it served above as a 
development corpus.) All results are normalized 
majority diagonal. 
 
 
Figure 1. Results of three clustering methods for differ-
ent-genre pairs 
 
 
Figure 2. Results of three clustering methods for same-
genre pairs 
    
As is evident, for different-genre pairs, even the 
simplest method works quite well, though not as 
well as the two-stage method, which is perfect for 
five of six such pairs. The real advantage of the 
two-stage method is for same-genre pairs. For 
1361
these the simple method is quite erratic, while the 
two-stage method is near perfect. We note that the 
synonym method without the second stage is 
slightly worse than generic words for different-
genre pairs (probably because these pairs share 
relatively few synsets) but is much more consistent 
for same-genre pairs, giving results in the area of 
90% for each such pair. The second stage reduces 
the errors considerably over the synonym method 
for both same-genre and different-genre pairs. 
7  Decomposing Unsegmented Documents 
Up to now, we have considered the case where 
we are given text that has been pre-segmented into 
pure authorial units. This does not capture the kind 
of decomposition problems we face in real life. For 
example, in the Pentateuch problem, the text is 
divided up according to chapter, but there is no 
indication that the chapter breaks are correlated 
with crossovers between authorial units. Thus, we 
wish now to generalize our two-stage method to 
handle unsegmented text. 
7.1 Generating Composite Documents 
To make the problem precise, let?s consider 
how we might create the kind of document that we 
wish to decompose. For concreteness, let?s think 
about Jeremiah and Ezekiel. We create a composite 
document, called Jer-iel, as follows: 
1. Choose the first k1 available verses of Jeremiah, 
where k1 is a random integer drawn from the 
uniform distribution over the integers 1 to m. 
2. Choose the first k2 available verses of Ezekiel, 
where k2 is a new random integer drawn from 
the above distribution. 
3. Repeat until one of the books is exhausted; then 
choose the remaining verses of the other book. 
For the experiments discussed below, we use 
m=100 (though further experiments, omitted for 
lack of space, show that results shown are essen-
tially unchanged for any m?60). Furthermore, to 
simulate the Pentateuch problem, we break Jer-iel 
into initial units by beginning a new unit whenever 
we reach the first verse of one of the original chap-
ters of Jeremiah or Ezekiel. (This does not leak any 
information since there is no inherent connection 
between these verses and actual crossover points.) 
7.2 Applying the Two-Stage Method 
Our method works as follows. First, we refine 
the initial units (each of which might be a mix of 
verses from Jeremiah and Ezekiel) by splitting 
them into smaller units that we hope will be pure 
(wholly from Jeremiah or from Ezekiel). We say 
that a synset is doubly-represented in a unit if the 
unit includes two different synonyms of that syn-
set. Doubly-represented synsets are an indication 
that the unit might include verses from two differ-
ent books. Our object is thus to split the unit in a 
way that minimizes doubly-represented synonyms. 
Formally, let M(x) represent the number of synsets 
for which more than one synonym appear in x. Call 
?x1,x2? a split of x if x=x1x2. A split ?x1',x2'? is optim-
al if ?x1',x2'?= argmax M(x)-max(M(x1),M(x2)) where 
the maximum is taken over all splits of x. If for an 
initial unit, there is some split for which M(x)-
max(M(x1),M(x2)) is greater than 0, we split the unit 
optimally; if there is more than one optimal split, 
we choose the one closest to the middle verse of 
the unit. (In principle, we could apply this proce-
dure iteratively; in the experiments reported here, 
we split only the initial units but not split units.) 
Next, we run the first six steps of the two-stage 
method on the units of Jer-iel obtained from the 
splitting process, as described above, until the 
point where the SVM classifier has been learned. 
Now, instead of classifying chapters as in Step 7 of 
the algorithm, we classify individual verses. 
The problem with classifying individual verses 
is that verses are short and may contain few or no 
relevant features. In order to remedy this, and also 
to take advantage of the stickiness of classes across 
consecutive verses (if a given verse is from a cer-
tain book, there is a good chance that the next 
verse is from the same book), we use two smooth-
ing tactics. 
Initially, each verse is assigned a raw score by 
the SVM classifier, representing its signed distance 
from the SVM boundary. We smooth these scores 
by computing for each verse a refined score that is 
a weighted average of the verse?s raw score and 
the raw scores of the two verses preceding and 
succeeding it. (In our scheme, the verse itself is 
given 1.5 times as much weight as its immediate 
neighbors and three times as much weight as sec-
ondary neighbors.) 
Moreover, if the refined score is less than 1.0 
(the width of the SVM margin), we do not initially 
1362
assign the verse to either class. Rather, we check 
the class of the last assigned verse before it and the 
first assigned verse after it. If these are the same, 
the verse is assigned to that class (an operation we 
call ?filling the gaps?). If they are not, the verse 
remains unassigned.  
To illustrate on the case of Jer-iel, our original 
?munged? book has 96 units. After pre-splitting, 
we have 143 units. Of these, 105 are pure units. 
Our two cluster cores, include 33 and 39 units, re-
spectively; 27 of the former are pure Jeremiah and 
30 of the latter are pure Ezekiel; no pure units are 
in the ?wrong? cluster core. Applying the SVM 
classifier learned on the cluster cores to individual 
verses, 992 of the 2637 verses in Jer-iel lie outside 
the SVM margin and are assigned to some class. 
All but four of these are assigned correctly. Filling 
the gaps assigns a class to 1186 more verses, all 
but ten of them correctly. Of the remaining 459 
unassigned verses, most lie along transition points 
(where smoothing tends to flatten scores and where 
preceding and succeeding assigned verses tend to 
belong to opposite classes). 
7.3 Empirical Results 
We randomly generated composite books for 
each of the book pairs considered above. In Fig-
ures 3 and 4, we show for each book pair the per-
centage of all verses in the munged document that 
are ?correctly? classed (that is, in the majority di-
agonal), the percentage incorrectly classed (minori-
ty diagonal) and the percentage not assigned to 
either class. As is evident, in each case the vast 
majority of verses are correctly assigned and only a 
small fraction are incorrectly assigned. That is, we 
can tease apart the components almost perfectly.  
 
 
Figure 3. Percentage of verses in each munged differ-
ent-genre pair of books that are correctly and incorrectly 
assigned or remain unassigned. 
 
Figure 4. Percentage of verses in each munged same-
genre pair of books that are correctly and incorrectly 
assigned or remain unassigned. 
8 Conclusions and Future Work 
We have shown that documents can be decom-
posed into authorial components with very high 
accuracy by using a two-stage process. First, we 
establish a reliable partial clustering of units by 
using synonym choice and then we use these par-
tial clusters as training texts for supervised learn-
ing using generic words as features. 
We have considered only decompositions into 
two components, although our method generalizes 
trivially to more than two components, for example 
by applying it iteratively. The real challenge is to 
determine the correct number of components, 
where this information is not given. We leave this 
for future work. 
Despite this limitation, our success on munged 
biblical books suggests that our method can be 
fruitfully applied to the Pentateuch, since the broad 
consensus in the field is that the Pentateuch can be 
divided into two main authorial categories: Priestly 
(P) and non-Priestly (Driver 1909). (Both catego-
ries are often divided further, but these subdivi-
sions are more controversial.) We find that our 
split corresponds to the expert consensus regarding 
P and non-P for over 90% of the verses in the Pen-
tateuch for which such consensus exists. We have 
thus been able to largely recapitulate several centu-
ries of painstaking manual labor with our auto-
mated method. We offer those instances in which 
we disagree with the consensus for the considera-
tion of scholars in the field. 
In this work, we have exploited the availability 
of tools for identifying synonyms in biblical litera-
ture. In future work, we intend to extend our me-
thods to texts for which such tools are unavailable. 
1363
References  
J. Astruc. 1753. Conjectures sur les m?moires originaux 
dont il paroit que Moyse s?est servi pour composer le 
livre de la Gen?se. Brussels. 
R. E. Bee. 1971. Statistical methods in the study of the 
Masoretic text of the Old Testament. J. of the Royal 
Statistical Society, 134(1):611-622. 
M. J. Berryman, A. Allison, and D. Abbott. 2003. Sta-
tistical techniques for text classification based on 
word recurrence intervals. Fluctuation and Noise Let-
ters, 3(1):L1-L10. 
J. E. Carpenter, G. Hartford-Battersby. 1900. The Hex-
ateuch: According to the Revised Version. London. 
J. Clark and C. Hannon. 2007. A classifier system for 
author recognition using synonym-based features. 
Proc. Sixth Mexican International Conference on Ar-
tificial Intelligence, Lecture Notes in Artificial Intel-
ligence, vol. 4827, pp. 839-849. 
I. S. Dhillon, Y. Guan, and B. Kulis. 2004. Kernel k-
means: spectral clustering and normalized cuts. Proc. 
ACM International Conference on Knowledge Dis-
covery and Data Mining (KDD), pp. 551-556. 
S. R. Driver. 1909. An Introduction to the Literature of 
the Old Testament (8th ed.). Clark, Edinburgh. 
N. Graham, G. Hirst, and B. Marthi. 2005. Segmenting 
documents by stylistic character. Natural Language 
Engineering, 11(4):397-415. 
D. Guthrie, L. Guthrie, and Y. Wilks. 2008. An unsu-
pervised probabilistic approach for the detection of 
outliers in corpora. Proc. Sixth International Lan-
guage Resources and Evaluation (LREC'08), pp. 28-
30. 
D. Holmes. 1994. Authorship attribution, Computers 
and the Humanities, 28(2):87-106.  
P. Juola. 2008. Author Attribution. Series title: 
Foundations and Trends in Information Retriev-
al. Now Publishing, Delft. 
M. Koppel, N. Akiva, and I. Dagan. 2006. Feature in-
stability as a criterion for selecting potential style 
markers. J. of the American Society for Information 
Science and Technology, 57(11):1519-1525. 
M. Koppel, J.  Schler, and S. Argamon. 2009.  Compu-
tational methods in authorship attribution. J. of the 
American Society for Information Science and Tech-
nology, 60(1):9-26. 
D. L. Mealand. 1995. Correspondence analysis of Luke. 
Lit. Linguist Computing, 10(3):171-182. 
S. Meyer zu Eisen and B. Stein. 2006. Intrinsic plagiar-
ism detection. Proc. European Conference on Infor-
mation Retrieval (ECIR 2006), Lecture Notes in 
Computer Science, vol. 3936, pp. 565?569. 
K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mit-
chell. 2000. Text classification from labeled and un-
labeled documents using EM, Machine Learning, 
39(2/3):103-134. 
M. H. Pope. 1965. Job (The Anchor Bible, Vol. XV). 
Doubleday, New York, NY. 
M. H. Pope. 1952. Isaiah 34 in relation to Isaiah 35, 40-
66. Journal of Biblical Literature, 71(4):235-243. 
Y. Radday. 1970. Isaiah and the computer: A prelimi-
nary report, Computers and the Humanities, 5(2):65-
73. 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. J. of the American Society for 
Information Science and Technology, 60(3):538-556. 
J. Strong. 1890. The Exhaustive Concordance of the 
Bible. Nashville, TN. (Online edition: 
http://www.htmlbible.com/sacrednamebiblecom/kjvs
trongs/STRINDEX.htm; accessed 14 November 
2010.) 
 
 
 
 
 
 
 
1364
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289?295,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automatic Detection of Machine Translated Text and Translation Quality
Estimation
Roee Aharoni
Dept. of Computer Science
Bar Ilan University
Ramat-Gan, Israel 52900
roee.aharoni@gmail.com
Moshe Koppel
Dept. of Computer Science
Bar Ilan University
Ramat-Gan, Israel 52900
moishk@gmail.com
Yoav Goldberg
Dept. of Computer Science
Bar Ilan University
Ramat-Gan, Israel 52900
yoav.goldberg@gmail.com
Abstract
We show that it is possible to automati-
cally detect machine translated text at sen-
tence level from monolingual corpora, us-
ing text classification methods. We show
further that the accuracy with which a
learned classifier can detect text as ma-
chine translated is strongly correlated with
the translation quality of the machine
translation system that generated it. Fi-
nally, we offer a generic machine transla-
tion quality estimation technique based on
this approach, which does not require ref-
erence sentences.
1 Introduction
The recent success and proliferation of statistical
machine translation (MT) systems raise a number
of important questions. Prominent among these
are how to evaluate the quality of such a system
efficiently and how to detect the output of such
systems (for example, to avoid using it circularly
as input for refining MT systems).
In this paper, we will answer both these ques-
tions. First, we will show that using style-related
linguistic features, such as frequencies of parts-
of-speech n-grams and function words, it is pos-
sible to learn classifiers that distinguish machine-
translated text from human-translated or native
English text. While this is a straightforward and
not entirely novel result, our main contribution is
to relativize the result. We will see that the suc-
cess of such classifiers are strongly correlated with
the quality of the underlying machine translation
system. Specifically, given a corpus consisting of
both machine-translated English text (English be-
ing the target language) and native English text
(not necessarily the reference translation of the
machine-translated text), we measure the accuracy
of the system in classifying the sentences in the
corpus as machine-translated or not. This accu-
racy will be shown to decrease as the quality of
the underlying MT system increases. In fact, the
correlation is strong enough that we propose that
this accuracy measure itself can be used as a mea-
sure of MT system quality, obviating the need for
a reference corpus, as for example is necessary for
BLEU (Papineni et al, 2001).
The paper is structured as follows: In the next
section, we review previous related work. In the
third section, we describe experiments regarding
the detection of machine translation and in the
fourth section we discuss the use of detection tech-
niques as a machine translation quality estimation
method. In the final section we offer conclusions
and suggestions for future work.
2 Previous Work
2.1 Translationese
The special features of translated texts have been
studied widely for many years. Attempts to de-
fine their characteristics, often called ?Translation
Universals?, include (Toury, 1980; Blum-Kulka
and Levenston, 1983; Baker, 1993; Gellerstam,
1986). The differences between native and trans-
lated texts found there go well beyond systematic
translation errors and point to a distinct ?Transla-
tionese? dialect.
Using automatic text classification methods in
the field of translation studies had many use cases
in recent years, mainly as an empirical method
of measuring, proving or contradicting translation
universals. Several works (Baroni and Bernar-
dini, 2006; Kurokawa et al, 2009; Ilisei et al,
2010) used text classification techniques in order
to distinguish human translated text from native
language text at document or paragraph level, us-
ing features like word and POS n-grams, propor-
tion of grammatical words in the text, nouns, fi-
nite verbs, auxiliary verbs, adjectives, adverbs, nu-
289
merals, pronouns, prepositions, determiners, con-
junctions etc. Koppel and Ordan (2011) classi-
fied texts to original or translated, using a list
of 300 function words taken from LIWC (Pen-
nebaker et al, 2001) as features. Volanski et
al. (2013) also tested various hypotheses regarding
?Translationese?, using 32 different linguistically-
informed features, to assess the degree to which
different sets of features can distinguish between
translated and original texts.
2.2 Machine Translation Detection
Regarding the detection of machine translated
text, Carter and Inkpen (2012) translated the
Hansards of the 36th Parliament of Canada us-
ing the Microsoft Bing MT web service, and
conducted three detection experiments at docu-
ment level, using unigrams, average token length,
and type-token ratio as features. Arase and
Zhou (2013) trained a sentence-level classifier to
distinguish machine translated text from human
generated text on English and Japanese web-page
corpora, translated by Google Translate, Bing and
an in-house SMT system. They achieved very high
detection accuracy using application-specific fea-
ture sets for this purpose, including indicators of
the ?Phrase Salad? (Lopez, 2008) phenomenon or
?Gappy-Phrases? (Bansal et al, 2011).
While Arase and Zhou (2013) considered MT
detection at sentence level, as we do in this pa-
per, they did not study the correlation between the
translation quality of the machine translated text
and the ability to detect it. We show below that
such detection is possible with very high accuracy
only on low-quality translations. We examine this
detection accuracy vs. quality correlation, with
various MT systems, such as rule-based and sta-
tistical MT, both commercial and in-house, using
various feature sets.
3 Detection Experiments
3.1 Features
We wish to distinguish machine translated En-
glish sentences from either human-translated sen-
tences or native English sentences. Due to the
sparseness of the data at the sentence level, we
use common content-independent linguistic fea-
tures for the classification task. Our features are
binary, denoting the presence or absence of each
of a set of part-of-speech n-grams acquired using
the Stanford POS tagger (Toutanova et al, 2003),
as well as the presence or absence of each of 467
function words taken from LIWC (Pennebaker et
al., 2001). We consider only those entries that ap-
pear at least ten times in the entire corpus, in order
to reduce sparsity in the data. As our learning al-
gorithm we use SVM with sequential minimal op-
timization (SMO), taken from the WEKA machine
learning toolkit (Hall et al, 2009).
3.2 Detecting Different MT Systems
In the first experiment set, we explore the ability
to detect outputs of machine translated text from
different MT systems, in an environment contain-
ing both human generated and machine translated
text. For this task, we use a portion of the Cana-
dian Hansard corpus (Germann, 2001), containing
48,914 parallel sentences from French to English.
We translate the French portion of the corpus using
several MT systems, respectively: Google Trans-
late, Systran, and five other commercial MT sys-
tems available at the http://itranslate4.eu website,
which enables to query example MT systems built
by several european MT companies. After trans-
lating the sentences, we take 20,000 sentences
from each engine output and conduct the detection
experiment by labeling those sentences as MT sen-
tences, and another 20,000 sentences, which are
the human reference translations, labeled as ref-
erence sentences. We conduct a 10-fold cross-
validation experiment on the entire 40,000 sen-
tence corpus. We also conduct the same exper-
iment using 20,000 random, non-reference sen-
tences from the same corpus, instead of the ref-
erence sentences. Using simple linear regression,
we also obtain an R
2
value (coefficient of deter-
mination) over the measurements of detection ac-
curacy and BLEU score, for each of three feature
set combinations (function words, POS tags and
mixed) and the two data combinations (MT vs.
reference and MT vs. non reference sentences).
The detection and R
2
results are shown in Table 1.
As can be seen, best detection results are ob-
tained using the full combined feature set. It can
also be seen that, as might be expected, it is easier
to distinguish machine-translated sentences from
a non-reference set than from the reference set. In
Figure 1, we show the relationship of the observed
detection accuracy for each system with the BLEU
score of that system. As is evident, regardless
of the feature set or non-MT sentences used, the
correlation between detection accuracy and BLEU
290
10 20 30
60
70
80
90
BLEU
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
mix-nr
mix-r
fw-nr
fw-r
pos-nr
pos-r
Figure 1: Correlation between detection accu-
racy and BLEU score on commercial MT systems,
using POS, function words and mixed features
against reference and non-reference sentences.
score is very high, as we can also see from the R
2
values in Table 1.
3.3 In-House SMT Systems
Parallel Monolingual BLEU
SMT-1 2000k 2000k 28.54
SMT-2 1000k 1000k 27.76
SMT-3 500k 500k 29.18
SMT-4 100k 100k 23.83
SMT-5 50k 50k 24.34
SMT-6 25k 25k 22.46
SMT-7 10k 10k 20.72
Table 3: Details for Moses based SMT systems
In the second experiment set, we test our de-
tection method on SMT systems we created, in
which we have control over the training data and
the expected overall relative translation quality. In
order to do so, we use the Moses statistical ma-
chine translation toolkit (Koehn et al, 2007). To
train the systems, we take a portion of the Europarl
corpus (Koehn, 2005), creating 7 different SMT
systems, each using a different amount of train-
ing data, for both the translation model and lan-
guage model. We do this in order to create dif-
ferent quality translation systems, details of which
are described in Table 3. For purposes of classifi-
cation, we use the same content independent fea-
tures as in the previous experiment, based on func-
20 22 24 26 28 30
72
73
74
75
76
BLEU
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.789
Figure 2: Correlation between detection accu-
racy and BLEU score on in-house Moses-based
SMT systems against non-reference sentences us-
ing content independent features.
tion words and POS tags, again with SMO-based
SVM as the classifier. For data, we use 20,000 ran-
dom, non reference sentences from the Hansard
corpus, against 20,000 sentences from one MT
system per experiment, again resulting in 40,000
sentence instances per experiment. The relation-
ship between the detection results for each MT
system and the BLEU score for that system, re-
sulting in R
2
= 0.774, is shown in Figure 2.
4 Machine Translation Evaluation
4.1 Human Evaluation Experiments
As can be seen in the above experiments, there is
a strong correlation between the BLEU score and
the MT detection accuracy of our method. In fact,
results are linearly and negatively correlated with
BLEU, as can be seen both on commercial systems
and our in-house SMT systems. We also wish to
consider the relationship between detection accu-
racy and a human quality estimation score. To
do this, we use the French-English data from the
8th Workshop on Statistical Machine Translation
- WMT13? (Bojar et al, 2013), containing out-
puts from 13 different MT systems and their hu-
man evaluations. We conduct the same classifi-
cation experiment as above, with features based
on function words and POS tags, and SMO-based
SVM as the classifier. We first use 3000 refer-
291
Features Data Google Moses Systran ProMT Linguatec Skycode Trident R
2
mixed MT/non-ref 63.34 72.02 72.36 78.2 79.57 80.9 89.36 0.946
mixed MT/ref 59.51 69.47 69.77 75.86 78.11 79.24 88.85 0.944
func. w. MT/non-ref 60.43 69.17 69.87 69.78 71.38 75.46 84.97 0.798
func. w. MT/ref 57.27 66.05 67.48 67.06 68.58 73.37 84.79 0.779
POS MT/non-ref 60.32 64.39 66.61 73 73.9 74.33 79.6 0.978
POS MT/ref 57.21 65.55 64.12 70.29 73.06 73.04 78.84 0.948
Table 1: Classifier performance, including the R
2
coefficient describing the correlation with BLEU.
MT Engine Example
Google Translate ?These days, all but one were subject to a vote,
and all had a direct link to the post September 11th.?
Moses ?these days , except one were the subject of a vote ,
and all had a direct link with the after 11 September .?
Systran ?From these days, all except one were the object of a vote,
and all were connected a direct link with after September 11th.?
Linguatec ?Of these days, all except one were making the object of a vote
and all had a straightforward tie with after September 11.?
ProMT ?These days, very safe one all made object a vote,
and had a direct link with after September 11th.?
Trident ?From these all days, except one operated object voting,
and all had a direct rope with after 11 septembre.?
Skycode ?In these days, all safe one made the object in a vote
and all had a direct connection with him after 11 of September.?
Table 2: Outputs from several MT systems for the same source sentence (function words marked in bold)
0.3 0.4 0.5 0.6
58
60
62
64
human evaluation score
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.774
Figure 3: Correlation between detection accuracy
and human evaluation scores on systems from
WMT13? against reference sentences.
0.3 0.4 0.5 0.6
73
74
75
76
77
human evaluation score
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.556
Figure 4: Correlation between detection accu-
racy and human evaluation scores on systems from
WMT 13? against non-reference sentences.
292
0.3 0.4 0.5 0.6
62
64
66
human evaluation score
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.829
Figure 5: Correlation between detection accu-
racy and human evaluation scores on systems from
WMT 13? against non-reference sentences, using
the syntactic CFG features described in section 4.2
ence sentences from the WMT13? English refer-
ence translations, against the matching 3000 out-
put sentences from one MT system at a time, re-
sulting in 6000 sentence instances per experiment.
As can be seen in Figure 3, the detection accuracy
is strongly correlated with the evaluations scores,
yielding R
2
= 0.774. To provide another mea-
sure of correlation, we compared every pair of
data points in the experiment to get the proportion
of pairs ordered identically by the human evalu-
ators and our method, with a result of 0.846 (66
of 78). In the second experiment, we use 3000
random, non reference sentences from the new-
stest 2011-2012 corpora published in WMT12?
(Callison-Burch et al, 2012) against 3000 output
sentences from one MT system at a time, again re-
sulting in 6000 sentence instances per experiment.
While applying the same classification method as
with the reference sentences, the detection accu-
racy rises, while the correlation with the transla-
tion quality yields R
2
= 0.556, as can be seen in
Figure 4. Here, the proportion of identically or-
dered pairs is 0.782 (61 of 78).
4.2 Syntactic Features
We note that the second leftmost point in Figures
3, 4 is an outlier: that is, our method has a hard
time detecting sentences produced by this system
although it is not highly rated by human evalu-
ators. This point represents the Joshua (Post et
al., 2013) SMT system. This system is syntax-
based, which apparently confound our POS and
FW-based classifier, despite it?s low human evalu-
ation score. We hypothesize that the use of syntax-
based features might improve results. To ver-
ify this intuition, we create parse trees using the
Berkeley parser (Petrov and Klein, 2007) and ex-
tract the one-level CFG rules as features. Again,
we represent each sentence as a boolean vector,
in which each entry represents the presence or ab-
sence of the CFG rule in the parse-tree of the sen-
tence. Using these features alone, without the FW
and POS tag based features presented above, we
obtain an R
2
= 0.829 with a proportion of iden-
tically ordered pairs at 0.923 (72 of 78), as shown
in Figure 5.
5 Discussion and Future Work
We have shown that it is possible to detect ma-
chine translation from monolingual corpora con-
taining both machine translated text and human
generated text, at sentence level. There is a strong
correlation between the detection accuracy that
can be obtained and the BLEU score or the human
evaluation score of the machine translation itself.
This correlation holds whether or not a reference
set is used. This suggests that our method might be
used as an unsupervised quality estimation method
when no reference sentences are available, such
as for resource-poor source languages. Further
work might include applying our methods to other
language pairs and domains, acquiring word-level
quality estimation or integrating our method in
a machine translation system. Furthermore, ad-
ditional features and feature selection techniques
can be applied, both for improving detection ac-
curacy and for strengthening the correlation with
human quality estimation.
Acknowledgments
We would like to thank Noam Ordan and Shuly
Wintner for their help and feedback on the early
stages of this work. This research was funded in
part by the Intel Collaborative Research Institute
for Computational Intelligence.
293
References
Yuki Arase and Ming Zhou. 2013. Machine transla-
tion detection from monolingual web-text. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1597?1607, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Mona Baker. 1993. Corpus linguistics and transla-
tion studies: Implications and applications. Text and
technology: in honour of John Sinclair, 233:250.
Mohit Bansal, Chris Quirk, and Robert C. Moore.
2011. Gappy phrasal alignment by agreement. In
Dekang Lin, Yuji Matsumoto, and Rada Mihalcea,
editors, ACL, pages 1308?1317. The Association for
Computer Linguistics.
Marco Baroni and Silvia Bernardini. 2006. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. LLC, 21(3):259?274.
Shoshana Blum-Kulka and Eddie A. Levenston. 1983.
Universals of lexical simplification. Strategies in In-
terlanguage Communication, pages 119?139.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Dave Carter and Diana Inkpen. 2012. Searching
for poor quality machine translated text: Learning
the difference between human writing and machine
translations. In Leila Kosseim and Diana Inkpen,
editors, Canadian Conference on AI, volume 7310
of Lecture Notes in Computer Science, pages 49?60.
Springer.
Martin Gellerstam. 1986. Translationese in swedish
novels translated from english. In Lars Wollin
and Hans Lindquist, editors, Translation Studies in
Scandinavia, pages 88?95.
Ulrich Germann. 2001. Aligned hansards of the 36th
parliament of canada release 2001-1a.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov. 2010. Identification of transla-
tionese: A machine learning approach. In Alexan-
der F. Gelbukh, editor, CICLing, volume 6008 of
Lecture Notes in Computer Science, pages 503?511.
Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL. The Association for Computer Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Dekang Lin, Yuji Matsumoto,
and Rada Mihalcea, editors, ACL, pages 1318?1326.
The Association for Computer Linguistics.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic Detection of Translated Text and
its Impact on Machine Translation. In Conference
Proceedings: the twelvth Machine Translation Sum-
mit.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys (CSUR), 40(3):8.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical report,
IBM Research Report.
J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: Liwc 2001.
Mahway: Lawrence Erlbaum Associates.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Proceedings of the Main Confer-
ence, pages 404?411.
Marius Popescu. 2011. Studying translationese at
the character level. In Galia Angelova, Kalina
Bontcheva, Ruslan Mitkov, and Nicolas Nicolov, ed-
itors, RANLP, pages 634?639. RANLP 2011 Organ-
ising Committee.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, August 8-9, 2013., pages 206?
212. Association for Computational Linguistics.
Gideon Toury. 1980. In Search of a Theory of Transla-
tion.
294
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In IN PROCEEDINGS OF HLT-NAACL, pages 252?
259.
Hans van Halteren. 2008. Source language markers
in europarl translations. In Donia Scott and Hans
Uszkoreit, editors, COLING, pages 937?944.
Vered Volansky, Noam Ordan, and Shuly Wintner.
2013. On the features of translationese. Literary
and Linguistic Computing.
295
