Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036?1046,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Reducing Dimensions of Tensors in Type-Driven Distributional Semantics
Tamara Polajnar Luana F
?
ag
?
ar
?
as?an Stephen Clark
Computer Laboratory
University of Cambridge
Cambridge, UK
first.last@cl.cam.ac.uk
Abstract
Compositional distributional semantics is
a subfield of Computational Linguistics
which investigates methods for represent-
ing the meanings of phrases and sen-
tences. In this paper, we explore im-
plementations of a framework based on
Combinatory Categorial Grammar (CCG),
in which words with certain grammatical
types have meanings represented by multi-
linear maps (i.e. multi-dimensional arrays,
or tensors). An obstacle to full implemen-
tation of the framework is the size of these
tensors. We examine the performance of
lower dimensional approximations of tran-
sitive verb tensors on a sentence plausi-
bility/selectional preference task. We find
that the matrices perform as well as, and
sometimes even better than, full tensors,
allowing a reduction in the number of pa-
rameters needed to model the framework.
1 Introduction
An emerging subfield of computational linguis-
tics is concerned with learning compositional dis-
tributional representations of meaning (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Coecke et al., 2010; Grefenstette and Sadrzadeh,
2011; Clarke, 2012; Socher et al., 2012; Clark,
2013). The advantage of such representations lies
in their potential to combine the benefits of dis-
tributional approachs to word meaning (Sch?utze,
1998; Turney and Pantel, 2010) with the more tra-
ditional compositional methods from formal se-
mantics (Dowty et al., 1981). Distributional repre-
sentations have the properties of robustness, learn-
ability from data, ease of handling ambiguity,
and the ability to represent gradations of mean-
ing; whereas compositional models handle the un-
bounded nature of natural language, as well as
providing established accounts of logical words,
quantification, and inference.
One promising approach which attempts to
combine elements of compositional and distribu-
tional semantics is by Coecke et al. (2010). The
underlying idea is to take the type-driven approach
from formal semantics ? in particular the idea
that the meanings of complex grammatical types
should be represented as functions ? and ap-
ply it to distributional representations. Since the
mathematics of distributional semantics is pro-
vided by linear algebra, a natural set of functions
to consider is the set of linear maps. Coecke et
al. recognize that there is a natural correspon-
dence from complex grammatical types to tensors
(multi-linear maps), so that the meaning of an ad-
jective, for example, is represented by a matrix (a
2nd-order tensor)
1
and the meaning of a transitive
verb is represented by a 3rd-order tensor.
Coecke et al. use the grammar of pregroups
as the syntactic machinery to construct distribu-
tional meaning representations, since both pre-
groups and vector spaces can be seen as exam-
ples of the same abstract structure, which leads
to a particularly clean mathematical description of
the compositional process. However, the approach
applies more generally, for example to other forms
of categorial grammar, such as Combinatory Cate-
gorial Grammar (Steedman, 2000; Maillard et al.,
2014), and also to phrase-structure grammars in a
way that a formal linguist would recognize (Ba-
roni et al., 2014). Clark (2013) provides a descrip-
tion of the tensor-based framework aimed more at
computational linguists, relying only on the math-
ematics of multi-linear algebra rather than the cat-
egory theory used in Coecke et al. (2010). Sec-
tion 2 repeats some of this description.
A major open question associated with the
tensor-based semantic framework is how to learn
1
This same insight lies behind the work of Baroni and
Zamparelli (2010).
1036
the tensors representing the meanings of words
with complex types, such as verbs and adjec-
tives. The framework is essentially a composi-
tional framework, providing a recipe for how to
combine distributional representations, but leav-
ing open what the underlying vector spaces are and
how they can be acquired. One significant chal-
lenge is an engineering one: in a wide-coverage
grammar, which is able to handle naturally occur-
ring text, there will be a) a large lexicon with many
word-category pairs requiring tensor representa-
tions; and b) many higher-order tensors with large
numbers of parameters which need to be learned.
In this paper we take a first step towards learning
such representations, by learning tensors for tran-
sitive verbs.
One feature of the tensor-based framework is
that it allows the meanings of words and phrases
with different basic types, for example nouns and
sentences, to live in different vector spaces. This
means that the sentence space is task specific, and
must be defined in advance. For example, to calcu-
late sentence similarity, we would have to learn a
vector space where distances between vectors rep-
resenting the meanings of sentences reflect simi-
larity scores assigned by human annotators.
In this paper we describe an initial investi-
gation into the learning of word meanings with
complex syntactic types, together with a simple
sentence space. The space we consider is the
?plausibility space? described by Clark (2013), to-
gether with sentences of the form subject-verb-
object. This space is defined to distinguish se-
mantically plausible sentences (e.g. Animals eat
plants) from implausible ones (e.g. Animals eat
planets). Plausibility can be either represented
as a single continuous variable between 0 and 1,
or as a two-dimensional probability distribution
over the classes plausible (>) and implausible (?).
Whether we consider a one- or two-dimensional
sentence space depends on the architecture of the
logistic regression classifier that is used to learn
the verb (Section 3).
We begin with this simple plausibility sentence
space to determine if, in fact, the tensor-based rep-
resentation can be learned to a sufficiently useful
degree. Other simple sentence spaces which can
perhaps be represented using one or two variables
include a ?sentence space? for the sentiment anal-
ysis task (Socher et al., 2013), where one variable
represents positive sentiment and the other nega-
tive. We also expect that the insights gained from
research on this task can be applied to more com-
plex sentence spaces, for example a semantic simi-
larity space which will require more than two vari-
ables.
2 Syntactic Types to Tensors
The syntactic type of a transitive verb in English
is (S\NP)/NP (using notation from Steedman
(2000)), meaning that a transitive verb is a func-
tion which takes an NP argument to the right, an
NP argument to the left, and results in a sentence
S . Such categories with slashes are complex cate-
gories; S and NP are basic or atomic categories.
Interpreting such categories under the Coecke et
al. framework is straightforward. First, for each
atomic category there is a corresponding vector
space; in this case the sentence space S and the
noun space N.
2
Hence the meaning of a noun or
noun phrase, for example people, will be a vector
in the noun space:
????
people ? N. In order to obtain
the meaning of a transitive verb, each slash is re-
placed with a tensor product operator, so that the
meaning of eat, for example, is a 3rd-order tensor:
eat ? S?N?N. Just as in the syntactic case,
the meaning of a transitive verb is a function (a
multi-linear map) which takes two noun vectors as
arguments and returns a sentence vector.
Meanings combine using tensor contraction,
which can be thought of as a multi-linear gen-
eralisation of matrix multiplication (Grefenstette,
2013). Consider first the adjective-noun case, for
example black cat. The syntactic type of black
is N /N ; hence its meaning is a 2nd-order tensor
(matrix): black ? N?N. In the syntax, N /N
combines with N using the rule of forward appli-
cation (N /N N ? N ), which is an instance of
function application. Function application is also
used in the tensor-based semantics, which, for a
matrix and vector argument, corresponds to ma-
trix multiplication.
Figure 1 shows how the syntactic types com-
bine with a transitive verb, and the corresponding
tensor-based semantic types. Note that, after the
verb has combined with its object NP , the type
of the verb phrase is S\NP , with a correspond-
ing meaning tensor (matrix) in S ?N. This ma-
trix then combines with the subject vector, through
2
In practice, for example using the CCG parser of Clark
and Curran (2007), there will be additional atomic categories,
such as PP , but not many more.
1037
people eat fish
NP (S\NP)/NP NP
N S?N?N N
>
S\NP
S?N
<
S
S
Figure 1: Syntactic reduction and tensor-based se-
mantic types for a transitive verb sentence
matrix multiplication, to give a sentence vector.
In practice, using for example the wide-
coverage grammar from CCGbank (Hockenmaier
and Steedman, 2007), there will be many types
with more than 3 slashes, with corresponding
higher-order tensors. For example, a com-
mon category for a preposition is the follow-
ing: ((S\NP)\(S\NP))/NP , which would be
assigned to WITH in eat WITH a fork. (The way
to read the syntactic type is as follows: with re-
quires an NP argument to the right ? a fork in
this example ? and then a verb phrase to the
left ? eat with type S\NP ? resulting in a verb
phrase S\NP .) The corresponding meaning ten-
sor lives in the tensor space S?N?S?N?N,
i.e. a 5th-order tensor. Categories with even
more slashes are not uncommon, for example
((N /N )/(N /N ))/((N /N )/(N /N )). Clearly
learning parameters for such tensors is highly
challenging, and it is likely that lower dimensional
approximations will be required.
3 Methods
In this paper we compare five different methods
for modelling the type-driven semantic represen-
tation of subject-verb-object sentences. The ten-
sor is a function that encodes the meaning of a
verb. It takes two vectors from the K-dimensional
noun space as input, and produces a representa-
tion of the sentence in the S-dimensional sentence
space. In this paper, we consider a plausibility
space where S is either a single variable or a two-
dimensional space over two classes: plausible (>)
and implausible (?).
The first method (Tensor) follows Krishna-
murthy and Mitchell (2013) by learning a tensor as
parameters in a softmax classifier. We introduce
three related methods (2Mat, SKMat, KKMat),
all of which model the verb as a matrix or a pair of
matrices (Figure 2). Table 1 gives the number of
Tensor 2Mat SKMat KKMat DMat
V 2K
2
4K 2K K
2
K
2
? 4 8 4 0 0
Table 1: Number of parameters per method.
parameters for each method. Tensor, 2Mat, and
SKMat all have a two-dimensional S space, while
KKMat produces a scalar value. In all of these
learning-based methods the derivatives were ob-
tained via the chain rule with respect to each set
of parameters and gradient descent was performed
using the Adagrad algorithm (Duchi et al., 2011).
We also reimplement a distributional method
(DMat), which was previously used in SVO
experiments with the type-driven framework
(Grefenstette and Sadrzadeh, 2011). While the
other methods are trained as plausibility classi-
fiers, in DMat we estimate the class boundary
from cosine similarity via training data (see expla-
nation below).
Tensor If subject (n
s
) and object (n
o
) nouns are
K-dimensional vectors and the plausibility vec-
tor is S-dimensional with S = 2, we can learn
the values of the K ? K ? S tensor represent-
ing the verb as parameters (V) of a regression al-
gorithm. To represent this space as a distribution
over two classes (>,?) we apply a sigmoid func-
tion (?) to restrict the output to the [0,1] range and
the softmax activation function (g) to balance the
class probabilities. The full parameter set which
needs to be optimised for is B = {V,?}, where
? = {?
>
, ?
?
} are the softmax parameters for
the two classes. For each verb we optimise the
KL-divergence L between the training labels t
i
and classifier predictions using the following reg-
ularised objective:
O(B) =
N?
i=1
L
(
t
i
, g
(
?
(
h
V
(
n
i
s
, n
i
o
))
,?
))
+
?
2
||B||
2
(1)
where n
i
s
and n
i
o
are the subject and object of
the training instance i ? N , and h
V
(
n
i
s
, n
i
o
)
=
(n
i
s
)V(n
i
o
)
T
describes tensor contraction. The
function h
V
is described diagrammatically in Fig-
ure 2-(a), where the verb tensor parameters are
drawn as a cube with the subject and object noun
vectors as operands on either side. The output
is a two-dimensional vector which is then trans-
formed using the sigmoid and softmax functions.
1038
K

S
K
KK
S
(a) K

K S
K
S
K
2*S
x
S
S
x
(b)
K

K
S
K
K
00
0 0
00 K S
S
x
x
(c) K
 K
K
K
x
x
(d)
Figure 2: Illustrations of the h
V
function for the regression-based methods (a)-Tensor, (b)-2Mat, (c)-
SKMat, (d)-KKMat. The operation in (a) is tensor contraction, T denotes transpose, and ? denotes
matrix multiplication.
The gold-standard distribution over training labels
is defined as (1, 0) or (0, 1), depending on whether
the training instance is a positive (plausible) or
negative (implausible) example. Tensor contrac-
tion is implemented using the Matlab Tensor Tool-
box (Bader et al., 2012).
2Mat An alternative approach is to decouple
the interaction between the object and subject by
learning a pair of S ? K (S = 2) matrices (V
s
,
V
o
) for each of the input noun vectors (one ma-
trix for the subject slot of the verb and one for the
object slot). The resulting S-vectors are concate-
nated, after the subject and object nouns have been
combined with their matrices, and combined with
the softmax component to produce the output dis-
tribution. Therefore the objective function is the
same as in Equation 1, but h
V
is defined as:
h
V
(
n
i
s
, n
i
o
)
=
(
(n
i
s
)V
T
s
)
||
(
V
o
(n
i
o
)
T
)
T
where || represents vector concatenation. The in-
tention is to test whether we can learn the verb
without directly multiplying subject and object
features, n
i
s
and n
j
o
. The function h
V
is shown in
Figure 2-(b), where the verb tensor parameters are
drawn as two 2?K matrices, one of which inter-
acts with the subject and the other with the object
noun vector. The output is a four-dimensional vec-
tor whose values are then restricted to [0,1] using
the sigmoid function and then transformed into a
two-dimensional distribution over the classes us-
ing the softmax function.
SKMat A third option for generating a sentence
vector with S = 2 dimensions is to consider the
verb as an S ?K matrix. If we transform the ob-
ject vector into a K ?K matrix with the noun on
the diagonal and zeroes elsewhere, we can com-
bine the verb and object to produce a new S ?K
matrix, which is encoding the meaning of the verb
phrase. We can then complete the sentence re-
duction by multiplying the subject vector with this
verb phrase vector to produce an S-dimensional
sentence vector. Formally, we define SKMat as:
h
V
(
n
i
s
, n
i
o
)
= n
i
s
(
Vdiag(n
i
o
)
)
T
and use it in Equation 1. The function h
V
is
described in Figure 2-(c), where the verb ten-
sor parameters are drawn as a matrix, the sub-
ject as a vector, and the object as a diagonal ma-
1039
trix. The graphic demonstrates the two-step com-
bination and the intermediate S ? K verb phrase
matrix, as well as the the noun vector product
that results in a two-dimensional vector which is
then transformed using the sigmoid and softmax
functions. Whilst the tensor method captures the
interactions between all pairs of context features
(n
s
i
? n
o
j
), SKMat only captures the interactions
between matching features (n
s
i
? n
o
i
).
KKMat Given a two-class problem, such as
plausibility classification, the softmax implemen-
tation is overparameterised because the class
membership can be estimated with a single vari-
able. To produce a scalar output, we can learn the
parameters for a single K ? K matrix (V) using
standard logistic regression with the mean squared
error cost function:
O(V) = ?
1
m
[
N?
i=1
t
i
log h
V
(
n
i
s
, n
i
o
)
+ (1? t
i
) log h
V
(
n
i
s
, n
i
o
)
]
where h
V
(
n
i
s
, n
i
o
)
= (n
i
s
)V(n
i
o
)
T
and the objec-
tive is regularised: O(V) +
?
2
||V||
2
. This function
is shown in Figure 2-(d), where the verb parame-
ters are shown as a matrix, while the subject and
object are vectors. The output is a single scalar,
which is then transformed with the sigmoid func-
tion. Values over 0.5 are considered plausible.
DMat The final method produces a scalar as in
KKMat, but is distributional and based on corpus
counts rather than regression-based. Grefenstette
and Sadrzadeh (2011) introduced a corpus-based
approach for generating a K ?K matrix for each
verb from an average of Kronecker products of the
subject and object vectors from the positively la-
belled subset of the training data. The intuition is
that, for example, the matrix for eat may have a
high value for the contextual topic pair describing
animate subjects and edible objects. To determine
the plausibility of a new subject-object pair for a
particular verb, we calculate the Kronecker prod-
uct of the subject and object noun vectors for this
pair, and compare the resulting matrix with the av-
erage verb matrix using cosine similarity.
For label prediction, we calculate the similar-
ity between each of the training data pairs and the
learned average matrix. Unlike for KKmat, the
class cutoff is estimated at the break-even point
of the receiver operator characteristic (ROC) gen-
erated by comparing the training labels with this
cosine similarity value. The break-even point is
when the true positive rate is equal to the false pos-
itive rate. In practice it would be more accurate
to estimate the cutoff on a validation dataset, but
some of the verbs have so few training instances
that this was not possible.
4 Experiments
In order to examine the quality of learning we run
several experiments where we compare the differ-
ent methods. In these experiments we consider
the DMat method as the baseline. Some of the
experiments employ cross-validation, in particular
five repetitions of 2-fold cross validation (5x2cv),
which has been shown to be statistically more ro-
bust than the traditional 10-fold cross validation
(Alpaydin, 1999; Ulas? et al., 2012). The results of
5x2cv experiments can be compared using the reg-
ular paired t-test, but the specially designed 5x2cv
F-test has been proven to produce fewer statistical
errors (Ulas? et al., 2012).
The performance was evaluated using the area
under the ROC (AUC) and the F
1
measure (based
on precision and recall over the plausible class).
The AUC evaluates whether a method is ranking
positive examples above negative ones, regardless
of the class cutoff value. F
1
shows how accurately
a method assigns the correct class label. Another
way to interpret the results is to consider the AUC
as the measure of the quality of the parameters in
the verb matrix or tensor, while the F-score indi-
cates how well the softmax, the sigmoid, and the
DMat cutoff algorithm are estimating class partic-
ipation.
Ex-1. In the first experiment, we compare the
different transitive verb representations by running
5x2cv experiments on ten verbs chosen to cover a
range of concreteness and frequency values (Sec-
tion 4.2).
Ex-2. In the initial experiments we found that
some models had low performance, so we applied
the column normalisation technique, which is of-
ten used with regression learning to standardise
the numerical range of features:
~x :=
~x?min(~x)
max(~x)?min(~x)
(2)
This preserves the relative values of features be-
tween training samples, while moving the values
to the [0,1] range.
1040
Ex-3. There are varying numbers of training ex-
amples for each of the verbs, so we repeated the
5x2cv with datasets of 52 training points for each
verb, since this is the size of the smallest dataset of
the verb CENSOR. The points were randomly sam-
pled from the datasets used in the first experiment.
Finally, the four verbs with the largest datasets
were used to examine how the performance of the
methods changes as the amount of training data
increases. The 4,000 training samples were ran-
domised and half were used for testing. We sam-
pled between 10 and 1000 training triples from the
other half (Figure 4).
4.1 Noun vectors
Distributional semantic models (Turney and Pan-
tel, 2010) encode word meaning in a vector for-
mat by counting co-occurrences with other words
within a specified context window. We con-
structed the vectors from the October 2013 dump
of Wikipedia articles, which was tokenised us-
ing the Stanford NLP tools
3
, lemmatised with the
Morpha lemmatiser (Minnen et al., 2001), and
parsed with the C&C parser (Clark and Curran,
2007). In this paper we use sentence boundaries to
define context windows and the top 10,000 most
frequent lemmatised words in the whole corpus
(excluding stopwords) as context words. The raw
co-occurrence counts are re-weighted using the
standard tTest weighting scheme (Curran, 2004),
where f
w
i
c
j
is the number of times target noun w
i
occurs with context word c
j
:
tTest( ~w
i
, c
j
) =
p(w
i
, c
j
)? p(w
i
)p(c
j
)
?
p(w
i
)p(c
j
)
(3)
where p(w
i
) =
?
j
f
w
i
c
j?
k
?
l
f
w
k
c
l
, p(c
j
) =
?
i
f
w
i
c
j?
k
?
l
f
w
k
c
l
,
and p(w
i
, c
j
) =
f
w
i
c
j?
k
?
l
f
w
k
c
l
.
Using all 10,000 context words would result in
a large number of parameters for each verb ten-
sor, and so we apply singular value decomposition
(SVD) (Turney and Pantel, 2010) with 40 latent
dimensions to the target-context word matrix. We
use context selection (with N = 140) and row
normalisation as described in Polajnar and Clark
(2014) to markedly improve the performance of
SVD on smaller dimensions (K) and enable us to
train the verb tensors using very low-dimensional
3
http://nlp.stanford.edu/software/index.shtml
Verb Concreteness # of Positive Frequency
APPLY 2.5 5618 47361762
CENSOR 3 26 278525
COMB 5 164 644447
DEPOSE 2.5 118 874463
EAT 4.44 5067 26396728
IDEALIZE 1.17 99 485580
INCUBATE 3.5 82 833621
JUSTIFY 1.45 5636 10517616
REDUCE 2 26917 40336784
WIPE 4 1090 6348595
Table 2: The 10 chosen verbs together with their
concreteness scores. The number of positive SVO
examples was capped at 2000. Frequency is the
frequency of the verb in the GSN corpus.
noun vectors. Performance of the noun vectors
was measured on standard word similarity datasets
and the results were comparable to those reported
by Polajnar and Clark (2014).
4.2 Training data
In order to generate training data we made use
of two large corpora: the Google Syntactic N-
grams (GSN) (Goldberg and Orwant, 2013) and
the Wikipedia October 2013 dump. We first chose
ten transitive verbs with different concreteness
scores (Brysbaert et al., 2013) and frequencies, in
order to obtain a variety of verb types. Then the
positive (plausible) SVO examples were extracted
from the GSN corpus. More precisely, we col-
lected all distinct syntactic trigrams of the form
nsubj ROOT dobj, where the root of the phrase was
one of our target verbs. We lemmatised the words
using the NLTK
4
lemmatiser and filtered these ex-
amples to retain only the ones that contain nouns
that also occur in Wikipedia, obtaining the counts
reported in Table 2.
For every positive training example, we con-
structed a negative (implausible) one by replac-
ing both the subject and the object with a con-
founder, using a standard technique from the se-
lectional preference literature (Chambers and Ju-
rafsky, 2010). A confounder was generated by
choosing a random noun from the same frequency
bucket as the original noun.
5
Frequency buckets
of size 10 were constructed by collecting noun fre-
quency counts from the Wikipedia corpus. For ex-
4
http://nltk.org/
5
Note that the random selection of the confounder could
result in a plausible negative example by chance, but man-
ual inspection of a subset of the data suggests this happens
infrequently for those verbs which select strongly for their
arguments, but more often for those verbs that don?t.
1041
Verb Tensor DMat KKMat SKMat 2Mat
AUC
APPLY 85.68? 81.46? 88.88?? 68.02 88.92??
CENSOR 79.40 85.54 80.55 78.52 83.19
COMB 89.41 85.65 88.38 69.20?? 89.56
DEPOSE 92.70 94.44 93.12 84.47? 93.20
EAT 94.62 93.81 95.17 67.92 95.88?
IDEALIZE 69.56 75.84 72.46 61.19 70.23
INCUBATE 89.33 85.53 88.61 70.59 91.40
JUSTIFY 85.27? 88.70? 89.97? 73.56 90.10?
REDUCE 96.13 95.48 96.69? 79.32 97.21
WIPE 85.19 84.47 87.84? 64.93?? 81.29
MEAN 86.93 87.29 88.37 71.96 88.30
Tensor DMat KKMat SKMat 2Mat
F
1
79.27 64.00 81.24? 54.06 80.80?
70.66 47.93 73.52 37.86 71.07
81.15 45.02 81.38 39.67 82.36
84.60 54.77 84.79 43.79 86.15
88.91 52.45 88.83 56.22 89.95
66.53 48.28 68.39 31.03 67.43
80.30 50.84 80.90 31.99 84.55
79.73 73.71 81.10 54.09 82.52
91.24 71.24? 87.46 76.67? 92.22
78.57 47.62 80.65 39.50 78.90
80.30 55.79 81.03 46.69 81.79
Table 3: The best AUC and F
1
results for all the verbs, where ? denotes statistical significance compared
to DMat and ? denotes significance compared to Tensor according to the 5x2cv F-test with p < 0.05.
ample, for the plausible triple animal EAT plant,
we generate the implausible triple mountain EAT
product. Some verbs were well represented in the
corpus, so we used up to the top 2,000 most fre-
quent triples for training.
0
0.5
1
AUC
 
 
APPL
Y
CENS
OR COMBDEPO
SE EAT
IDEAL
IZE
INCU
BATEJUST
IFY
REDU
CE WIPE
TensorTensor*SKMatSKMat*
?0.2
0
0.2
0.4
0.6
0.8
1
1.2
F?S
core
 
 
APPLYCENS
OR COMBDEPO
SE EAT
IDEAL
IZE
INCUB
ATEJUSTI
FY
REDU
CE WIPE
TensorTensor*SKMatSKMat*
Figure 3: The effect of column normalisation (*)
on Tensor and SKMat. Top table shows AUC and
the bottom F
1
-score, while the error bars indicate
standard deviation.
5 Results
The results from Ex-1 are summarised in Ta-
ble 3. We can see that linear regression can lead
to models that are able to distinguish between
plausible and implausible SVO triples. The Ten-
sor method outperforms DMat, which was pre-
viously shown to produce reasonable verb repre-
sentations in related experiments (Grefenstette and
Sadrzadeh, 2011). 2Mat and KKMat, in turn,
outperform Tensor demonstrating that it is pos-
sible to learn lower dimensional approximations
of the tensor-based framework. 2Mat is an appro-
priate approximation for functions with two inputs
and a sentence space of any dimensionality, while
KKMat is only appropriate for a single valued
sentence space, such as the plausibility or senti-
ment space. Due to method variance and dataset
size there are very few AUC results that are sig-
nificantly better than DMat and even fewer that
outperform Tensor. All methods perform poorly
on the verb IDEALIZE, probably because it has
the lowest concreteness value and is in one of the
smallest datasets. This verb is also particularly dif-
ficult because it does not select strongly for either
its subject or object, and so some of the pseudo-
negative examples are in fact somewhat plausible
(e.g. town IDEALIZE authority or child IDEALIZE
racehorse). In general, this would indicate that
more concrete verbs are easier to learn, as they
have a clearer pattern of preferred property types,
but there is no distinct correlation.
The results of the normalisation experiments
(Ex-2) are shown in Table 4. We can see that the
SKMat method, which performed poorly in Ex-
1 notably improves with normalisation. Tensor
AUC scores also improve through normalisation,
but the F-scores decrease. The rest of the methods,
and in particular DMat are negatively affected by
column normalisation. The results from Ex-1 and
Ex-2 for SKMat and Tensor are summarised in
1042
Verb Tensor DMat KKMat SKMat 2Mat
AUC
APPLY 86.16? 48.63? 82.63?? 85.73? 85.65?
CENSOR 75.74 71.20 78.00 82.77 78.64
COMB 91.67? 62.42? 90.85? 89.79? 91.42?
DEPOSE 93.96? 54.93? 93.56? 93.87? 93.81?
EAT 95.64? 47.68? 92.92? 94.99?? 94.76?
IDEALIZE 69.64 55.98 72.20?? 76.71?? 71.85?
INCUBATE 90.97? 61.31? 89.69? 90.19? 90.05?
JUSTIFY 89.76? 54.87? 87.26?? 89.64? 89.05?
REDUCE 96.63? 59.58? 94.99?? 96.14? 96.53?
WIPE 86.82? 58.02? 84.18? 83.65? 86.02?
MEAN 87.90 57.66 86.83 88.55 87.98
Tensor DMat KKMat SKMat 2Mat
F
1
45.57 46.99 46.17 60.86 76.60?
30.43 55.16 65.19 49.59 44.22
33.37 61.05 71.20 64.56 75.96
42.73 39.71 73.07 54.51 56.54
60.42 47.42 58.80 69.05 87.44?
39.14 49.16 41.75 31.57 50.59
46.35 53.33 70.45 41.57 63.61
47.38 51.40 41.91 63.96 80.55?
51.63 54.27 69.18 69.76 90.77?
44.04 55.19 47.84 49.89 75.80
44.31 51.57 58.76 55.73 70.41
Table 4: The best AUC and F
1
results for all the verbs with normalised vectors, where ? denotes statistical
significance compared to DMat and ? denotes significance compared to Tensor according to the 5x2cv
F-test with p < 0.05.
Figure 3. This figure also shows that AUC values
have much lower variance, but that high variance
in F-score leads to results that are not statistically
significant.
When considering the size of the datasets (Ex-
3), it would seem from Table 5 that 2Mat is able to
learn from less data than DMat or Tensor. While
this may be true over a 5x2cv experiment on small
data, Figure 4 shows that this view may be overly
simplistic and that different training examples can
influence learning. Analysis of errors shows that
the baseline method mostly generates false nega-
tive errors (i.e. predicting implausible when the
gold standard label is plausible). In contrast, Ten-
sor produces almost equal numbers of false posi-
tives and false negatives, but sometimes produces
false negatives with low frequency nouns (e.g.
bourgeoisie IDEALIZE work), presumably because
there is not enough information in the noun vec-
tor to decide on the correct class. It also produces
some false positive errors when either of the nouns
is plausible (but the triple is implausible), which
would suggest results may be improved by train-
ing with data where only one noun is confounded
or by treating negative data as possibly positive
(Lee and Liu, 2003).
6 Discussion
Current methods which derive distributed repre-
sentations for phrases, for example the work of
Socher et al. (2012), typically use only matrix rep-
resentations, and also assume that words, phrases
and sentences all live in the same vector space.
The tensor-based semantic framework is more
flexible, in that it allows different spaces for dif-
ferent grammatical types, which results from it be-
Verb Tensor DMat 2Mat
APPLY 95.76 86.50 86.31
CENSOR 82.97 84.09 77.79
COMB 90.13 92.93 95.18
DEPOSE 92.41 91.27 95.61
EAT 99.64 98.25 99.58
IDEALIZE 75.03 76.68 88.98
INCUBATE 91.10 87.20 96.42
JUSTIFY 88.96 88.99 87.31
REDUCE 100.0 99.87 99.46
WIPE 97.20 91.63 96.36
MEAN 91.52 89.94 92.50
Table 5: Results show average of 5x2cv AUC on
small data (26 positive + 26 negative per verb).
None of the results are significant.
ing tied more closely to a type-driven syntactic de-
scription; however, this flexibility comes at a cost,
since there are many more paramaters to learn.
Various communities are beginning to recog-
nize the additional power that tensor representa-
tions can provide, through the capturing of interac-
tions that are difficult to represent with vectors and
matrices (see e.g. (Ranzato et al., 2010; Sutskever
et al., 2009; Van de Cruys et al., 2012)). Hierar-
chical recursive structures in language potentially
represent a large number of such interactions ? the
obvious example for this paper being the interac-
tion between a transitive verb?s subject and object
? and present a significant challenge for machine
learning.
This paper is a practical extension of the work
in Krishnamurthy and Mitchell (2013), which in-
troduced learning of CCG-based function tensors
with logistic regression on a compositional se-
mantics task, but was implemented as a proof-of-
concept with vectors of length 2 and on small,
manually created datasets based on propositional
1043
10 20 40 80 150 300 600 800 1000 2000
0.650.7
0.750.8
0.850.9
0.951
# Training Examples
AUC
apply
 
 
DMatTensor2Mat
10 20 40 80 150 300 600 800 1000 20000.7
0.75
0.8
0.85
0.9
0.95
1
# Training Examples
AUC
eat
 
 
DMatTensor2Mat
10 20 40 80 150 300 600 800 1000 2000
0.650.7
0.750.8
0.850.9
0.951
# Training Examples
AUC
justify
 
 
DMatTensor2Mat
10 20 40 80 150 300 600 800 1000 20000.8
0.85
0.9
0.95
1
# Training Examples
AUC
reduce
 
 
DMatTensor2Mat
Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances in-
creases.
logic examples. Here, we go beyond this by learn-
ing tensors using corpus data and by deriving sev-
eral different matrix representations for the verb in
the subject-verb-object (SVO) sentence.
This work can also be thought of as applying
neural network learning techniques to the clas-
sic problem of selectional preference acquisition,
since the design of the pseudo-disambiguation ex-
periments is taken from the literature on selec-
tional preferences (Clark and Weir, 2002; Cham-
bers and Jurafsky, 2010). We do not compare di-
rectly with methods from this literature, e.g. those
based on WordNet (Resnik, 1996; Clark and Weir,
2002) or topic modelling techniques (Seaghdha,
2010), since our goal in this paper is not to ex-
tend the state-of-the-art in that area, but rather to
use selectional preference acquisition as a test bed
for the tensor-based semantic framework.
7 Conclusion
In this paper we introduced three dimensionally
reduced representations of the transitive verb ten-
sor defined in the type-driven framework for com-
positional distributional semantics (Coecke et al.,
2010). In a comprehensive experiment on ten dif-
ferent verbs we find no significant difference be-
tween the full tensor representation and the re-
duced representations. The SKMat and 2Mat rep-
resentations have the lowest number of parame-
ters and offer a promising avenue of research for
more complex sentence structures and sentence
spaces. KKMat and DMat also had high scores
on some verbs, but these representations are appli-
cable only in spaces where a single-value output is
appropriate.
In experiments where we varied the amount of
training data, we found that in general more con-
crete verbs can learn from less data. Low con-
creteness verbs require particular care with dataset
design, since some of the seemingly random ex-
amples can be plausible. This problem may be
circumvented by using semi-supervised learning
techniques.
We also found that simple numerical tech-
niques, such as column normalisation, can
markedly alter the values and quality of learning.
On our data, column normalisation has a side-
effect of removing the negative values that were
introduced by the use of tTest weighting measure.
The use of the PPMI weighting scheme and non-
negative matrix factorisation (NMF) (Grefenstette
et al., 2013; Van de Cruys, 2010) could lead to a
similar effect, and should be investigated. Further
numerical techniques for improving the estimation
of the class decision boundary, and consequently
the F-score, will also constitute future work.
1044
References
Ethem Alpaydin. 1999. Combined 5x2 CV F-test
for comparing supervised classification learning al-
gorithms. Neural Computation, 11(8):1885?1892,
November.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), pages 1183?1193,
Cambridge, MA.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technology, 9:5?110.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known English word lemmas. Be-
havior research methods, pages 1?8.
Nathanael Chambers and Dan Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of ACL 2010,
Uppsala, Sweden.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Stephen Clark. 2013. Type-driven syntax and seman-
tics for composing meaning vectors. In Chris He-
unen, Mehrnoosh Sadrzadeh, and Edward Grefen-
stette, editors, Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse, pages
359?377. Oxford University Press.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. In J. van
Bentham, M. Moortgat, and W. Buszkowski, edi-
tors, Linguistic Analysis (Lambek Festschrift), vol-
ume 36, pages 345?384.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
David R. Dowty, Robert E. Wall, and Stanley Peters.
1981. Introduction to Montague Semantics. Dor-
drecht.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of English books. In Second Joint Conference on
Lexical and Computational Semantics, pages 241?
247, Atlanta,Georgia.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404,
Edinburgh, Scotland, UK, July.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Jayant Krishnamurthy and Tom M Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Wee Sun Lee and Bing Liu. 2003. Learning with posi-
tive and unlabeled examples using weighted logistic
regression. In Proceedings of the Twentieth Interna-
tional Conference on Machine Learning (ICML).
Jean Maillard, Stephen Clark, and Edward Grefen-
stette. 2014. A type-driven tensor-based semantics
for CCG. In Proceedings of the EACL 2014 Type
Theory and Natural Language Semantics Workshop
(TTNLS), Gothenburg, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236?244, Columbus, OH.
Tamara Polajnar and Stephen Clark. 2014. Improving
distributional semantic vectors through context se-
lection and normalisation. In 14th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL?14, Gothenburg, Sweden.
1045
M. Ranzato, A. Krizhevsky, and G. E. Hinton. 2010.
Factored 3-way restricted boltzmann machines for
modeling natural images. In Proceedings of the
Thirteenth International Conference on Artificial In-
telligence and Statistics (AISTATS), Sardinia, Italy.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Diarmuid O Seaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of ACL
2010, Uppsala, Sweden.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201?
1211, Jeju, Korea.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2013), Seat-
tle, USA.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum.
2009. Modelling relational data using bayesian clus-
tered tensor factorization. In Proceedings of Ad-
vances in Neural Information Processing Systems
(NIPS 2009), Vancouver, Canada.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Ayd?n Ulas?, Olcay Taner Y?ld?z, and Ethem Alpayd?n.
2012. Cost-conscious comparison of supervised
learning algorithms over multiple data sets. Pattern
Recognition, 45(4):1772?1781, April.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and
Anna Korhonen. 2012. Multi-way tensor factor-
ization for unsupervised lexical acquisition. In Pro-
ceedings of COLING 2012, Mumbai, India.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induc-
tion. Journal of Natural Language Engineering,
16(4):417?437.
1046
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230?238,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Distributional Semantic Vectors through Context Selection and
Normalisation
Tamara Polajnar
University of Cambridge
Computer Laboratory
tp366@cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
Distributional semantic models (DSMs)
have been effective at representing seman-
tics at the word level, and research has re-
cently moved on to building distributional
representations for larger segments of text.
In this paper, we introduce novel ways of
applying context selection and normalisa-
tion to vary model sparsity and the range
of values of the DSM vectors. We show
how these methods enhance the quality of
the vectors and thus result in improved
low dimensional and composed represen-
tations. We demonstrate these effects on
standard word and phrase datasets, and on
a new definition retrieval task and dataset.
1 Introduction
Distributional semantic models (DSMs) (Turney
and Pantel, 2010; Clarke, 2012) encode word
meaning by counting co-occurrences with other
words within a context window and recording
these counts in a vector. Various IR and NLP
tasks, such as word sense disambiguation, query
expansion, and paraphrasing, take advantage of
DSMs at a word level. More recently, researchers
have been exploring methods that combine word
vectors to represent phrases (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010) and sentences
(Coecke et al., 2010; Socher et al., 2012). In this
paper, we introduce two techniques that improve
the quality of word vectors and can be easily tuned
to adapt the vectors to particular lexical and com-
positional tasks.
The quality of the word vectors is generally as-
sessed on standard datasets that consist of a list of
word pairs and a corresponding list of gold stan-
dard scores. These scores are gathered through an
annotation task and reflect the similarity between
the words as perceived by human judges (Bruni et
al., 2012). Evaluation is conducted by comparing
the word similarity predicted by the model with
the gold standard using a correlation test such as
Spearman?s ?.
While words, and perhaps some frequent
shorter phrases, can be represented by distri-
butional vectors learned through co-occurrence
statistics, infrequent phrases and novel construc-
tions are impossible to represent in that way. The
goal of compositional DSMs is to find methods of
combining word vectors, or perhaps higher-order
tensors, into a single vector that represents the
meaning of the whole segment of text. Elemen-
tary approaches to composition employ simple op-
erations, such as addition and elementwise prod-
uct, directly on the word vectors. These have been
shown to be effective for phrase similarity evalua-
tion (Mitchell and Lapata, 2010) and detection of
anomalous phrases (Kochmar and Briscoe, 2013).
The methods that will be introduced in this pa-
per can be applied to co-occurrence vectors to pro-
duce improvements on word similarity and com-
positional tasks with simple operators. We chose
to examine the use of sum, elementwise prod-
uct, and circular convolution (Jones and Mewhort,
2007), because they are often used due to their
simplicity, or as components of more complex
models (Zanzotto and Dell?Arciprete, 2011).
The first method is context selection (CS), in
which the top N highest weighted context words
per vector are selected, and the rest of the values
are discarded (by setting to zero). This technique
is similar to the way that Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) se-
lects the number of topics that represent a word,
and the word filtering approach in Gamallo and
Bordag (2011). It has the advantage of improv-
ing word representations and vector sum represen-
tations (for compositional tasks) while using vec-
tors with fewer non-zero elements. Programming
languages often have efficient strategies for stor-
230
ing these sparse vectors, leading to lower memory
usage. As an example of the resulting accuracy
improvements, when vectors with up to 10,000
non-zero elements are reduced to a maximum of
N  240 non-zero elements, the Spearman ? im-
proves from 0.61 to 0.76 on a standard word sim-
ilarity task. We also see an improvement when
used in conjunction with further, standard dimen-
sionality reduction techniques: the CS sparse vec-
tors lead to reduced-dimensional representations
that produce higher correlations with human simi-
larity judgements than the original full vectors.
The second method is a weighted l
2
-
normalisation of the vectors prior to application of
singular value decomposition (SVD) (Deerwester
et al., 1990) or compositional vector operators. It
has the effect of drastically improving SVD with
100 or fewer dimensions. For example, we find
that applying normalisation before SVD improves
correlation from ?  0.48 to ?  0.70 for 20
dimensions, on the word similarity task. This
is an essential finding as many more complex
models of compositional semantics (Coecke et al.,
2010; Baroni and Zamparelli, 2010; Andreas and
Ghahramani, 2013) work with tensor objects and
require good quality low-dimensional represen-
tations of words in order to lower computational
costs. This technique also improves the perfor-
mance of vector addition on texts of any length
and vector elementwise product on shorter texts,
on both the similarity and definitions tasks.
The definition task and dataset are an additional
contribution. We produced a new dataset of words
and their definitions, which is separated into nine
parts, each consisting of definitions of a particular
length. This allows us to examine how composi-
tional operators interact with CS and normalisa-
tion as the number of vector operations increases.
This paper is divided into three main sections.
Section 2 describes the construction of the word
vectors that underlie all of our experiments and the
two methods for adaptation of the vectors to spe-
cific tasks. In Section 3 we assess the effects of
CS and normalisation on standard word similar-
ity datasets. In Section 4 we present the compo-
sitional experiments on phrase data and our new
definitions dataset.
2 Word Vector Construction
The distributional hypothesis assumes that words
that occur within similar contexts share similar
meanings; hence semantic vector construction first
requires a defintition of context. Here we use
a window method, where the context is defined
as a particular sequence of words either side of
the target word. The vectors are then populated
through traversal of a large corpus, by recording
the number of times each of the target words co-
occurs with a context word within the window,
which gives the raw target-context co-occurrence
frequency vectors (Freq).
The rest of this section contains a description
of the particular settings used to construct the raw
word vectors and the weighting schemes (tTest,
PPMI) that we considered in our experiments.
This is followed by a detailed description of the
context selection (CS) and normalisation tech-
niques. Finally, dimensionality reduction (SVD) is
proposed as a way of combating sparsity and ran-
dom indexing (RI) as an essential step of encoding
vectors for use with the convolution operator.
Raw Vectors We used a cleaned-up corpus
of 1.7 billion lemmatised tokens (Minnen et
al., 2001) from the October, 2013 snapshot of
Wikipedia, and constructed context vectors by us-
ing sentence boundaries to provide the window.
The set of context wordsC consisted of the 10,000
most frequent words occurring in this dataset, with
the exception of stopwords from a standard stop-
word list. Therefore, a frequency vector for a tar-
get word w
i
PW is represented as ~w
i
 tf
w
i
c
j
u
j
,
where c
j
P C (|C|  10, 000), W is a set of target
words in a particular evaluation dataset, and f
w
i
c
j
is the co-occurrence frequency between the target
word, w
i
and context word, c
j
.
Vector Weighting We used the tTest and PPMI
weighting schemes, since they both performed
well on the development data. The vectors result-
ing from the application of the weighting schemes
are as follows, where the tTest and PPMI functions
give weighted values for the basis vector corre-
sponding to context word c
j
for target word w
i
:
tTestp ~w
i
, c
j
q 
ppw
i
, c
j
q  ppw
i
qppc
j
q
a
ppw
i
qppc
j
q
(1)
PPMIp ~w
i
, c
j
q  ppw
i
, c
j
q log

ppw
i
, c
j
q
ppw
i
qppc
j
q


(2)
where ppw
i
q 
?
j
f
w
i
c
j
?
k
?
l
f
w
k
c
l
, ppc
j
q 
?
i
f
w
i
c
j
?
k
?
l
f
w
k
c
l
, and
ppw
i
, c
j
q 
f
w
i
c
j
?
k
?
l
f
w
k
c
l
.
231
Original Normalised Normalised*10?1?0.5
00.5
11.5
22.5
33.5
Figure 1: The range of context weights on tTest
weighted vectors before and after normalisation.
Context Ranking and Selection The weight-
ing schemes change the importance of individ-
ual target-context raw co-occurrence counts by
considering the frequency with which each con-
text word occurs with other target words. This
is similar to term-weighting in IR and many re-
trieval functions are also used as weighting func-
tions in DSMs. In the retrieval-based model ESA
(Gabrilovich and Markovitch, 2007), only the N
highest-weighted contexts are kept as a represen-
tative set of ?topics? for a particular target word,
and the rest are set to zero. Here we use a sim-
ilar technique and, for each target word, retain
only the N -highest weighted context words, using
a word-similarity development set to choose the
N that maximises correlation across all words in
that dataset. Throughout the paper, we will refer
to this technique as context selection (CS) and use
N to indicate the maximum number of contexts
per word. Hence all word vectors have at most N
non-zero elements, effectively adjusting the spar-
sity of the vectors, which may have an effect on
the sum and elementwise product operations when
composing vectors.
Normalisation PPMI has only positive values
that span the range r0,8s, while tTest spans
r1, 1s, but generally produces values tightly con-
centrated around zero. We found that these ranges
can produce poor performance due to numerical
problems, so we corrected this through weighted
row normalisation: ~w : ?
~w
||~w||
2
. With ?  10 this
has the effect of restricting the values to r10, 10s
for tTest and r0, 10s for PPMI. Figure 1 shows the
range of values for tTest. In general we use ?  1,
but for some experiments we use ?  10 to push
the highest weights above 1, as a way of combat-
ing the numerical errors that are likely to arise due
to repeated multiplications of small numbers. This
normalisation has no effect on the ordering of con-
text weights or cosine similarity calculations be-
tween single-word vectors. We apply normalisa-
tion prior to dimensionality reduction and RI.
SVD SVD transforms vectors from their target-
context representation into a target-topic space.
The resulting space is dense, in that the vectors
no longer contain any zero elements. If M is a
|w|  |C| matrix whose rows are made of word
vectors ~w
i
, then the lower dimensional representa-
tion of those vectors is encoded in the |W |  K
matrix
?
M
K
 U
K
S
K
where SVDpM,Kq 
U
K
S
K
V
K
(Deerwester et al., 1990). We also
tried non-negative matrix factorisation (NNMF)
(Seung and Lee, 2001), but found that it did not
perform as well as SVD. We used the standard
Matlab implementation of SVD.
Random Indexing There are two ways of creat-
ing RI-based DSMs, the most popular being to ini-
tialise all target word vectors to zero and to gener-
ate a random vector for each context word. Then,
while traversing through the corpus, each time a
target word and a context word co-occur, the con-
text word vector is added to the vector represent-
ing the target word. This method allows the RI
vectors to be created in one step through a single
traversal of the corpus. The other method, follow-
ing Jones and Mewhort (2007), is to create the RI
vectors through matrix multiplication rather than
sequentially. We employ this method and assign
each context word a random vector ~e
c
j
 tr
k
u
k
where r
k
are drawn from the normal distribution
N p0,
1
D
q and | ~e
c
j
|  D  4096. The RI repre-
sentation of a target word RIp ~w
i
q  ~w
i
R is con-
structed by multiplying the word vector ~w
i
, ob-
tained as before, by the |C|  D matrix R where
each column represents the vectors ~e
c
j
. Weighting
is performed prior to random indexing.
3 Word Similarity Experiments
In this section we investigate the effects of context
selection and normalisation on the quality of word
vectors using standard word similarity datasets.
The datasets consist of word pairs and a gold stan-
dard score that indicates the human judgement of
the similarity between the words within each pair.
We calculated the similarity between word vectors
for each pair and compared our results with the
gold standard using Spearman correlation.
232
tTest PPMI Freq
Data Max ? Full ? Max ? Full ? Max ? Full ?
MENdev: 0.75 0.73 0.76 0.61 0.66 0.57
MENtest 0.76 0.73 0.76 0.61 0.66 0.56
WS353 0.70 0.63 0.70 0.41 0.57 0.41
Table 1: ValuesN learned on dev (:) also improve
performance on the test data. Max ? indicates cor-
relation at the values of N that lead to the high-
est Spearman correlation on the development data.
For each weighting scheme these are: 140 (tTest),
240 (PPMI), and 20 (Freq). Full ? indicates the
correlation when using full vectors without CS.
The cosine, Jaccard, and Lin similarity mea-
sures (Curran, 2004) were all used to ensure the
results reflect genuine effects of context selection,
and not an artefact of any particular similarity
measure. The similarity measure and value of N
were chosen, given a particular weighting scheme,
to maximise correlation on the development part
of the MEN data (Bruni et al., 2012) (MENdev).
Testing was performed on the remaining section
of MEN and the entire WS353 dataset (Finkelstein
et al., 2002). The MEN dataset consists of 3,000
word pairs rated for similarity, which is divided
into a 2,000-pair development set and a 1,000-pair
test set. WS353 consists only of 353 pairs, but has
been consistently used as a benchmark word simi-
larity dataset throughout the past decade.
Results Figure 2 shows how correlation varies
with N for the MEN development data. The
peak performance for tTest is achieved when using
around 140 top-ranked contexts per word, while
for PPMI it is at N  240, and for Freq N  20.
The dramatic drop in performance is demonstrated
when using all three similarity measures, although
Jaccard seems particularly sensitive to the nega-
tive tTest weights that are introduced when lower-
ranked contexts are added to the vectors. The re-
maining experiments only consider cosine similar-
ity. We also find that context selection improves
correlation for tTest, PPMI, and the unweighted
Freq vectors on the test data (Table 1). Moreover,
the lower the correlation from the full vectors, the
larger the improvement when using CS.
3.1 Dimensionality Reduction
Figure 3 shows the effects of dimensionality re-
duction described in the following experiments.
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
Maximum nonzero elements per vector
Spearm
an
 
 
ttestppmifreqmaxmaxmax
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 100000.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Maximum nonzero elements per vector
Spearman
 
 
ttestppmifreqmaxmaxmax
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 100000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Maximum nonzero elements per vector
Spearman
 
 ttestppmifreqmaxmaxmax
Figure 2: Correlation decreases as more lower-
ranked context words are introduced (MENdev),
with cosine (top), Lin (bottom left), and Jaccard
(bottom right) simialrity measures.
3.1.1 SVD and CS
To check whether CS improves the correlation
through increased sparsity or whether it improves
the contextual representation of the words, we in-
vestigated the behaviour of SVD on three differ-
ent levels of vector sparsity. To construct the most
sparse vectors, we chose the best performing N
for each weighting scheme (from Table 1). Thus
sparse tTest vectors had
140
10000
 0.0140, or 1.4%,
non-zero elements. We also chose a mid-range
of N  3300 for up to 33% of non-zero ele-
ments per vector, and finally the full vectors with
N  10000.
Results In general the CS-tuned vectors lead
to better lower-dimensional representations. The
mid-range contexts in the tTest weighting scheme
seem to hold information that hinders SVD, while
the lowest-ranked negative weights appear to help
(when the mid-range contexts are present as well).
For the PPMI weighting, fewer contexts consis-
tently lead to better representations, while the un-
weighted vectors seem to mainly hold information
in the top 20 most frequent contexts for each word.
3.1.2 SVD, CS, and Normalisation
We also consider the combination of normalisation
and context selection followed by SVD.
233
0 100 200 300 400 500 600 700 8000.3
0.4
0.5
0.6
0.7
0.8
Number of dimensions (K)
Spearma
n
tTest
 
 
140N=3300N=10000norm 140norm N=3300norm N=10000all 140all N=3300all N=10000 0 100 200 300 400 500 600 700 8000.10.2
0.30.4
0.50.6
0.70.8
Number of dimensions (K)
Spearma
n
PPMI
 
 
240N=3300N=10000norm 240norm N=3300norm N=10000all 240all N=3300all N=10000 0 100 200 300 400 500 600 700 8000.20.3
0.4
0.5
0.6
0.7
Number of dimensions (K)
Spearma
n
Freq
 
 
20N=3300N=10000norm 20norm N=3300norm N=10000all 20all N=3300all N=10000
Figure 3: Vectors tuned for sparseness (blue) consistently produce equal or better dimensionality reduc-
tions (results on MENdev). The solid lines show improvement in lower dimensional representations of
SVD when dimensionality reduction is applied after normalisation.
Results Normalisation leads to more stable SVD
representations, with a large improvement for
small numbers of dimensions (K) as demonstrated
by the solid lines in Figure 3. At K  20 the
Spearman correlation increases from 0.61 to 0.71.
In addition, for tTest there is an improvement in
the mid-range vectors, and a knock-on effect for
the full vectors. As the tTest values effectively
range from 0.1 to 0.1, the mid-range values are
very small numbers closely grouped around zero.
Normalisation spreads and increases these num-
bers, perhaps making them more relevant to the
SVD algorithm. The effect is also visible for
PPMI weighting where at K  20 the correlation
increases from 0.48 to 0.70. For PPMI and Freq
we also see that, for the full and mid-range vec-
tors, the SVD representations have slightly higher
correlations than the unreduced vectors.
3.2 Random Indexing
We use random indexing primarily to produce a
vector representation for convolution (Section 4).
While this produces a lower-dimensional repre-
sentation, it may not use less memory since the re-
sulting vectors, although smaller, are fully dense.
In summary, the RI encoded vectors with di-
mensions of D  4096 lead to only slightly re-
duced correlation values compared to their unen-
coded counterparts. We find that for tTest we
get similar performance with or without CS at
any level, while for PPMI CS helps especially for
D ? 512. On Freq we find that CS with N  60
leads to higher correlation, but mid-range and full
vectors have equivalent performance. For Freq,
the correlation is equivalent to full vectors from
D  128, while for the weighted vectors 512 di-
mensions appear to be sufficient. Unlike for SVD,
normalisation slightly reduces the performance for
mid-range dimensions.
4 Compositional Experiments
We examine the performance of vectors aug-
mented by CS and normalisation in two compo-
sitional tasks. The first is an extension of the word
similarity task to phrase pairs, using the dataset
of Mitchell and Lapata (2010). Each entry in the
dataset consists of two phrases, each consisting of
two words (in various syntactic relations, such as
verb-object and adjective noun), and a gold stan-
dard score. We combine the two word vectors into
a single phrase vector using various operators de-
scribed below. We then calculate the similarity
between the phrase vectors using cosine and com-
pare the resulting scores against the gold standard
using Spearman correlation. The second task is
our new definitions task where, again, word vec-
tors from each definition are composed to form a
single vector, which can then be compared for sim-
ilarity with the target term.
We use PPMI- and tTest-weighted vectors at
three CS cutoff points: the best chosen N from
Section 3, the top third of the ranked contexts at
N  3300, and the full vectors without CS at
N  10000. This gives us a range of values to
examine, without directly tuning on this dataset.
For dimensionality reduction we consider vectors
reduced with SVD to 100 and 700 dimensions. In
some cases we exclude the results for SVD
700
be-
cause they are very close to the scores for unre-
duced vectors. We experiment with 3 values of D
from t512, 1024, 4096u for the RI vectors.
Operators To combine distributional vectors
into a single-vector sentence representation, we
use a representative set of methods from Mitchell
and Lapata (2010). In particular, we use vector
addition, elementwise (Hadamard) product, Kro-
necker product, and circular convolution (Plate,
1991; Jones and Mewhort, 2007), which are de-
234
fined as follows for two word vectors ~x, ~y:
Sum ~x  ~y  t~x
i
  ~y
i
u
i
Prod ~xd ~y  t~x
i
 ~y
i
u
i
Kron ~xb ~y  t~x
i
 ~y
j
u
ij
Conv ~xg ~y 
!
?
n
j0
p~xq
j%n
 p~yq
pijq%n
)
i
Repeated application of the Sum operation adds
contexts for each of the words that occur in a
phrase, which maintains (and mixes) any noisy
parts of the component word vectors. Our inten-
tion was that use of the CS vectors would lead
to less noisy word vectors and hence less noisy
phrase and sentence vectors. The Prod operator,
on the other hand, provides a phrase or sentence
representation consisting only of the contexts that
are common to all of the words in the sentence
(since zeros in any of the word vectors lead to
zeros in the same position in the sentence vec-
tor). This effect is particularly problematic for rare
words which may have sparse vectors, leading to
a sparse vector for the sentence.
1
We address the
sparsity problem through the use of dimensional-
ity reduction, which produces more dense vectors.
Kron, the Kronecker (or tensor) product of two
vectors, produces a matrix (second order tensor)
whose diagonal matches the result of the Prod
operation, but whose off-diagonal entries are all
the other products of elements of the two vectors.
We only apply Kron to SVD-reduced vectors, and
to compare two matrices we turn them into vec-
tors by concatenating matrix rows, and use co-
sine similarity on the resulting vectors. While in
the more complex, type-driven methods (Baroni
and Zamparelli, 2010; Coecke et al., 2010) ten-
sors represent functions, and off-diagonal entries
have a particular transformational interpretation as
part of a linear map, the significance of the off-
diagonal elements is difficult to interpret in our
setting, apart from their role as encoders of the or-
der of operands. We only examine Kron as the un-
encoded version of the Conv operator to see how
the performance is affected by the random index-
ing and the modular summation by which Conv
differs from Kron.
2
We cannot use Kron for com-
bining more than two words as the size of the re-
sulting tensor grows exponentially with the num-
1
Sparsity is a problem that may be addressable through
smoothing (Zhai and Lafferty, 2001), although we do not in-
vestigate that avenue in this paper.
2
Conv also differs from Kron in that it is commutative,
unless one of the operands is permuted. In this paper we do
not permute the operands.
Oper N=140 N=3300 N=10000
sum
ttest 0.40 (0.41) 0.40 (0.40) 0.40 (0.40)
SVD
100
0.37 (0.42) 0.35 (0.41) 0.37 (0.40)
prod
ttest 0.32 (0.32) 0.40 (0.40) 0.32 (0.32)
SVD
100
0.25 (0.23) 0.23 (0.23) 0.21 (0.23)
kron
SVD
100
0.31 (0.34) 0.34 (0.38) 0.29 (0.32)
SVD
700
0.39 (0.39) 0.37 (0.37) 0.30 (0.30)
conv
RI
512
0.10 (0.12) 0.26 (0.21) 0.25 (0.25)
RI
1024
0.22 (0.15) 0.29 (0.27) 0.25 (0.26)
RI
4096
0.16 (0.19) 0.33 (0.34) 0.28 (0.30)
Table 2: Behaviour of vector operators with tTest
vectors on ML2010 (Spearman correlation). Val-
ues for normalised vectors in parentheses.
Oper N=240 N=3300 N=10000
sum
ppmi 0.40 (0.39) 0.40 (0.39) 0.29 (0.29)
SVD
100
0.40 (0.40) 0.38 (0.40) 0.29 (0.30)
prod
ppmi 0.28 (0.28) 0.40 (0.40) 0.30 (0.30)
SVD
100
0.23 (0.17) 0.18 (0.22) 0.14 (0.12)
kron
SVD
100
0.37 (0.30) 0.36 (0.38) 0.27 (0.27)
SVD
700
0.38 (0.37) 0.37 (0.37) 0.26 (0.26)
conv
RI
512
0.09 (0.09) 0.27 (0.30) 0.25 (0.24)
RI
1024
0.08 (0.14) 0.33 (0.37) 0.25 (0.27)
RI
4096
0.18 (0.19) 0.37 (0.38) 0.27 (0.27)
Table 3: Behaviour of vector operators with PPMI
vectors on ML2010 (Spearman correlation). Val-
ues for normalised vectors in parentheses.
ber of vector operations, but we can use Conv as
an encoded alternative as it results in a vector of
the same dimension as the two operands.
4.1 Phrase Similarity
To test how CS, normalisation, and dimensional-
ity reduction affect simple compositional vector
operations we use the test portion of the phrasal
similarity dataset from Mitchell and Lapata (2010)
(ML2010). This dataset consists of pairs of two-
word phrases and a human similarity judgement
on the scale of 1-7. There are three types of
phrases: noun-noun, adjective-noun, and verb-
object. In the original paper, and some subse-
quent works, these were treated as three different
datasets; however, here we combine the datasets
into one single phrase pair dataset. This allows us
to summarise the effects of different types of vec-
tors on phrasal composition in general.
Results Our results (Tables 2 and 3) are compa-
rable to those in Mitchell and Lapata (2010) av-
eraged across the phrase-types (?  0.44), but
are achieved with much smaller vectors. We find
that with normalisation, and the optimal choice
of N , there is little difference between Prod and
Sum. Sum and Kron benefit from normalisa-
tion, especially in combination with SVD, but for
Prod it either makes no difference or reduces per-
formance. Product-based methods (Prod, Kron,
235
Conv) have a preference for context selection that
includes the mid-rank contexts (N  3300), but
not the full vector (N  10000). On tTest vec-
tors Sum is relatively stable across different CS
and SVD settings, but with PPMI weighting, there
is a preference for lower N . SVD reduces perfor-
mance for Prod, but not for Kron. Finally, Conv
gets higher correlation with higher-dimensional RI
vectors and with PPMI weights.
4.2 Definition Retrieval
In this task, which is formulated as a retrieval task,
we investigate the behaviour of different vector
operators as multiple operations are chained to-
gether. We first encode each definition into a sin-
gle vector through repeated application of one of
the operators on the distributional vectors of the
content words in the definition. Then, for each
head (defined) word, we rank all the different defi-
nition vectors in decreasing order according to in-
ner product (unnormalised cosine) similarity with
the head word?s distributional vector.
Performance is measured using precision and
Mean Reciprocal Rank (MRR). If the correct defi-
nition is ranked first, the precision (P@1) is 1, oth-
erwise 0. Since there is only one definition per
head word, the reciprocal rank (RR) is the inverse
of the rank of the correct definition. So if the cor-
rect definition is ranked fourth, for example, then
RR is
1
4
. MRR is the average of the RR across all
head words.
The difficulty of the task depends on how many
words there are in the dataset and how similar their
definitions are. In addition, if a head word oc-
curs in the definition of another word in the same
dataset, it may cause the incorrect definition to be
ranked higher than the correct one. These prob-
lems are more likely to occur with higher fre-
quency words and in a larger dataset. In order
to counter these effects, we average our results
over ten repeated random samplings of 100 word-
definition pairs. The sampling also gives us a ran-
dom baseline for P@1 of 0.01300.0106 and for
MRR 0.0576  0.0170, which can be interpreted
as there is a chance of slightly more than 1 in 100
of ranking the correct definition first, and on aver-
age the correct definition is ranked around the 20
mark.
For this task all experiments were performed
using the tTest-weighted vectors. When applying
normalisation we use ?  1 (Norm) and ?  10
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD10
346 547 594 537 409 300 216 150 287
Table 4: Number of definitions per dataset.
(Norm10). In addition, we examine the effect of
continually applying Norm after every operation
(CNorm).
Dataset We developed a new dataset (DD) con-
sisting of 3,386 definitions from the Wiktionary
BNC spoken-word frequency list.
3
Most of the
words have several definitions, but we considered
only the first definition with at least two non-
stopwords. The word-definition pairs were di-
vided into nine separate datasets according to the
number of non-stopwords in the definition. For ex-
ample, all of the definitions that have five content
words are in DD5. The exception is DD10, which
contains all the definitions of ten or more words.
Table 4 shows the number of definitions in each
dataset.
Results Figure 4 shows how the MRR varies
with different DD datasets for Sum, Prod, and
Conv. The CS, SVD, and RI settings for each op-
erator correspond to the best average settings from
Table 5. In some cases other settings had simi-
lar performance, but we chose these for illustrative
purposes. We can see that all operators have rel-
atively higher MRR on smaller datasets (DD6-9).
Compensating for that effect, we can hypothesise
that Sum has a steady performance across differ-
ent definition sizes, while the performance of both
Prod and Conv declines as the number of oper-
ations increases. Normalisation helps with Sum
throughout, with little difference in performance
between Norm and Norm10, but with a slight de-
crease when CNorm is used. On the other hand,
only CNorm improves the ranking of Prod-based
vectors. Normalisation makes no difference for RI
vectors combined with convolution and the results
in Table 5 show that, on average, Conv performs
worse than the random baseline.
In Figure 5 we can see that, although dimen-
sionality reduction leads to lower MRR, for Sum,
normalisation prior to SVD counteracts this effect,
while, for Prod, dimensionality reduction, in gen-
eral, reduces the performance.
3
http://simple.wiktionary.org/wiki/Wiktionary:BNC spoken freq
236
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
MRR
 
 
SumSum+NormProdProd+NormConvConv+NormDDsize/1000
Figure 4: Per-dataset breakdown of best nor-
malised and unnormalised vectors for each vector
operator. Stars indicate the dataset size from Ta-
ble 4 divided by 1000.
Sum Prod Conv
Norm No Yes No CN No Yes
CS (N ) 140 140 3300 10000 140 3300
SVD(K)/RI(D) 700 700 None None 2048 512
mean P@1 0.18 0.23 0.01 0.11 0.00 0.00
mean MRR 0.28 0.35 0.06 0.17 0.02 0.02
Table 5: Best settings for operators calculated
from the highest average MRR across all the
datasets, with and without normalisation. The
results for vectors with no normalisation or CS
are: Sum - P@1=0.1567, MRR=0.2624; Prod -
P@1=0.0147, MRR=0.0542; Conv P@1=0.0027,
MRR=0.0192.
5 Discussion
In this paper we introduced context selection and
normalisation as techniques for improving the se-
mantic vector space representations of words. We
found that, although our untuned vectors perform
better on WS353 data (?  0.63) than vectors used
by Mitchell and Lapata (2010) (?  0.42), our
best phrase composition model (Sum, ?  0.40)
produces a lower performance than an estimate of
their best model (Prod, ?  0.44).
4
This indicates
that better performance on word-similarity data
does not directly translate into better performance
on compositional tasks; however, CS and normal-
isation are both effective in increasing the qual-
ity of the composed representation (?  0.42).
Since CS and normalisation are computationally
inexpensive, they are an excellent way to improve
model quality compared to the alternative, which
4
The estimate is computed as an average across the three
phrase-type results.
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
MRR
 
 Sum BestSum+SVDSum+SVD+Norm10Prod BestProd+SVDProd+SVD+CNorm
Figure 5: Per-dataset breakdown of best nor-
malised and unnormalised SVD vectors for Sum
and Prod. For both operators the best CS and SVD
settings for normalised vectors were N  140,
K  700, and for unnormalised wereN  10000,
K  700.
is building several models with various context
types, in order to find which one suits the data best.
Furthermore, we show that, as the number of
vector operations increases, Sum is the most sta-
ble operator and that it benefits from sparser rep-
resentations (low N ) and normalisation. Employ-
ing both of these methods, we are able to build an
SVD-based representation that performs as well
as full-dimensional vectors which, together with
Sum, give the best results on both phrase and def-
inition tasks. In fact, normalisation and CS both
improve the SVD representations of the vectors
across different weighting schemes. This is a key
result, as many of the more complex composi-
tional methods require low dimensional represen-
tations for computational reasons.
Future work will include application of CS
and normalised lower-dimensional vectors to more
complex compositional methods, and investiga-
tions into whether these strategies apply to other
context types and other dimensionality reduction
methods such as LDA (Blei et al., 2003).
Acknowledgements
Tamara Polajnar is supported by ERC Starting
Grant DisCoTex (306920). Stephen Clark is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We would like
to thank Laura Rimell for helpful discussion, and
Laura and the anonymous reviewers for helpful
comments on the paper.
237
References
Jacob Andreas and Zoubin Ghahramani. 2013. A gen-
erative model of vector space semantics. In Pro-
ceedings of the ACL 2013 Workshop on Continu-
ous Vector Space Models and their Compositional-
ity, Sofia, Bulgaria.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in technicolor. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 136?145,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Comput. Linguist., 38(1):41?71, March.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345?384.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
Scott Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the Society for Information Science, 41(6):391?
407.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th international joint conference on Artifical
intelligence, IJCAI?07, pages 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Pablo Gamallo and Stefan Bordag. 2011. Is singu-
lar value decomposition useful for word similarity
extraction? Language Resources and Evaluation,
45(2):95?119.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114:1?37.
Ekaterina Kochmar and Ted Briscoe. 2013. Capturing
anomalies in the choice of content words in compo-
sitional distributional semantic space. In Proceed-
ings of the Recent Advances in Natural Language
Processing (RANLP-2013), Hissar, Bulgaria.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
T. A. Plate. 1991. Holographic reduced Repre-
sentations: Convolution algebra for compositional
distributed representations. In J. Mylopoulos and
R. Reiter, editors, Proceedings of the 12th Inter-
national Joint Conference on Artificial Intelligence,
Sydney, Australia, August 1991, pages 30?35, San
Mateo, CA. Morgan Kauffman.
D Seung and L Lee. 2001. Algorithms for non-
negative matrix factorization. Advances in neural
information processing systems, 13:556?562.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Jeju Island, Korea.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2011. Distributed structures and distributional
meaning. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, DiSCo-
11, pages 10?15, Portland, Oregon. Association for
Computational Linguistics.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
the 24th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?01, pages 334?342, New York,
NY, USA. ACM.
238
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 85?89, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UCAM-CORE: Incorporating structured distributional similarity into STS
Tamara Polajnar Laura Rimell Douwe Kiela
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
{tamara.polajnar,laura.rimell,douwe.kiela}@cl.cam.ac.uk
Abstract
This paper describes methods that were sub-
mitted as part of the *SEM shared task on
Semantic Textual Similarity. Multiple kernels
provide different views of syntactic structure,
from both tree and dependency parses. The
kernels are then combined with simple lex-
ical features using Gaussian process regres-
sion, which is trained on different subsets of
training data for each run. We found that the
simplest combination has the highest consis-
tency across the different data sets, while in-
troduction of more training data and models
requires training and test data with matching
qualities.
1 Introduction
The Semantic Textual Similarity (STS) shared task
consists of several data sets of paired passages of
text. The aim is to predict the similarity that hu-
man annotators have assigned to these aligned pairs.
Text length and grammatical quality vary between
the data sets, so our submissions to the task aimed to
investigate whether models that incorporate syntac-
tic structure in similarity calculation can be consis-
tently applied to diverse and noisy data.
We model the problem as a combination of ker-
nels (Shawe-Taylor and Cristianini, 2004), each of
which calculates similarity based on a different view
of the text. State-of-the-art results on text classifi-
cation have been achieved with kernel-based classi-
fication algorithms, such as the support vector ma-
chine (SVM) (Joachims, 1998), and the methods
here can be adapted for use in multiple kernel classi-
fication, as in Polajnar et al (2011). The kernels are
combined using Gaussian process regression (GPR)
(Rasmussen and Williams, 2006). It is important
to note that the combination strategy described here
is only a different way of viewing the regression-
combined mixture of similarity measures approach
that is already popular in STS systems, including
several that participated in previous SemEval tasks
(Croce et al, 2012; Ba?r et al, 2012). Likewise, oth-
ers, such as Croce et al (2012), have used tree and
dependency parse information as part of their sys-
tems; however, we use a tree kernel approach based
on a novel encoding method introduced by Zanzotto
et al (2011) and from there derive two dependency-
based methods.
In the rest of this paper we will describe our sys-
tem, which consists of distributional similarity (Sec-
tion 2.1), several kernel measures (Section 2.2), and
a combination method (Section 2.3). This will be
followed by the description of our three submissions
(Section 3), and a discussion of the results (Sec-
tion 4).
2 Methods
At the core of all the kernel methods is either sur-
face, distributional, or syntactic similarity between
sentence constituents. The methods themselves en-
code sentences into vectors or sets of vectors, while
the similarity between any two vectors is calculated
using cosine.
2.1 Distributional Similarity
Target words are the non-stopwords that occur
within our training and test data. The two distri-
butional methods we use here both represent target
85
words as vectors that encode word occurrence within
a set of contexts. The first method is a variation on
BEAGLE (Jones and Mewhort, 2007), which con-
siders contexts to be words that surround targets.
The second method is based on ESA (Gabrilovich
and Markovitch, 2007), which considers contexts to
be Wikipedia documents that contain target words.
To gather the distributional data with both of
these approaches we used 316,305 documents from
the September 2012 snapshot of Wikipedia. The
training corpus for BEAGLE is generated by pool-
ing the top 20 documents retrieved by querying the
Wikipedia snapshot index for each target word in the
training and test data sets.
2.1.1 BEAGLE
Random indexing (Kaski, 1998) is a technique for
dimensionality reduction where pseudo-orthogonal
bases are generated by randomly sampling a distri-
bution. BEAGLE is a model where random indexing
is used to represent word co-occurrence vectors in a
distributional model.
Each context word is represented as a D-
dimensional vector of normally distributed random
values drawn from the Gaussian distribution
N (0, ?2), where ? =
1
?
D
and D = 4096 (1)
A target word is represented as the sum of the
vectors of all the context words that occur within a
certain context window around the target word. In
BEAGLE this window is considered to be the sen-
tence in which the target word occurs; however, to
avoid segmenting the entire corpus, we assume the
window to include 5 words to either side of the tar-
get. This method has the advantage of keeping the
dimensionality of the context space constant even
if more context words are added, but we limit the
context words to the top 10,000 most frequent non-
stopwords in the corpus.
2.1.2 ESA
ESA represents a target word as a weighted
ranked list of the top N documents that contain the
word, retrieved from a high quality collection. We
used the BM25F (Robertson et al, 2004) weighting
function and the topN = 700 documents. These pa-
rameters were chosen by testing on the WordSim353
dataset.1 The list of retrieved documents can be rep-
resented as a very sparse vector whose dimensions
match the number of documents in the collection,
or in a more computationally efficient manner as
a hash map linking document identifiers to the re-
trieval weights. Similarity between lists was calcu-
lated using the cosine measure augmented to work
on the hash map data type.
2.2 Kernel Measures
In our experiments we use six basic kernel types,
which are described below. Effectively we have
eight kernels, because we also use the tree and de-
pendency kernels with and without distributional in-
formation. Each kernel is a function which is passed
a pair of short texts, which it then encodes into a spe-
cific format and compares using a defined similarity
function. LK uses the regular cosine similarity func-
tion, but LEK, TK, DK, MDK, DGK use the follow-
ing cosine similarity redefined for sets of vectors. If
the texts are represented as sets of vectors X and Y ,
the set similarity kernel function is:
?set(X,Y ) =
?
i
?
j
cos(~xi, ~yj) (2)
and normalisation is accomplished in the standard
way for kernels by:
?set?n(X,Y ) =
?set(X,Y )
?
(?set(X,X)?set(Y, Y ))
(3)
LK - The lexical kernel calculates the overlap be-
tween the tokens that occur in each of the paired
texts, where the tokens consist of Porter stemmed
(Porter, 1980) non-stopwords. Each text is repre-
sented as a frequency vector of tokens that occur
within it and the similarity between the pair is cal-
culated using cosine.
LEK - The lexical ESA kernel represents each
example in the pair as the set of words that do not
occur in the intersection of the two texts. The simi-
larity is calculated as in Equation (3) with X and Y
being the ESA vectors of each word from the first
and second text representations, respectively.
TK - The tree kernel representation is based on
the definition by Zanzotto et al (2011). Briefly,
1http://www.cs.technion.ac.il/?gabr/resources/
data/wordsim353/
86
each piece of text is parsed2; the non-terminal
nodes of the parse tree, stopwords, and out-of-
dictionary terms are all assigned a new random vec-
tor (Equation 1); while the leaves that occurred
in the BEAGLE training corpus are assigned their
learned distributional vectors (Section 2.1.1).
Each subtree of a tree is encoded recursively as
a vector, where the distributional vectors represent-
ing each node are combined using the circular con-
volution operator (Plate, 1994; Jones and Mewhort,
2007). The whole tree is represented as a set of vec-
tors, one for each subtree.
DK - The dependency kernel representation en-
codes each dependency pair as a separate vector, dis-
counting the labels. The non-stopword terminals are
represented as their distributional vectors, while the
stopwords and out-of-dictionary terms are given a
unique random vector. The vector for the depen-
dency pair is obtained via a circular convolution of
the individual word vectors.
MDK - The multiple dependency kernel is con-
structed like the dependency kernel, but similarity is
calculated separately between all the the pairs that
share the same dependency label. The combined
similarity for all dependency labels in the parse is
then calculated using least squares linear regression.
While at the later stage we use GPR to combine all
of the different kernels, for MDK we found that lin-
ear regression provided better performance.
DGK - The depgram kernel represents each de-
pendency pair as an ESA vector obtained by search-
ing the ESA collection for the two words in the
dependency pair joined by the AND operator. The
DGK representation only contains the dependencies
that occur in one similarity text or the other, but not
in both.
2.3 Regression
Each of the kernel measures above is used to calcu-
late a similarity score between a pair of texts. The
different similarity scores are then combined using
2Because many of the datasets contained incomplete or un-
grammatical sentences, we had to approximate some parses.
The parsing was done using the Stanford parser (Klein and
Manning, 2003), which failed on some overly long sentences,
which we therefore segmented at conjunctions or commas.
Since our methods only compared subtrees of parses, we simply
took the union of all the partial parses for a given sentence.
Gaussian process regression (GPR) (Rasmussen and
Williams, 2006). GPR is a probabilistic regression
method where the weights are modelled as Gaussian
random variables. GPR is defined by a covariance
function, which is akin to the kernel function in the
support vector machine. We used the squared expo-
nential isotropic covariance function (also known as
the radial basis function):
cov(xi, xj) = p
2
1e
(xi?xj)
T ?(p2?I)
?1?(xi?xj)
2 + p23?ij
with parameters p1 = 1, p2 = 1, and p3 = 0.01. We
found that training for parameters increased overfit-
ting and produced worse results in validation exper-
iments.
3 Submitted Runs
We submitted three runs. This is not sufficient for
a full evaluation of the new methods we proposed
here, but it gives us an inkling of general trends. To
choose the composition of the submissions, we used
STS 2012 training data for training, and STS 2012
test data for validation (Agirre et al, 2012). The
final submitted runs also used some of the STS 2012
test data for training.
Basic - With this run we were examining if a sim-
ple introduction of syntactic structure can improve
over the baseline performance. We trained a GPR
combination of the linear and tree kernels (LK-TK)
on the MSRpar training data. In validation experi-
ments we found that this data set in general gave the
most consistent performance for regression training.
Custom - Here we tried to approximate the best
training setup for each type of data. We only had
training data for OnWN and for this dataset we were
able to improve over the LK-TK setup; however, the
settings for the rest of the data sets were guesses
based on observations from the validation experi-
ments and overall performed poorly. OnWN was
trained on MSRpar train with LK and DK. The head-
lines model was trained on MSRpar train and Eu-
roparl test, with LK-LEK-TK-DK-TKND-DKND-
MDK (trained on Europarl).3 FNWN was trained on
MSRpar train and OnWN test with LK-LEK-DGK-
TK-DK-TKND-DKND. Finally, the SMT model
3TKND and DKND are the versions of the tree and depen-
dency kernels where no distributional vectors were used.
87
010
20
30
40
50
60
0 1 2 3 4 5
Number 
of STS p
airs
Score
Gold Standard
0
5
10
15
20
25
0 1 2 3 4 5
Number 
of STS p
airs
Score
Basic
0
5
10
15
20
25
30
35
0 1 2 3 4 5
Number 
of STS p
airs
Score
Custom
0
5
10
15
20
2 2.5 3 3.5 4 4.5 5
Number 
of STS p
airs
Score
All
Figure 1: Score distributions of different runs on the OnWN dataset
was trained on MSRpar train and Europarl test with
LK-LEK-TK-DK-TKND-DKND-MDK (trained on
MSRpar).
All - As in the LK-TK experiment, we used
the same model on all of the data sets. It was
trained on all of the training data except MSRvid,
using all eight kernel types defined above. In sum-
mary we used the LK-LEK-TK-TKND-DK-DKND-
MDK-DGK kernel combination. MDK was trained
on the 2012 training portion of MSRpar.
4 Discussion
From the shared task results in Table 1, we can see
that Basic is our highest ranked run. It has also
achieved the best performance on all data sets. The
LK on its own improves slightly on the task baseline
by removing stop words and using stemming, while
the introduction of TK contributes syntactic and dis-
tributional information. With the Custom run, we
were trying to manually estimate which training data
would best reflect properties of particular test data,
and to customise the kernel combination through
validation experiments. The only data set for which
this led to an improvement is OnWN, indicating
that customised settings can be beneficial, but that
a more scientific method for matching of training
and test data properties is required. In the All run,
we were examining the effects that maximising the
amount of training data and the number of kernel
hdlns OnWN FNWN SMT mean rank
BL 0.5399 0.2828 0.2146 0.2861 0.3639 71
Basic 0.6399 0.4440 0.3995 0.3400 0.4709 51
Cstm 0.4962 0.5639 0.1724 0.3006 0.4207 60
All 0.5510 0.3099 0.2385 0.1171 0.3200 78
Table 1: Shared task results: Pearson correlation with the
gold standard
measures has on the output predictions. The results
show that swamping the regression with models and
training data leads to overly normalised output and
a decrease in performance.
While the evaluation measure, Pearson correla-
tion, does not take into account the shape of the out-
put distribution, Figure 1 shows that this informa-
tion may be a useful indicator of model quality and
behaviour. In particular, the role of the regression
component in our approach is to learn a transforma-
tion from the output distributions of the models to
the distribution of the training data gold standard.
This makes it sensitive to the choice of training data,
which ideally would have similar characteristics to
the individual kernels, as well as a similar gold stan-
dard distribution to the test data. We can see in Fig-
ure 1 that the training data and choice of kernels in-
fluence the output distribution.
Analysis of the minimum, first quartile, median,
third quartile, and maximum statistics of the distri-
butions in Figure 1 demonstrates that, while it is dif-
ficult to visually evaluate the similarities of the dif-
ferent distributions, the smallest squared error is be-
tween the gold standard and the Custom run. This
suggests that properties other than the rank order
may also be good indicators in training and testing
of STS methods.
Acknowledgments
Tamara Polajnar is supported by the ERC Starting
Grant, DisCoTex, awarded to Stephen Clark, and
Laura Rimell and Douwe Kiela by EPSRC grant
EP/I037512/1: A Unified Model of Compositional
and Distributional Semantics: Theory and Applica-
tions.
88
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montre?al, Canada,
June. Association for Computational Linguistics.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. 2012. UNITOR: Combining semantic text
similarity functions through sv regression. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation, held in conjunction with the 1st Joint
Conference on Lexical and Computational Semantics,
pages 597?602, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In Proceedings of the 10th European Con-
ference on Machine Learning, ECML ?98, pages 137?
142, London, UK, UK. Springer-Verlag.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114:1?37.
S. Kaski. 1998. Dimensionality reduction by random
mapping: fast similarity computation for clustering.
In Proceedings of the 1998 IEEE International Joint
Conference on Neural Networks, volume 1, pages
413?418 vol.1, May.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. A. Plate. 1994. Distributed Representations and
Nested Compositional Structure. Ph.D. thesis, Univer-
sity of Toronto.
T Polajnar, T Damoulas, and M Girolami. 2011. Protein
interaction sentence detection using multiple semantic
kernels. J Biomed Semantics, 2(1):1?1.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137, July.
C. E. Rasmussen and C. K. I. Williams. 2006. Gaussian
Processes for Machine Learning. MIT Press.
Stephen Robertson, Hugo Zaragoza, and Michael Taylor.
2004. Simple BM25 extension to multiple weighted
fields. In Proceedings of the thirteenth ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?04, pages 42?49, New York, NY,
USA. ACM.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2011. Distributed structures and distributional mean-
ing. In Proceedings of the Workshop on Distributional
Semantics and Compositionality, DiSCo ?11, pages
10?15, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
89
