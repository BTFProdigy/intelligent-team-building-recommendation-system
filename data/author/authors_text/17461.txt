Proceedings of the SIGDIAL 2013 Conference, pages 157?159,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Multi-step Natural Language Understanding
Pierrick Milhorat, Stephan Schlo?gl, Ge?rard Chollet
Institut Mines-Te?le?com
Te?le?com ParisTech, CNRS LTCI
Paris, France
{lastname}@enst.fr
Je?ro?me Boudy
Institut Mines-Te?le?com
Te?le?com SudParis
Paris, France
boudy@telecom-sudparis.eu
Abstract
While natural language as an interaction
modality is increasingly being accepted by
users, remaining technological challenges
still hinder its widespread employment.
Tools that better support the design, devel-
opment and improvement of these types
of applications are required. This demo
presents a prototyping framework for Spo-
ken Dialog System (SDS) design which
combines existing language technology
components for Automatic Speech Recog-
nition (ASR), Dialog Management (DM),
and Text-to-Speech Synthesis (TTS) with
a multi-step component for Natural Lan-
guage Understanding (NLU).
1 Introduction
Recently speech and other types of natural lan-
guage are experiencing an increased acceptance
when being used for interacting with ?intelli-
gent? computing systems. This trend is particu-
larly reflected by products such as Apple?s Siri1,
Google?s Now2 and Nuance?s Dragon Solutions3.
While these applications demonstrate the indus-
try?s vision of how we should be interacting with
our current and future devices, they also highlight
some of the great challenges that still exist. One
of these challenges may be seen in the fact that
Automatic Speech Recognition (ASR) remains a
highly error-prone technology which influences
subsequent natural language processing compo-
nents such as Natural Language Understanding
(NLU) and Dialog Management (DM) and leads
to often unsatisfying user experiences. Hence we
require appropriate tools that better support the
testing and studying of language as an interaction
1http://www.apple.com/ios/siri/
2http://www.google.com/landing/now/
3http://www.nuance.com/dragon/
modality and consequently allow us to build bet-
ter, more user-centered applications.
This demo presents our approach of develop-
ing a prototyping tool for Spoken Dialog Systems
(SDS). Our solution is particularly focusing on
the natural language understanding aspect of SDS
design. The overall framework is composed of
a set of existing open-source technology compo-
nents (i.e. ASR, DM, TTS) which are expanded
by several additional NLP modules responsible for
natural language understanding as well as genera-
tion. The following sections first provide a general
overview of the entire framework and then focus
particularly on the NLU part of our solution and
the different sub-modules it integrates.
2 Spoken Dialog System Design
A state-of-the-art SDS usually consists of a set of
technology components that are integrated to form
a consecutive processing chain. Starting on the
input side the ASR module produces a hypothe-
sis about the orthographic content of a spoken ut-
terance. The NLU takes this recognized utterance
and converts it into a machine readable command
or input Dialog Act (DA). The DM processes this
input DA and sends the relevant output DA to the
Natural Language Generation (NLG) component.
The NLG is then responsible for converting the
output DA into appropriate natural language text.
Finally, the Text-to-Speech (TTS) synthesis com-
ponent takes the text transmitted by the NLG and
speaks it to a user.
According to this general architecture different
open-source language components have been in-
tegrated to form a loosely coupled SDS frame-
work. The framework includes ASR performed by
the Julius Large Vocabulary Continuous Speech
Recognition engine4, dialog management based
on the Disco DM library (Rich, 2009; Rich
4http://julius.sourceforge.jp/en index.php
157
and Sidner, 2012) and TTS achieved through the
MARY Text-to-Speech Synthesis Platform5. Ad-
ditionally, we have integrated the WebWOZ Wiz-
ard of Oz Prototyping Platform6 (Schlo?gl et al,
2010) in order to allow for the simulation of (flaw-
less) natural language understanding. Expanding
these existing components we have then developed
as a set of modules responsible for actual system-
based natural language processing. The following
section describes these modules in more detail and
highlights the types of challenges they try to over-
come.
3 Natural Language Understanding
Within the processing chain of a spoken/text-
based dialog system, the NLU component is the
link between the wide and informal communica-
tion space of a user?s input and the formal and
rather restrictive semantic space that can be pro-
cessed by the DM (Mori et al, 2007). Trying to
bridge these two spaces we have connected sev-
eral modules to form an NLU processing segment
whose different modules are described below.
3.1 Semantic Parsing
First we use a Semantic Parsing (SP) module to
convert the transcribed speech provided by the
ASR into so-called Semantic Frames (SFs). To
achieve this mapping Jurc???c?ek et al (2009) de-
signed a Transformation-Based Learning Seman-
tic Parser (Brill, 1995) which we adapted to inte-
grate it with our framework. The algorithm applies
an ordered set of rules to hypothetical [utterance,
SF] pairs in order to find the closest matching SF.
3.2 Semantic Unification
Next we use what we call the Semantic Unifier
and Reference Resolver (SURR) module to con-
vert input SFs into SFs that can be processed by
the DM input interface. To do this we imple-
mented a bottom-up search algorithm for rewrit-
ing trees whose nodes contain lists of valued slots.
The algorithm looks for a group of root nodes that
can be reached in the forest (i.e. the existing num-
ber of trees) by transforming an input SF?s set of
slots according to the given rewriting rules. It suc-
ceeds when all slots can be rewritten into a root
list of slots. This module is supported by exter-
nal knowledge sources such as for example the
5http://mary.dfki.de/
6https://github.com/stephanschloegl/WebWOZ
context in which an utterance has been produced
(i.e. it receives input from the Context Catcher
module described below). Furthermore it could
call operating system functions, sensor readings
7 or other knowledge sources capable of provid-
ing relevant data, in order to resolve and disam-
biguate input. For instance, special-valued slots
like ?date=today? are dynamically resolved to the
correct data type and value, making the NLU more
sensitive to its surrounding environment.
3.3 Context Inclusion
In order to optimize information exchange
Human-Human interactions usually build up a
common knowledge between dialog participants.
This inherent grounding process can be compared
to the dialog history recorded in an SDS?s DM.
Using these recordings we have introduced a so-
called Context Catcher (CC) module. The way
this module is currently working is as follows: The
DM requests information from the user to progress
through the task-oriented dialog. The user replies
without specifying the type of data he/she is pro-
viding, the overall intent of the utterance or the re-
lation to any dialog slot. The CC evaluates the re-
quest expressed by the DM and consequently up-
dates various parameters of the SURR component.
Consequently the SURR is able to provide a better,
more context-specific mapping between raw SFs
provided by the SP module and the expected slots
to be filled by the DM component.
3.4 Dialog Act Conversion
An SDS?s DM expects formal meaning represen-
tations to be converted to actual dialog moves or
Dialog Acts (DA); similar to parametrized dialog
commands. A DA is the smallest unit of determin-
istic action to support the dialogue flow. The num-
ber of DAs that are available at any given point is
finite, dynamic and depends on the current state of
the dialog (Note: Here a state does not refer to a
?real? state, such as the ones used in Markov De-
cision Processes or Partially Observable Markov
Decision Processes, but rather to a general status
of the dialog). In other words, two input utter-
ances carrying the same meaning may lead to dif-
ferent consequences depending on a given dialog
state. The right action, i.e. the accurate DA, is to
be determined by the NLU component. As there
7Note: At the moment sensor readings are not imple-
mented as they are currently not available in the developing
environment
158
is usually a many-to-many matching between SFs
and actual DAs we integrated an additional Dialog
Act Converter (DAC) module. This module uses
the context to generate a list of expected slots for
which a user may provide a value (i.e. it converts
possible DAs to SFs). Then a matching between
the actual inputs and the expectations is applied in
order to find the most probable DA.
4 Supporting Mixed Initiatives
SDS dialog designs usually run along an initia-
tive scale that ranges from user-driven to strictly
machine-driven interaction. In the case of a
machine-driven dialog a user has to follow the re-
quests of the system. Interactions that lie out of the
scope of this dialog design are not understood and
may either be discarded or, in the worst case, lead
to a system failure. Despite this potential for fail-
ure, machine-driven designs make the dialog eas-
ier to control and thus less prone to errors, yet,
due to the lack of adaptability exposed by the sys-
tem, also less human-like. On the other hand, pure
user-driven dialog designs minimize the functional
range of a system as they only react to commands
without assuring their functional integrity.
The above described modular approach to NLU
aims to support a mixed initiative design where a
system?s integrity and its goals are sufficiently de-
fined; the user, however, is not restricted by the
type and amount of spoken input he/she can use
to interact. To offer this type of interaction the
system needs to handle three kinds of potential
mis-usages: (1) out-of-application cases, (2) out-
of-dialog cases and (3) out-of-turn cases. To ad-
dress the first one our training corpus has been
augmented so that it includes examples of garbage
SFs. As a result an out-of-application utterance
triggers a generic reply from the system, notifying
the user that he/she is outside the scope of the ap-
plication. In the case where a user stays within
the scope of the application but tries to initiate
a new unrelated dialog (i.e. out-of-dialog case),
the DM?s stack of tasks is incremented with the
new dialog. The system will lead the user back
to the previous topic once the newly added one
is completed. Finally, as for the out-of-turn cases
i.e. the cases where a user would answer a sys-
tem request with a non-expected utterance such as
an over-complete one, the NLU process, retriev-
ing the DM?s expectations, discards unrelated or
over-complete information.
5 Demo Description
Focusing on the NLU aspect of the SDS pipeline
this demo will demonstrate how the different mod-
ules described above (i.e. SP, SURR, CC, and
DAC) work together. An application scenario
from the ambient assisted living domain (i.e. the
operation of a ?Pillbox? application) will serve as
an example use case. It will be shown how the
natural language input potentially recognized by
an ASR component is further interpreted by our
NLU processing segment. All the steps discussed
in Section 3 will be visible.
6 Conclusion
In this paper we described a set of NLU compo-
nents that were integrated as part of a loosely cou-
pled SDS. Separate modules for semantic parsing,
semantic unification and reference resolution, con-
text inclusion as well as dialog act conversion have
been described. Furthermore we have highlighted
how our system offers support for mixed-initiative
dialog interactions. A first test of this NLU pro-
cessing chain showed that the use of our multi-
component approach is feasible, and we believe
that this solution can be seen as a valuable test and
development framework for natural language pro-
cessing research.
References
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational lin-
guistics.
F. Jurc???c?ek, F. Mairesse, M. Gas?ic?, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Transformation-
based Learning for semantic parsing. Proceedings
of INTERSPEECH, pages 2719?2722.
R. De Mori, F. Be?chet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2007. Spoken language
understanding: A survey. Proceedings of ASRU.
C. Rich and C. L. Sidner. 2012. Using collaborative
discourse theory to partially automate dialogue tree
authoring. Intelligent Virtual Agents, pages 327?
340.
C. Rich. 2009. Building task-based user interfaces
with ANSI/CEA-2018. Computer.
S. Schlo?gl, G. Doherty, S. Luz, and N. Karamanis.
2010. WebWOZ: A Wizard of Oz Prototyping
Framework. In Proceedings of ACM EICS, pages
109?114.
159
Proceedings of the SIGDIAL 2013 Conference, pages 160?162,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
WebWOZ: A Platform for Designing and Conducting Web-based Wizard
of Oz Experiments
Stephan Schlo?gl
Institut Mines-Te?le?com
Te?le?com ParisTech, CNRS LTCI
Paris, France
schlogl@enst.fr
Saturnino Luz, Gavin Doherty
Trinity College
University of Dublin
Dublin, Ireland
{firstname.lastname}@scss.tcd.ie
Abstract
The Wizard of Oz (WOZ) method has
been used for a variety of purposes in
early-stage development of dialogue sys-
tems and language technology applica-
tions, from data collection, to experimen-
tation, prototyping and evaluation. How-
ever, software to support WOZ experimen-
tation is often developed ad hoc for spe-
cific application scenarios. In this demo
we present WebWOZ, a web-based WOZ
prototyping platform that aims at support-
ing a variety of experimental settings and
combinations of different language tech-
nology components. We argue that a
generic and distributed platform such as
WebWOZ can increase the usefulness of
the WOZ method.
1 Introduction
The use of language technologies such as Auto-
matic Speech Recognition (ASR), Machine Trans-
lation (MT) and Text-to-Speech Synthesis (TTS)
has significantly increased in recent years. Drivers
of adoption have been enhanced quality and in-
creasingly ubiquitous access to products and ser-
vices. However, the technology is still far from
perfect and typically substantial engineering effort
is needed before prototypes can deliver a user ex-
perience robust enough to allow potential applica-
tions to be evaluated with real users. For graph-
ical interfaces, well-known prototyping methods
like sketching and wire-framing support the de-
signer in obtaining early impressions and initial
user feedback. These low-fidelity prototyping
techniques do, however, not map well onto sys-
tems based around speech and natural language.
Wizard of Oz (WOZ) tries to fill this gap by using
a human ?wizard? to mimic some of the function-
ality of a system, which allows for evaluating po-
tential user experiences and interaction strategies
without the need for building a fully functional
product first (Gould et al, 1983).
2 The WebWOZ Platform
WebWOZ is an entirely web-based, open-source
Wizard of Oz prototyping platform1. It allows for
testing interaction scenarios that employ one or
more Language Technology Components (LTC).
The integration of these LTCs is done via web ser-
vices. Currently we have integrated ASR from
Google using HTML-based Speech Input2, on-
the-fly MT from Microsoft3 and TTS provided
by the Muse Speech Technology Research Plat-
form4. In addition we support pre-recorded audio
and video files that are accessible through a web
server. Table 1 shows the different components
currently integrated into WebWOZ. Depending on
the application scenario those components can be
turned on and off as well as be used in combina-
tion (Schlo?gl et al, 2010; Schlo?gl et al, 2011).
2.1 Software Requirements
WebWOZ is written in Java and therefore can be
hosted on a typical application server (e.g. Apache
Tomcat). In addition a relational database (e.g.
MySQL) is needed. In order to run experiments
we further recommend the use of an up-to-date
web browser that is able to adequately interpret
recent HTML5 commands. For the moment, the
Chrome browser is probably the best choice, since
it supports speech input without the need for in-
stalling an additional plug-in. However, we are
convinced that soon most web browsers will sup-
port the majority of HTML5 features required by
WebWOZ.
1https://github.com/stephanschloegl/WebWOZ/
2http://lists.w3.org/Archives/Public/public-xg-
htmlspeech/2011Feb/att-0020/api-draft.html
3http://msdn.microsoft.com/en-us/library/ff512419.aspx
4http://muster.ucd.ie/content/muse-speech-technology-
research-platform
160
Table 1: WebWOZ Component List
ASR HTML Speech Input
MT Microsoft Translate
TTS Muse Speech Technology
Pre-recorded Audio Files
2.2 Supported Scenarios
One of the main features of WebWOZ is its in-
tegrated CMS-like editing functionality. This
permits researchers/designers to create their own
WOZ experiments without requiring from them
any programming skills. They can add, edit, and
delete utterances and organize them in different
tabs (dialogue stages) using the wizard interface
(cf. demo video5). Corresponding client (i.e. non-
wizard) user/password combinations can be added
and distinct interaction modes for the experiment
can be set (e.g. ASR on/off, TTS on/off, MT
on/off, etc.). The client interface itself runs in
a separate browser window, which allows for an
easy integration into already existing web applica-
tions.
Following this architecture WebWOZ supports
the design of a variety of experimental settings.
Different scenarios from classic monolingual text-
to-text to multi-lingual speech-to-speech interac-
tions are possible. From a wizard?s perspective,
tasks can reach from pure dialogue management
to augmenting LTC output. That is, in WebWOZ
a wizard can act as the substitute for a working di-
alogue manager, linking a test persons? input with
an appropriate response by choosing from a set
of pre-defined answer possibilities. Alternatively,
however, one could be focusing on enhancing the
quality of a single LTC by augmenting its output.
Examples might include choosing from an n-best
list of recognition results or the post-editing of
output produced by an MT service.
3 Why a Web-based Solution?
The WOZ technique is usually used for four main
purposes related to the design and implementation
of dialogue systems: (1) it is used for dialogue
data collection, (2) for controlled experimentation
(including system evaluation), (3) for exploration
of design alternatives and (4) for teaching of sys-
tem design. Given this context, why should one
build a web-based WOZ platform? What are the
5http://youtu.be/VPqHfXHq4X0
benefits of such a solution? As it turns out, one can
identify benefits to each of the above mentioned
main uses of the WOZ method.
In terms of data collection, the gathering of mul-
timodal dialogue corpora is often a complex and
time consuming enterprise. It requires standard-
ization and uniformity with respect to data format,
timing and encoding, as well as collection settings
and procedures. WOZ techniques have been in-
creasingly used for this purpose, particularly in the
gathering of data for studying multimodal infor-
mation presentation and interaction e.g. (Rieser et
al., 2011). A Web-based platform such as Web-
WOZ can facilitate data collection by geographi-
cally distributed groups while guaranteeing adher-
ence to the requisite standards.
As regards experiments, a crucial requirement
from the perspective of scientific methodology is
reproducibility. Different research groups need to
be able to replicate experiments according to pre-
cisely prescribed procedures and settings. Wiz-
ard of OZ experiments, however, are usually con-
ducted using purpose built, ad hoc tools and soft-
ware. This makes replication difficult, if not im-
possible. WebWOZ provides a widely available,
standardized environment in which experimental
protocols can be precisely specified and shared
with interested research groups, thus supporting
reproducibility. These features are similarly im-
portant for extrinsic system components evalua-
tion e.g. (Schneider and Luz, 2011) where the
overall system functionality should be kept con-
stant while a specific component to be tested (say,
an MT module) is varied.
WOZ techniques are also employed for explo-
ration (through prototyping) of design ideas and
alternatives, particularly at the early design stages
of interactive systems that involve diverse lan-
guage technology components. In this case, repro-
ducibility and controlled conditions are less im-
portant. However, as distributed system develop-
ment becomes a common practice WebWOZ can
be used in such scenarios as a shared design arti-
fact to support the activities of geographically dis-
tributed design teams as well as the communica-
tion among them.
Finally, WebWOZ can be (and has been) used in
support of teaching the development of dialogue
systems. While students are usually introduced to
WOZ (i.e. written on a lecture slide) only a small
portion of them receives actual hands-on experi-
161
ence. One reason for this lack of practical usage
might be that in order to be applicable in a teaching
context, any approach would have to have a low
logistical and technical overhead to enable stu-
dents to quickly design and carry out evaluations.
Our experience with WebWOZ has shown that the
web-based approach significantly lowers this bar-
rier. To date more than 50 students were able to de-
sign experiments and hence improve their under-
standing of the complexity of dialogue systems.
4 Uses of WebWOZ in Research
WebWOZ has already been employed in two dif-
ferent research studies. The first study explored
the effects of MT when it is used in combination
with TTS (Schneider et al, 2010). The second
study aimed at building and evaluating a corpus of
feedback utterances sent to language learners who
try to improve their pronunciation (Cabral et al,
2012).
The experimental set-up of these two stud-
ies differed greatly, highlighting the flexibility of
WebWOZ. The first study tested the scenario of
an intelligent computer system recommending ap-
propriate Internet connection bundles to German
speaking customers. To support this scenario a
set of pre-defined dialogue utterances as well as
the relevant domain utterances (i.e. examples of
Internet connection bundles) were collected, auto-
matically translated and then added to WebWOZ.
On-the-fly translation was not used as the experi-
menters wanted to control for any possible incon-
sistencies. The TTS part of the experiment did
not utilize a synthesis directly, but rather used the
possibility of WebWOZ handling pre-synthesized
audio files. ASR was simulated by the wizard.
Voice-over-IP was used to transmit the partici-
pant?s voice to the wizard, who then selected an
appropriate response.
The second study was less restrictive. Here the
researcher?s goal was to built up and evaluate a
corpus of feedback utterances, for which the wiz-
ard could be more open in terms of responses.
Similarly to the first study a set of pre-defined
responses was added to WebWOZ. However, in
cases were those utterances were not sufficient, the
wizard could use a free-text field to reply. Again
Voice-over-IP was used to transfer speech input
from a test user to the wizard and TTS was turned
off, as the experiment design used textual feed-
back only.
5 Conclusion and Future Work
We presented WebWOZ a Wizard of Oz proto-
typing platform that is developed in our research
group. WebWOZ differs from existing WOZ tools
by being entirely web-based and through its goal
of supporting various types of application scenar-
ios. The different features of WebWOZ were high-
lighted and it was described how two independent
studies already made use of them. Future work
aims to optimize WebWOZ, to generalise it to fur-
ther experimental settings and to extend it by inte-
grating additional modalities. To do so the system
has been installed in our partner institutions where
it has currently been adapted to support additional
settings in at least two other research projects. Al-
though we are aware of the fact that the great
difference between the interests of individual re-
searchers pose challenges to the design of a truly
generic WOZ tool, we believe that our platform
can be a helpful starting point for a variety of re-
searchers and designers who may wish to use the
WOZ method.
References
J. P. Cabral, M. Kane, Z. Ahmed, M. Abou-Zleikha,
E?. Sze?kely, A. Zahra, K. U. Ogbureke, P. Cahill,
J. Carson-Berndsen, and S. Schlo?gl. 2012. Rapidly
Testing the Interaction Model of a Pronunciation
Training System via Wizard-of-Oz. In Proceedings
of LREC.
J. D. Gould, J. Conti, and T. Hovanyecz. 1983. Com-
posing letters with a simulated listening typewriter.
Communications of the ACM, 26:295?308.
V. Rieser, S. Keizer, X. Liu, and O. Lemon. 2011.
Adaptive Information Presentation for Spoken Dia-
logue Systems: Evaluation with human subjects. In
Proceedings of ENLG, pages 102?109.
S. Schlo?gl, G. Doherty, N. Karamanis, A. H. Schneider,
and S. Luz. 2010. Observing the wizard: In search
of a generic interface for wizard of oz studies. In
Proceedings of Irish HCI, pages 43?50.
S. Schlo?gl, A. H. Schneider, S. Luz, and G. Doherty.
2011. Supporting the wizard: Interface improve-
ments in wizard of oz studies. In Proceedings of
BCS HCI.
A. H. Schneider and S. Luz. 2011. Speaker alignment
in synthesised, machine translated communication.
In Proceedings of IWSLT, pages 254?260.
A. H. Schneider, I. Van der Sluis, and S. Luz. 2010.
Comparing intrinsic and extrinsic evaluation of mt
output in a dialogue system. In Proceedings of the
IWSLT, pages 329?336.
162
