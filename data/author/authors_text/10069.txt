Coling 2008: Companion volume ? Posters and Demonstrations, pages 83?86
Manchester, August 2008
A Language-Independent Approach to
Keyphrase Extraction and Evaluation
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti Po?lla? and Timo Honkela
Adaptive Informatics Research Centre, Helsinki University of Technology
first.last@tkk.fi
Abstract
We present Likey, a language-independent
keyphrase extraction method based on sta-
tistical analysis and the use of a reference
corpus. Likey has a very light-weight pre-
processing phase and no parameters to be
tuned. Thus, it is not restricted to any sin-
gle language or language family. We test
Likey having exactly the same configura-
tion with 11 European languages. Further-
more, we present an automatic evaluation
method based on Wikipedia intra-linking.
1 Introduction
Keyphrase generation is an approach to collect the
main topics of a document into a list of phrases.
The methods for automatic keyphrase generation
can be divided into two groups: keyphrase assign-
ment and keyphrase extraction (Frank et al, 1999).
In keyphrase assignment, all potential keyphrases
appear in a predefined vocabulary and the task is to
classify documents to different keyphrase classes.
In keyphrase extraction, keyphrases are supposed
to be available in the processed documents them-
selves, and the aim is to extract these most mean-
ingful words and phrases from the documents.
Most of the traditional methods for keyphrase
extraction are highly dependent on the language
used and the need for preprocessing is extensive,
e.g. including part-of-speech tagging, stemming,
and use of stop word lists and other language-
dependent filters.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1.1 Related Work
In the statistical keyphrase extraction, many vari-
ations for term frequency counts have been
proposed in the literature including relative
frequencies (Damerau, 1993), collection fre-
quency (Hulth, 2003), term frequency?inverse
document frequency (tf.idf) (Salton and Buck-
ley, 1988), among others. Additional fea-
tures to frequency that have been experimented
are e.g. relative position of the first occur-
rence of the term (Frank et al, 1999), impor-
tance of the sentence in which the term oc-
curs (HaCohen-Kerner, 2003), and widely stud-
ied part-of-speech tag patterns, e.g. Hulth (2003).
Matsuo and Ishizuka (2004) present keyword ex-
traction method using word co-occurrence statis-
tical information. Most of the presented methods
need a reference corpus or a training corpus to pro-
duce keyphrases. The reference corpus acts as a
sample of general language, whereas the training
corpus is used to tune the parameters of the sys-
tem.
Statistical keyphrase extraction methods with-
out reference corpora have also been proposed,
e.g. (Matsuo and Ishizuka, 2004; Bracewell et al,
2005). The later study is carried out for bilingual
corpus.
1.2 Reference Corpora
The reference corpus of natural language pro-
cessing systems acts as a sample of general lan-
guage. The corpus should be as large as possi-
ble to get sufficiently many examples of language
use. In our study, we used the Europarl corpus that
consists of transcriptions of European Parliament
speeches in eleven European languages, includ-
ing four Romance languages (Spanish, French,
Italian and Portuguese), five Germanic languages
83
(Danish, German, English, Dutch and Swedish),
Finnish and Greek (Koehn, 2005). The number
of words in the corpora is between 23 million in
Finnish and 38 million in French, while the num-
ber of word types differs from 98 thousand in En-
glish to 563 thousand in Finnish.
2 The Likey Method
We present a keyphrase extraction method Likey
that is an extension of Damerau?s method (Honkela
et al, 2007). In Damerau?s (1993) method, terms
are ranked according to the likelihood ratio and the
top m terms are used as index terms. Both sin-
gle words and bigrams are considered to be terms.
Likey produces keyphrases using relative ranks
of n-gram frequencies. It is a simple language-
independent method: The only language-specific
component is a reference corpus in the correspond-
ing language. Likey keyphrases may be single
words as well as longer phrases.
The preprocessing phase of Likey consists of ex-
traction of the main text body without captions of
figures and tables, and removing special characters
(except for some hyphens and commas). Numbers
are replaced with <NUM> tags.
An integer rank value is assigned to each phrase
according to its frequency of occurrence, where
the most frequent phrase has rank value one and
phrases with the same frequency are assigned the
same rank. Rank values rank
a
and rank
r
are cal-
culated from the text and the reference corpus, re-
spectively, for each phrase. Rank order rank is cal-
culated separately for each phrase length n. Thus
we get ranks from unity to max rank for each n.
This way n-gram frequencies for n ? 2 are scaled
to follow approximately the same distribution as
1-grams in the corpus. The ratio
ratio =
rank
a
rank
r
(1)
of ranks is used to compare the phrases.
In highly inflective languages, such as Finnish,
and languages with frequent word concatenation,
such as German, many of the phrases occurring in
the analysed document do not occur in the refer-
ence corpus. Thus, their ratio value is related to
the maximum rank value, according to Eq. 2,
ratio =
rank
a
max rank
r
+ 1
(2)
where max rank
r
is the maximum rank in the ref-
erence corpus. The ratios are sorted in increasing
order and the phrases with the lowest ratios are se-
lected as the extracted keyphrases. Phrases occur-
ring only once in the document cannot be selected
as keyphrases.
3 Evaluation
The most straightforward way to evaluate the ex-
tracted keyphrases is to first decide which phrases
are appropriate to the document and then calculate
how many of the extracted keyphrases belong to
the appropriate phrases set, e.g. by using precision
and recall measures.
There are two widely used approaches for defin-
ing the appropriate phrases for a document. The
first method is to use human evaluators for rat-
ing extracted keyphrases. The other approach is to
analyse documents that have author-provided key-
word lists. Each document has a list of keyphrases
which are easy to accept to be correct. Anyway,
automated keyphrase extraction methods are usu-
ally poor in predicting author-provided keyphrases
since many of the provided phrases do not exist
in the document at all but they are sort of super-
concepts.
3.1 Multilingual Approach
In our framework, there are keyphrases in 11
languages to be evaluated. Due to many prob-
lems related to human evaluation in such a con-
text, we needed a new way of evaluating the re-
sults of our language-independent keyphrase ex-
traction method. We took our evaluation data from
Wikipedia, a free multilingual online encyclope-
dia.1 We present a novel way to use Wikipedia ar-
ticles in evaluation of a multilingual keyphrase ex-
traction method. Wikipedia corpus has lately been
used as a resource for automatic keyword extrac-
tion for English (Mihalcea and Csomai, 2007) as
well as to many other tasks.
We suppose that those articles which are linked
from the article at hand and which link back to the
article, are potential keyphrases of the article. For
example, a Wikipedia article about some concept
may link to its higher-level concept. Likewise, the
higher-level concept may list all concepts includ-
ing to the group.
3.2 Evaluation Data
Finding Wikipedia articles of adequate extent in all
the languages is quite challenging, basically due
1http://wikipedia.org
84
to generally quite short articles in Greek, Finnish
and Danish. We gathered 10 articles that have suf-
ficient amount of content in each of the 11 Eu-
roparl languages. These 110 selected Wikipedia
articles were collected in March 2008 and their En-
glish names are Beer, Cell (biology), Che Guevara,
Leonardo da Vinci, Linux, Paul the Apostle, Sun,
Thailand, Vietnam War, and Wolfgang Amadeus
Mozart.
The average lengths of articles in Finnish, Dutch
and Swedish are below 2 000 words, the lengths
of articles in Portuguese, Greek and Danish are
around 3 000 words and the rest are between 5 000
and 7 000 words. The normalised lengths would
switch the order of the languages slightly.
Among the 67 links extracted from the En-
glish Wikipedia article Cell include phrases such
as adenosine triphosphate, amino acid, anabolism,
archaea, bacteria, binary fission, cell division, cell
envelope, cell membrane, and cell nucleus. The
extracted links serve as evaluation keyphrases for
the article.
4 Results
In our study, we extracted keyphrases of length
n = 1 . . . 4 words. Longer phrases than
four words did not occur in the keyphrase list
in our preliminary tests. As a baseline, the
state-of-the-art keyphrase extraction method tf.idf
keyphrases were extracted from the same material.
Tf.idf (Salton and Buckley, 1988) is another sim-
ple and non-parameterized language-independent
method that can be used for keyphrase extraction.
For tf.idf we split the Europarl reference corpora
in ?documents? of 100 sentences and used the same
preprocessing that for Likey. To remove uninterest-
ing tf.idf-produced phrases like of the cell, a Likey-
like post processing was tried, and it gave slightly
better results. Thus the post processing is used for
all the reported results of tf.idf.
Generally, Likey produces longer phrases than
tf.idf. Each keyphrase list characterises the topic
quite well, and most of the extracted keyphrases
recur in every language. Both methods extracted a
French word re that is frequently used in the article
as an acronym for re?ticulum endoplasmique. The
same word in Dutch is extracted by tf.idf in a form
endoplasmatisch reticulum er.
We compared our Likey keyphrase extraction
method to the baseline method tf.idf by calculat-
ing precision and recall measures according to the
Wikipedia-based evaluation keyphrases for both
methods. We extracted 60 keyphrases from each
document for the first evaluation round and the
number of keyphrases available in the evaluation
keyphrase list for the document for the second
evaluation round. Precision and recall values of
both Likey and tf.idf evaluated with Wikipedia
intra-links are comparatively low (Table 1) but one
has to take into account the nature of the evalua-
tion set with notably varying number of ?correct
keyphrases?.
60 keyphrases N keyphrases
Method Prec. Recall Prec. Recall
Likey 0.1475 0.2470 0.1795 0.1795
tf.idf 0.1225 0.2203 0.1375 0.1375
tf.idf + p 0.1343 0.2341 0.1622 0.1622
Table 1: Average precisions and recalls for Likey,
tf.idf and tf.idf with post processing (p). N
keyphrases refers to the amount of evaluation
keyphrases available for each article.
The obtained precisions and recalls of the
first evaluation differed significantly between lan-
guages. In Figure 1, the precision and recall of
Likey and tf.idf with post processing for each lan-
guage is given. Within the 11 European languages,
English and German performed best according to
the precision (Likey: 23.0% and 22.8%, respec-
tively), but not that well according to the recall,
where best performed Dutch and Greek (Likey:
33.4% and 31.8%, respectively).
5 Conclusions and Discussion
In this paper, we have introduced Likey, a sta-
tistical keyphrase extraction method that is able
to analyse texts independently of the language in
question. In the experiments, we have focused
on European languages among which Greek and
Finnish differ considerably from Romance and
Germanic languages. Regardless of these differ-
ences, the method gave comparable results for
each language.
The method enables independence from the
language being analysed. It is possible to ex-
tract keyphrases from text in previously un-
known language provided that a suitable refer-
ence corpus is available. The method includes
only lightweight preprocessing, and no auxil-
iary language-dependent methods such as part-of-
speech tagging are required. No particular param-
85
el da fi sv nl pt es it de fr en
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
 
 
Likey
tf.idf
el da fi sv nl pt es it de fr en
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Figure 1: Average precisions (left-hand side) and recalls (right-hand side) of Likey and tf.idf with post
processing for each language. The number of extracted keyphrases is 60.
eter tuning is needed either. A web-based demon-
stration of Likey is available at http://cog.hut.
fi/likeydemo/ as well as more detailed infor-
mation on the method. The system highlights
keyphrases of a document written in one of eleven
languages.
Future research includes an extension of Likey in
which unsupervised detection of morphologically
motivated intra-word boundaries (Creutz, 2006) is
used. This extension could also handle languages
that have no white space between words. We also
plan to apply the method within statistical ma-
chine translation. A methodological comparison
of keyphrase-based dimension reduction and e.g.
PCA will also be conducted.
Acknowledgements
This work was supported by the Academy of Fin-
land through the Adaptive Informatics Research
Centre that is a part of the Finnish Centre of Ex-
cellence Programme. We warmly thank Jaakko
J. Va?yrynen and Sami Hanhija?rvi for their useful
comments and ideas.
References
Bracewell, David B., Fuji Ren, and Shingo Kuriowa.
2005. Multilingual single document keyword ex-
traction for information retrieval. In Proceedings of
NLP-KE?05.
Creutz, Mathias. 2006. Induction of the Morphol-
ogy of Natural Language: Unsupervised Morpheme
Segmentation with Application to Automatic Speech
Recognition. Ph.D. thesis, Helsinki University of
Technology.
Damerau, Fred. 1993. Generating and evaluating
domain-oriented multi-word terms from text. In-
formation Processing and Management, 29(4):433?
447.
Frank, Eibe, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of IJCAI?99, pages 668?673.
HaCohen-Kerner, Yaakov. 2003. Automatic extrac-
tion of keywords from abstracts. In Palade, V., R.J.
Howlett, and L.C. Jain, editors, KES 2003, LNAI
2773, pages 843?849. Springer-Verlag.
Honkela, Timo, Matti Po?lla?, Mari-Sanna Paukkeri, Ilari
Nieminen, and Jaakko J. Va?yrynen. 2007. Termi-
nology extraction based on reference corpora. Tech-
nical Report E12, Helsinki University of Technol-
ogy, Laboratory of Computer and Information Sci-
ence, Espoo. Unpublished.
Hulth, Anette. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 216?223.
Koehn, Philipp. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Matsuo, Yutaka and Mitsuru Ishizuka. 2004. Keyword
extraction from a single document using word co-
occurrence statistical information. Int?l Journal on
Artificial Intelligence Tools, 13(1):157?169.
Mihalcea, Rada and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
CIKM ?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowl-
edge management, pages 233?242, New York, NY,
USA. ACM.
Salton, G. and C. Buckley. 1988. Term weighting
approaches in automatic text retrieval. Information
Processing and Management, 24(5):513?523.
86
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 123?130,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Speech to speech machine translation:
Biblical chatter from Finnish to English
David Ellis
Brown University
Providence, RI 02912
Mathias Creutz Timo Honkela
Helsinki University of Technology
FIN-02015 TKK, Finland
Mikko Kurimo
Abstract
Speech-to-speech machine translation is in
some ways the peak of natural language pro-
cessing, in that it deals directly with our
original, oral mode of communication (as
opposed to derived written language). As
such, it presents challenges that are not to be
taken lightly. Although existing technology
covers each of the steps in the process, from
speech recognition to synthesis, deriving a
model of translation that is effective in the
domain of spoken language is an interesting
and challenging task. If we could teach our
algorithms to learn as children acquire lan-
guage, the result would be useful both for
language technology and cognitive science.
We propose several potential approaches, an
implementation of a multi-path model that
translates recognized morphemes alongside
words, and a web-interface to test our speech
translation tool as trained for Finnish to En-
glish. We also discuss current approaches to
machine translation and the problems they
face in adapting simultaneously to morpho-
logically rich languages and to the spoken
modality.
1 Introduction
Effective and fluent machine translation poses many
challenges, and often requires a variety of resources.
Some are language-specific, some domain-specific,
and others manage to be relatively independent (one
might even say context-free), and thus generally ap-
plicable in a wide variety of circumstances. There
are still untapped resources, however, that might
benefit machine translation systems. Most statistical
approaches do not take into account any similarities
in word forms, so words that share a common root,
(like ?blanche? and ?bianca?, meaning ?white? in
French and Italian respectively) are no more likely to
be aligned than others (like ?vache? and ?guardare?,
meaning ?cow? and ?to watch? respectively). Such
a root is sometimes subject to vowel shift and conso-
nant gradation, and may not be reflected in orthog-
raphy, since it is often purely phonetic.
This means we are not taking advantage of every-
thing that normally benefits human speakers, hear-
ers and translators. It may be that a more natural
approach to translation would first involve under-
standing of the input, stored in some mental rep-
resentation (an interlingua), and then generation of
an equivalent phrase in the target language, directly
from the knowledge sources.
In order to allow for more dramatic differences
in grammar like agglutinativity, it seems that the
statistical machine translation (SMT) system must
be more aware of sub-word units (morphemes) and
features (phonetic similarity). This general sort of
morphological approach could potentially benefit
any language pair, but might be crucial for a sys-
tem that handles Finnish, Turkish, Hungarian, Es-
tonian or any other highly inflectional language. In
the following section we discuss the confounds pre-
sented by agglutinative languages, and how aware-
ness of morphemes might improve the situation.
This is followed by a brief foray into semantics
and natural language generation as a component of
123
SMT. Capturing phonetic features is most applicable
to speech-to-speech translation, which will be dis-
cussed in the penultimate section. A description of
the Bible conversation experiment and some of its
results can be found in the final section.
2 Agglutinative Confounds
Traditional n-gram language models and phrase-
based translation models do not work terribly well
for Finnish because each lexical item can appear in
dozens of inflected or declined forms. If an SMT
system is presented with ?taloosi? (to your house), it
will not know if that is another form of a word it saw
in training (like ?taloissaan?, in their houses). Align-
ment data are thus unnaturally sparse and test sen-
tences often contain several unknown items, which
share their stems with trained words. It has been
assumed that morphological analysis would be es-
sential for handling agglutinative languages. How-
ever, although several effective segmenters and an-
alyzers for specific languages exist, and even unsu-
pervised language-neutral versions such as Morfes-
sor (Creutz and Lagus, 2007), only recently have
similar approaches been successfully used in the
context of machine translation to improve the BLEU
score (Oflazer and El-Kahlout, 2007), and none yet
in Finnish.
In our experience, building a translation model
through stemmed (truncated) word-alignment out-
performs full-form alignment, or any morph-
segmented alignment. But once one has generated
such a translation model, including phrase tables
where stemmed forms (keys in source language)
are associated with full forms (values in target lan-
guage), is there anything to be gained from induction
of morphology? Our research in this area has yet to
reveal any positive results, but we are still working
on it. It is also worth considering the effectiveness of
the evaluation metrics. Does BLEU accurately cap-
ture the accuracy of a translation, particularly in an
agglutinative language? Unfortunately not.
We think the word segmentation in the BLEU
metric is biased against progress in morpheme-level
translation. Some other metrics have been set forth,
but none is widely accepted, in part due to inertia,
but also because translation cannot be objectively
evaluated, unless both the communicative intent and
its effectiveness can be quantified. The same prob-
lem occurs for teachers grading essays ? what was
the student intending to convey, was the phrasing
correct, the argument sound, and where does all this
diverge from the underlying power of words, written
or well said, to transmit information? Translation is
an art, and maybe in addition to human evaluation
by linguists and native speakers of the language, we
should consider the equivalent of an art or literary
critic. On the other hand, that might only be worth-
while for poetry, wherein automated translation is
perhaps not the best approach.
One might think that the stemmed model de-
scribed above would lose track of closed-class func-
tion items (like prepositions), particularly when they
are represented as inflectional morphemes in one
language but as separate words in the other. How-
ever, it seems that the language model for the target
takes care of that quite well in most cases. There
are some languages (like Japanese) with underspec-
ified noun phrases, in which efforts to preserve def-
initeness (i.e., the book, kirjan; a book, kirjaa) seem
futile, but given the abundance of monolingual data
to train LM?s on, these are contextually inferred and
corrected at the tail end of the production line. Ag-
glutinative confounds are thus very closely related to
other issues found throughout machine translation,
and perhaps an integrated solution (including a new
evaluation metric) is necessary.
3 Knowledge-Based Approaches
Incorporating statistical natural language generation
into a machine translation system involves some
modifications to the above. First, the source lan-
guage is translated or parsed into ontological rep-
resentations. This is similar to sentence parsing
techniques that can be used to induce a context-free
grammar for a language (Charniak, 1997), and could
in fact be considered one of their more useful appli-
cations. The parsing generally depends on a proba-
bilistic model trained on sentences aligned with their
syntactic and semantic representations, often in a
tree that could be generated by a context-free gram-
mar. The resulting semantic representation can then
be used as the source of a target-language generation
process.
The algorithm that generates such a representa-
124
tion from raw input could be trained on a tree-
bank, and an annotated form of the same corpus
(where the derivations in the generation space are
associated with counts for each decision made) can
be used to train the output component to generate
language. (Belz, 2005) To incorporate the statisti-
cal component, which allows for robust generaliza-
tion, per (Knight and Hatzivassiloglou, 1995), the
NLG on the target side is filtered through a language
model (described above). This helps address many
of the knowledge gap problems introduced by lin-
guistic differences or in a component of the system
- the analyzer or generator.
This approach does have significant advantages,
particularly in that it is more focused on semantics
(as opposed to statistical cooccurrence), so it may
be less likely to distort meaning. On the other hand,
it could misinterpret or miscommunicate (or both),
just like a human translator. Perhaps the crucial dif-
ference is that, while machine learning often has lit-
tle to do with our understanding of cognitive pro-
cesses, this sort of machine translation has greater
potential for illuminating mysterious areas of the hu-
man process. It is not an ersatz brain, nor neural
network, but in many ways it has more in common
with those technologies (particularly in that they
model cognition) than many natural language pro-
cessing algorithms. That is because, if we can get
a semantically-aware machine translation system to
work, it may more closely mirror human cognition.
Humans certainly do not ignore meaning when they
translate, but today?s statistical machine translation
has no awareness of it at all.
Potential disadvantages of the system include its
dependence on more resources. However, this is
less of a problem with WordNet(Miller, 1995) and
other such semantic webs. It is also worth men-
tioning again that humans always have an incred-
ible amount of information at their disposal when
translating. Not only all of their past experience and
word-knowledge, but their interlocutor?s demeanor,
manner, intonation, facial expressions, gestures, and
so on. There are often things that would be obvi-
ous in the context of a conversation, but are missing
from the transcribed text. For instance, the referent
of many pronouns is ambiguous, but usually there is
a unique individual or item picked out by the speak-
ers? shared information. This is true for simple sen-
tences like ?He hit him,? which are normally dis-
ambiguated by conversational context, but a purely
statistical, pseudo-syntactic interpretation would get
little of the meaning a human would glean from that
utterance.
4 Spoken Features
Speech-to-speech machine translation is in some
ways the peak of natural language processing, in that
it deals directly with our (humans?) original, oral
mode of communication (as opposed to derived writ-
ten language). As such, it presents challenges that
are not to be taken lightly. Much of the pipeline in-
volved is at least relatively straightforward: acoustic
modeling and language modeling on the input side
can take advantage of the latest advances without
extensive adaptation; similarly, speech synthesis on
the output can be directly connected with the system
(i.e., not work with text output, but a richer repre-
sentation).
Although such a system might seem quite com-
plicated, it could better take advantage of all the
available data. Natural language understanding and
generation could even be incorporated to an extent,
perhaps to add further confidence measures based
on semantic equivalence. Designing it in this way
also allows for a variety of methods to be tried with
ease, in a modular fashion. It may be that yet an-
other source of information can be found to improve
the translation by adding features to the translation
model ? perhaps leveraging multilingual corpora in
other languages, segmenting into morphemes earlier
in the process, or even incorporating intonation in
some fashion. Weights for all such features could
be learned during training, such that no language-
specific tuning would be necessary. This framework
would certainly not make speech-to-speech transla-
tion simple, but its flexibility might make research
and improvement in this area more tractable.
Efficiency is crucial in online translation of con-
versation, so a word alignment model with collapsed
Gibbs sampling, rather than EM, at its core is worth
experimenting with. We have written up a bare-
bones IBM Model 1 in both C++ and Python, us-
ing the standard EM approach and a Gibbs sampling
one. The latter allows for optimizations using lin-
ear algebra, and although it does not quite match the
125
perplexity or log-likelihood achieved by EM, it is
significantly faster, particularly on longer sentences.
Since morpheme segmentation is at least somewhat
helpful in speech recognition (Creutz, 2006; Creutz
et al, 2007), it should still be considered a potential
component in speech-to-speech translation. In terms
of incorporating the knowledge-based approach into
such a system, we think it may yet be too early,
but if existing understanding-and-generation frame-
works for machine translation could be adapted to
this use, it could be very fruitful, in particular since
spoken language generation might be more effective
from a knowledge base, since it would know what
it was trying to say, instead of relying on statistics
alone, hoping the phonemes end up in a meaningful
order.
The critical step of SST is, of course, transla-
tion. In an integrated system, as described above,
the translation model could be trained on a parallel
spoken corpus (perhaps tokenized into phonemes, or
segmented into morphemes), since there might be
advantages to limiting the intermediate steps in the
process. The Bible is a massively multilingual publi-
cation, and as it happens, its text is available aligned
between Finnish and English, and it is possible to
find corresponding recordings in both languages.
So, this corpus would enable a direct approach
to speech-to-speech translation. Alternatively, one
could treat the speech recognition and synthesis as
distinct from the translation, in which case text cor-
pora corresponding to the style and genre of speech
would be necessary. This would be particularly fea-
sible when, for instance, translating UN parliamen-
tary proceedings from a recording, for which trans-
lated transcripts are readily available. For a more
general and robust solution, we might advocate a
combined approach, in the hope that some potential
weaknesses of one might be avoided or compensated
for by using whatever limited resources are available
to add features from the other. Thus, a direct trans-
lation from speech to speech could be informed, in a
sense, by a derived translation from the recognized
text.
5 Biblical Chatter
Here, we present a system for translating Finnish to
English speech, in a restricted and ancient domain:
the Bible.
5.1 Introduction
Speech to speech translation attacks a variety of
problems at once, from speech recognition to syn-
thesis, and can similarly be used for several pur-
poses. If a system is efficient enough to be used
without introducing significant delay, it can trans-
late conversational speech online, acting as an in-
terpreter in place of (or in cooperation with) a hu-
man professional. On the other hand, a slow speech
translation system is still useful because it can make
news broadcasts (radio or television) accessible to
wider audiences through offline multilingual dub-
bing, allowing international viewers to enjoy a de-
layed broadcast.
5.2 System Description
The domain selected for our experiments was heav-
ily influenced by the available data. We needed a
bilingual (Finnish and English) and bimodal (text
and speech) corpus, and unfortunately none is read-
ily available, but we put one together using the
Bible. Both Old and New Testaments were used,
with one book from each left out for testing pur-
poses. We used multiple editions of the Bible to
train the translation model: the American Standard
Version (first published in 1901, updated 1997),
and Finnish translations (from 1992 and 1933,38).
The spoken recordings used were the World English
Bible (1997) and Finnish Bible (Raamattu) readings
(recorded at TKK 2004).
Our approach was to use existing components,
and try weaving them together in an optimal way.
First, there is the open vocabulary automatic speech
recognition (ASR) task, where the goal is to de-
tect phonemes in an acoustic signal and map them
to words. Here, we use an ?unlimited vocabu-
lary? continuous speech recognizer (Hirsima?ki et al,
2006), trained on a multi-speaker Finnish acoustic
model with a varigram (Siivola et al, 2007) lan-
guage model that includes Bible n-grams. Then,
for translation, Moses (Koehn et al, 2007) is trained
on words and morphemes (derived from Morfessor
Baseline (Creutz and Lagus, 2005)). For speech syn-
thesis, we used Festival (Taylor, 1999), including the
built-in English voice and a Finnish voice developed
at Helsinki University.
126
5.3 Results
The following is an example fragment, taken from
the test corpus.
Niin Daavid meni David slept with his
lepoon isiensa? luo, fathers, and was
ja ha?nethaudattiin buried in the
Daavidin kaupunkiin. city of David. The days
Nelja?kymmenta? vuotta that David reigned
ha?n oli ollut over Israel were
Israelin kuninkaana. forty years; seven
Hebronissa ha?n years reigned he
hallitsi seitsema?n in Hebron, and
vuotta, Jerusalemissa thirty-three years
kolmenkymmenenkolmen reigned he
vuoden ajan. in Jerusalem.
Salomo nousi Solomon sat on
isa?nsa? Daavidin the throne of David
valtaistuimelle,ja his father; and
ha?nen kuninkuutensa his kingdom was
vahvistui lujaksi. established greatly.
A translation of the reference text skips recogni-
tion, and runs the system from translation to synthe-
sis. The following shows how the sample text was
translated by our system (BLEU = 0.735):
Niin Daavid meni so david slept with his
lepoon isiensa? luo, fathers and was
ja ha?net haudattiin buried in the
Daavidin kaupunkiin. city of david
Nelja?kymmenta? vuotta forty years he
ha?n oli ollut was king over
Israelin kuninkaana. israel and in
Hebronissa ha?n hebron he reigned
hallitsi seitsema?n seven years
vuotta, Jerusalemissa in jerusalem
kolmenkymmenenkolmen thirty and three
vuoden ajan. years solomon
Salomo nousi went up to
isa?nsa? Daavidin the throne of
valtaistuimelle, ja david his father
ha?nen kuninkuutensa and his kingdom
vahvistui lujaksi. was strong for luja
The following recognized translation (BLEU =
0.541) represents a complete run of the system. The
recognition (on the left) had a LER of 12.9% and a
WER of 56.8%.
niintaa meni niintaa went
lepoon isiensa?lla isiensa?lla rest and was
ja ha?net haudattiin buried in the
daavidin kaupunkiin city of david the king
nelja?kymmenta? of israel was
vuotta ha?n oli ollut forty years he was
israelin kuninkaan in hebron he
hebronissa ha?n reigned seven years
hallitsi seitsema?n in jerusalem
vuotta jerusalemissa kymmenenkolmen
kolmen kymmenenkolmen three years
vuoden ajan after the new
salomon uusi on the throne of david
isa?nsa? daavidin and solomon
valtaistuimelle ja his father
ha?nenkuninkuutensa ha?nenkuninkuutensa
valmistulujaksi valmistulujaksi
Here we have an alternative path through the sys-
tem, which uses Morfessor on the recognized text,
and then translates using a model trained on the
morpheme-segmented corpus. This results in a re-
duced score (BLEU = 0.456), but fewer unknown
words.
n iin taa# meni# iin behind went to
lepo on# isi ensa? lla# the sabbath that
ja# ha?n et# hauda ttiin# is with ensa? and he
daavid in# kaupunki in# shall not at the grave of abner
nelja?kymmenta?# vuotta# was forty years of the
ha?n# oli# ollut# city of david and
israeli n kuninkaan# he was israeli to
hebron issa# ha?n# the king of hebron
hallitsi# seitsema?n# and he reigned
vuotta# jerusalem seven years in
issa# kolmen# jerusalem three tenth
kymmenen kolmen# three years of
vuoden# ajan# the new solomon his
salomo n# uusi# isa? istuim to david
nsa?# daavid in# my father of the
valta istuim elle# kingdoms of
ja# ha?n en kun ink ink and he
uutensa# valmistu luja ksi# uutensa valmistu to luja
The morphemes might have been more effective
in translation if they had been derived through rule-
based morphological analysis. Or, they could still be
statistical, but optimized for the translation phase by
minimizing perplexity during word alignment.
A significant barrier to thorough and concrete
evaluation of our system involves segmentation of
the speech stream into sentences (or verses) to match
the text. In the above examples, we manually
clipped the audio files. Evaluating performance on
the entire test set reduced the BLEU score if the
data were streamed through each component unseg-
mented. When the recognizer was set to insert a pe-
riod for detected pauses of a certain length, or at sen-
tence boundaries identified by its language model,
127
input to the translation phase became considerably
more problematic. In particular, the lattice input
ought to be split into sentences, but there would usu-
ally be a period in every time slice (but with low
probability).
5.4 Discussion
There were significant difficulties in the process,
particularly in the English to Finnish direction.
Whereas Finnish speech recognition is relatively
straightforward, since its orthography is consistent,
English speech recognition is more dependent on
a pronunciation dictionary. Although many such
dictionaries are available, and the pronunciation of
novel words can be estimated, neither of these re-
sources is terribly effective within the Bible domain,
where there are many archaic words and names. In
the second step, translation into Finnish is demon-
strably difficult from any source language, and re-
sults in consistently lower BLEU scores (Virpioja
et al, 2007). And although using morphemes can
reduce the frequency of unknown words, it also re-
duces the BLEU score.
It might improve translation quality if we use the
recognizer lattice as translator input, since acous-
tically improbable segments may lead to the most
fluent translation. Having access to many possibili-
ties might help the translation model, but then again,
second-guessing the recognizer might not be help-
ful. There were some difficulties with the Moses in-
tegration, in part because the word-sausage format
varies from SRILM?s. Also, the recognizer output
indicates word boundaries as <w>, not string-final
hash-marks (#). This is problematic since the for-
mer are separate symbols, occupying a node in the
lattice, whereas the latter are appended to another
symbol (e.g., ?<w> morph eme </w>?, 4 nodes,
versus ?morph eme#?, 2 nodes). Using the lattice,
final output from Moses tends to be more fluent,
but less on-topic, and often truncated. Although we
have no improvements thus far, it is likely that with
further parameter tuning, we could achieve better re-
sults. On the other hand, we seek a general, robust,
domain-independent solution, so focusing on Bible
translation may not be worthwhile.
Our speech-to-speech translation system is
accessible through a web interface.
http://www.cis.hut.fi/projects/speech/
translate/
It accepts a sound file, with recorded Finnish
bible-style chatter, an optional reference text and
translation, and within a half hour (usually much
less) sends a detailed report, including a sound file
with the synthesized English.
Ideas for future research include online speech-
to-speech translation, which must be efficient, light-
weight and robust. A potential barrier to this and
other applications is the lack of spoken language
training texts. It might be possible to adaptively train
to new speakers and contexts, perhaps taking advan-
tage of an efficient alternative to EM in word align-
ment (see discussion of Gibbs sampling). As men-
tioned elsewhere, it might be worth using prosodic
features captured during recognition as factors in
translation. Adapting existing resources to new lan-
guage pairs is particularly essential in an area where
so much is necessary, and so little available.
6 Conclusion
We cannot yet say for sure whether linguistic or
statistically optimized morphemes derived from text
corpora could be useful somehow in machine trans-
lation, but it has been demonstrated helpful in
speech recognition. Awareness of sub-word units
could benefit a speech-to-speech translation system,
and it may in fact help to maintain information
from the speech recognizer about morpheme seg-
mentation throughout the translation process, even
in speech generation. Incorporating natural lan-
guage understanding may also be fruitful, but for
compact, efficient systems (like a handheld transla-
tor) might not have access to the necessary resources
or computational power to support that. On the other
hand, it is our duty as researchers to stay ahead of the
technology and push its limits.
We are by no means the first to imagine this, but
perhaps we will soon be speaking into wrist watches
that understand our query, seemingly instantly shift
through more information than Google has currently
indexed, and reply in fluent English, Finnish, or Pun-
jabi with as much detail as could be hoped for after
hours of painstaking research with current technol-
ogy. In this case (and computational linguists must
always be optimistic), knowledge-based natural lan-
guage processing certainly has a crucial place.
128
Morphemes and agglutinative languages do pose
unique problems for computational linguists, but
many of the general techniques developed for lan-
guages like Arabic and Chinese, which are equally
far from English in grammar (and even orthogra-
phy), might surmount those problems without any
manual adaptation. Discriminative training of fea-
tures used in the translation model allows for such
solutions to be molded automatically to whatever
language pair (and set of corpora) they are being
used for. There is, as always, much more to be done
in this area, and we hope our research into efficient,
online Bible-conversational translation ? a modern
Babelfish in an ancient genre? is fruitful, and helps
to shed light on lemmatization.
Acknowledgments
Many thanks to Teemu Hirsima?ki, Antti Puurula,
Sami-Virpioja and Jaakko J. Va?yrynen for their
help with components of the system and for their
thoughts and comments at various stages of the
project.
References
Anja Belz. 2005. Statistical generation: Three methods
compared and evaluated. In Proceedings of the 10th
European Workshop on Natural Language Generation
(ENLG05), pages 15?23.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI/IAAI, pages 598?603.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical Re-
port A81, Publications in Computer and Information
Science, Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1), January.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkknen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Saraclar, and Andreas
Stolcke. 2007. Analysis of morph-based speech
recognition and the modeling of out-of-vocabulary
words across languages. In Proceedings of Hu-
man Language Technologies / The Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL 2007),
Rochester, NY, USA.
Mathias Creutz. 2006. Morfessor in the morpho chal-
lenge. In Mikko Kurimo, Mathias Creutz, and Krista
Lagus, editors, Proceedings of the PASCAL Challenge
Workshop on Unsupervised Segmentation of Words
into Morphemes, Venice, Italy.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkko?nen. 2006. Unlimited vocabu-
lary speech recognition with morph language models
applied to Finnish. Computer Speech and Language,
20(4):515?541.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, pages 252?260, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Ondrej Bojar, Alexandra Constantin, and Evan
Herb. 2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of the ACL 2007
Demo and Poster Sessions, pages 177?180.
George A. Miller. 1995. Wordnet: a lexical database for
English. Commun. ACM, 38(11):39?41.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages 25?
32.
Vesa Siivola, Teemu Hirsima?ki, and Sami Virpioja. 2007.
On growing and pruning Kneser-Ney smoothed n-
gram models. IEEE Transactions on Audio, Speech
and Language Processing, 15(5):1617?1624.
Paul Taylor. 1999. The Festival Speech Architecture.
Web page.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology?aware statis-
tical machine translation based on morphs induced in
an unsupervised manner. In Proceedings of the Ma-
chine Translation Summit XI, Copenhagen, Denmark.
To appear.
129
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
130
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 162?165,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Likey: Unsupervised Language-independent Keyphrase Extraction
Mari-Sanna Paukkeri and Timo Honkela
Adaptive Informatics Research Centre
Aalto University School of Science and Technology
P.O. Box 15400, FI-00076 AALTO, Finland
mari-sanna.paukkeri@tkk.fi
Abstract
Likey is an unsupervised statistical ap-
proach for keyphrase extraction. The
method is language-independent and the
only language-dependent component is
the reference corpus with which the doc-
uments to be analyzed are compared.
In this study, we have also used an-
other language-dependent component: an
English-specific Porter stemmer as a pre-
processing step. In our experiments
of keyphrase extraction from scientific
articles, the Likey method outperforms
both supervised and unsupervised baseline
methods.
1 Introduction
Keyphrase extraction is a natural language pro-
cessing task for collecting the main topics of a
document into a list of phrases. Keyphrases are
supposed to be available in the processed docu-
ments themselves, and the aim is to extract these
most meaningful words and phrases from the doc-
uments. Keyphrase extraction summarises the
content of a document as few phrases and thus
provides a quick way to find out what the docu-
ment is about. Keyphrase extraction is a basic text
mining procedure that can be used as a ground
for other, more sophisticated text analysis meth-
ods. Automatically extracted keyphrases may be
used to improve the performance of information
retrieval, automatic user model generation, docu-
ment collection clustering and visualisation, sum-
marisation and question-answering, among others.
This article describes the participation of the
Likey method in the Task 5 of the SemEval 2010
challenge, automatic keyphrase extraction from
scientific articles (Kim et al, 2010).
1.1 Related work
In statistical keyphrase extraction, many variations
for term frequency counts have been proposed in
the literature including relative frequencies (Dam-
erau, 1993), collection frequency (Hulth, 2003),
term frequency?inverse document frequency (tf-
idf) (Salton and Buckley, 1988), among others.
Additional features to frequency that have been
experimented are e.g., relative position of the first
occurrence of the term (Frank et al, 1999), im-
portance of the sentence in which the term oc-
curs (HaCohen-Kerner, 2003), and widely stud-
ied part-of-speech tag patterns, e.g. Hulth (2003).
Matsuo and Ishizuka (2004) present keyword ex-
traction method using word co-occurrence statis-
tics. An unsupervised keyphrase extraction
method by Liu et al (2009) uses clustering to find
exemplar terms that are then used for keyphrase
extraction. Most of the presented methods require
a reference corpus or a training corpus to produce
keyphrases. Statistical keyphrase extraction meth-
ods without reference corpora have also been pro-
posed, e.g. (Matsuo and Ishizuka, 2004; Bracewell
et al, 2005). The later study is carried out for
bilingual corpus.
2 Data
The data used in this work are from the SemEval
2010 challenge Task 5, automatic keyphrase ex-
traction from scientific articles. The data consist
of train, trial, and test data sets. The number of
scientific articles and the total number of word to-
kens in each of the original data sets (before pre-
processing) are given in Table 1.
Three sets of ?correct? keyphrases are pro-
vided for each article in each data set: reader-
assigned keyphrases, author-provided keyphrases,
and a combination of them. All reader-assigned
keyphrases have been extracted manually from
the papers whereas some of author-provided
162
Data set Articles Word tokens
train 144 1 159 015
trial 40 334 379
test 100 798 049
Table 1: Number of scientific articles and total
number of word tokens in the data sets.
keyphrases may not occur in the content. The
numbers of correct keyphrases in each data set are
shown in Table 2.
Data set Reader Author Combined
train 1 824 559 2 223
trial 526 149 621
test 1 204 387 1 466
Table 2: Number of correct answers in reader, au-
thor, and combined answer sets for each data set.
More detailed information on the data set can
be found in (Kim et al, 2010).
3 Methods
Likey keyphrase extraction approach comes
from the tradition of statistical machine learn-
ing (Paukkeri et al, 2008). The method has
been developed to be as language-independent as
possible. The only language-specific component
needed is a corpus in each language. This kind
of data is readily available online or from other
sources.
Likey selects the words and phrases that best
crystallize the meaning of the documents by com-
paring ranks of frequencies in the documents to
those in the reference corpus. The Likey ra-
tio (Paukkeri et al, 2008) for each phrase is de-
fined as
L(p, d) =
rank
d
(p)
rank
r
(p)
, (1)
where rank
d
(p) is the rank value of phrase p in
document d and rank
r
(p) is the rank value of
phrase p in the reference corpus. The rank val-
ues are calculated according to the frequencies of
phrases of the same length n. If the phrase p does
not exist in the reference corpus, the value of the
maximum rank for phrases of length n is used:
rank
r
(p) = max rank
r
(n) + 1. The Likey ra-
tio orders the phrases in a document in such a way
that the phrases that have the smallest ratio are the
best candidates for being a keyphrase.
As a post-processing step, the phrases of length
n > 1 face an extra removal process: if one of
the words composing the phrase has a rank of less
than a threshold ? in the reference corpus, the
phrase is removed from the keyphrase list. This
procedure excludes phrases that contain function
words such as ?of? or ?the?. As another post-
processing step, phrases that are subphrases of
those that have occurred earlier on the keyphrase
list are removed, excluding e.g. ?language model?
if ?unigram language model? has been already ac-
cepted as a keyphrase.
3.1 Reference corpus
Likey needs a reference corpus that is seen as a
sample of the general language. In the present
study, we use a combination of the English part of
Europarl, European Parliament plenary speeches
(Koehn, 2005) and the preprocessed training set as
the reference corpus. All XML tags of meta infor-
mation are excluded from the Europarl data. The
size of the Europarl corpus is 35 800 000 words
after removal of XML tags.
3.2 Preprocessing
The scientific articles are preprocessed by remov-
ing all headers including the names and addresses
of the authors. Also the reference section is re-
moved from the articles, as well as all tables, fig-
ures, equations and citations. Both scientific arti-
cles and the Europarl data is lowercased, punctua-
tion is removed (the hyphens surrounded by word
characters and apostrophes are kept) and the num-
bers are changed to <NUM> tag.
The data is stemmed with English Porter stem-
mer implementation provided by the challenge or-
ganizers, which differs from our earlier experi-
ments.
3.3 Baselines
We use three baseline methods for keyphrase ex-
traction. The baselines use uni-, bi-, and trigrams
as candidates of keyphrases with tf-idf weight-
ing scheme. One of the baselines is unsuper-
vised and the other two are supervised approaches.
The unsupervised method is to rank the candidates
according to their tf-idf scores. The supervised
methods are Na??ve Bayes (NB) and Maximum En-
tropy (ME) implementations from WEKA pack-
age1.
1http://www.cs.waikato.ac.nz/
?
ml/weka/
163
4 Experiments
We participated the challenge with Likey results of
three different parameter settings. The settings are
given in Table 3. Likey-1 has phrases up to 3 words
and Likey-2 and Likey-3 up to 4 words. The thresh-
old value for postprocessing was selected against
the trial set, with ? = 100 performing best. It
is used for Likey-1 and Likey-2. Also a bit larger
threshold ? = 130 was tried for Likey-3 to exclude
more function words.
Repr. n ?
Likey-1 1?3 100
Likey-2 1?4 100
Likey-3 1?4 130
Table 3: Different parametrizations for Likey: n-
gram length and threshold value ?.
An example of the resulting keyphrases ex-
tracted by Likey-1 from the first scientific arti-
cle in the test set (article C-1) is given in Ta-
ble 4. Also the corresponding ?correct? answers in
reader-assigned and author-provided answer sets
are shown. The keyphrases are given in stemmed
versions. Likey keyphrases that can be found in the
reader or author answer sets are emphasized.
Likey-1 uddi registri, proxi registri, servic
discoveri, grid servic discoveri, uddi kei, uniqu
uddi kei, servic discoveri mechan, distribut
hash tabl, web servic, dht, servic name, web
servic discoveri, local proxi registri, local uddi
registri, queri multipl registri
Reader grid servic discoveri, uddi, distribut
web-servic discoveri architectur, dht base uddi
registri hierarchi, deploy issu, bamboo dht
code, case-insensit search, queri, longest avail
prefix, qo-base servic discoveri, autonom
control, uddi registri, scalabl issu, soft state
Author uddi, dht, web servic, grid comput,
md, discoveri
Table 4: Extracted keyphrases by Likey-1 from ar-
ticle C-1 and the corresponding correct answers in
reader and author answer sets.
The example shows clearly that many of the ex-
tracted keyphrases contain the same words that
can be found in the correct answer sets but the
length of the phrases vary and thus they cannot be
counted as successfully extracted keyphrases.
The results for the three different Likey
parametrizations and the three baselines are given
in Table 5 for reader-assigned keyphrases and Ta-
ble 6 for the combined set of reader and author-
assigned keyphrases. The evaluation is conducted
by calculating precision (P), recall (R) and F-
measure (F) for top 5, 10, and 15 keyphrase candi-
dates for each method, using the reader-assigned
and author-provided lists as correct answers. The
baseline methods are unsupervised tf-idf and su-
pervised Na??ve Bayes (NB) and Maximum Entropy
(ME).
Likey-1 performed best in the competition and
is thus selected as the official result of Likey in the
task. Anyway, all Likey parametrizations outper-
form the baselines, Likey-1 having the best pre-
cision 24.60% for top-5 candidates in the reader
data set and 29.20% for top-5 candidates in the
combined data set. The best F-measure is obtained
with Likey-1 for top-10 candidates for both reader
and combined data set: 16.24% and 17.11%,
respectively. Likey seems to produce the best
keyphrases in the beginning of the keyphrase list:
for reader-assigned keyphrases the top 5 keyphrase
precision for Likey-1 is 6.8 points better than
the best-performing baseline tf-idf and the cor-
responding F-measure is 4.0 points better. For
the combined set, the numbers are 7.2 and 3.7
points, respectively. The difference decreases for
the larger keyphrase sets.
5 Conclusions and discussion
This article describes our submission to SemEval
2010 Task 5, keyphrase extraction from scien-
tific articles. Our unsupervised and language-
independent method Likey uses reference corpus
and is able to outperform both the unsupervised
and supervised baseline methods. The best results
are obtained with the top-5 keyphrases: precision
of 24.60% with reader-assigned keyphrases and
29.20% with the combination of reader-assigned
and author-provided keyphrases.
There are some keyphrases in the answer sets
that our method does not find: due to the com-
paratively large threshold value ? many phrases
that contain function words, e.g. ?of?, cannot be
found. We also extract keyphrases of maximum
length of three or four words and thus cannot find
keyphrases longer than that. The next step of this
research would be to take these problems into ac-
count.
164
Method Top 5 candidates Top 10 candidates Top 15 candidatesP % R % F % P % R % F % P % R % F %
Likey-1 24.60 10.22 14.44 17.90 14.87 16.24 13.80 17.19 15.31
Likey-2 23.80 9.88 13.96 16.90 14.04 15.34 13.40 16.69 14.87
Likey-3 23.40 9.72 13.73 16.80 13.95 15.24 13.73 17.11 15.23
tf-idf 17.80 7.39 10.44 13.90 11.54 12.61 11.60 14.45 12.87
NB 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65
ME 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65
Table 5: Results for Likey and the baselines for the reader data set. The best precision (P), recall (R) and
F-measure (F) are highlighted.
Method Top 5 candidates Top 10 candidates Top 15 candidatesP % R % F % P % R % F % P % R % F %
Likey-1 29.20 9.96 14.85 21.10 14.39 17.11 16.33 16.71 16.52
Likey-2 28.40 9.69 14.45 19.90 13.57 16.14 15.73 16.10 15.91
Likey-3 28.00 9.55 14.24 19.60 13.37 15.90 16.07 16.44 16.25
tf-idf 22.00 7.50 11.19 17.70 12.07 14.35 14.93 15.28 15.10
NB 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70
ME 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70
Table 6: Results for Likey and the baselines for the combined (reader+author) data set. The best precision
(P), recall (R) and F-measure (F) are highlighted.
Acknowledgements
This work was supported by the Finnish Graduate
School in Language Studies (Langnet) funded by
Ministry of Education of Finland.
References
David B. Bracewell, Fuji Ren, and Shingo Kuriowa.
2005. Multilingual single document keyword ex-
traction for information retrieval. In Proceedings of
NLP-KE?05.
Fred Damerau. 1993. Generating and evaluating
domain-oriented multi-word terms from text. In-
formation Processing and Management, 29(4):433?
447.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of IJCAI?99, pages 668?673.
Yaakov HaCohen-Kerner. 2003. Automatic extrac-
tion of keywords from abstracts. In V. Palade, R.J.
Howlett, and L.C. Jain, editors, KES 2003, LNAI
2773, pages 843?849. Springer-Verlag.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 216?223.
Su Nam Kim, Alyona Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the ACL 2010 Workshop on
Evaluation Exercises on Semantic Evaluation (Se-
mEval 2010). to appear.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257?266, Singapore, Au-
gust. Association for Computational Linguistics.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157?
169.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti Po?lla?,
and Timo Honkela. 2008. A language-independent
approach to keyphrase extraction and evaluation. In
Coling 2008: Companion volume: Posters, pages
83?86, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Gerard Salton and Chris Buckley. 1988. Term weight-
ing approaches in automatic text retrieval. Informa-
tion Processing and Management, 24(5):513?523.
165
