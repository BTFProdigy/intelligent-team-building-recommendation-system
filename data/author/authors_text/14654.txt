From image descriptions to visual denotations:
New similarity metrics for semantic inference over event descriptions
Peter Young Alice Lai Micah Hodosh Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
{pyoung2, aylai2, mhodosh2, juliahmr}@illinois.edu
Abstract
We propose to use the visual denotations of
linguistic expressions (i.e. the set of images
they describe) to define novel denotational
similarity metrics, which we show to be at
least as beneficial as distributional similarities
for two tasks that require semantic inference.
To compute these denotational similarities, we
construct a denotation graph, i.e. a subsump-
tion hierarchy over constituents and their de-
notations, based on a large corpus of 30K im-
ages and 150K descriptive captions.
1 Introduction
The ability to draw inferences from text is a prereq-
uisite for language understanding. These inferences
are what makes it possible for even brief descrip-
tions of everyday scenes to evoke rich mental im-
ages. For example, we would expect an image of
people shopping in a supermarket to depict aisles
of produce or other goods, and we would expect
most of these people to be customers who are either
standing or walking around. But such inferences
require a great deal of commonsense world knowl-
edge. Standard distributional approaches to lexical
similarity (Section 2.1) are very effective at iden-
tifying which words are related to the same topic,
and can provide useful features for systems that per-
form semantic inferences (Mirkin et al., 2009), but
are not suited to capture precise entailments between
complex expressions. In this paper, we propose a
novel approach for the automatic acquisition of de-
notational similarities between descriptions of ev-
eryday situations (Section 2). We define the (visual)
denotation of a linguistic expression as the set of im-
ages it describes. We create a corpus of images of
everyday activities (each paired with multiple cap-
tions; Section 3) to construct a large scale visual de-
notation graph which associates image descriptions
with their denotations (Section 4). The algorithm
that constructs the denotation graph uses purely syn-
tactic and lexical rules to produce simpler captions
(which have a larger denotation). But since each
image is originally associated with several captions,
the graph can also capture similarities between syn-
tactically and lexically unrelated descriptions. We
apply these similarities to two different tasks (Sec-
tions 6 and 7): an approximate entailment recogni-
tion task for our domain, where the goal is to decide
whether the hypothesis (a brief image caption) refers
to the same image as the premises (four longer cap-
tions), and the recently introduced Semantic Textual
Similarity task (Agirre et al., 2012), which can be
viewed as a graded (rather than binary) version of
paraphrase detection. Both tasks require semantic
inference, and our results indicate that denotational
similarities are at least as effective as standard ap-
proaches to similarity. Our code and data set, as
well as the denotation graph itself and the lexical
similarities we define over it are available for re-
search purposes at http://nlp.cs.illinois.edu/
Denotation.html.
2 Towards Denotational Similarities
2.1 Distributional Similarities
The distributional hypothesis posits that linguistic
expressions that appear in similar contexts have a
67
Transactions of the Association for Computational Linguistics, 2 (2014) 67?78. Action Editor: Lillian Lee.
Submitted 6/2013; Revised 10/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
Gray haired man in black suit and yellow tie working in a financial environment.
A graying man in a suit is perplexed at a business meeting.
A businessman in a yellow tie gives a frustrated look.
A man in a yellow tie is rubbing the back of his neck.
A man with a yellow tie looks concerned.
A butcher cutting an animal to sell.
A green-shirted man with a butcher?s apron uses a knife to carve out the hanging carcass of a cow.
A man at work, butchering a cow.
A man in a green t-shirt and long tan apron hacks apart the carcass of a cow
while another man hoses away the blood.
Two men work in a butcher shop; one cuts the meat from a butchered cow, while the other hoses the floor.
Figure 1: Two images from our data set and their five captions
similar meaning (Harris, 1954). This has led to the
definition of vector-based distributional similarities,
which represent each word w as a vector w derived
from counts of w?s co-occurrence with other words.
These vectors can be used directly to compute the
lexical similarities of words, either via the cosine
of the angle between them, or via other, more com-
plex metrics (Lin, 1998). More recently, asymmetric
similarities have been proposed as more suitable for
semantic inference tasks such as entailment (Weeds
and Weir, 2003; Szpektor and Dagan, 2008; Clarke,
2009; Kotlerman et al., 2010). Distributional word
vectors can also be used to define the compositional
similarity of longer strings (Mitchell and Lapata,
2010). To compute the similarity of two strings, the
lexical vectors of the words in each string are first
combined into a single vector (e.g. by element-wise
addition or multiplication), and then an appropriate
vector similarity (e.g. cosine) is applied to the re-
sulting pair of vectors.
2.2 Visual Denotations
Our approach is inspired by truth-conditional se-
mantic theories in which the denotation of a declar-
ative sentence is assumed to be the set of all situa-
tions or possible worlds in which the sentence is true
(Montague, 1974; Dowty et al., 1981; Barwise and
Perry, 1980). Restricting our attention to visually
descriptive sentences, i.e. non-negative, episodic
(Carlson, 2005) sentences that can be used to de-
scribe an image (Figure 1), we propose to instantiate
the abstract notions of possible worlds or situations
with concrete sets of images. The interpretation
function J?K maps sentences to their visual denota-
tions JsK, which is the set of images i ? Us ? U in
a ?universe? of images U that s describes:
JsK = {i ? U | s is a truthful description of i} (1)
Similarly, we map nouns and noun phrases to the
set of images that depict the objects they describe,
and verbs and verb phrases to the set of images that
depict the events they describe.
2.3 Denotation Graphs
Denotations induce a partial ordering over descrip-
tions: if s (e.g. ?a poodle runs on the beach?) en-
tails a description s? (e.g. ?a dog runs?), its denota-
tion is a subset of the denotation of s? (JsK ? Js?K),
and we say that s? subsumes the more specific s
(s? v s). In our domain of descriptive sentences,
we can obtain more generic descriptions by simple
syntactic and lexical operations ? ? O ? S ? S
that preserve upward entailment, so that if ?(s) =
s?, JsK ? Js?K. We consider three types of oper-
ations: the removal of optional material (e.g PPs
like on the beach), the extraction of simpler con-
stituents (NPs, VPs, or simple Ss), and lexical sub-
stitutions of nouns by their hypernyms (poodle ?
dog). These operations are akin to the atomic ed-
its of MacCartney and Manning (2008)?s NatLog
system, and allow us to construct large subsump-
tion hierarchies over image descriptions, which we
call denotation graphs. Given a set of (upward
entailment-preserving) operations O ? S ? S, the
denotation graph DG = ?E, V ? of a set of images I
and a set of strings S represents a subsumption hier-
archy in which each node V = ?s, JsK? corresponds
to a string s ? S and its denotation JsK ? I . Di-
rected edges e = (s, s?) ? E ? V ? V indicate a
subsumption relation s v s? between a more generic
expression s and its child s?. An edge from s to s?
68
exists if there is an operation ? ? O that reduces the
string s? to s (i.e. ?(s?) = s) and its inverse ??1
expands the string s to s? (i.e. ??1(s) = s?).
2.4 Denotational Similarities
Given a denotation graph over N images, we esti-
mate the denotational probability of an expression s
with a denotation of size |JsK| as PJK(s) = |JsK|/N ,
and the joint probability of two expressions analo-
gously as PJK(s, s?) = |JsK ? Js?K|/N . The condi-
tional probability PJK(s | s?) indicates how likely
s is to be true when s? holds, and yields a simple
directed denotational similarity. The (normalized)
pointwise mutual information (PMI) (Church and
Hanks, 1990) defines a symmetric similarity:
nPMI JK(s, s?) =
log
( PJK(s,s?)
PJK(s)PJK(s?)
)
? log(PJK(s, s?))
We set PJK(s|s) = nPMI JK(s, s) = 1, and, if s or
s? are not in the denotation graph, nPMI JK(s, s?) =
PJK(s, s?) = 0.
3 Our Data Set
Our data set (Figure 1) consists of 31,783 pho-
tographs of everyday activities, events and scenes
(all harvested from Flickr) and 158,915 captions
(obtained via crowdsourcing). It contains and ex-
tends Hodosh et al. (2013)?s corpus of 8,092 im-
ages. We followed Hodosh et al. (2013)?s approach
to collect images. We also use their annotation
guidelines, and use similar quality controls to cor-
rect spelling mistakes, eliminate ungrammatical or
non-descriptive sentences. Almost all of the im-
ages that we add to those collected by Hodosh et
al. (2013) have been made available under a Cre-
ative Commons license. Each image is described in-
dependently by five annotators who are not familiar
with the specific entities and circumstances depicted
in them, resulting in captions such as ?Three people
setting up a tent?, rather than the kind of captions
people provide for their own images (?Our trip to
the Olympic Peninsula?). Moreover, different an-
notators use different levels of specificity, from de-
scribing the overall situation (performing a musical
piece) to specific actions (bowing on a violin). This
variety of descriptions associated with the same im-
age is what allows us to induce denotational similari-
ties between expressions that are not trivially related
by syntactic rewrite rules.
4 Constructing the Denotation Graph
The construction of the denotation graph consists
of the following steps: preprocessing and linguistic
analysis of the captions, identification of applicable
transformations, and generation of the graph itself.
Preprocessing and Linguistic Analysis We use
the Linux spell checker, the OpenNLP tok-
enizer, POS tagger and chunker (http://opennlp.
apache.org), and the Malt parser (Nivre et al.,
2006) to analyze the captions. Since the vocabulary
of our corpus differs significantly from the data these
tools are trained on, we resort to a number of heuris-
tics to improve the analyses they provide. Since
some heuristics require us to identify different entity
types, we developed a lexicon of the most common
entity types in our domain (people, clothing, bodily
appearance (e.g. hair or body parts), containers of
liquids, food items and vehicles).
After spell-checking, we normalize certain words
and compounds with several spelling variations, e.g.
barbecue (barbeque, BBQ), gray (grey), waterski
(water ski), brown-haired (brown haired), and to-
kenize the captions using the OpenNLP tokenizer.
The OpenNLP POS tagger makes a number of sys-
tematic errors on our corpus (e.g. mistagging main
verbs as nouns). Since these errors are highly sys-
tematic, we are able to correct them automatically
by applying deterministic rules (e.g. climbs is never
a noun in our corpus, stand is a noun if it is pre-
ceded by vegetable but a verb when preceded by a
noun that refers to people). These fixes apply to
27,784 (17% of the 158,915 image captions). Next,
we use the OpenNLP chunker to create a shallow
parse. Fixing its (systematic) errors affects 28,587
captions. We then analyze the structure of each
NP chunk to identify heads, determiners and pre-
nominal modifiers. The head may include more than
a single token if WordNet (or our hypernym lexi-
con, described below) contains a corresponding en-
try (e.g. little girl). Determiners include phrases
such as a couple or a few. Although we use the
Malt parser (Nivre et al., 2006) to identify subject-
verb-object dependencies, we have found it more ac-
curate to develop deterministic heuristics and lexi-
69
cal rules to identify the boundaries of complex (e.g.
conjoined) NPs, allowing us to treat ?a man with red
shoes and a white hat? as an NP followed by a sin-
gle PP, but ?a man with red shoes and a white-haired
woman? as two NPs, and to transform e.g. ?stand-
ing by a man and a woman? into ?standing? and not
?standing and a woman? when dropping the PP.
Hypernym Lexicon We use our corpus and Word-
Net to construct a hypernym lexicon that allows us
to replace head nouns with more generic terms. We
only consider hypernyms that occur themselves with
sufficient frequency in the original captions (replac-
ing ?adult? with ?person?, but not with ?organ-
ism?). Since the language in our corpus is very
concrete, each noun tends to have a single sense, al-
lowing us to always replace it with the same hyper-
nyms.1 But since WordNet provides us with mul-
tiple senses for most nouns, we first have to iden-
tify which sense is used in our corpus. To do this,
we use the heuristic cross-caption coreference algo-
rithm of Hodosh et al. (2010) to identify coreferent
NP chunks among the original five captions of each
image.2 For each ambiguous head noun, we con-
sider every non-singleton coreference chains it ap-
pears in, and reduce its synsets to those that stand
in a hypernym-hyponym relation with at least one
other head noun in the chain. Finally, we apply a
greedy majority voting algorithm to iteratively nar-
row down each term?s senses to a single synset that
is compatible with the largest number of coreference
chains it occurs in.
Caption Normalization In order to increase the
recall of the denotations we capture, we drop all
punctuation marks, and lemmatize nouns, verbs, and
adjectives that end in ?-ed? or ?-ing? before gener-
1Descriptions of people that refer to both age and gen-
der (e.g. ?man?) can have multiple distinct hypernyms
(?adult?/??male?). Because our annotators never describe
young children or babies as ?persons?, we only allow terms
that are likely to describe adults or teenagers (including occu-
pations) to be replaced by the term ?person?. This means that
the term ?girl? has two senses: a female child (the default) or a
younger woman. We distinguish the two senses in a preprocess-
ing step: if the other captions of the same image do not mention
children, but refer to teenaged or adult women, we assign girl
the woman-sense. Some nouns that end in -er (e.g. ?diner?,
?pitcher? also violate our monosemy assumption.
2Coreference resolution has also been used for word sense
disambiguation by Preiss (2001) and Hu and Liu (2011).
ating the denotation graph. In order to distinguish
between frequently occurring homonyms where the
noun is unrelated to the verb, we change all forms of
the verb dress to dressed, all forms of the verb stand
to standing and all forms of the verb park to park-
ing. Finally, we drop sentence-initial there/here/this
is/are (as in there is a dog splashing in the water),
and normalize the expressions in X and dressed (up)
in X (where X is an article of clothing or a color) to
wear X. We reduce plural determiners to {two, three,
some}, and drop singular determiners except for no.
4.1 Rule Templates
The denotation graph contains a directed edge from
s to s? if there is a rule ? that reduces s? to s, with an
inverse ??1 that expands s to s?. Reduction rules can
drop optional material, extract simpler constituents,
or perform lexical substitutions.
Drop Pre-Nominal Modifiers: ?red shirt? ?
?shirt? In an NP of the form ?X Y Z?, where
X and Y both modify the head Z, we only allow
X and Y to be dropped separately if ?X Z? and
?Y Z? both occur elsewhere in the corpus. Since
?white building? and ?stone building? occur else-
where in the corpus, we generate both ?white build-
ing? and ?stone building? from the NP ?white stone
building?. But since ?ice player? is not used,
we replace ?ice hockey player? only with ?hockey
player? (which does occur) and then ?player?.
Drop Other Modifiers ?run quickly? ? ?run?
We drop ADVP chunks and adverbs in VP chunks.
We also allow a prepositional phrase (a preposi-
tion followed by a possibly conjoined NP chunk)
to be dropped if the preposition is locational
(?in?, ?on?, ?above?, etc.), directional (?towards?,
?through?, ?across?, etc.), or instrumental (?by?,
?for?, ?with?). Similarly, we also allow the drop-
ping of all ?wear NP? constructions. Since the dis-
tinction between particles and prepositions is often
difficult, we also use a predefined list of phrasal
verbs that commonly occur in our corpus to identify
constructions such as ?climb up a mountain?, which
is transformed into ?climb a mountain? or ?walk
down a street?, which is transformed into ?walk?.
Replace Nouns by Hypernyms: ?red shirt? ?
?red clothing? We iteratively use our hypernym
70
GENERATEGRAPH():
Q,Captions,Rules? ?
for all c ? ImageCorpus do
Rules(c)? GenerateRules(sc)
pushAll(Q, {c} ? RootNodes(sc,Rules(c)))
while ?empty(Q) do
(c, s)? pop(Q)
Captions(s)? Captions(s) ? {c}
if |Captions(s)| = 2 then
for all c? ? Captions(s) do
pushAll(Q, {c?} ? Children(s,Rules(c?)))
else if |Captions(s)| > 2 then
pushAll(Q, {c} ? Children(s,Rules(c)))
Figure 2: Generating the graph
lexicon to make head nouns more generic. We only
allow head nouns to be replaced by their hypernyms
if any age based modifiers have already been re-
moved: ?toddler? can be replaced with ?child?, but
not ?older toddler? with ?older child?.
Handle Partitive NPs: cup of tea? ?cup?, ?tea?
In most partitive NP1-of-NP2 constructions (?cup of
tea?, ?a team of football players?) the correspond-
ing entity can be referred to by both the first or the
second NP. Exceptions include the phrase ?body of
water?, and expressions such as ?a kind/type/sort
of?, which we treat similar to determiners.
Handle VP1-to-VP2 Cases Depending on the first
verb, we replace VPs of the form X to Y with both X
and Y if X is a movement or posture (jump to catch,
etc.). Otherwise we distinguish between cases we
can only replace with X (wait to jump) and those we
can only replace with Y (seem to jump).
Extract Simpler Constituents Any noun phrase
or verb phrase can also be used as a node in the
graph and simplified further. We use the Malt de-
pendencies (and the person terms in the entity type
lexicon) to identify and extract subject-verb-object
chunks which correspond to simpler sentences that
we would otherwise not be able to obtain: from
?man laugh(s) while drink(ing)?, we extract ?man
laugh? and ?man drink?, and then further split those
into ?man?, ?laugh(s)?, and ?drink?.
4.2 Graph Generation
The naive approach to graph generation would be to
generate all possible strings for each caption. How-
ever, this would produce far more strings than can be
processed in a reasonable amount of time, and most
of these strings would have uninformative denota-
tions, consisting of only a single image. To make
graph generation tractable, we use a top-down al-
gorithm which generates the graph from the most
generic (root) nodes, and stops at nodes that have a
singleton denotation (Figure 2). We first identify the
set of rules that can apply to each original caption
(GenerateRules). These rules are then used to re-
duce each caption as much as possible. The resulting
(maximally generic) strings are added as root nodes
to the graph (RootNodes), and added to the queue
Q. Q keeps track of all currently possible node ex-
pansions. It contains items ?c, s?, which pair the ID
of an original caption and its image (c) with a string
(s) that corresponds to an existing node in the graph
and can be derived from c?s caption. When ?c, s? is
processed, we check how many captions have gen-
erated s so far (Captions(s)). If s has more than a
single caption, we use each of the applicable rewrite
rules of c?s caption to create new strings s? that cor-
respond to the children of s in the graph, and push
all resulting ?c, s?? onto Q. If c is the second caption
of s, we also use all of the applicable rewrite rules
from the first caption c? to create its children.
A post-processing step (not shown in Figure 2)
attaches each original caption to all leaf nodes of the
graph to which it can be reduced. Finally, we obtain
the denotation of each node s from the set of images
whose captions are in Captions(s).
5 The Denotation Graph
Size and Coverage On our corpus of 158,439
unique captions and 31,783 images, the denotation
graph contains 1,749,097 captions, out of which
230,811 describe more than a single image. Ta-
ble 1 provides the distribution of the size of deno-
tations. It is perhaps surprising that the 161 cap-
tions which describe each over 1,000 images do
not just consist of nouns such as person, but also
contain simple sentences such as woman standing,
adult work, person walk street, or person play in-
strument. Since the graph is derived from the origi-
nal captions by very simple syntactic operations, the
denotations it captures are most likely incomplete:
Jsoccer playerK contains 251 images, Jplay soccerK
contains 234 images, and Jsoccer gameK contains
71
Size of denotations |JsK| ? 1 |JsK| ? 2 |JsK| ? 5 |JsK| ? 10 |JsK| ? 100 |JsK| ? 1000
Nr. of captions 1,749,096 230,811 53,341 22,683 1,921 161
Table 1: Distribution of the size of denotations in our graph
119 images. We have not yet attempted to iden-
tify variants in word order (?stick tongue out? vs.
?stick out tongue?) or equivalent choices of prepo-
sition (?look into mirror? vs. ?look in mirror?). De-
spite this brittleness, the current graph already gives
us a large number of semantic associations.
Denotational Similarities The following exam-
ples of the similarities found by nPMI JK and PJK
show that denotational similarities do not simply
find topically related events, but instead find events
that are related by entailment:
PJK(x|y) x y
0.962 sit eat lunch
0.846 play guitar strum
0.811 surf catch wave
0.800 ride horse rope calf
0.700 listen sit in classroom
If someone is eating lunch, it is likely that they
are sitting, and people who sit in a classroom are
likely to be listening to somebody. These entail-
ments can be very precise: ?walk up stair? entails
?ascend?, but not ?descend?; the reverse is true for
?walk down stair?:
PJK(x|y) x =ascend x =descend
y =walk up stair 32.0 0.0
y =walk down stair 0.0 30.8
nPMI JK captures paraphrases as well as closely
related events: people look in a mirror when shav-
ing their face, and baseball players may try to tag
someone who is sliding into base:
nPMI JK x y
0.835 open present unwrap
0.826 lasso try to rope
0.791 get ready to kick run towards ball
0.785 try to tag slide into base
0.777 shave face look in mirror
Comparing the expressions that are most similar
to ?play baseball? or ?play football? according to
the denotational nPMI JK and the compositional ?
similarities reveals that the denotational similarity
finds a number of actions that are part of the partic-
ular sport, while the compositional similarity finds
events that are similar to playing baseball (football):
play baseball
nPMI JK ?
0.674 tag him 0.859 play softball
0.637 hold bat 0.782 play game
0.616 try to tag 0.768 play ball
0.569 slide into base 0.741 play catch
0.516 pitch ball 0.739 play cricket
play football
nPMI JK ?
0.623 tackle person 0.826 play game
0.597 hold football 0.817 play rugby
0.545 run down field 0.811 play soccer
0.519 wear white jersey 0.796 play on field
0.487 avoid 0.773 play ball
6 Task 1: Approximate Entailment
A caption never provides a complete description of
the depicted scene, but commonsense knowledge
often allows us to draw implicit inferences: when
somebody mentions a bride, it is quite likely that the
picture shows a woman in a wedding dress; a pic-
ture of a parent most likely also has a child or baby,
etc. In order to compare the utility of denotational
and distributional similarities for drawing these in-
ferences, we apply them to an approximate entail-
ment task, which is loosely modeled after the Rec-
ognizing Textual Entailment problem (Dagan et al.,
2006), and consists of deciding whether a brief cap-
tion h (the hypothesis) can describe the same image
as a set of captions P = {p1, ...,pN} known to de-
scribe the same image (the premises).
Data We generate positive and negative items
?P,h,?? (Figure 3) as follows: Given an image,
any subset of four of its captions form a set of
premises. A hypothesis is either a short verb phrase
or sentence that corresponds to a node in the deno-
tation graph. By focusing on short hypotheses, we
minimize the possibility that they contain extrane-
ous details that cannot be inferred from the premises.
Positive examples are generated by choosing a node
h as hypothesis and an image i ? JhK such that ex-
actly one caption of i generates h and the other four
captions of i are not descendants of h and hence
do not trivially entail h, giving an unfair advantage
to denotational approaches. Negative examples are
generated by choosing a node h as hypothesis and
selecting four of the captions of an image i 6? JhK.
72
Premises: A woman with dark hair in bending, open mouthed, towards the back of a dark headed toddler?s head.
A dark-haired woman has her mouth open and is hugging a little girl while sitting on a red blanket.
A grown lady is snuggling on the couch with a young girl and the lady has a frightened look.
A mom holding her child on a red sofa while they are both having fun.
VP Hypothesis: make face
Premises: A man editing a black and white photo at a computer with a pencil in his ear.
A man in a white shirt is working at a computer.
A guy in white t-shirt on a mac computer.
A young main is using an Apple computer.
S Hypothesis: man sit
Figure 3: Positive examples from the Approximate Entailment tasks.
Since our items are created automatically, a posi-
tive hypothesis is not necessarily logically entailed
by its premises. We have performed a small-scale
human evaluation on 300 items (200 positive, 100
negative), each judged independently by the same
three judges (inter-annotator agreement: Fleiss-? =
0.74). Our results indicate that over half (55%) of
the positive hypotheses can be inferred from their
premises alone without looking at the original im-
age, while almost none of the negative hypotheses
(100% for sentences, 96% for verb phrases) can be
inferred from their premises. The training items are
generated from the captions of 25,000 images, and
the test items are generated from a disjoint set of
3,000 images. The VP data set consists of 290,000
training items and 16,000 test items, while the S data
set consists of 400,000 training items and 22,000 test
items. Half of the items in each set are positive, and
the other half are negative.
Models All of our models are binary MaxEnt clas-
sifiers, trained using MALLET (McCallum, 2002).
We have two baseline models: a plain bag-of-words
model (BOW) and a bag-of-words model where we
add all hypernyms in our lexicon to the captions be-
fore computing their overlap (BOW-H). This is in-
tended to minimize the advantage the denotational
features obtain from the hypernym lexicon used to
construct the denotation graph. In both cases, a
global BOW feature captures the fraction of tokens
in the hypothesis that are contained in the premises.
Word-specific BOW features capture the product of
the frequencies of each word in h and P. All other
models extend the BOW-H model.
Denotational Similarity Features We compute
denotational similarities nPMI JK and PJK (Sec-
tion 2.4) over the pairs of nodes in a denotation
graph that is restricted to the training images. We
only consider pairs of nodes n,n? if their denota-
tions contain at least 10 images and their intersection
contains at least 2 images.
To map an item ?P,h? to denotational simi-
larity features, we represent the premises as the
set of all nodes P that are ancestors of its cap-
tions. A sentential hypothesis is represented as
the set of nodes H = {hS , hsbj , hV P , hv, hdobj}
that correspond to the sentence (h itself), its sub-
ject, its VP and its direct object. A VP hypothe-
sis has only the nodes H = {hV P , hv, hdobj}. In
both cases, hdobj may be empty. Both of the de-
notational similarities nPMI JK(h, p) and PJK(h|p)
for h ? H , p ? P lead to two constituent-
specific features, sumx and maxx, (e.g. sumsbj =?
p sim(hsbj , p), maxdobj = maxp sim(hdobj , p))
and two global features sump,h = ?p,h sim(h, p)
and maxp,h = maxp,h sim(h, p). Each constituent
type also has a set of node-specific sumx,s and
maxx,s features that are on when constituent x in
h is equal to the string s and whose value is equal
to the constituent-based feature. For PJK, each con-
stituent (and each constituent-node pair) has an ad-
ditional feature P (h|P ) = 1 ??n(1 ? PJK(h|pn))
that estimates the probability that h is generated by
some node in the premise.
Lexical Similarity Features We use two sym-
metric lexical similarities: standard cosine distance
(cos), and Lin (1998)?s similarity (Lin):
cos(w,w?) = w?w??w??w??
Lin(w,w?) =
?
i:w(i)>0?w?(i)>0w(i)+w
?(i)?
iw(i)+
?
iw?(i)
73
We use two directed lexical similarities: Clarke
(2009)?s similarity (Clk), and Szpektor and Dagan
(2008)?s balanced precision (Bal), which builds on
Lin and on Weeds and Weir (2003)?s similarity (W):
Clk(w | w?) =
?
i:w(i)>0?w?(i)>0 min(w(i),w?(i))?
iw(i)
Bal(w | w?) =
?
W(w | w?)? Lin(w,w?)
W(w | w?) =
?
i:w(i)>0?w?(i)>0 w(i)?
iw(i)
We also use two publicly available resources that
provide precomputed similarities, Kotlerman et al.
(2010)?s DIRECT noun and verb rules and Chklovski
and Pantel (2004)?s VERBOCEAN rules. Both are
motivated by the need for numerically quantifiable
semantic inferences between predicates. We only
use entries that correspond to single tokens (ignor-
ing e.g. phrasal verbs).
Each lexical similarity results in the follow-
ing features: words in the output are represented
by a max-simw feature which captures its max-
imum similarity with any word in the premises
(max-simw = maxw??P sim(w,w?)) and by a
sum-simw feature which captures the sum of its sim-
ilarities to the words in the premises (sum-simw =?
w??P sim(w,w?)). Global max sim and sum sim
features capture the maximal (resp. total) similarity
of any word in the hypothesis to the premise.
We compute distributional and compositional
similarities (cos, Lin, Bal, Clk, ?, ?) on our im-
age captions (?cap?), the BNC and Gigaword. For
each corpus C, we map each word w that appears
at least 10 times in C to a vector wC of the non-
negative normalized pointwise mutual information
scores (Section 2.4) of w and the 1,000 words (ex-
cluding stop words) that occur in the most sentences
of C. We generally define P (w) (and P (w,w?)) as
the fraction of sentences in C in which w (and w?)
occur. To allow a direct comparison between dis-
tributional and denotational similarities, we first de-
fine P (w) (and P (w,w?)) over individual captions
(?cap?), and then, to level the playing field, we rede-
fine P (w) (and P (w,w?)) as the fraction of images
in whose captions w (and w?) occur (?img?), and
then we use our lexicon to augment captions with
all hypernyms (?+hyp?). Finally, we include BNC
and Gigaword similarity features (?all?).
VP task S task
Baseline 1: BoW 58.7 71.2
Baseline 2: BoW-H 59.0 73.6
External 1: DIRECT 59.2 73.5
External 2: VerbOcean 60.8 74.0
Cap All Cap All
Distributional cos 67.5 71.9 76.1 78.9
Distributional Lin 62.6 70.2 75.4 77.8
Distributional Bal 62.3 69.6 74.7 75.3
Distributional Clk 62.4 69.2 75.4 77.5
Compositional ? 68.4 70.3 75.3 77.3
Compositional ? 67.8 71.4 76.9 79.2
Compositional ?,? 69.8 72.7 77.0 79.6
Denotational nPMI JK 74.9 80.2
Denotational PJK 73.8 79.5
nPMI JK, PJK 75.5 81.2
Combined cos, ?,? 71.1 72.6 77.4 79.2
nPMI JK, PJK, ?,? 75.6 75.9 80.2 80.7
nPMI JK, PJK, cos 75.6 75.7 80.2 81.2
nPMI JK, PJK, cos, ?,? 75.8 75.9 81.2 80.5
Table 2: Test accuracy on Approximate Entailment.
Compositional Similarity Features We use two
standard compositional baselines to combine the
word vectors of a sentence into a single vector: ad-
dition (s? = w1 + ... + wn, which can be inter-
preted as a disjunctive operation), and element-wise
(Hadamard) multiplication (s? = w1  ...  wn,
which can be seen as a conjunctive operation). In
both cases, we represent the premises (which con-
sist of four captions) as a the sum of each caption?s
vector p = p1 + ...p4. This gives two composi-
tional similarity features: ? = cos(p?,h?), and
? = cos(p?,h?).
6.1 Experimental Results
Table 2 provides the test accuracy of our mod-
els on the VP and S tasks. Adding hypernyms
(BOW-H) yields a slight improvement over the ba-
sic BOW model. Among the external resources,
VERBOCEAN is more beneficial than DIRECT, but
neither help as much as in-domain distributional
similarities (this may be due to sparsity).
Table 2 shows only the simplest (?Cap?) and
the most complex (?all?) distributional and com-
positional models, but Table 3 provides accuracies
of these models as we go from standard sentence-
based co-occurrence counts towards more denota-
tion graph-like co-occurrence counts that are based
on all captions describing the same image (?Img?),
74
VP task S task
Cap Img +Hyp All Cap Img +Hyp All
cos 67.5 69.3 69.8 71.9 76.1 76.8 77.5 78.9
Lin 62.6 63.4 61.3 70.0 75.4 74.8 75.2 77.8
Bal 62.3 61.9 62.8 69.6 74.7 75.5 75.1 75.3
Clk 62.4 67.3 68.0 69.2 75.4 75.5 76.0 77.5
? 68.4 70.5 70.5 70.3 75.3 76.6 77.1 77.3
? 67.8 71.4 71.6 71.4 76.9 78.1 79.1 79.2
?,? 69.8 72.7 72.9 72.7 77.0 78.6 79.3 79.6
nPMI JK 74.9 80.2
PJK 73.8 79.5
nPMI JK, PJK 75.5 81.2
Table 3: Accuracy on hypotheses as various additions are
made to the vector corpora. Cap is the image corpus with
caption co-occurrence. Img is the image corpus with im-
age co-occurrence. +Hyp augments the image corpus
with hypernyms and uses image co-occurrence. All adds
the BNC and Gigaword corpora to +Hyp.
VP task S task
Words in h 1 2 3+ 2 3 4+
% of items 72.8 13.9 13.3 65.3 22.8 11.9
BoW-H 52.0 75.0 80.1 69.1 80.8 84.4
cos (All) 68.8 79.4 81.1 75.9 83.9 85.7? (All) 68.1 80.8 79.5 76.5 83.9 85.1
nPMI JK 72.0 82.9 82.2 77.3 85.4 86.2
Table 4: Accuracy on hypotheses of varying length.
include hypernyms (?+Hyp?), and add informa-
tion from other corpora (?All?). The ?+Hyp? col-
umn in Table 3 shows that the denotational metrics
clearly outperform any distributional metric when
both have access to the same information. Al-
though the distributional models benefit from the
BNC and Gigaword-based similarities (?All?), their
performance is still below that of the denotational
models. Among the distributional model, the simple
cos performs better than Lin, or the directed Clk and
Bal similarities. In all cases, giving models access to
different similarity features improves performance.
Table 4 shows the results by hypothesis length.
As the length of h increases, classifiers that use sim-
ilarities between pairs of words (BOW-H and cos)
continue to improve in performance relative to the
classifiers that use similarities between phrases and
sentences (? and nPMI JK). Most likely, this is due
to the lexical similarities having a larger set of fea-
tures to work with for longer h. nPMI JK does espe-
cially well on shorter h, likely due to the shorter h
having larger denotations.
7 Task 2: Semantic Textual Similarity
To assess how the denotational similarities perform
on a more established task and domain, we apply
them to the 1500 sentence pairs from the MSR Video
Description Corpus (Chen and Dolan, 2011) that
were annotated for the SemEval 2012 Semantic Tex-
tual Similarity (STS) task (Agirre et al., 2012). The
goal of this task is to assign scores between 0 and 5
to a pair of sentences, where 5 indicates equivalence,
and 0 unrelatedness. Since this is a symmetric task,
we do not consider directed similarities. And be-
cause the goal of this experiment is not to achieve
the best possible performance on this task, but to
compare the effectiveness of denotational and more
established similarities, we only compare the impact
of denotational similarities with compositional sim-
ilarities computed on our own corpus. Since the
MSR Video corpus associates each video with mul-
tiple sentences, it is in principle also amenable to a
denotational treatment, but the STS task description
explicitly forbids its use.
7.1 Models
Baseline and Compositional Features Our start-
ing point is Ba?r et al. (2013)?s DKPro Similarity,
one of the top-performing models from the 2012
STS shared task, which is available and easily mod-
ified. It consists of a log-linear regression model
trained on multiple text features (word and charac-
ter n-grams, longest common substring and longest
common subsequence, Gabrilovich and Markovitch
(2007)?s Explicit Semantic Analysis, and Resnik
(1995)?s WordNet-based similarity). We investigate
the effects of adding compositional (computed on
the vectors obtained from the image-caption train-
ing data) and denotational similarity features to this
state-of-the-art system.
Denotational Features Since the STS task is
symmetric, we only consider nPMI JK similari-
ties. We again represent each sentence s by fea-
tures based on 5 types of constituents: S =
{sS , ssbj , sV P , sv, sdobj}. Since sentences might be
complex, they might contain multiple constituents
of the same type, and we therefore think of each
feature as a feature over sets of nodes. For each
constituent C we consider two sets of nodes in the
denotation graph: C itself (typically leaf nodes),
75
DKPro +?,? (img) +nPMI JK +both
Pearson r 0.868 0.880 0.888 0.890
Table 5: Performance on the STS MSRvid task: DKPro
(Ba?r et al., 2013) plus compositional (?,?) and/or deno-
tational similarities (nPMI JK) from our corpus
and Canc, their parents and grandparents. For
each pair of sentences, C-C similarities compute
the similarity of the constituents of the same type,
while C-all similarities compute the similarity of
a C constituent in one sentence against all con-
stituents in the other sentence. For each pair of
constituents we consider three similarity features:
sim(C1, C2), max(sim(C1Canc2 ), sim(Canc1 , C2)),
sim(Canc1 , Canc2 ). The similarity of two sets of
nodes is determined by the maximal similarity
of any pair of their elements: sim(C1, C2) =
maxc1?C1,c2?C2 nPMI JK(c1, c2). This gives us 15
C-C features and 15 C-all features.
7.2 Experiments
We use the STS 2012 train/test data, normalized in
the same way as the image captions for the deno-
tation graph (i.e. we re-tokenize, lemmatize, and
remove determiners). Table 5 shows experimental
results for four models: DKPro is the off-the-shelf
DKProSimilarity model (Ba?r et al., 2013). From
our corpus, we either add additive and multiplicative
compositional features (?,?) from Section 6 (img),
the C-C and C-All denotational features based on
nPMI JK, or both compositional and denotational
features. Systems are evaluated by the Pearson cor-
relation (r) of their predicted similarity scores to the
human-annotated ones. We see that the denotational
similarities outperform the compositional similari-
ties, and that including compositional similarity fea-
tures in addition to denotational similarity features
has little effect. For additional comparison, the
published numbers for the TakeLab Semantic Text
Similarity System (S?aric? et al., 2012), another top-
performing model from the 2012 shared task, are
r = 0.880 on this dataset.
8 Conclusion
Summary of Contributions We have defined
novel denotational metrics of linguistic similarity
(Section 2), and have shown them to be at least
competitive with, if not superior to, distributional
similarities for two tasks that require simple se-
mantic inferences (Sections 6, 7), even though our
current method of computing them is somewhat
brittle (Section 5). We have also introduced two
new resources: a large data set of images paired
with descriptive captions, and a denotation graph
that pairs generalized versions of these captions
with their visual denotations, i.e. the sets of im-
ages they describe. Both of these resources are
freely available (http://nlp.cs.illinois.edu/
Denotation.html) Although the aim of this paper
is to show their utility for a purely linguistic task,
we believe that they should also be of great interest
for people who aim to build systems that automat-
ically associate image with sentences that describe
them (Farhadi et al., 2010; Kulkarni et al., 2011; Li
et al., 2011; Yang et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh
et al., 2013).
Related Work and Resources We believe that the
work reported in this paper has the potential to open
up promising new research directions. There are
other data sets that pair images or video with de-
scriptive language, but we have not yet applied our
approach to them. Chen and Dolan (2011)?s MSR
Video Description Corpus (of which the STS data
is a subset) is most similar to ours, but its curated
part is significantly smaller. Instead of several in-
dependent captions, Grubinger et al. (2006)?s IAPR
TC-12 data set contains longer descriptions. Or-
donez et al. (2011) harvested 1 million images and
their user-generated captions from Flickr to create
the SBU Captioned Photo Dataset. These captions
tend to be less descriptive of the image. The de-
notation graph is similar to Berant et al. (2012)?s
?entailment graph?, but differs from it in two ways:
first, entailment relations in the denotation graph
are defined extensionally in terms of the images de-
scribed by the expressions at each node, and sec-
ond, nodes in Berant et al.?s entailment graph corre-
spond to generic propositional templates (X treats
Y), whereas nodes in our denotation graph corre-
spond to complete propositions (a dog runs).
76
Acknowledgements
We gratefully acknowledge the support of the
National Science Foundation under NSF awards
0803603 ?INT2-Medium: Understanding the mean-
ing of images?, 1053856 ?CAREER: Bayesian Mod-
els for Lexicalized Grammars?, and 1205627 ?CI-
P:Collaborative Research: Visual entailment data
set and challenge for the Language and Vision Com-
munity?, as well as via an NSF Graduate Research
Fellowship to Alice Lai.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: a pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, SemEval ?12, pages 385?393.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An Open Source Framework for
Text Similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics: System Demonstrations, pages 121?126, Sofia,
Bulgaria, August.
Jon Barwise and John Perry. 1980. Situations and atti-
tudes. Journal of Philosophy, 78:668?691.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Greg Carlson, 2005. The Encyclopedia of Language and
Linguistics, chapter Generics, Habituals and Iteratives.
Elsevier, 2nd edition.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 190?200, Portland, Oregon, USA,
June.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, Barcelona, Spain, July.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112?119, Athens, Greece,
March.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
challenge. In Machine Learning Challenges, volume
3944 of Lecture Notes in Computer Science, pages
177?190. Springer.
David Dowty, Robert Wall, and Stanley Peters. 1981. In-
troduction to Montague Semantics. Reidel, Dordrecht.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a
story: Generating sentences from images. In Proceed-
ings of the European Conference on Computer Vision
(ECCV), Part IV, pages 15?29, Heraklion, Greece,
September.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
international joint conference on Artifical intelligence,
IJCAI?07, pages 1606?1611.
Michael Grubinger, Paul Clough, Henning Mu?ller, and
Thomas Deselaers. 2006. The IAPR benchmark: A
new evaluation resource for visual information sys-
tems. In OntoImage 2006, Workshop on Language
Resources for Content-based Image Retrieval during
LREC 2006, pages 13?23, Genoa, Italy, May.
Ankush Gupta, Yashaswi Verma, and C. Jawahar. 2012.
Choosing linguistics over vision to describe images.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence, Toronto, Ontario, Canada,
July.
Zellig S Harris. 1954. Distributional structure. Word,
10:146?162.
Micah Hodosh, Peter Young, Cyrus Rashtchian, and Julia
Hockenmaier. 2010. Cross-caption coreference reso-
lution for automatic image understanding. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 162?171, Uppsala,
Sweden, July.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Arti-
ficial Intelligence Research (JAIR), 47:853?899.
Shangfeng Hu and Chengfei Liu. 2011. Incorporating
coreference resolution into word sense disambigua-
tion. In Alexander F. Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing, volume
6608 of Lecture Notes in Computer Science, pages
265?276. Springer Berlin Heidelberg.
77
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359?389.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Proceedings of the
2011 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gener-
ation of natural image descriptions. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
359?368, Jeju Island, Korea, July.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
220?228, Portland, OR, USA, June.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning (ICML),
pages 296?304, Madison, WI, USA, July.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 521?528, Manchester, UK,
August.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
558?566, Athens, Greece, March.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume III. 2012.
Midge: Generating image descriptions from computer
vision detections. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 747?756,
Avignon, France, April.
Richard Montague. 1974. Formal philosophy: papers
of Richard Montague. Yale University Press, New
Haven. Edited by Richmond H. Thomason.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the International Confer-
ence on Language Resources and Evaluation (LREC),
pages 2216?2219.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Infor-
mation Processing Systems 24, pages 1143?1151.
Judita Preiss. 2001. Anaphora resolution with word
sense disambiguation. In Proceedings of SENSEVAL-
2 Second International Workshop on Evaluating
Word Sense Disambiguation Systems, pages 143?146,
Toulouse, France, July.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th international joint conference on Artificial
intelligence - Volume 1, IJCAI?95, pages 448?453.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 849?856, Manchester, UK,
August. Coling 2008 Organizing Committee.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 81?88.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 444?454, Edin-
burgh, UK, July.
78
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 139?147,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Collecting Image Annotations Using Amazon?s Mechanical Turk
Cyrus Rashtchian Peter Young Micah Hodosh Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
201 North Goodwin Ave, Urbana, IL 61801-2302
{crashtc2, pyoung2, mhodosh2, juliahmr}@illinois.edu
Abstract
Crowd-sourcing approaches such as Ama-
zon?s Mechanical Turk (MTurk) make it pos-
sible to annotate or collect large amounts of
linguistic data at a relatively low cost and high
speed. However, MTurk offers only limited
control over who is allowed to particpate in
a particular task. This is particularly prob-
lematic for tasks requiring free-form text en-
try. Unlike multiple-choice tasks there is no
correct answer, and therefore control items
for which the correct answer is known can-
not be used. Furthermore, MTurk has no ef-
fective built-in mechanism to guarantee work-
ers are proficient English writers. We de-
scribe our experience in creating corpora of
images annotated with multiple one-sentence
descriptions on MTurk and explore the effec-
tiveness of different quality control strategies
for collecting linguistic data using Mechani-
cal MTurk. We find that the use of a qualifi-
cation test provides the highest improvement
of quality, whereas refining the annotations
through follow-up tasks works rather poorly.
Using our best setup, we construct two image
corpora, totaling more than 40,000 descriptive
captions for 9000 images.
1 Introduction
Although many generic NLP applications can be de-
veloped by using existing corpora or text collections
as test and training data, there are many areas where
NLP could be useful if there was a suitable corpus
available. For example, computer vision researchers
are becoming interested in developing methods that
can predict not just the presence and location of cer-
tain objects in an image, but also the relations be-
tween objects, their attributes, or the actions and
events they participate in. Such information can
neither be obtained from standard computer vision
data sets such as the COREL collection nor from
the user-provided keyword tag annotations or cap-
tions on photo-sharing sites such as Flickr. Simi-
larly, although the text near an image on a website
may provide cues about the entities depicted in the
image, an explicit description of the image content
itself is typically only provided if it is not immedi-
ately obvious to a human what is depicted (in which
case we may not expect a computer vision system
to be able to recognize the image content either).
We therefore set out to collect a corpus of images
annotated with simple full-sentence descriptions of
their content. To obtain these descriptions, we used
Amazon?s Mechanical Turk (MTurk).1 MTurk is
an online framework that allows researchers to post
annotation tasks, called HITs (?Human Intelligence
Task?), then, for a small fee, be completed by thou-
sands of anonymous non-expert users (Turkers). Al-
though MTurk has been used for a variety of tasks in
NLP, our use of MTurk differs from other research
in NLP that uses MTurk mostly for annotation of
existing text. Similar to crowdsourcing-based an-
notation, quality control is an essential component
of crowdsourcing-based data collection efforts, and
needs to be factored into the overall costs. For us,
the quality of the text produced by the Turkers is
particularly important since we are interested in us-
1All of our experiments on Mechanical Turk were adminis-
tered and paid for through the services offered by Dolores Labs.
139
ing this corpus for future research at the intersection
of computer vision and natural language processing.
However, MTurk provides limited ways to imple-
ment such quality control directly. For example, our
initial experiments yielded a data set that contained
many sentences that were clearly not written by na-
tive speakers. We learned that several steps must be
taken to ensure that Turkers both understand the task
and produce quality data.
This paper describes our experiences with Turk
(based on data collection efforts in spring and sum-
mer 2009), comparing two different approaches to
quality control. Although we did not set out to run a
scientific experiment comparing different strategies
of how to collect linguistic data on Turk, our expe-
rience points towards certain recommendations for
how to collect linguistic data on Turk.
2 The core task: image annotation
The PASCAL Data Set Every year, the Pat-
tern Analysis, Statistical Modeling, and Computa-
tional Learning (PASCAL) organization hosts the
Visual Object Classes Challenge (Everingham et al,
2008). This is a competition similar to the shared
tasks familiar to the ACL community, where a com-
mon data set of images with classification and de-
tection information is released, and computer vision
researchers compete to create the best classification,
detection, and segmentation systems. We chose to
use this collection of images because it is a standard
resource for computer vision, and will therefore fa-
ciliate further research.
The VOC2008 development and training set con-
tains around 6000 images. It is categorized by ob-
jects that appear in the image, with some images ap-
pearing in multiple categories.2. The images con-
tain a wide variety of actions and scenery. Our cor-
pus consists of 1000 of these images, fifty randomly
chosen from each of the twenty categories.
MTurk setup We asked Turkers to write one de-
scriptive sentence for each of ten images. An ex-
ample annotation screen is shown in Figure 1. We
2The twenty categories include people, various animals,
vehicles and other objects: person, bird, cat, cow,
dog, horse, sheep, aeroplane, bicycle,
boat, bus, car, motorbike, train, bottle,
chair, dining table, potted plant, sofa,
tv/monitor
Figure 1: Screenshot of the image annotation task.
first showed the Turkers a list of instructive guide-
lines describing the task (Figure 6). The instruc-
tions told them to write ten complete but simple sen-
tences, to include adjectives if possible, to describe
the main characters, the setting, or the relation of
the objects in the image, to pay attention to gram-
mar and spelling, and to try to be concise. These
instructions were meant to both explain the task and
to prepare Turkers to write quality sentences. We
then showed each Turker a set of ten images, chosen
randomly from the 1000 total images, and displayed
one at a time. The Turkers navigated using ?Next?
buttons through the ten annotation screens, each dis-
playing one image and one text-box. We allowed
Turkers ten minutes to complete one task.3 We re-
stricted the task to Turkers who have previously had
at least 95% of their results approved. We paid $0.10
to complete one task. The total cost for all 5000 de-
scriptions was $50 (plus Amazon?s 10% fee).
2.1 Results
On average, Turkers wrote the ten sentences in a to-
tal of four minutes. The average pay rate was $1.30
per hour, and the whole experiment finished in under
two days. Five different people described each im-
age, and in the end, most of the Turkers completed
the task successfully, although 2.5% of the 5000 sen-
tences were empty strings. Turkers varied in the time
they took to complete the experiment, in the length
of their sentences, and in the level of detail they in-
cluded about the image. An example captioned im-
age is shown in Figure 2.
Problems with the data The quality of descrip-
tions varied greatly. We were hoping to collect sim-
ple sentences, written in correct English, describ-
ing the entities and actions in the images. More-
3This proved to be more than enough time for the task.
140
Figure 2: An image along with the five captions that were written by Turkers.
over, these are explicitly the types of descriptions we
asked for in the MTurk task instructions. Although
we found the descriptions acceptable more than half
of the time, a large number of the remaining descrip-
tions had at least one of the following two problems:
1. Some descriptions did not mention the salient
entities in the image, some were simply noun
phrases (or less), and some were humorous or
speculative.4 We find all of these to be prob-
lems because future computer vision and nat-
ural language processing research will require
accurate and consistent image captions.
2. A number of Turkers were not sufficiently pro-
ficient in English. Many descriptions contained
grammar and spelling errors, and some in-
cluded very awkward constructions. For exam-
ple, the phrase ?X giving pose? showed up sev-
eral times in descriptions of images containing
people (e.g. ?The lady and man giving pose.?).
Such spelling and grammar errors will pose dif-
ficulties for any standard text-processing algo-
rithms trained on native English.
Spell checking Due to the large number of mis-
spellings in in the initial data set, we first ran the sen-
tences first through our spell checker before putting
them up on Turk to assess their quality. We tok-
enized the captions with OpenNLP, and first checked
a manually created list of spelling corrections for
each token. These included canonicalizations (cor-
recting ?surf board? as ?surfboard?), words our au-
tomatic spell checker did not recognize (?mown?),
and the most common misspellings in our data set
4For example, some Turkers commented on the feelings of
animals (e.g. ?the dog is not very happy next to the dumpster?),
and others made jokes about the content of the image (e.g. ?The
goat is ready for hair cut?)
(?shepard? to ?shepherd?). If the token was not in
our manual list, we passed the word to aspell. From
aspell?s candidate corrections, we selected the most
frequent word that appeared either in other captions
of the same image, of images of the same topic, or
any caption in our data set.
3 Post-hoc quality control
Because our initial data collection efforts resulted in
relatively noisy data, we created a new set of MTurk
tasks designed to provide post-hoc quality control.
Our aim was to filter out captions containing mis-
spellings and incorrect grammar.
MTurk setup Each HIT consisted of fifty differ-
ent image descriptions and asked Turkers to decide
for each of them whether they contained correct
grammar and spelling or not. At the beginning of
each HIT, we included a brief training phase, where
we showed the Turkers five example descriptions la-
beled as ?correct? or ?incorrect? (Figure 7). In the
HIT itself, the fifty descriptions were displayed in
blocks of five (albeit not for the same image) , and
each description was followed by two radio buttons
labeled ?correct? and ?incorrect?. We did not show
the corresponding images. A screenshot is shown in
Figure 3. Each block of five captions contained one
control item that we use for later assessment of the
Turkers? spell-checking ability. We wrote these con-
trol captions ourselves, modeling them after actual
image descriptions. We paid $0.08 for one task, and
three people completed each task.
3.1 Results
On average, Turkers completed a HIT (judging fifty
sentences) in four minutes, at an average hourly rate
of $1.04. Each sentence in our data set was judged
by three Turkers. The whole experiment finished
141
Figure 3: Screenshot from the grammar/spelling checking task. This is a block of five sentences that Turkers had
to label as using correct or incorrect grammar and spelling. The first sentence is a control item that we included to
monitor the Turkers? performance, and the other four are captions generated by other Turkers in a previous task.
Data set Quality control % Votes for ?correct English?
produced by... performed by... 0 1 2 3
Unqualified writers three Turkers 18.9% 31.2% 26.4% 23.5%
Unqualified writers three experts 11.8% 12.7% 15.3% 60.2%
Qualified writers three experts 0.5% 2.5% 15.0% 82.0%
Table 1: Quality control by Turkers and Experts. The three experts judged 600 sentences from each data set. 565
sentences produced by unqualified workers were also judged by three Turkers.
in under two days, at a total cost of $28.80 (plus
Amazon?s 10% fee). We also selected randomly
600 spell-checked sentences for expert annotation.
Three members of our team (all native speakers of
English) judged each of these sentences in the same
manner as the Turkers. Each sentence could there-
fore get between 0 and 3 Turker votes and between
0 and 3 expert votes for good English. The top two
rows of Table 1 show the distribution of votes in
each of the two groups. We also assess whether the
judgments of the Turkers correlate with our own ex-
pert judgments. Table 2(a) shows the overall agree-
ment between Turkers and expert annotators. The
rest of Table 2 shows how performance of the Turk-
ers on the control items affected agreement with ex-
pert judgments. We define the performance of a
Turker in terms of the average the number of con-
trol items that they got right in each HIT they took.
For each threshold in Tables 2(a)-(d), we considered
only those images for which we have three quality
judgments by workers whose performance is above
the specified threshold.
Our results show that the effectiveness of using
Turkers to filter for grammar and spelling issues is
limited. Overall, the Turker judgments were overly
harsh. The majority Turker vote agrees with the ma-
jority vote of the trained annotators on only 65.1%
of the sentences. Manual inspection of the differ-
ences reveals that the Turkers marked many per-
fectly grammatical English sentences as incorrect
(although they also marked a few which we had
missed). Agreement with experts decreases among
those Turkers that performed better on the control
sentences, with only 56.7% agreement for Turkers
that got all the controls right. In addition, the Turk-
ers are significantly more likely to report false nega-
tives over false positives and this also increases with
performance on the control sentences. (Overall, the
Turkers marked 29.9% of the sentences as false neg-
atives, whereas the Turkers that scored perfectly on
the controls marked 39.3% as false negatives.) Ex-
amination of the areas of high disagreement reveal
that the Turkers were much more likely to vote down
noun phrases than the experts were. The correct ex-
ample captions provided in the instructions of the
quality control test were complete sentences. Some
of the control captions were noun phrases, but all
of the noun phrase controls had some other error
in them. Thus it was possible to either believe that
noun phrases were correct or incorrect, and still be
consistent with the provided examples, and provide
correct judgments on the control sentences.
142
(a) ? 0 controls correct: 565 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 4.4% 3.7% 3.9%
1 3.2% 5.7% 5.0% 17.3%
2 1.8% 2.8% 3.5% 18.2%
3 0.0% 0.4% 2.5% 20.7%
(b) ? 5 controls correct: 553 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 4.5% 3.8% 4.0%
1 3.1% 5.4% 5.1% 17.5%
2 1.8% 2.7% 3.6% 18.4%
3 0.0% 0.4% 2.5% 20.3%
(c) ? 7 controls correct: 331 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 6.3% 3.9% 5.1%
1 3.0% 4.5% 5.1% 24.5%
2 1.8% 1.8% 2.4% 15.1%
3 0.0% 0.0% 2.1% 17.2%
(d) ? 9 controls correct: 127 sentences
Turk Expert votes
votes 0 1 2 3
0 7.9% 6.3% 3.1% 6.3%
1 1.6% 4.7% 6.3% 23.6%
2 0.8% 3.1% 1.6% 15.7%
3 0.0% 0.0% 1.6% 17.3%
Table 2: Quality control: Agreement between Turker and Expert votes, depending on the average number of control
items the Turker voters got right.
4 Quality control through pre-screening
Quality control can also be imposed through a pre-
screening of the Turkers allowed to take the HIT. We
collected another set of five descriptions per image,
but restricted participation to Turkers residing in the
US5, and created a brief qualification test to check
their English. We would like to be able to restrict our
tasks to Turkers who are native speakers and com-
petent spellers and writers of English, regardless of
their country of residence. However, this seems to
be difficult to verify within the current MTurk setup.
Qualification Test Design The qualification test
consists of forty binary questions: fifteen testing
spelling, fifteen testing grammar, and ten testing the
ability to identify good image descriptions.
In all three cases, we started the section with a
set of instructions displaying examples of positive
and negative answers to the tasks. Each spelling
question consisted of a single sentence, and Turk-
ers were asked to determine if all of the words in
the sentence were spelled correctly and if the correct
word was being used (?lose? versus ?loose?). Each
grammar question consisted of a single sentence that
was either correct or included a grammatical error.
Both spelling and grammar checking questions were
based on common mistakes made by foreign English
5As of March 2010, 46.80% of Turkers reside in the U.S
(http://behind-the-enemy-lines.blogspot.
com/ 03/09/2010)
Figure 4: Average caption length (5000 images)
speakers and on grammatical or spelling errors that
occurred in our initial set of image captions. The
grammar and spelling questions are listed in Table
3. The image description questions consisted of one
image shown with two actual captions, and the Turk-
ers were asked which caption better described the
image. In order to pass the qualification test, we
required each annotator to correctly answer at least
twenty-four spelling and grammar questions and at
least eight image description questions. To prevent
Turkers from using the number of question they got
correct to do a brute force search for the correct an-
swers, we simply told them if they passed (?1?) or
failed (?0?). Currently, 1504 people have taken the
qualification test, with a 67.2% passing rate. Since
this qualification test was only required for our HITs
that were restricted to US residents, we assume (but
are not able to verify) that most, if not all, of the
people who took this test are actually US residents.
143
MTurk Set-up We use the same MTurk set-up as
before, but to encourage Turkers to complete the
task even though they first have to pass a qualifica-
tion test, we pay them $0.10 to annotate five images.
4.1 Results
We found that the Turkers who passed the qualifica-
tion provided much better captions for the images.
The average time spent on each image was longer
(four minutes per ten images for the non-qualified
workers versus five minutes per ten images for the
qualified workers). On average, qualified Turk-
ers produced slightly longer sentences (avg. 10.7
words) than non-qualified workers (avg. 10.0 words)
(Figure 4), and the awkward constructions produced
by the unqualified workers were mostly absent. The
entire corpus was annotated in 253 hours at a cost of
$100.00 (plus Amazon?s 10% fee).
We also looked at the rate of misspellings (ap-
proximated by how often our spell-checker indicated
a misspelling). Without the qualification test, Out
of the 600 sentences produced without the qualifica-
tion test, 78 contained misspellings, whereas only 25
sentences out of the 600 produced by the qualified
workers contained misspellings. Furthermore, mis-
spellings in the no-qualification group include many
genuine errors (?the boys are playing in tabel?,
?bycycles?, ?eatting?), whereas misspellings in the
qualification group are largely typos (e.g. Ywo for
Two, tableclothe, chari for chair). Furthermore, the
spell checker corrected all 25 misspellings in the
qualified data set to the intended word, but 27 out of
the 78 misspellings in the data produced by the un-
qualified workers got changed to some other word.
The same three members of our team rated again
the English of 600 randomly selected sentences writ-
ten by Turkers residing in the US who passed our
test. We found a significant improvement in quality
(Table 1, bottom row), with the majority expert vote
accepting over 97% of the sentences. This is also
corroborated by qualitative analysis of the data (see
Figure 5 for examples). Inspection reveals that sen-
tences that are deemed ungrammatical by the experts
typically contain some undetected typo, and would
be correct if these typos could be fixed. Without a
qualification test, there is a significantly greater per-
centage of nonsensical responses such as: ?Is this a
bird squirrel?? and ?thecentury?. In addition, gram-
matically correct but useless fragments such as ?very
dark? and ?peace? only appear without a test. After
requiring the qualification test, the major reasons for
rejection by Turkers are typos such as in ?The two
dogs blend in with the stuff animals? or missing de-
terminers such as in ?a train on tracks in town?.
Overall cost effectiveness Using the no qualifica-
tion test approach, we first paid $50.00 to get 5000
sentences written by unqualified Turkers (which re-
sulted in 4851 non-empty sentences). This resulted
in low-quality data which required further verifica-
tion. Since this is too time-consuming for expert an-
notators, we then paid another $28.80 to get each of
these sentences subsequently checked by three Turk-
ers for grammaticality, resulting in 2222 sentences
which received at least two positive votes for gram-
maticality. With the qualification test approach, we
paid $100.00 to get 5000 sentences written. Based
on our experiments on the set of 600 sentences, ex-
perts would judge over 97% of these sentences as
correct, thus obviating the immediate need for fur-
ther control. That is, it effectively costs more for
non-qualified Turkers to produce sentences that are
judged to be good than for qualified Turkers. Fur-
thermore, their sentences will probably be of lower
quality even after they have been judged acceptable.
5 A corpus of captions for Flickr photos
Encouraged by the success of the qualification test
approach, we extended our corpus to contain 8000
images collected from Flickr. We again paid the
Turkers $0.10 to annotate five images. Our data set
consists of 8108 hand-selected images from Flickr,
depicting actions and events (rather than images de-
picting scenery and mood). These images are more
likely to require full sentence descriptions than the
PASCAL images. We chose six large Flickr groups6
and downloaded a few thousand images from each,
giving us a total of 15,000 candidate images. We re-
moved all black and white or sepia images as well as
images containing photographer signatures or seals.
Next, we manually identified pictures that depicted
the actions of people or animals. For example, we
kept images of people walking in parks, but not of
6The groups: strangers!, Wild-Child (Kids in Action), Dogs
in Action (Read the Rules), Outdoor Activities, Action Photog-
raphy and Flickr-Social (two or more people in the photo)
144
Without qualification test
(1) lady with birds
(2) Some parrots are have speaking skill.
(3) A lady in their dining table with birds on her shoulder and head.
(4) Asian woman with two cockatiels, on shoulder
head, room with oak cabinets.,
(5) The lady loves the parrot
With qualification test
(1) A woman has a bird on her shoulder, and another bird on her head
(2) A woman with a bird on her head and a bird on her shoulder.
(3) A women sitting at a dining table with two small birds sitting on her.
(4) A young Asian woman sitting at a kitchen
table with a bird on her head and another on her shoulder.
(5) Two birds are perched on a woman sitting in a kitchen.
Figure 5: Comparison of captions written by Turkers with and without qualification test
empty parks; we kept several people posing, but not
a close-up of a single person.7 Each HIT asked
Turkers to describe five images. We required the
qualification test and US residency. Average com-
pletion time was a little above 3 minutes for 5 sen-
tences. The corpus was annotated in 284 hours8, at
a total cost of $812.00 (plus Amazon?s 10% fee).
6 Related work and conclusions
Related work MTurk has been used for many dif-
ferent NLP and vision tasks (Tietze et al, 2009;
Zaidan and Callison-Burch, 2009; Snow et al, 2008;
Sorokin and Forsyth, 2008). Due to the noise in-
herent in non-expert annotations, many other at-
tempts at quality control have been made. Kit-
tur et al (2008) solicit ratings about different as-
pects of Wikipedia articles. At first they receive
very noisy results, due to Turkers? not paying at-
tention when completing the task or specifically try-
ing to cheat the requester. They remade the task,
this time starting by asking the Turkers verifiable
questions, speculating that the users would produce
better quality responses when they suspect their an-
swers will be checked. They also added a question
that required the Turkers to comprehend the con-
tent of the Wikipedia article. With this new set-
up, they find that the quality greatly increases and
carelessness is reduced. Kaisser and Lowe (2008)
7Our final data set consists of 1482 pictures from action pho-
tography, 1904 from dogs, 776 from flickr-social, 916 from out-
door, 1257 from strangers and 1773 from wild-child.
8Note that the annotation process scaled pretty well, con-
sidering that annotating more than eight times the number of
images took only 31 hours longer.
collected question and answer pairs by presenting
Turkers with a question and telling them to copy and
paste from a document of text they know to contain
the answer. They achieve a good but far from per-
fect interannotator agreement based on the extracted
answers. We speculate that the quality would be
much worse if the Turkers wrote the sentences them-
selves. Callison-Burch (2009) asks Turkers to pro-
duce translations when given reference sentences in
other languages. Overall, he finds find that Turk-
ers produce better translations than machine transla-
tion systems. To eliminate translations from Turkers
who simply put the reference sentence into an online
translation website, he performs a follow-up task,
where he asks other Turkers to vote on if they believe
that sentences were generated using an online trans-
lation system. Mihalcea and Strapparava (2009) ask
Turkers to produce 4-5 sentence opinion paragraphs
about the death penalty, about abortion and describ-
ing a friend. They report that aside from a small
number of invalid responses, all of the paragraphs
were of good quality and followed their instructions.
Their success is surprising to us because they do not
report using a qualification test, and when we did
this our responses contained a large amount of in-
correct English spelling and grammar.
The TurKit toolkit (Little et al, 2009) provides
another approach to improving the quality of MTurk
annotations. Their iterative framework allows the
requester to set up a series of tasks that first solic-
its text annotations from Turkers and then asks other
Turkers to improve the annotations. They report suc-
cessful results using this methodology, but we chose
145
to stick with simply using the qualification test be-
cause it achieves the desired results already. Fur-
thermore, although using TurKit would have proba-
bly done away with our few remaining grammar and
spelling mistakes, it may have caused the captions
for an image to be a little too similar, and we value
a diversity in the use of words and points of view.
Our experiences We have described our experi-
ences in using Amazon?s Mechanical Turk in the
first half of 2009 to create a corpus of images anno-
tated with descriptive sentences. We implemented
two different approaches to quality control: first, we
did not impose any restrictions on who could write
image descriptions. This was then followed by a sec-
ond set of MTurk tasks where Turkers had to judge
the quality of the sentences generated in our initial
Turk experiments. This approach to quality control
would be cost-effective if the initial data were not
too noisy and the subsequent judgments were ac-
curate and cheap. However, this was not the case,
and quality control on the judgments in the form of
control items turned out to result in even lower ac-
curacy. We then repeated our data collection effort,
but required that Turkers live in the US and take a
brief qualification test that we created to test their
English. This is cost-effective if English proficiency
can be accurately assessed in such a brief qualifica-
tion test. We found that the latter approach was in-
deed far cheaper, and produced significantly better
data. We did not set out to run a scientific experi-
ment comparing different strategies of how to col-
lect linguistic data on Turk, and therefore there may
be multiple explanations for the effects we observe.
Nevertheless, our experience indicates strongly that
even very simple prescreening measures can provide
very effective quality control.
We also extended our corpus to include 8000 im-
ages collected from Flickr. We hope to release this
data to the public for future natural language pro-
cessing and computer vision research.
Recommended practices for usingMTurk in NLP
Our experience indicates that with simple prescreen-
ing, linguistic data can be elicited fairly cheaply and
rapidly from crowd-sourcing services such as Me-
chanical Turk. However, many applications may re-
quire more control over where the data comes from.
Even though NLP data collection differs fundamen-
tally from psycholinguistic experiments that may
elicit production data, our community will typically
also need to know whether data was produced by na-
tive speakers or not. Until MTurk provides a better
mechanism to check the native language of its work-
ers, linguistic data collection on MTurk will have to
rely on potentially very noisy input.
Acknowledgements
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Meaning
of Images. We are grateful for David Forsyth?s ad-
vice and for Alex Sorokin?s support with MTurk.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP 2009.
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2008. The
PASCAL Visual Object Classes Challenge
2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop/.
Michael Kaisser and John Lowe. 2008. Creating a re-
search collection of question answer sentence pairs
with amazons mechanical turk. In LREC 2008.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk. In
Proceedings of SIGCHI 2008.
Greg Little, Lydia B. Chilton, Max Goldman, and
Robert C. Miller. 2009. Turkit: tools for iterative tasks
on mechanical turk. In HCOMP ?09: Proceedings of
the ACM SIGKDD Workshop on Human Computation.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP 2008.
Alexander Sorokin and David Forsyth. 2008. Utility data
annotation with amazon mechanical turk. In Com-
puter Vision and Pattern Recognition Workshop.
Martin I. Tietze, Andi Winterboer, and Johanna D.
Moore. 2009. The effect of linguistic devices in infor-
mation presentation messages on comprehension and
recall. In Proceedings of ENLG 2009.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of EMNLP 2009.
146
Are all of the words correctly spelled and correctly used? Is the sentence grammatically correct?
A group of children playing with thier toys (N) A man giving pose to camera. (N)
He accepts the crowd?s praise graciously. (Y) The white sheep walks on the grass. (Y)
The coffee is kept at a very hot temperture. (N) She is good woman. (N)
A green car is parked in front of a resturant. (N) He should have talk to him. (N)
An orange cat sleeping with a dog that is much larger then it. (N) He has many wonderful toy. (N)
I ate a tasty desert after lunch. (N) He sended the children home to their parents. (N)
A group of people getting ready for a surprise party. (Y) The passage through the hills was narrow. (Y)
A small refrigerator filled with colorful fruits and vegetables. (Y) A sleeping dog. (Y)
Two men fly by in a red plain. (N) The questions on the test was difficult. (N)
A causal picture of a man and a woman. (N) In Finland, we are used to live in a cold climate. (N)
Three men are going out for a special occasion. (Y) Three white sheeps graze on the grassy field. (N)
Woman eatting lots of food. (N) Between you and me, this is wrong. (Y)
Dyning room with chairs. (N) They are living there during six months. (N)
A woman recieving a package. (N) I was given lots of advices about buying new furnitures. (N)
This is a relatively uncommon occurance. (Y) A horse being led back to it?s stall. (N)
Table 3: The spelling and grammar portions of the qualification test. The test may be found on MTurk by searching
for the qualification entitled ?Image Annotation Qualification?.
Figure 6: Screenshot of the image annotation instruc-
tions: guidelines (top) and examples (bottom).
Figure 7: Screenshot of the quality control test instruc-
tions: guidelines (top) and examples (bottom).
147
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 162?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Cross-caption coreference resolution for automatic image understanding
Micah Hodosh Peter Young Cyrus Rashtchian Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
{mhodosh2, pyoung2, crashtc2, juliahmr}@illinois.edu
Abstract
Recent work in computer vision has aimed
to associate image regions with keywords
describing the depicted entities, but ac-
tual image ?understanding? would also re-
quire identifying their attributes, relations
and activities. Since this information can-
not be conveyed by simple keywords, we
have collected a corpus of ?action? photos
each associated with five descriptive cap-
tions. In order to obtain a consistent se-
mantic representation for each image, we
need to first identify which NPs refer to
the same entities. We present three hierar-
chical Bayesian models for cross-caption
coreference resolution. We have also cre-
ated a simple ontology of entity classes
that appear in images and evaluate how
well these can be recovered.
1 Introduction
Many photos capture a moment in time, telling a
brief story of people, animals and objects, their at-
tributes, and their relationship to each other. Al-
though different people may give different inter-
pretations to the same picture, people can read-
ily interpret photos and describe the entities and
events they perceive in complex sentences. This
level of image understanding still remains an elu-
sive goal for computer vision: although current
methods may be able to identify the overall scene
(Quattoni and Torralba, 2009) or some specific
classes of entities (Felzenszwalb et al, 2008), they
are only starting to be able to identify attributes
of entities (Farhadi et al, 2009), and are far from
recovering a complete semantic interpretation of
the depicted situation. Like natural language pro-
cessing, computer vision requires suitable training
data, and there are currently no publicly available
data sets that would enable the development of
such systems.
Photo sharing sites such as Flickr allow users
to annotate images with keywords and other de-
scriptions, and vision researchers have access to
large collections of images annotated with key-
words (e.g. the Corel collection). A lot of recent
work in computer vision has been aimed at pre-
dicting these keywords (Blei et al, 2003; Barnard
et al, 2003; Feng and Lapata, 2008; Deschacht
and Moens, 2007; Jeon et al, 2003). But key-
words alone are not expressive enough to capture
relations between entities. Some research has used
the text that surrounds an image in a news arti-
cle as a proxy (Feng and Lapata, 2008; Deschacht
and Moens, 2007). However, in many cases, the
surrounding text or a user-provided caption does
not simply describe what is depicted in the image
(since this is usually obvious to the human reader
for which this text is intended), but provides ad-
ditional information. We have collected a corpus
of 8108 images associated with several simple de-
scriptive captions. In contrast to the text near an
image on the web, the captions in our corpus pro-
vide direct, if partial and slightly noisy, descrip-
tions of the image content. Our data set differs
from paraphrase corpora (Barzilay and McKeown,
2001; Dolan et al, 2004) in that the different cap-
tions of an image are produced independently by
different writers. There are many ways of describ-
ing the same image, because it is often possible
to focus on different aspects of the depicted situ-
ation, and because certain aspects of the situation
may be unclear to the human viewer.
One of our goals is to use these captions to
obtain a semantic representation of each image
that is consistent with all of its captions. In or-
der to obtain such a representation, it is neces-
sary to identify the entities that appear in the im-
age, and to perform cross-caption coreference res-
olution, i.e. to identify all mentions of the same
entity in the five captions associated with an im-
age. In this paper, we compare different meth-
162
A golden retriever (ANIMAL) is playing with a smaller black and brown dog(ANIMAL) in a pink collar (CLOTHING).
A smaller black dog (ANIMAL) is fighting with a larger brown dog (ANIMAL) in a forest (NAT_BACKGROUND).
A smaller black and brown dog (ANIMAL) is jumping on a large orange dog (ANIMAL).
Brown dog (ANIMAL) with mouth (BODY_PART) open near head(BODY_PART) of black and tan dog (ANIMAL).
Two dogs (ANIMAL) playing near the woods (NAT_BACKGROUND).
Figure 1: An image with five captions from our corpus. Coreference chains and ontological classes are
indicated in color.
ods of cross-caption coreference resolution on our
corpus. In order to facilitate further computer vi-
sion research, we have also defined a set of coarse-
grained ontological classes that we use to automat-
ically categorize the entities in our data set.
2 A corpus of action images and captions
Image collection and sentence annotation We
have constructed a corpus consisting of 8108 pho-
tographs from Flickr.com, each paired with five
one-sentence descriptive captions written by Ama-
zon?s Mechanical Turk1 workers. We downloaded
a few thousand images from each of six selected
Flickr groups2. To facilitate future computer vi-
sion research on our data, we filtered out images in
black-and-white or sepia, as well as images with
watermarks, signatures, borders or other obvious
editing. Since our collection focuses on images
depicting actions, we then filtered out images of
scenery, portraits, and mood photography. This
was done independently by two members of our
group and adjudicated by a third.
We paid Turk workers $0.10 to write one de-
scriptive sentence for each of five distinct and ran-
domly chosen images that were displayed one at
a time. We required a small qualification test
that examined the workers? English grammar and
spelling and we restricted the task to U.S. work-
ers (see Rashtchian et al (2010) for more details).
Our final corpus contains five sentences for each
of our 8108 images, totaling 478,317 word tokens,
and an average sentence length of 11.8 words.
We first spell-checked3 these sentences, and used
OpenNLP4 to POS-tag them. We identified NPs
using OpenNLP?s chunker, followed by a semi-
1https://www.mturk.com
2The groups:?strangers!?, ?Wild-Child (Kids in Action)?,
?Dogs in Action (Read the Rules)?, ?Outdoor Activities?,
?Action Photography?, ?Flickr-Social (two or more people in
the photo)?.
3We used Unix?s aspell to generate possible correc-
tions and chose between them based on corpus frequencies.
4http://opennlp.sourceforge.net
automatic procedure to correct for a number of
systematic chunking errors that could easily be
corrected. We randomly selected 200 images for
further manual annotation, to be used as test and
development data in our experiments.
Gold standard coreference annotation We
manually annotated NP chunks, ontological
classes, and cross-caption coreference chains for
each of the 200 images in our test and development
data. Each image was annotated independently by
two annotators and adjudicated by a third.5 The
development set contains 1604 mentions. On av-
erage, each caption has 3.2 mentions, and each im-
age has 5.9 coreference chains (distinct entities).
Ontological annotation of entities In order to
understand the role entities mentioned in the sen-
tences play in the image, we have defined a simple
ontology of entity classes (Table 1). We distin-
guish entities that constitute the background of an
image from those that appear in the foreground.
These entities can be animate (people or animals)
or inanimate. For inanimate objects, we distin-
guish static objects from ?movable? objects. We
also distinguish man-made from natural objects
and backgrounds, since this matters for computer
vision algorithms. We have labeled the entity
mentions in our test and development data with
classes from this ontology. Again, two of us an-
notated each image?s mentions, and adjudication
was performed by a single person. Our ontology
is similar to, but smaller than the one proposed
by Hollink and Worring (2005) for video retrieval,
which in turn is based on Hoogs et al (2003) and
Hunter (2001).
3 Predicting image entities from captions
Figure 1 shows an image from our corpus. Dif-
ferent captions use different words to refer to the
5We used MMAX2 (Mu?ller and Strube, 2006) both for
annotation and adjudication.
163
Ontological Class Examples
animal dog, horse, cow
background man-made street, pool, carpet
background natural ocean, field, air
body part hair, mouth, arms
clothing shirt, hat, sunglasses
event trick, sunset, game
fixed object man-made furniture, statue, ramp
fixed object natural rock, puddle, bush
image attribute camera, picture, closeup
material man-made paint, frosting
material natural water, snow, dirt
movable man-made ball, toy, bowl
movable natural leaves, snowball
nondepictable something, Batman
orientation front, top, [the] distance
part of edge, side, top, tires
person family, skateboarder
property of shadow, shade, theme
vehicle surfboard, bike, boat
writing graffiti, map
Table 1: Our ontology for entities in images.
same entity, or even seemingly contradictory mod-
ifiers (?orange? vs. ?brown? dog). In order to
predict what entities appear in an image from its
captions, we need to identify how many entities
each sentence describes, and what role these enti-
ties play in the image (e.g. person, animal, back-
ground). Because we have five sentences asso-
ciated with each image, we also need to identify
which noun phrases in the different captions of
the same image refer to the same entity. Because
the captions were generated independently, there
are no discourse cues such as anaphora to identify
coreference. This creates problems for standard
coreference resolution systems trained on regular
text. Our data also differs from standard corefer-
ence data sets in that entities are rarely referred to
by proper nouns.
Our first task is to identify which noun phrases
may refer to the same entity. We do this by identi-
fying the set of entity types that each NP may refer
to. We use WordNet (Fellbaum, 1998) to iden-
tify the possible entity types (WordNet synsets) of
each head noun. Since the salient entities in each
image are likely to be mentioned by more than one
caption writer, we then aim to restrict those types
to those that may be shared by some head nouns in
the other captions of the same image. This gives
us an inventory of entity types for each mention,
which we use to identify coreferences, restricted
by the constraint that all coreferent mentions refer
to an entity of the same type.
4 Using WordNet to identify entity types
WordNet (Fellbaum, 1998) provides a rich ontol-
ogy of entity types that facilitates our coreference
task.6 We use WordNet to obtain a lexicon of pos-
sible entity types for each mention (based on their
lexical heads, assumed to be the last word with a
nominal POS tag7). We first generate a set of can-
didate synsets based solely on the lexical heads,
and then generate lexicon entries based on rela-
tions between the candidates.
WordNet synsets provide us with synonyms,
and hypernym/hyponym relations. For each men-
tion, we generate a list of candidate synsets.
We require that the candidates are one of the
first four synsets reported and that their fre-
quency is to be at least one-tenth of the most
frequent synset. We limit candidates to ones
with ?physical entity#n#1?, ?event#n#1?, or ?vi-
sual property#n#1? as a hypernym, in order to en-
sure that the synset describes something that is de-
pictable. To avoid word senses that refer to a per-
son in a metaphorical fashion, (e.g. pig meaning
slovenly person or red meaning communist), we
ignore synsets that refer to people if the word has
a synset that is an animal or color.8
In general, we would like for mentions to be
able to take on more specific word senses. For ex-
ample, we would like to be able to identify that
?woman? and ?person? may refer to the same
entity, whereas ?man? and ?woman? typically
would not. However, we also do not want a type
inventory that is too large or too fine-grained.
Once the candidate synsets are generated, we
consider all pairs of nouns (n1, n2) that occur in
different captions of the same image and exam-
ine all corresponding pairs of candidate synsets
(s1, s2). If s2 is a synonym or hypernym of s1, it
is possible that two captions have different words
describing the same entity, so we add s1 and s29
to the lexicon of n1. Adding s2 to n1?s lexicon al-
lows it to act as an umbrella sense covering other
nouns describing the same entity.10 We add s2 to
6For the prediction of ontological classes, we use our own
ontology because WordNet is too fine-grained for this pur-
pose.
7If there are two NP chunks that form a ?[NP ... group] of
[NP... ]? construction, we only use the second NP chunk.
8An exception list handles cases (diver, blonde), where the
human sense is more likely than the animal or color sense.
9We don?t add s2 if it is ?object#n#1? or ?clothing#n#1?.
10This is needed when captions use different aspects of
the entity to describe it (for example, ?skier? and ?a skiing
man?).
164
the lexicon of n2 (since if n1 is using the sense s1,
then n2 must be using the sense s2) and if n1 oc-
curs at least five times in the corpus, we add s1 to
the lexicon of n2.
5 A heuristic coreference algorithm
Based on WordNet candidate synsets, we define
a heuristic algorithm that finds the optimal entity
assignment for the mentions associated with each
image. This algorithm is based on the principles
driving our generative model described below, and
on the observation that salient entities will be men-
tioned in many captions and that captions tend to
use similar words to describe the same entity.
Simple heuristic algorithm:
1. For each noun, choose the synset that appears
in the most number of captions of an image,
and break ties by choosing the synset that
covers the fewest distinct lemmatized nouns.
2. Group all of the noun phrase chunks that
share a synset into a single coreference chain.
6 Bayesian coreference models
Since we cannot afford to manually annotate our
entire data set with coreference information, we
follow Haghighi and Klein (2007)?s work on un-
supervised coreference resolution, and develop a
series of generative Bayesian models for our task.
6.1 Model 0: Simple Mixture Model
In our first model, based on Haghighi and Klein?s
baseline Dirichlet Process model, each image i
corresponds to the set of observed mentions wi
from across its captions. Image i has a hidden
global topic Ti, drawn from a distribution with a
GEM prior with hyperparameter ? as explained by
Teh et al (2006). In a Dirichlet process, the GEM
distribution is an infinite analog of the Dirich-
let distribution, allowing for a potentially infinite
number of mixture components. P (Ti = t) is pro-
portional to ? if t is a new component, or to the
number of times t has been drawn before other-
wise. Given a topic choice Ti = t, entity type
assignments Zj for all mentions wj in image i
are in turn drawn from a topic-specific multino-
mial ?t over all possible entity types E that was
drawn from a Dirichlet prior with hyperparameter
?. Similarly, given an entity type Zi = z, each
corresponding (observed) head word wj is drawn
from an entity type-specific multinomial ?z over
all possible words V, drawn from a finite Dirich-
let prior with hyperparameter ?. The set of all im-
ages belonging to the same topic is analogous to
an individual document in Haghighi and Klein?s
baseline model.11 All headwords of the same en-
tity type are assumed to be coreferent, similar to
Haghighi and Klein?s model. As described in sec-
tion 4, we use WordNet to identify the subset of
types that can actually produce the given words.
Therefore, similar to the way Andrzejewski and
Zhu (2009) handled a priori knowledge of topics,
we will define an indicator variable ?ij that is 1
iff the WordNet information allows word i to be
produced from entity set j and 0 otherwise.
6.1.1 Sampling Model 0
We find argmaxZ,TP (Z,T|X) with Gibbs sam-
pling. Here, Z and T are the collection of type
and topic assignments, with Z?j = Z? {Zj} and
T?i = T ? {Ti}. This style of notation will be
extended analogously to other variables. Let ne,x
represent the number of times word x is produced
from entity e across all topics and let pj be the
number of images assigned to topic j. Let mt,e
represent the number of times entity type e is
generated by topic t. Each iteration consists of
two steps: first, each Zi is resampled, fixing T;
and then each Ti is resampled based on Z12.
1. Sampling Zj:
P (Zj=e|wj ? w
i,Z?j ,T) ? P (wj |Zj=e)P (Zj=e|Ti)
P (wj = x|Zj = e) ?
?
n?je,x + ?
P
x? n
?j
e,x? + ?
?
?xe
P (Zj = e|Ti = t) =
m?jt,e + ?
P
e? m
?j
t,e? + ?
2. Sampling Ti:
P (Ti=j|w,Z,T
?i) ? P (Ti=j|T
?i)P (Z|Ti=j,T
?i)
? P (Ti = j|T
?i)
Y
k?wi
P (Zk|Ti = j)
= P (Ti = j|T
?i)
Y
k?wi
m?ij,Zk + ?P
e? m
?i
j,e? + ?
P (Ti = j|T
?i) ?
(
?, If its a new topic
pj Otherwise
11Since we do not have multiple images of the same well-
known people or places, referred to by their names, we do not
perform any cross-image coreference
12Sampling on the exponentiated posterior to find the mode
as Haghighi and Klein (2007) did was found to not signifi-
cantly affect results on our tasks
165
Caption 1: {x21,1:a golden retriever; x21,2 :a smaller black and brown dog; x21,3:a pink collar}
Caption 2: {x21,4:a smaller black dog; x21,5:a larger brown dog; x21,6:a forest}
Caption 3: {x21,7:small black and brown dog; x21,8:a large orange dog}
Caption 4: {x21,9:brown dog; x21,10:mouth; x21,11:head; x21,12:black and tan dog}
Caption 5: {x21,13:two dogs; x21,14:the woods}
DOG
attr:5
DOG
attr:3
CLOTHING
attr:2
FOREST
attr:8
Image 21:
x21,1
x21,5
x21,8
x21,9
MOUTH
attr:0 ...
x21,2 
x21,4 
x21,7 
x21,12 
x21,3 x21,6
x21,14
x21,10 Restaurant 211
Figure 2: Models 1 and 2 as Chinese restaurant franchises: each image topic is a franchise, each image
is a restaurant, each entity is a table, each mention is a customer. Model 2 adds attributes (in italics).
6.2 Model 1: Explicit Entities
Model 0 does not have an explicit representation
of entities beyond their type and thus cannot dis-
tinguish multiple entities of the same type in an
image. Although Model 1 also represents men-
tions only by their head words (and thus cannot
distinguish black dog from brown dog), it creates
explicit entities based on the Chinese restaurant
franchise interpretation of the hierarchical Dirich-
let Process model (Teh et al, 2006). Figure 2 (ig-
noring the modifiers / attributes for now) illustrates
the Chinese restaurant franchise interpretation of
our model. Using this metaphor, there are a se-
ries of restaurants (= images), each consisting of
a potentially infinite number of tables (= entities),
that are frequented by customers (= entity men-
tions) who will be seated at the tables. Restau-
rants belong to franchises (= image topics). Each
table is served one dish (= entity type, e.g. DOG,
CLOTHING) shared by all the customers. The head
word of a mention xi,j is generated in the follow-
ing manner: customer j enters restaurant i (be-
longing to franchise Ti) and sits down at one of
the existing tables with probability proportional to
the number of other customers there, or sits at a
new table with probability proportional to a con-
stant. A dish eia (DOG) from a potentially infinite
menu is served to each new table a, with probabil-
ity proportional to the total number of tables it is
served at in the franchise Ti (or to a constant if it
is a new dish). The (observed) head word of the
mention xj,i (dog, retriever) is then drawn from
the multinomial distribution over words defined by
the entity type (DOG) at the table. The menu (set
of dishes) available to each restaurant and table is
restricted by our lexicon of WordNet synsets for
each mention. More formally, each image topic
t defines a distribution over entities drawn from a
global GEM prior with hyperparameter ?. There-
fore, the probability of an entity a is proportional
to the number of its existing mentions in images of
the same topic, or to ?, if it is previoulsy unmen-
tioned. The type of each entity, ea, is drawn from
a topic-dependent multinomial with global Dirich-
let prior. The head words of mentions are gener-
ated by their entity type as in Model 0. Mentions
assigned to the same entity are considered to be
coreferent. Based on the nature of our corpus, we
again assume that two words cannot be coreferent
within a sentence, restrict the distribution to not
allow inter-sentence coreference and renormalize
the values accordingly.
6.2.1 Sampling Model 1
There are three parts to our resampling procedure:
resampling the entity assignment for each word,
resampling the entity type for each entity, and re-
sampling the topic of each image. The kth word
of image i, sentence j, will now be wij,k; e
i
a is the
entity type of entity a in image i; aij,k is the en-
tity that word k of sentence j is produced from in
image i, and Zij,k represents that entity?s type. a
is the set of all current entity assignments and e
are the type assignments for entities. m is now de-
fined as the number of entities of a certain type be-
ing drawn for an image, n is defined as before and
ci,a is the number of times entity a is expressed in
image i. Topics are resampled as in Model 0.
Entity Assignment Resampling Entity assign-
ments for words are resampled one sentence at a
time in the order the headwords appear in the sen-
tence. For each word in the sentence, entity as-
signments are defined by the distribution of Fig-
ure 3. The headword is assigned to an existing
entity with probability proportional to the number
of entities already assigned to that entity and the
probability that the entity emits that word. The
word is assigned to a new entity with a newly
166
Model 1:
P (eia=e|a,w, Ti = t, e
?i,a) ? (m?e
i,a
t,e + ?)
Q
{wij,k=x|a
i
j,k=a}
n?e
i,a
e,x + ?
P
y(n
?ei,a
e,y + ?)
?x,e
P (aij,k=a|a
?(i,j,k?|k??k), e,T) ?
(
c?j,k
?
i,a P (w
i
j,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))?ij,a, if a is not new
?P (eia|e
?i,a, Ti)P (wij,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k)), o/w
With:
P (wij,k=x|Z
i
j,k=e,a
?(i,j,k?|k??k)) ?
n?(i,j,k
?|k??k)
e,x + ?
P
y(n
?(i,j,k?|k??k)
e,y + ?)
?x,e
P (eia=e|e
?i,a, Ti = t) ?
m?e
i,a
t,e + ?
P
e?(m
?ei,a
t,e? + ?)
Model 2:
P (aij,k=a|a
?(i,j,k?|k??k), e,T,b) ?
(
c?j,k
?
i,a P (w
i
j,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))?ij,aP (d
i
j,k|b
i
a), if a is not new
?P (eia|e
?i,a, Ti)P (wij,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))P (bia)P (d
i
a|b
i
a), o/w
P (bia=b|D
i
a,b
?i,a)?P (bia=b|b
?i,a)
Q
d?Dia
(s?i,ab,d + ?)
P
d?(s
?i,a
b,d? + ?)
Figure 3: Sampling equations for Models 1 and 2
drawn entity type with probability proportional ?,
the probability that the entity type is for an im-
age of the given topic (normalized over WordNet?s
possible entities for the word), and the probability
the drawn type produces the word. ?ij,a = 1 iff
entity a of image i does not appear in sentence j
and ?ij,a = 0 otherwise. a
?(i,j,k?|k??k) represents
removing the kth or later words in sentence j of
image i
Entity Type Resampling Fixing the assign-
ments, the type of each entity is redrawn based
on the distribution in Figure 3. It is proportional
to the probability that a certain entity type is in an
image of a given topic and, independently for each
of the words, the probability that the given word
expresses the type. n?e
i,a
e,x is the number of times
entity type e is expressed as word x not counting
the words attached to the currently entity being re-
sampled and m?e
i,a
t,e is the number of times an en-
tity of type e appears in an image of topic t not
counting the current entity being resampled. The
probability of a given image belonging to a topic is
proportional the number of images already in the
topic (or ?) followed by the probability that each
of the entities in the image were drawn from that
topic.
6.3 Model 2: Explicit Entities and Modifiers
Certain entities cannot be distinguished simply by
head word alone, such in the example in Figure 2.
Model 2 augments Model 1 with the ability to gen-
erate modifiers. In addition to an entity type, each
entity draws an attribute from a global distribution
drawn from a GEM distribution with hyperparam-
eter ?. An attribute is a multinomial distribution,
on possible modifier words, drawn from a Dirich-
let prior with parameter ?. From the attribute, each
modifier word is drawn independently. There-
fore given an attribute b and a set of modifiers d:
P (d|b) ?
?
d?d(sd + ?) where sd is number of
times modifier d is produced by attribute b. In ad-
dition, the probability of a certain attribute b given
all other assignments is given by:
P (bia = b|b
?i,a) ?
(
?, If its a new attribute
rb, Otherwise
where rb is the number of entities with at-
tribute b. As in Model 1, mentions assigned to
the same entity are considered coreferent. Con-
sider the ?smaller black dog? mention in Figure 2.
When the mention is being resampled, the at-
tribute choice for each table will bias the probabil-
ity distribution towards the table whose attribute
is more likely to produce ?smaller? and ?black?.
Therefore, the model can now better distinguish
the two dogs in the image.
6.3.1 Sampling Model 2
The addition of modifiers only directly effects the
distribution when resampling entity assignments
since attributes are independent of entity types,
image topics, and headwords of noun phrases. The
sampling distribution are again shown in Figure 3.
In a separate sampling step, it is now necessary to
resample the attribute assigned to each entity: The
probability of drawing a certain attribute is illus-
trated in Figure 3 with Dia as the set of all the mod-
ifiers of all the noun phrases currently assigned to
167
entity a of image i, and s?i,ab,d as the number of
times attribute b produces modifier d without the
current assignment of entity a of image i.
6.4 Implementation
The topic assignments for each image are initial-
ized to correspond to the Flickr groups the images
were taken from. Each mention was initialized as
its own entity with type and attribute sampled from
a uniform distribution.
As our training is unsupervised, each of the
models were ran on the entire dataset. For Model
0, after burn-in, 20 samples of Z were taken
spaced 200 iterations apart, while for Model 1
samples were taken spaced 100 apart, and 25 apart
for Model 2. The implementation of Model 2 ran
too slow to effectively judge when burn in oc-
curred, impacting the results.
The values of parameters ?, ?, ?, ?, ?, ?, and
the number of initial attributes were hand-tuned
based on the average performance on our anno-
tated development subset of 100 images.13
7 Evaluation of coreference resolution
We evaluate each of the generative models and the
heuristic coreference algorithm on the annotated
test subset of our corpus consisting of 100 images
with both the OpenNLP chunking and the gold
standard chunking. We report our scores based
on the MUC evaluation metric. The results are
reported in Table 2 as the average scores across
all the samples of two independent runs of each
model. We also present results on Model 0 with-
out using WordNet where every word can be an
expression of one of 200 fake entity sets. The
same table also shows the performance of a base-
line model and the upper bound on performance
imposed by WordNet.
A baseline model: In our baseline model, two
noun phrases in captions of the same image are
coreferent if they share the same head noun.
Upper bound on performance: Although
WordNet synsets provide a good indication of
whether two mentions can refer to the same
entity or not, they may also be overly restrictive
in other cases. We measure the upper bound
on performance that our reliance on WordNet
imposes by finding the best-scoring coreference
assignment that is consistent with our lexicon.
13(0.1, 0.1, 100, 0.001875, 100, 0.0002, 20) respectively.
This achieves an F-score of 90.2 on the test data
with gold chunks.
Performance increases in each subsequent
model. The heuristic beats each of the models, but
in some sense it is an extreme version of Model
1. Both it and Model 1 attempt to produce en-
tity sets that cover as many captions as possible,
while minimizing the number of distinct words in-
volved. The heuristic locally forces this case, at
the expense of no longer being a generative model.
8 Ontological Class Prediction
As a further step towards understanding the se-
mantics of images, we develop a model that labels
each entity with one of the ontological classes de-
fined in section 2. The immediate difficulty of this
task is that our ontology includes not only seman-
tic distinctions, but also spatial and visual ones.
While it may be easy to tell which words are an-
imals and which are people, there is only a fine
distinction at the language level whether an object
is movable, fixed, or part of the background.14
8.1 Model and Features
We define our task as a classification problem,
where each entity must be assigned to one of
twenty classes defined by our ontology. We use a
Maximum Entropy classifier, implemented in the
MALLET toolkit (McCallum, 2002), and define
the following text features:
NP Chunk: We include all the words in the NP
chunk, unfiltered.
WordNet Synsets and Hypernyms: The most
likely synset is either the first one that appears in
WordNet or one of the ones predicted by our coref-
erence system. For each of these possibilities, we
include all of that synset?s hypernyms.
Syntactic Role: We parsed our captions with the
C&C parser (Clark and Curran, 2007), and record
whether the word appears as a direct object of a
verb, as the object of a preposition, as the subject
of the sentence, or as a modifier. If it is a modi-
fier, we also add the head word of the phrase being
modified.
14For example, we deem bowls and silverware to be mov-
able objects; furniture, fixed; and carpets, background. More-
over, in all three cases, we must correctly distinguish that
these objects are man-made and not found in nature.
168
Model OpenNLP chunks Gold chunks
Rec. Prec. F1 Rec. Prec. F1
Baseline 57.3 89.5 69.9 64.1 96.2 77.0
Upper bound 82.1 100 90.2
WN Heuristic 70.6 84.8 77.0 80.4 90.6 85.2
Model 0 w/o WN 79.7 59.8 68.4 85.1 62.7 72.2
Model 0 66.8 83.1 74.1 75.9 90.3 82.5
Model 1 69.6 83.8 76.0 78.0 90.8 83.9
Model 2 69.2 84.4 76.1 77.9 91.5 84.1
Table 2: Coreference resolution results (MUC scores; Models 0-2 are averaged over all samples)
8.2 Experiments
We use two baselines. The naive baseline catego-
rizes words by selecting the most frequent class
of the word. If no instances of the word have oc-
curred, it uses the overall most frequent class. The
WordNet baseline works by finding the most fre-
quent class amongst the most relevant synsets for
a word. It calculates the class frequency for each
synset by assuming each word has the sense of its
first synset and incrementing the frequency of the
first synset and its hypernyms. When categorizing
a word, it finds the set of closest hypernyms of the
word that have a non-zero frequency, and chooses
the class with the greatest sum of frequency counts
amongst those hypernyms.
We train the MaxEnt classifier using semi-
supervised learning. Initially, we train a classifier
using the 500 sentence gold standard development
set. For each class, we use the top 5%15 of the la-
bels to label the unlabeled data and provide addi-
tional training data. We then retrain the classifier
on the newly labeled examples and the develop-
ment set, and run it on the test set. For each coref-
erence chain in the test set, we relabel all of the
mentions in the chain to use the majority class, if
a clear majority exists. If no such majority exists,
we leave the labels as is. The MaxEnt classifier
experiments were conducted by varying the source
of the synset assigned to each word. For each of
our coreference systems, we report two scores (Ta-
ble 3). The first is the average accuracy when us-
ing the output from two runs of each model with
about 20 samples per run, and the second uses the
output that performs best on the coreference task
when scored on the development data.
Discussion Although we use WordNet to clas-
sify our entity mentions, we designed our ontology
by considering only the images and their captions,
with no particular mapping to WordNet in mind.
15This was tuned using 10-fold cross-validation of the de-
velopment set.
Classifier (synset prediction) Accuracy (gold chunks)
Naive Baseline 72.0
WordNet Baseline 81.0
MaxEnt (1st-synset) 84.4
MaxEnt (WN heuristic) 82.7
Avg. ? Best-Coref
MaxEnt (Model 1) 83.9 0.5 84.5
MaxEnt (Model 2) 84.1 0.4 85.3
Table 3: Prediction of ontological classes
Therefore, these experiments provide of a proof of
concept for the semi-supervised labeling of a cor-
pus using any semantic/visual ontology.
Overall, Model 2 had the best performance for
this task. This demonstrates that the additional
features of Model 2 force synset selections that are
consistent across the entire corpus, and are sen-
sitive to the modifiers appearing with them. The
WordNet heuristic selects synsets in a fairly ar-
bitrary manner - all other things being equal, the
synsets are chosen without reference to what other
synsets are chosen by similar clusters of nouns.
9 Evaluating entity prediction
Together, the coreference resolution algorithm and
ontology classification model provide us with a set
of distinct, ontologically-categorized entities ap-
pearing in each image. We perform a final experi-
ment to evaluate how well our models can recover
the mentioned entities and their ontological types
for each image. We now represent each entity as a
tuple (L, c), where L is its coreference chain, and
c is the ontological class of these mentions. 16
We compute the precision and recall between
the predicted and gold standard tuples for each im-
age. We consider a tuple (L?, c?) correctly pre-
dicted only when a copy of (L?, c?) occurs both
in the set of predicted tuples and the set of gold
standard tuples.17 Then, as usual, for precision we
16Note that for each image, the tuples of all entities corre-
spond to a partition of the set of the head-word mentions in
an image.
17We assign no partial credit because incorrect typing or
169
Model Recall Precision F-score
Baseline 28.4 20.6 23.9
WordNet Heuristic 48.3 43.9 46.0
Model 1 (avg) 51.7 42.8 46.8
Model 1 (best-coref) 50.9 45.4 48.0
Model 2 (avg) 52.2 42.7 47.0
Model 2 (best-coref) 52.3 46.0 49.0
Table 4: Overall entity recovery. We measure
how many entities we identify correctly (requiring
complete recovery of their coreference chains and
correct prediction of their ontological class.
normalize the number of overlapping tuples by the
number of predicted tuples, and for recall, by the
number of gold standard tuples. We report average
precision and recall over all images in our test set.
We report scores for four different pairs of on-
tological class and coreference chain predictions.
As a baseline, we use the ontological classes pre-
dicted by the our naive baseline and the chains pre-
dicted by the ?same-words-are-coreferent? coref-
erence resolution baseline.
We also report results using the classes and
chains predicted by Model 1, Model 2, and the
WordNet Heuristic Algorithm. The influence of
the different coreference algorithms comes from
the entity types that are used to determine corefer-
ence chains, and that also correspond to WordNet
candidate synsets. In other words, although the
final coreference chain may be predicted by two
different models, the synsets they use to do so may
differ, affecting the synset and hypernym features
used for ontological prediction. We present results
in Table 4 for these four different set-ups.
The synsets chosen by the different corefer-
ence algorithms clearly have different applicabil-
ity when it comes to ontological class prediction.
Although Model 2 performs comparably to Model
1 and does worse than the WordNet heuristic al-
gorithm for coreference chain prediction, it cer-
tainly does better on this task. Since our end goal
is creating a unified semantic representation, this
final task judges the effectiveness of our models to
capture the most detailed entity information. The
success of Model 2 means that the incorporation
of adjectives informs the proper choice of synsets
that are useful in predicting ontological classes.
10 Conclusion
As a first step towards automatic image under-
standing, we have collected a corpus of images as-
incomplete coreference chaining both completely change the
semantics of an image.
sociated with several simple descriptive captions,
which provide more detailed information about
the image than simple keywords. We plan to make
this data set available for further research in com-
puter vision and natural language processing. In
order to enable the creation of a semantic repre-
sentation of the image content that is consistent
with the captions in our data set, we use Word-
Net and a series of Bayesian models to perform
cross-caption coreference resolution. Similar to
Haghighi and Klein (2009), who find that linguis-
tic heuristics can provide very strong baselines for
standard coreference resultion, relatively simple
heuristics based on WordNet alne perform sur-
prisingly well on our task, although they are out-
performed by our Bayesian models for overall en-
tity prediction. Since our generative models are
based on Dirichlet Process priors, they are de-
signed to favor a small number of unique entities
per image. In the heuristic algorithm, this bias
is built in explicitly, resulting in slightly higher
performance on the coreference resolution task.
However, while the generative models can use
global information to learn what entity type each
word is likely to represent, the heuristic is unable
to capture any non-local information about the en-
tities, and thus provides less useful input for the
prediction of ontological classes.
Future work will aim to improve on these re-
sults by overcoming the upper bound on perfor-
mance imposed by WordNet, and through a more
sophisticated model of modifiers. We will also in-
vestigate how image features can be incorporated
into our model to improve performance on entity
detection. Ultimately, identifying the depicted en-
tities from multiple image captions will require
novel ways to correctly handle the semantics of
plural NPs (i.e. that one caption?s ?two dogs? con-
sist of another?s ?golden retreiver? and ?smaller
black dog?). We foresee similar challenges when
dealing with verbs and events.
The creation of an actual semantic representa-
tion of the image content is a challenging problem
in itself, since the different captions often focus
on different aspects of the depicted situation, or
provide different interpretation of ambiguous sit-
uations. We believe that this poses many inter-
esting challenges for natural language processing,
and will ultimately require ways to integrate the
information conveyed in the caption with visual
features extracted from the image.
170
Acknowledgements
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Mean-
ing of Images. We are grateful for David Forsyth
and Dan Roth?s advice, and for Alex Sorokins sup-
port with MTurk.
References
David Andrzejewski and Xiaojin Zhu. 2009. Latent
Dirichlet alocation with topic-in-set knowledge. In
NAACL HLT 2009 Workshop on Semi-Supervised
Learning for Natural Language Processing, pages
43?48.
Kobus Barnard, Pinar Duygulu, David Forsyth,
Nando De Freitas, David M. Blei, and Michael I.
Jordan. 2003. Matching words and pictures. Jour-
nal of Machine Learning Research, 3:1107?1135.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of the 39th annual meeting of the Asso-
ciation for Computational Linguistics, pages 50?57,
Toulouse, France, July.
David M. Blei, Michael I, David M. Blei, and Michael
I. 2003. Modeling annotated data. In Proceedings
of the 26th International ACM SIGIR Conference,
pages 127?134.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of Coling 2004, pages 350?356,
Geneva, Switzerland, August. COLING.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing objects by their attributes. In IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 1778?1785, June.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
P. Felzenszwalb, D. McAllester, and D. Ramanan.
2008. A discriminatively trained, multiscale, de-
formable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1?8,
June.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of ACL-08: HLT, pages 272?280,
Columbus, Ohio, June.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855, Prague, Czech Republic.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152?1161, Singapore, August. Associ-
ation for Computational Linguistics.
L. Hollink and M. Worring. 2005. Building a vi-
sual ontology for video retrieval. In MULTIMEDIA
?05: Proceedings of the 13th annual ACM interna-
tional conference on Multimedia, pages 479?482,
New York, NY, USA. ACM.
A. Hoogs, J. Rittscher, G. Stein, and J. Schmiederer.
2003. Video content annotation using visual anal-
ysis and a large semantic knowledgebase. In IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, volume 2, pages II?327 ?
II?334 vol.2, June.
Jane Hunter. 2001. Adding multimedia to the semantic
web - building an mpeg-7 ontology. In In Interna-
tional Semantic Web Working Symposium (SWWS,
pages 261?281.
Lavrenko Manmatha Jeon, V. Lavrenko, R. Manmatha,
and J. Jeon. 2003. A model for learning the seman-
tics of pictures. In Seventeenth Annual Conference
on Neural Information Processing Systems (NIPS).
MIT Press.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Christoph Mu?ller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Sabine Braun et al editor, Corpus Technology and
Language Pedagogy, pages 197?214. Peter Lang,
Frankfurt a.M., Germany.
Ariadna Quattoni and Antonio B. Torralba. 2009.
Recognizing indoor scenes. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
413?420. IEEE.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazons mechanical turk. In NAACL
Workshop Creating Speech and Language Data With
Amazons Mechanical Turk.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
171
