MATCH: An Architecture for Multimodal Dialogue Systems
Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent
Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932, USA
johnston,srini,guna,ehlen,walker,stevew,pmaloor@research.att.com
Now at SUNY Stonybrook, stent@cs.sunysb.edu
Abstract
Mobile interfaces need to allow the user
and system to adapt their choice of com-
munication modes according to user pref-
erences, the task at hand, and the physi-
cal and social environment. We describe a
multimodal application architecture which
combines finite-state multimodal language
processing, a speech-act based multimodal
dialogue manager, dynamic multimodal
output generation, and user-tailored text
planning to enable rapid prototyping of
multimodal interfaces with flexible input
and adaptive output. Our testbed appli-
cation MATCH (Multimodal Access To
City Help) provides a mobile multimodal
speech-pen interface to restaurant and sub-
way information for New York City.
1 Multimodal Mobile Information Access
In urban environments tourists and residents alike
need access to a complex and constantly changing
body of information regarding restaurants, theatre
schedules, transportation topology and timetables.
This information is most valuable if it can be de-
livered effectively while mobile, since places close
and plans change. Mobile information access devices
(PDAs, tablet PCs, next-generation phones) offer
limited screen real estate and no keyboard or mouse,
making complex graphical interfaces cumbersome.
Multimodal interfaces can address this problem by
enabling speech and pen input and output combining
speech and graphics (See (Andre?, 2002) for a detailed
overview of previous work on multimodal input and
output). Since mobile devices are used in different
physical and social environments, for different tasks,
by different users, they need to be both flexible in in-
put and adaptive in output. Users need to be able to
provide input in whichever mode or combination of
modes is most appropriate, and system output should
be dynamically tailored so that it is maximally effec-
tive given the situation and the user?s preferences.
We present our testbed multimodal application
MATCH (Multimodal Access To City Help) and the
general purpose multimodal architecture underlying
it, that: is designed for highly mobile applications;
enables flexible multimodal input; and provides flex-
ible user-tailored multimodal output.
Figure 1: MATCH running on Fujitsu PDA
Highly mobile MATCH is a working city guide
and navigation system that currently enables mobile
users to access restaurant and subway information for
New York City (NYC). MATCH runs standalone on
a Fujitsu pen computer (Figure 1), and can also run
in client-server mode across a wireless network.
Flexible multimodal input Users interact with a
graphical interface displaying restaurant listings and
a dynamic map showing locations and street infor-
mation. They are free to provide input using speech,
by drawing on the display with a stylus, or by us-
ing synchronous multimodal combinations of the two
modes. For example, a user might ask to see cheap
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 376-383.
                         Proceedings of the 40th Annual Meeting of the Association for
Italian restaurants in Chelsea by saying show cheap
italian restaurants in chelsea, by circling an area on
the map and saying show cheap italian restaurants
in this neighborhood; or, in a noisy or public envi-
ronment, by circling an area and writing cheap and
italian (Figure 2). The system will then zoom to the
appropriate map location and show the locations of
restaurants on the map. Users can ask for information
about restaurants, such as phone numbers, addresses,
and reviews. For example, a user might circle three
restaurants as in Figure 3 and say phone numbers for
these three restaurants (or write phone). Users can
also manipulate the map interface directly. For exam-
ple, a user might say show upper west side or circle
an area and write zoom.
Figure 2: Unimodal pen command
Flexible multimodal output MATCH provides
flexible, synchronized multimodal generation and
can take initiative to engage in information-seeking
subdialogues. If a user circles the three restaurants in
Figure 3 and writes phone, the system responds with
a graphical callout on the display, synchronized with
a text-to-speech (TTS) prompt of the phone number,
for each restaurant in turn (Figure 4).
Figure 3: Two area gestures
Figure 4: Phone query callouts
The system also provides subway directions. If the
user says How do I get to this place? and circles one
of the restaurants displayed on the map, the system
will ask Where do you want to go from? The user
can then respond with speech (e.g., 25th Street and
3rd Avenue), with pen by writing (e.g., 25th St & 3rd
Ave), or multimodally ( e.g, from here with a circle
gesture indicating location). The system then calcu-
lates the optimal subway route and dynamically gen-
erates a multimodal presentation of instructions. It
starts by zooming in on the first station and then grad-
ually zooms out, graphically presenting each stage of
the route along with a series of synchronized TTS
prompts. Figure 5 shows the final display of a sub-
way route heading downtown on the 6 train and trans-
ferring to the L train Brooklyn bound.
Figure 5: Multimodal subway route
User-tailored generation MATCH can also pro-
vide a user-tailored summary, comparison, or rec-
ommendation for an arbitrary set of restaurants, us-
ing a quantitative model of user preferences (Walker
et al, 2002). The system will only discuss restau-
rants that rank highly according to the user?s dining
preferences, and will only describe attributes of those
restaurants the user considers important. This per-
mits concise, targeted system responses. For exam-
ple, the user could say compare these restaurants and
circle a large set of restaurants (Figure 6). If the user
considers inexpensiveness and food quality to be the
most important attributes of a restaurant, the system
response might be:
Compare-A: Among the selected restaurants, the following
offer exceptional overall value. Uguale?s price is 33 dollars. It
has excellent food quality and good decor. Da Andrea?s price is
28 dollars. It has very good food quality and good decor. John?s
Pizzeria?s price is 20 dollars. It has very good food quality and
mediocre decor.
Figure 6: Comparing a large set of restaurants
2 Multimodal Application Architecture
The multimodal architecture supporting MATCH
consists of a series of agents which communicate
through a facilitator MCUBE (Figure 7).
Figure 7: Multimodal Architecture
MCUBE is a Java-based facilitator which enables
agents to pass messages either to single agents or
groups of agents. It serves a similar function to sys-
tems such as OAA (Martin et al, 1999), the use of
KQML for messaging in Allen et al(2000), and the
Communicator hub (Seneff et al, 1998). Agents may
reside either on the client device or elsewhere on the
network and can be implemented in multiple differ-
ent languages. MCUBE messages are encoded in
XML, providing a general mechanism for message
parsing and facilitating logging.
Multimodal User Interface Users interact with
the system through the Multimodal UI, which is
browser-based and runs in Internet Explorer. This
greatly facilitates rapid prototyping, authoring, and
reuse of the system for different applications since
anything that can appear on a webpage (dynamic
HTML, ActiveX controls, etc.) can be used in
the visual component of a multimodal user inter-
face. A TCP/IP control enables communication with
MCUBE.
MATCH uses a control that provides a dynamic
pan-able, zoomable map display. The control has ink
handling capability. This enables both pen-based in-
teraction (on the map) and normal GUI interaction
(on the rest of the page) without requiring the user to
overtly switch ?modes?. When the user draws on the
map their ink is captured and any objects potentially
selected, such as currently displayed restaurants, are
identified. The electronic ink is broken into a lat-
tice of strokes and sent to the gesture recognition
and handwriting recognition components which en-
rich this stroke lattice with possible classifications of
strokes and stroke combinations. The UI then trans-
lates this stroke lattice into an ink meaning lattice
representing all of the possible interpretations of the
user?s ink and sends it to MMFST.
In order to provide spoken input the user must tap
a click-to-speak button on the Multimodal UI. We
found that in an application such as MATCH which
provides extensive unimodal pen-based interaction, it
is preferable to use click-to-speak rather than pen-
to-speak or open-mike. With pen-to-speak, spurious
speech results received in noisy environments can
disrupt unimodal pen commands.
The Multimodal UI also provides graphical output
capabilities and performs synchronization of multi-
modal output. For example, it synchronizes the dis-
play actions and TTS prompts in the answer to the
route query mentioned in Section 1.
Speech Recognition MATCH uses AT&T?s Wat-
son speech recognition engine. A speech manager
running on the device gathers audio and communi-
cates with a recognition server running either on the
device or on the network. The recognition server pro-
vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture
and handwriting recognition agents provide possible
classifications of electronic ink for the UI. Recogni-
tions are performed both on individual strokes and
combinations of strokes in the input ink lattice. The
handwriting recognizer supports a vocabulary of 285
words, including attributes of restaurants (e.g. ?chi-
nese?,?cheap?) and zones and points of interest (e.g.
?soho?,?empire?,?state?,?building?). The gesture rec-
ognizer recognizes a set of 10 basic gestures, includ-
ing lines, arrows, areas, points, and question marks.
It uses a variant of Rubine?s classic template-based
gesture recognition algorithm (Rubine, 1991) trained
on a corpus of sample gestures. In addition to classi-
fying gestures the gesture recognition agent also ex-
tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-
written words provide a rich visual vocabulary for
multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-
tice as symbol complexes of the following form: G
FORM MEANING (NUMBER TYPE) SEM. FORM
indicates the physical form of the gesture and has val-
ues such as area, point, line, arrow. MEANING indi-
cates the meaning of that form; for example an area
can be either a loc(ation) or a sel(ection). NUMBER
and TYPE indicate the number of entities in a selec-
tion (1,2,3, many) and their type (rest(aurant), the-
atre). SEM is a place holder for the specific content
of the gesture, such as the points that make up an area
or the identifiers of objects in a selection.
When multiple selection gestures are present
an aggregation technique (Johnston and Bangalore,
2001) is employed to overcome the problems with
deictic plurals and numerals described in John-
ston (2000). Aggregation augments the ink meaning
lattice with aggregate gestures that result from com-
bining adjacent selection gestures. This allows a de-
ictic expression like these three restaurants to com-
bine with two area gestures, one which selects one
restaurant and the other two, as long as their sum is
three. For example, if the user makes two area ges-
tures, one around a single restaurant and the other
around two restaurants (Figure 3), the resulting ink
meaning lattice will be as in Figure 8. The first ges-
ture (node numbers 0-7) is either a reference to a
location (loc.) (0-3,7) or a reference to a restaurant
(sel.) (0-2,4-7). The second (nodes 7-13,16) is either
a reference to a location (7-10,16) or to a set of two
restaurants (7-9,11-13,16). The aggregation process
applies to the two adjacent selections and adds a se-
lection of three restaurants (0-2,4,14-16). If the user
says show chinese restaurants in this neighborhood
and this neighborhood, the path containing the two
locations (0-3,7-10,16) will be taken when this lat-
tice is combined with speech in MMFST. If the user
says tell me about this place and these places, then
the path with the adjacent selections is taken (0-2,4-
9,11-13,16). If the speech is tell me about these or
phone numbers for these three restaurants then the
aggregate path (0-2,4,14-16) will be chosen.
Multimodal Integrator (MMFST) MMFST re-
ceives the speech lattice (from the Speech Manager)
and the ink meaning lattice (from the UI) and builds
a multimodal meaning lattice which captures the po-
tential joint interpretations of the speech and ink in-
puts. MMFST is able to provide rapid response times
by making unimodal timeouts conditional on activity
in the other input mode. MMFST is notified when the
user has hit the click-to-speak button, when a speech
result arrives, and whether or not the user is inking on
the display. When a speech lattice arrives, if inking
is in progress MMFST waits for the ink meaning lat-
tice, otherwise it applies a short timeout (1 sec.) and
treats the speech as unimodal. When an ink meaning
lattice arrives, if the user has tapped click-to-speak
MMFST waits for the speech lattice to arrive, other-
wise it applies a short timeout (1 sec.) and treats the
ink as unimodal.
MMFST uses the finite-state approach to multi-
modal integration and understanding proposed by
Johnston and Bangalore (2000). Possibilities for
multimodal integration and understanding are cap-
tured in a three tape device in which the first tape
represents the speech stream (words), the second the
ink stream (gesture symbols) and the third their com-
bined meaning (meaning symbols). In essence, this
device takes the speech and ink meaning lattices as
inputs, consumes them using the first two tapes, and
writes out a multimodal meaning lattice using the
third tape. The three tape finite-state device is sim-
ulated using two transducers: G:W which is used to
align speech and ink and G W:M which takes a com-
posite alphabet of speech and gesture symbols as in-
put and outputs meaning. The ink meaning lattice
G and speech lattice W are composed with G:W and
the result is factored into an FSA G W which is com-
posed with G W:M to derive the meaning lattice M.
In order to capture multimodal integration using
finite-state methods, it is necessary to abstract over
specific aspects of gestural content (Johnston and
Bangalore, 2000). For example, all possible se-
quences of coordinates that could occur in an area
gesture cannot be encoded in the finite-state device.
We employ the approach proposed in (Johnston and
Bangalore, 2001) in which the ink meaning lattice is
converted to a transducer I:G, where G are gesture
symbols (including SEM) and I contains both gesture
symbols and the specific contents. I and G differ only
in cases where the gesture symbol on G is SEM, in
which case the corresponding I symbol is the specific
interpretation. After multimodal integration a pro-
jection G:M is taken from the result G W:M machine
and composed with the original I:G in order to rein-
corporate the specific contents that were left out of
the finite-state process (I:G o G:M = I:M).
The multimodal finite-state transducers used at
runtime are compiled from a declarative multimodal
context-free grammar which captures the structure
Figure 8: Ink Meaning Lattice
and interpretation of multimodal and unimodal com-
mands, approximated where necessary using stan-
dard approximation techniques (Nederhof, 1997).
This grammar captures not just multimodal integra-
tion patterns but also the parsing of speech and ges-
ture, and the assignment of meaning. In Figure 9 we
present a small simplified fragment capable of han-
dling MATCH commands such as phone numbers for
these three restaurants. A multimodal CFG differs
from a normal CFG in that the terminals are triples:
W:G:M, where W is the speech stream (words), G
the ink stream (gesture symbols) and M the meaning
stream (meaning symbols). An XML representation
for meaning is used to facilate parsing and logging
by other system components. The meaning tape sym-
bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is
empty in a given terminal.
When the user says phone numbers for these
three restaurants and circles two groups of restau-
rants (Figure 3). The gesture lattice (Figure 8) is
turned into a transducer I:G with the same sym-
bol on each side except for the SEM arcs which are
split. For example, path 15-16 SEM([id1,id2,id3])
becomes [id1,id2,id3]:SEM. After G and the speech
W are integrated using G:W and G W:M. The G path
in the result is used to re-establish the connection
between SEM symbols and their specific contents
in I:G (I:G o G:M = I:M). The meaning read off
I:M is<cmd><phone><restaurant> [id1,id2,id3]
</restaurant> </phone> </cmd>. This is passed
to the multimodal dialog manager (MDM) and from
there to the Multimodal UI resulting in a display like
Figure 4 with coordinated TTS output. Since the
speech input is a lattice and there is also potential
for ambiguity in the multimodal grammar, the output
from MMFST to MDM is an N-best list of potential
multimodal interpretations.
Multimodal Dialog Manager (MDM) The MDM
is based on previous work on speech-act based mod-
els of dialog (Stent et al, 1999; Rich and Sidner,
1998). It uses a Java-based toolkit for writing dialog
managers that is similar in philosophy to TrindiKit
(Larsson et al, 1999). It includes several rule-based
S ! eps:eps:<cmd> CMD eps:eps:</cmd>
CMD ! phone:eps:<phone> numbers:eps:eps
for:eps:eps DEICTICNP
eps:eps:</phone>
DEICTICNP ! DDETPL eps:area:eps eps:selection:eps
NUM RESTPL eps:eps:<restaurant>
eps:SEM:SEM eps:eps:</restaurant>
DDETPL ! these:G:eps
RESTPL ! restaurants:restaurant:eps
NUM ! three:3:eps
Figure 9: Multimodal grammar fragment
processes that operate on a shared state. The state
includes system and user intentions and beliefs, a di-
alog history and focus space, and information about
the speaker, the domain and the available modalities.
The processes include interpretation, update, selec-
tion and generation processes.
The interpretation process takes as input an N-best
list of possible multimodal interpretations for a user
input from MMFST. It rescores them according to a
set of rules that encode the most likely next speech
act given the current dialogue context, and picks the
most likely interpretation from the result. The update
process updates the dialogue context according to the
system?s interpretation of user input. It augments the
dialogue history, focus space, models of user and sys-
tem beliefs, and model of user intentions. It also al-
ters the list of current modalities to reflect those most
recently used by the user.
The selection process determines the system?s next
move(s). In the case of a command, request or ques-
tion, it first checks that the input is fully specified
(using the domain ontology, which contains informa-
tion about required and optional roles for different
types of actions); if it is not, then the system?s next
move is to take the initiative and start an information-
gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or
answer the question; to do this, MDM communicates
with the UI. Since MDM is aware of the current set
of preferred modalities, it can provide feedback and
responses tailored to the user?s modality preferences.
The generation process performs template-based
generation for simple responses and updates the sys-
tem?s model of the user?s intentions after generation.
The text planner is used for more complex genera-
tion, such as the generation of comparisons.
In the route query example in Section 1, MDM first
receives a route query in which only the destination
is specified How do I get to this place? In the se-
lection phase it consults the domain model and de-
termines that a source is also required for a route.
It adds a request to query the user for the source to
the system?s next moves. This move is selected and
the generation process selects a prompt and sends it
to the TTS component. The system asks Where do
you want to go from? If the user says or writes 25th
Street and 3rd Avenue then MMFST will assign this
input two possible interpretations. Either this is a re-
quest to zoom the display to the specified location or
it is an assertion of a location. Since the MDM dia-
logue state indicates that it is waiting for an answer
of the type location, MDM reranks the assertion as
the most likely interpretation. A generalized overlay
process (Alexandersson and Becker, 2001) is used to
take the content of the assertion (a location) and add
it into the partial route request. The result is deter-
mined to be complete. The UI resolves the location
to map coordinates and passes on a route request to
the SUBWAY component.
We found this traditional speech-act based dia-
logue manager worked well for our multimodal inter-
face. Critical in this was our use of a common seman-
tic representation across spoken, gestured, and multi-
modal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users
flexibility in the mode they choose to advance the di-
alogue. On the other hand, mode sensitivity is also
important since user modality choice can be used to
determine system mode choice for confirmation and
other responses.
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database
of the NYC subway system. When it receives a route
request with the desired source and destination points
from the Multimodal UI, it explores the search space
of possible routes to identify the optimal one, using a
cost function based on the number of transfers, over-
all number of stops, and the walking distance from
the station at each end. It builds a list of actions re-
quired to reach the destination and passes them to the
multimodal generator.
Multimodal Generator and Text-to-speech The
multimodal generator processes action lists from
SUBWAY and other components and assigns appro-
priate prompts for each action using a template-based
generator. The result is a ?score? of prompts and ac-
tions which is passed to the Multimodal UI. The Mul-
timodal UI plays this ?score? by coordinating changes
in the interface with the corresponding TTS prompts.
AT&T?s Natural Voices TTS engine is used to pro-
vide the spoken output. When the UI receives a mul-
timodal score, it builds a stack of graphical actions
such as zooming the display to a particular location
or putting up a graphical callout. It then sends the
prompts to be rendered by the TTS server. As each
prompt is synthesized the TTS server sends progress
notifications to the Multimodal UI, which pops the
next graphical action off the stack and executes it.
Text Planner and User Model The text plan-
ner receives instructions from MDM for execution
of ?compare?, ?summarize?, and ?recommend? com-
mands. It employs a user model based on multi-
attribute decision theory (Carenini and Moore, 2001).
For example, in order to make a comparison between
the set of restaurants shown in Figure 6, the text
planner first ranks the restaurants within the set ac-
cording to the predicted ranking of the user model.
Then, after selecting a small set of the highest ranked
restaurants, it utilizes the user model to decide which
restaurant attributes are important to mention. The
resulting text plan is converted to text and sent to TTS
(Walker et al, 2002). A user model for someone who
cares most highly about cost and secondly about food
quality and decor leads to a system response such as
that in Compare-A above. A user model for someone
whose selections are driven by food quality and food
type first, and cost only second, results in a system
response such as that shown in Compare-B.
Compare-B: Among the selected restaurants, the following of-
fer exceptional overall value. Babbo?s price is 60 dollars. It has
superb food quality. Il Mulino?s price is 65 dollars. It has superb
food quality. Uguale?s price is 33 dollars. It has excellent food.
Note that the restaurants selected for the user who
is not concerned about cost includes two rather more
expensive restaurants that are not selected by the text
planner for the cost-oriented user.
Multimodal Logger User studies, multimodal data
collection, and debugging were accomplished by in-
strumenting MATCH agents to send details of user
inputs, system processes, and system outputs to a log-
ger agent that maintains an XML log designed for
multimodal interactions. Our critical objective was
to collect data continually throughout system devel-
opment, and to be able to do so in mobile settings.
While this rendered the common practice of video-
taping user interactions impractical, we still required
high fidelity records of each multimodal interaction.
To address this problem, MATCH logs the state of
the UI and the user?s ink, along with detailed data
from other components. These components can in
turn dynamically replay the user?s speech and ink as
they were originally received, and show how the sys-
tem responded. The browser- and component-based
architecture of the Multimodal UI facilitated its reuse
in a Log Viewer that reads multimodal log files, re-
plays interactions between the user and system, and
allows analysis and annotation of the data. MATCH?s
logging system is similar in function to STAMP (Ovi-
att and Clow, 1998), but does not require multimodal
interactions to be videotaped and allows rapid re-
configuration for different annotation tasks since it
is browser-based. The ability of the system to log
data standalone is important, since it enables testing
and collection of multimodal data in realistic mobile
environments without relying on external equipment.
3 Experimental Evaluation
Our multimodal logging infrastructure enabled
MATCH to undergo continual user trials and evalu-
ation throughout development. Repeated evaluations
with small numbers of test users both in the lab and
in mobile settings (Figure 10) have guided the design
and iterative development of the system.
Figure 10: Testing MATCH in NYC
This iterative development approach highlighted
several important problems early on. For example,
while it was originally thought that users would for-
mulate queries and navigation commands primarily
by specifying the names of New York neighborhoods,
as in show italian restaurants in chelsea, early field
test studies in the city revealed that the need for
neighborhood names in the grammar was minimal
compared to the need for cross-streets and points of
interest; hence, cross-streets and a sizable list of land-
marks were added. Other early tests revealed the
need for easily accessible ?cancel? and ?undo? fea-
tures that allow users to make quick corrections. We
also discovered that speech recognition performance
was initially hindered by placement of the ?click-to-
speak? button and the recognition feedback box on
the bottom-right side of the device, leading many
users to speak ?to? this area, rather than toward the
microphone on the upper left side. This placement
also led left-handed users to block the microphone
with their arms when they spoke. Moving the but-
ton and the feedback box to the top-left of the device
resolved both of these problems.
After initial open-ended piloting trials, more struc-
tured user tests were conducted, for which we devel-
oped a set of six scenarios ordered by increasing level
of difficulty. These required the test user to solve
problems using the system. These scenarios were left
as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner
later this evening at a Thai restaurant on the Upper West Side
near her apartment on 95th St. and Broadway. Unfortunately,
you forgot what time you?re supposed to meet her, and you can?t
reach her by phone. Use MATCH to find the restaurant and write
down the restaurant?s telephone number so you can check on the
reservation time.
Test users received a brief tutorial that was inten-
tionally vague and broad in scope so the users might
overestimate the system?s capabilities and approach
problems in new ways. Figure 11 summarizes re-
sults from our last scenario-based data collection for
a fixed version of the system. There were five sub-
jects (2 male, 3 female) none of whom had been in-
volved in system development. All of these five tests
were conducted indoors in offices.
exchanges 338 asr word accuracy 59.6%
speech only 171 51% asr sent. accuracy 36.1%
multimodal 93 28% handwritten sent. acc. 64%
pen only 66 19% task completion rate 85%
GUI actions 8 2% average time/scenario 6.25m
Figure 11: MATCH study
There were an average of 12.75 multimodal ex-
changes (pairs of user input and system response) per
scenario. The overall time per scenario varied from
1.5 to to 15 minutes. The longer completion times
resulted from poor ASR performance for some of the
users. Although ASR accuracy was low, overall task
completion was high, suggesting that the multimodal
aspects of the system helped users to complete tasks.
Unimodal pen commands were recognized more suc-
cessfully than spoken commands; however, only 19%
of commands were pen only. In ongoing work, we
are exploring strategies to increase users? adoption of
more robust pen-based and multimodal input.
MATCH has a very fast system response time.
Benchmarking a set of speech, pen, and multimodal
commands, the average response time is approxi-
mately 3 seconds (time from end of user input to sys-
tem response). We are currently completing a larger
scale scenario-based evaluation and an independent
evaluation of the functionality of the text planner.
In addition to MATCH, the same multimodal ar-
chitecture has been used for two other applications:
a multimodal interface to corporate directory infor-
mation and messaging and a medical application to
assist emergency room doctors. The medical proto-
type is the most recent and demonstrates the utility of
the architecture for rapid prototyping. System devel-
opment took under two days for two people.
4 Conclusion
The MATCH architecture enables rapid develop-
ment of mobile multimodal applications. Combin-
ing finite-state multimodal integration with a speech-
act based dialogue manager enables users to interact
flexibly using speech, pen, or synchronized combina-
tions of the two depending on their preferences, task,
and physical and social environment. The system
responds by generating coordinated multimodal pre-
sentations adapted to the multimodal dialog context
and user preferences. Features of the system such
as the browser-based UI and general purpose finite-
state architecture for multimodal integration facili-
tate rapid prototyping and reuse of the technology for
different applications. The lattice-based finite-state
approach to multimodal understanding enables both
multimodal integration and dialogue context to com-
pensate for recognition errors. The multimodal log-
ging infrastructure has enabled an iterative process
of pro-active evaluation and data collection through-
out system development. Since we can replay multi-
modal interactions without video we have been able
to log and annotate subjects both in the lab and in
NYC throughout the development process and use
their input to drive system development.
Acknowledgements
Thanks to AT&T Labs and DARPA (contract MDA972-99-3-
0003) for financial support. We would also like to thank Noemie
Elhadad, Candace Kamm, Elliot Pinson, Mazin Rahim, Owen
Rambow, and Nika Smith.
References
J. Alexandersson and T. Becker. 2001. Overlay as the ba-
sic operation for discourse processing in a multimodal
dialogue system. In 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
E. Andre?. 2002. Natural language in multime-
dia/multimodal systems. In Ruslan Mitkov, editor,
Handbook of Computational Linguistics. OUP.
G. Carenini and J. D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument
effectiveness. In IJCAI, pages 1307?1314.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING 2000, Saarbru?cken, Germany.
M. Johnston and S. Bangalore. 2001. Finite-state meth-
ods for multimodal parsing and integration. In ESSLLI
Workshop on Finite-state Methods, Helsinki, Finland.
M. Johnston. 2000. Deixis and conjunction in mul-
timodal systems. In Proceedings of COLING 2000,
Saarbru?cken, Germany.
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999.
TrindiKit manual. Technical report, TRINDI Deliver-
able D2.2.
D. Martin, A. Cheyer, and D. Moran. 1999. The Open
Agent Architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13(1?2):91?128.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
S. L. Oviatt and J. Clow. 1998. An automated tool for
analysis of multimodal system performance. In Pro-
ceedings of ICSLP.
C. Rich and C. Sidner. 1998. COLLAGEN: A collabora-
tion manager for software interface agents. User Mod-
eling and User-Adapted Interaction, 8(3?4):315?350.
D. Rubine. 1991. Specifying gestures by example. Com-
puter graphics, 25(4):329?337.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In ICSLP-98.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of ACL?99.
M. A. Walker, S. J. Whittaker, P. Maloor, J. D. Moore,
M. Johnston, and G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in spoken dialogue. In
In Proceedings of INLG-02.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 273?281,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Who is ?You?? Combining Linguistic and Gaze Features to Resolve
Second-Person References in Dialogue?
Matthew Frampton1, Raquel Ferna?ndez1, Patrick Ehlen1, Mario Christoudias2,
Trevor Darrell2 and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{frampton, raquelfr, ehlen, peters}@stanford.edu
2International Computer Science Institute, University of California at Berkeley
cmch@icsi.berkeley.edu, trevor@eecs.berkeley.edu
Abstract
We explore the problem of resolving the
second person English pronoun you in
multi-party dialogue, using a combination
of linguistic and visual features. First, we
distinguish generic and referential uses,
then we classify the referential uses as ei-
ther plural or singular, and finally, for the
latter cases, we identify the addressee. In
our first set of experiments, the linguistic
and visual features are derived from man-
ual transcriptions and annotations, but in
the second set, they are generated through
entirely automatic means. Results show
that a multimodal system is often prefer-
able to a unimodal one.
1 Introduction
The English pronoun you is the second most fre-
quent word in unrestricted conversation (after I
and right before it).1 Despite this, with the ex-
ception of Gupta et al (2007b; 2007a), its re-
solution has received very little attention in the lit-
erature. This is perhaps not surprising since the
vast amount of work on anaphora and reference
resolution has focused on text or discourse - medi-
ums where second-person deixis is perhaps not
as prominent as it is in dialogue. For spoken di-
alogue pronoun resolution modules however, re-
solving you is an essential task that has an impor-
tant impact on the capabilities of dialogue summa-
rization systems.
?We thank the anonymous EACL reviewers, and Surabhi
Gupta, John Niekrasz and David Demirdjian for their com-
ments and technical assistance. This work was supported by
the CALO project (DARPA grant NBCH-D-03-0010).
1See e.g. http://www.kilgarriff.co.uk/BNC_lists/
Besides being important for computational im-
plementations, resolving you is also an interesting
and challenging research problem. As for third
person pronouns such as it, some uses of you are
not strictly referential. These include discourse
marker uses such as you know in example (1), and
generic uses like (2), where you does not refer to
the addressee as it does in (3).
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button se-
quences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
However, unlike it, you is ambiguous between sin-
gular and plural interpretations - an issue that is
particularly problematic in multi-party conversa-
tions. While you clearly has a plural referent in
(4), in (3) the number of its referent is ambigu-
ous.2
(4) I don?t know if you guys have any questions.
When an utterance contains a singular referen-
tial you, resolving the you amounts to identifying
the individual to whom the utterance is addressed.
This is trivial in two-person dialogue since the cur-
rent listener is always the addressee, but in conver-
sations with multiple participants, it is a complex
problem where different kinds of linguistic and vi-
sual information play important roles (Jovanovic,
2007). One of the issues we investigate here is
2In contrast, the referential use of the pronoun it (as well
as that of some demonstratives) is ambiguous between NP
interpretations and discourse-deictic ones (Webber, 1991).
273
how this applies to the more concrete problem of
resolving the second person pronoun you.
We approach this issue as a three-step prob-
lem. Using the AMI Meeting Corpus (McCowan
et al, 2005) of multi-party dialogues, we first dis-
criminate between referential and generic uses of
you. Then, within the referential uses, we dis-
tinguish between singular and plural, and finally,
we resolve the singular referential instances by
identifying the intended addressee. We use multi-
modal features: initially, we extract discourse fea-
tures from manual transcriptions and use visual in-
formation derived from manual annotations, but
then we move to a fully automatic approach, us-
ing 1-best transcriptions produced by an automatic
speech recognizer (ASR) and visual features auto-
matically extracted from raw video.
In the next section of this paper, we give a brief
overview of related work. We describe our data in
Section 3, and explain how we extract visual and
linguistic features in Sections 4 and 5 respectively.
Section 6 then presents our experiments with man-
ual transcriptions and annotations, while Section
7, those with automatically extracted information.
We end with conclusions in Section 8.
2 Related Work
2.1 Reference Resolution in Dialogue
Although the vast majority of work on reference
resolution has been with monologic text, some re-
cent research has dealt with the more complex
scenario of spoken dialogue (Strube and Mu?ller,
2003; Byron, 2004; Arstein and Poesio, 2006;
Mu?ller, 2007). There has been work on the iden-
tification of non-referential uses of the pronoun it:
Mu?ller (2006) uses a set of shallow features au-
tomatically extracted from manual transcripts of
two-party dialogue in order to train a rule-based
classifier, and achieves an F-score of 69%.
The only existing work on the resolution of you
that we are aware of is Gupta et al (2007b; 2007a).
In line with our approach, the authors first disam-
biguate between generic and referential you, and
then attempt to resolve the reference of the ref-
erential cases. Generic uses of you account for
47% of their data set, and for the generic vs. ref-
erential disambiguation, they achieve an accuracy
of 84% on two-party conversations and 75% on
multi-party dialogue. For the reference resolution
task, they achieve 47%, which is 10 points over
a baseline that always classifies the next speaker
as the addressee. These results are achieved with-
out visual information, using manual transcripts,
and a combination of surface features and manu-
ally tagged dialogue acts.
2.2 Addressee Detection
Resolving the referential instances of you amounts
to determining the addressee(s) of the utterance
containing the pronoun. Recent years have seen
an increasing amount of research on automatic
addressee detection. Much of this work focuses
on communication between humans and computa-
tional agents (such as robots or ubiquitous com-
puting systems) that interact with users who may
be engaged in other activities, including interac-
tion with other humans. In these situations, it
is important for a system to be able to recognize
when it is being addressed by a user. Bakx et
al. (2003) and Turnhout et al (2005) studied this
issue in the context of mixed human-human and
human-computer interaction using facial orienta-
tion and utterance length as clues for addressee
detection, while Katzenmaier et al (2004) inves-
tigated whether the degree to which a user utter-
ance fits the language model of a conversational
robot can be useful in detecting system-addressed
utterances. This research exploits the fact that hu-
mans tend to speak differently to systems than to
other humans.
Our research is closer to that of Jovanovic
et al (2006a; 2007), who studied addressing in
human-human multi-party dialogue. Jovanovic
and colleagues focus on addressee identification in
face-to-face meetings with four participants. They
use a Bayesian Network classifier trained on sev-
eral multimodal features (including visual features
such as gaze direction, discourse features such as
the speaker and dialogue act of preceding utter-
ances, and utterance features such as lexical clues
and utterance duration). Using a combination of
features from various resources was found to im-
prove performance (the best system achieves an
accuracy of 77% on a portion of the AMI Meeting
Corpus). Although this result is very encouraging,
it is achieved with the use of manually produced
information - in particular, manual transcriptions,
dialogue acts and annotations of visual focus of at-
tention. One of the issues we aim to investigate
here is how automatically extracted multimodal
information can help in detecting the addressee(s)
of you-utterances.
274
Generic Referential Ref Sing. Ref Pl.
49.14% 50.86% 67.92% 32.08%
Table 1: Distribution of you interpretations
3 Data
Our experiments are performed using the AMI
Meeting Corpus (McCowan et al, 2005), a collec-
tion of scenario-driven meetings among four par-
ticipants, manually transcribed and annotated with
several different types of information (including
dialogue acts, topics, visual focus of attention, and
addressee). We use a sub-corpus of 948 utterances
containing you, and these were extracted from 10
different meetings. The you-utterances are anno-
tated as either discourse marker, generic or refer-
ential.
We excluded the discourse marker cases, which
account for only 8% of the data, and of the refer-
ential cases, selected those with an AMI addressee
annotation.3 The addressee of a dialogue act can
be unknown, a single meeting participant, two
participants, or the whole audience (three partici-
pants in the AMI corpus). Since there are very few
instances of two-participant addressee, we distin-
guish only between singular and plural addressees.
The resulting distribution of classes is shown in
Table 1.4
We approach the reference resolution task as a
two-step process, first discriminating between plu-
ral and singular references, and then resolving the
reference of the singular cases. The latter task re-
quires a classification scheme for distinguishing
between the three potential addressees (listeners)
for the given you-utterance.
In their four-way classification scheme,
Gupta et al (2007a) label potential addressees in
terms of the order in which they speak after the
you-utterance. That is, for a given you-utterance,
the potential addressee who speaks next is labeled
1, the potential addressee who speaks after that is
2, and the remaining participant is 3. Label 4 is
used for group addressing. However, this results
in a very skewed class distribution because the
next speaker is the intended addressee 41% of
the time, and 38% of instances are plural - the
3Addressee annotations are not provided for some dia-
logue act types - see (Jovanovic et al, 2006b).
4Note that the percentages of the referential singular and
referential plural are relative to the total of referential in-
stances.
L1 L2 L3
35.17% 30.34% 34.49%
Table 2: Distribution of addressees for singular you
remaining two classes therefore make up a small
percentage of the data.
We were able to obtain a much less skewed class
distribution by identifying the potential addressees
in terms of their position in relation to the current
speaker. The meeting setting includes a rectangu-
lar table with two participants seated at each of
its opposite longer sides. Thus, for a given you-
utterance, we label listeners as either L1, L2 or
L3 depending on whether they are sitting opposite,
diagonally or laterally from the speaker. Table 2
shows the resulting class distribution for our data-
set. Such a labelling scheme is more similar to Jo-
vanovic (2007), where participants are identified
by their seating position.
4 Visual Information
4.1 Features from Manual Annotations
We derived per-utterance visual features from the
Focus Of Attention (FOA) annotations provided
by the AMI corpus. These annotations track meet-
ing participants? head orientation and eye gaze
during a meeting.5 Our first step was to use the
FOA annotations in order to compute what we re-
fer to as Gaze Duration Proportion (GDP) values
for each of the utterances of interest - a measure
similar to the ?Degree of Mean Duration of Gaze?
described by (Takemae et al, 2004). Here a GDP
value denotes the proportion of time in utterance u
for which subject i is looking at target j:
GDPu(i, j) =
?
j
T (i, j)/Tu
were Tu is the length of utterance u in millisec-
onds, and T (i, j), the amount of that time that i
spends looking at j. The gazer i can only refer to
one of the four meeting participants, but the tar-
get j can also refer to the white-board/projector
screen present in the meeting room. For each utter-
ance then, all of the possible values of i and j are
used to construct a matrix of GDP values. From
this matrix, we then construct ?Highest GDP? fea-
tures for each of the meeting participants: such
5A description of the FOA labeling scheme is avail-
able from the AMI Meeting Corpus website http://corpus.
amiproject.org/documentations/guidelines-1/
275
For each participant Pi
? target for whole utterance
? target for first third of utterance
? target for second third of utterance
? target for third third of utterance
? target for -/+ 2 secs from you start time
? ratio 2nd hyp. target / 1st hyp. target
? ratio 3rd hyp. target / 1st hyp. target
? participant in mutual gaze with speaker
Table 3: Visual Features
features record the target with the highest GDP
value and so indicate whom/what the meeting par-
ticipant spent most time looking at during the ut-
terance.
We also generated a number of additional fea-
tures for each individual. These include firstly,
three features which record the candidate ?gazee?
with the highest GDP during each third of the ut-
terance, and which therefore account for gaze tran-
sitions. So as to focus more closely on where par-
ticipants are looking around the time when you
is uttered, another feature records the candidate
with the highest GDP -/+ 2 seconds from the start
time of the you. Two further features give some
indication of the amount of looking around that
the speaker does during an utterance - we hypoth-
esized that participants (especially the speaker)
might look around more in utterances with plu-
ral addressees. The first is the ratio of the sec-
ond highest GDP to the highest, and the second
is the ratio of the third highest to the highest. Fi-
nally, there is a highest GDP mutual gaze feature
for the speaker, indicating with which other indi-
vidual, the speaker spent most time engaged in a
mutual gaze.
Hence this gives a total of 29 features: seven
features for each of the four participants, plus one
mutual gaze feature. They are summarized in Ta-
ble 3. These visual features are different to those
used by Jovanovic (2007) (see Section 2). Jo-
vanovic?s features record the number of times that
each participant looks at each other participant
during the utterance, and in addition, the gaze di-
rection of the current speaker. Hence, they are not
highest GDP values, they do not include a mutual
gaze feature and they do not record whether par-
ticipants look at the white-board/projector screen.
4.2 Automatic Features from Raw Video
To perform automatic visual feature extraction, a
six degree-of-freedom head tracker was run over
each subject?s video sequence for the utterances
containing you. For each utterance, this gave 4 se-
quences, one per subject, of the subject?s 3D head
orientation and location at each video frame along
with 3D head rotational velocities. From these
measurements we computed two types of visual
information: participant gaze and mutual gaze.
The 3D head orientation and location of each
subject along with camera calibration information
was used to compute participant gaze information
for each video frame of each sequence in the form
of a gaze probability matrix. More precisely, cam-
era calibration is first used to estimate the 3D head
orientation and location of all subjects in the same
world coordinate system.
The gaze probability matrix is a 4 ? 5 matrix
where entry i, j stores the probability that subject
i is looking at subject j for each of the four sub-
jects and the last column corresponds to the white-
board/projector screen (i.e., entry i, j where j = 5
is the probability that subject i is looking at the
screen). Gaze probability G(i, j) is defined as
G(i, j) = G0e
??i,j2/?2
where ?i,j is the angular difference between the
gaze of subject i and the direction defined by the
location of subjects i and j. G0 is a normalization
factor such that
?
j G(i, j) = 1 and ? is a user-
defined constant (in our experiments, we chose
? = 15 degrees).
Using the gaze probability matrix, a 4 ? 1 per-
frame mutual gaze vector was computed that for
entry i stores the probability that the speaker and
subject i are looking at one another.
In order to create features equivalent to those
described in Section 4.1, we first collapse the
frame-level probability matrix into a matrix of bi-
nary values. We convert the probability for each
frame into a binary judgement of whether subject
i is looking at target j:
H(i, j) = ?G(i, j)
? is a binary value to evaluate G(i, j) > ?, where
? is a high-pass thresholding value - or ?gaze prob-
ability threshold? (GPT) - between 0 and 1.
Once we have a frame-level matrix of binary
values, for each subject i, we compute GDP val-
ues for the time periods of interest, and in each
case, choose the target with the highest GDP as the
candidate. Hence, we compute a candidate target
for the utterance overall, for each third of the ut-
terance, and for the period -/+ 2 seconds from the
276
you start time, and in addition, we compute a can-
didate participant for mutual gaze with the speaker
for the utterance overall.
We sought to use the GPT threshold which pro-
duces automatic visual features that agree best
with the features derived from the FOA annota-
tions. Hence we experimented with different GPT
values in increments of 0.1, and compared the re-
sulting features to the manual features using the
kappa statistic. A threshold of 0.6 gave the best
kappa scores, which ranged from 20% to 44%.6
5 Linguistic Information
Our set of discourse features is a simplified ver-
sion of those employed by Galley et al (2004) and
Gupta et al (2007a). It contains three main types
(summarized in Table 4):
? Sentential features (1 to 13) encode structural,
durational, lexical and shallow syntactic patterns
of the you-utterance. Feature 13 is extracted us-
ing the AMI ?Named Entity? annotations and in-
dicates whether a particular participant is men-
tioned in the you-utterance. Apart from this fea-
ture, all other sentential features are automatically
extracted, and besides 1, 8, 9, and 10, they are all
binary.
? Backward Looking (BL)/Forward Looking (FL)
features (14 to 22) are mostly extracted from ut-
terance pairs, namely the you-utterance and the
BL/FL (previous/next) utterance by each listener
Li (potential addressee). We also include a few
extra features which are not computed in terms of
utterance pairs. These indicate the number of par-
ticipants that speak during the previous and next 5
utterances, and the BL and FL speaker order. All
of these features are computed automatically.
? Dialogue Act (DA) features (23 to 24) use the
manual AMI dialogue act annotations to represent
the conversational function of the you-utterance
and the BL/FL utterance by each potential ad-
dressee. Along with the sentential feature based
on the AMI Named Entity annotations, these are
the only discourse features which are not com-
puted automatically. 7
6The fact that our gaze estimator is getting any useful
agreement with respect to these annotations is encouraging
and suggests that an improved tracker and/or one that adapts
to the user more effectively could work very well.
7Since we use the manual transcripts of the meetings, the
transcribed words and the segmentation into utterances or di-
alogue acts are of course not given automatically. A fully
automatic approach would involve using ASR output instead
of manual transcriptions? something which we attempt in
(1) # of you pronouns
(2) you (say|said|tell|told| mention(ed)|mean(t)|
sound(ed))
(3) auxiliary you
(4) wh-word you
(5) you guys
(6) if you
(7) you know
(8) # of words in you-utterance
(9) duration of you-utterance
(10) speech rate of you-utterance
(11) 1st person
(12) general case
(13) person Named Entity tag
(14) # of utterances between you- and BL/FL utt.
(15) # of speakers between you- and BL/FL utt.
(16) overlap between you- and BL/FL utt. (binary)
(17) duration of overlap between you- and BL/FL utt.
(18) time separation between you- and BL/FL utt.
(19) ratio of words in you- that are in BL/FL utt.
(20) # of participants that speak during prev. 5 utt.
(21) # of participants that speak during next 5 utt.
(22) speaker order BL/FL
(23) dialogue act of the you-utterance
(24) dialogue act of the BL/FL utterance
Table 4: Discourse Features
6 First Set of Experiments & Results
In this section we report our experiments and re-
sults when using manual transcriptions and anno-
tations. In Section 7 we will present the results
obtained using ASR output and automatically ex-
tracted visual information. All experiments (here
and in the next section) are performed using a
Bayesian Network classifier with 10-fold cross-
validation.8 In each task, we give raw overall ac-
curacy results and then F-scores for each of the
classes. We computed measures of information
gain in order to assess the predictive power of the
various features, and did some experimentation
with Correlation-based Feature Selection (CFS)
(Hall, 2000).
6.1 Generic vs. Referential Uses of You
We first address the task of distinguishing between
generic and referential uses of you.
Baseline. A majority class baseline that classi-
fies all instances of you as referential yields an ac-
curacy of 50.86% (see Table 1).
Results. A summary of the results is given in Ta-
ble 5. Using discourse features only we achieve
an accuracy of 77.77%, while using multimodal
Section 7.
8We use the the BayesNet classifier implemented in the
Weka toolkit http://www.cs.waikato.ac.nz/ml/weka/.
277
Features Acc F1-Gen F1-Ref
Baseline 50.86 0 67.4
Discourse 77.77 78.8 76.6
Visual 60.32 64.2 55.5
MM 79.02 80.2 77.7
Dis w/o FL 78.34 79.1 77.5
MM w/o FL 78.22 79.0 77.4
Dis w/o DA 69.44 71.5 67.0
MM w/o DA 72.75 74.4 70.9
Table 5: Generic vs. referential uses
(MM) yields 79.02%, but this increase is not sta-
tistically significant.
In spite of this, visual features do help to dis-
tinguish between generic and referential uses -
note that the visual features alone are able to beat
the baseline (p < .005). The listeners? gaze is
more predictive than the speaker?s: if listeners
look mostly at the white-board/projector screen in-
stead of another participant, then the you is more
likely to be referential. More will be said on this
in Section 6.2.1 in the analysis of the results for
the singular vs. plural referential task.
We found sentential features of the you-
utterance to be amongst the best predictors, es-
pecially those that refer to surface lexical proper-
ties, such as features 1, 11, 12 and 13 in Table 4.
Dialogue act features provide useful information
as well. As pointed out by Gupta et al (2007b;
2007a), a you pronoun within a question (e.g.
an utterance tagged as elicit-assess or
elicit-inform) is more likely to be referen-
tial. Eliminating information about dialogue acts
(w/o DA) brings down performance (p < .005),
although accuracy remains well above the baseline
(p < .001). Note that the small changes in perfor-
mance when FL information is taken out (w/o FL)
are not statistically significant.
6.2 Reference Resolution
We now turn to the referential instances of you,
which can be resolved by determining the ad-
dressee(s) of the given utterance.
6.2.1 Singular vs. Plural Reference
We start by trying to discriminate singular vs. plu-
ral interpretations. For this, we use a two-way
classification scheme that distinguishes between
individual and group addressing. To our knowl-
edge, this is the first attempt at this task using lin-
guistic information.9
9But see e.g. (Takemae et al, 2004) for an approach that
uses manually extracted visual-only clues with similar aims.
Baseline. A majority class baseline that consid-
ers all instances of you as referring to an individual
addressee gives 67.92% accuracy (see Table 1).
Results. A summary of the results is shown in
Table 6. There is no statistically significant differ-
ence between the baseline and the results obtained
when visual features are used alone (67.92% vs.
66.28%). However, we found that visual informa-
tion did contribute to identifying some instances of
plural addressing, as shown by the F-score for that
class. Furthermore, the visual features helped to
improve results when combined with discourse in-
formation: using multimodal (MM) features pro-
duces higher results than the discourse-only fea-
ture set (p < .005), and increases from 74.24% to
77.05% with CFS.
As in the generic vs. referential task, the white-
board/projector screen value for the listeners? gaze
features seems to have discriminative power -
when listeners? gaze features take this value, it is
often indicative of a plural rather than a singular
you. It seems then, that in our data-set, the speaker
often uses the white-board/projector screen when
addressing the group, and hence draws the listen-
ers? gaze in this direction. We should also note
that the ratio features which we thought might be
useful here (see Section 4.1) did not prove so.
Amongst the most useful discourse features
are those that encode similarity relations between
the you-utterance and an utterance by a potential
addressee. Utterances by individual addressees
tend to be more lexically cohesive with the you-
utterance and so if features such as feature 19 in
Table 4 indicate a low level of lexical similarity,
then this increases the likelihood of plural address-
ing. Sentential features that refer to surface lexical
patterns (features 6, 7, 11 and 12) also contribute
to improved results, as does feature 21 (number of
speakers during the next five utterances) - fewer
speaker changes correlates with plural addressing.
Information about dialogue acts also plays a
role in distinguishing between singular and plu-
ral interpretations. Questions tend to be addressed
to individual participants, while statements show a
stronger correlation with plural addressees. When
no DA features are used (w/o DA), the drop in per-
formance for the multimodal classifier to 71.19%
is statistically significant (p < .05). As for the
generic vs. referential task, FL information does
not have a significant effect on performance.
278
Features Acc F1-Sing. F1-Pl.
Baseline 67.92 80.9 0
Discourse 71.19 78.9 54.6
Visual 66.28 74.8 48.9
MM* 77.05 83.3 63.2
Dis w/o FL 72.13 80.1 53.7
MM w/o FL 72.60 79.7 58.1
Dis w/o DA 68.38 78.5 40.5
MM w/o DA 71.19 78.8 55.3
Table 6: Singular vs. plural reference; * = with Correlation-
based Feature Selection (CFS).
6.2.2 Detection of Individual Addressees
We now turn to resolving the singular referential
uses of you. Here we must detect the individual
addressee of the utterance that contains the pro-
noun.
Baselines. Given the distribution shown in Ta-
ble 2, a majority class baseline yields an accu-
racy of 35.17%. An off-line system that has access
to future context could implement a next-speaker
baseline that always considers the next speaker to
be the intended addressee, so yielding a high raw
accuracy of 71.03%. A previous-speaker base-
line that does not require access to future context
achieves 35% raw accuracy.
Results. Table 7 shows a summary of the re-
sults, and these all outperform the majority class
(MC) and previous-speaker baselines. When all
discourse features are available, adding visual in-
formation does improve performance (74.48% vs.
60.69%, p < .005), and with CFS, this increases
further to 80.34% (p < .005). Using discourse or
visual features alone gives scores that are below
the next-speaker baseline (60.69% and 65.52% vs.
71.03%). Taking all forward-looking (FL) infor-
mation away reduces performance (p < .05), but
the small increase in accuracy caused by taking
away dialogue act information is not statistically
significant.
When we investigated individual feature contri-
bution, we found that the most predictive features
were the FL and backward-looking (BL) speaker
order, and the speaker?s visual features (including
mutual gaze). Whomever the speaker spent most
time looking at or engaged in a mutual gaze with
was more likely to be the addressee. All of the vi-
sual features had some degree of predictive power
apart from the ratio features. Of the other BL/FL
discourse features, features 14, 18 and 19 (see Ta-
ble 4) were more predictive. These indicate that
utterances spoken by the intended addressee are
Features Acc F1-L1 F1-L2 F1-L3
MC baseline 35.17 52.0 0 0
Discourse 60.69 59.1 60.0 62.7
Visual 65.52 69.1 63.5 64.0
MM* 80.34 80.0 82.4 79.0
Dis w/o FL 52.41 50.7 51.8 54.5
MM w/o FL 66.55 68.7 62.7 67.6
Dis w/o DA 61.03 58.5 59.9 64.2
MM w/o DA 73.10 72.4 69.5 72.0
Table 7: Addressee detection for singular references; * =
with Correlation-based Feature Selection (CFS).
often adjacent to the you-utterance and lexically
similar.
7 A Fully Automatic Approach
In this section we describe experiments which
use features derived from ASR transcriptions and
automatically-extracted visual information. We
used SRI?s Decipher (Stolcke et al, 2008)10 in or-
der to generate ASR transcriptions, and applied
the head-tracker described in Section 4.2 to the
relevant portions of video in order to extract the
visual information. Recall that the Named Entity
features (feature 13) and the DA features used in
our previous experiments had been manually an-
notated, and hence are not used here. We again
divide the problem into the same three separate
tasks: we first discriminate between generic and
referential uses of you, then singular vs. plural
referential uses, and finally we resolve the ad-
dressee for singular uses. As before, all exper-
iments are performed using a Bayesian Network
classifier and 10-fold cross validation.
7.1 Results
For each of the three tasks, Figure 7 compares
the accuracy results obtained using the fully-
automatic approach with those reported in Section
6. The figure shows results for the majority class
baselines (MCBs), and with discourse-only (Dis),
and multimodal (MM) feature sets. Note that the
data set for the automatic approach is smaller,
and that the majority class baselines have changed
slightly. This is because of differences in the ut-
terance segmentation, and also because not all of
the video sections around the you utterances were
processed by the head-tracker.
In all three tasks we are able to significantly
outperform the majority class baseline, but the vi-
sual features only produce a significant improve-
10Stolcke et al (2008) report a word error rate of 26.9% on
AMI meetings.
279
Figure 1: Results for the manual and automatic systems; MCB = majority class baseline, Dis = discourse features, MM =
multimodal, * = with Correlation-based Feature Selection (CFS), FL = forward-looking, man = manual, auto = automatic.
ment in the individual addressee resolution task.
For the generic vs. referential task, the discourse
and multimodal classifiers both outperform the
majority class baseline (p < .001), achieving
accuracy scores of 68.71% and 68.48% respec-
tively. In contrast to when using manual transcrip-
tions and annotations (see Section 6.1), removing
forward-looking (FL) information reduces perfor-
mance (p < .05). For the referential singular
vs. plural task, the discourse and multimodal with
CFS classifier improve over the majority class
baseline (p < .05). Multimodal with CFS does
not improve over the discourse classifier - indeed
without feature selection, the addition of visual
features causes a drop in performance (p < .05).
Here, taking away FL information does not cause
a significant reduction in performance. Finally,
in the individual addressee resolution task, the
discourse, visual (60.78%) and multimodal clas-
sifiers all outperform the majority class baseline
(p < .005, p < .001 and p < .001 respec-
tively). Here the addition of visual features causes
the multimodal classifier to outperform the dis-
course classifier in raw accuracy by nearly ten per-
centage points (67.32% vs. 58.17%, p < .05), and
with CFS, the score increases further to 74.51%
(p < .05). Taking away FL information does
cause a significant drop in performance (p < .05).
8 Conclusions
We have investigated the automatic resolution of
the second person English pronoun you in multi-
party dialogue, using a combination of linguistic
and visual features. We conducted a first set of
experiments where our features were derived from
manual transcriptions and annotations, and then a
second set where they were generated by entirely
automatic means. To our knowledge, this is the
first attempt at tackling this problem using auto-
matically extracted multimodal information.
Our experiments showed that visual informa-
tion can be highly predictive in resolving the ad-
dressee of singular referential uses of you. Visual
features significantly improved the performance of
both our manual and automatic systems, and the
latter achieved an encouraging 75% accuracy. We
also found that our visual features had predictive
power for distinguishing between generic and ref-
erential uses of you, and between referential sin-
gulars and plurals. Indeed, for the latter task,
they significantly improved the manual system?s
performance. The listeners? gaze features were
useful here: in our data set it was apparently the
case that the speaker would often use the white-
board/projector screen when addressing the group,
thus drawing the listeners? gaze in this direction.
Future work will involve expanding our data-
set, and investigating new potentially predictive
features. In the slightly longer term, we plan to
integrate the resulting system into a meeting as-
sistant whose purpose is to automatically extract
useful information from multi-party meetings.
280
References
Ron Arstein and Massimo Poesio. 2006. Identifying
reference to abstract objects in dialogue. In Pro-
ceedings of the 10th Workshop on the Semantics and
Pragmatics of Dialogue (Brandial?06), pages 56?
63, Potsdam, Germany.
Ilse Bakx, Koen van Turnhout, and Jacques Terken.
2003. Facial orientation during multi-party inter-
action with information kiosks. In Proceedings of
INTERACT, Zurich, Switzerland.
Donna Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, University
of Rochester, Department of Computer Science.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium, September.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Mark Hall. 2000. Correlation-based Feature Selection
for Machine Learning. Ph.D. thesis, University of
Waikato.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006a. Addressee identification in face-to-
face meetings. In Proceedings of the 11th Confer-
ence of the European Chapter of the ACL (EACL),
pages 169?176, Trento, Italy.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006b. A corpus for studying addressing
behaviour in multi-party dialogues. Language Re-
sources and Evaluation, 40(1):5?23. ISSN=1574-
020X.
Natasa Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, Enschede, The
Netherlands.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings of the 6th International
Conference on Multimodal Interfaces, pages 144?
151, State College, Pennsylvania.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 49?56, Trento, Italy.
Christoph Mu?ller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 816?823, Prague,
Czech Republic.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The icsi-sri spring
2007 meeting and lecture recognition system. In
Proceedings of CLEAR 2007 and RT2007. Springer
Lecture Notes on Computer Science.
Michael Strube and Christoph Mu?ller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of ACL?03, pages
168?175.
Yoshinao Takemae, Kazuhiro Otsuka, and Naoki
Mukawa. 2004. An analysis of speakers? gaze
behaviour for automatic addressee identification in
multiparty conversation and its application to video
editing. In Proceedings of IEEE Workshop on Robot
and Human Interactive Communication, pages 581?
586.
Koen van Turnhout, Jacques Terken, Ilse Bakx, and
Berry Eggen. 2005. Identifying the intended
addressee in mixed human-humand and human-
computer interaction from non-verbal features. In
Proceedings of ICMI, Trento, Italy.
Bonnie Webber. 1991. Structure and ostension in
the interpretation of discourse deixi. Language and
Cognitive Processes, 6(2):107?135.
281
NAACL HLT Demonstration Program, pages 17?18,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
The CALO Meeting Assistant 
 
L. Lynn Voss Patrick Ehlen 
Engineering and Systems Division CSLI 
SRI International Stanford University 
Menlo Park, CA 94025 Stanford, CA 94305 
loren.voss@sri.com ehlen@stanford.edu 
and  
The DARPA? CALO Meeting Assistant Project Team* 
 
Abstract 
The CALO Meeting Assistant is an inte-
grated, multimodal meeting assistant tech-
nology that captures speech, gestures, and 
multimodal data from multiparty interactions 
during meetings, and uses machine learning 
and robust discourse processing to provide a 
rich, browsable record of a meeting. 
1 Introduction 
Technologies that assist in making meetings more 
productive have a long history. The latest chapter in 
that history involves projects that integrate recent 
advances in speech, natural language understanding, 
vision, and multimodal interaction technologies in 
an effort to produce tools that can perceive what 
happens at a meeting, extract salient events and in-
teractions, and produce a record of the meeting that 
people can later consult or analyze. 
Research projects such as the ICSI Meeting Pro-
ject (Janin et al 2004) have sought to produce auto-
mated and segmented transcripts from natural, multi-
party speech as it occurs in meetings. Others, like 
the ISL Smart Meeting Room Task (Waibel et al 
2003), and the M4 and AMI projects (Nijholt, op 
den Akker, & Heylen 2005), employ instrumented 
                                                          
? This material is based upon work supported by the Defense 
Advanced Research Projects Agency (DARPA) under Contract 
No. NBCHD030010. Any opinions, findings, and conclusions or 
recommendations expressed in this material are those of the 
authors and do not necessarily reflect the views of DARPA or 
the Department of Interior-National Business Center. 
* The DARPA CALO MA Project is a collaborative effort 
among researchers at Adapx, CMU, Georgia Tech, MIT, SRI, 
Stanford University, UC Berkeley, and UC Santa Cruz. 
meeting rooms to collect multiple streams of behav-
ior data and analyze the interactions of meeting par-
ticipants to produce a rich and flexible record of 
their meeting activities, while also providing a sup-
portive environment for collaboration. 
The CALO Meeting Assistant is similar to the lat-
ter in that it collects multiple streams of information 
about the behaviors of people in meetings, and as-
similates speech, movement, and note-taking behav-
ior to create a rich representation of the meeting that 
can be analyzed and reviewed at many levels. How-
ever, a primary aim of the CALO Meeting Assistant 
is to integrate its observations with those of a larger 
system of agents, which can assess the meeting data 
it collects in the context of the ongoing projects and 
workflow in the work lives of each of the meeting 
participants. Thus, our meeting assistant aims to 
reach beyond an intelligent room that understands 
only the activities of people in meetings, and at-
tempts to understand their overarching concerns and 
interpret their behaviors from the perspective of 
what their meetings mean to them. 
That overarching system of agents is being devel-
oped under the DARPA CALO (Cognitive Assistant 
that Learns and Organizes) Program, which seeks to 
produce machine learning technology in the form of 
personalized agents that support high-level knowl-
edge workers in carrying out their professional ac-
tivities. The CALO system handles a broad range of 
interrelated decision-making tasks that are tradition-
ally resistant to automation; doing so partly by inter-
acting with, being advised by, and learning from its 
users. The CALO system can take initiative on com-
pleting routine tasks, and on assisting when the un-
expected happens. 
CALO is designed from the ground up as a cogni-
tive system. Whereas conventional, hand-coded soft-
ware excels at a narrow set of capabilities in a 
17
particular domain, cognitive systems maintain ex-
plicit, declarative models of their capabilities, ongo-
ing activities, and operating environments. These 
models enable CALO to extend and improve its ca-
pabilities through learning and adaptation. Cognitive 
systems are better equipped to cope with unexpected 
developments, learn to improve over time, and adapt 
to the contexts and requirements of different situa-
tions. CALO also uses natural interfaces that enable 
simple, effective interactions with humans and other 
cognitive systems. 
The CALO Meeting Assistance Project is devel-
oping capabilities to enable CALO to participate in 
group discussions and meetings. Unlike instru-
mented ?intelligent room? meeting projects, this sys-
tem is designed for users in an office environment 
with access to the Internet, a laptop, and some small, 
off-the-shelf peripheral devices (such as headsets, 
webcams, and digital writing devices) to capture 
speech, gestures, and handwriting. It aims to be un-
obtrusive by leveraging cross-training, unsupervised 
learning, and lightweight supervision captured from 
normal user interaction (e.g., users reviewing and 
editing notes, or adding detected action items to a to-
do list). 
These data are transparently processed at a central 
server location and redistributed, so the meeting as-
sistant interacts seamlessly with other CALO desk-
top functionalities, using a common ontology. 
2 What it does 
The CALO Meeting Assistant helps its owners by 
capturing and interpreting meeting conversations 
and activities and, as appropriate, retrieving relevant 
information. Information gleaned from a meeting 
can be incorporated in the respective owner?s CALO 
knowledge stores to, for example, track commit-
ments and remember references to projects, people, 
places, and dates. An archive of each meeting pro-
vides a searchable record for users, as well as a his-
tory of training data for CALO?s learning 
components. Learning areas include the following: 
Speech processing?Automatic transcriptions are pro-
duced from conversational speech among multiple 
speakers while adapting to speaker and background 
noise; recognizing prosodic cues; learning new vo-
cabulary; and constructing person, role, and topic-
specific language models. 
Visual recognition?Faces, gaze direction, gestures, 
and activities are detected, and detection is improved 
through lightly-supervised learning and unsuper-
vised cross-training. 
Discourse understanding?Dialog moves are recog-
nized, topics are segmented and grouped through 
supervised and unsupervised generative models, ac-
tion items are detected, and discussions can be 
threaded across documents and email. 
Multimodal reinforcement?Pen, speech, and text in-
puts combine to offer natural communications. 
Meeting activity?Speech and note-taking activities 
combine to provide cross-training for recognizing 
meeting phases, and for tracking agendas and docu-
ment usage. 
3 Demo 
We demonstrate how the CALO Meeting Assistant 
captures speech, pen, and other meeting data using 
an ordinary laptop; produces an automated tran-
script; segments by topic; and performs shallow dis-
course understanding to produce a list of probable 
action items arising from a single, pre-recorded 
meeting. We then demonstrate a Meeting Rapporteur 
that provides a meeting summary and allows partici-
pants to review and organize the meeting transcript, 
audio, notes, action items, and topics?all while 
providing actions in a feedback loop that supports 
the meeting assistant?s semi-supervised learning 
process. Finally, we discuss the potential and current 
development of real-time capabilities that allow us-
ers to interact with the meeting assistant during an 
ongoing meeting. 
References 
Janin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J., 
Marcias-Guarasa, J., Morgan, N., Peskin, B., Shriberg, 
E., Stolcke, A., Wooters, C., and Wrede, B. 2004. The 
ICSI meeting project: Resources and research. In Pro-
ceedings of the 2004 IEEE International Conference 
on Acoustics, Speech, and Signal Processing (ICASSP 
?04) Meeting Recognition Workshop (NIST RT-04). 
Nijholt, A., op den Akker, R., and Heylen, D. 2005. Meet-
ings and meeting modeling in smart environments. AI 
& Society, 20(2):202-220.  
Waibel, A., Schultz, T., Bett, M., Denecke, M., Malkin, 
R., Rogina, I., Stiefelhagen, R., and Yang, J. 2003. 
SMaRT: The smart meeting room task at ISL. In Pro-
ceedings of the 2003 IEEE International Conference 
on Acoustics, Speech, and Signal Processing (ICASSP 
'03), pp 752-755. 
18
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31?34,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Shallow Discourse Structure for Action Item Detection
Matthew Purver, Patrick Ehlen, and John Niekrasz
Center for the Study of Language and Information
Stanford University
Stanford, CA 94305
{mpurver,ehlen,niekrasz}@stanford.edu
Abstract
We investigated automatic action item
detection from transcripts of multi-party
meetings. Unlike previous work (Gruen-
stein et al, 2005), we use a new hierarchi-
cal annotation scheme based on the roles
utterances play in the action item assign-
ment process, and propose an approach
to automatic detection that promises im-
proved classification accuracy while en-
abling the extraction of useful information
for summarization and reporting.
1 Introduction
Action items are specific kinds of decisions common
in multi-party meetings, characterized by the con-
crete assignment of tasks together with certain prop-
erties such as an associated timeframe and reponsi-
ble party. Our aims are firstly to automatically de-
tect the regions of discourse which establish action
items, so their surface form can be used for a tar-
geted report or summary; and secondly, to identify
the important properties of the action items (such as
the associated tasks and deadlines) that would fos-
ter concise and informative semantically-based re-
porting (for example, adding task specifications to a
user?s to-do list). We believe both of these aims are
facilitated by taking into account the roles different
utterances play in the decision-making process ? in
short, a shallow notion of discourse structure.
2 Background
Related Work Corston-Oliver et al (2004) at-
tempted to identify action items in e-mails, using
classifiers trained on annotations of individual sen-
tences within each e-mail. Sentences were anno-
tated with one of a set of ?dialogue? act classes; one
class Task included any sentence containing items
that seemed appropriate to add to an ongoing to-
do list. They report good inter-annotator agreement
over their general tagging exercise (? > 0.8), al-
though individual figures for the Task class are not
given. They then concentrated on Task sentences,
establishing a set of predictive features (in which
word n-grams emerged as ?highly predictive?) and
achieved reasonable per-sentence classification per-
formance (with f-scores around 0.6).
While there are related tags for dialogue act tag-
ging schema ? like DAMSL (Core and Allen, 1997),
which includes tags such as Action-Directive
and Commit, and the ICSI MRDA schema
(Shriberg et al, 2004) which includes a commit
tag ? these classes are too general to allow iden-
tification of action items specifically. One compa-
rable attempt in spoken discourse took a flat ap-
proach, annotating utterances as action-item-related
or not (Gruenstein et al, 2005) over the ICSI and
ISL meeting corpora (Janin et al, 2003; Burger et
al., 2002). Their inter-annotator agreement was low
(? = .36). While this may have been partly due
to their methods, it is notable that (Core and Allen,
1997) reported even lower agreement (? = .15) on
their Commit dialogue acts. Morgan et al (forth-
coming) then used these annotations to attempt auto-
31
matic classification, but achieved poor performance
(with f-scores around 0.3 at best).
Action Items Action items typically embody the
transfer of group responsibility to an individual.
This need not be the person who actually performs
the action (they might delegate the task to a subor-
dinate), but publicly commits to seeing that the ac-
tion is carried out; we call this person the owner of
the action item. Because this action is a social ac-
tion that is coordinated by more than one person,
its initiation is reinforced by agreement and uptake
among the owner and other participants that the ac-
tion should and will be done. And to distinguish
this action from immediate actions that occur during
the meeting and from more vague future actions that
are still in the planning stage, an action item will be
specified as expected to be carried out within a time-
frame that begins at some point after the meeting and
extends no further than the not-too-distant future. So
an action item, as a type of social action, often com-
prises four components: a task description, a time-
frame, an owner, and a round of agreement among
the owner and others. The related discourse tends to
reflect this, and we attempt to exploit this fact here.
3 Baseline Experiments
We applied Gruenstein et al (2005)?s flat annotation
schema to transcripts from a sequence of 5 short re-
lated meetings with 3 participants recorded as part
of the CALO project. Each meeting was simulated
in that its participants were given a scenario, but
was not scripted. In order to avoid entirely data-
or scenario-specific results (and also to provide an
acceptable amount of training data), we then added
a random selection of 6 ICSI and 1 ISL meetings
from Gruenstein et al (2005)?s annotations. Like
(Corston-Oliver et al, 2004) we used support vec-
tor machines (Vapnik, 1995) via the classifier SVM-
light (Joachims, 1999). Their full set of features are
not available to us, but we experimented with com-
binations of words and n-grams and assessed classi-
fication performance via a 5-fold validation on each
of the CALO meetings. In each case, we trained
classifiers on the other 4 meetings in the CALO se-
quence, plus the fixed ICSI/ISL training selection.
Performance (per utterance, on the binary classifica-
tion problem) is shown in Table 1; overall f-score
figures are poor even on these short meetings. These
figures were obtained using words (unigrams, after
text normalization and stemming) as features ? we
investigated other discriminative classifier methods,
and the use of 2- and 3-grams as features, but no
improvements were gained.
Mtg. Utts AI Utts. Precision Recall F-Score
1 191 22 0.31 0.50 0.38
2 156 27 0.36 0.33 0.35
3 196 18 0.28 0.55 0.37
4 212 15 0.20 0.60 0.30
5 198 9 0.19 0.67 0.30
Table 1: Baseline Classification Performance
4 Hierarchical Annotations
Two problems are apparent: firstly, accuracy is
lower than desired; secondly, identifying utterances
related to action items does not allow us to ac-
tually identify those action items and extract their
properties (deadline, owner etc.). But if the ut-
terances related to these properties form distinct
sub-classes which have their own distinct features,
treating them separately and combining the results
(along the lines of (Klein et al, 2002)) might al-
low better performance, while also identifying the
utterances where each property?s value is extracted.
Thus, we produced an annotation schema which
distinguishes among these four classes. The first
three correspond to the discussion and assignment
of the individual properties of the action item (task
description, timeframe and owner); the fi-
nal agreement class covers utterances which ex-
plicitly show that the action item is agreed upon.
Since the task description subclass ex-
tracts a description of the task, it must include any
utterances that specify the action to be performed,
including those that provide required antecedents for
anaphoric references. The owner subclass includes
any utterances that explicitly specify the responsible
party (e.g. ?I?ll take care of that?, or ?John, we?ll
leave that to you?), but not those whose function
might be taken to do so implicitly (such as agree-
ments by the responsible party). The timeframe
subclass includes any utterances that explicitly refer
to when a task may start or when it is expected to
be finished; note that this is often not specified with
32
a date or temporal expression, but rather e.g. ?by
the end of next week,? or ?before the trip to Aruba?.
Finally, the agreement subclass includes any ut-
terances in which people agree that the action should
and will be done; not only acknowledgements by the
owner themselves, but also when other people ex-
press their agreement.
A single utterance may be assigned to more than
one class: ?John, you need to do that by next
Monday? might count as owner and timeframe.
Likewise, there may be more than one utterance of
each class for a single action item: John?s response
?OK, I?ll do that? would also be classed as owner
(as well as agreement). While we do not require
all of these subclasses to be present for a set of ut-
terances to qualify as denoting an action item, we
expect any action item to include most of them.
We applied this annotation schema to the same
12 meetings. Initial reliability between two anno-
tators on the single ISL meeting (chosen as it pre-
sented a significantly more complex set of action
items than others in this set) was encouraging. The
best agreement was achieved on timeframe utter-
ances (? = .86), with owner utterances slightly
less good (between ? = .77), and agreement and
description utterances worse but still accept-
able (? = .73). Further annotation is in progress.
5 Experiments
We trained individual classifiers for each of the utter-
ance sub-classes, and cross-validated as before. For
agreement utterances, we used a naive n-gram
classifier similar to that of (Webb et al, 2005) for di-
alogue act detection, scoring utterances via a set of
most predictive n-grams of length 1?3 and making a
classification decision by comparing the maximum
score to a threshold (where the n-grams, their scores
and the threshold are automatically extracted from
the training data). For owner, timeframe and
task description utterances, we used SVMs
as before, using word unigrams as features (2- and
3-grams gave no improvement ? probably due to the
small amount of training data). Performance var-
ied greatly by sub-class (see Table 2), with some
(e.g. agreement) achieving higher accuracy than the
baseline flat classifications, but others being worse.
As there is now significantly less training data avail-
able to each sub-class than there was for all utter-
ances grouped together in the baseline experiment,
worse performance might be expected; yet some
sub-classes perform better. The worst performing
class is owner. Examination of the data shows
that owner utterances are more likely than other
classes to be assigned to more than one category;
they may therefore have more feature overlap with
other classes, leading to less accurate classification.
Use of relevant sub-strings for training (rather than
full utterances) may help; as may part-of-speech in-
formation ? while proper names may be useful fea-
tures, the name tokens themselves are sparse and
may be better substituted with a generic tag.
Class Precision Recall F-Score
description 0.23 0.41 0.29
owner 0.12 0.28 0.17
timeframe 0.19 0.38 0.26
agreement 0.48 0.44 0.40
Table 2: Sub-class Classification Performance
Even with poor performance for some of the sub-
classifiers, we should still be able to combine them
to get a benefit as long as their true positives cor-
relate better than their false positives (intuitively, if
they make mistakes in different places). So far we
have only conducted an initial naive experiment, in
which we combine the individual classifier decisions
in a weighted sum over a window (currently set to
5 utterances). If the sum over the window reaches
a given threshold, we hypothesize an action item,
and take the highest-confidence utterance given by
each sub-classifier in that window to provide the
corresponding property. As shown in Table 3, this
gives reasonable performance on most meetings, al-
though it does badly on meeting 5 (apparently be-
cause no explicit agreement takes place, while our
manual weights emphasized agreement).1 Most en-
couragingly, the correct examples provide some use-
ful ?best? sub-class utterances, from which the rele-
vant properties could be extracted.
These results can probably be significantly im-
proved: rather than sum over the binary classifica-
tion outputs of each classifier, we can use their con-
fidence scores or posterior probabilities, and learn
1Accuracy here is currently assessed only over correct de-
tection of an action item in a window, not correct assignment of
all sub-classes.
33
Mtg. AIs Correct False+ False- F-Score
1 3 2 1 1 0.67
2 4 1 0 3 0.40
3 5 2 1 3 0.50
4 4 4 0 0 1.00
5 3 0 1 3 0.00
Table 3: Combined Classification Performance
the combination weights to give a more robust ap-
proach. There is still a long way to go to evaluate
this approach over more data, including the accu-
racy and utility of the resulting sub-class utterance
hypotheses.
6 Discussion and Future Work
So accounting for the structure of action items ap-
pears essential to detecting them in spoken dis-
course. Otherwise, classification accuracy is lim-
ited. We believe that accuracy can be improved, and
the detected utterances can be used to provide the
properties of the action item itself. An interesting
question is how and whether the structure we use
here relates to discourse structure in more general
use. If a relation exists, this would shed light on the
decision-making process we are attempting to (be-
gin to) model, and might allow us to use other (more
plentiful) annotated data.
Our future efforts focus on annotating more meet-
ings to obtain large training and testing sets. We also
wish to examine performance when working from
speech recognition hypotheses (as opposed to the
human transcripts used here), and the best way to in-
corporate multiple hypotheses (either as n-best lists
or word confusion networks). We are actively inves-
tigating alternative approaches to sub-classifier com-
bination: better performance (and a more robust and
trainable overall system) might be obtained by using
a Bayesian network, or a maximum entropy classi-
fier as used by (Klein et al, 2002). Finally, we are
developing an interface to a new large-vocabulary
version of the Gemini parser (Dowding et al, 1993)
which will allow us to use semantic parse informa-
tion as features in the individual sub-class classifiers,
and also to extract entity and event representations
from the classified utterances for automatic addition
of entries to calendars and to-do lists.
References
S. Burger, V. MacLaren, and H. Yu. 2002. The ISLMeet-
ing Corpus: The impact of meeting type on speech
style. In Proceedings of the 6th International Confer-
ence on Spoken Language Processing (ICSLP 2002).
M. Core and J. Allen. 1997. Coding dialogues with
the DAMSL annotation scheme. In D. Traum, edi-
tor, AAAI Fall Symposium on Communicative Action
in Humans and Machines.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Task-focused summarization of email. In
Proceedings of the Text Summarization Branches Out
ACL Workshop.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proc. 31st Annual Meeting of the Association for
Computational Linguistics.
A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meet-
ing structure annotation: Data and tools. In Proceed-
ings of the 6th SIGdial Workshop on Discourse and
Dialogue.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C.Wooters. 2003. The ICSI Meeting Corpus.
In Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2003).
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector
Learning. MIT Press.
D. Klein, K. Toutanova, H. T. Ilhan, S. D. Kamvar, and
C. D.Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceedings
of the ACL Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions.
W. Morgan, S. Gupta, and P.-C. Chang. forthcoming.
Automatically detecting action items in audio meeting
recordings. Ms., under review.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI Meeting Recorder Dialog Act
Corpus. In Proceedings of the 5th SIGdial Workshop
on Discourse and Dialogue.
S. Siegel and J. N. J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue act
classification using intra-utterance features. In Proc.
AAAI Workshop on Spoken Language Understanding.
34
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 17?24,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Multimodal Presentation Dashboard 
 
Michael Johnston 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
johnston 
@research. 
att.com 
Patrick Ehlen 
CSLI 
 Stanford University 
Palo Alto, CA  
ehlen@csli. 
stanford.edu 
David Gibbon 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
dcg@research. 
att.com 
Zhu Liu 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
zliu@research. 
att.com 
 
Abstract 
The multimodal presentation dashboard al-
lows users to control and browse presenta-
tion content such as slides and diagrams 
through a multimodal interface that sup-
ports speech and pen input. In addition to 
control commands (e.g. ?take me to slide 
10?), the system allows multimodal search 
over content collections. For example, if 
the user says ?get me a slide about internet 
telephony,? the system will present a 
ranked series of candidate slides that they 
can then select among using voice, pen, or 
a wireless remote. As presentations are 
loaded, their content is analyzed and lan-
guage and understanding models are built 
dynamically. This approach frees the user 
from the constraints of linear order allow-
ing for a more dynamic and responsive 
presentation style. 
1 Introduction 
Anthropologists have long informed us that the 
way we work?whether reading, writing, or giving 
a presentation?is tightly bound to the tools we 
use. Web browsers and word processors changed 
the way we read and write from linear to nonlinear 
activities, though the linear approach to giving a 
presentation to a roomful of people has evolved 
little since the days of Mylar sheets and notecards, 
thanks to presentation software that reinforces?or 
even further entrenches?a linear bias in our no-
tion of what ?giving a presentation? means to us. 
While today?s presentations may be prettier and 
flashier, the spontaneity once afforded by holding a 
stack of easily re-arrangeable sheets has been lost. 
 
 
 
Figure 1 Presentation dashboard in action 
Instead, a question from the audience or a change 
in plan at the podium results in a whizzing-by of 
all the wrong slides as the presenter sweats through 
an awkward silence while hammering an arrow 
key to track down the right one. In theory there are 
?search? functions that presenters could use to find 
another slide in the same presentation, or even in 
another presentation on the same machine, though 
none of the authors of this paper has ever seen a 
presenter do this. A likely reason is that these 
search functions are designed for desktop ergo-
17
nomics rather than for standing at a podium or 
walking around the room, making them even more 
disruptive to the flow of a presentation than frantic 
arrow key hammering. 
In some utopian future, we envision presenters 
who are unhindered by limitations imposed by 
their presentation tools, and who again possess, as 
Aristotle counseled, ?all available means of per-
suasion? at the tips of their fingers?or their 
tongues. They enjoy freeform interactions with 
their audiences, and benefit from random access to 
their own content with no arrow hammering and no 
disruption in flow. Their tools help to expand their 
possible actions rather than limiting them. We are 
hardly alone in this vision. 
In that spirit, many tools have been developed of 
late?both within and outside of research labs?
with the aim of helping people work more effec-
tively when they are involved in those assemblies 
of minds of mutual interest we often call ?meet-
ings.? Tools that capture the content of meetings, 
perform semantic understanding, and provide a 
browsable summary promise to free meeting par-
ticipants from the cognitive constraints of worrying 
about trying to record and recall what happened 
when a meeting takes place (e.g., Ehlen, Purver & 
Niekrasz, 2007; Tucker & Whittaker, 2005).  
Presentations are a kind of meeting, and several 
presentation tools have also sought to free present-
ers from similar constraints. For example, many 
off-the-shelf products provide speech interfaces to 
presentation software. These often replace the lin-
ear arrow key with the voice, offering command-
based navigation along a one-dimensional vector 
of slides by allowing a presenter to say ?next slide 
please? or ?go to the last slide.?  
A notable exception is the Jabberwocky inter-
face to PowerPoint (Franklin, Bradshaw & 
Hammond, 1999; 2000), which aims to follow 
along with a presenter?s talk?like a human assis-
tant might do?and switch to the appropriate slide 
when the presenter seems to be talking about it. 
Using a method similar to topic modeling, words 
spoken by the presenter are compared to a prob-
ability distribution of words across slides. Jabber-
wocky changes to a different slide when a 
sufficient probability mass has been reached to 
justify the assumption that the speaker is now talk-
ing about a different slide from the one that?s al-
ready showing. 
A similar effort (Rogina & Schaaf, 2002) uses 
words extracted from a presentation to augment a 
class-based language model and attempt automatic 
tracking of a presentation as it takes place. This 
intelligent meeting room system then aligns the 
presenter?s spoken words with parts of a presenta-
tion, hoping to determine when a presenter has 
moved on to a new slide. 
A major drawback of this ?machine-initiative? 
approach to presentation assistance is that a pre-
senter must speak enough words associated with a 
new slide for a sufficient probability mass to be 
reached before the slide is changed. The resulting 
delay is likely to make an audience feel like the 
presentation assistant is rather dim-witted. And any 
errors that change slides before the presenter is 
ready can be embarrassing and disruptive in front 
of potentially important audiences. 
So, in fashioning our own presentation control 
interface, we chose to allow the presenter to retain 
full initiative in changing slides, while offering a 
smarter and more flexible way to navigate through 
a presentation than the single degree of freedom 
afforded by arrow keys that simply traverse a pre-
determined order. The result is the Multimodal 
Presentation Dashboard, a presentation interface 
that integrates command-based control with prob-
abilistic, content-based search. Our method starts 
with a context-free grammar of speech commands, 
but embeds a stochastic language model generated 
from the presenter?s slide deck content so a pre-
senter can request any slide from the deck?or 
even a large set of decks?just by asking for its 
contents. Potentially ambiguous results are re-
solved multimodally, as we will explain. 
2 Multimodal interface for interactive 
presentations  
The presentation dashboard provides presenters 
with the ability to control and adapt their presenta-
tions on the fly in the meeting room. In addition to 
the traditional next/previous approach to navigat-
ing a deck of slides, they can access slides by posi-
tion in the active deck (e.g., ?show slide 10? or 
?last slide please?) or they can multimodally com-
bine voice commands with pen or remote control 
to browse for slides by content, saying, for in-
stance, ?show the slide on internet telephony,? and 
then using the pen to select among a ranked list of 
alternatives. 
18
2.1 Setup configuration 
Though the dashboard offers many setup configu-
rations, the preferred arrangement uses a single PC 
with two displays (Figure 1). Here, the dashboard 
is running on a tablet PC with a large monitor as a 
second external display. On the tablet, the 
dashboard UI is visible only to the presenter. On 
the external display, the audience sees the current 
slide, as they would with a normal presentation. 
The presenter can interact with the dashboard 
using either the microphone onboard the tablet PC, 
or, preferably, a wireless microphone. A wireless 
remote functions as a presentation control, which 
can be used to manually change slides in the tradi-
tional manner, and also provides a ?push to talk? 
button to tell the dashboard when to listen. A wire-
less microphone combined with the wireless pres-
entation control and voice selection mode (see 
Section 2.3) allows a presenter to stroll around the 
room or stage completely untethered.  
2.2 Presenter UI 
The presenter?s primary control of the system is 
through the presenter UI, a graphical user interface 
augmented with speech and pen input. The inter-
face has three main screens: a presentation panel 
for controlling an ongoing presentation (Figure 2), 
a loader panel for selecting a set of presentations to 
load (Figure 4), and a control panel for adjusting 
system settings and bundling shareable index and 
grammar models. The user can select among the 
panels using the tabs at the top left.  
 
  
Figure 2 The presentation panel 
 
The presentation panel has three distinct functional 
areas from top to bottom. The first row shows the 
current slide, along with thumbnails of the previ-
ous and next slides to provide context. The user 
can navigate to the next or previous slide by click-
ing on these thumbnails. The next row shows a 
scrolling list of search results from content-based 
queries. The last row contains interaction informa-
tion. There is a click & speak button for activating 
the speech recognizer and a feedback window that 
displays recognized speech.  
Some user commands are independent of the 
content of slide decks, as with basic commands for 
slide navigation: 
- ?next slide please? 
- ?go back? 
- ?last slide? 
In practice, however, navigation to next and previ-
ous slides is much easier using buttons on the wire-
less control. The presenter can also ask for slides 
by position number, allowing random access: 
- ?take me to slide 10? 
- ?slide 4 please? 
But not many presenters can remember the posi-
tion numbers of some 40 or 50 slides, we?d guess, 
so we added content-based search, a better method 
of random access slide retrieval by simply saying 
key words or phrases from the desired slide, e.g.:  
- ?slides about internet telephony? 
- ?get me the slide with the 
  system architecture? 
- ?2006 highlights? 
- ?budget plan, please? 
When the presenter gives this kind of request, the 
system identifies any slides that match the query 
and displays them in a rank ordered list in the mid-
dle row of the presenter?s panel. The presenter can 
then scroll through the list of thumbnails and click 
one to display it to the audience. 
This method of ambiguity resolution offers the 
presenter some discretion in selecting the correct 
slide to display from multiple search results, since 
search results appear first on the presenter?s private 
interface rather than being displayed to the audi-
ence. However, it requires the presenter to return to 
the podium (or wherever the tablet is located) to 
select the correct slide.  
 
19
2.3 Voice selection mode 
Alternatively, the presenter may sacrifice discre-
tion for mobility and use a ?voice selection mode,? 
which lets the presenter roam freely throughout the 
auditorium while making and resolving content-
based queries in plain view of the audience. In this 
mode, if a presenter issues a content-based query 
(e.g., ?shows slides about multimodal access?), 
thumbnails of the slides returned by the query ap-
pear as a dynamically-generated interactive 
?chooser? slide (Figure 3) in the main presentation 
viewed by the audience. The presenter can then 
select the desired slide by voice (e.g., ?slide three?) 
or by using the previous, next, and select controls 
on the wireless remote. If more than six slides are 
returned by the query, multiple chooser slides are 
generated with six thumbnails to each slide, which 
can be navigated with the remote.  
While voice selection mode allows the presenter 
greater mobility, it has the drawback of allowing 
the audience to see thumbnails of every slide re-
turned by a content-based query, regardless of 
whether the presenter intended for them to be seen. 
Hence this mode is more risky, but also more im-
pressive! 
 
 
Figure 3 Chooser slide for voice selection mode 
2.4 Compiling deck sets 
Sometimes a presenter wishes to have access to 
more than one presentation deck at a time, in order 
to respond to unexpected questions or comments, 
or to indulge in a whimsical tangent. We respond 
to this wish by allowing the presenter to compile a 
deck set, which is, quite simply, a user-defined 
bundle of multiple presentations that can all be 
searched at once, with their slides available for 
display when the user issues a query. In fact, this 
option makes it easy for a presenter to follow spon-
taneous tangents by switching from one presenta-
tion to another, navigating through the alternate 
deck for a while, and then returning to the original 
presentation, all without ever walking to the po-
dium or disrupting the flow of a presentation by 
stopping and searching through files. 
Deck sets are compiled in the loader panel (Fig-
ure 4), which provides a graphical browser for se-
lecting a set of active decks from the file system. 
When a deck set is chosen, the system builds ASR 
and language understanding models and a retrieval 
index for all the slides in the deck set. A compiled 
deck set is also portable, with all of the grammar 
and understanding model files stored in a single 
archive that can be transferred via e-mail or thumb 
drive and speedily loaded on another machine.  
A common use of deck sets is to combine a 
main presentation with a series of other slide decks 
that provide background information and detail for 
answering questions and expanding points, so the 
presenter can adapt to the interests of the audience. 
 
Figure 4 The loader panel 
3 Multimodal architecture  
The Multimodal Presentation Dashboard uses an 
underlying multimodal architecture that inherits 
core components from the MATCH architecture 
(Johnston et al2002). The components communi-
cate through a central messaging facilitator and 
include a speech recognition client, speech recog-
nition server (Goffin et al2005), a natural lan-
guage understanding component (Johnston & 
Bangalore 2005), an information retrieval engine, 
20
and a graphical user interface client.  The graphical 
UI runs in a web browser and controls PowerPoint 
via its COM interface.  
We first describe the compilation architecture, 
which builds models and performs indexing when 
the user selects a series of decks to activate. We 
then describe the runtime architecture that operates 
when the user gives a presentation using the sys-
tem. In Section 3.3, we provide more detail on the 
slide indexing mechanism and in Section 3.4 we 
describe a mechanism used to determine key-
phrases from the slide deck that are used on a drop 
down menu and for determining relevancy.  
3.1 Compilation architecture 
In a sense, the presentation dashboard uses neither 
static nor dynamic grammars; the grammars com-
piled with each deck set lie somewhere in-between 
those two concepts. Command-based speech inter-
faces often fare best when they rely on the predict-
ability of a fixed, context-free grammar, while 
interfaces that require broader vocabulary coverage 
and a wider range of syntax are better off leverag-
ing the flexibility of stochastic language models. 
To get the best of both worlds for our ASR model, 
we use a context-free command ?wrapper? to a 
stochastic language model (c.f. Wang & Acero 
2003). This is coupled to the understanding 
mechanism using a transducer with a loop over the 
content words extracted from the slides.  
This combined grammar is best thought of as a 
fixed, context-free template which contains an em-
bedded SLM of dynamic slide contents. Our 
method allows a static background grammar and 
understanding model to happily co-exist with a 
dynamic grammar component which is compiled 
on the fly when presentations are loaded, enabling 
custom, content-based queries.  
When a user designates a presentation deck set 
and compiles it, the slides in the set are processed 
to create the combined grammar by composing an 
SLM training corpus based on the slide content.  
First, a slide preprocessor extracts sentences, ti-
tles, and captions from each slide of each deck, and 
normalizes the text by converting numerals and 
symbols to strings, Unicode to ASCII, etc. These 
content phrases are then used to compose (1) a 
combined corpus to use for training an SLM for 
speech recognition, and (2) a finite-state transducer 
to use for multimodal natural language understand-
ing (Johnston & Bangalore 2005). 
Combined Corpus
Presentations
Slide 
index
Keyphrases
Slide PreprocessorSlide Preprocessor
Sentences
Index 
Server
Index 
Server
SLM 
for ASR
SLM 
for ASR NLU
MODEL
NLU
MODEL
GUI 
Menu
GUI 
Menu
Grammar
Template
Class-tagged
Corpus
Grammar
Words
 
Figure 5 Compilation architecture 
To create a combined corpus for the SLM, the con-
tent phrases extracted from slides are iterated over 
and folded into a static template of corpus classes. 
For instance, the template entry, 
<POLITE> <SHOWCON> <CONTENT_PHRASE> 
could generate the phrase ?please show the slide 
about <CONTENT_PHRASE>? for each content 
phrase?as well as many others.  These templates 
are currently manually written but could poten-
tially be induced from data as it becomes available. 
The content corpus is appended to a command 
corpus of static command classes that generate 
phrases like ?next slide please? or ?go back to the 
last one.? Since the number of these command 
phrases remains constant for every grammar while 
the number of content phrases depends on how 
many phrases are extracted from the deck set, a 
weighting factor is needed to ensure the number of 
examples of both content and command phrases is 
balanced in the SLM training data. The resulting 
combined corpus is used to build a stochastic lan-
guage model that can handle variations on com-
mands and slide content.  
In parallel to the combined corpus, a stack of 
slide content words is compiled for the finite state 
understanding machine. Phrases extracted for the 
combined corpus are represented as a terminal 
_CWORD class. (Terminals for tapes in each gram-
mar class are separated by colons, in the format 
speech:meaning, with empty transitions repre-
21
sented as ?) For example, the phrase ?internet 
telephony? on a slide would appear in the under-
standing grammar like so: 
_CWORD internet:internet 
_CWORD telephony:telephony 
These content word classes are then ?looped? in 
the FSM (Figure 6) into a flexible understanding 
model of potential slide content results using only 
a few grammar rules, like: 
_CONTENT _CWORD _CONTENT 
_CONTENT _CWORD 
The SLM and the finite-state understanding ma-
chine now work together to extract plausible mean-
ings from dynamic and inexact speech queries. 
 
 
Figure 6 Understanding FSM 
To provide an example of how this combined ap-
proach to understanding comes together in the run-
ning system, let?s say a presenter?s slide contains 
the title ?Report for Third Quarter? and she asks 
for it by saying, ?put up the third quarter report 
slide.? Though she asks for the slide with language 
that doesn?t match the phrase on the slide, our for-
giving stochastic model might return a speech re-
sult like, ?put up third quarter report mine.? The 
speech result is then mapped to the finite-state 
grammar, which catches ?third quarter report 
mine? as a possible content phrase, and returns, 
?third,quarter,report,mine? as a con-
tent-based meaning result. That result is then used 
for information retrieval and ranking to determine 
which slides best match the query (Section 3.3). 
3.2 Runtime architecture  
A primary goal of the presentation dashboard was 
that it should run standalone on a single laptop. A 
tablet PC works best for selecting slides with a 
pen, though a mouse or touch screen can also be 
used for input. We also developed a networked 
version of the dashboard system where indexing, 
compilation, speech recognition, and understand-
ing are all network services accessed over HTTP 
and SIP, so any web browser-based client can log 
in, upload a presentation, and present without in-
stalling software aside from PowerPoint and a SIP 
plug-in. However, our focus in this paper is on the 
tablet PC standalone version. 
ASR SERVERASR SERVER
Multimodal Dashboard 
UI (Browser)
Multimodal Dashboard 
UI (Browser)
NLUNLU
Powerpoint
Application
Powerpoint
Application
Index Server (http)Index Server (http)
Language
Model
Slide index
HTTP
Commands
Images
FACILITATORFACILITATOR
SPEECH
CLIENT
SPEECH
CLIENT
Understanding
Model
 
Figure 7 Multimodal architecture 
The multimodal user interface client is browser-
based, using dynamic HTML and Javascript. Inter-
net Explorer provides COM access to the Power-
Point object model, which reveals slide content and 
controls the presentation. Speech recognition, un-
derstanding, and compilation components are ac-
cessed through a java-based facilitator via a socket 
connection provided by an ActiveX control on the 
client page (Figure 7). When the user presses or 
taps the click & speak button, a message is sent to 
the Speech client, which sends audio to the ASR 
Server. The recognizer?s speech result is processed 
by the NLU component using a finite-state trans-
ducer to translate from the input string to an XML 
meaning representation. When the multimodal UI 
receives XML for simple commands like ?first 
slide? or ?take me to slide ten,? it calls the appro-
priate function through the PowerPoint API. For 
content-based search commands, an SQL query is 
constructed and issued to the index server as an 
HTTP query. When the results are returned, mul-
timodal thumbnail images of each slide appear in 
the middle row of the UI presenter panel. The user 
can then review the choices and switch to the ap-
propriate slide by clicking on it?or, in voice se-
lection mode, by announcing or selecting a slide 
shown in the dynamically-generated chooser slide.  
The system uses a three stage strategy in search-
ing for slides. First it attempts an exact match by 
looking for slides which have the words of the 
query in the same order on the same slide in a sin-
gle phrase. If no exact matches are found, the sys-
tem backs off to an AND query and shows slides 
which contain all of the words, in any order. If that 
22
fails, the system resorts to an OR query and shows 
slides which have any of the query terms.  
3.3 Information retrieval 
When the slide preprocessor extracts text from a 
presentation, it retains the document structure as 
much as possible and stores this in a set of hier-
archal XML documents. The structure includes 
global document metadata such as creation date 
and title, as well as more detailed data such as slide 
titles. It also includes information about whether 
the text was part of a bullet list or text box. With 
this structure, queries can be executed against the 
entire text or against specified textual attributes 
(e.g. ?show me the chart titled ?project budget??). 
For small document collections, XPath queries 
can search the entire collection with good response 
time, providing a stateless search method. But as 
the collection of presentation decks to be searched 
grows, a traditional inverted index information re-
trieval system achieves better response times. We 
use a full text retrieval system that employs stem-
ming, proximity search, and term weighting, and 
supports either a simplified query syntax or SQL. 
Global metadata can also constrain queries. Incre-
mental indexing ensures that new presentation 
decks cause the index to update automatically 
without being rebuilt from scratch. 
3.4 Key phrase extraction 
Key phrases and keywords are widely used for in-
dexing and retrieving documents in large data-
bases. For presentation slides, they can also help 
rank a slide?s relevance to a query. We extract a 
list of key phrases with importance scores for each 
slide deck, and phrases from a set of decks are 
merged and ranked based on their scores. 
A popular approach to selecting keywords from 
a document within a corpus is to find keywords 
that frequently occur in one document but seldom 
occur in others, based on term frequency-inverse 
document frequency (TF-IDF). Our task is slightly 
different, since we wish to choose key phrases for 
a single document (the slide deck), independent of 
other documents. So our approach uses term fre-
quency-inverse term probability (TF-ITP), which 
expresses the probability of a term calculated over 
a general language rather than a set of documents. 
Assuming a term Tk occurs tfk times in a docu-
ment, and its term probability is tpk, the TF-ITP of 
Tk is defined as, wTk = tfk / tpk. This method can be 
extended to assign an importance score to each 
phrase. For a phrase Fk = {T1 T2 T3 ? TN}, which 
contains a sequence of N terms, assuming it ap-
pears ffk times in a document, its importance score, 
ISk, is defined as, 
?
=
=
N
i i
k
k T
ff
IS
1
. 
To extract a set of key phrases, we first segment 
the document into sentences based on punctuation 
and some heuristics. A Porter stemming algorithm 
(Porter 1980) eliminates word variations, and 
phrases up to N=4 terms long are extracted, remov-
ing any that start or end with noise words. An im-
portance score ranks each phrase, where term 
probabilities are estimated from transcripts of 600 
hours of broadcast news data. A term that is out of 
the vocabulary with a term frequency of more than 
2 is given a default term probability value, defined 
as the minimum term probability in the vocabulary. 
Phrases with high scores are chosen as key 
phrases, eliminating any phrases that are contained 
in other phrases with higher scores. For an overall 
list of key phrases in a set of documents, we merge 
individual key phrase lists and sum the importance 
scores for key phrases that recur in different lists, 
keeping the top 10 phrases. 
4 Performance and future work 
The dashboard is fully implemented, and has been 
used by staff and management in our lab for inter-
nal presentations and talks. It can handle large 
decks and collections (100s to 1000s of slides). A 
tablet PC with a Pentium M 1.6Ghz processor and 
1GB of RAM will compile a presentation of 50 
slides?with ASR, understanding models, and 
slide index?in under 30 seconds.  
In ongoing work, we are conducting a usability 
test of the system with users in the lab. Effective 
evaluation of a tool of this kind is difficult without 
fielding the system to a large number of users. An 
ideal evaluation would measure how users fare 
when giving their own presentations, responding to 
natural changes in narrative flow and audience 
questions. Such interaction is difficult to simulate 
in a lab, and remains an active area of research. 
23
We also hope to extend current retrieval meth-
ods to operate at the level of concepts, rather than 
words and phrases, so a request to show ?slides 
about mortgages? might return a slide titled ?home 
loans.? Thesauri, gazetteers, and lexicons like 
WordNet will help achieve this. Analyzing non-
textual elements like tables and charts could also 
allow a user to say, ?get the slide with the network 
architecture diagram.? And, while we now use a 
fixed lexicon of common abbreviations, an auto-
mated analysis based on web search and other 
techniques could identify likely expansions. 
5 Conclusion 
Our goal with the multimodal presentation 
dashboard was to create a meeting/presentation 
assistance tool that would change how people be-
have, inspiring presenters to expand the methods 
they use to interact with audiences and with their 
own material. To this end, our dashboard runs on a 
single laptop, leaves the initiative in the hands of 
the presenter, and allows slides from multiple pres-
entations to be dynamically retrieved from any-
where in the room. Our assistant requires no 
?intelligent room?; only an intelligent presenter, 
who may now offer the audience a presentation 
that is as dynamic or as dull as imagination allows. 
As Tufte (2006) reminds us in his analysis of 
how PowerPoint presentations may have precipi-
tated the Columbia shuttle tragedy, the way infor-
mation is presented can have a profound?even 
life-threatening?impact on the decisions we 
make. With the multimodal presentation 
dashboard, we hope to free future presenters from 
that single, arrow-key dimension, offering access 
to presentation slides and diagrams in any order, 
using a diverse combination of modes. Presenters 
can now pay more attention to the needs of their 
audiences than to the rigid determinism of a fixed 
presentation. Whether they will break free of the 
linear presentation style imposed by current tech-
nology if given a chance remains to be seen. 
References  
Patrick Ehlen, Matthew Purver, and John Niekrasz. 
2007. A meeting browser that learns. In Proceedings 
of the AAAI Spring Symposium on Interaction Chal-
lenges for Intelligent Assistants. 
David Franklin, Shannon Bradshaw, and Kristian 
Hammond. 1999. Beyond ?Next slide, please?: The 
use of content and speech in multi-modal control. In 
Working Notes of the AAAI-99 Workshop on Intelli-
gent Information Systems. 
David Franklin, Shannon Bradshaw, and Kristian 
Hammond. 2000. Jabberwocky: You don't have to be 
a rocket scientist to change slides for a hydrogen 
combustion lecture. In Proceedings of Intelligent 
User Interfaces 2000 (IUI-2000). 
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, Dilek 
Hakkani-T?r, Andrej Ljolje, Sarangarajan Partha-
sarathy, Mazin Rahim, Giuseppe Riccardi, and Murat 
Saraclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP.   
Michael Johnston, Srinivas Bangalore, Guna Vasireddy, 
Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve 
Whittaker, Preetam Maloor. 2002. MATCH: An Ar-
chitecture for Multimodal Dialogue Systems. In Pro-
ceedings of the 40th ACL. 376-383. 
Michael Johnston  and Srinivas Bangalore. 2005. Finite-
state multimodal integration and understanding. 
Journal of Natural Language Engineering. 11.2, pp. 
159-187, Cambridge University Press. 
Martin F. Porter. 1980. An algorithm for suffix strip-
ping, Program, 14, 130-137. 
Ivica Rogina and Thomas Schaaf. 2002. Lecture and 
presentation tracking in an intelligent meeting room. 
In Proceedings of the 4th IEEE International Confer-
ence on Multimodal Interfaces. 47-52. 
Simon Tucker and Steve Whittaker. 2005. Accessing 
multimodal meeting data: Systems, problems and 
possibilities. In Samy Bengio and Herv? Bourlard 
(Eds.) Lecture Notes in Computer Science, 3361, 1-
11 
Edward Tufte. 2006. The Cognitive Style of PowerPoint. 
Graphics Press, Cheshire, CT. 
Ye-Yi Wang and Alex Acero. 2003. Combination of 
CFG and N-gram Modeling in Semantic Grammar 
Learning. Proceedings of Eurospeech conference, 
Geneva, Switzerland. 
Acknowledgements We would like to thank Srinivas Banga-
lore, Rich Cox, Mazin Gilbert, Vincent Goffin, and Behzad 
Shahraray for their help and support.  
24
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156?163,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Modelling and Detecting Decisions in Multi-party Dialogue
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters
Center for the Study of Language and Information
Stanford University
{raquel|frampton|ehlen|mpurver|peters}@stanford.edu
Abstract
We describe a process for automatically de-
tecting decision-making sub-dialogues in tran-
scripts of multi-party, human-human meet-
ings. Extending our previous work on ac-
tion item identification, we propose a struc-
tured approach that takes into account the dif-
ferent roles utterances play in the decision-
making process. We show that this structured
approach outperforms the accuracy achieved
by existing decision detection systems based
on flat annotations, while enabling the extrac-
tion of more fine-grained information that can
be used for summarization and reporting.
1 Introduction
In collaborative and organized work environments,
people share information and make decisions exten-
sively through multi-party conversations, usually in
the form of meetings. When audio or video record-
ings are made of these meetings, it would be valu-
able to extract important information, such as the
decisions that were made and the trains of reason-
ing that led to those decisions. Such a capability
would allow work groups to keep track of courses
of action that were shelved or rejected, and could al-
low new team members to get quickly up to speed.
Thanks to the recent availability of substantial meet-
ing corpora?such as the ISL (Burger et al, 2002),
ICSI (Janin et al, 2004), and AMI (McCowan et
al., 2005) Meeting Corpora?current research on the
structure of decision-making dialogue and its use for
automatic decision detection has helped to bring this
vision closer to reality (Verbree et al, 2006; Hsueh
and Moore, 2007b).
Our aim here is to further that research by ap-
plying a simple notion of dialogue structure to the
task of automatically detecting decisions in multi-
party dialogue. A central hypothesis underlying our
approach is that this task is best addressed by tak-
ing into account the roles that different utterances
play in the decision-making process. Our claim is
that this approach facilitates both the detection of
regions of discourse where decisions are discussed
and adopted, and also the identification of important
aspects of the decision discussions themselves, thus
opening the way to better and more concise report-
ing.
In the next section, we describe prior work on re-
lated efforts, including our own work on action item
detection (Purver et al, 2007). Sections 3 and 4 then
present our decision annotation scheme, which dis-
tinguishes several types of decision-related dialogue
acts (DAs), and the corpus used as data (in this study
a section of the AMI Meeting Corpus). Next, in Sec-
tion 5, we describe our experimental methodology,
including the basic conception of our classification
approach, the features we used in classification, and
our evaluation metrics. Section 6 then presents our
results, obtained with a hierarchical classifier that
first trains individual sub-classifiers to detect the dif-
ferent types of decision DAs, and then uses a super-
classifier to detect decision regions on the basis of
patterns of these DAs, achieving an F-score of 58%.
Finally, Section 7 presents some conclusions and di-
rections for future work.
2 Related Work
Recent years have seen an increasing interest in re-
search on decision-making dialogue. To a great
extent, this is due to the fact that decisions have
156
been shown to be a key aspect of meeting speech.
User studies (Lisowska et al, 2004; Banerjee et al,
2005) have shown that participants regard decisions
as one of the most important outputs of a meeting,
while Whittaker et al (2006) found that the develop-
ment of an automatic decision detection component
is critical to the re-use of meeting archives. Identify-
ing decision-making regions in meeting transcripts
can thus be expected to support development of a
wide range of applications, such as automatic meet-
ing assistants that process, understand, summarize
and report the output of meetings; meeting tracking
systems that assist in implementing decisions; and
group decision support systems that, for instance,
help in constructing group memory (Romano and
Nunamaker, 2001; Post et al, 2004; Voss et al,
2007).
Previously researchers have focused on the in-
teractive aspects of argumentative and decision-
making dialogue, tackling issues such as the detec-
tion of agreement and disagreement and the level
of emotional involvement of conversational partic-
ipants (Hillard et al, 2003; Wrede and Shriberg,
2003; Galley et al, 2004; Gatica-Perez et al, 2005).
From a perhaps more formal perspective, Verbree et
al. (2006) have created an argumentation scheme in-
tended to support automatic production of argument
structure diagrams from decision-oriented meeting
transcripts. Only Hsueh and Moore (2007a; 2007b),
however, have specifically investigated the auto-
matic detection of decisions.
Using the AMI Meeting Corpus, Hsueh and
Moore (2007b) attempt to identify the dialogue acts
(DAs) in a meeting transcript that are ?decision-
related?. The authors define these DAs on the ba-
sis of two kinds of manually created summaries: an
extractive summary of the whole meeting, and an
abstractive summary of the decisions made in the
meeting. Those DAs in the extractive summary that
support any of the decisions in the abstractive sum-
mary are then manually tagged as decision-related
DAs. They trained a Maximum Entropy classifier
to recognize this single DA class, using a variety of
lexical, prosodic, dialogue act and topical features.
The F-score they achieved was 0.35, which gives a
good indication of the difficulty of this task.
In our previous work (Purver et al, 2007), we at-
tempted to detect a particular kind of decision com-
mon in meetings, namely action items?public com-
mitments to perform a given task. In contrast to
the approach adopted by Hsueh and Moore (2007b),
we proposed a hierarchical approach where indi-
vidual classifiers were trained to detect distinct ac-
tion item-related DA classes (task description, time-
frame, ownership and agreement) followed by a
super-classifier trained on the hypothesized class la-
bels and confidence scores from the individual clas-
sifiers that would detect clusters of multiple classes.
We showed that this structured approach produced
better classification accuracy (around 0.39 F-score
on the task of detecting action item regions) than a
flat-classifier baseline trained on a single action item
DA class (around 0.35 F-score).
In this paper we extend this approach to the more
general task of detecting decisions, hypothesizing
that?as with action items?the dialogue acts in-
volved in decision-making dialogue form a rather
heterogeneous set, whose members co-occur in par-
ticular kinds of patterns, and that exploiting this
richer structure can facilitate their detection.
3 Decision Dialogue Acts
We are interested in identifying the main conver-
sational units in a decision-making process. We ex-
pect that identifying these units will help in detect-
ing regions of dialogue where decisions are made
(decision sub-dialogues), while also contributing to
identification and extraction of specific decision-
related bits of information.
Decision-making dialogue can be complex, often
involving detailed discussions with complicated ar-
gumentative structure (Verbree et al, 2006). Deci-
sion sub-dialogues can thus include a great deal of
information that is potentially worth extracting. For
instance, we may be interested in knowing what a
decision is about, what alternative proposals were
considered during the decision process, what argu-
ments were given for and against each of them, and
last but not least, what the final resolution was.
Extracting these and other potential decision com-
ponents is a challenging task, which we do not in-
tend to fully address in this paper. This initial study
concentrates on three main components we believe
constitute the backbone of decision sub-dialogues.
A typical decision sub-dialogue consists of three
main components that often unfold in sequence. (a)
157
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the decision that is adopted
RP ? proposal ? utterances where the decision adopted is proposed
RR ? restatement ? utterances where the decision adopted is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision made
Table 1: Set of decision dialogue act (DDA) classes
A topic or issue that requires some sort of conclu-
sion is initially raised. (b) One or more proposals are
considered. And (c) once some sort of agreement is
reached upon a particular resolution, a decision is
adopted.
Dialogue act taxonomies often include tags
that can be decision-related. For instance, the
DAMSL taxonomy (Core and Allen, 1997) in-
cludes the tags agreement and commit, as well
as a tag open-option for utterances that ?sug-
gest a course of action?. Similarly, the AMI
DA scheme1 incorporates tags like suggest,
elicit-offer-or-suggestion and assess.
These tags are however very general and do not cap-
ture the distinction between decisions and more gen-
eral suggestions and commitments.2 We therefore
devised a decision annotation scheme that classifies
utterances according to the role they play in the pro-
cess of formulating and agreeing on a decision. Our
scheme distinguishes among three main decision di-
alogue act (DDA) classes: issue (I), resolution (R),
and agreement (A). Class R is further subdivided into
resolution proposal (RP) and resolution restatement
(RR). A summary of the classes is given in Table 1.
Annotation of the issue class includes any utter-
ances that introduce the topic of the decision discus-
sion. For instance, in example (1) below, the utter-
ances ?Are we going to have a backup?? and ?But
would a backup really be necessary?? are tagged as
I. The classes RP and RR are used to annotate those
utterances that specify the resolution adopted?i.e.
the decision made. Annotation with the class RP
includes any utterances where the resolution is ini-
1A full description of the AMI Meeting Corpus DA scheme
is available at http://mmm.idiap.ch/private/ami/
annotation/dialogue acts manual 1.0.pdf, after
free registration.
2Although they can of course be used to aid the identification
process?see Section 5.3.
tially proposed (like the utterance ?I think maybe we
could just go for the kinetic energy. . . ?). Sometimes
decision discussions include utterances that sum up
the resolution adopted, like the utterance ?Okay,
fully kinetic energy? in (1). This kind of utterance
is tagged with the class RR. Finally, the agreement
class includes any utterances in which participants
agree with the (proposed) resolution, like the utter-
ances ?Yeah? and ?Good? as well as ?Okay? in di-
alogue (1).
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.3
Note that an utterance can be assigned to more
than one of these classes. For instance, the utter-
ance ?Okay, fully kinetic energy? is annotated both
as RR and A. Similarly, each decision sub-dialogue
may contain more than one utterance corresponding
to each class, as we saw above for issue. While
we do not a priori require each of these classes to
be present for a set of utterances to be considered
a decision sub-dialogue, all annotated decision sub-
dialogues in our corpus include the classes I, RP and
A. The annotation process and results are described
in detail in the next section.
3This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
158
4 Data: Corpus & Annotation
In this study, we use 17 meetings from the AMI
Meeting Corpus (McCowan et al, 2005), a pub-
licly available corpus of multi-party meetings con-
taining both audio recordings and manual transcrip-
tions, as well as a wide range of annotated infor-
mation including dialogue acts and topic segmenta-
tion. Conversations are all in English, but they can
include native and non-native English speakers. All
meetings in our sub-corpus are driven by an elicita-
tion scenario, wherein four participants play the role
of project manager, marketing expert, interface de-
signer, and industrial designer in a company?s de-
sign team. The overall sub-corpus makes up a total
of 15,680 utterances/dialogue acts (approximately
920 per meeting). Each meeting lasts around 30
minutes.
Two authors annotated 9 and 10 dialogues each,
overlapping on two dialogues. Inter-annotator
agreement on these two dialogues was similar to
(Purver et al, 2007), with kappa values ranging
from 0.63 to 0.73 for the four DDA classes. The
highest agreement was obtained for class RP and the
lowest for class A.4
On average, each meeting contains around 40
DAs tagged with one or more of the DDA sub-
classes in Table 1. DDAs are thus very sparse, cor-
responding to only 4.3% of utterances. When we
look at the individual DDA sub-classes this is even
more pronounced. Utterances tagged as issue make
up less than 0.9% of utterances in a meeting, while
utterances annotated as resolution make up around
1.4%?1% corresponding to RP and less than 0.4%
to RR on average. Almost half of DDA utterances
(slightly over 2% of all utterances on average) are
tagged as belonging to class agreement.
We compared our annotations with the annota-
tions of Hsueh and Moore (2007b) for the 17 meet-
ings of our sub-corpus. The overall number of ut-
terances annotated as decision-related is similar in
the two studies: 40 vs. 30 utterances per meeting on
average, respectively. However, the overlap of the
annotations is very small leading to negative kappa
scores. As shown in Figure 1, only 12.22% of ut-
4The annotation guidelines we used are available on-
line at http://godel.stanford.edu/twiki/bin/
view/Calo/CaloDecisionDiscussionSchema

  









  !"
##Proceedings of the SIGDIAL 2013 Conference, pages 329?333,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Spoken Dialog Systems for Automated Survey Interviewing 
Michael Johnston1, Patrick Ehlen2, Frederick G. Conrad3, Michael F. Schober4,  Christopher Antoun3, Stefanie Fail4, Andrew Hupp3, Lucas Vickers4,  Huiying Yan3, Chan Zhang3 AT&T Labs Research, Florham Park, NJ, USA1, AT&T, San Francisco, CA, USA2 Survey Research Center, University of Michigan, Ann Arbor, USA3 The New School, New York, NY, USA4 johnston@research.att.com, ehlen@research.att.com,  fconrad@umich.edu, schober@newschool.edu,  antoun@umich.edu, stefaniefail@gmail.com, ahupp@umich.edu,  lucasvickers@gmail.com, yanhuier@umich.edu, chanzh@umich.edu 
Abstract We explore the plausibility of using automated spoken dialog systems (SDS) for administer-ing survey interviews. Because the goals of a survey dialog system differ from more tradi-tional information-seeking and transactional applications, different measures of task accu-racy and success may be warranted. We report a large-scale experimental evaluation of an SDS that administered survey interviews with questions drawn from government and social scientific surveys. We compare two dialog confirmation strategies: (1) a traditional strate-gy of explicit confirmation on low-confidence recognition; and (2) no confirmation. With ex-plicit confirmation, the small percentage of re-sidual errors had little to no impact on survey data measurement. Even without confirmation, while there are significantly more errors, im-pact on the substantive conclusions of the sur-vey is still very limited. 1 Introduction Survey interviews play a critical role in the oper-ation of government and commerce. Large-scale social scientific surveys provide key indicators of the success or failure of economic and social pol-icies, driving critical policy and funding deci-sions. Market research surveys are key in evalu-ating products and services for business. Survey interviews are typically conducted ei-ther via telephone or face-to-face by skilled hu-man interviewers. But ongoing changes in com-munication technology threaten the viability of these methods. As people migrate from landline telephony to mobile-only (Ehlen and Ehlen 2007) and Voice-over-IP (Fuchs 2008) as prima-ry modes of communication, they undermine the effectiveness of traditional survey sampling techniques that rely on random selection of num-
bers within a dial code. Telephone respondents were once reachable at a fixed geographic loca-tion in a largely predictable conversational envi-ronment. Now they are increasingly mobile, and more apt to prefer asynchronous communication. Thus it is imperative to understand how these changing behaviors affect survey results. The work described here is part of a larger re-search project (see Schober et al 2012; Conrad et al 2013) that investigates the viability of four different modes for administering a survey inter-view over a smartphone: automated voice, hu-man voice, automated SMS text, and human SMS text. Here we focus specifically on the au-tomated voice mode and explore the use of a spoken dialog system for survey administration. Spoken dialog systems are widely used in te-lephony applications such as customer service, information access, and transaction fulfillment. They are also now common in virtual assistant applications for smartphones and mobile devices. But survey designers seeking automation have mostly eschewed spoken dialog in favor of tex-tual web surveys or touchtone DTMF response systems. A preliminary comparison of spoken dialog and touchtone survey systems is available in Bloom (2008), and Stent et al (2007) offer an evaluation of a spoken dialog system for aca-demic course ratings. The work presented here describes the first large-scale investigation into spoken dialog technology as a viable means of administering the kinds of surveys that produce official statistics and social scientific data.  Survey interview designers should be interest-ed in using spoken dialog systems for several reasons. The most obvious reason is to curtail the error and bias that human interviewers are known to introduce to survey results data. Dec-ades of research and investment led to ?standard-ized interviewing techniques? to reduce this error (Fowler and Mangione 1990), and limit a survey 
329
interviewer?s ability to offer help or clarification in ways that might affect results. Automated dia-log systems can be thought of as the ultimate in standardization, as they can be designed to pro-vide exactly the same interaction possibilities to all respondents. In effect, everyone can be inter-viewed by the same ?interviewer.? Or, if survey designers want to allow clarification in an inter-view, an automated spoken dialog system can ensure that the same possibilities are available to all respondents (Schober and Conrad 1997). Unlike systems that use human interviewers, there is marginal additional cost per interview after the initial investment of building a system. This offers significant potential for cost savings in large cross-sectional samples or repeated panel surveys, such as the U.S. Current Population Survey or the American Community Survey. Re-peated data collection allows refinement and re-training of speech models to improve perfor-mance. Spoken dialog system surveys can be administered on demand at any time of day, al-lowing a better fit with respondents? circum-stances and schedules. Compared to asynchro-nous text-based interviews like web or paper-and-pencil surveys, spoken dialog systems can capture richer verbal paradata (Couper 2009) or process data like pauses, disfluencies and proso-dy (Ehlen et al 2007). Finally, survey tasks fit nicely within the limitations of current recogni-tion and dialog technology, since they tend to have a purposefully structured and controlled interaction flow and generally require only a lim-ited number of responses to each question. While spoken dialog systems have the poten-tial to remove data error that is introduced by variation in human interviewer behaviors, they also introduce risks to survey data quality due to speech recognition and understanding error.  Numerous strategies for mitigating error have been explored in research on dialog systems (Bohus and Rudnicky 2005, Litman et al 2006). One approach is to use either an explicit or im-plicit confirmation of the user's input. Following previous research showing that explicit confir-mation is less confusing for users (Shin et al 2002), we adopt an explicit confirmation strate-gy, which is also more in keeping with standard-ized interview techniques. The effects of speech recognition and under-standing errors may be different in a survey dia-log system than in most current spoken dialog applications. One consideration is speaker initia-tive, and the stake of the user in the interaction. In systems for customer service, information ac-
cess, or transactions, the user generally initiates contact with the system and seeks to accomplish a task where the system?s recognition accuracy will affect success of the user?s own goal. But in a survey dialog, the system initiates contact, and most respondents do not have a stake in whether the designers of the survey system succeed at collecting high quality data from them.  This is a key point where a survey interview-ing system might differ from traditional SDS: From the survey researchers? perspective, the critical question is not whether individual users achieve some goal, but rather the extent to which individual errors in system recognition and un-derstanding affect the distribution of responses across the population sample, affecting the quali-ty of the estimates produced. If recognition errors do not affect the substantive conclusions based on the survey data, then survey researchers should be able to tolerate the imprecision of recognition error. This situation makes survey system evaluation rather different from how one would expect to evaluate the task success of a traditional SDS, like a customer service system.  In Section 2, we characterize the content of the survey items, describe the dialog strategy, and provide examples of interaction. Section 3 de-scribes the technical architecture of the survey dialog system. We provide experimental evalua-tion in Section 4, and conclusions in Section 5. 2 Survey interview dialogs After an initial question assessing whether the respondent is in an environment where it is safe for them to talk, our system administers a series of 32 questions drawn from major U.S. social surveys, including the Behavioral Risk Factor Surveillance System (BRFSS), National Survey of Drug Use and Health (NSDUH), General So-cial Survey (GSS), and the Pew Internet and American Life Project. The sample transcribed dialogs in Appendix 1 illustrate various features of interaction with the system. Question types include Yes/No, categorical (where users pick from a specified set of response options), and numerical questions. Some categorical items are grouped into battery questions with the same re-sponse options for all the items. The system supports explicit requests to repeat the question or ask for help, and mimics a ?standardized interviewing? style of interaction that trained interviewers would use to repeat or clarify a question when the answer is rejected or requires confirmation. Thresholds set on acoustic and language confidence scores are used to de-
330
cide whether to reject, explicitly confirm, or ac-cept a response. The final question in the dialog in Appendix A (?Thinking about ??) illustrates the importance of confirmation in ensuring the correct survey response is recorded. In this case, the system misrecognized ?None? for ?Nine,? but this was caught by the explicit confirmation prompt. Two terms are introduced in the final example that we will return to in the evaluation. First hypothesis indicates the speech recognition and semantic result produced by the system the first time the question is asked. Last hypothesis indicates the speech recognition and semantic result that the system produced the last time the question was asked within the segment.  3 System Architecture The survey dialog system is directly integrated with a custom-built survey data case collection management system (PAMSS). When a survey case is administered, the case management sys-tem makes an HTTP request to a voice gateway, which initiates a call to the respondent. When the respondent answers, it bridges the call to a spo-ken dialog system running within the AT&T WatsonSM speech platform. The system uses pre-recorded prompts for survey questions and re-prompts. Confirmations for numeric responses combine prompts with TTS output. 
 Figure 1: Survey Dialog System Architecture Users? spoken inputs are recognized using state-specific grammars for each question. Data were not initially available for training statistical mod-els, so SRGS (Hunt and McGlashan 2004) grammars were built for each answer. These were tuned in an initial pilot phase. The gram-mars included standard responses for the ques-tion, along with common paraphrases and fram-ing words from the question. In the Watson plat-form, a dialog manager  (built in Python) is inte-grated with ASR and TTS engines. Questions to be administered are represented in a declarative format in a survey item specification along with 
references to the appropriate prompts and gram-mars. The dialog manager interprets this specifi-cation to administer the survey and control the interaction flow. As the user responds to ques-tions, the answers are posted back to the survey case management system.  4 Experimental Evaluation We evaluated the survey dialog system as part of the first phase of a larger experiment comparing different survey interaction modes (Schober et al 2012). In this phase, 642 subjects were recruited from Craigslist, Facebook, Google Ads, and Amazon Mechanical Turk. A web-based screener application verified respondents to be over 21 and collected their zip code. Of these, 158 re-spondents were randomly assigned to the auto-mated voice condition. A $20 iTunes gift card was given as an incentive after completion of a post-interview web questionnaire. This included multiple-choice questions examining user satis-faction with their experience. In total there were 8,228 spoken inputs over the 158 respondent dia-logs. These responses were transcribed, coded, and annotated for semantic content. The questions we sought to answer were: What is the performance of a spoken dialog sys-tem on a typical survey task? What impact does speech recognition and concept error have on overall survey estimates? Does an automated survey system benefit from implementing a tradi-tional confirmation strategy, where responses with low confidence scores are verified with con-firmation dialog? We also examine the impact of dialog length and confirmation prompts on a qualitative measure of user satisfaction. 4.1 ASR and concept accuracy We evaluated overall word, sentence, and con-cept accuracy for all 8,228 spoken utterances to the system, shown in the first row of Table 1. Accuracy: Word  Sentence  Concept  All  80% 78.2% 90.3% First  81.2% 78% 88.9% Last 88.5% 85.4% 95.6% Table 1: System Performance An input is ?concept accurate? if the semantic value assigned by the system exactly matches that assigned by the annotator. First shows the performance on the first response made by a user to each question before any confirmation dialog. Last shows performance on the last time each question was asked. Concept accuracy on last responses is 95.6%, showing that the confirma-
AT&T$WATSONSM$SPEECH$PLATFORM$SURVEY$CASE$$MANAGEMENT$$SYSTEM$$(ISR,$PARSONS)$
ASR$NLU$
DIALOG$MANAGER$
GRAMMARS$ PROMPTS$
AUDIO/TTS$
SURVEY$$ITEMS$SPEC$(XML)$
CASE$DB$
SURVEY$RESPONSE$DB$
VOICE$GATEWAY$(ASTERISK)$
h9p$$request:$iniDate$voice$survey$ Request$handler$
calls$$respondent$
Telephony$$Gateway$
bridges$to$$Watson$
?? Manages$cases$?? Collects$survey$results$
survey$results$returned$to$case$management$as$quesDons$completed$
LOGS$
331
tion strategy resulted in a 60% relative reduction in error compared to the first response. 4.2 Impact of Errors on Survey Estimates Recognition error is undoubtedly a key factor in overall user experience. But unlike dialog sys-tems for information access, search, and transac-tions, the most important factor in a survey dia-log system is the impact of errors on the quality of the estimates derived from the survey. To ex-amine the impact of the residual 4.4% concept error on overall survey error, we compared an-swer distributions derived from the system hy-pothesis for the last response versus the annota-tion of the last response using paired t-tests. For the 18 categorical questions, we conducted t-tests comparing the counts for each response option of each question. For all 18 questions (a total of 77 response categories) none of the dif-ferences were statistically significant (p<0.05). For the 14 numerical questions, for only one (?Number of times shopping in a grocery store in the last month?) did the interpretations differ significantly (Annotated: 7.8 times, Hypothesis: 7.6 times, p=0.04).1 This is strong evidence that speech recognition errors in this system did not have a major effect on survey estimates. How much survey error would have occurred without the dialog strategy?  To test this, we compared the annotated last response to the sys-tem hypothesis for the first response, simulating an interaction without confirmation dialog, and thus lower recognition accuracy?see Table 1 (This is not a perfect simulation, as we have no independent evidence on whether the first or fi-nal response is true). There would indeed have been more survey error without dialog, although the overall level was still surprisingly low. For the 18 categorical questions, 14 of the 77 re-sponse categories show significant differences (p<0.05). For the 14 numerical questions, two showed significant differences.  4.3 User Satisfaction One of the post-interview questionnaire items provided a qualitative measure of user satisfac-tion: ?Overall, how satisfied were you with the interview?? The results were: Very satisfied (47.3%), Somewhat satisfied (41.8%), Somewhat dissatisfied (7.1%), and Very dissatisfied (0.6%).  We examined the impact of various dialog fea-tures that seemed on intuitive grounds plausibly                                                 1 If we treat the two interpretations as independent samples, the response distributions did not differ significantly at all. 
connected with satisfaction: average number of turns per question, average number of clarifica-tion prompts per session, and average number of no input response prompts. We conducted a se-ries of logistic regressions with one variable con-trolled at a time to see the extent to which each of these features affected satisfaction. A Chi-squared test was used to measure significance. All three features were significant predictors when comparing Somewhat/Very Dissatisfied to Very/Somewhat satisfied (Table 2). Feature Odds ratio  SE p # turns per Q 10.411 0.787 0.003 # clarifications  1.043 0.033 0.024 # no input  2.001 0.176 <0.001 Table 2: User satisfaction regression 5 Conclusion Our results demonstrate the viability of conduct-ing survey interviews of the sort from which im-portant national statistics are derived with spoken dialog systems.  In our system, the speech recog-nition errors (with an overall concept recognition rate of 95.6%) did not substantially affect the error of the survey estimates; for only one of 32 questions was there a significant difference in the survey estimate determined by the automated spoken dialog system compared to the annotated result. Of course, we don?t know whether these results generalize to dialog systems with other features, different questions, or different re-spondents; much remains to be learned.  Nonetheless, our results provide some guid-ance for improving respondent satisfaction and minimizing survey error in future development of survey dialog systems.  For example, for nu-merical questions, which generally involve larger numbers of response options, recognition errors may be reduced by adopting the strategy of ask-ing the respondent to select among categories representing ranges (e.g. ?none?, ?1 to 5 times?, ?6 to 10 times?).  Recognition performance could be improved by tuning confirmation strategies, e.g. applying a tighter confidence threshold for numerical vs. categorical questions. In a broad scale application of a repeated spoken dialog survey, greater amounts of data could be availa-ble for training statistical models for the respons-es, for improved recognition accuracy and fur-ther reduced concept error. Finally, it is worth exploring the trade-offs for survey error and re-spondent satisfaction between adding potentially frustrating confirmation dialog and accepting lower-confidence recognition for subsequent human annotation and processing.  
332
Acknowledgments: NSF #SES-1025645 and SES-1026225 to Conrad and Schober. References  Jonathan Bloom. 2008. The Speech IVR as a Survey Interviewing Methodology. In Conrad and Schober (eds.), Envisioning the Survey Interview of the Fu-ture. Wiley, New York. Dan Bohus and Alex Rudnicky. 2005. Sorry, I didn?t Catch That: An Investigation of Non-Understanding Errors and Recovery Strategies. Proceedings of SIGdial-2005, Lisbon, Portugal. Frederick G. Conrad, Michael F. Schober, Chan Zhang, Huiying Yan, Lucas Vickers, Michael Johnston, Andrew L. Hupp, Lloyd Hemingway, Stefanie Fail, Patrick Ehlen, and Chris Antoun. 2013. Mode Choice on an iPhone Increases Survey Data Quality. 68th Annual Conference of the American Association for Public Opinion Research (AAPOR), Boston, MA. Mick P. Couper, 2009. The Role of Paradata in Meas-uring and Reducing Measurement Error in Surveys. NCRM Network for Methodological Innovation 2009: The Use of Paradata in UK Social Surveys. John Ehlen and Patrick Ehlen. 2007. Cellular-Only Substitution in the United States as Lifestyle Adop-tion. Public Opinion Quarterly: Special Issue Vol 71 (5), pp. 717-733.  Patrick Ehlen, Michael Schober, and Frederick G. Conrad. 2007. Modeling Speech Disfluency to Predict Conceptual Misalignment in Speech Sur-vey Interfaces. Discourse Processes 44:3, pp. 245?265. Floyd J. Fowler and Thomas W. Mangione 1990. Standardized Survey Interviewing; Minimizing In-terviewer Related Error. Sage Publications, CA. Marek Fuchs, 2008. Mobile Web Surveys: A Prelimi-nary Discussion of Methodological Implications. In Conrad and Schober (eds.), Envisioning the Survey Interview of the Future. Wiley, New York. Andrew Hunt and Scott McGlashan. 2004. Speech Recognition Grammar Specification Version 1.0. W3C Recommendation 16 March 2004. http://www.w3.org/TR/speech-grammar/. Diane Litman, Julia Hirschberg, and M. G. J. Swerts. 2006. Characterizing and Predicting Corrections in Spoken Dialogue Systems. Computational Linguis-tics 32:3, pp. 417-438. Michael F. Schober and Frederick G. Conrad. 1997. Does conversational interviewing reduce survey measurement error? Public Opinion Quar-terly, 61, pp. 576-602.  Michael F. Schober, Frederick G. Conrad, Chris An-toun, Carroll, Patrick Ehlen, Stefanie Fail, Andrew 
L. Hupp, Michael Johnston, Courtney Kellner, Kelly Nichols, Leif Percifield, Lucas Vickers, Hui-ying Yan, and Chan Zhang. 2012. Disclosure and Quality of Answers in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Opinion Research (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd. 2002. Analysis of User Behavior under Error Conditions in Spo-ken Dialogs. Proceedings of ICSLP-2002. Amanda Stent, Svetlana Stenchikova, and Matthew Marge. 2006. Dialog systems for surveys: The Rate-a-Course system. Proceedings of the IEEE/ACL 2006 Workshop on Spoken Language Technology. pp. 210-213. Appendix ? Sample Survey Dialogs I: Have you smoked at least 100 cigarettes in your entire life? Yes or no. R: No I: Thanks  I: Now I'd like to ask you some questions specific to your diet. For the next group of questions, I'll ask you how much you favor or oppose engaging in certain behaviors. After I say the behavior, please say one of the following: strongly favor, somewhat favor, neither favor nor oppose, somewhat oppose or strongly op-pose. The first behavior...avoiding fast food. R: Strongly oppose I: You said strongly oppose, right? R: Yes I: Got it I: The next behavior: Maintaining a healthy diet R: Repeat I: The next behavior: Maintaining a healthy diet R: Strongly agree I: Sorry, I didn't hear you. When thinking about 'maintaining a healthy diet', please say one of the fol-lowing: strongly favor, somewhat favor, neither favor nor oppose, somewhat oppose or strongly oppose.  R: Somewhat favor I: You said somewhat favor, right?  R: Yes  I: Thanks ? I: Thinking about the time since your eighteenth birthday (including the recent past that you've already told us about), how many male partners have you had sex with?  R: None  (First hypothesis: ?Nine?) I: I think you said '9'. Is that right? Yes or No.  R: No  I: Thinking about the time since your eighteenth birthday (including the recent past that you've already told us about), how many male partners have you had sex with?  R: Zero (Last hypothesis: ?Zero?) I: Thanks 
333
Proceedings of the SIGDIAL 2014 Conference, pages 257?259,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
MVA: The Multimodal Virtual Assistant
Michael Johnston
1
, John Chen
1
, Patrick Ehlen
2
, Hyuckchul Jung
1
, Jay Lieske
2
, Aarthi Reddy
1
,
Ethan Selfridge
1
, Svetlana Stoyanchev
1
, Brant Vasilieff
2
, Jay Wilpon
1
AT&T Labs Research
1
, AT&T
2
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
Abstract
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
1 Introduction
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as ?Jazz
concerts around San Francisco next Saturday?. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
Figure 1: MVA User Interface
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, ?Movies playing
tonight in this area? while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
?Restaurants? while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
257
2 Sample Interaction
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user?s constraints. The user
then reviews the results, and follows on with a
refinement: ?What about blues??. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as ?Blues
concerts near San Francisco tomorrow?. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
U: ?Jazz concerts near San Francisco tomorrow.?
S: ?Where did you want to see jazz tomorrow??
U: ?San Francisco.?
S: ?I found 20 jazz concerts in San
Francisco tomorrow.?
[Zooms map to San Francisco and displays
pins on map and list of results]
U: ?What about blues??
S: ?I found 20 blues concerts in
San Francisco tomorrow.?
U: [Clicks on a concert listing and adds it
to the plan]
U: ?Sushi restaurants near there.?
S: ?I found 10 sushi restaurants.?
U: ?What about here??
[Circles adjoining area on map]
S: ?I found 5 sushi restaurants in
the area you indicated?
Figure 2: Sample Interaction
3 System Architecture
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
Figure 3: MVA Multimodal assistant Architecture
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&T
Watson
SM
speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLUmodules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LEDmodule contains two maximum entropy clas-
sifiers that independently predict whether a con-
258
cept is present in the input, and whether a con-
cept?s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action withMVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. ?Where did
you want to see jazz concerts??. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like ?tomorrow
night? or ?next week? onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL?rather than
relying on any time and location handling in the
underlying information APIs?to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
Acknowledgements
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
References
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32?39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303?304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101?
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033?1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25?30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137?141.
259
