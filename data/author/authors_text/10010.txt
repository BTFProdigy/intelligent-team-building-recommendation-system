Proceedings of the Third Workshop on Statistical Machine Translation, pages 163?166,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Statistical Transfer Systems for French?English
and German?English Machine Translation
Greg Hanneman and Edmund Huber and Abhaya Agarwal and Vamshi Ambati
and Alok Parlikar and Erik Peterson and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, ehuber, abhayaa, vamshi, aup, eepeter, alavie}@cs.cmu.edu
Abstract
We apply the Stat-XFER statistical transfer
machine translation framework to the task of
translating from French and German into En-
glish. We introduce statistical methods within
our framework that allow for the principled
extraction of syntax-based transfer rules from
parallel corpora given word alignments and
constituency parses. Performance is evaluated
on test sets from the 2007 WMT shared task.
1 Introduction
The Carnegie Mellon University statistical trans-
fer (Stat-XFER) framework is a general search-
based and syntax-driven framework for develop-
ing MT systems under a variety of data condi-
tions (Lavie, 2008). At its core is a transfer en-
gine using two language-pair-dependent resources:
a grammar of weighted synchronous context-free
rules (possibly augmented with unification-style fea-
ture constraints), and a probabilistic bilingual lexi-
con of syntax-based word- and phrase-level transla-
tions. The Stat-XFER framework has been used to
develop research MT systems for a number of lan-
guage pairs, including Chinese?English, Hebrew?
English, Urdu?English, and Hindi?English.
In this paper, we describe our use of the frame-
work to create new French?English and German?
English MT systems for the 2008 Workshop on Sta-
tistical Machine Translation shared translation task.
We first describe the acquisition and processing of
resources for each language pair and the roles of
those resources within the Stat-XFER system (Sec-
tion 2); we then report results on common test sets
(Section 3) and share some early analysis and future
directions (Section 4).
2 System Description
Building a new machine translation system under
the Stat-XFER framework involves constructing a
bilingual translation lexicon and a transfer gram-
mar. Over the past six months, we have developed
new methods for extracting syntax-based translation
lexicons and transfer rules fully automatically from
parsed and word-aligned parallel corpora. These
new methods are described in detail by Lavie et
al. (2008). Below, we detail the statistical meth-
ods by which these resources were extracted for our
French?English and German?English systems.
2.1 Lexicon
The bilingual lexicon is automatically extracted
from automatically parsed and word-aligned paral-
lel corpora. To obtain high-quality statistical word
alignments, we run GIZA++ (Och and Ney, 2003)
in both the source-to-target and target-to-source di-
rections, then combine the resulting alignments with
the Sym2 symmetric alignment heuristic of Ortiz-
Mart??nez et al (2005)1. From this data, we extract a
lexicon of both word-to-word and syntactic phrase-
to-phrase translation equivalents.
The word-level correspondences are extracted di-
rectly from the word alignments: parts of speech for
these lexical entries are obtained from the preter-
1We use Sym2 over more well-known heuristics such as
?grow-diag-final? because Sym2 has been shown to give the
best results for the node-alignment subtask that is part of our
processing chain.
163
ws cs wt ct r
paru V appeared V 0.2054
paru V seemed V 0.1429
paru V found V 0.0893
paru V published V 0.0804
paru V felt V 0.0714
.
.
.
.
.
.
.
.
.
paru V already ADV 0.0089
paru V appear V 0.0089
paru V authoritative ADJ 0.0089
Table 1: Part of the lexical entry distribution for the
French (source) word paru.
minal nodes of parse trees of the source and target
sentences. If parsers are unavailable for either lan-
guage, we have also experimented with determin-
ing parts of speech with independent taggers such
as TreeTagger (Schmid, 1995). Alternatively, parts
of speech may be projected through the word align-
ments from one language to the other if the infor-
mation is available on at least one side. Syntactic
phrase-level correspondences are extracted from the
parallel data by applying the PFA node alignment
algorithm described by Lavie et al (2008). The
yields of the aligned parse tree nodes are extracted
as constituent-level translation equivalents.
Each entry in the lexicon is assigned a rule score,
r, based on its source-side part of speech cs, source-
side text ws, target-side part of speech ct, and target-
side text wt. The score is a maximum-likelihood es-
timate of the distribution of target-language transla-
tion and source- and target-language parts of speech,
given the source word or phrase.
r = p(wt, ct, cs |ws) (1)
? #(wt, ct, ws, cs)#(ws) + 1
(2)
We employ add-one smoothing in the denominator
of Equation 2 to counteract overestimation in the
case that #(ws) is small. Rule scores provide a way
to promote the more likely translation alternatives
while still retaining a high degree of diversity in the
lexicon. Table 1 shows part of the lexical distribu-
tion for the French (source) word paru.
The result of the statistical word alignment pro-
cess and lexical extraction is a bilingual lexicon con-
taining 1,064,755 entries for French?English and
1,111,510 entries for German?English. Sample lex-
ical entries are shown in Figure 1.
2.2 Grammar
Transfer grammars for our earlier statistical transfer
systems were manually created by in-house experts
of the languages involved and were therefore small.
The Stat-XFER framework has since been extended
with procedures for automatic grammar acquisition
from a parallel corpus, given constituency parses for
source or target data or both. Our French and Ger-
man systems used the context-free grammar rule ex-
traction process described by Lavie et al (2008).
For French, we used 300,000 parallel sentences from
the Europarl training data parsed on the English side
with the Stanford parser (Klein and Manning, 2003)
and on the French side with the Xerox XIP parser
(A??t-Mokhtar et al, 2001). For German, we used
300,000 Europarl sentence pairs parsed with the En-
glish and German versions of the Stanford parser2.
The set of rules extracted from the parsed corpora
was filtered down after scoring to improve system
performance and run time. The final French rule set
was comprised of the 1500 most frequently occur-
ring rules. For German, rules that occurred less than
twice were filtered out, leaving a total of 16,469. In
each system, rule scores were again calculated by
Equation 2, with ws and wt representing the full
right-hand sides of the source and target grammar
rules.
A secondary version of our French system used a
word-level lexicon extracted from the intersection,
rather than the symmetricization, of the GIZA++
alignments in each direction; we hypothesize that
this tends to improve precision at the expense of re-
call. The word-level lexicon was supplemented with
syntax-based phrase-level entries obtained from the
PFA node alignment algorithm. The grammar
contained the 700 highest-frequency and the 500
highest-scoring rules extracted from the parallel
parsed corpus. This version had a total lexicon size
of 2,023,531 entries and a total grammar of 1034
rules after duplicates were removed. Figure 2 shows
2Due to a combination of time constraints and paucity of
computational resources, only a portion of the Europarl parallel
corpus was utilized, and none of the supplementary news com-
mentary training data was integrated.
164
)(
{VS,248840}
V::V |: ["paru"] ?> ["appeared"]
  (*score* 0.205357142857143)
)
  (*score* 0.763636363636364)
{NP,2000012}
NP::NP |: ["ein" "Beispiel"] ?> ["an" "example"]
(
Figure 1: Sample lexical entries for French and German.
sample grammar rules automatically learned by the
process described above.
2.3 Transfer Engine
The Stat-XFER transfer engine runs in a two-stage
process, first applying the grammar and lexicon
to an input sentence, then running a decoder over
the resulting lattice of scored translation pieces.
Scores for each translation piece are based on a
log-linear combination of several features: language
model probability, rule scores, source-given-target
and target-given-source lexical probabilities, parse
fragmentation, and length. For more details, see
Lavie (2008). The use of a German transfer gram-
mar an order of magnitude larger than the corre-
sponding French grammar was made possible due to
a recent optimization made in the engine. When en-
abled, it constrains the search of translation hypothe-
ses to the space of hypotheses whose structure satis-
fies the consituent structure of a source-side parse.
3 Evaluation
We trained our model parameters on a subset of
the provided ?dev2006? development set, optimiz-
ing for case-insensitive IBM-style BLEU (Papineni
et al, 2002) with several iterations of minimum error
rate training on n-best lists. In each iteration?s list,
we also included the lists from previous iterations in
order to maintain a diversity of hypothesis types and
scores. The provided ?test2007? and ?nc-test2007?
data sets, identical with the test data from the 2007
Workshop on Statistical Machine Translation shared
task, were used as internal development tests.
Tables 2, 3, and 4 report scores on these data sets
for our primary French, secondary French, and Ger-
man systems. We report case-insensitive scores for
version 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-style
BLEU (Papineni et al, 2002), and version 5 of TER
(Snover et al, 2006).
Data Set METEOR BLEU TER
dev2006 0.5332 0.2063 64.81
test2007 0.5358 0.2078 64.75
nc-test2007 0.5369 0.1719 69.83
Table 2: Results for the primary French?English system
on provided development and development test sets.
Data Set METEOR BLEU TER
dev2006 0.5330 0.2086 65.02
test2007 0.5386 0.2129 64.29
nc-test2007 0.5311 0.1680 70.90
Table 3: Results for the secondary French?English sys-
tem on provided development and development test sets.
4 Analysis and Conclusions
From the development test results in Section 3, we
note that the Stat-XFER systems? performance cur-
rently lags behind the state-of-the-art scores on the
2007 test data3. This may be in part due to the low
volume of training data used for rule learning. A key
research question in our approach is how to distin-
guish low-frequency correct and useful transfer rules
from ?noisy? rules that are due to parser errors and
incorrect word alignments. We believe that learning
rules from more data will help alleviate this prob-
lem by proportionally increasing the counts of good
rules compared to incorrect ones. We also plan to
study methods for more effective rule set pruning,
regardless of the volume of training data used.
The difference in metric scores between in-
domain and out-of-domain data is partly due to ef-
fects of reference length on the metrics used. De-
tailed output from METEOR and BLEU shows that
the reference translations for the test2007 set are
about 94% as long as the primary French?English
3Top scores on the 2007 test data are approximately 0.60
METEOR, 0.33 BLEU, and 57.6 TER. See Callison-Burch et
al. (2007) for full results.
165
(
  (*score* 0.866050808314088
)
{PP,1627955}
PP:PP [PRE "d?" "autres" N] ?> [PRE "other" N]
  (X1::Y1)
  (X4::Y3)
)
(
{PP,3000085}
PP:ADVP ["vor" CARD "Monaten"] ?> [NUM "months" "ago"]
  (*score* 0.9375)
  (X2::Y1)
)
Figure 2: Sample grammar rules for French and German.
Data Set METEOR BLEU TER
dev2006 0.4967 0.1794 68.68
test2007 0.5052 0.1878 67.94
nc-test2007 0.4939 0.1347 74.38
Table 4: Results for the German?English system on pro-
vided development and development test sets.
system?s translations. On this set, our system has
approximately balanced precision (0.62) and recall
(0.66). However, the nc-test2007 references are only
84% as long as our output, a situation that hurts our
system?s precision (0.57) but boosts its recall (0.68).
METEOR, as a metric that favors recall, shows a
negligible increase in score between these two test
sets, while BLEU and TER report significant relative
drops of 17.3% and 7.8%. This behavior appears to
be consistent on the test2007 and nc-test2007 data
sets across systems (Callison-Burch et al, 2007).
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank the members of the Parsing and Semantics
group at Xerox Research Centre Europe for assisting
in parsing the French data using their XIP parser.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, China, October.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed paral-
lel corpora. In Proceedings of the Second Work-
shop on Syntax and Structure in Statistical Transla-
tion, Columbus, OH, June. To appear.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: A toolkit to train
phrase-based models for statistical machine transla-
tion. In Proceedings of the 10th Machine Translation
Summit, pages 141?148, Phuket, Thailand, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
166
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 56?60,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Machine Translation System Combination with Flexible Word Ordering
Kenneth Heafield, Greg Hanneman, Alon Lavie
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
{kheafiel,ghannema,alavie}@cs.cmu.edu
Abstract
We describe a synthetic method for com-
bining machine translations produced by
different systems given the same input.
One-best outputs are explicitly aligned
to remove duplicate words. Hypotheses
follow system outputs in sentence order,
switching between systems mid-sentence
to produce a combined output. Experi-
ments with the WMT 2009 tuning data
showed improvement of 2 BLEU and 1
METEOR point over the best Hungarian-
English system. Constrained to data pro-
vided by the contest, our system was sub-
mitted to the WMT 2009 shared system
combination task.
1 Introduction
Many systems for machine translation, with dif-
ferent underlying approaches, are of competitive
quality. Nonetheless these approaches and sys-
tems have different strengths and weaknesses. By
offsetting weaknesses with strengths of other sys-
tems, combination can produce higher quality than
does any component system.
One approach to system combination uses con-
fusion networks (Rosti et al, 2008; Karakos et
al., 2008). In the most common form, a skele-
ton sentence is chosen from among the one-best
system outputs. This skeleton determines the or-
dering of the final combined sentence. The re-
maining outputs are aligned with the skeleton, pro-
ducing a list of alternatives for each word in the
skeleton, which comprises a confusion network. A
decoder chooses from the original skeleton word
and its alternatives to produce a final output sen-
tence. While there are a number of variations on
this theme, our approach differs fundamentally in
that the effective skeleton changes on a per-phrase
basis.
Our system is an enhancement of our previous
work (Jayaraman and Lavie, 2005). A hypothesis
uses words from systems in order, switching be-
tween systems at phrase boundaries. Alignments
and a synchronization method merge meaning-
equivalent output from different systems. Hy-
potheses are scored based on system confidence,
alignment support, and a language model.
We contribute a few enhancements to this pro-
cess. First, we introduce an alignment-sensitive
method for synchronizing available hypothesis ex-
tensions across systems. Second, we pack similar
partial hypotheses, which allows greater diversity
in our beam search while maintaining the accuracy
of n-best output. Finally, we describe an improved
model selection process that determined our sub-
missions to the WMT 2009 shared system combi-
nation task.
The remainder of this paper is organized as fol-
lows. Section 2 describes the system with empha-
sis on our modifications. Tuning, our experimen-
tal setup, and submitted systems are described in
Section 3. Section 4 concludes.
2 System
The system consists of alignment (Section 2.1)
and phrase detection (Section 2.2) followed by de-
coding. The decoder constructs hypothesis sen-
tences one word at a time, starting from the left. A
partially constructed hypothesis comprises:
Word The most recently decoded word. Initially,
this is the beginning of sentence marker.
Used The set of used words from each system.
Initially empty.
Phrase The current phrase constraint from Sec-
tion 2.2, if any. The initial hypothesis is not
in a phrase.
Features Four feature values defined in Section
2.4 and used in Section 2.5 for beam search
56
and hypothesis ranking. Initially, all features
are 1.
Previous A set of preceding hypothesis pointers
described in Section 2.5. Initially empty.
The leftmost unused word from each system
corresponds to a continuation of the partial hy-
pothesis. Therefore, for each system, we extend a
partial hypothesis by appending that system?s left-
most unused word, yielding several new hypothe-
ses. The appended word, and those aligned with it,
are marked as used in the new hypothesis. Since
systems do not align perfectly, too few words may
be marked as used, a problem addressed in Sec-
tion 2.3. As described in Section 2.4, hypotheses
are scored using four features based on alignment,
system confidence, and a language model. Since
the search space is quite large, we use these partial
scores for a beam search, where the beam contains
hypotheses of equal length. This space contains
hypotheses that extend in precisely the same way,
which we exploit in Section 2.5 to increase diver-
sity. Finally, a hypothesis is complete when the
end of sentence marker is appended.
2.1 Alignment
Sentences from different systems are aligned in
pairs using a modified version of the METEOR
(Banerjee and Lavie, 2005) matcher. This iden-
tifies alignments in three phases: exact matches
up to case, WordNet (Fellbaum, 1998) morphol-
ogy matches, and shared WordNet synsets. These
sources of alignments are quite precise and unable
to pick up on looser matches such as ?mentioned?
and ?said? that legitimately appear in output from
different systems. Artificial alignments are in-
tended to fill gaps by using surrounding align-
ments as clues. If a word is not aligned to any
word in some other sentence, we search left and
right for words that are aligned into that sentence.
If these alignments are sufficiently close to each
other in the other sentence, words between them
are considered for artificial alignment. An arti-
ficial alignment is added if a matching part of
speech is found. The algorithm is described fully
by Jayaraman and Lavie (2005).
2.2 Phrases
Switching between systems is permitted outside
phrases or at phrase boundaries. We find phrases
in two ways. Alignment phrases are maximally
long sequences of words which align, in the same
order and without interruption, to a word se-
quence from at least one other system. Punctua-
tion phrases place punctuation in a phrase with the
preceding word, if any. When the decoder extends
a hypothesis, it considers the longest phrase in
which no word is used. If a punctuation phrase is
partially used, the decoder marks the entire phrase
as used to avoid extraneous punctuation.
2.3 Synchronization
While phrases address near-equal pieces of trans-
lation output, we must also deal with equally
meaningful output that does not align. The im-
mediate effect of this issue is that too few words
are marked as used by the decoder, leading to du-
plication in the combined output. In addition, par-
tially aligned system output results in lingering un-
used words between used words. Often these are
function words that, with language model scoring,
make output unnecessarily verbose. To deal with
this problem, we expire lingering words by mark-
ing them as used. Specifically, we consider the
frontier of each system, which is the leftmost un-
used word. If a frontier lags behind, words as used
to advance the frontier. Our two methods for syn-
chronization differ in how frontiers are compared
across systems and the tightness of the constraint.
Previously, we measured frontiers from the be-
ginning of sentence. Based on this measurement,
the synchronization constraint requires that the
frontiers of each system differ by at most s. Equiv-
alently, a frontier is lagging if it is more than s
words behind the rightmost frontier. Lagging fron-
tiers are advanced until the synchronization con-
straint becomes satisfied. We found this method
can cause problems in the presence of variable
length output. When the variability in output
length exceeds s, proper synchronization requires
distances between frontiers greater than s, which
this constraint disallows.
Alignments indicate where words are syn-
chronous. Words near an alignment are also likely
to be synchronous even without an explicit align-
ment. For example, in the fragments ?even more
serious, you? and ?even worse, you? from WMT
2008, ?serious? and ?worse? do not align but
do share relative position from other alignments,
suggesting these are synchronous. We formalize
this by measuring the relative position of fron-
tiers from alignments on each side. For example,
57
if the frontier itself is aligned then relative posi-
tion is zero. For each pair of systems, we check
if these relative positions differ by at most s un-
der an alignment on either side. Confidence in a
system?s frontier is the sum of the system?s own
confidence plus confidence in systems for which
the pair-wise constraint is satisfied. If confidence
in any frontier falls below 0.8, the least confident
lagging frontier is advanced. The process repeats
until the constraint becomes satisfied.
2.4 Scores
We score partial and complete hypotheses using
system confidence, alignments, and a language
model. Specifically, we have four features which
operate at the word level:
Alignment Confidence in the system from which
the word came plus confidence in systems to
which the word aligns.
Language Model Score from a suffix array lan-
guage model (Zhang and Vogel, 2006)
trained on English from monolingual and
French-English data provided by the contest.
N -Gram
(
1
3
)order?ngram
using language model
order and length of ngram found.
Overlap overlaporder?1 where overlap is the length of
intersection between the preceding and cur-
rent n-grams.
The N -Gram and Overlap features are intended to
improve fluency across phrase boundaries. Fea-
tures are combined using a log-linear model
trained as discussed in Section 3. Hypotheses are
scored using the geometric average score of each
word in the hypothesis.
2.5 Search
Of note is that a word?s score is impacted only by
its alignments and the n-gram found by the lan-
guage model. Therefore two partial hypotheses
that differ only in words preceding the n-gram and
in their average score are in some sense duplicates.
With the same set of used words and same phrase
constraint, they extend in precisely the same way.
In particular, the highest scoring hypothesis will
never use a lower scoring duplicate.
We use duplicate detecting beam search to ex-
plore our hypothesis space. A beam contains par-
tial hypotheses of the same length. Duplicate
hypotheses are detected on insertion and packed,
with the combined hypothesis given the highest
score of those packed. Once a beam contains the
top scoring partial hypotheses of length l, these
hypotheses are extended to length l+1 and placed
in another beam. Those hypotheses reaching end
of sentence are placed in a separate beam, which is
equivalent to packing them into one final hypoth-
esis. Once we remove partial hypothesis that did
not extend to the final hypothesis, the hypotheses
are a lattice connected by parent pointers.
While we submitted only one-best hypotheses,
accurate n-best hypotheses are important for train-
ing as explained in Section 3. Unpacking the hy-
pothesis lattice into n-best hypotheses is guided
by scores stored in each hypothesis. For this task,
we use an n-best beam of paths from the end of
sentence hypothesis to a partial hypothesis. Paths
are built by induction, starting with a zero-length
path from the end of sentence hypothesis to itself.
The top scoring path is removed and its terminal
hypothesis is examined. If it is the beginning of
sentence, the path is output as a complete hypoth-
esis. Otherwise, we extend the path to each parent
hypothesis, adjusting each path score as necessary,
and insert into the beam. This process terminates
with n complete hypotheses or an empty beam.
3 Tuning
Given the 502 sentences made available for tun-
ing by WMT 2009, we selected feature weights for
scoring, a set of systems to combine, confidence in
each selected system, and the type and distance s
of synchronization. Of these, only feature weights
can be trained, for which we used minimum error
rate training with version 1.04 of IBM-style BLEU
(Papineni et al, 2002) in case-insensitive mode.
We treated the remaining parameters as a model
selection problem, using 402 randomly sampled
sentences for training and 100 sentences for eval-
uation. This is clearly a small sample on which
to evaluate, so we performed two folds of cross-
validation to obtain average scores over 200 un-
trained sentences. We chose to do only two folds
due to limited computational time and a desire to
test many models.
We scored systems and our own output using
case-insensitive IBM-style BLEU 1.04 (Papineni
et al, 2002), METEOR 0.6 (Lavie and Agarwal,
2007) with all modules, and TER 5 (Snover et
al., 2006). For each source language, we ex-
58
In Sync s BLEU METE TER Systems and Confidences
cz length 8 .236 .507 59.1 google .46 cu-bojar .27 uedin .27
cz align 5 .226 .499 57.8 google .50 cu-bojar .25 uedin .25
cz align 7 .211 .508 65.9 cu-bojar .60 google .20 uedin .20
cz .231 .504 57.8 google
de length 7 .255 .531 54.2 google .40 uka .30 stuttgart .15 umd .15
de length 6 .260 .532 55.2 google .50 systran .25 umd .25
de align 9 .256 .533 55.5 google .40 uka .30 stuttgart .15 umd .15
de align 6 .200 .514 54.2 google .31 uedin .22 systran .18 umd .16 uka .14
de .244 .523 57.5 google
es align 8 .297 .560 52.7 google .75 uedin .25
es length 5 .289 .548 52.1 google .50 talp-upc .17 uedin .17 rwth .17
es .297 .558 52.7 google
fr align 6 .329 .574 49.9 google .70 lium1 .30
fr align 8 .314 .596 48.6 google .50 lium1 .30 limsi1 .20
fr length 8 .323 .570 48.5 google .50 lium1 .25 limsi1 .25
fr .324 .576 48.7 google
hu length 5 .162 .403 69.2 umd .50 morpho .40 uedin .10
hu length 8 .158 .407 69.5 umd .50 morpho .40 uedin .10
hu align 7 .153 .392 68.0 umd .33 morpho .33 uedin .33
hu .141 .391 66.1 umd
xx length 5 .326 .584 49.6 google-fr .61 google-es .39
xx align 4 .328 .580 49.5 google-fr .80 google-es .20
xx align 5 .324 .576 48.6 google-fr .61 google-es .39
xx align 7 .319 .587 51.1 google-fr .50 google-es .50
xx .324 .576 48.7 google-fr
Table 1: Combination models used for submission to WMT 2009. For each language, we list our pri-
mary combination, contrastive combinations, and a high-scoring system for comparison in italic. All
translations are into English. The xx source language combines translations from different languages,
in our case French and Spanish. Scores from BLEU, METEOR, and TER are the average of two cross-
validation folds with 100 evaluation sentences each. Numbers following system names indicate con-
trastive systems. More evaluation, including human scores, will be published by WMT.
perimented with various sets of high-scoring sys-
tems to combine. We also tried confidence val-
ues proportional to various powers of BLEU and
METEOR scores, as well as hand-picked values.
Finally we tried both variants of synchronization
with values of s ranging from 2 to 9. In total, 405
distinct models were evaluated. For each source
source language, our primary system was chosen
by performing well on all three metrics. Models
that scored well on individual metrics were sub-
mitted as contrastive systems. In Table 1 we report
the models underlying each submitted system.
4 Conclusion
We found our combinations are quite sensitive to
presence of and confidence in the underlying sys-
tems. Further, we show the most improvement
when these systems are close in quality, as is the
case with our Hungarian-English system. The
two methods of synchronization were surprisingly
competitive, a factor we attribute to short sentence
length compared with WMT 2008 Europarl sen-
tences. Opportunities for further work include per-
sentence system confidence, automatic training of
more parameters, and different alignment models.
We look forward to evaluation results from WMT
2009.
Acknowledgments
The authors wish to thank Jonathan Clark for
training the language model and other assistance.
This work was supported in part by the DARPA
GALE program and by a NSF Graduate Research
Fellowship.
59
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Proc.
ACLWorkshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65?72.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Shyamsundar Jayaraman and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proc. EAMT, pages 143?152.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL-08: HLT, Short Papers (Companion Vol-
ume), pages 81?84.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An automatic metric for MT evaluation with
high levels of correlation with human judgments.
In Proc. Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proc. 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, PA, July.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proc. Seventh Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Language Technologies Institute, School of Com-
puter Science, Carnegie Mellon University, Pitts-
burgh, PA, Dec.
60
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 140?144,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
An Improved Statistical Transfer System for French?English
Machine Translation
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark, Alok Parlikar, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,vamshi,jhclark,aup,alavie}@cs.cmu.edu
Abstract
This paper presents the Carnegie Mellon
University statistical transfer MT system
submitted to the 2009 WMT shared task
in French-to-English translation. We de-
scribe a syntax-based approach that incor-
porates both syntactic and non-syntactic
phrase pairs in addition to a syntactic
grammar. After reporting development
test results, we conduct a preliminary anal-
ysis of the coverage and effectiveness of
the system?s components.
1 Introduction
The statistical transfer machine translation group
at Carnegie Mellon University has been devel-
oping a hybrid approach combining a traditional
rule-based MT system and its linguistically ex-
pressive formalism with more modern techniques
of statistical data processing and search-based de-
coding. The Stat-XFER framework (Lavie, 2008)
provides a general environment for building new
MT systems of this kind. For a given language
pair or data condition, the framework depends on
two main resources extracted from parallel data: a
probabilistic bilingual lexicon, and a grammar of
probabilistic synchronous context-free grammar
rules. Additional monolingual data, in the form of
an n-gram language model in the target language,
is also used. The statistical transfer framework op-
erates in two stages. First, the lexicon and gram-
mar are applied to synchronously parse and trans-
late an input sentence; all reordering is applied
during this stage, driven by the syntactic grammar.
Second, a monotonic decoder runs over the lat-
tice of scored translation pieces produced during
parsing and assembles the highest-scoring overall
translation according to a log-linear feature model.
Since our submission to last year?s Workshop
on Machine Translation shared translation task
(Hanneman et al, 2008), we have made numerous
improvements and extensions to our resource ex-
traction and processing methods, resulting in sig-
nificantly improved translation scores. In Section
2 of this paper, we trace our current methods for
data resource management for the Stat-XFER sub-
mission to the 2009 WMT shared French?English
translation task. Section 3 explains our tuning pro-
cedure, and Section 4 gives our experimental re-
sults on various development sets and offers some
preliminary analysis.
2 System Construction
Because of the additional data resources provided
for the 2009 French?English task, our system this
year is trained on nearly eight times as much
data as last year?s. We used three officially pro-
vided data sets to make up a parallel corpus for
system training: version 4 of the Europarl cor-
pus (1.43 million sentence pairs), the News Com-
mentary corpus (0.06 million sentence pairs), and
the pre-release version of the new Giga-FrEn cor-
pus (8.60 million sentence pairs)1. The combined
corpus of 10.09 million sentence pairs was pre-
processed to remove blank lines, sentences of 80
words or more, and sentence pairs where the ra-
tio between the number of English and French
words was larger than 5 to 1 in either direction.
These steps removed approximately 3% of the cor-
pus. Given the filtered corpus, our data prepara-
tion pipeline proceeded according to the descrip-
tions below.
1Because of data processing time, we were unable to use
the larger verions 1 or 2 of Giga-FrEn released later in the
evaluation period.
140
2.1 Parsing and Word Alignment
We parsed both sides of our parallel corpus with
independent automatic constituency parsers. We
used the Berkeley parser (Petrov and Klein, 2007)
for both English and French, although we obtained
better results for French by tokenizing the data
with our own script as a preprocessing step and
not allowing the parser to change it. There were
approximately 220,000 English sentences that did
not return a parse, which further reduced the size
of our training corpus by 2%.
After parsing, we re-extracted the leaf nodes
of the parse trees and statistically word-aligned
the corpus using a multi-threaded implementa-
tion (Gao and Vogel, 2008) of the GIZA++ pro-
gram (Och and Ney, 2003). Unidirectional align-
ments were symmetrized with the ?grow-diag-
final? heuristic (Koehn et al, 2005).
2.2 Phrase Extraction and Combination
Phrase extraction for last year?s statistical transfer
system used automatically generated parse trees
on both sides of the corpus as absolute constraints:
a syntactic phrase pair was extracted from a given
sentence only when a contiguous sequence of En-
glish words exactly made up a syntactic con-
stituent in the English parse tree and could also
be traced though symmetric word alignments to a
constituent in the French parse tree. While this
?tree-to-tree? extraction method is precise, it suf-
fers from low recall and results in a low-coverage
syntactic phrase table. Our 2009 system uses an
extended ?tree-to-tree-string? extraction process
(Ambati and Lavie, 2008) in which, if no suit-
able equivalent is found in the French parse tree
for an English node, a copy of the English node is
projected into the French tree, where it spans the
French words aligned to the yield of the English
node. This method can result in a 50% increase
in the number of extracted syntactic phrase pairs.
Each extracted phrase pair retains a syntactic cat-
egory label; in our current system, the node label
in the English parse tree is used as the category for
both sides of the bilingual phrase pair, although we
subsequently map the full set of labels used by the
Berkeley parser down to a more general set of 19
syntactic categories.
We also ran ?standard? phrase extraction on the
same corpus using Steps 4 and 5 of the Moses sta-
tistical machine translation training script (Koehn
et al, 2007). The two types of phrases were then
merged in a syntax-prioritized combination that
removes all Moses-extracted phrase pairs that have
source sides already covered by the tree-to-tree-
string syntactic phrase extraction. The syntax pri-
oritization has the advantage of still including a se-
lection of non-syntactic phrases while producing a
much smaller phrase table than a direct combina-
tion of all phrase pairs of both types. Previous ex-
periments we conducted indicated that this comes
with only a minor drop in automatic metric scores.
In our current submission, we modify the proce-
dure slightly by removing singleton phrase pairs
from the syntactic table before the combination
with Moses phrases. The coverage of the com-
bined table is not affected ? our syntactic phrase
extraction algorithm produces a subset of the non-
syntactic phrase pairs extracted from Moses, up to
phrase length constraints ? but the removal al-
lows Moses-extracted versions of some phrases to
survive syntax prioritization. In effect, we are lim-
iting the set of category-labeled syntactic transla-
tions we trust to those that have been seen more
than once in our training data. For a given syn-
tactic phrase pair, we also remove all but the most
frequent syntactic category label for the pair; this
removes a small number of entries from our lexi-
con in order to limit label ambiguity, but does not
affect coverage.
From our training data, we extracted 27.6 mil-
lion unique syntactic phrase pairs after single-
ton removal, reducing this set to 27.0 million en-
tries after filtering for category label ambiguity.
Some 488.7 million unique phrase pairs extracted
from Moses were reduced to 424.0 million after
syntax prioritization. (The remaining 64.7 mil-
lion phrase pairs had source sides already covered
by the 27.0 million syntactically extracted phrase
pairs, so they were thrown out.) This means non-
syntactic phrases outnumber syntactic phrases by
nearly 16 to 1. However, when filtering the phrase
table to a particular development or test set, we
find the syntactic phrases play a larger role, as this
ratio drops to approximately 3 to 1.
Sample phrase pairs from our system are shown
in Figure 1. Each pair includes two rule scores,
which we calculate from the source-side syntac-
tic category (cs), source-side text (ws), target-side
category (ct), and target-side text (wt). In the
case of Moses-extracted phrase pairs, we use the
?dummy? syntactic category PHR. Rule score rt|s
is a maximum likelihood estimate of the distri-
141
cs ct ws wt rt|s rs|t
ADJ ADJ espagnols Spanish 0.8278 0.1141
N N repre?sentants officials 0.0653 0.1919
NP NP repre?sentants de la Commission Commission officials 0.0312 0.0345
PHR PHR haute importance a` very important to 0.0357 0.0008
PHR PHR est charge? de has responsibility for 0.0094 0.0760
Figure 1: Sample lexical entries, including non-syntactic phrases, with rule scores (Equations 1 and 2).
bution of target-language translations and source-
and target-language syntactic categories given the
source string (Equation 1). The rs|t score is simi-
lar, but calculated in the reverse direction to give a
source-given-target probability (Equation 2).
rt|s =
#(wt, ct, ws, cs)
#(ws) + 1
(1)
rs|t =
#(wt, ct, ws, cs)
#(wt) + 1
(2)
Add-one smoothing in the denominators counter-
acts overestimation of the rule scores of lexical en-
tries with very infrequent source or target sides.
2.3 Syntactic Grammar
Syntactic phrase extraction specifies a node-to-
node alignment across parallel parse trees. If these
aligned nodes are used as decomposition points,
a set of synchronous context-free rules that pro-
duced the trees can be collected. This is our pro-
cess of syntactic grammar extraction (Lavie et al,
2008). For our 2009 WMT submission, we ex-
tracted 11.0 million unique grammar rules, 9.1
million of which were singletons, from our paral-
lel parsed corpus. These rules operate on our syn-
tactically extracted phrase pairs, which have cat-
egory labels, but they may also be partially lexi-
calized with explicit source or target word strings.
Each extracted grammar rule is scored according
to Equations 1 and 2, where now the right-hand
sides of the rule are used as ws and wt.
As yet, we have made only minimal use of the
Stat-XFER framework?s grammar capabilities, es-
pecially for large-scale MT systems. For the cur-
rent submission, the syntactic grammar consisted
of 26 manually chosen high-frequency grammar
rules that carry out some reordering between En-
glish and French. Since rules for high-level re-
ordering (near the top of the parse tree) are un-
likely to be useful unless a large amount of parse
structure can first be built, we concentrate our
rules on low-level reorderings taking place within
or around small constituents. Our focus for this
selection is the well-known repositioning of adjec-
tives and adjective phrases when translating from
French to English, such as from le Parlement eu-
rope?en to the European Parliament or from l? in-
tervention forte et substantielle to the strong and
substantial intervention. Our grammar thus con-
sists of 23 rules for building noun phrases, two
rules for building adjective phrases, and one rule
for building verb phrases.
2.4 English Language Model
We built a suffix-array language model (Zhang and
Vogel, 2006) on approximately 700 million words
of monolingual data: the unfiltered English side of
our parallel training corpus, plus the 438 million
words of English monolingual news data provided
for the WMT 2009 shared task. With the relatively
large amount of data available, we made the some-
what unusual decision of building our language
model (and all other data resources for our system)
in mixed case, which adds approximately 12.3%
to our vocabulary size. This saves us the need to
build and run a recaser as a postprocessing step
on our output. Our mixed-case decision may also
be validated by preliminary test set results, which
show that our submission has the smallest drop in
BLEU score (0.0074) between uncased and cased
evaluation of any system in the French?English
translation task.
3 System Tuning
Stat-XFER uses a log-linear combination of seven
features in its scoring of translation fragments:
language model probability, source-given-target
and target-given-source rule probabilities, source-
given-target and target-given-source lexical prob-
abilities, a length score, and a fragmentation score
based on the number of parsed translation frag-
ments that make up the output sentence. We tune
the weights for these features with several rounds
of minimum error rate training, optimizing to-
142
Primary Contrastive
Data Set METEOR BLEU TER METEOR BLEU TER
news-dev2009a-425 0.5437 0.2299 60.45 ? ? ?
news-dev2009a-600 ? ? ? 0.5134 0.2055 63.46
news-dev2009b 0.5263 0.2073 61.96 0.5303 0.2104 61.74
nc-test2007 0.6194 0.3282 51.17 0.6195 0.3226 51.49
Figure 2: Primary and contrastive system results on tuning and development test sets.
wards the BLEU metric. For each tuning itera-
tion, we save the n-best lists output by the sys-
tem from previous iterations and concatenate them
onto the current n-best list in order to present the
optimizer with a larger variety of translation out-
puts and score values.
From the provided ?news-dev2009a? develop-
ment set we create two tuning sets: one using the
first 600 sentences of the data, and a second using
the remaining 425 sentences. We tuned our sys-
tem separately on each set, saving the additional
?news-dev2009b? set as a final development test to
choose our primary and contrastive submissions2.
At run time, our full system takes on average be-
tween four and seven seconds to translate each in-
put sentence, depending on the size of the final
bilingual lexicon.
4 Evaluation and Analysis
Figure 2 shows the results of our primary and con-
trastive systems on four data sets. First, we report
final (tuned) performance on our two tuning sets
? the last 425 sentences of news-dev2009a for the
primary system, and the first 600 sentences of the
same set for the contrastive. We also include our
development test (news-dev2009b) and, for addi-
tional comparison, the ?nc-test2007? news com-
mentary test set from the 2007 WMT shared task.
For each, we give case-insensitive scores on ver-
sion 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-
style BLEU (Papineni et al, 2002), and version 5
of TER (Snover et al, 2006).
From these results, we highlight two interest-
ing areas of analysis. First, the low tuning and
development test set scores bring up questions
about system coverage, given that the news do-
main was not strongly represented in our system?s
2Due to a data processing error, the choice of the primary
submission was based on incorrectly computed scores. In
fact, the contrastive system has better performance on our de-
velopment test set.
training data. We indeed find a significantly larger
proportion of out-of-vocabulary (OOV) words in
news-domain sets: the news-dev2009b set is trans-
lated by our primary submission with 402 of 6263
word types (6.42%) or 601 of 27,821 word tokens
(2.16%) unknown. The same system running on
the 2007 WMT ?test2007? set of Europarl-derived
data records an OOV rate of only 87 of 7514
word types (1.16%) or 105 of 63,741 word tokens
(0.16%).
Second, we turn our attention to the usefulness
of the syntactic grammar. Though small, we find
it to be both beneficial and precise. In the 1026-
sentence news-dev2009b set, for example, we find
351 rule applications ? the vast majority of them
(337) building noun phrases. The three most fre-
quently occurring rules are those for reordering the
sequence [DET N ADJ] to [DET ADJ N] (52 oc-
currences), the sequence [N ADJ] to [ADJ N] (51
occurrences), and the sequence [N1 de N2] to [N2
N1] (45 occurrences). We checked precision by
manually reviewing the 52 rule applications in the
first 150 sentences of news-dev2009b. There, 41
of the occurrences (79%) were judged to be cor-
rect and beneficial to translation output. Of the
remainder, seven were judged incorrect or detri-
mental and four were judged either neutral or of
unclear benefit.
We expect to continue to analyze the output and
effectiveness of our system in the coming months.
In particular, we would like to learn more about
the usefulness of our 26-rule grammar with the
view of using significantly larger grammars in fu-
ture versions of our system.
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank Yahoo! for the use of theM45 research com-
puting cluster, where we ran the parsing stage of
our data processing.
143
References
Vamshi Ambati and Alon Lavie. 2008. Improving
syntax driven translation models by re-structuring
divergent and non-isomorphic parse tree structures.
In Proceedings of the Eighth Conference of the As-
sociation for Machine Translation in the Americas,
pages 235?244, Waikiki, HI, October.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems
for French?English and German?English machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 163?166,
Columbus, OH, June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of IWSLT 2005, Pittsburgh, PA, Oc-
tober.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL 2007 Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Carnegie Mellon University, Pittsburgh, PA, Decem-
ber.
144
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Decoding with Syntactic and Non-Syntactic Phrases in a Syntax-Based
Machine Translation System
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,alavie}@cs.cmu.edu
Abstract
A key concern in building syntax-based ma-
chine translation systems is how to improve
coverage by incorporating more traditional
phrase-based SMT phrase pairs that do not
correspond to syntactic constituents. At the
same time, it is desirable to include as much
syntactic information in the system as pos-
sible in order to carry out linguistically mo-
tivated reordering, for example. We apply
an extended and modified version of the ap-
proach of Tinsley et al (2007), extracting
syntax-based phrase pairs from a large parallel
parsed corpus, combining them with PBSMT
phrases, and performing joint decoding in a
syntax-based MT framework without loss of
translation quality. This effectively addresses
the low coverage of purely syntactic MT with-
out discarding syntactic information. Further,
we show the potential for improved transla-
tion results with the inclusion of a syntactic
grammar. We also introduce a new syntax-
prioritized technique for combining syntactic
and non-syntactic phrases that reduces overall
phrase table size and decoding time by 61%,
with only a minimal drop in automatic trans-
lation metric scores.
1 Introduction
The dominance of traditional phrase-based statisti-
cal machine translation (PBSMT) models (Koehn et
al., 2003) has recently been challenged by the de-
velopment and improvement of a number of new
models that explicity take into account the syntax
of the sentences being translated. One simple ap-
proach is to limit the phrases learned by a standard
PBSMT translation model to only those contiguous
sequences of words that additionally correspond to
constituents in a syntactic parse tree. However, a to-
tal reliance on such syntax-based phrases has been
shown to be detrimental to translation quality, as the
space of phrase segmentation of a parallel sentence
is heavily constrained by both the source-side and
target-side tree structures. Noting that the number
of phrase pairs extracted from a corpus is reduced by
around 80% when they are required to correspond to
syntactic constituents, Koehn et al (2003) observed
that many non-constituent phrase pairs that would
not be included in a syntax-only model are in fact
extremely important to system performance. Since
then, researchers have explored effective ways for
combining phrase pairs derived from syntax-aware
methods with those extracted from more traditional
PBSMT. Briefly stated, the goal is to retain the high
level of coverage provided by non-syntactic PBSMT
phrases while simultaneously incorporating and ex-
ploiting specific syntactic knowledge.
Zollmann and Venugopal (2006) overcome the re-
strictiveness of the syntax-only model by starting
with a complete set of phrases as produced by tra-
ditional PBSMT heuristics, then annotating the tar-
get side of each phrasal entry with the label of the
constituent node in the target-side parse tree that
subsumes the span. They then introduce new con-
stituent labels to handle the cases where the phrasal
entries do not exactly correspond to the syntactic
constituents. Liu et al (2006) also add non-syntactic
PBSMT phrases into their tree-to-string translation
system. Working from the other direction, Marton
and Resnik (2008) extend a hierarchical PBSMT
1
system with a number of features to prefer or dis-
prefer certain types of syntactic phrases in different
contexts. Restructuring the parse trees to ease their
restrictiveness is another recent approach: in partic-
ular, Wang et al (2007) binarize source-side parse
trees in order to provide phrase pair coverage for
phrases that are partially syntactic.
Tinsley et al (2007) showed an improvement over
a PBSMT baseline on four tasks in bidirectional
German?English and Spanish?English translation
by incorporating syntactic phrases derived from par-
allel trees into the PBSMT translation model. They
first word align and extract phrases from a parallel
corpus using the open-source Moses PBSMT toolkit
(Koehn et al, 2007), which provides a baseline SMT
system. Then, both sides of the parallel corpus are
parsed with independent automatic parsers, subtrees
from the resulting parallel treebank are aligned, and
an additional set of phrases (with each phrase corre-
sponding to a syntactic constituent in the parse tree)
is extracted. The authors report statistically signif-
icant improvements in translation quality, as mea-
sured by a variety of automatic metrics, when the
two types of phrases are combined in the Moses de-
coder.
Our approach in this paper is structurally similar
to that of Tinsley et al (2007), but we extend or
modify it in a number of key ways. First, we ex-
tract both non-syntactic PBSMT and syntax-driven
phrases from a parallel corpus that is two orders of
magnitude larger, making our system competitive
in size to state-of-the-art SMT systems elsewhere.
Second, we apply a different algorithm for subtree
alignment, proposed by Lavie et al (2008), which
proceeds bottom-up from existing statistical word
alignments, rather than inducing them top-down
from lexical alignment probabilities. Third, in addi-
tion to straightforwardly combining syntax-derived
phrases with traditional PBSMT phrases, we demon-
strate a new combination technique that removes
PBSMT phrases whose source-language strings are
already covered by a syntax-derived phrase. This
new syntax-prioritized technique results in a 61%
reduction in the size of the combined phrase table
with only a minimal decrease in automatic transla-
tion metric scores. Finally, and crucially, we carry
out the joint decoding over both syntactic and non-
syntactic phrase pairs in a syntax-aware MT sys-
tem, which allows a syntactic grammar to be put in
place on top of the phrase pairs to carry out linguis-
tically motivated reordering, hierarchical decoding,
and other operations.
After this introduction, we first describe the base
MT system we used, its formalism for specify-
ing translation rules, and the method for extract-
ing syntax-derived phrase pairs from a parallel cor-
pus (Section 2). Section 3 gives the two methods
for combining PBSMT phrases with our syntactic
phrases, and introduces our first steps with includ-
ing a grammar in the syntax-based translation frame-
work. The results of our experiments are described
in Section 4 and are further discussed in Section 5.
Finally, Section 6 offers some conclusions and di-
rections for future work.
2 Base Translation System
The base MT system used for our experiments is the
statistical transfer (?Stat-XFER?) framework (Lavie,
2008). The core of the framework is a transfer en-
gine using two language-pair-dependent resources:
a grammar of weighted synchronous context-free
rules, and a probabilistic bilingual lexicon. Once
the resources have been provided, the Stat-XFER
framework carries out translation in a two-stage pro-
cess, first applying the lexicon and grammar to syn-
chronously parse an input sentence, then running
a monotonic decoder over the resulting lattice of
scored translation pieces assembled during parsing
to produce a final string output. Reordering is ap-
plied only in the first stage, driven by the syntactic
grammar; the second-stage monotonic decoder only
assembles translation fragments into complete hy-
potheses.
2.1 Lexicon and Grammar Formalism
Each Stat-XFER bilingual lexicon entry has a syn-
chronous context-free grammar (SCFG) expression
of the source- and target-language production rules,
shown in abbreviated format below, where cs and ct
represent source- and target-side syntactic category
labels and ws and wt represent source- and target-
side word or phrase strings.
cs :: ct ? [ws] :: [wt]
2
Each entry in the lexicon is assigned a pair of rule
scores (rt|s and rs|t) based on cs, ws, ct, and wt1.
The rt|s score is a maximum-likelihood estimate
of the distribution of target-language translations
and source- and target-language syntactic categories
given the source string (Equation 1); this is similar
to the usual ?target-given-source? phrasal probabil-
ity in standard SMT systems. The rs|t score is sim-
ilar, but calculated in the reverse direction to give a
source-given-target probability (Equation 2).
rt|s = #(wt, ct, ws, cs)#(ws) + 1 (1)
rs|t = #(wt, ct, ws, cs)#(wt) + 1 (2)
The add-one smoothing in the denominators coun-
teracts overestimation of the rule scores of lexical
entries with very infrequent source or target sides.
Stat-XFER grammar rules have a similar form,
shown below via an example.
NP :: NP ? [DET1 N2 de N3] :: [DET1 N3 N2]
The SCFG backbone may include lexicalized items,
as above, as well as non-terminals and pre-terminals
from the grammar. Constituent alignment infor-
mation, shown here as co-indexes on the non-
terminals, specifies one-to-one correspondences be-
tween source-language and target-language con-
stituents on the right-hand side of the SCFG rule.
Rule scores rt|s and rs|t for grammar rules, if they
are learned from data, are calculated in the same way
as the scores for lexical entries.
2.2 Syntax-Based Phrase Extraction
In this section, we briefly summarize the automatic
resource extraction approach described by Lavie et
al. (2008) and recently extended by Ambati and
Lavie (2008), which we use here, specifically as ap-
plied to the extraction of syntax-based phrase pairs
for the bilingual lexicon.
The grammar and lexicon are extracted from a
large parallel corpus that has been statistically word-
aligned and independently parsed on both sides with
1If no syntactic category information is available, cs and ct
can be set to dummy values, but the rule score equations remain
unchanged.
automatic parsers. Word-level entries for the bilin-
gual lexicon are directly taken from the word align-
ments; corresponding syntactic categories for the
left-hand side of the SCFG rules are obtained from
the preterminal nodes of the parse trees. Phrase-
level entries for the lexicon are based on node-to-
node alignments in the parallel parse trees. In the
straightforward ?tree-to-tree? scenario, a given node
ns in one parse tree S will be aligned to a node nt
in the other parse tree T if the words in the yield of
ns are all either aligned to words within the yield of
nt or have no alignment at all. If there are multiple
nodes nt satisfying this constraint, the node in the
tree closest to the leaves is selected. Each aligned
node pair (ns, nt) produces a phrase-level entry in
the lexicon, where the left-hand sides of the SCFG
rule are the labels of ns and nt, and the right-hand
sides are the yields of those two nodes in their re-
spective trees. In the expanded ?tree-to-tree-string?
configuration, if no suitable node nt exists, a new
node n?s is introduced into T as a projection of ns,
spanning the yield of the words in T aligned to the
yield of ns. At the end of the extraction process in
either case, the entry counts are collected and scored
in the manner described in Section 2.1.
3 Combination with PBSMT Phrases
Conceptually, we take the opposite approach to that
of Tinsley et al (2007) by adding traditional PBSMT
phrases into a syntax-based MT system rather than
the other way around. We begin by running steps
3 through 5 of the Moses training script (Koehn et
al., 2007)2, which results in a list of phrase pair in-
stances for the same word-aligned corpus to which
we applied the syntax-based extraction methods in
Section 2.2. Given the two sets of phrases, we ex-
plore two methods of combining them.
? Direct Combination. Following the method of
Tinsley et al (2007), we directly combine the
counts of observed syntax-based phrase pairs
with the counts of observed PBSMT phrase
pairs. This results in a modified probability
model in which a higher likelihood is moved
onto syntactic phrase pairs that were also ex-
tractable using traditional PBSMT heuristics. It
2See also www.statmt.org/moses.
3
Decoder Phrase Type # Phrases METEOR BLEU TER
Stat-XFER Syntactic only, PHR 917,266 0.5654 0.2734 56.49
Stat-XFER Syntactic only, frag 1,081,233 0.5653 0.2741 56.54
Stat-XFER Syntactic only, gra 1,081,233 0.5665 0.2772 56.26
Stat-XFER PBSMT only 8,069,480 0.5835 0.3018 54.26
Stat-XFER Direct combination, PHR 8,071,773 0.5835 0.3009 54.21
Stat-XFER Direct combination, frag 9,150,713 0.5841 0.3026 54.52
Stat-XFER Direct combination, gra 9,150,713 0.5855 0.3034 54.28
Stat-XFER Syntax-prioritized, PHR 2,888,154 0.5800 0.2961 54.79
Stat-XFER Syntax-prioritized, frag 3,052,121 0.5802 0.2979 54.78
Stat-XFER Syntax-prioritized, gra 3,052,121 0.5813 0.2991 54.73
Moses PBSMT only, mono 8,145,083 0.5911 0.3139 53.77
Moses PBSMT only, lex RO 8,145,083 0.5940 0.3190 53.48
Figure 1: Results on the test set for all phrase table configurations. For BLEU, bold type indicates the best Stat-XFER
baseline and the configurations statistically equivalent to it (paired bootstrap resampling with n = 1000, p = 0.05).
also allows either extraction mechanism to in-
troduce new entries into the combined phrase
table that were not extracted by the other, thus
permitting the system to take full advantage of
complementary information provided by PB-
SMT phrases that do not correspond to syntac-
tic constituents.
? Syntax-Prioritized Combination. Under this
method, we take advantage of the fact that
syntax-based phrase pairs are likely to be
more precise translational equivalences than
traditional PBSMT phrase pairs, since con-
stituent boundaries are taken into account dur-
ing phrase extraction. PBSMT phrases whose
source-side strings are already covered by an
entry from the syntactic phrase table are re-
moved; the remaining PBSMT phrases are
combined as in the direct combination method
above. The effect on the overall system is
to trust the syntactic phrase pairs in the cases
where they exist, supplementing with PBSMT
phrase pairs for non-constituents.
For each type of phrase-pair combination, we test
three variants when jointly decoding syntax-based
phrases, which come with syntactic information,
along with PBSMT phrases, which do not. In the
first configuration (?PHR?), all extracted phrase la-
bels for syntactic phrases are mapped to a generic
?PHR? tag to simulate standard SMT monotonic de-
coding; this matches the treatment given throughout
to our extracted non-syntactic phrases. In the sec-
ond variant (?frag?), the phrase labels in the large
nonterminal sets used by our source- and target-side
parsers are mapped down to a smaller set of 19 la-
bels that we use for both sides. The same translation
phrase pair may occur with multiple category labels
in this case if it was extracted with different syn-
tactic categories from different trees in the corpus.
In a third variant (?gra?), a small manually devel-
oped grammar is additionally inserted into the sys-
tem. The Stat-XFER system behaves the same way
in each variant. All phrase pairs are applied jointly
to the input sentence during the parsing stage, get-
ting added to the translation according to their syn-
tactic category and scores, although phrases tagged
as PHR cannot participate in any grammar rules.
The second-stage decoder then receives the joint lat-
tice and assembles complete output hypotheses re-
gardless of syntactic category labels.
4 Experiments
We extracted the lexical resources for our MT sys-
tem from version 3 of the French?English Europarl
parallel corpus (Koehn, 2005), using the officially
released training set from the 2008 Workshop in
Statistical Machine Translation (WMT)3. This gives
us a corpus of approximately 1.2 million sentence
3www.statmt.org/wmt08/shared-task.html
4
Phrase Table # Entries # Source Sides Amb. Factor
Total syntax-prioritized table 3,052,121 113,988 26.8
Syntactic component 1,081,233 39,105 27.7
PBSMT component 1,970,888 74,883 26.3
Total baseline PBSMT table 8,069,480 113,972 70.8
Overlap with syntax-prioritized 6,098,592 39,089 156.0
Figure 2: Statistical characteristics of the syntax-prioritized phrase table (top) compared with the baseline PBSMT
phrase table (bottom). The ambiguity factor is the ratio of the number of unique entries to the number of unique
source sides, or the average number of target-language alternatives per source phrase.
pairs. Statistical word alignments are learned in both
directions with GIZA++ (Och and Ney, 2003), then
combined with the ?grow-diag-final? heuristic. For
the extraction of syntax-based phrase pairs, we ob-
tain English-side constituency parses using the Stan-
ford parser (Klein and Manning, 2003), and French-
side constituency parses using the Xerox XIP parser
(A??t-Mokhtar et al, 2001). In phrase extraction,
we concentrate on the expanded tree-to-tree-string
scenario described in Section 2.2, as it results in
a nearly 50% increase in the number of extracted
phrase pairs over the tree-to-tree method. For de-
coding, we construct a suffix-array language model
(Zhang and Vogel, 2006) from a corpus of 430 mil-
lion words, including the English side of our train-
ing data, the English side of the Hansard corpus, and
newswire data. The ?gra? variant uses a nine-rule
grammar that is meant to address the most common
low-level reorderings between French and English,
focusing mainly on the reordering between nouns or
noun phrases and adjectives or adjective phrases.
Our test set is the 2000-sentence ?test2007? data
set, also released as part of the WMT workshop
series. We report case-insensitive scores on ver-
sion 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-style
BLEU (Papineni et al, 2002), and version 5 of TER
(Snover et al, 2006).
Figure 1 gives an overall summary of our results
on the test2007 data. Overall, we train and test 10
different configurations of phrase pairs in the Stat-
XFER decoder. We begin by testing each type of
phrase separately, producing one set of baseline sys-
tems with only phrase pairs that correspond to syn-
tactic constituents (?Syntactic only?) and one base-
line system with only phrase pairs that were ex-
tracted from Moses (?PBSMT only?). We then test
our two combination techniques, and their variants,
as described in Section 3. Statistical significance
is tested on the BLEU metric using paired boot-
strap resampling (Koehn, 2004) with n = 1000 and
p = 0.05. In the figure, the best baseline system and
the configurations statistically equivalent to it are in-
dicated in bold type. In addition to automatic met-
ric scores, we also list the number of unique phrase
pairs extracted for each configuration. (Because of
the large number of phrase pairs, we pre-filter them
to only the set whose source sides appear in the test
data; these numbers are the ones reported.)
As an additional point of comparison, we build
and tune a Moses MT system on the same data
as our Stat-XFER experiments. The Moses system
with a 4-gram language model and a distance-6 lex-
ical reordering model (?lex RO?) scores similarly to
state-of-the-art systems of this type on the test2007
French?English data (Callison-Burch et al, 2007).
Without the reordering model (?mono?), the Moses
system is as comparable as possible in design and
resources to the Stat-XFER PBSMT-only configu-
ration. We do not propose in this paper a head-
to-head performance comparison between the Stat-
XFER and Moses decoders; rather, we report results
on both to gain a better understanding of the im-
pact of the non-syntactic lexical reordering model
in Moses compared with the impact of the syntactic
grammar in Stat-XFER.
5 Discussion
5.1 Phrasal Coverage and Precision
One observation apparent in Figure 1 is that we have
again confirmed that a total restriction to syntax-
5
Source: Il faut que l? opinion publique soit informe?e pleinement sur les caracte?ristiques du
test dont je parle .
Reference: Public opinion must be fully informed of the characteristics of the test I am talking
about .
Syntax only: It | is | that | the public | be informed | fully | on | the characteristics | of the test | I
am talking about | .
PBSMT only: We must | that public opinion gets noticed | fully | on the characteristics of the |
test | above .
Direct comb.: We must | that public opinion gets noticed | fully on | the characteristics of the |
test | above .
Syntax-prioritized: It is important that | the public | be informed | fully on | the characteristics | of the
test | I am talking about | .
Figure 3: A translation example from the test set showing the output?s division into phrases. In the syntax-prioritized
translation, English phrases that derived from syntax-based phrasal entries are shown in italics.
based phrases is detrimental to output quality. A
likely reason for this, as Tinsley et al (2007) sug-
gested, is that the improved precision and infor-
mativeness of the syntactic phrases is not enough
to overcome their relative scarcity when compared
to non-syntactic PBSMT phrases. (The syntactic
phrase table is only 11 to 13% of the size of the PB-
SMT phrase table.) It is important to note that this
scarcity occurs at the phrasal level: though there are
294 unknown word types in our test set when trans-
lating with only syntactic phrase pairs, this num-
ber only drops to 277 with the inclusion of PBSMT
phrases. The largest phrase table configuration, di-
rect combination, yields statistically equivalent per-
formance to the baseline system created using stan-
dard PBSMT extraction heuristics. Its key benefit
is that the inclusion of syntactic information in the
phrase pairs, where possible, leaves open the door to
further improvement in scores with the addition of a
larger syntactic grammar. We have thus addressed
the syntax-only phrase coverage problem without
giving up syntactic information.
An interesting conclusion is revealed in the anal-
ysis of the sizes and relative overlaps of the phrase
tables in each of our translation conditions. In
the absence of significant grammar, the equiva-
lence of scores between the PBSMT-only and direct-
combination scenarios is understandable given the
minimal change in the size of the phrase table. Out
of nearly 8.1 million entries, only 2293 entirely new
entries are provided by adding the syntactic phrase
table; further, these phrases are relatively rare long
phrases that do not have much effect on the trans-
lation of the overall test set. On the other hand, the
syntax-prioritized phrase table is extremely different
in nature ? and only 37.8% of the size of the base-
line PBSMT phrase table ? yet still attains nearly
the same automatic metric scores. There, we can
clearly see the effect of the syntactic phrases, since
the 3,052,121 phrases used in the fragmented vari-
ant of that scenario are more noticibly split between
1,970,888 PBSMT phrases (64.6%) and 1,081,233
syntax-based phrases (35.4%).
Some statistics for the makeup of the syntax-
prioritized phrase table, compared to the baseline
PBSMT phrase table, are shown in Figure 2. For
each, we calculate the ?ambiguity factor,? or the
average number of target-language alternatives for
each source-language phrase in the table. This anal-
ysis shows not only that the distribution of tradi-
tional PBSMT phrases is rather different from that
of the syntactic phrases, it is also different from the
non-syntactic PBSMT phrases that are preserved in
the syntax-prioritized table. In effect, given a base-
line PBSMT phrase table, the syntax prioritization
replaces phrase entries for 39,089 source-language
phrases, each with an average of 156 different target-
language translations, with 39,105 source phrases,
each with an average of 27.7 syntactically motivated
target translations ? a net savings of 5.0 million
6
Source: Je veux saluer , a` mon tour , l? intervention forte et substantielle du pre?sident Prodi .
Reference: I too would like to welcome Mr Prodi ?s forceful and meaningful intervention .
PHR
I welcome
S
, in turn ,
NP
the strong and substantial speech
ADJP
strong and substantial
ADJ
substantial
CON
and
ADJ
strong
N
speech
DET
the
PP
of President Prodi
PU
.
Figure 4: A translation example from the test set showing the result of including the nine-rule grammar in the syntax-
prioritized combination. The SMT-only translation of the noun phrase is the decisive intervention and substantial.
phrase pairs. This is a strong indication that, be-
cause of the more accurate phrase boundary detec-
tion, the syntactic phrases are a much more precise
representation of translational equivalence. An ad-
ditional benefit is a significant reduction in decoding
time, from an average of 27.3 seconds per sentence
with the baseline PBSMT phrase table to 10.7 sec-
onds per sentence with the syntax-prioritized table
with the grammar included.
Improved precision due to the inclusion of syn-
tactic phrases can be seen by examining a translation
example and the phrasal chunks that produce it (Fig-
ure 3). In the syntax-prioritized output, the English
phrases deriving from syntax-based phrase pairs are
shown in italic, while the phrases deriving from PB-
SMT pairs are in normal type. The example shows
an effective combination of on-target translations for
syntactic constituents, when they are available, with
non-syntactic phrases to handle constituent bound-
aries or places where parallel constituents are dif-
ficult to extract. The translation pieces be informed
and I am talking about, though they exist in the base-
line PBSMT phrase table, do not make it into the
top-best translation in the PBSMT-only scenario be-
cause of its high ambiguity factor.
5.2 Effect of Syntactic Information
Although our current experiments do not show a sig-
nificant increase in automatic metric scores with the
addition of a small grammar, we can see the po-
tential power of grammar in examining further sen-
tences from the output. For example, in Figure 4,
standard PBSMT phrase extraction is able to pick up
the adjective?noun reordering when translating from
intervention forte to decisive intervention. However,
in this sentence we have an adjective phrase follow-
ing the noun, and there is no pre-extracted phrase
pair for the entire constituent, so our system built
from only PBSMT phrases produces the incorrect
noun phrase translation the decisive intervention and
substantial. Our nine-rule grammar, specifically tar-
geted for this scenario, is able to correct the structure
of the sentence by applying two rules to produce the
strong and substantial speech.
Analysis of the entire test set further suggests that
even our small grammar produces correct and pre-
cise output across all phrase table configurations, al-
though the total number of applications of the nine
rules remains low. There are 590 rule applications
in the one-best output on the test set in the syntax-
only configuration, 472 applications in the syntax-
prioritized configuration, and 216 applications in the
direct combination. In each configuration, we man-
ually inspected all rule applications in the first 200
sentences and classified them as correctly reordering
words in the English output (?good?), incorrectly re-
ordering (?bad?), or ?null.? This last category de-
notes applications of monotonic structure-building
rules that did not feed into a higher-level reordering
rule. The results of this analysis are shown in Fig-
ure 5. Overall, we find that the grammar is 97% ac-
curate in its applications, making helpful reordering
changes 88% of the time.
Given the preceding analysis ? and the fact that
our inclusion of a lexicalized reordering model in
7
Phrase Table Good Bad Null
Syntactic only 47 3 8
Syntax-prioritized 45 1 3
Direct combination 25 0 0
Figure 5: Manual analysis of grammar rule applications
in the first 200 sentences of the test set.
Moses resulted in automatic metric gains of only
0.0051 BLEU, 0.0029 METEOR, and 0.29 TER ?
we believe that further experiments with a much
larger syntactic grammar will lead to a more signif-
icant improvement in automatic metric scores and
translation quality.
6 Conclusions and Future Work
We have extended and applied an algorithm for com-
bining syntax-based phrases from a parallel parsed
corpus with non-syntactic phrases from phrase-
based SMT within the context of a statistical syntax-
based translation framework. Using a much larger
corpus than has previously been employed for this
approach, we produce jointly decoded output sta-
tistically equivalent to a monotonic decoding using
standard PBSMT phrase-extraction heuristics, re-
taining syntactic information and setting the stage
for further improvements by incorporating a syntac-
tic grammar into the translation framework. Our
preliminary nine-rule grammar, targeted for two spe-
cific English?French linguistic phenomena, already
shows promise in performing linguistically moti-
vated reordering that cannot be captured formally by
a standard PBSMT model.
We present a syntax-prioritized method of com-
bining phrase types into a single phrase table by al-
ways selecting a syntax-based phrase pair when one
is available for a given source string. This new com-
bination style reduces the size of the resulting phrase
table and total decoding time by 61%, with only
a minor degradation in MT performance. We sug-
gest that this is because the syntax-derived phrases,
when they can be extracted, are a much more precise
method of describing correct translational equiva-
lences.
As yet, we have made only minimal use of the
Stat-XFER framework?s grammar capabilities. In
our experiments, the full tree-to-tree-string rule-
extraction process of Ambati and Lavie (2008) pro-
duces more than 2 million unique SCFG rules when
applied to a corpus the size of the Europarl. Not only
is translating with such a large set computationally
intractable, but empirically nearly 90% of the rules
were observed only once in the parallel parsed cor-
pus, making it difficult to separate rare but correct
rules from those due to noise in the parses and word
alignments. With the view of moving beyond our
manually written nine-rule grammar, but wanting to
get only the most useful rules from the entire auto-
matically extracted set, we are currently investigat-
ing methods for automatic scoring or selection of a
reasonable number of grammar rules for a particular
language pair. Given that the majority of our phrase
pairs, even in the syntax-prioritized combination, are
non-syntactic, we have also conducted preliminary
experiments with ?syntactifying? them so that they
may also be used by grammar rules to produce larger
translation fragments.
The experiments in this paper used the grow-diag-
final heuristic for word alignment combination be-
cause it has been shown to provide the highest preci-
sion on the subtree node alignment method by which
we extract syntax-based phrase pairs (Lavie et al,
2008). However, this is a trade-off that sacrifices
some amount of recall. Experimenting with differ-
ent symmetric alignment heuristics may lead to a
more optimal configuration for phrase-pair extrac-
tion or combination with PBSMT phrases. We also
suspect that the choice of source- and target-side
parsers plays a significant role in the number and
nature of phrase pairs we extract; to address this,
we are in the process of re-trying our line of exper-
iments using the Berkeley parser (Petrov and Klein,
2007) for English, French, or both.
Acknowledgments
This research was supported in part by NSF grant
IIS-0534217 (LETRAS) and the DARPA GALE
program. We thank the members of the Parsing and
Semantics group at Xerox Research Center Europe
for parsing the French data with their XIP parser.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
8
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, China, October.
Vamshi Ambati and Alon Lavie. 2008. Improving syntax
driven translation models by re-structuring divergent
and non-isomorphic parse tree structures. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 235?244,
Waikiki, HI, October.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 48?54, Edmonton,
Alberta, May?June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the 10th
Machine Translation Summit, pages 79?86, Phuket,
Thailand, September.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609?616, Sydney, Aus-
tralia, July.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011,
Columbus, OH, June.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
John Tinsley, Mary Hearne, and Andy Way. 2007. Ex-
ploiting parallel treebanks to improve phrase-based
statistical machine translation. In Proceedings of the
Sixth International Workshop on Treebanks and Lin-
guistic Theories, pages 175?187, Bergen, Norway, De-
cember.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 746?754, Prague, Czech Re-
public, June.
Ying Zhang and Stephan Vogel. 2006. Suffix array and
its applications in empirical natural language process-
ing. Technical Report CMU-LTI-06-010, Carnegie
Mellon University, Pittsburgh, PA, December.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
9
Proceedings of NAACL-HLT 2013, pages 288?297,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving Syntax-Augmented Machine Translation by
Coarsening the Label Set
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,alavie}@cs.cmu.edu
Abstract
We present a new variant of the Syntax-
Augmented Machine Translation (SAMT) for-
malism with a category-coarsening algorithm
originally developed for tree-to-tree gram-
mars. We induce bilingual labels into the
SAMT grammar, use them for category coars-
ening, then project back to monolingual la-
beling as in standard SAMT. The result is a
?collapsed? grammar with the same expres-
sive power and format as the original, but
many fewer nonterminal labels. We show that
the smaller label set provides improved trans-
lation scores by 1.14 BLEU on two Chinese?
English test sets while reducing the occur-
rence of sparsity and ambiguity problems
common to large label sets.
1 Introduction
The formulation of statistical machine translation in
terms of synchronous parsing has become both the-
oretically and practically successful. In a parsing-
based MT formalism, synchronous context-free
grammar rules that match a source-language input
can be hierarchically composed to produce a corre-
sponding target-language output. SCFG translation
grammars can be extracted automatically from data.
While formally syntactic approaches with a single
grammar nonterminal have often worked well (Chi-
ang, 2007), the desire to exploit linguistic knowl-
edge has motivated the use of translation grammars
with richer, linguistically syntactic nonterminal in-
ventories (Galley et al, 2004; Liu et al, 2006; Lavie
et al, 2008; Liu et al, 2009).
Linguistically syntactic MT systems can derive
their label sets, either monolingually or bilingually,
from parallel corpora that have been annotated with
source- and/or target-side parse trees provided by
a statistical parser. The MT system may exactly
adopt the parser?s label set or modify it in some way.
Larger label sets are able to represent more precise,
fine-grained categories. On the other hand, they also
exacerbate a number of computational and modeling
problems by increasing grammar size, derivational
ambiguity, and data sparsity.
In this paper, we focus on the Syntax-Augmented
MT formalism (Zollmann and Venugopal, 2006), a
monolingually labeled version of Hiero that can cre-
ate up to 4000 ?extended? category labels based on
pairs of parse nodes. We take a standard SAMT
grammar with target-side labels and extend its label-
ing to a bilingual format (Zollmann, 2011). We then
coarsen the bilingual labels following the ?label col-
lapsing? algorithm of Hanneman and Lavie (2011).
This represents a novel extension of the tree-to-tree
collapsing algorithm to the SAMT formalism. Af-
ter removing the source-side labels, we obtain a new
SAMT grammar with coarser target-side labels than
the original.
Coarsened grammars provide improvement of up
to 1.14 BLEU points over the baseline SAMT results
on two Chinese?English test sets; they also outper-
form a Hiero baseline by up to 0.60 BLEU on one
of the sets. Aside from improved translation quality,
in analysis we find significant reductions in deriva-
tional ambiguity and rule sparsity, two problems that
make large nonterminal sets difficult to work with.
Section 2 provides a survey of large syntax-based
288
MT label sets, their associated problems of deriva-
tional ambiguity and rule sparsity, and previous at-
tempts at addressing those problems. The section
also summarizes the tree-to-tree label collapsing al-
gorithm and the process of SAMT rule extraction.
We then describe our method of label collapsing in
SAMT grammars in Section 3. Experimental results
are presented in Section 4 and analyzed in Section
5. Finally, Section 6 offers some conclusions and
avenues for future work.
2 Background
2.1 Working with Large Label Sets
Aside from the SAMT method of grammar extrac-
tion, which we treat more fully in Section 2.3, sev-
eral other lines of work have explored increasing
the nonterminal set for syntax-based MT. Huang and
Knight (2006), for example, augmented the standard
Penn Treebank labels for English by adding lexi-
calization to certain types of nodes. Chiang (2010)
and Zollmann (2011) worked with a bilingual exten-
sion of SAMT that used its notion of ?extended cat-
egories? on both the source and target sides. Taking
standard monolingual SAMT as a baseline, Baker et
al. (2012) developed a tagger to augment syntactic
labels with some semantically derived information.
Ambati et al (2009) extracted tree-to-tree rules with
similar extensions for sibling nodes, resulting again
in a large number of labels.
Extended categories allow for the extraction of
a larger number of rules, increasing coverage and
translation performance over systems that are lim-
ited to exact constituent matches only. However,
the gains in coverage come with a corresponding
increase in computational and modeling complexity
due to the larger label set involved.
Derivational ambiguity ? the condition of hav-
ing multiple derivations for the same output string
? is a particular problem for parsing-based MT sys-
tems. The same phrase pair may be represented with
a large number of different syntactic labels. Fur-
ther, new hierarchical rules are created by abstract-
ing smaller phrase pairs out of larger ones; each of
these substitutions must also be marked by a label
of some kind. Keeping variantly labeled copies of
the same rules fragments probabilities during gram-
mar scoring and creates redundant hypotheses in the
decoder at run time.
A complementary problem ? when a desired rule
application is impossible because its labels do not
match ? has been variously identified as ?data spar-
sity,? the ?matching constraint,? and ?rule sparsity?
in the grammar. It arises from the definition of
SCFG rule application: in order to compose two
rules, the left-hand-side label of the smaller rule
must match a right-hand-side label in the larger rule
it is being plugged in to. With large label sets, it
becomes less likely that two arbitrarily chosen rules
can compose, making the grammar less flexible for
representing new sentences.
Previous research has attempted to address both
of these problems in different ways. Preference
grammars (Venugopal et al, 2009) are a technique
for reducing derivational ambiguity by summing
scores over labeled variants of the same deriva-
tion during decoding. Chiang (2010) addressed rule
sparsity by introducing a soft matching constraint:
the decoder may pay a learned label-pair-specific
penalty for substituting a rule headed by one label
into a substitution slot marked for another. Combin-
ing properties of both of the above methods, Huang
et al (2010) modeled monolingual labels as distribu-
tions over latent syntactic categories and calculated
similarity scores between them for rule composition.
2.2 Label Collapsing in Tree-to-Tree Rules
Aiming to reduce both derivational ambiguity and
rule sparsity, we previously presented a ?label col-
lapsing? algorithm for systems in which bilingual
labels are used (Hanneman and Lavie, 2011). It
coarsens the overall label set by clustering monolin-
gual labels based on which labels they appear joined
with in the other language.
The label collapsing algorithm takes as its input
a set of SCFG rule instances extracted from a par-
allel corpus. Each time a tree-to-tree rule is ex-
tracted, its left-hand side is a label of the form s::t,
where s is a label from the source-language cate-
gory set S and t is a label from the target-language
category set T . Operationally, the joint label means
that a source-side subtree rooted at s was the trans-
lational equivalent of a target-side subtree rooted at
t in a parallel sentence. Figure 1 shows several such
subtrees, highlighted in grey and numbered. Joint
left-hand-side labels for the collapsing algorithm,
289
Figure 1: Sample extraction of bilingual nonterminals for
label collapsing. Labels extracted from this tree pair in-
clude VBD::VV and NP::AD.
such as VBD::VV and NP::AD, can be assembled
by matching co-numbered nodes.
From the counts of the extracted rules, it is thus
straightforward to compute for all values of s and
t the observed P (s | t) and P (t | s), the probability
of one half of a joint nonterminal label appearing
in the grammar given the other half. In the figure,
for example, P (JJ |NN) = 0.5. The conditional
probabilities accumulated over the whole grammar
give rise to a simple L1 distance metric over any pair
of monolingual labels:
d(s1, s2) =
?
t?T
|P (t | s1)? P (t | s2)| (1)
d(t1, t2) =
?
s?S
|P (s | t1)? P (s | t2)| (2)
An agglomerative clustering algorithm then com-
bines labels in a series of greedy iterations. At each
step, the algorithm finds the pair of labels that is cur-
rently the closest together according to the distance
metrics of Equations (1) and (2), combines those two
labels into a new one, and updates the set of P (s | t)
and P (t | s) values appropriately. The choice of la-
bel pair to collapse in each iteration can be expressed
formally as
argmin
(si,sj)?S2,(tk,t`)?T 2
{d(si, sj), d(tk, t`)} (3)
That is, either a source label pair or a target label pair
may be chosen by the algorithm in each iteration.
2.3 SAMT Rule Extraction
SAMT grammars pose a challenge to the label col-
lapsing algorithm described above because their la-
bel sets are usually monolingual. The classic SAMT
formulation (Zollmann and Venugopal, 2006) pro-
duces a grammar labeled on the target side only.
Nonterminal instances that exactly match a target-
language syntactic constituent in a parallel sentence
are given labels of the form t. Labels of the form
t1+t2 are assigned to nonterminals that span exactly
two contiguous parse nodes. Categorial grammar la-
bels such as t1/t2 and t1\t2 are given to nontermi-
nals that span an incomplete t1 constituent missing
a t2 node to its right or left, respectively. Any non-
terminal that cannot be labeled by one of the above
three schemes is assigned the default label X.
Figure 2(a) shows the extraction of a VP-level
SAMT grammar rule from part of a parallel sen-
tence. At the word level, the smaller English phrase
supported each other (and its Chinese equivalent) is
being abstracted as a nonterminal within the larger
phrase supported each other in international affairs.
The larger phrase corresponds to a parsed VP node
on the target side; this will become the label of
the extracted rule?s left-hand side. Since the ab-
stracted sub-phrase does not correspond to a single
constituent, the SAMT labeling conventions assign
it the label VBD+NP. We can thus write the ex-
tracted rule as:
(4)
While the SAMT label formats can be trivially
converted into joint labels X::t, X::t1+t2, X::t1/t2,
X::t1\t2, and X::X, they cannot be usefully fed into
the label collapsing algorithm because the necessary
conditional label probabilities are meaningless. To
acquire meaningful source-side labels, we turn to a
290
(a) (b)
Figure 2: Sample extraction of an SAMT grammar rule: (a) with monolingual syntax and (b) with bilingual syntax.
bilingual SAMT extension used by Chiang (2010)
and Zollmann (2011). Both a source- and a target-
side parse tree are used to extract rules from a par-
allel sentence; two SAMT-style labels are worked
out independently on each side for each nonterminal
instance, then packed into a joint label. It is there-
fore possible for a nonterminal instance to be labeled
s::t, s1\s2::t, s1+s2::t1/t2, or various other combi-
nations depending on what parse nodes the nonter-
minal spans in each tree.
Such a bilingually labeled rule is extracted in Fig-
ure 2(b). The target-side labels from Figure 2(a) are
now paired with source-side labels extracted from an
added Chinese parse tree. In this case, the abstracted
sub-phrase supported each other is given the joint
label VP::VBD+NP, while the rule?s left-hand side
becomes LCP+VP::VP.
We implement bilingual SAMT grammar extrac-
tion by modifying Thrax (Weese et al, 2011), an
open-source, Hadoop-based framework for extract-
ing standard SAMT grammars. By default, Thrax
can produce grammars labeled either on the source
or target side, but not both. It also outputs rules
that are already scored according to a user-specified
set of translation model features, meaning that the
raw rule counts needed to compute the label condi-
tional probabilities P (s | t) and P (t | s) are not di-
rectly available. We implement a new subclass of
grammar extractor with logic for independently la-
beling both sides of an SAMT rule in order to get the
necessary bilingual labels; an adaptation to the exist-
ing Thrax ?rarity? feature provides the rule counts.
3 Label Collapsing in SAMT Rules
Our method of producing label-collapsed SAMT
grammars is shown graphically in Figure 3.
We first obtain an SAMT grammar with bilingual
labels, together with the frequency count for each
rule, using the modified version of Thrax described
in Section 2.3. The rules can be grouped according
to the target-side label of their left-hand sides (Fig-
ure 3(a)).
The rule counts are then used to compute label-
ing probabilities P (s | t) and P (t | s) over left-hand-
side usages of each source label s and each target
label t. These are simple maximum-likelihood es-
timates: if #(si, tj) represents the combined fre-
quency counts of all rules with si::tj on the left-hand
291
(a) (b) (c) (d)
Figure 3: Stages of preparing label-collapsed rules for SAMT grammars. (a) SAMT rules with bilingual nonterminals
are extracted and collected based on their target left-hand sides. (b) Probabiliites P (t | s) and P (s | s) are computed. (c)
Nonterminals are clustered according to the label collapsing algorithm. (d) Source sides of nonterminals are removed
to create a standard SAMT grammar.
side, the source-given-target labeling probability is:
P (si | tj) =
#(si::tj)
?
t?T #(si::t)
(5)
The computation for target given source is analo-
gous. Each monolingual label can thus be repre-
sented as a distribution over the labels it is aligned
to in the opposite language (Figure 3(b)).
Such distributions over labels are the input to the
label-collapsing algorithm, as described in Section
2.2. As shown in Figure 3(c), the algorithm results
in the original target-side labels being combined into
different groups, denoted in this case as new labels
CA and CB. We run label collapsing for varying
numbers of iterations to produce varying degrees of
coarsened label sets.
Given a mapping from original target-side labels
to collapsed groups, all nonterminals in the original
SAMT grammar are overwritten accordingly. The
source-side labels are dropped at this point: we use
them only for the purpose of label collapsing, but not
in assembling or scoring the final grammar. The re-
sulting monolingual SAMT-style grammar with col-
lapsed labels (Figure 3(d)) can now be scored and
used for decoding in the usual way.
For constructing a baseline SAMT grammar with-
out label collapsing, we merely extract a bilingual
grammar as in the first step of Figure 3, immediately
remove the source-side labels from it, and proceed
to grammar scoring.
All grammars are scored according to a set of
eight features. For an SCFG rule with left-hand-side
label t, source right-hand side f , and target right-
hand side e, they are:
? Standard maximum-likelihood phrasal transla-
tion probabilities P (f | e) and P (e | f)
? Maximum-likelihood labeling probability
P (t | f, e)
? Lexical translation probabilities Plex(f | e) and
Plex(e | f), as calculated by Thrax
? Rarity score
exp( 1c )?1
exp(1)?1 for a rule with extracted
count c
? Binary indicator features that mark phrase pair
(as opposed to hierarchical) rules and glue rules
Scored grammars are filtered down to the sen-
tence level, retaining only those rules whose source-
side terminals match an individual tuning or testing
sentence. In addition to losslessly filtering gram-
mars in this way, we also carry out two types of
lossy pruning in order to reduce overall grammar
292
System Labels Rules Per Sent.
SAMT 4181 69,401,006 48,444
Collapse 1 913 64,596,618 35,004
Collapse 2 131 60,526,479 24,510
Collapse 3 72 58,483,310 20,445
Hiero 1 36,538,657 7,738
Table 1: Grammar statistics for different degrees of label
collapsing: number of target-side labels, unique rules in
the whole grammar, and average number of pruned rules
after filtering to individual sentences.
size. One pruning pass keeps only the 80 most fre-
quently observed target right-hand sides for each
source right-hand side. A second pass globally re-
moves hierarchical rules that were extracted fewer
than six times in the training data.
4 Experiments
We conduct experiments on Chinese-to-English MT,
using systems trained from the FBIS corpus of ap-
proximately 302,000 parallel sentence pairs. We
parse both sides of the training data with the Berke-
ley parsers (Petrov and Klein, 2007) for Chinese
and English. The English side is lowercased after
parsing; the Chinese side is segmented beforehand.
Unidirectional word alignments are obtained with
GIZA++ (Och and Ney, 2003) and symmetrized, re-
sulting in a parallel parsed corpus with Viterbi word
alignments for each sentence pair. Our modified ver-
sion of Thrax takes the parsed and aligned corpus as
input and returns a list of rules, which can then be
label-collapsed and scored as previously described.
In Thrax, we retain most of the default settings for
Hiero- and SAMT-style grammars as specified in the
extractor?s configuration file. Inheriting from Hiero,
we require the right-hand side of all rules to con-
tain at least one pair of aligned terminals, no more
than two nonterminals, and no more than five termi-
nals and nonterminal elements combined. Nonter-
minals are not allowed to be adjacent on the source
side, and they may not contain unaligned boundary
words. Rules themselves are not extracted from any
span in the training data longer than 10 tokens.
Our initial bilingual SAMT grammar uses 2699
unique source-side labels and 4181 unique target-
side labels, leading to the appearance of 29,088 joint
bilingual labels in the rule set. We provide the joint
labels (along with their counts) to the label collaps-
ing algorithm, while we strip out the source-side
labels to create the baseline SAMT grammar with
4181 unique target-side labels. Table 1 summarizes
how the number of target labels, unique extracted
rules, and the average number of pruned rules avail-
able per sentence change as the initial grammar is
label-collapsed to three progressively coarser de-
grees. Once the collapsing process has occurred ex-
haustively, the original SAMT grammar becomes a
Hiero-format grammar with a single nonterminal.
Each of the five grammars in Table 1 is used to
build an MT system. All systems are tuned and de-
coded with cdec (Dyer et al, 2010), an open-source
decoder for SCFG-based MT with arbitrary rule for-
mats and nonterminal labels. We tune the systems
on the 1664-sentence NIST Open MT 2006 data set,
optimizing towards the BLEU metric. Our test sets
are the NIST 2003 data set of 919 sentences and the
NIST 2008 data set of 1357 sentences. The tun-
ing set and both test sets all have four English ref-
erences.
We evaluate systems on BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011), and
TER (Snover et al, 2006), as calculated in all three
cases by MultEval version 0.5.0.1 These scores for
the MT ?03 test set are shown in Table 2, and those
for the MT ?08 test set in Table 3, combined by Mult-
Eval over three optimization runs on the tuning set.
MultEval also implements statistical significance
testing between systems based on multiple optimizer
runs and approximate randomization. This process
(Clark et al, 2011) randomly swaps outputs between
systems and estimates the probability that the ob-
served score difference arose by chance. We report
these results in the tables as well for three MERT
runs and a p-value of 0.05. Systems that were judged
statistically different from the SAMT baseline have
triangles in the appropriate ?Sig. SAMT?? columns;
systems judged different from the Hiero baseline
have triangles under the ?Sig. Hiero?? columns. An
up-triangle (N) indicates that the system was better,
while a down-triangle (O) means that the baseline
was better.
1https://github.com/jhclark/multeval
293
Metric Scores Sig. SAMT? Sig. Hiero?
System BLEU MET TER B M T B M T
SAMT 31.18 30.64 61.02 O O O
Collapse 1 31.42 31.31 60.95 N O O
Collapse 2 31.90 31.73 60.98 N N O N O
Collapse 3 32.32 31.75 60.54 N N N N O
Hiero 32.30 31.42 60.10 N N N
Table 2: MT ?03 test set results. The first section gives automatic metric scores; the remaining sections indicate
whether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.
Metric Scores Sig. SAMT? Sig. Hiero?
System BLEU MET TER B M T B M T
SAMT 22.10 24.94 63.78 O O O
Collapse 1 23.01 26.03 63.35 N N N N
Collapse 2 23.53 26.50 63.29 N N N N N
Collapse 3 23.61 26.37 63.07 N N N N N N
Hiero 23.01 25.72 63.53 N N N
Table 3: MT ?08 test set results. The first section gives automatic metric scores; the remaining sections indicate
whether each system is statistically significantly better (N) or worse (O) than the SAMT and Hiero baselines.
Figure 4: Extracted frequency of each target-side label, with labels arranged in order of decreasing frequency count.
Note the log?log scale of the plot.
294
5 Analysis
Tables 2 and 3 show that the coarsened grammars
significantly improve translation performance over
the SAMT baseline. This is especially true for the
?Collapse 3? setting of 72 labels, which scores 1.14
BLEU higher on MT ?03 and 1.51 BLEU higher on
MT ?08 than the uncollapsed system.
On the easier MT ?03 set, label-collapsed systems
do not generally outperform Hiero, although Col-
lapse 3 achieves a statistical tie according to BLEU
(+0.02) and a statistical improvement over Hiero ac-
cording to METEOR (+0.33). MT ?08 appears as
a significantly harder test set: metric scores for all
systems are drastically lower, and we find approxi-
mately 7% to 8% fewer phrase pair matches per sen-
tence. In this case the label-collapsed systems per-
form better, with all three of them achieving statisti-
cal significance over Hiero in at least one metric and
statistical ties in the other. The coarsened systems?
comparatively better performance on the harder test
set suggests that the linguistic information encoded
in multiple-nonterminal grammars helps the systems
more accurately parse new types of input.
Table 1 already showed at a global scale the strong
effect of label collapsing on reducing derivational
ambiguity, as labeled variants of the same basic
structural rule were progressively combined. Since
category coarsening is purely a relabeling operation,
any reordering pattern implemented in the original
SAMT grammar still exists in the collapsed ver-
sions; therefore, any reduction in the size of the
grammar is a reduction in variant labelings. Figure
4 shows this process in more detail for the baseline
SAMT grammar and the three collapsed grammars.
For each grammar, labels are arranged in decreas-
ing order of extracted frequency, and the frequency
count of each label is plotted. The long tail of rare
categories in the SAMT grammar (1950 labels seen
fewer than 100 times each) is combined into a pro-
gressively sharper distribution at each step. Not only
are there fewer rare labels, but these hard-to-model
categories consume a proportionally smaller fraction
of the total label set: from 47% in the baseline gram-
mar down to 26% in Collapse 3.
We find that label collapsing disproportionately
affects frequently extracted and hierarchical rules
over rarer rules and phrase pairs. The 15.7% re-
duction in total grammar size between the SAMT
baseline and the Collapse 3 system affects 18.0% of
the hierarchical rules, but only 1.6% of the phrase
pairs. If rules are counted separately each time they
match another source sentence, the average reduc-
tion in size of a sentence-filtered grammar is 57.8%.
Intuitively, hierarchical rules are more affected by
label collapsing because phrase pairs do not have
many variant left-hand-side labels to begin with,
while the same hierarchical rule pattern may be in-
stantiated in the grammar by a large number of vari-
ant labelings. We can see this situation in more de-
tail by counting variants of a particular set of rules.
Labeled forms of the Hiero-style rule
X ? [X1 X2] :: [the X2 of X1] (6)
are among the most frequently used rules in all five
of our systems. The way they are treated by label
collapsing thus has a strong impact on the results of
runtime decoding.
In the SAMT baseline, Rule (6) appears in the
grammar with 221 different labels in the X1 nonter-
minal slot, 53 labels for the X2 slot, and 90 choices
of left-hand side ? a total of 1330 different label-
ings all together. More than three-fourths of these
variants were extracted three times or fewer from the
training data; even if they can be used in a test sen-
tence, statistical features for such low-count rules
are poorly estimated. During label collapsing, the
number of labeled variations of Rule (6) drops from
1330 to 325, to 96, and finally to 63 in the Collapse
3 grammar. There, the pattern is instantiated with 14
possible X1 labels, five X2 labels, and three different
left-hand sides.
It is difficult to measure rule sparsity directly (i.e.
to count the number of rules that are missing during
decoding), but a reduction in rule sparsity between
systems should be manifested as an increased num-
ber of hierarchical rule applications. Figure 5 shows
the average number of hierarchical rules applied per
sentence, distinguishing syntactic rules from glue
rules, on both test sets. The collapsed grammars al-
low for approximately one additional syntactic rule
application per sentence compared to the SAMT
baseline, or three additional applications compared
to Hiero. This shows an implicit reduction in miss-
ing syntactic rules in the collapsed grammars. In the
295
MT 2003 MT 2008
Figure 5: Average number of hierarchical rules (both syntactic and glue rules) applied per sentence on each test set.
glue rule columns, we note that label collapsing also
promotes a shift away from generic glue rules, pos-
sibly via the creation of more permissive ? but still
meaningfully labeled ? syntactic rules.
6 Conclusion
We demonstrated a viable technique for reducing the
label set size in SAMT grammars by temporarily in-
ducing bilingual syntax and using it in an existing
tree-to-tree category coarsening algorithm. In col-
lapsing SAMT category labels, we were able to sig-
nificantly improve translation quality while using a
grammar less than half the size of the original. We
believe it is also more robust to test-set or domain
variation than a single-nonterminal Hiero grammar.
Collapsed grammars confer practical benefits during
both model estimation and runtime decoding. We
showed that, in particular, they suffer less from rule
sparsity and derivational ambiguity problems that
are common to larger label sets.
We can highlight two areas for potential improve-
ments in future work. In our current implementation
of label collapsing, we indiscriminately allow either
source labels or target labels to be collapsed at each
iteration of the algorithm (see Equation 3). This is
an intuitively sensible setting when collapsing bilin-
gual labels, but it is perhaps less obviously so for a
monolingually labeled system such as SAMT. An al-
ternative would be to collapse target-side labels only,
leaving the source-side labels alone since they do not
appear in the final grammar anyway. In this case, the
target labels would be represented and clustered as
distributions over a static set of latent categories.
A larger area of future concern is the stopping
point of the collapsing algorithm. In our previ-
ous work (Hanneman and Lavie, 2011), we manu-
ally identified iterations in our run of the algorithm
where the L1 distance between the most recently
collapsed label pair was markedly lower than the
L1 difference of the pair in the previous iteration.
Such an approach is more feasible in our previous
runs of 120 iterations than in ours here of nearly
2100, where it is not likely that three manually cho-
sen stopping points represent the optimal collapsing
results. In future work, we plan to work towards the
development of an automatic stopping criterion, a
more principled test for whether each successive it-
eration of label collapsing provides some useful ben-
efit to the underlying grammar.
Acknowledgments
This research work was supported in part by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
Thanks to Chris Dyer for providing the word-
aligned and preprocessed corpus we used in our ex-
periments. We also thank the anonymous reviewers
for helpful comments and suggestions for analysis.
References
Vamshi Ambati, Alon Lavie, and Jaime Carbonell. 2009.
Extraction of syntactic translation models from paral-
lel data using syntax from source and target languages.
296
In Proceedings of the 12th Machine Translation Sum-
mit, pages 190?197, Ottawa, Canada, August.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic MT. Computational Linguistics, 38(2):411?
438.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Crontrolling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Short
Papers, pages 176?181, Portland, OR, June.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 85?91, Edinburgh, United Kingdom, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, MA, May.
Greg Hanneman and Alon Lavie. 2011. Automatic cate-
gory label coarsening for syntax-based machine trans-
lation. In Proceedings of SSST-5: Fifth Workshop on
Syntax, Semantics, and Structure in Statistical Trans-
lation, pages 98?106, Portland, OR, June.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 240?247, New York, NY, June.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 138?147, Cambridge, MA, October.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609?616, Sydney, Aus-
tralia, July.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the 47th Annual Meeting of the ACL and
the Fourth IJCNLP of the AFNLP, pages 558?566,
Suntec, Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the ACL, pages 236?244, Boulder, CO,
June.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 478?
484, Edinburgh, United Kingdom, July.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
Andreas Zollmann. 2011. Learning Multiple-
Nonterminal Synchronous Grammars for Machine
Translation. Ph.D. thesis, Carnegie Mellon University.
297
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82?87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Features and Grammar Selection for Syntax-Based MT
Greg Hanneman and Jonathan Clark and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, jhclark, alavie}@cs.cmu.edu
Abstract
We present the Carnegie Mellon Univer-
sity Stat-XFER group submission to the
WMT 2010 shared translation task. Up-
dates to our syntax-based SMT system
mainly fell in the areas of new feature for-
mulations in the translation model and im-
proved filtering of SCFG rules. Compared
to our WMT 2009 submission, we report
a gain of 1.73 BLEU by using the new
features and decoding environment, and a
gain of up to 0.52 BLEU from improved
grammar selection.
1 Introduction
From its earlier focus on linguistically rich ma-
chine translation for resource-poor languages, the
statistical transfer MT group at Carnegie Mellon
University has expanded in recent years to the in-
creasingly successful domain of syntax-based sta-
tistical MT in large-data scenarios. Our submis-
sion to the 2010 Workshop on Machine Transla-
tion is a syntax-based SMT system with a syn-
chonous context-free grammar (SCFG), where the
SCFG rules are derived from full constituency
parse trees on both the source and target sides of
parallel training sentences. We participated in the
French-to-English shared translation task.
This year, we focused our efforts on making
more and better use of syntactic grammar. Much
of the work went into formulating a more expan-
sive feature set in the translation model and a new
method of assigning scores to phrase pairs and
grammar rules. Following a change of decoder
that allowed us to experiment with systems using
much larger syntactic grammars than previously,
we also adapted a technique to more intelligently
pre-filter grammar rules to those most likely to be
useful.
2 System Overview
We built our system on a partial selection of
the provided French?English training data, us-
ing the Europarl, News Commentary, and UN
sets, but ignoring the Giga-FrEn data. After
tokenization and some pruning of our training
data, this left us with a corpus of approximately
8.6 million sentence pairs. We word-aligned the
corpus with MGIZA++ (Gao and Vogel, 2008),
a multi-threaded implementation of the standard
word alignment tool GIZA++ (Och and Ney,
2003). Word alignments were symmetrized with
the ?grow-diag-final-and? heuristic. We automati-
cally parsed the French side of the corpus with the
Berkeley parser (Petrov and Klein, 2007), while
we used the fast vanilla PCFG model of the Stan-
ford parser (Klein and Manning, 2003) for the
English side. These steps resulted in a parallel
parsed corpus from which to extract phrase pairs
and grammar rules.
Phrase extraction involves three distinct steps.
In the first, we perform standard (non-syntactic)
phrase extraction according to the heuristics of
phrase-based SMT (Koehn et al, 2003). In the
second, we obtain syntactic phrase pairs using
the tree-to-tree matching method of Lavie et al
(2008). Briefly, this method aligns nodes in par-
allel parse trees by projecting up from the word
alignments. A source-tree node s will be aligned
to a target-tree node t if the word alignments in the
yield of s all land within the yield of t, and vice
versa. This node alignment is similar in spirit to
the subtree alignment method of Zhechev and Way
(2008), except our method is based on the spe-
cific Viterbi word alignment links found for each
82
sentence rather than on the general word trans-
lation probabilities computed for the corpus as a
whole. This enables us to use efficient dynamic
programming to infer node alignments, rather than
resorting to a greedy search or the enumeration of
all possible alignments. Finally, in the third step,
we use the node alignments from syntactic phrase
pair extraction to extract grammar rules. Each
aligned node in a tree pair specifies a decompo-
sition point for breaking the parallel trees into a
series of SCFG rules. Like Galley et al (2006),
we allow ?composed? (non-minimal) rules when
they build entirely on lexical items. However, to
control the size of the grammar, we do not produce
composed rules that build on other non-terminals,
nor do we produce multiple possible rules when
we encounter unaligned words. Another differ-
ence is that we discard internal structure of com-
posed lexical rules so that we produce SCFG rules
rather than synchronous tree substitution grammar
rules.
The extracted phrase pairs and grammar rules
are collected together and scored according to a
variety of features (Section 3). Instead of decod-
ing with the very large complete set of extracted
grammar rules, we select only a small number of
rules meeting certain criteria (Section 4).
In contrast to previous years, when we used the
Stat-XFER decoder, this year we switched to the
the Joshua decoder (Li et al, 2009) to take advan-
tage of its more efficient architecture and imple-
mentation of modern decoding techniques, such as
cube pruning and multi-threading. We also man-
aged system-building workflows with LoonyBin
(Clark and Lavie, 2010), a toolkit for managing
multi-step experiments across different servers or
computing clusters. Section 5 details our experi-
mental results.
3 Translation Model Construction
One major improvement in our system this year
is the feature scores we applied to our grammar
and phrase pairs. Inspired largely by the Syntax-
Augmented MT system (Zollmann and Venu-
gopal, 2006), our translation model contains 22
features in addition to the language model. In con-
trast to earlier formulations of our features (Han-
neman and Lavie, 2009), our maximum-likelihood
features are now based on a strict separation be-
tween counts drawn from non-syntactic phrase ex-
traction heuristics and our syntactic rule extractor;
no feature is estimated from counts in both spaces.
We define an aggregate rule instance as a 5-
tuple r = (L,S, T,Cphr, Csyn) that contains a
left-hand-side label L, a sequence of terminals
and non-terminals for the source (S) and target
(T ) right-hand sides, and aggregated counts from
phrase-based SMT extraction heuristics Cphr and
the syntactic rule extractor Csyn.
In preparation for feature scoring, we:
1. Run phrase instance extraction using stan-
dard phrase-based SMT heuristics to obtain
tuples (PHR, S, T,Cphr, ?) where S and T
never contain non-terminals
2. Run syntactic rule instance extraction as de-
scribed in Section 2 above to obtain tuples
(L,S, T, ?, Csyn)
3. Share non-syntactic counts such that, for
any two tuples r1 = (PHR, S, T,Cphr, ?)
and r2 = (L2, S, T, ?, Csyn) with equiv-
alent S and T values, we produce r2 =
(L2, S, T,Cphr, Csyn)
Note that there is no longer any need to retain
PHR rules (PHR, S, T ) that have syntactic equiv-
alents (L 6= PHR, S, T ) since they have the same
features In addition, we assume there will be no
tuples where S and T contain non-terminals while
Cphr = 0 and Csyn > 0. That is, the syntactic
phrases are a subset of non-syntactic phrases.
3.1 Maximum-Likelihood Features
Our most traditional features are Pphr(T |S) and
Pphr(S |T ), estimated using only counts Cphr.
These features apply only to rules not con-
taining any non-terminals. They are equiva-
lent to the phrase P (T |S) and P (S |T ) fea-
tures from the Moses decoder, even when L 6=
PHR. In contrast, we used Psyn?phr(L,S |T ) and
Psyn?phr(L, T |S) last year, which applied to all
rules. The new features are no longer subject to
increased sparsity as the number of non-terminals
in the grammar increases.
We also have grammar rule probabili-
ties Psyn(T |S), Psyn(S |T ), Psyn(L |S),
Psyn(L |T ), and Psyn(L |S, T ) estimated using
Csyn; these apply only to rules where S and T
contain non-terminals. By no longer including
counts from phrase-based SMT extraction heuris-
tics in these features, we encourage rules where
L 6= PHR since the smaller counts from the rule
learner would have otherwise been overshadowed
83
by the much larger counts from the phrase-based
SMT heuristics.
Finally, we estimate ?not labelable? (NL) fea-
tures Psyn(NL |S) and Psyn(NL |T ). With R de-
noting the set of all extracted rules,
Psyn(NL |S) =
Csyn
?
r??R s.t. S?=S C ?syn
(1)
Psyn(NL |T ) =
Csyn
?
r??R s.t. T ?=T C ?syn
(2)
We use additive smoothing (with n = 1 for our ex-
periments) to avoid a probability of 0 when there
is no syntactic label for an (S, T ) pair. These fea-
tures can encourage syntactic rules when syntax
is likely given a particular string since probability
mass is often distributed among several different
syntactic labels.
3.2 Instance Features
We add several features that use sufficient statis-
tics local to each rule. First, we add three binary
low-count features that take on the value 1 when
the frequency of the rule is exactly 1, 2, or 3. There
are also two indicator features related to syntax:
one each that fires when L = PHR and when
L 6= PHR. Other indicator features analyze the
abstractness of grammar rules: AS = 1 when the
source side contains only non-terminals, AT = 1
when the target side contains only non-terminals,
TGTINSERTION = 1 when AS = 1, AT = 0,
SRCDELETION = 1 when AS = 0, AT = 1, and
INTERLEAVED = 1 when AS = 0, AT = 0.
Bidirectional lexical probabilities for each rule
are calculated from a unigram lexicon MLE-
estimated over aligned word pairs in the training
corpus, as is the default in Moses.
Finally, we include a glue rule indicator feature
that fires whenever a glue rule is applied during
decoding. In the Joshua decoder, these monotonic
rules stitch syntactic parse fragments together at
no model cost.
4 Grammar Selection
With extracted grammars typically reaching tens
of millions of unique rules ? not to mention
phrase pairs ? our systems clearly face an en-
gineering challenge when attempting to include
the full grammar at decoding time. Iglesias et al
(2009) classified SCFG rules according to the pat-
tern of terminals and non-terminals on the rules?
right-hand sides, and found that certain patterns
could be entirely left out of the grammar without
loss of MT quality. In particular, large classes of
monotonic rules could be removed without a loss
in automatic metric scores, while small classes of
reordering rules contributed much more to the suc-
cess of the system. Inspired by that approach, we
passed our full set of extracted grammar rule in-
stances through a filter after scoring. Using the
rule notation from Section 3, the filter retained
only those rules that matched one of the follow-
ing patterns:
S = X1 w, T = w X1
S = w X1, T = X1 w
S = X1 X2, T = X2 X1
S = X1 X2, T = X1 X2
where X represents any non-terminal and w rep-
resents any span of one or more terminals. The
choice of the specific reordering patterns above
captures our intuition that binary swaps are a fun-
damental ordering divergence between languages,
while the inclusion of the abstract monotonic pat-
tern (X1 X2,X1 X2) ensures that the decoder is
not disproportionately biased towards applying re-
ordering rules without supporting lexical evidence
merely because in-order rules are left out.
Orthogonally to the pattern-based pruning, we
also selected grammars by sorting grammar rules
in decreasing order of frequency count and using
the top n in the decoder. We experimented with
n = 0, 100, 1000, and 10,000. In all cases of
grammar selection, we disallowed rules that in-
serted unaligned target-side terminals unless the
inserted terminals were among the top 100 most
frequent unigrams in the target-side vocabulary.
5 Results and Analysis
5.1 Comparison with WMT 2009 Results
We performed our initial development work on
an updated version of our previous WMT sub-
mission (Hanneman et al, 2009) so that the ef-
fects of our changes could be directly compared.
Our 2009 system was trained from the full Eu-
roparl and News Commentary data available that
year, plus the pre-release version of the Giga-FrEn
data, for a total of 9.4 million sentence pairs. We
used the news-dev2009a set for minimum error-
rate training and tested system performance on
news-dev2009b. To maintain continuity with our
previously reported scores, we report new scores
here using the same training, tuning, and test-
ing sets, using the uncased versions of IBM-style
84
System Configuration METEOR BLEU
1. WMT ?09 submission 0.5263 0.2073
2. Joshua decoder 0.5231 0.2158
3. New TM features 0.5348 0.2246
Table 1: Dev test results (on news-dev2009b) from
our WMT 2009 system when updating decoding
environment and feature formulations.
System Configuration METEOR BLEU
1. n = 100 0.5314 0.2200
2. n = 100, filtered 0.5341 0.2242
3. n = 1000 0.5324 0.2206
4. n = 1000, filtered 0.5330 0.2233
5. n = 10,000 0.5332 0.2198
6. n = 10,000, filtered 0.5350 0.2250
Table 2: Dev test results (on news-dev2009b) from
our WMT 2009 system with and without pattern-
based grammar selection.
BLEU 1.04 (Papineni et al, 2002) and METEOR
0.6 (Lavie and Agarwal, 2007).
Table 1 shows the effect of our new scoring and
decoding environment. Line 2 uses the same ex-
tracted phrase pairs and grammar rules as line 1,
but the system is tuned and tested with the Joshua
decoder instead of Stat-XFER. For line 3, we re-
scored the extracted phrase pairs from lines 1 and
2 using the updated features discussed in Sec-
tion 3.1 The difference in automatic metric scores
shows a significant benefit from both the new de-
coder and the updated feature formulations: 0.8
BLEU points from the change in decoder, and 0.9
BLEU points from the expanded set of 22 transla-
tion model features.
Our next test was to examine the usefulness of
the pattern-based grammar selection described in
Section 4. For various numbers of rules n, Ta-
ble 2 shows the scores obtained with and without
filtering the grammar before the n most frequent
rules are skimmed off for use. We observe a small
but consistent gain in scores from the grammar se-
lection process, up to half a BLEU point in the
largest-grammar systems (lines 5 and 6).
1In line 2, we did not control for difference in formulation
of the translation length feature: Stat-XFER uses a length
ratio, while Joshua uses a target word count. Line 3 does
not include 26 manually selected grammar rules present in
lines 1 and 2; this is because our new feature scoring requires
information from the grammar rules that was not present in
our 2009 extracted resources.
Source Target
un ro?le AP1 ADJP1 roles
l? instabilite? AP1 ADJP1 instability
l? argent PP1 NP1 money
une pression AP1 ADJP1 pressure
la gouvernance AP1 ADJP1 governance
la concurrence AP1 ADJP1 competition
des preuves AP1 ADJP1 evidence
les outils AP1 ADJP1 tools
des changements AP1 ADJP1 changes
Table 3: Rules fitting the pattern (S = w X1, T =
X1 w) that applied on the news-test2010 test set.
5.2 WMT 2010 Results and Analysis
We built the WMT 2010 version of our system
from the training data described in Section 2. (The
system falls under the strictly constrained track:
we used neither the Giga-FrEn data for training
nor the LDC Gigaword corpora for language mod-
eling.) We used the provided news-test2008 set
for system tuning, while news-test2009 served
as our 2010 dev test set. Based on the results
in Table 2, our official submission to this year?s
shared task was constructed as in line 6, with
10,000 syntactic grammar rules chosen after a
pattern-based grammar selection step. On the
news-test2010 test set, this system scored 0.2327
on case-insensitive IBM-style BLEU 1.04, 0.5614
on METEOR 0.6, and 0.5519 on METEOR 1.0
(Lavie and Denkowski, 2009).
The actual application of grammar rules in the
system is quite surprising. Despite having a gram-
mar of 10,000 rules at its disposal, the decoder
chose to only apply a total of 20 unique rules
in 392 application instances in the 2489-sentence
news-test2010 set. On a per-sentence basis, this
is actually fewer rule applications than our sys-
tem performed last year with a 26-rule handpicked
grammar! The most frequently applied rules are
fully abstract, monotonic structure-building rules,
such as for stitching together compound noun
phrases with adverbial phrases or prepositional
phrases. Nine of the 20 rules, listed in Table 3,
demonstrate the effect of our pattern-based gram-
mar selection. These partially lexicalized rules fit
the pattern (S = w X1, T = X1 w) and han-
dle cases of lexicalized binary reordering between
French and English. Though the overall impact of
these rules on automatic metric scores is presum-
85
ably quite small, we believe that the key to effec-
tive syntactic grammars in our MT approach lies
in retaining precise rules of this type for common
linguistically motivated reordering patterns.
The above pattern of rule applications is also
observed in our dev test set, news-test2009, where
16 distinct rules apply a total of 352 times. Seven
of the fully abstract rules and three of the lexical-
ized rules that applied on news-test2009 also ap-
plied on news-test2010, while a further two ab-
stract and four lexicalized rules applied on news-
test2009 alone. We thus have a general trend of a
set of general rules applying with higher frequency
across test sets, while the set of lexicalized rules
used varies according to the particular set.
Since, overall, we still do not see as much gram-
mar application in our systems as we would like,
we plan to concentrate future work on further im-
proving this aspect. This includes a more detailed
study of grammar filtering or refinement to select
the most useful rules. We would also like to ex-
plore the effect of the features of Section 3 individ-
ually, on different language pairs, and using differ-
ent grammar types.
Acknowledgments
This research was supported in part by NSF grant
IIS-0534217 (LETRAS) and the DARPA GALE
program. We thank Yahoo! for the use of the M45
research computing cluster, where we ran many
steps of our experimental pipeline.
References
Jonathan Clark and Alon Lavie. 2010. LoonyBin:
Keeping language technologists sane through
automated management of experimental (hy-
per)workflows. In Proceedings of the Seventh
International Language Resources and Evaluation
(LREC ?10), Valletta, Malta, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman and Alon Lavie. 2009. Decoding
with syntactic and non-syntactic phrases in a syntax-
based machine translation system. In Proceedings of
the Third Workshop on Syntax and Structure in Sta-
tistical Translations, pages 1?9, Boulder, CO, June.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An improved
statistical transfer systems for French?English ma-
chine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
140?144, Athens, Greece, March.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the ACL, pages 380?388, Athens, Greece, March?
April.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3?10. MIT Press,
Cambridge, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 48?54, Ed-
monton, Alberta, May?June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In Proceed-
ings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 135?139, Athens, Greece,
March.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
86
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics, pages 1105?1112, Manchester,
England, August.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, NY,
June.
87
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 98?106,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Automatic Category Label Coarsening for Syntax-Based Machine
Translation
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, alavie}@cs.cmu.edu
Abstract
We consider SCFG-basedMT systems that get
syntactic category labels from parsing both
the source and target sides of parallel train-
ing data. The resulting joint nonterminals of-
ten lead to needlessly large label sets that are
not optimized for an MT scenario. This pa-
per presents a method of iteratively coarsening
a label set for a particular language pair and
training corpus. We apply this label collaps-
ing on Chinese?English and French?English
grammars, obtaining test-set improvements of
up to 2.8 BLEU, 5.2 TER, and 0.9 METEOR
on Chinese?English translation. An analysis
of label collapsing?s effect on the grammar
and the decoding process is also given.
1 Introduction
A common modeling choice among syntax-based
statistical machine translation systems is the use of
synchronous context-free grammar (SCFG), where a
source-language string and a target-language string
are produced simultaneously by applying a series of
re-write rules. Given a parallel corpus that has been
statistically word-aligned and annotated with con-
stituency structure on one or both sides, SCFG mod-
els for MT can be learned via a variety of methods.
Parsing may be applied on the source side (Liu et al,
2006), on the target side (Galley et al, 2004), or on
both sides of the parallel corpus (Lavie et al, 2008;
Zhechev and Way, 2008).
In any of these cases, using the raw label set from
source- and/or target-side parsers can be undesir-
able. Label sets used in statistical parsers are usu-
ally inherited directly from monolingual treebank
projects, where the inventory of category labels was
designed by independent teams of human linguists.
These labels sets are not necessarily ideal for sta-
tistical parsing, let alne for bilingual syntax-based
translation models. Further, the side(s) on which
syntax is represented defines the nonterminal label
space used by the resulting SCFG. A pair of aligned
adjectives, for example, may be labeled ADJ if only
source-side syntax is used, JJ if only target-side syn-
tax is used, or ADJ::JJ if syntax from both sides
is used in the grammar. Beyond such differences,
however, most existing SCFG-based MT systems
do not further modify the nonterminal label set in
use. Those that do require either specialized de-
coders or complicated parameter tuning, or the la-
bel set may be unsatisfactory from a computational
point of view (Section 2).
We believe that representing both source-side and
target-side syntax is important. Even assuming two
monolingually perfect label sets for the source and
target languages, using label information from only
one side ignores any meaningful constraints ex-
pressed in the labels of the other. On the other hand,
using the default node labels from both sides gener-
ates a joint nonterminal set of thousands of unique
labels, not all of which may be useful. Our real pref-
erence is to use a joint nonterminal set adapted to
our particular language pair or translation task.
In this paper, we present the first step towards
a tailored label set: collapsing syntactic categories
to remove the most redundant labels and shrink the
overall source?target nonterminal set.1 There are
1The complementary operation, splitting existing labels, is
beyond the scope of this paper and is left for future work.
98
two problems with an overly large label set:
First, it encourages labeling ambiguity among
rules, a well-known practical problem in SCFG-
based MT. Most simply, the same right-hand side
may be observed in rule extraction with a variety of
left-hand-side labels, each leading to a unique rule
in the grammar. The grammar may further contain
many rules with the same structure and reordering
pattern that differ only with respect to the actual la-
bels in use. Together, these properties can cause an
SCFG-based MT system to process a large number
of alternative syntactic derivations that use different
rules but produce identical output strings. Limiting
the possible number of variant labelings cuts down
on ambiguous derivations.
Second, a large label set leads to rule sparsity. A
rule whose right-hand side can only apply on a very
tightly specified set of labels is unlikely to be es-
timated reliably from a parallel corpus or to apply
in all needed cases at test time. However, a coarser
version of its application constraints may be more
frequently observed in training data and more likely
to apply on test data.
We therefore introduce a method for automati-
cally clustering and collapsing category labels, on
either one or both sides of SCFG rules, for any lan-
guage pair and choice of statistical parsers (Section
3). Turning to alignments between source and tar-
get parse nodes as an additional source of informa-
tion, we calculate a distance metric between any
two labels in one language based on the difference
in alignment probabilities to labels in the other lan-
guage. We then apply a greedy label collapsing al-
gorithm that repeatedly merges the two labels with
the closest distance until some stopping criterion is
reached. The resulting coarsened labels are used in
the SCFG rules of a syntactic machine translation
system in place of the original labels.
In experiments on Chinese?English translation
(Section 4), we find significantly improved perfor-
mance of up to 2.8 BLEU points, 5.2 TER points,
and 0.9 METEOR points by applying varying de-
grees of label collapsing to a baseline syntax-based
MT system (Section 5). In our analysis of the results
(Section 6), we find that the largest immediate effect
of coarsening the label set is to reduce the number of
fully abstract hierarchical SCFG rules present in the
grammar. These rules? increased permissiveness, in
turn, directs the decoder?s search into a largely dis-
joint realm from the search space explored by the
baseline system. A full summary and ideas for fu-
ture work are given in Section 7.
2 Related Work
One example of modifying the SCFG nonterminal
set is seen in the Syntax-Augmented MT (SAMT)
system of Zollmann and Venugopal (2006). In
SAMT rule extraction, rules whose left-hand sides
correspond exactly to a target-side parse node t re-
tain that label in the grammar. Additional nontermi-
nal labels of the form t1+ t2 are created for rules
spanning two adjacent parse nodes, while catego-
rial grammar?style nonterminals t1/t2 and t1\t2 are
used for rules spanning a partial t1 node that is miss-
ing a t2 node to its right or left.
These compound nonterminals in practice lead to
a very large label set. Probability estimates for rules
with the same structure up to labeling can be com-
bined with the use of a preference grammar (Venu-
gopal et al, 2009), which replaces the variant label-
ings with a single SCFG rule using generic ?X? la-
bels. The generic rule?s ?preference? over possible
labelings is stored as a probability distribution inside
the rule for use at decoding time. Preference gram-
mars thus reduce the label set size to one for the pur-
poses of some feature calculations ? which avoids
the fragmentation of rule scores due to labeling am-
biguity ? but the original labels persist for specify-
ing which rules may combine with which others.
Chiang (2010) extended SAMT-style labels to
both source- and target-side parses, also introducing
a mechanism by which SCFG rules may apply at run
time even if their labels do not match. Under Chi-
ang?s soft matching constraint, a rule headed by a la-
bel A::Z may still plug into a substitution site labeled
B::Y by paying additional model costs substB?A
and substY?Z . This is an on-the-fly method of
coarsening the effective label set on a case-by-case
basis. Unfortunately, it also requires tuning a sep-
arate decoder feature for each pair of source-side
and each pair of target-side labels. This tuning can
become prohibitively complex when working with
standard parser label sets, which typically contain
between 30 and 70 labels on each side.
99
JJ JJR JJS
Figure 1: Alignment distributions over French labels for the English adjective labels JJ, JJR, and JJS.
3 Label Collapsing Algorithm
We begin with an initial set of SCFG rules extracted
from a parallel parsed corpus, where S denotes the
set of labels used on the source side and T denotes
the set of labels used on the target side. Each rule has
a left-hand side of the form s :: t, where s ? S and
t ? T , meaning that a node labeled s was aligned to
a node labeled t in a parallel sentence. From the left-
hand sides of all extracted rule instances, we com-
pute label alignment distribution P (s | t) by simple
counting and normalizing:
P (s | t) =
#(s :: t)
#(t)
(1)
We use an analogous equation to calculate P (t | s).
For two target-language labels t1 and t2, we have
an equally simple metric of alignment distribution
difference d: the total of the absolute differences in
likelihood for each aligned source-language label.
d(t1, t2) =
?
s?S
|P (s | t1) ? P (s | t2)| (2)
Again, the calculation for d(s1, s2) is analogous.
If t1 and t2 are plotted as points in |S|-
dimensional space such that each point?s position in
dimension s is equal to P (s | t), then this metric is
equivalent to the L1 distance between t1 and t2.
Sample alignment distributions into French for
three English adjective labels are shown in Figure
1. Bars in the chart represent alignment probabili-
ties between French and English according to Equa-
tion 1, with the various French labels as s and JJ,
JJR, or JJS as t. To compute an L1 alignment dis-
tribution difference between a pair of English ad-
jective tags, we sum the absolute differences in bar
heights for each column of two graphs, as in Equa-
tion 2. It is already visually clear from Figure 1
that all three English labels are somewhat related
in terms of distribution, but it appears that JJR and
JJS are more closely related to each other than either
is to JJ. This is reflected in the actual L1 distances:
d(JJ, JJR) = 0.9941 and d(JJ, JJS) = 0.8730, but
d(JJR, JJS) = 0.3996.
Given the above method for computing an align-
ment distribution difference for any pair of labels,
we develop an iterative greedy method for label col-
lapsing. At each step, we compute the L1 distance
between all pairs of labels, then collapse the pair
with the smallest distance into a single label. Then
L1 distances are recomputed over the new, smaller
label set, and again the label pair with the smallest
distance is collapsed. This process continues until
some stopping criterion is reached. Label pairs be-
ing considered for collapsing may be only source-
side labels, only target-side labels, or both. In gen-
eral, we choose to allow label collapsing to apply on
either side during each iteration of our algorithm.
In the limit, label collapsing can be applied it-
eratively until all syntactic categories on both the
source and target sides have been collapsed into a
single label. In Section 5, we explore several earlier
and more meaningful stopping points.
4 Experimental Setup
Experiments are conducted on Chinese-to-English
translation using approximately 300,000 sentence
pairs from the FBIS corpus. To obtain parse trees
over both sides of each parallel corpus, we used
the English and Chinese grammars of the Berkeley
100
parser (Petrov and Klein, 2007).
Given a parsed and word-aligned parallel sen-
tence, we extract SCFG rules from it following the
procedure of Lavie et al (2008). The method first
identifies node alignments between the two parse
trees according to support from the word alignments.
A node in the source parse tree will be aligned to
a node in the target parse tree if all the words in
the yield of the source node are either all aligned to
words within the yield of the target node or have no
alignments at all. Then SCFG rules can be extracted
from adjacent levels of aligned nodes, which spec-
ify points at which the tree pair can be decomposed
into minimal SCFG rules. In addition to producing
a minimal rule, each decomposition point also pro-
duces a phrase pair rule with the node pair?s yields
as the right-hand side, as long as the length of the
yield is less than a specified threshold.
Following grammar extraction, labels are option-
ally clustered and collapsed according to the algo-
rithm in Section 3. The grammar is re-written with
the modified nonterminals, then scored as usual ac-
cording to our translation model features. Feature
weights themselves are learned via minimum error
rate training as implemented in Z-MERT (Zaidan,
2009) with the BLEU metric (Papineni et al, 2002).
Decoding is carried out with Joshua (Li et al, 2009),
an open-source platform for SCFG-based MT.
Due to engineering limitations in decoding with
a large grammar, we apply three additional error-
correction and filtering steps to every system. First,
we observed that the syntactic parsers were most
likely to make labeling errors for cardinal numbers
in English and punctuation marks in all languages.
We thus post-process the parses of our training data
to tag all English cardinal numbers as CD and to
overwrite the labels of various punctuation marks
with the correct labels as defined by each language?s
label set. Second, after rule extraction, we com-
pute the distribution of left-hand-side labels for each
unique labeled right-hand side in the grammar, and
we remove the labels in the least frequent 10% of the
distribution. This puts a general-purpose limit on la-
beling ambiguity. Third, we filter and prune the final
scored grammar to each individual development and
test set before decoding: all matching phrase pairs
are retained, along with the most frequent 10,000 hi-
erarchical grammar rules.
5 Experiments and Results
In our first set of experiments, we sought to explore
the effect of increasing degrees of label collapsing
on a baseline system and to determine a reasonable
stopping point. Starting with the baseline grammar,
we ran the label collapsing algorithm of Section 3
until all the constituent labels on each side had been
collapsed into a single category. We next examined
the L1 distances between the label pairs that had
been merged in each iteration of the algorithm. This
data is shown in Figure 2 as a plot of L1 distance
versus iteration number. The distances between the
successive labels merged in the first 29 iterations of
the algorithm are nearly monotonically increasing,
followed by a much larger discontinuity at iteration
30. Similar patterns emerge for iterations 30 to 45
and for iterations 46 to 60. The next regions of the
graph, from iterations 61 to 81 and from iterations
82 to 99, show an increasing prevalence of disconti-
nuities. Finally, from iterations 100 to 123, the suc-
cessive L1 distances entirely alternate between very
high and very low values.
Discontinuities are merely the result of a label
pair in one language suddenly scoring much lower
on the distribution difference metric than previously,
thanks to some change that has occurred in the la-
bel set of the other language. Looking back to Fig-
ure 1, for example, we could bring the distributions
for JJ and JJS much closer together by merging A
and ADV on the French side. Although such sudden
drops in distribution difference value are expected,
they may provide an indication of when the label
collapsing algorithm has progressed too far, since
we have so reduced the label set that categories pre-
viously very different have become much less dis-
tinguishable. On the other hand, further reduction of
the label set may have a variety of pratical benefits.
We tested this trade-off empirically by building
five Chinese?English MT systems, each exhibiting
an increasing degree of label collapsing compared to
the original label set, which serves as our baseline.
The degree of label collapsing in each of the five
systems corresponds to one of the major discontinu-
ity features highlighted in the right-hand side Figure
2. The systems were tuned on the NIST MT 2006
data set, and we evaluated performance on the NIST
MT 2003 and 2008 sets. (All data sets have four
101
Iter. L1 Dist.
29 0.3646
45 0.5607
60 0.6155
81 0.8665
99 1.1303
Figure 2: Observed L1 distance values for the labels merged in each iteration of our algorithm on a Chinese?English
SCFG. We divide the graph into six distinct regions using the cutoffs at right.
Chinese?English MT 2003 Test Set MT 2008 Test Set
System METEOR BLEU TER METEOR BLEU TER
Baseline 54.35 24.39 68.01 45.68 18.27 69.18
Collapsed, 29 iterations 55.24 27.03 63.77 46.25 19.78 65.88
Collapsed, 45 iterations 54.65 26.69 62.76 46.02 19.60 64.88
Collapsed, 60 iterations 55.11 27.23 63.06 46.30 20.19 65.18
Collapsed, 81 iterations 54.87 26.87 64.92 45.70 20.48 66.75
Collapsed, 99 iterations 54.86 26.16 64.17 45.87 19.52 65.61
Table 1: Results of applying increasing degrees of label collapsing on our Chinese?English baseline system. Bold
figures indicate the best score in each column.
references.) Table 1 reports automatic metric results
for version 1.0 of METEOR (Lavie and Denkowski,
2009) using the default settings, uncased IBM-style
BLEU (Papineni et al, 2002), and uncased TER ver-
sion 0.7 (Snover et al, 2006).
No matter the degree of label collapsing, we find
significant improvements in BLEU and TER scores
on both test sets. On the MT 2003 set, label-
collapsed systems score 1.77 to 2.84 BLEU points
and 3.09 to 5.25 TER points better than the baseline.
OnMT 2008, improvements range from 1.25 to 2.21
points on BLEU and from 2.43 to 4.30 points on
TER. Improvements on both sets according to ME-
TEOR, though smaller, are still noticable (up to 0.89
points). In the case of BLEU, we verified the sig-
nificance of the improvements by conducting paired
bootstrap resampling (Koehn, 2004) on theMT 2003
output. With n = 1000 and p < 0.05, all five label-
collapsed systems were statistically significant im-
provements over the baseline, and all other collapsed
systems were significant improvements over the 99-
iteration system.
Thus, though the system that provides the highest
score changes across metrics and test sets, the over-
all pattern of scores suggests that over-collapsing la-
bels may start to weaken results. A more moderate
stopping point is thus preferable, but beyond that we
suspect the best result is determined more by the test
set, automatic metric choice, and MERT instability
than systematic changes in the label set.
6 Analysis
Table 1 showed a strong practical benefit to running
the label collapsing algorithm. In this section, we
102
seek to further understand where this benefit comes
from, tracing the effects of label collapsing via its
modification of labels themselves, the differences in
the resulting grammars, and collapsing?s effect on
decoding and output.
6.1 Labels Selected for Collapsing
Our first concern is for the size of the grammar?s
overall nonterminal set. The baseline system uses a
total of 55 labels on the Chinese side and 71 on the
English side, leading to an observed joint nontermi-
nal set of 1556 unique labels. After 29 iterations
of label collapsing, this is reduced to 46 Chinese,
51 English, and 1035 joint labels ? a reduction of
33%. In the grammar of our most collapsed gram-
mar variant (99 iterations), the nonterminal set is re-
duced to 14 English and 14 Chinese labels, for a to-
tal of 106 joint labels and a reduction of 93% from
the baseline grammar. This demonstrates one facet
of our introductory claim from Section 1: since we
have improved translation results by removing the
vast majority of our grammar nonterminals, most of
the initial joint Chinese?English syntactic categories
were not necessary for Chinese?English translation.
We identify three broad trends in the sets of labels
that are collapsed:
? Full Subtype Collapsing. The Chinese-side
parses include six phrase-level tags for various
types of verb compounds. As label collapsing
progresses, these labels are all combined with
each other at relatively low L1 distances.
? Partial Subtype Collapsing. In English, three
of the four noun labels (NN, NNS, and NNPS)
form a cohesive cluster early on in Chinese?
English collapsing. However, the fourth tag
(NNP, for singular proper nouns) remains sep-
arate, then later joins a cluster for more
adjective-like labels.
? Combination by Syntactic Function. In
French?English label collapsing (see below),
we find the creation of a combined label in
English for reduced relative clauses (RRC),
adjective phrases headed by a wh-adjective
(WHADJP), and interjections (INTJ). Even
though these tags are unrelated in surface form,
at some level they all represent parenthetical in-
sertions or explanatory phrases.
The formulation of the L1 distance metric in Sec-
tion 3 means that our label collapsing algorithm will
naturally produce different label clusters for differ-
ent input grammars ? any change in the Viterbi
word alignments, underlying parallel corpus, initial
label set, or choice of automatic parser will neces-
sarily change the label alignment distributions on
which the collapsing algorithm is based. In par-
ticular, the label clusters formed in one language
are likely to be markedly different depending on
which other language it is paired with. We exam-
ine these differences in more detail for the case of
English when paired with either Chinese or with
French. Our 29-iteration run of label collapsing for
Chinese?English merged labels on the English side
19 times. For an exact comparison, we run iterations
of label collapsing on a large-scale French?English
grammar, extracted in the same way as the Chinese?
English grammar, until the same number of English-
side merges have been carried out, then examine the
results.
Table 2 shows the English label clusters cre-
ated from the Chinese?English and French?English
grammars, arranged by broad syntactic categories.
The differences in English label clusters hint at dif-
ferences in the source-side label sets, as well as
structural divergences relevant for translating Chi-
nese versus French into English.
For example, Table 2 shows partial subtype col-
lapsing of the English verb tags when paired with
French. The French Berkeley parser has a single tag,
V, to represent all verbs, and most English verb tags
as well as the tag for modals very consistently align
to it. The exception is VBG, for present-progressive
or gerundive verb forms, which is more easily con-
flatable in French?English translation with a noun or
an adjective. In translation from Chinese, however,
it is VBG that is combined early on with a smaller
selection of English verb labels that correspond most
strongly to a basic Chinese verb. Other English verb
tags are more likely to align to Chinese copulas, ex-
istential verbs, and nouns; they are not combined
with the group for more ?typical? verbs until itera-
tion 67. The adverb series presents another example
of translational divergence between language pairs.
103
Cluster Chinese?English French?English
Nouns NN NNS NNPS # NN NNS $
Verbs VB VBG VBN VB VBD VBN VBP VBZ MD
Adverbs RB RBR RBR RBS
Punctuation LRB RRB ? ? , . ? ?
Prepositions IN TO SYM
Determiners DT PRP$
Noun phrases NP NX QP UCP NAC NP WHNP NX WHADVP NAC
Adjective phrases ADJP WHADJP
Adverb phrases ADVP WHADVP
Prepositional phrases PP WHPP
Sentences S SINV SBARQ FRAG S SQ SBARQ
Table 2: English-side label clusters created after partial label collapsing of a Chinese?English and a French?English
grammar. In each case, the algorithm has been run until merges have occurred 19 times on the English side.
6.2 Effect on the Grammar
With a smaller label set, we also expect a reduc-
tion in the overall size of our various label-collapsed
grammars as labeling ambiguity is removed. In the
aggregate, however, even 99 iterations of Chinese?
English label collapsing has a minimal effect on
the total number of unique rules in the resulting
SCFG. A clearer picture emerges when we sepa-
rate rules according to their form. Figure 3 parti-
tions the grammar into three parts: one for phrase
pairs, where the rules? right-hand sides are made up
entirely of terminals (?P-type? rules); one for hier-
archical rules whose right-hand sides are made up
entirely of nonterminals (abstract or ?A-type? rules);
and one for hierarchical rules whose right-hand sides
include a mix of terminals and nonterminals (re-
maining grammar or ?G-type? rules).
This separation reveals two interesting facts.
First, although the size of the label set continues
to shrink considerably between iterations 29 and 81,
the number of unique rules in the grammar remains
relatively unchanged. Second, the reduction in the
size of the grammar is largely due to a reduction in
the number of fully abstract grammar rules, rather
than phrase pairs or partially lexicalized grammar
rules. From these observations, we infer that the ma-
jor practical benefit of label collapsing is a reduction
in rule sparsity rather than a reduction in left-hand-
side labeling ambiguity. Many highly ambiguous
rules have had their possible left-hand-side labels ef-
fectively pruned down by the pre-processing steps
we described in Section 4, which in preliminary ex-
Figure 3: The effect of label collapsing on the number of
unique phrase pairs, partially lexicalized grammar rules,
and fully abstract grammar rules.
periments had a larger effect on the overall size of
the grammar than label collapsing. As a more com-
plementary technique, increasing the applicability of
the fully abstract rules via label collapsing is impor-
tant for performance. Such rules make up 49% to
59% of the hierarchical rules retained at decoding
time, and they account for 76% to 87% of the rule
application instances on the MT 2003 test set.
6.3 Effect on Decoding and Output
Interestingly, the label collapsing algorithm does
not owe its success at decoding time to a signif-
icant increase in the number of rule applications.
Among our systems, both the 45-iteration and the
104
60-iteration collapsed versions scored highly ac-
cording to automatic metrics. Nevertheless, the 45-
iteration system used 32% and 38% more rule appli-
cations than the baseline on the MT 2003 and MT
2008 test sets, respectively, while the 60-iteration
system used 15% and 11% fewer. The number of
unique rule types and the number of reordering rules
applied on a test set may also go up or down.
Instead, the practical effect of making the gram-
mar more permissive seems to be a significant
change in the search space explored during decod-
ing. This can be seen superficially via an exam-
ination of output n-best lists. On both test sets
combined (2276 sentences), the 60-iteration label-
collapsed system?s top-best output appears in the
baseline?s 100-best list in only 81 sentences. When
it does appear in the baseline, the improved system?s
translation is ranked fairly highly ? always 30th
place or higher. Conversely, the baseline?s top-best
output tends to be ranked lower in the improved sys-
tem?s n-best list: among the 114 times it appears, it
is placed as low as 87th.
We ran a small follow-up analysis on the transla-
tion fragments explored during decoding. Using a
modified version of the Joshua decoder, we dumped
lists of hypergraph entries that were explored by
cube pruning during Joshua?s lazy generation of a
100-best list. These entries represent the decoder?s
approximative search through the larger space of
translations licenced by the grammar for each test
sentence. We then compared the hypergraph entries,
excluding glue rules, produced on the first 100 sen-
tences of the MT 2003 test set by both the baseline
and the 60-iteration label-collapsed system.
A full 90% of the entries produced by the label-
collapsed system had no analogue in the baseline
system. The average length of the entries that do
match is 2.3 source words, compared with an aver-
age of 6.2 words for the non-matched entries. We
believe that the increased permissiveness of the hi-
erarchical grammar rules is again the root cause of
these results. Low-level constituents are more likely
to be matched in both the baseline and the label-
collapsed system, but different applications of the
grammar rules, perhaps combined with retuned fea-
ture weights, leads the search for larger translation
fragments into new areas.
7 Conclusions and Future Work
This paper has presented a language-specific method
for automatically coarsening the label set used in
an SCFG-based MT system. Our motivation for
collapsing labels comes from the intuition that the
full cross-product of joint source?target labels, as
produced by statistical parsers, is too large and not
specifically created for bilingual MT modeling. The
greedy collapsing algorithm we developed is based
on iterative merging of the two single-language la-
bels whose alignment distributions are most similar
according to a simple L1 distance metric.
In applying varying degrees of label collapsing to
a baseline MT system, we found significantly im-
proved automatic metric results even when the size
of the joint label set had been reduced by 93%. The
best results, however, were obtained with more mod-
erate coarsening. The coarser labels that our method
produces are syntactically meaningful and represent
specific cross-language behaviors of the language
pair involved. At the grammar level, label collaps-
ing primarily caused a reduction in the number of
rules whose right-hand sides are made up entirely of
nonterminals. The coarser labels made the grammar
more permissive, cutting down on the problem of
rule sparsity. Labeling ambiguity, on the other hand,
was more effectively addressed by pre-processing
we applied to the grammar beforehand. At run time,
the more permissive collapsed grammar allowed the
decoder to search a markedly different region of the
allowable translation space than in the baseline sys-
tem, generally leading to improved output.
One shortcoming of our current algorithm is that
it is based entirely on label alignment distribution
without regard to the different contexts in which la-
bels occur. It thus cannot distinguish between two
labels that align similarly but appear in very different
rules. For example, singular common nouns (NN)
and plural proper nouns (NNPS) in English both
most frequently align to French nouns (N) and are
thus strong candidates for label collapsing under our
algorithm. However, when building noun phrases,
an N::NNPS will more likely require a rule to delete
a French-side determiner, while an N::NN will typ-
ically require a determiner in both French and En-
glish. Thus, collapsing NN and NNPS may lead to
additional ambiguity or incorrect choices when ap-
105
plying larger rules.
Another dimension to be explored is the trade-off
between greedy collapsing and other methods that
cluster all labels at once. K-means clustering could
be a reasonable contrast in this respect; its down-
side would be that all labels in one language must
be assigned to clusters without knowledge of what
clusters are being formed in the other language.
Finally, label collapsing is only the first step in a
broader exploration of SCFG labeling for MT. We
also plan to investigate methods for refining exist-
ing category labels in order to find finer-grained sub-
types that are useful for translating a particular lan-
guage pair. By running label collapsing and refining
together, our end goal is to be able to adapt standard
parser labels to individual translation scenarios.
Acknowledgments
This research was supported in part by U.S. Na-
tional Science Foundation grants IIS-0713402 and
IIS-0915327 and by the DARPA GALE program.
Thanks to Chris Dyer for providing the word-
aligned and preprocessed FBIS corpus we used in
our Chinese?English experiments, and to Jon Clark
for suggesting and setting up the hypergraph com-
parison analysis. We also thank Yahoo! for the use
of the M45 research computing cluster, where we
ran many steps of our experimental pipeline.
References
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, MA, May.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 609?616, Sydney, Aus-
tralia, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the ACL, pages 236?244, Boulder, CO,
June.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 1105?1112, Manchester, England, Au-
gust.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
106
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 135?144,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
A General-Purpose Rule Extractor for SCFG-Based Machine Translation
Greg Hanneman and Michelle Burroughs and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, mburroug, alavie}@cs.cmu.edu
Abstract
We present a rule extractor for SCFG-based
MT that generalizes many of the contraints
present in existing SCFG extraction algo-
rithms. Our method?s increased rule coverage
comes from allowing multiple alignments, vir-
tual nodes, and multiple tree decompositions
in the extraction process. At decoding time,
we improve automatic metric scores by signif-
icantly increasing the number of phrase pairs
that match a given test set, while our experi-
ments with hierarchical grammar filtering in-
dicate that more intelligent filtering schemes
will also provide a key to future gains.
1 Introduction
Syntax-based machine translation systems, regard-
less of the underlying formalism they use, depend
on a method for acquiring bilingual rules in that for-
malism to build the system?s translation model. In
modern syntax-based MT, this formalism is often
synchronous context-free grammar (SCFG), and the
SCFG rules are obtained automatically from parallel
data through a large variety of methods.
Some SCFG rule extraction techniques require
only Viterbi word alignment links between the
source and target sides of the input corpus (Chi-
ang, 2005), while methods based on linguistic con-
stituency structure require the source and/or target
side of the input to be parsed. Among such tech-
niques, most retain the dependency on Viterbi word
alignments for each sentence (Galley et al, 2004;
Zollmann and Venugopal, 2006; Lavie et al, 2008;
Chiang, 2010) while others make use of a general,
corpus-level statistical lexicon instead of individual
alignment links (Zhechev and Way, 2008). Each
method may also place constraints on the size, for-
mat, or structure of the rules it returns.
This paper describes a new, general-purpose rule
extractor intended for cases in which two parse trees
and Viterbi word alignment links are provided for
each sentence, although compatibility with single-
parse-tree extraction methods can be achieved by
supplying a flat ?dummy? parse for the missing tree.
Our framework for rule extraction is thus most sim-
ilar to the Stat-XFER system (Lavie et al, 2008;
Ambati et al, 2009) and the tree-to-tree situation
considered by Chiang (2010). However, we signif-
icantly broaden the scope of allowable rules com-
pared to the Stat-XFER heuristics, and our approach
differs from Chiang?s system in its respect of the lin-
guistic constituency constraints expressed in the in-
put tree structure. In summary, we attempt to extract
the greatest possible number of syntactically moti-
vated rules while not allowing them to violate ex-
plicit constituent boundaries on either the source or
target side. This is achieved by allowing creation of
virtual nodes, by allowing multiple decompositions
of the same tree pair, and by allowing extraction of
SCFG rules beyond the minimial set required to re-
generate the tree pair.
After describing our extraction method and com-
paring it to a number of existing SCFG extraction
techniques, we present a series of experiments ex-
amining the number of rules that may be produced
from an input corpus. We also describe experiments
on Chinese-to-English translation that suggest that
filtering a very large extracted grammar to a more
135
Figure 1: Sample input for our rule extraction algorithm. It consists of a source-side parse tree (French) and a target-
side parse tree (English) connected by a Viterbi word alignment.
moderate-sized translation model is an important
consideration for obtaining strong results. Finally,
this paper concludes with some suggestions for fu-
ture work.
2 Rule Extraction Algorithm
We begin with a parallel sentence consisting of a
source-side parse tree S, a target-side parse tree T ,
and a Viterbi word alignment between the trees?
leaves. A sample sentence of this type is shown in
Figure 1. Our goal is to extract a number of SCFG
rules that are licensed by this input.
2.1 Node Alignment
Our algorithm first computes a node alignment be-
tween the parallel trees. A node s in tree S is aligned
to a node t in tree T if the following constraints are
met. First, all words in the yield of s must either
be aligned to words within the yield of t, or they
must be unaligned. Second, the reverse must also
hold: all words in the yield of t must be aligned to
words within the yield of s or again be unaligned.
This is analogous to the word-alignment consistency
constraint of phrase-based SMT phrase extraction
(Koehn et al, 2003). In Figure 1, for example, the
NP dominating the French words les voitures bleues
is aligned to the equivalent English NP node domi-
nating blue cars.
As in phrase-based SMT, where a phrase in one
language may be consistent with multiple possible
phrases in the other language, we allow parse nodes
in both trees to have multiple node alignments. This
is in contrast to one-derivation rule extractors such
as that of Lavie et al (2008), in which each node
136
in S may only be aligned to a single node in T and
vice versa. The French NP node Ma me`re, for exam-
ple, aligns to both the NNP and NP nodes in English
producing Mother.
Besides aligning existing nodes in both parse trees
to the extent possible, we also permit the introduc-
tion of ?virtual? nodes into either tree. Virtual nodes
are created when two or more contiguous children of
an existing node are aligned consistently to a node or
a similar set of two or more contiguous children of
a node in the opposite parse tree. Virtual nodes may
be aligned to ?original? nodes in the opposite tree or
to other virtual nodes.
In Figure 1, the existing English NP node blue
cars can be aligned to a new virtual node in French
that dominates the N node voitures and the AP node
bleues. The virtual node is inserted as the parent
of N and AP, and as the child of the NP node di-
rectly above. In conjunction with node alignments
between existing nodes, this means that the English
NP blue cars is now aligned twice: once to the orig-
inal French NP node and once to the virtual node
N+AP. We thus replicate the behavior of ?growing
into the gaps? from phrase-based SMT in the pres-
ence of unaligned words. As another example, a vir-
tual node in French covering the V node avait and
the ADV node toujours could be created to align
consistently with a virtual node in English covering
the VBD node had and the ADVP node always.
Since virtual nodes are always created out of chil-
dren of the same node, they are always consis-
tent with the existing syntactic structure of the tree.
Within the constraints of the existing tree structure
and word alignments, however, all possible virtual
nodes are considered. This is in keeping with our
philosophy of allowing multiple alignments with-
out violating constituent boundaries. Near the top
of the trees in Figure 1, for example, French virtual
nodes NP+VN+NP (aligned to English NP+VP) and
VN+NP+PU (aligned to VP+PU) both exist, even
though they overlap. In our procedure, we do allow a
limit to be placed the number of child nodes that can
be combined into a virtual node. Setting this limit
to two, for instance, will constrain node alignment
to the space of possible synchronous binarizations
consistent with the Viterbi word alignments.
2.2 Grammar Extraction
Given the final set of node alignments between the
source tree and the target tree, SCFG rules are ob-
tained via a grammar extraction step. Rule extrac-
tion proceeds in a depth-first manner, such that rules
are extracted and cached for all descendents of a
source node s before rules in which s is the left-hand
side are considered. Extracting rules where source
node s is the left-hand side consists of two phases:
decomposition and combination.
The first phase is decomposition of node s into
all distinct sets D = {d1, d2, . . . , dn} of descendent
nodes such that D spans the entire yield of node s,
where di ? D is node-aligned or is an unaligned ter-
minal for all i, and di has no ancestor a where a is a
descendent of s and a is node-aligned. Each D thus
represents the right-hand side of a minimal SCFG
rule rooted at s. Due to the introduction of overlap-
ping virtual nodes, the decomposition step may in-
volve finding multiple sets of decomposition points
when there are multiple nodes with the same span at
the same level of the tree.
The second phase involves composition of all
rules derived from each element of D subject to cer-
tain constraints. Rules are constructed using s, the
set of nodes Ts = {t | s is aligned to t}, and each
decomposed node set D. The set of left-hand sides
is {s} ? Ts, but there may be many right-hand sides
for a given t and D. Define rhs(d) as the set of
right-hand sides of rules that are derived from d, plus
all alignments of d to its aligned set Td. If d is a
terminal, word alignments are used in the place of
node alignments. To create a set of right-hand sides,
we generate the set R = rhs(d1) ? . . . ? rhs(dn).
For each r ? R, we execute a combine operation
such that combine(r) creates a new right-hand side
by combining the component right-hand sides and
recalculating co-indexes between the source- and
target-side nonterminals. Finally, we insert any un-
aligned terminals on either side.
We work through a small example of grammar ex-
traction using Figure 2, which replicates a fragment
of Figure 1 with virtual nodes included. The En-
glish node JJ is aligned to the French nodes A and
AP, the English node NNS is aligned to the French
node N and the virtual node D+N, and the English
node NP is aligned to the French node NP and the
137
Figure 2: A fragment of Figure 1 with virtual nodes (sym-
bolized by dashed lines) added on the French side. Nodes
D, N, and AP are all original children of the French NP.
virtual node N+AP. To extract rules from the French
node NP, we consider two potential decompositions:
D1 = {D+N,AP} and D2 = {les,N+AP}. Since
the French NP is aligned only to the English NP, the
set of left-hand sides is {NP::NP}, where we use the
symbol ?::? to separate the source and target sides
of joint nonterminal label or a rule.
In the next step, we use cached rules and
alignments to generate all potential right-hand-side
pieces from these top-level nodes:
rhs(D+N) =
{
[D+N1] :: [NNS1],
[les voitures] :: [cars]
}
rhs(AP) =
?
?
?
[AP1] :: [JJ1],
[A1] :: [JJ1],
[bleues] :: [blue]
?
?
?
rhs(les) = ?
rhs(N+AP) =
?
?????????
?????????
[N+AP1] :: [NP1],
[N1 AP2] :: [JJ2 NNS1],
[N1 A2] :: [JJ2 NNS1],
[voitures AP1] :: [JJ1 cars],
[voitures A1] :: [JJ1 cars],
[N1 bleues] :: [blue NNS1],
[voitures bleues] :: [blue cars]
?
?????????
?????????
Next we must combine these pieces. For example,
from D1 we derive the full right-hand sides
1. combine([les voitures]::[cars], [bleues]::[blue])
= [les voitures bleues]::[blue cars]
2. combine([les voitures]::[cars], [A1]::[JJ1])
= [les voitures A1]::[JJ1 cars]
3. combine([les voitures]::[cars], [AP1]::[JJ1])
= [les voitures AP1]::[JJ1 cars]
4. combine([D+N1]::[NNS1], [bleues]::[blue])
= [D+N1 bleues]::[blue NNS1]
5. combine([D+N1]::[NNS1], [A1]::[JJ1])
= [D+N1 A2]::[JJ2 NNS1]
6. combine([D+N1]::[NNS1], [AP1]::[JJ1])
= [D+N1 AP2]::[JJ2 NNS1]
Similarly, we derive seven full right-hand sides from
D2. Since rhs(les) is empty, rules derived have
right-hand sides equivalent to rhs(N+AP) with the
unaligned les added on the source side to com-
plete the span of the French NP. For example,
combine([N+AP1]::[NP1]) = [les N+AP1]::[NP1].
In the final step, the left-hand side is added to each
full right-hand side. Thus,
NP :: NP? [les voitures A1] :: [JJ1 cars]
is one example rule extracted from this tree.
The number of rules can grow rapidly: if the parse
tree has a branching factor of b and a depth of h,
there are potentially O(2b
h
) rules extracted. To con-
trol this, we allow certain constraints on the rules ex-
tracted that can short-circuit right-hand-side forma-
tion. We allow separate restrictions on the number
of items that may appear on the right-hand side of
phrase pair rules (maxp) and hierarchical grammar
rules (maxg). We also optionally allow the exclu-
sion of parallel unary rules ? that is, rules whose
right-hand sides consist solely of a pair of aligned
nonterminals.
138
Tree Multiple Virtual Multiple
System Constraints Alignments Nodes Derivations
Hiero No ? ? Yes
Stat-XFER Yes No Some No
GHKM Yes No No Yes
SAMT No No Yes Yes
Chiang (2010) No No Yes Yes
This work Yes Yes Yes Yes
Table 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods.
3 Comparison to Other Methods
Table 1 compares the rule extractor described in Sec-
tion 2 to other SCFG extraction methods described
in the literature. We include comparisons of our
work against the Hiero system (Chiang, 2005), the
Stat-XFER system rule learner most recently de-
scribed by Ambati et al (2009), the composed ver-
sion of GHKM rule extraction (Galley et al, 2006),
the so-called Syntax-Augmented MT (SAMT) sys-
tem (Zollmann and Venugopal, 2006), and a Hiero?
SAMT extension with source- and target-side syntax
described by Chiang (2010). Note that some of these
methods make use of only target-side parse trees ?
or no parse trees at all, in the case of Hiero ? but
our primary interest in comparison is the constraints
placed on the rule extraction process rather than the
final output form of the rules themselves. We high-
light four specific dimensions along these lines.
Tree Constraints. As we mentioned in this pa-
per?s introduction, we do not allow any part of our
extracted rules to violate constituent boundaries in
the input parse trees. This is in contrast to Hiero-
derived techniques, which focus on expanding gram-
mar coverage by extracting rules for all spans in
the input sentence pair that are consistently word-
aligned, regardless of their correspondence to lin-
guistic constituents. Practitioners of both phrase-
based and syntax-based SMT have reported severe
grammar coverage issues when rules are required to
exactly match parse constituents (Koehn et al, 2003;
Chiang, 2010). In our work, we attempt to improve
the coverage of the grammar by allowing multiple
node alignments, virtual nodes, and multiple tree
decompositions rather than ignoring structure con-
straints.
Multiple Alignments. In contrast to all other ex-
traction methods in Table 1, ours allows a node in
one parse tree to be aligned with multiple nodes
in the other tree, as long as the word-alignment
and structure constraints are satisfied. However, we
do not allow a node to have multiple simultaneous
alignments ? a single node alignment must be cho-
sen for extracting an individual rule. In practice,
this prevents extraction of ?triangle? rules where the
same node appears on both the left- and right-hand
side of the same rule.1
Virtual Nodes. In keeping with our philosophy
of representing multiple alignments, our use of mul-
tiple and overlapping virtual nodes is less restrictive
than the single-alignment constraint of Stat-XFER.
Another key difference is that Stat-XFER requires
all virtual nodes to be aligned to original nodes in
the other language, while we permit virtual?virtual
node alignments. In respecting existing tree struc-
ture constraints, our virtual node placement is more
restrictive than SAMT or Chiang, where extracted
nodes may cross existing constituent boundaries.
Multiple Derivations. Galley et al (2006) ar-
gued that breaking a single tree pair into multiple
decompositions is important for correct probability
modeling. We agree, and we base our rule extrac-
tor?s acquisition of multiple derivations per tree pair
on techniques from both GHKM and Hiero. More
specifically, we borrow from Hiero the idea of cre-
ating hierarchical rules by subtracting and abstract-
ing all possible subsets of smaller phrases (aligned
nodes in our case) from larger phrases. Like GHKM,
1Figure 2 includes a potential triangle rule, D+N :: NNS ?
[les N1] :: [NNS1], where the English NNS node appears on
both sides of the rule. It is simultaneously aligned to the French
D+N and N nodes.
139
we do this exhaustively within some limit, although
in our case we use a rank limit on a rule?s right-hand
side rather than a limit on the depth of the subn-
ode subtractions. Our constraint achieves the goal
of controlling the size of the rule set while remaining
flexibile in terms of depth depending on the shape of
the parse trees.
4 Experiments
We conducted experiments with our rule extrac-
tor on the FBIS corpus, made up of approximately
302,000 Chinese?English sentence pairs. We parsed
the corpus with the Chinese and English grammars
of the Berkeley parser (Petrov and Klein, 2007) and
word-aligned it with GIZA++ (Och and Ney, 2003).
The parsed and word-aligned FBIS corpus served as
the input to our rule extractor, which we ran with a
number of different settings.
First, we acquired a baseline rule extraction
(?xfer-orig?) from our corpus using an implementa-
tion of the basic Stat-XFER rule learner (Lavie et al,
2008), which decomposes each input tree pair into a
single set of minimal SCFG rules2 using only origi-
nal nodes in the parse trees. Next, we tested the ef-
fect of allowing multiple decompositions by running
our own rule learner, but restricting its rules to also
only make use of original nodes (?compatible?). Fi-
nally, we investigated the total number of extractable
rules by allowing the creation of virtual nodes from
up to four adjacent sibling nodes and placing two
different limits on the length of the right-hand side
(?full-short? and ?full-long?). These configurations
are summarized in Table 2.
Rule Set maxp maxg Virtual Unary
xfer-orig 10 ? No Yes
compatible 10 5 No Yes
full-short 5 5 Yes No
full-long 7 7 Yes No
Table 2: Rule sets considered by a Stat-XFER baseline
(?xfer-orig?) and our own rule extractor.
2In practice, some Stat-XFER aligned nodes produce two
rules instead of one: a minimal hierarchical SCFG rule is al-
ways produced, and a phrase pair rule will also be produced for
node yields within the maxp cutoff.
4.1 Rules Extracted
As expected, we find that allowing multiple decom-
positions of each tree pair has a significant effect on
the number of extracted rules. Table 3 breaks the ex-
tracted rules for each configuration down into phrase
pairs (all terminals on the right-hand side) and hier-
archical rules (containing at least one nonterminal
on the right-hand side). We also count the num-
ber of extracted rule instances (tokens) against the
number of unique rules (types). The results show
that multiple decomposition leads to a four-fold in-
crease in the number of extracted grammar rules,
even when the length of the Stat-XFER baseline
rules is unbounded. The number of extracted phrase
pairs shows a smaller increase, but this is expected:
the number of possible phrase pairs is proportional
to the square of the sentence length, while the num-
ber of possible hierarchical rules is exponential, so
there is more room for coverage improvement in the
hierarchical grammar.
With virtual nodes included, there is again a large
jump in both the number of extracted rule tokens and
types, even at relatively short length limits. When
both maxp and maxg are set to 7, our rule ex-
tractor produces 1.5 times as many unique phrase
pairs and 20.5 times as many unique hierarchical
rules as the baseline Stat-XFER system, and nearly
twice the number of hierarchical rules as when us-
ing length limits of 5. Ambati et al (2009) showed
the usefulness of extending rule extraction from ex-
act original?original node alignments to cases in
which original?virtual and virtual?original align-
ments were also permitted. Our experiments con-
firm this, as only 60% (full-short) and 54% (full-
long) of our extracted rule types are made up of only
original?original node alignments. Further, we find
a contribution from the new virtual?virtual case: ap-
proximately 8% of the rules extracted in the ?full-
long? configuration from Table 3 are headed by a
virtual?virtual alignment, and a similar number have
a virtual?virtual alignment on their right-hand sides.
All four of the extracted rule sets show Zipfian
distributions over rule frequency counts. In the xfer-
orig, full-short, and full-long configurations, be-
tween 82% and 86% of the extracted phrase pair
rules, and between 88% and 92% of the extracted
hierarchical rules, were observed only once. These
140
Extracted Instances Unique Rules
Rule Set Phrase Hierarchical Phrase Hierarchical
xfer-orig 6,646,791 1,876,384 1,929,641 767,573
compatible 8,709,589 6,657,590 2,016,227 3,590,184
full-short 10,190,487 14,190,066 2,877,650 8,313,690
full-long 10,288,731 22,479,863 2,970,403 15,750,695
Table 3: The number of extracted rule instances (tokens) and unique rules (types) produced by the Stat-XFER system
(?xfer-orig?) and three configurations of our rule extractor.
percentages are remarkably consistent despite sub-
stantial changes in grammar size, meaning that our
more exhaustive method of rule extraction does not
produce a disproportionate number of singletons.3
On the other hand, it does weaken the average count
of an extracted hierarchical rule type. From Table 3,
we can compute that the average phrase pair count
remains at 3.5 when we move from xfer-orig to the
two full configurations; however, the average hier-
archical rule count drops from 2.4 to 1.7 (full-short)
and finally 1.4 (full-long). This likely again reflects
the exponential increase in the number of extractable
hierarchical rules compared to the quadratic increase
in the phrase pairs.
4.2 Translation Results
The grammars obtained from our rule extractor can
be filtered and formatted for use with a variety of
SCFG-based decoders and rule formats. We car-
ried out end-to-end translation experiments with the
various extracted rule sets from the FBIS corpus us-
ing the open-source decoder Joshua (Li et al, 2009).
Given a source-language string, Joshua translates by
producing a synchronous parse of it according to a
scored SCFG and a target-side language model. A
significant engineering challenge in building a real
MT system of this type is selecting a more moderate-
sized subset of all extracted rules to retain in the final
translation model. This is an especially important
consideration when dealing with expanded rule sets
derived from virtual nodes and multiple decomposi-
tions in each input tree.
In our experiments, we pass all grammars through
3The compatible configuration is somewhat of an outlier. It
has proportionally fewer singleton phrase pairs (80%) than the
other variants, likely because it allows multiple alignments and
multiple decompositions without allowing virtual nodes.
two preprocessing steps before any translation
model scoring. First, we noticed that English car-
dinal numbers and punctuation marks in many lan-
guages tend to receive incorrect nonterminal labels
during parsing, despite being closed-class items with
clearly defined tags. Therefore, before rule extrac-
tion, we globally correct the nodel labels of all-
numeral terminals in English and certain punctua-
tion marks in both English and Chinese. Second,
we attempt to reduce derivational ambiguity in cases
where the same SCFG right-hand side appears in
the grammar after extraction with a large number of
possible left-hand-side labels. To this end, we sort
the possible left-hand sides by frequency for each
unique right-hand side, and we remove the least fre-
quent 10 percent of the label distribution.
Our translation model scoring is based on the fea-
ture set of Hanneman et al (2010). This includes
the standard bidirectional conditional maximum-
likelihood scores at both the word and phrase level
on the right-hand side of rules. We also include
maximum-likelihood scores for the left-hand-side
label given all or part of the right-hand side. Using
statistics local to each rule, we set binary indicator
features for rules whose frequencies are ? 3, plus
five additional indicator features according to the
format of the rule?s right-hand side, such as whether
it is fully abstract. Since the system in this paper
is not constructed using any non-syntactic rules, we
do not include the Hanneman et al (2010) ?not la-
belable? maximum-likelihood features or the indica-
tor features related to non-syntactic labels.
Beyond the above preprocessing and scoring
common to all grammars, we experiment with three
different solutions to the more difficult problem of
selecting a final translation grammar. In any case,
we separate phrase pair rules from hierarchical rules
141
Rule Set Filter BLEU TER MET
xfer-orig 10k 24.39 68.01 54.35
xfer-orig 5k+100k 25.95 66.27 54.77
compatible 10k 24.28 65.30 53.58
full-short 10k 25.16 66.25 54.33
full-short 100k 25.51 65.56 54.15
full-short 5k+100k 26.08 64.32 54.58
full-long 10k 25.74 65.52 54.55
full-long 100k 25.53 66.24 53.68
full-long 5k+100k 25.83 64.55 54.35
Table 4: Automatic metric results using different rule
sets, as well as different grammar filtering methods.
and include in the grammar all phrase pair rules
matching a given tuning or testing set. Any im-
provement in phrase pair coverage during the extrac-
tion stage is thus directly passed along to decoding.
For hierarchical rules, we experiment with retain-
ing the 10,000 or 100,000 most frequently extracted
unique rules. We also separate fully abstract hier-
archical rules from partially lexicalized hierarchical
rules, and in a further selection technique we retain
the 5,000 most frequent abstract and 100,000 most
frequent partially lexicalized rules.
Given these final rule sets, we tune our MT sys-
tems on the NIST MT 2006 data set using the min-
imum error-rate training package Z-MERT (Zaidan,
2009), and we test on NIST MT 2003. Both sets
have four reference translations. Table 4 presents
case-insensitive evaluation results on the test set ac-
cording to the automatic metrics BLEU (Papineni et
al., 2002), TER (Snover et al, 2006), and METEOR
(Lavie and Denkowski, 2009).4 The trend in the
results is that including a larger grammar is gener-
ally better for performance, but filtering techniques
also play a substantial role in determining how well
a given grammar will perform at run time.
We first compare the results in Table 4 for dif-
ferent rule sets all filtered the same way at decod-
ing time. With only 10,000 hierarchical rules in use
(?10k?), the improvements in scores indicate that an
important contribution is being made by the addi-
tional phrase pair coverage provided by each suc-
4For METEOR scoring we use version 1.0 of the metric,
tuned to HTER with the exact, stemming, and synonymy mod-
ules enabled.
cessive rule set. The original Stat-XFER rule ex-
traction provides 244,988 phrase pairs that match
the MT 2003 test set. This is already increased to
520,995 in the compatible system using multiple de-
compositions. With virtual nodes enabled, the full
system produces 766,379 matching phrase pairs up
to length 5 or 776,707 up to length 7. These systems
both score significantly higher than the Stat-XFER
baseline according to BLEU and TER, and the ME-
TEOR scores are likely statistically equivalent.
Across all configurations, we find that changing
the grammar filtering technique ? possibly com-
bined with retuned decoder feature weights ? also
has a large influence on automatic metric scores.
Larger hierarchical grammars tend to score better, in
some cases to the point of erasing the score differ-
ences between rule sets. From this we conclude that
making effective use of the extracted grammar, no
matter its size, with intelligent filtering techniques
is at least as important as the number and type of
rules extracted overall. Though the filtering results
in Table 4 are still somewhat inconclusive, the rel-
ative success of the ?5k+100k? setting shows that
filtering fully abstract and partially lexicalized rules
separately is a reasonable starting approach. While
fully abstract rules do tend to be more frequently ob-
served in grammar extraction, and thus more reliably
scored in the translation model, they also have the
ability to overapply at decoding time because their
use is not restricted to any particular lexical context.
5 Conclusions and Future Work
We demonstrated in Section 4.1 that the general
SCFG extraction algorithm described in this paper
is capable of producing very large linguistically mo-
tivated rule sets. These rule sets can improve auto-
matic metric scores at decoding time. At the same
time, we see the results in Section 4.2 as a spring-
board to more advanced and more intelligent meth-
ods of grammar filtering. Our major research ques-
tion for future work is to determine how to make the
best runtime use of the grammars we can extract.
As we saw in Section 2, multiple decompositions
of a single parse tree allow the same constituent to
be built in a variety of ways. This is generally good
for coverage, but its downside at run time is that the
decoder must manage a larger number of competing
142
derivations that, in the end, produce the same output
string. Grammar filtering that explicitly attempts to
limit the derivational ambiguity of the retained rules
may prevent the translation model probabilities of
correct outputs from getting fragmented into redun-
dant derivations. So far we have only approximated
this by using fully abstract rules as a proxy for the
most derivationally ambiguous rules.
Filtering based on the content of virtual nodes
may also be a reasonable strategy for selecting use-
ful grammar rules and discarding those whose con-
tributions are less necessary. For example, we find
in our current output many applications of rules
involving virtual nodes that consist of an open-
class category and a mark of punctuation, such as
VBD+COMMA and NN+PU. While there is noth-
ing technically wrong with these rules, they may not
be as helpful in translation as rules for nouns and
adjectives such as JJ+NNP+NN or NNP+NNP in flat
noun phrase structures such as former U.S. president
Bill Clinton.
A final concern in making use of our large ex-
tracted grammars is the effect virtual nodes have
on the size of the nonterminal set. The Stat-XFER
baseline grammar from our ?xfer-orig? configura-
tion uses a nonterminal set of 1,577 unique labels.
In our rule extractor so far, we have adopted the con-
vention of naming virtual nodes with a concatena-
tion of their component sibling labels, separated by
?+?s. With the large number of virtual node labels
that may be created, this gives our ?full-short? and
?full-long? extracted grammars nonterminal sets of
around 73,000 unique labels. An undesirable conse-
quence of such a large label set is that a particular
SCFG right-hand side may acquire a large variety
of left-hand-side labels, further contributing to the
derivational ambiguity problems discussed above.
In future work, the problem could be addressed by
reconsidering our naming scheme for virtual nodes,
by allowing fuzzy matching of labels at translation
time (Chiang, 2010), or by other techniques aimed
at reducing the size of the overall nonterminal set.
Acknowledgments
This research was supported in part by U.S. National
Science Foundation grants IIS-0713402 and IIS-
0915327 and the DARPA GALE program. We thank
Vamshi Ambati and Jon Clark for helpful discus-
sions regarding implementation details of the gram-
mar extraction algorithm. Thanks to Chris Dyer for
providing the word-aligned and preprocessed FBIS
corpus. Finally, we thank Yahoo! for the use of
the M45 research computing cluster, where we ran
many steps of our experimental pipeline.
References
Vamshi Ambati, Alon Lavie, and Jaime Carbonell. 2009.
Extraction of syntactic translation models from paral-
lel data using syntax from source and target languages.
In Proceedings of the 12th Machine Translation Sum-
mit, pages 190?197, Ottawa, Canada, August.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, MI, June.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
HLT-NAACL 2004: Main Proceedings, pages 273?
280, Boston, MA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
ACL, pages 961?968, Sydney, Australia, July.
Greg Hanneman, Jonathan Clark, and Alon Lavie. 2010.
Improved features and grammar selection for syntax-
based MT. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 82?87, Uppsala, Sweden, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 48?54, Edmonton,
Alberta, May?June.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
143
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 1105?1112, Manchester, England, Au-
gust.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY, June.
144
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 365?371,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU Syntax-Based Machine Translation at WMT 2011
Greg Hanneman and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, alavie}@cs.cmu.edu
Abstract
We present the Carnegie Mellon University
Stat-XFER group submission to the WMT
2011 shared translation task. We built a hy-
brid syntactic MT system for French?English
using the Joshua decoder and an automati-
cally acquired SCFG. New work for this year
includes training data selection and grammar
filtering. Expanded training data selection
significantly increased translation scores and
lowered OOV rates, while results on grammar
filtering were mixed.
1 Introduction
During the past year, the statistical transfer ma-
chine translation group at Carnegie Mellon Univer-
sity has continued its work on large-scale syntactic
MT systems based on automatically acquired syn-
chronous context-free grammars (SCFGs). For the
2011 Workshop on Machine Translation, we built
a hybrid MT system, including both syntactic and
non-syntactic rules, and submitted it as a constrained
entry to the French?English translation task. This
is our fourth yearly submission to the WMT shared
translation task.
In design and construction, the system is sim-
ilar to our submission from last year?s workshop
(Hanneman et al, 2010), with changes in the meth-
ods we employed for training data selection and
SCFG filtering. Continuing WMT?s general trend,
we worked with more data than in previous years,
basing our 2011 system on 13.9 million sentences
of parallel French?English training data and an En-
glish language model of 1.8 billion words. Decod-
ing was carried out in Joshua (Li et al, 2009), an
open-source framework for parsing-based MT. We
managed our experiments with LoonyBin (Clark and
Lavie, 2010), an open-source tool for defining, mod-
ifying, and running complex experimental pipelines.
We describe our system-building process in more
detail in Section 2. In Section 3, we evaluate the sys-
tem?s performance on WMT development sets and
examine the aftermath of training data selection and
grammar filtering. Section 4 concludes with possi-
ble directions for future work.
2 System Construction
2.1 Training Data Selection
WMT 2011?s provided French?English training data
consisted of 36.8 million sentence pairs from the Eu-
roparl, news commentary, UN documents, and Giga-
FrEn corpora (Table 1). The first three of these are,
for the most part, clean data resources that have been
successfully employed as MT corpora for a number
of years. The Giga-FrEn corpus, though the largest,
is also the least precise, as its Web-crawled data
sources are less homogeneous and less structured
than the other corpora. Nevertheless, Pino et al
(2010) found significant improvements in French?
English MT output quality by including it. Our goal
for this year was to strike a middle ground: to avoid
computational difficulties in using the entire 36.8
million sentence pairs of training data, but to mine
the Giga-FrEn corpus for sentences to increase our
system?s vocabulary coverage.
Our method of training data selection proceeded
as follows. We first tokenized all the parallel training
365
Corpus Released Used
Europarl 1,825,077 1,614,111
News commentary 115,562 95,138
UN documents 12,317,600 9,352,232
Giga-FrEn 22,520,400 2,839,466
Total 36,778,639 13,900,947
Table 1: Total number of training sentence pairs released,
by corpus, and the number used in building our system.
data using the Stanford parser?s tokenizer (Klein and
Manning, 2003) for English and our own in-house
script for French. We then passed the Europarl, news
commentary, and UN data through a filtering script
that removed lines longer than 95 tokens in either
language, empty lines, lines with excessively imbal-
anced length ratios, and lines containing tokens of
more than 25 characters in either language. From
the filtered data, we computed a list of the source-
side vocabulary words along with their frequency
counts. Next, we searched the Giga-FrEn corpus for
relatively short lines on the source side (up to 50 to-
kens long) that contained either a new vocabulary
word or a word that had been previously seen fewer
than 20 times. Such lines were added to the filtered
training data to make up our system?s final parallel
training corpus.
The number of sentences retained from each data
source is listed in Table 1; in the end, we trained our
system from 13.9 million parallel sentences. With
the Giga-FrEn data included, the source side of our
parallel corpus had a vocabulary of just over 1.9
million unique words, compared with a coverage of
545,000 words without using Giga-FrEn.
We made the decision to leave the training data in
mixed case for our entire system-building process.
At the cost of slightly sparser estimates for word
alignments and translation probabilities, a mixed-
case system avoids the extra step of building a sta-
tistical recaser to treat our system?s output.
2.2 Grammar Extraction and Scoring
Once we had assembled the final training corpus,
we annotated it with statistical word alignments and
constituent parse trees on both sides. Unidirec-
tional word alignments were provided by MGIZA++
(Gao and Vogel, 2008), then symmetrized with the
grow-diag-final-and heuristic (Koehn et al, 2005).
For generating parse trees, we used the French and
English grammars of the Berkeley statistical parser
(Petrov and Klein, 2007).
Except for minor bug fixes, our method for ex-
tracting and scoring a translation grammar remains
the same as in our WMT 2010 submission. We ex-
tracted both syntactic and non-syntactic portions of
the translation grammar. The non-syntactic gram-
mar was extracted from the parallel corpus and
word alignments following the standard heuristics
of phrase-based SMT (Koehn et al, 2003). The
syntactic grammar was produced using the method
of Lavie et al (2008), which decomposes each pair
of word-aligned parse trees into a series of minimal
SCFG rules. The word alignments are first gener-
alized to node alignments, where nodes s and t are
aligned between the source and target parse trees if
all word alignments in the yield of s land within
the yield of t and vice versa. Minimal SCFG rules
are derived from adjacent levels of node alignments:
the labels from each pair of aligned nodes forms a
rule?s left-hand side, and the right-hand side is made
up of the labels from the frontier of aligned nodes
encountered when walking the left-hand side?s sub-
trees. Within a phrase length limit, each aligned
node pair generate an all-terminal phrase pair rule
as well.
Since both grammars are extracted from the same
Viterbi word alignments using similar alignment
consistency constraints, the phrase pair rules from
the syntactic grammar make up a subset of the rules
extracted according to phrase-based SMT heuristics.
We thus share instance counts between identical
phrases extracted in both grammars, then delete the
non-syntactic versions. Remaining non-syntactic
phrase pairs are converted to SCFG rules, with the
phrase pair forming the right-hand side and the
dummy label PHR::PHR as the left-hand side. Ex-
cept for the dummy label, all nonterminals in the fi-
nal SCFG are made up of a syntactic category label
from French joined with a syntactic category label
from English, as extracted in the syntactic grammar.
A sampling of extracted SCFG rules is shown in Fig-
ure 1.
The combined grammar was scored according to
the 22 translation model features we used last year.
For a generic SCFG rule of the form ?s :: ?t ?
366
PHR :: PHR ? [, ainsi qu?] :: [as well as]
V :: VBN ? [modifie?es] :: [modified]
NP :: NP ? [les conflits arme?s] :: [armed conflict]
AP :: SBAR ? [tel qu? VPpart1] :: [as VP1]
NP :: NP ? [D1 N2 A3] :: [CD1 JJ3 NNS2]
Figure 1: Sample extracted SCFG rules. They include
non-syntactic phrase pairs, single-word and multi-word
syntactic phrase pairs, partially lexicalized hierarchical
rules, and fully abstract hierarchical rules.
[rs ] :: [rt ], we computed 11 maximum-likelihood
features as follows:
? Phrase translation scores P (rs | rt) and
P (rt | rs) for phrase pair rules, using the larger
non-syntactic instance counts for rules that
were also extracted syntactically.
? Hierarchical translation scores P (rs | rt) and
P (rt | rs) for syntactic rules with nonterminals
on the right-hand side.
? Labeling scores P (?s :: ?t | rs), P (?s :: ?t | rt),
and P (?s :: ?t | rs, rt) for syntactic rules.
? ?Not syntactically labelable? scores P (?s ::
?t = PHR :: PHR | rs) and P (?s :: ?t =
PHR :: PHR | rt), with additive smoothing
(n = 1), for all rules.
? Bidirectional lexical scores for all rules with
lexical items, calculated from a unigram lexi-
con over Viterbi-aligned word pairs as in the
Moses decoder (Koehn et al, 2007).
We also included the following 10 binary indicator
features using statistics local to each rule:
? Three low-count features that equal 1 when the
extracted frequency of the rule is exactly equal
to 1, 2, or 3.
? A syntactic feature that equals 1 when the rule?s
label is syntactic, and a corresponding non-
syntactic feature that equals 1 when the rule?s
label is PHR::PHR.
? Five rule format features that equal 1 when the
rule?s right-hand side has a certain composi-
tion. If as and at are true when the source and
target sides contain only nonterminals, respec-
tively, our rule format features are equal to as,
at, as ? a?t, a?s ? at, and a?s ? a?t.
Finally, our model includes a glue rule indicator fea-
ture that equals 1 when the rule is a generic glue
rule. In the Joshua decoder, glue rules monotoni-
cally stitch together adjacent parsed translation frag-
ments at no model cost.
2.3 Language Modeling
This year, our constrained-track system made use of
part of the English Gigaword data, along with other
provided text, in its target-side language model.
From among the data released directly for WMT
2011, we used the English side of the Europarl,
news commentary, French?English UN document,
and English monolingual news corpora. From the
English Gigaword corpus, we included the entire
Xinhua portion and the most recent 13 million sen-
tences of the AP Wire portion. Some of these cor-
pora contain many lines that are repeated a dispro-
portionate number of times ? the monolingual news
corpus in particular, when filtered to only one oc-
currence of each sentence, reaches only 27% of its
original line count. As part of preparing our lan-
guage modeling data, we deduplicated both the En-
glish news and the UN documents, the corpora with
the highest percentages of repeated sentences. We
also removed lines containing more than 750 char-
acters (about 125 average English words) before to-
kenization.
The final prepared corpus was made up of approx-
imately 1.8 billion words of running text. We built
a 5-gram language model from it with the SRI lan-
guage modeling toolkit (Stolcke, 2002). To match
the treatment given to the training data, the language
model was also built in mixed case.
2.4 Grammar Filtering for Decoding
As is to be expected from a training corpus of 13.9
million sentence pairs, the grammars we extract ac-
cording to the procedure of Section 2.2 are quite
large: approximately 2.53 billion non-syntactic and
440 million syntactic rule instances, for a combined
grammar of 1.26 billion unique rules. In preparation
for tuning or decoding, we are faced with the engi-
neering challenge of selecting a subset of the gram-
367
mar that contains useful rules and fits in a reasonable
amount of memory.
Before even extracting a syntactic grammar, we
passed the automatically generated parse trees on the
training corpus through a small tag-correction script
as a pre-step. In previous experimentation, we no-
ticed that a surprising proportion of cardinal num-
bers in English had been tagged with labels other
than CD, their correct tag. We also found errors in
labeling marks of punctuation in both English and
French, when again the canonical labels are unam-
biguous. To fix these errors, we forcibly overwrote
the labels of English tokens made up of only digits
with CD, and we overwrote the labels of 25 English
and 24 French marks of punctuation or other sym-
bols with the appropriate tag as defined by the rele-
vant treebank tagging guidelines.
After grammar extraction and combination of
syntactic and non-syntactic rules, we ran an addi-
tional filtering step to reduce derivational ambiguity
in the case where the same SCFG right-hand side ap-
peared with more than one left-hand-side label. For
each right-hand side, we sorted its possible labels by
extracted frequency, then threw out the labels in the
bottom 10% of the left-hand-side distribution.
Finally, we ran a main grammar filtering step prior
to tuning or decoding, experimenting with two dif-
ferent filtering methods. In both cases, the phrase
pair rules in the grammar were split off and filtered
so that only those whose source sides completely
matched the tuning or test set were retained.
The first, more naive grammar filtering method
sorted all hierarchical rules by extracted frequency,
then retained the most frequent 10,000 rules to join
all matching phrase pair rules in the final translation
grammar. This is similar to the basic grammar filter-
ing we performed for our WMT 2010 submission.
It is based on the rationale that the most frequently
extracted rules in the parallel training data are likely
to be the most reliably estimated and also frequently
used in translating a new data set. However, it also
passes through a disproportionate number of fully
abstract rules ? that is, rules whose right-hand sides
are made up entirely of nonterminals ? which can
apply more recklessly on the test set because they
are not lexically grounded.
Our second, more advanced method of filtering
made two improvements over the naive approach.
First, it controlled for the imbalance of hierarchi-
cal rules by splitting the grammar?s partially lexical-
ized rules into a separate group that can be filtered
independently. Second, it applied a lexical-match
filter such that a partially lexicalized rule was re-
tained only if all its lexicalized source phrases up
to bigrams matched the intended tuning or testing
set. The final translation grammar in this case was
made up of three parts: all phrase pair rules match-
ing the test set (as before), the 100,000 most fre-
quently extracted partially lexicalized rules whose
bigrams match the test set, and the 2000 most fre-
quently extracted fully abstract rules.
3 Experimental Results and Analysis
We tuned each system variant on the newstest2008
data set, using the Z-MERT package (Zaidan, 2009)
for minimum error-rate training to the BLEU metric.
We ran development tests on the newstest2009 and
newstest2010 data sets; Table 2 reports the results
obtained according to various automatic metrics.
The evaluation consists of case-insensitive scoring
according to METEOR 1.0 (Lavie and Denkowski,
2009) tuned to HTER with the exact, stemming,
and synonymy modules enabled, case-insensitive
BLEU (Papineni et al, 2002) as implemented by
the NIST mteval-v13 script, and case-insensitive
TER 0.7.25 (Snover et al, 2006).
Table 2 gives comparative results for two major
systems: one based on our WMT 2011 data selec-
tion as outlined in Section 2.1, and one based on
the smaller WMT 2010 training data that we used
last year (8.6 million sentence pairs). Each system
was run with the two grammar filtering variants de-
scribed in Section 2.4: the 10,000 most frequently
extracted hierarchical rules of any type (?10k?), and
a combination of the 2000 most frequently extracted
abstract rules and the 100,000 most frequently ex-
tracted partially lexicalized rules that matched the
test set (?2k+100k?). Our primary submission to the
WMT 2011 shared task was the fourth line of Ta-
ble 2 (?WMT 2011 2k+100k?); we also made a con-
strastive submission with the system from the sec-
ond line (?WMT 2010 2k+100k?).
Using part of the Giga-FrEn data ? along with
the additions to the Europarl, news commentary,
and UN document courses released since last year
368
newstest2009 newstest2010
System METEOR BLEU TER METEOR BLEU TER
WMT 2010 10k 54.94 24.77 56.53 56.66 25.78 55.06
WMT 2010 2k+100k 55.16 24.88 56.19 56.89 26.05 54.66
WMT 2011 10k 55.82 26.02 54.77 58.13 27.71 52.96
WMT 2011 2k+100k 55.77 26.01 54.70 57.88 27.38 53.04
Table 2: Development test results for systems based on WMT 2010 data (without the Giga-FrEn corpus) and WMT
2011 data (with some Giga-FrEn). The fourth line is our primary shared-task submission.
Applications 10k 2k+100k
Unique rules 1,305 1,994
Rule instances 14,539 12,130
Table 3: Summary of 2011 system syntactic rule applica-
tions on both test sets.
? is beneficial to translation quality, as there is
a clear improvement in metric scores between the
2010 and 2011 systems. Our BLEU score improve-
ments of 1.2 to 1.9 points are statistically significant
according to the paired bootstrap resampling method
(Koehn, 2004) with n = 1000 and p < 0.01. They
are also larger than the 0.7- to 1.1-point gains re-
ported by Pino et al (2010) when the full Giga-FrEn
was added. The 2011 system also shows a signifi-
cant reduction in the out-of-vocabulary (OOV) rate
on both test sets: 38% and 47% fewer OOV types,
and 44% and 45% fewer OOV tokens, when com-
pared to the 2010 system.
Differences between grammar filtering tech-
niques, on the other hand, are much less signifi-
cant according to all three metrics. Under paired
bootstrap resampling on the newstest2009 set, the
grammar variants in both the 2010 and 2011 systems
are statistically equivalent according to BLEU score.
On newstest2010, the 2k+100k grammar improves
over the 10k version (p < 0.01) in the 2010 system,
but the situation is reversed in the 2011 system.
We investigated differences in grammar use with
an analysis of rule applications in the two variants
of the 2011 system, the results of which are summa-
rized in Table 3. Though the configuration with the
2k+100k grammar does apply syntactic rules 20%
more frequently than its 10k counterpart, the 10k
system uses overall 53% more unique rules. One
contributing factor to this situation could be that the
fully abtract rule cutoff is set too low compared to
the increase in partially lexicalized rules. The ef-
fect of the 2k+100k filtering is to reduce the number
of abstract rules from 4000 to 2000 while increas-
ing the number of partially lexicalized rules from
6000 to 100,000. However, we find that the 10k
system makes heavy use of some short, meaningful
abstract rules that were excluded from the 2k+100k
system. The 2k+100k grammar, by contrast, in-
cludes a long tail of less frequently used partially
lexicalized grammar rules.
In practice, there is a balance between the use
of syntactic and non-syntactic grammar rules dur-
ing decoding. We highlight an example of how
both types of rules work together in Figure 2, which
shows our primary system?s translation of part of
newstest2009 sentence 2271. The French source
text is given in italics and segmented into phrases.
The SCFG rules used in translation are shown
above each phrase, where numerical superscripts on
the nonterminal labels indicate those constituents?
relative ordering in the original French sentence.
(Monotonic glue rules are not shown.) While non-
syntactic rules can be used for short-distance re-
ordering and fixed phrases, such as te?le?phones mo-
biles ? mobile phones, the model prefers syntac-
tic translations for more complicated patterns, such
as the head?children reversal in appareils musicaux
portables ? portable music devices.
4 Conclusions and Future Work
Compared to last year, the two main differences in
our current WMT submission are: (1) a new train-
ing data selection strategy aimed at increasing sys-
tem vocabulary without hugely increasing corpus
size, and (2) a new method of grammar filtering that
emphasizes partially lexicalized rules over fully ab-
369
PHR::PHR
young people who
PHR::PHR
frequently use
NP::NP
N::NNS1
devices
A::NN2
music
A::JJ3
portable
PHR::PHR
and mobile phones
jeunes qui utilisent fre?quemment des appareils musicaux portables et des te?le?phones mobiles
PHR::PHR
at full
N::NN
volume
,::,
,
V::MD
can
VPpart::VP
NP::NP3
N::NN2
hearing
D::PRP$1
their
V::VBG1
damaging
ADV::RB2
unknowingly
a` plein volume , puissent endommager inconsciemment leur audition
Figure 2: Our primary submission?s translation of a partial sentence from the newstest2009 set, showing a combination
of syntactic and non-syntactic rules.
stract ones.
Based on the results presented in Section 3, we
feel confident in declaring vocabulary-based filter-
ing of the Giga-FrEn corpus a success. By increas-
ing the size of our parallel corpus by 26%, we more
than tripled the number of unique words appearing
in the source text. In conjunction with supplements
to the Europarl, news commentary, and UN docu-
ment corpora, this improvement led to 44% fewer
OOV tokens at decoding time on two different test
sets, as well as a boost in automatic metric scores
of 0.6 METEOR, 1.2 BLEU, and 1.5 TER points
compared to last year?s system. We expect to em-
ploy similar data selection techniques when building
future systems, especially as the amount of parallel
data available continues to increase.
We did not, however, find significant improve-
ments in translation quality by changing the gram-
mar filtering method. As discussed in Section 3, lim-
iting the grammar to only 2000 fully abstract rules
may not have been enough, since additional abstract
rules applied fairly frequently in test data if they
were available. We plan to experiment with larger
filtering cutoffs in future work. A complementary
solution could be to increase the number of par-
tially lexicalized rules. Although we found mixed
results in their application within our current sys-
tem, the success of Hiero-derived MT systems (Chi-
ang, 2005; Chiang, 2010) shows that high transla-
tion quality can be achieved with rules that are only
partially abstract. A major difference between such
systems and our current implementation is that ours,
at 102,000 rules, has a much smaller grammar.
Acknowledgments
This research was supported in part by U.S. Na-
tional Science Foundation grants IIS-0713402 and
IIS-0915327, as well as by the DARPA GALE pro-
gram. Thanks to Kenneth Heafield for processing
the English monolingual data and building the lan-
guage model file, and to Jonathan Clark for Loony-
Bin support and bug fixes. We also thank Yahoo!
for the use of the M45 research computing clus-
ter, where we ran many steps of our experimental
pipeline.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, MI, June.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July.
370
Jonathan Clark and Alon Lavie. 2010. LoonyBin: Keep-
ing language technologists sane through automated
management of experimental (hyper)workflows. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, pages 1301?
1308, Valletta, Malta, May.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, pages 49?57, Columbus, OH, June.
Greg Hanneman, Jonathan Clark, and Alon Lavie. 2010.
Improved features and grammar selection for syntax-
based MT. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 82?87, Uppsala, Sweden, July.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 48?54, Edmonton,
Alberta, May?June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of IWSLT 2005, Pittsburgh, PA, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Juan Pino, Gonzalo Iglesias, Adria` de Gispert, Graeme
Blackwood, Jaime Brunning, and William Byrne.
2010. The CUED HiFST system for the WMT10
translation shared task. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 155?160, Uppsala, Sweden,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901?904, Denver, CO, September.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
371
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 261?266,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The CMU-Avenue French-English Translation System
Michael Denkowski Greg Hanneman Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mdenkows,ghannema,alavie}@cs.cmu.edu
Abstract
This paper describes the French-English trans-
lation system developed by the Avenue re-
search group at Carnegie Mellon University
for the Seventh Workshop on Statistical Ma-
chine Translation (NAACL WMT12). We
present a method for training data selection,
a description of our hierarchical phrase-based
translation system, and a discussion of the im-
pact of data size on best practice for system
building.
1 Introduction
We describe the French-English translation sys-
tem constructed by the Avenue research group at
Carnegie Mellon University for the shared trans-
lation task in the Seventh Workshop on Statistical
Machine Translation. The core translation system
uses the hierarchical phrase-based model described
by Chiang (2007) with sentence-level grammars ex-
tracted and scored using the methods described by
Lopez (2008). Improved techniques for data selec-
tion and monolingual text processing significantly
improve the performance of the baseline system.
Over half of all parallel data for the French-
English track is provided by the Giga-FrEn cor-
pus (Callison-Burch et al, 2009). Assembled from
crawls of bilingual websites, this corpus is known to
be noisy, containing sentences that are either not par-
allel or not natural language. Rather than simply in-
cluding or excluding the resource in its entirety, we
use a relatively simple technique inspired by work in
machine translation quality estimation to select the
best portions of the corpus for inclusion in our train-
ing data. Including around 60% of the Giga-FrEn
chosen by this technique yields an improvement of
0.7 BLEU.
Prior to model estimation, we process all parallel
and monolingual data using in-house tokenization
and normalization scripts that detect word bound-
aries better than the provided WMT12 scripts. After
translation, we apply a monolingual rule-based post-
processing step to correct obvious errors and make
sentences more acceptable to human judges. The
post-processing step alone yields an improvement of
0.3 BLEU to the final system.
We conclude with a discussion of the impact of
data size on important decisions for system building.
Experimental results show that ?best practice? deci-
sions for smaller data sizes do not necessarily carry
over to systems built with ?WMT-scale? data, and
provide some explanation for why this is the case.
2 Training Data
Training data provided for the French-English trans-
lation task includes parallel corpora taken from Eu-
ropean Parliamentary proceedings (Koehn, 2005),
news commentary, and United Nations documents.
Together, these sets total approximately 13 million
sentences. In addition, a large, web-crawled parallel
corpus termed the ?Giga-FrEn? (Callison-Burch et
al., 2009) is made available. While this corpus con-
tains over 22 million parallel sentences, it is inher-
ently noisy. Many parallel sentences crawled from
the web are neither parallel nor sentences. To make
use of this large data source, we employ data se-
lection techniques discussed in the next subsection.
261
Corpus Sentences
Europarl 1,857,436
News commentary 130,193
UN doc 11,684,454
Giga-FrEn 1stdev 7,535,699
Giga-FrEn 2stdev 5,801,759
Total 27,009,541
Table 1: Parallel training data
Parallel data used to build our final system totals 27
million sentences. Precise figures for the number of
sentences in each data set, including selections from
the Giga-FrEn, are found in Table 1.
2.1 Data Selection as Quality Estimation
Drawing inspiration from the workshop?s featured
task, we cast the problem of data selection as one
of quality estimation. Specia et al (2009) report
several estimators of translation quality, the most ef-
fective of which detect difficult-to-translate source
sentences, ungrammatical translations, and transla-
tions that align poorly to their source sentences. We
can easily adapt several of these predictive features
to select good sentence pairs from noisy parallel cor-
pora such as the Giga-FrEn.
We first pre-process the Giga-FrEn by removing
lines with invalid Unicode characters, control char-
acters, and insufficient concentrations of Latin char-
acters. We then score each sentence pair in the re-
maining set (roughly 90% of the original corpus)
with the following features:
Source language model: a 4-gram modified
Kneser-Ney smoothed language model trained on
French Europarl, news commentary, UN doc, and
news crawl corpora. This model assigns high scores
to grammatical source sentences and lower scores to
ungrammatical sentences and non-sentences such as
site maps, large lists of names, and blog comments.
Scores are normalized by number of n-grams scored
per sentence (length + 1). The model is built using
the SRILM toolkit (Stolke, 2002).
Target language model: a 4-gram modified
Kneser-Ney smoothed language model trained on
English Europarl, news commentary, UN doc, and
news crawl corpora. This model scores grammati-
cality on the target side.
Word alignment scores: source-target and
target-source MGIZA++ (Gao and Vogel, 2008)
force-alignment scores using IBM Model 4 (Och
and Ney, 2003). Model parameters are estimated
on 2 million words of French-English Europarl and
news commentary text. Scores are normalized by
the number of alignment links. These features mea-
sure the extent to which translations are parallel with
their source sentences.
Fraction of aligned words: source-target and
target-source ratios of aligned words to total words.
These features balance the link-normalized align-
ment scores.
To determine selection criteria, we use this feature
set to score the news test sets from 2008 through
2011 (10K parallel sentences) and calculate the
mean and standard deviation of each feature score
distribution. We then select two subsets of the Giga-
FrEn, ?1stdev? and ?2stdev?. The 1stdev set in-
cludes sentence pairs for which the score for each
feature is above a threshold defined as the develop-
ment set mean minus one standard deviation. The
2stdev set includes sentence pairs not included in
1stdev that meet the per-feature threshold of mean
minus two standard deviations. Hard, per-feature
thresholding is motivated by the notion that a sen-
tence pair must meet al the criteria discussed above
to constitute good translation. For example, high
source and target language model scores are irrel-
evant if the sentences are not parallel.
As primarily news data is used for determining
thresholds and building language models, this ap-
proach has the added advantage of preferring par-
allel data in the domain we are interested in translat-
ing. Our final translation system uses data from both
1stdev and 2stdev, corresponding to roughly 60% of
the Giga-FrEn corpus.
2.2 Monolingual Data
Monolingual English data includes European Parlia-
mentary proceedings (Koehn, 2005), news commen-
tary, United Nations documents, news crawl, the En-
glish side of the Giga-FrEn, and the English Giga-
word Fourth Edition (Parker et al, 2009). We use all
available data subject to the following selection de-
cisions. We apply the initial filter to the Giga-FrEn
to remove non-text sections, leaving approximately
90% of the corpus. We exclude the known prob-
262
Corpus Words
Europarl 59,659,916
News commentary 5,081,368
UN doc 286,300,902
News crawl 1,109,346,008
Giga-FrEn 481,929,410
Gigaword 4th edition 1,960,921,287
Total 3,903,238,891
Table 2: Monolingual language modeling data (uniqued)
lematic New York Times section of the Gigaword.
As many data sets include repeated boilerplate text
such as copyright information or browser compat-
ibility notifications, we unique sentences from the
UN doc, news crawl, Giga-FrEn, and Gigaword sets
by source. Final monolingual data totals 4.7 billion
words before uniqueing and 3.9 billion after. Word
counts for all data sources are shown in Table 2.
2.3 Text Processing
All monolingual and parallel system data is run
through a series of pre-processing steps before
construction of the language model or translation
model. We first run an in-house normalization script
over all text in order to convert certain variably en-
coded characters to a canonical form. For example,
thin spaces and non-breaking spaces are normalized
to standard ASCII space characters, various types of
?curly? and ?straight? quotation marks are standard-
ized as ASCII straight quotes, and common French
and English ligatures characters (e.g. ?, fi) are re-
placed with standard equivalents.
English text is tokenized with the Penn Treebank-
style tokenizer attached to the Stanford parser (Klein
and Manning, 2003), using most of the default op-
tions. We set the tokenizer to Americanize vari-
ant spellings such as color vs. colour or behavior
vs. behaviour. Currency-symbol normalization is
avoided.
For French text, we use an in-house tokenization
script. Aside from the standard tokenization based
on punctuation marks, this step includes French-
specific rules for handling apostrophes (French eli-
sion), hyphens in subject-verb inversions (includ-
ing the French t euphonique), and European-style
numbers. When compared to the default WMT12-
provided tokenization script, our custom French
rules more accurately identify word boundaries, par-
ticularly in the case of hyphens. Figure 1 highlights
the differences in sample phrases. Subject-verb in-
versions are broken apart, while other hyphenated
words are unaffected; French aujourd?hui (?today?)
is retained as a single token to match English.
Parallel data is run through a further filtering step
to remove sentence pairs that, by their length char-
acteristics alone, are very unlikely to be true parallel
data. Sentence pairs that contain more than 95 to-
kens on either side are globally discarded, as are sen-
tence pairs where either side contains a token longer
than 25 characters. Remaining pairs are checked for
length ratio between French and English, and sen-
tences are discarded if their English translations are
either too long or too short given the French length.
Allowable ratios are determined from the tokenized
training data and are set such that approximately the
middle 95% of the data, in terms of length ratio, is
kept for each French length.
3 Translation System
Our translation system uses cdec (Dyer et al,
2010), an implementation of the hierarchical phrase-
based translation model (Chiang, 2007) that uses the
KenLM library (Heafield, 2011) for language model
inference. The system translates from cased French
to cased English; at no point do we lowercase data.
The Parallel data is aligned in both directions us-
ing the MGIZA++ (Gao and Vogel, 2008) imple-
mentation of IBM Model 4 and symmetrized with
the grow-diag-final heuristic (Och and Ney,
2003). The aligned corpus is then encoded as a
suffix array to facilitate sentence-level grammar ex-
traction and scoring (Lopez, 2008). Grammars are
extracted using the heuristics described by Chiang
(Chiang, 2007) and feature scores are calculated ac-
cording to Lopez (2008).
Modified Knesser-Ney smoothed (Chen and
Goodman, 1996) n-gram language models are built
from the monolingual English data using the SRI
language modeling toolkit (Stolke, 2002). We ex-
periment with both 4-gram and 5-gram models.
System parameters are optimized using minimum
error rate training (Och, 2003) to maximize the
corpus-level cased BLEU score (Papineni et al,
263
Base: Y a-t-il un colle`gue pour prendre la parole
Custom: Y a -t-il un colle`gue pour prendre la parole
Base: Peut-e?tre , a` ce sujet , puis-je dire a` M. Ribeiro i Castro
Custom: Peut-e?tre , a` ce sujet , puis -je dire a` M. Ribeiro i Castro
Base: le proce`s-verbal de la se?ance d? aujourd? hui
Custom: le proce`s-verbal de la se?ance d? aujourd?hui
Base: s? e?tablit environ a` 1,2 % du PIB
Custom: s? e?tablit environ a` 1.2 % du PIB
Figure 1: Customized French tokenization rules better identify word boundaries.
pre?-e?l?ectoral ? pre-electoral
mosa??que ? mosaique
de?ragulation ? deragulation
Figure 2: Examples of cognate translation
2002) on news-test 2008 (2051 sentences). This de-
velopment set is chosen for its known stability and
reliability.
Our baseline translation system uses Viterbi de-
coding while our final system uses segment-level
Minimum Bayes-Risk decoding (Kumar and Byrne,
2004) over 500-best lists using 1 - BLEU as the loss
function.
3.1 Post-Processing
Our final system includes a monolingual rule-based
post-processing step that corrects obvious transla-
tion errors. Examples of correctable errors include
capitalization, mismatched punctuation, malformed
numbers, and incorrectly split compound words. We
finally employ a coarse cognate translation system
to handle out-of-vocabulary words. We assume that
uncapitalized French source words passed through
to the English output are cognates of English words
and translate them by removing accents. This fre-
quently leads to (in order of desirability) fully cor-
rect translations, correct translations with foreign
spellings, or correct translations with misspellings.
All of the above are generally preferable to untrans-
lated foreign words. Examples of cognate transla-
tions for OOV words in newstest 2011 are shown in
Figure 2.1
1Some OOVs are caused by misspellings in the dev-test
source sentences. In these cases we can salvage misspelled En-
glish words in place of misspelled French words
BLEU (cased) Meteor TER
base 5-gram 28.4 27.4 33.7 53.2
base 4-gram 29.1 28.1 34.0 52.5
+1stdev GFE 29.3 28.3 34.2 52.1
+2stdev GFE 29.8 28.9 34.5 51.7
+5g/1K/MBR 29.9 29.0 34.5 51.5
+post-process 30.2 29.2 34.7 51.3
Table 3: Newstest 2011 (dev-test) translation results
4 Experiments
Beginning with a baseline translation system, we in-
crementally evaluate the contribution of additional
data and components. System performance is eval-
uated on newstest 2011 using BLEU (uncased and
cased) (Papineni et al, 2002), Meteor (Denkowski
and Lavie, 2011), and TER (Snover et al, 2006).
For full consistency with WMT11, we use the NIST
scoring script, TER-0.7.25, and Meteor-1.3 to eval-
uate cased, detokenized translations. Results are
shown in Table 3, where each evaluation point is the
result of a full tune/test run that includes MERT for
parameter optimization.
The baseline translation system is built from 14
million parallel sentences (Europarl, news commen-
tary, and UN doc) and all monolingual data. Gram-
mars are extracted using the ?tight? heuristic that
requires phrase pairs to be bounded by word align-
ments. Both 4-gram and 5-gram language models
are evaluated. Viterbi decoding is conducted with a
cube pruning pop limit (Chiang, 2007) of 200. For
this data size, the 4-gram model is shown to signifi-
cantly outperform the 5-gram.
Adding the 1stdev and 2stdev sets from the Giga-
FrEn increases the parallel data size to 27 million
264
BLEU (cased) Meteor TER
587M tight 29.1 28.1 34.0 52.5
587M loose 29.3 28.3 34.0 52.5
745M tight 29.8 28.9 34.5 51.7
745M loose 29.6 28.6 34.3 52.0
Table 4: Results for extraction heuristics (dev-test)
sentences and further improves performance. These
runs require new grammars to be extracted, but
use the same 4-gram language model and decoding
method as the baseline system. With large training
data, moving to a 5-gram language model, increas-
ing the cube pruning pop limit to 1000, and using
Minimum Bayes-Risk decoding (Kumar and Byrne,
2004) over 500-best lists collectively show a slight
improvement. Monolingual post-processing yields
further improvement. This decoding/processing
scheme corresponds to our final translation system.
4.1 Impact of Data Size
The WMT French-English track provides an oppor-
tunity to experiment in a space of data size that is
generally not well explored. We examine the impact
of data sizes of hundreds of millions of words on
two significant system building decisions: grammar
extraction and language model estimation. Compar-
ative results are reported on the newstest 2011 set.
In the first case, we compare the ?tight? extrac-
tion heuristic that requires phrases to be bounded
by word alignments to the ?loose? heuristic that al-
lows unaligned words at phrase edges. Lopez (2008)
shows that for a parallel corpus of 107 million
words, using the loose heuristic produces much
larger grammars and improves performance by a full
BLEU point. However, even our baseline system
is trained on substantially more data (587 million
words on the English side) and the addition of the
Giga-FrEn sets increases data size to 745 million
words, seven times that used in the cited work. For
each data size, we decode with grammars extracted
using each heuristic and a 4-gram language model.
As shown in Table 4, the differences are much
smaller and the tight heuristic actually produces the
best result for the full data scenario. We believe
this to be directly linked to word alignment quality:
smaller training data results in sparser, noisier word
BLEU (cased) Meteor TER
587M 4-gram 29.1 28.1 34.0 52.5
587M 5-gram 28.4 27.4 33.7 53.2
745M 4-gram 29.8 28.9 34.5 51.7
745M 5-gram 29.8 28.9 34.4 51.7
Table 5: Results for language model order (dev-test)
alignments while larger data results in denser, more
accurate alignments. In the first case, accumulating
unaligned words can make up for shortcomings in
alignment quality. In the second, better rules are ex-
tracted by trusting the stronger alignment model.
We also compare 4-gram and 5-gram language
model performance with systems using tight gram-
mars extracted from 587 million and 745 million
sentences. As shown in Table 5, the 4-gram sig-
nificantly outperforms the 5-gram with smaller data
while the two are indistinguishable with larger data2.
With modified Kneser-Ney smoothing, a lower or-
der model will outperform a higher order model if
the higher order model constantly backs off to lower
orders. With stronger grammars learned from larger
parallel data, the system is able to produce output
that matches longer n-grams in the language model.
5 Summary
We have presented the French-English translation
system built for the NAACL WMT12 shared transla-
tion task, including descriptions of our data selection
and text processing techniques. Experimental re-
sults have shown incremental improvement for each
addition to our baseline system. We have finally
discussed the impact of the availability of WMT-
scale data on system building decisions and pro-
vided comparative experimental results.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of ACL WMT 2009.
2We find that for the full data system, also increasing the
cube pruning pop limit and using MBR decoding yields a very
slight improvement with the 5-gram model over the same de-
coding scheme with the 4-gram.
265
Stanley F. Chen and Joshua Goodman. 1996. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. In Proc. of ACL 1996.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proc. of
the EMNLP WMT 2011.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models. In
Proc. of ACL 2010.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Proc. of ACL
WSETQANLP 2008.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. of EMNLP WMT
2011.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL 2003.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of MT Sum-
mit 2005.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Transla-
tion. In Proc. of NAACL/HLT 2004.
Adam Lopez. 2008. Tera-Scale Translation Models via
Pattern Matching. In Proc. of COLING 2008.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of ACL
2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL 2002.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. Linguistic Data Consortium, LDC2009T13.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA 2006.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
Confidence of Machine Translation Quality Estimates.
In Proc. of MT Summit XII.
Andreas Stolke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002.
266
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2014
Austin Matthews Waleed Ammar Archna Bhatia Weston Feely
Greg Hanneman Eva Schlinger Swabha Swayamdipta Yulia Tsvetkov
Alon Lavie Chris Dyer
?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?
Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submitted
to the 2014 WMT shared translation task.
We participated in two language pairs,
German?English and Hindi?English. Our
innovations include: a label coarsening
scheme for syntactic tree-to-tree transla-
tion, a host of new discriminative features,
several modules to create ?synthetic trans-
lation options? that can generalize beyond
what is directly observed in the training
data, and a method of combining the out-
put of multiple word aligners to uncover
extra phrase pairs and grammar rules.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute partici-
pated in two language pairs for the 2014 Workshop
on Machine Translation shared translation task:
German?English and Hindi?English. Our systems
showcase our multi-phase approach to translation,
in which synthetic translation options supple-
ment the default translation rule inventory that is
extracted from word-aligned training data.
In the German?English system, we used our
compound splitter (Dyer, 2009) to reduce data
sparsity, and we allowed the translator to back
off to translating lemmas when it detected case-
inflected OOVs. We also demonstrate our group?s
syntactic system with coarsened nonterminal types
(Hanneman and Lavie, 2011) as a contrastive
German?English submission.
In both the German?English and Hindi?English
systems, we used an array of supplemental ideas to
enhance translation quality, ranging from lemma-
tization and synthesis of inflected phrase pairs to
novel reordering and rule preference features.
2 Core System Components
The decoder infrastructure we used was cdec
(Dyer et al., 2010). For our primary systems,
all data was tokenized using cdec?s tokenization
tool. Only the constrained data resources pro-
vided for the shared task were used for training
both the translation and language models. Word
alignments were generated using both FastAlign
(Dyer et al., 2013) and GIZA++ (Och and Ney,
2003). All our language models were estimated
using KenLM (Heafield, 2011). Translation model
parameters were chosen using MIRA (Eidelman,
2012) to optimize BLEU (Papineni et al., 2002)
on a held-out development set.
Our data was filtered using qe-clean
(Denkowski et al., 2012), with a cutoff of
two standard deviations from the mean. All
data was left in fully cased form, save the first
letter of each segment, which was changed to
whichever form the first token more commonly
used throughout the data. As such, words like The
were lowercased at the beginning of segments,
while words like Obama remained capitalized.
Our primary German?English and Hindi?
English systems were Hiero-based (Chiang,
2007), while our contrastive German?English sys-
tem used cdec?s tree-to-tree SCFG formalism.
Before submitting, we ran cdec?s implementa-
tion of MBR on 500-best lists from each of our
systems. For both language pairs, we used the
Nelder?Mead method to optimize the MBR pa-
rameters. In the German?English system, we ran
MBR on 500 hypotheses, combining the output of
the Hiero and tree-to-tree systems.
The remainder of the paper will focus on our
primary innovations in the two language pairs.
142
3 Common System Improvements
A number of our techniques were used for both our
German?English and Hindi?English primary sub-
missions. These techniques each fall into one of
three categories: those that create translation rules,
those involving language models, or those that add
translation features. A comparison of these tech-
niques and their performance across the two lan-
guage pairs can be found in Section 6.
3.1 Rule-Centric Enhancements
While many of our methods of enhancing the
translation model with extra rules are language-
specific, three were shared between language
pairs.
First, we added sentence-boundary tokens <s>
and </s> to the beginning and end of each line in
the data, on both the source and target sides.
Second, we aligned all of our training data us-
ing both FastAlign and GIZA++ and simply con-
catenated two copies of the training corpus, one
aligned with each aligner, and extracted rules from
the resulting double corpus.
Third, we hand-wrote a list of rules that trans-
form numbers, dates, times, and currencies into
well-formed English equivalents, handling differ-
ences such as the month and day reversal in dates
or conversion from 24-hour time to 12-hour time.
3.2 Employed Language Models
Each of our primary systems uses a total of three
language models.
The first is a traditional 4-gram model gen-
erated by interoplating LMs built from each of
the available monolingual corpora. Interpolation
weights were calculated used the SRILM toolkit
(Stolcke, 2002) and 1000 dev sentences from the
Hindi?English system.
The second is a model trained on word clus-
ters instead of surface forms. For this we mapped
the LM vocabulary into 600 clusters based on the
algorithm of Brown et al. (1992) and then con-
structed a 7-gram LM over the resulting clusters,
allowing us to capture more context than our tra-
ditional surface-form language model.
The third is a bigram model over the source side
of each language?s respective bitext. However, at
run time this LM operates on the target-side out-
put of the translator, just like the other two. The
intuition here is that if a source-side LM likes our
output, then we are probably passing through more
than we ought to.
Both source and target surface-form LM used
modified Kneser-Ney smoothing (Kneser and Ney,
1995), while the model over Brown clusters
(Brown et al., 1992) used subtract-0.5 smoothing.
3.3 New Translation Features
In addition to the standard array of features, we
added four new indicator feature templates, lead-
ing to a total of nearly 150,000 total features.
The first set consists of target-side n-gram fea-
tures. For each bigram of Brown clusters in the
output string generated by our translator, we fire
an indicator feature. For example, if we have the
sentence, Nato will ihren Einfluss im Osten st?arken
translating as NATO intends to strengthen its influ-
ence in the East, we will fire an indicator features
NGF C367 C128=1, NGF C128 C31=1, etc.
The second set is source-language n-gram fea-
tures. Similar to the previous feature set, we fire
an indicator feature for each ngram of Brown clus-
ters in the output. Here, however, we use n = 1,
and we use the map of source language words to
Brown clusters, rather than the target language?s,
despite the fact that this is examining target lan-
guage output. The intuition here is to allow this
feature to penalize passthroughs differently de-
pending on their source language Brown cluster.
For example, passing through the German word
zeitung (?newspaper?) is probably a bad idea, but
passing through the German word Obama proba-
bly should not be punished as severely.
The third type of feature is source path features.
We can imagine translation as a two-step process
in which we first permute the source words into
some order, then translate them phrase by phrase.
This set of features examines that intermediate
string in which the source words have been per-
muted. Again, we fire an indicator feature for each
bigram in this intermediate string, this time using
surface lexical forms directly instead of first map-
ping them to Brown clusters.
Lastly, we create a new type of rule shape fea-
ture. Traditionally, rule shape features have indi-
cated, for each rule, the sequence of terminal and
non-terminal items on the right-hand side. For ex-
ample, the rule [X] ? der [X] :: the [X] might
have an indicator feature Shape TN TN, where
T represents a terminal and N represents a non-
terminal. One can also imagine lexicalizing such
rules by replacing each T with its surface form.
We believe such features would be too sparse, so
instead of replacing each terminal by its surface
form, we instead replace it with its Brown cluster,
143
creating a feature like Shape C37 N C271 N.
4 Hindi?English Specific Improvements
In addition to the enhancements common to the
two primary systems, our Hindi?English system
includes improved data cleaning of development
data, a sophisticated linguistically-informed tok-
enization scheme, a transliteration module, a syn-
thetic phrase generator that improves handling of
function words, and a synthetic phrase generator
that leverages source-side paraphrases. We will
discuss each of these five in turn.
4.1 Development Data Cleaning
Due to a scarcity of clean development data, we
augmented the 520 segments provided with 480
segments randomly drawn from the training data
to form our development set, and drew another
random 1000 segments to serve as a dev test set.
After observing large discrepencies between the
types of segments in our development data and the
well-formed news domain sentences we expected
to be tested on, we made the decision to prune our
tuning set by removing any segment that did not
appear to be a full sentence on both the Hindi and
English sides. While this reduced our tuning set
from 1000 segments back down to 572 segments,
we believe it to be the single largest contributor to
our success on the Hindi?English translation task.
4.2 Nominal Normalization
Another facet of our system was normalization of
Hindi nominals. The Hindi nominal system shows
much more morphological variation than English.
There are two genders (masculine and feminine)
and at least six noun stem endings in pronuncia-
tion and 10 in writing.
The pronominal system also is much richer than
English with many variants depending on whether
pronouns appear with case markers or other post-
positions.
Before normalizing the nouns and pronouns, we
first split these case markers / postpositions from
the nouns / pronouns to result in two words in-
stead of the original combined form. If the case
marker was n (ne), the ergative case marker in
Hindi, we deleted it as it did not have any trans-
lation in English. All the other postpositions were
left intact while splitting from and normalizing the
nouns and pronouns.
These changes in stem forms contribute to the
sparsity in data; hence, to reduce this sparsity, we
construct for each input segment an input lattice
that allows the decoder to use the split or original
forms of all nouns or pronouns, as well as allowing
it to keep or delete the case marker ne.
4.3 Transliteration
We used the 12,000 Hindi?English transliteration
pairs from the ACL 2012 NEWS workshop on
transliteration to train a linear-chained CRF tag-
ger
1
that labels each character in the Hindi token
with a sequence of zero or more English characters
(Ammar et al., 2012). At decoding, unseen Hindi
tokens are fed to the transliterator, which produces
the 100 most probable transliterations. We add
a synthetic translation option for each candidate
transliteration.
In addition to this sophisticated transliteration
scheme, we also employ a rule-based translitera-
tor that specifically targets acronyms. In Hindi,
many acronyms are spelled out phonetically, such
as NSA being rendered as enese (en.es.e). We
detected such words in the input segments and
generated synthetic translation options both with
and without periods (e.g. N.S.A. and NSA).
4.4 Synthetic Handling of Function Words
In different language pairs, individual source
words may have many different possible trans-
lations, e.g., when the target language word has
many different morphological inflections or is sur-
rounded by different function words that have no
direct counterpart in the source language. There-
fore, when very large quantities of parallel data
are not available, we can expect our phrasal inven-
tory to be incomplete. Synthetic translation option
generation seeks to fill these gaps using secondary
generation processes that exploit existing phrase
pairs to produce plausible phrase translation alter-
natives that are not directly extractable from the
training data (Tsvetkov et al., 2013; Chahuneau et
al., 2013).
To generate synthetic phrases, we first remove
function words from the source and target sides
of existing non-gappy phrase pairs. We manually
constructed English and Hindi lists of common
function words, including articles, auxiliaries, pro-
nouns, and adpositions. We then employ the
SRILM hidden-ngram utility (Stolcke, 2002) to re-
store missing function words according to an n-
gram language model probability, and add the re-
sulting synthetic phrases to our phrase table.
1
https://github.com/wammar/transliterator
144
4.5 Paraphrase-Based Synthetic Phrases
We used a graph-based method to obtain transla-
tion distributions for source phrases that are not
present in the phrase table extracted from the par-
allel corpus. Monolingual data is used to construct
separate similarity graphs over phrases (word se-
quences or n-grams), using distributional features
extracted from the corpora. The source similar-
ity graph consists of phrase nodes representing se-
quences of words in the source language. In our
instance, we restricted the phrases to bigrams, and
the bigrams come from both the phrase table (the
labeled phrases) and from the evaluation set but
not present in the phrase table (unlabeled phrases).
The labels for these source phrases, namely the
target phrasal inventory, can also be represented
in a graph form, where the distributional features
can also be computed from the target monolingual
data. Translation information is then propagated
from the labeled phrases to the unlabeled phrases
in the source graph, proportional to how similar
the phrases are to each other on the source side,
as well as how similar the translation candidates
are to each other on the target side. The newly
acquired translation distributions for the unlabeled
phrases are written out to a secondary phrase table.
For more information, see Saluja et al. (2014).
5 German?English Specific
Improvements
Our German?English system also had its own
suite of tricks, including the use of ?pseudo-
references? and special handling of morphologi-
cally inflected OOVs.
5.1 Pseudo-References
The development sets provided have only a sin-
gle reference, which is known to be sub-optimal
for tuning of discriminative models. As such,
we use the output of one or more of last year?s
top performing systems as pseudo-references dur-
ing tuning. We experimented with using just one
pseudo-reference, taken from last year?s Spanish?
English winner (Durrani et al., 2013), and with
using four pseudo-references, including the out-
put of last year?s winning Czech?English, French?
English, and Russian?English systems (Pino et al.,
2013).
5.2 Morphological OOVs
Examination of the output of our baseline sys-
tems lead us to conclude that the majority of our
system?s OOVs were due to morphologically in-
flected nouns in the input data, particularly those
in genitive case. As such, for each OOV in the
input, we attempt to remove the German genitive
case marker -s or -es. We then run the resulting
form f through our baseline translator to obtain a
translation e of the lemma. Finally, we add two
translation rules to our translation table: f ? e,
and f ? e?s.
6 Results
As we added each feature to our systems, we
first ran a one-off experiment comparing our base-
line system with and without each individual fea-
ture. The results of that set of experiments are
shown in Table 1 for Hindi?English and Table 2
for German?English. Features marked with a *
were not included in our final system submission.
The most surprising result is the strength of
our Hindi?English baseline system. With no extra
bells or whistles, it is already half a BLEU point
ahead of the second best system submitted to this
shared task. We believe this is due to our filter-
ing of the tuning set, which allowed our system to
generate translations more similar in length to the
final test set.
Another interesting result is that only one fea-
ture set, namely our rule shape features based on
Brown clusters, helped on the test set in both lan-
guage pairs. No feature hurt the BLEU score on
the test set in both language pairs, meaning the
majority of features helped in one language and
hurt in the other.
If we compare results on the tuning sets, how-
ever, some clearer patterns arise. Brown cluster
language models, n-gram features, and our new
rule shape features all helped.
Furthermore, there were a few features, such as
the Brown cluster language model and tuning to
Meteor (Denkowski and Lavie, 2011), that helped
substantially in one language pair while just barely
hurting the other. In particular, the fact that tuning
to Meteor instead of BLEU can actually help both
BLEU and Meteor scores was rather unexpected.
7 German?English Syntax System
In addition to our primary German?English sys-
tem, we also submitted a contrastive German?
English system showcasing our group?s tree-to-
tree syntax-based translation formalism.
145
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 15.7 25.3 68.0 11.4 22.9 70.3
*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3
Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0
Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2
Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3
Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8
*Source LM 14.2 25.1 72.1 11.3 23.0 72.3
N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2
Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5
Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4
Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6
Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3
CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1
Acronym Translit. 15.8 25.8 68.8 12.4 23.4 70.2
Synth. Func. Words 15.7 25.3 67.8 11.4 22.8 70.4
Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9
Final Submission 16.7
Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?
English system. Each line is the baseline plus that one feature, non-cumulatively. Lines marked with a *
were not included in our final WMT submission.
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 25.3 30.4 52.6 26.2 31.3 53.6
*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4
Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3
Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6
Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4
Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2
*Source LM 25.8 30.6 52.4 26.4 31.5 53.4
N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0
Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4
Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3
Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2
One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3
*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1
OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3
Final Submission 27.1
Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero
German?English system. Each line is the baseline plus that one feature, non-cumulatively.
Dev (2013) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 20.98 29.81 58.47 18.65 28.72 61.80
+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16
+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87
+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51
+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18
+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73
Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?English
system. The system in the bottom line was submitted to WMT as a contrastive entry.
7.1 Basic System Construction
Since all training data for the tree-to-tree system
must be parsed in addition to being word-aligned,
we prepared separate copies of the training, tun-
ing, and testing data that are more suitable for in-
put into constituency parsing. Importantly, we left
the data in its original mixed-case format. We used
the Stanford tokenizer to replicate Penn Treebank
tokenization on the English side. On the German
side, we developed new in-house normalization
and tokenization script.
We filtered tokenized training sentences by sen-
146
tence length, token length, and sentence length ra-
tio. The final corpus for parsing and word align-
ment contained 3,897,805 lines, or approximately
86 percent of the total training resources released
under the WMT constrained track. Word align-
ment was carried out using FastAlign (Dyer et
al., 2013), while for parsing we used the Berke-
ley parser (Petrov et al., 2006).
Given the parsed and aligned corpus, we ex-
tracted synchronous context-free grammar rules
using the method of Hanneman et al. (2011).
In addition to aligning subtrees that natively ex-
ist in the input trees, our grammar extractor also
introduces ?virtual nodes.? These are new and
possibly overlapping constituents that subdivide
regions of flat structure by combining two adja-
cent sibling nodes into a single nonterminal for
the purposes of rule extraction. Virtual nodes
are similar in spirit to the ?A+B? extended cate-
gories of SAMT (Zollmann and Venugopal, 2006),
and their nonterminal labels are constructed in the
same way, but with the added restriction that they
do not violate any existing syntactic structure in
the parse tree.
7.2 Improvements
Nonterminals in our tree-to-tree grammar are
made up of pairs of symbols: one from the source
side and one from the target side. With virtual
nodes included, this led to an initial German?
English grammar containing 153,219 distinct non-
terminals ? a far larger set than is used in SAMT,
tree-to-string, string-to-tree, or Hiero systems. To
combat the sparsity introduce by this large nonter-
minal set, we coarsened the label set with an ag-
glomerative label-clustering technique(Hanneman
and Lavie, 2011; Hanneman and Lavie, 2013).
The stopping point was somewhat arbitrarily cho-
sen to be a grammar of 916 labels.
Table 3 shows a significant improvement in
translation quality due to coarsening the label set:
approximately +1.8 BLEU, +0.6 Meteor, and ?1.6
TER on our dev test set, newtest2012.
2
In the MERT runs, however, we noticed that the
length of the MT output can be highly variable,
ranging on the tuning set from a low of 92.8% of
the reference length to a high of 99.1% in another.
We were able to limit this instability by tuning to
Meteor instead of BLEU. Aside from a modest
2
We follow the advice of Clark et al. (2011) and eval-
uate our tree-to-tree experiments over multiple independent
MERT runs. All scores in Table 3 are averages of two or
three runs, depending on the row.
score improvement, we note that the variability in
length ratio is reduced from 6.3% to 2.8%.
Specific difficulties of the German?English lan-
guage pair led to three additional system compo-
nents to try to combat them.
First, we introduced a second language model
trained on Brown clusters instead of surface forms.
Next we attempted to overcome the sparsity
of German input by making use of cdec?s lattice
input functionality introduce compound-split ver-
sions of dev and test sentences.
Finally, we attempted to improve our grammar?s
coverage of new German words by introducing
synthetic rules for otherwise out-of-vocabulary
items. Each token in a test sentence that the gram-
mar cannot translate generates a synthetic rule al-
lowing the token to be translated as itself. The left-
hand-side label is decided heuristically: a (coars-
ened) ?noun? label if the German OOV starts with
a capital letter, a ?number? label if the OOV con-
tains only digits and select punctuation characters,
an ?adjective? label if the OOV otherwise starts
with a lowercase letter or a number, or a ?symbol?
label for anything left over.
The effect of all three of these improvements
combined is shown in the fourth row of Table 3.
By default our previous experiments were per-
formed with a span limit of 12 tokens. Increasing
this limit to 15 has a mixed effect on metric scores,
as shown in the fifth row of Table 3. Since two out
of three metrics report improvement, we left the
longer span limit in effect in our final system.
Our final improvement was to augment our tun-
ing set with the same set of pseudo-references
as our Hiero systems. We found that using one
pseudo-reference versus four pseudo-references
had negligible effect on the (single-reference) tun-
ing scores, but four produced a better improve-
ment on the test set.
The best MERT run of this final system (bottom
line of Table 3) was submitted to the WMT 2014
evaluation as a contrastive entry.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
147
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine transla-
tion systems for european language pairs.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 406?414. Association for Computational Lin-
guistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-
pert, Federico Flego, and William Byrne. 2013.
The university of cambridge russian-english system
at wmt13.
148
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, Maryland, June.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
149
