Sinica Treebank: 
Design Criteria, Annotation Guidelines, and On-line Interface 
Chu-Ren Huang t, Feng-Yi Chen 2, Keh-Jiann Chen 2, Zhao-ming Gao s, & 
Kuang-Yu Chen 2 
churen(a3,sinica.edu.tw, aoole(~jis.sinica.edu.tw, kchen(~,iis.sinica.edu.tw, 
zmgao@ccms.ntu.edu.tw, sasami@iis.sinica.edu.tw 
=Institute ofLinguistics, Academia Siniea, Taipei, Taiwan 
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
3Dept. of Foreign Languages & Literatures, National Taiwan University, Taipei, Taiwan 
Abstract 
This paper describes the design 
criteria and annotation guidelines of 
Sinica Treebank. The three design 
criteria are: Maximal Resource Sharing, 
Minimal Structural Complexity, and 
Optimal Semantic Information. One of 
the important design decisions 
following these criteria is the encoding 
of thematic role information. An on-line 
interface facilitating empirical studies of 
Chinese phrase structure is also 
described. 
1. Introduction 
The Penn Treebank (Marcus et al 
1993) initiated a new paradigm in 
corpus-based research. The English. 
Penn Treebank has enabled and 
motivated corpus and computational 
linguistic research based on information 
extractable from structurally annotated 
corpora. Recently, the research has 
focused on the following two issues: 
first, when and how can a structurally 
annotated corpus of language X be 
built? 
Second, what information should or 
can be annotated? A good sample of 
issues in these two directions can be 
found in the papers collected in Abeille 
(1999). 
The construction of the Sinica 
Treebank deals with both issues. First, it 
is one of the first structurally annotated 
corpora in Mandarin Chinese. Second, 
as a design feature, the Sinica Treebank 
annotation includes thematic role 
information in addition to syntactic 
categories. In this paper, we will discuss 
the design criteria and annotation 
guidelines of the Sinica Treebank. We 
will also give a preliminary research 
result based on the Sinica Treebank. 
2. Design Cr i te r ia  
There are three important design 
criteria for the Sinica Treebank: 
maximal resource sharing, minimal 
structural complexity, and optimal 
semantic information. 
First, to achieve maximal resource 
sharing, the construction of the Sinica 
Treebank is bootstrapped from existing 
29 
Chinese computational linguistic 
resources. The textual material is 
extracted from the tagged Sinica Corpus 
(hRp:l/www.sinica.edu.tw/ftms-bin/ 
kiwi.sh, Chen et al 1996). In other 
words, the tasks and issues involving 
tokenization / word segmentation and 
category assignment are previously 
resolved. It is worth noting that the 
segmentation and tagging of Sinica 
Corpus have undergone vigorous 
post-editing. Hence the precision of 
category-assignment is much higher 
than with an automatically tagged 
corpora. In addition, since the same 
research team carried out the tagging of 
Sinica Corpus and annotation of Sinica 
Treebank, consistency of the 
interpretation of texts and tags are 
ensured. For structure-assigument, an 
automatic parser (Chen 1996) is applied 
before human post-editing. 
Second) the criterion of minimal 
structural complexity is motivated to 
ensure that the assigned structural 
information can be shared regardless of 
users' theoretical presupposition. It is 
observed that theory-internal 
motivations often require abstract 
intermediate phrasal levels (such as in 
various versions of the X-bar theory). 
Other theories may also call for an 
abstract covert phrasal category (such as 
INFL in the GB theory for Chinese). In 
either case, although the phrasal 
categories are well-motivated within the 
theory, their significance cannot be 
maintained in the context of other 
theoretical frameworks. Since a primary 
goal of annotated corpora is to serve as 
the empirical base of linguistic 
investigations, it is desirable to annotate 
structure divisions that are the most 
commonly shared among theories. We 
came to the conclusion that the minimal 
basic level structures are the ones that 
are shared by all theories. Thus our 
annotation is designed to achieve 
minimal structural complexity. All 
abstract phrasal levels are eliminated 
and only canonical phrasal categories 
are marked. 
Third) a critical issue involving 
Treebank construction as well as 
theories of NLP is how much semantic 
information, if any, should be 
incorporated. The original Penn 
Treebank took a fairly straightforward 
syntactic approach. A purely semantic 
approach, though tempting in terms of 
theoretical and practical considerations, 
has never been attempted yet. A third 
approach is to annotate partial semantic 
information, especially those pertaining 
to argument-relations. This is an 
approach shared by us and the Prague 
Dependency Treebank (e.g. Bohmova 
and Hajikova 1999). In this approach, 
the thematic relation between a predicate 
and an argument is marked in addition to 
grammatical category. Note that the 
predicate-argument relation is usually 
grammatically instantiated and generally 
considered to be the semantic relation 
that interacts most closely with syntactic 
behavior. This allows optimal semantic 
30 
information to be encoded without going 
too beyond the partially automatic 
process of argument identification. 
3. Annotation Guidelines I: 
Category and Hierarchy 
The basic structure of a tree in a 
treebank is a hierarchy of nodes with 
categorical denotation. As in any 
standard phrase structure grammar, the 
lexieal (i.e. terrninal) symbols are 
defined.by the lexicon (CKIP 1992). 
And following the recent lexicon-driven 
and information=based trends in 
linguistic theory, linguistic information 
will be projected from encoded lexical 
information. Please refer to CKIP (1993) 
for the definition of lexieal categories 
that we followed. We will give below 
the inventory of the restricted set of 
phrasal categories used and their 
interpretation. This set defines the 
domain of expressed syntactic 
information (instead of projected or 
inherited information). Readers can also 
consult Chen et al's (2000) general 
description of how the Siniea Treebank 
is constructed for a more complete list of 
tags as well as explanation i Chinese. 
3.1. Defining Phrasal Categories 
There are only 6 non-terminal 
phrasal categories annotated in the 
Sinica Treebank. 
(1) Phrasal Categories 
1. S: An S is a complete tree headed by a 
predicate (i.e. S is the start symbol). 
2.VP: A VP is a phrase headed by a 
predicate. However, it lacks a 
subject and cannot function alone. 
3. NP: An NP is beaded by an N. 
4.GP: A GP is a phrase headed by 
locational noun or locational adjunct. 
Since the thematic role is often 
determined by the governing 
predicate and not encoded locally; 
nominal phrases are given a tentative 
role of DUMMY so that it can 
inherit he correct role from the main 
predicate. 
5. PP: A PP is headed by a preposition. 
The thematic role of its argument is 
inherited from the mother, hence its 
argument is marked with a 
DUMMY. 
6. XP: A XP is a conjunctive phrase that 
is headed by a conjunction. Its 
syntactic head is the conjunction. 
However, since the actual category 
depends on the interactive 
inheritance from possibly 
non-identical conjoined elements, X 
in XP stands for an under-specified 
category. 
3.2. Defining Inheritance Relations 
Following unification-based 
grammatical theories, categorical 
assignments in Sinica Treebank are both 
lexicon-driven and head-driven. In 
principle, all grammatical information is
lexically encoded. Structurally heads 
indicate the direction of information 
inheritance and define possible 
predicate-argument relations. However, 
since the notion 'head' can have several 
31 
different linguistic definitions, we 
attempt to allow at least the discrepancy 
between syntactic and semantic heads. 
In Sinica Treebank, three different kinds 
of grammatical heads are annotated. 
(2) Heads 
1.Head: indicates a grammatical head in 
an endocentrie phrasal category. 
Unless a different semantic head is 
explicitly marked, a Head marks a 
category that serves simultaneously as
the syntactic and semantic heads of the 
construction. 
2.head: indicates a semantic head which 
does not simultaneously function as a 
syntactic head. For instance in 
constructions involving 
grammatiealized 'particles,' such as in 
the 'VP-de' construction, the 
grammatical head ('de' in this case) 
does not carry any semantic 
information. In these cases, the head 
marks the semantic head ('VP" in this 
case) to indicate the flow of content 
information. 
3. DUMMY: indicates the semantic 
head(s) whose categorical or thematic 
identity cannot be locally determined. 
The two most likely scenarios 
involving DUMMY are (a) in a 
coordination construction, where the 
head category depends on the sum of 
all conjuncts. And (b) in a non-NP 
argument phrase, such as PP, where 
the semantic head carries a thematic 
role assigned not by the immediate 
governing syntactic head ("P" in this 
case), but by a higher predicate. In 
these cases, DUMMY allows a parser 
to determine the correct categorical / 
thematic relation later, while 
maintaining identical local structures. 
3.3. Beyond Simple Inheritance 
When simple inheritance fails, the 
following principles derived from our 
design criteria serve to predict the 
structural assignments of a phrasal 
category: default inheritance, sisters only, 
and left most. 
3.3.1. Default Inheritance 
This principle deals primarily and 
most effectively with coordinations and 
conjunctions. The theoretical motivation 
of this account follows Sag et al's (1985) 
proposal. In essence, the category of a 
conjunctive construction must be 
inherited from its semantic heads. 
However, since conjunctions are not 
restricted to same categories, languages 
must have principled ways to determine 
the categorical identity when different 
semantic heads carry different 
information. 
First, in the trivial case when all 
head daughters are of the same category, 
the mother will inherit hat category. 
Second, when the different head 
daughters are an elaboration of the same 
basic category (e.g. both Nd and Ne are 
elaboration of N), then the basic 
category is the default inheritance 
category for the mother. This can be 
illustrated by (3). 
32 
(3) \[\[\[da4\]VH1 l\[er2\]Caa \[yuan2\]VH13\]\] 
VP 
big 
Third," 
mechanisms 
categorical 
and round 
when other inheritance 
fail to provide a clear 
choice, the default 
inheritance is activated. There are two 
default hierarchies. The first one deals 
with when the head daughters are all 
lexical categories (4a), and the second 
one deals with when they are all phrasal 
categories (4b). If there is a disparity 
between lexical and phrasal categories, 
then a lexical category will be expanded 
to a phrasal category first. 
(4)Default Inheritance Hierarchy for 
Categories 
a) Lexical Categories: V > N > P > Ng 
b) Phrasal Categories: S> VP> NP> 
PP> GP 
When phrasal conjuncts are involved, S 
is the privileged category since it is the 
start symbol of the grammar. VP comes 
next since its structural composition is
identical to that of S. If the structure 
involved is not a predicate (i.e. head of a 
sentence), then it must be a role. For 
argument roles, NP's are more 
privileged than PP's, and PP's are more 
privileged than GP's. (5) is an instance 
of the application of this default 
hierarchy. 
(5) \[\[da41iang4\]Neqa \[ r2\]Caa 
\[feng l sheng4\]VH11 \]V\]VP 
big-quantity and 
bountiful 
"bountiful and of big quantity" 
When lexical conjuncts are involved, the 
same principle is used. The priority is 
given to the predicate head of the 
sentence. Among possible argument 
roles, the nominal category is the default. 
An illustrative xample can be found in 
(6). 
(6) \[\[wei4lan2 de tianlkongl\]NP 
\[yu3\]Caa\[zhul qun2biao l han4\]S\]S 
aqua-blue DE sky 
and people ferocious 
'That the sky being aqua blue and 
that he people being ferocious...' 
3.3.2 Sisters Only 
Following most current linguistic 
theory, argument roles and adjunct 
complements must be sisters of a lexieal 
head. However, driven by our design 
criteria of minimal structural complexity, 
no same level iteration is allowed. Thus 
these arguments and adjuncts can be 
located by the straightforward definition 
of sisterhood: that they share the same 
mother-daughter r lation with the head. 
The result is a flat structure. 
33.3 Left First 
This principle is designed to 
account for possible internal structure 
when there are more than two sisters 
-without having to add on hierarchical 
complexity. Hence, the default 
interpretation of internal structure of 
multiple sisters is that the internal 
association starts from leR to right. 
33 
4. Annotation Guidelines II: 
Structural Annotation of 
Thematic Information 
A thematic relation contains a 
compact bundle of syntactic and 
semantic information. Although 
thematic relations are lexically encoded 
on a predicate, they can only be 
instantiated when that information is 
projected to phrasal arguments. In other 
words, the only empirical evidence for 
the existence of a thematic relation is a 
realized argument. However, a realized 
argument cannot by itself determine the 
thematic relation. The exact nature of 
the relation must be determined based 
on the lexical information fi'om the 
predicate as well as checking of the 
compatibility of that realized argument. 
Since structural information alone 
cannot determine thematic relations, 
prototypical structural annotation, such 
as in the original Penn Treebank, does 
not include thematic roles since they 
contain on-structural information. 
On the other hand, in theories 
where lexical heads drive the structural 
derivation / construction (e.g. ICG and 
HPSG and LFG), thematic relations are 
critical. Hence, we decided to encode 
realized thematic relations on each 
phrasal argument. The list of  thematic 
relations encoded on the head predicate 
is consulted whenever a phrasal 
argument is constructed, and a 
contextually appropriate relation 
sanctioned by the lexical information is 
encoded. It is worth noting that in our 
account., we not only mark the thematic 
relations of a verbal predicate, but we 
also mark the thematic relations 
governed by a deverbal noun, among 
others. Also note that an argument of a 
preposition is marked as a placeholder 
DUMMY. This is because a preposition 
only governs an argument syntactically, 
while its thematic relation is determined 
by a higher verb. 
(7) Thematic Roles: Classification and 
Inventory 
34 
THEMATIC ROLES 
I OUMMY I 
I 
I '~ '  I 
e~edeneer Ioe~on 
I ?*"~'*" t *o.v,~ 
\[ be.erect ~ *errnlemr~ 
\[ ?~mdi~m conjunction 
\[ e~eluaem negae~ 
\[ exrJudon incl~on 
\[ f l~*cy  -{ impera~ 
\[ quamiler quamiol 
\[ s~ndard 
I ~ deg~e 
I dei~$ ma.~0n 
re~.dz uncondJllon 
I hylxnl'~s: oondusion 
I wl '~u~f con~rdon 
I a'uDidanoe puq)ose 
I l- 
I I PR~?''~'?N I I I "?UN I 
o8,o~ I -L - - - t .oM,~T,o . I  I 
I 
I OtJMMY \[ 
5. Current  Status of  the Sinica 
Treebank and On- l ine 
Interface 
Following the above criteria and 
principles, we have already f inished 
Sinica Treebank 1.0. It contains 
annotations of 38,725 Chinese structural 
trees containing 239,532 words. It 
covers ubject areas that include politics, 
traveling, sports, finance, society, etc. 
This version of the Sinica Treebank will 
be released in the near future as soon as 
the licensing documents are cleared by 
the legal dep~,ent  of  Academia Sinica. 
A small subset of  it (1,000 sentences) is
already available for researchers to 
download from the website 
http ://godel.i is.sinica, edu. tw/CKIP/ 
treeslOOO.htm. A searchable interface is 
also being developed and tested for 
researchers o that they can directly 
access the complete treebank 
information. 
As an annotated corpus, one of the 
most important roles that a treebank can 
play is that it can serve as a shared 
35 
source of data for linguistic, especially 
syntactic studies. Following the example 
of the successful Sinica Corpus, we have 
developed an on-line interface for 
extraction of grammatical information 
from the Sinica Treebank. Although the 
users that we have in mind are 
theoretical linguists who do not 
necessarily have computational 
background; we hope that non-linguists 
can also benefit from the ready 
availability of such grammatical 
information. And of course, 
computational linguists should be able 
to use this interface for quick references 
before going into a more in-depth study 
of the annotated corpus. 
Currently, the beta site allows users 
specify a variety of conditions to search 
for structurally annotated sentences. 
Conditions can be specified in terms of 
keywords, grammatical tags (lexical or 
phrasal), thematic relations, or any 
boolean combination of the above 
elements. The search result can be 
presented aseither annotated structure or 
simply the example sentences. Simply 
statistics, based on either straightforward 
frequency count or mutual information, 
are also available. For linguistically 
interesting information, such as the 
heads of various phrasal constructions, a 
user can simply look up the explicitly 
syntactic Head or semantic head; as 
well as DUMMY when it serves as a 
head placeholder. The website of this 
interface, as well as the general release 
of the Sinica Treebank 1.0, is scheduled 
to be announced at the second ACL 
workshop on Chinese Language 
Processing inOctober 2000. 
6. Conclusion 
The construction of the Sinica 
Treebank is only a first step towards 
application of structurally annotated 
corpora. Continuing expansion and 
correction will make this database an 
invaluable resource for linguistic and 
computational studies of Chinese. 
References 
I.ABEILI..E, Anne. 1999. Ed. 
Proceedings ofATALA Workshop - 
Treebanks. Paris, June 18-19, 1999. 
Univ. de Paris VII. 
2.BOHMOVA, Alla and Eva Hajicova. 
1999. How Much of the Underlying 
Syntactic Structure Can be Tagged 
Automatically? In Abeille (Ed). 
1999.31-40. 
3.CHEN, Feng-Yi, Pi-Fang Tsai, 
Keh-Jiann Chen, and Chu-Ren Huang. 
2000. Sinica Treebank. \[in Chinese\] 
Computational Linguistics and 
Chinese Language Processing. 
4.2.87-103. 
4.CHEN, Keh-Jiarm. 1996. A Model for 
Robust Chinese Parser. Computational 
Linguistics and Chinese Language 
Processing. 1.1.183-204. 
5.CHEN, Keh-Jiann, Chu-Ren Huang. 
1996. Information-based Case 
Grammar: A Unification-based 
Formalism for Parsing Chinese. In 
Journal of Chinese Linguistics 
Monograph Series No. 9. Chu-Ren 
Huang, Keh-Jiaun Chen, and 
Benjamin K. T'sou Eds. Readings in 
Chinese Natural Language Processing. 
23-45. Berkeley: JCL. 
6.CHEN, Keh-Jiann, Chu-Ren Huang, 
Li-Ping Chang, Hui-Li Hsu. 1996. 
36 
Sinica Corpus: Design Methodology 
for Balanced Corpora. Proceedings of 
the 11th Pacific Asia Conference on 
Language, Information, and 
Computation (PA CLIC I1). Seoul 
Korea. 167-176. 
7.CHEN, Keh-Jiann and Shing-Huan 
Liu. 1992. Word Identification for 
Mandarin Chinese Sentences. 
Proceedings of COLING-92.101 - 105. 
8.CHEN, Keh- Jiann, Shing-Huan Liu, 
Li-Ping Chang, Yeh-Hao Chin. 1994. 
A Practical Tagger for Chinese 
Corpora." Proceedings of R OCLING 
V/I. 111-126. 
9.CHEN, Keh-Jiann, Chi-Ching Luo, 
Zhao-Ming Gao, Ming-Chung Chang, 
Feng-Yi Chen, and Chao-Ran Chert. 
1999. The CKIP Chinese Treebank: 
Guidelines for Annotation. In Abeille 
(Ed). 1999.85-96. 
I0. CKIP (Chinese Knowledge 
Information Processing). 1993. The 
Categorical Analysis of Chinese. CKIP 
Technical Report 93-05. Nankang: 
Academia Sinica. 
11. HUANG, Chu-Ren, Keh-Jiann 
Chen, Feng-Yi Chen, and Li-Li Chang. 
1997. Segmentation Standard for 
Chinese Natural Language Processing. 
Computational Linguistics and 
Chinese Language Processing. 
2.2.47-62 
12. Lin, Fu-Wen. 1992. Some 
Reflections on the Thematic System of 
Information-based Case Grammar 
(ICG). CKIP Technical Report 92-01. 
Nankang: Academia Sinica. 
13. Marcus, Miteh P., Beatrice 
Santorini, and M. A. Marcinkiewiicz. 
1993. Building a Large Annotated 
Corpus of English: The Peen Treebank. 
Computational Linguistics. 
19.2.313-330. 
14. SAG, Ivan, Gerald Gazdar, Thomas 
Wasow, and Steven Weisler. 1985. 
Coordination and How to Distinguish 
Categories. Natural Language and 
Linguistic Theories. 117-171. 
Appendix 
1. Lexical Categories 
(1) NON-PREDCITIVE ADJVECTIVE: A 
(2) CONJUNCTION: C 
(3) ADVERB: D 
(4) INTERJECTION: I 
(5) NOUN: N 
(6) DETERMINATIVES: Ne 
(7) MEASURE WORD / CLASSIFIER: 
Nf 
(8) POSTPOSITION WORD: Ng 
(9) PRONOUN: Nh 
(10) PREPOSITION: P 
(11) PARTICLES: T 
(12) VERB: V 
2. Sample Sentence and Tree 
nage wanfi de nyuren baifa zhihou 
bian buzai lihui 
? that hair-style DE woman white-hair after 
then never pay-attention 
ting qian tingting .FuR de 
qingcao 
courtyard front slender-ly standing-erect DE 
green-grass 
'After her hair had turned white, that 
coiffured woman ever paid any more 
attention tothe nicely standing green grass 
in the front courtyard." 
S(agent:NP(quantifier:DM:~l 
property:VP- ~j(head:VP(Head:VA4:~-) 
IHead:DE: ~)lHead:Nab:~A.)ltime:GP 
(DUMMY:VP(Head:VI-I 11:~1 ~)1 Head: 
Ng:-~-~.)\[firne:Dd:~l~ I time:Dd: ~"~'1 Head: 
VC2:J~ ~'\[goal:NP (property:VP ? ~(head: 
VP (location:NP(property:Neb:/~.l 
Head:Neda: ~,f)lHead:VH11: ;~ ,~ ~ 2Y_)\[ 
Head:DE: ~)I  Head:Nab: ff ~)) 
37 
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 1?8, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Applications of Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items
Chao-Lin Liu? Chun-Hung Wang? Zhao-Ming Gao? Shang-Ming Huang?
?Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan
?Dept. of Foreign Lang. and Lit., National Taiwan University, Taipei 10617, Taiwan
?chaolin@nccu.edu.tw, ?zmgao@ntu.edu.tw
ABSTRACT1
We report experience in applying techniques for nat-
ural language processing to algorithmically generat-
ing test items for both reading and listening cloze
items. We propose a word sense disambiguation-
based method for locating sentences in which des-
ignated words carry specific senses, and apply a
collocation-based method for selecting distractors
that are necessary for multiple-choice cloze items.
Experimental results indicate that our system was
able to produce a usable item for every 1.6 items it
returned. We also attempt to measure distance be-
tween sounds of words by considering phonetic fea-
tures of the words. With the help of voice synthe-
sizers, we were able to assist the task of compos-
ing listening cloze items. By providing both reading
and listening cloze items, we would like to offer a
somewhat adaptive system for assisting Taiwanese
children in learning English vocabulary.
1 Introduction
Computer-assisted item generation (CAIG) allows
the creation of large-scale item banks, and has at-
tracted active study in the past decade (Deane and
Sheehan, 2003; Irvine and Kyllonen, 2002). Ap-
plying techniques for natural language processing
(NLP), CAIG offers the possibility of creating a
large number of items of different challenging lev-
els, thereby paving a way to make computers more
adaptive to students of different competence. More-
over, with the proliferation of Web contents, one
may search and sift online text files for candidate
sentences, and come up with a list of candidate cloze
1A portion of results reported in this paper will be expanded
in (Liu et al, 2005; Huang et al, 2005).
items economically. This unleashes the topics of the
test items from being confined by item creators? per-
sonal interests.
NLP techniques serve to generate multiple-choice
cloze items in different ways. (For brevity, we use
cloze items or items for multiple-choice cloze items
henceforth.) One may create sentences from scratch
by applying template-based methods (Dennis et al,
2002) or more complex methods based on some pre-
determined principles (Deane and Sheehan, 2003).
Others may take existing sentences from a corpus,
and select those that meet the criteria for becoming
test items. The former approach provides specific
and potentially well-controlled test items at the costs
of more complex systems than the latter, e.g., (Shee-
han et al, 2003). Nevertheless, as the Web provides
ample text files at our disposal, we may filter the
text sources stringently for obtaining candidate test
items of higher quality. Administrators can then se-
lect really usable items from these candidates at a
relatively lower cost.
Some researchers have already applied NLP tech-
niques to the generation of sentences for multiple-
choice cloze items. Stevens (1991) employs the con-
cepts of concordance and collocation for generating
items with general corpora. Coniam (1997) relies on
factors such as word frequencies in a tagged corpus
for creating test items of particular types.
There are other advanced NLP techniques that
may help to create test items of higher quality. For
instance, many words in English may carry multiple
senses, and test administrators usually want to test a
particular usage of the word in an item. In this case,
blindly applying a keyword matching method, such
as a concordancer, may lead us to a list of irrelevant
sentences that would demand a lot of postprocess-
1
Figure 1: A multiple-choice cloze item for English
ing workload. In addition, composing a cloze item
requires not just a useful sentence.
Figure 1 shows a multiple-choice item, where we
call the sentence with a gap the stem, the answer to
the gap the key, and the other choices the distrac-
tors. Given a sentence, we still need distractors for
a multiple-choice item. The selection of distractors
affects the item facility and item discrimination of
the cloze items (Poel and Weatherly, 1997). There-
fore, the selection of distractors calls for deliberate
strategies, and simple considerations alone, such as
word frequencies, may not satisfy the demands.
To remedy these shortcomings, we employ the
techniques for word sense disambiguation (WSD)
for choosing sentences in which the keys carries spe-
cific senses, and utilize the techniques for comput-
ing collocations (Manning and Schu?tze, 1999) for
selecting distractors. Results of empirical evaluation
show that our methods could create items of satisfac-
tory quality, and we have actually used the generated
cloze items in freshmen-level English classes.
For broadening the formats of cloze items, we
also design software that assists teachers to create
listening cloze items. After we defining a metric
for measuring similarity between pronunciations of
words, our system could choose distractors for lis-
tening cloze items. This addition opens a door to
offering different challenging levels of cloze items.
We sketch the flow of the item generation pro-
cess in Section 2, and explain the preparation of the
source corpus in Section 3. In Section 4, we elab-
orate on the application of WSD to selecting sen-
tences for cloze items, and, in Section 5, we delve
into the application of collocations to distractor gen-
eration. Results of evaluating the created reading
cloze items are presented in Section 6. We then
outline methods for creating listening cloze items in
Section 7 before making some concluding remarks.
2 System Architecture
Figure 2 shows major steps for creating cloze items.
Constrained by test administrator?s specifications
and domain dependent requirements, the Sentence
Retriever chooses a candidate sentence from the
7DJJHG
&RUSXV
7DUJHW'HSHQGHQW
,WHP5HTXLUHPHQWV
,WHP
6SHFLILFDWLRQ
7DUJHW
6HQWHQFH
6HQWHQFH
5HWULHYHUZLWK:6'
'LVWUDFWRU
*HQHUDWRU
&OR]H
,WHP
Figure 2: Main components of our item generator
Tagged Corpus. Target-Dependent Item Require-
ments specify general principles that should be fol-
lowed by all items for a particular test. For example,
the number of words in cloze items for College En-
trance Examinations in Taiwan (CEET) ranges be-
tween 6 and 28 (Liu et al, 2005), and one may want
to follow this tradition in creating drill tests.
Figure 3 shows the interface to the Item Specifi-
cation. Through this interface, test administrators
select the key for the desired cloze item, and specify
part-of-speech and sense of the key that will be used
in the item. Our system will attempt to create the re-
quested number of items. After retrieving the target
sentence, the Distractor Generator considers such
constraining factors as word frequencies and collo-
cations in selecting the distractors at the second step.
Figure 3: Interface for specifying cloze items
Figure 4 shows a sample output for the specifica-
tion shown in Figure 3. Given the generated items,
the administrator may choose and edit the items, and
save the edited items into the item bank. It is possi-
ble to retrieve previously saved items from the item
bank, and compile the items for different tests.
3 Source Corpus and Lexicons
Employing a web crawler, we retrieve the con-
tents of Taiwan Review <publish.gio.gov.tw>, Tai-
wan Journal <taiwanjournal.nat.gov.tw>, and China
Post <www.chinapost.com.tw>. Currently, we have
127,471 sentences that consist of 2,771,503 words
in 36,005 types in the corpus. We look for use-
ful sentences from web pages that are encoded in
the HTML format. We need to extract texts from
2
Figure 4: An output after Figure 3
the mixture of titles, main body of the reports,
and multimedia contents, and then segment the ex-
tracted paragraphs into individual sentences. We
segment sentences with the help of MXTERMINA-
TOR (Reynar and Ratnaparkhi, 1997). We then tok-
enize words in the sentences before assigning useful
tags to the tokens.
We augment the text with an array of tags that
facilitate cloze item generation. We assign tags of
part-of-speech (POS) to the words with MXPOST
that adopts the Penn Treebank tag set (Ratnaparkhi,
1996). Based on the assigned POS tags, we annotate
words with their lemmas. For instance, we annotate
classified with classify and classified, respectively,
when the original word has VBN and JJ as its POS
tag. We also employ MINIPAR (Lin, 1998) to ob-
tain the partial parses of sentences that we use exten-
sively in our system. Words with direct relationships
can be identified easily in the partially parsed trees,
and we rely heavily on these relationships between
words for WSD. For easy reference, we will call
words that have direct syntactic relationship with a
word W as W ?s signal words or simply signals.
Since we focus on creating items for verbs, nouns,
adjectives, and adverbs (Liu et al, 2005), we care
about signals of words with these POS tags in sen-
tences for disambiguating word senses. Specifically,
the signals of a verb include its subject, object, and
the adverbs that modify the verb. The signals of a
noun include the adjectives that modify the noun and
the verb that uses the noun as its object or predicate.
For instance, in ?Jimmy builds a grand building.?,
both ?build? and ?grand? are signals of ?building?.
The signals of adjectives and adverbs include the
words that they modify and the words that modify
the adjectives and adverbs.
When we need lexical information about English
words, we resort to electronic lexicons. We use
WordNet <www.cogsci.princeton.edu/?wn/> when
we need definitions and sample sentences of words
for disambiguating word senses, and we employ
HowNet <www.keenage.com> when we need infor-
mation about classes of verbs, nouns, adjectives, and
adverbs.
HowNet is a bilingual lexicon. An entry in
HowNet includes slots for Chinese words, English
words, POS information, etc. We rely heavily on the
slot that records the semantic ingredients related to
the word being defined. HowNet uses a limited set
of words in the slot for semantic ingredient, and the
leading ingredient in the slot is considered to be the
most important one generally.
4 Target Sentence Retriever
The sentence retriever in Figure 2 extracts qualified
sentences from the corpus. A sentence must contain
the desired key of the requested POS to be consid-
ered as a candidate target sentence. Having identi-
fied such a candidate sentence, the item generator
needs to determine whether the sense of the key also
meets the requirement. We conduct this WSD task
based on an extended notion of selectional prefer-
ences.
4.1 Extended Selectional Preferences
Selectional preferences generally refer to the phe-
nomenon that, under normal circumstances, some
verbs constrain the meanings of other words in
a sentence (Manning and Schu?tze, 1999; Resnik,
1997). We can extend this notion to the relation-
ships between a word of interest and its signals, with
the help of HowNet. Let w be the word of interest,
and pi be the first listed class, in HowNet, of a signal
word that has the syntactic relationship ? with w.
We define the strength of the association of w and pi
as follows:
A?(w, pi) = Pr?(w, pi)Pr?(w) , (1)
where Pr?(w) is the probability of w participating in
the ? relationship, and Pr?(w, pi) is the probability
that both w and pi participate in the ? relationship.
4.2 Word Sense Disambiguation
We employ the generalized selectional preferences
to determine the sense of a polysemous word in a
sentence. Consider the task of determining the sense
3
of ?spend? in the candidate target sentence ?They
say film makers don?t spend enough time developing
a good story.? The word ?spend? has two possible
meanings in WordNet.
1. (99) spend, pass ? (pass (time) in a specific
way; ?How are you spending your summer va-
cation??)
2. (36) spend, expend, drop ? (pay out; ?I spend
all my money in two days.?)
Each definition of the possible senses include (1)
the head words that summarize the intended mean-
ing and (2) a sample sentence for sense. When we
work on the disambiguation of a word, we do not
consider the word itself as a head word in the follow-
ing discussion. Hence, ?spend? has one head word,
i.e., ?pass?, in the first sense and two head words,
i.e., ?extend? and ?drop?, in the second sense.
An intuitive method for determining the mean-
ing of ?spend? in the target sentence is to replace
?spend? with its head words in the target sentence.
The head words of the correct sense should go with
the target sentence better than head words of other
senses. This intuition leads to the a part of the scores
for senses, i.e., St that we present shortly.
In addition, we can compare the similarity of the
contexts of ?spend? in the target sentence and sam-
ple sentences, where context refers to the classes of
the signals of the word being disambiguated. For the
current example, we can check whether the subject
and object of ?spend? in the target sentence have the
same classes as the subjects and objects of ?spend?
in the sample sentences. The sense whose sample
sentence offers a more similar context for ?spend? in
the target sentence receives a higher score. This in-
tuition leads to the other part of the scores for senses,
i.e., Ss that we present below.
Assume that the key w has n senses. Let ? =
{?1, ?2, ? ? ? , ?n} be the set of senses of w. Assume
that sense ?j of word w has mj head words in Word-
Net. (Note that we do not consider w as its own head
word.) We use the set ?j = {?j,1, ?j,2, ? ? ? , ?j,mj}
to denote the set of head words that WordNet pro-
vides for sense ?j of word w.
When we use the partial parser to parse the tar-
get sentence T for a key, we obtain information
about the signal words of the key. Moreover, for
each of these signals, we look up their classes in
HowNet, and adopt the first listed class for each of
the signals when the signal covers multiple classes.
Assume that there are ?(T ) signals for the key
w in a sentence T . We use the set ?(T,w) =
{?1,T , ?2,T , ? ? ? , ??(T ),T } to denote the set of sig-
nals for w in T . Correspondingly, we use ?j,T to de-
note the syntactic relationship between w and ?j,T
in T , use ?(T,w) = {?1,T , ?2,T , ? ? ? , ??(T ),T } for
the set of relationships between signals in ?(T,w)
and w, use pij,T for the class of ?j,T , and use
?(T,w) = {pi1,T , pi2,T , ? ? ? , pi?(T ),T } for the set of
classes of the signals in ?(T,w).
Equation (2) measures the average strength of as-
sociation of the head words of a sense with signals
of the key in T , so we use (2) as a part of the score
for w to take the sense ?j in the target sentence T .
Note that both the strength of association and St fall
in the range of [0,1].
St(?j |w, T )
= 1mj
mj?
k=1
1
?(T )
?(T )?
l=1
A?l,T (?j,k, pil,T ) (2)
In (2), we have assumed that the signal words
are not polysemous. If they are polysemous, we as-
sume that each of the candidate sense of the signal
words are equally possible, and employ a slightly
more complicated formula for (2). This assumption
may introduce errors into our decisions, but relieves
us from the needs to disambiguate the signal words
in the first place (Liu et al, 2005).
Since WordNet provides sample sentences for im-
portant words, we also use the degrees of similarity
between the sample sentences and the target sen-
tence to disambiguate the word senses of the key
word in the target sentence. Let T and S be the tar-
get sentence of w and a sample sentence of sense ?j
of w, respectively. We compute this part of score,
Ss, for ?j using the following three-step procedure.
If there are multiple sample sentences for a given
sense, say ?j of w, we will compute the score in (3)
for each sample sentence of ?j , and use the average
score as the final score for ?j .
Procedure for computing Ss(?j |w, T )
1. Compute signals of the key and their relation-
ships with the key in the target and sample sen-
tences.
4
?(T,w) = {?1,T , ?2,T , ? ? ? , ??(T ),T },
?(T,w) = {?1,T , ?2,T , ? ? ? , ??(T ),T },
?(S,w) = {?1,S , ?2,S , ? ? ? , ??(S),S}, and
?(S,w) = {?1,S , ?2,S , ? ? ? , ??(S),S}
2. We look for ?j,T and ?k,S such that ?j,T =
?k,S , and then check whether pij,T = pik,S .
Namely, for each signal of the key in T , we
check the signals of the key in S for matching
syntactic relationships and word classes, and
record the counts of matched relationship in
M(?j , T ) (Liu et al, 2005).
3. The following score measures the proportion of
matched relationships among all relationships
between the key and its signals in the target sen-
tence.
Ss(?j |w, T ) = M(?j , T )?(T ) (3)
The score for w to take sense ?j in a target sen-
tence T is the sum of St(?j |w, T ) defined in (2)
and Ss(?j |w, T ) defined in (3), so the sense of w
in T will be set to the sense defined in (4) when
the score exceeds a selected threshold. When the
sum of St(?j |w, T ) and Ss(?j |w, T ) is smaller than
the threshold, we avoid making arbitrary decisions
about the word senses. We discuss and illustrate ef-
fects of choosing different thresholds in Section 6.
argmax
?j??
St(?j |w, T ) + Ss(?j |w, T ) (4)
5 Distractor Generation
Distractors in multiple-choice items influence the
possibility of making lucky guesses to the answers.
Should we use extremely impossible distractors in
the items, examinees may be able to identify the
correct answers without really knowing the keys.
Hence, we need to choose distractors that appear to
fit the gap, and must avoid having multiple answers
to items in a typical cloze test at the same time.
There are some conceivable principles and al-
ternatives that are easy to implement and follow.
Antonyms of the key are choices that average exam-
inees will identify and ignore. The part-of-speech
tags of the distractors should be the same as the
key in the target sentence. We may also take cul-
tural background into consideration. Students in
Taiwan tend to associate English vocabularies with
their Chinese translations. Although this learning
strategy works most of the time, students may find
it difficult to differentiate English words that have
very similar Chinese translations. Hence, a culture-
dependent strategy is to use English words that have
similar Chinese translations with the key as the dis-
tractors.
To generate distractors systematically, we employ
ranks of word frequencies for selecting distractors
(Poel and Weatherly, 1997). Assume that we are
generating an item for a key whose part-of-speech
is ?, that there are n word types whose part-of-
speech may be ? in the dictionary, and that the rank
of frequency of the key among these n types is m.
We randomly select words that rank in the range
[m?n/10,m+n/10] among these n types as candi-
date distractors. These distractors are then screened
by their fitness into the target sentence, where fitness
is defined based on the concept of collocations of
word classes, defined in HowNet, of the distractors
and other words in the stem of the target sentence.
Recall that we have marked words in the corpus
with their signals in Section 3. The words that have
more signals in a sentence usually contribute more to
the meaning of the sentence, so should play a more
important role in the selection of distractors. Since
we do not really look into the semantics of the tar-
get sentences, a relatively safer method for selecting
distractors is to choose those words that seldom col-
locate with important words in the target sentence.
Let T = {t1, t2, ? ? ? , tn} denote the set of words
in the target sentence. We select a set T ? ? T such
that each t?i ? T ? has two or more signals in T and is
a verb, noun, adjective, or adverb. Let ? be the first
listed class, in HowNet, of the candidate distractor,
and ? = {?i|?i is the first listed class of a t?i ? T ?}.
The fitness of a candidate distractor is defined in (5).
?1
|?|
?
?i??
log Pr(?, ?i)Pr(?) Pr(?i) (5)
The candidate whose score is better than 0.3 will
be admitted as a distractor. Pr(?) and Pr(?i) are
the probabilities that each word class appears indi-
vidually in the corpus, and Pr(?, ?i) is the proba-
bility that the two classes appear in the same sen-
tence. Operational definitions of these probabilities
5
Table 1: Accuracy of WSD
POS baseline threshold=0.4 threshold=0.7
verb 38.0%(19/50) 57.1%(16/28) 68.4%(13/19)
noun 34.0%(17/50) 63.3%(19/30) 71.4%(15/21)
adj. 26.7%(8/30) 55.6%(10/18) 60.0%(6/10)
adv. 36.7%(11/30) 52.4%(11/21) 58.3%(7/12)
are provided in (Liu et al, 2005). The term in the
summation is a pointwise mutual information, and
measures how often the classes ? and ?i collocate
in the corpus. We negate the averaged sum so that
classes that seldom collocate receive higher scores.
We set the threshold to 0.3, based on statistics of (5)
that are observed from the cloze items used in the
1992-2003 CEET.
6 Evaluations and Applications
6.1 Word Sense Disambiguation
Different approaches to WSD were evaluated in dif-
ferent setups, and a very wide range of accuracies in
[40%, 90%] were reported (Resnik, 1997; Wilks and
Stevenson, 1997). Objective comparisons need to be
carried out on a common test environment like SEN-
SEVAL, so we choose to present only our results.
We arbitrarily chose, respectively, 50, 50, 30,
and 30 sentences that contained polysemous verbs,
nouns, adjectives, and adverbs for disambiguation.
Table 1 shows the percentage of correctly disam-
biguated words in these 160 samples.
The baseline column shows the resulting accu-
racy when we directly use the most frequent sense,
recorded in WordNet, for the polysemous words.
The rightmost two columns show the resulting accu-
racy when we used different thresholds for applying
(4). As we noted in Section 4.2, our system selected
fewer sentences when we increased the threshold, so
the selected threshold affected the performance. A
larger threshold led to higher accuracy, but increased
the rejection rate at the same time. Since the cor-
pus can be extended to include more and more sen-
tences, we afford to care about the accuracy more
than the rejection rate of the sentence retriever.
We note that not every sense of all words have
sample sentences in the WordNet. When a sense
does not have any sample sentence, this sense will
receive no credit, i.e., 0, for Ss. Consequently,
our current reliance on sample sentences in Word-
Table 2: Correctness of the generated sentences
POS of the key # of items % of correct sentences
verb 77 66.2%
noun 62 69.4%
adjective 35 60.0%
adverb 26 61.5%
overall 65.5%
Table 3: Uniqueness of answers
item category key?s POS number of items results
verb 64 90.6%
noun 57 94.7%
cloze adjective 46 93.5%
adverb 33 84.8%
overall 91.5%
Net makes us discriminate against senses that do not
have sample sentences. This is an obvious draw-
back in our current design, but the problem is not
really detrimental and unsolvable. There are usually
sample sentences for important and commonly-used
senses of polysemous words, so the discrimination
problem does not happen frequently. When we do
want to avoid this problem once and for all, we can
customize WordNet by adding sample sentences to
all senses of important words.
6.2 Cloze Item Generation
We asked the item generator to create 200 items in
the evaluation. To mimic the distribution over keys
of the cloze items that were used in CEET, we used
77, 62, 35, and 26 items for verbs, nouns, adjectives,
and adverbs, respectively, in the evaluation.
In the evaluation, we requested one item at a time,
and examined whether the sense and part-of-speech
of the key in the generated item really met the re-
quests. The threshold for using (4) to disambiguate
word sense was set to 0.7. Results of this experi-
ment, shown in Table 2, do not differ significantly
from those reported in Table 1. For all four major
classes of cloze items, our system was able to re-
turn a correct sentence for less than every 2 items
it generated. In addition, we checked the quality of
the distractors, and marked those items that permit-
ted unique answers as good items. Table 3 shows
that our system was able to create items with unique
answers for another 200 items most of the time.
6
 
Figure 5: A phonetic concordancer
6.3 More Applications
We have used the generated items in real tests in a
freshman-level English class at National Chengchi
University, and have integrated the reported item
generator in a Web-based system for learning En-
glish. In this system, we have two major subsys-
tems: the authoring and the assessment subsystems.
Using the authoring subsystem, test administrators
may select items from the interface shown in Fig-
ure 4, save the selected items to an item bank, edit
the items, including their stems if necessary, and fi-
nalize the selection of the items for a particular ex-
amination. Using the assessment subsystem, stu-
dents answer the test items via the Internet, and
can receive grades immediately if the administra-
tors choose to do so. The answers of students are
recorded for student modelling and analysis of the
item facility and the item discrimination.
7 Generating Listening Cloze Items
We apply the same infrastructure for generating
reading cloze items, shown in Figure 2, for the gen-
eration of listening cloze items (Huang et al, 2005).
Due to the educational styles in Taiwan, students
generally find it more difficult to comprehend mes-
sages by listening than by reading. Hence, we can
regard listening cloze tests as an advanced format of
reading cloze tests. Having constructed a database
of sentences, we can extract sentences that contain
the key for which the test administrator would like
to have a listening cloze, and employ voice synthe-
sizers to create the necessary recordings.
Figure 5 shows an interface through which ad-
ministrators choose and edit sentences for listening
cloze items. Notice that we employ the concept that
is related to ordinary concordance in arranging the
extracted sentences. By defining a metric for mea-
suring similarity between sounds, we can put sen-
tences that have similar phonetic contexts around the
key near each other. We hope this would better help
teachers in selecting sentences by this rudimentary
 
Figure 6: The most simple form of listening cloze
clustering of sentences.
Figure 6 shows the most simple format of listen-
ing cloze items. In this format, students click on the
options, listen to the recorded sounds, and choose
the option that fit the gap. The item shown in this
figure is very similar to that shown in Figure 1, ex-
cept that students read and hear the options. From
this most primitive format, we can image and imple-
ment other more challenging formats. For instance,
we can replace the stem, currently in printed form in
Figure 6, into clickable links, demanding students
to hear the stem rather than reading the stem. A
middle ground between this more challenging for-
mat and the original format in the figure is to allow
the gap to cover more words in the original sentence.
This would require the students to listen to a longer
stream of sound, so can be a task more challenging
than the original test. In addition to controlling the
lengths of the answer voices, we can try to modulate
the speed that the voices are replayed. Moreover,
for multiple-word listening cloze, we may try to find
word sequences that sound similar to the answer se-
quence to control the difficulty of the test item.
Defining a metric for measuring similarity be-
tween two recordings is the key to support the afore-
mentioned functions. In (Huang et al, 2005), we
consider such features of phonemes as place and
manner of pronunciation in calculating the similarity
between sounds. Using this metric we choose as dis-
tractors those sounds of words that have similar pro-
nunciation with the key of the listening cloze. We
have to define the distance between each phoneme
so that we could employ the minimal-edit-distance
algorithm for computing the distance between the
sounds of different words.
7
8 Concluding Remarks
We believe that NLP techniques can play an impor-
tant role in computer assisted language learning, and
this belief is supported by papers in this workshop
and the literature. What we have just explored is
limited to the composition of cloze items for English
vocabulary. With the assistance of WSD techniques,
our system was able to identify sentences that were
qualified as candidate cloze items 65% of the time.
Considering both word frequencies and collocation,
our system recommended distractors for cloze items,
resulting in items that had unique answers 90% of
the time. In addition to assisting the composition
of cloze items in the printed format, our system is
also capable of helping the composition of listening
cloze items. The current system considers features
of phonemes in computing distances between pro-
nunciations of different word strings.
We imagine that NLP and other software tech-
niques could empower us to create cloze items for a
wide range of applications. We could control the for-
mats, contents, and timing of the presented material
to manipulate the challenging levels of the test items.
As we have indicated in Section 7, cloze items in the
listening format are harder than comparable items in
the printed format. We can also control when and
what the students can hear to fine tune the difficul-
ties of the listening cloze items.
We must admit, however, that we do not have suf-
ficient domain knowledge in how human learn lan-
guages. Consequently, tools offered by computing
technologies that appear attractive to computer sci-
entists or computational linguists might not provide
effective assistance for language learning or diagno-
sis. Though we have begun to study item compari-
son from a mathematical viewpoint (Liu, 2005), the
current results are far from being practical. Exper-
tise in psycholinguistics may offer a better guidance
on our system design, we suppose.
Acknowledgements
We thank anonymous reviewers for their invaluable
comments on a previous version of this report. We
will respond to some suggestions that we do not have
space to do so in this report in the workshop. This
research was supported in part by Grants 93-2213-E-
004-004 and 93-2411-H-002-013 from the National
Science Council of Taiwan.
References
D. Coniam. 1997. A preliminary inquiry into using corpus
word frequency data in the automatic generation of English
language cloze tests. Computer Assisted Language Instruc-
tion Consortium, 16(2?4):15?33.
P. Deane and K. Sheehan. 2003. Automatic item gen-
eration via frame semantics. Education Testing Service:
http://www.ets.org/research/dload/ncme03-deane.pdf.
I. Dennis, S. Handley, P. Bradon, J. Evans, and S. Nestead.
2002. Approaches to modeling item-generative tests. In
Item generation for test development (Irvine and Kyllonen,
2002), pages 53?72.
S.-M. Huang, C.-L. Liu, and Z.-M. Gao. 2005. Computer-
assisted item generation for listening cloze tests and dictation
practice in English. In Proc. of the 4th Int. Conf. on Web-
based Learning. to appear.
S. H. Irvine and P. C. Kyllonen, editors. 2002. Item Genera-
tion for Test Development. Lawrence Erlbaum Associates,
Mahwah, NJ.
D. Lin. 1998. Dependency-based evaluation of MINIPAR. In
Proc. of the Workshop on the Evaluation of Parsing Systems
in the 1st Int. Conf. on Language Resources and Evaluation.
C.-L. Liu, C.-H. Wang, and Z.-M. Gao. 2005. Using lexi-
cal constraints for enhancing computer-generated multiple-
choice cloze items. Int. J. of Computational Linguistics and
Chinese Language Processing, 10:to appear.
C.-L. Liu. 2005. Using mutual information for adaptive item
comparison and student assessment. J. of Educational Tech-
nology & Society, 8(4):to appear.
C. D. Manning and H. Schu?tze. 1999. Foundations of Statisti-
cal Natural Language Processing. MIT Press, Cambridge.
C. J. Poel and S. D. Weatherly. 1997. A cloze look at place-
ment testing. Shiken: JALT (Japanese Assoc. for Language
Teaching) Testing & Evaluation SIG Newsletter, 1(1):4?10.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proc. of the Conf. on Empirical Methods in Natural
Language Processing, pages 133?142.
P. Resnik. 1997. Selectional preference and sense disambigua-
tion. In Proc. of the Applied NLP Workshop on Tagging Text
with Lexical Semantics: Why, What and How, pages 52?57.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In Proc. of the
Conf. on Applied Natural Language Processing, pages 16?
19.
K. M. Sheehan, P. Deane, and I. Kostin. 2003. A partially auto-
mated system for generating passage-based multiple-choice
verbal reasoning items. Paper presented at the Nat?l Council
on Measurement in Education Annual Meeting.
V. Stevens. 1991. Classroom concordancing: vocabulary ma-
terials derived from relevant authentic text. English for Spe-
cific Purposes, 10(1):35?46.
Y. Wilks and M. Stevenson. 1997. Combining independent
knowledge sources for word sense disambiguation. In Proc.
of the Conf. on Recent Advances in Natural Language Pro-
cessing, pages 1?7.
8
