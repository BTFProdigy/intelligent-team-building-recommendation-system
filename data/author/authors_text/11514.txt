Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 112?119,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Context-theoretic Semantics for Natural Language: an Overview
Daoud Clarke
University of Sussex
Falmer, Brighton, UK
daoud.clarke@gmail.com
Abstract
We present the context-theoretic frame-
work, which provides a set of rules for the
nature of composition of meaning based
on the philosophy of meaning as context.
Principally, in the framework the composi-
tion of the meaning of words can be repre-
sented as multiplication of their represen-
tative vectors, where multiplication is dis-
tributive with respect to the vector space.
We discuss the applicability of the frame-
work to a range of techniques in natu-
ral language processing, including subse-
quence matching, the lexical entailment
model of Dagan et al (2005), vector-based
representations of taxonomies, statistical
parsing and the representation of uncer-
tainty in logical semantics.
1 Introduction
Techniques such as latent semantic analysis (Deer-
wester et al, 1990) and its variants have been
very successful in representing the meanings of
words as vectors, yet there is currently no theory
of natural language semantics that explains how
we should compose these representations: what
should the representation of a phrase be, given the
representation of the words in the phrase? In this
paper we present such a theory, which is based
on the philosophy of meaning as context, as epit-
omised by the famous sayings of Wittgenstein
(1953), ?Meaning just is use? and Firth (1957),
?You shall know a word by the company it keeps?.
For the sake of brevity we shall present only a
summary of our research, which is described in
full in (Clarke, 2007), and we give a simplified
version of the framework, which nevertheless suf-
fices for the examples which follow.
We believe that the development of theories that
can take vector representations of meaning beyond
the word level, to the phrasal and sentence lev-
els and beyond are essential for vector based se-
mantics to truly compete with logical semantics,
both in their academic standing and in application
to real problems in natural language processing.
Moreover the time is ripe for such a theory: never
has there been such an abundance of immediately
available textual data (in the form of the world-
wide web) or cheap computing power to enable
vector-based representations of meaning to be ob-
tained. The need to organise and understand the
new abundance of data makes these techniques all
the more attractive since meanings are determined
automatically and are thus more robust in compar-
ison to hand-built representations of meaning. A
guiding theory of vector based semantics would
undoubtedly be invaluable in the application of
these representations to problems in natural lan-
guage processing.
The context-theoretic framework does not pro-
vide a formula for how to compose meaning;
rather it provides mathematical guidelines for the-
ories of meaning. It describes the nature of the
vector space in which meanings live, gives some
restrictions on how meanings compose, and pro-
vides us with a measure of the degree of entail-
ment between strings for any implementation of
the framework.
The remainder of the paper is structured as fol-
lows: in Section 2 we present the framework; in
Section 3 we present applications of the frame-
work:
? We describe subsequence matching (Section
3.1) and the lexical entailment model of (Da-
gan et al, 2005) (Section 3.2), both of which
have been applied to the task of recognising
textual entailment.
? We show how a vector based representation
of a taxonomy incorporating probabilistic in-
formation about word meanings can be con-
112
d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6
orange fruit orange ? fruit
Figure 1: Vector representations of two terms in
a space L1(S) where S = {d1, d2, d3, d4, d5, d6}
and their vector lattice meet (the darker shaded
area).
structed in Section 3.3.
? We show how syntax can be represented
within the framework in Section 3.4.
? We summarise our approach to representing
uncertainty in logical semantics in Section
3.5.
2 Context-theoretic Framework
The context-theoretic framework is based on the
idea that the vector representation of the meaning
of a word is derived from the contexts in which it
occurs. However it extends this idea to strings of
any length: we assume there is some set S con-
taining all the possible contexts associated with
any string. A context theory is an implementa-
tion of the context-theoretic framework; a key re-
quirement for a context theory is a mapping from
strings to vectors formed from the set of contexts.
In vector based techniques, the set of contexts
may be the set of possible dependency relations
between words, or the set of documents in which
strings may occur; in context-theoretic semantics
however, the set of ?contexts? can be any set.
We continue to refer to it as a set of contexts
since the intuition and philosophy which forms the
basis for the framework derives from this idea;
in practice the set may even consist of logical
sentences describing the meanings of strings in
model-theoretic terms.
An important aspect of vector-based techniques
is measuring the frequency of occurrence of
strings in each context. We model this in a gen-
eral way as follows: let A be a set consisting of
the words of the language under consideration.
The first requirement of a context theory is a map-
ping x 7? x? from a string x ? A? to a vector
x? ? L1(S)+, where L1(S) means the set of all
functions from S to the real numbers R which are
finite under the L1 norm,
?u?1 =
?
s?S
|u(s)|
and L1(S)+ restricts this to functions to the non-
negative real numbers, R+; these functions are
called the positive elements of the vector space
L1(S). The requirement that the L1 norm is finite,
and that the map is only to positive elements re-
flects the fact that the vectors are intended to repre-
sent an estimate of relative frequency distributions
of the strings over the contexts, since a frequency
distribution will always satisfy these requirements.
Note also that the l1 norm of the context vector of
a string is simply the sum of all its components
and is thus proportional to its probability.
The set of functions L1(S) is a vector space un-
der the point-wise operations:
(?u)(s) = ?u(s)
(u+ v)(s) = u(s) + v(s)
for u, v ? L1(S) and ? ? R, but it is also a lattice
under the operations
(u ? v)(s) = min(u(s), v(s))
(u ? v)(s) = max(u(s), v(s)).
In fact it is a vector lattice or Riesz space (Alipran-
tis and Burkinshaw, 1985) since it satisfies the fol-
lowing relationships
if u ? v then ?u ? ?v
if u ? v then u+ w ? v + w,
where ? ? R+ and ? is the partial ordering asso-
ciated with the lattice operations, defined by u ? v
if u ? v = u.
Together with the l1 norm, the vector lattice
defines an Abstract Lebesgue space (Abramovich
and Aliprantis, 2002) a vector space incorporating
all the properties of a measure space, and thus can
also be thought of as defining a probability space,
where ? and ? correspond to the union and inter-
section of events in the ? algebra, and the norm
corresponds to the (un-normalised) probability.
2.1 Distributional Generality
The vector lattice nature of the space under consid-
eration is important in the context-theoretic frame-
work since it is used to define a degree of entail-
ment between strings. Our notion of entailment is
113
based on the concept of distributional generality
(Weeds et al, 2004), a generalisation of the distri-
butional hypothesis of Harris (1985), in which it
is assumed that terms with a more general mean-
ing will occur in a wider array of contexts, an
idea later developed by Geffet and Dagan (2005).
Weeds et al (2004) also found that frequency
played a large role in determining the direction
of entailment, with the more general term often
occurring more frequently. The partial ordering
of the vector lattice encapsulates these properties
since x? ? y? if and only if y occurs more frequently
in all the contexts in which x occurs.
This partial ordering is a strict relationship,
however, that is unlikely to exist between any two
given vectors. Because of this, we define a degree
of entailment
Ent(u, v) = ?u ? v?1?u?1
.
This value has the properties of a conditional prob-
ability; in the case of u = x? and v = y? it is a
measure of the degree to which the contexts string
x occurs in are shared by the contexts string y oc-
curs in.
2.2 Multiplication
The map from strings to vectors already tells us ev-
erything we need to know about the composition
of words: given two words x and y, we have their
individual context vectors x? and y?, and the mean-
ing of the string xy is represented by the vector
x?y. The question we address is what relationship
should be imposed between the representation of
the meanings of individual words x? and y? and the
meaning of their composition x?y. As it stands, we
have little guidance on what maps from strings to
context vectors are appropriate.
The first restriction we propose is that vector
representations of meanings should be compos-
able in their own right, without consideration of
what words they originated from. In fact we place
a strong requirement on the nature of multiplica-
tion on elements: we require that the multiplica-
tion ? on the vector space defines a lattice-ordered
algebra. This means that multiplication is asso-
ciative, distributive with respect to addition, and
satisfies u ? v ? 0 if u ? 0 and v ? 0, i.e. the
product of positive elements is also positive.
We argue that composition of context vectors
needs to be compatible with concatenation of
words, i.e.
x? ? y? = x?y,
i.e. the map from strings to context vectors defines
a semigroup homomorphism. Then the require-
ment that multiplication is associative can be seen
to be a natural one since the homomorphism en-
forces this requirement for context vectors. Sim-
ilarly since all context vectors are positive their
product in the algebra must also be positive, thus it
is natural to extend this to all elements of the alge-
bra. The requirement for distributivity is justified
by our own model of meaning as context in text
corpora, described in full elsewhere.
2.3 Context Theory
The above requirements give us all we need to de-
fine a context theory.
Definition 1 (Context theory). ?A,S, ?, ? ? defines
a context theory if L1(S) is a lattice-ordered al-
gebra under the multiplication defined by ? and ?
defines a semigroup homomorphism x 7? x? from
A? to L1(S)+.
3 Context Theories for Natural
Language
In this section we describe applications of the
context-theoretic framework to applications in
computational linguistics and natural language
processing. We shall commonly use a construc-
tion in which there is a binary operation ? on S
that makes it a semigroup. In this case L1(S) is a
lattice-ordered algebra with convolution as multi-
plication:
(u ? v)(r) =
?
s?t=r
u(s)v(t)
for r, s, t ? S and u, v ? L1(S). We denote the
unit basis element associated with an element x ?
S by ex, that is ex(y) = 1 if and only if y = x,
otherwise ex(y) = 0.
3.1 Subsequence Matching
A string x ? A? is called a ?subsequence? of
y ? A? if each element of x occurs in y in the
same order, but with the possibility of other ele-
ments occurring in between, so for example abba
is a subsequence of acabcba in {a, b, c}?. We de-
note the set of subsequences of x (including the
empty string) by Sub(x). Subsequence match-
ing compares the subsequences of two strings: the
114
more subsequences they have in common the more
similar they are assumed to be. This idea has
been used successfully in text classification (Lodhi
et al, 2002) and recognising textual entailment
(Clarke, 2006).
We can describe such models using a context
theory ?A,A?, ?, ? ?, where ? is convolution in
L1(A?) and
x? = (1/2|x|)
?
y?Sub(x)
ey,
i.e. the context vector of a string is a weighted sum
of its subsequences. Under this context theory x? ?
y?, i.e. x completely entails y if x is a subsequence
of y.
Many variations on this context theory are pos-
sible, for example using more complex mappings
to L1(A?). The context theory can also be adapted
to incorporate a measure of lexical overlap be-
tween strings, an approach that, although simple,
performs comparably to more complex techniques
in tasks such as recognising textual entailment
(Dagan et al, 2005)
3.2 Lexical Entailment Model
Glickman and Dagan (2005) define their own
model of entailment and apply it to the task of
recognising textual entailment. They estimate
entailment between words based on occurrences
in documents: they estimate a lexical entailment
probability LEP(x, y) between two terms x and y
to be
LEP(x, y) ? nx,yny
where ny and nx,y denote the number of docu-
ments that the word y occurs in and the words x
and y both occur in respectively.
We can describe this using a context theory
?A,D, ?, ? ?, where D is the set of documents, and
x?(d) =
{
1 if x occurs in document d
0 otherwise. .
In this case the estimate of LEP(x, y) coincides
with our own degree of entailment Ent(x, y).
There are many ways in which the multiplica-
tion ? can be defined on L1(D). The simplest one
defines ed ? ef = ed if d = f and edef = 0 oth-
erwise. The effect of multiplication of the context
vectors of two strings is then set intersection:
(x??y?)(d) =
{
1 if x and y occur in document d
0 otherwise.
Model Accuracy CWS
Dirichlet (106) 0.584 0.630
Dirichlet (107) 0.576 0.642
Bayer (MITRE) 0.586 0.617
Glickman (Bar Ilan) 0.586 0.572
Jijkoun (Amsterdam) 0.552 0.559
Newman (Dublin) 0.565 0.6
Table 1: Results obtained with our Latent Dirichlet
projection model on the data from the first Recog-
nising Textual Entailment Challenge for two doc-
ument lengths N = 106 and N = 107 using a cut-
off for the degree of entailment of 0.5 at which
entailment was regarded as holding. CWS is the
confidence weighted score ? see (Dagan et al,
2005) for the definition.
Glickman and Dagan (2005) do not use this
measure, possibly because the problem of data
sparseness makes it useless for long strings. How-
ever the measure they use can be viewed as an ap-
proximation to this context theory.
We have also used this idea to determine en-
tailment, using latent Dirichlet alocation to get
around the problem of data sparseness. A model
was built using a subset of around 380,000 docu-
ments from the Gigaword corpus, and the model
was evaluated on the dataset from the first Recog-
nising Textual Entailment Challenge; the results
are shown in Table 1. In order to use the model, a
document length had to be chosen; it was found
that very long documents yielded better perfor-
mance at this task.
3.3 Representing Taxonomies
In this section we describe how the relationships
described by a taxonomy, the collection of is-
a relationships described by ontologies such as
WordNet (Fellbaum, 1989), can be embedded in
the vector lattice structure that is crucial to the
context-theoretic framework. This opens up the
way to the possibility of new techniques that
combine the vector-based representations of word
meanings with the ontological ones, for example:
? Semantic smoothing could be applied to
vector based representations of an ontology,
for example using distributional similarity
measures to move words that are distribution-
ally similar closer to each other in the vector
space. This type of technique may allow the
115
benefits of vector based techniques and on-
tologies to be combined.
? Automatic classification: representing the
taxonomy in a vector space may make it
easier to look for relationships between the
meanings in the taxonomy and meanings de-
rived from vector based techniques such as
latent semantic analysis, potentially aiding in
classifying word meanings in a taxonomy.
? The new vector representation could lead to
new measures of semantic distance, for ex-
ample, the Lp norms can all be used to
measure distance between the vector rep-
resentations of meanings in a taxonomy.
Moreover, the vector-based representation al-
lows ambiguity to be represented by adding
the weighted representations of individual
senses.
We assume that the is-a relation is a partial or-
dering; this is true for many ontologies. We wish
to incorporate the partial ordering of the taxonomy
into the partial ordering of the vector lattice. We
will make use of the following result relating to
partial orders:
Definition 2 (Ideals). A lower set in a partially
ordered set S is a set T such that for all x, y ? S,
if x ? T and y ? x then y ? T .
The principal ideal generated by an element x in
a partially ordered set S is defined to be the lower
set
?y(x) = {y ? S : y ? x}.
Proposition 3 (Ideal Completion). If S is a par-
tially ordered set, then
?y(?) can be considered as
a function from S to the powerset 2S . Under the
partial ordering defined by set inclusion, the set of
lower sets form a complete lattice, and ?y(?) is a
completion of S, the ideal completion.
We are also concerned with the probability of
concepts. This is an idea that has come about
through the introduction of ?distance measures?
on taxonomies (Resnik, 1995). Since terms can
be ascribed probabilities based on their frequen-
cies of occurrence in corpora, the concepts they re-
fer to can similarly be assigned probabilities. The
probability of a concept is the probability of en-
countering an instance of that concept in the cor-
pus, that is, the probability that a term selected
at random from the corpus has a meaning that is
subsumed by that particular concept. This ensures
that more general concepts are given higher proba-
bilities, for example if there is a most general con-
cept (a top-most node in the taxonomy, which may
correspond for example to ?entity?) its probability
will be one, since every term can be considered an
instance of that concept.
We give a general definition based on this idea
which does not require probabilities to be assigned
based on corpus counts:
Definition 4 (Real Valued Taxonomy). A real val-
ued taxonomy is a finite set S of concepts with a
partial ordering ? and a positive real function p
over S. The measure of a concept is then defined
in terms of p as
p?(x) =
?
y??(x)
p(y).
The taxonomy is called probabilistic if?
x?S p(s) = 1. In this case p? refers to the
probability of a concept.
Thus in a probabilistic taxonomy, the function
p corresponds to the probability that a term is ob-
served whose meaning corresponds (in that con-
text) to that concept. The function p? denotes the
probability that a term is observed whose meaning
in that context is subsumed by the concept.
Note that if S has a top element I then in the
probabilistic case, clearly p?(I) = 1. In studies of
distance measures on ontologies, the concepts in
S often correspond to senses of terms, in this case
the function p represents the (normalised) proba-
bility that a given term will occur with the sense
indicated by the concept. The top-most concept
often exists, and may be something with the mean-
ing ?entity??intended to include the meaning of
all concepts below it.
The most simple completion we consider is into
the vector lattice L1(S), with basis elements {ex :
x ? S}.
Proposition 5 (Ideal Vector Completion). Let S
be a probabilistic taxonomy with probability dis-
tribution function p that is non-zero everywhere on
S. The function ? from S to L1(S) defined by
?(x) =
?
y??(x)
p(y)ey
is a completion of the partial ordering of S un-
der the vector lattice order of L1(S), satisfying
??(x)?1 = p?(x).
116
entity
organism
plant
grass
cereal
oat rice barley
tree
beech chestnut oak
Figure 2: A small example taxonomy extracted
from WordNet (Fellbaum, 1989).
Proof. The function ? is clearly order-preserving:
if x ? y in S then since
?y(x) ?
?y(y) , neces-
sarily ?(x) ? ?(y). Conversely, the only way
that ?(x) ? ?(y) can be true is if
?y(x) ?
?y(y)
since p is non-zero everywhere. If this is the case,
then x ? y by the nature of the ideal completion.
Thus ? is an order-embedding, and since L1(S) is
a complete lattice, it is also a completion. Finally,
note that ??(x)?1 =
?
y??(x) p(y) = p?(x).
This completion allows us to represent concepts
as elements within a vector lattice so that not only
the partial ordering of the taxonomy is preserved,
but the probability of concepts is also preserved as
the size of the vector under the L1 norm.
3.4 Representing Syntax
In this section we give a description link grammar
(Sleator and Temperley, 1991) in terms of a con-
text theory. Link grammar is a lexicalised syntac-
tic formalism which describes properties of words
in terms of links formed between them, and which
is context-free in terms of its generative power; for
the sake of brevity we omit the details, although a
sample link grammar parse is show in Figure 3.
Our formulation of link grammar as a context
theory makes use of a construction called a free
inverse semigroup. Informally, the free inverse
semigroup on a set S is formed from elements
of S and their inverses, S?1 = {s?1 : s ? S},
satisfying no other condition than those of an in-
verse semigroup. Formally, the free inverse semi-
group is defined in terms of a congruence rela-
tion on (S?S?1)? specifying the inverse property
and commutativity of idempotents ? see (Munn,
they mashed their way through the thick mud
a
d
j
d
o
m
s
Figure 3: A link grammar parse. Link types:
s: subject, o: object, m: modifying phrases,
a: adjective, j: preposition, d: determiner.
1974) for details. We denote the free inverse semi-
group on S by FIS(S).
Free inverse semigroups were shown by Munn
(1974) to be equivalent to birooted word trees. A
birooted word-tree on a set A is a directed acyclic
graph whose edges are labelled by elements of A
which does not contain any subgraphs of the form
? a?? ? a?? ? or ? a?? ? a?? ?, together with
two distinguished nodes, called the start node, 2
and finish node, ?.
An element in the free semigroup FIS(S) is de-
noted as a sequence xd11 xd22 . . . xdnn where xi ? S
and di ? {1,?1}.
We construct the birooted word tree by starting
with a single node as the start node, and for each i
from 1 to n:
? Determine if there is an edge labelled xi leav-
ing the current node if di = 1, or arriving at
the current node if di = ?1.
? If so, follow this edge and make the resulting
node the current node.
? If not, create a new node and join it with an
edge labelled xi in the appropriate direction,
and make this node the current node.
The finish node is the current node after the n iter-
ations.
The product of two elements x and y in the free
inverse semigroup can be computed by finding the
birooted word-tree of x and that of y, joining the
graphs by equating the start node of y with the fin-
ish node of x (and making it a normal node), and
merging any other nodes and edges necessary to
remove any subgraphs of the form ? a?? ? a?? ?
or ? a?? ? a?? ?. The inverse of an element
has the same graph with start and finish nodes ex-
changed.
117
We can represent parses of sentences in link
grammar by translating words to syntactic cate-
gories in the free inverse semigroup. The parse
shown earlier for ?they mashed their way through
the thick mud? can be represented in the inverse
semigroup on S = {s,m, o, d, j, a} as
ss?1modd?1o?1m?1jdaa?1d?1j?1
which has the following birooted word-tree (the
words which the links derive from are shown in
brackets):
s(they,mashed)
m(mashed, through)
o(mashed,way)
d(their,way)
j(through,mud)
d(the,mud)
a(thick,mud)
Let A be the set of words in the natural lan-
guage under consideration, S be the set of link
types. Then we can form a context theory
?A,FIS(S), ?, ? ? where ? is multiplication defined
by convolution on FIS(S), and a word a ? A is
mapped to a probabilistic sum a? of its link possible
grammar representations (called disjuncts). Thus
we have a context theory which maps a string x
to elements of L1(FIS(S)); if there is a parse for
this string then there will be some component of
x? which corresponds to an idempotent element of
FIS(S). Moreover we can interpret the magnitude
of the component as the probability of that par-
ticular parse, thus the context theory describes a
probabilistic variation of link grammar.
3.5 Uncertainty in Logical Semantics
For the sake of brevity, we summarise our ap-
proach to representing uncertainty in logical se-
mantics, which is described in full elsewhere. Our
aim is to be able to reason with probabilistic infor-
mation about uncertainty in logical semantics. For
example, in order to represent a natural language
sentence as a logical statement, it is necessary
to parse it, which may well be with a statistical
parser. We may have hundreds of possible parses
and logical representations of a sentence, and as-
sociated probabilities. Alternatively, we may wish
to describe our uncertainty about word-sense dis-
ambiguation in the representation. Incorporating
such probabilistic information into the representa-
tion of meaning may lead to more robust systems
which are able to cope when one component fails.
The basic principle we propose is to first repre-
sent unambiguous logical statements as a context
theory. Our uncertainty about the meaning of a
sentence can then be represented as a probability
distribution over logical statements, whether the
uncertainty arises from parsing, word-sense dis-
ambiguation or any other source. Incorporating
this information is then straightforward: the rep-
resentation of the sentence is the weighted sum
of the representation of each possible meaning,
where the weights are given by the probability dis-
tribution.
Computing the degree of entailment using this
approach is computationally challenging, however
we have shown that it is possible to estimate the
degree of entailment by computing a lower bound
on this value by calculating pairwise degrees of
entailment for each possible logical statement.
4 Related Work
Mitchell and Lapata (2008) proposed a framework
for composing meaning that is extremely gen-
eral in nature: there is no requirement for linear-
ity in the composition function, although in prac-
tice the authors do adopt this assumption. Indeed
their ?multiplicative models? require composition
of two vectors to be a linear function of their ten-
sor product; this is equivalent to our requirement
of distributivity with respect to vector space addi-
tion.
Various ways of composing vector based repre-
sentations of meaning were investigated by Wid-
dows (2008), including the tensor product and di-
rect sum. Both of these are compatible with the
context theoretic framework since they are dis-
tributive with respect to the vector space addition.
Clark et al (2008) proposed a method of com-
posing meaning that generalises Montague seman-
tics; further work is required to determine how
their method of composition relates to the context-
theoretic framework.
Erk and Pado (2008) describe a method of com-
position that allows the incorporation of selec-
tional preferences; again further work is required
to determine the relation between this work and
the context-theoretic framework.
118
5 Conclusion
We have given an introduction to the context-
theoretic framework, which provides mathemat-
ical guidelines on how vector-based representa-
tions of meaning should be composed, how en-
tailment should be determined between these rep-
resentations, and how probabilistic information
should be incorporated.
We have shown how the framework can be ap-
plied to a wide range of problems in computational
linguistics, including subsequence matching, vec-
tor based representations of taxonomies and statis-
tical parsing. The ideas we have presented here are
only a fraction of those described in full in (Clarke,
2007), and we believe that even that is only the tip
of the iceberg with regards to what it is possible to
achieve with the framework.
Acknowledgments
I am very grateful to my supervisor David Weir
for all his help in the development of these ideas,
and to Rudi Lutz and the anonymous reviewers for
many useful comments and suggestions.
References
Y. A. Abramovich and Charalambos D. Aliprantis.
2002. An Invitation to Operator Theory. American
Mathematical Society.
Charalambos D. Aliprantis and Owen Burkinshaw.
1985. Positive Operators. Academic Press.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of
the Second Symposium on Quantum Interaction,
Oxford, UK, pages 133?140.
Daoud Clarke. 2006. Meaning as context and subse-
quence analysis for textual entailment. In Proceed-
ings of the Second PASCAL Recognising Textual En-
tailment Challenge.
Daoud Clarke. 2007. Context-theoretic Semantics
for Natural Language: an Algebraic Framework.
Ph.D. thesis, Department of Informatics, University
of Sussex.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entail-
ment.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41(6):391?407.
Katrin Erk and Sebastian Pado. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Christaine Fellbaum, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, Massachusetts.
John R. Firth. 1957. Modes of meaning. In Papers
in Linguistics 1934?1951. Oxford University Press,
London.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), University of Michigan.
Oren Glickman and Ido Dagan. 2005. A probabilis-
tic setting and lexical cooccurrence model for tex-
tual entailment. In ACL-05 Workshop on Empirical
Modeling of Semantic Equivalence and Entailment.
Zellig Harris. 1985. Distributional structure. In Jer-
rold J. Katz, editor, The Philosophy of Linguistics,
pages 26?47. Oxford University Press.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June. Association for Computational Linguistics.
W. D. Munn. 1974. Free inverse semigroup. Proceed-
ings of the London Mathematical Society, 29:385?
404.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In IJ-
CAI, pages 448?453.
Daniel D. Sleator and Davy Temperley. 1991. Pars-
ing english with a link grammar. Technical Report
CMU-CS-91-196, Department of Computer Sci-
ence, Carnegie Mellon University.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference of Computational Linguistics, COLING-
2004, Geneva, Switzerland.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Ox-
ford, UK.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Macmillan, New York. G. Anscombe, trans-
lator.
119
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2249?2259, Dublin, Ireland, August 23-29 2014.
Learning to Distinguish Hypernyms and Co-Hyponyms
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller
Department of Informatics,
University of Sussex,
Brighton, UK
juliewe,D.Clarke,J.P.Reffin,davidw,billk@sussex.ac.uk
Abstract
This work is concerned with distinguishing different semantic relations which exist between
distributionally similar words. We compare a novel approach based on training a linear Support
Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional
similarity. We show that the new supervised approach does better even when there is minimal
information about the target words in the training data, giving a 15% reduction in error rate over
unsupervised approaches.
1 Introduction
Over recent years there has been much interest in the field of distributional semantics, drawing on the
distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,
1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and
Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of ?nearest neighbours?)
automatically and applied them in a variety of applications, generally with a good deal of success.
In early research there was much interest in how these automatically generated thesauri compare with
human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).
More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Dis-
tributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala
et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), pre-
dicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,
2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), tax-
onomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).
A primary focus of distributional semantics has been on identifying words which are similar to each
other. However, semantic similarity encompasses a variety of different lexico-semantic and topical re-
lations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix
of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related
words. A central problem here is that whilst most measures of distributional similarity are symmetric,
some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)
which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and
determines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the co-
hyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is
symmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the
words cat, animal and dog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms
1
.
1
We read cat in the sense domestic cat rather than big cat, hence tiger is a co-hyponym rather than hyponym
of cat.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2249
cat dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,
snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23
animal bird 0.36, fish 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,
human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25
dog cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, fish
0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23
Table 1: Top 15 neighbours of cat, animal and dog generated using Lin?s similarity measure (Lin,
1998) considering all words and dependency features occurring 100 or more times in Wikipedia.
Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can
be useful to be able to distinguish between these different relationships. Consider the following two
sentences.
The cat ran across the road. (1)
The animal ran across the road. (2)
Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability
to determine whether entailment holds between the sentences, and in which direction, depends on the
ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we
know which is the hyponym and which is the hypernym?
In applying distributional semantics to the problem of textual entailment, there is a need to generalise
lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations
is crucial if approaches to the composition of distributional representations of meaning that are currently
receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual
entailment problem.
We formulate the challenge as follows: Consider a set of pairs of similar words ?A,B? where one of
three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are
related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section
2, we discuss existing attempts to address this problem through the use of various directional measures
of distributional similarity.
This paper considers the effectiveness of various supervised approaches, and makes the following
contributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,
achieving a significant reduction in error rate in comparison to existing state-of-the-art methods based
on the notion of distributional generality. Second, by comparing two different data sets, one built from
BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important
insights into the requirements of a valid evaluation of supervised approaches, and provide a data set
for further research in this area. Third, we show that when learning how to determine an ontological
relationship between a pair of similar words by means of the word?s distributional vectors, quite different
vector operations are useful when identifying different ontological relationships. In particular, using the
difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding
the vectors works well for the co-hyponym task.
2 Related Work
Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the
alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She
found that this measure improved results in language modelling, when a word?s distribution is smoothed
using the distributions of its nearest neighbours.
Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words
tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be
able to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible
2250
contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g.,
?the monstrous animal barked at the intruder?. Weeds et al. (2004) attempt to capture this asymmetry
by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision
and recall are defined as:
P
ww
(u, v) =
?
f?F (u)?F (v)
I(u, f)
?
f?F (u)
I(u, f)
and R
ww
(u, v) =
?
f?F (u)?F (v)
I(v, f)
?
f?F (v)
I(v, f)
where I(n, f) is the pointwise mutual information (PMI) between noun n and feature f and F(n) is the
set of all features f for which I(n, f) > 0.
By comparing the precision and recall of one word?s retrieval of another word?s contexts, they were
able to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet.
However, this was not significantly better than a baseline which proposed that the most frequent word
was the most general.
Clarke (2009) formalised the idea of distributional generality using a partially ordered vector space.
He also argued for using a variation of co-occurrence retrieval where precision and recall are defined as:
P
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (u)
I(u, f)
and R
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (v)
I(v, f)
Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should
have high recall and low precision, which would thus make it possible to distinguish them from other
related terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure
to identify hypernyms:
invCL(u, v) =
2
?
P
cl
(u, v) ? (1?R
cl
(u, v))
Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distin-
guishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009).
Geffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale
of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong cor-
relation between complete inclusion of prominent features and lexical entailment. However, they were
unable to assess this using an off-line corpus due to data sparseness.
Szpektor and Dagan (2008) found that the P
ww
measure tends to promote relationships between infre-
quent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed
using the geometric average of P
ww
and the symmetric similarity measure of Lin (1998) in order to
penalise low frequency words.
Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identi-
fying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similar-
ities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric
similarity measures previously proposed perform much better than symmetric similarity measures on
a directionality detection experiment, and that their method and that of Clarke (2009) outperform the
others with statistical significance. They also show that their measure is superior when used for term
expansion in an event detection task.
Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate
that support vector machines can generalise entailment relations between quantifier phrases to entailment
involving unseen quantifiers. They compare the performance of their system with the balAPinc measure.
The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large
corpora to find patterns that are indicative of hyponymy. For example, the pattern ?NP
X
and otherNP
Y
?
is an indication that NP
X
is a NP
Y
, i.e. that NP
X
is a hyponym of NP
Y
. They use machine learning
to identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to find
new relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over
valid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow
2251
et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic
relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms
themselves. Our approach is therefore more generally applicable to systems which compose distribu-
tional representations of meaning.
Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution.
In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional
similarity measure, WeightedCosine
2
, performs best. Also of note, Mikolov et al. (2013) propose a vector
offset method to capture syntactic and semantic regularities between word representations learnt by a
recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms
and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis.
Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which
is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative
than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS
dataset (Baroni and Lenci, 2011), this measure outperforms P
ww
at both discriminating hypernym test
pairs from other types of relation and at determining the direction of the entailment relation.
3 Methodology
The code used to perform our experiments has been open sourced, and is available online.
3
3.1 Vector Representations
Distributional information was collected for all of the nouns from Wikipedia provided they had oc-
curred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted
text using wp2txt
4
. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt
Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were ex-
tracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive
point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The
total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345.
Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds be-
tween two similar words A and B. Support vector machines (SVMs), which are effective across a variety
of classification scenarios, learn a boundary between two classes from a set of positive and negative ex-
ample vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we
do not start with a single vector, but with two distributional vectors v
A
and v
B
for the words A and B,
respectively. These vectors must be combined in some way to produce the SVM?s input, and a number
of ways were considered, defined in Table 2. Of these operations, the vector difference (used by svm-
DIFF and knnDIFF) and direct sum (used by svmCAT) are asymmetric, whereas the sum and pointwise
multiplication (used by svmADD and svmMULT) are symmetric.
We now motivate the use of each of these operations. First, we note that pointwise multiplication
(svmMULT) is intersective. Similar vectors will have a large intersection and it might be possible to
learn the features that nouns occurring in different semantic relations should share. However, it does
not retain any information about non-shared features and it is symmetric so it is difficult to see how it
would be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition (svmADD)
effectively performs the union of the features, giving emphasis to the shared features. Whilst it does
retain information about the non-shared features, it is also symmetric, making it difficult again to see
how it would be useful in determining the direction of an entailment relation
Vector difference (as used in svmDIFF and knnDIFF), on the other hand, is asymmetric. Further,
we might expect a small difference vector (containing many zeroes) to be indicative of similar nouns.
Further, considering the majority sign of features in this difference vector might indicate the direction of
2
The details of this measure are unpublished.
3
https://github.com/SussexCompSem/learninghypernyms
4
https://github.com/yohasebe/wp2txt
2252
entailment. Using an SVM, we might expect to be able to effectively learn which of these features should
be ignored and which should be combined, to decide the correct direction of entailment in the majority
number of cases in our training data. However, note that if one uses vector difference it is impossible to
distinguish between the case where a feature occurred with both nouns (to the same extent) and the case
where a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both
nouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct
sum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from
the original vectors. Finally, we consider the use of the single vector corresponding to the second word
(svmSING) as a baseline. High performance by this operation would indicate that we can learn features
of words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the
putative relationship.
We also note that the behaviour of these methods may differ depending on the weighting used for vec-
tors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition
where vectors are weighted using PMI to correspond to multiplication where vectors are weighted using
frequency or probability. However, the use of positive PMI (where negative PMI scores are regarded
equal to zero), which is consistent with other work in this area, means that this correspondence is lost.
Because of the nature of our datasets, we were concerned that systems could learn information about
the taxonomy from the relations in the training data, without making use of information in the vectors
themselves. To investigate this, we constructed random vectors to be used in place of the vectors derived
from Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially
exceeds the average number (398) of non-zero features in the Wikipedia vectors.
3.2 Classifiers
We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear
SVMs for speed and simplicity, since the point is to compare the different vector representations of
the pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly
supervised classifiers. These are listed in Table 2. For the linear SVMs and kNN classifier, we used the
scikit-learn implementations with default settings. For k nearest neighbours, we performed a parameter
search, using nested cross-validation, varying k between 1 and 50.
For weakly supervised approaches, we evaluated the measure on the training set, then found the best
threshold p on the training set that best divides the two classes using that measure. When classifying, we
determine that the relation holds if the value of the measure exceeds p.
svmDIFF A linear SVM trained on the vector difference v
B
? v
A
svmMULT A linear SVM trained on the pointwise product vector v
B
? v
A
svmADD A linear SVM trained on the vector sum v
B
+ v
A
svmCAT A linear SVM trained on the vector concatenation v
B
? v
A
svmSING A linear SVM trained on the vector v
B
knnDIFF k nearest neighbours (knn) trained on the vector difference v
B
? v
A
.1 < k < 50
widthdiff width(B) > width(A)? rel(A,B) where width(A) is number of non-zero features in A
singlewidth width(B) > p? rel(A,B)
cosineP sim
cos
(A,B) > p? rel(A,B) where sim
cos
(A,B) is cosine similarity using PPMI
linP sim
lin
(A,B) > p? rel(A,B) (Lin, 1998)
CRdiff P
ww
(A,B) > R
ww
(A,B)? rel(A,B) (Weeds et al., 2004)
clarkediff P
cl
(A,B) > R
cl
(A,B)? rel(A,B) (Clarke, 2009)
invCLP invCL(A,B) > p? rel(A,B) (Lenci and Benotto, 2012)
balAPincP balAPinc(A,B) > p? rel(A,B) (Kotlerman et al., 2010)
most freq The most frequent label in the training data is assigned to every test point.
Table 2: Implemented classifiers
2253
3.3 Data Sets
One of key the challenges of this work has been to construct a data set which accurately and validly tests
our hypotheses. All four of our datasets detailed below are available online
5
.
In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the
supervised classifiers making use of artefacts of the data. This would not only make it unfair to compare
the supervised approaches with the unsupervised approaches, but also make it unlikely that our results
would be generalisable to other data. Here, we outline the requirements for the data sets, the importance
of which is demonstrated by our initial results for a data set which does not satisfy all of them.
There should be an equal number of positive and negative examples of a semantic relation. Thus,
random guessing or labelling with the most frequently seen label in the training data will yield 50%
accuracy and precision. An advantage of incorporating this requirement means that evaluation can be in
terms of simple accuracy (or error rate).
It should not be possible to do well simply by considering the distributional similarity of the terms.
Hence, the negative examples need to be pairs of equally similar words, but where the relationship under
consideration does not hold.
It should not be possible to do well by pre-supposing an entailment relation and guessing the direction.
For example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from
WordNet, over 70% of the time the more frequent word is also the entailed word.
It should not be possible to do well using ontological information learnt about one or both of the
words from the training data that is not generalisable to their distributional representations. For example,
it should not be possible for the classifier simply to learn directly from the training pairs ?cat ISA
mammal? and ?mammal ISA animal? that ?cat ISA animal?. Furthermore, we must ensure that
a classifier cannot learn that a particular word is near the top of the ontological hierarchy, and, as a
result, do well by guessing that a particular pairing probably has an entailment relation. For example,
given many pairs such as ?cat ISA animal?, ?dog ISA animal?, a system which guessed ?rabbit
ISA animal? but not ?animal ISA rabbit? would do better than random guessing. Whilst both
of these types of information could be useful in a hybrid system, they do not require any distributional
information and therefore we would not be learning anything about the distributional features of animal
which make it likely to be a hypernym.
3.3.1 BLESS
We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of ex-
amples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete,
largely monosemous nouns. We will refer to these 200 nouns as the BLESS concepts.
hyponym
BLESS
is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms
were randomly selected to provide positive examples of entailment. The remaining hypernyms for the
given concept were reversed and taken with the same number of co-hyponyms, meronyms and random
words to form negative examples of entailment. A filter was applied to ensure that duplicate pairs were
not included (e.g., if ?cat,animal? is a positive pair then ?animal,cat? cannot be a negative pair).
cohyponym
BLESS
is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms
were taken as positive examples of this relation. The same total number of (and split evenly between)
hypernyms, meronyms and random words was taken to form the negative examples. The order of 50%
of the pairs was reversed and again duplicate pairs were disallowed.
In both cases the pairs are labelled as positive or negative for the specified semantic relation and in
both cases there are equal (?1) numbers of positive and negative examples. For 99% of the generated
BLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an
associated vector, the classifiers use a zero vector.
5
https://github.com/SussexCompSem/learninghypernyms
2254
3.3.2 WordNet
We constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS
data sets they more adequately satisfy the requirements laid out above
6
. We constructed a list of all non-
rare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to
have occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet
package) and to have occurred in Wikipedia at least 100 times. To be considered largely monosemous,
the predominant sense of the word needed to account for over 50% of the occurrences in the SemCor
frequency information provided with WordNet. This led to a list of 7613 nouns.
hyponym
WN
is a set of 2564 labelled pairs of nouns constructed in the following way. Pairs ?A,B? were
found in the list of nouns where B is an ancestor of A (i.e., A lexically entails B). Each found pair is
added either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already
been added and provided that each word has not previously been used in that position. Co-hyponym
pairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found
pair is added to the data set (as a negative) provided the reverse pairing has not already been added, and
provided that neither word has already been seen in that position in a pairing (either in the entailment
pairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives
is selected. This provides a balanced data set where half of the pairs are positive examples of entailment
and the other half are semantically similar but not entailing.
cohyponym
WN
is a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponym
WN
except the same number of co-hyponym pairs were selected as the total number of entailment pairs (in
either direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled
as negative. Thus, this provides a balanced data set where half of the pairs are positive examples of co-
hyponyms and the other half, the negative examples, are entailment pairs (with direction unspecified)
In both these sets, the average path distance between entailment pairs is 1.64, whereas path distance
between co-hyponym pairs is 2.
3.4 Experimental Setup
Most of our experiments were carried out using an implementation of five-fold cross-validation using
each combination of data set, vector set and classifier. In this setup, the pairs are randomly partitioned
into five subsets, one subset is held out for testing whilst the classifiers are trained on the remaining four,
and this process is repeated using each subset as the test set.
In initial experiments with the BLESS datasets, the SVM classifiers were able to achieve classification
accuracy of over 95% for hyponym
BLESS
and over 90% for cohyponym
BLESS
. However, the results us-
ing random vectors were not significantly different from using the distributional vectors harvested from
Wikipedia. This indicated that the classifiers were learning ontological information implicit in the train-
ing data. In order to address this, when using the BLESS datasets, we removed any pair from the training
data if either word was present in the test data. In order to preserve a reasonable amount of training data,
we implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all
datasets and classifiers, we found performance by the random vectors was no higher than 52%. This
indicates that the performance seen in Table 3 is due to learning from distributional features rather than
any ontological information implicit in the training set.
4 Results
In Table 3, we compare average accuracy for a number of different classifiers on each of two tasks,
distinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets.
Looking at the results for the hyponym
BLESS
data set, we can see that the SVM methods do generally
outperform the unsupervised methods. However, the best performing model is svmSING, suggesting
that, for this data set, it is best to try to learn the distributional features of more general terms, rather than
comparing the vector representations of the two terms under consideration.
6
Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only
provided for 200 nouns.
2255
dataset svmDIFF svmMULT svmADD svmCAT svmSING knnDIFF
hyponym
BLESS
0.74 0.56 0.66 0.68 0.75 0.54
cohyponym
BLESS
0.62 0.39 0.41 0.40 0.40 0.58
hyponym
WN
0.75 0.45 0.37 0.74 0.69 0.50
cohyponym
WN
0.37 0.60 0.68 0.64 0.58 0.50
dataset most freq cosineP linP widthdiff singlewidth CRdiff invCLP balAPincP
hyponym
BLESS
0.54 0.53 0.54 0.56 0.58 0.52 0.54 0.54
cohyponym
BLESS
0.61 0.79 0.78 - - - - -
hyponym
WN
0.50 0.53 0.52 0.70 0.65 0.70 0.66 0.53
cohyponym
WN
0.50 0.50 0.55 - - - - -
Table 3: Accuracy Figures for the data sets generated from BLESS and WordNet (standard errors <
0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also
omit results of clarkediff as these were consistently the same or less than CRdiff.
On the corresponding co-hyponym task, using the cohyponym
BLESS
data set, we see the best performing
classifier is the cosine measure. The cosine measure is able to perform relatively well here because a
substantial proportion of the negative examples (25%) are random unrelated words which will have low
cosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests
that measures such as the cosine measure ?prefer? words in symmetric semantic relationships such as co-
hyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of
the training data in this experimental set up with this data set. If, for example, our test concept is robin,
our approach requires that we will not have any training pairs containing robin, or any training pairs
containing any of the words to which robin is related in the test set. In a dataset as small as BLESS,
this requirement effectively removes all knowledge of the distributional features of words in the target
domain. Hence, the need for a larger dataset as we have extracted from WordNet.
Looking at the results for the hyponym
WN
data set, the directional SVM methods (svmDIFF and svm-
CAT) substantially outperform the symmetric SVM methods, and their performance is significantly better
(at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between
svmDIFF and knnDIFF. Both of these methods are trained on the differences of vectors. However, the
linear SVM outperforms kNN by 19?25%. This may suggest that the shape of the vector space inhabited
by the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative
pairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable.
Looking at the results for the cohyponym
WN
data set, it is clear that the unsupervised methods cannot
distinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially
better, with the best performance achieved by svmADD and svmCAT. Both of these methods essentially
retain information about all of the features of both words. svmMULT does much better than svmDIFF,
which suggests that the shared features are more indicative than the non-shared features for this task.
The reasonably high performance of svmSING on both data sets suggests that words which have co-
hyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which
are included as entailed words in the data set. We hypothesise that there are specific features which more
general words tend to share (regardless of their topic) which makes it possible to identify more general
words from more specific words. This is completely consistent with very recent results using SLQS, a
new entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical
contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms,
with some promising results. It would be plausible to hypothesise that svmSING is learning which nouns
typically have less informative contexts and are therefore likely to by hypernyms.
Given prior work, the performance of the balAPincP measure is lower than expected on the
hyponym
WN
dataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are deter-
mining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is
known that a relationship exists. It could be that the measure is particularly suited to the latter task.
2256
5 Conclusions and Further Work
We have shown that it is possible to predict to a large extent whether or not there is a specific semantic
relation between two words given their distributional vectors, using a supervised approach based on
linear SVMs. The increase in accuracy over unsupervised methods is significant at the 0.01% level and
corresponds to a substantial absolute reduction in error rate (over 15%).
We have also shown that the choice of vector operation is significant. Whilst concatenating the vectors,
and therefore retaining all of the information from both vectors including direction, generally performs
well, we have also shown that different vector operations are useful in establishing different relationships.
In particular, the vector difference operation, which loses information about the original vectors, achieved
performance indistinguishable from concatenation on the entailment task, where the classifier is required
to distinguish hyponyms from other semantically related words including hypernyms. On the other
hand, the addition operation, which also loses information, outperformed concatenation by 4% (which
is statistically significant at the 0.01% level) on the coordinate task, where the classifier is required to
distinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is
trying to establish between words determines the nature of the operation one should perform on their
associated vectors.
We have also shown that it is possible to outperform state-of-the-art unsupervised methods even when
a data set has been constructed without ontological information, and when target words have not previ-
ously been seen in that position of a relationship in the training data. Hence, we believe the supervised
methods are learning characteristics of the underlying feature space which are generalisable to new words
(inhabiting the same feature space).
In future work, we intend to apply this approach to the problem of labelling the distributional neigh-
bours found for a given word with specific semantic relations. We also plan to investigate the use of
bag-of-words (windowed) vectors instead of grammatical relations for this task.
Finally, we believe that the data sets constructed from WordNet, which we publish alongside this
paper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised
and unsupervised methods.
Acknowledgements
This work was funded by UK EPSRC project EP/IO37458/1 ?A Unified Model of Compositional and
Distributional Compositional Semantics: Theory and Applications?.
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 23?32. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, July. Association for Computational Linguistics.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic composi-
tionality of prefix verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 293?303, Cambridge, MA, October. Association for Computational Linguistics.
Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sen-
sitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011).
2257
Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography.
In Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ?89, pages
76?83, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop of Geometric Models for Natural Language Semantics.
James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.
Georgiana Dinu and Stefan Thater. 2012. Saarland: Vector-based models of semantic textual similarity. In
Proceedings of the First Joint Conference on Lexical and Computational Semantics.
Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge,
Massachusetts.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 466?476, Montr?eal, Canada, June.
Maayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceed-
ings of the 43rd meeting of the Association for Computational Liuguistics (ACL), pages 107?114.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Con-
ference on Computational Semantics (IWCS 2011), pages 125?134.
Zelig Harris. 1954. Distributional structure. Word, 10:146?162.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted
WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1532?1541, Uppsala, Sweden, July.
Adam Kilgarriff and Colin Yallop. 2000. What?s in a thesaurus? In Proceedings of the 2nd International
Conference on Language Resources and Evaluation (LREC2000).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional simi-
larity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics,
4(16):359?389.
Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, College Park, Maryland, USA, June.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceed-
ings of the First Joint Conference on Lexical and Computational Semantics (*Sem).
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International
Conference on Computational Linguistics (COLING 1998).
Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356?365, Cambridge, MA,
October. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751, Atlanta, Georgia, June.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for
lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012, pages
1781?1796, Mumbai, India, December.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, Ohio, June. Association for Computational Linguistics.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on
Incremental Parsing, pages 50?57.
2258
Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 391?400, Atlanta, Georgia, June. Association for Computational Linguistics.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector
spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics, pages 38?42, Gothenburg, Sweden, April.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Processing Systems 17.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 801?808. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using
delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 1131?1141, Atlanta, Georgia, June.
Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856, Manchester, UK,
August.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
pages 81?88.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of Coling 2004, pages 1015?1021, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Julie Weeds, David Weir, and Jeremy Reffin. 2014. Distributional composition using higher-order dependency
vectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality,
EACL 2014, Gothenburg, Sweden, April.
Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second
Symposium on Quantum Interaction, Oxford, UK, pages 1?8.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, July. Association for Computational Linguistics.
Chen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings
of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756?766, Cambridge,
MA, October. Association for Computational Linguistics.
2259
A Context-Theoretic Framework for
Compositionality in Distributional Semantics
Daoud Clarke?
University of Hertfordshire
Formalizing ?meaning as context? mathematically leads to a new, algebraic theory of meaning,
in which composition is bilinear and associative. These properties are shared by other methods
that have been proposed in the literature, including the tensor product, vector addition, point-
wise multiplication, and matrix multiplication.
Entailment can be represented by a vector lattice ordering, inspired by a strengthened
form of the distributional hypothesis, and a degree of entailment is defined in the form of a
conditional probability. Approaches to the task of recognizing textual entailment, including the
use of subsequence matching, lexical entailment probability, and latent Dirichlet alocation, can
be described within our framework.
1. Introduction
This article presents the thesis that defining meaning as context leads naturally to
a model in which meanings of strings are represented as elements of an associative
algebra over the real numbers, and entailment is described by a vector lattice ordering.
This model is general enough to encompass several proposed methods of composition
in vector-based representations of meaning.
In recent years, the abundance of text corpora and computing power has allowed
the development of techniques to analyze statistical properties of words. For example
techniques such as latent semantic analysis (Deerwester et al 1990) and its variants,
and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspects
of the meanings of words by statistical analysis, and statistical information is often
used when parsing to determine sentence structure (Collins 1997). These techniques
have proved useful in many applications within computational linguistics and natural
language processing (Grefenstette 1994; Schu?tze 1998; Bellegarda 2000; Choi, Wiemer-
Hastings, and Moore 2001; Lin 2003; McCarthy et al 2004), arguably providing evidence
that they capture something about the nature of words that should be included in
representations of their meaning. However, it is very difficult to reconcile these tech-
niques with existing theories of meaning in language, which revolve around logical
and ontological representations. The new techniques, almost without exception, can be
viewed as dealing with vector-based representations of meaning, placing meaning (at
least at the word level) within the realm of mathematics and algebra; conversely the
older theories of meaning dwell in the realm of logic and ontology. It seems there is
? Gorkana Group, Discovery House, 28?48 Banner Street, London EC1Y8QE.
E-mail: daoud.clarke@gorkana.com.
Submission received: 29 October 2008; revised submission received: 28 April 2011; accepted for publication:
29 May 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
no unifying theory of meaning to provide guidance to those making use of the new
techniques.
The problem appears to be a fundamental one in computational linguistics because
the whole foundation of meaning seems to be in question. The older, logical theories
often subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993;
Blackburn and Bos 2005). According to this approach, sentences should be translated to
a logical form that can be interpreted as a description of the state of the world. The new
vector-based techniques, on the other hand, are often closer in spirit to the philosophy
of meaning as context, the idea that the meaning of an expression is determined by how
it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who
said that ?meaning just is use,? Firth?s (1968) ?You shall know a word by the company
it keeps,? and the distributional hypothesis of Harris (1968), that words will occur in
similar contexts if and only if they have similar meanings. This hypothesis is justified
by the success of techniques such as latent semantic analysis as well as experimental
evidence (Miller and Charles 1991). Although the two philosophies are not obviously
incompatible?especially because the former applies mainly at the sentence level and
the latter mainly at the word level?it is not clear how they relate to each other.
The problem of how to compose vector representations of meanings of words has
recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and
Lapata 2008; Widdows 2008; Erk and Pado? 2009; Baroni and Zamparelli 2010; Guevara
2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier
work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998;
Kintsch 2001). A solution to this problem would have practical as well as philosophical
benefits. Current techniques such as latent semantic analysis work well at the word
level, but we cannot extend them much beyond this, to the phrase or sentence level,
without quickly encountering the data-sparseness problem: There are not enough occur-
rences of strings of words to determine what their vectors should be merely by looking
in corpora. If we knew how such vectors should compose then we would be able to
extend the benefits of the vector based techniques to the many applications that require
reasoning about the meaning of phrases and sentences.
This article describes the results of our own efforts to identify a theory that can unite
these two paradigms, introduced in the author?s DPhil thesis (Clarke 2007). In addition,
we also discuss the relationship between this theory and methods of composition that
have recently been proposed in the literature, showing that many of them can be
considered as falling within our framework.
Our approach in identifying the framework is summarized in Figure 1:
 Inspired by the philosophy of meaning as context and vector-based
techniques we developed a mathematical model of meaning as context,
in which the meaning of a string is a vector representing contexts in
which that string occurs in a hypothetical infinite corpus.
 The theory on its own is not useful when applied to real-world corpora
because of the problem of data sparseness. Instead we examine the
mathematical properties of the model, and abstract them to form a
framework which contains many of the properties of the model.
Implementations of the framework are called context theories because
they can be viewed as theories about the contexts in which strings
occur. By analogy with the term ?model-theoretic? we use the term
?context-theoretic? for concepts relating to context theories, thus we
call our framework the context-theoretic framework.
42
Clarke A Context-Theoretic Framework for Distributional Semantics
Figure 1
Our approach in developing the context-theoretic framework.
 In order to ensure that the framework was practically useful, context
theories were developed in parallel with the framework itself. The aim
was to be able to describe existing approaches to representing meaning
within the framework as fully as possible.
In developing the framework we were looking for specific properties; namely, we
wanted it to:
 provide some guidelines describing in what way the representation of a
phrase or sentence should relate to the representations of the individual
words as vectors;
 require information about the probability of a string of words to be
incorporated into the representation;
 provide a way to measure the degree of entailment between strings based
on the particular meaning representation;
 be general enough to encompass logical representations of meaning; and
 be able to incorporate the representation of ambiguity and uncertainty,
including statistical information such as the probability of a parse or the
probability that a word takes a particular sense.
The framework we present is abstract, and hence does not subscribe to a particular
method for obtaining word vectors: They may be raw frequency counts, or vectors ob-
tained by a method such as latent semantic analysis. Nor does the framework provide a
recipe for how to represent meaning in natural language; instead it provides restrictions
on the set of possibilities. The advantage of the framework is in ensuring that techniques
are used in a way that is well-founded in a theory of meaning. For example, given vector
representations of words, there is not one single way of combining these to give vector
representations of phrases and sentences, but in order to fit within the framework there
are certain properties of the representation that need to hold. Any method of combining
43
Computational Linguistics Volume 38, Number 1
these vectors in which these properties hold can be considered within the framework
and is thus justified according to the underlying theory; in addition the framework
instructs us as to how to measure the degree of entailment between strings according to
that particular method.
The contribution of this article is as follows:
 We define the context-theoretic framework and introduce the mathematics
necessary to understand it. The description presented here is cleaner than
that of Clarke (2007), and in addition we provide examples that should
provide intuition for the concepts we describe.
 We relate the framework to methods of composition that have been
proposed in the literature, namely:
? vector addition (Landauer and Dumais 1997; Foltz, Kintsch,
and Landauer 1998);
? the tensor product (Smolensky 1990; Clark and Pulman 2007;
Widdows 2008);
? the multiplicative models of Mitchell and Lapata (2008);
? matrix multiplication (Baroni and Zamparelli 2010; Rudolph
and Giesbrecht 2010); and
? the approach of Clark, Coecke, and Sadrzadeh (2008).
It is important to note that the purpose of describing related work in terms of our
framework is not merely to demonstrate the generality of our framework: In doing
so, we identify previously ignored features of this work such as the lattice structure
within the vector space. This allows any one of these approaches to be endowed with an
entailment property defined by this lattice structure, based on a philosophy of meaning
as context.
Although the examples described here show that existing approaches can be de-
scribed within the framework and show some of its potential, they cannot demonstrate
its full power. The mathematical structures we make use of are extremely general, and
we hope that in the future many interesting discoveries will be made by exploring the
realm we identify here.
Our approach in defining the framework may be perceived as overly abstract;
however, we believe this approach has many potential benefits, because approaches to
composition which may have been considered unrelated (such as the tensor product
and vector addition) are now shown to be related. This means that when studying
such constructions, work can be avoided by considering the general case, for the same
reason that class inheritance aids code reuse. For example, definitions given in terms
of the framework can be applied to all instances, such as our definition of a degree
of entailment. We also hope to motivate people to prove theorems in terms of the
framework, having demonstrated its wide applicability.
The remainder of the article is as follows: In Section 2 we define our framework,
introducing the necessary definitions, and showing how related work fits into the
framework. In Section 3 we introduce our motivating example, showing that a simple
mathematical definition of the notions of ?corpus? and ?context? leads to an instance
of our framework. In Section 4, we describe specific instances of our framework in
application to the task of recognizing textual entailment. In Section 5 we show how the
44
Clarke A Context-Theoretic Framework for Distributional Semantics
sophisticated approach of Clark, Coecke, and Sadrzadeh (2008) can be described within
our framework. Finally, in Section 6 we present our conclusions and plans for further
work.
2. Context Theory
In this section, we define the fundamental concept of our concern, a context theory,
and discuss its properties. The definition is an abstraction of both the more commonly
used methods of defining composition in vector-based semantics and our motivating
example of meaning as context, described in the next section. Because of its relation to
this motivating example, a context theory can be thought of as a hypothesis describing
in what contexts all strings occur.
Definition 1 (Context Theory)
A context theory is a tuple ?A,A,?,V,??, where A is a set (the alphabet), A is a unital
algebra over the real numbers, ? is a function from A to A, V is an abstract Lebesgue
space, and ? is an injective linear map from A to V.
We will explain each part of this definition, introducing the necessary mathematics
as we proceed. We assume the reader is familiar with linear algebra; see Halmos (1974)
for definitions that are not included here.
2.1 Algebra over a Field
We have identified an algebra over a field (or simply algebra when there is no am-
biguity) as an important construction because it generalizes nearly all the methods of
vector-based composition that have been proposed. An algebra adds a multiplication
operation to a vector space; the vector space is intended to describe meaning, and it
is this multiplication operation that defines the composition of meaning in context-
theoretic semantics.
Definition 2 (Algebra over a Field)
An algebra over a field is a vector space A over a field K together with a binary
operation (a, b) ? ab on A that is bilinear,
a(?b+ ?c) = ?ab+ ?ac (1)
(?a+ ?b)c = ?ac+ ?bc (2)
and associative, (ab)c = a(bc) for all a, b, c ? A and all ?,? ? K. Some authors do not
place the requirement that an algebra is associative, in which case our definition would
refer to an associative algebra. An algebra is called unital if it has a distinguished unity
element 1 satisfying 1x = x1 = x for all x ? A. We are generally only interested in real
algebras, where K is the field of real numbers, R.
Example 1
The square real-valued matrices of order n form a real unital associative algebra under
standard matrix multiplication. The vector operations are defined entry-wise. The unity
element of the algebra is the identity matrix.
45
Computational Linguistics Volume 38, Number 1
This means that our proposal is more general than that of Rudolph and Giesbrecht
(2010), who suggest using matrix multiplication as a framework for distributional
semantic composition. The main differences in our proposal are as follows.
 We allow dimensionality to be infinite, instead of restricting ourselves to
finite-dimensional matrices.
 Matrix algebras form a ?-algebra, whereas we do not currently impose this
requirement.
 Many of the vector spaces used in computational linguistics have an
implicit lattice structure; we emphasize the importance of this structure
and use the associated partial ordering to define entailment.
The purpose of ? in the context theory is to associate elements of the algebra with
strings of words. Considering only the multiplication of A (and ignoring the vector
operations), A is a monoid, because we assumed that the multiplication on A is asso-
ciative. Then ? induces a monoid homomorphism a ? a? from A? to A. We denote the
mapped value of a ? A? by a? ? A, which is defined as follows:
a? = ?(a1)?(a2) . . . ?(an) (3)
where a = a1a2 . . . an for ai ? A, and we define ? = 1, where  is the empty string. Thus,
the mapping defined by ? allows us to associate an element of the algebra with every
string of words.
The algebra is what tells us how meanings compose. A crucial part of our thesis
is that meanings can be represented by elements of an algebra, and that the type of
composition that can be defined using an algebra is general enough to describe the
composition of meaning in natural language. To go some way towards justifying this,
we give several examples of algebras that describe methods of composition that have
been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata
2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998),
and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008).
Example 2 (Point-wise Multiplication)
Consider the n-dimensional real vector space Rn. We describe a vector u ? Rn in terms
of its components as (u1,u2, . . . un) with each ui ? R. We can define a multiplication ? on
this space by
(u1,u2, . . . ,un) ? (v1, v2, . . . , vn) = (u1v1,u2v2, . . . unvn) (4)
It is easy to see that this satisfies the requirements for an algebra as specified earlier.
Table 1 shows a simple example of possible occurrences for three terms in three different
contexts, d1, d2, and d3, which may, for example, represent documents. We use this to
define the mapping ? from terms to vectors. Thus, in this example, we have ?(cat) =
(0, 2, 3) and ?(big) = (1, 3, 0). Under point-wise multiplication, we would have
b?ig cat = ?(big) ? ?(cat) = (1, 3, 0) ? (0, 2, 3) = (0, 6, 0) (5)
46
Clarke A Context-Theoretic Framework for Distributional Semantics
Table 1
Example of possible occurrences for three terms in three different contexts.
d1 d2 d3
cat 0 2 3
animal 2 1 2
big 1 3 0
One commonly used operation for composing vector-based representations of
meaning is vector addition. As noted by Rudolph and Giesbrecht (2010), this can be
described using matrix multiplication, by embedding an n-dimensional vector u into a
matrix of order n+ 1:
?
?
?
?
?
?
?
? u1 u2 ? ? ? un
0 ? 0 ? ? ? 0
0 0 ? ? ? ? 0
...
...
...
...
0 0 0 ? ? ? ?
?
?
?
?
?
?
?
(6)
where ? = 1. The set of all such matrices, for all real values of ?, forms a subalgebra of
the algebra of matrices of order n+ 1. A subalgebra of an algebraA is a sub-vector space
of A which is closed under the multiplication of A. This subalgebra can be equivalently
described as follows:
Example 3 (Additive Algebra)
For two vectors u = (?,u1,u2, . . . un) and v = (?, v1, v2 . . . vn) in R
n+1, we define the
additive product  by
u v = (??, ?v1 + ?u1, ?v2 + ?u2, . . . ?vn + ?un) (7)
To verify that this multiplication makes Rn+1 an algebra, we can directly verify the
bilinear and associativity requirements, or check that it is isomorphic to the subalgebra
of matrices discussed previously.
Using Table 1, we define ?+ so that it maps n-dimensional context vectors to R
n+1,
where the first component is 1, so ?+(big) = (1, 1, 3, 0) and ?+(cat) = (1, 0, 2, 3) and
b?ig cat = ?+(big) ?+(cat) = (1, 1, 5, 3) (8)
Point-wise multiplication and addition are not ideal as methods for composing
meaning in natural language because they are commutative; although it is often useful
to consider the simpler, commutative case, natural language itself is inherently non-
commutative. One obvious method of composing vectors that is not commutative is the
tensor product. This method of composition can be viewed as a product in an algebra by
considering the tensor algebra, which is formed from direct sums of all tensor powers
of a base vector space.
We assume the reader is familiar with the tensor product and direct sum (see
Halmos [1974] for definitions); we recall their basic properties here. Let Vn denote a
vector space of dimensionality n (note that all vector spaces of a fixed dimensionality
47
Computational Linguistics Volume 38, Number 1
are isomorphic). Then the tensor product space Vn ? Vm is isomorphic to a space Vnm of
dimensionality nm; moreover, given orthonormal bases B = {b1, b2, . . . , bn} for Vn and
C = {c1, c2, . . . , cm} for Vm there is an orthonormal basis for Vnm defined by
{bi ? cj : 1 ? i ? n and 1 ? j ? m} (9)
Example 4
The multiplicative models of Mitchell and Lapata (2008) correspond to the class of finite
dimensional algebras. LetA be a finite-dimensional vector space. Then every associative
bilinear product on A can be described by a linear function T from A?A to A, as
required in Mitchell and Lapata?s model. To see this, consider the action of the product ?
on two orthonormal basis vectors a and b of A. This is a vector in A, thus we can define
T(a? b) = a ? b. By considering all basis vectors, we can define the linear function T.
If the tensor product can loosely be viewed as ?multiplying? vector spaces, then the
direct sum is like adding them; the space Vn ? Vm has dimensionality n+m and has
basis vectors
{bi ? 0 : 1 ? i ? n} ? {0 ? cj : 1 ? j ? m}; (10)
it is usual to write b? 0 as b and 0 ? c as c.
Example 5 (Tensor Algebra)
If V is a vector space, then we define T(V), the free algebra of tensor algebra generated
by V, as:
T(V) = R? V ? (V ? V) ? (V ? V ? V) ? ? ? ? (11)
where we assume that the direct sum is commutative. We can think of it as the direct
sum of all tensor powers of V, with R representing the zeroth power. In order to make
this space an algebra, we define the product on elements of these tensor powers, viewed
as subspaces of the tensor algebra, as their tensor product. This is enough to define the
product on the whole space, because every element can be written as a sum of tensor
powers of elements of V. There is a natural embedding from V to T(V), where each
element maps to an element in the first tensor power. Thus for example we can think of
u, u? v, and u? v+ w as elements of T(V), for all u, v,w ? V.
This product defines an algebra because the tensor product is a bilinear operation.
Taking V = R3 and using ? as the natural embedding from the context vector of a string
T(V), our previous example becomes
b?ig cat = ?(big) ? ?(cat) (12)
= (1, 3, 0) ? (0, 2, 3) (13)
?= (1(0, 2, 3), 3(0, 2, 3), 0(0, 2, 3)) (14)
?= (0, 2, 3, 0, 6, 9, 0, 0, 0) (15)
where the last two lines demonstrate how a vector in R3 ? R3 can be described in the
isomorphic space R9.
48
Clarke A Context-Theoretic Framework for Distributional Semantics
2.2 Vector Lattices
The next part of the definition specifies an abstract Lebesgue space. This is a special
kind of vector lattice, or even more generally, a partially ordered vector space. This
lattice structure is implicit in most vector spaces used in computational linguistics, and
an important part of our thesis is that the partial ordering can be interpreted as an
entailment relation.
Definition 3 (Partially Ordered Vector Space)
A partially ordered vector spaceV is a real vector space together with a partial ordering
? such that:
if x ? y then x+ z ? y+ z
if x ? y then ?x ? ?y
for all x, y, z ? V, and for all ? ? 0. Such a partial ordering is called a vector space order
on V. An element u of V satisfying u ? 0 is called a positive element; the set of all
positive elements of V is denoted V+. If ? defines a lattice on V then the space is called
a vector lattice or Riesz space.
Example 6 (Lattice Operations on Rn)
A vector lattice captures many properties that are inherent in real vector spaces when
there is a distinguished basis. In Rn, given a specific basis, we can write two vectors u and
v as sequences of numbers: u = (u1,u2, . . . un) and v = (v1, v2, . . . vn). This allows us to
define the lattice operations of meet ? and join ? as
u ? v = (min(u1, v1),min(u2, v2), . . .min(un, vn)) (16)
u ? v = (max(u1, v1),max(u2, v2), . . .max(un, vn)) (17)
These are the component-wise minimum and maximum, respectively. The partial
ordering is then given by u ? v if and only if u ? v = u, or equivalently un ? vn for all
n. A graphical depiction of the meet operation is shown in Figure 2.
The vector operations of addition and multiplication by scalar, which can be defined
in a similar component-wise fashion, are nevertheless independent of the particular
Figure 2
Vector representations of the terms orange and fruit based on hypothetical occurrences in six
documents and their vector lattice meet (the darker shaded area).
49
Computational Linguistics Volume 38, Number 1
basis chosen. Conversely, the lattice operations depend on the choice of basis, so the
operations as defined herein would behave differently if the components were written
using a different basis. We argue that it makes sense for us to consider these properties
of vectors in the context of computational linguistics because we can often have a distin-
guished basis: namely, the one defined by the contexts in which terms occur. Of course it
is true that techniques such as latent semantic analysis introduce a new basis which does
not have a clear interpretation in relation to contexts; nevertheless they nearly always
identify a distinguished basis which we can use to define the lattice operations. Because
our aim is a theory of meaning as context, we should include in our theory a description
of the lattice structure which arises out of consideration of these contexts.
We argue that the mere association of words with vectors is not enough to constitute
a theory of meaning?a theory of meaning must allow us to interpret these vectors. In
particular it should be able to tell us whether one meaning entails or implies another;
indeed this is one meaning of the verb to mean. Entailment is an asymmetric relation: ?x
entails y? does not have the same meaning as ?y entails x?. Vector representations allow
the measurement of similarity or distance, through an inner product or metric; this is a
symmetric relation, however, and so cannot be suitable for describing entailment.
In propositional and first order logic, the entailment relation is a partial ordering;
in fact it is a Boolean algebra, which is a special kind of lattice. It seems natural to
consider whether the lattice structure that is inherent in the vector representations used
in computational linguistics can be used to model entailment.
We believe our framework is suited to all vector-based representations of natural
language meaning, however the vectors are obtained. Given this assumption, we can
only justify our assumption that the partial order structure of the vector space is suitable
to represent the entailment relation by observing that it has the right kind of properties
we would expect from this relation.
There may be more justification for this assumption, however, based on the case
where the vectors for terms are simply their frequencies of occurrences in n different
contexts, so that they are vectors in Rn. In this case, the relation ?(x) ? ?(y) means
that y occurs at least as frequently as x in every context. This means that y occurs in
at least as wide a range of contexts as x, and occurs as least as frequently as x. Thus the
statement ?x entails y if and only if ?(x) ? ?(y)? can be viewed as a stronger form of the
distributional hypothesis of Harris (1968).
In fact, this idea can be related to the notion of distributional generality, introduced
by Weeds, Weir, and McCarthy (2004) and developed by Geffet and Dagan (2005). A
term x is distributionally more general than another term y if x occurs in a subset of the
contexts that y occurs in. The idea is that distributional generality may be connected to
semantic generality. An example of this is the hypernymy or is-a relation that is used to
express generality of concepts in ontologies; for example, the term animal is a hypernym
of dog because a dog is an animal. Weeds, Weir, and McCarthy (2004, p. 1019) explain
the connection to distributional generality as follows:
Although one can obviously think of counter-examples, we would generally expect that
the more specific term dog can only be used in contexts where animal can be used and
that the more general term animal might be used in all of the contexts where dog is used
and possibly others. Thus, we might expect that distributional generality is correlated
with semantic generality. . .
Our proposal, in the case where words are represented by frequency vectors, can
be considered a stronger version of distributional generality, where the additional
50
Clarke A Context-Theoretic Framework for Distributional Semantics
requirement is on the frequency of occurrences. In practice, this assumption is unlikely
to be compatible with the ontological view of entailment. For example the term entity
is semantically more general than the term animal; however, entity is unlikely to occur
more frequently in each context, because it is a rarer word. A more realistic foundation
for this assumption might be if we were to consider the components for a word to
represent the plausibility of observing the word in each context. The question then,
of course, is how such vectors might be obtained. Another possibility is to attempt to
weight components in such a way that entailment becomes a plausible interpretation
for the partial ordering relation.
Even if we allow for such alternatives, however, in general it is unlikely that the
relation will hold between any two strings, because u ? v iff ui ? vi for each component,
ui, vi, of the two vectors. Instead, we propose to allow for degrees of entailment. We take
a Bayesian perspective on this, and suggest that the degree of entailment should take
the form of a conditional probability. In order to define this, however, we need some
additional structure on the vector lattice that allows it to be viewed as a description of
probability, by requiring it to be an abstract Lebesgue space.
Definition 4 (Banach Lattice)
A Banach lattice V is a vector lattice together with a norm ? ? ? such that V is complete
with respect to ? ? ?.
Definition 5 (Abstract Lebesgue Space)
An abstract Lebesgue (or AL) space is a Banach lattice V such that
?u+ v? = ?u?+ ?v? (18)
for all u, v in V with u ? 0, v ? 0 and u ? v = 0.
Example 7 (p Spaces)
Let u = (u1,u2, . . .) be an infinite sequence of real numbers. We can view ui as compo-
nents of the infinite-dimensional vector u. We call the set of all such vectors the sequence
space; it is a vector space where the operations are defined component-wise. We define
a set of norms, the p-norms, on the space of all such vectors by
?u?p =
(
?
i>0
|ui|p
)1/p
(19)
The space of all vectors u for which ?u?p is finite is called the p space. Considered
as vector spaces, these are Banach spaces, because they are complete with respect to
the associated norm, and under the component-wise lattice operations, they are Banach
lattices. In particular, the 1 space is an abstract Lebesgue space under the 1 norm.
The finite-dimensional real vector spaces Rn can be considered as special cases of
the sequence spaces (consisting of vectors in which all but n components are zero) and,
because they are finite-dimensional, we can use any of the p norms. Thus, our previous
examples, in which ? mapped terms to vectors in Rn, can be considered as mapping to
abstract Lebesgue spaces if we adopt the 1 norm.
51
Computational Linguistics Volume 38, Number 1
2.3 Degrees of Entailment
We propose that in vector-based semantics, a degree of entailment is more appropriate
than a black-and-white observation of whether or not entailment holds. If we think of
the vectors as describing ?degrees of meaning,? it makes sense that we should then look
for degrees of entailment.
Conditional probability is closely connected to entailment: If A entails B, then
P(B|A) = 1. Moreover, if A and B are mutually exclusive, then P(A|B) = P(B|A) = 0. It
is thus natural to think of conditional probability as a degree of entailment.
An abstract Lebesgue space has many of the properties of a probability space, where
the set operations of a probability space are replaced by the lattice operations of the
vector space. This means that we can think of an abstract Lebesgue space as a vector-
based probability space. Here, events correspond to positive elements with the norm
less than or equal to 1; the probability of an event u is given by the norm (which we shall
always assume is the 1 norm), and the joint probability of two events u and v is ?u ? v?1.
Definition 6 (Degree of Entailment)
We define the degree to which u entails v in the form of a conditional probability:
Ent(u, v) =
?u ? v?1
?u?1
(20)
If we are only interested in degrees of entailment (i.e., conditional probabilities) and
not probabilities, then we can drop the requirement that the norm should be less than
or equal to one, because conditional probabilities are automatically normalized. This
definition, together with the multiplication of the algebra, allows us to compute the
degree of entailment between any two strings according to the context theory.
Example 8
The vectors given in Table 1 give the following calculation for the degree to which cat
entails animal:
?(cat) = (0, 2, 3) (21)
?(animal) = (2, 1, 2) (22)
?(cat) ? ?(animal) = (0, 1, 2) (23)
Ent(?(cat),?(animal)) = ??(cat) ? ?(animal)?1/??(cat)?1 = 3/5 (24)
An important question is how this context-theoretic definition of the degree of
entailment relates to more familiar notions of entailment. There are three main ways
in which the term entailment is used:
 the model-theoretic sense of entailment in which a theory A entails a
theory B if every model of A is also a model of B. It was shown in Clarke
(2007) that this type of entailment can be described using context theories,
where sentences are represented as projections on a vector space.
 entailment between terms (as expressed for example in the WordNet
hierarchy), for example the hypernymy relation between the terms cat and
animal encodes the fact that a cat is an animal. In Clarke (2007) we showed
52
Clarke A Context-Theoretic Framework for Distributional Semantics
that such relations can be encoded in the partial order structure of a vector
lattice.
 Human common-sense judgments as to whether one sentence entails or
implies another sentence, as used in the Recognising Textual Entailment
Challenges (Dagan, Glickman, and Magnini 2005).
Our context-theoretic notion of entailment is thus intended to generalize both the first
two senses of entailment given here. In addition, we hope that context theories will be
useful in the practical application of recognizing textual entailment. Capturing this type
of entailment is not our initial aim because we are interested in foundational issues,
and doing well at this task poses major engineering challenges beyond the scope of
our work. Nevertheless, we believe the ability to represent the preceding two types of
entailment as well as standard distributional methods of composition bodes well for the
possibility of using our framework for this task. In Section 4 we describe several basic
approaches to textual entailment within the framework.
Our definition is more general than the model-theoretic and hypernymy notions of
entailment, however, as it allows the measurement of a degree of entailment between
any two strings: As an extreme example, one may measure the degree to which not a
entails in the. Although this may not be useful or philosophically meaningful, we view
it as a practical consequence of the fact that every string has a vector representation in
our model, which coincides with the current practice in vector-based compositionality
techniques (Clark, Coecke, and Sadrzadeh 2008; Widdows 2008).
2.4 Lattice Ordered Algebras
A lattice ordered algebra merges the lattice ordering of the vector space V with the
product of A. This structure encapsulates the ordering properties that are familiar from
multiplication in matrices and elementary arithmetic. For this reason, many proposed
methods of composing vector-based representations of meaning can be viewed as lattice
ordered algebras. The only reason we have not included it as a requirement of the
framework is because our motivating example (described in the next section) is not
guaranteed to have this property, although it does give us a partially ordered algebra.
Definition 7 (Partially Ordered Algebra)
A partially ordered algebra A is an algebra which is also a partially ordered vector
space, which satisfies u ? v ? 0 for all u, v ? A+. If the partial ordering is a lattice, then
A is called a lattice-ordered algebra.
Example 9 (Lattice-Ordered Algebra of Matrices)
The matrices of order n form a lattice-ordered algebra under normal matrix multi-
plication, where the lattice operations are defined as the entry-wise minimum and
maximum.
Example 10 (Operators on p Spaces)
Matrices can be viewed as operators on finite-dimensional vector spaces; in fact this lat-
tice property extends to operators on certain infinite-dimensional spaces, the p spaces,
53
Computational Linguistics Volume 38, Number 1
by the Riesz-Kantorovich theorem (Abramovich and Aliprantis 2002). The operations
are defined by:
(S ? T)(u) = sup{S(v)+ T(w) : v,w ? U+ and v+ w = u} (25)
(S ? T)(u) = inf{S(v)+ T(w) : v,w ? U+ and v+ w = u} (26)
If A is a lattice-ordered algebra which is also an abstract Lebesgue space, then
?A,A,?,A, 1? is a context theory. In this simplified situation, A plays the role of the
vector lattice as well as the algebra; ? maps from A to A as before, and 1 indicates the
identity map on A. Many of the examples we discuss will be of this form, so we will
use the shorthand notation, ?A,A,??. It is tempting to adopt this as the definition of
context theory; as we will see in the next section, however, this is not supported by our
prototypical example of a context theory as in this case the algebra is not necessarily
lattice-ordered.
3. Context Algebras
In this section we describe the prototypical examples of a context theory, the context
algebras. The definition of a context algebra originates in the idea that the notion of
?meaning as context? can be extended beyond the word level to strings of arbitrary
length. In fact, the notion of context algebra can be thought of as a generalization of the
syntactic monoid of a formal language: Instead of a set of strings defining the language,
we have a fuzzy set of strings, or more generally, a real-valued function on a free monoid.
We call such functions real-valued languages and they take the place of formal
languages in our theory. We attach a real number to each string which is intended as an
indication of its importance or likelihood of being observed; for example, those with a
value of zero are considered not to occur.
Definition 8 (Real-Valued Language)
Let A be a finite set of symbols. A real-valued language (or simply a language when
there is no ambiguity) L on A is a function from A? to R. If the range of L is a subset of
R+ then L is called a positive language. If the range of L is a subset of [0, 1] then L is
called a fuzzy language. If L is a positive language such that
?
x?A? L(x) = 1 then L is a
probability distribution over A?, a distributional language.
One possible interpretation for L when it is a distributional language is that L(x) is the
probability of observing the string x when selecting a document at random from an
infinite collection of documents.
The following inclusion relation applies among these classes of language:
distributional =? fuzzy =? positive =? real-valued (27)
Because A? is a countable set, the set RA
?
of functions from A? to R is isomorphic
to the sequence space, and we shall treat them equivalently. We denote by p(A?) the
set of functions with a finite p norm when considered as sequences. There is another
hierarchy of spaces given by the inclusion of the p spaces: p(A?) ? q(A?) if p ? q. In
particular,
1(A?) ? 2(A?) ? ?(A?) ? RA
?
(28)
54
Clarke A Context-Theoretic Framework for Distributional Semantics
where the ? norm gives the maximum value of the function and ?(A?) is the space of
all bounded real-valued functions on A?. Recall that a linear operator T from one vector
spaceU to anotherV is called bounded if there exists some ? > 0 such that ?Tu? ? ??u?
for all u ? U, where the norm on the left hand side is the norm inV, and that on the right
hand side is in U.
Note that probability distributions are in 1(A?) and fuzzy languages are in ?(A?).
If L ? 1(A?)+ (the space of positive functions on A? such that the sum of all values
of the function is finite) then we can define a probability distribution pL over A
? by
pL(x) = L(x)/?L?1. Similarly, if L ? ?(A?)+ (the space of bounded positive functions
on A?) then we can define a fuzzy language fL by fL(x) = L(x)/?L??.
Example 11
Given a finite set of strings C ? A?, which we may imagine to be a corpus of documents,
define L(x) = 1/|C| if x ? C, or 0 otherwise. Then L is a probability distribution over A?.
In general, we think of a real-valued language as an abstraction of a corpus; in par-
ticular, we think of a corpus as a finite sample of a distributional language representing
all possible documents that could ever be written.
Example 12
Let L be a language such that L(x) = 0 for all but a finite subset of A?. Then L ? p(A?)
for all p.
Example 13
Let L be the language defined by L(x) = |x| where x is the length of (i.e., number of
symbols in) string x. Then L is a positive language which is not bounded: For any string
y there exists a z such that L(z) > L(y), for example z = ay for a ? A.
Example 14
Let L be the language defined by L(x) = 1/2 for all x. Then L is a fuzzy language but
L /? 1(A?)
We will assume now that L is fixed, and consider the properties of contexts of strings
with respect to this language. As in a syntactic monoid, we consider the context to be
everything surrounding the string, although in this case instead of a set of pairs of
strings we have a function from pairs of strings to the real numbers. We emphasize
the vector nature of these real-valued functions by calling them ?context vectors.? Our
thesis is centered around these vectors, and it is their properties that form the inspiration
for the context-theoretic framework.
Definition 9 (Context Vectors)
Let L be a language on A. For x ? A?, we define the context of x as a vector x? ? RA
??A? :
x?(y, z) = L(yxz) (29)
In other words, x? is a function from pairs of strings to the real numbers, and the value
of x?(y, z) is the value of x in the context (y, z), which is L(yxz).
The question we are addressing is: Does there exist some algebra A containing
the context vectors of strings in A? such that x? ? y? = ?xy where x, y ? A? and ? indicates
multiplication in the algebra? As a first try, consider the vector space L?(A? ? A?) in
which the context vectors live. Is it possible to define multiplication on the whole vector
space such that the condition just specified holds?
55
Computational Linguistics Volume 38, Number 1
Example 15
Consider the language C on the alphabet A = {a, b, c, d, e, f} defined by C(abcd) =
C(aecd) = C(abfd) = 13 and C(x) = 0 for all other x ? A
?. Now if we take the shorthand
notation of writing the basis vector in L?(A? ? A?) corresponding to a pair of strings
as the pair of strings itself then
b? = 13 (a, cd)+
1
3 (a, fd) (30)
c? = 13 (ab, d)+
1
3 (ae, d) (31)
?bc = 13 (a, d) (32)
It would thus seem sensible to define multiplication of contexts so that 13 (a, cd) ?
1
3 (ab, d) =
1
3 (a, d). However we then find
e? ? f? = 13 (a, cd) ?
1
3 (ab, d) =
?ef = 0 (33)
showing that this definition of multiplication doesn?t provide us with what we are
looking for. In fact, if there did exist a way to define multiplication on contexts in
a satisfactory manner it would necessarily be far from intuitive, as, in this example,
we would have to define (a, cd) ? (ab, d) = 0 meaning the product b? ? c? would have to
have a non-zero component derived from the products of context vectors (a, fd) and
(ae, d) which don?t relate at all to the contexts of bc. This leads us to instead define
multiplication on a subspace of L?(A? ? A?).
Definition 10 (Generated Subspace A)
The subspace A of L?(A? ? A?) is the set defined by
A = {a : a =
?
x?A?
?xx? for some ?x ? R} (34)
In other words, it is the space of all vectors formed from linear combinations of context
vectors.
Because of the way we define the subspace, there will always exist some basis B =
{u? : u ? B} where B ? A?, and we can define multiplication on this basis by u? ? v? =?uv
where u, v ? B. Defining multiplication on this basis defines it for the whole vector
subspace, because we define multiplication to be linear, making A an algebra.
There are potentially many different bases we could choose, however, each corre-
sponding to a different subset of A?, and each giving rise to a different definition of
multiplication. Remarkably, this isn?t a problem.
Proposition 1 (Context Algebra)
Multiplication on A is the same irrespective of the choice of basis B.
Proof
We say B ? A? defines a basis B for A when B is a basis such that B = {x? : x ? B}.
Assume there are two sets B1,B2 ? A? that define corresponding bases B1 and B2 for
A. We will show that multiplication in basis B1 is the same as in the basis B2.
56
Clarke A Context-Theoretic Framework for Distributional Semantics
We represent two basis elements u?1 and u?2 of B1 in terms of basis elements of B2:
u?1 =
?
i
?iv?i and u?2 =
?
j
?jv?j (35)
for some ui ? B1, vj ? B2 and ?i,?j ? R. First consider multiplication in the basis B1.
Note that u?1 =
?
i ?iv?i means that L(xu1y) =
?
i ?iL(xviy) for all x, y ? A
?. This includes
the special case where y = u2y
? so
L(xu1u2y
?) =
?
i
?iL(xviu2y
?) (36)
for all x, y? ? A?. Similarly, we have L(xu2y) =
?
j ?jL(xvjy) for all x, y ? A
? which in-
cludes the special case x = x?vi, so L(x
?viu2y) =
?
j ?jL(x
?vivjy) for all x
?, y ? A?. Insert-
ing this into Equation (36) yields
L(xu1u2y) =
?
i,j
?i?jL(xvivjy) (37)
for all x, y ? A? which we can rewrite as
u?1 ? u?2 = ?u1u2 =
?
i,j
?i?j(v?i ? v?j) =
?
i,j
?i?j?vivj (38)
Conversely, the product of u1 and u2 using the basis B2 is
u?1 ? u?2 =
?
i
?iv?i ?
?
j
?jv?j =
?
i,j
?i?j(v?i ? v?j) (39)
thus showing that multiplication is defined independently of what we choose as the
basis. 
Example 16
Returning to the previous example, we can see that in this case multiplication is in fact
defined on L?(A? ? A?) because we can describe each basis vector in terms of context
vectors:
(a, fd) ? (ae, d) = 3(b?? e?) ? 3(c?? f? ) = ?3(a, d) (40)
(a, cd) ? (ae, d) = 3e? ? 3(c?? f? ) = 3(a, d) (41)
(a, fd) ? (ab, d) = 3(b?? e?) ? 3f? = 3(a, d) (42)
(a, cd) ? (ab, d) = 3e? ? 3f? = 0, (43)
thus confirming what we predicted about the product of b? and c?: The value is only
correct because of the negative correction from (a, fd) ? (ae, d). This example also serves
to demonstrate an important property of context algebras: They do not satisfy the
positivity condition; it is possible for positive vectors (those with all components
greater than or equal to zero) to have a non-positive product. This means they are not
necessarily partially ordered algebras under the normal partial order. Compare this
to the case of matrix multiplication, for example, where the product of two positive
matrices is always positive.
57
Computational Linguistics Volume 38, Number 1
The notion of a context theory is founded on the prototypical example given by
context vectors. So far we have shown that multiplication can be defined on the vector
space A generated by context vectors of strings; we have not discussed the lattice
properties of the vector space, however. In fact, A does not come with a natural lattice
ordering that makes sense for our purposes, although the original space RA
??A? does?
it is isomorphic to the sequence space. Thus ?A,A,?,RA
??A? ,?? will form our context
theory, where ?(a) = a? for a ? A and ? is the canonical map that simply maps elements
of A to themselves, but are considered as elements of RA
??A? . There is an important
caveat here, however: We required that the vector lattice be an abstract Lebesgue space,
which means we need to be able to define a norm on it. The 1 norm on RA
??A? is an
obvious candidate, although it is not guaranteed to be finite. This is where the nature of
the underlying language L becomes important.
We might hope that the most restrictive class of the languages we discussed, the
distributional languages, would guarantee that the norm is finite. Unfortunately, this is
not the case, as the following example demonstrates.
Example 17
Let L be the language defined by
L(a2
n
) = 1/2n+1 (44)
for integer n ? 0, and zero otherwise, where by an we mean n repetitions of a, so
for example, L(a) = 12 , L(aa) =
1
4 , L(aaa) = 0, and L(aaaa) =
1
8 . Then L is a probability
distribution over A?, because L is positive and ?L?1 = 1. However, ?a??1 is infinite,
because each string x for which L(x) > 0 contributes 1/2 to the value of the norm, and
there are an infinite number of such strings.
The problem in the previous example is that the average string length is infinite. If
we restrict ourselves to distributional languages in which the average string length is
finite, then the problem goes away.
Proposition 2
Let L be a probability distribution over A? such that
L? =
?
x?A?
L(x)|x| (45)
is finite, where |x| is the number of symbols in string x; we will call such languages finite
average length. Then ?y??1 is finite for each y ? A?.
Proof
Denote the number of occurrences of string y as a substring of string x by |x|y. Clearly
|x|y ? |x| for all x, y ? A?. Moreover,
?y??1 =
?
x?A?
L(x)|x|y ?
?
x?A?
L(x)|x| (46)
and so ?y??1 ? L? is finite for all y ? A?. 
If L is finite average length, then A ? 1(A? ? A?), and so ?A,A,?, 1(A? ? A?),??
is a context theory, where ? is the canonical map from A to 1(A? ? A?). Thus context
58
Clarke A Context-Theoretic Framework for Distributional Semantics
algebras of finite average length languages provide our prototypical examples of
context theories.
3.1 Discussion
The benefit of the context-theoretic framework is in providing a space of exploration for
models of meaning in language. Our effort has been in finding principles by which to
define the boundaries of this space. Each of the key boundaries, namely, bilinearity and
associativity of multiplication and entailment through vector lattice structure, can also
be viewed as limitations of the model.
Bilinearity is a strong requirement to place, and has wide-ranging implications for
the way meaning is represented in the model. It can be interpreted loosely as follows:
Components of meaning persist or diminish but do not spontaneously appear. This is
particularly counterintuitive in the case of idiom and metaphor in language. It means
that, for example, both red and herring must contain some components relating to the
meaning of red herring which only come into play when these two words are combined
in this particular order. Any other combination would give a zero product for these
components. It is easy to see how this requirement arises from a context-theoretic
perspective, nevertheless from a linguistic perspective it is arguably undesirable.
One potential limitation of the model is that it does not explicitly model syntax, but
rather syntactic restrictions are encoded into the vector space and product itself. For
example, we may assume the word square has some component of meaning in common
with the word shape. Then we would expect this component to be preserved in the
sentencesHe drew a square andHe drew a shape. However, in the case of the two sentences
The box is square and *The box is shape we would expect the second to be represented by
the zero vector because it is not grammatical; square can be a noun and an adjective,
whereas shape cannot. Distributivity of meaning means that the component of meaning
that square has in common with shape must be disjoint with the adjectival component of
the meaning of square.
Associativity is also a very strong requirement to place; indeed Lambek (1961)
introduced non-associativity into his calculus precisely to deal with examples that were
not satisfactorily dealt with by his associative model (Lambek 1958).
Our framework provides answers to someone considering the use of algebra for
natural language semantics. What field should be used? The real numbers. Need the
algebra be finite-dimensional? No. Should the algebra by unital? Yes. Some of these
answers impose restrictions on what is possible within the framework. The full impli-
cation of these restrictions for linguistics is beyond the scope of this article, and indeed
is not yet known.
Although we hope that these features or boundaries are useful in their current form,
it may be that with time, or for certain applications, there is a reason to expand or
contract certain of them, perhaps because of theoretical discoveries relating to the model
of meaning as context, or for practical or linguistic reasons, if, for example, the model is
found to be too restrictive to model certain linguistic phenomena.
4. Applications to Textual Entailment
In this section we analyze approaches to the problem of recognizing textual entailment,
showing how they can be related to the context-theoretic framework, and discussing
potential new approaches that are suggested by looking at them within the framework.
We first discuss some simple approaches to textual entailment based on subsequence
59
Computational Linguistics Volume 38, Number 1
matching and measuring lexical overlap. We then look at the approach of Glickman
and Dagan (2005), showing that it can be considered as a context theory in which words
are represented as projections on the vector space of documents. This leads us to an
implementation of our own in which we used latent Dirichlet alocation as an alternative
approach to overcoming the problem of data sparseness.
A fair amount of effort is required to describe these approaches within our frame-
work. Although there is no immediate practical benefit to be gained from this, our
main purpose in doing this is to demonstrate the generality of the framework. We also
hope that insight into these approaches may be gleaned by viewing them from a new
perspective.
4.1 Subsequence Matching and Lexical Overlap
A sequence x ? A? is a subsequence of y ? A? if each element of x occurs in y in the
same order, but with the possibility of other elements occurring in between, so for
example abba is a subsequence of acabcba in {a, b, c}?. Subsequence matching compares
the subsequences of two sequences: The more subsequences they have in common
the more similar they are assumed to be. This idea has been used successfully in text
classification (Lodhi et al 2002) and also formed the basis of the author?s entry to the
second Recognising Textual Entailment Challenge (Clarke 2006).
If S is a semigroup, 1(S) is a lattice-ordered algebra under the multiplication of
convolution:
( f ? g)(x) =
?
yz=x
f (y)g(z) (47)
where x, y, z ? S, f, g ? 1(S).
Example 18 (Subsequence Matching)
Consider the algebra 1(A?) for some alphabet A. This has a basis consisting of elements
ex for x ? A?, where ex the function that is 1 on x and 0 elsewhere. In particular e is
a unity for the algebra. Define ?(a) = 12 (ea + e); then ?A, 
1(A?),?? is a context theory.
Under this context theory, a sequence x completely entails y if and only if it is a sub-
sequence of y. In our experiments, we have shown that this type of context theory can
perform significantly better than straightforward lexical overlap (Clarke 2006). Many
variations on this idea are possible: for example, using more complex mappings from
A? to 1(A?).
Example 19 (Lexical Overlap)
The simplest approach to textual entailment is to measure the degree of lexical over-
lap: the proportion of words in the hypothesis sentence that are contained in the text
sentence (Dagan, Glickman, and Magnini 2005). This approach can be described as
a context theory in terms of a free commutative semigroup on a set A, defined by
A?/ ? where x ? y in A? if the symbols making up x can be reordered to make y. Then
define ?? by ??(a) = 12 (e[a] + e[] ) where [a] is the equivalence class of a in A
?/ ?. Then
?A, 1(A?/ ?),??? is a context theory in which entailment is defined by lexical overlap.
More complex definitions of x? can be used, for example, to weight different words by
their probabilities.
60
Clarke A Context-Theoretic Framework for Distributional Semantics
4.2 Document Projections
Glickman and Dagan (2005) give a probabilistic definition of entailment in terms of
?possible worlds? which they use to justify their lexical entailment model based on oc-
currences of words in Web documents. They estimate the lexical entailment probability
LEP(u, v) to be
LEP(u, v) 
nu,v
nv
(48)
where nv and nu,v denote the number of documents in which the word v occurs and in
which the words u and v both occur, respectively. From the context-theoretic perspec-
tive, we view the set of documents in which the word occurs as its context vector. To
describe this situation in terms of a context theory, consider the vector space ?(D)
where D is the set of documents. With each word u in some set A we associate an
operator Pu on this vector space by
Pued =
{
ed if u occurs in document d
0 otherwise.
(49)
where ed is the basis element associated with document d ? D. Pu is a projection, that is,
PuPu = Pu; it projects onto the space of documents that u occurs in. These projections are
clearly commutative (they are in fact band projections): PuPv = PvPu = Pu ? Pv projects
onto the space of documents in which both u and v occur.
In their paper, Glickman and Dagan (2005) assume that probabilities can be attached
to individual words, as we do, although they interpret these as the probability that a
word is ?true? in a possible world. In their interpretation, a document corresponds to a
possible world, and a word is true in that world if it occurs in the document.
They do not, however, determine these probabilities directly; instead they make
assumptions about how the entailment probability of a sentence depends on lexical
entailment probability. Although they do not state this, the reason for this is presumably
data sparseness: They assume that a sentence is true if all its lexical components are true;
this will only happen if all the words occur in the same document. For any sizeable
sentence this is extremely unlikely, hence their alternative approach.
It is nevertheless useful to consider this idea from a context-theoretic perspective.
We define a context theory ?A,B(?(D)),?, ?(D), p??, where:
 We denote by B(U) the set of bounded operators on the vector space U;
in this case we are considering the bounded operators on the vector space
indexed by the set of documents D. Because D is finite, all operators on
this space are in fact bounded; this property will be needed when we
generalize D to an infinite set, however.
 ? : A ? B(?(D)) is defined by ?(u) = Pu; it maps words to document
projections.
 p? : B(?(D)) ? ?(D) is a map defined by p?(T) = Tp, where
 p ? ?(D) is defined by p(d) = 1/|D| for all d ? D. This is defined such
that ?Pup?1 is the probability of the term u.
61
Computational Linguistics Volume 38, Number 1
The degree to which x entails y is then given by ?Pxp ? Pyp?1/?Px?1 =
?PxPyp?1/?Px?1. This corresponds directly to Glickman and Dagan?s (2005) entailment
?confidence?; it is simply the proportion of documents that contain all the terms of x
which also contain all the terms of y.
4.3 Latent Dirichlet Projections
The formulation in the previous section suggests an alternative approach to that of
Glickman and Dagan (2005) to cope with the data sparseness problem. We consider the
finite data available D as a sample from a distributional language D?; the vector p then
becomes a probability distribution over the documents in D?. In our own experiments,
we used latent Dirichlet alocation (Blei, Ng, and Jordan 2003) to build a model of the
corpus as a probabilistic language based on a subset of around 380,000 documents from
the Gigaword corpus. Having this model allows us to consider an infinite array of
possible documents, and thus we can use our context-theoretic definition of entailment
because there is no problem of data sparseness.
Latent Dirichlet alocation (LDA) follows the same vein as latent semantic analy-
sis (LSA; Deerwester et al 1990) and probabilistic latent semantic analysis (PLSA;
Hofmann 1999) in that it can be used to build models of corpora in which words within
a document are considered to be exchangeable, so that a document is treated as a bag
of words. LSA performs a singular value decomposition on the matrix of words and
documents which brings out hidden ?latent? similarities in meaning between words,
even though they may not occur together.
In contrast, PLSA and LDA provide probabilistic models of corpora using Bayesian
methods. LDA differs from PLSA in that, whereas the latter assumes a fixed number
of documents, LDA assumes that the data at hand are a sample from an infinite set of
documents, allowing new documents to be assigned probabilities in a straightforward
manner.
Figure 3 shows a graphical representation of the latent Dirichlet alocation genera-
tive model, and Figure 4 shows how the model generates a document of length N. In
this model, the probability of occurrence of a word w in a document is considered to be
a multinomial variable conditioned on a k-dimensional ?topic? variable z. The number
of topics k is generally chosen to be much fewer than the number of possible words,
so that topics provide a ?bottleneck? through which the latent similarity in meaning
between words becomes exposed.
The topic variable is assumed to follow a multinomial distribution parameterized
by a k-dimensional variable ?, satisfying
k
?
i=1
?i = 1 (50)
and which is in turn assumed to follow a Dirichlet distribution. The Dirichlet distribu-
tion is itself parameterized by a k-dimensional vector ?. The components of this vector
can be viewed as determining the marginal probabilities of topics, because
p(zi) =
?
p(zi|?)p(?)d? (51)
=
?
?ip(?)d? (52)
62
Clarke A Context-Theoretic Framework for Distributional Semantics
Figure 3
Graphical representation of the Dirichlet model. The inner box shows the choices that are
repeated for each word in the document; the outer box shows the choice that is made for each
document; the parameters outside the boxes are constant for the model.
This is just the expected value of ?i, which is given by
p(zi) =
?i
?
j ?j
(53)
The model is thus entirely specified by ? and the conditional probabilities p(w|z)
that we can assume are specified in a k? V matrix ? where V is the number of words in
the vocabulary. The parameters ? and ? can be estimated from a corpus of documents
by a variational expectation maximization algorithm, as described by Blei, Ng, and
Jordan (2003).
LDA was applied by Blei, Ng, and Jordan (2003) to the tasks of document model-
ing, document classification, and collaborative filtering. They compare LDA to several
techniques including PLSA; LDA outperforms these on all of the applications. LDA has
been applied to the task of word sense disambiguation (Boyd-Graber, Blei, and Zhu
2007; Cai, Lee, and Teh 2007) with significant success.
Consider the vector space ?(A?) for some alphabet A, the space of all bounded
functions on possible documents. In this approach, we define the representation of a
Figure 4
Generative process assumed in the Dirichlet model.
63
Computational Linguistics Volume 38, Number 1
string x to be a projection Px on the subspace representing the (infinite) set of documents
in which all the words in string x occur. We define a vector q(x) for x ? A? where q(x) is
the probability of string x in the probabilistic language.
Our context theory is then given by ?A,B(?(A?)),?, ?(A?), q??, where ? is defined
as before and q? is defined as p? earlier. In this case, we are considering an infinite set
of possible documents, A?, so the boundedness property becomes important. ?Pxq?1
is thus the probability that a document chosen at random contains all the words that
occur in string x. In order to estimate this value we have to integrate over the Dirichlet
parameter ?:
?Pxq?1 =
?
?
(
?
a?x
p?(a)
)
p(?)d? (54)
where by a ? x we mean that the word a occurs in string x, and p?(a) is the probability
of observing word a in a document generated by the parameter ?. We estimate this by
p?(a)  1 ?
(
1 ?
?
z
p(a|z)p(z|?)
)N
(55)
where we have assumed a fixed document length N. This formula is an estimate of the
probability of a word occurring at least once in a document of length N, and the sum
over the topic variable z is the probability that the word a occurs at any one point in
a document given the parameter ?. We approximated the integral using Monte Carlo
sampling to generate values of ? according to the Dirichlet distribution.
We built a latent Dirichlet alocation model using Blei, Ng, and Jordan?s (2003)
implementation on documents from the British National Corpus, using 100 topics. We
evaluated this model on the 800 entailment pairs from the first Recognizing Textual
Entailment Challenge test set.1 Results were comparable to those obtained by Glickman
and Dagan (2005) (see Table 2). In this table, Accuracy is the accuracy on the test set,
consisting of 800 entailment pairs, and CWS is the confidence weighted score; see
Dagan, Glickman, and Magnini (2005) for the definition. The differences between the
accuracy values in the table are not statistically significant because of the small data
set, although all accuracies in the table are significantly better than chance at the 1%
level. The accuracy of the model is considerably lower than the state of the art, which
is around 75% (Bar-Haim et al 2006). We experimented with various document lengths
and found very long documents (N = 106 and N = 107) to work best.
It is important to note that because the LDA model is commutative, the resulting
context algebra must also be commutative, which is clearly far from ideal in modeling
natural language.
5. The Model of Clark, Coecke, and Sadrzadeh
One of the most sophisticated proposals for a method of composition is that of Clark,
Coecke, and Sadrzadeh (2008) and the more recent implementation of Grefenstette et al
1 We have so far only used data from the first challenge, because we performed the experiment before the
other challenges had taken place.
64
Clarke A Context-Theoretic Framework for Distributional Semantics
Table 2
Results obtained with our latent Dirichlet projection model on the data from the first
Recognizing Textual Entailment Challenge for two document lengths N = 106 and N = 107
using a cut-off for the degree of entailment of 0.5 at which entailment was regarded as holding.
Model Accuracy CWS
Dirichlet (106) 0.584 0.630
Dirichlet (107) 0.576 0.642
Bayer (MITRE) 0.586 0.617
Glickman (Bar Ilan) 0.586 0.572
Jijkoun (Amsterdam) 0.552 0.559
Newman (Dublin) 0.565 0.6
(2011). In this section, we will show how their model can be described as a context
theory.
The authors describe the syntactic element of their construction using pregroups
(Lambek 2001), a formalism which simplifies the syntactic calculus of Lambek (1958).
These can be described in terms of partially orderedmonoids, a monoidGwith a partial
ordering ? satisfying x ? y implies xz ? yz and zx ? zy for all x, y, z ? G.
Definition 11 (Pregroup)
Let G be a partially ordered monoid. Then G is called a pregroup if for each x ? G there
are elements xl and xr in G such that
xlx ? 1 (56)
xxr ? 1 (57)
1 ? xxl (58)
1 ? xrx (59)
If x, y ? G, we call y a reduction of x if y can be obtained from x using only Rules (56)
and (57).
Pregroup grammars are defined by freely generating a pregroup on a set of basic
grammatical types. Words are then represented as elements formed from these basic
types, for example:
John likes Mary
? ?rsol o
(60)
where ?, s, and o are the basic types for first person singular, statement, and object,
respectively. It is easy to see that this sentence reduces to type s under the pregroup
reductions.
As Clark, Coecke, and Sadrzadeh (2008) note, their construction can be generalized
by endowing the grammatical type of a word with a vector nature, in addition to its
semantics. We use this slightly more general construction to allow us to formulate it
in the context-theoretic framework. We define an elementary meaning space to be the
tensor product space V = S? P where S is a vector space representing meanings of
words and P is a vector space with an orthonormal basis corresponding to the basic
65
Computational Linguistics Volume 38, Number 1
grammatical types in a pregroup grammar and their adjoints. We assume that meanings
of words live in the tensor algebra space T(V), defined by
T(V) = R? V ? (V ? V) ? (V ? V ? V) ? ? ? ?
For an element v in a particular tensor power of V, such that v = (s1 ? p1) ? (s2 ? p2) ?
? ? ? ? (sn ? pn), where the pi are basis vectors of P, then we can recover a complex gram-
matical type for v as the product ?(v) = ?1?2 ? ? ??n, where ?i is the basic grammatical
type corresponding to pi. We will call the vectors such as this which have a single
complex type (i.e., they are not formed from a weighted sum of more than one type)
unambiguous.
We also assume that words are represented by vectors whose grammatical type is
irreducible: There is no pregroup reduction possible on the type. We define ?(T(V)) as
the vector space generated by all such vectors.
We will now define a product ? on ?(T(V)) that will make it an algebra. To do this,
it suffices to define the product between two elements u1,u2 which are unambiguous
and whose grammatical type is basic, so that they can be viewed as elements of V.
The definition of the product on the rest of the space follows from the assumption of
distributivity. We define
u1 ? u2 =
?
?
?
u1 ? u2 if ?(u1 ? u2) is irreducible
?u1,u2? otherwise.
(61)
This product is bilinear, because for a particular pair of basis elements, only one of these
two conditions will apply, and both the tensor and inner products are bilinear functions.
Moreover, it corresponds to composed and reduced word vectors, as defined in Clark,
Coecke, and Sadrzadeh (2008).
To see how this works on our example sentence, we assume we have vectors for
the meanings of the three words, which we write as vword. We assume for the purpose
of this example that the word like is represented as a product state composed of three
vectors, one for each basic grammatical type. This removes any potentially interesting
semantics, but allows us to demonstrate the product in a simple manner. We write this
as follows:
John likes Mary
(vJohn ? e?) ? (vlikes,1 ? e?r ) ? (vlikes,2 ? es) ? (vlikes,3 ? eol ) ? (vMary ? eo)
(62)
where e? is the orthonormal basis vector corresponding to basic grammatical type ?.
More interesting representations of like would consist of sums over similar vectors.
Computing this product from left to right:
(vJohn ? e?) ? (vlikes,1 ? e?r )? (vlikes,2 ? es) ? (vlikes,3 ? eol ) ? (vMary ? eo)
= ?vJohn, vlikes,1? (vlikes,2 ? es) ? (vlikes,3 ? eol ) ? (vMary ? eo)
= ?vJohn, vlikes,1? (vlikes,2 ? es) ? (vlikes,3 ? eol ) ? (vMary ? eo)
= ?vJohn, vlikes,1??vlikes,3, vMary? (vlikes,2 ? es)
(63)
66
Clarke A Context-Theoretic Framework for Distributional Semantics
As we would expect in this simplified example the product is a scalar multiple of the
second vector for like, with the type of a statement.
This construction thus allows us to represent complex grammatical types, similar
to Clark, Coecke, and Sadrzadeh (2008), although it also allows us to take weighted
sums of these complex types, giving us a powerful method of expressing syntactic and
semantic ambiguity.
6. Conclusions and Future Work
We have presented a context-theoretic framework for natural language semantics. The
framework is founded on the idea that meaning in natural language can be determined
by context, and is inspired by techniques that make use of statistical properties of
language by analyzing large text corpora. Such techniques can generally be viewed as
representing language in terms of vectors. These techniques are currently used in appli-
cations such as textual entailment recognition, although the lack of a theory of meaning
that incorporates these techniques means that they are often used in a somewhat ad hoc
manner. The purpose behind the framework is to provide a unified theoretical founda-
tion for such techniques so that they may be used in a principled manner.
By formalizing the notion of ?meaning as context? we have been able to build a
mathematical model that informs us about the nature of meaning under this paradigm.
Specifically, it gives us a theory about how to represent words and phrases using
vectors, and tells us that the product of two meanings should be distributive and
associative. It also gives us an interpretation of the inherent lattice structure on these
vector spaces as defining the relation of entailment. It tells us how to measure the size
of the vector representation of a string in such a way that the size corresponds to the
probability of the string.
We have demonstrated that the framework encompasses several related approaches
to compositional distributional semantics, including those based on a predefined
composition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch,
and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky
1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolph
and Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, and
Sadrzadeh (2008).
6.1 Practical Investigations
Section 4 raises many possibilities for the design of systems to recognize textual entail-
ment within the framework.
 Variations on substring matching: experiments with different weighting
schemes for substrings, allowing partial commutativity of words or
phrases, and replacing words with vectors representing their context,
using tensor products of these vectors instead of concatenation.
 Extensions of Glickman and Dagan?s approach and our own
context-theoretic approach using LDA, perhaps using other distributional
languages based on n-grams or other models in which words do not
commute, or a combination of context theories based on commutative
and non-commutative models.
67
Computational Linguistics Volume 38, Number 1
 The LDA model we used is a commutative one. This is a considerable
simplification of what is possible within the context-theoretic framework;
it would be interesting to investigate methods of incorporating
non-commutativity into the model.
 Implementations based on the approach to representing uncertainty in
logical semantics similar to those described in Clarke (2007).
All of these ideas could be evaluated using the data sets from the Recognising Textual
Entailment Challenges.
There are many approaches to textual entailment that we have not considered here;
we conjecture that variations of many of them could be described within our frame-
work. We leave the task of investigating the relationship between these approaches and
our framework to further work.
Other areas that we are investigating, together with researchers at the University
of Sussex, is the possibility of learning finite-dimensional algebras directly from corpus
data, along the lines of Guevara (2011) and Baroni and Zamparelli (2010).
One question we have not addressed in this article is the feasibility of computing
with algebraic representations. Although this question is highly dependent on the
particular context theory chosen, it is possible that general algorithms for computation
within this framework could be found; this is another area that we intend to address in
further work.
6.2 Theoretical Investigations
Although the context-theoretic framework is an abstraction of the model of meaning as
context, it would be good to have a complete understanding of the model and the types
of context theories that it allows. Tying down these properties would allow us to define
algebras that could truly be called ?context theories.?
The context-theoretic framework shares a lot of properties with the study of free
probability (Voiculescu 1997). It would be interesting to investigate whether ideas from
free probability would carry over to context-theoretic semantics.
Although we have related our model to many techniques described in the literature,
we still have to investigate its relationship with other models such as that of Song and
Bruza (2003) and Guevara (2011).
We have not given much consideration here to the issue of multi-word expres-
sions and non-compositionality. What predictions does the context-theoretic framework
make about non-compositionality? Answering this may lead us to new techniques for
recognizing and handling multi-word expressions and non-compositionality.
Of course it is hard to predict the benefits that may result from what we have
presented, because we have given a way of thinking about meaning in natural language
that in many respects is new. This new way of thinking opens the door to the unification
of logic-based and vector-based methods in computational linguistics, and the potential
fruits of this union are many.
Acknowledgments
The ideas presented here have benefitted
enormously from the input and support of
my DPhil supervisor, David Weir, without
whom this work would not exist; Rudi Lutz;
and Stephen Clark, who really grokked this
and made many excellent suggestions for
improvements. I am also grateful for the
advice and encouragement of Bill Keller,
John Carroll, Peter Williams, Mark
W. Hopkins, Peter Lane, Paul Hender, and
Peter Hines. I am indebted to the anonymous
68
Clarke A Context-Theoretic Framework for Distributional Semantics
reviewers; their suggestions have
undoubtedly improved this article beyond
measure; the paragraph on the three uses of
the term entailment was derived directly from
one of their suggestions.
References
Abramovich, Yuri A. and Charalambos D.
Aliprantis. 2002. An Invitation to Operator
Theory. American Mathematical Society,
Providence, RI.
Bar-Haim, Roy, Ido Dagan, Bill Dolan, Lisa
Ferro, Danilo Giampiccolo, Bernardo
Magnini, and Idan Szpektor. 2006.
The second pascal recognising textual
entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?9,
Venice.
Baroni, Marco and Roberto Zamparelli.
2010. Nouns are vectors, adjectives are
matrices: Representing adjective-noun
constructions in semantic space. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2010), pages 1183?1193, East
Stroudsburg PA.
Bellegarda, Jerome R. 2000. Exploiting
latent semantic information in statistical
language modeling. Proceedings of the
IEEE, 88(8):1279?1296.
Blackburn, Patrick and Johan Bos.
2005. Representation and Inference for
Natural Language. CSLI Publications,
Stanford, CA.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet alocation.
Journal of Machine Learning Research,
3:993?1022.
Boyd-Graber, Jordan, David Blei, and
Xiaojin Zhu. 2007. A topic model for
word sense disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033,
Prague.
Cai, Junfu, Wee Sun Lee, and Yee Whye
Teh. 2007. Improving word sense
disambiguation using topic features.
In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 1015?1023, Prague.
Choi, Freddy, Peter Wiemer-Hastings,
and Johanna Moore. 2001. Latent
Semantic Analysis for text segmentation.
In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language
Processing, pages 109?117, Ithaca, NY.
Clark, Stephen, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional
distributional model of meaning.
In Proceedings of the Second Quantum
Interaction Symposium (QI-2008),
pages 133?140, Oxford.
Clark, Stephen and Stephen Pulman. 2007.
Combining symbolic and distributional
models of meaning. In Proceedings
of the AAAI Spring Symposium on
Quantum Interaction, pages 52?55,
Stanford, CA.
Clarke, Daoud. 2006. Meaning as context
and subsequence analysis for textual
entailment. In Proceedings of the Second
PASCAL Recognising Textual Entailment
Challenge, pages 134?139, Venice.
Clarke, Daoud. 2007. Context-theoretic
Semantics for Natural Language:
An Algebraic Framework. Ph.D. thesis,
Department of Informatics, University
of Sussex.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics and Eighth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 16?23,
Madrid.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2005. The PASCAL recognising
textual entailment challenge. In Proceedings
of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8,
Southampton, UK.
Deerwester, Scott, Susan Dumais,
George Furnas, Thomas Landauer, and
Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society for Information Science,
41(6):391?407.
Erk, Katrin and Sebastian Pado?. 2009.
Paraphrase assessment in structured
vector space: Exploring parameters and
datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language
Semantics, pages 57?65, Athens.
Firth, John R. 1968. A synopsis of linguistic
theory, 1930?1955. In John R. Firth, editor,
Selected Papers of JR Firth, 1952?59.
Indiana University Press, Bloomington,
pages 168?205.
Foltz, Peter W., Walter Kintsch, and
Thomas K. Landauer. 1998. The
measurement of textual coherence
69
Computational Linguistics Volume 38, Number 1
with latent semantic analysis. Discourse
Process, 15:285?307.
Geffet, M. and I. Dagan. 2005. The
distributional inclusion hypotheses
and lexical entailment. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 107?114,
Ann Arbor, MI.
Glickman, O. and I. Dagan. 2005.
A probabilistic setting and lexical
cooccurrence model for textual
entailment. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 43?48,
Ann Arbor, MI.
Grefenstette, Edward, Mehrnoosh
Sadrzadeh, Stephen Clark,
Bob Coecke, and Stephen Pulman.
2011. Concrete sentence spaces for
compositional distributional models
of meaning. Proceedings of the 9th
International Conference on Computational
Semantics (IWCS 2011), pages 125?134,
Oxford.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht,
Netherlands.
Guevara, Emiliano. 2011. Computing
semantic compositionality in distributional
semantics. In Proceedings of the 9th
International Conference on Computational
Semantics (IWCS 2011), pages 135?144,
Oxford.
Halmos, Paul. 1974. Finite Dimensional Vector
Spaces. Springer, Berlin.
Harris, Zellig. 1968. Mathematical Structures
of Language. Wiley, New York.
Hofmann, Thomas. 1999. Probabilistic
latent semantic analysis. In Proceedings of
the 15th Conference on Uncertainty in AI,
pages 289?296, Stockholm.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic: Introduction to
Model-theoretic Semantics of Natural
Language, Formal Logic and Discourse
Representation Theory, volume 42 of
Studies in Linguistics and Philosophy.
Kluwer, Dordrecht.
Kintsch, Walter. 2001. Predication. Cognitive
Science, 25:173?202.
Lambek, Joachim. 1958. The mathematics of
sentence structure. American Mathematical
Monthly, 65:154?169.
Lambek, Joachim. 1961. On the calculus of
syntactic types. In Roman Jakobson, editor,
Structure of Language and Its Mathematical
Aspects, pages 166?178, American
Mathematical Society, Providence, RI.
Lambek, Joachim. 2001. Type grammars as
pregroups. Grammars, 4(1):21?39.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem:
The latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211?240.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-1999), pages 23?32,
College Park, MD.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL ?98), pages 768?774,
Montreal.
Lin, Dekang. 2003. Dependency-based
evaluation of MINIPAR. In Anne Abeille?,
editor, Treebanks: Building and Using
Parsed Corpora, pages 317?330, Kluwer,
Dordrecht.
Lodhi, Huma, Craig Saunders, John
Shawe-Taylor, Nello Cristianini, and
Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine
Learning Research, 2:419?444.
McCarthy, Diana, Rob Koeling, Julie
Weeds, and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics
(ACL?04), pages 279?286, Barcelona.
Miller, George A. and Walter G. Charles.
1991. Contextual correlates of semantic
similarity. Language and Cognitive
Processes, 6(1):1?28.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, OH.
Preller, Anne and Mehrnoosh Sadrzadeh.
2011. Bell states and negative sentences
in the distributed model of meaning.
Electronic Notes in Theoretical Computer
Science, 270(2):141?153.
Rudolph, Sebastian and Eugenie Giesbrecht.
2010. Compositional matrix-space models
of language. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 907?916,
Uppsala.
Schu?tze, Heinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
70
Clarke A Context-Theoretic Framework for Distributional Semantics
Smolensky, Paul. 1990. Tensor product
variable binding and the representation
of symbolic structures in connectionist
systems. Artificial Intelligence,
46(1-2):159?216.
Song, Dawei and Peter D. Bruza. 2003.
Towards context-sensitive information
inference. Journal of the American Society
for Information Science and Technology
(JASIST), 54:321?334.
Voiculescu, Dan-Virgil. 1997. Free Probability
Theory. American Mathematical Society,
Providence, RI.
Weeds, Julie, David Weir, and Diana
McCarthy. 2004. Characterising measures
of lexical distributional similarity.
In Proceedings of CoLING 2004,
pages 1015?1021, Geneva.
Widdows, Dominic. 2008. Semantic vector
products: Some initial investigations.
In Proceedings of the Second Symposium
on Quantum Interaction, pages 1?8,
Oxford.
Wittgenstein, Ludwig. 1953. Philosophical
Investigations. Macmillan, New York. G.
Anscombe, translator.
71
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38?44,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Semantic Composition with Quotient Algebras
Daoud Clarke
University of Hertfordshire
Hatfield, UK
daoud@metrica.net
Rudi Lutz
University of Sussex
Brighton, UK
rudil@sussex.ac.uk
David Weir
University of Sussex
Brighton, UK
davidw@sussex.ac.uk
Abstract
We describe an algebraic approach for
computing with vector based semantics.
The tensor product has been proposed as
a method of composition, but has the un-
desirable property that strings of different
length are incomparable. We consider how
a quotient algebra of the tensor algebra can
allow such comparisons to be made, offer-
ing the possibility of data-driven models of
semantic composition.
1 Introduction
Vector based techniques have been exploited in a
wide array of natural language processing appli-
cations (Schu?tze, 1998; McCarthy et al, 2004;
Grefenstette, 1994; Lin, 1998; Bellegarda, 2000;
Choi et al, 2001). Techniques such as latent se-
mantic analysis and distributional similarity anal-
yse contexts in which terms occur, building up a
vector of features which incorporate aspects of the
meaning of the term. This idea has its origins in
the distributional hypothesis of Harris (1968), that
words with similar meanings will occur in similar
contexts, and vice-versa.
However, there has been limited attention paid
to extending this idea beyond individual words,
so that the distributional meaning of phrases and
whole sentences can be represented as vectors.
While these techniques work well at the word
level, for longer strings, data becomes extremely
sparse. This has led to various proposals explor-
ing methods for composing vectors, rather than de-
riving them directly from the data (Landauer and
Dumais, 1997; Foltz et al, 1998; Kintsch, 2001;
Widdows, 2008; Clark et al, 2008; Mitchell and
Lapata, 2008; Erk and Pado, 2009; Preller and
Sadrzadeh, 2009). Many of these approaches use
a pre-defined composition operation such as ad-
dition (Landauer and Dumais, 1997; Foltz et al,
1998) or the tensor product (Smolensky, 1990;
Clark and Pulman, 2007; Widdows, 2008) which
contrasts with the data-driven definition of com-
position developed here.
2 Tensor Algebras
Following the context-theoretic semantics of
Clarke (2007), we take the meaning of strings as
being described by a multiplication on a vector
space that is bilinear with respect to the addition
of the vector space, i.e.
x(y + z) = xy + xz (x+ y)z = xz + yz
It is assumed that the multiplication is associative,
but not commutative. The resulting structure is an
associative algebra over a field ? or simply an
algebra when there is no ambiguity.
One commonly used bilinear multiplication op-
erator on vector spaces is the tensor product (de-
noted ?), whose use as a method of combining
meaning was first proposed by Smolensky (1990),
and has been considered more recently by Clark
and Pulman (2007) and Widdows (2008), who also
looked at the direct sum (which Widdows calls
the direct product, denoted ?).
We give a very brief account of the tensor prod-
uct and direct sum in the finite-dimensional case;
see (Halmos, 1974) for formal and complete defi-
nitions. Roughly speaking, if u1, u2, . . . un form
an orthonormal basis for a vector space U and
v1, v2, . . . vm form an orthonormal basis for vector
space V , then the space U ?V has dimensionality
nm with an orthonormal basis formed by the set
of all ordered pairs (ui, vj), denoted by ui ? vj ,
of the individual basis elements. For arbitrary el-
ements u =
?n
i=1 ?iui and v =
?m
j=1 ?jvj the
tensor product of u and v is then given by
u? v =
n?
i
m?
j
?i?j ui ? vj
38
For two finite dimensional vector spaces U and
V (over a field F ) of dimensionality n and m re-
spectively, the direct sum U ? V is defined as the
cartesian product U ? V together with the oper-
ations (u1, v1) + (u2, v2) = (u1 + u2, v1 + v2),
and a(u1, v1) = (au1, av1), for u1, u2 ? U ,
v1, v2 ? V and a ? F . In this case the vectors
u1, u2, . . . un, v1, v2, . . . vm form an orthonormal
set of basis vectors in U ? V , which is thus of
dimensionality n + m. In this case one normally
identifies U with the set of vectors in U ?V of the
form (u, 0), and V with the set of vectors of the
form (0, v). This construction makes U ? V iso-
morphic to V ?U , and thus the direct sum is often
treated as commutative, as we do in this paper.
The motivation behind using the tensor product
to combine meanings is that it is very fine-grained.
So, if, for example, red is represented by a vector
u consisting of a feature for each noun that is mod-
ified by red, and apple is represented by a vector
v consisting of a feature for each verb that occurs
with apple as a direct object, then red apple will
be represented by u ? v with a non-zero compo-
nent for every pair of non-zero features (one from
u and one from v). So, there is a non-zero ele-
ment for each composite feature, something that
has been described as red, and something that has
been done with an apple, for example, sky and eat.
Both ? and ? are intuitively appealing as se-
mantic composition operators, since u and v are
reconstructible from each of u? v and u? v, and
thus no information is lost in composing u and v.
Conversely, this is not possible with ordinary vec-
tor addition, which also suffers from the fact that it
is strictly commutative (not simply up to isomor-
phism like?), whereas natural language composi-
tion is in general manifestly non-commutative.
We make use of a construction called the tensor
algebra on a vector space V (where V is a space
of context features), defined as:
T (V ) = R? V ? (V ? V )? (V ? V ? V )? ? ? ?
Any element of T (V ) can be described as a sum of
components with each in a different tensor power
of V . Multiplication is defined as the tensor prod-
uct on these components, and extended linearly to
the whole of T (V ). We define the degree of a vec-
tor u in T (V ) to be the tensor power of its high-
est dimensional non-zero component, and denote
it deg(u); so for example, both v?v and u?(v?v)
have degree two, for 0 6= u, v ? V . We restrict
T (V ) to only contain vectors of finite degree.
A standard way to compare elements of a vector
space is to make use of an inner product, which
provides a measure of semantic distance on that
space. Assuming we have an inner product ??, ?? on
V , T (V ) can be given an inner product by defining
??, ?? = ?? for ?, ? ? R, and
?x1 ? y1, x2 ? y2? = ?x1, x2??y1, y2?
for x1, y1, x2, y2 ? V , and then extending this in-
ductively (and by linearity) to the whole of T (V ).
We assume that words are associated with vec-
tors in V , and that the higher tensor powers repre-
sent strings of words. The problem with the tensor
product as a method of composition, given the in-
ner product as we have defined it, is that strings
of different lengths will have orthogonal vectors,
clearly a serious problem, since strings of different
lengths can have similar meanings. In our previous
example, the vector corresponding to the concept
red apple lives in the vector space U ? V , and so
we have no way to compare it to the space V of
nouns, even though red apple should clearly be re-
lated to apple.
Previous work has not made full use of the ten-
sor product space; only tensor products are used,
not sums of tensor products, giving us the equiva-
lent of the product states of quantum mechanics.
Our approach imposes relations on the vectors of
the tensor product space that causes some product
states to become equivalent to entangled states,
containing sums of tensor products of different de-
grees. This allows strings of different lengths to
share components. We achieve this by construct-
ing a quotient algebra.
3 Quotient Algebras
An ideal I of an algebra A is a sub-vector space
of A such that xa ? I and ax ? I for all a ? A
and all x ? I . An ideal introduces a congruence
? on A defined by x ? y if and only if x? y ? I .
For any set of elements ? ? A there is a unique
minimal ideal I? containing all elements of ?; this
is called the ideal generated by ?. The quotient
algebra A/I is the set of all equivalence classes
defined by this congruence. Multiplication is de-
fined on A/I by the multiplication on A, since ?
is a congruence.
By adding an element x ? y to the generating
set ? of an ideal, we are saying that we want to
set x ? y to zero in the quotient algebra, which
has the effect of setting x equal to y. Thus, if we
39
have a set of pairs of vectors that we wish to make
equal in the quotient algebra, we put their differ-
ences in the generating set of the ideal. Note that
putting a single vector v in the generating set can
have knock-on effects, since all products of v with
elements of A will also end up in the ideal.
Although we have an inner product defined on
T (V ), we are not aware of any satisfactory method
for defining an inner product on T (V )/I , a con-
sequence of the fact that both T (V ) and I are
not complete. Instead, we define an inner prod-
uct on a space which contains the quotient algebra,
T (V )/I . Rather than considering all elements of
the ideal when computing the quotient, we con-
sider a sub-vector space of the ideal, limiting our-
selves to the space Gk generated from ? by only
allowing multiplication by elements up to a certain
degree, k.
Let us denote the vector subspace generated by
linearity alone (no multiplications) from a sub-
set ? of T (V ) by G(?). Also suppose B =
{e1, . . . , eN} is a basis for V . We then define
the spaces Gk as follows. Define sets ?k (k =
0, 1, 2, . . .) inductively as follows:
?0 = ?
?k = ?k?1 ? {(ei ? ?k?1)|ei ? B}
? {(?k?1 ? ei)|ei ? B}
Define
Gk = G(?k)
We note that
G0 ? G1 ? . . . Gk ? . . . ? I ? T (V )
form an increasing sequence of linear vector sub-
spaces of T (V ), and that
I =
??
k=0
Gk
This means that for any x ? I there exists a small-
est k such that for all k? ? k we have that x ? Gk? .
Lemma. Let x ? I, x 6= 0 and let deg(x) = d.
Then for all k ? d ? mindeg(?) we have that
x ? Gk, where mindeg(?) is defined to be the
minimum degree of the non-zero components oc-
curring in the elements of ?.
Proof. We first note that for x ? I it must
be the case that deg(x) ? mindeg(?) since I
is generated from ?. Therefore we know d ?
mindeg(?) ? 0. We only need to show that
x ? Gd?mindeg(?). Let k
? be the smallest in-
teger such that x ? Gk? . Since x 6? Gk??1 it
must be the case that the highest degree term of
x comes from V ? Gk??1 ? Gk??1 ? V . There-
fore k? + mindeg(?) ? d ? k? + maxdeg(?).
From this it follows that the smallest k? for which
x ? Gk? satisfies k? ? d ? mindeg(?), and we
know x ? Gk for all k ? k?. In particular x ? Gk
for k ? d?mindeg(?).
We show that T (V )/Gk (for an appropriate
choice of k) captures the essential features of
T (V )/I in terms of equivalence:
Proposition. Let deg(a ? b) = d and let k ?
d ? mindeg(?). Then a ? b in T (V )/Gk if and
only if a ? b in T (V )/I .
Proof. Since Gk ? I , the equivalence class of an
element a in T (V )/I is a superset of the equiva-
lence class of a in T (V )/Gk, which gives the for-
ward implication. The reverse follows from the
lemma above.
In order to define an inner product on
T (V )/Gk, we make use of the result of Berbe-
rian (1961) that if M is a finite-dimensional
linear subspace of a pre-Hilbert space P , then
P = M ? M?, where M? is the orthogonal
complement of M in P . In our case this implies
T (V ) = Gk ? G?k and that every element
x ? T (V ) has a unique decomposition as
x = y + x?k where y ? Gk and x
?
k ? G
?
k . This
implies that T (V )/Gk is isomorphic to G?k , and
that for each equivalence class [x]k in T (V )/Gk
there is a unique corresponding element x?k ? G
?
k
such that x?k ? [x]k. This element x
?
k can be
thought of as the canonical representation of all
elements of [x]k in T (V )/Gk, and can be found
by projecting any element in an equivalence class
onto G?k . This enables us to define an inner
product on T (V )/Gk by ?[x]k, [y]k?k = ?x?k, y
?
k?.
The idea behind working in the quotient algebra
T (V )/I rather than in T (V ) is that the elements
of the ideal capture differences that we wish to ig-
nore, or alternatively, equivalences that we wish to
impose. The equivalence classes in T (V )/I repre-
sent this imposition, and the canonical representa-
tives in I? are elements which ignore the distinc-
tions between elements of the equivalence classes.
40
However, by using Gk, for some k, instead of
the full ideal I , we do not capture some of the
equivalences implied by I . We would, therefore,
like to choose k so that no equivalences of impor-
tance to the sentences we are considering are ig-
nored. While we have not precisely established a
minimal value for k that achieves this, in the dis-
cussion that follows, we set k heuristically as
k = l ?mindeg(?)
where l is the maximum length of the sentences
currently under consideration, and ? is the gen-
erating set for the ideal I . The intuition behind
this is that we wish all vectors occurring in ? to
have some component in common with the vec-
tor representation of our sentences. Since com-
ponents in the ideal are generated by multipli-
cation (and linearity), in order to allow the ele-
ments of ? containing the lowest degree compo-
nents to potentially interact with our sentences,
we will have to allow multiplication of those el-
ements (and all others) by components of degree
up to l ?mindeg(?).
Given a finite set ? ? T (V ) of elements gen-
erating the ideal I , to compute canonical repre-
sentations, we first compute a generating set ?k
for Gk following the inductive definition given
earlier, and removing any elements that are not
linearly independent using a standard algorithm.
Using the Gram-Schmidt process (Trefethen and
Bau, 1997), we then calculate an orthonormal ba-
sis ?? for Gk, and, by a simple extension of Gram-
Schmidt, compute the projection of a vector u onto
G?k using the basis ?
?.
We now show how ?, the set of vectors gener-
ating the ideal, can be constructed on the basis of
a tree-bank, ensuring that the vectors for any two
strings of the same grammatical type are compa-
rable.
4 Data-driven Composition
Suppose we have a tree-bank, its associated tree-
bank grammar G, and a way of associating a con-
text vector with every occurrence of a subtree in
the tree-bank (where the vectors indicate the pres-
ence of features occurring in that particular con-
text). The context vector associated with a spe-
cific occurrence of a subtree in the tree-bank is an
individual context vector.
We assume that for every rule, there is a distin-
guished non-terminal on the right hand side which
we call the head. We also assume that for every
production pi there is a linear function ?pi from the
space generated by the individual context vectors
of the head to the space generated by the individ-
ual context vectors of the left hand side. When
there is no ambiguity, we simply denote this func-
tion ?.
Let X? be the sum over all individual vectors of
subtrees rooted withX in the tree-bank. Similarly,
for each Xj in the right-hand-side of the rule pii :
X ? X1 . . . Xr(pii), where r(pi) is the rank of pi,
let i?,j be the sum over the individual vectors of
those subtrees rooted with Xj where the subtree
occurs as the jth daughter of a local tree involving
the production pii in the tree-bank.
For each rule pi : X ? X1 . . . Xr with headXh
we add vectors
?pi,i = ?(ei)?X?1?. . .?X?h?1?ei?X?h+1?. . .?X?r
for each basis element ei of VXh to the generating
set. The reasoning behind this is to ensure that the
meaning corresponding to a vector associated with
the head of a rule is maintained as it is mapped to
the vector space associated with the left hand side
of the rule.
It is often natural to assume that the individual
context vector of a non-terminal is the same as the
individual context vector of its head. In this case,
we can take ? to be the identity map. In particular,
for a rule of the form pi : X ? X1, then ?pi,i is
zero.
It is important to note at this point that we have
presented only one of many ways in which a gram-
mar could be used to generate an ideal. In partic-
ular, it is possible to add more vectors to the ideal,
allowing more fine-grained distinctions, for exam-
ple through the use of a lexicalised grammar.
For each sentence w, we compute the tensor
product w? = a?1 ? a?2 ? ? ? ? ? a?n where the string
of words a1 . . . an form w, and each a?i is a vector
in V . For a sentence w we find an element w?O of
the orthogonal complement of Gk in T (V ) such
that w?O ? [w?], where [w?] denotes the equivalence
class of w? given the subspace Gk.
5 Example
We show how our formalism applies in a simple
example. Assume we have a corpus which
consists of the following sentences:
41
ap
pl
e
bi
g
ap
pl
e
re
d
ap
pl
e
ci
ty
bi
g
ci
ty
re
d
ci
ty
bo
ok
bi
g
bo
ok
re
d
bo
ok
apple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080
big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11
red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33
city 1.0 0.26 0.24 0.0 0.0 0.0
big city 1.0 0.33 0.0 0.0 0.0
red city 1.0 0.0 0.0 0.0
book 1.0 0.26 0.24
big book 1.0 0.33
red book 1.0
Figure 1: Similarities between phrases
see red apple see big city
buy apple visit big apple
read big book modernise city
throw old small red book see modern city
buy large new book
together with the following productions.
1. N? ? Adj N?
2. N? ? N
where N and Adj are terminals representing nouns
and adjectives, along with rules for the terminals.
We consider the space of adjective/noun phrases,
generated by N?, and define the individual context
of a noun to be the verb it occurs with, and the in-
dividual context of an adjective to be the noun it
modifies. For each rule, we take ? to be the iden-
tity map, so the vector spaces associated with N
and N?, and the vector space generated by indi-
vidual contexts of the nouns are all the same. In
this case, the only non-zero vectors which we add
to the ideal are those for the second rule (ignoring
the first rule, since we do not consider verbs in this
example except as contexts), which has the set of
vectors
?i = ei ? A?dj? ei
where i ranges over the basis vectors for contexts
of nouns: see, buy , visit , read ,modernise , and
A?dj = 2eapple + 2ebook + ecity
In order to compute canonical representations
of vectors, we take k = 1.
5.1 Discussion
Figure 1 shows the similarity between the noun
phrases in our sample corpus. Note that the vec-
tors we have put in the generating set describe only
compositionality of meaning ? thus for example
the similarity of the non-compositional phrase big
apple to city is purely due to the distributional
similarity between apple and city and composition
with the adjective big.
Our preliminary investigations indicate that the
cosine similarity values are very sensitive to the
particular corpus and features chosen; we are cur-
rently investigating other ways of measuring and
computing similarity.
One interesting feature in the results is how ad-
jectives alter the similarity between nouns. For ex-
ample, red apple and red city have the same sim-
ilarity as apple and city, which is what we would
expect from a pure tensor product. This also ex-
plains why all phrases containing book are disjoint
to those containing city, since the original vector
for book is disjoint to city.
The contribution that the quotient algebra gives
is in comparing the vectors for nouns with those
for noun-adjective phrases. For example, red ap-
ple has components in common with apple, as we
would expect, which would not be the case with
just the tensor product.
6 Conclusion and Further Work
We have presented the outline of a novel approach
to semantic composition that uses quotient alge-
bras to compare vector representations of strings
of different lengths.
42
The dimensionality of the construction we use
increases exponentially in the length of the sen-
tence; this is a result of our use of the tensor prod-
uct. This causes a problem for computation us-
ing longer phrases; we hope to address this in fu-
ture work by looking at the representations we use.
For example, product states can be represented in
much lower dimensions by representing them as
products of lower dimensional vectors.
The example we have given would seem to in-
dicate that we intend putting abstract (syntactic)
information about meaning into the set of generat-
ing elements of the ideal. However, there is no rea-
son that more fine-grained aspects of meaning can-
not be incorporated, even to the extent of putting
in vectors for every pair of words. This would
automatically incorporate information about non-
compositionality of meaning. For example, by in-
cluding the vector ?big apple ? b?ig ? a?pple , we
would expect to capture the fact that the term big
apple is non-compositional, and more similar to
city than we would otherwise expect.
Future work will also include establishing the
implications of varying the constant k and explor-
ing different methods for choosing the set ? that
generates the ideal. We are currently preparing
an experimental evaluation of our approach, using
vectors obtained from large corpora.
7 Acknowledgments
We are grateful to Peter Hines, Stephen Clark, Pe-
ter Lane and Paul Hender for useful discussions.
The first author also wishes to thank Metrica for
supporting this research.
References
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279?1296.
Sterling K. Berberian. 1961. Introduction to Hilbert
Space. Oxford University Press.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text
segmentation. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 109?117.
Stephen Clark and Stephen Pulman. 2007. Combin-
ing symbolic and distributional models of meaning.
In Proceedings of the AAAI Spring Symposium on
Quantum Interaction, pages 52?55, Stanford, CA.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008),
pages 133?140, Oxford, UK.
Daoud Clarke. 2007. Context-theoretic Semantics
for Natural Language: an Algebraic Framework.
Ph.D. thesis, Department of Informatics, University
of Sussex.
Katrin Erk and Sebastian Pado. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the EACL
Workshop on Geometrical Methods for Natural Lan-
guage Semantics (GEMS).
P. W. Foltz, W. Kintsch, and T. K. Landauer. 1998.
The measurement of textual coherence with latent
semantic analysis. Discourse Process, 15:285?307.
Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers, Dordrecht, NL.
Paul Halmos. 1974. Finite dimensional vector spaces.
Springer.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato?s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211?
240.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics and the 17th International Conference
on Computational Linguistics (COLING-ACL ?98),
pages 768?774, Montreal.
43
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June. Association for Computational Linguistics.
Anne Preller and Mehrnoosh Sadrzadeh. 2009. Bell
states and negation in natural languages. In Pro-
ceedings of Quantum Physics and Logic.
Heinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1-
2):159?216, November.
Lloyd N. Trefethen and David Bau. 1997. Numerical
Linear Algebra. SIAM.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Ox-
ford, UK.
44
Algebraic Approaches to Compositional Distributional Semantics
Daoud Clarke
University of Hertfordshire
daoud@metrica.net
David Weir
University of Sussex
davidw@sussex.ac.uk
Rudi Lutz
University of Sussex
rudil@sussex.ac.uk
Abstract
The question of how to compose meaning in distributional representations of meaning has re-
cently been recognised as a central issue in computational linguistics. In this paper we describe
three general and powerful tools that can be used to describe composition in distributional seman-
tics: quotient algebras, learning of finite dimensional algebras, and the construction of algebras from
semigroups.
1 Introduction
Vector based representations of meaning have wide application in natural language processing. While
these techniques work well at the word level, for longer strings, data becomes extremely sparse. The
question of how the principle of compositionality might apply for such representations has thus been
recognised as an important one (Widdows, 2008; Clark et al, 2008).
Context-theoretic semantics (Clarke, 2007) is a framework for composing meanings in vector based
semantics, in which the composition of the meaning of strings is described by a multiplication on a real
vector space A that is bilinear with respect to the addition of the vector space, i.e.
x(y + z) = xy + xz (x + y)z = xz + yz (?x)(?y) = ??xy
where x, y, z ? A and ?, ? ? R. It is assumed that the multiplication is associative, but not commutative.
The resulting structure is an associative algebra over a field ? or simply an algebra when there is no
ambiguity. Clarke (2007) gives a mathematical model of meaning as context, and shows that under this
model, the meaning of natural language expressions can be described by an algebra. The framework
is also applied to models of textual entailment, and logical and ontological representations of natural
language meaning.
In this paper, we identify three general techniques for constructing algebras.
? Using quotient algebras to impose relations on a free algebra, as described in (Clarke et al, 2010).
? Defining finite-dimensional algebras using matrices. Any finite-dimensional algebra can be de-
scribed in this way; we have investigated the possibility of learning such algebras using least
squares regression.
? Constructing algebras from a semigroup to give it vector space properties. We sketch a possible
method of using this technique, identified by Clarke (2007), to endow logical semantics with a
vector space nature.
This paper presents a preliminary consideration of these general techniques, and our goal is simply to
show that they are worthy of further exploration.
325
ap
pl
e
bi
g
ap
pl
e
re
d
ap
pl
e
ci
ty
bi
g
ci
ty
re
d
ci
ty
bo
ok
bi
g
bo
ok
re
d
bo
ok
apple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080
big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11
red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33
city 1.0 0.26 0.24 0.0 0.0 0.0
big city 1.0 0.33 0.0 0.0 0.0
red city 1.0 0.0 0.0 0.0
book 1.0 0.26 0.24
big book 1.0 0.33
red book 1.0
Figure 1: Cosine similarity values between phrases
see red apple see big city
buy apple visit big apple
read big book modernise city
throw old small red book see modern city
buy large new book
Figure 2: The corpus used to compute the vectors
that formed the generating set for the ideal.
2 Quotient Algebras
One commonly used bilinear multiplication operator on vector spaces is the tensor product (denoted ?),
whose use as a method of combining meaning was first proposed by Smolensky (1990), and has been
considered more recently by Clark and Pulman (2007) and Widdows (2008), who also looked at the
direct sum (which Widdows calls the direct product, denoted ?).
The tensor algebra on a vector space V (where V is a space of context features) is defined as:
T (V ) = R? V ? (V ? V )? (V ? V ? V )? ? ? ?
Any element of T (V ) can be described as a sum of components with each in a different tensor power
of V . Multiplication is defined as the tensor product on these components, and extended linearly to the
whole of T (V ).
Previous work has not made full use of the tensor product space; only tensor products are used,
not sums of tensor products, giving us the equivalent of the product states of quantum mechanics. Our
approach imposes relations on the vectors of the tensor product space that causes some product states
to become equivalent to entangled states, containing sums of tensor products of different degrees. This
allows strings of different lengths to share components. We achieve this by constructing a quotient
algebra.
An ideal I of an algebra A is a sub-vector space of A such that xa ? I and ax ? I for all a ? A
and all x ? I . An ideal introduces a congruence ? on A defined by x ? y if and only if x? y ? I . For
any set of elements ? ? A there is a unique minimal ideal I? containing all elements of ?; this is called
the ideal generated by ?. The quotient algebra A/I is the set of all equivalence classes defined by this
congruence. Multiplication is defined on A/I by the multiplication on A, since ? is a congruence.
Elements that are congruent with respect to the ideal have equivalence classes that are equal in the
quotient algebra. The construction is thus a way of imposing relations between vector elements: we
simply choose a set of pairs that we wish to be equal, and put their difference in the generating set ?.
Clarke et al (2010), showed how an inner product can be computed for elements of the quotient
algebra by taking the quotient of a finite dimensional subspace of the ideal and how a treebank could be
used to identify suitable elements to put into the generating set for the ideal in such a way that strings of
different lengths become comparable. Figure 1 shows similarities between adjective phrases computed
using vectors derived from the corpus in figure 2. The construction allows many properties of the tensor
product to carry over into the quotient algebra, for example the similarity of red book to red apple is the
same as the similarity of book to apple, as we would expect from the tensor product. Unlike the tensor
product, strings of different length are comparable, so for example, the similarity of apple to red apple
is non-zero. The benefit of using quotient algebras for compositional distributional semantics lies in
this ability to extend the favourable properties of the tensor product by imposing linguistically plausible
relations between vectors.
326
3 Learning Finite-dimensional Algebras
Quotient algebras are useful constructions when we have a small number of relations which we wish
to impose on the tensor algebra. In highly lexicalised grammars, the number of relations we wish to
impose may become so large that the ideal generates the whole vector space, and is thus useless, since
the resulting quotient space will be trivial. An alternative to this is to restrict the space of exploration to
finite-dimensional algebras. In this case, we can explore the space of possible products in relation to the
set of relations we wish to hold; in other words, we can view this as an optimisation problem in which
we want to find the best possible product given the required relations.
We apply this to the situation where we obtain a vector x? for each individual word and pair of
words in sequence. We then find the product that best fits these observed vectors. Given a set W =
{w1, w2 . . . wm} of words, we want to define a product  to minimise the difference between w?i  w?j
and w?iwj , for 1 ? i, j ? m. Specifically, we can define this as minimising
?
i,j
?w?iwj ? w?i  w?j?
If word vectors have n dimensions, then  is defined by an n3 dimensional vector, which we denote frst
for 1 ? r, s, t ? n, where (er  es)t = frst and e is the vector with 1 in every component, and vt is the
tth component of v.
We can view this as a linear model:
(w?iwj)t = ijt +
n?
r,s=1
(w?i)r(w?j)sfrst
where we have m2 statistical units to learn n2 parameters relating to the tth component of the vector
space. Since these parameters are independent for each value of t, each set of n2 parameters can be learnt
in parallel. We are currently exploring ways of learning these parameters. The form of the equation
above suggests the use of least squares, and we have performed some experiments using this method
using a corpus extracted from the ukWaC corpus (Ferraresi et al, 2008). We extracted a list of verb
adjective?noun sequences, and used latent semantic analysis (Deerwester et al, 1990) to generate n-
dimensional vectors for the 160 most common adjectives and nouns, and pairs of these adjectives and
nouns. Our initial results indicate that the learnt parameters tend to get very large when using least
squares to find the parameters, leading to poor results; we plan to investigate other methods such as
linear optimisation.
Guevara (2010) proposed a related method of learning composition which used linear regression to
learn how components compose. His model is however much more restrictive than ours in that the value
of a component in the product depends only on that same component in the composed vectors, whereas
in our model, the value of the component can depend on all components in the composed vectors.
Baroni and Zamparelli (2010) took a similar approach, in which adjectives are modelled as matrices
acting on the space of nouns, and the matrices are learnt using least squares regression. The algebra
products we propose learning are more general than matrix products; in addition we do not need to
distinguish between words which are represented as matrices and words which are represented as vectors.
4 Constructing Algebras from Semigroups
Whilst the previous two techniques we have discussed are very general, and allow corpus data to be easily
incorporated into the composition definition, our implementations are currently a long way from being
able to represent the complexities of natural language semantics that is currently possible with logical
semantics. This has become the standard method of representing natural language meaning, originating
in the work of Montague (1973), however there is currently no way to incorporate statistical features of
meaning that are described by the distributional hypothesis of Harris (1968).
327
Term Context vector
fish (0, 0, 1)
big (1, 2, 0)
Figure 3: Example context vectors for terms.
ni = (N , ?x nouni(x))
ai = (N /N , ?p?y adji(y) ? p.y)
Figure 4: Equations describing syntax and semantics
of adjectives and nouns.
In related work, Clark et al (2008) described a method of composing meanings which they noted
was a generalisation of Montague semantics. However, their version of Montague semantics assumed a
particular model, and thus effectively mapped sentences to truth values. This omits much of the power
of Montague semantics in which sentences are mapped to logical forms which then provide restrictions
on the set of allowable models, allowing, for example, entailments to be computed between sentences.
We will sketch a method by which Montague semantics can be described within the context-theoretic
framework. We follow a standard method of representing logic in language, but instead of representing
words using logic, we represent an individual dimension of meaning of a word by a logical form ? we
call this dimension a ?aspect?. The general scheme is to represent aspects as elements of a semigroup,
from which we form an algebra. Words are then represented as weighted sums over individual aspects.
We define a set S of all aspects as the set of pairs (s, ?), where s is the syntactic type of an aspect
(for example in the Lambek calculus) and ? is the semantics of the aspect (for example described in
the lambda calculus). We can extend S by defining a product on such pairs reducing each element to a
normal form. This defines a semigroup: the Lambek calculus can be described in terms of a residuated
lattice, which is a partially ordered semigroup (Lambek, 1958), and the lambda calculus is equivalent to a
Cartesian closed category under ?-equivalence (Lambek, 1985), which can be considered as a semigroup
with additional structure.
Given any semigroup S we can construct an algebra L1(S) of real-valued functions on S which are
finite under the L1 norm with multiplication defined by convolution:
(u ? v)(x) =
?
y,z?S:yz=x
u(y)v(z).
For example, suppose we have context vectors for the terms big and fish as described in Figure
3. We represent the syntax and semantics of adjectives and nouns by elements ai and ni respectively
of a semigroup S (Figure 4), where we assume equivalence under ?-reduction is accounted for. The
predicates adji and nounj correspond to aspects, in this case each dimension i of the three dimensions
in the context vectors has a corresponding adji and nouni. We may then represent the vectors for these
terms as elements of the algebra b?ig = a1 + 2a2 and ? = n3, where we equate an element u of the
semigroup with the function in the algebra L1(S) which maps u to 1 and every other element to zero.
Then b?ig ? = a1n3 + 2a2n3, where
ainj = (N,?x(nounj(x) ? adji(x))).
Note that the elements ai form a commutative, idempotent subsemigroup of S, so they have a semilattice
structure. In order for this structure to carry over to the vector structure in the algebra, we would need
a more sophisticated construction, such as a C? enveloping algebra; we leave the investigation of this
possibility to further work.
5 Discussion
We have presented our initial investigations into the application of three powerful methods of construct-
ing algebras to representing natural language semantics. Each of these approaches has potential use in
representing meaning; here we have only touched the surface of what is possible with each technique. We
328
hope that with further work, these methods will lead to a true synthesis between logical and distributional
approaches to natural language semantics.
6 Acknowledgments
We are grateful to Peter Hines, Stephen Clark and Peter Lane for useful discussions. The first author also
wishes to thank Metrica for supporting this research.
References
Baroni, M. and R. Zamparelli (2010). Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2010), East Stroudsburg PA: ACL, pp. 1183?1193.
Clark, S., B. Coecke, and M. Sadrzadeh (2008). A compositional distributional model of meaning. In
Proceedings of the Second Quantum Interaction Symposium (QI-2008), Oxford, UK, pp. 133?140.
Clark, S. and S. Pulman (2007). Combining symbolic and distributional models of meaning. In Proceed-
ings of the AAAI Spring Symposium on Quantum Interaction, Stanford, CA, pp. 52?55.
Clarke, D. (2007). Context-theoretic Semantics for Natural Language: an Algebraic Framework. Ph. D.
thesis, Department of Informatics, University of Sussex.
Clarke, D., R. Lutz, and D. Weir (2010, July). Semantic composition with quotient algebras. In Proceed-
ings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, Uppsala, Sweden,
pp. 38?44. Association for Computational Linguistics.
Deerwester, S., S. Dumais, G. Furnas, T. Landauer, and R. Harshman (1990). Indexing by latent semantic
analysis. Journal of the American Society for Information Science 41(6), 391?407.
Ferraresi, A., E. Zanchetta, M. Baroni, and S. Bernardini (2008). Introducing and evaluating ukwac, a
very large web-derived corpus of english. In Workshop Programme, pp. 47.
Guevara, E. (2010). A Regression Model of Adjective-Noun Compositionality in Distributional Seman-
tics. ACL 2010, 33.
Harris, Z. (1968). Mathematical Structures of Language. Wiley, New York.
Lambek, J. (1958). The mathematics of sentence structure. American Mathematical Monthly 65, 154?
169.
Lambek, J. (1985, May). Cartesian closed categories and typed lambda-calculi. In G. Cousineau, P.-L.
Curien, and B. Robinet (Eds.), Combinators and Functional Programming Languages, Lecture Notes
in Computer Science. Springer-Verlag.
Montague, R. (1973). The proper treatment of quantification in ordinary English. Dordrecht, Holland:
D. Reidel Publishing Co.
Smolensky, P. (1990, November). Tensor product variable binding and the representation of symbolic
structures in connectionist systems. Artificial Intelligence 46(1-2), 159?216.
Widdows, D. (2008). Semantic vector products: Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Oxford, UK.
329
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 44?52,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Developing Robust Models for Favourability Analysis
Daoud Clarke Peter Lane
School of Computer Science
University of Hertfordshire
Hatfield, UK
daoud@metrica.net
peter.lane@bcs.org.uk
Paul Hender
Metrica
London, UK
paul@metrica.net
Abstract
Locating documents carrying positive or neg-
ative favourability is an important application
within media analysis. This paper presents
some empirical results on the challenges fac-
ing a machine-learning approach to this kind
of opinion mining. Some of the challenges in-
clude: the often considerable imbalance in the
distribution of positive and negative samples;
changes in the documents over time; and ef-
fective training and quantification procedures
for reporting results. This paper begins with
three datasets generated by a media-analysis
company, classifying documents in two ways:
detecting the presence of favourability, and as-
sessing negative vs. positive favourability. We
then evaluate a machine-learning approach to
automate the classification process. We ex-
plore the effect of using five different types of
features, the robustness of the models when
tested on data taken from a later time period,
and the effect of balancing the input data by
undersampling. We find varying choices for
the optimum classifier, feature set and training
strategy depending on the task and dataset.
1 Introduction
Media analysis is a discipline closely related to con-
tent analysis (Krippendorff, 2004), with an emphasis
on analysing content with respect to:
Favourability how favourable an article is with
respect to an entity. This will typically be on a five
point scale: very negative, negative, neutral, positive
or very positive.
Key messages topics or areas that a client is inter-
ested in. This allows the client to gain feedback on
the success of particular public relations campaigns,
for example.
Media analysis has traditionally been done manu-
ally, however the explosion of content on the world-
wide web, in particular social media, has led to the
introduction of automatic techniques for performing
media analysis, e.g. Tatzl and Waldhauser (2010).
In this paper, we discuss our recent findings in ap-
plying machine learning techniques to favourability
analysis. The work is part of a two-year collabo-
ration between Gorkana Group, which includes one
of the foremost media analysis companies, Metrica,
and the University of Hertfordshire. The goal is to
develop ways of automating media analysis, espe-
cially for social media. The data used are from tra-
ditional media (newspapers and magazines) since at
the time of starting the experiment there was more
manually analysed data available. We discuss the
typical problems that arise in this kind of text min-
ing, and the practical results we have found.
The documents are supplied by Durrants, the me-
dia monitoring company within the Gorkana Group,
and consist of text from newspaper and magazine
articles in electronic form. Each document is anal-
ysed by trained human analysts, given scores for
favourability, as well as other characteristics which
the client has requested. This dataset is used to pro-
vide feedback to the clients about how they are por-
trayed in the media, and is summarised by Metrica
for clients? monthly reports.
Favourability analysis is very closely related to
sentiment analysis, with the following distinction:
44
sentiment analysis generally focuses on a (subjec-
tive) sentiment implying an opinion of the author,
for example:1
(1) Microsoft is the greattteesssst at EVERY-
THING
expresses the author?s opinion (which others may
not share) whereas favourability analysis, whilst
also taking into account sentiment, also measures
favourable objective mentions of entities. For ex-
ample:2
(2) Halloween Eve Was The Biggest Instagram
Day Ever, Doubling Its Traffic
is an objective statement (no one can doubt that the
traffic doubled) that is favourable with respect to the
organisation, Instagram. Since the task is so simi-
lar to that of sentiment analysis, we hypothesise that
similar techniques will be useful.
The contributions of this paper are as follows: (1)
whilst automated sentiment analysis has received a
lot of attention in the academic literature, favourabil-
ity analysis has so far not benefited from an in-depth
analysis. (2) We provide results on a wide variety of
different classifiers, whereas previous work on sen-
timent analysis typically considers at most two or
three different classifiers. (3) We discuss the prob-
lem of imbalanced data, looking at how this impacts
on the training and evaluation techniques. (4) We
show that both attribute selection and balancing the
classifier?s training set can improve performance.
2 Background
There is a very large body of literature on both sen-
timent analysis and machine learning; for space rea-
sons, we will mention only a small sample.
2.1 Favourability Analysis
The most closely related task to ours is arguably
opinion mining, i.e. determining sentiment with re-
spect to a particular target. Balahur et al (2010)
examine this task for newspaper articles. They
show that separating out the objective favourabil-
ity from the expressed sentiment led to an increase
1Actually, this is an ironic comment on a blog post at
TechCrunch.
2A headline from TechCrunch
in inter-annotator agreement, which they report as
81%, after implementing improvements to the pro-
cess. Melville et al (2009) report on an automated
system for opinion mining applied to blogs, which
achieves between 64% and 91% accuracy, depend-
ing on the domain, while Godbole et al (2007) de-
scribe a system applied to news and blogs.
Pang et al (2002) introduced machine learning to
perform sentiment analysis. They used na??ve bayes,
support vector machines (SVMs) and maximum en-
tropy on the movie review domain, and report ac-
curacies between 77% and 83% depending on the
feature set, which included unigrams, bigrams, and
part-of-speech tagged unigrams. More recent work
along these lines is described in (Pang and Lee,
2008; Prabowo and Thelwall, 2009).
One approach to sentiment analysis is to build
up a lexicon of sentiment carrying words. Turney
(2002) described a way to automatically build such a
lexicon based on looking at co-occurrences of words
with other words whose sentiment is known. This
idea was extended by Gamon et al (2005) who also
considered the lack of co-occurrence as useful infor-
mation.
Koppel and Schler (2006) show that it is impor-
tant to distinguish the two tasks of determining neu-
tral from non-neutral sentiment, and positive versus
negative sentiment, and that doing so can signifi-
cantly improve the accuracy of automated systems.
2.2 Machine Learning Approaches
Document classification is an ideal domain for ma-
chine learning, because the raw data, the text, are
easily manipulated, and often large amounts of text
can be obtained, making the problems amenable to
statistical analysis.
A classification model is essentially a mapping,
from a document described as a set of feature values
to a class label. In most cases, this class label is a
simple yes-no choice, such as whether the document
is favourable or not. In the experimental section of
this paper we describe results from applying a range
of different classification algorithms.
In general, two issues that affect machine-
learning approaches are the selection of features,
and the presence of imbalanced data.
45
2.2.1 Features
Useful features for constructing classification
models from text documents include sets of uni-
grams, bigrams or trigrams, dependency relation-
ships or selected words: we review these features in
the next section. From a machine-learning perspec-
tive, it is useful for the features to include only rele-
vant information, and also to be independent of each
other. This feature-selection problem has been tack-
led by several authors in different ways, e.g. (Blum
and Langley, 1997; Forman, 2003; Green et al,
2010; Mladenic?, 1998; Rogati and Yang, 2002). In
our experiments, we evaluate a technique to reduce
the number of features using attribute selection.
Alternative approaches to understanding the sen-
timent of text attempt to go beyond the simple la-
belling of the presence of a word. Some authors
have described experiments augmenting the above
feature sets with additional information. Mullen and
Collier (2004), for example, uses WordNet to add in-
formation about words found within text, and conse-
quently reports improved classification performance
in a sentiment analysis task.
2.3 Imbalanced Data
Our datasets, as is usual in many real-world applica-
tions, present varying degrees of imbalance between
the two classes. Imbalanced data must be dealt with
at two parts of the process: during training, to ensure
the model is capable of working with both classes,
and in evaluation, to ensure a model with the best
performance is selected for use on novel data. These
two elements are often treated together, but need to
be considered separately. In particular, the appropri-
ate training method to handle imbalanced data can
vary between algorithm and domain.
First considering evaluation, the standard mea-
sure of accuracy (proportion of correctly classified
examples) is inappropriate if 90% of the documents
are within one class. A simple ZeroR classifier (se-
lecting the majority class) will score highly, but it
will never get any examples of the minority class
correct. A better evaluation technique uses a combi-
nation of the separate accuracy measures on the two
classes (a1 and a2), where ai denotes the proportion
of instances from class i that were judged correctly.
For example, the geometric mean, as proposed by
Kubat et al (1998), computes ?a1 ? a2. This has
the property that it strongly penalises poor perfor-
mance in any one class: if either a1 or a2 is zero then
the geometric mean will be zero. This characteristic
is important for our purposes, since it is ?easy? to
get high accuracy on the majority class, the measure
will favour classifiers that perform well on the mi-
nority class without significant loss of accuracy in
the majority class. In addition, the geometric mean
does not give preference to any one class, unlike, for
example, the F-measure. Measures such as the av-
erage precision and recall, or F-measure, may also
prove useful, especially if preference is being given
to one class.
Second considering the training process. An im-
balanced training set can lead to bias in the construc-
tion of a machine-learning model. Such effects are
well-known in the literature, and various approaches
have been proposed to address this problem, such as
balancing the training set using under or over sam-
pling, and altering the weighting of the classifier
based on the proportion of the expected class. In our
experiments we used undersampling (where a ran-
dom sample is taken from the majority class to bal-
ance the size of the minority class); this technique
has the disadvantage of discarding training data. In
contrast, the SMOTE (Chawla et al, 2004) algo-
rithm is a technique for creating new instances of the
minority class, to balance the number in the major-
ity class. We also used geometric-mean as the eval-
uation measure for algorithms such as SVMs, when
selecting parameters.
3 Our Approach
3.1 Description of Data
The source documents have been tagged by analysts
for favourability and unfavourability, both of which
are given a non-negative score that is indicative both
of the number of favourable/unfavourable mentions
of the organisation and the degree of favourabil-
ity/unfavourability. Neutral documents are assigned
a score of zero for both favourability and unfavoura-
bility. We assign each document a class based on its
favourability f and unfavourability u scores. Docu-
ments are categorised as follows:
46
Dataset Mixed V. Neg. Negative Neutral Positive V. Pos.
A 472 86 138 1610 1506 1664
C 7 0 5 2824 852 50
S 522 94 344 9580 2057 937
Table 1: Number of documents in each class for the datasets A, C and S.
Dataset Neutral Non-neutral
A 1610 3866
C 2824 914
S 9580 3954
Table 2: Class distributions for pseudo-subjectivity task
f > 0 and u > 0: mixed
f = 0 and u > 1: very negative
f = 0 and u = 1: negative
f = 0 and u = 0: neutral
f = 1 and u = 0: positive
f > 1 and u = 0: very positive
Table 1 shows the number of documents in each
category for three datasets A, C and S, which are
anonymised to protect Metrica?s clients? privacy. A
and S are datasets for high-tech companies, whereas
C is for a charity. This is reflected in the low oc-
curence of negative favourability with dataset C.
Datasets A and C contain only articles that are rele-
vant to the client, whereas S contains articles for the
client?s competitors. We only make use of favoura-
bility judgments with respect to the client, however,
so those that are irrelevant to the client we simply
treat as neutral. This explains the overwhelming bias
towards neutral sentiment in dataset S.
In our experiments, we consider only those doc-
uments which have been manually analysed and for
which the raw text is available. Duplicates were re-
moved from the dataset. Duplicate detection was
performed using a modified version of Ferret (Lane
et al, 2006) which compares occurrences of charac-
ter trigrams between documents. We considered two
documents to be duplicates if they had a similarity
score higher than 0.75.
This paper describes experiments for two tasks:
Pseudo-subjectivity ? detecting the presence or ab-
sence of favourability. This is thus a two-class prob-
lem with neutral documents in one class, and all
other documents in the other.
Dataset Positive Negative
A 3170 224
C 902 5
S 2994 438
Table 3: Class distributions for pseudo-sentiment task
Pseudo-sentiment ? distinguishing between docu-
ments with generally positive and negative favoura-
bility. In our experiments, we treat this as a two class
problem, with negative and very negative docu-
ments in one class and positive and very positive
documents in the other (ignoring mixed sentiment).
3.2 Method
We follow a similar approach to Pang et al (2002):
we generate features from the article text, and train
a classifier using the manually analysed data.
We sorted the documents by time, and then se-
lected the earliest two thirds as a training set, and
kept the remainder as a held out test set. This al-
lows us to get an idea of how the system will per-
form when it is in use, since the system will neces-
sarily be trained on documents from an earlier time
period. We performed cross validation on the ran-
domised training set, giving us an upper bound on
the performance of the system, and we also mea-
sured the accuracy of every system on the held out
dataset. We hypothesised that new topics would be
discussed in the later time frame, and thus the accu-
racy would be lower, since the system would not be
trained on data for these topics.
We also experimented with balancing the input
data to the classifiers; each system was run twice,
once with all the input data, and once with data
which had been undersampled so that the number
of documents in each class was the same. And also
we experimented with attribute selection: reducing
the number of features used to describe the dataset.
47
Type Relation Term
governor det the
governor rcmod sued
governor nn leader
dependent poss conference
dependent nsubj bullish
dependent dep beat
Table 4: Example dependency relations extracted from
the data. ?Type? indicates whether the term referring to
the organisation is the governor or the dependent in the
expression.
3.2.1 Features for documents
We used five types of features:
Unigrams, bigrams and trigrams: produced using
the WEKA tokenizer with the standard settings.3
EntityWords: unigrams of words occurring within
a sentence containing a mention of the organisation
in question. Mentions of the organisation were de-
tected using manually constructed regular expres-
sions, based on datasets for organisations collected
elsewhere in the company. Sentence boundary de-
tection was performed using an OpenNLP4 tool.
Dependencies: we extract dependencies using the
Stanford dependency parser. For the purpose of
this experiment, we only considered dependencies
directly connecting the term relating to the organ-
isation. Table 4 gives example dependencies ex-
tracted from the data. For example, the phrase
?. . . prompted [organisation name] to be bullish. . . ?
led to the extraction of the term bullish, where the
organisation name is the subject of the verb and the
organisation name is a dependent of the verb bullish.
For each dependency, all this information is com-
bined into a single feature.
3.3 Classification Algorithms
We used the following classifiers in our experiments:
na??ve Bayes, Support Vector Machines (SVMs), k-
nearest neighbours with k = 1 and k = 5, radial
basis function (RBF) networks, Bayesian networks,
decision trees (J48) and a propositional rule learner,
Repeated Incremental Pruning to Produce Error Re-
duction (JRip). We also included two baseline clas-
3We used the StringToWordVectorClass constructed with an
argument of 5,000.
4http://opennlp.sourceforge.net
sifiers, ZeroR, which simply chooses the most fre-
quent class in the training set, and Random, which
chooses classes at random based on their frequencies
in the training set.
These are taken from the WEKA toolkit (Witten
and Frank, 2005), with the exception of SVMs, for
which we used the LibSVM implementation, na??ve
Bayes (since the Weka implementation does not ap-
pear to treat the value occurring with a feature as
a frequency) and Random, both of which we imple-
mented ourselves. We used WEKA?s default settings
for classifiers where appropriate.
3.3.1 Parameter search for SVMs
We used a radial-basis kernel for our SVM algo-
rithm which requires two parameters to be optimised
experimentally. This was done for each fold of cross
validation. Each fold was further divided, and three-
fold cross validation was performed for each param-
eter combination. We varied the gamma parameter
exponentially between 10?5 and 105 in multiples of
100, and varied cost between 1 and 15 in increments
of 2. We used the geometric mean of the accuracies
on the two classes to choose the best combination of
parameters; using the geometric mean enables us to
train and evaluate the SVM from either balanced or
imbalanced datasets.
3.3.2 Attribute Selection
Because of the long training time of many of
the classifiers with numbers of features, we also
looked at whether reducing the dimensionality of the
data before training by performing attribute selec-
tion would enhance or hinder performance. The at-
tribute selection was done by ranking the features
using the Chi-squared measure and taking the top
250 with the most correlation with the class. The ex-
ception to this was k-nearest neighbours, for which
we used random projections with 250 dimensions.
For the RBF network we tried both attribute selec-
tion and random projections, and na??ve Bayes was
run both with and without attribute selection.
3.4 Results
Tables 5 and 6 show the best classifier on the cross-
validation evaluation for each dataset and feature
set for the pseudo-subjectivity and pseudo-sentiment
tasks respectively, together with the Random clas-
48
D
at
as
et
Features Best Classifier A
tt.
Se
l.
B
al
an
ce
Cross val. acc. Held out acc.
S Random 0.465 ? 0.008 0.461 ? 0.007
S EntityWords SVM X 0.912 ? 0.002 0.952 ? 0.001
S Unigrams JRip X X 0.907 ? 0.002 0.952 ? 0.002
S Bigrams SVM X X 0.875 ? 0.007 0.885 ? 0.004
S Trigrams Na??ve Bayes 0.791 ? 0.003 0.759 ? 0.003
S Dependencies RBFNet X 0.853 ? 0.005 0.766 ? 0.054
C Random 0.417 ? 0.017 0.419 ? 0.027
C EntityWords Na??ve Bayes X 0.704 ? 0.011 0.640 ? 0.018
C Unigrams Na??ve Bayes X 0.735 ? 0.007 0.659 ? 0.032
C Bigrams Na??ve Bayes 0.756 ? 0.012 0.640 ? 0.014
C Trigrams Na??ve Bayes 0.757 ? 0.004 0.679 ? 0.017
A Random 0.453 ? 0.004 0.453 ? 0.017
A EntityWords BayesNet X 0.691 ? 0.008 0.625 ? 0.019
A Unigrams SVM X X 0.696 ? 0.005 0.619 ? 0.010
A Bigrams SVM X X 0.680 ? 0.012 0.609 ? 0.026
A Trigrams Na??ve Bayes X 0.610 ? 0.011 0.536 ? 0.019
Table 5: Results for the pseudo-subjectivity task, distinguishing documents neutral with respect to favourability from
those which are not neutral. The accuracy was computed as the geometric mean of accuracy on the neutral documents
and the accuracy on the non-neutral documents. The best-performing classifier on cross-validation is shown for each
feature set, along with the Random classifier as a baseline. An indication is given of whether the best-performing
system used attribute selection and/or balancing on the input data.
D
at
as
et
Features Best Classifier Ba
la
n
ce
Cross val. acc. Held out acc.
S Random 0.332 ? 0.023 0.365 ? 0.03
S EntityWords Na??ve Bayes X 0.738 ? 0.008 0.552 ? 0.033
S Unigrams Na??ve Bayes X 0.718 ? 0.017 0.650 ? 0.024
S Bigrams Na??ve Bayes X 0.748 ? 0.013 0.682 ? 0.023
S Trigrams Na??ve Bayes X 0.766 ? 0.014 0.716 ? 0.038
S Dependencies Na??ve Bayes 0.566 ? 0.014 0.523 ? 0.060
A Random 0.253 ? 0.026 0.111 ? 0.072
A EntityWords Na??ve Bayes X 0.737 ? 0.016 0.656 ? 0.067
A Unigrams Na??ve Bayes X 0.769 ? 0.008 0.756 ? 0.031
A Bigrams Na??ve Bayes 0.755 ? 0.009 0.618 ? 0.157
A Trigrams Na??ve Bayes 0.800 ? 0.02 0.739 ? 0.088
Table 6: Results for the pseudo-sentiment task, distinguishing positive and negative favourability. See the preceding
table for details. None of the best performing systems used attribute selection on this task. No data is shown for dataset
C since there were not enough negative documents in the test set to compute the accuracies.
49
sifier baseline. The accuracies shown were com-
puted using the geometric mean of the accuracy on
the two classes. This was computed for each cross-
validation fold; the value shown is the (arithmetic)
mean of the accuracies on the five folds, together
with an estimate of the error in this mean. The val-
ues for the held out data were computed in the same
way, dividing the data into five, allowing us to esti-
mate the error in the accuracy.
4 Discussion
4.1 Overall accuracy
The most notable difference between the two tasks,
pseudo-subjectivity and pseudo-sentiment, is that
the best classifier for the sentiment task was na??ve
Bayes in every case, whereas the best classifier
varies with dataset and feature set for the pseudo-
subjectivity task. This is presumably because the in-
dependence assumption on which the na??ve Bayes
classifier is based holds very well for the pseudo-
sentiment task, at least with our datasets.
The level of accuracy we report for the pseudo-
sentiment task is lower than that typically reported
for sentiment analysis, e.g. Pang et al (2002), but
in line with that from other results, such as Melville
et al (2009). This could be because favourability
is harder to determine than sentiment. For exam-
ple it may require world knowledge in addition to
linguistic knowledge, in order to determine whether
the reporting of a particular event is good news for a
company, even if reported objectively.
Accuracy on the held out dataset is up to 10%
lower than the cross-validation accuracy on the
pseudo-subjectivity task, and up to 6% lower on the
pseudo-sentiment task. This is probably due to a
change in topics over time. This degradation in per-
formance could be reduced by techniques such as
those used to improve cross-domain sentiment anal-
ysis (Li et al, 2009; Wan, 2009).
4.2 Features
Trigrams proved the most effective feature type in
3 out of the 5 different experiments, with unigrams
and entity words proving the best in 1 case each.
However, in many cases, there is not a significant
difference between the results for different datasets.
Although we only computed dependencies for
one dataset, S, we found that they did not provide
significant benefit on their own. This may be due
to the sparseness of the data, since we only ex-
tracted dependencies with respect to the organisa-
tion in question. Dependencies may be useful when
combined with other features, such as unigrams.
Attribute selection was not always effective
in improving classification, even with the high-
dimensionality of the data. In the pseudo-sentiment
task, none of the best classifiers used attribute se-
lection. In the pseudo-subjectivity task, 8 out of 13
results showed a benefit in using attribute selection.
This issue deserves further exploration, not least be-
cause reducing the number of attributes can consid-
erably speed-up the training process.
4.3 Imbalance
Finally, we look at our results considering the im-
balanced data problem. Within some of the algo-
rithms, balance is actively taken account during the
training process: e.g. na??ve Bayes has a weighting
on its class output to compensate for different fre-
quencies, and the SVM training process uses geo-
metric mean for computing performance, which en-
courages a good performance on imbalanced data.
In addition, we have presented results on the differ-
ence between training with balanced and unbalanced
datasets. Better results are obtained in 5 out of the
13 results for the pseudo-subjectivity task (Table 5),
and in 6 out of 9 results for the pseudo-sentiment
task (Table 6), suggesting that balancing the training
data is a useful technique in most cases.
However, a surprising result is found in Table 7,
which shows selected pseudo-subjectivity results for
dataset S with and without balanced input data. This
dataset has an approximately 70:30 imbalance in
the class distribution. Interestingly, balancing the
data shows mixed results for this dataset. In par-
ticular, the accuracy of the Bayesian network, and
sometimes the na??ve Bayes classifier, are severely
reduced. We found similar behaviour with dataset
C (with a 75:25 imbalance), however, as shown in
Table 8, we found the converse on dataset A (with
a 30:70 imbalance): nearly every classifier per-
formed better with balanced data. Further, Table 6
shows that balancing data has proven effective for
the na??ve Bayes classifiers in the pseudo-sentiment
task, where the imbalance is more severe (94:6 for
50
Unbalanced Balanced
Features Classifier Neut. Non. Cross val. acc. Neut. Non. Cross val. acc.
EntityWords SVM 0.962 0.864 0.912 ? 0.003 0.959 0.864 0.911 ? 0.002
EntityWords Na??ve Bayes 0.969 0.850 0.908 ? 0.003 1 0 0 ? 0
Unigrams SVM 0.959 0.857 0.907 ? 0.002 0.954 0.859 0.905 ? 0.002
Unigrams Na??ve Bayes 0.774 0.789 0.781 ? 0.006 0.910 0.581 0.727 ? 0.008
Bigrams SVM 0.747 0.933 0.835 ? 0.006 0.849 0.901 0.875 ? 0.007
Bigrams Na??ve Bayes 0.883 0.716 0.795 ? 0.004 0.947 0.569 0.734 ? 0.005
Trigrams BayesNet 0.620 0.883 0.739 ? 0.009 0.975 0.118 0.289 ? 0.086
Trigrams J48 0.356 0.964 0.586 ? 0.012 0.441 0.942 0.644 ? 0.008
Trigrams JRip 0.422 0.963 0.637 ? 0.003 0.388 0.963 0.605 ? 0.042
Trigrams SVM 0.575 0.921 0.728 ? 0.008 0.604 0.909 0.740 ? 0.009
Trigrams Na??ve Bayes 0.810 0.758 0.784 ? 0.003 0.922 0.593 0.739 ? 0.005
Trigrams RBFNet 0.459 0.949 0.659 ? 0.010 0.478 0.934 0.667 ? 0.013
Table 7: Selected balanced versus unbalanced cross validation accuracies (geometric mean) for dataset S, pseudo-
subjectivity task, together with the accuracies on the individual classes, neutral and non-neutral. For consistency, only
results where attribute selection was performed are shown.
Unbalanced Balanced
Features Classifier Neut. Non. Cross val. acc. Neut. Non. Cross val. acc.
EntityWords SVM 0.872 0.394 0.587 ? 0.006 0.575 0.812 0.683 ? 0.007
EntityWords Na??ve Bayes 0.972 0.111 0.326 ? 0.021 0.944 0.192 0.426 ? 0.015
Unigrams SVM 0.837 0.464 0.622 ? 0.011 0.694 0.698 0.696 ? 0.005
Unigrams Na??ve Bayes 0.896 0.318 0.531 ? 0.018 0.736 0.582 0.652 ? 0.012
Bigrams SVM 0.852 0.36 0.553 ? 0.006 0.58 0.8 0.68 ? 0.012
Bigrams Na??ve Bayes 0.959 0.203 0.439 ? 0.017 0.86 0.433 0.605 ? 0.024
Trigrams SVM 0.935 0.173 0.401 ? 0.018 0.407 0.851 0.588 ? 0.009
Trigrams Na??ve Bayes 0.938 0.249 0.481 ? 0.013 0.84 0.446 0.61 ? 0.011
Table 8: Selected balanced versus unbalanced cross validation accuracies (geometric mean) for dataset A, pseudo-
subjectivity task (see the preceding table for details).
A, and 88:12 for S).
Given these results, we suggest that balancing the
training datasets is usually an effective strategy, al-
though sometimes the benefits are small if account
of balancing is also part of the parameter-selection
process for your learning algorithm.
5 Conclusion and Further Work
We have empirically analysed a range of machine-
learning techniques for developing favourability
classifiers in a commercial context. These tech-
niques include different classification algorithms,
use of attribute selection to reduce the feature sets,
and treatment of the imbalanced data problem. Also,
we used five different types of feature set to create
the datasets from the raw text. We have found a wide
variation, from less than 0.7 to over 0.9 geometric
mean of accuracy, depending on the particular set
of data analysed. We have shown how balancing
the class distribution in training data can be benefi-
cial in improving performance, but some algorithms
(i.e. na??ve Bayes) can be adversely affected. In fu-
ture work we will apply these techniques to larger
volumes of social media, and further explore the
questions of balancing datasets, other features and
feature selection, as well as embedding these algo-
rithms within the workflow of the company.
51
References
A. Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,
E. Van Der Goot, M. Halkia, B. Pouliquen, and
J. Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of LREC.
A.L. Blum and P. Langley. 1997. Selection of relevant
features and examples in machine learning. Artificial
intelligence, 97:245?271.
N.V. Chawla, N. Japkowicz, and A. Kotcz. 2004. Edi-
torial: special issue on learning from imbalanced data
sets. ACM SIGKDD Explorations Newsletter, 6:1?6.
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. The Jour-
nal of Machine Learning Research, 3:1289?1305.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
Advances in Intelligent Data Analysis VI, pages 121?
132.
N. Godbole, M. Srinivasaiah, and S. Skiena. 2007.
Large-scale sentiment analysis for news and blogs. In
Proceedings of the International Conference on We-
blogs and Social Media (ICWSM).
P.D. Green, P.C.R. Lane, A.W. Rainer, and S. Scholz.
2010. Selecting measures in origin analysis. In Pro-
ceedings of AI-2010, The Thirtieth SGAI International
Conference on Innovative Techniques and Applica-
tions of Artificial Intelligence, pages 379?392.
M. Koppel and J. Schler. 2006. The importance of neu-
tral examples for learning sentiment. Computational
Intelligence, 22:100?109.
K. Krippendorff. 2004. Content analysis: An introduc-
tion to its methodology. Sage Publications, Inc.
M. Kubat, R.C. Holte, and S. Matwin. 1998. Machine
learning for the detection of oil spills in satellite radar
images. Machine Learning, 30:195?215.
P.C.R. Lane, C. Lyon, and J.A. Malcolm. 2006. Demon-
stration of the Ferret plagiarism detector. In Proceed-
ings of the 2nd International Plagiarism Conference.
T. Li, V. Sindhwani, C. Ding, and Y. Zhang. 2009.
Knowledge transformation for cross-domain senti-
ment classification. In Proceedings of the 32nd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 716?717.
ACM.
P. Melville, W. Gryc, and R. D. Lawrence. 2009.
Sentiment analysis of blogs by combining lexical
knowledge with text classification. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 1275?1284, New York, NY, USA. ACM.
D. Mladenic?. 1998. Feature subset selection in text-
learning. Machine Learning: ECML-98, pages 95?
100.
T. Mullen and N. Collier. 2004. Sentiment analysis us-
ing support vector machines with diverse information
sources. In Proceedings of EMNLP, volume 4, pages
412?418.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing-
Volume 10, pages 79?86. Association for Computa-
tional Linguistics.
R. Prabowo and M. Thelwall. 2009. Sentiment analy-
sis: A combined approach. Journal of Informetrics,
3:143?157.
M. Rogati and Y. Yang. 2002. High-performing feature
selection for text classification. In Proceedings of the
eleventh international conference on Information and
knowledge management, pages 659?661. ACM.
G. Tatzl and C. Waldhauser. 2010. Aggregating opin-
ions: Explorations into Graphs and Media Content
Analysis. ACL 2010, page 93.
P.D. Turney. 2002. Thumbs up or thumbs down?: Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 417?424. Association for Computational
Linguistics.
X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
235?243. Association for Computational Linguistics.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques. Morgan
Kaufmann.
52
