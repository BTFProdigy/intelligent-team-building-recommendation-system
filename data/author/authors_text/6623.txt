Proceedings of the ACL 2007 Demo and Poster Sessions, pages 129?132,
Prague, June 2007. c?2007 Association for Computational Linguistics
WordNet-based Semantic Relatedness Measures in Automatic Speech
Recognition for Meetings
Michael Pucher
Telecommunications Research Center Vienna
Vienna, Austria
Speech and Signal Processing Lab, TU Graz
Graz, Austria
pucher@ftw.at
Abstract
This paper presents the application of
WordNet-based semantic relatedness mea-
sures to Automatic Speech Recognition
(ASR) in multi-party meetings. Differ-
ent word-utterance context relatedness mea-
sures and utterance-coherence measures are
defined and applied to the rescoring of N -
best lists. No significant improvements
in terms of Word-Error-Rate (WER) are
achieved compared to a large word-based n-
gram baseline model. We discuss our results
and the relation to other work that achieved
an improvement with such models for sim-
pler tasks.
1 Introduction
As (Pucher, 2005) has shown different WordNet-
based measures and contexts are best for word pre-
diction in conversational speech. The JCN (Sec-
tion 2.1) measure performs best for nouns using the
noun-context. The LESK (Section 2.1) measure per-
forms best for verbs and adjectives using a mixed
word-context.
Text-based semantic relatedness measures can
improve word prediction on simulated speech recog-
nition hypotheses as (Demetriou et al, 2000) have
shown. (Demetriou et al, 2000) generated N -best
lists from phoneme confusion data acquired from
a speech recognizer, and a pronunciation lexicon.
Then sentence hypotheses of varying Word-Error-
Rate (WER) were generated based on sentences
from different genres from the British National Cor-
pus (BNC). It was shown by them that the semantic
model can improve recognition, where the amount
of improvement varies with context length and sen-
tence length. Thereby it was shown that these mod-
els can make use of long-term information.
In this paper the best performing measures
from (Pucher, 2005), which outperform baseline
models on word prediction for conversational tele-
phone speech are used for Automatic Speech Recog-
nition (ASR) in multi-party meetings. Thereby we
want to investigate if WordNet-based models can be
used for rescoring of ?real? N -best lists in a difficult
task.
1.1 Word prediction by semantic similarity
The standard n-gram approach in language mod-
eling for speech recognition cannot cope with
long-term dependencies. Therefore (Bellegarda,
2000) proposed combining n-gram language mod-
els, which are effective for predicting local de-
pendencies, with Latent Semantic Analysis (LSA)
based models for covering long-term dependencies.
WordNet-based semantic relatedness measures can
be used for word prediction using long-term depen-
dencies, as in this example from the CallHome En-
glish telephone speech corpus:
(1) B: I I well, you should see what the
bstudentsc
B: after they torture them for six byearsc in
middle bschoolc and high bschoolc they
don?t want to do anything in bcollegec
particular.
In Example 1 college can be predicted from the
noun context using semantic relatedness measures,
129
here between students and college. A 3-gram model
gives a ranking of college in the context of anything
in. An 8-gram predicts college from they don?t want
to do anything in, but the strongest predictor is stu-
dents.
1.2 Test data
The JCN and LESK measure that are defined in the
next section are used for N -best list rescoring. For
the WER experiments N -best lists generated from
the decoding of conference room meeting test data
of the NIST Rich Transcription 2005 Spring (RT-
05S) meeting evaluation (Fiscus et al, 2005) are
used. The 4-gram that has to be improved by the
WordNet-based models is trained on various corpora
from conversational telephone speech to web data
that together contain approximately 1 billion words.
2 WordNet-based semantic relatedness
measures
2.1 Basic measures
Two similarity/distance measures from the Perl
package WordNet-Similarity written by (Pedersen et
al., 2004) are used. The measures are named af-
ter their respective authors. All measures are im-
plemented as similarity measures. JCN (Jiang and
Conrath, 1997) is based on the information content,
and LESK (Banerjee and Pedersen, 2003) allows
for comparison across Part-of-Speech (POS) bound-
aries.
2.2 Word context relatedness
First the relatedness between words is defined based
on the relatedness between senses. S(w) are the
senses of word w. Definition 2 also performs word-
sense disambiguation.
rel(w,w?) = max
ci?S(w) cj?S(w?)
rel(ci, cj) (2)
The relatedness of a word and a context (relW) is
defined as the average of the relatedness of the word
and all words in the context.
relW(w,C) =
1
| C |
?
wi?C
rel(w,wi) (3)
2.3 Word utterance (context) relatedness
The performance of the word-context relatedness
(Definition 3) shows how well the measures work
for algorithms that proceed in a left-to-right manner,
since the context is restricted to words that have al-
ready been seen. For the rescoring of N -best lists
it is not necessary to proceed in a left-to-right man-
ner. The word-utterance-context relatedness can be
used for the rescoring of N -best lists. This related-
ness does not only use the context of the preceding
words, but the whole utterance.
Suppose U = ?w1, . . . , wn? is an utterance. Let
pre(wi, U) be the set
?
j<i wj and post(wi, U) be
the set
?
j>i wj . Then the word-utterance-context
relatedness is defined as
relU1(wi, U, C) =
relW(wi,pre(wi, U) ? post(wi, U) ? C) . (4)
In this case there are two types of context. The
first context comes from the respective meeting, and
the second context comes from the actual utterance.
Another definition is obtained if the context C is
eliminated (C = ?) and just the utterance context U
is taken into account.
relU2(wi, U) =
relW(wi,pre(wi, U) ? post(wi, U)) (5)
Both definitions can be modified for usage with
rescoring in a left-to-right manner by restricting the
contexts only to the preceding words.
relU3(wi, U, C) = relW(wi,pre(wi, U) ? C) (6)
relU4(wi, U) = relW(wi,pre(wi, U)) (7)
2.4 Defining utterance coherence
Using Definitions 4-7 different concepts of utterance
coherence can be defined. For rescoring the utter-
ance coherence is used, when a score for each el-
ement of an N -best list is needed. U is again an
utterance U = ?w1, . . . , wn?.
130
cohU1(U,C) =
1
| U |
?
w?U
relU1(w,U,C) (8)
The first semantic utterance coherence measure
(Definition 8) is based on all words in the utterance
as well as in the context. It takes the mean of the
relatedness of all words. It is based on the word-
utterance-context relatedness (Definition 4).
cohU2(U) =
1
| U |
?
w?U
relU2(w,U) (9)
The second coherence measure (Definition 9) is
a pure inner-utterance-coherence, which means that
no history apart from the utterance is needed. Such
a measure is very useful for rescoring, since the his-
tory is often not known or because there are speech
recognition errors in the history. It is based on Defi-
nition 5.
cohU3(U,C) =
1
| U |
?
w?U
relU3(w,U,C) (10)
The third (Definition 10) and fourth (Defini-
tion 11) definition are based on Definition 6 and 7,
that do not take future words into account.
cohU4(U) =
1
| U |
?
w?U
relU4(w,U) (11)
3 Word-error-rate (WER) experiments
For the rescoring experiments the first-best element
of the previous N -best list is added to the context.
Before applying the WordNet-based measures, the
N -best lists are POS tagged with a decision tree
tagger (Schmid, 1994). The WordNet measures are
then applied to verbs, nouns and adjectives. Then
the similarity values are used as scores, which have
to be combined with the language model scores of
the N -best list elements.
The JCN measure is used for computing a noun
score based on the noun context, and the LESK mea-
sure is used for computing a verb/adjective score
based on the noun/verb/adjective context. In the end
there is a lesk score and a jcn score for each N -best
list. The final WordNet score is the sum of the two
scores.
The log-linear interpolation method used for the
rescoring is defined as
p(S) ? pwordnet(S)
? pn-gram(S)
1?? (12)
where ? denotes normalization. Based on all Word-
Net scores of an N -best list a probability is esti-
mated, which is then interpolated with the n-gram
model probability. If only the elements in an N -
best list are considered, log-linear interpolation can
be used since it is not necessary to normalize over
all sentences. Then there is only one parameter ? to
optimize, which is done with a brute force approach.
For this optimization a small part of the test data is
taken and the WER is computed for different values
of ?.
As a baseline the n-gram mixture model trained
on all available training data (? 1 billion words) is
used. It is log-linearly interpolated with the Word-
Net probabilities. Additionally to this sophisticated
interpolation, solely the WordNet scores are used
without the n-gram scores.
3.1 WER experiments for inner-utterance
coherence
In this first group of experiments Definitions 8 and 9
are applied to the rescoring task. Similarity scores
for each element in an N -best list are derived ac-
cording to the definitions. The first-best element of
the last list is always added to the context. The con-
text size is constrained to the last 20 words. Def-
inition 8 includes context apart from the utterance
context, Definition 9 only uses the utterance context.
No improvement over the n-gram baseline is
achieved for these two measures. Neither with the
log-linearly interpolated models nor with the Word-
Net scores alone. The differences between the meth-
ods in terms of WER are not significant.
3.2 WER experiments for utterance coherence
In the second group of experiments Definitions 10
and 11 are applied to the rescoring task. There is
again one measure that uses dialog context (10) and
one that only uses utterance context (11).
Also for these experiments no improvement over
the n-gram baseline is achieved. Neither with the
131
log-linearly interpolated models nor with the Word-
Net scores alone. The differences between the meth-
ods in terms of WER are also not significant. There
are also no significant differences in performance
between the second group and the first group of ex-
periments.
4 Summary and discussion
We showed how to define more and more complex
relatedness measures on top of the basic relatedness
measures between word senses.
The LESK and JCN measures were used for the
rescoring of N -best lists. It was shown that speech
recognition of multi-party meetings cannot be im-
proved compared to a 4-gram baseline model, when
using WordNet models.
One reason for the poor performance of the mod-
els could be that the task of rescoring simulated N -
best lists, as presented in (Demetriou et al, 2000), is
significantly easier than the rescoring of ?real? N -
best lists. (Pucher, 2005) has shown that Word-
Net models can outperform simple random mod-
els on the task of word prediction, in spite of the
noise that is introduced through word-sense disam-
biguation and POS tagging. To improve the word-
sense disambiguation one could use the approach
proposed by (Basili et al, 2004).
In the above WER experiments a 4-gram baseline
model was used, which was trained on nearly 1 bil-
lion words. In (Demetriou et al, 2000) a simpler
baseline has been used. 650 sentences were used
there to generate sentence hypotheses with different
WER using phoneme confusion data and a pronun-
ciation lexicon. Experiments with simpler baseline
models ignore that these simpler models are not used
in today?s recognition systems.
We think that these prediction models can still be
useful for other tasks where only small amounts of
training data are available. Another possibility of
improvement is to use other interpolation techniques
like the maximum entropy framework. WordNet-
based models could also be improved by using a
trigger-based approach. This could be done by not
using the whole WordNet and its similarities, but
defining word-trigger pairs that are used for rescor-
ing.
5 Acknowledgements
This work was supported by the European Union 6th
FP IST Integrated Project AMI (Augmented Multi-
party Interaction, and by Kapsch Carrier-Com AG
and Mobilkom Austria AG together with the Aus-
trian competence centre programme Kplus.
References
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th Int. Joint Conf. on Artificial
Intelligence, pages 805?810, Acapulco.
Roberto Basili, Marco Cammisa, and Fabio Massimo
Zanzotto. 2004. A semantic similarity measure for
unsupervised semantic tagging. In Proc. of the Fourth
International Conference on Language Resources and
Evaluation (LREC2004), Lisbon, Portugal.
Jerome Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Process-
ing, 8(1), January.
G. Demetriou, E. Atwell, and C. Souter. 2000. Using
lexical semantic knowledge from machine readable
dictionaries for domain independent language mod-
elling. In Proc. of LREC 2000, 2nd International Con-
ference on Language Resources and Evaluation.
Jonathan G. Fiscus, Nicolas Radde, John S. Garofolo,
Audrey Le, Jerome Ajot, and Christophe Laprun.
2005. The rich transcription 2005 spring meeting
recognition evaluation. In Rich Transcription 2005
Spring Meeting Recognition Evaluation Workshop,
Edinburgh, UK.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
Ted Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the relatedness of
concepts. In Proc. of Fifth Annual Meeting of the
North American Chapter of the ACL (NAACL-04),
Boston, MA.
Michael Pucher. 2005. Performance evaluation of
WordNet-based semantic relatedness measures for
word prediction in conversational speech. In IWCS
6, Sixth International Workshop on Computational Se-
mantics, Tilburg, Netherlands.
H Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Processing,
Manchester, UK, September.
132
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 65?69,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Phone set selection for HMM-based dialect speech synthesis
Michael Pucher
Telecommunications
Research Center (FTW)
Vienna, Austria
pucher@ftw.at
Nadja Kerschhofer-Puhalo
Acoustics Research
Institute (ARI)
Vienna, Austria
nadja.kerschhofer@oeaw.ac.at
Dietmar Schabus
Telecommunications
Research Center (FTW)
Vienna, Austria
schabus@ftw.at
Abstract
This paper describes a method for selecting an
appropriate phone set in dialect speech synthe-
sis for a so far undescribed dialect by applying
hidden Markov model (HMM) based training
and clustering methods. In this pilot study we
show how a phone set derived from the pho-
netic surface can be optimized given a small
amount of dialect speech training data.
1 Introduction
In acoustic modeling for dialect speech synthesis we
are confronted with two closely related major prob-
lems1, (1) to find an appropriate phone set for syn-
thesis and (2) to design a recording script with suf-
ficient phonetic and prosodic coverage. In HMM-
based synthesis, we can use the training process of
the voices itself to analyze the used phone set and to
try to optimize it for synthesis.
2 Corpus and phone set design
Goiserian, the dialect of Bad Goisern in the most
southern part of Upper Austria, is a local dialect
of the Middle Bavarian/Southern Bavarian transition
zone. The target variety for speech synthesis de-
scribed here demonstrates the typical problems re-
lated to scarcity of data. While several varieties of
the central and northern part of Upper Austria are
quite well described, detailed descriptions of the va-
rieties in this region do not exist. Lacking a lexi-
con, a phonological description, orthographic rules
1Apart from additional problems that have to do with text
analysis, orthography, and grapheme-to-phoneme conversion.
or a transcription system, a speech corpus and an
appropriate phone set have to be created. Our cur-
rent project aims at audio-visual dialect synthesis,
which is based on a systematic description of speech
data collected from spontaneous speech, word lists
and translated sentences by 10 speakers of the same
dialect. Although it would be ideal to use con-
versational speech data for dialect speech synthe-
sis (Campbell, 2006) we decided to use a hybrid ap-
proach for our full corpus where we plan to collect
a set of prompts from conversational dialect speech,
which will be realized by the dialect speakers.
The speech data for the first preliminary study
presented here consists of 150 sentences and col-
loquial phrases spoken in Goiserian by a female
speaker who can be described as a conservative
speaker of the original basic dialect of the region.
The prompts were translated spontaneously by the
speaker from Standard German into Goiserian and
contain typical phonetic and phonological character-
istics of local Bavarian varieties in multiple occur-
rences.
3 Voice building
The data was originally recorded at 96kHz, 24 bit
and was downsampled to 16kHz, 16 bit for synthesis
and voice building. A preliminary phone set (PS1)
was created on the basis of a fine phonetic transcrip-
tion including sub-phonemic details (e.g. nasaliza-
tion of vowels before nasals ?VN?). Phones occur-
ring less than twice were substituted prior to voice
training with phonetically similar phones or repre-
sentatives of the same phoneme. This leaves us with
a set of 72 phones (see Table 1 and 2).
65
The TRA voice was trained with a HMM-
based speaker-dependent system. Given the limited
amount of training data (150 prompts) and to be able
to analyze the decision trees we only used the cur-
rent, 2 preceding, and 2 succeeding phones as fea-
tures.
HTK IPA # HTK IPA #
s s 207 t t 204
d d 179 n n 171
m m 115 k k 98
h h 84 g g 79
v v 79 f f 62
r r 61 S S 49
N n
"
42 l l 41
b b 31 ts ? 27
ng N 19 p p 17
w B 14 L l
"
12
X x 11 c c 10
RX X 9 j j 7
R R 6 ks ks 3
pf pf 3
Table 1: Consonants (27) in phone set PS1 for training
(72 phones) (Blue = not in PS2).
Based on a phonetic transcription of the training
corpus, flat-start forced alignment with HTK was
carried out. Stops are split into two parts, one for
the closure and one for plosion plus burst. Ad-
ditionally, we applied forced alignment using pro-
nunciation variants2, which is the preferred method
when building a voice for dialect synthesis using a
larger corpus (Pucher, 2010). With this method it
is not necessary to have a phonetic transcription of
the recordings. Given our small corpus, this method
produced several errors ([tsvoa] / [tsvai], [tsum] /
[tsun] etc.) which led us to use the standard align-
ment method from a transcription of the corpus. Af-
ter the transcription we had to correct severe align-
ment errors. These errors are simple to find since
several segments within the utterance are affected.
From this corpus we selected 5 prompts contain-
ing only phonemes that appear at least more than 3
times in the rest of the corpus. This leaves us with
a training corpus of 145 prompts and a 5 prompt
2In a previous project on Viennese dialect synthesis, 33% of
the lexicon entries are pronunciation variants.
HTK IPA # HTK IPA #
a a 138 aa a: 10
A 6 80 AA 6: 3
AN 6? 80 Ai 6i 3
AuN 6?u 7
e e 100 ee e: 9
ei ei 22 eiN e?i 10
E E 20 EE E: 11
EN E? 4 EiN E?i 6
i i 175 ii i: 7
iN i? 6
o o 45 oo o: 3
ou ou 4 Ou O 4
u u 20 U U 15
UN U? 3
q ? 9 qY ?Y 3
QY ?Y 4
y y 9 yy y: 3
Y Y 4
eV @ 11 aV 5 89
ai ai 24 aiN a?i 9
au au 24 ea e5 7
eaN e?5 4 ia i5 30
oa o5 16 oaN o?5 9
Oi Oi 6 oi oi 26
ua u5 21 ui ui 6
Table 2: Vowels (33) and diphtongs (12) in phone set PS1
for training (72 phones) (Blue = not in PS2, Red = not in
PS2 and PS3, green = not in PS3).
test set. For the subjective evaluation, the entire re-
synthesized corpus was used to show us how well
the used phone set covers the data.
The 145 prompts were then used for training
a speaker-dependent HMM-based synthetic voice.
Figure 1 shows the architecture of the HMM-based
speaker dependent system (Zen, 2005). For synthe-
sis we used the full-context label files of the corpus
without duration information. By that text analysis
is not necessary for synthesis. Our implicit assump-
tion is that the letter-to-sound rules and text analysis
produce exactly the string of phones from the tran-
scription. In this way we can evaluate the acoustic
modeling part separately, independently from text
analysis.
66
Excitation
parameter
extraction
Spectral
parameter
extraction
Training of MSD-HSMM
Parameter generation
from MSD-HSMMText analysis
Excitation 
generation Synthesisfilter
Speech signal
Labels
Single speaker
speech database
Training
Synthesis
Spectral parametersExcitation parameters
Labels
Context-dependent
multi-stream MSD-HSMMs
TEXT
SYNTHESIZED
SPEECH
Figure 1: HMM-based speaker dependent speech synthe-
sis system.
4 Voice analysis
To be able to analyze the decision trees we used
phone features only. The HMM-based voice con-
sists of a mel-cepstrum, duration, F0, and an aperi-
odicity model. In a first step we defined the phones
that are not used for modeling, or are used for a cer-
tain model only.
Figure 3 shows those phones that are not used for
clustering of the different models. This may be due
to their rare occurrence in the data (3-4 times) or due
to possible inconsistencies in their phonetic realiza-
tion. The F0 model is not shown since all phonemes
were used in the F0 tree in some context.
To define other possible phone sets we decided
to substitute the phones only occurring in the F0
model but not in the other 3 models, namely the
mel-cepstrum, duration, and the aperiodicity model.
We therefore merged ?Ai?, ?AuN?, ?EN?, ?ks?, ?L?,
?Ou?, ?qY?, ?yy? with their phonetically most sim-
ilar equivalents (e.g. syllabic ?L? with ?l?, ?ks?
with ?k?+?s?, or a nasalized ?EN? or ?AuN? before
nasals with the non-nasal phone) and thus obtained
a new smaller phone set (PS2), which was used for
training a second voice model.
Another possible set of phones (PS 3) is defined
by merging long (VV) and short (V) vowels of the
same quality, namely ?ii?, ?yy?, ?ee?, ?EE?, ?aa?,
?AA?, ?oo? with their short counterpart. From a lin-
guistic point of view, the phonemic status of vowel
C-sil
C-s
no
mcep_s4_1
yes
C-A
no
L-sil
yes
C-t
no
mcep_s4_2
yes
mcep_s4_35
no
mcep_s4_34
yes
C-n
no
mcep_s4_3
yes
C-m
no
mcep_s4_4
yes
C-e
no
mcep_s4_5
yes
C-a
no
L-h
yes
C-AN
no
mcep_s4_6
yes
mcep_s4_37
no
mcep_s4_36
yes
C-k
no
mcep_s4_7
yes
C-d
no
mcep_s4_8
yes
C-g
no
mcep_s4_9
yes
C-b
no
mcep_s4_10
yes
C-N
no
mcep_s4_11
yes
C-f
no
mcep_s4_12
yes
C-i
no
mcep_s4_13
yes
C-r
no
mcep_s4_14
yes
C-h
no
mcep_s4_15
yes
C-X
no
mcep_s4_16
yes
C-c
no
mcep_s4_17
yes
C-oa
no
mcep_s4_18
yes
C-ua
no
mcep_s4_19
yes
C-oaN
no
mcep_s4_20
yes
C-EE
no
mcep_s4_21
yes
C-ia
no
mcep_s4_22
yes
C-ei
no
mcep_s4_23
yes
C-ng
no
mcep_s4_24
yes
C-au
no
mcep_s4_25
yes
C-ai
no
mcep_s4_26
yes
C-aV
no
mcep_s4_27
yes
C-v
no
mcep_s4_28
yes
C-ea
no
mcep_s4_29
yes
C-aa
no
mcep_s4_30
yes
C-ee
no
mcep_s4_31
yes
C-oi
no
mcep_s4_32
yes
R-n
no
mcep_s4_33
yes
C-y
no
C-Oi
yes
C-eV
no
mcep_s4_38
yes
mcep_s4_40
no
mcep_s4_39
yes
mcep_s4_42
no
mcep_s4_41
yes
Figure 2: Part of the mel-cepstrum clustering tree for the
3rd state of the HMM.
duration as a primary feature in Austrian German
is a controversial issue. While differences in length
do exist at the phonetic surface, these differences are
not necessarily of phonemic relevance (Moosmu?ller,
2007; Scheutz, 1985). We obtain thus a third phone
set (PS3) by merging long and short vowels.
Model # #C #L #LL #R #RR
Mel-cep. 42 38 2 0 1 0
Aperiod. 36 31 0 3 0 1
F0 196 54 37 38 30 36
Duration 83 32 14 9 14 13
Table 3: Number of models and questions in mel-
cepstrum, aperiodicity, F0, and duration model for central
HMM state.
4.1 Mel-cepstrum and aperiodicity model
The mel-cepstrum model contains a separate model
for each phone that is used in the cepstral clus-
tering. In Figure 2 this is shown with the model
?mcep s4 32?, which is used in case that the cur-
rent phone is an ?ee? (C-ee) and with the model
?mcep s4 33?, which is used in case that the cur-
rent phone is an ?oi?. These two models are special
models which only cover certain phones. The only
effect of the clustering is that some phones are not
modeled separately, resulting in an unbalanced tree.
However there is one instance of context cluster-
67
MEL-CEP DURATION
APERIODICITY
aa ea ii iN ou
Ai AuN EN ks 
L Ou qY yy
AA ai aiN au
eaN ee EE ei EiN j
ng oa oaN oi Oi q 
QY R u ua w X y  
E eiN
oo pf RX
U ui UN Y
p 
Figure 3: Phones that were not used for clustering in the
trees for mel-cepstrum, duration, and aperiodicity in any
context (current, 2 preceding, and 2 succeeding phones)
and any of the 5 states.
ing in the central state of the mel-cepstrum HMMs.
If the right phone is an ?n? (R-n) there are two dif-
ferent models used (?mcep s4 39?, ?mcep s4 40?),
depending on whether the current phone is an ?Oi?
(C-Oi) or not (Figure 2).
All phones that are not modeled through a sepa-
rate model are modeled by the model at the end of
the tree (model ?mcep s4 42?).
The aperiodicity model is very similar to the mel-
cepstrum model, as can be seen in Table 3 and Fig-
ure 3.
4.2 F0 and duration model
The F0 model uses all phones as shown in Figure 3
and is the most complex model in terms of context
questions as can be seen from Table 3.
The duration model contains the lowest number of
phone related questions as shown by Figure 3 but is
still more complex than the spectrum related models
in terms of context-dependent questions as shown
in Table 3. Similarly to the F0 model, it is rather
difficult to analyze this model directly.
5 Voice evaluation
After the analysis of the voice that was trained with
our basic phoneset PS1 we defined two new phone-
sets PS2 and PS3. These phonesets were used to
train additional voice models for the same speaker.
With these voice models, we synthesized our small
set of 5 test sentences. To evaluate the suitabil-
ity of the phonesets for the training data, we re-
synthesized the training corpus of 145 prompts.
In a pair-wise comparison test of the 150 prompts
we evaluated the three voice models in a subjective
listening test with three expert listeners. The experts
listened to a set of prompts, each prompt synthesized
with two different voice models. They were asked
to compare them and to decide which prompt they
would prefer in terms of overall quality, or whether
they would rate them as ?equally good?.
PS1 PS2 PS3
56 102 105
Table 4: Number of winning comparisons per phone set
(PS1-PS3).
Table 4 illustrates that both approaches to reduce
and redefine the phoneset (PS2, PS3) improved the
overall quality estimation considerably compared to
the initial phoneset PS1.
6 Conclusion
One major challenge for speech synthesis of so far
undescribed varieties is the lack of an appropriate
phoneset and sufficient training data. We met this
challenge by deriving a phoneset directly from the
phonetic surface of a very restricted corpus of natu-
ral speech. This phone set was used for voice train-
ing. Based on the outcome of the first voice training
we reconsidered the choice of phones and created
new phone sets following 2 approaches: (1) remov-
ing phones that are not used in the clustering, and
(2) a linguistically motivated choice of phone sub-
stitutions based on clustering results. Both methods
yielded a considerable improvement of voice qual-
ity. Thus, HMM-based machine learning methods
and supervised optimization can be used for the def-
inition of the phoneset of an unkown dialect. Our
future work will elaborate this method with dialect
speech training corpora of different size to show
whether it can be applied to adaptive methods in-
volving multiple-speaker training. The considera-
tion of inter- and intra-speaker variation and style
shifting will be a crucial question for further study.
68
References
Nick Campbell. 2006. Conversational speech synthesis
and the need for some laughter. IEEE Transactions
on Speech and Audio Processing, 14(4), pages 1171-
1178.
Michael Pucher, Friedrich Neubarth, Volker Strom,
Sylvia Moosmu?ller, Gregor Hofer, Christian Kranzler,
Gudrun Schuchmann and Dietmar Schabus. 2010. Re-
sources for speech synthesis of Viennese varieties. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC), Valletta,
Malta.
Sylvia Moosmu?ller. 2007. Vowels in Standard Aus-
trian German. An Acoustic-Phonetic and Phonologi-
cal Analysis. Habilitationsschrift, Vienna.
Hannes Scheutz. 1985. Strukturen der Lautvera?nderung.
Braumu?ller, Vienna.
Heiga Zen and Tomoki Toda. 2005. An Overview
of Nitech HMM-based Speech Synthesis System for
Blizzard Challenge 2005. In Proceedings of the 9th
European Conference on Speech Communication and
Technology (INTERSPEECH), Lisboa, Portugal.
69
