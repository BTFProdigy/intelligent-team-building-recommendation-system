Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 84?89,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Unsupervised Recognition of Dialogue Acts
Nicole Novielli
Dept. of Informatics, University of Bari
via Orabona 4
I-70125 Bari, Italy
novielli@di.uniba.it
Carlo Strapparava
FBK-irst
via Sommarive, Povo
I-38050 Trento, Italy
strappa@fbk.eu
Abstract
When engaged in dialogues, people per-
form communicative actions to pursue specific
communicative goals. Speech acts recogni-
tion attracted computational linguistics since
long time and could impact considerably a
huge variety of application domains. We study
the task of automatic labeling dialogues with
the proper dialogue acts, relying on empiri-
cal methods and simply exploiting lexical se-
mantics of the utterances. In particular, we
present some experiments in supervised and
unsupervised framework on both an English
and an Italian corpus of dialogue transcrip-
tions. The evaluation displays encouraging re-
sults in both languages, especially in the unsu-
pervised version of the methodology.
1 Introduction
People proceed in their conversations through a se-
quence of dialogue acts to yield some specific com-
municative goal. They can ask for information,
agree or disagree with their partner, state some facts
and express opinions.
Dialogue Acts (DA) attracted linguistics (Austin,
1962; Searle, 1969) and computational linguistics
research (Core and Allen, 1997; Traum, 2000) since
long time. With the advent of the Web, a large
amount of material about natural language inter-
actions (e.g. blogs, chats, conversation transcripts)
has become available, raising the attractiveness of
empirical methods analyses on this field. There is
a large number of application domains that could
benefit from automatically labeling DAs: e.g. con-
versational agents for monitoring and supporting
human-human remote conversations, blogs, forums
and chat logs analysis for opinion mining, interper-
sonal stances modeling by mean of conversational
analysis, automatic meeting summarizations and so
on. These applications require a deep understanding
of the conversational structure and the ability of the
system to understand who is telling what to whom.
This study defines a method for automatically la-
beling dialogues with the proper speech acts by re-
lying on empirical methods. Even if prosody and
intonation surely play a role (e.g. (Stolcke et al,
2000; Warnke et al, 1997)), nonetheless language
and words are what the speaker uses to convey the
communicative message and are just what we have
at disposal when we consider texts found on the
Web. Hence, we decided to simply exploit lexical
semantics of the sentences. We performed some ex-
periments in a supervised and unsupervised frame-
work on both an English and an Italian corpora of
dialogue transcriptions, achieving good results in all
settings. Unsupervised performance is particularly
encouraging, independently from the used language.
The paper is organized as follows. Section 2 gives
a brief sketch of the NLP background on Dialogue
Acts recognition. In Section 3 we introduce the En-
glish and Italian corpora of dialogues, their charac-
teristics and DA labeling. In Section 4 we describe
the preprocessing of the data sets. Then Section 5
explains the supervised and unsupervised settings,
showing the experimental results obtained on the
two corpora and providing an error analysis. Finally,
in Section 6 we conclude the paper with a brief dis-
cussion and some directions for future work.
84
Speaker Dialogue Act Utterance
A OPENING Hello Ann.
B OPENING Hello Chuck.
A STATEMENT Uh, the other day, I attended a conference here at Utah State University on recycling
A STATEMENT and, uh, I was kind of interested to hear cause they had some people from the EPA and
lots of different places, and, uh, there is going to be a real problem on solid waste.
B OPINION Uh, I didn?t think that was a new revelation.
A AGREE /ACCEPT Well, it?s not too new.
B INFO-REQUEST So what is the EPA recommending now?
Table 1: An excerpt from the Switchboard corpus
2 Background
A DA can be identified with the communicative goal
of a given utterance (Austin, 1962). Researchers use
different labels and definitions to address this con-
cept: speech act (Searle, 1969), adjacency pair part
(Schegloff, 1968) (Sacks et al, 1974), game move
(Power, 1979)
Traditionally, the NLP community has employed
DA definitions with the drawback of being do-
main or application oriented. Recently some efforts
have been made towards unifying the DA annotation
(Traum, 2000). In the present study we refer to a
domain-independent framework for DA annotation,
the DAMSL architecture (Dialogue Act Markup in
Several Layers) by (Core and Allen, 1997).
Recently, the problem of DA recognition has
been addressed with promising results: Poesio and
Mikheev (1998) combine expectations about the
next likely dialogue ?move? with information de-
rived from the speech signal features; Stolcke et
al. (2000) employ a discourse grammar, formal-
ized in terms of Hidden Markov Models, combining
also evidences about lexicon and prosody; Keizer et
al. (2002) make use of Bayesian networks for DA
recognition in dutch dialogues; Grau et al (2004)
consider naive Bayes classifiers as a suitable ap-
proach to the DA classification problem; a partially
supervised framework has also been explored by
Venkataraman et al (2005)
Regardless of the model they use (discourse
grammars, models based on word sequences or on
the acoustic features or a combination of all these)
the mentioned studies are developed in a supervised
framework. In this paper, one goal is to explore also
the use of a fully unsupervised methodology.
3 Data Sets
In the experiments of the present paper we exploit
two corpora, both annotated with DAs labels. We
aim at developing a recognition methodology as
general as possible, so we selected corpora which
are different in content and language: the Switch-
board corpus (Godfrey et al, 1992), a collection
of transcriptions of spoken English telephone con-
versations about general interest topics, and an Ital-
ian corpus of dialogues in the healthy-eating domain
(Clarizio et al, 2006).
In this section we describe the two corpora, their
features, the set of labels used for annotating the di-
alogue acts with their distributions and the data pre-
processing.
3.1 Description
The Switchboard corpus is a collection of English
human-human telephone conversations (Godfrey et
al., 1992) between couples of randomly selected
strangers. They were asked to choose one general
interest topic and to talk informally about it. Full
transcripts of these dialogues are distributed by the
Linguistic Data Consortium. A part of this cor-
pus is annotated (Jurafsky et al, 1997) with DA
labels (overall 1155 conversations, for a total of
205,000 utterances and 1.4 million words)1. Table
1 shows a short sample fragments of dialogues from
the Switchboard corpus.
The Italian corpus had been collected in the scope
of some previous research about Human-ECA inter-
action. A Wizard of Oz tool was employed (Clarizio
et al, 2006) and during the interaction, a conver-
sational agent (i.e. the ?wizard?) played the role of
1ftp.ldc.upenn.edu/pub/ldc/public\_data/
swb1\_dialogact\_annot.tar.gz
85
Label Description Example Italian English
INFO-REQUEST Utterances that are pragmatically, semantically,
and syntactically questions
?What did you do when your kids
were growing up??
34% 7%
STATEMENT Descriptive, narrative, personal statements ?I usually eat a lot of fruit? 37% 57%
S-OPINION Directed opinion statements ?I think he deserves it.? 6% 20%
AGREE-ACCEPT Acceptance of a proposal, plan or opinion ?That?s right? 5% 9%
REJECT Disagreement with a proposal, plan, or opinion ?I?m sorry no? 7% .3%
OPENING Dialogue opening or self-introduction ?Hello, my name is Imma? 2% .2%
CLOSING Dialogue closing (e.g. farewell and wishes) ?It?s been nice talking to you.? 2% 2%
KIND-ATT Kind attitude (e.g. thanking and apology) ?Thank you very much.? 9% .1%
GEN-ANS Generic answers to an Info-Request ?Yes?, ?No?, ?I don?t know? 4% 4%
total cases 1448 131,265
Table 2: The set of labels employed for Dialogue Acts annotation and their distribution in the two corpora
an artificial therapist. The users were free to inter-
act with it in natural language, without any partic-
ular constraint. This corpus is about healthy eating
and contains (overall 60 dialogues, 1448 users? ut-
terances and 15,500 words).
3.2 Labelling
Both corpora are annotated following the Dialogue
Act Markup in Several Layers (DAMSL) annotation
scheme (Core and Allen, 1997). In particular the
Switchboard corpus employs a revision (Jurafsky et
al., 1997).2
Table 2 shows the set of labels employed with
their definitions, examples and distributions in the
two data sets. The categories maintain the DAMSL
main characteristic of being domain-independent
and can be easily mapped back into SWBD-DAMSL
ones, and maintain their original semantics. Thus,
the original SWBD-DAMSL annotation had been
automatically converted into the categories included
in our markup language.3
4 Data preprocessing
To reduce the data sparseness, we used a POS-tagger
and morphological analyzer (Pianta et al, 2008) for
preprocessing both corpora. So we considered lem-
mata instead of tokens in the format lemma#POS. In
addition, we augment the features of each sentence
with a set of linguistic markers, defined according to
2The SWBD-DAMSL modifies the original DAMSL frame-
work by further specifying some categories or by adding extra
features (mainly prosodic) which were not originally included
in the scheme.
3Also we did not consider the utterances formed only by
non-verbal material (e.g. laughter).
the semantic of the DA categories. We hypothesize,
in fact, these features could play an important role
in defining the linguistic profile of each DA. The ad-
dition of these markers is performed automatically,
by just exploiting the output of the POS-tagger and
of the morphological analyzer, according to the fol-
lowing rules:
? WH-QTN, used whenever an interrogative de-
terminer (e.g. ?what?) is found, according to the
output of the POS-tagger;
? ASK-IF, used whenever an utterance presents
the pattern of a ?Yes/No? question. ASK-IF and
WH-QTN markers are supposed to be relevant
for the INFO-REQUEST category;
? I-PERS, used for all declarative utterances
whenever a verb is in the first person form, sin-
gular or plural (relevant for the STATEMENT);
? COND, used for conditional form is detected.
? SUPER, used for superlative adjectives.
? AGR-EX, used whenever an agreement ex-
pression (e.g.?You?re right?, ?I agree?) is de-
tected (relevant for AGREE-ACCEPT);
? NAME, used whenever a proper name follows
a self-introduction expression (e.g. ?My name
is?) (relevant for the OPENING);
? OR-CLAUSE, used for or-clauses, that is ut-
terance starting by ?or? (should be helpful for
the characterization of the INFO-REQUEST);
? VB, used only for the Italian, when a dialectal
form of agreement expression is detected.
5 Dialogue Acts Recognition
We conducted some experiments both in a super-
vised and unsupervised settings.
86
5.1 Supervised
Regarding the supervised experiments, we used
Support Vector Machines (Vapnik, 1995), in partic-
ular SVM-light package (Joachims, 1998) under its
default configuration. We randomly split the two
corpora into 80/20 training/test partitions. SVMs
have been used in a large range of problems, in-
cluding text classification, image recognition tasks,
bioinformatics and medical applications, and they
are regarded as the state-of-the-art in supervised
learning. We got .71 and .77 of F1 measures respec-
tively for the Italian and English corpus. Table 4
reports the performance for each direct act.
5.2 Unsupervised
It is not always easy to collect large training, partly
because of manual labeling effort and moreover be-
cause often it is not possible to find it.
Schematically, our unsupervised methodology is:
(i) building a semantic similarity space in which
words, set of words, text fragments can be repre-
sented homogeneously, (ii) finding seeds that prop-
erly represent dialogue acts and considering their
representations in the similarity space, and (iii)
checking the similarity of the utterances.
To get a similarity space with the required charac-
teristics, we used Latent Semantic Analysis (LSA),
a corpus-based measure of semantic similarity pro-
posed by Landauer (Landauer et al, 1998). In LSA,
term co-occurrences in a corpus are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-by-
document matrix T representing the corpus.
SVD decomposes the term-by-document matrix
T into three matrices T = U?kVT where ?k is
the diagonal k ? k matrix containing the k singu-
lar values of T, ?1 ? ?2 ? . . . ? ?k, and U
and V are column-orthogonal matrices. When the
three matrices are multiplied together the original
term-by-document matrix is re-composed. Typically
we can choose k?  k obtaining the approximation
T ' U?k?VT .
LSA can be viewed as a way to overcome some
of the drawbacks of the standard vector space model
(sparseness and high dimensionality). In fact, the
LSA similarity is computed in a lower dimensional
space, in which second-order relations among terms
and texts are exploited. The similarity in the result-
ing vector space is then measured with the standard
cosine similarity. Note also that LSA yields a vec-
tor space model that allows for a homogeneous rep-
resentation (and hence comparison) of words, sen-
tences, and texts. For representing a word set or
a sentence in the LSA space we use the pseudo-
document representation technique, as described by
Berry (1992). In practice, each text segment is repre-
sented in the LSA space by summing up the normal-
ized LSA vectors of all the constituent words, using
also a tf.idf weighting scheme (Gliozzo and Strappa-
rava, 2005).
Label Seeds
INFO-REQ WH-QTN, Question Mark, ASK-IF, huh
STATEMENT I-PERS, I
S-OPINION Verbs which directly express opinion or
evaluation (guess, think, suppose, affect)
AGREE-ACC AGR-EX, yep, yeah, absolutely, correct
REJECT Verbs which directly express disagreement
(disagree, refute)
OPENING Greetings (hi, hello), words and markers re-
lated to self-introduction (name, NAME)
CLOSING Interjections/exclamations ending dis-
course (alright, okeydoke), Expressions
of thanking (thank) and farewell (bye,
bye-bye, goodnight, goodbye)
KIND-ATT Wishes (wish), apologies (apologize),
thanking (thank) and sorry-for (sorry,
excuse)
GEN-ANS no, yes, uh-huh, nope
Table 3: The seeds for the unsupervised experiment
The methodology is completely unsupervised.
We run the LSA using 400 dimensions (i.e. k?, as
suggested by (Landauer et al, 1998)) respectively
on the English and Italian corpus, without any DA
label information. Starting from a set of seeds
(words) representing the communicative acts (see
the complete sets in Table 3), we build the corre-
sponding vectors in the LSA space and then we com-
pare the utterances to find the communicative act
with higher similarity. To compare with SVM, the
performance is measured on the same test set parti-
tion used in the supervised experiment (Table 4).
We defined seeds by only considering the commu-
nicative goal and the specific semantic of every sin-
gle DA, just avoiding as much as possible the over-
lapping between seeds groups. We wanted to design
87
Italian English
SVM LSA SVM LSA
Label prec rec f1 prec rec f1 prec rec f1 prec rec f1
INFO-REQ .92 .99 .95 .96 .88 .92 .92 .84 .88 .93 .70 .80
STATEMENT .85 .68 .69 .76 .66 .71 .79 .92 .85 .70 .95 .81
S-OPINION .28 .42 .33 .24 .42 .30 .66 .44 .53 .41 .07 .12
AGREE-ACC .50 .80 .62 .56 .50 .53 .69 .74 .71 .68 .63 .65
REJECT - - - .09 .25 .13 - - - .01 .01 .01
OPENING .60 1.00 .75 .55 1.00 .71 .96 .55 .70 .20 .43 .27
CLOSING .67 .40 .50 .25 .40 .31 .83 .59 .69 .76 .34 .47
KIND-ATT .82 .53 .64 .43 .18 .25 .85 .34 .49 .09 .47 .15
GEN-ANS .20 .63 .30 .27 .38 .32 .56 .25 .35 .54 .33 .41
micro .71 .71 .71 .66 .66 .66 .77 .77 .77 .69 .69 .69
Table 4: Evaluation of the two methods on both corpora
an approach which is as general as possible, so we
did not consider domain words. The seeds are the
same for both languages, which is coherent with our
goal of defining a language-independent method.
5.3 Experimental Results and Discussion
We evaluate the performance of our method in terms
of precision, recall and f1-measure (see Table 4) ac-
cording to the DA labels given by annotators in the
datasets. As baselines we consider (i) most-frequent
label assignment (respectively 37% for Italian, 57%
for English) for the supervised setting, and (ii) ran-
dom DA selection (11%) for the unsupervised one.
Results are quite satisfying (Table 4). In particu-
lar, the unsupervised technique is largely above the
baselines, for both the Italian and the English exper-
iments. The methodology is independent from the
language and the domain: the Italian corpus is a col-
lection of dialogue about a very restricted domain
while the Switchboard conversations are essentially
task-free. Moreover, in the unsupervised setting we
use in practice the same seed definitions. Secondly,
it is independent on the differences in the linguis-
tic style due to the specific interaction scenario and
input modality. Finally, the performance is not af-
fected by the difference in size of the two data sets.
Error analysis. After conducting an error analy-
sis, we noted that many utterances are misclassi-
fied as STATEMENT. One possible reason is that
statements usually are quite long and there is a high
chance that some linguistic markers that character-
ize other dialogue acts are present in those sen-
tences. On the other hand, looking at the corpora we
observed that many utterances which appear to be
linguistically consistent with the typical structure of
statements have been annotated differently, accord-
ing to the actual communicative role they play. For
similar reasons, we observed some misclassifica-
tion of S-OPINION as STATEMENT. The only sig-
nificative difference between the two labels seems
to be the wider usage of ?slanted? and affectively
loaded lexicon when conveying an opinion. Another
cause of confounding is the confusion among the
backchannel labels (GEN-ANS, AGREE-ACC and
REJECT) due to the inherent ambiguity of common
words like yes, no, yeah, ok.
Recognition of such cases could be improved (i)
by enabling the classifiers to consider not only the
lexical semantics of the given utterance (local con-
text) but also the knowledge about a wider context
window (e.g. the previous n utterances), (ii) by en-
riching the data preprocessing (e.g. by exploiting in-
formation about lexicon polarity and subjectivity pa-
rameters). We intend to follow both these directions
in our future research.
6 Conclusions and Future Work
This study aims at defining a method for Dialogue
Acts recognition by simply exploiting the lexical se-
mantics of dialogue turns. The technique had to
be independent from some important features of the
corpus being used such as domain, language, size,
interaction scenario. In a long-term perspective, we
will employ the technique in conversational analysis
for user attitude classification (Martalo et al, 2008).
The methodology starts with automatically en-
88
riching the corpus with additional features, such as
linguistic markers. Then the unsupervised case con-
sists of defining a very simple and intuitive set of
seeds that profiles the specific dialogue acts, and
subsequently performing a similarity analysis in a
latent semantic space. The performance of the unsu-
pervised experiment has been compared with a su-
pervised state-of-art technique such as Support Vec-
tor Machines, and the results are quite encouraging.
Regarding future developments, we will investi-
gate how to include in the framework a wider con-
text (e.g. the previous n utterances), and the intro-
duction of new linguistic markers by enriching the
preprocessing techniques. In particular, it would be
interesting to exploit the role of slanted or affective-
loaded lexicon to deal with the misclassification of
opinions as statements. Along this perspective, DA
recognition could serve also as a basis for conver-
sational analysis aimed at improving a fine-grained
opinion mining in dialogues.
References
J. Austin. 1962. How to do Things with Words. Oxford
University Press, New York.
M. Berry. 1992. Large-scale sparse singular value com-
putations. International Journal of Supercomputer
Applications, 6(1).
G. Clarizio, I. Mazzotta, N. Novielli, and F. deRosis.
2006. Social attitude towards a conversational char-
acter. In Proceedings of the 15th IEEE International
Symposium on Robot and Human Interactive Commu-
nication, pages 2?7, Hatfield, UK, September.
M. Core and J. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In Working Notes of the
AAAI Fall Symposium on Communicative Action in
Humans and Machines, Cambridge, MA.
A. Gliozzo and C. Strapparava. 2005. Domains kernels
for text categorization. In Proceedengs of (CoNLL-
2005), University of Michigan, Ann Arbor, June.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP-
92, pages 517?520, San Francisco, CA. IEEE.
S. Grau, E. Sanchis, M. J. Castro, and D. Vilar. 2004. Di-
alogue act classification using a bayesian approach. In
Proceedings of SPECOM-04, pages 495?499, Saint-
Petersburg, Russia, September.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with many relevant fea-
tures. In Proceedings of the European Conference on
Machine Learning.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL shallow-discourse-function an-
notation coders manual, draft 13. Technical Report
97-01, University of Colorado.
S. Keizer, R. op den Akker, and A. Nijholt. 2002. Dia-
logue act recognition with bayesian networks for dutch
dialogues. In K. Jokinen and S. McRoy, editors, Pro-
ceedings 3rd SIGdial Workshop on Discourse and Di-
alogue, pages 88?94, Philadelphia, PA, July.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
A. Martalo, N. Novielli, and F. deRosis. 2008. Attitude
display in dialogue patterns. In AISB 2008 Conven-
tion on Communication, Interaction and Social Intel-
ligence, Aberdeen, Scotland, April.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In Proceedings of LREC, Marrakech (Mo-
rocco), May.
M. Poesio and A. Mikheev. 1998. The predictive power
of game structure in dialogue act recognition: Experi-
mental results using maximum entropy estimation. In
Proceedings of ICSLP-98, Sydney, December.
R. Power. 1979. The organisation of purposeful dia-
logues. Linguistics, 17:107?152.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organization of turn-taking for
conversation. Language, 50(4):696?735.
E. Schegloff. 1968. Sequencing in conversational open-
ings. American Anthropologist, 70:1075?1095.
J. Searle. 1969. Speech Acts: An Essay in the Philoso-
phy of Language. Cambridge University Press, Cam-
bridge, London.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
D. Traum. 2000. 20 questions for dialogue act tax-
onomies. Journal of Semantics, 17(1):7?30.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer-Verlag.
A. Venkataraman, Y. Liu, E. Shriberg, and A. Stol-
cke. 2005. Does active learning help automatic dia-
log act tagging in meeting data? In Proceedings of
EUROSPEECH-05, Lisbon, Portugal.
V. Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated dialog act segmentation and classification
using prosodic features and language models. In Pro-
ceedings of 5th European Conference on Speech Com-
munication and Technology, volume 1, pages 207?
210, Rhodes, Greece.
89
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309?312,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
The Lie Detector: Explorations in the Automatic Recognition
of Deceptive Language
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Carlo Strapparava
FBK-IRST
strappa@fbk.eu
Abstract
In this paper, we present initial experi-
ments in the recognition of deceptive lan-
guage. We introduce three data sets of true
and lying texts collected for this purpose,
and we show that automatic classification
is a viable technique to distinguish be-
tween truth and falsehood as expressed in
language. We also introduce a method for
class-based feature analysis, which sheds
some light on the features that are charac-
teristic for deceptive text.
You should not trust the devil, even if he tells the truth.
? Thomas of Aquin (medieval philosopher)
1 Introduction and Motivation
The discrimination between truth and falsehood
has received significant attention from fields as
diverse as philosophy, psychology and sociology.
Recent advances in computational linguistics mo-
tivate us to approach the recognition of deceptive
language from a data-driven perspective, and at-
tempt to identify the salient features of lying texts
using natural language processing techniques.
In this paper, we explore the applicability of
computational approaches to the recognition of
deceptive language. In particular, we investigate
whether automatic classification techniques repre-
sent a viable approach to distinguish between truth
and lies as expressed in written text. Although
acoustic and other non-linguistic features were
also found to be useful for this task (Hirschberg
et al, 2005), we deliberately focus on written lan-
guage, since it represents the type of data most fre-
quently encountered on the Web (e.g., chats, fo-
rums) or in other collections of documents.
Specifically, we try to answer the following two
questions. First, are truthful and lying texts sep-
arable, and does this property hold for different
datasets? To answer this question, we use three
different data sets that we construct for this pur-
pose ? consisting of true and false short statements
on three different topics ? and attempt to automat-
ically separate them using standard natural lan-
guage processing techniques.
Second, if truth and lies are separable, what are
the distinctive features of deceptive texts? In an-
swer to this second question, we attempt to iden-
tify some of the most salient features of lying texts,
and analyse their occurrence in the three data sets.
The paper is organized as follows. We first
briefly review the related work, followed by a de-
scription of the three data sets that we constructed.
Next, we present our experiments and results using
automatic classification, and introduce a method
for the analysis of salient features in deceptive
texts. Lastly, we conclude with a discussion and
directions for future work.
2 Related Work
Very little work, if any, has been carried out on the
automatic detection of deceptive language in writ-
ten text. Most of the previous work has focused
on the psychological or social aspects of lying, and
there are only a few previous studies that have con-
sidered the linguistic aspects of falsehood.
In psychology, it is worthwhile mentioning the
study reported in (DePaulo et al, 2003), where
more than 100 cues to deception are mentioned.
However, only a few of them are linguistic in na-
ture, as e.g., word and phrase repetitions, while
most of the cues involve speaker?s behavior, in-
cluding facial expressions, eye shifts, etc. (New-
man et al, 2003) also report on a psycholinguistic
study, where they conduct a qualitative analysis of
true and false stories by using word counting tools.
Computational work includes the study of
(Zhou et al, 2004), which studied linguistic cues
for deception detection in the context of text-based
asynchronous computer mediated communication,
and (Hirschberg et al, 2005) who focused on de-
ception in speech using primarily acoustic and
prosodic features.
Our work is also related to the automatic clas-
sification of text genre, including work on author
profiling (Koppel et al, 2002), humor recognition
309
TRUTH LIE
ABORTION
I believe abortion is not an option. Once a life has been
conceived, it is precious. No one has the right to decide
to end it. Life begins at conception,because without con-
ception, there is no life.
A woman has free will and free choice over what goes
on in her body. If the child has not been born, it is under
her control. Often the circumstances an unwanted child
is born into are worse than death. The mother has the
responsibility to choose the best course for her child.
DEATH PENALTY
I stand against death penalty. It is pompous of anyone
to think that they have the right to take life. No court of
law can eliminate all possibilities of doubt. Also, some
circumstances may have pushed a person to commit a
crime that would otherwise merit severe punishment.
Death penalty is very important as a deterrent against
crime. We live in a society, not as individuals. This
imposes some restrictions on our actions. If a person
doesn?t adhere to these restrictions, he or she forfeits her
life. Why should taxpayers? money be spent on feeding
murderers?
BEST FRIEND
I have been best friends with Jessica for about seven
years now. She has always been there to help me out.
She was even in the delivery room with me when I had
my daughter. She was also one of the Bridesmaids in
my wedding. She lives six hours away, but if we need
each other we?ll make the drive without even thinking.
I have been friends with Pam for almost four years now.
She?s the sweetest person I know. Whenever we need
help she?s always there to lend a hand. She always has
a kind word to say and has a warm heart. She is my
inspiration.
Table 1: Sample true and deceptive statements
(Mihalcea and Strapparava, 2006), and others.
3 Data Sets
To study the distinction between true and decep-
tive statements, we required a corpus with explicit
labeling of the truth value associated with each
statement. Since we were not aware of any such
data set, we had to create one ourselves. We fo-
cused on three different topics: opinions on abor-
tion, opinions on death penalty, and feelings about
the best friend. For each of these three topics
an annotation task was defined using the Amazon
Mechanical Turk service.
For the first two topics (abortion and death
penalty), we provided instructions that asked the
contributors to imagine they were taking part in
a debate, and had 10-15 minutes available to ex-
press their opinion about the topic. First, they were
asked to prepare a brief speech expressing their
true opinion on the topic. Next, they were asked
to prepare a second brief speech expressing the op-
posite of their opinion, thus lying about their true
beliefs about the topic. In both cases, the guide-
lines asked for at least 4-5 sentences and as many
details as possible.
For the third topic (best friend), the contributors
were first asked to think about their best friend and
describe the reasons for their friendship (including
facts and anecdotes considered relevant for their
relationship). Thus, in this case, they were asked
to tell the truth about how they felt about their best
friend. Next, they were asked to think about a per-
son they could not stand, and describe it as if s/he
were their best friend. In this second case, they
had to lie about their feelings toward this person.
As before, in both cases the instructions asked for
at least 4-5 detailed sentences.
We collected 100 true and 100 false statements
for each topic, with an average of 85 words per
statement. Previous work has shown that data
collected through the Mechanical Turk service is
reliable and comparable in quality with trusted
sources (Snow et al, 2008). We also made a man-
ual verification of the quality of the contributions,
and checked by hand the quality of all the contri-
butions. With two exceptions ? two entries where
the true and false statements were identical, which
were removed from the data ? all the other entries
were found to be of good quality, and closely fol-
lowing our instructions.
Table 1 shows an example of true and deceptive
language for each of the three topics.
4 Experimental Setup and Results
For the experiments, we used two classifiers:
Na??ve Bayes and SVM, selected based on their
performance and diversity of learning methodolo-
gies. Only minimal preprocessing was applied
to the three data sets, which included tokeniza-
tion and stemming. No feature selection was per-
formed, and stopwords were not removed.
Table 2 shows the ten-fold cross-validation re-
sults using the two classifiers. Since all three data
sets have an equal distribution between true and
false statements, the baseline for all the topics is
50%. The average classification performance of
70% ? significantly higher than the 50% baseline
? indicates that good separation can be obtained
310
between true and deceptive language by using au-
tomatic classifiers.
Topic NB SVM
ABORTION 70.0% 67.5%
DEATH PENALTY 67.4% 65.9%
BEST FRIEND 75.0% 77.0%
AVERAGE 70.8% 70.1%
Table 2: Ten-fold cross-validation classification
results, using a Na??ve Bayes (NB) or Support Vec-
tor Machines (SVM) classifier
To gain further insight into the variation of ac-
curacy with the amount of data available, we also
plotted the learning curves for each of the data
sets, as shown in Figure 1. The overall growing
trend indicates that more data is likely to improve
the accuracy, thus suggesting the collection of ad-
ditional data as a possible step for future work.
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ic
at
io
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Abortion
Death penalty
Best friend
Figure 1: Classification learning curves.
We also tested the portability of the classifiers
across topics, using two topics as training data and
the third topic as test. The results are shown in Ta-
ble 3. Although below the in-topic performance,
the average accuracy is still significantly higher
than the 50% baseline, indicating that the learning
process relies on clues specific to truth/deception,
and it is not bound to a particular topic.
5 Identifying Dominant Word Classes in
Deceptive Text
In order to gain a better understanding of the char-
acteristics of deceptive text, we devised a method
to calculate a score associated with a given class
of words, as a measure of saliency for the given
word class inside the collection of deceptive (or
truthful) texts.
Given a class of words C = {W1,W2, ...,WN},
we define the class coverage in the deceptive cor-
pus D as the percentage of words from D belong-
ing to the class C:
CoverageD(C) =
?
Wi?C
FrequencyD(Wi)
SizeD
where FrequencyD(Wi) represents the total
number of occurrences of word Wi inside the cor-
pus D, and SizeD represents the total size (in
words) of the corpus D.
Similarly, we define the class C coverage for the
truthful corpus T :
CoverageT (C) =
?
Wi?C
FrequencyT (Wi)
SizeT
The dominance score of the class C in the de-
ceptive corpus D is then defined as the ratio be-
tween the coverage of the class in the corpus D
with respect to the coverage of the same class in
the corpus T :
DominanceD(C) =
CoverageD(C)
CoverageT (C)
(1)
A dominance score close to 1 indicates a similar
distribution of the words in the class C in both the
deceptive and the truthful corpus. Instead, a score
significantly higher than 1 indicates a class that is
dominant in the deceptive corpus, and thus likely
to be a characteristic of the texts in this corpus.
Finally, a score significantly lower than 1 indicates
a class that is dominant in the truthful corpus, and
unlikely to appear in the deceptive corpus.
We use the classes of words as defined in
the Linguistic Inquiry and Word Count (LIWC),
which was developed as a resource for psycholin-
guistic analysis (Pennebaker and Francis, 1999).
The 2001 version of LIWC includes about 2,200
words and word stems grouped into about 70
broad categories relevant to psychological pro-
cesses (e.g., EMOTION, COGNITION). The LIWC
lexicon has been validated by showing significant
correlation between human ratings of a large num-
ber of written texts and the rating obtained through
LIWC-based analyses of the same texts.
All the word classes from LIWC are ranked ac-
cording to the dominance score calculated with
formula 1, using a mix of all three data sets to
create the D and T corpora. Those classes that
have a high score are the classes that are dom-
inant in deceptive text. The classes that have a
small score are the classes that are dominant in
truthful text and lack from deceptive text. Table 4
shows the top ranked classes along with their dom-
inance score and a few sample words that belong
to the given class and also appeared in the decep-
tive (truthful) texts.
Interestingly, in both truthful and deceptive lan-
guage, three of the top five dominant classes are
related to humans. In deceptive texts however, the
311
Training Test NB SVM
DEATH PENALTY + BEST FRIEND ABORTION 62.0% 61.0%
ABORTION + BEST FRIEND DEATH PENALTY 58.7% 58.7%
ABORTION + DEATH PENALTY BEST FRIEND 58.7% 53.6%
AVERAGE 59.8% 57.8%
Table 3: Cross-topic classification results
Class Score Sample words
Deceptive Text
METAPH 1.71 god, die, sacred, mercy, sin, dead, hell, soul, lord, sins
YOU 1.53 you, thou
OTHER 1.47 she, her, they, his, them, him, herself, himself, themselves
HUMANS 1.31 person, child, human, baby, man, girl, humans, individual, male, person, adult
CERTAIN 1.24 always, all, very, truly, completely, totally
Truthful Text
OPTIM 0.57 best, ready, hope, accepts, accept, determined, accepted, won, super
I 0.59 I, myself, mine
FRIENDS 0.63 friend, companion, body
SELF 0.64 our, myself, mine, ours
INSIGHT 0.65 believe, think, know, see, understand, found, thought, feels, admit
Table 4: Dominant word classes in deceptive text, along with sample words.
human-related word classes (YOU, OTHER, HU-
MANS) represent detachment from the self, as if
trying not to have the own self involved in the
lies. Instead, the classes of words that are closely
connected to the self (I, FRIENDS, SELF) are lack-
ing from deceptive text, being dominant instead in
truthful statements, where the speaker is comfort-
able with identifying herself with the statements
she makes.
Also interesting is the fact that words related
to certainty (CERTAIN) are more dominant in de-
ceptive texts, which is probably explained by the
need of the speaker to explicitly use truth-related
words as a means to emphasize the (fake) ?truth?
and thus hide the lies. Instead, belief-oriented vo-
cabulary (INSIGHT), such as believe, feel, think,
is more frequently encountered in truthful state-
ments, where the presence of the real truth does
not require truth-related words for emphasis.
6 Conclusions
In this paper, we explored automatic techniques
for the recognition of deceptive language in writ-
ten texts. Through experiments carried out on
three data sets, we showed that truthful and ly-
ing texts are separable, and this property holds
for different data sets. An analysis of classes of
salient features indicated some interesting patterns
of word usage in deceptive texts, including detach-
ment from the self and vocabulary that emphasizes
certainty. In future work, we plan to explore the
role played by affect and the possible integration
of automatic emotion analysis into the recognition
of deceptive language.
References
B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,
K. Charlton, and H. Cooper. 2003. Cues to decep-
tion. Psychological Bulletin, 129(1):74?118.
J. Hirschberg, S. Benus, J. Brenier, F. Enos, S. Fried-
man, S. Gilman, C. Girand, M. Graciarena,
A. Kathol, L. Michaelis, B. Pellom, E. Shriberg,
and A. Stolcke. 2005. Distinguishing decep-
tive from non-deceptive speech. In Proceedings of
INTERSPEECH-2005, Lisbon, Portugal.
M. Koppel, S. Argamon, and A. Shimoni. 2002. Au-
tomatically categorizing written texts by author gen-
der. Literary and Linguistic Computing, 4(17):401?
412.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Computational Intelligence,
22(2):126?142.
M. Newman, J. Pennebaker, D. Berry, and J. Richards.
2003. Lying words: Predicting deception from lin-
guistic styles. Personality and Social Psychology
Bulletin, 29:665?675.
J. Pennebaker and M. Francis. 1999. Linguistic in-
quiry and word count: LIWC. Erlbaum Publishers.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good? evaluating non-
expert annotations for natural language tasks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Honolulu,
Hawaii.
L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.
2004. Automating linguistics-based cues for detect-
ing deception in text-based asynchronous computer-
mediated communication. Group Decision and Ne-
gotiation, 13:81?106.
312
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 70?74,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 14: Affective Text
Carlo Strapparava
FBK ? irst
Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Povo, Trento, Italy
strappa@itc.it
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX, 76203, USA
rada@cs.unt.edu
Abstract
The ?Affective Text? task focuses on the
classification of emotions and valence (pos-
itive/negative polarity) in news headlines,
and is meant as an exploration of the connec-
tion between emotions and lexical seman-
tics. In this paper, we describe the data set
used in the evaluation and the results ob-
tained by the participating systems.
1 Introduction
All words can potentially convey affective mean-
ing. Every word, even those apparently neutral, can
evoke pleasant or painful experiences due to their
semantic relation with emotional concepts or cate-
gories. Some words have emotional meaning with
respect to an individual story, while for many others
the affective power is part of the collective imagina-
tion (e.g., words such as ?mum?, ?ghost?, ?war?).
The automatic detection of emotion in texts is
becoming increasingly important from an applica-
tive point of view. Consider for example the tasks
of opinion mining and market analysis, affective
computing, or natural language interfaces such as
e-learning environments or educational/edutainment
games. Possible beneficial effects of emotions on
memory and attention of the users, and in general on
fostering their creativity are also well-known in the
field of psychology.
For instance, the following represent examples
of applicative scenarios in which affective analysis
would give valuable and interesting contributions:
Sentiment Analysis. Text categorization according
to affective relevance, opinion exploration for
market analysis, etc. are just some exam-
ples of application of these techniques. While
positive/negative valence annotation is an ac-
tive field of sentiment analysis, we believe that
a fine-grained emotion annotation would in-
crease the effectiveness of these applications.
Computer Assisted Creativity. The automated
generation of evaluative expressions with
a bias on some polarity orientation are a
key component for automatic personalized
advertisement and persuasive communication.
Verbal Expressivity in Human Computer Interaction.
Future human-computer interaction, accord-
ing to a widespread view, will emphasize
naturalness and effectiveness and hence the
incorporation of models of possibly many hu-
man cognitive capabilities, including affective
analysis and generation. For example, emo-
tion expression by synthetic characters (e.g.,
embodied conversational agents) is considered
now a key element for their believability.
Affective words selection and understanding is
crucial for realizing appropriate and expressive
conversations.
The ?Affective Text? task was intended as an ex-
ploration of the connection between lexical seman-
tics and emotions, and an evaluation of various au-
tomatic approaches to emotion recognition.
The task is not easy. Indeed, as (Ortony et
al., 1987) indicates, besides words directly refer-
ring to emotional states (e.g., ?fear?, ?cheerful?) and
for which an appropriate lexicon would help, there
are words that act only as an indirect reference to
70
emotions depending on the context (e.g. ?monster?,
?ghost?). We can call the former direct affective
words and the latter indirect affective words (Strap-
parava et al, 2006).
2 Task Definition
We proposed to focus on the emotion classification
of news headlines extracted from news web sites.
Headlines typically consist of a few words and are
often written by creative people with the intention
to ?provoke? emotions, and consequently to attract
the readers? attention. These characteristics make
this type of text particularly suitable for use in an
automatic emotion recognition setting, as the affec-
tive/emotional features (if present) are guaranteed to
appear in these short sentences.
The structure of the task was as follows:
Corpus: News titles, extracted from news web sites
(such as Google news, CNN) and/or newspa-
pers. In the case of web sites, we can easily
collect a few thousand titles in a short amount
of time.
Objective: Provided a set of predefined six emotion
labels (i.e., Anger, Disgust, Fear, Joy, Sadness,
Surprise), classify the titles with the appropri-
ate emotion label and/or with a valence indica-
tion (positive/negative).
The emotion labeling and valence classification
were seen as independent tasks, and thus a team was
able to participate in one or both tasks. The task
was carried out in an unsupervised setting, and con-
sequently no training was provided. The reason be-
hind this decision is that we wanted to emphasize the
study of emotion lexical semantics, and avoid bias-
ing the participants toward simple ?text categoriza-
tion? approaches. Nonetheless supervised systems
were not precluded from participation, and in such
cases the teams were allowed to create their own su-
pervised training sets.
Participants were free to use any resources they
wanted. We provided a set words extracted from
WordNet Affect (Strapparava and Valitutti, 2004),
relevant to the six emotions of interest. However,
the use of this list was entirely optional.
2.1 Data Set
The data set consisted of news headlines drawn from
major newspapers such as New York Times, CNN,
and BBC News, as well as from the Google News
search engine. We decided to focus our attention on
headlines for two main reasons. First, news have
typically a high load of emotional content, as they
describe major national or worldwide events, and are
written in a style meant to attract the attention of the
readers. Second, the structure of headlines was ap-
propriate for our goal of conducting sentence-level
annotations of emotions.
Two data sets were made available: a develop-
ment data set consisting of 250 annotated headlines,
and a test data set with 1,000 annotated headlines.
2.2 Data Annotation
To perform the annotations, we developed a Web-
based annotation interface that displayed one head-
line at a time, together with six slide bars for emo-
tions and one slide bar for valence. The interval for
the emotion annotations was set to [0, 100], where 0
means the emotion is missing from the given head-
line, and 100 represents maximum emotional load.
The interval for the valence annotations was set to
[?100, 100], where 0 represents a neutral headline,
?100 represents a highly negative headline, and 100
corresponds to a highly positive headline.
Unlike previous annotations of sentiment or sub-
jectivity (Wiebe et al, 2005; Pang and Lee, 2004),
which typically relied on binary 0/1 annotations, we
decided to use a finer-grained scale, hence allow-
ing the annotators to select different degrees of emo-
tional load.
The test data set was independently labeled by six
annotators. The annotators were instructed to select
the appropriate emotions for each headline based on
the presence of words or phrases with emotional
content, as well as the overall feeling invoked by
the headline. Annotation examples were also pro-
vided, including examples of headlines bearing two
or more emotions to illustrate the case where sev-
eral emotions were jointly applicable. Finally, the
annotators were encouraged to follow their ?first in-
tuition,? and to use the full-range of the annotation
scale bars.
71
2.3 Inter-Annotator Agreement
We conducted inter-tagger agreement studies for
each of the six emotions and for the valence an-
notations. The agreement evaluations were carried
out using the Pearson correlation measure, and are
shown in Table 1. To measure the agreement among
the six annotators, we first measured the agreement
between each annotator and the average of the re-
maining five annotators, followed by an average
over the six resulting agreement figures.
EMOTIONS
Anger 49.55
Disgust 44.51
Fear 63.81
Joy 59.91
Sadness 68.19
Surprise 36.07
VALENCE
Valence 78.01
Table 1: Pearson correlation for inter-annotator
agreement
2.4 Fine-grained and Coarse-grained
Evaluations
Fine-grained evaluations were conducted using the
Pearson measure of correlation between the system
scores and the gold standard scores, averaged over
all the headlines in the data set.
We have also run a coarse-grained evaluation,
where each emotion was mapped to a 0/1 classifica-
tion (0 = [0,50), 1 = [50,100]), and each valence was
mapped to a -1/0/1 classification (-1 = [-100,-50],
0 = (-50,50), 1 = [50,100]). For the coarse-grained
evaluations, we calculated accuracy, precision, and
recall. Note that the accuracy is calculated with re-
spect to all the possible classes, and thus it can be
artificially high in the case of unbalanced datasets
(as some of the emotions are, due to the high num-
ber of neutral headlines). Instead, the precision and
recall figures exclude the neutral annotations.
3 Participating Systems
Five teams have participated in the task, with five
systems for valence classification and three systems
for emotion labeling. The following represents a
short description of the systems.
UPAR7: This is a rule-based system using a lin-
guistic approach. A first pass through the data ?un-
capitalizes? common words in the news title. The
system then used the Stanford syntactic parser on
the modified title, and tried to identify what is being
said about the main subject by exploiting the depen-
dency graph obtained from the parser.
Each word was first rated separately for each emo-
tion (the six emotions plus Compassion) and for va-
lence. Next, the main subject rating was boosted.
Contrasts and accentuations between ?good? or
?bad? were detected, making it possible to identify
surprising good or bad news. The system also takes
into account: human will (as opposed to illness or
natural disasters); negation and modals; high-tech
context; celebrities.
The lexical resource used was a combination
of SentiWordNet (Esuli and Sebastiani, 2006) and
WordNetAffect (Strapparava and Valitutti, 2004),
which were semi-automatically enriched on the ba-
sis of the original trial data.
SICS: The SICS team used a very simple ap-
proach for valence annotation based on a word-space
model and a set of seed words. The idea was to cre-
ate two points in a high-dimensional word space -
one representing positive valence, the other repre-
senting negative valence - and then projecting each
headline into this space, choosing the valence whose
point was closer to the headline.
The word space was produced from a lemmatized
and stop list filtered version of the LA times cor-
pus (consisting of documents from 1994, released
for experimentation in the Cross Language Eval-
uation Forum (CLEF)) using documents as con-
texts and standard TFIDF weighting of frequencies.
No dimensionality reduction was used, resulting in
a 220,220-dimensional word space containing pre-
dominantly syntagmatic relations between words.
Valence vectors were created in this space by sum-
ming the context vectors of a set of manually se-
lected seed words (8 positive and 8 negative words).
For each headline in the test data, stop words and
words with frequency above 10,000 in the LA times
corpus were removed. The context vectors of the re-
maining words were then summed, and the cosine of
the angles between the summed vector and each of
the valence vectors were computed, and the head-
line was ascribed the valence value (computed as
72
[cosine * 100 + 50]) of the closest valence vector
(headlines that were closer to the negative valence
vector were assigned a negative valence value). In
11 cases, a value of -0.0 was ascribed either because
no words were left in the headline after frequency
and stop word filtering, or because none of the re-
maining words occurred in the LA times corpus and
thus did not have any context vector.
CLaC: This team submitted two systems to the
competition: an unsupervised knowledge-based sys-
tem (ClaC) and a supervised corpus-based system
(CLaC-NB). Both systems were used for assigning
positive/negative and neutral valence to headlines on
the scale [-100,100].
CLaC: The CLaC system relies on a knowledge-
based domain-independent unsupervised approach
to headline valence detection and scoring. The
system uses three main kinds of knowledge: a
list of sentiment-bearing words, a list of valence
shifters and a set of rules that define the scope and
the result of the combination of sentiment-bearing
words and valence shifters. The unigrams used for
sentence/headline classification were learned from
WordNet dictionary entries. In order to take advan-
tage of the special properties of WordNet glosses
and relations, we developed a system that used the
list of human-annotated adjectives from (Hatzivas-
siloglou and McKeown, 1997) as a seed list and
learned additional unigrams from WordNet synsets
and glosses. The list was then expanded by adding
to it all the words annotated with Positive or Neg-
ative tags in the General Inquirer. Each unigram in
the resulting list had the degree of membership in the
category of positive or negative sentiment assigned
to it using the fuzzy Net Overlap Score method de-
scribed in the team?s earlier work (Andreevskaia and
Bergler, 2006). Only words with fuzzy member-
ship score not equal to zero were retained in the
list. The resulting list contained 10,809 sentiment-
bearing words of different parts of speech.
The fuzzy Net Overlap Score counts were com-
plemented with the capability to discern and take
into account some relevant elements of syntactic
structure of the sentences. Two components were
added to the system to enable this capability: (1)
valence shifter handling rules and (2) parse tree
analysis. The list of valence shifters was a com-
bination of a list of common English negations
and a subset of the list of automatically obtained
words with increase/decrease semantics, comple-
mented with manual annotation. The full list con-
sists of 450 words and expressions. Each entry in
the list of valence shifters has an action and scope
associated with it, which are used by special han-
dling rules that enable the system to identify such
words and phrases in the text and take them into ac-
count in sentence sentiment determination. In order
to correctly determine the scope of valence shifters
in a sentence, the system used a parse tree analysis
using MiniPar.
As a result of this processing, every headline re-
ceived a system score assigned based on the com-
bined fuzzy Net Overlap Score of its constituents.
This score was then mapped into the [-100 to 100]
scale as required by the task.
CLaC-NB: In order to assess the performance of
basic Machine Learning techniques on headlines,
a second system ClaC-NB was also implemented.
This system used a Na??ve Bayes classifier in order to
assign valence to headlines. It was trained on a small
corpus composed of the development corpus of 250
headlines provided for this competition, plus an ad-
ditional 200 headlines manually annotated and 400
positive and negative news sentences. The probabil-
ities assigned by the classifier were mapped to the [-
100, 100] scale as follows: all negative headlines re-
ceived the score of -100, all positive headlines were
assigned the score of +100, and the neutral headlines
obtained the score of 0.
UA: In order to determine the kind and the amount
of emotions in a headline, statistics were gathered
from three different web Search Engines: MyWay,
AlltheWeb and Yahoo. This information was used to
observe the distribution of the nouns, the verbs, the
adverbs and the adjectives extracted from the head-
line and the different emotions.
The emotion scores were obtained through Point-
wise Mutual Information (PMI). First, the number
of documents obtained from the three web search
engines using a query that contains all the headline
words and an emotion (the words occur in an inde-
pendent proximity across the web documents) was
divided by the number of documents containing only
an emotion and the number of documents containing
all the headline words. Second, an associative score
between each content word and an emotion was es-
73
timated and used to weight the final PMI score. The
obtained results were normalized in the 0-100 range.
SWAT: SWAT is a supervised system using an u-
nigram model trained to annotate emotional content.
Synonym expansion on the emotion label words was
also performed, using the Roget Thesaurus. In addi-
tion to the development data provided by the task
organizers, the SWAT team annotated an additional
set of 1000 headlines, which was used for training.
Fine Coarse
r Acc. Prec. Rec. F1
CLaC 47.70 55.10 61.42 9.20 16.00
UPAR7 36.96 55.00 57.54 8.78 15.24
SWAT 35.25 53.20 45.71 3.42 6.36
CLaC-NB 25.41 31.20 31.18 66.38 42.43
SICS 20.68 29.00 28.41 60.17 38.60
Table 2: System results for valence annotations
Fine Coarse
r Acc. Prec. Rec. F1
Anger
SWAT 24.51 92.10 12.00 5.00 7.06
UA 23.20 86.40 12.74 21.6 16.03
UPAR7 32.33 93.60 16.67 1.66 3.02
Disgust
SWAT 18.55 97.20 0.00 0.00 -
UA 16.21 97.30 0.00 0.00 -
UPAR7 12.85 95.30 0.00 0.00 -
Fear
SWAT 32.52 84.80 25.00 14.40 18.27
UA 23.15 75.30 16.23 26.27 20.06
UPAR7 44.92 87.90 33.33 2.54 4.72
Joy
SWAT 26.11 80.60 35.41 9.44 14.91
UA 2.35 81.80 40.00 2.22 4.21
UPAR7 22.49 82.20 54.54 6.66 11.87
Sadness
SWAT 38.98 87.70 32.50 11.92 17.44
UA 12.28 88.90 25.00 0.91 1.76
UPAR7 40.98 89.00 48.97 22.02 30.38
Surprise
SWAT 11.82 89.10 11.86 10.93 11.78
UA 7.75 84.60 13.70 16.56 15.00
UPAR7 16.71 88.60 12.12 1.25 2.27
Table 3: System results for emotion annotations
4 Results
Tables 2 and 3 show the results obtained by the par-
ticipating systems. The tables show both the fine-
grained Pearson correlation measure and the coarse-
grained accuracy, precision and recall figures.
While further analysis is still needed, the results
indicate that the task of emotion annotation is diffi-
cult. Although the Pearson correlation for the inter-
tagger agreement is not particularly high, the gap
between the results obtained by the systems and the
upper bound represented by the annotator agreement
suggests that there is room for future improvements.
Acknowledgments
Carlo Strapparava was partially supported by the
HUMAINE Network of Excellence.
References
A. Andreevskaia and S. Bergler. 2006. Senses and senti-
ments: Sentiment tagging of adjectives at the meaning
level. In Proceedings of the 19th Canadian Confer-
ence on Artificial Intelligence, AI?06, Quebec, Canada.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion mining.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
Genoa, Italy, May.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In Proceedings
of the 35th Annual Meeting of the ACL, Madrid, Spain,
July.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, (11).
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain, July.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, Lisbon.
C. Strapparava, A. Valitutti, and O. Stock. 2006. The
affective weight of lexicon. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
74
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 129?136, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Investigating Unsupervised Learning
for Text Categorization Bootstrapping
Alfio Gliozzo and Carlo Strapparava
ITC-irst
Istituto per la Ricerca Scientifica e Tecnologica
I-38050 Trento, Italy
{gliozzo,strappa}@itc.it
Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
We propose a generalized bootstrapping
algorithm in which categories are de-
scribed by relevant seed features. Our
method introduces two unsupervised steps
that improve the initial categorization step
of the bootstrapping scheme: (i) using La-
tent Semantic space to obtain a general-
ized similarity measure between instances
and features, and (ii) the Gaussian Mixture
algorithm, to obtain uniform classification
probabilities for unlabeled examples. The
algorithm was evaluated on two Text Cate-
gorization tasks and obtained state-of-the-
art performance using only the category
names as initial seeds.
1 Introduction
Supervised classification is the task of assigning cat-
egory labels, taken from a predefined set of cate-
gories (classes), to instances in a data set. Within the
classical supervised learning paradigm, the task is
approached by providing a learning algorithm with
a training data set of manually labeled examples. In
practice it is not always easy to apply this schema
to NLP tasks. For example supervised systems for
Text Categorization (TC) require a large amount of
hand labeled texts, while in many applicative cases
it is quite difficult to collect the required amounts of
hand labeled data. Unlabeled text collections, on the
other hand, are in general easily available.
An alternative approach is to provide the neces-
sary supervision by means of sets of ?seeds? of in-
tuitively relevant features. Adopting terminology
from computability theory, we refer to the stan-
dard example-based supervision mode as Exten-
sional Learning (EL), as classes are being specified
by means of examples of their elements (their ex-
tension). Feature-based supervision is referred to as
Intensional Learning (IL), as features may often be
perceived as describing the intension of a category,
such as providing the name or prominent key terms
for a category in text categorization.
The IL approach reflects on classical rule-based
classification methods, where the user is expected
to specify exact classification rules that operate in
the feature space. Within the machine learning
paradigm, IL has been incorporated as a technique
for bootstrapping an extensional learning algorithm,
as in (Yarowsky, 1995; Collins and Singer, 1999;
Liu et al, 2004). This way the user does not
need to specify exact classification rules (and fea-
ture weights), but rather perform a somewhat sim-
pler task of specifying few typical seed features for
the category. Given the list of seed features, the
bootstrapping scheme consists of (i) preliminary un-
supervised categorization of the unlabeled data set
based on the seed features, and (ii) training an (ex-
tensional) supervised classifier using the automatic
classification labels of step (i) as the training data
(the second step is possibly reiterated, such as by
an Expectation-Maximization schema). The core
part of IL bootstrapping is step (i), i.e. the initial
unsupervised classification of the unlabeled dataset.
This step was often approached by relatively sim-
ple methods, which are doomed to obtain mediocre
quality. Even so, it is hoped that the second step of
supervised training would be robust enough to the
noise in the initial training set.
129
The goal of this paper is to investigate additional
principled unsupervised mechanisms within the ini-
tial classification step, applied to the text catego-
rization. In particular, (a) utilizing a Latent Se-
mantic Space to obtain better similarity assessments
between seeds and examples, and (b) applying a
Gaussian Mixture (GM) algorithm, which provides a
principled unsupervised estimation of classification
probability. As shown in our experiments, incor-
porating these steps consistently improved the ac-
curacy of the initial categorization step, which in
turn yielded a better final classifier thanks to the
more accurate training set. Most importantly, we ob-
tained comparable or better performance than previ-
ous IL methods using only the category names as the
seed features; other IL methods required collecting
a larger number of seed terms, which turns out to be
a somewhat tricky task.
Interesting results were revealed when compar-
ing our IL method to a state-of-the-art extensional
classifier, trained on manually labeled documents.
The EL classifier required 70 (Reuters dataset) or
160 (Newsgroup dataset) documents per category to
achieve the same performance that IL obtained using
only the category names. These results suggest that
IL may provide an appealing cost-effective alterna-
tive when sub-optimal accuracy suffices, or when it
is too costly or impractical to obtain sufficient la-
beled training. Optimal combination of extensional
and intensional supervision is raised as a challeng-
ing topic for future research.
2 Bootstrapping for Text Categorization
The TC task is to assign category labels to docu-
ments. In the IL setting, a category Ci is described
by providing a set of relevant features, termed an
intensional description (ID), idci ? V , where V
is the vocabulary. In addition a training corpus
T = {t1, t2, . . . tn} of unlabeled texts is provided.
Evaluation is performed on a separate test corpus
of labeled documents, to which standard evaluation
metrics can be applied.
The approach of categorizing texts based on lists
of keywords has been attempted rather rarely in the
literature (McCallum and Nigam, 1999; Ko and Seo,
2000; Liu et al, 2004; Ko and Seo, 2004). Several
names have been proposed for it ? such as TC by
bootstrapping with keywords, unsupervised TC, TC
by labelling words ? where the proposed methods
fall (mostly) within the IL settings described here1.
It is possible to recognize a common structure of
these works, based on a typical bootstrap schema
(Yarowsky, 1995; Collins and Singer, 1999):
Step 1: Initial unsupervised categorization. This
step was approached by applying some similar-
ity criterion between the initial category seed
and each unlabeled document. Similarity may
be determined as a binary criterion, consider-
ing each seed keyword as a classification rule
(McCallum and Nigam, 1999), or by applying
an IR style vector similarity measure. The re-
sult of this step is an initial categorization of (a
subset of) the unlabeled documents. In (Ko and
Seo, 2004) term similarity techniques were ex-
ploited to expand the set of seed keywords, in
order to improve the quality of the initial cate-
gorization.
Step 2: Train a supervised classifier on the ini-
tially categorized set. The output of Step
1 is exploited to train an (extensional) su-
pervised classifier. Different learning algo-
rithms have been tested, including SVM, Naive
Bayes, Nearest Neighbors, and Rocchio. Some
works (McCallum and Nigam, 1999; Liu et
al., 2004) performed an additional Expectation
Maximization algorithm over the training data,
but reported rather small incremental improve-
ments that do not seem to justify the additional
effort.
(McCallum and Nigam, 1999) reported catego-
rization results close to human agreement on the
same task. (Liu et al, 2004) and (Ko and Seo,
2004) contrasted their word-based TC algorithm
with the performance of an extensional supervised
algorithm, achieving comparable results, while in
general somewhat lower. It should be noted that it
has been more difficult to define a common evalua-
tion framework for comparing IL algorithms for TC,
due to the subjective selection of seed IDs and to the
lack of common IL test sets (see Section 4).
1The major exception is the work in (Ko and Seo, 2004),
which largely follows the IL scheme but then makes use of la-
beled data to perform a chi-square based feature selection be-
fore starting the bootstrap process. This clearly falls outside the
IL setting, making their results incomparable to other IL meth-
ods.
130
3 Incorporating Unsupervised Learning
into Bootstrap Schema
In this section we show how the core Step 1 of the IL
scheme ? the initial categorization ? can be boosted
by two unsupervised techniques. These techniques
fit the IL setting and address major constraints of it.
The first is exploiting a generalized similarity metric
between category seeds (IDs) and instances, which
is defined in a Latent Semantic space. Applying
such unsupervised similarity enables to enhance the
amount of information that is exploited from each
seed feature, aiming to reduce the number of needed
seeds. The second technique applies the unsuper-
vised Gaussian Mixture algorithm, which maps sim-
ilarity scores to a principled classification probabil-
ity value. This step enables to obtain a uniform scale
of classification scores across all categories, which
is typically obtained only through calibration over
labeled examples in extensional learning.
3.1 Similarity in Latent Semantic Space
As explained above, Step 1 of the IL scheme as-
sesses a degree of ?match? between the seed terms
and a classified document. It is possible first to
follow the intuitively appealing and principled ap-
proach of (Liu et al, 2004), in which IDs (category
seeds) and instances are represented by vectors in a
usual IR-style Vector Space Model (VSM), and sim-
ilarity is measured by the cosine function:
simvsm(idci , tj) = cos (~idci , ~tj) (1)
where ~idci ? R|V | and ~tj ? R|V | are the vectorial
representations in the space R|V | respectively of the
category ID idci and the instance tj , and V is the set
of all the features (the vocabulary).
However, representing seeds and instances in a
standard feature space is severely affected in the IL
setting by feature sparseness. In general IDs are
composed by short lists of features, possibly just
a single feature. Due to data sparseness, most in-
stances do not contain any feature in common with
any category?s ID, which makes the seeds irrelevant
for most instances (documents in the text categoriza-
tion case). Furthermore, applying direct matching
only for a few seed terms is often too crude, as it ig-
nores the identity of the other terms in the document.
The above problems may be reduced by consid-
ering some form of similarity in the feature space,
as it enables to compare additional document terms
with the original seeds. As mentioned in Section
2, (Ko and Seo, 2004) expanded explicitly the orig-
inal category IDs with more terms, using a con-
crete query expansion scheme. We preferred using a
generalized similarity measure based on represent-
ing features and instances a Latent Semantic (LSI)
space (Deerwester et al, 1990). The dimensions of
the Latent Semantic space are the most explicative
principal components of the feature-by-instance ma-
trix that describes the unlabeled data set. In LSI
both coherent features (i.e. features that often co-
occur in the same instances) and coherent instances
(i.e. instances that share coherent features) are rep-
resented by similar vectors in the reduced dimen-
sionality space. As a result, a document would be
considered similar to a category ID if the seed terms
and the document terms tend to co-occur overall in
the given corpus.
The Latent Semantic Vectors for IDs and docu-
ments were calculated by an empirically effective
variation (self-reference omitted for anonymity) of
the pseudo-document methodology to fold-in docu-
ments, originally suggested in (Berry, 1992). The
similarity function simlsi is computed by the cosine
metric, following formula 1, where ~idci and ~tj are
replaced by their Latent Semantic vectors. As will
be shown in section 4.2, using such non sparse rep-
resentation allows to drastically reduce the number
of seeds while improving significantly the recall of
the initial categorization step.
3.2 The Gaussian Mixture Algorithm and the
initial classification step
Once having a similarity function between category
IDs and instances, a simple strategy is to base the
classification decision (of Step 1) directly on the
obtained similarity values (as in (Liu et al, 2004),
for example). Typically, IL works adopt in Step 1
a single-label classification approach, and classify
each instance (document) to only one category. The
chosen category is the one whose ID is most simi-
lar to the classified instance amongst all categories,
which does not require any threshold tuning over la-
beled examples. The subsequent training in Step 2
yields a standard EL classifier, which can then be
used to assign multiple categories to a document.
Using directly the output of the similarity func-
tion for classification is problematic, because the ob-
tained scales of similarity values vary substantially
across different categories. The variability in sim-
131
ilarity value ranges is caused by variations in the
number of seed terms per category and the levels of
their generality and ambiguity. As a consequence,
choosing the class with the highest absolute similar-
ity value to the instance often leads to selecting a
category whose similarity values tend to be gener-
ally higher, while another category could have been
more similar to the classified instance if normalized
similarity values were used.
As a solution we propose using an algorithm
based on unsupervised estimation of Gaussian Mix-
tures (GM), which differentiates relevant and non-
relevant category information using statistics from
unlabeled instances. We recall that mixture mod-
els have been widely used in pattern recognition and
statistics to approximate probability distributions. In
particular, a well-known nonparametric method for
density estimation is the so-called Kernel Method
(Silverman, 1986), which approximates an unknow
density with a mixture of kernel functions, such as
gaussians functions. Under mild regularity condi-
tions of the unknown density function, it can be
shown that mixtures of gaussians converge, in a sta-
tistical sense, to any distribution.
More formally, let ti ? T be an instance described
by a vector of features ~ti ? R|V | and let idci ? V
be the ID of category Ci; let sim(idci , tj) ? R be
a similarity function among instances and IDs, with
the only expectation that it monotonically increases
according to the ?closeness? of idci and tj (see Sec-
tion 3.1).
For each category Ci, GM induces a mapping
from the similarity scores between its ID and any
instance tj , sim(idci , tj), into the probability of Ci
given the text tj , P (Ci|tj). To achieve this goal GM
performs the following operations: (i) it computes
the set Si = {sim(idci , tj)|tj ? T} of the sim-
ilarity scores between the ID idci of the category
Ci and all the instances tj in the unlabeled train-
ing set T ; (ii) it induces from the empirical distri-
bution of values in Si a Gaussian Mixture distribu-
tion which is composed of two ?hypothetic? distri-
butions Ci and Ci, which are assumed to describe re-
spectively the distributions of similarity scores for
positive and negative examples; and (iii) it estimates
the conditional probability P (Ci|sim(idci , tj)) by
applying the Bayes theorem on the distributions Ci
and Ci. These steps are explained in more detail be-
low.
The core idea of the algorithm is in step (ii). Since
we do not have labeled training examples we can
only obtain the set Si which includes the similar-
ity scores for all examples together, both positive
and negative. We assume, however, that similar-
ity scores that correspond to positive examples are
drawn from one distribution, P (sim(idci , tj)|Ci),
while the similarity scores that correspond to neg-
ative examples are drawn from another distribution,
P (sim(idci , tj)|Ci). The observed distribution of
similarity values in Si is thus assumed to be a mix-
ture of the above two distributions, which are recov-
ered by the GM estimation.
Figure 1 illustrates the mapping induced by GM
from the empirical mixture distribution: dotted lines
describe the Probability Density Functions (PDFs)
estimated by GM for Ci, Ci, and their mixture from
the empirical distribution (Si) (in step (ii)). The
continuous line is the mapping induced in step (iii)
of the algorithm from similarity scores between in-
stances and IDs (x axis) to the probability of the in-
stance to belong to the category (y axis).
0
1
2
3
4
5
6
7
8
9
-0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
GM-score
Ci gaussian (relevant)
Ci gaussian (not relevant)Mixture
Similarity Score
Pro
bab
ility
 / P
DF
Figure 1: Mapping induced by GM for the category
rec.motorcycles in the 20newsgroups data set.
The probabilistic mapping estimated in step (iii)
for a category Ci given an instance tj is computed
by applying Bayes rule:
P (Ci|tj) = P (Ci|sim(idci , tj)) = (2)
=
P (sim(idci ,tj)|Ci)P (Ci)
P (sim(idci ,tj)|Ci)P (Ci)+P (sim(Ci,tj)|Ci)P (Ci)
where P (sim(idci , tj)|Ci) is the value of
the PDF of Ci at the point sim(idci , tj),
P (sim(idci , tj)|Ci) is the value of the PDF of Ci at
the same point, P (Ci) is the area of the distribution
132
Ci and P (Ci) is the area of the distribution Ci. The
mean and variance parameters of the two distribu-
tions Ci and Ci, used to evaluate equation 2, are esti-
mated by the rather simple application of the Expec-
tation Maximization (EM) algorithm for Gaussian
Mixtures, as summarized in (Gliozzo et al, 2004).
Finally, following the single-labeled categoriza-
tion setting of Step 1 in the IL scheme, the most
likely category is assigned to each instance, that is,
argmaxCiP (Ci|tj).
3.3 Summary of the Bootstrapping Algorithm
step 1.a: Latent Semantic Space. Instances and
Intensional Descriptions of categories (the seeds) are
represented by vectors in Latent Semantic space. As
an option, the algorithm can work with the classi-
cal Vector Space Model using the original feature
space. Similarity scores between IDs and instances
are computed by the Cosine measure.
step 1.b: GM. The mapping functions P (Ci|tj)
for each category, conditioned on instances tj , are
induced by the GM algorithm. To that end, an Ex-
pectation Maximization algorithm estimates the pa-
rameters of the two component distributions of the
observed mixture, which correspond to the distribu-
tions of similarity values for positive and negative
examples. As an option, the GM mapping can be
avoided.
step 1.c: Categorization. Each instance
is classified to the most probable category -
argmaxCiP (Ci|tj).
step 2: Bootstrapping an extensional classifier.
An EL classifier (SVM) is trained on the set of la-
beled instances resulting from step 1.c.
4 Evaluation
4.1 Intensional Text Categorization Datasets
Even though some typical data sets have been used
in the TC literature (Sebastiani, 2002), the datasets
used for IL learning were not standard. Often there
is not sufficient clarity regarding details such as the
exact version of the corpus used and the training/test
splitting. Furthermore, the choice of categories was
often not standard: (Ko and Seo, 2004) omitted 4
categories from the 20-Newsgroup dataset, while
(Liu et al, 2004) evaluated their method on 4 sepa-
rate subsets of the 20-Newsgroups, each containing
only 4-5 categories. Such issues make it rather diffi-
cult to compare thoroughly different techniques, yet
we have conducted several comparisons in Subsec-
tion 4.5 below. In the remainder of this Subsection
we clearly state the corpora used in our experiments
and the pre-processing steps performed on them.
20newsgroups. The 20 Newsgroups data set is
a collection of newsgroup documents, partitioned
(nearly) evenly across 20 different newsgroups. As
suggested in the dataset Web site2, we used the
?bydate? version: the corpus (18941 documents)
is sorted by date and divided in advance into a
training (60%) set and a chronologically follow-
ing test set (40%) (so there is no randomness in
train/test set selection), it does not include cross-
posts (duplicates), and (more importantly) does not
include non-textual newsgroup-identifying headers
which often help classification (Xref, Newsgroups,
Path, Followup-To, Date).
We will first report results using initial seeds
for the category ID?s, which were selected using
only the words in the category names, with some
trivial transformations (i.e. cryptography#n
for the category sci.crypt, x-windows#n
for the category comp.windows.x). We
also tried to avoid ?overlapping? seeds, i.e.
for the categories rec.sport.baseball
and rec.sport.hockey the seeds are only
{baseball#n} and {hockey#n} respec-
tively and not {sport#n, baseball#n} and
{sport#n, hockey#n}3.
Reuters-10. We used the top 10 categories
(Reuters-10) in the Reuters-21578 collection
Apte` split4. The complete Reuters collection
includes 12,902 documents for 90 categories,
with a fixed splitting between training and test
data (70/30%). Both the Apte` and Apte`-10
splits are often used in TC tasks, as surveyed
in (Sebastiani, 2002). To obtain the Reuters-10
2The collection is available at
www.ai.mit.edu/people/jrennie/20Newsgroups.
3One could propose as a guideline for seed selection
those seeds that maximize their distances in the LSI vec-
tor space model. On this perspective the LSI vectors
built from {sport#n, baseball#n} and {sport#n,
hockey#n} are closer than the vectors that represent
{baseball#n} and {hockey#n}. It may be noticed that
this is a reason for the slight initial performance decrease in the
learning curve in Figure 2 below.
4available at http://kdd.ics.uci.edu/databases/-
reuters21578/reuters21578.html).
133
Apte` split we selected the 10 most frequent cate-
gories: Earn, Acquisition, Money-fx,
Grain, Crude, Trade, Interest,
Ship, Wheat and Corn. The final data set
includes 9296 documents. The initial seeds are only
the words appearing in the category names.
Pre-processing. In both data sets we tagged the
texts for part-of-speech and represented the docu-
ments by the frequency of each pos-tagged lemma,
considering only nouns, verbs, adjectives, and ad-
verbs. We induced the Latent Semantic Space from
the training part5 and consider the first 400 dimen-
sions.
4.2 The impact of LSI similarity and GM on IL
performance
In this section we evaluate the incremental impact
of LSI similarity and the GM algorithm on IL per-
formance. When avoiding both techniques the algo-
rithm uses the simple cosine-based method over the
original feature space, which can be considered as a
baseline (similar to the method of (Liu et al, 2004)).
We report first results using only the names of the
categories as initial seeds.
Table 1 displays the F1 measure for the 20news-
groups and Reuters data sets, with and without LSI
and with and without GM. The performance figures
show the incremental benefit of both LSI and GM. In
particular, when starting with just initial seeds and
do not exploit the LSI similarity mechanism, then
the performance is heavily penalized.
As mentioned above, the bootstrapping step of the
algorithm (Step 2) exploits the initially classified in-
stances to train a supervised text categorization clas-
sifier based on Support Vector Machines. It is worth-
while noting that the increment of performance after
bootstrapping is generally higher when GM and LSI
are incorporated, thanks to the higher quality of the
initial categorization which was used for training.
4.3 Learning curves for the number of seeds
This experiment evaluates accuracy change as a
function of the number of initial seeds. The ex-
5From a machine learning point of view, we could run the
LSA on the full corpus (i.e. training and test), the LSA being a
completely unsupervised technique (i.e. it does not take into ac-
count the data annotation). However, from an applicative point
of view it is much more sensible to have the LSA built on the
training part only. If we run the LSA on the full corpus, the
performance figures increase in about 4 points.
Reuters 20 Newsgroups
LSI GM F1 F1
no no 0.38 0.25
+ bootstrap 0.42 0.28
no yes 0.41 0.30
+ bootstrap 0.46 0.34
yes no 0.46 0.50
+ bootstrap 0.47 0.53
yes yes 0.58 0.60
+ bootstrap 0.74 0.65
Table 1: Impact of LSI vector space and GM
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
1 5 10 15 20
F1
number of seeds (1 means only the category names)
LSI VSMClassical VSM
Figure 2: Learning curves on initial seeds for 20
newsgroups, LSI and Classical VSM (no LSI)
periment was performed for the 20 newsgroups cor-
pus using both the LSI and the Classical vector
space model. Additional seeds, beyond the cate-
gory names, were identified by two lexicographers.
For each category, the lexicographers were provided
with a list of 100 seeds produced by the LSI similar-
ity function applied to the category name (one list of
100 candidate terms for each category). From these
lists the lexicographers selected the words that were
judged as significantly related to the respective cat-
egory, picking a mean of 40 seeds per category.
As seen in Figure 2, the learning curve using
LSI vector space model dramatically outperforms
the one using classical vector space. As can be
expected, when using the original vector space (no
generalization) the curve improves quickly with a
few more terms. More surprisingly, with LSI sim-
ilarity the best performance is obtained using the
minimal initial seeds of the category names, while
adding more seeds degrades performance. This
might suggest that category names tend to be highly
134
indicative for the intensional meaning of the cate-
gory, and therefore adding more terms introduces
additional noise. Further research is needed to find
out whether other methods for selecting additional
seed terms might yield incremental improvements.
The current results, though, emphasize the bene-
fit of utilizing LSI and GM. These techniques ob-
tain state-of-the-art performance (see comparisons
in Section 4.5) using only the category names as
seeds, allowing us to skip the quite tricky phase of
collecting manually a larger number of seeds.
4.4 Extensional vs. Intensional Learning
A major point of comparison between IL and EL is
the amount of supervision effort required to obtain a
certain level of performance. To this end we trained
a supervised classifier based on Support Vector Ma-
chines, and draw its learning curves as a function
of percentage of the training set size (Figure 3). In
the case of 20newsgroups, to achieve the 65% F1
performance of IL the supervised settings requires
about 3200 documents (about 160 texts per cate-
gory), while our IL method requires only the cate-
gory name. Reuters-10 is an easier corpus, there-
fore EL achieves rather rapidly a high performance.
But even here using just the category name is equal
on average to labeling 70 documents per-category
(700 in total). These results suggest that IL may pro-
vide an appealing cost-effective alternative in prac-
tical settings when sub-optimal accuracy suffices, or
when it is too costly or impractical to obtain suffi-
cient amounts of labeled training sets.
It should also be stressed that when using the
complete labeled training corpus state-of-the-art EL
outperforms our best IL performance. This result
deviates from the flavor of previous IL literature,
which reported almost comparable performance rel-
ative to EL. As mentioned earlier, the method of (Ko
and Seo, 2004) (as we understand it) utilizes labeled
examples for feature selection, and therefore cannot
be compared with our strict IL setting. As for the
results in (Liu et al, 2004), we conjecture that their
comparable performance for IL and EL may not be
sufficiently general, for several reasons: the easier
classification task (4 subsets of 20-Newsgroups of
4-5 categories each); the use of the usually weaker
Naive-Bayes as the EL device; the use of cluster-
ing as an aid for selecting the seed terms from the
20-Newsgroup subsets, which might not scale up
well when applied to a large number of categories
of varying size.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
Percentage of training
20 NewsgroupsReuters
3200 docs
700 docs
Figure 3: Extensional learning curves on as percent-
age of the training set.
4.5 Comparisons with other algorithms
As mentioned earlier it is not easy to conduct a thor-
ough comparison with other algorithms in the litera-
ture. Most IL data sets used for training and evalua-
tion are either not available (McCallum and Nigam,
1999) or are composed by somewhat arbitrary sub-
sets of a standard data set. Another crucial aspect
is the particular choice of the seed terms selected to
compose an ID, which affects significantly the over-
all performance of the algorithm.
As a baseline system, we implemented a rule
based approach in the spirit of (McCallum and
Nigam, 1999). It is based on two steps. First, all
the documents in the unlabeled training corpus con-
taining at least one word in common with one and
only one category ID are assigned to the respective
class. Second, a supervised classifier based on SVM
is trained on the labeled examples. Finally, the su-
pervised classifier is used to perform the final cate-
gorization step on the test corpus. Table 2 reports
the F1 measure of our replication of this method, us-
ing the category name as seed, which is substantially
lower than the performance of the method we pre-
sented in this paper.
Reuters 20 Newsgroups
0.34 0.30
+ bootstrap 0.42 0.47
Table 2: Rule-based baseline performance
135
We also tried to replicate two of the non-standard
data sets used in (Liu et al, 2004)6. Table 3 displays
the performance of our approach in comparison to
the results reported in (Liu et al, 2004). Follow-
ing the evaluation metric adopted in that paper we
report here accuracy instead of F1. For each data
set (Liu et al, 2004) reported several results vary-
ing the number of seed words (from 5 to 30), as well
as varying some heuristic thresholds, so in the ta-
ble we report their best results. Notably, our method
obtained comparable accuracy by using just the cat-
egory name as ID for each class instead of multiple
seed terms. This result suggests that our method en-
ables to avoid the somewhat fuzzy process of col-
lecting manually a substantial number of additional
seed words.
Our IDs per cat. Liu et al IDs per cat.
REC 0.94 1 0.95 5
TALK 0.80 1 0.80 20
Table 3: Accuracy on 4 ?REC? and 4 ?TALK? news-
groups categories
5 Conclusions
We presented a general bootstrapping algorithm for
Intensional Learning. The algorithm can be applied
to any categorization problem in which categories
are described by initial sets of discriminative fea-
tures and an unlabeled training data set is provided.
Our algorithm utilizes a generalized similarity mea-
sure based on Latent Semantic Spaces and a Gaus-
sian Mixture algorithm as a principled method to
scale similarity scores into probabilities. Both tech-
niques address inherent limitations of the IL setting,
and leverage unsupervised information from an un-
labeled corpus.
We applied and evaluated our algorithm on some
text categorization tasks and showed the contribu-
tion of the two techniques. In particular, we obtain,
for the first time, competitive performance using
only the category names as initial seeds. This mini-
mal information per category, when exploited by the
IL algorithm, is shown to be equivalent to labeling
about 70-160 training documents per-category for
state of the art extensional learning. Future work is
6We used sequential splitting (70/30) rather than random
splitting and did not apply any feature selection. This setting
might be somewhat more difficult than the original one.
needed to investigate optimal procedures for collect-
ing seed features and to find out whether additional
seeds might still contribute to better performance.
Furthermore, it may be very interesting to explore
optimal combinations of intensional and extensional
supervision, provided by the user in the forms of
seed features and labeled examples.
Acknowledgments
This work was developed under the collaboration
ITC-irst/University of Haifa.
References
M. Berry. 1992. Large-scale sparse singular value com-
putations. International Journal of Supercomputer
Applications, 6(1):13?49.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proc. of EMNLP99,
College Park, MD, USA.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
Y. Ko and J. Seo. 2000. Automatic text categorization by
unsupervised learning. In Proc. of COLING?2000.
Y. Ko and J. Seo. 2004. Learning with unlabeled data
for text categorization using bootstrapping abd fea-
ture projection techniques. In Proc. of the ACL-04,
Barcelona, Spain, July.
B. Liu, X. Li, W. S. Lee, and P. S. Yu. 2004. Text clas-
sification by labeling words. In Proc. of AAAI-04, San
Jose, July.
A. McCallum and K. Nigam. 1999. Text classification
by bootstrapping with keywords, em and shrinkage. In
ACL99 - Workshop for Unsupervised Learning in Nat-
ural Language Processing.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
B. W. Silverman. 1986. Density Estimation for Statistics
and Data Analysis. Chapman and Hall.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL-95, pages 189?196, Cambridge, MA.
136
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 531?538, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Making Computers Laugh:
Investigations in Automatic Humor Recognition
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX, 76203, USA
rada@cs.unt.edu
Carlo Strapparava
Istituto per la Ricerca Scientifica e Tecnologica
ITC ? irst
I-38050, Povo, Trento, Italy
strappa@itc.it
Abstract
Humor is one of the most interesting and
puzzling aspects of human behavior. De-
spite the attention it has received in fields
such as philosophy, linguistics, and psy-
chology, there have been only few at-
tempts to create computational models for
humor recognition or generation. In this
paper, we bring empirical evidence that
computational approaches can be success-
fully applied to the task of humor recogni-
tion. Through experiments performed on
very large data sets, we show that auto-
matic classification techniques can be ef-
fectively used to distinguish between hu-
morous and non-humorous texts, with sig-
nificant improvements observed over apri-
ori known baselines.
1 Introduction
... pleasure has probably been the main goal all along. But I hesitate
to admit it, because computer scientists want to maintain their image
as hard-working individuals who deserve high salaries. Sooner or
later society will realize that certain kinds of hard work are in fact
admirable even though they are more fun than just about anything
else. (Knuth, 1993)
Humor is an essential element in personal com-
munication. While it is merely considered a way
to induce amusement, humor also has a positive ef-
fect on the mental state of those using it and has the
ability to improve their activity. Therefore computa-
tional humor deserves particular attention, as it has
the potential of changing computers into a creative
and motivational tool for human activity (Stock et
al., 2002; Nijholt et al, 2003).
Previous work in computational humor has fo-
cused mainly on the task of humor generation (Stock
and Strapparava, 2003; Binsted and Ritchie, 1997),
and very few attempts have been made to develop
systems for automatic humor recognition (Taylor
and Mazlack, 2004). This is not surprising, since,
from a computational perspective, humor recogni-
tion appears to be significantly more subtle and dif-
ficult than humor generation.
In this paper, we explore the applicability of
computational approaches to the recognition of ver-
bally expressed humor. In particular, we investigate
whether automatic classification techniques are a vi-
able approach to distinguish between humorous and
non-humorous text, and we bring empirical evidence
in support of this hypothesis through experiments
performed on very large data sets.
Since a deep comprehension of humor in all of
its aspects is probably too ambitious and beyond
the existing computational capabilities, we chose
to restrict our investigation to the type of humor
found in one-liners. A one-liner is a short sen-
tence with comic effects and an interesting linguistic
structure: simple syntax, deliberate use of rhetoric
devices (e.g. alliteration, rhyme), and frequent use
of creative language constructions meant to attract
the readers attention. While longer jokes can have
a relatively complex narrative structure, a one-liner
must produce the humorous effect ?in one shot?,
with very few words. These characteristics make
this type of humor particularly suitable for use in an
automatic learning setting, as the humor-producing
features are guaranteed to be present in the first (and
only) sentence.
We attempt to formulate the humor-recognition
531
problem as a traditional classification task, and feed
positive (humorous) and negative (non-humorous)
examples to an automatic classifier. The humor-
ous data set consists of one-liners collected from
the Web using an automatic bootstrapping process.
The non-humorous data is selected such that it
is structurally and stylistically similar to the one-
liners. Specifically, we use three different nega-
tive data sets: (1) Reuters news titles; (2) proverbs;
and (3) sentences from the British National Corpus
(BNC). The classification results are encouraging,
with accuracy figures ranging from 79.15% (One-
liners/BNC) to 96.95% (One-liners/Reuters). Re-
gardless of the non-humorous data set playing the
role of negative examples, the performance of the
automatically learned humor-recognizer is always
significantly better than apriori known baselines.
The remainder of the paper is organized as fol-
lows. We first describe the humorous and non-
humorous data sets, and provide details on the Web-
based bootstrapping process used to build a very
large collection of one-liners. We then show experi-
mental results obtained on these data sets using sev-
eral heuristics and two different text classifiers. Fi-
nally, we conclude with a discussion and directions
for future work.
2 Humorous and Non-humorous Data Sets
To test our hypothesis that automatic classification
techniques represent a viable approach to humor
recognition, we needed in the first place a data set
consisting of both humorous (positive) and non-
humorous (negative) examples. Such data sets can
be used to automatically learn computational mod-
els for humor recognition, and at the same time eval-
uate the performance of such models.
2.1 Humorous Data
For reasons outlined earlier, we restrict our attention
to one-liners, short humorous sentences that have the
characteristic of producing a comic effect in very
few words (usually 15 or less). The one-liners hu-
mor style is illustrated in Table 1, which shows three
examples of such one-sentence jokes.
It is well-known that large amounts of training
data have the potential of improving the accuracy of
the learning process, and at the same time provide
insights into how increasingly larger data sets can
affect the classification precision. The manual con-
enumerations matching
stylistic constraint (2)?
yes
yes
seed one?liners
automatically identified
        one?liners
Web search
webpages matching 
thematic constraint (1)?
candidate
webpages
Figure 1: Web-based bootstrapping of one-liners.
struction of a very large one-liner data set may be
however problematic, since most Web sites or mail-
ing lists that make available such jokes do not usu-
ally list more than 50?100 one-liners. To tackle this
problem, we implemented a Web-based bootstrap-
ping algorithm able to automatically collect a large
number of one-liners starting with a short seed list,
consisting of a few one-liners manually identified.
The bootstrapping process is illustrated in Figure
1. Starting with the seed set, the algorithm auto-
matically identifies a list of webpages that include at
least one of the seed one-liners, via a simple search
performed with a Web search engine. Next, the web-
pages found in this way are HTML parsed, and ad-
ditional one-liners are automatically identified and
added to the seed set. The process is repeated sev-
eral times, until enough one-liners are collected.
An important aspect of any bootstrapping algo-
rithm is the set of constraints used to steer the pro-
cess and prevent as much as possible the addition of
noisy entries. Our algorithm uses: (1) a thematic
constraint applied to the theme of each webpage;
and (2) a structural constraint, exploiting HTML an-
notations indicating text of similar genre.
The first constraint is implemented using a set
of keywords of which at least one has to appear
in the URL of a retrieved webpage, thus poten-
tially limiting the content of the webpage to a
theme related to that keyword. The set of key-
words used in the current implementation consists
of six words that explicitly indicate humor-related
content: oneliner, one-liner, humor, humour, joke,
532
One-liners
Take my advice; I don?t use it anyway.
I get enough exercise just pushing my luck.
Beauty is in the eye of the beer holder.
Reuters titles
Trocadero expects tripling of revenues.
Silver fixes at two-month high, but gold lags.
Oil prices slip as refiners shop for bargains.
BNC sentences
They were like spirits, and I loved them.
I wonder if there is some contradiction here.
The train arrives three minutes early.
Proverbs
Creativity is more important than knowledge.
Beauty is in the eye of the beholder.
I believe no tales from an enemy?s tongue.
Table 1: Sample examples of one-liners, Reuters ti-
tles, BNC sentences, and proverbs.
funny. For example, http://www.berro.com/Jokes
or http://www.mutedfaith.com/funny/life.htm are the
URLs of two webpages that satisfy this constraint.
The second constraint is designed to exploit the
HTML structure of webpages, in an attempt to iden-
tify enumerations of texts that include the seed one-
liner. This is based on the hypothesis that enumer-
ations typically include texts of similar genre, and
thus a list including the seed one-liner is likely to
include additional one-line jokes. For instance, if a
seed one-liner is found in a webpage preceded by the
HTML tag <li> (i.e. ?list item?), other lines found
in the same enumeration preceded by the same tag
are also likely to be one-liners.
Two iterations of the bootstrapping process,
started with a small seed set of ten one-liners, re-
sulted in a large set of about 24,000 one-liners.
After removing the duplicates using a measure of
string similarity based on the longest common sub-
sequence metric, we were left with a final set of
approximately 16,000 one-liners, which are used in
the humor-recognition experiments. Note that since
the collection process is automatic, noisy entries are
also possible. Manual verification of a randomly se-
lected sample of 200 one-liners indicates an average
of 9% potential noise in the data set, which is within
reasonable limits, as it does not appear to signifi-
cantly impact the quality of the learning.
2.2 Non-humorous Data
To construct the set of negative examples re-
quired by the humor-recognition models, we tried
to identify collections of sentences that were non-
humorous, but similar in structure and composition
to the one-liners. We do not want the automatic clas-
sifiers to learn to distinguish between humorous and
non-humorous examples based simply on text length
or obvious vocabulary differences. Instead, we seek
to enforce the classifiers to identify humor-specific
features, by supplying them with negative examples
similar in most of their aspects to the positive exam-
ples, but different in their comic effect.
We tested three different sets of negative exam-
ples, with three examples from each data set illus-
trated in Table 1. All non-humorous examples are
enforced to follow the same length restriction as the
one-liners, i.e. one sentence with an average length
of 10?15 words.
1. Reuters titles, extracted from news articles pub-
lished in the Reuters newswire over a period of
one year (8/20/1996 ? 8/19/1997) (Lewis et al,
2004). The titles consist of short sentences with
simple syntax, and are often phrased to catch
the readers attention (an effect similar to the
one rendered by one-liners).
2. Proverbs extracted from an online proverb col-
lection. Proverbs are sayings that transmit, usu-
ally in one short sentence, important facts or
experiences that are considered true by many
people. Their property of being condensed, but
memorable sayings make them very similar to
the one-liners. In fact, some one-liners attempt
to reproduce proverbs, with a comic effect, as
in e.g. ?Beauty is in the eye of the beer holder?,
derived from ?Beauty is in the eye of the be-
holder?.
3. British National Corpus (BNC) sentences, ex-
tracted from BNC ? a balanced corpus covering
different styles, genres and domains. The sen-
tences were selected such that they were similar
in content with the one-liners: we used an in-
formation retrieval system implementing a vec-
torial model to identify the BNC sentence most
similar to each of the 16,000 one-liners1 . Un-
like the Reuters titles or the proverbs, the BNC
sentences have typically no added creativity.
However, we decided to add this set of negative
examples to our experimental setting, in order
1The sentence most similar to a one-liner is identified by
running the one-liner against an index built for all BNC sen-
tences with a length of 10?15 words. We use a tf.idf weighting
scheme and a cosine similarity measure, as implemented in the
Smart system (ftp.cs.cornell.edu/pub/smart)
533
to observe the level of difficulty of a humor-
recognition task when performed with respect
to simple text.
To summarize, the humor recognition experiments
rely on data sets consisting of humorous (positive)
and non-humorous (negative) examples. The posi-
tive examples consist of 16,000 one-liners automat-
ically collected using a Web-based bootstrapping
process. The negative examples are drawn from: (1)
Reuters titles; (2) Proverbs; and (3) BNC sentences.
3 Automatic Humor Recognition
We experiment with automatic classification tech-
niques using: (a) heuristics based on humor-specific
stylistic features (alliteration, antonymy, slang); (b)
content-based features, within a learning framework
formulated as a typical text classification task; and
(c) combined stylistic and content-based features,
integrated in a stacked machine learning framework.
3.1 Humor-Specific Stylistic Features
Linguistic theories of humor (Attardo, 1994) have
suggested many stylistic features that characterize
humorous texts. We tried to identify a set of fea-
tures that were both significant and feasible to im-
plement using existing machine readable resources.
Specifically, we focus on alliteration, antonymy, and
adult slang, which were previously suggested as po-
tentially good indicators of humor (Ruch, 2002; Bu-
caria, 2004).
Alliteration. Some studies on humor appreciation
(Ruch, 2002) show that structural and phonetic prop-
erties of jokes are at least as important as their con-
tent. In fact one-liners often rely on the reader?s
awareness of attention-catching sounds, through lin-
guistic phenomena such as alliteration, word repeti-
tion and rhyme, which produce a comic effect even if
the jokes are not necessarily meant to be read aloud.
Note that similar rhetorical devices play an impor-
tant role in wordplay jokes, and are often used in
newspaper headlines and in advertisement. The fol-
lowing one-liners are examples of jokes that include
one or more alliteration chains:
Veni, Vidi, Visa: I came, I saw, I did a little shopping.
Infants don?t enjoy infancy like adults do adultery.
To extract this feature, we identify and count the
number of alliteration/rhyme chains in each exam-
ple in our data set. The chains are automatically ex-
tracted using an index created on top of the CMU
pronunciation dictionary2 .
Antonymy. Humor often relies on some type of
incongruity, opposition or other forms of apparent
contradiction. While an accurate identification of
all these properties is probably difficult to accom-
plish, it is relatively easy to identify the presence of
antonyms in a sentence. For instance, the comic ef-
fect produced by the following one-liners is partly
due to the presence of antonyms:
A clean desk is a sign of a cluttered desk drawer.
Always try to be modest and be proud of it!
The lexical resource we use to identify antonyms
is WORDNET (Miller, 1995), and in particular the
antonymy relation among nouns, verbs, adjectives
and adverbs. For adjectives we also consider an in-
direct antonymy via the similar-to relation among
adjective synsets. Despite the relatively large num-
ber of antonymy relations defined in WORDNET,
its coverage is far from complete, and thus the
antonymy feature cannot always be identified. A
deeper semantic analysis of the text, such as word
sense disambiguation or domain disambiguation,
could probably help detecting other types of seman-
tic opposition, and we plan to exploit these tech-
niques in future work.
Adult slang. Humor based on adult slang is very
popular. Therefore, a possible feature for humor-
recognition is the detection of sexual-oriented lexi-
con in the sentence. The following represent exam-
ples of one-liners that include such slang:
The sex was so good that even the neighbors had a cigarette.
Artificial Insemination: procreation without recreation.
To form a lexicon required for the identification of
this feature, we extract from WORDNET DOMAINS3
all the synsets labeled with the domain SEXUALITY.
The list is further processed by removing all words
with high polysemy (? 4). Next, we check for the
presence of the words in this lexicon in each sen-
tence in the corpus, and annotate them accordingly.
Note that, as in the case of antonymy, WORDNET
coverage is not complete, and the adult slang fea-
ture cannot always be identified.
Finally, in some cases, all three features (alliteration,
2Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict
3WORDNET DOMAINS assigns each synset in WORDNET
with one or more ?domain? labels, such as SPORT, MEDICINE,
ECONOMY. See http://wndomains.itc.it.
534
antonymy, adult slang) are present in the same sen-
tence, as for instance the following one-liner:
Behind every greatal manant is a greatal womanant, and
behind every greatal womanant is some guy staring at her
behindsl!
3.2 Content-based Learning
In addition to stylistic features, we also experi-
mented with content-based features, through ex-
periments where the humor-recognition task is for-
mulated as a traditional text classification problem.
Specifically, we compare results obtained with two
frequently used text classifiers, Na??ve Bayes and
Support Vector Machines, selected based on their
performance in previously reported work, and for
their diversity of learning methodologies.
Na??ve Bayes. The main idea in a Na??ve Bayes text
classifier is to estimate the probability of a category
given a document using joint probabilities of words
and documents. Na??ve Bayes classifiers assume
word independence, but despite this simplification,
they perform well on text classification. While there
are several versions of Na??ve Bayes classifiers (vari-
ations of multinomial and multivariate Bernoulli),
we use the multinomial model, previously shown to
be more effective (McCallum and Nigam, 1998).
Support Vector Machines. Support Vector Ma-
chines (SVM) are binary classifiers that seek to find
the hyperplane that best separates a set of posi-
tive examples from a set of negative examples, with
maximum margin. Applications of SVM classifiers
to text categorization led to some of the best results
reported in the literature (Joachims, 1998).
4 Experimental Results
Several experiments were conducted to gain insights
into various aspects related to an automatic hu-
mor recognition task: classification accuracy using
stylistic and content-based features, learning rates,
impact of the type of negative data, impact of the
classification methodology.
All evaluations are performed using stratified ten-
fold cross validations, for accurate estimates. The
baseline for all the experiments is 50%, which rep-
resents the classification accuracy obtained if a label
of ?humorous? (or ?non-humorous?) would be as-
signed by default to all the examples in the data set.
Experiments with uneven class distributions were
also performed, and are reported in section 4.4.
4.1 Heuristics using Humor-specific Features
In a first set of experiments, we evaluated the classi-
fication accuracy using stylistic humor-specific fea-
tures: alliteration, antonymy, and adult slang. These
are numerical features that act as heuristics, and the
only parameter required for their application is a
threshold indicating the minimum value admitted for
a statement to be classified as humorous (or non-
humorous). These thresholds are learned automat-
ically using a decision tree applied on a small subset
of humorous/non-humorous examples (1000 exam-
ples). The evaluation is performed on the remaining
15,000 examples, with results shown in Table 24.
One-liners One-liners One-liners
Heuristic Reuters BNC Proverbs
Alliteration 74.31% 59.34% 53.30%
Antonymy 55.65% 51.40% 50.51%
Adult slang 52.74% 52.39% 50.74%
ALL 76.73% 60.63% 53.71%
Table 2: Humor-recognition accuracy using allitera-
tion, antonymy, and adult slang.
Considering the fact that these features represent
stylistic indicators, the style of Reuters titles turns
out to be the most different with respect to one-
liners, while the style of proverbs is the most sim-
ilar. Note that for all data sets the alliteration feature
appears to be the most useful indicator of humor,
which is in agreement with previous linguistic find-
ings (Ruch, 2002).
4.2 Text Classification with Content Features
The second set of experiments was concerned with
the evaluation of content-based features for humor
recognition. Table 3 shows results obtained using
the three different sets of negative examples, with
the Na??ve Bayes and SVM text classifiers. Learning
curves are plotted in Figure 2.
One-liners One-liners One-liners
Classifier Reuters BNC Proverbs
Na ??ve Bayes 96.67% 73.22% 84.81%
SVM 96.09% 77.51% 84.48%
Table 3: Humor-recognition accuracy using Na??ve
Bayes and SVM text classifiers.
4We also experimented with decision trees learned from a
larger number of examples, but the results were similar, which
confirms our hypothesis that these features are heuristics, rather
than learnable properties that improve their accuracy with addi-
tional training data.
535
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Naive Bayes
SVM
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Naive Bayes
SVM
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Naive Bayes
SVM
(a) (b) (c)
Figure 2: Learning curves for humor-recognition using text classification techniques, with respect to three
different sets of negative examples: (a) Reuters; (b) BNC; (c) Proverbs.
Once again, the content of Reuters titles appears
to be the most different with respect to one-liners,
while the BNC sentences represent the most simi-
lar data set. This suggests that joke content tends to
be very similar to regular text, although a reasonably
accurate distinction can still be made using text clas-
sification techniques. Interestingly, proverbs can be
distinguished from one-liners using content-based
features, which indicates that despite their stylistic
similarity (see Table 2), proverbs and one-liners deal
with different topics.
4.3 Combining Stylistic and Content Features
Encouraged by the results obtained in the first
two experiments, we designed a third experiment
that attempts to jointly exploit stylistic and con-
tent features for humor recognition. The feature
combination is performed using a stacked learner,
which takes the output of the text classifier, joins it
with the three humor-specific features (alliteration,
antonymy, adult slang), and feeds the newly created
feature vectors to a machine learning tool. Given
the relatively large gap between the performance
achieved with content-based features (text classifi-
cation) and stylistic features (humor-specific heuris-
tics), we decided to implement the second learning
stage in the stacked learner using a memory based
learning system, so that low-performance features
are not eliminated in the favor of the more accu-
rate ones5. We use the Timbl memory based learner
(Daelemans et al, 2001), and evaluate the classifica-
tion using a stratified ten-fold cross validation. Table
5Using a decision tree learner in a similar stacked learning
experiment resulted into a flat tree that takes a classification de-
cision based exclusively on the content feature, ignoring com-
pletely the remaining stylistic features.
4 shows the results obtained in this experiment, for
the three different data sets.
One-liners One-liners One-liners
Reuters BNC Proverbs
96.95% 79.15% 84.82%
Table 4: Humor-recognition accuracy for combined
learning based on stylistic and content features.
Combining classifiers results in a statistically sig-
nificant improvement (p < 0.0005, paired t-test)
with respect to the best individual classifier for the
One-liners/Reuters and One-liners/BNC data sets,
with relative error rate reductions of 8.9% and 7.3%
respectively. No improvement is observed for the
One-liners/Proverbs data set, which is not surpris-
ing since, as shown in Table 2, proverbs and one-
liners cannot be clearly differentiated using stylistic
features, and thus the addition of these features to
content-based features is not likely to result in an
improvement.
4.4 Discussion
The results obtained in the automatic classification
experiments reveal the fact that computational ap-
proaches represent a viable solution for the task of
humor-recognition, and good performance can be
achieved using classification techniques based on
stylistic and content features.
Despite our initial intuition that one-liners are
most similar to other creative texts (e.g. Reuters ti-
tles, or the sometimes almost identical proverbs),
and thus the learning task would be more difficult in
relation to these data sets, comparative experimental
results show that in fact it is more difficult to distin-
guish humor with respect to regular text (e.g. BNC
536
sentences). Note however that even in this case the
combined classifier leads to a classification accuracy
that improves significantly over the apriori known
baseline.
An examination of the content-based features
learned during the classification process reveals in-
teresting aspects of the humorous texts. For in-
stance, one-liners seem to constantly make reference
to human-related scenarios, through the frequent use
of words such as man, woman, person, you, I. Simi-
larly, humorous texts seem to often include negative
word forms, such as the negative verb forms doesn?t,
isn?t, don?t, or negative adjectives like wrong or bad.
A more extensive analysis of content-based humor-
specific features is likely to reveal additional humor-
specific content features, which could also be used in
studies of humor generation.
In addition to the three negative data sets, we also
performed an experiment using a corpus of arbitrary
sentences randomly drawn from the three negative
sets. The humor recognition with respect to this neg-
ative mixed data set resulted in 63.76% accuracy for
stylistic features, 77.82% for content-based features
using Na??ve Bayes and 79.23% using SVM. These
figures are comparable to those reported in Tables 2
and 3 for One-liners/BNC, which suggests that the
experimental results reported in the previous sec-
tions do not reflect a bias introduced by the negative
data sets, since similar results are obtained when the
humor recognition is performed with respect to ar-
bitrary negative examples.
As indicated in section 2.2, the negative exam-
ples were selected structurally and stylistically sim-
ilar to the one-liners, making the humor recognition
task more difficult than in a real setting. Nonethe-
less, we also performed a set of experiments where
we made the task even harder, using uneven class
distributions. For each of the three types of nega-
tive examples, we constructed a data set using 75%
non-humorous examples and 25% humorous exam-
ples. Although the baseline in this case is higher
(75%), the automatic classification techniques for
humor-recognition still improve over this baseline.
The stylistic features lead to a classification accu-
racy of 87.49% (One-liners/Reuters), 77.62% (One-
liners/BNC), and 76.20% (One-liners/Proverbs),
and the content-based features used in a Na??ve
Bayes classifier result in accuracy figures of 96.19%
(One-liners/Reuters), 81.56% (One-liners/BNC),
and 87.86% (One-liners/Proverbs).
Finally, in addition to classification accuracy, we
were also interested in the variation of classifica-
tion performance with respect to data size, which
is an aspect particularly relevant for directing fu-
ture research. Depending on the shape of the learn-
ing curves, one could decide to concentrate future
work either on the acquisition of larger data sets, or
toward the identification of more sophisticated fea-
tures. Figure 2 shows that regardless of the type of
negative data, there is significant learning only un-
til about 60% of the data (i.e. about 10,000 positive
examples, and the same number of negative exam-
ples). The rather steep ascent of the curve, especially
in the first part of the learning, suggests that humor-
ous and non-humorous texts represent well distin-
guishable types of data. An interesting effect can
be noticed toward the end of the learning, where for
both classifiers the curve becomes completely flat
(One-liners/Reuters, One-liners/Proverbs), or it even
has a slight drop (One-liners/BNC). This is probably
due to the presence of noise in the data set, which
starts to become visible for very large data sets6.
This plateau is also suggesting that more data is not
likely to help improve the quality of an automatic
humor-recognizer, and more sophisticated features
are probably required.
5 Related Work
While humor is relatively well studied in scientific
fields such as linguistics (Attardo, 1994) and psy-
chology (Freud, 1905; Ruch, 2002), to date there
is only a limited number of research contributions
made toward the construction of computational hu-
mour prototypes.
One of the first attempts is perhaps the work de-
scribed in (Binsted and Ritchie, 1997), where a for-
mal model of semantic and syntactic regularities was
devised, underlying some of the simplest types of
puns (punning riddles). The model was then ex-
ploited in a system called JAPE that was able to au-
tomatically generate amusing puns.
Another humor-generation project was the HA-
HAcronym project (Stock and Strapparava, 2003),
whose goal was to develop a system able to au-
tomatically generate humorous versions of existing
6We also like to think of this behavior as if the computer
is losing its sense of humor after an overwhelming number of
jokes, in a way similar to humans when they get bored and stop
appreciating humor after hearing too many jokes.
537
acronyms, or to produce a new amusing acronym
constrained to be a valid vocabulary word, starting
with concepts provided by the user. The comic ef-
fect was achieved mainly by exploiting incongruity
theories (e.g. finding a religious variation for a tech-
nical acronym).
Another related work, devoted this time to the
problem of humor comprehension, is the study re-
ported in (Taylor and Mazlack, 2004), focused on
a very restricted type of wordplays, namely the
?Knock-Knock? jokes. The goal of the study was
to evaluate to what extent wordplay can be automati-
cally identified in ?Knock-Knock? jokes, and if such
jokes can be reliably recognized from other non-
humorous text. The algorithm was based on auto-
matically extracted structural patterns and on heuris-
tics heavily based on the peculiar structure of this
particular type of jokes. While the wordplay recog-
nition gave satisfactory results, the identification of
jokes containing such wordplays turned out to be
significantly more difficult.
6 Conclusion
A conclusion is simply the place where you got tired of thinking.
(anonymous one-liner)
The creative genres of natural language have been
traditionally considered outside the scope of any
computational modeling. In particular humor, be-
cause of its puzzling nature, has received little atten-
tion from computational linguists. However, given
the importance of humor in our everyday life, and
the increasing importance of computers in our work
and entertainment, we believe that studies related to
computational humor will become increasingly im-
portant.
In this paper, we showed that automatic classifi-
cation techniques can be successfully applied to the
task of humor-recognition. Experimental results ob-
tained on very large data sets showed that computa-
tional approaches can be efficiently used to distin-
guish between humorous and non-humorous texts,
with significant improvements observed over apriori
known baselines. To our knowledge, this is the first
result of this kind reported in the literature, as we
are not aware of any previous work investigating the
interaction between humor and techniques for auto-
matic classification.
Finally, through the analysis of learning curves
plotting the classification performance with respect
to data size, we showed that the accuracy of the au-
tomatic humor-recognizer stops improving after a
certain number of examples. Given that automatic
humor-recognition is a rather understudied problem,
we believe that this is an important result, as it pro-
vides insights into potentially productive directions
for future work. The flattened shape of the curves
toward the end of the learning process suggests that
rather than focusing on gathering more data, fu-
ture work should concentrate on identifying more
sophisticated humor-specific features, e.g. semantic
oppositions, ambiguity, and others. We plan to ad-
dress these aspects in future work.
References
S. Attardo. 1994. Linguistic Theory of Humor. Mouton de
Gruyter, Berlin.
K. Binsted and G. Ritchie. 1997. Computational rules for pun-
ning riddles. Humor, 10(1).
C. Bucaria. 2004. Lexical and syntactic ambiguity as a source
of humor. Humor, 17(3).
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner, ver-
sion 4.0, reference guide. Technical report, University of
Antwerp.
S. Freud. 1905. Der Witz und Seine Beziehung zum Unbe-
wussten. Deutike, Vienna.
T. Joachims. 1998. Text categorization with Support Vector
Machines: learning with many relevant features. In Pro-
ceedings of the European Conference on Machine Learning.
D.E. Knuth. 1993. The Stanford Graph Base: A Platform for
combinatorial computing. ACM Press.
D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1: A new
benchmark collection for text categorization research. The
Journal of Machine Learning Research, 5:361?397.
A. McCallum and K. Nigam. 1998. A comparison of event
models for Naive Bayes text classification. In Proceedings
of AAAI-98 Workshop on Learning for Text Categorization.
G. Miller. 1995. Wordnet: A lexical database. Communication
of the ACM, 38(11):39?41.
A. Nijholt, O. Stock, A. Dix, and J. Morkes, editors. 2003. Pro-
ceedings of CHI-2003 workshop: Humor Modeling in the
Interface, Fort Lauderdale, Florida.
W. Ruch. 2002. Computers with a personality? lessons to be
learned from studies of the psychology of humor. In Pro-
ceedings of the The April Fools Day Workshop on Computa-
tional Humour.
O. Stock and C. Strapparava. 2003. Getting serious about the
development of computational humour. In Proceedings of
the 8th International Joint Conference on Artificial Intelli-
gence (IJCAI-03), Acapulco, Mexico.
O. Stock, C. Strapparava, and A. Nijholt, editors. 2002. Pro-
ceedings of the The April Fools Day Workshop on Computa-
tional Humour, Trento.
J. Taylor and L. Mazlack. 2004. Computationally recognizing
wordplay in jokes. In Proceedings of CogSci 2004, Chicago.
538
Proceedings of the 43rd Annual Meeting of the ACL, pages 403?410,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Domain Kernels for Word Sense Disambiguation
Alfio Gliozzo and Claudio Giuliano and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica
I-38050, Trento, ITALY
{gliozzo,giuliano,strappa}@itc.it
Abstract
In this paper we present a supervised
Word Sense Disambiguation methodol-
ogy, that exploits kernel methods to model
sense distinctions. In particular a combi-
nation of kernel functions is adopted to
estimate independently both syntagmatic
and domain similarity. We defined a ker-
nel function, namely the Domain Kernel,
that allowed us to plug ?external knowl-
edge? into the supervised learning pro-
cess. External knowledge is acquired from
unlabeled data in a totally unsupervised
way, and it is represented by means of Do-
main Models. We evaluated our method-
ology on several lexical sample tasks in
different languages, outperforming sig-
nificantly the state-of-the-art for each of
them, while reducing the amount of la-
beled training data required for learning.
1 Introduction
The main limitation of many supervised approaches
for Natural Language Processing (NLP) is the lack
of available annotated training data. This problem is
known as the Knowledge Acquisition Bottleneck.
To reach high accuracy, state-of-the-art systems
for Word Sense Disambiguation (WSD) are de-
signed according to a supervised learning frame-
work, in which the disambiguation of each word
in the lexicon is performed by constructing a dif-
ferent classifier. A large set of sense tagged exam-
ples is then required to train each classifier. This
methodology is called word expert approach (Small,
1980; Yarowsky and Florian, 2002). However this
is clearly unfeasible for all-words WSD tasks, in
which all the words of an open text should be dis-
ambiguated.
On the other hand, the word expert approach
works very well for lexical sample WSD tasks (i.e.
tasks in which it is required to disambiguate only
those words for which enough training data is pro-
vided). As the original rationale of the lexical sam-
ple tasks was to define a clear experimental settings
to enhance the comprehension of WSD, they should
be considered as preceding exercises to all-words
tasks. However this is not the actual case. Algo-
rithms designed for lexical sample WSD are often
based on pure supervision and hence ?data hungry?.
We think that lexical sample WSD should regain
its original explorative role and possibly use a min-
imal amount of training data, exploiting instead ex-
ternal knowledge acquired in an unsupervised way
to reach the actual state-of-the-art performance.
By the way, minimal supervision is the basis
of state-of-the-art systems for all-words tasks (e.g.
(Mihalcea and Faruque, 2004; Decadt et al, 2004)),
that are trained on small sense tagged corpora (e.g.
SemCor), in which few examples for a subset of the
ambiguous words in the lexicon can be found. Thus
improving the performance of WSD systems with
few learning examples is a fundamental step towards
the direction of designing a WSD system that works
well on real texts.
In addition, it is a common opinion that the per-
formance of state-of-the-art WSD systems is not sat-
isfactory from an applicative point of view yet.
403
To achieve these goals we identified two promis-
ing research directions:
1. Modeling independently domain and syntag-
matic aspects of sense distinction, to improve
the feature representation of sense tagged ex-
amples (Gliozzo et al, 2004).
2. Leveraging external knowledge acquired from
unlabeled corpora.
The first direction is motivated by the linguistic
assumption that syntagmatic and domain (associa-
tive) relations are both crucial to represent sense
distictions, while they are basically originated by
very different phenomena. Syntagmatic relations
hold among words that are typically located close
to each other in the same sentence in a given tempo-
ral order, while domain relations hold among words
that are typically used in the same semantic domain
(i.e. in texts having similar topics (Gliozzo et al,
2004)). Their different nature suggests to adopt dif-
ferent learning strategies to detect them.
Regarding the second direction, external knowl-
edge would be required to help WSD algorithms to
better generalize over the data available for train-
ing. On the other hand, most of the state-of-the-art
supervised approaches to WSD are still completely
based on ?internal? information only (i.e. the only
information available to the training algorithm is the
set of manually annotated examples). For exam-
ple, in the Senseval-3 evaluation exercise (Mihal-
cea and Edmonds, 2004) many lexical sample tasks
were provided, beyond the usual labeled training
data, with a large set of unlabeled data. However,
at our knowledge, none of the participants exploited
this unlabeled material. Exploring this direction is
the main focus of this paper. In particular we ac-
quire a Domain Model (DM) for the lexicon (i.e.
a lexical resource representing domain associations
among terms), and we exploit this information in-
side our supervised WSD algorithm. DMs can be
automatically induced from unlabeled corpora, al-
lowing the portability of the methodology among
languages.
We identified kernel methods as a viable frame-
work in which to implement the assumptions above
(Strapparava et al, 2004).
Exploiting the properties of kernels, we have de-
fined independently a set of domain and syntagmatic
kernels and we combined them in order to define a
complete kernel for WSD. The domain kernels esti-
mate the (domain) similarity (Magnini et al, 2002)
among contexts, while the syntagmatic kernels eval-
uate the similarity among collocations.
We will demonstrate that using DMs induced
from unlabeled corpora is a feasible strategy to in-
crease the generalization capability of the WSD al-
gorithm. Our system far outperforms the state-of-
the-art systems in all the tasks in which it has been
tested. Moreover, a comparative analysis of the
learning curves shows that the use of DMs allows
us to remarkably reduce the amount of sense-tagged
examples, opening new scenarios to develop sys-
tems for all-words tasks with minimal supervision.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model. In particular
an automatic acquisition technique based on Latent
Semantic Analysis (LSA) is described. In Section 3
we present a WSD system based on a combination
of kernels. In particular we define a Domain Ker-
nel (see Section 3.1) and a Syntagmatic Kernel (see
Section 3.2), to model separately syntagmatic and
domain aspects. In Section 4 our WSD system is
evaluated in the Senseval-3 English, Italian, Spanish
and Catalan lexical sample tasks.
2 Domain Models
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let C = {t1, t2, . . . , tn} be a corpus, let
V = {w1, w2, . . . , wk} be its vocabulary, let T be
the k ? n term-by-document matrix representing C ,
such that ti,j is the frequency of word wi into the text
tj . The VSM is a k-dimensional space Rk, in which
the text tj ? C is represented by means of the vec-
tor ~tj such that the ith component of ~tj is ti,j. The
similarity among two texts in the VSM is estimated
by computing the cosine among them.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences ?he is affected by AIDS? and ?HIV is
a virus? do not have any words in common. In the
404
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences ?the laptop has
been infected by a virus? and ?HIV is a virus? would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM in which texts and
terms are represented in a uniform way.
A DM is composed by soft clusters of terms. Each
cluster represents a semantic domain, i.e. a set of
terms that often co-occur in texts having similar top-
ics. A DM is represented by a k?k? rectangular ma-
trix D, containing the degree of association among
terms and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Matrix
DMs can be used to describe lexical ambiguity
and variability. Lexical ambiguity is represented
by associating one term to more than one domain,
while variability is represented by associating dif-
ferent terms to the same domain. For example the
term virus is associated to both the domain COM-
PUTER SCIENCE and the domain MEDICINE (ambi-
guity) while the domain MEDICINE is associated to
both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2, ..., Dk?} be a
set of domains, such that k?  k. A DM is fully
defined by a k?k? domain matrix D representing in
each cell di,z the domain relevance of term wi with
respect to the domain Dz . The domain matrix D is
used to define a function D : Rk ? Rk? , that maps
the vectors ~tj expressed into the classical VSM, into
the vectors ~t?j in the domain VSM. D is defined by1
D(~tj) = ~tj(IIDFD) = ~t?j (1)
1In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
where IIDF is a k ? k diagonal matrix such that
iIDFi,i = IDF (wi), ~tj is represented as a row vector,
and IDF (wi) is the Inverse Document Frequency of
wi.
Vectors in the domain VSM are called Domain
Vectors (DVs). DVs for texts are estimated by ex-
ploiting the formula 1, while the DV ~w?i, correspond-
ing to the word wi ? V is the ith row of the domain
matrix D. To be a valid domain matrix such vectors
should be normalized (i,e. ? ~w?i, ~w?i? = 1).
In the Domain VSM the similarity among DVs is
estimated by taking into account second order rela-
tions among terms. For example the similarity of the
two sentences ?He is affected by AIDS? and ?HIV
is a virus? is very high, because the terms AIDS,
HIV and virus are highly associated to the domain
MEDICINE.
A DM can be estimated from hand made lexical
resources such as WORDNET DOMAINS (Magnini
and Cavaglia`, 2000), or by performing a term clus-
tering process on a large corpus. We think that the
second methodology is more attractive, because it
allows us to automatically acquire DMs for different
languages.
In this work we propose the use of Latent Seman-
tic Analysis (LSA) to induce DMs from corpora.
LSA is an unsupervised technique for estimating the
similarity among texts and terms in a corpus. LSA
is performed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix T
describing the corpus. The SVD algorithm can be
exploited to acquire a domain matrix D from a large
corpus C in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T ' V?k?UT where ?k? is the diagonal
k ? k matrix containing the highest k ?  k eigen-
values of T, and all the remaining elements set to
0. The parameter k? is the dimensionality of the Do-
main VSM and can be fixed in advance2 . Under this
setting we define the domain matrix DLSA as
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
q
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .3
2It is not clear how to choose the right dimensionality. In
our experiments we used 50 dimensions.
3When DLSA is substituted in Equation 1 the Domain VSM
405
3 Kernel Methods for WSD
In the introduction we discussed two promising di-
rections for improving the performance of a super-
vised disambiguation system. In this section we
show how these requirements can be efficiently im-
plemented in a natural and elegant way by using ker-
nel methods.
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping ?, we can use a kernel
function K : X ? X ? R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Exploiting the properties of the kernel func-
tions, it is possible to define the kernel combination
schema as
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj, xj)Kl(xi, xi)
(3)
Our WSD system is then defined as combination
of n basic kernels. Each kernel adds some addi-
tional dimensions to the feature space. In particular,
we have defined two families of kernels: Domain
and Syntagmatic kernels. The former is composed
by both the Domain Kernel (KD) and the Bag-of-
Words kernel (KBoW ), that captures domain aspects
(see Section 3.1). The latter captures the syntag-
matic aspects of sense distinction and it is composed
by two kernels: the collocation kernel (KColl) and
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
the Part of Speech kernel (KPoS) (see Section 3.2).
The WSD kernels (K ?WSD and KWSD) are then de-
fined by combining them (see Section 3.3).
3.1 Domain Kernels
In (Magnini et al, 2002), it has been claimed that
knowing the domain of the text in which the word
is located is a crucial information for WSD. For
example the (domain) polysemy among the COM-
PUTER SCIENCE and the MEDICINE senses of the
word virus can be solved by simply considering
the domain of the context in which it is located.
This assumption can be modeled by defining a
kernel that estimates the domain similarity among
the contexts of the words to be disambiguated,
namely the Domain Kernel. The Domain Kernel es-
timates the similarity among the topics (domains) of
two texts, so to capture domain aspects of sense dis-
tinction. It is a variation of the Latent Semantic Ker-
nel (Shawe-Taylor and Cristianini, 2004), in which a
DM (see Section 2) is exploited to define an explicit
mapping D : Rk ? Rk? from the classical VSM into
the Domain VSM. The Domain Kernel is defined by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(ti),D(tj)??D(ti),D(tj)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. Thus the Domain Kernel requires a Domain
Matrix D. For our experiments we acquire the ma-
trix DLSA, described in equation 2, from a generic
collection of unlabeled documents, as explained in
Section 2.
A more traditional approach to detect topic (do-
main) similarity is to extract Bag-of-Words (BoW)
features from a large window of text around the
word to be disambiguated. The BoW kernel, de-
noted by KBoW , is a particular case of the Domain
Kernel, in which D = I, and I is the identity ma-
trix. The BoW kernel does not require a DM, then it
can be applied to the ?strictly? supervised settings,
in which an external knowledge source is not pro-
vided.
3.2 Syntagmatic kernels
Kernel functions are not restricted to operate on vec-
torial objects ~x ? Rk. In principle kernels can be
defined for any kind of object representation, as for
406
example sequences and trees. As stated in Section 1,
syntagmatic relations hold among words collocated
in a particular temporal order, thus they can be mod-
eled by analyzing sequences of words.
We identified the string kernel (or word se-
quence kernel) (Shawe-Taylor and Cristianini, 2004)
as a valid instrument to model our assumptions.
The string kernel counts how many times a (non-
contiguous) subsequence of symbols u of length
n occurs in the input string s, and penalizes non-
contiguous occurrences according to the number of
gaps they contain (gap-weighted subsequence ker-
nel).
Formally, let V be the vocabulary, the feature
space associated with the gap-weighted subsequence
kernel of length n is indexed by a set I of subse-
quences over V of length n. The (explicit) mapping
function is defined by
?nu(s) =
?
i:u=s(i)
?l(i), u ? V n (5)
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and ? ?]0, 1] is the decay factor used to penal-
ize non-contiguous subsequences.
The associate gap-weighted subsequence kernel is
defined by
kn(si, sj) = ??n(si), ?n(sj)? =
X
u?V n
?n(si)?n(sj) (6)
We modified the generic definition of the string
kernel in order to make it able to recognize collo-
cations in a local window of the word to be disam-
biguated. In particular we defined two Syntagmatic
kernels: the n-gram Collocation Kernel and the n-
gram PoS Kernel. The n-gram Collocation ker-
nel KnColl is defined as a gap-weighted subsequence
kernel applied to sequences of lemmata around the
word l0 to be disambiguated (i.e. l?3, l?2, l?1, l0,
l+1, l+2, l+3). This formulation allows us to esti-
mate the number of common (sparse) subsequences
of lemmata (i.e. collocations) between two exam-
ples, in order to capture syntagmatic similarity. In
analogy we defined the PoS kernel KnPoS , by setting
s to the sequence of PoSs p?3, p?2, p?1, p0, p+1,
p+2, p+3, where p0 is the PoS of the word to be dis-
ambiguated.
The definition of the gap-weighted subsequence
kernel, provided by equation 6, depends on the pa-
rameter n, that represents the length of the sub-
sequences analyzed when estimating the similarity
among sequences. For example, K2Coll allows us to
represent the bigrams around the word to be disam-
biguated in a more flexible way (i.e. bigrams can be
sparse). In WSD, typical features are bigrams and
trigrams of lemmata and PoSs around the word to
be disambiguated, then we defined the Collocation
Kernel and the PoS Kernel respectively by equations
7 and 84.
KColl(si, sj) =
p
?
l=1
K lColl(si, sj) (7)
KPoS(si, sj) =
p
?
l=1
K lPoS(si, sj) (8)
3.3 WSD kernels
In order to show the impact of using Domain Models
in the supervised learning process, we defined two
WSD kernels, by applying the kernel combination
schema described by equation 3. Thus the following
WSD kernels are fully specified by the list of the
kernels that compose them.
Kwsd composed by KColl, KPoS and KBoW
K?wsd composed by KColl, KPoS , KBoW and KD
The only difference between the two systems is
that K ?wsd uses Domain Kernel KD. K ?wsd exploits
external knowledge, in contrast to Kwsd, whose only
available information is the labeled training data.
4 Evaluation and Discussion
In this section we present the performance of our
kernel-based algorithms for WSD. The objectives of
these experiments are:
? to study the combination of different kernels,
? to understand the benefits of plugging external
information using domain models,
? to verify the portability of our methodology
among different languages.
4The parameters p and ? are optimized by cross-validation.
The best results are obtained setting p = 2, ? = 0.5 for KColl
and ? ? 0 for KPoS .
407
4.1 WSD tasks
We conducted the experiments on four lexical sam-
ple tasks (English, Catalan, Italian and Spanish)
of the Senseval-3 competition (Mihalcea and Ed-
monds, 2004). Table 2 describes the tasks by re-
porting the number of words to be disambiguated,
the mean polysemy, and the dimension of training,
test and unlabeled corpora. Note that the organiz-
ers of the English task did not provide any unlabeled
material. So for English we used a domain model
built from a portion of BNC corpus, while for Span-
ish, Italian and Catalan we acquired DMs from the
unlabeled corpora made available by the organizers.
#w pol # train # test # unlab
Catalan 27 3.11 4469 2253 23935
English 57 6.47 7860 3944 -
Italian 45 6.30 5145 2439 74788
Spanish 46 3.30 8430 4195 61252
Table 2: Dataset descriptions
4.2 Kernel Combination
In this section we present an experiment to em-
pirically study the kernel combination. The basic
kernels (i.e. KBoW , KD , KColl and KPoS) have
been compared to the combined ones (i.e. Kwsd and
K ?wsd) on the English lexical sample task.
The results are reported in Table 3. The results
show that combining kernels significantly improves
the performance of the system.
KD KBoW KPoS KColl Kwsd K?wsd
F1 65.5 63.7 62.9 66.7 69.7 73.3
Table 3: The performance (F1) of each basic ker-
nel and their combination for English lexical sample
task.
4.3 Portability and Performance
We evaluated the performance of K ?wsd and Kwsd on
the lexical sample tasks described above. The results
are showed in Table 4 and indicate that using DMs
allowed K ?wsd to significantly outperform Kwsd.
In addition, K ?wsd turns out the best systems for
all the tested Senseval-3 tasks.
Finally, the performance of K ?wsd are higher than
the human agreement for the English and Spanish
tasks5.
Note that, in order to guarantee an uniform appli-
cation to any language, we do not use any syntactic
information provided by a parser.
4.4 Learning Curves
The Figures 1, 2, 3 and 4 show the learning curves
evaluated on K ?wsd and Kwsd for all the lexical sam-
ple tasks.
The learning curves indicate that K ?wsd is far su-
perior to Kwsd for all the tasks, even with few ex-
amples. The result is extremely promising, for it
demonstrates that DMs allow to drastically reduce
the amount of sense tagged data required for learn-
ing. It is worth noting, as reported in Table 5, that
K ?wsd achieves the same performance of Kwsd using
about half of the training data.
% of training
English 54
Catalan 46
Italian 51
Spanish 50
Table 5: Percentage of sense tagged examples re-
quired by K ?wsd to achieve the same performance of
Kwsd with full training.
5 Conclusion and Future Works
In this paper we presented a supervised algorithm
for WSD, based on a combination of kernel func-
tions. In particular we modeled domain and syn-
tagmatic aspects of sense distinctions by defining
respectively domain and syntagmatic kernels. The
Domain kernel exploits Domain Models, acquired
from ?external? untagged corpora, to estimate the
similarity among the contexts of the words to be dis-
ambiguated. The syntagmatic kernels evaluate the
similarity between collocations.
We evaluated our algorithm on several Senseval-
3 lexical sample tasks (i.e. English, Spanish, Ital-
ian and Catalan) significantly improving the state-ot-
the-art for all of them. In addition, the performance
5It is not clear if the inter-annotator-agreement can be con-
siderated the upper bound for a WSD system.
408
MF Agreement BEST Kwsd K ?wsd DM+
English 55.2 67.3 72.9 69.7 73.3 3.6
Catalan 66.3 93.1 85.2 85.2 89.0 3.8
Italian 18.0 89.0 53.1 53.1 61.3 8.2
Spanish 67.7 85.3 84.2 84.2 88.2 4.0
Table 4: Comparative evaluation on the lexical sample tasks. Columns report: the Most Frequent baseline,
the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of Kwsd, the F1 of K ?wsd,
DM+ (the improvement due to DM, i.e. K ?wsd ?Kwsd).
0.5
0.55
0.6
0.65
0.7
0.75
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 1: Learning curves for English lexical sample
task.
0.65
0.7
0.75
0.8
0.85
0.9
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 2: Learning curves for Catalan lexical sample
task.
of our system outperforms the inter annotator agree-
ment in both English and Spanish, achieving the up-
per bound performance.
We demonstrated that using external knowledge
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 3: Learning curves for Italian lexical sample
task.
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 4: Learning curves for Spanish lexical sam-
ple task.
inside a supervised framework is a viable method-
ology to reduce the amount of training data required
for learning. In our approach the external knowledge
is represented by means of Domain Models automat-
409
ically acquired from corpora in a totally unsuper-
vised way. Experimental results show that the use
of Domain Models allows us to reduce the amount
of training data, opening an interesting research di-
rection for all those NLP tasks for which the Knowl-
edge Acquisition Bottleneck is a crucial problem. In
particular we plan to apply the same methodology to
Text Categorization, by exploiting the Domain Ker-
nel to estimate the similarity among texts. In this im-
plementation, our WSD system does not exploit syn-
tactic information produced by a parser. For the fu-
ture we plan to integrate such information by adding
a tree kernel (i.e. a kernel function that evaluates the
similarity among parse trees) to the kernel combi-
nation schema presented in this paper. Last but not
least, we are going to apply our approach to develop
supervised systems for all-words tasks, where the
quantity of data available to train each word expert
classifier is very low.
Acknowledgments
Alfio Gliozzo and Carlo Strapparava were partially
supported by the EU project Meaning (IST-2001-
34460). Claudio Giuliano was supported by the EU
project Dot.Kom (IST-2001-34038). We would like
to thank Oier Lopez de Lacalle for useful comments.
References
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
B. Decadt, V. Hoste, W. Daelemens, and A. van den
Bosh. 2004. Gambl, genetic algorithm optimiza-
tion of memory-based wsd. In Proc. of Senseval-3,
Barcelona, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18(3):275?299.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413?1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceedings
of SENSEVAL-3, Barcelona, Spain, July.
R. Mihalcea and E. Faruque. 2004. Senselearner: Min-
imally supervised WSD for all words in open text. In
Proceedings of SENSEVAL-3, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
S. Small. 1980. Word Expert Parsing: A Theory of Dis-
tributed Word-based Natural Language Understand-
ing. Ph.D. Thesis, Department of Computer Science,
University of Maryland.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229?234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
D. Yarowsky and R. Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter space. Natural
Language Engineering, 8(4):293?310.
410
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 113?116, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
HAHAcronym: A Computational Humor System
Oliviero Stock and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica
I-38050 Trento, ITALY
{stock, strappa}@itc.it
Abstract
Computational humor will be needed in
interfaces, no less than other cognitive
capabilities. There are many practi-
cal settings where computational humor
will add value. Among them there are:
business world applications (such as ad-
vertisement, e-commerce, etc.), general
computer-mediated communication and
human-computer interaction, increase in
the friendliness of natural language inter-
faces, educational and edutainment sys-
tems. In particular in the educational
field it is an important resource for get-
ting selective attention, help in memoriz-
ing names and situations etc. And we all
know how well it works with children.
Automated humor production in general
is a very difficult task but we wanted to
prove that some results can be achieved
even in short time. We have worked at
a concrete limited problem, as the core
of the European Project HAHAcronym.
The main goal of HAHAcronym has been
the realization of an acronym ironic re-
analyzer and generator as a proof of con-
cept in a focalized but non restricted con-
text. To implement this system some gen-
eral tools have been adapted, or developed
for the humorous context. Systems output
has been submitted to evaluation by hu-
man subjects, with a very positive result.
1 Introduction
Society needs humor, not just for entertainment. In
the current business world, humor is considered to
be so important that companies may hire humor con-
sultants. Humor can be used ?to criticize without
alienating, to defuse tension or anxiety, to introduce
new ideas, to bond teams, ease relationships and
elicit cooperation?.
As far as human-computer interfaces are con-
cerned, in the future we will demand naturalness and
effectiveness that require the incorporation of mod-
els of possibly all human cognitive capabilities, in-
cluding the handling of humor (Stock, 1996). There
are many practical settings where computational hu-
mor will add value. Among them there are: busi-
ness world applications (such as advertisement, e-
commerce, etc.), general computer-mediated com-
munication and human-computer interaction, in-
crease in the friendliness of natural language inter-
faces, educational and edutainment systems.
Not necessarily applications need to emphasize
interactivity. For instance there are important
prospects for humor in automatic information pre-
sentation. In the Web age presentations will be-
come more and more flexible and personalized and
will require humor contributions for electronic com-
merce developments (e.g. product promotion, get-
ting selective attention, help in memorizing names
etc) more or less as it happened in the world of
advertisement within the old broadcast communica-
tion.
Little published research exists on whether humor
is valuable in task-oriented human-computer inter-
113
action (HCI). However (Morkes et al, 1999) did
some experiments concerning the effects of humor
in HCI and computer-mediated communication sit-
uations. Especially in computer-mediated commu-
nication tasks, participants who received jokes rated
the ?person? or computer they worked with as more
likable and competent, reported greater cooperation,
joked back more often etc. The experiments show
that, humor enhances the likeability of an interface
?without distracting users?.
There has been a considerable amount of research
on linguistics of humor and on theories of semantics
or pragmatics of humor (Attardo, 1994). Within the
artificial intelligence community, most writing on
humor has been speculative (Hofstadter et al, 1989).
Minsky (Minsky, 1980) made some preliminary re-
marks about formalizing some kind of humor within
an artificial intelligence/cognitive science perspec-
tive. He refined Freud?s notion that humor is a way
of bypassing our mental ?censors? which control
inappropriate thoughts and feelings (Freud, 1905).
So far, very limited effort has been put on building
computational humor prototypes. The few existing
ones are concerned with rather simple tasks, nor-
mally in limited domains. Probably the most impor-
tant attempt to create a computational humor proto-
type is the work of Binsted and Ritchie (Binsted and
Ritchie, 1994). They have devised a model of the
semantic and syntactic regularities underlying some
of the simplest types of punning riddles. A punning
riddle is a question-answer riddle that uses phono-
logical ambiguity. The three main strategies used to
create phonological ambiguity are syllable substitu-
tion, word substitution and metathesis. In general,
the constructive approaches are mostly inspired by
the incongruity theory (Raskin, 1985), interpreted at
various level of refinement. The incongruity theory
focuses on the element of surprise. It states that hu-
mor is created out of a conflict between what is ex-
pected and what actually occurs when the humorous
utterance or story is completed. In verbal humor this
means that at some level, different interpretations of
material must be possible (and some not detected
before the culmination of the humorous process) or
various pieces of material must cause perception of
specific forms of opposition. Natural language pro-
cessing research has often dealt with ambiguity in
language. A common view is that ambiguity is an
obstacle for deep comprehension. Exactly the oppo-
site is true here.
The work presented here refers to HAHAcronym,
the first European project devoted to computational
humor (EU project IST-2000-30039), part of the Fu-
ture Emerging Technologies section of the Fifth Eu-
ropean Framework Program. The main goal of HA-
HAcronym was the realization of an acronym ironic
re-analyzer and generator as a proof of concept in a
focalized but non restricted context. In the first case
the system makes fun of existing acronyms, in the
second case, starting from concepts provided by the
user, it produces new acronyms, constrained to be
words of the given language. And, of course, they
have to be funny.
HAHAcronym, fully described in (Stock and
Strapparava, 2003) (Stock and Strapparava, 2005),
is based on various resources for natural language
processing, adapted for humor. Many components
are present but simplified with respect to more com-
plex scenarios and some general tools have been de-
veloped for the humorous context. A fundamental
tool is an incongruity detector/generator: in prac-
tice there is a need to detect semantic mismatches
between expected sentence meaning and other read-
ings, along some specific dimension (i.e. in our case
the acronym and its context).
2 The HAHAcronym project
The realization of an acronym re-analyzer and gen-
erator was proposed to the European Commission
as a project that we would be able to develop in a
short period of time (less than a year), that would be
meaningful, well demonstrable, that could be eval-
uated along some pre-decided criteria, and that was
conducive to a subsequent development in a direc-
tion of potential applicative interest. So for us it was
essential that:
1. the work could have many components of a
larger system, simplified for the current setting;
2. we could reuse and adapt existing relevant lin-
guistic resources;
3. some simple strategies for humor effects could
be experimented.
114
One of the purposes of the project was to show
that using ?standard? resources (with some exten-
sions and modifications) and suitable linguistic the-
ories of humor (i.e. developing specific algorithms
that implement or elaborate theories), it is possi-
ble to implement a working prototype. For that,
we have taken advantage of specialized thesauri and
repositories and in particular of WORDNET DO-
MAINS, an extension developed at ITC-irst of the
well-known English WORDNET. In WORDNET
DOMAINS, synsets are annotated with subject field
codes (or domain labels), e.g. MEDICINE, ARCHI-
TECTURE, LITERATURE,. . . In particular for HA-
HAcronym, we have modelled an independent struc-
ture of domain opposition, such as RELIGION vs.
TECHNOLOGY, SEX vs. RELIGION, etc. . . , as a ba-
sic resource for the incongruity generator.
Other important computational tools we have used
are: a parser for analyzing input syntactically
and a syntactic generator of acronyms; general
lexical resources, e.g. acronym grammars, mor-
phological analyzers, rhyming dictionaries, proper
nouns databases, a dictionary of hyperbolic adjec-
tives/adverbs.
2.1 Implementation
To get an ironic or profaning re-analysis of a given
acronym, the system follows various steps and relies
on a number of strategies. The main elements of the
algorithm can be schematized as follows:
? acronym parsing and construction of a logical
form
? choice of what to keep unchanged (for example
the head of the highest ranking NP) and what
to modify (for example the adjectives)
? look for possible, initial letter preserving, sub-
stitutions
? using semantic field oppositions;
? reproducing rhyme and rhythm (the mod-
ified acronym should sound as similar as
possible to the original one);
? for adjectives, reasoning based mainly on
antonym clustering and other semantic re-
lations in WORDNET.
Making fun of existing acronyms amounts to ba-
sically using irony on them, desecrating them with
some unexpectedly contrasting but otherwise con-
sistently sounding expansion.
As far as acronym generation is concerned, the
problem is more complex. We constrain resulting
acronyms to be words of the dictionary. The system
takes in input some concepts (actually synsets, so
that input to this system can result from some other
processing, for instance sentence interpretation) and
some minimal structural indication, such as the se-
mantic head. The primary strategy of the system
is to consider as potential acronyms words that are
in ironic relation with input concepts. Structures
for the acronym expansion result from the specified
head indication and the grammar. Semantic reason-
ing and navigation over WORDNET, choice of spe-
cific word realizations, including morphosyntactic
variations, constrain the result. In this specific strat-
egy, ironic reasoning is developed mainly at the level
of acronym choice and in the incongruity resulting
in relation to the coherently combined words of the
acronym expansion.
3 Examples and Evaluation
Here below some examples of acronym re-analysis
are reported. As far as semantic field opposition is
concerned, we have slightly biased the system to-
wards the domains FOOD, RELIGION, and SEX. For
each example we report the original acronym and the
re-analysis.
ACM - Association for Computing Machinery
? Association for Confusing
Machinery
FBI - Federal Bureau of Investigation
? Fantastic Bureau of
Intimidation
PDA - Personal Digital Assistant
? Penitential Demoniacal
Assistant
IJCAI - International Joint Conference on Artifi-
cial Intelligence
? Irrational Joint Conference on
Antenuptial Intemperance
? Irrational Judgment Conference
on Artificial Indolence
115
ITS - Intelligent Tutoring Systems
? Impertinent Tutoring Systems
? Indecent Toying Systems
As far as generation from scratch is concerned,
a main concept and some attributes (in terms of
synsets) are given as input to the system. Here below
we report some examples of acronym generation.
Main concept: tutoring; Attribute: intelligent
FAINT - Folksy Acritical Instruction for Nescience
Teaching
NAIVE - Negligent At-large Instruction for Vulner-
able Extracurricular-activity
Main concept: writing; Attribute: creative
CAUSTIC - Creative Activity for Unconvincingly
Sporadically Talkative Individualistic Com-
mercials
We note that the system tries to keep all the ex-
pansions of the acronym coherent in the same se-
mantic field of the main concepts. At the same time,
whenever possible, it exploits some incongruity in
the lexical choices.
Testing the humorous quality of texts or other ver-
bal expressions is not an easy task. There are some
relevant studies though, such as (Ruch, 1996). For
HAHAcronym an evaluation was set with a group
of 30 American university students. They had to
evaluate the system production (80 reanalyzed and
80 generated acronyms), along a scale of five levels
of amusement (from very-funny to not-funny). The
results were very encouraging. The system perfor-
mance with humorous strategies and the one without
such strategies (i.e. random lexical choices, main-
taining only syntactic correctness) were totally dif-
ferent. None of the humorous re-analyses proposed
to the students were rejected as completely non-
humorous. Almost 70% were rated funny enough
(without humorous strategies the figure was less
than 8%). In the case of generation of new acronyms
results were positive in 53% of the cases.
A curiosity that may be worth mentioning: HA-
HAcronym participated to a contest about (human)
production of best acronyms, organized by RAI, the
Italian National Broadcasting Service. The system
won a jury?s special prize.
4 Conclusion
The results of the HAHAcronym project have been
positive and a neat prototype resulted, aimed at a
very specific task, but operating without restrictions
of domain. It turns out that it can be even useful per
se, but we think that the project opens the way to
developments for creative language. We believe that
an environment for proposing solutions to advertis-
ing professionals can be a realistic practical develop-
ment of computational humor. In the log run, elec-
tronic commerce, for instance, could include flexible
and individual-oriented humorous promotion.
References
S. Attardo. 1994. Linguistic Theory of Humor. Mouton
de Gruyter, Berlin.
K. Binsted and G. Ritchie. 1994. An implemented model
of punning riddles. In Proc. of the 12th National Con-
ference on Artificial Intelligence (AAAI-94), Seattle.
S. Freud. 1905. Der Witz und Seine Beziehung zum Un-
bewussten. Deutike, Leipzig and Vienna.
D. Hofstadter, L. Gabora, V. Raskin, and S. Attardo.
1989. Synopsis of the workshop on humor and cog-
nition. Humor, 2(4):293?347.
M. Minsky. 1980. Jokes and the logic of the cognitive
unconscious. Technical report, MIT Artificial Intelli-
gence Laboratory. AI memo 603.
J. Morkes, H. Kernal, and C. Nass. 1999. Effects
of humor in task-oriented human-computer interac-
tion and computer-mediated communication. Human-
Computer Interaction, 14:395?435.
V. Raskin. 1985. Semantic Mechanisms of Humor. Dor-
drecht/Boston/Lancaster.
W. Ruch. 1996. Special issue: Measurement approaches
to the sense of humor. Humor, 9(3/4).
O. Stock and C. Strapparava. 2003. Getting serious
about the development of computational humor. In
Proceedings of International Joint Conference on Ar-
tificial Intelligence (IJCAI03), Acapulco, Mexico.
O. Stock and C. Strapparava. 2005. The act of creating
humorous acronyms. Applied Artificial Intelligence,
19(2):137?151, February.
O. Stock. 1996. Password Swordfish: Verbal humor
in the interface. In J. Hulstijn and A. Nijholt, ed-
itors, Proc. of International Workshop on Computa-
tional Humour (TWLT 12), University of Twente, En-
schede, Netherlands.
116
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 449?456,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Direct Word Sense Matching for Lexical Substitution
Ido Dagan1, Oren Glickman1, Alfio Gliozzo2, Efrat Marmorshtein1, Carlo Strapparava2
1Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel
2ITC-Irst, via Sommarive, I-38050, Trento, Italy
Abstract
This paper investigates conceptually and
empirically the novel sense matching task,
which requires to recognize whether the
senses of two synonymous words match in
context. We suggest direct approaches to
the problem, which avoid the intermediate
step of explicit word sense disambigua-
tion, and demonstrate their appealing ad-
vantages and stimulating potential for fu-
ture research.
1 Introduction
In many language processing settings it is needed
to recognize that a given word or term may be sub-
stituted by a synonymous one. In a typical in-
formation seeking scenario, an information need
is specified by some given source words. When
looking for texts that match the specified need the
source words might be substituted with synony-
mous target words. For example, given the source
word ?weapon? a system may substitute it with the
target synonym ?arm?.
This scenario, which is generally referred here
as lexical substitution, is a common technique
for increasing recall in Natural Language Process-
ing (NLP) applications. In Information Retrieval
(IR) and Question Answering (QA) it is typically
termed query/question expansion (Moldovan and
Mihalcea, 2000; Negri, 2004). Lexical Substi-
tution is also commonly applied to identify syn-
onyms in text summarization, for paraphrasing in
text generation, or is integrated into the features of
supervised tasks such as Text Categorization and
Information Extraction. Naturally, lexical substi-
tution is a very common first step in textual en-
tailment recognition, which models semantic in-
ference between a pair of texts in a generalized ap-
plication independent setting (Dagan et al, 2005).
To perform lexical substitution NLP applica-
tions typically utilize a knowledge source of syn-
onymous word pairs. The most commonly used
resource for lexical substitution is the manually
constructed WordNet (Fellbaum, 1998). Another
option is to use statistical word similarities, such
as in the database constructed by Dekang Lin (Lin,
1998). We generically refer to such resources as
substitution lexicons.
When using a substitution lexicon it is assumed
that there are some contexts in which the given
synonymous words share the same meaning. Yet,
due to polysemy, it is needed to verify that the
senses of the two words do indeed match in a given
context. For example, there are contexts in which
the source word ?weapon? may be substituted by
the target word ?arm?; however one should recog-
nize that ?arm? has a different sense than ?weapon?
in sentences such as ?repetitive movements could
cause injuries to hands, wrists and arms.?
A commonly proposed approach to address
sense matching in lexical substitution is applying
Word Sense Disambiguation (WSD) to identify
the senses of the source and target words. Then,
substitution is applied only if the words have the
same sense (or synset, in WordNet terminology).
In settings in which the source is given as a sin-
gle term without context, sense disambiguation
is performed only for the target word; substitu-
tion is then applied only if the target word?s sense
matches at least one of the possible senses of the
source word.
One might observe that such application of WSD
addresses the task at hand in a somewhat indi-
rect manner. In fact, lexical substitution only re-
quires knowing that the source and target senses
449
do match, but it does not require that the match-
ing senses will be explicitly identified. Selecting
explicitly the right sense in context, which is then
followed by verifying the desired matching, might
be solving a harder intermediate problem than re-
quired. Instead, we can define the sense match-
ing problem directly as a binary classification task
for a pair of synonymous source and target words.
This task requires to decide whether the senses of
the two words do or do not match in a given con-
text (but it does not require to identify explicitly
the identity of the matching senses).
A highly related task was proposed in (Mc-
Carthy, 2002). McCarthy?s proposal was to ask
systems to suggest possible ?semantically similar
replacements? of a target word in context, where
alternative replacements should be grouped to-
gether. While this task is somewhat more com-
plicated as an evaluation setting than our binary
recognition task, it was motivated by similar ob-
servations and applied goals. From another per-
spective, sense matching may be viewed as a lex-
ical sub-case of the general textual entailment
recognition setting, where we need to recognize
whether the meaning of the target word ?entails?
the meaning of the source word in a given context.
This paper provides a first investigation of the
sense matching problem. To allow comparison
with the classical WSD setting we derived an
evaluation dataset for the new problem from the
Senseval-3 English lexical sample dataset (Mihal-
cea and Edmonds, 2004). We then evaluated alter-
native supervised and unsupervised methods that
perform sense matching either indirectly or di-
rectly (i.e. with or without the intermediate sense
identification step). Our findings suggest that in
the supervised setting the results of the direct and
indirect approaches are comparable. However, ad-
dressing directly the binary classification task has
practical advantages and can yield high precision
values, as desired in precision-oriented applica-
tions such as IR and QA.
More importantly, direct sense matching sets
the ground for implicit unsupervised approaches
that may utilize practically unlimited volumes
of unlabeled training data. Furthermore, such
approaches circumvent the sisyphean need for
specifying explicitly a set of stipulated senses.
We present an initial implementation of such an
approach using a one-class classifier, which is
trained on unlabeled occurrences of the source
word and applied to occurrences of the target
word. Our current results outperform the unsuper-
vised baseline and put forth a whole new direction
for future research.
2 WSD and Lexical Expansion
Despite certain initial skepticism about the useful-
ness of WSD in practical tasks (Voorhees, 1993;
Sanderson, 1994), there is some evidence that
WSD can improve performance in typical NLP
tasks such as IR and QA. For example, (Shu?tze
and Pederson, 1995) gives clear indication of the
potential for WSD to improve the precision of an IR
system. They tested the use of WSD on a standard
IR test collection (TREC-1B), improving precision
by more than 4%.
The use of WSD has produced successful exper-
iments for query expansion techniques. In partic-
ular, some attempts exploited WordNet to enrich
queries with semantically-related terms. For in-
stance, (Voorhees, 1994) manually expanded 50
queries over the TREC-1 collection using syn-
onymy and other WordNet relations. She found
that the expansion was useful with short and in-
complete queries, leaving the task of proper auto-
matic expansion as an open problem.
(Gonzalo et al, 1998) demonstrates an incre-
ment in performance over an IR test collection us-
ing the sense data contained in SemCor over a
purely term based model. In practice, they ex-
perimented searching SemCor with disambiguated
and expanded queries. Their work shows that
a WSD system, even if not performing perfectly,
combined with synonymy enrichment increases
retrieval performance.
(Moldovan and Mihalcea, 2000) introduces the
idea of using WordNet to extend Web searches
based on semantic similarity. Their results showed
that WSD-based query expansion actually im-
proves retrieval performance in a Web scenario.
Recently (Negri, 2004) proposed a sense-based
relevance feedback scheme for query enrichment
in a QA scenario (TREC-2003 and ACQUAINT),
demonstrating improvement in retrieval perfor-
mance.
While all these works clearly show the potential
usefulness of WSD in practical tasks, nonetheless
they do not necessarily justify the efforts for refin-
ing fine-grained sense repositories and for build-
ing large sense-tagged corpora. We suggest that
the sense matching task, as presented in the intro-
450
duction, may relieve major drawbacks of applying
WSD in practical scenarios.
3 Problem Setting and Dataset
To investigate the direct sense matching problem
it is necessary to obtain an appropriate dataset of
examples for this binary classification task, along
with gold standard annotation. While there is
no such standard (application independent) dataset
available it is possible to derive it automatically
from existing WSD evaluation datasets, as de-
scribed below. This methodology also allows
comparing direct approaches for sense matching
with classical indirect approaches, which apply an
intermediate step of identifying the most likely
WordNet sense.
We derived our dataset from the Senseval-3 En-
glish lexical sample dataset (Mihalcea and Ed-
monds, 2004), taking all 25 nouns, adjectives and
adverbs in this sample. Verbs were excluded since
their sense annotation in Senseval-3 is not based
on WordNet senses. The Senseval dataset includes
a set of example occurrences in context for each
word, split to training and test sets, where each ex-
ample is manually annotated with the correspond-
ing WordNet synset.
For the sense matching setting we need exam-
ples of pairs of source-target synonymous words,
where at least one of these words should occur in
a given context. Following an applicative moti-
vation, we mimic an IR setting in which a sin-
gle source word query is expanded (substituted)
by a synonymous target word. Then, it is needed
to identify contexts in which the target word ap-
pears in a sense that matches the source word. Ac-
cordingly, we considered each of the 25 words in
the Senseval sample as a target word for the sense
matching task. Next, we had to pick for each target
word a corresponding synonym to play the role of
the source word. This was done by creating a list
of all WordNet synonyms of the target word, under
all its possible senses, and picking randomly one
of the synonyms as the source word. For example,
the word ?disc? is one of the words in the Sense-
val lexical sample. For this target word the syn-
onym ?record? was picked, which matches ?disc?
in its musical sense. Overall, 59% of all possible
synsets of our target words included an additional
synonym, which could play the role of the source
word (that is, 41% of the synsets consisted of the
target word only). Similarly, 62% of the test exam-
ples of the target words were annotated by a synset
that included an additional synonym.
While creating source-target synonym pairs it
was evident that many WordNet synonyms corre-
spond to very infrequent senses or word usages,
such as the WordNet synonyms germ and source.
Such source synonyms are useless for evaluat-
ing sense matching with the target word since the
senses of the two words would rarely match in per-
ceivable contexts. In fact, considering our motiva-
tion for lexical substitution, it is usually desired to
exclude such obscure synonym pairs from substi-
tution lexicons in practical applications, since they
would mostly introduce noise to the system. To
avoid this problem the list of WordNet synonyms
for each target word was filtered by a lexicogra-
pher, who excluded manually obscure synonyms
that seemed worthless in practice. The source syn-
onym for each target word was then picked ran-
domly from the filtered list. Table 1 shows the 25
source-target pairs created for our experiments. In
future work it may be possible to apply automatic
methods for filtering infrequent sense correspon-
dences in the dataset, by adopting algorithms such
as in (McCarthy et al, 2004).
Having source-target synonym pairs, a classifi-
cation instance for the sense matching task is cre-
ated from each example occurrence of the target
word in the Senseval dataset. A classification in-
stance is thus defined by a pair of source and target
words and a given occurrence of the target word in
context. The instance should be classified as pos-
itive if the sense of the target word in the given
context matches one of the possible senses of the
source word, and as negative otherwise. Table 2
illustrates positive and negative example instances
for the source-target synonym pair ?record-disc?,
where only occurrences of ?disc? in the musical
sense are considered positive.
The gold standard annotation for the binary
sense matching task can be derived automatically
from the Senseval annotations and the correspond-
ing WordNet synsets. An example occurrence of
the target word is considered positive if the an-
notated synset for that example includes also the
source word, and Negative otherwise. Notice that
different positive examples might correspond to
different senses of the target word. This happens
when the source and target share several senses,
and hence they appear together in several synsets.
Finally, since in Senseval an example may be an-
451
source-target source-target source-target source-target source-target
statement-argument subdivision-arm atm-atmosphere hearing-audience camber-bank
level-degree deviation-difference dissimilar-different trouble-difficulty record-disc
raging-hot ikon-image crucial-important sake-interest bare-simple
opinion-judgment arrangement-organization newspaper-paper company-party substantial-solid
execution-performance design-plan protection-shelter variety-sort root-source
Table 1: Source and target pairs
sentence annotation
This is anyway a stunning disc, thanks to the playing of the Moscow Virtuosi with Spivakov. positive
He said computer networks would not be affected and copies of information should be made on
floppy discs.
negative
Before the dead soldier was placed in the ditch his personal possessions were removed, leaving
one disc on the body for identification purposes
negative
Table 2: positive and negative examples for the source-target synonym pair ?record-disc?
notated with more than one sense, it was consid-
ered positive if any of the annotated synsets for the
target word includes the source word.
Using this procedure we derived gold standard
annotations for all the examples in the Senseval-
3 training section for our 25 target words. For the
test set we took up to 40 test examples for each tar-
get word (some words had fewer test examples),
yielding 913 test examples in total, out of which
239 were positive. This test set was used to eval-
uate the sense matching methods described in the
next section.
4 Investigated Methods
As explained in the introduction, the sense match-
ing task may be addressed by two general ap-
proaches. The traditional indirect approach would
first disambiguate the target word relative to a pre-
defined set of senses, using standard WSD meth-
ods, and would then verify that the selected sense
matches the source word. On the other hand, a
direct approach would address the binary sense
matching task directly, without selecting explicitly
a concrete sense for the target word. This section
describes the alternative methods we investigated
under supervised and unsupervised settings. The
supervised methods utilize manual sense annota-
tions for the given source and target words while
unsupervised methods do not require any anno-
tated sense examples. For the indirect approach
we assume the standard WordNet sense repository
and corresponding annotations of the target words
with WordNet synsets.
4.1 Feature set and classifier
As a vehicle for investigating different classifica-
tion approaches we implemented a ?vanilla? state
of the art architecture for WSD. Following com-
mon practice in feature extraction (e.g. (Yarowsky,
1994)), and using the mxpost1 part of speech tag-
ger and WordNet?s lemmatization, the following
feature set was used: bag of word lemmas for the
context words in the preceding, current and fol-
lowing sentence; unigrams of lemmas and parts
of speech in a window of +/- three words, where
each position provides a distinct feature; and bi-
grams of lemmas in the same window. The SVM-
Light (Joachims, 1999) classifier was used in the
supervised settings with its default parameters. To
obtain a multi-class classifier we used a standard
one-vs-all approach of training a binary SVM for
each possible sense and then selecting the highest
scoring sense for a test example.
To verify that our implementation provides a
reasonable replication of state of the art WSD we
applied it to the standard Senseval-3 Lexical Sam-
ple WSD task. The obtained accuracy2 was 66.7%,
which compares reasonably with the mid-range of
systems in the Senseval-3 benchmark (Mihalcea
and Edmonds, 2004). This figure is just a few
percent lower than the (quite complicated) best
Senseval-3 system, which achieved about 73% ac-
curacy, and it is much higher than the standard
Senseval baselines. We thus regard our classifier
as a fair vehicle for comparing the alternative ap-
proaches for sense matching on equal grounds.
1ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
2The standard classification accuracy measure equals pre-
cision and recall as defined in the Senseval terminology when
the system classifies all examples, with no abstentions.
452
4.2 Supervised Methods
4.2.1 Indirect approach
The indirect approach for sense matching fol-
lows the traditional scheme of performing WSD
for lexical substitution. First, the WSD classifier
described above was trained for the target words
of our dataset, using the Senseval-3 sense anno-
tated training data for these words. Then, the clas-
sifier was applied to the test examples of the target
words, selecting the most likely sense for each ex-
ample. Finally, an example was classified as pos-
itive if the selected synset for the target word in-
cludes the source word, and as negative otherwise.
4.2.2 Direct approach
As explained above, the direct approach ad-
dresses the binary sense matching task directly,
without selecting explicitly a sense for the target
word. In the supervised setting it is easy to ob-
tain such a binary classifier using the annotation
scheme described in Section 3. Under this scheme
an example was annotated as positive (for the bi-
nary sense matching task) if the source word is
included in the Senseval gold standard synset of
the target word. We trained the classifier using the
set of Senseval-3 training examples for each tar-
get word, considering their derived binary anno-
tations. Finally, the trained classifier was applied
to the test examples of the target words, yielding
directly a binary positive-negative classification.
4.3 Unsupervised Methods
It is well known that obtaining annotated training
examples for WSD tasks is very expensive, and
is often considered infeasible in unrestricted do-
mains. Therefore, many researchers investigated
unsupervised methods, which do not require an-
notated examples. Unsupervised approaches have
usually been investigated within Senseval using
the ?All Words? dataset, which does not include
training examples. In this paper we preferred us-
ing the same test set which was used for the super-
vised setting (created from the Senseval-3 ?Lexi-
cal Sample? dataset, as described above), in order
to enable comparison between the two settings.
Naturally, in the unsupervised setting the sense la-
bels in the training set were not utilized.
4.3.1 Indirect approach
State-of-the-art unsupervised WSD systems are
quite complex and they are not easy to be repli-
cated. Thus, we implemented the unsupervised
version of the Lesk algorithm (Lesk, 1986) as a
reference system, since it is considered a standard
simple baseline for unsupervised approaches. The
Lesk algorithm is one of the first algorithms de-
veloped for semantic disambiguation of all-words
in unrestricted text. In its original unsupervised
version, the only resource required by the algo-
rithm is a machine readable dictionary with one
definition for each possible word sense. The algo-
rithm looks for words in the sense definitions that
overlap with context words in the given sentence,
and chooses the sense that yields maximal word
overlap. We implemented a version of this algo-
rithm using WordNet sense-definitions with con-
text length of ?10 words before and after the tar-
get word.
4.3.2 The direct approach: one-class learning
The unsupervised settings for the direct method
are more problematic because most of unsuper-
vised WSD algorithms (such as the Lesk algo-
rithm) rely on dictionary definitions. For this rea-
son, standard unsupervised techniques cannot be
applied in a direct approach for sense matching, in
which the only external information is a substitu-
tion lexicon.
In this subsection we present a direct unsuper-
vised method for sense matching. It is based on
the assumption that typical contexts in which both
the source and target words appear correspond to
their matching senses. Unlabeled occurrences of
the source word can then be used to provide evi-
dence for lexical substitution because they allow
us to recognize whether the sense of the target
word matches that of the source. Our strategy is
to represent in a learning model the typical con-
texts of the source word in unlabeled training data.
Then, we exploit such model to match the contexts
of the target word, providing a decision criterion
for sense matching. In other words, we expect that
under a matching sense the target word would oc-
cur in prototypical contexts of the source word.
To implement such approach we need a learning
technique that does not rely on the availability of
negative evidence, that is, a one-class learning al-
gorithm. In general, the classification performance
of one-class approaches is usually quite poor, if
compared to supervised approaches for the same
tasks. However, in many practical settings one-
class learning is the only available solution.
For our experiments we adopted the one-class
SVM learning algorithm (Scho?lkopf et al, 2001)
453
implemented in the LIBSVM package,3 and repre-
sented the unlabeled training examples by adopt-
ing the feature set described in Subsection 4.1.
Roughly speaking, a one-class SVM estimates the
smallest hypersphere enclosing most of the train-
ing data. New test instances are then classified
positively if they lie inside the sphere, while out-
liers are regarded as negatives. The ratio between
the width of the enclosed region and the number
of misclassified training examples can be varied
by setting the parameter ? ? (0, 1). Smaller val-
ues of ? will produce larger positive regions, with
the effect of increasing recall.
The appealing advantage of adopting one-class
learning for sense matching is that it allows us to
define a very elegant learning scenario, in which it
is possible to train ?off-line? a different classifier
for each (source) word in the lexicon. Such a clas-
sifier can then be used to match the sense of any
possible target word for the source which is given
in the substitution lexicon. This is in contrast to
the direct supervised method proposed in Subsec-
tion 4.2, where a different classifier for each pair
of source - target words has to be defined.
5 Evaluation
5.1 Evaluation measures and baselines
In the lexical substitution (and expansion) set-
ting, the standard WSD metrics (Mihalcea and Ed-
monds, 2004) are not suitable, because we are in-
terested in the binary decision of whether the tar-
get word matches the sense of a given source word.
In analogy to IR, we are more interested in positive
assignments, while the opposite case (i.e. when the
two words cannot be substituted) is less interest-
ing. Accordingly, we utilize the standard defini-
tions of precision, recall and F1 typically used in
IR benchmarks. In the rest of this section we will
report micro averages for these measures on the
test set described in Section 3.
Following the Senseval methodology, we evalu-
ated two different baselines for unsupervised and
supervised methods. The random baseline, used
for the unsupervised algorithms, was obtained by
choosing either the positive or the negative class
at random resulting in P = 0.262, R = 0.5,
F1 = 0.344. The Most Frequent baseline has
been used for the supervised algorithms and is ob-
tained by assigning the positive class when the
3Freely available from www.csie.ntu.edu.tw/
/?cjlin/libsvm.
percentage of positive examples in the training set
is above 50%, resulting in P = 0.65, R = 0.41,
F1 = 0.51.
5.2 Supervised Methods
Both the indirect and the direct supervised meth-
ods presented in Subsection 4.2 have been tested
and compared to the most frequent baseline.
Indirect. For the indirect methodology we
trained the supervised WSD system for each tar-
get word on the sense-tagged training sample. As
described in Subsection 4.2, we implemented a
simple SVM-based WSD system (see Section 4.2)
and applied it to the sense-matching task. Results
are reported in Table 3. The direct strategy sur-
passes the most frequent baseline F1 score, but the
achieved precision is still below it. We note that in
this multi-class setting it is less straightforward to
tradeoff recall for precision, as all senses compete
with each other.
Direct. In the direct supervised setting, sense
matching is performed by training a binary clas-
sifier, as described in Subsection 4.2.
The advantage of adopting a binary classifica-
tion strategy is that the precision/recall tradeoff
can be tuned in a meaningful way. In SVM learn-
ing, such tuning is achieved by varying the param-
eter J , that allows us to modify the cost function
of the SVM learning algorithm. If J = 1 (default),
the weight for the positive examples is equal to the
weight for the negatives. When J > 1, negative
examples are penalized (increasing recall), while,
whenever 0 < J < 1, positive examples are penal-
ized (increasing precision). Results obtained by
varying this parameter are reported in Figure 1.
Figure 1: Direct supervised results varying J
454
Supervised P R F1 Unsupervised P R F1
Most Frequent Baseline 0.65 0.41 0.51 Random Baseline 0.26 0.50 0.34
Multiclass SVM Indirect 0.59 0.63 0.61 Lesk Indirect 0.24 0.19 0.21
Binary SVM (J = 0.5) Direct 0.80 0.26 0.39 One-Class ? = 0.3 Direct 0.26 0.72 0.39
Binary SVM (J = 1) Direct 0.76 0.46 0.57 One-Class ? = 0.5 Direct 0.29 0.56 0.38
Binary SVM (J = 2) Direct 0.68 0.53 0.60 One-Class ? = 0.7 Direct 0.28 0.36 0.32
Binary SVM (J = 3) Direct 0.69 0.55 0.61 One-Class ? = 0.9 Direct 0.23 0.10 0.14
Table 3: Classification results on the sense matching task
Adopting the standard parameter settings (i.e.
J = 1, see Table 3), the F1 of the system
is slightly lower than for the indirect approach,
while it reaches the indirect figures when J in-
creases. More importantly, reducing J allows us
to boost precision towards 100%. This feature is
of great interest for lexical substitution, particu-
larly in precision oriented applications like IR and
QA, for filtering irrelevant candidate answers or
documents.
5.3 Unsupervised methods
Indirect. To evaluate the indirect unsupervised
settings we implemented the Lesk algorithm, de-
scribed in Subsection 4.3.1, and evaluated it on
the sense matching task. The obtained figures,
reported in Table 3, are clearly below the base-
line, suggesting that simple unsupervised indirect
strategies cannot be used for this task. In fact, the
error of the first step, due to low WSD accuracy
of the unsupervised technique, is propagated in
the second step, producing poor sense matching.
Unfortunately, state-of-the-art unsupervised sys-
tems are actually not much better than Lesk on all-
words task (Mihalcea and Edmonds, 2004), dis-
couraging the use of unsupervised indirect meth-
ods for the sense matching task.
Direct. Conceptually, the most appealing solu-
tion for the sense matching task is the one-class
approach proposed for the direct method (Section
4.3.2). To perform our experiments, we trained a
different one-class SVM for each source word, us-
ing a sample of its unlabeled occurrences in the
BNC corpus as training set. To avoid huge train-
ing sets and to speed up the learning process, we
fixed the maximum number of training examples
to 10000 occurrences per word, collecting on av-
erage about 6500 occurrences per word.
For each target word in the test sample, we ap-
plied the classifier of the corresponding source
word. Results for different values of ? are reported
in Figure 2 and summarized in Table 3.
Figure 2: One-class evaluation varying ?
While the results are somewhat above the base-
line, just small improvements in precision are re-
ported, and recall is higher than the baseline for
? < 0.6. Such small improvements may suggest
that we are following a relevant direction, even
though they may not be useful yet for an applied
sense-matching setting.
Further analysis of the classification results for
each word revealed that optimal F1 values are ob-
tained by adopting different values of ? for differ-
ent words. In the optimal (in retrospect) param-
eter settings for each word, performance for the
test set is noticeably boosted, achieving P = 0.40,
R = 0.85 and F1 = 0.54. Finding a principled un-
supervised way to automatically tune the ? param-
eter is thus a promising direction for future work.
Investigating further the results per word, we
found that the correlation coefficient between the
optimal ? values and the degree of polysemy of
the corresponding source words is 0.35. More in-
terestingly, we noticed a negative correlation (r
= -0.30) between the achieved F1 and the degree
of polysemy of the word, suggesting that polyse-
mous source words provide poor training models
for sense matching. This can be explained by ob-
serving that polysemous source words can be sub-
stituted with the target words only for a strict sub-
455
set of their senses. On the other hand, our one-
class algorithm was trained on all the examples
of the source word, which include irrelevant ex-
amples that yield noisy training sets. A possible
solution may be obtained using clustering-based
word sense discrimination methods (Pedersen and
Bruce, 1997; Schu?tze, 1998), in order to train dif-
ferent one-class models from different sense clus-
ters. Overall, the analysis suggests that future re-
search may obtain better binary classifiers based
just on unlabeled examples of the source word.
6 Conclusion
This paper investigated the sense matching task,
which captures directly the polysemy problem in
lexical substitution. We proposed a direct ap-
proach for the task, suggesting the advantages of
natural control of precision/recall tradeoff, avoid-
ing the need in an explicitly defined sense reposi-
tory, and, most appealing, the potential for novel
completely unsupervised learning schemes. We
speculate that there is a great potential for such
approaches, and suggest that sense matching may
become an appealing problem and possible track
in lexical semantic evaluations.
Acknowledgments
This work was partly developed under the collab-
oration ITC-irst/University of Haifa.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with wordnet synsets can improve
text retrieval. In ACL, Montreal, Canada.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
ACM-SIGDOC Conference, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Automatic identification of infre-
quent word senses. In Proceedings of COLING,
pages 1220?1226.
Diana McCarthy. 2002. Lexical substitution as a task
for wsd evaluation. In Proceedings of the ACL-
02 workshop on Word sense disambiguation, pages
109?115, Morristown, NJ, USA. Association for
Computational Linguistics.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Anal-
ysis of Text, Barcelona, Spain, July.
D. Moldovan and R. Mihalcea. 2000. Using wordnet
and lexical operators to improve internet searches.
IEEE Internet Computing, 4(1):34?43, January.
M. Negri. 2004. Sense-based blind relevance feedback
for question answering. In SIGIR-2004 Workshop
on Information Retrieval For Question Answering
(IR4QA), Sheffield, UK, July.
T. Pedersen and R. Bruce. 1997. Distinguishing word
sense in untagged text. In EMNLP, Providence, Au-
gust.
M. Sanderson. 1994. Word sense disambiguation and
information retrieval. In SIGIR, Dublin, Ireland,
June.
B. Scho?lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Compu-
tation, 13:1443?1471.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
H. Shu?tze and J. Pederson. 1995. Information retrieval
based on word senses. In Proceedings of the 4th
Annual Symposium on Document Analysis and In-
formation Retrieval, Las Vegas.
E. Voorhees. 1993. Using WordNet to disambiguate
word sense for text retrieval. In SIGIR, Pittsburgh,
PA.
E. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th ACM
SIGIR Conference, Dublin, Ireland, June.
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration
in spanish and french. In ACL, pages 88?95, Las
Cruces, New Mexico.
456
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553?560,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Comparable Corpora and Bilingual Dictionaries
for Cross-Language Text Categorization
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
{gliozzo,strappa}@itc.it
Abstract
Cross-language Text Categorization is the
task of assigning semantic classes to docu-
ments written in a target language (e.g. En-
glish) while the system is trained using la-
beled documents in a source language (e.g.
Italian).
In this work we present many solutions ac-
cording to the availability of bilingual re-
sources, and we show that it is possible
to deal with the problem even when no
such resources are accessible. The core
technique relies on the automatic acquisi-
tion of Multilingual Domain Models from
comparable corpora.
Experiments show the effectiveness of our
approach, providing a low cost solution for
the Cross Language Text Categorization
task. In particular, when bilingual dictio-
naries are available the performance of the
categorization gets close to that of mono-
lingual text categorization.
1 Introduction
In the worldwide scenario of the Web age, mul-
tilinguality is a crucial issue to deal with and
to investigate, leading us to reformulate most of
the classical Natural Language Processing (NLP)
problems into a multilingual setting. For in-
stance the classical monolingual Text Categoriza-
tion (TC) problem can be reformulated as a Cross
Language Text Categorization (CLTC) task, in
which the system is trained using labeled exam-
ples in a source language (e.g. English), and it
classifies documents in a different target language
(e.g. Italian).
The applicative interest for the CLTC is im-
mediately clear in the globalized Web scenario.
For example, in the community based trade (e.g.
eBay) it is often necessary to archive texts in dif-
ferent languages by adopting common merceolog-
ical categories, very often defined by collections
of documents in a source language (e.g. English).
Another application along this direction is Cross
Lingual Question Answering, in which it would
be very useful to filter out the candidate answers
according to their topics.
In the literature, this task has been proposed
quite recently (Bel et al, 2003; Gliozzo and Strap-
parava, 2005). In those works, authors exploited
comparable corpora showing promising results. A
more recent work (Rigutini et al, 2005) proposed
the use of Machine Translation techniques to ap-
proach the same task.
Classical approaches for multilingual problems
have been conceived by following two main direc-
tions: (i) knowledge based approaches, mostly im-
plemented by rule based systems and (ii) empirical
approaches, in general relying on statistical learn-
ing from parallel corpora. Knowledge based ap-
proaches are often affected by low accuracy. Such
limitation is mainly due to the problem of tun-
ing large scale multilingual lexical resources (e.g.
MultiWordNet, EuroWordNet) for the specific ap-
plication task (e.g. discarding irrelevant senses,
extending the lexicon with domain specific terms
and their translations). On the other hand, em-
pirical approaches are in general more accurate,
because they can be trained from domain specific
collections of parallel text to represent the appli-
cation needs. There exist many interesting works
about using parallel corpora for multilingual appli-
cations (Melamed, 2001), such as Machine Trans-
lation (Callison-Burch et al, 2004), Cross Lingual
553
Information Retrieval (Littman et al, 1998), and
so on.
However it is not always easy to find or build
parallel corpora. This is the main reason why
the ?weaker? notion of comparable corpora is a
matter of recent interest in the field of Computa-
tional Linguistics (Gaussier et al, 2004). In fact,
comparable corpora are easier to collect for most
languages (e.g. collections of international news
agencies), providing a low cost knowledge source
for multilingual applications.
The main problem of adopting comparable cor-
pora for multilingual knowledge acquisition is that
only weaker statistical evidence can be captured.
In fact, while parallel corpora provide stronger
(text-based) statistical evidence to detect transla-
tion pairs by analyzing term co-occurrences in
translated documents, comparable corpora pro-
vides weaker (term-based) evidence, because text
alignments are not available.
In this paper we present some solutions to deal
with CLTC according to the availability of bilin-
gual resources, and we show that it is possible
to deal with the problem even when no such re-
sources are accessible. The core technique relies
on the automatic acquisition of Multilingual Do-
main Models (MDMs) from comparable corpora.
This allows us to define a kernel function (i.e. a
similarity function among documents in different
languages) that is then exploited inside a Support
Vector Machines classification framework. We
also investigate this problem exploiting synset-
aligned multilingual WordNets and standard bilin-
gual dictionaries (e.g. Collins).
Experiments show the effectiveness of our ap-
proach, providing a simple and low cost solu-
tion for the Cross-Language Text Categorization
task. In particular, when bilingual dictionar-
ies/repositories are available, the performance of
the categorization gets close to that of monolin-
gual TC.
The paper is structured as follows. Section 2
briefly discusses the notion of comparable cor-
pora. Section 3 shows how to perform cross-
lingual TC when no bilingual dictionaries are
available and it is possible to rely on a compa-
rability assumption. Section 4 present a more
elaborated technique to acquire MDMs exploiting
bilingual resources, such as MultiWordNet (i.e.
a synset-aligned WordNet) and Collins bilingual
dictionary. Section 5 evaluates our methodolo-
gies and Section 6 concludes the paper suggesting
some future developments.
2 Comparable Corpora
Comparable corpora are collections of texts in dif-
ferent languages regarding similar topics (e.g. a
collection of news published by agencies in the
same period). More restrictive requirements are
expected for parallel corpora (i.e. corpora com-
posed of texts which are mutual translations),
while the class of the multilingual corpora (i.e.
collection of texts expressed in different languages
without any additional requirement) is the more
general. Obviously parallel corpora are also com-
parable, while comparable corpora are also multi-
lingual.
In a more precise way, let L =
{L1, L2, . . . , Ll} be a set of languages, let
T i = {ti1, ti2, . . . , tin} be a collection of texts ex-
pressed in the language Li ? L, and let ?(tjh, tiz)
be a function that returns 1 if tiz is the translation
of tjh and 0 otherwise. A multilingual corpus is
the collection of texts defined by T ? = ?i T i. If
the function ? exists for every text tiz ? T ? and
for every language Lj , and is known, then the
corpus is parallel and aligned at document level.
For the purpose of this paper it is enough to as-
sume that two corpora are comparable, i.e. they
are composed of documents about the same top-
ics and produced in the same period (e.g. possibly
from different news agencies), and it is not known
if a function ? exists, even if in principle it could
exist and return 1 for a strict subset of document
pairs.
The texts inside comparable corpora, being
about the same topics, should refer to the same
concepts by using various expressions in different
languages. On the other hand, most of the proper
nouns, relevant entities and words that are not yet
lexicalized in the language, are expressed by using
their original terms. As a consequence the same
entities will be denoted with the same words in
different languages, allowing us to automatically
detect couples of translation pairs just by look-
ing at the word shape (Koehn and Knight, 2002).
Our hypothesis is that comparable corpora contain
a large amount of such words, just because texts,
referring to the same topics in different languages,
will often adopt the same terms to denote the same
entities1 .
1According to our assumption, a possible additional cri-
554
However, the simple presence of these shared
words is not enough to get significant results in
CLTC tasks. As we will see, we need to exploit
these common words to induce a second-order
similarity for the other words in the lexicons.
2.1 The Multilingual Vector Space Model
Let T = {t1, t2, . . . , tn} be a corpus, and V =
{w1, w2, . . . , wk} be its vocabulary. In the mono-
lingual settings, the Vector Space Model (VSM)
is a k-dimensional space Rk, in which the text
tj ? T is represented by means of the vector ~tj
such that the zth component of ~tj is the frequency
of wz in tj . The similarity among two texts in the
VSM is then estimated by computing the cosine of
their vectors in the VSM.
Unfortunately, such a model cannot be adopted
in the multilingual settings, because the VSMs of
different languages are mainly disjoint, and the
similarity between two texts in different languages
would always turn out to be zero. This situation
is represented in Figure 1, in which both the left-
bottom and the rigth-upper regions of the matrix
are totally filled by zeros.
On the other hand, the assumption of corpora
comparability seen in Section 2, implies the pres-
ence of a number of common words, represented
by the central rows of the matrix in Figure 1.
As we will show in Section 5, this model is
rather poor because of its sparseness. In the next
section, we will show how to use such words as
seeds to induce a Multilingual Domain VSM, in
which second order relations among terms and
documents in different languages are considered
to improve the similarity estimation.
3 Exploiting Comparable Corpora
Looking at the multilingual term-by-document
matrix in Figure 1, a first attempt to merge the
subspaces associated to each language is to exploit
the information provided by external knowledge
sources, such as bilingual dictionaries, e.g. col-
lapsing all the rows representing translation pairs.
In this setting, the similarity among texts in dif-
ferent languages could be estimated by exploit-
ing the classical VSM just described. However,
the main disadvantage of this approach to esti-
mate inter-lingual text similarity is that it strongly
terion to decide whether two corpora are comparable is to
estimate the percentage of terms in the intersection of their
vocabularies.
relies on the availability of a multilingual lexical
resource. For languages with scarce resources a
bilingual dictionary could be not easily available.
Secondly, an important requirement of such a re-
source is its coverage (i.e. the amount of possible
translation pairs that are actually contained in it).
Finally, another problem is that ambiguous terms
could be translated in different ways, leading us to
collapse together rows describing terms with very
different meanings. In Section 4 we will see how
the availability of bilingual dictionaries influences
the techniques and the performance. In the present
Section we want to explore the case in which such
resources are supposed not available.
3.1 Multilingual Domain Model
A MDM is a multilingual extension of the concept
of Domain Model. In the literature, Domain Mod-
els have been introduced to represent ambiguity
and variability (Gliozzo et al, 2004) and success-
fully exploited in many NLP applications, such as
Word Sense Disambiguation (Strapparava et al,
2004), Text Categorization and Term Categoriza-
tion.
A Domain Model is composed of soft clusters
of terms. Each cluster represents a semantic do-
main, i.e. a set of terms that often co-occur in
texts having similar topics. Such clusters iden-
tify groups of words belonging to the same seman-
tic field, and thus highly paradigmatically related.
MDMs are Domain Models containing terms in
more than one language.
A MDM is represented by a matrix D, contain-
ing the degree of association among terms in all
the languages and domains, as illustrated in Table
1. For example the term virus is associated to both
MEDICINE COMPUTER SCIENCE
HIV e/i 1 0
AIDSe/i 1 0
viruse/i 0.5 0.5
hospitale 1 0
laptope 0 1
Microsofte/i 0 1
clinicai 1 0
Table 1: Example of Domain Matrix. we denotes
English terms, wi Italian terms and we/i the com-
mon terms to both languages.
the domain COMPUTER SCIENCE and the domain
MEDICINE while the domain MEDICINE is associ-
ated to both the terms AIDS and HIV. Inter-lingual
555
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
English texts Italian texts
te1 te2 ? ? ? ten?1 ten ti1 ti2 ? ? ? tim?1 tim
we1 0 1 ? ? ? 0 1 0 0 ? ? ?
English
Lexicon
we2 1 1 ? ? ? 1 0 0
. . .
... . . . . . . . . . . . . . . . . . . . . . . . ... 0 ...
wep?1 0 1 ? ? ? 0 0
. . . 0
wep 0 1 ? ? ? 0 0 ? ? ? 0 0
common wi we/i1 0 1 ? ? ? 0 0 0 0 ? ? ? 1 0... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
wi1 0 0 ? ? ? 0 1 ? ? ? 1 1
Italian
Lexicon
wi2 0
. . . 1 1 ? ? ? 0 1
... ... 0 ... . . . . . . . . . . . . . . . . . . . . . . . .
wiq?1
. . . 0 0 1 ? ? ? 0 1
wiq ? ? ? 0 0 0 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Multilingual term-by-document matrix
domain relations are captured by placing differ-
ent terms of different languages in the same se-
mantic field (as for example HIV e/i, AIDSe/i,
hospitale, and clinicai). Most of the named enti-
ties, such as Microsoft and HIV are expressed us-
ing the same string in both languages.
Formally, let V i = {wi1, wi2, . . . , wiki} be the
vocabulary of the corpus T i composed of doc-
ument expressed in the language Li, let V ? =
?
i V i be the set of all the terms in all the lan-
guages, and let k? = |V ?| be the cardinality of
this set. Let D = {D1, D2, ..., Dd} be a set of do-
mains. A DM is fully defined by a k? ? d domain
matrix D representing in each cell di,z the domain
relevance of the ith term of V ? with respect to the
domain Dz . The domain matrix D is used to de-
fine a function D : Rk? ? Rd, that maps the doc-
ument vectors ~tj expressed into the multilingual
classical VSM (see Section 2.1), into the vectors
~t?j in the multilingual domain VSM. The function
D is defined by2
D(~tj) = ~tj(IIDFD) = ~t?j (1)
where IIDF is a diagonal matrix such that iIDFi,l =
IDF (wli), ~tj is represented as a row vector, and
IDF (wli) is the Inverse Document Frequency of
2In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM
is a particular instance.
wli evaluated in the corpus T l.
In this work we exploit Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) to automat-
ically acquire a MDM from comparable corpora.
LSA is an unsupervised technique for estimating
the similarity among texts and terms in a large
corpus. In the monolingual settings LSA is per-
formed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix
T describing the corpus. SVD decomposes the
term-by-document matrix T into three matrixes
T ' V?k?UT where ?k? is the diagonal k ? k
matrix containing the highest k?  k eigenval-
ues of T, and all the remaining elements are set
to 0. The parameter k? is the dimensionality of
the Domain VSM and can be fixed in advance (i.e.
k? = d).
In the literature (Littman et al, 1998) LSA
has been used in multilingual settings to define
a multilingual space in which texts in different
languages can be represented and compared. In
that work LSA strongly relied on the availability
of aligned parallel corpora: documents in all the
languages are represented in a term-by-document
matrix (see Figure 1) and then the columns corre-
sponding to sets of translated documents are col-
lapsed (i.e. they are substituted by their sum) be-
fore starting the LSA process. The effect of this
step is to merge the subspaces (i.e. the right and
the left sectors of the matrix in Figure 1) in which
556
the documents have been originally represented.
In this paper we propose a variation of this strat-
egy, performing a multilingual LSA in the case in
which an aligned parallel corpus is not available.
It exploits the presence of common words among
different languages in the term-by-document ma-
trix. The SVD process has the effect of creating a
LSA space in which documents in both languages
are represented. Of course, the higher the number
of common words, the more information will be
provided to the SVD algorithm to find common
LSA dimension for the two languages. The re-
sulting LSA dimensions can be perceived as mul-
tilingual clusters of terms and document. LSA can
then be used to define a Multilingual Domain Ma-
trix DLSA. For further details see (Gliozzo and
Strapparava, 2005).
As Kernel Methods are the state-of-the-art su-
pervised framework for learning and they have
been successfully adopted to approach the TC task
(Joachims, 2002), we chose this framework to per-
form all our experiments, in particular Support
Vector Machines3 . Taking into account the exter-
nal knowledge provided by a MDM it is possible
estimate the topic similarity among two texts ex-
pressed in different languages, with the following
kernel:
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(2)
where D is defined as in equation 1.
Note that when we want to estimate the similar-
ity in the standard Multilingual VSM, as described
in Section 2.1, we can use a simple bag of words
kernel. The BoW kernel is a particular case of the
Domain Kernel, in which D = I, and I is the iden-
tity matrix. In the evaluation typically we consider
the BoW Kernel as a baseline.
4 Exploiting Bilingual Dictionaries
When bilingual resources are available it is possi-
ble to augment the the ?common? portion of the
matrix in Figure 1. In our experiments we ex-
ploit two alternative multilingual resources: Mul-
tiWordNet and the Collins English-Italian bilin-
gual dictionary.
3We adopted the efficient implementation freely available
at http://svmlight.joachims.org/.
MultiWordNet4. It is a multilingual computa-
tional lexicon, conceived to be strictly aligned
with the Princeton WordNet. The available lan-
guages are Italian, Spanish, Hebrew and Roma-
nian. In our experiment we used the English and
the Italian components. The last version of the
Italian WordNet contains around 58,000 Italian
word senses and 41,500 lemmas organized into
32,700 synsets aligned whenever possible with
WordNet English synsets. The Italian synsets
are created in correspondence with the Princeton
WordNet synsets, whenever possible, and seman-
tic relations are imported from the corresponding
English synsets. This implies that the synset index
structure is the same for the two languages.
Thus for the all the monosemic words, we aug-
ment each text in the dataset with the correspond-
ing synset-id, which act as an expansion of the
?common? terms of the matrix in Figure 1. Adopt-
ing the methodology described in Section 3.1, we
exploit these common sense-indexing to induce
a second-order similarity for the other terms in
the lexicons. We evaluate the performance of the
cross-lingual text categorization, using both the
BoW Kernel and the Multilingual Domain Kernel,
observing that also in this case the leverage of the
external knowledge brought by the MDM is effec-
tive.
It is also possible to augment each text with all
the synset-ids of all the words (i.e. monosemic and
polysemic) present in the dataset, hoping that the
SVM machine learning device cut off the noise
due to the inevitable spurious senses introduced in
the training examples. Obviously in this case, dif-
ferently from the ?monosemic? enrichment seen
above, it does not make sense to apply any dimen-
sionality reduction supplied by the Multilingual
Domain Model (i.e. the resulting second-order re-
lations among terms and documents produced on
a such ?extended? corpus should not be meaning-
ful)5.
Collins. The Collins machine-readable bilingual
dictionary is a medium size dictionary includ-
ing 37,727 headwords in the English Section and
32,602 headwords in the Italian Section.
This is a traditional dictionary, without sense in-
dexing like the WordNet repository. In this case
4Available at http://multiwordnet.itc.it.
5The use of a WSD system would help in this issue. How-
ever the rationale of this paper is to see how far it is possible
to go with very few resources. And we suppose that a multi-
lingual all-words WSD system is not easily available.
557
English Italian
Categories Training Test Total Training Test Total
Quality of Life 5759 1989 7748 5781 1901 7682
Made in Italy 5711 1864 7575 6111 2068 8179
Tourism 5731 1857 7588 6090 2015 8105
Culture and School 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
Table 2: Number of documents in the data set partitions
we follow the way, for each text of one language,
to augment all the present words with the transla-
tion words found in the dictionary. For the same
reason, we chose not to exploit the MDM, while
experimenting along this way.
5 Evaluation
The CLTC task has been rarely attempted in the
literature, and standard evaluation benchmark are
not available. For this reason, we developed
an evaluation task by adopting a news corpus
kindly put at our disposal by AdnKronos, an im-
portant Italian news provider. The corpus con-
sists of 32,354 Italian and 27,821 English news
partitioned by AdnKronos into four fixed cat-
egories: QUALITY OF LIFE, MADE IN ITALY,
TOURISM, CULTURE AND SCHOOL. The En-
glish and the Italian corpora are comparable, in
the sense stated in Section 2, i.e. they cover the
same topics and the same period of time. Some
news stories are translated in the other language
(but no alignment indication is given), some oth-
ers are present only in the English set, and some
others only in the Italian. The average length of
the news stories is about 300 words. We randomly
split both the English and Italian part into 75%
training and 25% test (see Table 2). We processed
the corpus with PoS taggers, keeping only nouns,
verbs, adjectives and adverbs.
Table 3 reports the vocabulary dimensions of
the English and Italian training partitions, the vo-
cabulary of the merged training, and how many
common lemmata are present (about 14% of the
total). Among the common lemmata, 97% are
nouns and most of them are proper nouns. Thus
the initial term-by-document matrix is a 43,384 ?
45,132 matrix, while the DLSA was acquired us-
ing 400 dimensions.
As far as the CLTC task is concerned, we tried
the many possible options. In all the cases we
trained on the English part and we classified the
Italian part, and we trained on the Italian and clas-
# lemmata
English training 22,704
Italian training 26,404
English + Italian 43,384
common lemmata 5,724
Table 3: Number of lemmata in the training parts
of the corpus
sified on the English part. When used, the MDM
was acquired running the SVD only on the joint
(English and Italian) training parts.
Using only comparable corpora. Figure 2 re-
ports the performance without any use of bilingual
dictionaries. Each graph show the learning curves
respectively using a BoW kernel (that is consid-
ered here as a baseline) and the multilingual do-
main kernel. We can observe that the latter largely
outperform a standard BoW approach. Analyzing
the learning curves, it is worth noting that when
the quantity of training increases, the performance
becomes better and better for the Multilingual Do-
main Kernel, suggesting that with more available
training it could be possible to improve the results.
Using bilingual dictionaries. Figure 3 reports
the learning curves exploiting the addition of the
synset-ids of the monosemic words in the corpus.
As expected the use of a multilingual repository
improves the classification results. Note that the
MDM outperforms the BoW kernel.
Figure 4 shows the results adding in the English
and Italian parts of the corpus all the synset-ids
(i.e. monosemic and polisemic) and all the transla-
tions found in the Collins dictionary respectively.
These are the best results we get in our experi-
ments. In these figures we report also the perfor-
mance of the corresponding monolingual TC (we
used the SVM with the BoW kernel), which can
be considered as an upper bound. We can observe
that the CLTC results are quite close to the perfor-
mance obtained in the monolingual classification
tasks.
558
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on English, test on Italian)
Multilingual Domain Kernel
Bow Kernel
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on Italian, test on English)
Multilingual Domain Kernel
Bow Kernel
Figure 2: Cross-language learning curves: no use of bilingual dictionaries
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on English, test on Italian)
Multilingual Domain Kernel
Bow Kernel
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on Italian, test on English)
Multilingual Domain Kernel
Bow Kernel
Figure 3: Cross-language learning curves: monosemic synsets from MultiWordNet
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on English, test on Italian)
Monolingual (Italian) TC
Collins
MultiWordNet
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on Italian, test on English)
Monolingual (English) TC
Collins
MultiWordNet
Figure 4: Cross-language learning curves: all synsets from MultiWordNet // All translations from Collins
559
6 Conclusion and Future Work
In this paper we have shown that the problem of
cross-language text categorization on comparable
corpora is a feasible task. In particular, it is pos-
sible to deal with it even when no bilingual re-
sources are available. On the other hand when it is
possible to exploit bilingual repositories, such as a
synset-aligned WordNet or a bilingual dictionary,
the obtained performance is close to that achieved
for the monolingual task. In any case we think
that our methodology is low-cost and simple, and
it can represent a technologically viable solution
for multilingual problems. For the future we try to
explore also the use of a word sense disambigua-
tion all-words system. We are confident that even
with the actual state-of-the-art WSD performance,
we can improve the actual results.
Acknowledgments
This work has been partially supported by the ON-
TOTEXT (From Text to Knowledge for the Se-
mantic Web) project, funded by the Autonomous
Province of Trento under the FUP-2004 program.
References
N. Bel, C. Koster, and M. Villegas. 2003. Cross-
lingual text categorization. In Proceedings of Eu-
ropean Conference on Digital Libraries (ECDL),
Trondheim, August.
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-and
sentence-aligned parallel corpora. In Proceedings of
ACL-04, Barcelona, Spain, July.
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41(6):391?407.
E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and
H. Dejean. 2004. A geometric view on bilingual
lexicon extraction from comparable corpora. In Pro-
ceedings of ACL-04, Barcelona, Spain, July.
A. Gliozzo and C. Strapparava. 2005. Cross language
text categorization by acquiring multilingual domain
models from comparable corpora. In Proc. of the
ACL Workshop on Building and Using Parallel Texts
(in conjunction of ACL-05), University of Michigan,
Ann Arbor, June.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
T. Joachims. 2002. Learning to Classify Text using
Support Vector Machines. Kluwer Academic Pub-
lishers.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of ACL Workshop on Unsupervised Lexical Acquisi-
tion, Philadelphia, July.
M. Littman, S. Dumais, and T. Landauer. 1998. Auto-
matic cross-language information retrieval using la-
tent semantic indexing. In G. Grefenstette, editor,
Cross Language Information Retrieval, pages 51?
62. Kluwer Academic Publishers.
D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
L. Rigutini, M. Maggini, and B. Liu. 2005. An EM
based training algorithm for cross-language text cat-
egorizaton. In Proceedings of Web Intelligence Con-
ference (WI-2005), Compie`gne, France, September.
C. Strapparava, A. Gliozzo, and C. Giuliano.
2004. Pattern abstraction and term similarity for
word sense disambiguation. In Proceedings of
SENSEVAL-3, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985.
Generalized vector space model in information re-
trieval. In Proceedings of the 8th ACM SIGIR Con-
ference.
560
Experiments in Word Domain Disambiguation for Parallel 
Texts 
Bernardo  Magnin i  and Car lo  S t rapparava  
ITC- i rs t ,  I s t i tuto  per  la R icerca  Scienti f ica e Tecnologica,  1-38050 Trento ,  ITALY  
email :  {magnin i , s t rappa)@irs t . i t  c. it 
Abst rac t  
This paper describes ome preliminary 
results about Word Domain Disambigua- 
tion, a variant of Word Sense Disam- 
bignation where words in a text are 
tagged with a domain label in place of a 
sense label. The English WoRDNET and 
its aligned Italian version, MULTIWORD- 
NET, both augmented with domain la- 
bels, are used as the main information 
repositories. A baseline algorithm for 
Word Domain Disambiguation is pre- 
sented and then compared with a mu- 
tual help disambignation strategy, which 
takes advantages of the shared senses of 
parallel bilingual texts. 
1 In t roduct ion  
This work describes some preliminary results 
about Word Domain Disambignation (WDD), a 
variant of Word Sense Disambiguation (WSD) 
where for each word in a text a domain label 
(among those allowed by the word) has to be cho- 
sen instead of a sense label. Domain labels, such 
as MEDICINE and ARCHITECTURE, provide a nat- 
ural way to establish semantic relations among 
word senses, grouping them into homogeneous 
clusters. A relevant consequence of the appli- 
cation of domain clustering over the WORDNET 
senses is the reduction of the word polysemy (i.e. 
the number of domains for a word is generally 
lower than the number of senses for that word). 
We wanted to investigate the hypothesis that 
the polysemy reduction caused by domain clus- 
tering can profitably help the word domain disam- 
bignation process. A preliminary experiment has 
been set up with two goals: first, providing exper- 
imental evidences that a frequency based WDD 
algorithm can outperform a WSD baseline algo- 
rithm; second, exploring WDD in the context of 
parallel, not aligned, text disambignation. 
The English WOR~DNET and the Italian aligned 
version MULTIWORDNET, both augmented with 
domain labels, are used as the main information 
repositories. A baseline algorithm for Word Do- 
main Disambignation is presented and then com- 
pared with a mutual help disambignation strategy, 
which makes use of the shared senses of parallel 
bilingual texts. 
Several works in the literature have remarked 
that for many practical purposes the fine-grained 
sense distinctions provided by WoRDNET are not 
necessary (see for example \[Wilks and Stevenson, 
98\], \[Gonzalo et al, 1998\], \[Kilgarriff and Yallop, 
2000\] and the SENSEVAL initiative) and make 
it hard word sense disambignation. Two related 
works are also \[Buitelaar, 1998\] and \[Buitelaar, 
2000\], where the reduction of the WORDNET pol- 
? ysemy is obtained on the basis of regnlar polysemy 
relations. Our approach is based on sense clusters 
derived by domain proximity, which in some case 
may overlap with regular polysemy derived clus- 
ters (e.g. both "book" as composition and "book" 
as physical object belong to PUBLISHIN6), but in 
many cases may not (e.g. "lamb" as animal be- 
longs to ZOOLOQY, while "lamb" as meat belongs 
to FooD). Following this line we propose Word 
Domain Disambiguation asa practical alternative 
for applications that do not require fine grained 
sense distinctions. 
The paper is organized as follows. Section 2 
introduces domain labels, their organization and 
the extensions to WORDNET. Section 3 discusses 
Word Domain Disambiguation and presents the 
algorithms used in the experiment. Section 4 gives 
the experimental setting. Results are discussed in 
Section 5. 
2 WordNet  and  Sub ject  F ie ld  
Codes  
In this work we will make use of an augmented 
WoRDNET, whose synsets have been annotated 
with one or more subject field codes. This re- 
source, discussed in \[Magnini and Cavagli~, 2000\], 
27 
currently covers all the noun synsets of WORD- 
NET 1.6 \[Miller, 1990\], and it is under develop- 
ment for the remaining lexical categories. 
Subject Field Codes (SFC) group together 
words relevant for a specific domain. The best 
approximation of SFCs are the field labels used 
in dictionaries (e.g. MEDICINE, A~;CHITECTURE), 
even if their use is restricted to word usages be- 
longing to specific terminological domains. In 
WORDNET, too, SFCs seem to be treed occasion- 
ally and without a consistent desi~;n. 
Information brought by SFCs is complemen- 
tary to what is already in WoRDNET. First of 
all a SFC may include synsets of different syntac- 
tic categories: for instance MEDICINE 1 groups to- 
gether senses from Nouns, such as doctora l  and 
hospital#I, and from Verbs such as operate#7. 
Second, a SFC may also contain .,~nses from dif- 
ferent WORDNET sub-hierarchies (i.e. deriving 
from different "unique beginners, or from dif- 
ferent "lexicographer files"). For example, the 
SPORT SFC contains senses such as athlete#I, 
deriving from li:~e~orm#1, game_equipment#1 
from physical_object#1, sport#1 from act#2, 
and playingJield#1 from location#1. 
We have organized about 250 SFCs in a hier- 
archy, where each level is made up of codes of 
the same degree of specificity: for example, the 
second level includes SFCs such as BOTANY, LIN- 
GUISTICS, HISTORY, SPORT and RELIGION, while 
at the third level we can find specializations such 
as AMERICAN.HISTORY, GRAMMAR,  PHONETICS 
and TENNIS. 
A problem arises for synsets that do not belong 
to a specific SFC, but rather can appear in almost 
all of them. For this reason, a FACTOTUM SFC has 
been created which basically includes two types of 
synsets: 
Gener/c synsets, which are hard to classify in 
a particular SFC, are generally placed high 
in the WoRDNET hierarchy and are related 
senses of highly polysemous words. For ex- 
ample: 
man#1 an adult male person (as opposed to a 
woman) 
man#3 the generic use of the word to refer to any 
human being 
date#1 day of the month 
aThroughout the paper subject field codes are ino 
cUcated with this TYPEFACE while word senses are re- 
ported with this typeface#l, with their corresponding 
numbering in WORDNET 1.6. Moreover, we use sub. 
ject field code, domain label and semantic field with 
the same meaning. 
dal;e#3 appointment, engagement 
? Stop Senses ynsets which appear frequently 
in different contexts, such as numbers, week 
days, colors, etc. These synsets usually be- 
long to non polysemous words and they be- 
have much as stop words, because they do not 
significantly contribute to the overall mean- 
ing of a text. 
A single domain label may group together more 
than one word sense, resulting in a reduction of 
the polysemy. Figure 1 shows an example. The 
word "book" has seven different senses in WORD- 
NET 1.6: three of them are grouped under the 
PUBLISHING domain, causing the reduction of the 
polysemy from 7 to 5 senses. 
3 Word  Domain  D isambiguat ion  
In this section we present wo baseline algorithms 
for word domain disambiguation and we propose 
some variants of them to deal with WDD in the 
context of parallel texts. 
3.1 Basel ine a lgor i thms 
To decide a proper baseline for Word Domain Dis- 
ambiguation we wanted to be sure that it was ap- 
plicable to both the languages (i.e. English and 
Italian) used in the experiment. This caused the 
exclusion of a selection based on the domain fre- 
quency computed as a function of the frequency 
of the WORDNET senses, because we did not 
have a frequency estimation for Italian senses. 
We adopted two alternative frequency measures, 
based respectively on the intra text frequency and 
the intra word frequency of a domain label. Both 
of them are computed with a two-stage disam- 
bignation process, structurally similar to the al- 
gorithm used in \[Voorhees, 1998\]. 
Baseline 1: Intra text domain  frequency. 
The baseline algorithm follows two steps. First, 
all the words in the text are considered and for 
each domain label allowed by the word the label 
score is incremented by one. At the second step 
each word is reconsidered, and the domain label 
(or labels, depending on how many best solutions 
are requested) with the highest score is selected 
as the result of the disambiguation. 
Basel ine 2: In t ra  word  domain  f requency.  
In this version of the baseline algorithm, step 1 is 
modified in that each domain label allowed by the 
word is incremented by the frequency of the la- 
bel among the senses of that word. For instance, 
28 
{book#1 - p~blishtd co, npositio. }
PUBLISHING 
"book" 
{book#2 volume#3 - book a.? a physical objea} 
{daybook#2 book#7 ledger#2 - an accounting book as aphisical object} 
{book#6 - book of the B/b/e} PtrBHSmNG RELIGION 
THEA~ 
{script#1 book#4 playscript#1-mriuenvemionojraplay} 
~ ok#1 
COM~nZCE 
book#5 ledger#l - recorda of commercial acc.ount} 
FACTOTt~ 
record#5 recordbook#1 book#3-compilationoflmowfact~ 
regaMing $ome~ing or someone} 
Figure I: An  example of polysemy reduction 
if "book" is the word (see Figure 1), PUBLISH- 
ING will receive .42 (i.e. three senses out of seven 
belong to PUBLISHING), while the others domain 
labels will receive .14 each. 
3.1.1 The  " fac to tum"  effect 
As we mentioned in Section 2, a FACTOTUM la- 
bel is used to mark WORDNET senses that do not 
belong to a specific domain, but rather are highly 
widespread across texts of different domains. A 
consequence is that very often, at the end of step 1 
of the disambignation algorithm, FACTOTUM out- 
performs the other domains, this way affecting the 
selection carried out at step 2 (i.e. in case of am- 
biguity FACTOTUM is often preferred). 
For the purposes of the experiment described 
in the next sections the FACTOTUM problem has 
been resolved with a slight modification at step 2 
of the baseline algorithm: when FACTOTUM is the 
best selection for a word, also the second available 
choice is considered as a result of the disambigua- 
tion process. 
3.2 Extens ions  for paral le l  texts  
We started with the following working hypothe- 
sis. Using aligned wordnets to disambiguate par- 
allel texts allows us to calculate the intersection 
among the synsets accessible from an English text 
through the English WoRDNET and the synsets 
accessible from the parallel Italian text through 
the Italian WORDNET. It would seem reasonable 
that the synset intersection maximizes the num- 
ber of significant synsets for the two texts, and 
at the same time tends to exclude synsets whose 
meaning is not pertinent to the content of the text. 
Let us try to make the point clearer with an 
example. Suppose we find in an English text the 
word "bank" and in the Italian parallel text the 
word "banca',  which we do not know being the 
translation of "bank", because we do not have 
word alignments. For "bank" we get ten senses 
from WORDNET 1.6 (reported in Figure 2), while 
for "banca" we get two senses from MULTIWORD- 
NET (reported in Figure 2). As the two wordnets 
are aligned (i.e. they share synset offsets), the 
intersection can be straightforwardly determined. 
In this case it includes 06227059, corresponding to
bank#1 and banca#1, and 02247680, correspond- 
ing to bank#4 and banca#2, which both pertain 
to the BANKING domain, and excludes, among the 
others, bank#2, which happens to be an homonym 
sense in English but not in Italian. 
Incidentally, if "istituto di credito" were not in 
the synset 06227059 (e.g. because of the incom- 
pleteness of the Italian WORDNET) and it were 
the only word present in the Italian news to de- 
notate the bank#1 sense, the synset intersection 
would have been empty. 
As far as disambiguation is concerned it seems 
a reasonable hypothesis that the synset intersec- 
tion could bring constraints on the sense selection 
for a word (i.e. it is highly probable that the cor- 
rect choice belongs to the intersection). Following 
this line we have elaborated a mutua l  he lp  d i sam-  
biguation strategy where the synset intersection 
can be accessed to help the disambiguation pro- 
cess of both English and Italian texts. 
In addition to the synset intersection, we 
wanted to consider the intersection of domain la- 
bels, that is domains that are shared among the 
29 
Bank (from WordNet 1.6) 
1. J{06227059}\[ depos i tory  f inanc ia l  ins t i tu t ion ,  bank, banking concern, banking company 
I I 
-- (a financial institution that accepts deposits and channels the money into lending 
act iv i t ies ;  ) 
2. {06800223} bank -- (sloping land (especially the slope beside a body of water)) 
3. {09626760} bank -- (a supply or stock held in reserve especially for future use 
(especially in emergencies)) 
4. {02247680}\[ bank, bank building -- (a building in which commercial banking is 
transacted; ) 
B 
5. {06250735} bank -- (an ~rrangement of similar objects in a row or in tiers; ) 
6. {03277560} savings bank, coin bank, money box, bank -- (a container (usually with a 
slot in the top) for keeping money at home;) 
7. {06739355} bank -- (a long ridge or pile; "a huge bank of earth") 
8. {09616845} bank -- (the :funds held by a gambling house or the dealer in some gambling 
games; )
9. {06800468} bank, cant, camber -- (a slope in the turn of a road or track;) 
I0. {00109955} bank -- (a flight maneuver; aircraft tips laterally about its longitudinal 
axis  (espec ia l l y  in turn ing) )  
Banca (from MultiWordnet) 
1. 1{06227059}1 i s t i tu to .d i _c red i to  cassa banco banca 
q B 
I{0  4,680}\[ ban. 
Figure 2: An example of sysnet intersection i  MULTIWORDNET 
senses of the parallel texts. In the example above 
the domain intersection would include just one la- 
bel (i.e. BANKING), in place of the two synsets 
of the synset intersection. The hypothesis i that 
domain intersection could reduce problems due to 
possible misalignments among the synsets of the 
two wordnets. 
Two mutual help algorithms have been imple- 
mented, weak mutual help and strong mutual help, 
which are described in the following. 
Weak Mutua l  help. In this version of the mu- 
tual help algorithm, step 1 of the baseline is mod- 
itied in that, if the domain label is found in the 
synset or domain intersection, a bonus is assigned 
to that label, doubling its score. In case of empty 
intersection (i.e. either no synset or no domain is 
shared by the two texts) this algorithm guarantees 
the same performances of the baseline. 
Strong Mutua l  help. In the strong version of 
the mutual help strategy, step 1 of the baseline 
is modified in that the domain label is scored if 
and only if it is found in the synset or domain 
intersection. While this algorithm does not guar- 
antee the baseline performance (because the inter- 
section may not contain all the correct synsets or 
domains), the precision score will give us indica- 
tions about the quality of the synset intersection. 
4 Exper imenta l  Set t ing  
The goal of the experiment is to establish some 
reference figures for Word Domain Disambigua- 
tion. Only nouns have been considered, mostly 
because the coverage of both MULTIWORDNET 
and of the domain mapping for verbs is far from 
being complete. 
Lemmas I Senses \[Mean Polysemy 
WN 1.6 94474 116317 1.23 
Ital WN 19104 25226 1.32 
DISC 56134 118029 2.10 
Table 1: Overview of the used resources (Noun 
part of speech) 
4.1 Lexlcal resources 
Besides the English WORDNET 1.6 we used MUL- 
TIWoRDNET \[Artale et al, 1997; Magnini and 
Strapparava, 1997\], an Italian version of the En- 
glish WoRDNET. It is based on the assump- 
tion that a large part of the conceptual relations 
defined for the English language can be shared 
with Italian. From an architectural point of view, 
MULTIWORDNET implements an extension of the 
WoRDNET lexical matrix to a "multilingual lexi- 
30 
Mean Values for Nouns Italian News English News 
Lexical Coverage WN 1.6 
ItalWN 
Disc 
# Synsets English 
Italian 
Intersection 
93% 
100% 
111.38 
35.48 
98% 
155.21 
Table 2: Mean lexical coverage and synset amount for AdnKronos news 
Mean Values for Nouns \] Italian News English News 
Sense Polysemy WN 1.6 
ItalWN 
Disc 
Domain Polysemy English 
Italian 
3.22 
6.82 
2.68 
4.37 
3.58 
Table 3: Mean sense and domain polysemy for AdnKronos news 
cal matrix" through the addition of a third dimen- 
sion relative to the language. MULTIWORDNET 
currently includes about 30,000 lemmas. 
As a matter of comparison, in particular to es- 
timate the lack of coverage of MULTIWORDNET, 
we consider some data from the Italian dictionary 
"DISC" \[Sabatini and Coletti, 1997\], a large size 
monolingual dictionary, available both as printed 
version and as CD-ROM. 
Table 1 shows some general figures (only for 
nouns) about the number of lemmas, the number 
of senses and the average polysemy for the three 
lexical resources considered. 
4.2 Para l le l  Texts 
Experiments have been carried out on a news cor- 
pus kindly placed at our disposal by AdnKronos, 
an important Italian news provider. The corpus 
consists of 168 parallel news (i.e. each news has 
both an Italian and an English version) concerning 
various topics (e.g. politics, economy, medicine, 
food, motors, fashion, culture, holidays). The av- 
erage length of the news is about 265 words. 
Table 2 reports the average lexical coverage (i.e. 
percent of lemmas found in the news corpus) for 
WORDNET 1.6, MULT IWORDNET and the Disc 
dictionary. A practically zero variance among the 
various news is exhibited. We observe a full cov- 
erage for the Disc dictionary; in addition, the in- 
completeness of MULT IWORDNET is limited to 
5% with respect to WoRDNET 1.6. The table 
also reports the average amount of unique synsets 
for each news. In this case the incompleteness of 
Italian WoRDNET with respect to WORDNET 1.6 
raises to 30%, showing that a significant amount 
of word senses is missing. 
Table 3 shows the average polysemy of the news 
corpus considering both word senses and word do- 
main labels. The figures reveal a polysemy reduc- 
tion of 17-18% when we deal with domain poly- 
semy. 
Manua l  Annotation. A subset of forty news 
pairs (about half of the initial corpus) have been 
manually annotated with the correct domain la- 
bel. Annotators were instructed about the domain 
hierarchy and then asked to select one domain la- 
bel for each lemma among those allowed by that 
lemma. 
Uncertain cases have been reviewed by a sec- 
ond annotator and, in case of persisting conflict, a 
third annotator was consulted to take a decision. 
Lemmatization errors as well as cases of incom- 
plete coverage of domain labels have been detected 
and excluded. The whole manual set consists of 
about 2500 annotated nouns. 
Although we do not have empirical evidences, 
our practical experience confirms the intuition 
that annotating texts with domain labels is an 
easier task than sense annotation. 
Forty-two domain labels, representing the more 
informative level of the domain hierarchy men- 
tioned in Section 1, have been used for the ex- 
periment. Table 4 reports the complete list. 
5 Resu l ts  and  D iscuss ion  
WSD and WDD on the  Semcor  Brown Cor-  
pus. In the first experiment we wanted to verify 
that, because of the polysemy reduction induced 
by domain clustering, WDD is a simpler task than 
31 
administration 
art 
commerce 
fashion 
mathematics 
play 
sociology 
agriculture 
artisanship 
computer.science 
history 
medicine 
politics 
sport 
alimentation 
astrology 
earth 
industry 
military 
psychology 
telecommunication 
anthropology 
astronomy 
economy 
law 
pedagogy 
publishing 
tourism 
archaeology 
biology 
engineering 
linguistics 
philosophy 
religion 
transport 
architecture 
chemistry 
factotum 
literature 
physics 
sexuality 
veterinary 
Table 4: Domain labels used in the experiment. 
Baseline 1 Baseline ~,) Weak Mutual (baseline 2) 
Synset Inter. I Domain Inter. 
Italian .83 .86 .87 .88 
English .85 .86 .87 .87 
Stron 0 Mutual (baseline ~) 
Synset Inter. I Domain Inter. 
.74 1.68 I .77 / .91 
.7o/.57 I .8o/.9  
Table 5: Precision and B,ecall (English and Italian) for different WDD algorithms 
WSD. For the experiment we used a subset of the 
Semcor corpus. As for WSD we obtained .66 of 
correct disambiguation with a sense frequency al- 
gorithm on polysemous noun words and .80 on all 
nouns (this last is also reported in the literature, 
for example in \[Mihalcea nd Moldovan, 1999\]). 
As for WDD, precision has been computed consid- 
ering the intersection between the word senses be- 
longing to the domain label with the higher score 
and the sense tag for that word reported in Sem- 
cor. Baseline I and baseline 2, described in section 
3.1, respectively gave .81 and .82 in precision, with 
a significant improvement over the WSD baseline, 
which confirms the initial hypothesis. 
WDD in paral le l  texts .  In this experiment 
we wanted to test WDD in the context of par- 
allel texts. Table 5 reports the precision and re- 
call (just in case it is not I) scores for six dif- 
ferent WDD algorithms applied to parallel En- 
glish/Italian texts. Numbers refer to polysemous 
words only. 
Both the baseline algorithms perfbrm quite well: 
83% for Italian and 85% for English in case of 
baseline 1, and 86% for both languages in case of 
baseline 2 are similar to the results obtained on 
the SemCor corpus. 
The algorithm which includes word domain fre- 
quency (i.e. baseline 2) reaches the highest score 
in both languages, indicating that the combina- 
tion of domain word frequency (considered at step 
1 of the algorithm) and domain text frequency 
(considered at step 2) is a good one. In addition, 
the fact that results are the same for both lan- 
guages indicates that the method can smooth the 
coverage differences among the wordnets. 
We expected a better esult for the bilingual ex- 
tensions. The weak mutual strategy, either con- 
sidering the synset intersection or the domain la- 
bels intersection, brings just minor improvements 
with respect o the baselines; the strong mutual 
strategy lowers both the precision and the recall. 
There are several explanations for these results. 
The difference in sense coverage between the two 
wordnets, about 30%, may affect the quality of 
the synset intersection: this would also explain 
the low degree of recall (68% for Italian and 57% 
for English). This is particularly evident for the 
strong mutual strategy, where the relative lexi- 
cal poorness of the Italian synsets can strongly 
reduce the number of synsets in the intersection. 
Note also that the length of the synset intersec- 
tion is about 30-40% of the mean synset number 
for Italian and English news respectively. This 
means less material which the disambiguation al- 
gorithms can take advantage of: relevant sysnsets 
can be left out of the intersection. For these rea- 
sons it is crucial having wordnet resources at the 
same level of completion to exploit he mutual help 
hypothesis. 
Furthermore, there may be a significant amount 
of senses which are "quasi" aligned. This may 
happen when two parallel senses map into close 
synsets, but not in the same one (e.g. one is the di- 
rect hypernym of the other). This problem could 
be overcome considering the IS-A relations during 
the computation of the intersection. In this situ- 
ation it is also probable that the senses maintain 
the same domain label. This would explain why 
the domain intersection behaves better than the 
synset intersection (from 74%-68% to 77%-91% 
for the Italian and from 70%-57% to 80%-91% for 
the English). 
32 
6 Conc lus ions  
We have introduced Word Domain Disambigua- 
tion, a variant of Word Sensse Disambiguation 
where words in a text are tagged with a domain 
label in place of a sense label. Two baseline algo- 
rithms has been presented as well as some exten- 
sions to deal with domain disambiguation i  the 
context of parallel translation texts. 
Two aligned wordnets, the English WORDNET 
1.6 and the Italian MULTIWORDNET, both aug- 
mented with domain labels, have been used as the 
main information repositories. 
The experimental results encourage to further 
investigate the potentiality of word domain dis- 
ambiguation. There are two interesting perspec- 
tives for the future work: first, we want to ex- 
ploit the relations among different lexical cate- 
gories (mainly nouns and verbs) when they share 
the same domain label; second, it seems reason- 
able that the disambiguation process may take ad- 
vantage of both WDD and WSD, where the initial 
word ambiguity is first reduced with WDD and 
then resolved with more fine grained information. 
Finally, an in-depth investigation is necessary for 
what we called factotum effect, which is peculiar 
of WDD. 
As for the applicative scenarios, we want to ap- 
ply WDD to the problem of content based user 
modelling. In particular we are developing a per- 
sonal agent for a news web site that learns user's 
interests from the requested pages that are an- 
alyzed to generate or to update a model of the 
user \[Strapparava et al, 2000\]. Exploiting this 
model, the system anticipates which documents 
in the web site could be interesting for the user. 
Using MULTIWORDNET and domain disambigua- 
tion algorithms, a content-based user model can 
be built as a semantic network whose nodes, in- 
dependent from the language, represent the word 
sense frequency rather then word frequency. Fur- 
therrnore, the resulting user model is indepen- 
dent from the language of the documents browsed. 
This is particular valuable with muitilingual web 
sites, that are becoming very common especially 
in news sites or in electronic ommerce domains. 
Re ferences  
A. Artale, B. Magnini, and C. Strapparava. 
WoRDNET for italian and its use for lexical 
discrimination. In AI*IA97: Advances in Ar- 
tificial Intelligence. Springer Verlag, 1997. 
P. Buitelaar. CoPJ~LEX: An ontology of sys- 
tematic polysemous classes. In Proceedings of 
FOIS98, International Conference on Formal 
Ontology in Information Systems, Trento, Italy, 
June 6-8 1998. IOS Press, 1998. 
P. Buitelaar. Reducing lexical semantic omplex- 
ity with systematic polysemous classes and un- 
derspecification. In Proceedings of ANLP2000 
Workshop on Syntactic and Semantic Complex- 
ity in Natural Language Processing Systems, 
Seattle, USA, April 30 2000, 2000. 
J. Gonzalo, F. Verdejio, C. Peters, and N. Calzo- 
lari. Applying eurowordnet to cross-language 
text retrieval. Computers and Humanities, 
32(2-3):185--207, 1998. 
A. Kilgarriff and C. Yallop. What's in a the- 
sanrus? In Proceedings of LREC-BO00, Sec- 
ond International Conference on Language Re- 
sources and Evaluation, Athens, Greece, June 
2000. 
B. Maguini and G. Cavagli~. Integrating subject 
field codes into WordNet. In Proceedings of 
LREC-2000, Second International Conference 
on Language Resources and Evaluation, Athens, 
Greece, June 2000. 
B. Maguini and C. Strapparava. Costruzione di 
una base di conoscenza lessicale per l'italiano 
basata su WordNet. In M. Carapezza, D. Gam- 
barara, and F. Lo Piparo, editors, Linguaggio e 
Cognizione. Bulzoni, Palermo, Italy, 1997. 
K. Mihalcea nd D. Moldovan. A method for word 
sense disambiguation of unrestricted text. In 
Proc. of A CL-99, College Park Maryland, June 
1999. held in conjunction with UM'96. 
G. Miller. An on-line lexical database. Interna- 
tional Journal of Lexicography, 13(4):235-312, 
1990. 
F. Sabatini and V. Coletti. Dizionario Italiano 
Sabatini Coletti. Giunti, 1997. 
C. Strapparava, B. Magnini, and A. Stefani. 
Seuse-based user modelling for web sites. In 
Adaptive Hyperraedia nd Adaptive Web-Based 
Systems - Lecture Notes in Computer Science 
1892. Springer Verlag, 2000. 
E. Voorhees. Using wordnet for text retrieval. In 
C. Fellbaum, editor, WordNet - an Electronic 
Lexical Database. MIT Press, 1998. 
Y. Wilks and M. Stevenson. Word sense dis- 
ambiguation using optimised combination of 
knowledge sources. In Proc. of COLING- 
A CL '98, 98. 
33 
Pattern Abstraction and Term Similarity for Word Sense Disambiguation:
IRST at Senseval-3
Carlo Strapparava and Alfio Gliozzo and Claudio Giuliano
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY
{strappa, gliozzo, giuliano}@itc.it
Abstract
This paper summarizes IRST?s participation in
Senseval-3. We participated both in the English all-
words task and in some lexical sample tasks (En-
glish, Basque, Catalan, Italian, Spanish). We fol-
lowed two perspectives. On one hand, for the all-
words task, we tried to refine the Domain Driven
Disambiguation that we presented at Senseval-2.
The refinements consist of both exploiting a new
technique (Domain Relevance Estimation) for do-
main detection in texts, and experimenting with the
use of Latent Semantic Analysis to avoid reliance on
manually annotated domain resources (e.g. WORD-
NET DOMAINS). On the other hand, for the lexical
sample tasks, we explored the direction of pattern
abstraction and we demonstrated the feasibility of
leveraging external knowledge using kernel meth-
ods.
1 Introduction
The starting point for our research in the Word
Sense Disambiguation (WSD) area was to explore
the use of semantic domains in order to solve lex-
ical ambiguity. At the Senseval-2 competition we
proposed a new approach to WSD, namely Domain
Driven Disambiguation (DDD). This approach con-
sists of comparing the estimated domain of the con-
text of the word to be disambiguated with the do-
mains of its senses, exploiting the property of do-
mains to be features of both texts and words. The
domains of the word senses can be either inferred
from the learning data or derived from the informa-
tion in WORDNET DOMAINS.
For Senseval-3, we refined the DDD methodol-
ogy with a fully unsupervised technique - Domain
Relevance Estimation (DRE) - for domain detection
in texts. DRE is performed by an expectation maxi-
mization algorithm for the gaussian mixture model,
which is exploited to differentiate relevant domain
information in texts from noise. This refined DDD
system was presented in the English all-words task.
Originally DDD was developed to assess the use-
fulness of domain information for WSD. Thus it
did not exploit other knowledge sources commonly
used for disambiguation (e.g. syntactic patterns or
collocations). As a consequence the performance of
the DDD system is quite good for precision (it dis-
ambiguates well the ?domain? words), but as far as
recall is concerned it is not competitive compared
with other state of the art techniques. On the other
hand DDD outperforms the state of the art for unsu-
pervised systems, demonstrating the usefulness of
domain information for WSD.
In addition, the DDD approach requires domain
annotations for word senses (for the experiments we
used WORDNET DOMAINS, a lexical resource de-
veloped at IRST). Like all manual annotations, such
an operation is costly (more than two man years
have been spent for labeling the whole WORDNET
DOMAINS structure) and affected by subjectivity.
Thus, one drawback of the DDD methodology was
a lack of portability among languages and among
different sense repositories (unless we have synset-
aligned WordNets).
Besides the improved DDD, our other proposals
for Senseval-3 constitute an attempt to overcome
these previous issues.
To deal with the problem of having a domain-
annotated WORDNET, we experimented with a
novel methodology to automatically acquire domain
information from corpora. For this aim we esti-
mated term similarity from a large scale corpus, ex-
ploiting the assumption that semantic domains are
sets of very closely related terms. In particular we
implemented a variation of Latent Semantic Analy-
sis (LSA) in order to obtain a vector representation
for words, texts and synsets. LSA performs a di-
mensionality reduction in the feature space describ-
ing both texts and words, capturing implicitly the
notion of semantic domains required by DDD. In
order to perform disambiguation, LSA vectors have
been estimated for the synsets in WORDNET. We
participated in the English all-words task also with
a first prototype (DDD-LSA) that exploits LSA in-
stead of WORDNET DOMAINS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Task Systems
English All-Words DDD DDD-LSA
English Lex-sample Kernels-WSD Ties
Italian Lex-Sample Kernels-WSD Ties
Basque Lex-Sample Kernels-WSD
Catalan Lex-Sample Kernels-WSD
Spanish Lex-Sample Kernels-WSD
Table 1: IRST participation at Senseval-3
As far as lexical sample tasks are concerned, we
participated in the English, Italian, Spanish, Cata-
lan, and Basque tasks. For these tasks, we ex-
plored the direction of pattern abstraction for WSD.
Pattern abstraction is an effective methodology for
WSD (Mihalcea, 2002). Our preliminary experi-
ments have been performed using TIES, a general-
ized Information Extraction environment developed
at IRST that implements the boosted wrapper induc-
tion algorithm (Freitag and Kushmerick, 2000). The
main limitation of such an approach is, once more,
the integration of different knowledge sources. In
particular, paradigmatic information seems hard to
be represented in the TIES framework, motivating
our decision to exploit kernel methods for WSD.
Kernel methods is an area of recent interest in
Machine Learning. Kernels are similarity functions
between instances that allows to integrate different
knowledge sources and to model explicitly linguis-
tic insights inside the powerful framework of sup-
port vector machine classification. For Senseval-3
we implemented the Kernels-WSD system, which
exploits kernel methods to perform the following
operations: (i) pattern abstraction; (ii) combination
of different knowledge sources, in particular domain
information and syntagmatic information; (iii) inte-
gration of unsupervised term proximity estimation
in the supervised framework.
The paper is structured as follows. Section 2 in-
troduces LSA and its relations with semantic do-
mains. Section 3 presents the systems for the En-
glish all-words task (i.e. DDD and DDD-LSA). In
section 4 our supervised approaches are reported.
In particular the TIES system is described in section
4.1, while the approach based on kernel methods is
discussed in section 4.2.
2 Semantic Domains and LSA
Domains are common areas of human discussion,
such as economics, politics, law, science etc., which
are at the basis of lexical coherence. A substantial
part of the lexicon is composed by ?domain words?,
that refer to concepts belonging to specific domains.
In (Magnini et al, 2002) it has been claimed that
domain information provides generalized features at
the paradigmatic level that are useful to discriminate
among word senses.
The WORDNET DOMAINS1 lexical resource
is an extension of WORDNET which provides
such domain labels for all synsets (Magnini and
Cavaglia`, 2000). About 200 domain labels were se-
lected from a number of dictionaries and then struc-
tured in a taxonomy according to the Dewey Deci-
mal Classification (DDC). The annotation method-
ology was mainly manual and took about 2 person
years.
WORDNET DOMAINS has been proven a useful
resource for WSD. However some aspects induced
us to explore further developments. These issues
are: (i) it is difficult to find an objective a-priori
model for domains; (ii) the annotation procedure
followed to develop WORDNET DOMAINS is very
expensive, making hard the replicability of the lexi-
cal resource for other languages or domain specific
sub-languages; (iii) the domain distinctions are rigid
in WORDNET DOMAINS, while a more ?fuzzy? as-
sociation between domains and concepts is often
more appropriate to describe term similarity.
In order to generalize the domain approach and to
overcome these issues, we explored the direction of
unsupervised learning on a large-scale corpus (we
used the BNC corpus for all the experiments de-
scribed in this paper).
In particular, we followed the LSA approach
(Deerwester et al, 1990). In LSA, term co-
occurrences in the documents of the corpus are cap-
tured by means of a dimensionality reduction oper-
ated on the term-by-document matrix. The result-
ing LSA vectors can be exploited to estimate both
term and document similarity. Regarding document
similarity, Latent Semantic Indexing (LSI) is a tech-
nique that allows one to represent a document by
a LSA vector. In particular, we used a variation
of the pseudo-document methodology described in
(Berry, 1992). Each document can be represented in
the LSA space by summing up the normalized LSA
vectors of all the terms contained in it.
By exploiting LSA vectors for terms, it is pos-
sible to estimate domain vectors for the synsets of
WORDNET, in order to obtain similarity values be-
tween concepts that can be used for synset cluster-
ing and WSD. Thus, term and document vectors can
be used instead of WORDNET DOMAINS for WSD
and other applications in which term similarity and
domain relevance estimation is required.
1WORDNET DOMAINS is freely available for research pur-
poses at wndomains.itc.it
3 All-Words systems: DDD and DDD-LSA
DDD with DRE. DDD assignes the right sense of
a word in its context comparing the domain of the
context to the domain of each sense of the word.
This methodology exploits WORDNET DOMAINS
information to estimate both the domain of the tex-
tual context and the domain of the senses of the
word to disambiguate.
The basic idea to estimate domain relevance for
texts is to exploit lexical coherence inside texts. A
simple heuristic approach to this problem, used in
Senseval-2, is counting the occurrences of domain
words for every domain inside the text: the higher
the percentage of domain words for a certain do-
main, the more relevant the domain will be for the
text.
Unfortunately, the simple local frequency count
is not a good domain relevance measure for sev-
eral reasons. Indeed irrelevant senses of ambigu-
ous words contribute to augment the final score of
irrelevant domains, introducing noise. Moreover,
the level of noise is different for different domains
because of their different sizes and possible dif-
ferences in the ambiguity level of their vocabular-
ies. We refined the original Senseval-2 DDD system
with the Domain Relevance Estimation (DRE) tech-
nique. Given a certain domain, DRE distinguishes
between relevant and non-relevant texts by means
of a Gaussian Mixture model that describes the fre-
quency distribution of domain words inside a large-
scale corpus (in particular we used the BNC corpus
also in this case). Then, an Expectation Maximiza-
tion algorithm computes the parameters that maxi-
mize the likelihood of the model on the empirical
data (Gliozzo et al, 2004).
In order to represent domain information we in-
troduced the notion of Domain Vectors (DV), which
are data structures that collect domain information.
These vectors are defined in a multidimensional
space, in which each domain represents a dimen-
sion of the space. We distinguish between two
kinds of DVs: (i) synset vectors, which represent
the relevance of a synset with respect to each con-
sidered domain and (ii) text vectors, which repre-
sent the relevance of a portion of text with respect
to each domain in the considered set. The core of
the DDD algorithm is based on scoring the compar-
ison of these kinds of vectors. The synset vectors
are built considering WORDNET DOMAINS, while
in the calculation of scoring the system takes into
account synset probabilities on SemCor. The sys-
tem makes use of a threshold th-cut, ranging in the
interval [0,1], that allows us to tune the tradeoff be-
tween precision and recall.
th-cut Prec Recall Attempted
0.0 0.583 0.583 99.76
0.9 0.729 0.441 60.51
Table 2: DDD on the English all-words task.
Latent Semantic Domains for DDD. As seen in
Section 2, it is possible to implement a DDD ver-
sion that does not use WORDNET DOMAINS and
instead it exploits LSA term and document vectors
for estimating synset vectors and text vectors, leav-
ing the core of DDD algorithm unchanged. As for
text vectors, we used the psedo-document technique
also for building synset vectors: in this case we con-
sider the synonymous terms contained in the synset
itself.
The system presented at Senseval-3 does not
make use of any statistics on SemCor, and conse-
quently it can be considered fully unsupervised. Re-
sults are reported in table 3 and do not differ much
from the results obtained by DDD in the same task.
th-cut Prec Recall Attempted
0.5 0.661 0.496 75.01
Table 3: DDD-LSA on the English all-words task.
4 Lexical Sample Systems: Pattern
abstraction and Kernel Methods
One of the most discriminative features for lexi-
cal disambiguation is the lexical/syntactic pattern in
which the word appears. A well known issue in the
WSD area is the one sense per collocation claim
(Yarowsky, 1993) stating that the word meanings
are strongly associated with the particular colloca-
tion in which the word is located. Collocations are
sequences of words in the context of the word to
disambiguate, and can be associated to word senses
performing supervised learning.
Another important knowledge source for WSD is
the shallow-syntactic pattern in which a word ap-
pears. Syntactic patterns, like lexical patterns, can
be obtained by exploiting pattern abstraction tech-
niques on POS sequences. In the WSD literature
both lexical and syntactic patterns have been used
as features in a supervised learning schema by rep-
resenting each instance using bigrams and trigrams
in the surrounding context of the word to be ana-
lyzed2.
2More recently deep-syntactic features have been also con-
sidered by several systems, as for example modifiers of nouns
and verbs, object and subject of the sentence, etc. In order to
Representing each instance by a ?bag of features?
presents several disadvantages from the point of
view of both machine learning and computational
linguistics: (1) Sparseness in the learning data: most
of the collocations found in the learning data occur
just once, reducing the generalization power of the
learning algorithm. In addition most of the collo-
cations found in the test data are often unseen in
the training data. (2) Low flexibility for pattern ab-
straction purposes: bigram and trigram extraction
schemata are fixed in advance. (3) Knowledge ac-
quisition bottleneck: the size of the training data is
not large enough to cover each possible collocation
in the language.
To overcome problems 1 and 2 we investigated
some pattern abstraction techniques from the area
of Information Extraction (IE) and we adapted them
to WSD. To overcome problem 3 we developed La-
tent Semantic Kernels, which allow us to integrate
external knowledge provided by unsupervised term
similarity estimation.
4.1 TIES
Our first experiments have been performed exploit-
ing TIES, an environment developed at IRST for IE
that induces patterns from the marked entities in the
training phase, and then applies those patterns in the
test phase in order to assign a category if the pat-
tern is satisfied. For our experiments, we used the
Boosted Wrapper Induction (BWI) algorithm (Fre-
itag and Kushmerick, 2000) that is implemented in
TIES.
For Senseval-3 we used very few features (lemma
and POS). We proposed the system in this configu-
ration as a ?baseline? system for pattern abstraction.
Task Prec Recall Attempted
English LS 0.706 0.505 71.50
English LS (coarse) 0.767 0.548 71.50
Italian LS 0.552 0.309 55.92
Table 4: Performance of the TIES system
Our preliminary experiments with BWI have
shown that pattern abstraction is very attractive for
WSD, allowing us to achieve a very high precision
for a restricted number of words, in which the syn-
tagmatic information is sufficient for disambigua-
tion. However, we still had some restrictions. In
particular, the integration with different knowledge
sources for classification is not trivial.
obtain such features parsing of the data is required. However,
we decided to do not use such information, while we plan to
introduce it in the next future.
4.2 Kernel-WSD
Our choice of exploiting kernel methods for WSD
has been motivated by the observation that pattern-
based approaches for disambiguation are comple-
mentary to the domain based ones: they require dif-
ferent knowledge sources and different techniques
for classification and feature description. Both ap-
proaches have to be simultaneously taken into ac-
count in order to perform accurate disambiguation.
Our aim was to combine them into a common
framework.
Kernel methods, e.g. Support Vector Machines
(SVMs), are state-of-the-art learning algorithms,
and they are successfully adopted in many NLP
tasks.
The idea of SVM (Cristianini and Shawe-Taylor,
2000) is to map the set of training data into a higher-
dimensional feature space F via a mapping func-
tion ? : ? ? F , and construct a separating hy-
perplane with maximum margin (distance between
planes and closest points) in the new space. Gen-
erally, this yields a nonlinear decision boundary in
the input space. Since the feature space is high di-
mensional, performing the transformation has of-
ten a high computational cost. Rather than use the
explicit mapping ?, we can use a kernel function
K : ??? ? < , that corresponds to the inner prod-
uct in a feature space which is, in general, different
from the input space.
Therefore, a kernel function provides a way
to compute (efficiently) the separating hyperplane
without explicitly carrying out the map ? into the
feature space - this is called the kernel trick. In this
way the kernel acts as an interface between the data
and the learning algorithm by defining an implicit
mapping into the feature space. Intuitively, we can
see the kernel as a function that measures the sim-
ilarity between pairs of objects. The learning algo-
rithm, which compares all pairs of data items, ex-
ploits the information encoded in the kernel. An
important characteristic of kernels is that they are
not limited to vector objects but are applicable to
virtually any kind of object representation.
In this work we use kernel methods to combine
heterogeneous sources of information that we found
relevant for WSD. For each of these aspects it is
possible to define kernels independently. Then they
are combined by exploiting the property that the
sum of two kernels is still a kernel (i.e. k(x, y) =
k1(x, y) + k2(x, y)), taking advantage of each sin-
gle contribution in an intuitive way3.
3In order to keep the kernel values comparable for dif-
ferent values and to be independent from the length of the
examples, we considered the normalized version K?(x, y) =
lsa Task Prec Recall Attempted MF-Baseline
? English LS 0.726 0.726 100 0.552
? English LS (coarse) 0.795 0.795 100 0.645
- English LS (no-lsa) 0.704 0.704 100 0.552
- Basque LS 0.655 0.655 100 0.558
- Italian LS 0.531 0.531 100 0.183
- Catalan LS 0.858 0.846 98.62 0.663
- Spanish LS 0.842 0.842 100 0.677
Table 5: Performance of the Kernels-WSD system
The Word Sense Disambiguation Kernel is de-
fined in this way:
KWSD(x, y) = KS(x, y) + KP (x, y) (1)
where KS is the Syntagmatic Kernel and KP is
the Paradigmatic Kernel.
The Syntagmatic Kernel. The syntagmatic ker-
nel generalizes the word-sequence kernels defined
by (Cancedda et al, 2003) to sequences of lem-
mata and POSs. Word sequence kernels are based
on the following idea: two sequences are similar
if they have in common many sequences of words
in a given order. The similarity between two ex-
amples is assessed by the number (possibly non-
contiguous) of the word sequences matching. Non-
contiguous occurrences are penalized according to
the number of gaps they contain. For example the
sequence of words ?I go very quickly to school? is
less similar to ?I go to school? than ?I go quickly to
school?. Different than the bag-of-word approach,
word sequence kernels capture the word order and
allow gaps between words. The word sequence ker-
nels are parametric with respect to the length of the
(sparse) sequences they want to capture.
We have defined the syntagmatic kernel as the
sum of n distinct word-sequence kernels for lem-
mata (i.e. Collocation Kernel - KC ) and sequences
of POSs (i.e. POS Kernel - KPOS), according to the
formula (for our experiments we set n to 2):
KS(x, y) =
n
X
i=1
KCi(x, y) +
n
X
i=1
KPOSi(x, y) (2)
In the above definition of syntagmatic kernel,
only exact lemma/POS matches contribute to the
similarity. One shortcoming of this approach is
that (near-)synonyms will never be considered sim-
ilar. We address this problem by considering soft-
matching of words employing a term similarity
K(x, y)/sqrt(K(x, x)K(y, y))
measure based on LSA4. In particular we consid-
ered equivalent two words having the same POS and
a similarity value higher than an empirical thresh-
old. For example, if we consider as equivalent
the terms Ronaldo and football player the sequence
The football player scored the first goal can be con-
sidered equivalent to the sentence Ronaldo scored
the first goal. The properties of the kernel methods
offer a flexible way to plug additional information,
in this case unsupervised (we could also take this in-
formation from a semantic network such as WORD-
NET).
The Paradigmatic Kernel. The paradigmatic
kernel takes into account the paradigmatic aspect of
sense distinction (i.e. domain aspects) (Gliozzo et
al., 2004). For example the word virus can be dis-
ambiguated by recognizing the domain of the con-
text in which it is placed (e.g. computer science
vs. biology). Usually such an aspect is captured
by ?bag-of-words?, in analogy to the Vector Space
Model, widely used in Text Categorization and In-
formation Retrieval. The main limitation of this
model for WSD is the knowledge acquisition bot-
tleneck (i.e. the lack of sense tagged data). Bag of
words are very sparse data that require a large scale
corpus to be learned. To overcome such a limita-
tion, Latent Semantic Indexing (LSI) can provide a
solution.
Thus we defined a paradigmatic kernel composed
by the sum of a ?traditional? bag of words kernel
and an LSI kernel (Cristianini et al, 2002) as de-
fined by formula 3:
KP (x, y) = KBoW (x, y) + KLSI(x, y) (3)
where KBoW computes the inner product be-
tween the vector space model representations and
KLSI computes the cosine between the LSI vectors
representing the texts.
4For languages other than English, we did not exploit this
soft-matching and the KLSI kernel described below. See the
first column in the table 5.
Table 5 displays the performance of Kernel-
WSD. As a comparison, we also report the figures
on the English task without using LSA. The last col-
umn reports the recall of the most-frequent baseline.
Acknowledgments
Claudio Giuliano is supported by the IST-Dot.Kom
project sponsored by the European Commission
(Framework V grant IST-2001-34038). TIES and
the kernel package have been developed in the con-
text of the Dot.Kom project.
References
M. Berry. 1992. Large-scale sparse singular value
computations. International Journal of Super-
computer Applications, 6(1):13?49.
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Ren-
ders. 2003. Word-sequence kernels. Journal of
Machine Learning Research, 3(6):1059?1082.
N. Cristianini and J. Shawe-Taylor. 2000. Support
Vector Machines. Cambridge University Press.
N. Cristianini, J. Shawe-Taylor, and H. Lodhi.
2002. Latent semantic kernels. Journal of Intel-
ligent Information Systems, 18(2):127?152.
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science, 41(6):391?407.
D. Freitag and N. Kushmerick. 2000. Boosted
wrapper induction. In Proc. of AAAI-00, pages
577?583, Austin, Texas.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004.
Unsupervised and supervised exploitation of se-
mantic domains in lexical disambiguation. Com-
puter Speech and Language, Forthcoming.
B. Magnini and G. Cavaglia`. 2000. Integrating sub-
ject field codes into WordNet. In Proceedings of
LREC-2000, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
R. F. Mihalcea. 2002. Word sense disambiguation
with pattern learning and automatic feature selec-
tion. Natural Language Engineering, 8(4):343?
358.
D. Yarowsky. 1993. One sense per collocation. In
Proceedings of ARPA Human Language Technol-
ogy Workshop, pages 266?271, Princeton.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Unsupervised Domain Relevance Estimation
for Word Sense Disambiguation
Alfio Gliozzo and Bernardo Magnini and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY
{gliozzo, magnini, strappa}@itc.it
Abstract
This paper presents Domain Relevance Estima-
tion (DRE), a fully unsupervised text categorization
technique based on the statistical estimation of the
relevance of a text with respect to a certain cate-
gory. We use a pre-defined set of categories (we
call them domains) which have been previously as-
sociated to WORDNET word senses. Given a cer-
tain domain, DRE distinguishes between relevant
and non-relevant texts by means of a Gaussian Mix-
ture model that describes the frequency distribution
of domain words inside a large-scale corpus. Then,
an Expectation Maximization algorithm computes
the parameters that maximize the likelihood of the
model on the empirical data.
The correct identification of the domain of the
text is a crucial point for Domain Driven Dis-
ambiguation, an unsupervised Word Sense Disam-
biguation (WSD) methodology that makes use of
only domain information. Therefore, DRE has been
exploited and evaluated in the context of a WSD
task. Results are comparable to those of state-of-
the-art unsupervised WSD systems and show that
DRE provides an important contribution.
1 Introduction
A fundamental issue in text processing and under-
standing is the ability to detect the topic (i.e. the do-
main) of a text or of a portion of it. Indeed, domain
detection allows a number of useful simplifications
in text processing applications, such as, for instance,
in Word Sense Disambiguation (WSD).
In this paper we introduce Domain Relevance Es-
timation (DRE) a fully unsupervised technique for
domain detection. Roughly speaking, DRE can be
viewed as a text categorization (TC) problem (Se-
bastiani, 2002), even if we do not approach the
problem in the standard supervised setting requir-
ing category labeled training data. In fact, recently,
unsupervised approaches to TC have received more
and more attention in the literature (see for example
(Ko and Seo, 2000).
We assume a pre-defined set of categories, each
defined by means of a list of related terms. We
call such categories domains and we consider them
as a set of general topics (e.g. SPORT, MEDICINE,
POLITICS) that cover the main disciplines and ar-
eas of human activity. For each domain, the list
of related words is extracted from WORDNET DO-
MAINS (Magnini and Cavaglia`, 2000), an extension
of WORDNET in which synsets are annotated with
domain labels. We have identified about 40 domains
(out of 200 present in WORDNET DOMAINS) and
we will use them for experiments throughout the pa-
per (see Table 1).
DRE focuses on the problem of estimating a de-
gree of relatedness of a certain text with respect to
the domains in WORDNET DOMAINS.
The basic idea underlying DRE is to combine the
knowledge in WORDNET DOMAINS and a proba-
bilistic framework which makes use of a large-scale
corpus to induce domain frequency distributions.
Specifically, given a certain domain, DRE considers
frequency scores for both relevant and non-relevant
texts (i.e. texts which introduce noise) and represent
them by means of a Gaussian Mixture model. Then,
an Expectation Maximization algorithm computes
the parameters that maximize the likelihood of the
empirical data.
DRE methodology originated from the effort to
improve the performance of Domain Driven Dis-
ambiguation (DDD) system (Magnini et al, 2002).
DDD is an unsupervised WSD methodology that
makes use of only domain information. DDD as-
signes the right sense of a word in its context com-
paring the domain of the context to the domain of
each sense of the word. This methodology exploits
WORDNET DOMAINS information to estimate both
Domain #Syn Domain #Syn Domain #Syn
Factotum 36820 Biology 21281 Earth 4637
Psychology 3405 Architecture 3394 Medicine 3271
Economy 3039 Alimentation 2998 Administration 2975
Chemistry 2472 Transport 2443 Art 2365
Physics 2225 Sport 2105 Religion 2055
Linguistics 1771 Military 1491 Law 1340
History 1264 Industry 1103 Politics 1033
Play 1009 Anthropology 963 Fashion 937
Mathematics 861 Literature 822 Engineering 746
Sociology 679 Commerce 637 Pedagogy 612
Publishing 532 Tourism 511 Computer Science 509
Telecommunication 493 Astronomy 477 Philosophy 381
Agriculture 334 Sexuality 272 Body Care 185
Artisanship 149 Archaeology 141 Veterinary 92
Astrology 90
Table 1: Domain distribution over WORDNET synsets.
the domain of the textual context and the domain of
the senses of the word to disambiguate. The former
operation is intrinsically an unsupervised TC task,
and the category set used has to be the same used
for representing the domain of word senses.
Since DRE makes use of a fixed set of target cat-
egories (i.e. domains) and since a document col-
lection annotated with such categories is not avail-
able, evaluating the performance of the approach is
a problem in itself. We have decided to perform an
indirect evaluation using the DDD system, where
unsupervised TC plays a crucial role.
The paper is structured as follows. Section 2
introduces WORDNET DOMAINS, the lexical re-
source that provides the underlying knowledge to
the DRE technique. In Section 3 the problem of es-
timating domain relevance for a text is introduced.
In particular, Section 4 briefly sketchs the WSD sys-
tem used for evaluation. Finally, Section 5 describes
a number of evaluation experiments we have carried
out.
2 Domains, WORDNET and Texts
DRE heavily relies on domain information as its
main knowledge source. Domains show interesting
properties both from a lexical and a textual point of
view. Among these properties there are: (i) lexi-
cal coherence, since part of the lexicon of a text is
composed of words belonging to the same domain;
(ii) polysemy reduction, because the potential am-
biguity of terms is sensibly lower if the domain of
the text is specified; and (iii) lexical identifiability
of text?s domain, because it is always possible to as-
sign one or more domains to a given text by consid-
ering term distributions in a bag-of-words approach.
Experimental evidences of these properties are re-
ported in (Magnini et al, 2002).
In this section we describe WORDNET DO-
MAINS1 (Magnini and Cavaglia`, 2000), a lexical re-
source that attempts a systematization of relevant
aspects in domain organization and representation.
WORDNET DOMAINS is an extension of WORD-
NET (version 1.6) (Fellbaum, 1998), in which each
synset is annotated with one or more domain la-
bels, selected from a hierarchically organized set of
about two hundred labels. In particular, issues con-
cerning the ?completeness? of the domain set, the
?balancing? among domains and the ?granularity?
of domain distinctions, have been addressed. The
domain set used in WORDNET DOMAINS has been
extracted from the Dewey Decimal Classification
(Comaroni et al, 1989), and a mapping between the
two taxonomies has been computed in order to en-
sure completeness. Table 2 shows how the senses
for a word (i.e. the noun bank) have been associated
to domain label; the last column reports the number
of occurrences of each sense in Semcor2.
Domain labeling is complementary to informa-
tion already present in WORDNET. First of all,
a domain may include synsets of different syn-
tactic categories: for instance MEDICINE groups
together senses from nouns, such as doctor#1
and hospital#1, and from verbs, such as
operate#7. Second, a domain may include
senses from different WORDNET sub-hierarchies
(i.e. deriving from different ?unique beginners? or
from different ?lexicographer files?). For example,
SPORT contains senses such as athlete#1, deriv-
ing from life form#1, game equipment#1
from physical object#1, sport#1
1WORDNET DOMAINS is freely available at
http://wndomains.itc.it
2SemCor is a portion of the Brown corpus in which words
are annotated with WORDNET senses.
Sense Synset and Gloss Domains Semcor frequencies
#1 depository financial institution, bank, banking con-
cern, banking company (a financial institution. . . )
ECONOMY 20
#2 bank (sloping land. . . ) GEOGRAPHY, GEOLOGY 14
#3 bank (a supply or stock held in reserve. . . ) ECONOMY -
#4 bank, bank building (a building. . . ) ARCHITECTURE, ECONOMY -
#5 bank (an arrangement of similar objects...) FACTOTUM 1
#6 savings bank, coin bank, money box, bank (a con-
tainer. . . )
ECONOMY -
#7 bank (a long ridge or pile. . . ) GEOGRAPHY, GEOLOGY 2
#8 bank (the funds held by a gambling house. . . ) ECONOMY, PLAY
#9 bank, cant, camber (a slope in the turn of a road. . . ) ARCHITECTURE -
#10 bank (a flight maneuver. . . ) TRANSPORT -
Table 2: WORDNET senses and domains for the word ?bank?.
from act#2, and playing field#1 from
location#1.
Domains may group senses of the same word
into thematic clusters, which has the important side-
effect of reducing the level of ambiguity when we
are disambiguating to a domain. Table 2 shows
an example. The word ?bank? has ten differ-
ent senses in WORDNET 1.6: three of them (i.e.
bank#1, bank#3 and bank#6) can be grouped
under the ECONOMY domain, while bank#2 and
bank#7 both belong to GEOGRAPHY and GEOL-
OGY. Grouping related senses is an emerging topic
in WSD (see, for instance (Palmer et al, 2001)).
Finally, there are WORDNET synsets that do not
belong to a specific domain, but rather appear in
texts associated with any domain. For this reason,
a FACTOTUM label has been created that basically
includes generic synsets, which appear frequently
in different contexts. Thus the FACTOTUM domain
can be thought of as a ?placeholder? for all other
domains.
3 Domain Relevance Estimation for Texts
The basic idea of domain relevance estimation for
texts is to exploit lexical coherence inside texts.
From the domain point of view lexical coherence
is equivalent to domain coherence, i.e. the fact that
a great part of the lexicon inside a text belongs to
the same domain.
From this observation follows that a simple
heuristic to approach this problem is counting the
occurrences of domain words for every domain in-
side the text: the higher the percentage of domain
words for a certain domain, the more relevant the
domain will be for the text. In order to perform this
operation the WORDNET DOMAINS information is
exploited, and each word is assigned a weighted list
of domains considering the domain annotation of
its synsets. In addition, we would like to estimate
the domain of the text locally. Local estimation
of domain relevance is very important in order to
take into account domain shifts inside the text. The
methodology used to estimate domain frequency is
described in subsection 3.1.
Unfortunately the simple local frequency count
is not a good domain relevance measure for sev-
eral reasons. The most significant one is that very
frequent words have, in general, many senses be-
longing to different domains. When words are used
in texts, ambiguity tends to disappear, but it is not
possible to assume knowing their actual sense (i.e.
the sense in which they are used in the context) in
advance, especially in a WSD framework. The sim-
ple frequency count is then inadequate for relevance
estimation: irrelevant senses of ambiguous words
contribute to augment the final score of irrelevant
domains, introducing noise. The level of noise is
different for different domains because of their dif-
ferent sizes and possible differences in the ambigu-
ity level of their vocabularies.
In subsection 3.2 we propose a solution for that
problem, namely the Gaussian Mixture (GM) ap-
proach. This constitutes an unsupervised way to es-
timate how to differentiate relevant domain infor-
mation in texts from noise, because it requires only
a large-scale corpus to estimate parameters in an
Expectation Maximization (EM) framework. Using
the estimated parameters it is possible to describe
the distributions of both relevant and non-relevant
texts, converting the DRE problem into the problem
of estimating the probability of each domain given
its frequency score in the text, in analogy to the
bayesian classification framework. Details about
the EM algorithm for GM model are provided in
subsection 3.3.
3.1 Domain Frequency Score
Let t ? T , be a text in a corpus T composed by a list
of words wt1, . . . , wtq . Let D = {D1, D2, ..., Dd} be
the set of domains used. For each domain Dk the
domain ?frequency? score is computed in a window
of c words around wtj . The domain frequency scoreis defined by formula (1).
F (Dk, t, j) =
j+c
X
i=j?c
Rword(Dk, wti)G(i, j, (
c
2)
2
) (1)
where the weight factor G(x, ?, ?2) is the density
of the normal distribution with mean ? and standard
deviation ? at point x and Rword(D,w) is a function
that return the relevance of a domain D for a word
w (see formula 3). In the rest of the paper we use the
notation F (Dk, t) to refer to F (Dk, t,m), where m
is the integer part of q/2 (i.e. the ?central? point of
the text - q is the text length).
Here below we see that the information contained
in WORDNET DOMAINS can be used to estimate
Rword(Dk, w), i.e. domain relevance for the word
w, which is derived from the domain relevance of
the synsets in which w appears.
As far as synsets are concerned, domain informa-
tion is represented by the function Dom : S ?
P (D)3 that returns, for each synset s ? S, where
S is the set of synsets in WORDNET DOMAINS, the
set of the domains associated to it. Formula (2) de-
fines the domain relevance estimation function (re-
member that d is the cardinality of D):
Rsyn(D, s) =
8
<
:
1/|Dom(s)| : if D ? Dom(s)
1/d : if Dom(s) = {FACTOTUM}
0 : otherwise
(2)
Intuitively, Rsyn(D, s) can be perceived as an es-
timated prior for the probability of the domain given
the concept, as expressed by the WORDNET DO-
MAINS annotation. Under these settings FACTO-
TUM (generic) concepts have uniform and low rel-
evance values for each domain while domain con-
cepts have high relevance values for a particular do-
main.
The definition of domain relevance for a word is
derived directly from the one given for concepts. In-
tuitively a domain D is relevant for a word w if D
is relevant for one or more senses c of w. More
formally let V = {w1, w2, ...w|V |} be the vocab-
ulary, let senses(w) = {s|s ? S, s is a sense of
w} (e.g. any synset in WORDNET containing the
word w). The domain relevance function for a word
R : D ? V ? [0, 1] is defined as follows:
Rword(Di, w) =
1
|senses(w)|
X
s?senses(w)
Rsyn(Di, s) (3)
3P (D) denotes the power set of D
3.2 The Gaussian Mixture Algorithm
As explained at the beginning of this section, the
simple local frequency count expressed by formula
(1) is not a good domain relevance measure.
In order to discriminate between noise and rel-
evant information, a supervised framework is typ-
ically used and significance levels for frequency
counts are estimated from labeled training data. Un-
fortunately this is not our case, since no domain
labeled text corpora are available. In this section
we propose a solution for that problem, namely the
Gaussian Mixture approach, that constitutes an un-
supervised way to estimate how to differentiate rel-
evant domain information in texts from noise. The
Gaussian Mixture approach consists of a parameter
estimation technique based on statistics of word dis-
tribution in a large-scale corpus.
The underlying assumption of the Gaussian Mix-
ture approach is that frequency scores for a cer-
tain domain are obtained from an underlying mix-
ture of relevant and non-relevant texts, and that the
scores for relevant texts are significantly higher than
scores obtained for the non-relevant ones. In the
corpus these scores are distributed according to two
distinct components. The domain frequency distri-
bution which corresponds to relevant texts has the
higher value expectation, while the one pertaining to
non relevant texts has the lower expectation. Figure
1 describes the probability density function (PDF )
for domain frequency scores of the SPORT domain
estimated on the BNC corpus4 (BNC-Consortium,
2000) using formula (1). The ?empirical? PDF ,
describing the distribution of frequency scores eval-
uated on the corpus, is represented by the continu-
ous line.
From the graph it is possible to see that the empir-
ical PDF can be decomposed into the sum of two
distributions, D = SPORT and D = ?non-SPORT?.
Most of the probability is concentrated on the left,
describing the distribution for the majority of non
relevant texts; the smaller distribution on the right
is assumed to be the distribution of frequency scores
for the minority of relevant texts.
Thus, the distribution on the left describes the
noise present in frequency estimation counts, which
is produced by the impact of polysemous words
and of occasional occurrences of terms belonging
to SPORT in non-relevant texts. The goal of the
technique is to estimate parameters describing the
distribution of the noise along texts, in order to as-
4The British National Corpus is a very large (over 100 mil-
lion words) corpus of modern English, both spoken and written.
050
100
150
200
0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04
Density
Non-relevant
Relevant
F(D, t)
de
ns
ity
 fu
nc
tio
n
Figure 1: Gaussian mixture for D = SPORT
sociate high relevance values only to relevant fre-
quency scores (i.e. frequency scores that are not re-
lated to noise). It is reasonable to assume that such
noise is normally distributed because it can be de-
scribed by a binomial distribution in which the prob-
ability of the positive event is very low and the num-
ber of events is very high. On the other hand, the
distribution on the right is the one describing typical
frequency values for relevant texts. This distribution
is also assumed to be normal.
A probabilistic interpretation permits the evalu-
ation of the relevance value R(D, t, j) of a certain
domain D for a new text t in a position j only by
considering the domain frequency F (D, t, j). The
relevance value is defined as the conditional prob-
ability P (D|F (D, t, j)). Using Bayes theorem we
estimate this probability by equation (4).
R(D, t, j) = P (D|F (D, t, j)) = (4)
= P (F (D, t, j)|D)P (D)
P (F (D, t, j)|D)P (D) + P (F (D, t, j)|D)P (D)
where P (F (D, t, j)|D) is the value of the PDF
describing D calculated in the point F (D, t, j),
P (F (D, t, j)|D) is the value of the PDF describ-
ing D, P (D) is the area of the distribution describ-
ing D and P (D) is the area of the distribution for
D.
In order to estimate the parameters describing the
PDF of D and D the Expectation Maximization
(EM) algorithm for the Gaussian Mixture Model
(Redner and Walker, 1984) is exploited. Assuming
to model the empirical distribution of domain fre-
quencies using a Gaussian mixture of two compo-
nents, the estimated parameters can be used to eval-
uate domain relevance by equation (4).
3.3 The EM Algorithm for the GM model
In this section some details about the algorithm for
parameter estimation are reported.
It is well known that a Gaussian mixture (GM)
allows to represent every smooth PDF as a linear
combination of normal distributions of the type in
formula 5
p(x|?) =
m
?
j=1
ajG(x, ?j , ?j) (5)
with
aj ? 0 and
m
?
j=1
aj = 1 (6)
and
G(x, ?, ?) = 1?
2pi?
e?
(x??)2
2?2 (7)
and ? = ?a1, ?1, ?1, . . . , am, ?m, ?m? is a pa-
rameter list describing the gaussian mixture. The
number of components required by the Gaussian
Mixture algorithm for domain relevance estimation
is m = 2.
Each component j is univocally determined by its
weight aj , its mean ?j and its variance ?j . Weights
represent also the areas of each component, i.e. its
total probability.
The Gaussian Mixture algorithm for domain rele-
vance estimation exploits a Gaussian Mixture to ap-
proximate the empirical PDF of domain frequency
scores. The goal of the Gaussian Mixture algorithm
is to find the GM that maximize the likelihood on
the empirical data, where the likelihood function is
evaluated by formula (8).
L(T , D, ?) =
?
t?T
p(F (D, t)|?) (8)
More formally, the EM algorithm for GM models
explores the space of parameters in order to find the
set of parameters ? such that the maximum likeli-
hood criterion (see formula 9) is satisfied.
?D = argmax
??
L(T , D, ??) (9)
This condition ensures that the obtained model
fits the original data as much as possible. Estima-
tion of parameters is the only information required
in order to evaluate domain relevance for texts us-
ing the Gaussian Mixture algorithm. The Expecta-
tion Maximization Algorithm for Gaussian Mixture
Models (Redner and Walker, 1984) allows to effi-
ciently perform this operation.
The strategy followed by the EM algorithm is
to start from a random set of parameters ?0, that
has a certain initial likelihood value L0, and then
iteratively change them in order to augment like-
lihood at each step. To this aim the EM algo-
rithm exploits a growth transformation of the like-
lihood function ?(?) = ?? such that L(T , D, ?) 6
L(T , D, ??). Applying iteratively this transforma-
tion starting from ?0 a sequence of parameters is
produced, until the likelihood function achieve a
stable value (i.e. Li+1 ? Li 6 ). In our settings
the transformation function ? is defined by the fol-
lowing set of equations, in which all the parameters
have to be solved together.
?(?) = ?(?a1, ?1, ?1, a2, ?2, ?2?) (10)
= ?a?1, ??1, ??1, a?2, ??2, ??2?
a?j =
1
|T |
|T |
?
k=1
ajG(F (D, tk), ?j , ?j)
p(F (D, tk), ?)
(11)
??j =
?|T |
k=1 F (D, tk) ?
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?)
?|T |
k=1
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?)
(12)
??j =
?|T |
k=1 (F (D, tk) ? ??j)2 ?
aiG(F (D,tk),?i,?i)
p(F (D,tk),?)
?|T |
k=1
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?) (13)
As said before, in order to estimate distribu-
tion parameters the British National Corpus (BNC-
Consortium, 2000) was used. Domain frequency
scores have been evaluated on the central position
of each text (using equation 1, with c = 50).
In conclusion, the EM algorithm was used to es-
timate parameters to describe distributions for rele-
vant and non-relevant texts. This learning method
is totally unsupervised. Estimated parameters has
been used to estimate relevance values by formula
(4).
4 Domain Driven Disambiguation
DRE originates to improve the performance of Do-
main Driven Disambiguation (DDD). In this sec-
tion, a brief overview of DDD is given. DDD is a
WSD methodology that only makes use of domain
information. Originally developed to test the role of
domain information for WSD, the system is capable
to achieve a good precision disambiguation. Its re-
sults are affected by a low recall, motivated by the
fact that domain information is sufficient to disam-
biguate only ?domain words?. The disambiguation
process is done comparing the domain of the con-
text and the domains of each sense of the lemma to
disambiguate. The selected sense is the one whose
domain is relevant for the context5 .
In order to represent domain information we in-
troduced the notion of Domain Vectors (DV), that
are data structures that collect domain information.
These vectors are defined in a multidimensional
space, in which each domain represents a dimen-
sion of the space. We distinguish between two kinds
of DVs: (i) synset vectors, which represent the rel-
evance of a synset with respect to each considered
domain and (ii) text vectors, which represent the rel-
evance of a portion of text with respect to each do-
main in the considered set.
More formally let D = {D1, D2, ..., Dd} be the
set of domains, the domain vector ~s for a synset s
is defined as ?R(D1, s), R(D2, s), . . . , R(Dd, s)?
where R(Di, s) is evaluated using equation
(2). In analogy the domain vector ~tj for
a text t in a given position j is defined as
?R(D1, t, j), R(D2, t, j), . . . , R(Dd, t, j)? where
R(Di, t, j) is evaluated using equation (4).
The DDD methodology is performed basically in
three steps:
1. Compute ~t for the context t of the word w to be disam-
biguated
2. Compute s? = argmaxs?Senses(w)score(s, w, t) where
score(s,w, t) = P (s|w) ? sim(~s,
~t)
P
s?Senses(w) P (s|w) ? sim(~s,~t)
3. if score(s?, w, t) > k (where k ? [0, 1] is a confidence
threshold) select sense s?, else do not provide any answer
The similarity metric used is the cosine vector
similarity, which takes into account only the direc-
tion of the vector (i.e. the information regarding the
domain).
P (s|w) describes the prior probability of sense
s for word w, and depends on the distribution of
the sense annotations in the corpus. It is esti-
mated by statistics from a sense tagged corpus (we
used SemCor)6 or considering the sense order in
5Recent works in WSD demonstrate that an automatic es-
timation of domain relevance for texts can be profitable used
to disambiguate words in their contexts. For example, (Escud-
ero et al, 2001) used domain relevance extraction techniques
to extract features for a supervised WSD algorithm presented
at the Senseval-2 competion, improving the system accuracy of
about 4 points for nouns, 1 point for verbs and 2 points for ad-
jectives, confirming the original intuition that domain informa-
tion is very useful to disambiguate ?domain words?, i.e. words
which are strongly related to the domain of the text.
6Admittedly, this may be regarded as a supervised compo-
nent of the generally unsupervised system. Yet, we considered
this component as legitimate within an unsupervised frame-
WORDNET, which roughly corresponds to sense
frequency order, when no example of the word
to disambiguate are contained in SemCor. In the
former case the estimation of P (s|w) is based on
smoothed statistics from the corpus (P (s|w) =
occ(s,w)+?
occ(w)+|senses(w)|?? , where ? is a smoothing fac-
tor empirically determined). In the latter case
P (s|w) can be estimated in an unsupervised way
considering the order of senses in WORDNET
(P (s|w) = 2(|senses(w)|?sensenumber(s,w)+1)|senses(w)|(|senses(w)|+1) where
sensenumber(s, w) returns the position of sense
s of word w in the sense list for w provided by
WORDNET.
5 Evaluation in a WSD task
We used the WSD framework to perform an evalu-
ation of the DRE technique by itself.
As explained in Section 1 Domain Relevance Es-
timation is not a common Text Categorization task.
In the standard framework of TC, categories are
learned form examples, that are used also for test.
In our case information in WORDNET DOMAINS is
used to discriminate, and a test set, i.e. a corpus of
texts categorized using the domain of WORDNET
DOMAINS, is not available. To evaluate the accu-
racy of the domain relevance estimation technique
described above is thus necessary to perform an in-
direct evaluation.
We evaluated the DDD algorithm described in
Section 4 using the dataset of the Senseval-2 all-
words task (Senseval-2, 2001; Preiss and Yarowsky,
2002). In order to estimate domain vectors for the
contexts of the words to disambiguate we used the
DRE methodology described in Section 3. Varying
the confidence threshold k, as described in Section
4, it is possible to change the tradeoff between preci-
sion and recall. The obtained precision-recall curve
of the system is reported in Figure 2.
In addition we evaluated separately the perfor-
mance on nouns and verbs, suspecting that nouns
are more ?domain oriented? than verbs. The effec-
tiveness of DDD to disambiguate domain words is
confirmed by results reported in Figure 3, in which
the precision recall curve is reported separately for
both nouns and verbs. The performances obtained
for nouns are sensibly higher than the one obtained
for verbs, confirming the claim that domain infor-
mation is crucial to disambiguate domain words.
In Figure 2 we also compare the results ob-
tained by the DDD system that make use of the
DRE technique described in Section 3 with the re-
work since it relies on a general resource (SemCor) that does
not correspond to the test data (Senseval all-words task).
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6
Pr
ec
isi
on
Recall
DDD new
DDD old
Figure 2: Performances of the system for all POS
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Pr
ec
isi
on
Recall
Nouns
Verbs
Figure 3: Performances of the system for Nouns and
Verbs
sults obtained by the DDD system presented at the
Senseval-2 competition described in (Magnini et al,
2002), that is based on the same DDD methodol-
ogy and exploit a DRE technique that consists ba-
sically on the simply domain frequency scores de-
scribed in subsection 3.1 (we refer to this system
using the expression old-DDD, in contrast to the ex-
pression new-DDD that refers to the implementation
described in this paper).
Old-DDD obtained 75% precision and 35% re-
call on the official evaluation at the Senseval-2 En-
glish all words task. At 35% of recall the new-DDD
achieves a precision of 79%, improving precision
by 4 points with respect to old-DDD. At 75% pre-
cision the recall of new-DDD is 40%. In both cases
the new domain relevance estimation technique im-
proves the performance of the DDD methodology,
demonstrating the usefulness of the DRE technique
proposed in this paper.
6 Conclusions and Future Works
Domain Relevance Estimation, an unsupervised TC
technique, has been proposed and evaluated in-
side the Domain Driven Disambiguation frame-
work, showing a significant improvement on the
overall system performances. This technique also
allows a clear probabilistic interpretation providing
an operative definition of the concept of domain rel-
evance. During the learning phase annotated re-
sources are not required, allowing a low cost imple-
mentation. The portability of the technique to other
languages is allowed by the usage of synset-aligned
wordnets, being domain annotation language inde-
pendent.
As far as the evaluation of DRE is concerned, for
the moment we have tested its usefulness in the con-
text of a WSD task, but we are going deeper, con-
sidering a pure TC framework.
Acknowledgements
We would like to thank Ido Dagan and Marcello
Federico for many useful discussions and sugges-
tions.
References
BNC-Consortium. 2000. British national corpus,
http://www.hcu.ox.ac.uk/BNC/.
J. P. Comaroni, J. Beall, W. E. Matthews, and G. R.
New, editors. 1989. Dewey Decimal Classica-
tion and Relative Index. Forest Press, Albany,
New York, 20th edition.
G. Escudero, L. Ma`rquez, and G. Rigau. 2001.
Using lazy boosting for word sense disambigua-
tion. In Proc. of SENSEVAL-2 Second Inter-
national Workshop on Evaluating Word Sense
Disambiguation System, pages 71?74, Toulose,
France, July.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. The MIT Press.
Y. Ko and J. Seo. 2000. Automatic text categoriza-
tion by unsupervised learning. In Proceedings of
COLING-00, the 18th International Conference
on Computational Linguistics, Saarbru?cken, Ger-
many.
B. Magnini and G. Cavaglia`. 2000. Integrating sub-
ject field codes into WordNet. In Proceedings
of LREC-2000, Second International Conference
on Language Resources and Evaluation, Athens,
Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H.T. Dang. 2001. English tasks: All-words
and verb lexical sample. In Proceedings of
SENSEVAL-2, Second International Workshop on
Evaluating Word Sense Disambiguation Systems,
Toulouse, France, July.
J. Preiss and D. Yarowsky, editors. 2002. Pro-
ceedings of SENSEVAL-2: Second International
Workshop on Evaluating Word Sense Disam-
biguation Systems, Toulouse, France.
R. Redner and H. Walker. 1984. Mixture densi-
ties, maximum likelihood and the EM algorithm.
SIAM Review, 26(2):195?239, April.
F. Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
Senseval-2. 2001. http://www.senseval.org.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 56?63, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Domain Kernels for Text Categorization
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
{gliozzo,strappa}@itc.it
Abstract
In this paper we propose and evaluate
a technique to perform semi-supervised
learning for Text Categorization. In
particular we defined a kernel function,
namely the Domain Kernel, that allowed
us to plug ?external knowledge? into the
supervised learning process. External
knowledge is acquired from unlabeled
data in a totally unsupervised way, and it
is represented by means of Domain Mod-
els.
We evaluated the Domain Kernel in two
standard benchmarks for Text Categoriza-
tion with good results, and we compared
its performance with a kernel function that
exploits a standard bag-of-words feature
representation. The learning curves show
that the Domain Kernel allows us to re-
duce drastically the amount of training
data required for learning.
1 Introduction
Text Categorization (TC) deals with the problem of
assigning a set of category labels to documents. Cat-
egories are usually defined according to a variety
of topics (e.g. SPORT vs. POLITICS) and a set of
hand tagged examples is provided for training. In the
state-of-the-art TC settings supervised classifiers are
used for learning and texts are represented by means
of bag-of-words.
Even if, in principle, supervised approaches reach
the best performance in many Natural Language
Processing (NLP) tasks, in practice it is not always
easy to apply them to concrete applicative settings.
In fact, supervised systems for TC require to be
trained a large amount of hand tagged texts. This
situation is usually feasible only when there is some-
one (e.g. a big company) that can easily provide al-
ready classified documents to train the system.
In most of the cases this scenario is quite unprac-
tical, if not infeasible. An example is the task of
categorizing personal documents, in which the cate-
gories can be modified according to the user?s inter-
ests: new categories are often introduced and, pos-
sibly, the available labeled training for them is very
limited.
In the NLP literature the problem of providing
large amounts of manually annotated data is known
as the Knowledge Acquisition Bottleneck. Cur-
rent research in supervised approaches to NLP often
deals with defining methodologies and algorithms to
reduce the amount of human effort required for col-
lecting labeled examples.
A promising direction to solve this problem is to
provide unlabeled data together with labeled texts
to help supervision. In the Machine Learning lit-
erature this learning schema has been called semi-
supervised learning. It has been applied to the
TC problem using different techniques: co-training
(Blum and Mitchell, 1998), EM-algorithm (Nigam
et al, 2000), Transduptive SVM (Joachims, 1999b)
and Latent Semantic Indexing (Zelikovitz and Hirsh,
2001).
In this paper we propose a novel technique to per-
form semi-supervised learning for TC. The under-
lying idea behind our approach is that lexical co-
56
herence (i.e. co-occurence in texts of semantically
related terms) (Magnini et al, 2002) is an inherent
property of corpora, and it can be exploited to help a
supervised classifier to build a better categorization
hypothesis, even if the amount of labeled training
data provided for learning is very low.
Our proposal consists of defining a Domain
Kernel and exploiting it inside a Support Vector
Machine (SVM) classification framework for TC
(Joachims, 2002). The Domain Kernel relies on the
notion of Domain Model, which is a shallow repre-
sentation for lexical ambiguity and variability. Do-
main Models can be acquired in an unsupervised
way from unlabeled data, and then exploited to de-
fine a Domain Kernel (i.e. a generalized similarity
function among documents)1 .
We evaluated the Domain Kernel in two stan-
dard benchmarks for TC (i.e. Reuters and 20News-
groups), and we compared its performance with a
kernel function that exploits a more standard Bag-
of-Words (BoW) feature representation. The use of
the Domain Kernel got a significant improvement in
the learning curves of both tasks. In particular, there
is a notable increment of the recall, especially with
few learning examples. In addition, F1 measure in-
creases by 2.8 points in the Reuters task at full learn-
ing, achieving the state-of-the-art results.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model and describes
an automatic acquisition technique based on Latent
Semantic Analysis (LSA). In Section 3 we illustrate
the SVM approach to TC, and we define a Domain
Kernel that exploits Domain Models to estimate sim-
ilarity among documents. In Section 4 the perfor-
mance of the Domain Kernel are compared with a
standard bag-of-words feature representation, show-
ing the improvements in the learning curves. Section
5 describes the previous attempts to exploit semi-
supervised learning for TC, while section 6 con-
cludes the paper and proposes some directions for
future research.
1The idea of exploiting a Domain Kernel to help a super-
vised classification framework, has been profitably used also in
other NLP tasks such as word sense disambiguation (see for ex-
ample (Strapparava et al, 2004)).
2 Domain Models
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let T = {t1, t2, . . . , tn} be a corpus, let
V = {w1, w2, . . . , wk} be its vocabulary, let T be
the k ? n term-by-document matrix representing T ,
such that ti,j is the frequency of word wi into the text
tj . The VSM is a k-dimensional space Rk, in which
the text tj ? T is represented by means of the vec-
tor ~tj such that the ith component of ~tj is ti,j. The
similarity among two texts in the VSM is estimated
by computing the cosine.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences ?he is affected by AIDS? and ?HIV is
a virus? do not have any words in common. In the
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences ?the laptop has
been infected by a virus? and ?HIV is a virus? would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM, in which texts and
terms are represented in a uniform way.
A Domain Model is composed by soft clusters of
terms. Each cluster represents a semantic domain
(Gliozzo et al, 2004), i.e. a set of terms that often
co-occur in texts having similar topics. A Domain
Model is represented by a k ? k? rectangular matrix
D, containing the degree of association among terms
and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Matrix
Domain Models can be used to describe lexical
ambiguity and variability. Lexical ambiguity is rep-
57
resented by associating one term to more than one
domain, while variability is represented by associat-
ing different terms to the same domain. For example
the term virus is associated to both the domain
COMPUTER SCIENCE and the domain MEDICINE
(ambiguity) while the domain MEDICINE is associ-
ated to both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2, ..., Dk?} be
a set of domains, such that k?  k. A Domain
Model is fully defined by a k ? k? domain matrix
D representing in each cell di,z the domain rele-
vance of term wi with respect to the domain Dz .
The domain matrix D is used to define a function
D : Rk ? Rk? , that maps the vectors ~tj , expressed
into the classical VSM, into the vectors ~t?j in the do-
main VSM. D is defined by2
D(~tj) = ~tj(IIDFD) = ~t?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wi), ~tj is represented as a row vector, and
IDF (wi) is the Inverse Document Frequency of wi.
Vectors in the domain VSM are called Domain
Vectors. Domain Vectors for texts are estimated by
exploiting formula 1, while the Domain Vector ~w?i,
corresponding to the word wi ? V , is the ith row of
the domain matrix D. To be a valid domain matrix
such vectors should be normalized (i.e. ? ~w?i, ~w?i? =
1).
In the Domain VSM the similarity among Domain
Vectors is estimated by taking into account second
order relations among terms. For example the simi-
larity of the two sentences ?He is affected by AIDS?
and ?HIV is a virus? is very high, because the terms
AIDS, HIV and virus are highly associated to the
domain MEDICINE.
In this work we propose the use of Latent Se-
mantic Analysis (LSA) (Deerwester et al, 1990) to
induce Domain Models from corpora. LSA is an
unsupervised technique for estimating the similar-
ity among texts and terms in a corpus. LSA is per-
formed by means of a Singular Value Decomposi-
tion (SVD) of the term-by-document matrix T de-
scribing the corpus. The SVD algorithm can be ex-
ploited to acquire a domain matrix D from a large
2In (Wong et al, 1985) a similar schema is adopted to define
a Generalized Vector Space Model, of which the Domain VSM
is a particular instance.
corpus T in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T ' V?k?UT where ?k? is the diagonal
k ? k matrix containing the highest k ?  k eigen-
values of T, and all the remaining elements set to
0. The parameter k? is the dimensionality of the Do-
main VSM and can be fixed in advance3 . Under this
setting we define the domain matrix DLSA4 as
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
?
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .
3 The Domain Kernel
Kernel Methods are the state-of-the-art supervised
framework for learning, and they have been success-
fully adopted to approach the TC task (Joachims,
1999a).
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear
algorithm for discovering nonlinear patterns. Kernel
methods allow us to build a modular system, as the
kernel function acts as an interface between the data
and the learning algorithm. Thus the kernel function
becomes the only domain specific module of the sys-
tem, while the learning algorithm is a general pur-
pose component. Potentially a kernel function can
work with any kernel-based algorithm, such as for
example SVM.
During the learning phase SVMs assign a weight
?i ? 0 to any example xi ? X . All the labeled
instances xi such that ?i > 0 are called support vec-
tors. The support vectors lie close to the best sepa-
rating hyper-plane between positive and negative ex-
amples. New examples are then assigned to the class
of its closest support vectors, according to equation
3.
3It is not clear how to choose the right dimensionality. In
our experiments we used 400 dimensions.
4When DLSA is substituted in Equation 1 the Domain VSM
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
58
f(x) =
n
?
i=1
?iK(xi, x) + ?0 (3)
The kernel function K returns the similarity be-
tween two instances in the input space X , and can
be designed in order to capture the relevant aspects
to estimate similarity, just by taking care of satis-
fying set of formal requirements, as described in
(Scho?lkopf and Smola, 2001).
In this paper we define the Domain Kernel and we
apply it to TC tasks. The Domain Kernel, denoted
by KD, can be exploited to estimate the topic simi-
larity among two texts while taking into account the
external knowledge provided by a Domain Model
(see section 2). It is a variation of the Latent Seman-
tic Kernel (Shawe-Taylor and Cristianini, 2004), in
which a Domain Model is exploited to define an ex-
plicit mapping D : Rk ? Rk? from the classical
VSM into the domain VSM. The Domain Kernel is
defined by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. To be fully defined, the Domain Kernel re-
quires a Domain Matrix D. In principle, D can be
acquired from any corpora by exploiting any (soft)
term clustering algorithm. Anyway, we belive that
adequate Domain Models for particular tasks can be
better acquired from collections of documents from
the same source. For this reason, for the experi-
ments reported in this paper, we acquired the matrix
DLSA, defined by equation 2, using the whole (un-
labeled) training corpora available for each task, so
tuning the Domain Model on the particular task in
which it will be applied.
A more traditional approach to measure topic sim-
ilarity among text consists of extracting BoW fea-
tures and to compare them in a vector space. The
BoW kernel, denoted by KBoW , is a particular case
of the Domain Kernel, in which D = I, and I is the
identity matrix. The BoW Kernel does not require
a Domain Model, so we can consider this setting
as ?purely? supervised, in which no external knowl-
edge source is provided.
4 Evaluation
We compared the performance of both KD and
KBoW on two standard TC benchmarks. In sub-
section 4.1 we describe the evaluation tasks and the
preprocessing steps, in 4.2 we describe some algo-
rithmic details of the TC system adopted. Finally
in subsection 4.3 we compare the learning curves of
KD and KBoW .
4.1 Text Categorization tasks
For the experiments reported in this paper, we se-
lected two evaluation benchmarks typically used in
the TC literature (Sebastiani, 2002): the 20news-
groups and the Reuters corpora. In both the data sets
we tagged the texts for part of speech and we consid-
ered only the noun, verb, adjective, and adverb parts
of speech, representing them by vectors containing
the frequencies of each disambiguated lemma. The
only feature selection step we performed was to re-
move all the closed-class words from the document
index.
20newsgroups. The 20Newsgroups data set5 is
a collection of approximately 20,000 newsgroup
documents, partitioned (nearly) evenly across 20
different newsgroups. This collection has become
a popular data set for experiments in text appli-
cations of machine learning techniques, such as
text classification and text clustering. Some of
the newsgroups are very closely related to each
other (e.g. comp.sys.ibm.pc.hardware
/ comp.sys.mac.hardware), while others
are highly unrelated (e.g. misc.forsale /
soc.religion.christian). We removed
cross-posts (duplicates), newsgroup-identifying
headers (i.e. Xref, Newsgroups, Path, Followup-To,
Date), and empty documents from the original
corpus, so to obtain 18,941 documents. Then we
randomly divided it into training (80%) and test
(20%) sets, containing respectively 15,153 and
3,788 documents.
Reuters. We used the Reuters-21578 collec-
tion6, and we splitted it into training and test
5Available at http://www.ai.mit.edu-
/people/jrennie/20Newsgroups/.
6Available at http://kdd.ics.uci.edu/databases/-
reuters21578/reuters21578.html.
59
partitions according to the standard ModApt

e
split. It includes 12,902 documents for 90 cat-
egories, with a fixed splitting between training
and test data. We conducted our experiments by
considering only the 10 most frequent categories,
i.e. Earn, Acquisition, Money-fx,
Grain, Crude, Trade, Interest,
Ship, Wheat and Corn, and we included in
our dataset al the non empty documents labeled
with at least one of those categories. Thus the final
dataset includes 9295 document, of which 6680 are
included in the training partition, and 2615 are in
the test set.
4.2 Implementation details
As a supervised learning device, we used the SVM
implementation described in (Joachims, 1999a).
The Domain Kernel is implemented by defining an
explicit feature mapping according to formula 1, and
by normalizing each vector to obtain vectors of uni-
tary length. All the experiments have been per-
formed on the standard parameter settings, using a
linear kernel.
We acquired a different Domain Model for each
corpus by performing the SVD processes on the
term-by-document matrices representing the whole
training partitions, and we considered only the first
400 domains (i.e. k? = 400)7.
As far as the Reuters task is concerned, the TC
problem has been approached as a set of binary fil-
tering problems, allowing the TC system to pro-
vide more than one category label to each document.
For the 20newsgroups task, we implemented a one-
versus-all classification schema, in order to assign a
single category to each news.
4.3 Domain Kernel versus BoW Kernel
Figure 1 and Figure 2 report the learning curves for
both KD and KBoW , evaluated respectively on the
Reuters and the 20newgroups task. Results clearly
show that KD always outperforms KBoW , espe-
cially when very limited amount of labeled data is
provided for learning.
7To perform the SVD operation we adopted
LIBSVDC, an optimized package for sparse ma-
trix that allows to perform this step in few minutes
even for large corpora. It can be downloaded from
http://tedlab.mit.edu/?dr/SVDLIBC/.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 1: Micro-F1 learning curves for Reuters
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 2: Micro-F1 learning curves for 20news-
groups
Table 2 compares the performances of the two
kernels at full learning. KD achieves a better micro-
F1 than KBoW in both tasks. The improvement is
particularly significant in the Reuters task (+ 2.8 %).
Tables 3 shows the number of labeled examples
required by KD and KBoW to achieve the same
micro-F1 in the Reuters task. KD requires only
146 examples to obtain a micro-F1 of 0.84, while
KBoW requires 1380 examples to achieve the same
performance. In the same task, KD surpass the per-
formance of KBoW at full learning using only the
10% of the labeled data. The last column of the ta-
ble shows clearly that KD requires 90% less labeled
data than KBoW to achieve the same performances.
A similar behavior is reported in Table 4 for the
60
F1 Domain Kernel Bow Kernel
Reuters 0.928 0.900
20newsgroups 0.886 0.880
Table 2: Micro-F1 with full learning
F1 Domain Kernel Bow Kernel Ratio
.54 14 267 5%
.84 146 1380 10%
.90 668 6680 10%
Table 3: Number of training examples needed by
KD and KBoW to reach the same micro-F1 on the
Reuters task
20newsgroups task. It is important to notice that the
number of labeled documents is higher in this corpus
than in the previous one. The benefits of using Do-
main Models are then less evident at full learning,
even if they are significant when very few labeled
data are provided.
Figures 3 and 4 report a more detailed analysis
by comparing the micro-precision and micro-recall
learning curves of both kernels in the Reuters task8.
It is clear from the graphs that the main contribute
of KD is about increasing recall, while precision is
similar in both cases9. This last result confirms our
hypothesis that the information provided by the Do-
main Models allows the system to generalize in a
more effective way over the training examples, al-
lowing to estimate the similarity among texts even if
they have just few words in common.
Finally, KD achieves the state-of-the-art in the
Reuters task, as reported in section 5.
5 Related Works
To our knowledge, the first attempt to apply the
semi-supervised learning schema to TC has been
reported in (Blum and Mitchell, 1998). Their co-
training algorithm was able to reduce significantly
the error rate, if compared to a strictly supervised
8For the 20-newsgroups task both micro-precision and
micro-recall are equal to micro-F1 because a single category
label has been assigned to every instance.
9It is worth noting that KD gets a F1 measure of 0.54 (Preci-
sion/Recall of 0.93/0.38) using just 14 training examples, sug-
gesting that it can be profitably exploited for a bootstrapping
process.
F1 Domain Kernel Bow Kernel Ratio
.50 30 500 6%
.70 98 1182 8%
.85 2272 7879 29%
Table 4: Number of training examples needed by
KD and KBoW to reach the same micro-F1 on the
20newsgroups task
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pr
ec
is
io
n
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 3: Learning curves for Reuters (Precision)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R
ec
al
l
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 4: Learning curves for Reuters (Recall)
classifier.
(Nigam et al, 2000) adopted an Expectation Max-
imization (EM) schema to deal with the same prob-
lem, evaluating extensively their approach on sev-
eral datasets. They compared their algorithm with
a standard probabilistic approach to TC, reporting
substantial improvements in the learning curve.
61
A similar evaluation is also reported in (Joachims,
1999b), where a transduptive SVM is compared
to a state-of-the-art TC classifier based on SVM.
The semi-supervised approach obtained better re-
sults than the standard with few learning data, while
at full learning results seem to converge.
(Bekkerman et al, 2002) adopted a SVM classi-
fier in which texts have been represented by their as-
sociations to a set of Distributional Word Clusters.
Even if this approach is very similar to ours, it is not
a semi-supervised learning schema, because authors
did not exploit any additional unlabeled data to in-
duce word clusters.
In (Zelikovitz and Hirsh, 2001) background
knowledge (i.e. the unlabeled data) is exploited to-
gether with labeled data to estimate document sim-
ilarity in a Latent Semantic Space (Deerwester et
al., 1990). Their approach differs from the one pro-
posed in this paper because a different categoriza-
tion algorithm has been adopted. Authors compared
their algorithm with an EM schema (Nigam et al,
2000) on the same dataset, reporting better results
only with very few labeled data, while EM performs
better with more training.
All the semi-supervised approaches in the liter-
ature reports better results than strictly supervised
ones with few learning, while with more data the
learning curves tend to converge.
A comparative evaluation among semi-supervised
TC algorithms is quite difficult, because the used
data sets, the preprocessing steps and the splitting
partitions adopted affect sensibly the final results.
Anyway, we reported the best F1 measure on the
Reuters corpus: to our knowledge, the state-of-the-
art on the 10 top most frequent categories of the
ModApte split at full learning is F1 92.0 (Bekker-
man et al, 2002) while we obtained 92.8. It is im-
portant to notice here that this results has been ob-
tained thanks to the improvements of the Domain
Kernel. In addition, on the 20newsgroups task, our
methods requires about 100 documents (i.e. five
documents per category) to achieve 70% F1, while
both EM (Nigam et al, 2000) and LSI (Zelikovitz
and Hirsh, 2001) requires more than 400 to achieve
the same performance.
6 Conclusion and Future Works
In this paper a novel technique to perform semi-
supervised learning for TC has been proposed and
evaluated. We defined a Domain Kernel that allows
us to improve the similarity estimation among docu-
ments by exploiting Domain Models. Domain Mod-
els are acquired from large collections of non anno-
tated texts in a totally unsupervised way.
An extensive evaluation on two standard bench-
marks shows that the Domain Kernel allows us to re-
duce drastically the amount of training data required
for learning. In particular the recall increases sen-
sibly, while preserving a very good accuracy. We
explained this phenomenon by showing that the sim-
ilarity scores evaluated by the Domain Kernel takes
into account both variability and ambiguity, being
able to estimate similarity even among texts that do
not have any word in common.
As future work, we plan to apply our semi-
supervised learning method to some concrete ap-
plicative scenarios, such as user modeling and cat-
egorization of personal documents in mail clients.
In addition, we are going deeper in the direction of
semi-supervised learning, by acquiring more com-
plex structures than clusters (e.g. synonymy, hyper-
onymy) to represent domain models. Furthermore,
we are working to adapt the general framework pro-
vided by the Domain Models to a multilingual sce-
nario, in order to apply the Domain Kernel to a Cross
Language TC task.
Acknowledgments
This work has been partially supported by the ON-
TOTEXT (From Text to Knowledge for the Se-
mantic Web) project, funded by the Autonomous
Province of Trento under the FUP-2004 research
program.
References
R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Win-
ter. 2002. Distributional word clusters vs. words for
text categorization. Journal of Machine Learning Re-
search, 1:1183?1208.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory, Morgan Kaufmann Publishers.
62
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
T. Joachims. 1999a. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. MIT Press,
Cambridge, MA, USA.
T. Joachims. 1999b. Transductive inference for text
classification using support vector machines. In Pro-
ceedings of ICML-99, 16th International Conference
on Machine Learning, pages 200?209. Morgan Kauf-
mann Publishers, San Francisco, US.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer Academic Publishers.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
K. Nigam, A. K. McCallum, S. Thrun, and T. M.
Mitchell. 2000. Text classification from labeled and
unlabeled documents using EM. Machine Learning,
39(2/3):103?134.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
B. Scho?lkopf and A. J. Smola. 2001. Learning with Ker-
nels. Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229?234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
S. Zelikovitz and H. Hirsh. 2001. Using LSI for text clas-
sification in the presence of background text. In Hen-
rique Paques, Ling Liu, and David Grossman, editors,
Proceedings of CIKM-01, 10th ACM International
Conference on Information and Knowledge Manage-
ment, pages 113?118, Atlanta, US. ACM Press, New
York, US.
63
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 9?16,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Cross language Text Categorization by acquiring
Multilingual Domain Models from Comparable Corpora
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
{gliozzo,strappa}@itc.it
Abstract
In a multilingual scenario, the classical
monolingual text categorization problem
can be reformulated as a cross language
TC task, in which we have to cope with
two or more languages (e.g. English and
Italian). In this setting, the system is
trained using labeled examples in a source
language (e.g. English), and it classifies
documents in a different target language
(e.g. Italian).
In this paper we propose a novel ap-
proach to solve the cross language text
categorization problem based on acquir-
ing Multilingual Domain Models from
comparable corpora in a totally unsuper-
vised way and without using any external
knowledge source (e.g. bilingual dictio-
naries). These Multilingual Domain Mod-
els are exploited to define a generalized
similarity function (i.e. a kernel function)
among documents in different languages,
which is used inside a Support Vector Ma-
chines classification framework. The re-
sults show that our approach is a feasi-
ble and cheap solution that largely outper-
forms a baseline.
1 Introduction
Text categorization (TC) is the task of assigning cat-
egory labels to documents. Categories are usually
defined according to a variety of topics (e.g. SPORT,
POLITICS, etc.) and, even if a large amount of
hand tagged texts is required, the state-of-the-art su-
pervised learning techniques represent a viable and
well-performing solution for monolingual catego-
rization problems.
On the other hand in the worldwide scenario of
the web age, multilinguality is a crucial issue to deal
with and to investigate, leading us to reformulate
most of the classical NLP problems. In particular,
monolingual Text Categorization can be reformu-
lated as a cross language TC task, in which we have
to cope with two or more languages (e.g. English
and Italian). In this setting, the system is trained
using labeled examples in a source language (e.g.
English), and it classifies documents in a different
target language (e.g. Italian).
In this paper we propose a novel approach to solve
the cross language text categorization problem based
on acquiring Multilingual Domain Models (MDM)
from comparable corpora in an unsupervised way.
A MDM is a set of clusters formed by terms in dif-
ferent languages. While in the monolingual settings
semantic domains are clusters of related terms that
co-occur in texts regarding similar topics (Gliozzo et
al., 2004), in the multilingual settings such clusters
are composed by terms in different languages ex-
pressing concepts in the same semantic field. Thus,
the basic relation modeled by a MDM is the domain
similarity among terms in different languages. Our
claim is that such a relation is sufficient to capture
relevant aspects of topic similarity that can be prof-
itably used for TC purposes.
The paper is organized as follows. After a brief
discussion about comparable corpora, we introduce
9
a multilingual Vector Space Model, in which docu-
ments in different languages can be represented and
then compared. In Section 4 we define the MDMs
and we present a totally unsupervised technique
to acquire them from comparable corpora. This
methodology does not require any external knowl-
edge source (e.g. bilingual dictionaries) and it is
based on Latent Semantic Analysis (LSA) (Deer-
wester et al, 1990). MDMs are then exploited to
define a Multilingual Domain Kernel, a generalized
similarity function among documents in different
languages that exploits a MDM (see Section 5). The
Multilingual Domain Kernel is used inside a Sup-
port Vector Machines (SVM) classification frame-
work for TC (Joachims, 2002). In Section 6 we will
evaluate our technique in a Cross Language catego-
rization task. The results show that our approach is
a feasible and cheap solution, largely outperforming
a baseline. Conclusions and future works are finally
reported in Section 7.
2 Comparable Corpora
Comparable corpora are collections of texts in dif-
ferent languages regarding similar topics (e.g. a col-
lection of news published by agencies in the same
period). More restrictive requirements are expected
for parallel corpora (i.e. corpora composed by texts
which are mutual translations), while the class of
the multilingual corpora (i.e. collection of texts ex-
pressed in different languages without any addi-
tional requirement) is the more general. Obviously
parallel corpora are also comparable, while compa-
rable corpora are also multilingual.
In a more precise way, let L = {L1, L2, . . . , Ll}
be a set of languages, let T i = {ti1, ti2, . . . , tin} be a
collection of texts expressed in the language Li ? L,
and let ?(tjh, tiz) be a function that returns 1 if tiz is
the translation of tjh and 0 otherwise. A multilingual
corpus is the collection of texts defined by T ? =
?
i T i. If the function ? exists for every text tiz ? T ?
and for every language Lj , and is known, then the
corpus is parallel and aligned at document level.
For the purpose of this paper it is enough to as-
sume that two corpora are comparable, i.e. they are
composed by documents about the same topics and
produced in the same period (e.g. possibly from dif-
ferent news agencies), and it is not known if a func-
tion ? exists, even if in principle it could exist and
return 1 for a strict subset of document pairs.
There exist many interesting works about us-
ing parallel corpora for multilingual applications
(Melamed, 2001), such as Machine Translation,
Cross language Information Retrieval (Littman et
al., 1998), lexical acquisition, and so on.
However it is not always easy to find or build par-
allel corpora. This is the main reason because the
weaker notion of comparable corpora is a matter re-
cent interest in the field of Computational Linguis-
tics (Gaussier et al, 2004).
The texts inside comparable corpora, being about
the same topics (i.e. about the same semantic do-
mains), should refer to the same concepts by using
various expressions in different languages. On the
other hand, most of the proper nouns, relevant enti-
ties and words that are not yet lexicalized in the lan-
guage, are expressed by using their original terms.
As a consequence the same entities will be denoted
with the same words in different languages, allow-
ing to automatically detect couples of translation
pairs just by looking at the word shape (Koehn and
Knight, 2002). Our hypothesis is that comparable
corpora contain a large amount of such words, just
because texts, referring to the same topics in differ-
ent languages, will often adopt the same terms to
denote the same entities1 .
However, the simple presence of these shared
words is not enough to get significant results in TC
tasks. As we will see, we need to exploit these com-
mon words to induce a second-order similarity for
the other words in the lexicons.
3 The Multilingual Vector Space Model
Let T = {t1, t2, . . . , tn} be a corpus, and V =
{w1, w2, . . . , wk} be its vocabulary. In the mono-
lingual settings, the Vector Space Model (VSM) is a
k-dimensional space Rk, in which the text tj ? T
is represented by means of the vector ~tj such that
the zth component of ~tj is the frequency of wz in tj .
The similarity among two texts in the VSM is then
estimated by computing the cosine of their vectors
in the VSM.
1According to our assumption, a possible additional crite-
rion to decide whether two corpora are comparable is to esti-
mate the percentage of terms in the intersection of their vocab-
ularies.
10
Unfortunately, such a model cannot be adopted in
the multilingual settings, because the VSMs of dif-
ferent languages are mainly disjoint, and the similar-
ity between two texts in different languages would
always turn out zero. This situation is represented
in Figure 1, in which both the left-bottom and the
rigth-upper regions of the matrix are totally filled by
zeros.
A first attempt to solve this problem is to ex-
ploit the information provided by external knowl-
edge sources, such as bilingual dictionaries, to col-
lapse all the rows representing translation pairs. In
this setting, the similarity among texts in different
languages could be estimated by exploiting the clas-
sical VSM just described. However, the main dis-
advantage of this approach to estimate inter-lingual
text similarity is that it strongly relies on the avail-
ability of a multilingual lexical resource containing
a list of translation pairs. For languages with scarce
resources a bilingual dictionary could be not eas-
ily available. Secondly, an important requirement
of such a resource is its coverage (i.e. the amount
of possible translation pairs that are actually con-
tained in it). Finally, another problem is that am-
biguos terms could be translated in different ways,
leading to collapse together rows describing terms
with very different meanings.
On the other hand, the assumption of corpora
comparability seen in Section 2, implies the pres-
ence of a number of common words, represented by
the central rows of the matrix in Figure 1.
As we will show in Section 6, this model is rather
poor because of its sparseness. In the next section,
we will show how to use such words as seeds to in-
duce a Multilingual Domain VSM, in which second
order relations among terms and documents in dif-
ferent languages are considered to improve the sim-
ilarity estimation.
4 Multilingual Domain Models
A MDM is a multilingual extension of the concept
of Domain Model. In the literature, Domain Mod-
els have been introduced to represent ambiguity and
variability (Gliozzo et al, 2004) and successfully
exploited in many NLP applications, such us Word
Sense Disambiguation (Strapparava et al, 2004),
Text Categorization and Term Categorization.
A Domain Model is composed by soft clusters of
terms. Each cluster represents a semantic domain,
i.e. a set of terms that often co-occur in texts hav-
ing similar topics. Such clusters identifies groups of
words belonging to the same semantic field, and thus
highly paradigmatically related. MDMs are Domain
Models containing terms in more than one language.
A MDM is represented by a matrix D, contain-
ing the degree of association among terms in all the
languages and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV e/i 1 0
AIDSe/i 1 0
viruse/i 0.5 0.5
hospitale 1 0
laptope 0 1
Microsofte/i 0 1
clinicai 1 0
Table 1: Example of Domain Matrix. we denotes
English terms, wi Italian terms and we/i the com-
mon terms to both languages.
MDMs can be used to describe lexical ambiguity,
variability and inter-lingual domain relations. Lexi-
cal ambiguity is represented by associating one term
to more than one domain, while variability is rep-
resented by associating different terms to the same
domain. For example the term virus is associated
to both the domain COMPUTER SCIENCE and the
domain MEDICINE while the domain MEDICINE is
associated to both the terms AIDS and HIV. Inter-
lingual domain relations are captured by placing dif-
ferent terms of different languages in the same se-
mantic field (as for example HIV e/i, AIDSe/i,
hospitale, and clinicai). Most of the named enti-
ties, such as Microsoft and HIV are expressed using
the same string in both languages.
When similarity among texts in different lan-
guages has to be estimated, the information con-
tained in the MDM is crucial. For example the two
sentences ?I went to the hospital to make an HIV
check? and ?Ieri ho fatto il test dell?AIDS in clin-
ica? (lit. yesterday I did the AIDS test in a clinic)
are very highly related, even if they share no to-
kens. Having an ?a priori? knowledge about the
inter-lingual domain similarity among AIDS, HIV,
hospital and clinica is then a useful information to
11
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
English documents Italian documents
de1 de2 ? ? ? den?1 den di1 di2 ? ? ? dim?1 dim
we1 0 1 ? ? ? 0 1 0 0 ? ? ?
English
Lexicon
we2 1 1 ? ? ? 1 0 0
. . .
... . . . . . . . . . . . . . . . . . . . . . . . . ... 0 ...
wep?1 0 1 ? ? ? 0 0
. . . 0
wep 0 1 ? ? ? 0 0 ? ? ? 0 0
common wi we/i1 0 1 ? ? ? 0 0 0 0 ? ? ? 1 0... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
wi1 0 0 ? ? ? 0 1 ? ? ? 1 1
Italian
Lexicon
wi2 0
. . . 1 1 ? ? ? 0 1
... ... 0 ... . . . . . . . . . . . . . . . . . . . . . . . . .
wiq?1
. . . 0 0 1 ? ? ? 0 1
wiq ? ? ? 0 0 0 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Multilingual term-by-document matrix
recognize inter-lingual topic similarity. Obviously
this relation is less restrictive than a stronger associ-
ation among translation pair. In this paper we will
show that such a representation is sufficient for TC
puposes, and easier to acquire.
In the rest of this section we will provide a formal
definition of the concept of MDM, and we define
some similarity metrics that exploit it.
Formally, let V i = {wi1, wi2, . . . , wiki} be the vo-
cabulary of the corpus T i composed by document
expressed in the language Li, let V ? = ?i V i be
the set of all the terms in all the languages, and
let k? = |V ?| be the cardinality of this set. Let
D = {D1, D2, ..., Dd} be a set of domains. A DM
is fully defined by a k? ? d domain matrix D rep-
resenting in each cell di,z the domain relevance of
the ith term of V ? with respect to the domain Dz .
The domain matrix D is used to define a function
D : Rk? ? Rd, that maps the document vectors ~tj
expressed into the multilingual classical VSM, into
the vectors ~t?j in the multilingual domain VSM. The
function D is defined by2
2In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
D(~tj) = ~tj(IIDFD) = ~t?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wli), ~tj is represented as a row vector, and
IDF (wli) is the Inverse Document Frequency of wli
evaluated in the corpus T l.
The matrix D can be determined for example us-
ing hand-made lexical resources, such as WORD-
NET DOMAINS (Magnini and Cavaglia`, 2000). In
the present work we followed the way to acquire
D automatically from corpora, exploiting the tech-
nique described below.
4.1 Automatic Acquisition of Multilingual
Domain Models
In this work we propose the use of Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990) to in-
duce a MDM from comparable corpora. LSA is an
unsupervised technique for estimating the similar-
ity among texts and terms in a large corpus. In the
monolingual settings LSA is performed by means
of a Singular Value Decomposition (SVD) of the
term-by-document matrix T describing the corpus.
SVD decomposes the term-by-document matrix T
into three matrixes T ' V?k?UT where ?k? is the
diagonal k?k matrix containing the highest k ?  k
12
eigenvalues of T, and all the remaining elements are
set to 0. The parameter k? is the dimensionality of
the Domain VSM and can be fixed in advance (i.e.
k? = d).
In the literature (Littman et al, 1998) LSA has
been used in multilingual settings to define a mul-
tilingual space in which texts in different languages
can be represented and compared. In that work LSA
strongly relied on the availability of aligned parallel
corpora: documents in all the languages are repre-
sented in a term-by-document matrix (see Figure 1)
and then the columns corresponding to sets of trans-
lated documents are collapsed (i.e. they are substi-
tuted by their sum) before starting the LSA process.
The effect of this step is to merge the subspaces (i.e.
the right and the left sectors of the matrix in Figure
1) in which the documents have been originally rep-
resented.
In this paper we propose a variation of this strat-
egy, performing a multilingual LSA in the case in
which an aligned parallel corpus is not available.
It exploits the presence of common words among
different languages in the term-by-document matrix.
The SVD process has the effect of creating a LSA
space in which documents in both languages are rep-
resented. Of course, the higher the number of com-
mon words, the more information will be provided
to the SVD algorithm to find common LSA dimen-
sion for the two languages. The resulting LSA di-
mensions can be perceived as multilingual clusters
of terms and document. LSA can then be used to
define a Multilingual Domain Matrix DLSA.
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
?
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .
Thus DLSA3 can be exploited to estimate simi-
larity among texts expressed in different languages
(see Section 5).
3When DLSA is substituted in Equation 1 the Domain VSM
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema, widely adopted in Information Retrieval.
4.2 Similarity in the multilingual domain space
As an example of the second-order similarity pro-
vided by this approach, we can see in Table 2 the five
most similar terms to the lemma bank. The similar-
ity among terms is calculated by cosine among the
rows in the matrix DLSA, acquired from the data
set used in our experiments (see Section 6.2). It is
worth noting that the Italian lemma banca (i.e. bank
in English) has a high similarity score to the English
lemma bank. While this is not enough to have a pre-
cise term translation, it is sufficient to capture rele-
vant aspects of topic similarity in a cross-language
text categorization task.
Lemma#Pos Similarity Score Language
banking#n 0.96 Eng
credit#n 0.90 Eng
amro#n 0.89 Eng
unicredito#n 0.85 Ita
banca#n 0.83 Ita
Table 2: Terms with high similarity to the English
lemma bank#n, in the Multilingual Domain Model
5 The Multilingual Domain Kernel
Kernel Methods are the state-of-the-art supervised
framework for learning, and they have been success-
fully adopted to approach the TC task (Joachims,
2002).
The basic idea behind kernel methods is to em-
bed the data into a suitable feature space F via a
mapping function ? : X ? F , and then to use a
linear algorithm for discovering nonlinear patterns.
Kernel methods allow us to build a modular system,
as the kernel function acts as an interface between
the data and the learning algorithm. Thus the ker-
nel function becomes the only domain specific mod-
ule of the system, while the learning algorithm is a
general purpose component. Potentially any kernel
function can work with any kernel-based algorithm,
as for example Support Vector Machines (SVMs).
During the learning phase SVMs assign a weight
?i ? 0 to any example xi ? X . All the labeled
instances xi such that ?i > 0 are called Support
Vectors. Support Vectors lie close to the best sepa-
rating hyper-plane between positive and negative ex-
amples. New examples are then assigned to the class
13
of the closest support vectors, according to equation
3.
f(x) =
n
?
i=1
?iK(xi, x) + ?0 (3)
The kernel function K(xi, x) returns the simi-
larity between two instances in the input space X ,
and can be designed just by taking care that some
formal requirements are satisfied, as described in
(Scho?lkopf and Smola, 2001).
In this section we define the Multilingual Domain
Kernel, and we apply it to a cross language TC task.
This kernel can be exploited to estimate the topic
similarity among two texts expressed in different
languages by taking into account the external knowl-
edge provided by a MDM. It defines an explicit map-
ping D : Rk ? Rk? from the Multilingual VSM
into the Multilingual Domain VSM. The Multilin-
gual Domain Kernel is specified by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. Thus the Multilingual Domain Kernel re-
quires Multilingual Domain Matrix D, in particular
DLSA that can be acquired from comparable cor-
pora, as explained in Section 4.1.
To evaluate the Multilingual Domain Kernel we
compared it to a baseline kernel function, namely the
bag of words kernel, that simply estimates the topic
similarity in the Multilingual VSM, as described in
Section 3. The BoW kernel is a particular case of
the Domain Kernel, in which D = I, and I is the
identity matrix.
6 Evaluation
In this section we present the data set (two compara-
ble English and Italian corpora) used in the evalua-
tion, and we show the results of the Cross Language
TC tasks. In particular we tried both to train the
system on the English data set and classify Italian
documents and to train using Italian and classify the
English test set. We compare the learning curves of
the Multilingual Domain Kernel with the standard
BoW kernel, which is considered as a baseline for
this task.
6.1 Implementation details
As a supervised learning device, we used the SVM
implementation described in (Joachims, 1999). The
Multilingual Domain Kernel is implemented by
defining an explicit feature mapping as explained
above, and by normalizing each vector. All the ex-
periments have been performed with the standard
SVM parameter settings.
We acquired a Multilingual Domain Model by
performing the Singular Value Decomposition pro-
cess on the term-by-document matrices representing
the merged training partitions (i.e. English and Ital-
ian), and we considered only the first 400 dimen-
sions4.
6.2 Data set description
We used a news corpus kindly put at our dis-
posal by ADNKRONOS, an important Italian news
provider. The corpus consists of 32,354 Ital-
ian and 27,821 English news partitioned by
ADNKRONOS in a number of four fixed cate-
gories: Quality of Life, Made in Italy,
Tourism, Culture and School. The corpus
is comparable, in the sense stated in Section 2, i.e.
they covered the same topics and the same period of
time. Some news are translated in the other language
(but no alignment indication is given), some others
are present only in the English set, and some others
only in the Italian. The average length of the news
is about 300 words. We randomly split both the En-
glish and Italian part into 75% training and 25% test
(see Table 3). In both the data sets we postagged the
texts and we considered only the noun, verb, adjec-
tive, and adverb parts of speech, representing them
by vectors containing the frequencies of each lemma
with its part of speech.
6.3 Monolingual Results
Before going to a cross-language TC task, we con-
ducted two tests of classical monolingual TC by
training and testing the system on Italian and En-
glish documents separately. For these tests we used
the SVM with the BoW kernel. Figures 2 and 3 re-
port the results.
4To perform the SVD operation we used LIBSVDC
http://tedlab.mit.edu/?dr/SVDLIBC/.
14
English Italian
Categories Training Test Total Training Test Total
Quality of Life 5759 1989 7748 5781 1901 7682
Made in Italy 5711 1864 7575 6111 2068 8179
Tourism 5731 1857 7588 6090 2015 8105
Culture and School 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
Table 3: Number of documents in the data set partitions
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
BoW Kernel
Figure 2: Learning curves for the English part of the
corpus
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
BoW Kernel
Figure 3: Learning curves for the Italian part of the
corpus
6.4 A Cross Language Text Categorization task
As far as the cross language TC task is concerned,
we tried the two possible options: we trained on the
English part and we classified the Italian part, and
we trained on the Italian and classified on the En-
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
Multilingual Domain Kernel
Bow Kernel
Figure 4: Cross-language (training on Italian, test on
English) learning curves
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
Multilingual Domain Kernel
Bow Kernel
Figure 5: Cross-language (training on English, test
on Italian) learning curves
glish part. The Multilingual Domain Model was ac-
quired running the SVD only on the joint (English
and Italian) training parts.
Table 4 reports the vocabulary dimensions of the
English and Italian training partitions, the vocabu-
15
# lemmata
English training 22,704
Italian training 26,404
English + Italian 43,384
common lemmata 5,724
Table 4: Number of lemmata in the training parts of
the corpus
lary of the merged training, and how many com-
mon lemmata are present (about 14% of the total).
Among the common lemmata, 97% are nouns and
most of them are proper nouns. Thus the initial term-
by-document matrix is a 43,384 ? 45,132 matrix,
while the DLSA matrix is 43,384 ? 400. For this
task we consider as a baseline the BoW kernel.
The results are reported in Figures 4 and 5. An-
alyzing the learning curves, it is worth noting that
when the quantity of training increases, the per-
formance becomes better and better for the Multi-
lingual Domain Kernel, suggesting that with more
available training it could be possible to go closer to
typical monolingual TC results.
7 Conclusion
In this paper we proposed a solution to cross lan-
guage Text Categorization based on acquiring Mul-
tilingual Domain Models from comparable corpora
in a totally unsupervised way and without using any
external knowledge source (e.g. bilingual dictionar-
ies). These Multilingual Domain Models are ex-
ploited to define a generalized similarity function
(i.e. a kernel function) among documents in differ-
ent languages, which is used inside a Support Vec-
tor Machines classification framework. The basis of
the similarity function exploits the presence of com-
mon words to induce a second-order similarity for
the other words in the lexicons. The results have
shown that this technique is sufficient to capture rel-
evant aspects of topic similarity in cross-language
TC tasks, obtaining substantial improvements over
a simple baseline. As future work we will investi-
gate the performance of this approach to more than
two languages TC task, and a possible generaliza-
tion of the assumption about equality of the common
words.
Acknowledgments
This work has been partially supported by the
ONTOTEXT project, funded by the Autonomous
Province of Trento under the FUP-2004 program.
References
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and
H. Dejean. 2004. A geometric view on bilingual lexi-
con extraction from comparable corpora. In Proceed-
ings of ACL-04, Barcelona, Spain, July.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. The MIT Press.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer Academic Publishers.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
ACL Workshop on Unsupervised Lexical Acquisition,
Philadelphia, July.
M. Littman, S. Dumais, and T. Landauer. 1998. Auto-
matic cross-language information retrieval using latent
semantic indexing. In G. Grefenstette, editor, Cross
Language Information Retrieval, pages 51?62. Kluwer
Academic Publishers.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, Athens, Greece, June.
D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
B. Scho?lkopf and A. J. Smola. 2001. Learning with Ker-
nels. Support Vector Machines, Regularization, Opti-
mization, and Beyond. The MIT Press.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation. In Proceedings of SENSEVAL-3,
Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
16
Syntagmatic Kernels:
a Word Sense Disambiguation Case Study
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Trento, ITALY
{giuliano,gliozzo,strappa}@itc.it
Abstract
In this paper we present a family of ker-
nel functions, named Syntagmatic Ker-
nels, which can be used to model syn-
tagmatic relations. Syntagmatic relations
hold among words that are typically collo-
cated in a sequential order, and thus they
can be acquired by analyzing word se-
quences. In particular, Syntagmatic Ker-
nels are defined by applying a Word Se-
quence Kernel to the local contexts of the
words to be analyzed. In addition, this
approach allows us to define a semi su-
pervised learning schema where external
lexical knowledge is plugged into the su-
pervised learning process. Lexical knowl-
edge is acquired from both unlabeled data
and hand-made lexical resources, such as
WordNet. We evaluated the syntagmatic
kernel on two standard Word Sense Dis-
ambiguation tasks (i.e. English and Ital-
ian lexical-sample tasks of Senseval-3),
where the syntagmatic information plays
a crucial role. We compared the Syntag-
matic Kernel with the standard approach,
showing promising improvements in per-
formance.
1 Introduction
In computational linguistics, it is usual to deal with
sequences: words are sequences of letters and syn-
tagmatic relations are established by sequences of
words. Sequences are analyzed to measure morpho-
logical similarity, to detect multiwords, to represent
syntagmatic relations, and so on. Hence modeling
syntagmatic relations is crucial for a wide variety
of NLP tasks, such as Named Entity Recognition
(Gliozzo et al, 2005a) and Word Sense Disambigua-
tion (WSD) (Strapparava et al, 2004).
In general, the strategy adopted to model syntag-
matic relations is to provide bigrams and trigrams of
collocated words as features to describe local con-
texts (Yarowsky, 1994), and each word is regarded
as a different instance to classify. For instance, oc-
currences of a given class of named entities (such
as names of persons) can be discriminated in texts
by recognizing word patterns in their local contexts.
For example the token Rossi, whenever is preceded
by the token Prof., often represents the name of a
person. Another task that can benefit from modeling
this kind of relations is WSD. To solve ambiguity it
is necessary to analyze syntagmatic relations in the
local context of the word to be disambiguated. In
this paper we propose a kernel function that can be
used to model such relations, the Syntagmatic Ker-
nel, and we apply it to two (English and Italian)
lexical-sample WSD tasks of the Senseval-3 com-
petition (Mihalcea and Edmonds, 2004).
In a lexical-sample WSD task, training data are
provided as a set of texts, in which for each text
a given target word is manually annotated with a
sense from a predetermined set of possibilities. To
model syntagmatic relations, the typical supervised
learning framework adopts as features bigrams and
trigrams in a local context. The main drawback of
this approach is that non contiguous or shifted col-
57
locations cannot be identified, decreasing the gener-
alization power of the learning algorithm. For ex-
ample, suppose that the verb to score has to be dis-
ambiguated into the sentence ?Ronaldo scored the
goal?, and that the sense tagged example ?the foot-
ball player scores#1 the first goal? is provided for
training. A traditional feature mapping would ex-
tract the bigram w+1 w+2:the goal to represent the
former, and the bigram w+1 w+2:the first to index
the latter. Evidently such features will not match,
leading the algorithm to a misclassification.
In the present paper we propose the Syntagmatic
Kernel as an attempt to solve this problem. The
Syntagmatic Kernel is based on a Gap-Weighted
Subsequences Kernel (Shawe-Taylor and Cristian-
ini, 2004). In the spirit of Kernel Methods, this
kernel is able to compare sequences directly in the
input space, avoiding any explicit feature mapping.
To perform this operation, it counts how many times
a (non-contiguous) subsequence of symbols u of
length n occurs in the input string s, and penalizes
non-contiguous occurrences according to the num-
ber of the contained gaps. To define our Syntag-
matic Kernel, we adapted the generic definition of
the Sequence Kernels to the problem of recognizing
collocations in local word contexts.
In the above definition of Syntagmatic Kernel,
only exact word-matches contribute to the similar-
ity. One shortcoming of this approach is that (near-
)synonyms will never be considered similar, lead-
ing to a very low generalization power of the learn-
ing algorithm, that requires a huge amount of data
to converge to an accurate prediction. To solve this
problem we provided external lexical knowledge to
the supervised learning algorithm, in order to define
a ?soft-matching? schema for the kernel function.
For example, if we consider as equivalent the terms
Ronaldo and football player, the proposition ?The
football player scored the first goal? is equivalent to
the sentence ?Ronaldo scored the first goal?, pro-
viding a strong evidence to disambiguate the latter
occurrence of the verb.
We propose two alternative soft-matching criteria
exploiting two different knowledge sources: (i) hand
made resources and (ii) unsupervised term similar-
ity measures. The first approach performs a soft-
matching among all those synonyms words in Word-
Net, while the second exploits domain relations, ac-
quired from unlabeled data, for the same purpose.
Our experiments, performed on two standard
WSD benchmarks, show the superiority of the Syn-
tagmatic Kernel with respect to a classical flat vector
representation of bigrams and trigrams.
The paper is structured as follows. Section 2 in-
troduces the Sequence Kernels. In Section 3 the
Syntagmatic Kernel is defined. Section 4 explains
how soft-matching can be exploited by the Collo-
cation Kernel, describing two alternative criteria:
WordNet Synonymy and Domain Proximity. Sec-
tion 5 gives a brief sketch of the complete WSD
system, composed by the combination of different
kernels, dealing with syntagmatic and paradigmatic
aspects. Section 6 evaluates the Syntagmatic Kernel,
and finally Section 7 concludes the paper.
2 Sequence Kernels
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping ?, we can use a kernel
function K : X ? X ? R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Sequence Kernels (or String Kernels) are a fam-
ily of kernel functions developed to compute the
inner product among images of strings in high-
dimensional feature space using dynamic program-
ming techniques (Shawe-Taylor and Cristianini,
2004). The Gap-Weighted Subsequences Kernel is
the most general Sequence Kernel. Roughly speak-
ing, it compares two strings by means of the num-
ber of contiguous and non-contiguous substrings of
a given length they have in common. Non contigu-
ous occurrences are penalized according to the num-
ber of gaps they contain.
58
Formally, let ? be an alphabet of |?| symbols,
and s = s1s2 . . . s|s| a finite sequence over ? (i.e.
si ? ?, 1 6 i 6 |s|). Let i = [i1, i2, . . . , in], with
1 6 i1 < i2 < . . . < in 6 |s|, be a subset of the
indices in s: we will denote as s[i] ? ?n the sub-
sequence si1si2 . . . sin . Note that s[i] does not nec-
essarily form a contiguous subsequence of s. For
example, if s is the sequence ?Ronaldo scored the
goal? and i = [2, 4], then s[i] is ?scored goal?. The
length spanned by s[i] in s is l(i) = in ? i1 + 1.
The feature space associated with the Gap-Weighted
Subsequences Kernel of length n is indexed by I =
?n, with the embedding given by
?nu(s) =
X
i:u=s[i]
?l(i), u ? ?n, (1)
where ? ?]0, 1] is the decay factor used to penalize
non-contiguous subsequences1 . The associate ker-
nel is defined as
Kn(s, t) = ??n(s), ?n(t)? =
X
u??n
?nu(s)?nu(t). (2)
An explicit computation of Equation 2 is unfea-
sible even for small values of n. To evaluate more
efficiently Kn, we use the recursive formulation pro-
posed in (Lodhi et al, 2002; Saunders et al, 2002;
Cancedda et al, 2003) based on a dynamic program-
ming implementation. It is reported in the following
equations:
K?0(s, t) = 1, ?s, t, (3)
K?i(s, t) = 0, if min(|s|, |t|) < i, (4)
K??i (s, t) = 0, if min(|s|, |t|) < i, (5)
K??i (sx, ty) =
(
?K??i (sx, t), if x 6= y;
?K??i (sx, t) + ?2K?i?1(s, t), otherwise.
(6)
K?i(sx, t) = ?K?i(s, t) + K??i (sx, t), (7)
Kn(s, t) = 0, if min(|s|, |t|) < n, (8)
Kn(sx, t) = Kn(s, t) +
X
j:tj=x
?2K?n?1(s, t[1 : j ? 1]),
(9)
K ?n and K ??n are auxiliary functions with a sim-
ilar definition as Kn used to facilitate the compu-
tation. Based on all definitions above, Kn can be
1Notice that by choosing ? = 1 sparse subsequences are
not penalized. On the other hand, the kernel does not take into
account sparse subsequences with ? ? 0.
computed in O(n|s||t|). Using the above recursive
definition, it turns out that computing all kernel val-
ues for subsequences of lengths up to n is not signif-
icantly more costly than computing the kernel for n
only.
In the rest of the paper we will use the normalised
version of the kernel (Equation 10) to keep the val-
ues comparable for different values of n and to be
independent from the length of the sequences.
K?(s, t) = K(s, t)p
K(s, s)K(t, t)
. (10)
3 The Syntagmatic Kernel
As stated in Section 1, syntagmatic relations hold
among words arranged in a particular temporal or-
der, hence they can be modeled by Sequence Ker-
nels. The Syntagmatic Kernel is defined as a linear
combination of Gap-Weighted Subsequences Ker-
nels that operate at word and PoS tag level. In partic-
ular, following the approach proposed by Cancedda
et al (2003), it is possible to adapt sequence kernels
to operate at word level by instancing the alphabet ?
with the vocabulary V = {w1, w2, . . . , wk}. More-
over, we restricted the generic definition of the Gap-
Weighted Subsequences Kernel to recognize collo-
cations in the local context of a specified word. The
resulting kernel, called n-gram Collocation Kernel
(KnColl), operates on sequences of lemmata around a
specified word l0 (i.e. l?3, l?2, l?1, l0, l+1, l+2, l+3).
This formulation allows us to estimate the number of
common (sparse) subsequences of lemmata (i.e. col-
locations) between two examples, in order to capture
syntagmatic similarity.
Analogously, we defined the PoS Kernel (KnPoS)
to operate on sequences of PoS tags p?3, p?2, p?1,
p0, p+1, p+2, p+3, where p0 is the PoS tag of l0.
The Collocation Kernel and the PoS Kernel are
defined by Equations 11 and 12, respectively.
KColl(s, t) =
n
?
l=1
K lColl(s, t) (11)
and
KPoS(s, t) =
n
?
l=1
K lP oS(s, t). (12)
Both kernels depend on the parameter n, the length
of the non-contiguous subsequences, and ?, the de-
59
cay factor. For example, K2Coll allows us to repre-
sent all (sparse) bi-grams in the local context of a
word.
Finally, the Syntagmatic Kernel is defined as
KSynt(s, t) = KColl(s, t) + KPoS(s, t). (13)
We will show that in WSD, the Syntagmatic Ker-
nel is more effective than standard bigrams and tri-
grams of lemmata and PoS tags typically used as
features.
4 Soft-Matching Criteria
In the definition of the Syntagmatic Kernel only ex-
act word matches contribute to the similarity. To
overcome this problem, we further extended the def-
inition of the Gap-Weigthed Subsequences Kernel
given in Section 2 to allow soft-matching between
words. In order to develop soft-matching criteria,
we follow the idea that two words can be substi-
tuted preserving the meaning of the whole sentence
if they are paradigmatically related (e.g. synomyns,
hyponyms or domain related words). If the meaning
of the proposition as a whole is preserved, the mean-
ing of the lexical constituents of the sentence will
necessarily remain unchanged too, providing a vi-
able criterion to define a soft-matching schema. This
can be implemented by ?plugging? external paradig-
matic information into the Collocation kernel.
Following the approach proposed by (Shawe-
Taylor and Cristianini, 2004), the soft-matching
Gap-Weighted Subsequences Kernel is now calcu-
lated recursively using Equations 3 to 5, 7 and 8,
replacing Equation 6 by the equation:
K??i (sx, ty) = ?K??i (sx, t) + ?2axyK?i?1(s, t),?x, y, (14)
and modifying Equation 9 to:
Kn(sx, t) = Kn(s, t) +
|t|
X
j
?2axtjK
?
n?1(s, t[1 : j ? 1]).
(15)
where axy are entries in a similarity matrix A be-
tween symbols (words). In order to ensure that the
resulting kernel is valid, A must be positive semi-
definite.
In the following subsections, we describe two al-
ternative soft-matching criteria based on WordNet
Synonymy and Domain Proximity. In both cases, to
show that the similarity matrices are a positive semi-
definite we use the following result:
Proposition 1 A matrix A is positive semi-definite
if and only if A = BTB for some real matrix B.
The proof is given in (Shawe-Taylor and Cristianini,
2004).
4.1 WordNet Synonymy
The first solution we have experimented exploits a
lexical resource representing paradigmatic relations
among terms, i.e. WordNet. In particular, we used
WordNet-1.7.1 for English and the Italian part of
MultiWordNet2.
In order to find a similarity matrix between terms,
we defined a vector space where terms are repre-
sented by the WordNet synsets in which such terms
appear. Hence, we can view a term as vector in
which each dimension is associated with one synset.
The term-by-synset matrix S is then the matrix
whose rows are indexed by the synsets. The en-
try xij of S is 1 if the synset sj contains the term
wi, and 0 otherwise. The term-by-synset matrix S
gives rise to the similarity matrix A = SST be-
tween terms. Since A can be rewritten as A =
(ST )TST = BTB, it follows directly by Proposi-
tion 1 that it is positive semi-definite.
It is straightforward to extend the soft-matching
criterion to include hyponym relation, but we
achieved worse results. In the evaluation section we
will not report such results.
4.2 Domain Proximity
The approach described above requires a large scale
lexical resource. Unfortunately, for many languages,
such a resource is not available. Another possibility
for implementing soft-matching is introducing the
notion of Semantic Domains.
Semantic Domains are groups of strongly
paradigmatically related words, and can be acquired
automatically from corpora in a totally unsuper-
vised way (Gliozzo, 2005). Our proposal is to ex-
ploit a Domain Proximity relation to define a soft-
matching criterion on the basis of an unsupervised
similarity metric defined in a Domain Space. The
Domain Space can be determined once a Domain
2http://multiwordnet.itc.it
60
Model (DM) is available. This solution is evidently
cheaper, because large collections of unlabeled texts
can be easily found for every language.
A DM is represented by a k ? k? rectangular ma-
trix D, containing the domain relevance for each
term with respect to each domain, as illustrated in
Table 1. DMs can be acquired from texts by exploit-
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Model.
ing a lexical coherence assumption (Gliozzo, 2005).
To this aim, Term Clustering algorithms can be used:
a different domain is defined for each cluster, and
the degree of association between terms and clusters,
estimated by the unsupervised learning algorithm,
provides a domain relevance function. As a clus-
tering technique we exploit Latent Semantic Analy-
sis (LSA), following the methodology described in
(Gliozzo et al, 2005b). This operation is done off-
line, and can be efficiently performed on large cor-
pora.
LSA is performed by means of SVD of the term-
by-document matrixT representing the corpus. The
SVD algorithm can be exploited to acquire a domain
matrix D from a large corpus in a totally unsuper-
vised way. SVD decomposes the term-by-document
matrix T into three matrices T = V?kUT where
?k is the diagonal k ? k matrix containing the k
singular values of T. D = V?k? where k?  k.
Once a DM has been defined by the matrixD, the
Domain Space is a k? dimensional space, in which
both texts and terms are represented by means of
Domain Vectors (DVs), i.e. vectors representing the
domain relevances among the linguistic object and
each domain. The DV ~w?i for the term wi ? V is the
ith row of D, where V = {w1, w2, . . . , wk} is the
vocabulary of the corpus.
The term-by-domain matrix D gives rise to the
term-by-term similarity matrix A = DDT among
terms. It follows from Proposition 1 that A is posi-
tive semi-definite.
5 Kernel Combination for WSD
To improve the performance of a WSD system, it
is possible to combine different kernels. Indeed,
we followed this approach in the participation to
Senseval-3 competition, reaching the state-of-the-
art in many lexical-sample tasks (Strapparava et al,
2004). While this paper is focused on Syntagmatic
Kernels, in this section we would like to spend some
words on another important component for a com-
plete WSD system: the Domain Kernel, used to
model domain relations.
Syntagmatic information alone is not sufficient to
define a full kernel for WSD. In fact, in (Magnini
et al, 2002), it has been claimed that knowing the
domain of the text in which the word is located is a
crucial information for WSD. For example the (do-
main) polysemy among the COMPUTER SCIENCE
and the MEDICINE senses of the word virus can
be solved by simply considering the domain of the
context in which it is located.
This fundamental aspect of lexical polysemy can
be modeled by defining a kernel function to esti-
mate the domain similarity among the contexts of
the words to be disambiguated, namely the Domain
Kernel. The Domain Kernel measures the similarity
among the topics (domains) of two texts, so to cap-
ture domain aspects of sense distinction. It is a vari-
ation of the Latent Semantic Kernel (Shawe-Taylor
and Cristianini, 2004), in which a DM is exploited
to define an explicit mapping D : Rk ? Rk? from
the Vector Space Model (Salton and McGill, 1983)
into the Domain Space (see Section 4), defined by
the following mapping:
D(~tj) = ~tj(IIDFD) = ~t?j (16)
where IIDF is a k ? k diagonal matrix such that
iIDFi,i = IDF (wi), ~tj is represented as a row vector,
and IDF (wi) is the Inverse Document Frequency of
wi. The Domain Kernel is then defined by:
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(17)
The final system for WSD results from a com-
bination of kernels that deal with syntagmatic and
paradigmatic aspects (i.e. PoS, collocations, bag of
words, domains), according to the following kernel
61
combination schema:
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj , xj)Kl(xi, xi)
(18)
6 Evaluation
In this section we evaluate the Syntagmatic Kernel,
showing that it improves over the standard feature
extraction technique based on bigrams and trigrams
of words and PoS tags.
6.1 Experimental settings
We conducted the experiments on two lexical sam-
ple tasks (English and Italian) of the Senseval-3
competition (Mihalcea and Edmonds, 2004). In
lexical-sample WSD, after selecting some target
words, training data is provided as a set of texts.
For each text a given target word is manually anno-
tated with a sense from a predetermined set of pos-
sibilities. Table 2 describes the tasks by reporting
the number of words to be disambiguated, the mean
polysemy, and the dimension of training, test and
unlabeled corpora. Note that the organizers of the
English task did not provide any unlabeled material.
So for English we used a domain model built from
the training partition of the task (obviously skipping
the sense annotation), while for Italian we acquired
the DM from the unlabeled corpus made available
by the organizers.
#w pol # train # test # unlab
English 57 6.47 7860 3944 7860
Italian 45 6.30 5145 2439 74788
Table 2: Dataset descriptions.
6.2 Performance of the Syntagmatic Kernel
Table 3 shows the performance of the Syntagmatic
Kernel on both data sets. As baseline, we report
the result of a standard approach consisting on ex-
plicit bigrams and trigrams of words and PoS tags
around the words to be disambiguated (Yarowsky,
1994). The results show that the Syntagmatic Ker-
nel outperforms the baseline in any configuration
(hard/soft-matching). The soft-matching criteria
further improve the classification performance. It
is interesting to note that the Domain Proximity
methodology obtained better results than WordNet
Standard approach
English Italian
Bigrams and trigrams 67.3 51.0
Syntagmatic Kernel
Hard matching 67.7 51.9
Soft matching (WordNet) 67.3 51.3
Soft matching (Domain proximity) 68.5 54.0
Table 3: Performance (F1) of the Syntagmatic Ker-
nel.
Synonymy. The different results observed between
Italian and English using the Domain Proximity
soft-matching criterion are probably due to the small
size of the unlabeled English corpus.
In these experiments, the parameters n and ? are
optimized by cross-validation. For KnColl, we ob-
tained the best results with n = 2 and ? = 0.5. For
KnPoS , n = 3 and ? ? 0. The domain cardinality k?
was set to 50.
Finally, the global performance (F1) of the full
WSD system (see Section 5) on English and Italian
lexical sample tasks is 73.3 for English and 61.3 for
Italian. To our knowledge, these figures represent
the current state-of-the-art on these tasks.
7 Conclusion and Future Work
In this paper we presented the Syntagmatic Kernels,
i.e. a set of kernel functions that can be used to
model syntagmatic relations for a wide variety of
Natural Language Processing tasks. In addition, we
proposed two soft-matching criteria for the sequence
analysis, which can be easily modeled by relax-
ing the constraints in a Gap-Weighted Subsequences
Kernel applied to local contexts of the word to be
analyzed. Experiments, performed on two lexical
sample Word Sense Disambiguation benchmarks,
show that our approach further improves the stan-
dard techniques usually adopted to deal with syntag-
matic relations. In addition, the Domain Proximity
soft-matching criterion allows us to define a semi-
supervised learning schema, improving the overall
results.
For the future, we plan to exploit the Syntagmatic
Kernel for a wide variety of Natural Language Pro-
cessing tasks, such as Entity Recognition and Re-
lation Extraction. In addition we are applying the
soft matching criteria here defined to Tree Kernels,
62
in order to take into account lexical variability in
parse trees. Finally, we are going to further improve
the soft-matching criteria here proposed by explor-
ing the use of entailment criteria for substitutability.
Acknowledgments
The authors were partially supported by the Onto-
Text Project, funded by the Autonomous Province
of Trento under the FUP-2004 research program.
References
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Renders.
2003. Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059?1082.
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
A. Gliozzo, C. Giuliano, and R. Rinaldi. 2005a. Instance
filtering for entity recognition. ACM SIGKDD Explo-
rations, special Issue on Natural Language Processing
and Text Mining, 7(1):11?18, June.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005b. Do-
main kernels for word sense disambiguation. In Pro-
ceedings of the 43rd annual meeting of the Association
for Computational Linguistics (ACL-05), pages 403?
410, Ann Arbor, Michigan, June.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
H. Lodhi, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Learning Research,
2(3):419?444.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Analy-
sis of Text, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other string kernel extensions. In Pro-
ceedings of 19th International Conference onMachine
Learning (ICML02).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, C. Giuliano, and A. Gliozzo. 2004. Pat-
tern abstraction and term similarity for word sense dis-
ambiguation: IRST at Senseval-3. In Proceedings of
SENSEVAL-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, Barcelona, Spain, July.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in span-
ish and french. In Proceedings of the 32nd Annual
Meeting of the ACL, pages 88?95, Las Cruces, New
Mexico.
63
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 145?148,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-irst: Lexical Substitution Task Exploiting
Domain and Syntagmatic Coherence
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
FBK-irst, I-38050, Povo, Trento, ITALY
{giuliano, gliozzo, strappa}@itc.it
Abstract
This paper summarizes FBK-irst participa-
tion at the lexical substitution task of the
SEMEVAL competition. We submitted two
different systems, both exploiting synonym
lists extracted from dictionaries. For each
word to be substituted, the systems rank the
associated synonym list according to a simi-
larity metric based on Latent Semantic Anal-
ysis and to the occurrences in the Web 1T
5-gram corpus, respectively. In particular,
the latter system achieves the state-of-the-art
performance, largely surpassing the baseline
proposed by the organizers.
1 Introduction
The lexical substitution (Glickman et al, 2006a) can
be regarded as a subtask of the lexical entailment,
in which for a given word in context the system is
asked to select an alternative word that can be re-
placed in that context preserving the meaning. Lex-
ical Entailment, and in particular lexical reference
(Glickman et al, 2006b)1 , is in turn a subtask of tex-
tual entailment, which is formally defined as a rela-
tionship between a coherent text T and a language
expression, the hypothesis H . T is said to entail H ,
denoted by T ? H , if the meaning of H can be in-
ferred from the meaning of T (Dagan et al, 2005;
Dagan and Glickman., 2004). Even though this no-
tion has been only recently proposed in the computa-
tional linguistics literature, it attracts more and more
attention due to the high generality of its settings and
to the usefulness of its (potential) applications.
1In the literature, slight variations of this problem have been
also referred to as sense matching (Dagan et al, 2006).
With respect to lexical entailment, the lexical sub-
stitution task has a more restrictive criterion. In
fact, two words can be substituted when meaning is
preserved, while the criterion for lexical entailment
is that the meaning of the thesis is implied by the
meaning of the hypothesis. The latter condition is in
general ensured by substituting either hyperonyms
or synonyms, while the former is more rigid because
only synonyms are in principle accepted.
Formally, in a lexical entailment task a system is
asked to decide whether the substitution of a par-
ticular term w with the term e in a coherent text
Hw = H lwHr generates a sentence He = H leHr
such that Hw ? He, where H l and Hr denote the
left and the right context of w, respectively. For
example, given the source word ?weapon? a system
may substitute it with the target synonym ?arm?, in
order to identify relevant texts that denote the sought
concept using the latter term.
A particular case of lexical entailment is recog-
nizing synonymy, where both Hw ? He and He ?
Hw hold. The lexical substitution task at SEMEVAL
addresses exactly this problem. The task is not easy
since lists of candidate entailed words are not pro-
vided by the organizers. Therefore the system is
asked first to identify a set of candidate words, and
then to select only those words that fit in a particu-
lar context. To promote unsupervised methods, the
organizers did not provide neither labeled data for
training nor dictionaries or list of synonyms explain-
ing the meanings of the entailing words.
In this paper, we describe our approach to the
Lexical Substitution task at SEMEVAL 2007. We
developed two different systems (named IRST1-lsa
and IRST2-syn in the official task ranking), both ex-
ploiting a common lists of synonyms extracted from
dictionaries (i.e. WordNet and the Oxford Dictio-
145
nary) and ranking them according to two different
criteria:
Domain Proximity: the similarity between each
candidate entailed word and the context of the
entailing word is estimated by means of a co-
sine between their corresponding vectors in the
LSA space.
Syntagmatic Coherence: querying a large corpus,
the system finds all occurrences of the target
sentence, in which the entailing word is substi-
tuted with each synonym, and it assigns scores
proportional to the occurrence frequencies.
Results show that both methods are effective. In
particular, the second method achieved the best per-
formance in the competition, defining the state-of-
the-art for the lexical substitution task.
2 Lexical Substitution Systems
The lexical substitution task is a textual entailment
subtask in which the system is asked to provide one
or more terms e ? E ? syn(w) that can be sub-
stituted to w in a particular context Hw = H lwHr
generating a sentence He = H leHr such that both
Hw ? He and He ? Hw hold, where syn(w) is the
set of synonyms lemmata obtained from all synset in
which w appears in WordNet and H l and Hr denote
the left and the right context of w, respectively.
The first step, common to both systems, consists
of determining the set of synonyms syn(w) for each
entailing word (see Section 2.1). Then, each system
ranks the extracted lists according to the criteria de-
scribed in Section 2.2 and 2.3.
2.1 Used Lexical Resources
For selecting the synonym candidates we used two
lexical repositories: WordNet 2.0 and the Oxford
American Writer Thesaurus (1st Edition). For each
target word, we simply collect all the synonyms for
all the word senses in both these resources.
We exploited two corpora for our systems: the
British National Corpus for acquiring the LSA space
for ranking with domain proximity measure (Sec-
tion 2.2) and the Web 1T 5-gram Version 1 corpus
from Google (distributed by Linguistic Data Consor-
tium)2 for ranking the proposed synonyms accord-
ing to syntagmatic coherence (Section 2.3).
2Available from http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
No other resources were used and the sense rank-
ing in WordNet was not considered at all. Therefore
our system is fully unsupervised.
2.2 Domain Proximity
Semantic Domains are common areas of human dis-
cussion, such as Economics, Politics, Law (Magnini
et al, 2002). Semantic Domains can be described
by DMs (Gliozzo, 2005), by defining a set of term
clusters, each representing a Semantic Domain, i.e.
a set of terms having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain.
DMs can be acquired from texts by exploiting
term clustering algorithms. The degree of associ-
ation among terms and clusters, estimated by the
learning algorithm, provides a domain relevance
function. For our experiments we adopted a clus-
tering strategy based on Latent Semantic Analy-
sis (LSA) (Deerwester et al, 1990), following the
methodology described in (Gliozzo, 2005).
The input of the LSA process is a Term by Docu-
ment matrix T of the frequencies in the whole cor-
pus for each term. In this work we indexed all lem-
matized terms. The so obtained matrix is then de-
composed by means of a Singular Value Decompo-
sition, identifying the principal components of T.
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevance with respect to each domain. The DV ~t?i
for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the cor-
pus. The DVs for texts are obtained by mapping the
document vectors ~dj , represented in the vector space
model, into the vectors ~d?j in the Domain Space, de-fined by
D(~dj) = ~dj(IIDFD) = ~d?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wi) and IDF (wi) is the Inverse Document
Frequency of wi. The similarity among both texts
and terms in the Domain Space is then estimated by
the cosine operation.
To implement our lexical substitution criterion we
ranked the candidate entailed words according to
their domain proximity, following the intuition that
if two words can be substituted in a particular con-
text, then the entailed word should belong to the
146
same semantic domain of the context in which the
entailing word is located.
The intuition above can be modeled by estimating
the similarity in the LSA space between the pseudo
document, estimated by Equation 1, formed by all
the words in the context of the entailing word (i.e.
the union of H l and Hr), and each candidate en-
tailed word in syn(w).
2.3 Syntagmatic Coherence
The syntagmatic coherence criterion is based on the
following observation. If the entailing word w in
its context Hw = H lwHr is actually entailed by
a word e, then there exist some occurrences on the
WEB of the expression He = H leHr, obtained
by replacing the entailing word with the candidate
entailed word. This intuition can be easily imple-
mented by looking for occurrences of He in the Web
1T 5-gram Version 1 corpus.
Figure 1 presents pseudo-code for the synonym
scoring procedure. The procedure takes as input the
set of candidate entailed words E = syn(w) for the
entailing word w, the context Hw in which w oc-
curs, the length of the n-gram (2 6 n 6 5) and the
target word itself. For each candidate entailed word
ei, the procedure ngrams(Hw, w, ei, n) is invoked
to substitute w with ei in Hw, obtaining Hei , and re-turns the set Q of all n-grams containing ei. For ex-
ample, all 3-grams obtained replacing ?bright? with
the synonym ?intelligent? in the sentence ?He was
bright and independent and proud.? are ?He was in-
telligent?, ?was intelligent and? and ?intelligent and
independent?. The maximum number of n-grams
generated is ?5n=2 n. Each candidate synonym isthen assigned a score by summing all the frequen-
cies in the Web 1T corpus of the so generated n-
grams3. The set of synonyms is ranked according
the so obtained scores. However, candidates which
appear in longer n-grams are preferred to candidates
appearing in shorter ones. Therefore, the ranked list
contains first the candidate entailed words appearing
in 5-grams, if any, then those appearing in 4-grams,
and so on. For example, a candidate e1 that appears
only once in 5-grams is preferred to a candidate e2
that appears 1000 times in 4-grams. Note that this
strategy could lead to an output list with repetitions.
3Note that n-grams with frequency lower than 40 are not
present in the corpus.
1: Given E, the set of candidate synonyms
2: Given H , the context in which w occurs
3: Given n, the length of the n-gram
4: Given w, the word to be substituted
5: E? ? ?
6: for each ei in E do
7: Q? ngrams(H,w, ei, n)
8: scorei ? 0
9: for each qj in Q do
10: Get the frequency fj of qj
11: scorei ? scorei + fj
12: end for
13: if scorei > 0 then add the pair {scorei, ei}
in E?
14: end for
15: Return E?
Figure 1: The synonym scoring procedure
3 Evaluation
There are basically two scoring methodologies: (i)
BEST, which scores the best substitute for a given
item, and (ii) OOT, which scores for the best 10 sub-
stitutes for a given item, and systems do not benefit
from providing less responses4 .
BEST. Table 1 and 2 report the performance for the
domain proximity and syntagmatic coherence rank-
ing. Please note that in Table 2 we report both the
official score and a score that takes into account just
the first proposal of the systems, as the usual in-
terpretation of BEST score methodology would sug-
gest5.
OOT. Table 4 and 5 report the performance for the
domain proximity and syntagmatic coherence rank-
ing, scoring for the 10 best substitutes. The results
are quite good especially in the case of syntagmatic
coherence ranking.
Baselines. Table 3 displays the baselines respec-
tively for the BEST and OOT using WordNet 2.1
as calculated by the task organizers. They pro-
pose many baseline measures, but we report only the
4The task proposed a third scoring measure MW that scores
precision and recall for detection and identification of multi-
words in the input sentences. However our systems were not
designed for this functionality. For the details of all scoring
methodologies please refer to the task description documents.
5We misinterpreted that the official scorer divides anyway
the figures by the number of proposals. So for the competition
we submitted the oot result file without cutting the words after
the first one.
147
P R Mode P Mode R
all 8.06 8.06 13.09 13.09
Table 1: BEST results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 12.93 12.91 20.33 20.33
all (official) 6.95 6.94 20.33 20.33
Table 2: BEST results for Syntagmatic ranking
(IRST2-syn)
WordNet one, as it is the higher scoring baseline. We
can observe that globally our systems perform quite
good with respect to the baselines.
4 Conclusion
In this paper we reported a detailed description of
the FBK-irst systems submitted to the Lexical En-
tailment task at the SEMEVAL 2007 evaluation cam-
paign. Our techniques are totally unsupervised, as
they do not require neither the availability of sense
tagged data nor an estimation of sense priors, not
considering the WordNet sense order information.
Results are quite good, as in general they signifi-
cantly outperform all the baselines proposed by the
organizers. In addition, the method based on syn-
tagmatic coherence estimated on the WEB outper-
forms, to our knowledge, the other systems sub-
mitted to the competition. For the future, we plan
to avoid the use of dictionaries by adopting term
similarity techniques to select the candidate entailed
words and to exploit this methodology in some spe-
cific applications such as taxonomy induction and
ontology population.
Acknowledgments
Claudio Giuliano is supported by the X-Media
project (http://www.x-media-project.
org), sponsored by the European Commission
as part of the Information Society Technologies
(IST) programme under EC grant number IST-FP6-
026978. Alfio Gliozzo is supported by FIRB-Israel
P R Mode P Mode R
WN BEST 9.95 9.95 15.28 15.28
WN OOT 29.70 29.35 40.57 40.57
Table 3: WordNet Baselines
P R Mode P Mode R
all 41.23 41.20 55.28 55.28
Table 4: OOT results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 69.03 68.90 58.54 58.54
Table 5: OOT results for Syntagmatic ranking
(IRST2-syn)
research project N. RBIN045PXH.
References
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In proceedings of the PASCAL Workshop
on Learning Methods for Text Understanding and Min-
ing, Grenoble.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. Proceed-
ings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings ACL-2006,
pages 449?456, Sydney, Australia, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
O. Glickman, I. Dagan, M. Keller, S. Bengio, and
W. Daelemans. 2006a. Investigating lexical substi-
tution scoring for subtitle generation tenth conference
on computational natural language learning. In Pro-
ceedings of CoNLL-2006.
O. Glickman, E. Shnarch, and I. Dagan. 2006b. Lexical
reference: a semantic matching subtask. In proceed-
ings of EMNLP 2006.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
148
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 590?599, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Lyrics, Music, and Emotions
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Carlo Strapparava
FBK-irst
strappa@fbk.eu
Abstract
In this paper, we explore the classification of
emotions in songs, using the music and the
lyrics representation of the songs. We intro-
duce a novel corpus of music and lyrics, con-
sisting of 100 songs annotated for emotions.
We show that textual and musical features can
both be successfully used for emotion recog-
nition in songs. Moreover, through compar-
ative experiments, we show that the joint use
of lyrics and music brings significant improve-
ments over each of the individual textual and
musical classifiers, with error rate reductions
of up to 31%.
1 Introduction
Language and music are peculiar characteristics of
human beings. The capability of producing and
enjoying language and music appears in every hu-
man society, regardless of the richness of its culture
(Nettl, 2000).
Importantly, language and music complement
each other in many different ways. For instance,
looking at music and language in terms of fea-
tures, we can observe that music organizes pitch
and rhythm in ways that language does not, and it
lacks the specificity of language in terms of seman-
tic meaning. On the other hand, language is built
from categories that are absent in music (e.g., nouns
and verbs), whereas music seems to have a deeper
power over our emotions than does ordinary speech.
Composers, musicians, and researchers in poetry
and literature alike have been long fascinated by the
combination of language and music, even since the
time of the earliest written records of music encoun-
tered in musical settings for poetry. Despite this in-
terest, and despite the long history of the interaction
between music and lyrics, there is only little work
that explicitly focuses on the connection between
music and lyrics.
In this paper, we focus on the connection between
the musical and linguistic representations in popu-
lar songs, and their role in the expression of affect.
We introduce a novel corpus of lyrics and music, an-
notated for emotions at line level, and explore the
automatic recognition of emotions using both tex-
tual and musical features. Through comparative ex-
periments, we show that emotion recognition can be
performed using either textual or musical features,
and that the joint use of lyrics and music can im-
prove significantly over classifiers that use only one
dimension at a time. We believe our results demon-
strate the promise of using joint music-lyric models
for song processing.
2 Related Work
The literature on music analysis is noticeably large,
and there are several studies concerning the music?s
power over emotions (Juslin and Sloboda, 2001),
thinking (Rauscher et al1993), or physical effort
(Karageorghis and Priest, 2008).
In particular, there has been significant research
in music and psychology focusing on the idea of a
parallel between affective cues in music and speech
(Sundberg, 1982; Scherer, 1995). For instance,
(Scherer, 2004) investigated the types of emotions
that can be induced by music, their mechanisms, and
how they can be empirically measured. (Juslin and
590
Laukka, 2003) conducted a comprehensive review
of vocal expressions and music performance, find-
ing substantial overlap in the cues used to convey
basic emotions in speech and music.
The work most closely related to ours is the com-
bination of audio and lyrics for emotion classifica-
tion in songs, as thoroughly surveyed in (Kim et al
2010). Although several methods have been pro-
posed, including a combination of textual features
and beats per minute and MPEG descriptors (Yang
and Lee, 2004); individual audio and text classifiers
for arousal and valence, followed by a combination
through meta-learning (Yang et al2008); and the
use of crowdsourcing labeling from Last.fm to col-
lect large datasets of songs annotated for emotions
(Laurier et al2008; Hu et al2009), all this pre-
vious work was done at song level, and most of
it focused on valence-arousal classifications. None
of the previous methods considered the fine-grained
classification of emotions at line level, as we do, and
none of them considered the six Ekman emotions
used in our work.
Other related work consists of the development
of tools for music accessing, filtering, classification,
and retrieval, focusing primarily on music in digital
format such as MIDI. For instance, the task of music
retrieval and music recommendation has received a
lot of attention from both the arts and the computer
science communities (see for instance (Orio, 2006)
for an introduction to this task). There are also sev-
eral works on MIDI analysis. Among them, partic-
ularly relevant to our research is the work by (Das
et al2000), who described an analysis of predom-
inant up-down motion types within music, through
extraction of the kinematic variables of music veloc-
ity and acceleration from MIDI data streams. (Catal-
tepe et al2007) addressed music genre classifica-
tion (e.g., classic, jazz, pop) using MIDI and au-
dio features, while (Wang et al2004) automati-
cally aligned acoustic musical signals with their cor-
responding textual lyrics. MIDI files are typically
organized into one or more parallel ?tracks? for in-
dependent recording and editing. A reliable system
to identify the MIDI track containing the melody1
is very relevant for music information retrieval, and
1A melody can be defined as a ?cantabile? sequence of
notes, usually the sequence that a listener can remember after
hearing a song.
there are several approaches that have been proposed
to address this issue (Rizo et al2006; Velusamy et
al., 2007).
Another related study concerned with the interac-
tion of lyrics and music using an annotated corpus is
found in (O?Hara, 2011), who presented preliminary
research that checks whether the expressive meaning
of a particular harmony or harmonic sequence could
be deduced from the lyrics it accompanies, by us-
ing harmonically annotated chords from the Usenet
group alt.guitar.tab.
Finally, in natural language processing, there are
a few studies that mainly exploited the lyrics com-
ponent of the songs, while generally ignoring the
musical component. For instance, (Mahedero et al
2005) dealt with language identification, structure
extraction, and thematic categorization for lyrics.
(Xia et al2008) addressed the task of sentiment
classification in lyrics, recognizing positive and neg-
ative moods in a large dataset of Chinese pop songs,
while (Yang and Lee, 2009) approached the problem
of emotion identification in lyrics, classifying songs
from allmusic.com using a set of 23 emotions.
3 A Corpus of Music and Lyrics
Annotated for Emotions
To enable our exploration of emotions in songs, we
compiled a corpus of 100 popular songs (e.g., Danc-
ing Queen by ABBA, Hotel California by Eagles,
Let it Be by The Beatles). Popular songs exert a
lot of power on people, both at an individual level
as well as on groups, mainly because of the mes-
sage and emotions they convey. Songs can lift our
moods, make us dance, or move us to tears. Songs
are able to embody deep feelings, usually through a
combined effect of both music and lyrics.
The corpus is built starting with the MIDI tracks
of each song, by extracting the parallel alignment
of melody and lyrics. Given the non-homogeneous
quality of the MIDI files available on the Web, we
asked a professional MIDI provider for high quality
MIDI files produced for singers and musicians. The
MIDI files, which were purchased from the provider,
contain also lyrics that are synchronized with the
notes. In these MIDI files, the melody channel is un-
equivocally decided by the provider, making it easier
to extract the music and the corresponding lyrics.
591
MIDI format. MIDI is an industry-standard pro-
tocol that enables electronic musical instruments,
computers, and other electronic equipment to com-
municate and synchronize with each other. Unlike
analog devices, MIDI does not transmit an audio
signal: it sends event messages about musical no-
tation, pitch, and intensity, control signals for pa-
rameters such as volume, vibrato, and panning, and
cues and clock signals to set the tempo. As an elec-
tronic protocol, it is notable for its widespread adop-
tion throughout the music industry.
MIDI files are typically created using computer-
based sequencing software that organizes MIDI
messages into one or more parallel ?tracks? for in-
dependent recording, editing, and playback. In most
sequencers, each track is assigned to a specific MIDI
channel, which can be then associated to specific in-
strument patches. MIDI files can also contain lyrics,
which can be displayed in synchrony with the music.
Starting with the MIDI tracks of a song, we ex-
tract and explicitly encode the following features.
At the song level, the key of the song (e.g., G ma-
jor, C minor). At the line level, we represent the
raising, which is the musical interval (in half-steps)
between the first note in the line and the most impor-
tant note (i.e., the note in the line with the longest
duration). Finally, at the note level, we encode the
time code of the note with respect to the beginning
of the song; the note aligned with the corresponding
syllable; the degree of the note with relation to the
key of the song; and the duration of the note.
Table 1 shows statistics on the corpus. An exam-
ple from the corpus, consisting of the first two lines
from the Beatles? song A hard day?s night, is illus-
trated in Figure 3.
SONGS 100
SONGS IN ?MAJOR? KEY 59
SONGS IN ?MINOR? KEY 41
LINES 4,976
ALIGNED SYLLABLES / NOTES 34,045
Table 1: Some statistics of the corpus
Emotion Annotations with Mechanical Turk. In
order to explore the classification of emotions in
songs, we needed a gold standard consisting of man-
ual emotion annotations of the songs. Following
previous work on emotion annotation of text (Alm
et al2005; Strapparava and Mihalcea, 2007), to
annotate the emotions in songs we use the six ba-
sic emotions proposed by (Ekman, 1993): ANGER,
DISGUST, FEAR, JOY, SADNESS, SURPRISE. To col-
lect the annotations, we use the Amazon Mechanical
Turk service, which was previously found to pro-
duce reliable annotations with a quality comparable
to those generated by experts (Snow et al2008).
The annotations are collected at line level, with a
separate annotation for each of the six emotions. We
collect numerical annotations using a scale between
0 and 10, with 0 corresponding to the absence of an
emotion, and 10 corresponding to the highest inten-
sity. Each HIT (i.e., annotation session) contains an
entire song, with a number of lines ranging from 14
to 110, for an average of 50 lines per song.
The annotators were instructed to: (1) Score the
emotions from the writer perspective, not their own
perspective; (2) Read and interpret each line in con-
text; i.e., they were asked to read and understand
the entire song before producing any annotations;
(3) Produce the six emotion annotations independent
from each other, accounting for the fact that a line
could contain none, one, or multiple emotions. In
addition to the lyrics, the song was also available
online, so they could listen to it in case they were
not familiar with it. The annotators were also given
three different examples to illustrate the annotation.
While the use of crowdsourcing for data annota-
tion can result in a large number of annotations in
a very short amount of time, it also has the draw-
back of potential spamming that can interfere with
the quality of the annotations. To address this aspect,
we used two different techniques to prevent spam.
First, in each song we inserted a ?checkpoint? at a
random position in the song ? a fake line that reads
?Please enter 7 for each of the six emotions.? Those
annotators who did not follow this concrete instruc-
tion were deemed as spammers who produce anno-
tations without reading the content of the song, and
thus removed. Second, for each remaining annota-
tor, we calculated the Pearson correlation between
her emotion scores and the average emotion scores
of all the other annotators. Those annotators with a
correlation with the average of the other annotators
below 0.4 were also removed, thus leaving only the
reliable annotators in the pool.
592
<token time=5760 orig?note=D? degree=5 duration=810>HARD </token>
<song filename=AHARDDAY.m2a>
<key time=0>G major</key>
<line pvers=1 raising=3 anger=1.5 disgust=0.7 sadness=2.5 surprise=0.8 > 
</line>
<line pvers=2 raising=5 anger=3.5 disgust=2 sadness=1.2 surprise=0.2 > 
</line>
<token time=5040 orig?note=B degree=3 duration=210>IT</token>
<token time=5050 orig?note=B degree=3 duration=210>?S </token>
<token time=5280 orig?note=C? degree=4 duration=210>BEEN </token>
<token time=5520 orig?note=B degree=3 duration=210>A </token>
<token time=6720 orig?note=D? degree=5 duration=570>DAY</token>
<token time=6730 orig?note=D? degree=5 duration=570>?S </token>
<token time=7440 orig?note=D? degree=5 duration=690>NIGHT</token>
<token time=8880 orig?note=C? degree=4 duration=212>AND </token>
<token time=9120 orig?note=D? degree=5 duration=210>I</token>
<token time=9130 orig?note=D? degree=5 duration=210>?VE </token>
<token time=9600 orig?note=D? degree=5 duration=210>WOR</token>
<token time=9840 orig?note=F? degree=7? duration=930>KING </token>
<token time=10800 orig?note=D? degree=5 duration=210>LI</token>
<token time=11040 orig?note=C? degree=4 duration=210>KE </token>
<token time=11050 orig?note=C? degree=4 duration=210>A </token>
<token time=11280 orig?note=D? degree=5 duration=330>D</token>
<token time=11640 orig?note=C? degree=4 duration=90>O</token>
<token time=11760 orig?note=B degree=3 duration=330>G</token>
<token time=9360 orig?note=C? degree=4 duration=210>BEEN </token>
Figure 1: Two lines of a song in the corpus: It-?s been a hard day-?s night, And I-?ve been wor-king li-ke a d-o-g
For each song, we start by asking for ten annota-
tions. After spam removal, we were left with about
two-five annotations per song. The final annotations
are produced by averaging the emotions scores pro-
duced by the reliable annotators. Figure 3 shows
an example of the emotion scores produced for two
lines. The overall correlation between the remain-
ing reliable annotators was calculated as 0.73, which
represents a strong correlation.
For each of the six emotions, Table 2 shows the
number of lines that had that emotion present (i.e.,
the score of the emotion was different from 0), as
well as the average score for that emotion over all
4,976 lines in the corpus. Perhaps not surprisingly,
the emotions that are dominant in the corpus are JOY
and SADNESS ? which are the emotions that are of-
ten invoked by people as the reason behind a song.
Note that the emotions do not exclude each other:
i.e., a line that is labeled as containing JOY may also
contain a certain amount of SADNESS, which is the
reason for the high percentage of songs containing
both JOY and SADNESS. The emotional load for
the overlapping emotions is however very different.
For instance, the lines that have a JOY score of 5
or higher have an average SADNESS score of 0.34.
Conversely, the lines with a SADNESS score of 5 or
Number
Emotion lines Average
ANGER 2,516 0.95
DISGUST 2,461 0.71
FEAR 2,719 0.77
JOY 3,890 3.24
SADNESS 3,840 2.27
SURPRISE 2,982 0.83
Table 2: Emotions in the corpus of 100 songs: number
of lines including a certain emotion, and average emotion
score computed over all the 4,976 lines.
higher have a JOY score of 0.22.
4 Experiments and Evaluations
Through our experiments, we seek to determine the
extent to which we can automatically determine the
emotional load of each line in a song, for each of the
six emotion dimensions.
We use two main classes of features: textual fea-
tures, which build upon the textual representation of
the lyrics; and musical features, which rely on the
musical notation associated with the songs. We run
three sets of experiments. The first one is intended to
determine the usefulness of the textual features for
593
emotion classification. The second set specifically
focuses on the musical features. Finally, the last set
of experiments makes joint use of textual and musi-
cal features.
The experiments are run using linear regression,2
and the results are evaluated by measuring the Pear-
son correlation between the classifier predictions
and the gold standard. For each experiment, a ten-
fold cross validation is run on the entire dataset.3
4.1 Textual Features
First, we attempt to identify the emotions in a line
by relying exclusively on the features that can be de-
rived from the lyrics of the song. We decided to fo-
cus on those features that were successfully used in
the past for emotion classification (Strapparava and
Mihalcea, 2008). Specifically, we use: (1) unigram
features obtained from a bag-of-words representa-
tion, which are the features typically used by corpus-
based methods; and (2) lexicon features, indicating
the appartenance of a word to a semantic class de-
fined in manually crafted lexicons, which are often
used by knowledge-based methods.
Unigrams. We use a bag-of-words representation
of the lyrics to derive unigram counts, which are
then used as input features. First, we build a vo-
cabulary consisting of all the words, including stop-
words, occurring in the lyrics of the training set. We
then remove those words that have a frequency be-
low 10 (value determined empirically on a small de-
velopment set). The remaining words represent the
unigram features, which are then associated with a
value corresponding to the frequency of the unigram
inside each line. Note that we also attempted to use
higher order n-grams (bigrams and trigrams), but
evaluations on a small development dataset did not
show any improvements over the unigram model,
and thus all the experiments are run using unigrams.
Semantic Classes. We also derive and use coarse
textual features, by using mappings between words
and semantic classes. Specifically, we use the Lin-
2We use the Weka machine learning toolkit.
3There is no clear way to determine a baseline for these
experiments. A simple baseline that we calculated, which as-
sumed by default an emotional score equal to the average of the
scores on the training data, and measured the correlation be-
tween these default scores and the gold standard, consistently
led to correlations close to 0 (0.0081-0.0221).
guistic Inquiry and Word Count (LIWC) and Word-
Net Affect (WA) to derive coarse textual features.
LIWC was developed as a resource for psycholin-
guistic analysis (Pennebaker and Francis, 1999;
Pennebaker and King, 1999). The 2001 version of
LIWC includes about 2,200 words and word stems
grouped into about 70 broad categories relevant to
psychological processes (e.g., emotion, cognition).
WA (Strapparava and Valitutti, 2004) is a resource
that was created starting with WordNet, by annotat-
ing synsets with several emotions. It uses several re-
sources for affective information, including the emo-
tion classification of Ortony (Ortony et al1987).
From WA, we extract the words corresponding to
the six basic emotions used in our experiments. For
each semantic class, we infer a feature indicating the
number of words in a line belonging to that class.
Table 3 shows the Pearson correlations obtained
for each of the six emotions, when using only uni-
grams, only semantic classes, or both.
Semantic All
Emotion Unigrams Classes Textual
ANGER 0.5525 0.3044 0.5658
DISGUST 0.4246 0.2394 0.4322
FEAR 0.3744 0.2443 0.4041
JOY 0.5636 0.3659 0.5769
SADNESS 0.5291 0.3006 0.5418
SURPRISE 0.3214 0.2153 0.3392
AVERAGE 0.4609 0.2783 0.4766
Table 3: Evaluations using textual features: unigrams,
semantic classes, and all the textual features.
4.2 Musical Features.
In a second set of experiments, we explore the role
played by the musical features. While the musical
notation of a song offers several characteristics that
could be potentially useful for our classification ex-
periments (e.g., notes, measures, dynamics, tempo),
in these initial experiments we decided to focus on
two main features, namely the notes and the key.
Notes. A note is a sign used in the musical nota-
tion associated with a song, to represent the relative
duration and pitch of a sound. In traditional mu-
sic theory, the notes are represented using the first
seven letters of the alphabet (C-D-E-F-G-A-B), al-
594
though other notations can also be used. Notes can
be modified by ?accidentals? ? a sharp or a flat sym-
bol that can change the note by half a tone. A written
note can also have associated a value, which refers
to its duration (e.g., whole note; eighth note). Simi-
lar to the unigram features, for each note, we record
a feature indicating the frequency of that note inside
a line.
Key. The key of a song refers to the harmony or
?pitch class? used for a song, e.g., C major, or F#.
Sometime the term minor or major can be appended
to a key, to indicate a minor or a major scale. For
instance, a song in ?the key of C minor? means that
the song is harmonically centered on the note C, and
it makes use of the minor scale whose first note is C.
The key system is the structural foundation of most
of the Western music. We use a simple feature that
reflects the key of the song. Note that with a few ex-
ceptions, when more than one key is used in a song,
all the lines in a song will have the same key.
Table 4 shows the results obtained in these clas-
sification experiments, when using only the notes as
features, only the key, or both.
All
Emotion Notes Key Musical
ANGER 0.2453 0.4083 0.4405
DISGUST 0.1485 0.2922 0.3199
FEAR 0.1361 0.2203 0.2450
JOY 0.1533 0.3835 0.4001
SADNESS 0.1738 0.3502 0.3762
SURPRISE 0.0983 0.2241 0.2412
AVERAGE 0.1592 0.3131 0.3371
Table 4: Evaluations using musical features: notes, key,
and all the musical features.
4.3 Joint Textual and Musical Features.
To explore the usefulness of the joint lyrics and mu-
sic representation, we also run a set of experiments
that use all the textual and musical features. Table 5
shows the Pearson correlations obtained when us-
ing all the features. To facilitate the comparison,
the table also includes the results obtained with the
textual-only and musical-only features (reported in
Tables 3 and 4).
All All Textual &
Emotion Textual Musical Musical
ANGER 0.5658 0.4405 0.6679
DISGUST 0.4322 0.3199 0.5068
FEAR 0.4041 0.2450 0.4384
JOY 0.5769 0.4001 0.6456
SADNESS 0.5418 0.3762 0.6193
SURPRISE 0.3392 0.2412 0.3855
AVERAGE 0.4766 0.3371 0.5439
Table 5: Evaluations using both textual and musical fea-
tures.
5 Discussion
One clear conclusion can be drawn from these ex-
periments: the textual and musical features are both
useful for the classification of emotions in songs,
and, more importantly, their joint use leads to the
highest classification results. Specifically, the joint
model gives an error rate reduction of 12.9% with
respect to the classifier that uses only textual fea-
tures, and 31.2% with respect to the classifier that
uses only musical features. This supports the idea
that lyrics and music represent orthogonal dimen-
sions for the classification of emotions in songs.
Among the six emotions considered, the largest
improvements are observed for JOY, SADNESS, and
ANGER. This was somehow expected for the first
two emotions, since they appear to be dominant in
the corpus (see Table 2), but comes as a surprise
for ANGER, which is less dominant. Further explo-
rations are needed to determine the reason for this
effect.
Looking at the features considered, textual fea-
tures appear to be the most useful. Nonetheless,
the addition of the musical features brings clear im-
provements, as shown in the last column from the
same table.
Additionally, we made several further analyses of
the results, as described below.
Feature ablation. To determine the role played by
each of the feature groups we consider, we run an
ablation study where we remove one feature group
at a time from the complete set of features and mea-
sure the accuracy of the resulting classifier. Table 6
shows the feature ablation results. Note that feature
ablation can also be done in the reverse direction, by
595
All features, excluding
All Semantic Semantic Classes
Emotion Features Unigrams Classes Notes Key and Notes
ANGER 0.6679 0.4996 0.5525 0.6573 0.6068 0.6542
DISGUST 0.5068 0.3831 0.4246 0.5013 0.4439 0.4814
FEAR 0.4384 0.3130 0.3744 0.4313 0.4150 0.4114
JOY 0.6456 0.5141 0.5636 0.6432 0.5829 0.6274
SADNESS 0.6193 0.4586 0.5291 0.6176 0.5540 0.6029
SURPRISE 0.3855 0.3083 0.3214 0.3824 0.3421 0.3721
AVERAGE 0.5439 0.4127 0.4609 0.5388 0.4908 0.5249
Table 6: Ablation studies excluding one feature group at a time.
Textual and
Emotion Baseline Textual Musical Musical
ANGER 89.27% 91.14% 89.63% 92.40%
DISGUST 93.85% 94.67% 93.85% 94.77%
FEAR 93.58% 93.87% 93.58% 93.87%
JOY 50.26% 70.92% 61.95% 75.64%
SADNESS 67.40% 75.84% 70.65% 79.42%
SURPRISE 94.83% 94.83% 94.83% 94.83%
AVERAGE 81.53% 86.87% 84.08% 88.49%
Table 7: Evaluations using a coarse-grained binary classification.
keeping only one group of features at a time; the re-
sults obtained with the individual feature groups are
already reported in Tables 3 and 4.
The ablation studies confirm the findings from our
earlier experiments: while the unigrams and the keys
are the most predictive features, the semantic classes
and the notes are also contributing to the final clas-
sification even if to a lesser extent. To measure the
effect of these groups of somehow weaker features
(semantic classes and notes), we also perform an ab-
lation experiment where we remove both these fea-
ture groups from the feature set. The results are re-
ported in the last column of Table 6.
Coarse-grained classification. As an additional
evaluation, we transform the task into a binary clas-
sification by using a threshold empirically set at 3.
Thus, to generate the coarse binary annotations, if
the score of an emotion is below 3, we record it as
?negative? (i.e., the emotion is absent), whereas if
the score is equal to or above 3, we record it as ?pos-
itive? (i.e., the emotion is present).
For the classification, we use Support Vector Ma-
chines (SVM), which are binary classifiers that seek
to find the hyperplane that best separates a set of pos-
itive examples from a set of negative examples, with
maximum margin (Vapnik, 1995). Applications of
SVM classifiers to text categorization led to some of
the best results reported in the literature (Joachims,
1998).
Table 7 shows the results obtained for each of the
six emotions, and for the three major settings that
we considered: textual features only, musical fea-
tures only, and a classifier that jointly uses the tex-
tual and the musical features. As before, the classi-
fication accuracy for each experiment is reported as
the average of the accuracies obtained during a ten-
fold cross-validation on the corpus. The table also
shows a baseline, computed as the average of the
accuracies obtained when using the most frequent
class observed on the training data for each fold.
As seen from the table, on average, the joint use of
textual and musical features is also beneficial for this
binary coarser-grained classification. Perhaps not
surprisingly, the effect of the classifier is stronger for
596
1,000 news headlines 4,976 song lines
Best result (Strapparava Joint Text
Emotion SEMEVAL?07 and Mihalcea, 08) and Music
ANGER 0.3233 0.1978 0.6679
DISGUST 0.1855 0.1354 0.5068
FEAR 0.4492 0.2956 0.4384
JOY 0.2611 0.1381 0.6456
SADNESS 0.4098 0.1601 0.6193
SURPRISE 0.1671 0.1235 0.3855
AVERAGE 0.2993 0.1750 0.5439
Table 8: Results obtained in previous work on emotion classification.
those emotions that are dominant in the corpus, i.e.,
JOY and SADNESS (see Table 2). The improvement
obtained with the classifiers is much smaller for the
other emotions (or even absent, e.g., for SURPRISE),
which is also explained by their high baseline of over
90%.
Comparison to previous work. There is no pre-
vious research that has considered the joint use of
lyrics and songs representations for emotion classifi-
cation at line level, and thus we cannot draw a direct
comparison with other work on emotion classifica-
tion in songs.
Nonetheless, as a point of reference, we consider
the previous work done on emotion classification of
texts. Table 8 shows the results obtained in previ-
ous work for the recognition of emotions in a corpus
consisting of 1,000 news headlines (Strapparava and
Mihalcea, 2007) annotated for the same six emo-
tions. Specifically, the table shows the best over-
all correlation results obtained by the three emotion
recognition systems in the SEMEVAL task on Affec-
tive Text (Strapparava and Mihalcea, 2007): (Chau-
martin, 2007; Kozareva et al2007; Katz et al
2007). The table also shows the best results obtained
in follow up work carried out on the same dataset
(Strapparava and Mihalcea, 2008).
Except for one emotion (FEAR), the correlation
figures we obtain are significantly higher than those
reported in previous work. As mentioned before,
however, a direct comparison cannot be made, since
the earlier work used a different, smaller dataset.
Moreover, our corpus of songs is likely to be more
emotionally loaded than the news titles used in pre-
vious work.
6 Conclusions
Popular songs express universally understood mean-
ings and embody experiences and feelings shared by
many, usually through a combined effect of both mu-
sic and lyrics. In this paper, we introduced a novel
corpus of music and lyrics, annotated for emotions at
line level, and we used this corpus to explore the au-
tomatic recognition of emotions in songs. Through
experiments carried out on the dataset of 100 songs,
we showed that emotion recognition can be per-
formed using either textual or musical features, and
that the joint use of lyrics and music can improve
significantly over classifiers that use only one di-
mension at a time.
The dataset introduced in this paper is available
by request from the authors of the paper.
Acknowledgments
The authors are grateful to Rajitha Schellenberg
for her help with collecting the emotion annota-
tions. Carlo Strapparava was partially supported by
a Google Research Award. Rada Mihalcea?s work
was in part supported by the National Science Foun-
dation award #0917170. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of the National Science Foun-
dation.
References
C. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion predic-
tion. In Proceedings of the Conference on Empirical
597
Methods in Natural Language Processing, pages 347?
354, Vancouver, Canada.
Z. Cataltepe, Y. Yaslan, and A. Sonmez. 2007. Mu-
sic genre classification using MIDI and audio features.
Journal on Advances in Signal Processing.
F.R. Chaumartin. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), Prague, Czech Republic, June.
M. Das, D. Howard, and S. Smith. 2000. The kinematic
analysis of motion curves through MIDI data analysis.
Organised Sound, 5(1):137?145.
P. Ekman. 1993. Facial expression of emotion. Ameri-
can Psychologist, 48:384?392.
X. Hu, J. S. Downie, and A. F. Ehmann. 2009. Lyric
text mining in music mood classification. In Proceed-
ings of the International Society for Music Information
Retrieval Conference, Kobe, Japan.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142, Chemnitz, Germany.
P. Juslin and P. Laukka. 2003. Communication of emo-
tion in vocal expression and music performance: Dif-
ferent channels, same code? Psychological Bulletin,
129:770?814.
P. N. Juslin and J. A. Sloboda, editors. 2001. Music and
Emotion: Theory and Research. Oxford University
Press.
C. Karageorghis and D. Priest. 2008. Music in sport and
exercise : An update on research and application. The
Sport Journal, 11(3).
P. Katz, M. Singleton, and R. Wicentowski. 2007.
Swat-mp:the semeval-2007 systems for task 5 and
task 14. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic, June.
Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richard-
son, J. Scott, J. Speck, and D. Turnbull. 2010. Mu-
sic emotion recognition: A state of the art review. In
International Symposium on Music Information Re-
trieval.
Z. Kozareva, B. Navarro, S. Vazquez, and A. Mon-
toyo. 2007. Ua-zbsa: A headline emotion classifi-
cation through web information. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, June.
C. Laurier, J. Grivolla, and P. Herrera. 2008. Multimodal
music mood classification using audio and lyrics. In
Proceedings of the International Conference on Ma-
chine Learning and Applications, Barcelona, Spain.
J. Mahedero, A. Martinez, and P. Cano. 2005. Natu-
ral language processing of lyrics. In Proceedings of
MM?05, Singapore, November.
B. Nettl. 2000. An ethnomusicologist contemplates
universals in musical sound and musical culture. In
N. Wallin, B. Merker, and S. Brown, editors, The
origins of music, pages 463?472. MIT Press, Cam-
bridge, MA.
T. O?Hara. 2011. Inferring the meaning of chord se-
quences via lyrics. In Proceedings of 2nd Workshop
on Music Recommendation and Discovery (WOMRAD
2011), Chicago, IL, October.
N. Orio. 2006. Music retrieval: A tutorial and re-
view. Foundations and Trends in Information Re-
trieval, 1(1):1?90, November.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, (11).
J. Pennebaker and M. Francis. 1999. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
J. Pennebaker and L. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. Journal of Per-
sonality and Social Psychology, (77).
F. Rauscher, G. Shaw, and K. Ky. 1993. Music and spa-
tial task performance. Nature, 365.
D. Rizo, P. Ponce de Leon, C. Perez-Sancho, A. Pertusa,
and J. Inesta. 2006. A pattern recognition approach
for melody track selection in MIDI files. In Proceed-
ings of 7th International Symposyum on Music Infor-
mation Retrieval (ISMIR-06), pages 61?66, Victoria,
Canada, October.
K. Scherer. 1995. Expression of emotion in voice and
music. Journal of Voice, 9:235?248.
K. Scherer. 2004. Which emotions can be induced by
music? what are the underlying mechanisms? and
how can we measure them ? Journal of New Music
Research, 33:239?251.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Honolulu, Hawaii.
C. Strapparava and R. Mihalcea. 2007. Semeval-2007
task 14: Affective text. In Proceedings of the 4th
International Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic.
C. Strapparava and R. Mihalcea. 2008. Learning to iden-
tify emotions in text. In Proceedings of the ACM Con-
ference on Applied Computing ACM-SAC 2008, Fort-
aleza, Brazile.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, Lisbon.
J. Sundberg. 1982. Speech, song, and emotions. In
M. Clynes, editor, Music, Mind and Brain: The Neu-
ropsychology of Music. Plenum Press, New York.
598
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
S. Velusamy, B. Thoshkahna, and K. Ramakrishnan.
2007. Novel melody line identification algorithm for
polyphonic MIDI music. In Proceedings of 13th In-
ternational Multimedia Modeling Conference (MMM
2007), Singapore, January.
Y. Wang, M. Kan, T. Nwe, A. Shenoy, and J. Yin. 2004.
LyricAlly: Automatic synchronization of acoustic mu-
sical signals and textual lyrics. In Proceedings of
MM?04, New York, October.
Y. Xia, L. Wang, K.F. Wong, and M. Xu. 2008. Lyric-
based song sentiment classification with sentiment
vector space model. In Proceedings of the Association
for Computational Linguistics, Columbus, Ohio.
D. Yang and W. Lee. 2004. Disambiguating music
emotion using software agents. In Proceedings of the
International Conference on Music Information Re-
trieval, Barcelona, Spain.
D. Yang and W. Lee. 2009. Music emotion identification
from lyrics. In Proceedings of 11th IEEE Symposium
on Multimedia.
Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-C. Ho,
and H. Chen. 2008. Toward multi-modal music emo-
tion classification. In Proceedings of the 9th Pacific
Rim Conference on Multimedia: Advances in Multi-
media Information Processing.
599
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 414?418,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Automatic Domain Assignment for Word Sense Alignment
Tommaso Caselli
TrentoRISE / Via Sommarive, 18
38123 Povo, Italy
t.caselli@gmail.com
Carlo Strapparava
FBK / Via Sommarive, 18
38123 Povo, Italy
strappa@fbk.eu
Abstract
This paper reports on the development of a hy-
brid and simple method based on a machine
learning classifier (Naive Bayes), Word Sense
Disambiguation and rules, for the automatic
assignment of WordNet Domains to nominal
entries of a lexicographic dictionary, the Senso
Comune De Mauro Lexicon. The system ob-
tained an F1 score of 0.58, with a Precision
of 0.70. We further used the automatically as-
signed domains to filter out word sense align-
ments between MultiWordNet and Senso Co-
mune. This has led to an improvement in the
quality of the sense alignments showing the
validity of the approach for domain assign-
ment and the importance of domain informa-
tion for achieving good sense alignments.
1 Introduction and Problem Statement
Lexical knowledge, i.e. how words are used and ex-
press meaning, plays a key role in Natural Language
Processing. Lexical knowledge is available in many
different forms, ranging from unstructured terminolo-
gies (i.e. word list), to full fledged computational lexica
and ontologies (e.g. WordNet (Fellbaum, 1998)). The
process of creation of lexical resources is costly both
in terms of money and time. To overcome these lim-
its, semi-automatic approaches have been developed
(e.g. MultiWordNet (Pianta et al., 2002)) with differ-
ent levels of success. Furthermore, important informa-
tion is scattered in different resources and difficult to
use. Semantic interoperability between resources could
represent a viable solution to allow reusability and de-
velop more robust and powerful resources. Word sense
alignment (WSA) qualifies as the preliminary require-
ment for achieving this goal (Matuschek and Gurevych,
2013).
WSA aims at creating lists of pairs of senses from
two, or more, (lexical-semantic) resources which de-
note the same meaning. Different approaches to WSA
have been proposed and they all share some common
elements, namely: i.) the extensive use of sense de-
scriptions of the words (e.g. WordNet glosses); and ii.)
the extension of the basic sense descriptions with addi-
tional information such as hypernyms, synonyms and
domain or category labels.
The purpose of this work is two folded: first, we exper-
iment on the automatic assignment of domain labels to
sense descriptions, and then, evaluate the impact of this
information for improving an existing sense aligned
dataset for nouns. Previous works has demonstrated
that domain labels are a good feature for obtaining high
quality alignments of entries (Navigli, 2006; Toral et
al., 2009; Navigli and Ponzetto, 2012). The Word-
Net (WN) Domains (Magnini and Cavaglia, 2000; Ben-
tivogli et al., 2004) have been selected as reference do-
main labels. We will use as candidate lexico-semantic
resources to be aligned two Italian lexica, namely, Mul-
tiWordNet (MWN) and the Senso Comune De Mauro
Lexicon (SCDM) (Vetere et al., 2011).
The two resources differ in terms of modelization: the
former, MWN, is an Italian version of WN obtained
through the ?expand model? (Vossen, 1996) and per-
fectly aligned to Princeton WN 1.6, while the latter,
SCDM, is a machine readable dictionary obtained from
a paper-based reference lexicographic dictionary, De
Mauro GRADIT. Major issues for WSA of the lexica
concern the following aspects:
? SCMD has no structure of word senses (i.e. no
taxonomy, no synonymy relations, no distinction
between core senses and subsenses for polyse-
mous entries) unlike MWN;
? SCDM has no domain or category labels associ-
ated to senses (with the exception of specific ter-
minological entries) unlike MWN;
? the Italian section of MWN has only 2,481 glosses
in Italian over 28,517 synsets for nouns (i.e.
8.7%).
The remainder of this paper is organized as follows:
Section 2 will report on the methodology and exper-
iments implemented for the automatic assignment of
the WN Domains to the SCDM entries. Section 3 will
describe the dataset used for the evaluation of the WSA
experiments and the use of the WN Domains for filter-
ing the sense alignments. Finally, Section 4 illustrates
conclusion and future work.
2 Methodology and Experiments
The WN Domains consist of a set of 166 hierarchically
organized labels which have been associated to each
414
Classifiers P R F1 10-Fold F1
NaiveBayes
lemma
0.77 0.58 0.66 0.66
MaxEnt
lemma
0.70 0.49 0.58 0.63
NaiveBayes
wsd
0.77 0.58 0.66 0.69
MaxEnt
wsd
0.74 0.54 0.62 0.67
Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers.
synset
1
and express a subject field label (e.g. SPORT,
MEDICINE). A special label, FACTOTUM, has been
used for those synsets which can appear in almost all
subject fields.
The identification of a domain label to the nominal en-
tries in the SCDM Lexicon is based the ?One Domain
per Discourse? (ODD) hypothesis applied to the sense
descriptions. We have used a reduced set of domains
labels (45 normalized domains) following (Magnini et
al., 2001).
To assign the WN domain label to the SCDM entries,
we have developed a hybrid method: first a binary clas-
sifier is applied to the SCDM sense descriptions to dis-
criminate between two domain values, FACTOTUM
and OTHER, where the OTHER value includes all re-
maining 44 normalized domains. After this, all entries
classified with the OTHER value are analyzed by a rule
based system and associated with a specific domain la-
bel (i.e. SPORT, MEDICINE, FOOD . . . ).
2.1 Classifier and feature selection
We have developed a training set by manually align-
ing noun senses between the two lexica. The sense
alignment allows us to associate all the information of a
synset to a corresponding entry in the SCDM lexicon,
including the WN Domain label. Concerning the test
set, we have used an existing dataset of aligned noun
pairs as in (Caselli et al., 2014). We report in Table 1
the figures for the training and test sets. Multiple align-
ments with the same domain label have been excluded
from the training set.
Characteristics Training Set Test Set
# lemmas 131 46
# of aligned pairs 369 166
# of SCDM senses 747 216
# of MWN synsets 675 229
# SCDM with
WN Domain label 350 118
Table 1: Training and test sets for the classifier.
In order for the classifier to predict the binary do-
main labels (FACTOTUM and OTHER), each sense
description of the SCDM Lexicon has been repre-
sented by means of a two-dimensional feature vector
(e.g. for training data: BINARY DOMAIN LABEL
1
The full set of labels and hierarchy is available at
http://wndomains.fbk.eu/hierarchy.html
GENERIC:val SPECIFIC:val). Feature values have
been obtained through two strategies:
? lemma label: we extract all normalized domain
labels associated to each sense of each lemma in
the sense description from MWN. The value of
the feature GENERIC corresponds to the sum of
the FACTOTUM labels. The value of the fea-
ture SPECIFIC corresponds to the sum of all other
specific domain labels (e.g. MEDICINE, SPORT
etc.) after they have been collapsed into a single
value (i.e. NOT-FACTOTUM).
? word sense label: for each sense description, we
have first performed Word Sense Disambiguation
by means of an adapted version to Italian of the
UKB package
2
(Agirre et al., 2010; Agirre et al.,
2014)
3
. Only the highest ranked synset, and as-
sociated WN Domain(s), was retained as good.
Similarly to the lemma label strategy, the sum of
the domain label FACTOTUM is assigned to the
feature GENERIC, while the sum of all other do-
main labels collapsed into the single value NOT-
FACTOTUM is assigned to the feature SPECIFIC.
We experimented with two classifiers: Naive Bayes
and Maximum Entropy as implemented in the MAL-
LET package (McCallum, 2002). We illustrate the re-
sults in Table 2. The classifiers have been evaluated
with respect to standard Precision (P), Recall (R) and
F1 against the test set. Ten-fold cross validation has
been performed on the training set as well. Classifiers
trained with the first strategy will be associated with the
label lemma, while those trained with the second strat-
egy with the label wsd.
Both classifiers obtains good results with respect to
the test data in terms of Precision and Recall. The
Naive Bayes classifier outperforms the Maximum En-
tropy one in both training approaches, suggesting better
generalization capabilities even in presence of a small
training set and basic features. The role of WSD has
a positive impact, namely for the Maximum Entropy
classifier (Precision +4 points, Recall +5 points with
respect to the lemma label). Although such a positive
effect of the WSD does not emerge for the Naive Bayes
classifier with respect to the test set, we can still ob-
serve an improvement over the ten-fold cross valida-
tion (F1= 0.69 vs. F1=0.66). We finally selected the
2
Available at http://ixa2.si.ehu.es/ukb/
3
We used the WN Multilingual Central Repository as
knowledge base and the MWN entries as dictionary
415
predictions of Naive Bayes
wsd
classifier as input to the
rule-based system as it provides the highest scores.
2.2 Rules for WN Domain assignment
The rule based classifier for final WN Domain assign-
ment works as follows:
? lemmatized and word sense disambiguated lem-
mas in the sense descriptions are associated with
the corresponding WN Domains from MWN;
? frequency counts on the WN Domain labels is ap-
plied; the most frequent WN Domain is assigned
as the correct WN Domain of the nominal entry;
? in case two or more WN Domains have same fre-
quency, the following assignment strategy is ap-
plied: if the frequency scores of the WN Do-
mains is equal to 1, the value FACTOTUM is se-
lected; on the contrary, if the frequency score is
higher than 1, all WN Domain labels are retained
as good.
We report the results on final domain assignment
in Table 3. The final system, NaiveBayes+Rules, has
been compared to two baselines. Both baselines ap-
ply frequency counts over the WN Domains labels
of the lemmas of the sense descriptions for the en-
tire set of the 45 normalized domain values, including
the FACTOTUM label, as explained in Section2. The
Baseline
lemma
assigns the domain by taking into ac-
count every WN Domain associated to each lemma. On
the other hand, the Baseline
wsd
selects only the WN
Domain of sense disambiguated lemmas. WSD for the
second baseline has been performed by applying the
same method described in Section 2.1. The results of
both baselines have high values for Precision (0.58 for
Baseline
lemma
, 0.70 for Baseline
wsd
). We consider
this as a further support to the validity of the ODD hy-
pothesis which seems to hold even for text descriptions
like dictionary glosses which normally use generic lex-
ical items to illustrate word senses. It is also interesting
to notice that WSD on its own has a positive impact in
Baseline
wsd
system for the assignment of specific do-
main labels (F1=0.53).
The hybrid system performs better than both base-
lines in terms of F1 scores (F1=0.58 vs. F1=0.45 for
Baseline
lemma
vs. F1=0.53 for Baseline
wsd
). How-
ever, both the hybrid system and the Baseline
wsd
ob-
tain the same Precision. To better evaluate the per-
formance of our hybrid approach, we computed the
paired t-test. The results of the hybrid system are sta-
tistically significant with respect to the Baseline
lemma
(p < 0.05) and for Recall only when compared to the
Baseline
wsd
.
To further analyze the difference between the hybrid
system and the Baseline
wsd
, we performed an error
analysis on their outputs. We have identified that the
hybrid system is more accurate in the prediction of the
System P R F1
NaiveBayes
wsd
+Rules 0.70? 0.50?? 0.58?
Baseline
lemma
0.58 0.36 0.45
Baseline
wsd
0.70 0.43 0.53
Table 3: Results of WN Domain Assignment over the
SDCM entries. Statistical significance of the Naive-
Bayes+Rules system has been marked with a ? for the
Baseline
lemma
and with a ? for the Baseline
wsd
FACTOTUM class with respect to the baseline. In par-
ticular, the accuracy of the hybrid system on this class
is 79% while that of the baseline is only 65%. In addi-
tion to this, the hybrid system provides better results in
terms of Recall (R=0.50 vs. R=0.43). Although compa-
rable, the hybrid system provides more accurate results
with respect to the baseline.
3 Domain Filtering for WSA
This section reports on the experiments for improving
existing WSA for nouns between SDCM and MWN. In
this work we have used the same dataset and alignment
methods as in (Caselli et al., 2014), shortly described
here:
? Lexical Match: for each word w and for each
sense s in the given resources R ? {MWN,
SCDM}, we constructed a sense descriptions
d
R
(s) as a bag of words in Italian. The alignment
is based on counting the number of overlapping
tokens between the two strings, normalized by the
length of the strings;
? Cosine Similarity: we used the Personalized Page
Rank (PPR) algorithm (Agirre et al., 2010) with
WN 3.0 as knowledge base extended with the
?Princeton Annotated Gloss Corpus?. Once the
PPR vector pairs are obtained, the alignment is
obtained on the basis of the cosine score for each
pair
4
.
The dataset consists of 166 pairs of aligned senses
from MWN and SCDM for 46 nominal lemmas
(see also column ?Test set? in Table 1). Overall,
SCDM covers 53.71The main difference with respect
to (Caselli et al., 2014) is that the proposed alignments
have been additionally filtered on the basis of the output
of the WN domain system (NaiveBayes
wsd
+Rules). In
particular, for each aligned pair which was considered
as good in (Caselli et al., 2014), we have applied a fur-
ther filtering based on the WN domain system results
as follows: if two senses are aligned but do not have
the same domain, they are excluded from the WSA re-
sults, otherwise they are retained. Table 4 illustrates
4
The vectors for the SCDM entries were obtained by, first,
applying Google Translate API to get the English translations
and, then, PPR over WN 3.0.
416
System P R F1
LexicalMatch 0.76 (0.69) 0.27 (0.44) 0.40 (0.55)
Cosine noThreshold 0.27 (0.12) 0.47 (0.94) 0.35 (0.21)
Cosine > 0.1 0.77 (0.52) 0.21 (0.32) 0.33 (0.40)
Cosine > 0.2 0.87 (0.77) 0.14 (0.21) 0.24 (0.33)
LexicalMatch+Cosine > 0.1 0.73 (na) 0.40 (na) 0.51 (na)
LexicalMatch+Cosine > 0.2 0.77 (0.67) 0.37 (0.61) 0.50 (0.64)
Table 4: Results for WSA of nouns with domain filtering.
the results of the WSA approaches with domain fil-
ters. We report in brackets the results from (Caselli et
al., 2014). The filtering based on WN Domains has a
big impact on Precision and contributes to increase the
quality of the aligned senses. Although, in general, we
have a downgrading of the performance with respect to
Recall, the increase in Precision will reduce the man-
ual post-processing effort to fully aligned the two re-
sources
5
. Furthermore, it is interesting to notice that,
when merging together the results of the pre-filtered
alignments from the two alignment approaches (Lex-
icalMatch+Cosine > 0.1 and LexicalMatch+Cosine >
0.2), we still have a very high Precision (> 0.70) and an
increase in Recall (> 0.40) with respect to the results of
each approach. Finally, we want to point out that what
was reported as the best alignment results in (Caselli
et al., 2014), namely LexicalMatch+Cosine > 0.2, can
be obtained, at least for Precision, with a lower filtering
cut-off threshold on the Cosine Similarity approach (i.e
cut-off threshold at or higher than 0.1)
4 Conclusions and Future Work
This work describes a hybrid approach based on a
Naive Bayes classifier, Word Sense Disambiguation
and rules for assigning WN Domains to nominal sense
descriptions of a lexicographic dictionary, the Senso
Comune De Mauro Lexicon. The assignment of do-
main labels has been used to improve WSA results on
nouns between the Senso Comune Lexicon and Mul-
tiWordNet. The results support some observations,
namely: i.) domain filtering plays an important role
in WSA, namely as a strategy to exclude wrong align-
ments (false positives) and improve the quality of the
aligned pairs; ii.) the method we have proposed is a vi-
able approach for automatically enriching existing lex-
ical resources in a reliable way; and iii.) the ODD hy-
pothesis also apply to sense descriptions.
An advantage of our approach is its simplicity. We have
used features based on frequency counts and obtained
good results, with a Precision of 0.70 for automatic WN
Domain assignment. Nevertheless, an important role
is played by Word Sense Disambiguation. The use of
domain labels obtained from sense disambiguated lem-
mas improves both the results of the classifier and those
5
The F1 of 0.64 in (Caselli et al., 2014) is obtained with a
Precision of 0.67, suggesting that some alignments are false
positives
of the rules. The absence of statistical significance with
respect to the Baseline
wsd
is not to be considered as a
negative result. As the error analysis has showed, the
classifier mostly contributes to the identification of the
FACTOTUM value, which tends to be overestimated
even with sense disambiguated lemmas, and to Recall.
We are planning to extend this work to include do-
main clusters to improve the domain assignment re-
sults, namely in terms of Recall.
Acknowledgments
One of the author wants to thank Vrije Univerisiteit
Amsterdam for sponsoring the attendance to the
EMNLP conference.
References
Eneko Agirre, Montse Cuadros, German Rigau, and
Aitor Soroa. 2010. Exploring knowledge bases
for similarity. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
may. European Language Resources Association
(ELRA).
Eneko Agirre, Oier L?opez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57?84.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini,
and Emanuele Pianta. 2004. Revising the wordnet
domains hierarchy: semantics, coverage and balanc-
ing. In Proceedings of the Workshop on Multilin-
gual Linguistic Resources, pages 101?108. Associa-
tion for Computational Linguistics.
Tommaso Caselli, Carlo Strapparava, Laure Vieu, and
Guido Vetere. 2014. Aligning an italianwordnet
with a lexicographic dictionary: Coping with limited
data. In Proceedings of the Seventh Global WordNet
Conference, pages 290?298.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). MIT Press.
417
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating subject field codes into wordnet. In Proceed-
ings of the conference on International Language
Resources and Evaluation (LREC 2000).
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2001. Using domain in-
formation for word sense disambiguation. In The
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
pages 111?114. Association for Computational Lin-
guistics.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-wsa: A graph-based approach to word sense
alignment. Transactions of the Association for Com-
putational Linguistics (TACL), 2:to appear.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Rada Mihalcea. 2007. Using Wikipedia for automatic
word sense disambiguation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, Rochester, New York.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 44
th
Annual Meet-
ing of the Association for Computational Linguis-
tics joint with the 21
st
International Conference on
Computational Linguistics (COLING-ACL), Sydney,
Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
peoples web meets linguistic knowledge: Automatic
sense alignment of Wikipedia and WordNet. In
Proceedings of the 9th International Conference on
Computational Semantics, pages 205?214, Singa-
pore, January.
Emanuele Pianta, Luisa Bentivogli, and Cristian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, Mysore, India.
German Rigau and Agirre Eneko. 1995. Disambiguat-
ing bilingual nominal entries against WordNet. In
Proceedings of workshop The Computational Lexi-
con, 7th European Summer School in Logic, Lan-
guage and Information, Barcelona, Spain.
Adriana Roventini, Nilda Ruimy, Rita Marinelli,
Marisa Ulivieri, and Michele Mammini. 2007.
Mapping concrete entities from PAROLE-SIMPLE-
CLIPS to ItalWordNet: Methodology and results. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, Prague, Czech Republic, June.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Pro-
ceedings of the Third international conference on
Advances in Web Intelligence, AWIC?05, Berlin,
Heidelberg. Springer-Verlag.
Antonio Toral, Oscar Ferr?andez, Eneko Aguirre, and
Rafael Munoz. 2009. A study on linking and disam-
biguating wikipedia categories to wordnet using text
similarity. Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP 2009).
Guido Vetere, Alessandro Oltramari, Isabella Chiari,
Elisabetta Jezek, Laure Vieu, and Fabio Massimo
Zanzotto. 2011. Senso Comune, an open knowl-
edge base for italian. JTraitement Automatique des
Langues, 53(3):217?243.
Piek Vossen. 1996. Right or wrong: Combining lexi-
cal resources in the eurowordnet project. In Euralex,
volume 96, pages 715?728. Citeseer.
418
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1511?1521,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Sensicon: An Automatically Constructed Sensorial Lexicon
Serra Sinem Tekiro?lu
University of Trento
Fondazione Bruno Kessler
Trento, Italy
tekiroglu@fbk.eu
G?zde ?zbal
Trento RISE
Trento, Italy
gozbalde@gmail.com
Carlo Strapparava
Fondazione Bruno Kessler
Trento, Italy
strappa@fbk.eu
Abstract
Connecting words with senses, namely,
sight, hearing, taste, smell and touch, to
comprehend the sensorial information in
language is a straightforward task for hu-
mans by using commonsense knowledge.
With this in mind, a lexicon associating
words with senses would be crucial for the
computational tasks aiming at interpreta-
tion of language. However, to the best of
our knowledge, there is no systematic at-
tempt in the literature to build such a re-
source. In this paper, we present a senso-
rial lexicon that associates English words
with senses. To obtain this resource, we
apply a computational method based on
bootstrapping and corpus statistics. The
quality of the resulting lexicon is evaluated
with a gold standard created via crowd-
sourcing. The results show that a sim-
ple classifier relying on the lexicon out-
performs two baselines on a sensory clas-
sification task, both at word and sentence
level, and confirm the soundness of the
proposed approach for the construction of
the lexicon and the usefulness of the re-
source for computational applications.
1 Introduction
Sensorial information interpenetrates languages
with various semantic roles in different levels since
the main interaction instrument of humans with the
outside world is the sensory organs. The trans-
formation of the raw sensations that we receive
through the sensory organs into our understand-
ing of the world has been an important philo-
sophical topic for centuries. According to a clas-
sification that dates back to Aristotle (Johansen,
1997), senses can be categorized into five modali-
ties, namely, sight, hearing, taste, smell and touch.
With the help of perception, we can process the
data coming from our sensory receptors and be-
come aware of our environment. While interpret-
ing sensory data, we unconsciously use our exist-
ing knowledge and experience about the world to
create a private experience (Bernstein, 2010).
Language has a significant role as our main
communication device to convert our private ex-
periences to shared representations of the environ-
ment that we perceive (Majid and Levinson, 2011).
As a basic example, onomatopoeic words, such as
knock or woof, are acquired by direct imitation of
the sounds allowing us to share the experience of
what we hear. As another example, where an im-
itation is not possible, is that giving a name to a
color, such as blue, provides a tool to describe a
visual feature of an object. In addition to the words
that describe the direct sensorial features of ob-
jects, languages include many other lexical items
that are connected to sensory modalities in various
semantic roles. For instance, while some words
can be used to describe a perception activity (e.g.,
to sniff, to watch, to feel), others can simply be
physical phenomena that can be perceived by sen-
sory receptors (e.g., light, song, salt, smoke).
Common usage of language, either written or
spoken, can be very dense in terms of sensorial
words. As an example, the sentence ?I felt the cold
breeze.? contains three sensorial words: to feel as
a perception activity, cold as a perceived sensorial
feature and breeze as a physical phenomenon. The
connection to the sense modalities of the words
might not be mutually exclusive, that is to say a
word can be associated with more than one senses.
For instance, the adjective sweet could be associ-
ated with both the senses of taste and smell. While
we, as humans, have the ability to connect words
with senses intuitively by using our commonsense
knowledge, it is not straightforward for machines
to interpret sensorial information.
Making use of a lexicon containing sensorial
words could be beneficial for many computa-
tional scenarios. Rodriguez-Esteban and Rzhetsky
1511
(2008) report that using words related to senses
in a text could clarify the meaning of an abstract
concept by facilitating a more concrete imagina-
tion. To this respect, an existing text could be au-
tomatically modified with sensory words for vari-
ous purposes such as attracting attention or biasing
the audience towards a specific concept. Addition-
ally, sensory words can be utilized to affect private
psychology by inducing a positive or negative sen-
timent (Majid and Levinson, 2011). For instance,
de Araujo et al. (2005) show that the pleasantness
level of the same odor can be altered by labeling it
as body odor or cheddar cheese. As another moti-
vation, the readability and understandability of text
could also be enhanced by using sensory words
(Rodriguez-Esteban and Rzhetsky, 2008). A com-
pelling use case of a sensorial lexicon is that auto-
matic text modification to change the density of a
specific sense could help people with sensory dis-
abilities. For instance, while teaching a concept to
a congenitally blind child, an application that elim-
inates color-related descriptions would be benefi-
cial. A sensorial lexicon could also be exploited by
search engines to personalize the results according
to user needs.
Advertising is another broad area which would
benefit from such a resource especially by using
synaesthesia1, as it strengthens creative thinking
and it is commonly exploited as an imagination
boosting tool in advertisement slogans (Pricken,
2008). As an example, we can consider the slogans
?The taste of a paradise? where the sense of sight
is combined with the sense of taste or ?Hear the
big picture? where sight and hearing are merged.
Various studies have been conducted both
in computational linguistics and cognitive sci-
ence that build resources associating words with
several cognitive features such as abstractness-
concreteness (Coltheart, 1981; Turney et al.,
2011), emotions (Strapparava and Valitutti, 2004;
Mohammad and Turney, 2010), colors (?zbal et
al., 2011; Mohammad, 2011) and imageability
(Coltheart, 1981). However, to the best of our
knowledge, there is no attempt in the literature to
build a resource that associates words with senses.
In this paper, we propose a computational method
to automatically generate a sensorial lexicon that
associates words in English with senses. Our
method consists of two main steps. First, we gen-
1American Heritage Dictionary (http://
ahdictionary.com/) defines synaesthesia in linguis-
tics as the description of one kind of sense impression by
using words that normally describe another.
erate a set of seed words for each sense category
with the help of a bootstrapping approach. In the
second step, we exploit a corpus based probabilis-
tic technique to create the final lexicon. We eval-
uate this lexicon with the help of a gold standard
that we obtain by using the crowdsourcing service
of CrowdFlower2.
The sensorial lexicon, which we named Sen-
sicon, embodies 22,684 English lemmas together
with their part-of-speech (POS) information that
have been linked to one or more of the five senses.
Each entry in this lexicon consists of a lemma-POS
pair and a score for each sensory modality that in-
dicates the degree of association. For instance, the
verb stink has the highest score for smell as ex-
pected while the scores for the other four senses
are very low. The noun tree, which is a concrete
object and might be perceived by multiple senses,
has high scores for sight, touch and smell.
The rest of the paper is organized as follows.
We first review previous work relevant to this task
in Section 2. Then in Section 3, we describe the
proposed approach in detail. In Section 4, we ex-
plain the annotation process that we conducted and
the evaluation strategy that we employed. Finally,
in Section 5, we draw our conclusions and outline
possible future directions.
2 Related Work
Since to the best of our knowledge there is no at-
tempt in the literature to automatically associate
words with human senses, in this section we will
summarize the most relevant studies that focused
on linking words with various other cognitive fea-
tures.
There are several studies focusing on word-
emotion associations. WordNet Affect Lexicon
(Strapparava and Valitutti, 2004) maps WordNet
(Fellbaum, 1998) synsets to various cognitive fea-
tures (e.g., emotion, mood, behaviour). This re-
source is created by using a small set of synsets
as seeds and expanding them with the help of se-
mantic and lexical relations among these synsets.
Yang et al. (2007) propose a collocation model
with emoticons instead of seed words while creat-
ing an emotion lexicon from a corpus. Perrie et al.
(2013) build a word-emotion association lexicon
by using subsets of a human-annotated lexicon as
seed sets. The authors use frequencies, counts, or
unique seed words extracted from an n-gram cor-
pus to create lexicons in different sizes. They pro-
2http://www.crowdflower.com/
1512
pose that larger lexicons with less accurate genera-
tion method perform better than the smaller human
annotated lexicons. While a major drawback of
manually generated lexicons is that they require a
great deal of human labor, crowdsourcing services
provide an easier procedure for manual annota-
tions. Mohammad and Turney (2010) generate an
emotion lexicon by using the crowdsourcing ser-
vice provided by Amazon Mechanical Turk3 and
it covers 14,200 term-emotion associations.
Regarding the sentiment orientations and sub-
jectivity levels of words, Sentiwordnet (Esuli and
Sebastiani, 2006) is constructed as an extension
to WordNet and it provides sentiments in synset
level. Positive, negative and neutral values are as-
signed to synsets by using ternary classifiers and
synset glosses. Another study that has been inspi-
rational for the design of our approach is Banea
et al. (2008). The authors generate a subjectivity
lexicon starting with a set of seed words and then
using a similarity measure among the seeds and the
candidate words.
Another cognitive feature relevant to sensorial
load of the words is the association between col-
ors and words. Mohammad (2011) builds a color-
word association lexicon by organizing a crowd-
sourcing task on Amazon Mechanical Turk. In-
stead, ?zbal et al. (2011) aim to automate this
process and propose three computational methods
based on image analysis, language models and la-
tent semantic analysis (LSA) (Landauer and Du-
mais, 1997). The authors compare these meth-
ods against a gold standard obtained by the crowd-
sourcing service of Amazon Mechanical Turk.
The best performance is obtained by using image
features while LSA performs slightly better than
the baseline.
Finally, there have been efforts in the liter-
ature about the association of words with their
abstractness-concreteness and imageability levels.
MRC Psycholinguistic Database (Coltheart, 1981)
includes abstractness-concreteness and imageabil-
ity ratings of a small set of words determined
according to psycholinguistic experiments. Tur-
ney et al. (2011) propose to use LSA similarities
of words with a set of seed words to automati-
cally calculate the abstractness and concreteness
degrees of words.
3http://www.mturk.com/mturk
3 Automatic Association of Senses with
Words
We adopt a two phased computational approach to
construct a large sensorial lexicon. First, we em-
ploy a bootstrapping strategy to generate a suffi-
cient number of sensory seed words from a small
set of manually selected seed words. In the sec-
ond phase, we perform a corpus based probabilistic
method to estimate the association scores to build
a larger lexicon.
3.1 Selecting Seed Words
The first phase of the lexicon construction pro-
cess aims to collect sensorial seed words, which
are directly related to senses (e.g., sound, tasty
and sightedness). To achieve that, we utilized
a lexical database called FrameNet (Baker et al.,
1998), which is built upon semantic frames of con-
cepts in English and lexical units (i.e., words) that
evoke these frames. The basic idea behind this
resource is that meanings of words can be under-
stood on the basis of a semantic frame. A semantic
frame consists of semantic roles called frame ele-
ments, which are manually annotated in more than
170,000 sentences. We have considered FrameNet
to be especially suitable for the collection of sen-
sorial seed words since it includes semantic roles
and syntactic features of sensational and percep-
tional concepts.
In order to determine the seed lemma-POS pairs
in FrameNet, we first manually determined 31
frames that we found to be highly connected to
senses such as Hear, Color, Temperature and Per-
ception_experience. Then, we conducted an an-
notation task and asked 3 annotators to determine
which senses the lemma-POS pairs evoking the
collected frames are associated with. At the end of
this task, we collected all the pairs (i.e. 277) with
100% agreement to constitute our initial seed set.
This set contains 277 lemma-POS pairs associated
with a specific sense such as the verb click with
hearing, the noun glitter with sight and aromatic
with smell.
3.2 Seed Expansion via Bootstrapping
In this step, we aim to extend the seed list that we
obtained from FrameNet with the help of a boot-
strapping approach. To achieve that, we adopt a
similar approach to Dias et al. (2014), who pro-
pose a repetitive semantic expansion model to au-
tomatically build temporal associations of synsets
in WordNet. Figure 1 provides an overview of
the bootstrapping process. At each iteration, we
1513
WordNet
MapNet
SVM Cross-
validation
Synset Set
Expansion
Sense
Seed
Synsets
FrameNet
Break-Point
Detection
Figure 1: Bootstrapping procedure to expand the
seed list.
first expand the seed list by using semantic rela-
tions provided by WordNet. We then evaluate the
accuracy of the new seed list for sense classifica-
tion by means of cross-validation against WordNet
glosses. For each sense, we continue iterating un-
til the cross-validation accuracy becomes stable or
starts to decrease. The following sections explain
the whole process in detail.
3.2.1 Extending the Seed List with WordNet
While the initial sensory seed list obtained from
FrameNet contains only 277 lemma-POS pairs,
we extend this list by utilizing the semantic re-
lations provided by WordNet. To achieve that,
we first map each lemma-POS pair in the seed
list to WordNet synsets with the help of Map-
Net (Tonelli and Pighin, 2009), which is a re-
source providing direct mapping between Word-
Net synsets and FrameNet lexical units. Then, we
add to the list the synsets that have WordNet re-
lations direct antonymy, similarity, derived-from,
derivationally-related, pertains-to, attribute and
also-see with the already existing seeds. For in-
stance, we add the synset containing the verb laugh
for the synset of the verb cry with the relation di-
rect antonymy, or the synset containing the ad-
jective chilly for the synset of the adjective cold
with the relation similarity. We prefer to use these
relations as they might allow us to preserve the
semantic information as much as possible during
the extension process. It is worth mentioning that
these relations were also found to be appropriate
for preserving the affective connotation by Vali-
tutti et al. (2004). Additionally, we use the rela-
tions hyponym and hyponym-instance to enrich the
seed set with semantically more specific synsets.
For instance, for the noun seed smell, we expand
the list with the hyponyms of its synset such as the
nouns bouquet, fragrance, fragrancy, redolence
and sweetness.
3.2.2 Cross-validation of Sensorial Model
After obtaining new synsets with the help ofWord-
Net relations in each bootstrapping cycle, we build
a five-class sense classifier over the seed synsets
defined by their glosses provided in WordNet.
Similarly to Dias et al. (2014), we assume that
the sense information of sensorial synsets is pre-
served in their definitions. Accordingly, we em-
ploy a support vector machine (SVM) (Boser et
al., 1992; Vapnik, 1998) model with second de-
gree polynomial kernel by representing the gloss
of each synset as a vector of lemmas weighted by
their counts. For each synset, its gloss is lemma-
tized by using Stanford Core NLP4 and cleaned
from the stop words. After each iteration cycle, we
perform a 10-fold cross-validation in the updated
seed list to detect the accuracy of the new sensorial
model. For each sense class, we continue iterating
and thereby expanding the seed list until the clas-
sifier accuracy steadily drops.
Table 1 lists the precision (P), recall (R) and
F1 values obtained for each sense after each it-
eration until the bootstrapping mechanism stops.
While the iteration number is provided in the first
column, the values under the last column group
present the micro-average of the resulting multi-
class classifier. The change in the performance
values of each class in each iteration reveals that
the number of iterations required to obtain the seed
lists varies for each sense. For instance, the F1
value of touch continues to increase until the fourth
cycle whereas hearing records a sharp decrease af-
ter the first iteration.
After the bootstrapping process, we create the
final lexicon by repeating the expansion for each
class until the optimal number of iterations is
reached. The last row of Table 1, labeled as Final,
demonstrates the accuracy of the classifier trained
and tested on the final lexicon, i.e., using the seeds
selected after iteration 2 for Sight, iteration 1 for
Hearing, iteration 3 for Taste and Smell and it-
eration 4 for Touch. According to F1 measure-
ments of each iteration, while hearing and taste
have a lower value for the final model, sight, smell
and touch have higher results. It should also be
noted that the micro-average of the F1 values of
the final model shows an increase when compared
to the third iteration, which has the highest av-
erage F1 value among the iterations. At the end
4http://nlp.stanford.edu/software/corenlp.
shtml
1514
of this step we have a seed synset list consisting
of 2572 synsets yielding the highest performance
when used to learn a sensorial model.
3.3 Sensorial Lexicon Construction Using
Corpus Statistics
After generating the seed lists consisting of synsets
for each sense category with the help of a set of
WordNet relations and a bootstrapping process, we
use corpus statistics to create our final sensorial
lexicon. More specifically, we exploit a proba-
bilistic approach based on the co-occurrence of
the seeds and the candidate lexical entries. Since
working on the synset level would raise the data
sparsity problem in synset tagged corpora such as
SemCor (Miller et al., 1993) and we need a cor-
pus that provides sufficient statistical information,
we migrate from synset level to lexical level. Ac-
cordingly, we treat each POS role of the same lem-
mas as a distinct seed and extract 4287 lemma-POS
pairs from 2572 synsets. In this section, we explain
the steps to construct our final sensorial lexicon in
detail.
3.3.1 Corpus and Candidate Words
As a corpus, we use a subset of English Giga-
Word 5th Edition released by Linguistic Data Con-
sortium (LDC)5. This resource is a collection of
almost 10 million English newswire documents
collected in recent years, whose content sums up
to nearly 5 billion words. The richly annotated
GigaWord data comprises automatic parses ob-
tained with the Stanford parser (Klein and Man-
ning, 2003) so that we easily have access to the
lemma and POS information of each word in the
resource. For the scope of this study, we work
on a randomly chosen subset that contains 79800
sentences and we define a co-occurrence event as
the co-existence of a candidate word and a seed
word within a window of 9 words(the candidate
word, 4 words to its left and 4 words to its right).
In this manner, we analyze the co-occurrence of
each unique lemma-POS pair in the corpuswith the
sense seeds. We eliminate the candidates which
have less than 5 co-occurrences with the sense cat-
egories.
3.3.2 Normalized Pointwise Mutual
Information
For the co-occurrence analysis of the candidate
words and seeds, we use pointwise mutual in-
formation (PMI), which is simply a measure of
5http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2011T07
association between the probability of the co-
occurrence of two events and their individual prob-
abilities when they are assumed to be independent
(Church and Hanks, 1990). PMI can be exploited
as a semantic similarity measure (Han et al., 2013)
and it is calculated as:
PMI(x, y) = log
[
p(x, y)
p(x)p(y)
]
(1)
To calculate the PMI value of a candidate word
and a specific sense, we consider p(x) as the proba-
bility of the candidate word to occur in the corpus.
Therefore, p(x) is calculated as p(x) = c(x)/N ,
where c(x) is the total count of the occurrences of
the candidate word x in the corpus and N is the to-
tal co-occurrence count of all words in the corpus.
Similarly, we calculate p(y) as the total occurrence
count of all the seeds for the sense considered (y).
p(y) can thus be formulated as c(y)/N . p(x,y) is
the probability of the co-occurrence of a candidate
word x with a sense event y.
Amajor shortcoming of PMI is its sensitivity for
low frequency data (Bouma, 2009). As one pos-
sible solution, the author introduces Normalized
PointwiseMutual Information (NPMI), which nor-
malizes the PMI values to the range (-1, +1) with
the following formula:
NPMI(x, y) =
PMI(x, y)
? log p(x, y) (2)
We adopt the proposed solution and calculate
NPMI values for each candidate word and five
sense events in the corpus. Sensicon covers 22,684
lemma-POS pairs and a score for each sense class
that denotes their association degrees.
4 Evaluation
To evaluate the performance of the sensorial clas-
sification and the quality of Sensicon, we first cre-
ated a gold standard with the help of a crowdsourc-
ing task. Then, we compared the decisions com-
ing from Sensicon against the gold standard. In
this section, we explain the annotation process that
we conducted and the evaluation technique that we
adopted in detail. We also provide a brief discus-
sion about the obtained results.
4.1 Crowdsourcing to Build a Gold Standard
The evaluation phase of Sensicon requires a gold
standard data to be able to conduct a meaningful
assessment. Since to our best knowledge there is
no resource with sensory associations of words or
1515
Sight Hearing Taste Smell Touch Micro-average
It# P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
1 .873 .506 .640 .893 .607 .723 .716 .983 .828 .900 .273 .419 .759 .320 .451 .780 .754 .729
2 .666 .890 .762 .829 .414 .552 .869 .929 .898 .746 .473 .579 .714 .439 .543 .791 .787 .772
3 .643 .878 .742 .863 .390 .538 .891 .909 .900 .667 .525 .588 .720 .482 .578 .796 .786 .776
4 .641 .869 .738 .832 .400 .540 .866 .888 .877 .704 .500 .585 .736 .477 .579 .784 .774 .765
5 .640 .869 .737 .832 .400 .540 .866 .888 .877 .704 .500 .585 .738 .474 .578 .784 .774 .764
Final .805 .827 .816 .840 .408 .549 .814 .942 .873 .685 .534 .600 .760 .582 .659 .800 .802 .790
Table 1: Bootstrapping cycles with validation results.
majority class 3 4 5 6 7 8 9 10
word 0 0.98 3.84 9.96 11.63 16.66 34.41 12.42
sentence 0.58 2.35 7.07 10.91 13.27 15.63 21.23 16.51
Table 2: Percentage of words and sentences in each majority class.
sentences, we designed our own annotation task
using the crowdsourcing service of CrowdFlower.
For the annotation task, we first compiled a col-
lection of sentences to be annotated. Then, we de-
signed two questions that the annotators were ex-
pected to answer for a given sentence. While the
first question is related to the sense association of a
whole sentence, the second asks the annotators to
collect a fine-grained gold standard for word-sense
associations.
We collected a dataset of 340 sentences consist-
ing of 300 advertisement slogans from 11 adver-
tisement categories (e.g., fashion, food, electron-
ics) and 40 story sentences from a story corpus. We
collected the slogans from various online resources
such as http://slogans.wikia.com/wiki and
http://www.adslogans.co.uk/. The story
corpus is generated as part of a dissertation re-
search (Alm, 2008) and it provides stories as a col-
lection of sentences.
In both resources, we first determined the can-
didate sentences that had at least five tokens and
contained at least one adjective, verb or noun. In
addition, we replaced the brand names in the ad-
vertisement slogans with X to prevent any bias.
For instance, the name of a well-known restaurant
in a slogan might cause a bias towards taste. Fi-
nally, the slogans used in the annotation task were
chosen randomly among the candidate sentences
by considering a balanced number of slogans from
each category. Similarly, 40 story sentences were
selected randomly among the candidate story sen-
tences. To give a more concrete idea, for our
dataset we obtained an advertisement slogan such
as ?X?s Sugar Frosted Flakes They?re Great!? or a
story sentence such as ?The ground is frozen, and
besides the snow has covered everything.?
In the crowdsourcing task we designed, the an-
notators were required to answer 2 questions for
a given sentence. In the first question, they were
asked to detect the human senses conveyed or di-
rectly described by a given sentence. To exemplify
these cases, we provided two examples such as ?I
saw the cat? that directly mentions the action of
seeing and ?The sun was shining on the blue wa-
ter.? that conveys the sense of sight by using vi-
sual descriptions or elements like ?blue? or ?shine?
which are notable for their visual properties. The
annotators were able to select more than one sense
for each sentence and together with the five senses
we provided another option as None which should
be selected when an annotator could not associate
a sentence with any sense. The second question
was devoted do determining word-sense associa-
tions. Here, the annotators were expected to asso-
ciate the words in each sentence with at least one
sense. Again, annotators could choose None for
every word that they could not confidently asso-
ciate with a sense.
The reliability of the annotators was evaluated
on the basis of 20 control sentences which were
highly associated with a specific sense and which
included at least one sensorial word. For instance,
for the control sentence ?The skin you love to
touch?, we only considered as reliable the anno-
tators who associated the sentence with touch and
the word touch with the sense touch6. Similarly,
for the slogan ?The most colourful name in cos-
metics.?, an annotator was expected to associate
the sentence with at least the sense sight and the
word colorful to at least the sense sight. The raters
who scored at least 70% accuracy on average on
6If the annotators gave additional answers to the expected
ones, we considered their answers as correct.
1516
the control questions for the two tasks were con-
sidered to be reliable. Each unit was annotated by
at least 10 reliable raters.
Similarly to Mohammad (2011) and ?zbal et al.
(2011), we calculated the majority class of each
annotated item to measure the agreement among
the annotators. Table 2 demonstrates the observed
agreement at both word and sentence level. Since
10 annotators participated in the task, the annota-
tions with a majority class greater than 5 can be
considered as reliable (?zbal et al., 2011). In-
deed, for 85.10% of the word annotations the ab-
solute majority agreed on the same decision, while
77.58% of the annotations in the sentence level
have majority class greater than 5. The high agree-
ment observed among the annotators in both cases
confirms the quality of the resulting gold standard
data.
In Table 3, we present the results of the anno-
tation task by providing the association percent-
age of each category with each sense, namely sight
(Si), hear (He), taste (Ta), smell (Sm) and touch
(To). As demonstrated in the table, while the sense
of sight can be observed in almost every advertise-
ment category and in story, smell and taste are very
rare. We observe that the story sentences invoke all
sensory modalities except taste, although the per-
centage of sentences annotated with smell is rela-
tively low. Similarly, personal care category has
an association with four of the senses while the
other categories have either very low or no asso-
ciation with some of the sense classes. Indeed, the
perceived sensorial effects in the sentences vary
according to the category such that the slogans in
the travel category are highly associated with sight
whereas the communication category is highly as-
sociated with hearing. While the connection of the
food and beverages categories with taste is very
high as expected, they have no association with the
sense of smell. This kind of analysis could be use-
ful for copywriters to decide which sensorymodal-
ities to invoke while creating a slogan for a specific
product category.
4.2 Evaluation Measures
Based on the annotation results of our crowdsourc-
ing task, we propose an evaluation technique con-
sidering that a lemma-POS or a sentence might be
associated with more than one sensory modalities.
Similar to the evaluation framework defined by
?zbal et al. (2011), we adapt the evaluation mea-
sures of SemEval-2007 English Lexical Substitu-
tion Task (McCarthy and Navigli, 2007), where
Category Si He Ta Sm To
personal care 49.36 10.75 0.00 13.29 26.58
travel 58.18 0.00 29.09 0.00 12.72
fashion 43.47 0.00 0.00 26.08 30.43
beauty 84.56 0.00 0.00 0.00 15.43
computing 32.25 59.13 0.00 0.00 8.60
food 0.00 5.46 94.53 0.00 0.00
beverages 22.68 0.00 59.79 0.00 17.52
communications 25.00 67.50 0.00 0.00 0.075
electronics 45.94 54.05 0.00 0.00 0.00
education 28.57 42.85 0.00 0.00 28.57
transport 61.81 38.18 0.00 0.00 0.00
story 58.37 20.81 0.00 7.23 13.57
Table 3: The categories of the annotated data and
their sense association percentages.
a system generates one or more possible substitu-
tions for a target word in a sentence preserving its
meaning.
For a given lemma-POS or a sentence, which
we will name as item in the rest of the section, we
allow our system to provide as many sensorial as-
sociations as it determines by using a specific lex-
icon. While evaluating a sense-item association of
a method, a best and an oot score are calculated by
considering the number of the annotators who as-
sociate that sense with the given item, the number
of the annotators who associate any sense with the
given item and the number of the senses the sys-
tem gives as an answer for that item. More specif-
ically, best scoring provides a credit for the best
answer for a given item by dividing it to the num-
ber of the answers of the system. oot scoring, on
the other hand, considers only a certain number of
system answers for a given item and does not di-
vide the credit to the total number of the answers.
Unlike the lexical substitution task, a limited set
of labels (i.e., 5 sense labels and none) are allowed
for the sensorial annotation of sentences or lemma-
POS pairs. For this reason, we reformulate out-of-
ten (oot) scoring used by McCarthy and Navigli
(2007) as out-of-two.
In Equation 3, best score for a given item i from
the set of items I, which consists of the items an-
notated with a specific sense by a majority of 5
annotators, is formulated where H
i
is the multiset
of gold standard sense associations for item i and
S
i
is the set of sense associations provided by the
system. oot scoring, as formulated in Equation 4,
accepts up to 2 sense associations s from the an-
swers of system S
i
for a given item i and the credit
is not divided by the number of the answers of the
1517
system.
best (i) =
?
s?S
i
freq (s ? H
i
)
|H
i
| ? |S
i
|
(3)
oot (i) =
?
s?S
i
freq (s ? H
i
)
|H
i
|
(4)
As formulated in Equation 5, to calculate the
precision of an item-sense association task with a
specific method, the sum of the scores (i.e., best
or oot) for each item is divided by the number of
items A, for which the method can provide an an-
swer. In recall, the denominator is the number of
the items in the gold standard for which an answer
is given by the annotators.
P =
?
i?A
score
i
|A|
R =
?
i?I
score
i
|I|
(5)
4.3 Evaluation Method
For the evaluation, we compare the accuracy of
a simple classifier based on Sensicon against two
baselines on a sense classification task both at
word and sentence level. To achieve that, we use
the gold standard that we obtain from the crowd-
sourcing task and the evaluation measures best and
oot. The lexicon-based classifier simply assigns
to each word in a sentence the sense values found
in the lexicon. The first baseline assigns the most
frequently annotated sensory modality, which is
sight, via crowdsourcing task with a float value of
1.0 to each lemma-POS pair in the sensorial lexi-
con. The second baseline instead builds the associ-
ations by using a Latent Semantic Analysis space
generated from the same subset of LDC that we ex-
ploit for constructing Sensicon. More specifically,
this baseline calculates the LSA similarities be-
tween each candidate lemma-POS pair and sense
class by taking the cosine similarity between the
vector of the target lemma-POS pair and the aver-
age of the vectors of the related sensory word (i.e.,
see, hear, touch, taste, and smell) for each possi-
ble POS tag. For instance, to get the association
score of a lemma-POS pair with the sense sight,
we first average the vectors of see (noun) and see
(verb) before calculating its cosine similarity with
the target lemma-POS pair.
For the first experiment, i.e., word-sense as-
sociation, we automatically associate the lemma-
POS pairs obtained from the annotated dataset with
senses by using i) Sensicon, ii) the most-frequent-
sense baseline (MFS), iii) the LSA baseline. To
achieve that, we lemmatize and POS tag each sen-
tence in the dataset by using Stanford Core NLP.
In the end, for each method and target word, we
obtain a list of senses sorted according to their
sensorial association values in decreasing order.
It is worth noting that we only consider the non-
negative sensorial associations for Sensicon and
both baselines. For instance, Sensicon associates
the noun wine with [smell, taste, sight]. In this
experiment, best scoring considers the associated
senses as the best answer, smell, taste, sight ac-
cording to the previous example, and calculates a
score with respect to the best answer in the gold
standard and the number of the senses in this an-
swer. Instead, oot scoring takes the first two an-
swers, smell and taste according to the previous
example, and assigns the score accordingly.
To determine the senses associated with a sen-
tence for the second experiment, we use a method
similar to the one proposed by Turney (2002). For
each sense, we simply calculate the average score
of the lemma-POS pairs in a sentence. We set a
threshold value of 0 to decide whether a sentence
is associated with a given sense. In this manner,
we obtain a sorted list of average sensory scores
for each sentence according to the three methods.
For instance, the classifier based on Sensicon as-
sociates the sentence Smash it to pieces, love it to
bits. with [touch, taste]. For the best score, only
touch would be considered, whereas oot would
consider both touch and taste.
4.4 Evaluation Results
In Table 4, we list the F1 values that we obtained
with the classifier using Sensicon and the two base-
lines (MFS and LSA) according to both best and
oot measures. In addition, we provide the perfor-
mance of Sensicon in two preliminary steps, before
bootstrapping (BB) and after bootstrapping (AB)
to observe the incremental progress of the lexicon
construction method. As can be observed from the
table, the best performance for both experiments is
achieved by Sensicon when compared against the
baselines.
While in the first experiment the lexicon gen-
erated after the bootstrapping step (AB) provides
a very similar performance to the final lexicon
according to the best measure, it can only build
sense associations for 69 lemmas out of 153 ap-
pearing in the gold standard. Instead, the final lex-
icon attempts to resolve 129 lemma-sense associa-
tions and results in a better recall value. Addition-
ally, AB yields a very high precision as expected,
1518
since it is created by a controlled semantical ex-
pansion from manually annotated sensorial words.
BB lexicon includes only 573 lemmas which are
collected from 277 synsets and we can not ob-
tain 2 sense association scores for oot in this lexi-
con since each lemma is associated with only one
sense with a value of 1. The LSA baseline yields
a very low performance in the best measure due to
its tendency to derive positive values for all sen-
sorial associations of a given lemma-POS tuple.
Another observed shortcoming of LSA is its fail-
ure to correlate the names of the colors with sight
while this association is explicit for the annotators.
On the other hand, LSA baseline significantly im-
proves the MFS baseline with a p-value of 0.0009
in oot measures. This result points out that even
though LSA provides very similar positive asso-
ciation values for almost all the sensory modali-
ties for a given item, the first two sensorial asso-
ciations with the highest values yield a better per-
formance on guessing the sensorial characteristics
of a lemma-POS. Nevertheless, Sensicon signifi-
cantly outperforms the LSA baseline in both best
and oot measures with the p-values of 0.0009 and
0.0189 respectively. The statistical significance
tests are conducted using one-sided bootstrap re-
sampling (Efron and Tibshirani, 1994).
Concerning the sentence classification experi-
ment, the classifier using Sensicon yields the high-
est performance in both measures. The very high
F1 value obtained with the oot scoring indicates
that the right answer for a sentence is included
in the first two decisions in many cases. Sensi-
con significantly outperforms the LSA baseline on
the best measure (p-value = 0.0069). On the other
hand, when systems are allowed to provide two an-
swers (oot), the performance of LSA comes close
to Sensicon in terms of F1 measure.
After the manual analysis of Sensicon and gold
standard data, we observe that the sensorial clas-
sification task could be nontrivial. For instance, a
story sentence ?He went to sleep again and snored
until the windows shook.? has been most fre-
quently annotated as hearing. While the sensorial-
lexicon classifier associates this sentence with
touch as the best answer, it can provide the cor-
rect association hearing as the second best answer.
To find out the best sensorial association for a sen-
tence, a classification method which exploits var-
ious aspects of sensorial elements in a sentence,
such as the number of sensorial words or their de-
pendencies, could be a better approach than using
only the average sensorial values.
Lemma Sentence
Model best oot best oot
Most-Frequent-Sense 33.33 33.33 38.90 38.90
LSA 18.80 70.38 53.44 76.51
Lexicon-BB 45.22 45.22 49.60 51.12
Lexicon-AB 55.85 55.85 59.89 63.21
Sensicon 55.86 80.13 69.76 80.73
Table 4: Evaluation results.
Based on our observations of the error cases,
we believe that synaesthesia, which is one of the
most common metaphoric transfers in language
(Williams, 1976), should be further explored for
sense classification. As an example observation,
the advertisement slogan ?100% pure squeezed
sunshine? is associated with touch as the best an-
swer by Sensicon and taste by LSA baseline while
it is most frequently annotated as sight in the
gold standard. This slogan is an example usage
of synaesthesia and metaphors in advertising lan-
guage. To clarify, a product from the category of
beverages, which might be assumed to have a taste
association, is described by a metaphorical substi-
tution of a taste-related noun, most probably the
name of a fruit, with a sight-related noun; sun-
shine. This metaphorical substitution, then used
as the object of a touch-related verb, to squeeze,
produces a synaesthetic expression with touch and
sight.
5 Conclusion
In this paper we have presented the construction
of Sensicon, a sensorial lexicon, which associates
words with sensory modalities. This novel aspect
of word semantics is captured by employing a two-
step strategy. First, we collected seed words by
using a bootstrapping approach based on a set of
WordNet relations. Then, we performed a cor-
pus based statistical analysis to produce the final
lexicon. Sensicon consists of 22,684 lemma-POS
pairs and their association degrees with five sen-
sory modalities. To the best of our knowledge,
this is the first systematic attempt to build a sen-
sorial lexicon and we believe that our contribution
constitutes a valid starting point for the commu-
nity to consider sensorial information conveyed by
text as a feature for various tasks and applications.
The results that we obtain by comparing our lexi-
con against the gold standard and two baselines are
promising even though not conclusive. The results
confirm the soundness of the proposed approach
for the construction of the lexicon and the useful-
1519
ness of the resource for text classification and pos-
sibly other computational applications.
Sensicon is publicly available upon request to
the authors so that the community can benefit from
it for relevant tasks. From a resource point of
view, we would like to explore the effect of us-
ing different kinds of WordNet relations during
the bootstrapping phase. It would also be interest-
ing to experiment with relations provided by other
resources such as ConceptNet (Liu and Singh,
2004), which is a semantic network containing
common sense, cultural and scientific knowledge.
We would also like to use the sensorial lexicon for
various applicative scenarios such as slanting ex-
isting text towards a specific sense with text modi-
fication. We believe that our resource could be ex-
tremely useful for automatic content personaliza-
tion according to user profiles. As an example, one
can imagine a system that automatically replaces
hearing based expressions with sight based ones in
pieces of texts for a hearing-impaired person. Au-
tomating the task of building sensorial associations
could also be beneficial for various tasks that need
linguistic creativity. For instance, copywriters can
take advantage of a system detecting the sensorial
load of a piece of text to generate more appropri-
ate advertisement slogans for specific product cat-
egories. Finally, we plan to investigate the impact
of using sensory information for metaphor detec-
tion and interpretation based on our observations
during the evaluation. For instance, the synaes-
thetic metaphor bittersweet symphony could be de-
tected by determining the sensorial characteriza-
tions of its components.
Acknowledgements
We would like to thank Daniele Pighin for his in-
sightful comments and valuable suggestions.
This work was partially supported by the PerTe
project (Trento RISE).
References
Ebba Cecilia Ovesdotter Alm. 2008. Affect in Text
and Speech. Ph.D. thesis, University of Illinois at
Urbana-Champaign.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. pages 86?90.
Association for Computational Linguistics.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC.
Douglas A. Bernstein. 2010. Essentials of Psychol-
ogy. PSY 113 General Psychology Series. Cengage
Learning.
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-
nik. 1992. A Training Algorithm for Optimal Mar-
gin Classifiers. In Proceedings of the 5th Annual
Workshop on Computational learning theory.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31?40.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22?29, March.
Max Coltheart. 1981. The mrc psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Ivan E. de Araujo, Edmund T. Rolls, Maria In?s Ve-
lazco, Christian Margot, and Isabelle Cayeux. 2005.
Cognitive modulation of olfactory processing. Neu-
ron, 46(4):671?679.
Ga?l Harry Dias, Mohammed Hasanuzzaman,
St?phane Ferrari, and Yann Mathet. 2014.
Tempowordnet for sentence time tagging. In
Proceedings of the Companion Publication of the
23rd International Conference on World Wide Web
Companion, WWW Companion ?14, pages 833?
838, Republic and Canton of Geneva, Switzerland.
InternationalWorldWideWeb Conferences Steering
Committee.
Bradley Efron and Robert J. Tibshirani. 1994. An in-
troduction to the bootstrap, volume 57. CRC press.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC, volume 6,
pages 417?422.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London.
Lushan Han, Tim Finin, Paul McNamee, Anupam
Joshi, and Yelena Yesha. 2013. Improving word
similarity by augmenting pmi with estimates of word
polysemy. Knowledge and Data Engineering, IEEE
Transactions on, 25(6):1307?1322.
Thomas Kjeller Johansen. 1997. Aristotle on the
Sense-organs. Cambridge University Press.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1520
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Hugo Liu and Push Singh. 2004. Conceptnet - a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226, October.
Asifa Majid and Stephen C. Levinson. 2011. The
senses in language and culture. The Senses and So-
ciety, 6(1):5?18.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the 4th International Workshop on
Semantic Evaluations, pages 48?53. Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the workshop on Human Language
Technology, HLT ?93, pages 303?308, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, pages 26?34. Association
for Computational Linguistics.
Saif M. Mohammad. 2011. Colourful language: Mea-
suring word-colour associations. In Proceedings of
the 2nd Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 97?106. Association
for Computational Linguistics.
G?zde ?zbal, Carlo Strapparava, Rada Mihalcea, and
Daniele Pighin. 2011. A comparison of unsuper-
vised methods to associate colors with words. In Af-
fective Computing and Intelligent Interaction, pages
42?51. Springer.
Jessica Perrie, Aminul Islam, Evangelos Milios, and
Vlado Keselj. 2013. Using google n-grams to ex-
pand word-emotion association lexicon. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 137?148. Springer.
Mario Pricken. 2008. Creative Advertising Ideas
and Techniques from the World?s Best Campaigns.
Thames & Hudson, 2nd edition.
Raul Rodriguez-Esteban and Andrey Rzhetsky. 2008.
Six senses in the literature. The bleak sensory land-
scape of biomedical texts. EMBO reports, 9(3):212?
215, March.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension ofWordNet.
In Proceedings of LREC, volume 4, pages 1083?
1086.
Sara Tonelli and Daniele Pighin. 2009. New features
for framenet - wordnet mapping. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL?09), Boulder, CO,
USA.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th an-
nual meeting on association for computational lin-
guistics, pages 417?424. Association for Computa-
tional Linguistics.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology Journal, 2(1):61?83.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Joseph M. Williams. 1976. Synaesthetic adjectives: A
possible law of semantic change. Language, pages
461?478.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007. Building emotion lexicon from weblog
corpora. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstration
Sessions, pages 133?136. Association for Computa-
tional Linguistics.
1521
Kernel Methods for Minimally Supervised WSD
Claudio Giuliano?
Fondazione Bruno Kessler ? IRST
Alfio Massimiliano Gliozzo??
Fondazione Bruno Kessler ? IRST
Carlo Strapparava?
Fondazione Bruno Kessler ? IRST
We present a semi-supervised technique for word sense disambiguation that exploits external
knowledge acquired in an unsupervised manner. In particular, we use a combination of basic
kernel functions to independently estimate syntagmatic and domain similarity, building a set of
word-expert classifiers that share a common domain model acquired from a large corpus of un-
labeled data. The results show that the proposed approach achieves state-of-the-art performance
on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although
it uses a considerably smaller number of training examples than other methods.
1. Introduction
A significant challenge in many natural language processing tasks is to reduce the need
for labeled training data while maintaining an acceptable performance. This is espe-
cially true for word sense disambiguation (WSD) because when moving from the some-
what artificial lexical-sample task to the more realistic all-words task it is practically
impossible to collect a large number of training examples for each word sense. Thus,
many supervised approaches, explicitly designed for the lexical-sample task, cannot be
applied to the all-words task, even though they exhibit excellent performance. This has
led to the somewhat paradoxical situation in which completely different methods have
been developed for the two tasks, although they represent two sides of the same coin.
To address this problem, in recent work we presented a semi-supervised approach
based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo,
Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006). In particular,
we explored the following research directions: (1) independently modeling domain and
syntagmatic aspects of sense distinction to improve feature representativeness; and
(2) exploiting external knowledge acquired from unlabeled data, with the purpose of
drastically reducing the amount of labeled training data. The first direction is based on
the linguistic assumption that syntagmatic and domain (associative) relations are crucial
for representing sense distinctions, but they are originated by different phenomena.
Regarding the second direction, one can hope to obtain a more accurate prediction
? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: giuliano@fbk.eu.
?? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: gliozzo@fbk.eu.
? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: strappa@fbk.eu.
Submission received: 23 December 2006; revised submission received: 28 February 2008; accepted for
publication: 17 April 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
by taking into account unlabeled data relevant to the learning problem (Chapelle,
Scho?lkopf, and Zien 2006). As a matter of fact, to test this hypothesis, most of the lexical
sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large
amount of unlabeled training data, as well as the usual labeled training data. However,
at that time, we were the only team to use the unlabeled data (Strapparava, Gliozzo,
and Giuliano 2004).
In this article, we review our technique that combines domain and syntagmatic
information in order to define a complete kernel for WSD. The rest of the article is
organized as follows. In Section 2, we provide a general introduction to the kernel
methods, in which we give the basis for understanding our approach. Exploiting kernel
methods, we can define and combine individual kernels representing information from
different sources in a principled way. After this introductory section, in Section 3 we
present the kernels that we developed for WSD. This includes a detailed description
of the individual kernels and the way we define the composite ones. We present our
experiments in Section 4. The results obtained on a range of lexical-sample tasks and on
the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our
approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclu-
sions and some directions for future research.
2. Kernel Methods
Kernel methods are a popular machine learning approach within the natural lan-
guage processing community. They are theoretically well founded in statistical learn-
ing theory and have shown good empirical results in many applications (Vapnik 1999;
Cristianini and Shawe-Taylor 2000; Scho?lkopf and Smola 2002; Shawe-Taylor and
Cristianini 2004).
The strategy adopted by kernel methods consists of splitting the learning problem
into two parts. They first embed the input data in a suitable feature space, and then
use a linear algorithm to discover nonlinear patterns in the input space. Typically, the
mapping is performed implicitly by a so-called kernel function. The kernel function
is a similarity measure between the input data that depends exclusively on the specific
data type and domain. A typical similarity function is the inner product between feature
vectors. Characterizing the similarity of the inputs plays a crucial role in determining
the success or failure of the learning algorithm, and it is one of the central questions in
the field of machine learning.
Formally, the kernel is a function K : X? X ? R that takes as input two data objects
(e.g., vectors, texts, or parse trees) and outputs a real number characterizing their
similarity, with the property that the function is symmetric and positive semi-definite.
That is, for all xi, xj ? X satisfies
K(xi, xj) = ??(xi),?(xj)? (1)
where ? is an (implicit) mapping from X to an (inner product) feature space F .
Kernels are used inside learning algorithms such as support vector machines (SVM)
or kernel perceptrons as the interface between the algorithm and the data. The kernel
function is then the only domain specific element of the system, while the learning
algorithm is a general purpose component.
The idea behind the SVM (one of the best known kernel-based learning algorithms)
is to map the set of training data into a high-dimensional feature space F via a mapping
function? : X ? F , and construct a separating hyperplane with maximummargin (i.e.,
514
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
the minimum distance between the hyperplane and data points) in that space. The use
of an appropriate non-linear transformation ? of the input yields a nonlinear decision
boundary in the input space. Kernel functions make possible the use of feature spaces
with an exponential or even infinite number of dimensions. Instead of performing the
explicit feature mapping ?, one can use a kernel function, which permits the (efficient)
computation of inner products in high-dimensional feature spaces without explicitly
carrying out the mapping ?. This is called the kernel trick in the machine learning
literature (Boser, Guyon, and Vapnik 1992).
Finally, we point out the theoretical tools required to create new kernels, and com-
bine individual kernels to form composite ones. Of course, not every similarity function
is a valid kernel because, by definition, kernels should be equivalent to some inner
product in a feature space. The function K : X? X ? R is a valid kernel provided that
its kernel matrices1 are positive semi-definite2 for all training sets S = {x1, ..., xl}, the
so-called finitely positive semi-definite property. Note that defining similarity measures
by means of kernels may be more intuitive than performing the explicit mapping in the
feature space. Furthermore, this formulation does not require the set X to be a vector
space: for example, we shall define kernels that take strings as input.
This result is not only useful because it opens new perspectives to define kernel
functions that only implicitly correspond to a feature mapping ?. Another consequence
is that it can be used to prove a set of rules for combining basic kernels to obtain compos-
ite ones. This will allow us to integrate heterogeneous sources of information in a simple
and effective way.We shall use the following properties of kernels to define our compos-
ite kernels. Let k1 and k2 be kernels over X? X; then the following functions are kernels:
 k(xi, xj) = k1(xi, xj)+ k2(xi, xj)
 k(xi, xj) = c ? k1(xi, xj), c ? R+
 k(xi, xj) =
k1(xi,xj )?
k1(xi,xi )?k1(xj,xj )
(normalization)
In summary, we can define a kernel function by following different strategies: (1)
providing an explicit feature mapping ? : X ? Rn; (2) defining a similarity function
that is symmetric and positive semi-definite; and (3) composing different valid kernels,
using the closure properties of kernels. This forms the basis for the approach described
in the following section.
3. Kernel Methods for WSD
Our approach toWSD consists of representing linguistic phenomena independently and
then defining a combinationmethod to integrate them. As described in the previous sec-
tion, the kernel function is the only task-specific component of the learning algorithm.
Thus, to develop a WSD system, we only need to define appropriate kernel functions to
represent the domain and syntagmatic aspects of sense distinction and, second, exploit
the properties of kernel functions to define a composite kernel to combine and extend
the individual kernels.
The resulting WSD system consists of two families of kernels: the domain and the
syntagmatic kernels. The former family, described in Section 3.1, models the domain
1 Given a set of vectors S = {x1, ..., xl}, the kernel matrix K is defined as the l? lmatrix Kwhose entries
are Kij = k(xi, xj ) = ??(xi ),?(xj )?, where k is a kernel function that evaluates the inner products in a
feature space with feature map ?.
2 A symmetric matrix is positive semi-definite if its eigenvalues are all non-negative. Actually, as we will
see in Section 3.2 using Proposition 1, it is quite easy to verify this property.
515
Computational Linguistics Volume 35, Number 4
Table 1
An example of a domain matrix.
Medicine Computer Science
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-of-
words kernel (KBoW). The latter, described in Section 3.2, represents the syntagmatic
aspects of sense distinction; it is composed of the collocation kernel (KColl) and the part-
of-speech kernel (KPoS). Finally, Section 3.3 describes the composite kernel for WSD.
3.1 Domain Kernels
It has been shown that domain information is fundamental for WSD (Magnini et al
2002). For instance, the (domain) polysemy between the computer science and the
medicine senses of the word virus can be solved by considering the domain of the
context in which it appears. Gliozzo, Strapparava, and Dagan (2004) proposed a WSD
method that exploits only domain information.
In the context of kernel methods, domain information can be exploited by defining
a kernel function that estimates the domain similarity between the contexts of the
words to be disambiguated. The simplest method to estimate the domain similarity
between two texts is to compute the cosine similarity of their vector representations
in the vector space model (VSM). The VSM is a k-dimensional space Rk, in which the
text tj is represented by a vector tj, where the i
th component is the term frequency of
the term wi in tj. However, such an approach does not deal well with lexical variability
and ambiguity. For instance, despite the fact that the sentences He is affected by AIDS
and HIV is a virus express closely-related concepts, their similarity is zero in the VSM
because they have no words in common (they are represented by orthogonal vectors).
On the other hand, due to the ambiguity of the word virus, the similarity between the
sentences The laptop has been infected by a virus and HIV is a virus is greater than zero,
even though they convey very different messages.
To overcome this problem, we introduce the domain model (DM) and show how to
use it to define a domain VSM in which texts and terms are represented in a uniform
way. A DM is composed of soft clusters of terms. Each cluster represents a semantic
domain, that is, a set of terms that often co-occur in texts having similar topics. A DM
is represented by a k? k? rectangular matrix D, containing the degree of association
among terms and domains, as illustrated in Table 1.
The matrix D is used to define a function D : Rk ? Rk
?
, that maps the vector tj
represented in the standard VSM into the vector t?j in the domain VSM. D is defined
as follows:3
D(tj) = tj(I
IDFD) = t?j (2)
3 In Wong, Ziarko, and Wong (1985), Equation (2) is used to define a generalized vector space model, of
which the domain VSM is a particular instance.
516
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where tj is represented as a row vector, I
IDF is a k? k diagonal matrix such that iIDFi,i =
IDF(wi), and IDF(wi) is the inverse document frequency of wi.
In the domain space, the similarity is estimated by taking into account second order
relations among terms. For example, the similarity of the two sentences He is affected
by AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus are
strongly associated with the medicine domain.
A DM can be estimated frommanually constructed lexical resources, such as Word-
Net Domains (Magnini and Cavaglia` 2000), or by performing a term-clustering process
on a (large) corpus. However, the second approach is more attractive because it allows
us to automatically acquire DMs for different languages and domains.
In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposi-
tion (SVD) to acquire DMs from a corpus represented by its term-by-document matrix
T, in a unsupervised way.4 SVD decomposes the term-by-document matrix T into
three matrixes T  V?k?UT, where V and U are orthogonal matrices (i.e., VTV = I and
UTU = I) whose columns are the eigenvectors of TTT and TTT, respectively, and ?k?
is the diagonal k? k matrix containing the highest k?  k eigenvalues of T, and all the
remaining elements set to 0. The parameter k? is the dimensionality of the domain VSM
and can be fixed in advance. Under this setting, we define the domain matrix D as
follows:
D = INV
?
?k? (3)
where IN is a diagonal matrix such that iNi,i =
1
?
? w?i ,
w?i ?
, w?i is the i
th row of the matrix
V
?
?k? .
5
Note that in this case, with respect to Table 1, the domains are represented by the
columns of the matrix D and they do not have an explicit name. By using a small
number of domains, we can define a very compact representation of the DM and, con-
sequently, reduce the memory requirements while preserving most of the information.
There exist very efficient algorithms to perform the SVD process on sparse matrices,
allowing us to perform this operation on large corpora in a very limited time and with
reduced memory requirements.6
Therefore, we can define the domain kernel to estimate the domain similarity
between the contexts of the words to be disambiguated. It is a variant of the latent
semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to
define an explicit mapping D : Rk ? Rk
?
from the classical VSM into the domain VSM.
The domain kernel is explicitly defined as follows:
KD(ti, tj) = ?D(ti),D(tj)? (4)
where D is the domain mapping defined in Equation (2).
4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic
indexing of documents in large corpora (Deerwester et al 1990).
5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space
(Deerwester et al 1990). The only difference in our formulation is that the vectors representing the terms
in the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by
matrix IIDF. Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widely
used in information retrieval.
6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows
us to perform this step in a few minutes even for large corpora. It can be downloaded from
http://tedlab.mit.edu/?dr/SVDLIBC/.
517
Computational Linguistics Volume 35, Number 4
A standard approach for detecting topic (domain) similarity is to extract bag-of-
words features from a wide window of text around the words to be disambiguated.
Based on this representation, we define a linear kernel called the bag-of-words kernel
(KBoW). KBoW is a particular case of the domain kernel in which D = I in Equation (2),
where I is the identity matrix. The BoW kernel does not require a DM; therefore, it
can be applied to the strictly supervised settings, in which external knowledge is not
available.
To summarize, the domain kernel allows us to plug external knowledge into the
supervised learning process; it will be compared and combined with the standard bag-
of-words approach in Section 4. In the following section, we shall see that domain
models are also useful for defining soft-matching collocation kernels.
3.2 Syntagmatic Kernels
Collocations (such as bigrams and trigrams) extracted from the local context of the word
to be disambiguated are typically used to capture syntagmatic relations (Yarowsky
1994). However, traditional approaches to WSD fail to represent non-contiguous or
shifted collocations, and fail to consider lexical variability. For example, suppose we
have to disambiguate the verb to score in the sentence Ronaldo scored the first goal, given
the labeled example The football player scored two goals in the second half as training. A
traditional approach has no clues to return the right answer because the two sentences
have no features in common.
The use of kernels on strings allows us to overcome the aforementioned problems
by representing (non-contiguous) collocations and exploiting external lexical knowl-
edge sources to define non-zero measures of similarity between words (soft-matching
criteria). In this formulation, words taken in their context are compared by kernels that
sum the number of common (non-contiguous) collocations of words, considering lexical
variability, and part-of-speech tags, avoiding an explicit feature mapping that would
lead to an exponential number of features.
String kernels (or sequence kernels) are a family of kernel functions developed
to compute the inner product among images of strings in high-dimensional feature
space using dynamic programming techniques. The gap-weighted subsequences kernel
is one of the most general types of kernel based on sequences. Roughly speaking,
it compares two strings by means of the number of contiguous and non-contiguous
substrings of a given length they have in common. Non-contiguous occurrences are
penalized according to the number of gaps they contain. Formally, let ? be an al-
phabet of |?| symbols, and s = s1s2 . . . s|s| be a finite sequence over ? (i.e., si ? ?, 1 
i  |s|). Let i = [i1, i2, . . . , in], with 1  i1 < i2 < . . . < in  |s|, be a subset of the indices
in s; we will denote as s[i] ? ?n the subsequence si1si2 . . . sin . Note that s[i] does not
necessarily form a contiguous subsequence of s; for example, if s is the sequence
?Ronaldo scored the first goal? and i = [2, 5], then s[i] is ?scored goal?. The length
spanned by s[i] in s is l(i) = in ? i1 + 1. The feature space associated with the gap-
weighted subsequences kernel of length n is indexed by I = ?n, with the embedding
given by
?nu(s) =
?
i:u=s[i]
?l(i),u ? ?n (5)
518
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where 0<? 1 is the decay factor used to penalize non-contiguous subsequences.7 The
associate kernel is defined as
Kn(s, t) = ??n(s),?n(t)? =
?
u??n
?nu(s)?
n
u(t) (6)
An explicit computation of Equation (6) is unfeasible even for small values of n.
To evaluate Kn more efficiently, we use the recursive formulation based on a dynamic
programming implementation (Lodhi et al 2002; Saunders, Tschach, and Shawe-Taylor
2002; Cancedda et al 2003). It is defined in the following equations:
K?0(s, t) = 1,?s, t (7)
K?i (s, t) = 0, if min(|s|, |t|) < i (8)
K??i (s, t) = 0, if min(|s|, |t|) < i (9)
K??i (sx, ty) =
{
?K??i (sx, t) if x = y;
?K??i (sx, t)+ ?
2K?i?1(s, t) otherwise.
(10)
K?i (sx, t) = ?K
?
i (s, t)+ K
??
i (sx, t) (11)
Kn(s, t) = 0, if min(|s|, |t|) < n (12)
Kn(sx, t) = Kn(s, t)+
?
j:tj=x
?2K?n?1(s, t[1 : j? 1]) (13)
where K?n and K
??
n are auxiliary functions with a similar definition to Kn used to facilitate
the computation. Based on these definitions, Kn can be computed in O(n|s||t|). Using
this recursive definition, it turns out that computing all kernel values for subsequences
of lengths up to n is not significantly more costly than computing the kernel for n only.
The syntagmatic kernel is defined as a sum of gap-weighted subsequences kernels
that operate at word and part-of-speech tag level. In particular, following the approach
proposed by Cancedda et al (2003), it is possible to adapt sequence kernels to operate
at word level by instancing the alphabet ? with the vocabulary V = {w1,w2, . . . ,wk}.
Moreover, we restrict the generic definition of the gap-weighted subsequences kernel
to recognize collocations in the local context of a specified word. The resulting kernel,
called the n-gram collocation kernel (KnColl), operates on sequences of lemmata around
a specified word l0 (i.e., l?3, l?2, l?1, l0, l+1, l+2, l+3). This formulation allows us to
estimate the number of common (sparse) subsequences of lemmata (i.e., collocations)
between two examples, in order to capture syntagmatic similarity. Analogously, we
define the part-of-speech kernel (KnPoS) to operate on sequences of part-of-speech tags
p?3, p?2, p?1, p0, p+1, p+2, p+3, where p0 is the part-of-speech tag of l0.
The collocation kernel and the part-of-speech kernel are defined by Equations (14)
and (15), respectively.
KColl(s, t) =
n
?
l=1
KlColl(s, t) (14)
7 Notice that by choosing ? = 1, sparse subsequences are not penalized. On the other hand, the kernel does
not take into account sparse subsequences with ? ? 0.
519
Computational Linguistics Volume 35, Number 4
KPoS(s, t) =
n
?
l=1
KlPoS(s, t) (15)
Both kernels depend on the parameter n, the length of the non-contiguous subse-
quences, and ?, the decay factor. For example, K2Coll allows us to represent all (sparse)
bigrams in the local context of a word. Finally, the syntagmatic kernel is defined as
KSynt(s, t) = KColl(s, t)+ KPoS(s, t) (16)
In the preceding definition, only exact word-matches contribute to the similarity.
To solve this problem, external lexical knowledge is fed into the supervised learning
process, allowing us to define the soft-matching collocation kernel. In particular, we de-
fine two alternative soft-matching criteria by exploiting synonymy relations inWordNet
and DMs acquired from corpora. Both criteria are based on the assumption that every
word in a sentence can be substituted by another preserving the original meaning, if
these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related
words). For example, if we consider as equivalent the terms Ronaldo and football player,
then the sentence The football player scores the first goal is equivalent to Ronaldo scores the
first goal, providing a strong evidence to disambiguate the verb to score in the second
sentence.
Following the approach proposed by Shawe-Taylor and Cristianini (2004), the soft-
matching gap-weighted subsequences kernel is now calculated recursively using Equa-
tions (7)?(9), (11), and (12), replacing Equation (10) by the equation:
K??i (sx, ty) = ?K
??
i (sx, t)+ ?
2axyK
?
i?1(s, t),?x, y (17)
and modifying Equation (13) to:
Kn(sx, t) = Kn(s, t)+
|t|
?
j
?2axtjK
?
n?1(s, t[1 : j? 1]) (18)
where axy are entries in a similarity matrix A between terms. In order to ensure that the
resulting kernel is still valid, Amust be positive semi-definite.
In the following sections, we describe the two alternative soft-matching criteria
based on WordNet Synonymy and Domain Proximity, respectively. To show that the
similarity matrices are positive semi-definite, we use the following result.
Proposition 1
A matrix A is positive semi-definite if and only if A = BTB for some real matrix B.
The proof is given in Shawe-Taylor and Cristianini (2004).
WordNet Synonymy. The first soft-matching criterion is based on WordNet8 to define
a similarity matrix between words. In particular, we substitute two words if they are
synonyms. To this end, a word is represented as vector whose dimensions are associated
8 We used WordNet 1.7.1 and MultiWordNet for English and Italian experiments, respectively.
520
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
with the synsets. Formally, we define the term-by-synset matrix S as the matrix whose
rows are indexed by the terms and whose columns are indexed by the synsets. The
(i, j)th entry of S is 1 if the synset sj contains the term wi; 0 otherwise. The matrix
S gives rise to the similarity matrix A = SST between terms. Because A can be re-
written as A = (ST )TST = BTB, it follows directly from Proposition 1 that it is positive
semi-definite.
Domain Proximity. The second soft-matching criterion exploits the domain models intro-
duced in Section 3.1 to define a similarity matrix between words. Once a DM has been
defined by the matrixD, the domain space is a k? dimensional space, in which both texts
and terms are represented by means of domain vectors, that is, vectors representing the
domain relevances among the linguistic object and each domain. The domain vector w?i
for the term wi ? V is the ith row of D, where V = {w1,w2, . . . ,wk} is the vocabulary of
the corpus. The term-by-domain matrix D gives rise to the similarity matrix A = DDT
between terms. It follows by Proposition 1 that A is positive semi-definite.
We shall show that the syntagmatic kernel is more effective than standard bigrams
and trigrams of lemmata and part-of-speech tags typically used as features in WSD.
3.3 Composite Kernel
Having defined all the individual kernels representing syntagmatic and domain aspects
of sense distinction, we can define the composite kernel to combine and extend the
individual kernels. The closure properties of the kernel functions allows us to define
the composite kernel as
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj, xj)Kl(xi, xi)
(19)
where Kl is a valid individual kernel. The individual kernels are normalized?this plays
an important role in allowing us to integrate information from heterogeneous feature
spaces.
Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and
Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effec-
tiveness of combining kernels in this way: The composite kernel consistently improves
the performance of the individual ones. In addition, this formulation allows us to
evaluate the individual contribution of each information source.
In order to show the effectiveness of the proposed domain model in supervised
learning, we defined twoWSD kernels, Kwsd and K
?
wsd. They are completely specified by
the n individual kernels that compose them in Equation (19).
Kwsd is composed by KColl, KPoS, and KBoW ;
K?wsd is composed by KColl, KPoS, KBoW , and KD.
The only difference between the two is that K?wsd uses the domain kernel KD to exploit
external knowledge while Kwsd only uses the labeled training data.
521
Computational Linguistics Volume 35, Number 4
4. Evaluation
Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds
2004). First of all, we conducted a preliminary set of experiments on the Catalan,
English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.
Second, in order to show the general applicability of the proposed method, we evalu-
ated the system on the English all-words task; the results are presented in Section 4.2.
All the experiments were performed using the SVM package (Chang and Lin 2001)
customized to embed our own kernels. The parameters were optimized by five-fold
cross-validation on the training set.
4.1 Lexical-Sample Tasks
In this section, we report the evaluation of our method on the Catalan, English, Italian,
and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004). Table 2
describes the tasks we have considered. For each task, it summarizes the number of
words to be disambiguated, the mean polysemy, the size of the labeled training set,
the size of the test set, and the size of the unlabeled training set, respectively. For the
Catalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corpora
made available by the task organizers. For the English task, we used a DM acquired
from the British National Corpus (BNC) as the task organizers have not provided
any unlabeled training data. The objectives of these experiments are to (a) estimate
the impact of different knowledge sources in WSD; (b) study the effectiveness of the
kernel combination; (c) understand the benefits of plugging external information in a
supervised framework; and (d) verify the portability of our methodology to different
languages.
4.1.1 Results. Table 3 reports the results of the individual kernels KBoW , KD, KColl, and
KPoS and their combinations Kwsd and K
?
wsd (the baselines for the tasks are reported in
Table 5). In our experiments, the parameters n and ? (see Equation (5)) are optimized
by five-fold cross-validation. For KnColl, we obtained the best results with n = 2 and
? = 0.5. For KnPoS, n = 3 and ? ? 0. The domain cardinality k
? was set to 50. Table 4
shows the performance of the syntagmatic kernel in different configurations: hard and
soft matching. As a baseline, we report the result of a standard approach consisting of
explicit bigrams and trigrams of words and part-of-speech tags around the words to
be disambiguated (Yarowsky 1994). We evaluated the impact of the domain kernel on
the overall performance by comparing the learning curves of K?wsd and Kwsd on the four
lexical-sample tasks. Figure 1 shows the results of our experiments. The points of the
learning curves are obtained by sampling the same percentage of training examples for
Table 2
Description of the lexical-sample tasks of Senseval-3.
Task #w mean polysemy #train #test #unlab
Catalan 27 3.11 4,469 2,253 23,935
English 57 6.47 7,860 3,944 -
Italian 45 6.30 5,145 2,439 74,788
Spanish 46 3.30 8,430 4,195 61,252
522
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
Table 3
The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, and
Spanish lexical-sample tasks of Semeval-3.
Kernel Catalan English Italian Spanish
KBoW 81.3 63.7 43.3 78.2
KD 85.2 65.5 44.5 84.4
KColl 84.2 68.5 54.0 83.6
KPoS 79.6 64.0 44.4 79.5
Kwsd 85.2 69.7 53.1 84.2
K?wsd 89.0 73.3 61.3 88.2
Table 4
Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
Method Catalan English Italian Spanish
Bigrams and trigrams 82.6 67.3 51.0 81.9
Hard matching 83.8 67.7 51.9 82.9
Soft matching (WordNet) - 67.3 51.3 -
Soft matching (Domain proximity) 84.2 68.5 54.0 83.6
each word. Finally, Table 5 summarizes the results we obtained, providing a comparison
with the state of the art.
4.1.2 Discussion. Table 3 shows that domain information and syntagmatic information
are crucial for WSD, and their combination significantly outperforms the individual
kernels, showing the effectiveness of the kernel combination method.
In addition, the domain kernel KD outperforms the bag-of-words kernel KBoW ,
and the composite kernel K?wsd that makes use of domain information outperforms the
one Kwsd based only on the labeled training data, demonstrating our assumption (see
Section 3).
Table 4 shows that the syntagmatic kernel outperforms the baseline (bigrams and
trigrams) in any configuration (hard-/soft-matching). The soft-matching criteria further
improve the classification performance. It is interesting to note that the domain proxim-
ity obtained better results thanWordNet synonymy (note that we do not have a Catalan
or a Spanish WordNet). The different results observed for Italian and English using
the domain proximity soft-matching criterion are probably due to the small size of the
unlabeled English corpus.
Figure 1 shows that K?wsd outperforms Kwsd on all lexical sample tasks, even with a
small number of examples. It is worth noting, as reported in Table 5, that K?wsd achieves
the same performance as Kwsd using about half of the labeled training data. This result
shows that the proposed semi-supervised learning approach consisting of acquiring
domain models from unlabeled corpora is effective, as it allows us to drastically reduce
the amount of labeled training data and provide a viable solution for the knowledge
acquisition bottleneck problem in WSD.
To the best of our knowledge, K?wsd turns out to be the best system for all the tested
tasks of Senseval-3, further improving the state of the art by 0.4% to 8.2% for English
and Italian, respectively. Finally, we have demonstrated the language independency
523
Computational Linguistics Volume 35, Number 4
Figure 1
From left to right, top to bottom, learning curves for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
of our approach. The DMs have been acquired for different languages from different
unlabeled corpora by adopting exactly the same methodology, without requiring any
external lexical resource or ad hoc rule.
4.2 All-Words Task
Encouraged by the excellent results obtained on the lexical-sample tasks, we evaluated
our approach on the all-words task, in which a very small amount of labeled training
Table 5
Comparative evaluation on the lexical sample tasks.
Task MF Agreement BEST Kwsd K
?
wsd DM+ % of training
Catalan 66.3 93.1 85.2 85.2 89.0 3.8 46
English 55.2 67.3 72.9 69.7 73.3 3.6 54
Italian 18.0 89.0 53.1 53.1 61.3 8.2 51
Spanish 67.7 85.3 84.2 84.2 88.2 4.0 50
Columns report: theMost Frequent baseline, the inter-annotator agreement, the F1 of the best system
at Senseval-3, the F1 of Kwsd, the F1 of K
?
wsd, DM+ (the improvement due to DM, i.e., K
?
wsd ? Kwsd),
and the percentage of sense-tagged examples required by K?wsd to achieve the same performance
as Kwsd with full training.
524
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
Table 6
The performance (F1) of the basic kernels and composite kernels on the English all-words task of
Senseval-3.
basic kernels composite kernels
KbncD K
sem
D KBoW KPoS KColl Kwsd K?
bnc
wsd K?
sem
wsd
F1 63.0 63.2 63.2 63.4 64.0 64.4 65.0 65.2
data is typically available. We performed the evaluation on the English all-words task
of Senseval-3 (Snyder and Palmer 2004). The test set was extracted from twoWall Street
Journal articles and one text from the Brown Corpus. The test set consists of 945 words
(2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses. The inter-
annotator agreement rate in the preparation of the corpus was approximately 72.5%.
Themost frequent (MF) baseline using the firstWordNet sense heuristic obtained 60.9%.
We have trained and tested the system exploiting the following resources: (1) Word-
Net 1.7.1 as sense repository; (2) SemCor,9 considering only those words appearing
in the Senseval-3 all-words data set?we extracted about 61,700 tagged examples that
constitute the only labeled training set exploited by the system; and (3) the BNC, from
which we extracted the unlabeled training data.
4.2.1 Results.We trained 734 word-expert classifiers on the SemCor corpus. The labeled
examples for each classifier range from a minimum of one example to a maximum
of 2,275 examples. We return a random sense for those words that have no training
examples in SemCor.10 We have acquired two DMs, one from the BNC (i.e., K?bncD ; the
same we used in the lexical-sample task) and one from SemCor (i.e., K?semD ), obtaining a
slightly better performance with the latter.
Table 6 shows the performance of the individual kernels KBoW , KD, KColl, and KPoS,
and their composite kernels Kwsd, K?
bnc
D and K?
sem
D .
Since for 210 words in the test set we have no training examples, to better under-
stand the results obtained, we performed an evaluation on the subset of the test set for
which at least one training example is available in SemCor. Evaluating only on these
words the performance increases from 65.2% to 70.0%, and the most frequent baseline
becomes 65.7%. Tables 7 and 8 present a more detailed analysis that considers results
grouped according to the amount of training available and the mean polysemy of the
words in the test set, excluding from the data set the monosemous words. Table 7 shows
the results (F1) of K?semwsd at different ranges of polysemy. Table 8 presents the results (F1)
of K?semwsd on those words that have a given number of training examples. This evaluation
is limited to the best composite kernel K?semwsd.
4.2.2 Discussion. We compared our approach with the three best systems that par-
ticipated in the English all-words task of Senseval-3. The best system (Decadt et al
2004) has comparable performance (65.2) to ours; however, it uses a larger training set
composed of 563,129 sense-tagged words. The training corpus was built by merging
9 Texts semantically annotated with WordNet 1.6 senses (created at Princeton University), and
automatically mapped to WordNet 1.7, WordNet 1.7.1, and WordNet 2.0. Downloadable from
http://www.cs.unt.edu/?rada/downloads.html.
10 Note that for these words the WordNet first sense is not necessarily the most frequent sense.
525
Computational Linguistics Volume 35, Number 4
Table 7
The performance (F1) of K?semwsd at different ranges of polysemy. Most Frequent baseline (MF) is
also reported.
Range of polysemy
2?5 6?10 11?15 16?20 21?25 26?30 31+
K?semwsd 73.2 61.4 59.1 33.8 55.2 50.2 37.3
MF 70.0 53.4 56.4 25.7 47.2 39.0 21.7
Table 8
The performance (F1) of K?semwsd on words with a given number of training examples. Most
Frequent baseline (MF) and mean polysemy for each partition are also reported.
Range of training examples
1?10 11?50 51?100 101?200 201+
K?semwsd 76.1 70.8 54.2 67.4 60.0
MF 73.5 66.4 49.4 63.2 53.0
Mean polysemy 3 5 7 9 15
SemCor, and English lexical-sample and all-words data sets taken from all the previous
editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored sec-
ond (64.6). The dimension of their training set is comparable to ours; however, they also
use additional information drawn from WordNet to derive semantic generalizations
using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using
a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind
Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks).
The small difference between the two domain models seems to indicate that a
limited amount of unlabeled data is sufficient to improve the overall performance,
and the use of unlabeled data taken from the training set helps to slightly improve
the overall performance. However, the domain model can be acquired from a different
corpus (e.g., the BNC) without significantly affecting the overall performance.
Finally, the results reported in Tables 7 and 8 show that our approach is able to dis-
ambiguate with good accuracy (F1 = 76%) words with a number of training examples
that ranges from 1 to 10, outperforming the most frequent baseline by 3%. This is an
interesting result given the extremely small number of training examples available. On
the other hand, the more training is available for a given word, the more polysemous
that word is. Nevertheless, the algorithm always outperforms the baseline and has a
more significant difference for increasing values of the mean polysemy (from 3% to
16%). These results, together with the ones obtained in the lexical sample tasks, show
that the domain kernel is able to boost the overall performance when little training data
are available, as well as with enough training data. The benefit is evenmore pronounced
for the latter case, even though the disambiguation task is more complex due to the high
polysemy of highly frequent words.
5. Conclusions
This article summarizes the results of a word expert semi-supervised algorithm for
WSD based on a combination of kernel functions. First, we evaluated our methodology
526
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
on four lexical-sample tasks of Senseval-3, significantly improving the state of the art
for all of them. In particular, we demonstrated that using external knowledge inside a
supervised framework is a viable methodology to reduce the amount of training data
required for learning. In our approach, the external knowledge is represented by means
of domain models automatically acquired from corpora in a totally unsupervised way.
Then, we applied the method so defined to the English all-words task of Senseval-
3, achieving state-of-the-art performance while requiring less labeled training data
compared to the other systems we have found in the literature.
Some slight improvement may be possible by exploiting syntactic information pro-
duced by a parser. In the framework of kernel methods, this expansion can be done by
adding a tree kernel (i.e., a kernel function that evaluates the similarity among parse
trees) to our composite kernel. However, the performance achieved is close to the upper
bound, if we consider the inter-annotator agreement as an indication of the upper-
bound performance.
Finally, we think that our semi-supervised approach is at the moment an effective
solution for developing a sense-tagging system. Indeed, we tested the system on the
English lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance
(Pradhan et al 2007). Therefore, we plan to make available an optimized version of
our system, and to exploit it for ontology learning, textual entailment, and information
retrieval.
Acknowledgments
Claudio Giuliano was supported by the
X-Media project (www.x-media-project.org),
sponsored by the European Commission as
part of the Information Society Technologies
(IST) programme under EC grant IST-FP6-
026978. Alfio Massimiliano Gliozzo and
Carlo Strapparava were supported by the
ONTOTEXT project, sponsored by the
Autonomous Province of Trento under the
FUP-2004 research program.
References
Boser, Bernhard, Isabelle Guyon, and
Vladimir Vapnik. 1992. A training
algorithm for optimal margin classifier.
In Proceedings of the 5th Annual ACM
Workshop on Computational Learning
Theory, pages 144?152, Pittsburgh, PA.
Cancedda, Nicola, Eric Gaussier, Cyril
Goutte, and Jean-Michel Renders. 2003.
Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059?1082.
Chang, Chih-Chung and Chih-Jen Lin,
2001. LIBSVM: A library for support vector
machines. Software available at www.csie.
ntu.edu.tw/?cjlin/libsvm.
Chapelle, Olivier, Bernhard Scho?lkopf, and
Alexander Zien. 2006. Semi-Supervised
Learning. MIT Press, Cambridge, MA.
Cristianini, Nello and John Shawe-Taylor.
2000. An Introduction to Support Vector
Machines. Cambridge University Press.
Decadt, Bart, Veronique Hoste, Walter
Daelemans, and Antal van den Bosch.
2004. GAMBL, genetic algorithm
optimization of memory-based WSD. In
Proceedings of Senseval-3, pages 108?112,
Barcelona.
Deerwester, Scott, Susan Dumais, George
Furnas, Thomas Landauer, and Richard
Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American
Society of Information Science, 41:391?407.
Giuliano, Claudio, Alfio Gliozzo, and Carlo
Strapparava. 2006. Syntagmatic kernels: A
word sense disambiguation case study. In
In Proceedings of the EACL-06 Workshop on
Learning Structured Information in Natural
Language Applications, pages 57?63, Trento.
Giuliano, Claudio, Alberto Lavelli, and
Lorenza Romano. 2006. Exploiting shallow
linguistic information for relation
extraction from biomedical literature. In
Proceedings of the Eleventh Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-2006),
pages 401?408, Trento.
Gliozzo, Alfio, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for
word sense disambiguation. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-05),
pages 403?410, Ann Arbor, MI.
Gliozzo, Alfio, Carlo Strapparava, and
Ido Dagan. 2004. Unsupervised and
supervised exploitation of semantic
527
Computational Linguistics Volume 35, Number 4
domains in lexical disambiguation.
Computer Speech and Language,
18(3):275?299.
Lodhi, Huma, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal
of Machine Learning Research, 2(3):419?444.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings of Senseval-3: Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text.
Barcelona.
Mihalcea, Rada and Ehsanul Faruque. 2004.
SenseLearner: Minimally supervised WSD
for all words in open text. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 155?158, Barcelona.
Moschitti, Alessandro. 2004. A study on
convolution kernels for shallow statistic
parsing. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 335?342,
Barcelona.
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007.
Semeval-2007 task-17: English lexical
sample, SRL and all words. In Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
pages 87?92, Prague.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Saunders, Craig, Hauke Tschach, and John
Shawe-Taylor. 2002. Syllables and other
string kernel extensions. In Proceedings of
19th International Conference on Machine
Learning (ICML02), pages 530?537, Sydney.
Scho?lkopf, Bernhard and Alexander Smola.
2002. Learning with Kernels. MIT Press,
Cambridge, MA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 41?43, Barcelona.
Strapparava, Carlo, Alfio Gliozzo, and
Claudio Giuliano. 2004. Pattern abstraction
and term similarity for word sense
disambiguation: Irst at senseval-3. In
Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic
Analysis of Text, pages 229?234, Barcelona.
Vapnik, Vladimir N. 1999. The Nature of
Statistical Learning Theory (Information
Science and Statistics). Springer, Berlin.
Wong, S. K. M., Wojciech Ziarko, and
Patrick C. N. Wong. 1985. Generalized
vector space model in information
retrieval. In Proceedings of the 8th ACM
SIGIR Conference, pages 18?25, Montreal.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics (ACL 1994), pages 88?95,
Las Cruces, NM.
Yuret, Deniz. 2004. Some experiments with a
naive Bayes WSD system. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 265?268, Barcelona.
Zhao, Shubin and Ralph Grishman. 2005.
Extracting relations with integrated
information using kernel methods. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL 2005), pages 419?426, Ann Arbor, MI.
528
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 703?711,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Computational Approach to the Automation of Creative Naming
Go?zde O?zbal
FBK-Irst / Trento, Italy
gozbalde@gmail.com
Carlo Strapparava
FBK-Irst / Trento, Italy
strappa@fbk.eu
Abstract
In this paper, we propose a computational ap-
proach to generate neologisms consisting of
homophonic puns and metaphors based on the
category of the service to be named and the
properties to be underlined. We describe all
the linguistic resources and natural language
processing techniques that we have exploited
for this task. Then, we analyze the perfor-
mance of the system that we have developed.
The empirical results show that our approach
is generally effective and it constitutes a solid
starting point for the automation of the naming
process.
1 Introduction
A catchy, memorable and creative name is an im-
portant key to a successful business since the name
provides the first image and defines the identity of
the service to be promoted. A good name is able to
state the area of competition and communicate the
promise given to customers by evoking semantic as-
sociations. However, finding such a name is a chal-
lenging and time consuming activity, as only few
words (in most cases only one or two) can be used to
fulfill all these objectives at once. Besides, this task
requires a good understanding of the service to be
promoted, creativity and high linguistic skills to be
able to play with words. Furthermore, since many
new products and companies emerge every year, the
naming style is continuously changing and creativ-
ity standards need to be adapted to rapidly changing
requirements.
The creation of a name is both an art and a science
(Keller, 2003). Naming has a precise methodology
and effective names do not come out of the blue. Al-
though it might not be easy to perceive all the effort
behind the naming process just based on the final
output, both a training phase and a long process con-
sisting of many iterations are certainly required for
coming up with a good name.
From a practical point of view, naming agencies
and branding firms, together with automatic name
generators, can be considered as two alternative ser-
vices that facilitate the naming process. However,
while the first type is generally expensive and pro-
cessing can take rather long, the current automatic
generators are rather na??ve in the sense that they are
based on straightforward combinations of random
words. Furthermore, they do not take semantic rea-
soning into account.
To overcome the shortcomings of these two alter-
native ways (i.e. naming agencies and na??ve gener-
ators) that can be used for obtaining name sugges-
tions, we propose a system which combines several
linguistic resources and natural language processing
(NLP) techniques to generate creative names, more
specifically neologisms based on homophonic puns
and metaphors. In this system, similarly to the pre-
viously mentioned generators, users are able to de-
termine the category of the service to be promoted
together with the features to be emphasized. Our
improvement lies in the fact that instead of random
generation, we take semantic, phonetic, lexical and
morphological knowledge into consideration to au-
tomatize the naming process.
Although various resources provide distinct tips
for inventing creative names, no attempt has been
made to combine all means of creativity that can be
used during the naming process. Furthermore, in
addition to the devices stated by copywriters, there
703
might be other latent methods that these experts un-
consciously use. Therefore, we consider the task
of discovering and accumulating all crucial features
of creativity to be essential before attempting to au-
tomatize the naming process. Accordingly, we cre-
ate a gold standard of creative names and the corre-
sponding creative devices that we collect from var-
ious sources. This resource is the starting point of
our research in linguistic creativity for naming.
The rest of the paper is structured as follows.
First, we review the state-of-the-art relevant to the
naming task. Then, we give brief information about
the annotation task that we have conducted. Later
on, we describe the model that we have designed
for the automatization of the naming process. Af-
terwards, we summarize the annotation task that we
have carried out and analyze the performance of
the system with concrete examples by discussing its
virtues and limitations. Finally, we draw conclu-
sions and outline ideas for possible future work.
2 Related Work
In this section, we will analyze the state of the art
concerning the naming task from three different as-
pects: i) linguistic ii) computational iii) commercial.
2.1 Linguistic
Little research has been carried out to investigate
the linguistic aspects of the naming mechanism.
B. V. Bergh (1987) built a four-fold linguistic topol-
ogy consisting of phonetic, orthographic, morpho-
logical and semantic categories to evaluate the fre-
quency of linguistic devices in brand names. Bao
et al (2008) investigated the effects of relevance,
connotation, and pronunciation of brand names on
preferences of consumers. Klink (2000) based
his research on the area of sound symbolism (i.e.
?the direct linkage between sound and meaning?
(Leanne Hinton, 2006)) by investigating whether the
sound of a brand name conveys an inherent mean-
ing and the findings showed that both vowels and
consonants of brand names communicate informa-
tion related to products when no marketing com-
munications are available. Kohli et al (2005) ana-
lyzed consumer evaluations of meaningful and non-
meaningful brand names and the results suggested
that non-meaningful brand names are evaluated less
favorably than meaningful ones even after repeated
exposure. Lastly, cog (2011) focused on the seman-
tics of branding and based on the analysis of several
international brand names, it was shown that cogni-
tive operations such as domain reduction/expansion,
mitigation, and strengthening might be used uncon-
sciously while creating a new brand name.
2.2 Computational
To the best of our knowledge, there is only one com-
putational study in the literature that can be applied
to the automatization of name generation. Stock and
Strapparava (2006) introduce an acronym ironic re-
analyzer and generator called HAHAcronym. This
system both makes fun of existing acronyms, and
produces funny acronyms that are constrained to be
words of the given language by starting from con-
cepts provided by users. HAHAcronym is mainly
based on lexical substitution via semantic field op-
position, rhyme, rhythm and semantic relations such
as antonyms retrieved from WordNet (Stark and
Riesenfeld, 1998) for adjectives.
As more na??ve solutions, automatic name gener-
ators can be used as a source of inspiration in the
brainstorming phase to get ideas for good names.
As an example, www.business-name-generators.
com randomly combines abbreviations, syllables and
generic short words from different domains to ob-
tain creative combinations. The domain genera-
tor on www.namestation.com randomly generates
name ideas and available domains based on allit-
erations, compound words and custom word lists.
Users can determine the prefix and suffix of the
names to be generated. The brand name generator
on www.netsubstance.com takes keywords as in-
puts and here users can configure the percentage of
the shifting of keyword letters. Lastly, the mecha-
nism of www.naming.net is based on name combi-
nations among common words, Greek and Latin pre-
fixes, suffixes and roots, beginning and ending word
parts and rhymes. A shortcoming of these kinds of
automatic generators is that random generation can
output so many bad suggestions and users have to be
patient to find the name that they are looking for. In
addition, these generations are based on straightfor-
ward combinations of words and they do not include
a mechanism to also take semantics into account.
2.3 Commercial
Many naming agencies and branding firms1 provide
professional service to aid with the naming of new
1e.g. www.eatmywords.com, www.designbridge.
com, www.ahundredmonkeys.com
704
products, domains, companies and brands. Such ser-
vices generally require customers to provide brief
information about the business to be named, fill in
questionnaires to learn about their markets, competi-
tors, and expectations. In the end, they present a list
of name candidates to be chosen from. Although the
resulting names can be successful and satisfactory,
these services are very expensive and the processing
time is rather long.
3 Dataset and Annotation
In order to create a gold standard for linguistic cre-
ativity in naming, collect the common creativity de-
vices used in the naming process and determine the
suitable ones for automation, we conducted an an-
notation task on a dataset of 1000 brand and com-
pany names from various domains (O?zbal et al,
2012). These names were compiled from a book
dedicated to brand naming strategies (Botton and
Cegarra, 1990) and various web resources related
to creative naming such as adslogans.co.uk and
brandsandtags.com.
Our list contains names which were invented via
various creativity methods. While the creativity in
some of these names is independent of the context
and the names themselves are sufficient to realize the
methods used (e.g. alliteration in Peak Performance,
modification of one letter in Vimeo), for some of
them the context information such as the description
of the product or the area of the company is also
necessary to fully understand the methods used. For
instance, Thanks a Latte is a coffee bar name where
the phonetic similarity between ?lot? and ?latte? (a
coffee type meaning ?milk? in Italian) is exploited.
The name Caterpillar, which is an earth-moving
equipment company, is used as a metaphor. There-
fore, we need extra information regarding the do-
main description in addition to the names. Accord-
ingly, while building our dataset, we conducted two
separate branches of annotation. The first branch re-
quired the annotators to fill in the domain descrip-
tion of the names in question together with their et-
ymologies if required, while the second asked them
to determine the devices of creativity used in each
name.
In order to obtain the list of creativity devices, we
collected a total of 31 attributes used in the naming
process from various resources including academic
papers, naming agents, branding and advertisement
experts. To facilitate the task for the annotators,
we subsumed the most similar attributes when re-
quired. Adopting the four-fold linguistic topology
suggested by Bergh et al (B. V. Bergh, 1987), we
mapped these attributes into phonetic, orthographic,
morphological and semantic categories. The pho-
netic category includes attributes such as rhyme (i.e.
repetition of similar sounds in two or more words
- e.g. Etch-a-sketch) and reduplication (i.e. repeat-
ing the root or stem of a word or part of it exactly
or with a slight change - e.g. Teenie Weenie), while
the orthographic category consists of devices such as
acronyms (e.g. BMW) and palindromes (i.e. words,
phrases, numbers that can be read the same way in
either direction e.g. Honda ?Civic?). The third cat-
egory is the morphology which contains affixation
(i.e. forming different words by adding morphemes
at the beginning, middle or end of words - e.g.
Nutella) and blending (i.e. forming a word by blend-
ing sounds from two or more distinct words and
combining their meanings - e.g. Wikipedia by blend-
ing ?Wiki? and ?encyclopedia?). Finally, the seman-
tic category includes attributes such as metaphors
(i.e. Expressing an idea through the image of another
object - e.g. Virgin) and punning (i.e. using a word
in different senses or words with sound similarity to
achieve specific effect such as humor - e.g. Thai Me
Up for a Thai restaurant).
4 System Description
The resource that we have obtained after the anno-
tation task provides us with a starting point to study
and try to replicate the linguistic and cognitive pro-
cesses behind the creation of a successful name. Ac-
cordingly, we have made a systematic attempt to
replicate these processes, and implemented a system
which combines methods and resources used in var-
ious areas of Natural Language Processing (NLP) to
create neologisms based on homophonic puns and
metaphors. While the variety of creativity devices
is actually much bigger, our work can be consid-
ered as a starting point to investigate which kinds of
technologies can successfully be exploited in which
way to support the naming process. The task that we
deal with requires: 1) reasoning of relations between
entities and concepts; 2) understanding the desired
properties of entities determined by users; 3) identi-
fying semantically related terms which are also con-
sistent with the objectives of the advertisement; 4)
finding terms which are suitable metaphors for the
properties that need to be emphasized; 5) reasoning
705
about phonetic properties of words; 6) combining
all this information to create natural sounding neol-
ogisms.
In this section, we will describe in detail the work
flow of the system that we have designed and imple-
mented to fulfill these requirements.
4.1 Specifying the category and properties
Our design allows users to determine the category
of the product/brand/company to be advertised (e.g.
shampoo, car, chocolate) optionally together with
the properties (e.g. softening, comfortable, addic-
tive) that they want to emphasize. In the current
implementation, categories are required to be nouns
while properties are required to be adjectives. These
inputs that are specified by users constitute the main
ingredients of the naming process. After the de-
termination of these ingredients, several techniques
and resources are utilized to enlarge the ingredient
list, and thereby to increase the variety of new and
creative names.
4.2 Adding common sense knowledge
After the word defining the category is determined
by the user, we need to automatically retrieve more
information about this word. For instance, if the cat-
egory has been determined as ?shampoo?, we need
to learn that ?it is used for washing hair? or ?it
can be found in the bathroom?, so that all this ex-
tra information can be included in the naming pro-
cess. To achieve that, we use ConceptNet (Liu and
Singh, 2004), which is a semantic network contain-
ing common sense, cultural and scientific knowl-
edge. This resource consists of nodes representing
concepts which are in the form of words or short
phrases of natural language, and labeled relations
between them.
ConceptNet has a closed class of relations ex-
pressing connections between concepts. After the
analysis of these relations according to the require-
ments of the task, we have decided to use the ones
listed in Table 1 together with their description in
the second column. The third column states whether
the category word should be the first or second ar-
gument of the relation in order for us to consider
the new word that we discover with that relation.
Since, for instance, the relations MadeOf(milk, *)
and MadeOf(*, milk) can be used for different goals
(the former to obtain the ingredients of milk, and
the latter to obtain products containing milk), we
Relation Description # POS
HasA What does it possess? 1 n
PartOf What is it part of? 2 n
UsedFor What do you use it for? 1 n,v
AtLocation Where would you find it? 2 n
MadeOf What is it made of 1 n
CreatedBy How do you bring it into existence? 1 n
HasSubevent What do you do to accomplish it? 2 v
Causes What does it make happen? 1 n,v
Desires What does it want? 1 n,v
CausesDesire What does it make you want to do? 1 n,v
HasProperty What properties does it have? 1 a
ReceivesAction What can you do to it? 1 v
Table 1: ConceptNet relations.
need to make this differentiation. Via ConceptNet 5,
the latest version of ConceptNet, we obtain a list of
relations such as AtLocation(shampoo, bathroom),
UsedFor(shampoo, clean) and MadeOf(shampoo,
perfume) with the query word ?shampoo?. We add
all the words appearing in relations with the category
word to our ingredient list. Among these new words,
multiwords are filtered out since most of them are
noisy and for our task a high precision is more im-
portant than a high recall.
Since sense information is not provided, one of
the major problems in utilizing ConceptNet is the
difficulty in disambiguating the concepts. In our
current design, we only consider the most common
senses of words. As another problem, the part-of-
speech (POS) information is not available in Con-
ceptNet. To handle this problem, we have deter-
mined the required POS tags of the new words that
can be obtained from the relations with an additional
goal of filtering out the noise. These tags are stated
in the fourth column of Table 1.
4.3 Adding semantically related words
To further increase the size of the ingredient list,
we utilize another resource called WordNet (Miller,
1995), which is a large lexical database for English.
In WordNet, nouns, verbs, adjectives and adverbs
are grouped into sets of cognitive synonyms called
synsets. Each synset in WordNet expresses a dif-
ferent concept and they are connected to each other
with lexical, semantic and conceptual relations.
We use the direct hypernym relation of WordNet
to retrieve the superordinates of the category word
(e.g. cleansing agent, cleanser and cleaner for the
category word shampoo). We prefer to use this re-
lation of WordNet instead of the relation ?IsA? in
706
ConceptNet to avoid getting too general words. Al-
though we can obtain only the direct hypernyms in
WordNet, no such mechanism exists in ConceptNet.
In addition, while WordNet has been built by lin-
guists, ConceptNet is built from the contributions of
many thousands of people across the Web and natu-
rally it also contains a lot of noise.
In addition to the direct hypernyms of the cate-
gory word, we increase the size of the ingredient list
by adding synonyms of the category word, the new
words coming from the relations and the properties
determined by the user.
It should be noted that we do not consider any
other statistical or knowledge based techniques for
semantic relatedness. Although they would allow us
to discover more concepts, it is difficult to under-
stand if and how these concepts pertain to the con-
text. In WordNet we can decide what relations to
explore, with the result of a more precise process
with possibly less recall.
4.4 Retrieving metaphors
A metaphor is a figure of speech in which an implied
comparison is made to indicate how two things that
are not alike in most ways are similar in one impor-
tant way. Metaphors are common devices for evo-
cation, which has been found to be a very important
technique used in naming according to the analysis
of our dataset.
In order to generate metaphors, we start with the
set of properties determined by the user and adopt
a similar technique to the one proposed by (Veale,
2011). In this work, to metaphorically ascribe a
property to a term, stereotypes for which the prop-
erty is culturally salient are intersected with stereo-
types to which the term is pragmatically compara-
ble. The stereotypes for a property are found by
querying on the web with the simile pattern ?as
?property? as *?. Unlike the proposed approach,
we do not apply any intersection with comparable
stereotypes since the naming task should favor fur-
ther terms to the category word in order to exagger-
ate, to evoke and thereby to be more effective.
The first constituent of our approach uses the
pattern ?as ?property? as *? with the addition of
??property? like *?, which is another important
block for building similes. Given a property, these
patterns are harnessed to make queries through the
web api of Google Suggest. This service performs
auto-completion of search queries based on popu-
lar searches. Although top 10 (or fewer) sugges-
tions are provided for any query term by Google
Suggest, we expand these sets by adding each let-
ter of the alphabet at the end of the provided phrase.
Thereby, we obtain 10 more suggestions for each of
these queries. Among the metaphor candidates that
we obtain, we filter out multiwords to avoid noise as
much as possible. Afterwards, we conduct a lemma-
tization process on the rest of the candidates. From
the list of lemmas, we only consider the ones which
appear in WordNet as a noun. Although the list
that we obtain in the end has many potentially valu-
able metaphors (e.g. sun, diamond, star, neon for
the property bright), it also contains a lot of uncom-
mon and unrelated words (e.g. downlaod, myspace,
house). Therefore, we need a filtering mechanism to
remove the noise and keep only the best metaphors.
To achieve that, the second constituent of the
metaphor retrieval mechanism makes a query in
ConceptNet with the given property. Then, all the
nouns coming from the relations in the form of
HasProperty(*, property) are collected to find words
having that property. The POS check to obtain only
nouns is conducted with a look-up in WordNet as
before. It should be noted that this technique would
not be enough to retrieve metaphors alone since it
can also return noise (e.g. blouse, idea, color, home-
schooler for the property bright).
After we obtain two different lists of metaphor
candidates with the two mechanisms mentioned
above, we take the intersection of these lists and
consider only the words appearing in both lists as
metaphors. In this manner, we aim to remove the
noise coming from each list and obtain more reli-
able metaphors. To illustrate, for the same example
property bright, the metaphors obtained at the end
of the process are sun, light and day.
4.5 Generating neologisms
After the ingredient list is complete, the phonetic
module analyzes all ingredient pairs to generate ne-
ologisms with possibly homophonic puns based on
phonetic similarity.
To retrieve the pronunciation of the ingredients,
we utilize the CMU Pronouncing Dictionary (Lenzo,
2007). This resource is a machine-readable pro-
nunciation dictionary of English which is suitable
for uses in speech technology, and it contains over
125,000 words together with their transcriptions. It
has mappings from words to their pronunciations
707
Input Successful output Unsuccessful output
Category Properties Word Ingredients Word Ingredients
bar
irish lively wooden traditional
warm hospitable friendly
beertender bartender, beer barkplace workplace, bar
barty party, bar barl girl, bar
giness guinness, gin bark work, bar
perfume
attractive strong intoxicating
unforgettable feminine mystic
sexy audacious provocative
mysticious mysterious, mystic provocadeepe provocative, deep
bussling buss, puzzling
mysteelious mysterious, steel
sunglasses
cool elite though authentic
cheap sporty
spectacools spectacles, cool spocleang sporting, clean
electacles spectacles, elect
polarice polarize, ice
restaurant
warm elegant friendly original
italian tasty cozy modern
eatalian italian, eat dusta pasta, dust
pastarant restaurant, pasta hometess hostess, home
peatza pizza, eat
shampoo
smooth bright soft volumizing
hydrating quality
fragrinse fragrance, rinse furl girl, fur
cleansun cleanser, sun sasun satin, sun
Table 2: A selection of succesful and unsuccessful neologisms generated by the model.
and the current phoneme set contains 39 phonemes
based on the ARPAbet symbol set, which has been
developed for speech recognition uses. We con-
ducted a mapping from the ARPAbet phonemes to
the international phonetic alphabet (IPA) phonemes
and we grouped the IPA phonemes based on the
phoneme classification documented in IPA. More
specifically, we grouped the ones which appear in
the same category such as p-b, t-d and s-z for the
consonants; i-y and e-? for the vowels.
After having the pronunciation of each word in
the ingredient list, shorter pronunciation strings are
compared against the substrings of longer ones.
Among the different possible distance metrics that
can be applied for calculating the phonetic similarity
between two pronunciation strings, we have chosen
the Levenshtein distance (Levenshtein, 1966). This
distance is a metric for measuring the amount of dif-
ference between two sequences, defined as the min-
imum number of edits required for the transforma-
tion of one sequence into the other. The allowable
edit operations for this transformation are insertion,
deletion, or substitution of a single character. For ex-
ample, the Levenshtein distance between the strings
?kitten? and ?sitting? is 3, since the following three
edits change one into the other, and there is no way
to do it with fewer than three edits: kitten? sitten
(substitution of ?k? with ?s?), sitten? sittin (substi-
tution of ?e? with ?i?), sittin ? sitting (insertion of
?g? at the end). For the distance calculation, we em-
ploy relaxation by giving a smaller penalty for the
phonemes appearing in the same phoneme groups
mentioned previously. We normalize each distance
by the length of the pronunciation string considered
for the distance calculation and we only allow the
combination of word pairs that have a normalized
distance score less than 0.5, which was set empiri-
cally.
Since there is no one-to-one relationship between
letters and phonemes and no information about
which phoneme is related to which letter(s) is avail-
able, it is not straightforward to combine two words
after determining the pairs via Levenshtein distance
calculation. To solve this issue, we use the Berke-
ley word aligner2 for the alignment of letters and
phonemes. The Berkeley Word Aligner is a sta-
tistical machine translation tool that automatically
aligns words in a sentence-aligned parallel corpus.
To adapt this tool according to our needs, we split
all the words in our dictionary into letters and their
mapped pronunciation to their phonemes, so that the
aligner could learn a mapping from phonemes to
characters. The resulting alignment provides the in-
formation about from which index to which index
the replacement of the substring of a word should
occur. Accordingly, the substring of the word which
has a high phonetic similarity with a specific word
is replaced with that word. As an example, if the
first ingredient is bright and the second ingredient is
light, the name blight can be obtained at the end of
2http://code.google.com/p/berkeleyaligner/
708
this process.
4.6 Checking phonetic likelihood
To check the likelihood and well-formedness of the
new string after the replacement, we learn a 3-gram
language model with absolute smoothing. For learn-
ing the language model, we only consider the words
in the CMU pronunciation dictionary which also ex-
ist in WordNet. This filtering is required in order
to eliminate a large number of non-English trigrams
which would otherwise cause too high probabilities
to be assigned to very unlikely sequences of charac-
ters. We remove the words containing at least one
trigram which is very unlikely according to the lan-
guage model. The threshold to determine the un-
likely words is set to the probability of the least fre-
quent trigram observed in the training data.
5 Evaluation
We evaluated the performance of our system with
a manual annotation in which 5 annotators judged
a set of neologisms along 4 dimensions: 1) appro-
priateness, i.e. the number of ingredients (0, 1 or
2) used to generate the neologism which are appro-
priate for the input; 2) pleasantness, i.e. a binary de-
cision concerning the conformance of the neologism
to the sound patterns of English; 3) humor/wittiness,
i.e. a binary decision concerning the wittiness of the
neologism; 4) success, i.e. an assessment of the fit-
ness of the neologism as a name for the target cate-
gory/properties (unsuccessful, neutral, successful).
To create the dataset, we first compiled a list
of 50 categories by selecting 50 hyponyms of the
synset consumer goods in WordNet. To determine
the properties to be underlined, we asked two anno-
tators to state the properties that they would expect
to have in a product or company belonging to each
category in our category list. Then, we merged the
answers coming from the two annotators to create
the final set of properties for each category.
Although our system is actually able to produce
a limitless number of results for a given input, we
limited the number of outputs for each input to
reduce the effort required for the annotation task.
Therefore, we implemented a ranking mechanism
which used a hybrid scoring method by giving equal
weights to the language model and the normalized
phonetic similarity. Among the ranked neologisms
for each input, we only selected the top 20 to build
the dataset. It should be noted that for some input
Dimension
APP PLE HUM SUX
2 9.54 0 0 27.04
3 33.3 25.34 32.77 49.52
4 41.68 38.6 34.57 18.77
5 15.48 36.06 32.66 4.67
3+ 90.46 100 100 72.96
Table 3: Inter-annotator agreement (in terms of majority
class, MC) on the four annotation dimensions.
combinations the system produced less than 20 neol-
ogisms. Accordingly, our dataset consists of a total
number of 50 inputs and 943 neologisms.
To have a concrete idea about the agreement be-
tween annotators, we calculated the majority class
for each dimension. With 5 annotators, a majority
class greater than or equal to 3 means that the abso-
lute majority of the annotators agreed on the same
decision. Table 3 shows the distribution of majority
classes along the four dimensions of the annotation.
For pleasantness (PLE) and humor (HUM), the ab-
solute majority of the annotators (i.e. 3/5) agreed on
the same decision in 100% of the cases, while for ap-
propriateness (APP) the figure is only slightly lower.
Concerning success, arguably the most subjective of
the four dimensions, in 27% of the cases it is not
possible to take a majority decision. Nevertheless,
in almost 73% of the cases the absolute majority of
the annotators agreed on the annotation of this di-
mension.
Table 4 shows the micro and macro-average of
the percentage of cases in which at least 3 anno-
tators have labeled the ingredients as appropriate
(APP), and the neologisms as pleasant (PLE), hu-
morous (HUM) or successful (SUX). The system se-
lects appropriate ingredients in approximately 60%
of the cases, and outputs pleasant, English-sounding
names in ?87% of the cases. Almost one name out
of four is labeled as successful by the majority of the
annotators, which we regard as a very positive result
considering the difficulty of the task. Even though
we do not explicitly try to inject humor in the neol-
ogisms, more than 15% of the generated names turn
out to be witty or amusing. The system managed to
generate at least one successful name for all 50 input
categories and at least one witty name for 42. As ex-
pected, we found out that there is a very high corre-
lation (91.56%) between the appropriateness of the
709
Dimension
Accuracy APP PLE HUM SUX
micro 59.60 87.49 16.33 23.86
macro 60.76 87.01 15.86 24.18
Table 4: Accuracy of the generation process along the
four dimensions.
ingredients and the success of the name. A success-
ful name is also humorous in 42.67% of the cases,
while 62.34% of the humorous names are labeled as
successful. This finding confirms our intuition that
amusing names have the potential to be very appeal-
ing to the customers. In more than 76% of the cases,
a humorous name is the product of the combination
of appropriate ingredients.
In Table 2, we show a selection of successful
and unsuccessful outputs generated for the category
and the set of properties listed under the block of
columns labeled as Input according to the majority
of annotators (i.e. 3 or more). As an example of pos-
itive outcomes, we can focus on the columns under
Successful output for the input target word restau-
rant. The model correctly selects the ingredients
eat (a restaurant is UsedFor eating), pizza and pasta
(which are found AtLocation restaurant) to generate
an appropriate name. The three ?palatable? neolo-
gisms generated are eatalian (from the combination
of eat and Italian), pastarant (pasta + restaurant)
and peatza (pizza + eat). These three suggestions are
amusing and have a nice ring to them. As a matter
of fact, it turns out that the name Eatalian is actually
used by at least one real Italian restaurant located in
Los Angeles, CA3.
For the same set of stimuli, the model also se-
lects some ingredients which are not really related
to the use-case, e.g., dust and hostess (both of which
can be found AtLocation restaurant) and home (a
synonym for plate, which can be found AtLocation
restaurant, in the baseball jargon). With these in-
gredients, the model produces the suggestion dusta
which sounds nice but has a negative connotation,
and hometess which can hardly be associated to the
input category.
A rather common class of unsuccessful outputs
include words that, by pure chance, happen to be
already existing in English. In these cases, no actual
neologism is generated. Sometimes, the generated
3http://www.eataliancafe.com/
words have rather unpleasant or irrelevant meanings,
as in the case of bark for bar. Luckily enough, these
kinds of outputs can easily be eliminated by filtering
out all the output words which can already be found
in an English dictionary or which are found to have
a negative valence with state-of-the-art techniques
(e.g. SentiWordNet (Esuli and Sebastiani, 2006)).
Another class of negative results includes neolo-
gisms generated from ingredients that the model
cannot combine in a good English-sounding neol-
ogism (e.g. spocleang from sporting and clean for
sunglasses or sasun from satin and sun for sham-
poo).
6 Conclusion
In this paper, we have focused on the task of automa-
tizing the naming process and described a computa-
tional approach to generate neologisms with homo-
phonic puns based on phonetic similarity. This study
is our first step towards the systematic emulation of
the various creative devices involved in the naming
process by means of computational methods.
Due to the complexity of the problem, a unified
model to handle all the creative devices at the same
time seems outside the reach of the current state-of-
the-art NLP techniques. Nevertheless, the resource
that we collected, together with the initial imple-
mentation of this model should provide a good start-
ing point for other researchers in the area. We be-
lieve that our contribution will motivate other re-
search teams to invest more effort in trying to tackle
the related research problems.
As future work, we plan to improve the quality of
the output by considering word sense disambigua-
tion techniques to reduce the effect of inappropriate
ingredients. We also want to extend the model to in-
clude multiword ingredients and to generate not only
words but also short phrases. Then, we would like
to focus on other classes of creative devices, such
as affixation or rhyming. Lastly, we plan to make
the system that we have developed publicly avail-
able and collect user feedback for further develop-
ment and improvement.
Acknowledgments
The authors were partially supported by a Google
Research Award.
710
References
L. Oliver B. V. Bergh, K. Adler. 1987. Linguistic distinc-
tion among top brand names. Journal of Advertising
Research, pages 39?44.
Yeqing Bao, Alan T Shao, and Drew Rivers. 2008. Cre-
ating new brand names: Effects of relevance, conno-
tation, and pronunciation. Journal of Advertising Re-
search, 48(1):148.
Marcel Botton and Jean-Jack Cegarra, editors. 1990. Le
nom de marque. Paris McGraw Hill.
2011. Cognitive tools for successful branding. Applied
Linguistics, 32:369?388.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. pages 417?422.
Kevin Lane Keller. 2003. Strategic brand management:
building, measuring and managing brand equity. New
Jersey: Prentice Hall.
Richard R. Klink. 2000. Creating brand names with
meaning: The use of sound symbolism. Marketing
Letters, 11(1):5?20.
C Kohli, K Harich, and Lance Leuthesser. 2005. Creat-
ing brand identity: a study of evaluation of new brand
names. Journal of Business Research, 58(11):1506?
1515.
John J. Ohala Leanne Hinton, Johanna Nichols. 2006.
Sound Symbolism. Cambridge University Press.
Kevin Lenzo. 2007. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10:707?710.
H. Liu and P. Singh. 2004. Conceptnet ? a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Go?zde O?zbal, Carlo Strapparava, and Marco Guerini.
2012. Brand Pitt: A corpus to explore the art of nam-
ing. In Proceedings of the eighth international confer-
ence on Language Resources and Evaluation (LREC-
2012), Istanbul, Turkey, May.
Michael M. Stark and Richard F. Riesenfeld. 1998.
Wordnet: An electronic lexical database. In Proceed-
ings of 11th Eurographics Workshop on Rendering.
MIT Press.
Oliviero Stock and Carlo Strapparava. 2006. Laughing
with HAHAcronym, a computational humor system.
In proceedings of the 21st national conference on Arti-
ficial intelligence - Volume 2, pages 1675?1678. AAAI
Press.
Tony Veale. 2011. Creative language retrieval: A robust
hybrid of information retrieval and linguistic creativ-
ity. In Proceedings of ACL 2011, Portland, Oregon,
USA, June.
711
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 988?996,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Ecological Evaluation of Persuasive Messages Using Google AdWords
Marco Guerini
Trento-Rise
Via Sommarive 18, Povo
Trento ? Italy
marco.guerini@trentorise.eu
Carlo Strapparava
FBK-Irst
Via Sommarive 18, Povo
Trento ? Italy
strappa@fbk.eu
Oliviero Stock
FBK-Irst
Via Sommarive 18, Povo
Trento ? Italy
stock@fbk.eu
Abstract
In recent years there has been a growing in-
terest in crowdsourcing methodologies to be
used in experimental research for NLP tasks.
In particular, evaluation of systems and theo-
ries about persuasion is difficult to accommo-
date within existing frameworks. In this paper
we present a new cheap and fast methodology
that allows fast experiment building and eval-
uation with fully-automated analysis at a low
cost. The central idea is exploiting existing
commercial tools for advertising on the web,
such as Google AdWords, to measure message
impact in an ecological setting. The paper in-
cludes a description of the approach, tips for
how to use AdWords for scientific research,
and results of pilot experiments on the impact
of affective text variations which confirm the
effectiveness of the approach.
1 Introduction
In recent years there has been a growing interest in
finding new cheap and fast methodologies to be used
in experimental research, for, but not limited to, NLP
tasks. In particular, approaches to NLP that rely on
the use of web tools - for crowdsourcing long and
tedious tasks - have emerged. Amazon Mechani-
cal Turk, for example, has been used for collecting
annotated data (Snow et al, 2008). However ap-
proaches a la Mechanical Turk might not be suitable
for all tasks.
In this paper we focus on evaluating systems and
theories about persuasion, see for example (Fogg,
2009) or the survey on persuasive NL generation
studies in (Guerini et al, 2011a). Measuring the
impact of a message is of paramount importance in
this context, for example how affective text varia-
tions can alter the persuasive impact of a message.
The problem is that evaluation experiments repre-
sent a bottleneck: they are expensive and time con-
suming, and recruiting a high number of human par-
ticipants is usually very difficult.
To overcome this bottleneck, we present a specific
cheap and fast methodology to automatize large-
scale evaluation campaigns. This methodology al-
lows us to crowdsource experiments with thousands
of subjects for a few euros in a few hours, by tweak-
ing and using existing commercial tools for adver-
tising on the web. In particular we make reference
to the AdWords Campaign Experiment (ACE) tool
provided within the Google AdWords suite. One
important aspect of this tool is that it allows for real-
time fully-automated data analysis to discover sta-
tistically significant phenomena. It is worth noting
that this work originated in the need to evaluate the
impact of short persuasive messages, so as to assess
the effectiveness of different linguistic choices. Still,
we believe that there is further potential for opening
an interesting avenue for experimentally exploring
other aspects of the wide field of pragmatics.
The paper is structured as follows: Section 2 dis-
cusses the main advantages of ecological approaches
using Google ACE over traditional lab settings and
state-of-the-art crowdsourcing methodologies. Sec-
tion 3 presents the main AdWords features. Section
4 describes how AdWords features can be used for
defining message persuasiveness metrics and what
kind of stimulus characteristics can be evaluated. Fi-
nally Sections 5 and 6 describe how to build up an
988
experimental scenario and some pilot studies to test
the feasibility of our approach.
2 Advantages of Ecological Approaches
Evaluation of the effectiveness of persuasive sys-
tems is very expensive and time consuming, as the
STOP experience showed (Reiter et al, 2003): de-
signing the experiment, recruiting subjects, making
them take part in the experiment, dispensing ques-
tionnaires, gathering and analyzing data.
Existing methodologies for evaluating persuasion
are usually split in two main sets, depending on the
setup and domain: (i) long-term, in the field eval-
uation of behavioral change (as the STOP example
mentioned before), and (ii) lab settings for evaluat-
ing short-term effects, as in (Andrews et al, 2008).
While in the first approach it is difficult to take into
account the role of external events that can occur
over long time spans, in the second there are still
problems of recruiting subjects and of time consum-
ing activities such as questionnaire gathering and
processing.
In addition, sometimes carefully designed exper-
iments can fail because: (i) effects are too subtle to
be measured with a limited number of subjects or
(ii) participants are not engaged enough by the task
to provoke usable reactions, see for example what
reported in (Van Der Sluis and Mellish, 2010). Es-
pecially the second point is awkward: in fact, sub-
jects can actually be convinced by the message to
which they are exposed, but if they feel they do not
care, they may not ?react? at all, which is the case in
many artificial settings. To sum up, the main prob-
lems are:
1. Time consuming activities
2. Subject recruitment
3. Subject motivation
4. Subtle effects measurements
2.1 Partial Solution - Mechanical Turk
A recent trend for behavioral studies that is emerg-
ing is the use of Mechanical Turk (Mason and Suri,
2010) or similar tools to overcome part of these limi-
tations - such as subject recruitment. Still we believe
that this poses other problems in assessing behav-
ioral changes, and, more generally, persuasion ef-
fects. In fact:
1. Studies must be as ecological as possible, i.e.
conducted in real, even if controlled, scenarios.
2. Subjects should be neither aware of being ob-
served, nor biased by external rewards.
In the case of Mechanical Turk for example, sub-
jects are willingly undergoing a process of being
tested on their skills (e.g. by performing annota-
tion tasks). Cover stories can be used to soften this
awareness effect, nonetheless the fact that subjects
are being paid for performing the task renders the
approach unfeasible for behavioral change studies.
It is necessary that the only reason for behavior in-
duction taking place during the experiment (filling
a form, responding to a questionnaire, clicking on
an item, etc.) is the exposition to the experimental
stimuli, not the external reward. Moreover, Mechan-
ical Turk is based on the notion of a ?gold standard?
to assess contributors reliability, but for studies con-
cerned with persuasion it is almost impossible to de-
fine such a reference: there is no ?right? action the
contributor can perform, so there is no way to assess
whether the subject is performing the action because
induced to do so by the persuasive strategy, or just
in order to receive money. On the aspect of how to
handle subject reliability in coding tasks, see for ex-
ample the method proposed in (Negri et al, 2010).
2.2 Proposed Solution - Targeted Ads on the
Web
Ecological studies (e.g. using Google AdWords) of-
fer a possible solution to the following problems:
1. Time consuming activities: apart from experi-
mental design and setup, all the rest is automat-
ically performed by the system. Experiments
can yield results in a few hours as compared to
several days/weeks.
2. Subject recruitment: the potential pool of sub-
jects is the entire population of the web.
3. Subject motivation: ads can be targeted exactly
to those persons that are, in that precise mo-
ment throughout the world, most interested in
the topic of the experiment, and so potentially
more prone to react.
4. Subject unaware, unbiased: subjects are totally
unaware of being tested, testing is performed
during their ?natural? activity on the web.
989
5. Subtle effects measurements: if the are not
enough subjects, just wait for more ads to be
displayed, or focus on a subset of even more
interested people.
Note that similar ecological approaches are begin-
ning to be investigated: for example in (Aral and
Walker, 2010) an approach to assessing the social ef-
fects of content features on an on-line community is
presented. A previous approach that uses AdWords
was presented in (Guerini et al, 2010), but it crowd-
sourced only the running of the experiment, not data
manipulation and analysis, and was not totally con-
trolled for subject randomness.
3 AdWords Features
Google AdWords is Google?s advertising program.
The central idea is to let advertisers display their
messages only to relevant audiences. This is done
by means of keyword-based contextualization on the
Google network, divided into:
? Search network: includes Google search pages,
search sites and properties that display search
results pages (SERPs), such as Froogle and
Earthlink.
? Display network: includes news pages, topic-
specific websites, blogs and other properties -
such as Google Mail and The New York Times.
When a user enters a query like ?cruise? in the
Google search network, Google displays a variety of
relevant pages, along with ads that link to cruise trip
businesses. To be displayed, these ads must be asso-
ciated with relevant keywords selected by the adver-
tiser.
Every advertiser has an AdWords account that is
structured like a pyramid: (i) account, (ii) campaign
and (iii) ad group. In this paper we focus on ad
groups. Each grouping gathers similar keywords to-
gether - for instance by a common theme - around
an ad group. For each ad group, the advertiser sets a
cost-per-click (CPC) bid. The CPC bid refers to the
amount the advertiser is willing to pay for a click on
his ad; the cost of the actual click instead is based
on its quality score (a complex measure out of the
scope of the present paper).
For every ad group there could be multiple ads
to be served, and there are many AdWords measure-
ments for identifying the performance of each single
ad (its persuasiveness, from our point of view):
? CTR, Click Through Rate: measures the num-
ber of clicks divided by the number of impres-
sions (i.e. the number of times an ad has been
displayed in the Google Network).
? Conversion Rate: if someone clicks on an ad,
and buys something on your site, that click is
a conversion from a site visit to a sale. Con-
version rate equals the number of conversions
divided by the number of ad clicks.
? ROI: Other conversions can be page views or
signups. By assigning a value to a conversion
the resulting conversions represents a return on
investment, or ROI.
? Google Analytics Tool: Google Analytics is a
web analytics tool that gives insights into web-
site traffic, like number of visited pages, time
spent on the site, location of visitors, etc.
So far, we have been talking about text ads, -
Google?s most traditional and popular ad format -
because they are the most useful for NLP analysis.
In addition there is also the possibility of creating
the following types of ads:
? Image (and animated) ads
? Video ads
? Local business ads
? Mobile ads
The above formats allow for a greater potential
to investigate persuasive impact of messages (other
than text-based) but their use is beyond the scope of
the present paper1.
4 The ACE Tool
AdWords can be used to design and develop vari-
ous metrics for fast and fully-automated evaluation
experiments, in particular using the ACE tool.
This tool - released in late 2010 - allows testing,
from a marketing perspective, if any change made to
a promotion campaign (e.g. a keyword bid) had a
statistically measurable impact on the campaign it-
self. Our primary aim is slightly different: we are
1For a thorough description of the AdWords tool see:
https://support.google.com/adwords/
990
interested in testing how different messages impact
(possibly different) audiences. Still the ACE tool
goes exactly in the direction we aim at, since it in-
corporates statistically significant testing and allows
avoiding many of the tweaking and tuning actions
which were necessary before its release.
The ACE tool also introduces an option that was
not possible before, that of real-time testing of sta-
tistical significance. This means that it is no longer
necessary to define a-priori the sample size for the
experiment: as soon as a meaningful statistically
significant difference emerges, the experiment can
be stopped.
Another advantage is that the statistical knowl-
edge to evaluate the experiment is no longer nec-
essary: the researcher can focus only on setting up
proper experimental designs2.
The limit of the ACE tool is that it only allows
A/B testing (single split with one control and one ex-
perimental condition) so for experiments with more
than two conditions or for particular experimental
settings that do not fit with ACE testing bound-
aries (e.g. cross campaign comparisons) we suggest
taking (Guerini et al, 2010) as a reference model,
even if the experimental setting is less controlled
(e.g. subject randomness is not equally guaranteed
as with ACE).
Finally it should be noted that even if ACE allows
only A/B testing, it permits the decomposition of al-
most any variable affecting a campaign experiment
in its basic dimensions, and then to segment such
dimensions according to control and experimental
conditions. As an example of this powerful option,
consider Tables 3 and 6 where control and experi-
mental conditions are compared against every single
keyword and every search network/ad position used
for the experiments.
5 Evaluation and Targeting with ACE
Let us consider the design of an experiment with 2
conditions. First we create an ad Group with 2 com-
peting messages (one message for each condition).
Then we choose the serving method (in our opin-
ion the rotate option is better than optimize, since it
2Additional details about ACE features and statistics can be
found at http://www.google.com/ads/innovations/ace.html
guarantees subject randomness and is more transpar-
ent) and the context (language, network, etc.). Then
we activate the ads and wait. As soon as data begins
to be collected we can monitor the two conditions
according to:
? Basic Metrics: the highest CTR measure in-
dicates which message is best performing. It
indicates which message has the highest initial
impact.
? Google Analytics Metrics: measures how much
the messages kept subjects on the site and how
many pages have been viewed. Indicates inter-
est/attitude generated in the subjects.
? Conversion Metrics: measures how much the
messages converted subjects to the final goal.
Indicates complete success of the persuasive
message.
? ROI Metrics: by creating specific ROI values
for every action the user performs on the land-
ing page. The more relevant (from a persuasive
point of view) the action the user performs, the
higher the value we must assign to that action.
In our view combined measurements are better:
for example, there could be cases of messages
with a lower CTR but a higher conversion rate.
Furthermore, AdWords allows very complex tar-
geting options that can help in many different evalu-
ation scenarios:
? Language (see how message impact can vary in
different languages).
? Location (see how message impact can vary in
different cultures sharing the same language).
? Keyword matching (see how message impact
can vary with users having different interests).
? Placements (see how message impact can vary
among people having different values - e.g. the
same message displayed on Democrat or Re-
publican web sites).
? Demographics (see how message impact can
vary according to user gender and age).
5.1 Setting up an Experiment
To test the extent to which AdWords can be ex-
ploited, we focused on how to evaluate lexical varia-
tions of a message. In particular we were interested
991
in gaining insights about a system for affective varia-
tions of existing commentaries on medieval frescoes
for a mobile museum guide that attracts the attention
of visitors towards specific paintings (Guerini et al,
2008; Guerini et al, 2011b). The various steps for
setting up an experiment (or a series of experiments)
are as follows:
Choose a Partner. If you have the opportunity
to have a commercial partner that already has the in-
frastructure for experiments (website, products, etc.)
many of the following steps can be skipped. We as-
sume that this is not the case.
Choose a scenario. Since you may not be
equipped with a VAT code (or with the commercial
partner that furnishes the AdWords account and in-
frastructure), you may need to ?invent something to
promote? without any commercial aim. If a ?social
marketing? scenario is chosen you can select ?per-
sonal? as a ?tax status?, that do not require a VAT
code. In our case we selected cultural heritage pro-
motion, in particular the frescoes of Torre Aquila
(?Eagle Tower?) in Trento. The tower contains a
group of 11 frescoes named ?Ciclo dei Mesi? (cy-
cle of the months) that represent a unique example
of non-religious medieval frescoes in Europe.
Choose an appropriate keyword on which to
advertise, ?medieval art? in our case. It is better
to choose keywords with enough web traffic in or-
der to speed up the experimental process. In our
case the search volume for ?medieval art? (in phrase
match) was around 22.000 hits per month. Another
suggestion is to restrict the matching modality on
Keywords in order to have more control over the
situations in which ads are displayed and to avoid
possible extraneous effects (the order of control
for matching modality is: [exact match], ?phrase
match? and broad match).
Note that such a technical decision - which key-
word to use - is better taken at an early stage of de-
velopment because it affects the following steps.
Write messages optimized for that keyword (e.g.
including it in the title or the body of the ad). Such
optimization must be the same for control and exper-
imental condition. The rest of the ad can be designed
in such a way to meet control and experimental con-
dition design (in our case a message with slightly
affective terms and a similar message with more af-
fectively loaded variations)
Build an appropriate landing page, according
to the keyword and populate the website pages with
relevant material. This is necessary to create a ?cred-
ible environment? for users to interact with.
Incorporate meaningful actions in the website.
Users can perform various actions on a site, and they
can be monitored. The design should include ac-
tions that are meaningful indicators of persuasive ef-
fect/success of the message. In our case we decided
to include some outbound links, representing:
? general interest: ?Buonconsiglio Castle site?
? specific interest: ?Eagle Tower history?
? activated action: ?Timetable and venue?
? complete success: ?Book a visit?
Furthermore, through new Google Analytics fea-
tures, we set up a series of time spent on site and
number of visited pages thresholds to be monitored
in the ACE tool.
5.2 Tips for Planning an Experiment
There are variables, inherent in the Google AdWords
mechanism, that from a research point of view we
shall consider ?extraneous?. We now propose tips
for controlling such extraneous variables.
Add negative matching Keywords: To add more
control, if in doubt, put the words/expressions of the
control and experimental conditions as negative key-
words. This will prevent different highlighting be-
tween the two conditions that can bias the results. It
is not strictly necessary since one can always control
which queries triggered a click through the report
menu. An example: if the only difference between
control and experimental condition is the use of the
adjectives ?gentle knights? vs. ?valorous knights?,
one can use two negative keyword matches: -gentle
and -valorous. Obviously if you are using a key-
word in exact matching to trigger your ads, such as
[knight], this is not necessary.
Frequency capping for the display network: if
you are running ads on the display network, you can
use the ?frequency capping? option set to 1 to add
more control to the experiment. In this way it is as-
sured that ads are displayed only one time per user
on the display network.
Placement bids for the search network: unfor-
tunately this option is no longer available. Basically
the option allowed to bid only for certain positions
992
on the SERPs to avoid possible ?extraneous vari-
ables effect? given by the position. This is best ex-
plained via an example: if, for whatever reason, one
of the two ads gets repeatedly promoted to the pre-
mium position on the SERPs, then the CTR differ-
ence between ads would be strongly biased. From
a research point of view ?premium position? would
then be an extraneous variable to be controlled (i.e.
either both ads get an equal amount of premium po-
sition impressions, or both ads get no premium po-
sition at all). Otherwise the difference in CTR is de-
termined by the ?premium position? rather than by
the independent variable under investigation (pres-
ence/absence of particular affective terms in the text
ad). However even if it is not possible to rule out this
?position effect? it is possible to monitor it by using
the report (Segment > Top vs. other + Experiment)
and checking how many times each ad appeared in
a given position on the SERPs, and see if the ACE
tool reports any statistical difference in the frequen-
cies of ads positions.
Extra experimental time: While planning an ex-
periment, you should also take into account the ads
reviewing time that can take up to several days, in
worst case scenarios. Note that when ads are in eli-
gible status, they begin to show on the Google Net-
work, but they are not approved yet. This means that
the ads can only run on Google search pages and can
only show for users who have turned off SafeSearch
filtering, until they are approved. Eligible ads cannot
run on the Display Network. This status will provide
much less impressions than the final ?approved? sta-
tus.
Avoid seasonal periods: for the above reason,
and to avoid extra costs due to high competition,
avoid seasonal periods (e.g. Christmas time).
Delivery method: if you are planning to use the
Accelerated Delivery method in order to get the re-
sults as quick as possible (in the case of ?quick and
dirty? experiments or ?fast prototyping-evaluation
cycles?) you should consider monitoring your ex-
periment more often (even several times per day) to
avoid running out of budget during the day.
6 Experiments
We ran two pilot experiments to test how affective
variations of existing texts alter their persuasive im-
pact. In particular we were interested in gaining
initial insights about an intelligent system for affec-
tive variations of existing commentaries on medieval
frescoes.
We focused on adjective variations, using a
slightly biased adjective for the control conditions
and a strongly biased variation for the experimen-
tal condition. In these experiments we took it for
granted that affective variations of a message work
better than a neutral version (Van Der Sluis and Mel-
lish, 2010), and we wanted to explore more finely
grained tactics that involve the grade of the vari-
ation (i.e. a moderately positive variation vs. an
extremely positive variation). Note that this is a
more difficult task than the one proposed in (Van
Der Sluis and Mellish, 2010), where they were test-
ing long messages with lots of variations and with
polarized conditions, neutral vs. biased. In addition
we wanted to test how quickly experiments could be
performed (two days versus the two week sugges-
tion of Google).
Adjectives were chosen according to MAX bi-
gram frequencies with the modified noun, using the
Web 1T 5-gram corpus (Brants and Franz, 2006).
Deciding whether this is the best metric for choosing
adjectives to modify a noun or not (e.g. also point-
wise mutual-information score can be used with a
different rationale) is out of the scope of the present
paper, but previous work has already used this ap-
proach (Whitehead and Cavedon, 2010). Top ranked
adjectives were then manually ordered - according to
affective weight - to choose the best one (we used a
standard procedure using 3 annotators and a recon-
ciliation phase for the final decision).
6.1 First Experiment
The first experiment lasted 48 hour with a total of 38
thousand subjects and a cost of 30 euros (see Table
1 for the complete description of the experimental
setup). It was meant to test broadly how affective
variations in the body of the ads performed. The two
variations contained a fragment of a commentary of
the museum guide; the control condition contained
?gentle knight? and ?African lion?, while in the ex-
perimental condition the affective loaded variations
were ?valorous knight? and ?indomitable lion? (see
Figure 1, for the complete ads). As can be seen from
Table 2, the experiment did not yield any significant
993
result, if one looks at the overall analysis. But seg-
menting the results according to the keyword that
triggered the ads (see Table 3) we discovered that
on the ?medieval art? keyword, the control condition
performed better than the experimental one.
Starting Date: 1/2/2012
Ending Date: 1/4/2012
Total Time: 48 hours
Total Cost: 30 euros
Subjects: 38,082
Network: Search and Display
Language: English
Locations: Australia; Canada; UK; US
KeyWords: ?medieval art?, pictures middle ages
Table 1: First Experiment Setup
ACE split Clicks Impr. CTR
Control 31 18,463 0.17%
Experiment 20 19,619 0.10%
Network Clicks Impr. CTR
Search 39 4,348 0.90%
Display 12 34,027 0.04%
TOTAL 51 38,082 0.13%
Table 2: First Experiment Results
Keyword ACE split Impr. CTR
?medieval art? Control 657 0.76%
?medieval art? Experiment 701 0.14%*
medieval times history Control 239 1.67%
medieval times history Experiment 233 0.86%
pictures middle ages Control 1114 1.35%
pictures middle ages Experiment 1215 0.99%
Table 3: First Experiment Results Detail. * indicates a
statistically significant difference with ? < 0.01
Discussion. As already discussed, user moti-
vation is a key element for success in such fine-
grained experiments: while less focused keywords
did not yield any statistically significant differences,
the most specialized keyword ?medieval art? was the
one that yielded results (i.e. if we display messages
like those in Figure 1, that are concerned with me-
dieval art frescoes, only those users really interested
in the topic show different reaction patterns to the af-
fective variations, while those generically interested
in medieval times behave similarly in the two con-
ditions). In the following experiment we tried to see
whether such variations have different effects when
modifying a different element in the text.
Figure 1: Ads used in the first experiment
6.2 Second Experiment
The second experiment lasted 48 hours with a to-
tal of one thousand subjects and a cost of 17 euros
(see Table 4 for the description of the experimen-
tal setup). It was meant to test broadly how affec-
tive variations introduced in the title of the text Ads
performed. The two variations were the same as in
the first experiment for the control condition ?gentle
knight?, and for the experimental condition ?valor-
ous knight? (see Figure 2 for the complete ads). As
can be seen from Table 5, also in this case the experi-
ment did not yield any significant result, if one looks
at the overall analysis. But segmenting the results
according to the search network that triggered the
ads (see Table 6) we discovered that on the search
partners at the ?other? position, the control condition
performed better than the experimental one. Unlike
the first experiment, in this case we segmented ac-
cording to the ad position and search network typol-
ogy since we were running our experiment only on
one keyword in exact match.
Starting Date: 1/7/2012
Ending Date: 1/9/2012
Total Time: 48 hours
Total Cost: 17.5 euros
Subjects: 986
Network: Search
Language: English
Locations: Australia; Canada; UK; US
KeyWords: [medieval knights]
Table 4: Second Experiment Setup
994
Figure 2: Ads used in the second experiment
ACE split Clicks Impr. CTR
Control 10 462 2.16%
Experiment 8 524? 1.52%
TOTAL 18 986 1.82%
Table 5: Second Experiment Results. ? indicates a statis-
tically significant difference with ? < 0.05
Top vs. Other ACE split Impr. CTR
Google search: Top Control 77 6.49%
Google search: Top Experiment 68 2.94%
Google search: Other Control 219 0.00%
Google search: Other Experiment 277* 0.36%
Search partners: Top Control 55 3.64%
Search partners: Top Experiment 65 6.15%
Search partners: Other Control 96 3.12%
Search partners: Other Experiment 105 0.95%?
Total - Search ? 986 1.82%
Table 6: Second Experiment Results Detail. ? indicates a
statistical significance with ? < 0.05, * indicates a sta-
tistical significance with ? < 0.01
Discussion. From this experiment we can confirm
that at least under some circumstances a mild af-
fective variation performs better than a strong varia-
tion. This mild variations seems to work better when
user attention is high (the difference emerged when
ads are displayed in a non-prominent position). Fur-
thermore it seems that modifying the title of the ad
rather than the content yields better results: 0.9% vs.
1.83% CTR (?2 = 6.24; 1 degree of freedom; ? <
0,01) even if these results require further assessment
with dedicated experiments.
As a side note, in this experiment we can see
the problem of extraneous variables: according to
AdWords? internal mechanisms, the experimental
condition was displayed more often in the Google
search Network on the ?other? position (277 vs. 219
impressions - and overall 524 vs. 462), still from a
research perspective this is not a interesting statisti-
cal difference, and ideally should not be present (i.e.
ads should get an equal amount of impressions for
each position).
Conclusions and future work
AdWords gives us an appropriate context for evalu-
ating persuasive messages. The advantages are fast
experiment building and evaluation, fully-automated
analysis, and low cost. By using keywords with a
low CPC it is possible to run large-scale experiments
for just a few euros. AdWords proved to be very ac-
curate, flexible and fast, far beyond our expectations.
We believe careful design of experiments will yield
important results, which was unthinkable before this
opportunity for studies on persuasion appeared.
The motivation for this work was exploration of
the impact of short persuasive messages, so to assess
the effectiveness of different linguistic choices. The
experiments reported in this paper are illustrative ex-
amples of the method proposed and are concerned
with the evaluation of the role of minimal affective
variations of short expressions. But there is enor-
mous further potential in the proposed approach to
ecological crowdsourcing for NLP: for instance, dif-
ferent rhetorical techniques can be checked in prac-
tice with large audiences and fast feedback. The as-
sessment of the effectiveness of a change in the title
as opposed to the initial portion of the text body pro-
vides a useful indication: one can investigate if vari-
ations inside the given or the new part of an expres-
sion or in the topic vs. comment (Levinson, 1983)
have different effects. We believe there is potential
for a concrete extensive exploration of different lin-
guistic theories in a way that was simply not realistic
before.
Acknowledgments
We would like to thank Enrique Alfonseca and
Steve Barrett, from Google Labs, for valuable hints
and discussion on AdWords features. The present
work was partially supported by a Google Research
Award.
995
References
P. Andrews, S. Manandhar, and M. De Boni. 2008. Ar-
gumentative human computer dialogue for automated
persuasion. In Proceedings of the 9th SIGdial Work-
shop on Discourse and Dialogue, pages 138?147. As-
sociation for Computational Linguistics.
S. Aral and D. Walker. 2010. Creating social contagion
through viral product design: A randomized trial of
peer influence in networks. In Proceedings of the 31th
Annual International Conference on Information Sys-
tems.
T. Brants and A. Franz. 2006. Web 1t 5-gram corpus
version 1.1. Linguistic Data Consortium.
BJ Fogg. 2009. Creating persuasive technologies: An
eight-step design process. Proceedings of the 4th In-
ternational Conference on Persuasive Technology.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
M. Guerini, C. Strapparava, and O. Stock. 2010. Evalu-
ation metrics for persuasive nlp with google adwords.
In Proceedings of LREC-2010.
M. Guerini, O. Stock, M. Zancanaro, D.J. O?Keefe,
I. Mazzotta, F. Rosis, I. Poggi, M.Y. Lim, and
R. Aylett. 2011a. Approaches to verbal persuasion in
intelligent user interfaces. Emotion-Oriented Systems,
pages 559?584.
M. Guerini, C. Strapparava, and O. Stock. 2011b. Slant-
ing existing text with Valentino. In Proceedings of the
16th international conference on Intelligent user inter-
faces, pages 439?440. ACM.
S.C. Levinson. 1983. Pragmatics. Cambridge Univer-
sity Press.
W. Mason and S. Suri. 2010. Conducting behavioral
research on amazon?s mechanical turk. Behavior Re-
search Methods, pages 1?23.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2010. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. Proc. of EMNLP 2011.
E. Reiter, R. Robertson, and L. Osman. 2003. Lesson
from a failure: Generating tailored smoking cessation
letters. Artificial Intelligence, 144:41?58.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
I. Van Der Sluis and C. Mellish. 2010. Towards empir-
ical evaluation of affective tactical nlg. In Empirical
methods in natural language generation, pages 242?
263. Springer-Verlag.
S. Whitehead and L. Cavedon. 2010. Generating shifting
sentiment for a conversational agent. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 89?97, Los Angeles, CA, June. Associa-
tion for Computational Linguistics.
996
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651?659,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bridging Languages through Etymology:
The case of cross language text categorization
Vivi Nastase and Carlo Strapparava
Human Language Technologies, Fondazione Bruno Kessler
Trento, Italy
{nastase, strappa}@fbk.eu
Abstract
We propose the hypothesis that word ety-
mology is useful for NLP applications as
a bridge between languages. We support
this hypothesis with experiments in cross-
language (English-Italian) document cat-
egorization. In a straightforward bag-of-
words experimental set-up we add etymo-
logical ancestors of the words in the docu-
ments, and investigate the performance of
a model built on English data, on Italian
test data (and viceversa). The results show
not only statistically significant, but a large
improvement ? a jump of almost 40 points
in F1-score ? over the raw (vanilla bag-of-
words) representation.
1 Introduction
When exposed to a document in a language he
does not know, a reader might be able to glean
some meaning from words that are the same (e.g.
names) or similar to those in a language he knows.
As an example, let us say that an Italian speaker
is reading an English text that contains the word
expense, which he does not know. He may be re-
minded however of the Latin word expensa which
is also the etymological root of the Italian word
spesa, which usually means ?cost?/?shopping?,
and may thus infer that the English word refers
to the cost of things. In the experiments presented
here we investigate whether an automatic text cat-
egorization system could benefit from knowledge
about the etymological roots of words. The cross
language text categorization (CLTC) task consists
of categorizing documents in a target language Lt
using a model built from labeled examples in a
source language Ls. The task becomes more diffi-
cult when the data consists of comparable corpora
in the two languages ? documents on the same top-
ics (e.g. sports, economy) ? instead of parallel cor-
pora ? there exists a one-to-one correspondence
between documents in the corpora for the two lan-
guages, one document being the translation of the
other.
To test the usefulness of etymological in-
formation we work with comparable collec-
tions of news articles in English and Ital-
ian, whose articles are assigned one of four
categories: culture and school, tourism, qual-
ity of life, made in Italy. We perform a progres-
sion of experiments, which embed etymological
information deeper and deeper into the model. We
start with the basic set-up, representing the doc-
uments as bag-of-words, where we train a model
on the English training data, and use this model
to categorize documents from the Italian test data
(and viceversa). The results are better than ran-
dom, but quite low. We then add the etymological
roots of the words in the data to the bag-of-words,
and notice a large ? 21 points ? increase in per-
formance in terms of F1-score. We then use the
bag-of-words representation of the training data to
build a semantic space using LSA, and use the
generated word vectors to represent the training
and test data. The improvement is an additional
16 points in F1-score.
Compared to related work, presented in Sec-
tion 3, where cross language text categorization
is approached through translation or mapping of
features (i.e. words) from the source to the target
language, word etymologies are a novel source of
cross-lingual knowledge. Instead of mapping fea-
tures between languages, we introduce new fea-
tures which are shared, and thus do not need trans-
lation or other forms of mapping.
The experiments presented show unequivocally
that word etymology is a useful addition to com-
putational models, just as they are to readers
who have such knowledge. This is an interest-
ing and useful result, especially in the current
research landscape where using and exploiting
multi-linguality is a desired requirement.
651
morpheme relation related morpheme
eng: ex- rel:etymological origin of eng: excentric
eng: expense rel:etymology lat: expensa
eng: -ly rel:etymological origin of eng: absurdly
eng: -ly rel:etymological origin of eng: admirably
...
ita: spesa rel:etymology lat: expensa
ita: spesa rel:has derived form ita: spese
...
ita: spesare rel:etymologically related ita: spesa
...
lat: expensa rel:etymological origin of eng: expense
lat: expensa rel:etymological origin of ita: spesa
...
lat: expensa rel:is derived from lat: expensus
...
English: muscle
?
French: muscle
?
Latin: musculus
?
Latin: mus
?
Proto Indo-European: muh2s
Figure 1: Sample entries from the Etymological WordNet, and a few etymological layers
2 Word Etymology
Word etymology gives us a glimpse into the evo-
lution of words in a language. Words may be
adopted from a language because of cultural,
scientific, economic, political or other reasons
(Hitchings, 2009). In time these words ?adjust? to
the language that adopted them ? their sense may
change to various degrees ? but they are still se-
mantically related to their etymological roots. To
illustrate the point, we show an example that the
reader, too, may find amusing: on the ticket vali-
dation machine on Italian buses, by way of instruc-
tion, it is written Per obliterare il biglietto .... A
native/frequent English speaker would most prob-
ably key in on, and be puzzled by, the word oblit-
erare, very similar to the English obliterate, whose
most used sense is to destroy completely / cause to
physically disappear . The Italian obliterare has
the ?milder? sense of cancellare ? cancel (which
is also shared by the English obliterate, but is less
frequent according to Merriam-Webster), and both
come from the Latin obliterare ? erase, efface,
cause to disappear. While there has been some
sense migration ? in English the more (physically)
destructive sense of the word has higher promi-
nence, while in Italian the word is closer in mean-
ing to its etymological root ? the Italian and the
English words are still semantically related.
Dictionaries customarily include etymologi-
cal information for their entries, and recently,
Wikipedia?s Wiktionary has joined this trend. The
etymological information can, and indeed has
been extracted and prepared for machine con-
sumption (de Melo and Weikum, 2010): Etymo-
logical WordNet1 contains 6,031,431 entries for
2,877,036 words (actually, morphemes) in 397
languages. A few sample entries from this re-
source are shown in Figure 1.
The information in Etymological WordNet is
organized around 5 relations: etymology with
its inverse etymological origin of; is derived from
with its inverse has derived form; and the sym-
metrical etymologically related. The etymology
relation links a word with its etymological ances-
tors, and it is the relation used in the experiments
presented here. Prefixes and suffixes ? such as ex-
and -ly shown in Figure 1 ? are filtered out, as
they bring in much noise by relating words that
merely share such a morpheme (e.g. absurdly and
admirably) but are otherwise semantically distant.
has derived form is also used, to capture morpho-
logical variations.
The depth of the etymological hierarchy (con-
sidering the etymology relations) is 10. Figure 1
shows an example of a word with several levels of
etymological ancestry.
1http://www1.icsi.berkeley.edu/
?demelo/etymwn/
652
?
??????????????????????????????????????????????
English texts Italian texts
te1 te2 ? ? ? ten?1 ten ti1 ti2 ? ? ? tim?1 tim
we1 0 1 ? ? ? 0 1 0 0 ? ? ?
English
Lexicon
we2 1 1 ? ? ? 1 0 0
. . .
... . . . . . . . . . . . . . . . . . . . . . . . . ... 0 ...
wep?1 0 1 ? ? ? 0 0
. . . 0
wep 0 1 ? ? ? 0 0 ? ? ? 0 0
shared
names and
words
we/i1 1 0 ? ? ? 0 0 0 0 ? ? ? 0 1
... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
common
etymology
wetym1 0 1 ? ? ? 0 0 0 0 ? ? ? 1 0
... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
wi1 0 0 ? ? ? 0 1 ? ? ? 1 1
Italian
Lexicon
wi2 0
. . . 1 1 ? ? ? 0 1
... ... 0 ... . . . . . . . . . . . . . . . . . . . . . . . . .
wiq?1
. . . 0 0 1 ? ? ? 0 1
wiq ? ? ? 0 0 0 1 ? ? ? 1 0
?
??????????????????????????????????????????????
Figure 2: Multilingual word-by-document matrix
3 Cross Language Text Categorization
Text categorization (also text classification), ?the
task of automatically sorting a set of documents
into categories (or classes or topics) from a prede-
fined set? (Sebastiani, 2005), allows for the quick
selection of documents from the same domain, or
the same topic. It is a very well research area, dat-
ing back to the 60s (Borko and Bernick, 1962).
The most frequently, and successfully, used docu-
ment representation is the bag-of-words (BoWs).
Results using this representation achieve accuracy
in the 90%s. Most variations include feature filter-
ing or weighing, and variations in learning algo-
rithms (Sebastiani, 2005).
Within the area of cross-language text catego-
rization (CLTC) several methods have been ex-
plored for producing the model for a target lan-
guage Lt using information and data from the
source language Ls. In a precursor task to CLTC,
cross language information retrieval (CLIR), Du-
mais et al (1997) find semantic correspondences
in parallel (different language) corpora through la-
tent semantic analysis (LSA). Most CLTC meth-
ods rely heavily on machine translation (MT). MT
has been used: to cast the cross-language text
categorization problem to the monolingual setting
(Fortuna and Shawe-Taylor, 2005); to cast the
cross-language text categorization problem into
two monolingual settings for active learning (Liu
et al, 2012); to translate and adapt a model built
on language Ls to language Lt (Rigutini et al,
2005), (Shi et al, 2010); to produce parallel
corpora for multi-view learning (Guo and Xiao,
2012). Wan et al (2011) also use machine trans-
lation, but enhance the processing through domain
adaptation by feature weighing, assuming that the
training data in one language and the test data in
the other come from different domains, or can ex-
hibit different linguistic phenomena due to linguis-
tic and cultural differences. Prettenhofer and Stein
(2010) use a word translation oracle to produce
pivots ? pairs of semantically similar words ? and
use the data partitions induced by these words to
find cross language structural correspondences.
In a computationally lighter framework, not de-
pendent on MT, Gliozzo and Strapparava (2006)
and Wu et al (2008) use bilingual lexicons and
aligned WordNet synsets to obtain shared features
between the training data in language Ls and the
testing data in language Lt. Gliozzo and Strap-
parava (2005), the first to use comparable as op-
653
posed to parallel corpora for CLTC, use LSA to
build multilingual domain models.
The bag-of-word document representation
maps a document di from a corpus D into a k-
dimensional space Rk, where k is the dimension
of the (possibly filtered) vocabulary of the corpus:
W = {w1, ..., wk}. Position j in the vector
representation of di corresponds to word wj , and
it may have different values, among the most
commonly used being: binary values ? wj appears
(1) or not (0) in di; frequency of occurrence of wj
in di, absolute or normalized (relative to the size
of the document or the size of the vocabulary); the
tf ? idf(wj , di, D).
For the task of cross language text categoriza-
tion, the problem of sharing a model across lan-
guages is that the dimensions, a.k.a the vocabu-
lary, of the two languages are largely different.
Limited overlap can be achieved through shared
names and words. As we have seen in the lit-
erature review, machine translation and bilingual
dictionaries can be used to cast these dimensions
from the source language Ls to the target language
Lt. In this work we explore expanding the shared
dimensions through word etymologies. Figure 2
shows schematically the binary k dimensional rep-
resentation for English and Italian data, and shared
dimensions.
Cross language text categorization could be
used to obtain comparable corpora for building
translation models. In such a situation, relying on
a framework that itself relies on machine transla-
tion is not helpful. Bilingual lexicons are available
for frequently studied languages, but less so for
those poorer in resources. Considering such short-
comings, we look into additional linguistic infor-
mation, in particular word etymology. This infor-
mation impacts the data representation, by intro-
ducing new shared features between the different
language corpora without the need for translation
or other forms of mapping. The newly produced
representation can be used in conjunction with any
of the previously proposed algorithms.
Word etymologies are a novel source of linguis-
tic information in NLP, possibly because resources
that capture this information in a machine readable
format are also novel. Fang et al (2009) used lim-
ited etymological information extracted from the
Collins English Dictionary (CED) for text catego-
rization on the British National Corpus (BNC): in-
formation on the provenance of words (ranges of
probability distribution of etymologies in different
versions of Latin ? New Latin, Late Latin, Me-
dieval Latin) was used in a ?home-made? range
classifier.
The experiments presented in this paper use the
bag-of-word document representation with abso-
lute frequency values. To this basic representation
we add word etymological ancestors and run clas-
sification experiments. We then use LSA ? previ-
ously shown by (Dumais et al, 1997) and (Gliozzo
and Strapparava, 2005) to be useful for this task ?
to induce the latent semantic dimensions of docu-
ments and words respectively, hypothesizing that
word etymological ancestors will lead to semantic
dimensions that transcend language boundaries.
The vectors obtained through LSA (on the training
data only) for words that are shared by the English
training data and the Italian test data (names, and
most importantly, etymological ancestors of words
in the original documents) are then used for re-
representing the training and test data. The same
process is applied for Italian training and English
test data. Classification is done using support vec-
tor machines (SVMs).
3.1 Data
The data we work with consists of compara-
ble corpora of news articles in English and Ital-
ian. Each news article is annotated with one of
the four categories: culture and school, tourism,
quality of life, made in Italy. Table 1 shows the
dataset statistics. The average document length is
approximately 300 words.
3.2 Raw cross-lingual text categorization
As is commonly done in text categorization (Se-
bastiani, 2005), the documents in our data are
represented as bag-of-words, and classification is
done using support vector machines (SVMs).
One experimental run consists of 4 binary ex-
periments ? one class versus the rest, for each of
the 4 classes. The results are reported through
micro-averaged precision, recall and F1-score for
the targeted class, as well as overall accuracy. The
high results, on a par with text categorization ex-
periments in the field, validates our experimental
set-up.
For the cross language categorization experi-
ments described in this paper, we use the data
described above, and train on one language (En-
glish/Italian), and test on the other, using the same
654
English Italian
Categories Training Test Total Training Test Total
quality of life 5759 1989 7748 5781 1901 7682
made in Italy 5711 1864 7575 6111 2068 8179
tourism 5731 1857 7588 6090 2015 8105
culture and school 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
Table 1: Dataset statistics
monolingual BoW categorization
Prec Rec F1 Acc
Train EN / Test EN 0.92 0.92 0.92 0.96
Train IT / Test IT 0.94 0.94 0.94 0.97
Table 2: Performance for monolingual raw text
categorization
experimental set-up as for the monolingual sce-
nario (4 binary problems). The categorization
baseline (BoW baseline in Figure 4) was obtained
in this set-up. This baseline is higher than the ran-
dom baseline or the positive class baseline2 (all in-
stances are assigned the target class in each of the
4 binary classification experiments) due to shared
words and names between the two languages.
3.3 Enriching the bag-of-word
representation with word etymology
As personal experience has shown us that etymo-
logical information is useful for comprehending
a text in a different language, we set out to test
whether this information can be useful in an auto-
matic processing setting. We first verified whether
the vocabularies of our two corpora, English and
Italian, have shared word etymologies. Relying
on word etymologies from the Etymological dic-
tionary, we found that from our data?s vocabulary,
518 English terms and 543 Italian terms shared
490 direct etymological ancestors. Etymological
ancestors also help cluster related terms within one
language ? 887 etymological ancestors for 4727
English and 864 ancestors for 5167 Italian terms.
This overlap further increases when adding de-
rived forms (through the has derived form rela-
tion). The fact that this overlap exists strengthens
the motivation to try using etymological ancestors
for the task of text categorization.
In this first step of integrating word etymology
2In this situation the random and positive class baseline
are the same: 25% F1 score.
into the experiment, we extract for each word in
each document in the dataset its ancestors from
the Etymological dictionary. Because each word
wj in a document di has associated an absolute
frequency value fij (the number of occurrences of
wj in di), for the added etymological ancestors ek
in document Di we associate as value the sum of
frequencies of their etymological children in di:
fiek =
?
wj?di
wjetymology ek
fij
We make the depth of extraction a parameter,
and generate data representation when consider-
ing only direct etymological antecedents (depth 1)
and then up to a distance of N. For our dataset we
noticed that the representation does not change af-
ter N=4, so this is the maximum depth we con-
sider. The bag-of-words representation for each
document is expanded with the corresponding et-
ymological features.
expansion training data vo-
cabulary size
vocabulary over-
lap with testing
Train EN /Test IT
raw 71122 14207 (19.9%)
depth 1 78936 18275 (23.1%)
depth 2 79068 18359 (23.2%)
depth 3 79100 18380 (23.2%)
depth 4 79103 18382 (23.2%)
Train IT /Test EN
raw 78750 14110 (17.9%)
depth 1 83656 18682 (22.3%)
depth 2 83746 18785 (22.4%)
depth 3 83769 18812 (22.5%)
depth 4 83771 18814 (22.5%)
Table 3: Feature expansion with word etymologies
Table 3 shows the training data vocabulary size
and increase in the overlap between the training
and test data with the addition of etymological fea-
655
tures. The increase is largest when introducing
the immediate etymological ancestors, of approx-
imately 4000 new (overlapping) features for both
combinations of training and testing. Without ety-
mological features the overlap was approximately
14000 for both configurations. The results ob-
tained with this enriched BoW representation for
etymological ancestor depth 1, 2 and 3 are pre-
sented in Figure 4.
3.4 Cross-lingual text categorization in a
latent semantic space adding etymology
Shared word etymologies can serve as a bridge be-
tween two languages as we have seen in the pre-
vious configuration. When using shared word et-
ymologies in the bag-of-words representation, we
only take advantage of the shallow association be-
tween these new features and the classes within
which they appear. But through the co-occurrence
of the etymological features and other words in
different documents in the training data, we can
induce a deeper representation for the words in
a document, that captures better the relationship
between the features (words) and the classes to
which the documents belong. We use latent se-
mantic analysis (LSA) (Deerwester et al, 1990)
to perform this representational transformation.
The process relies on the assumption that word
co-occurrences across different documents are the
surface manifestation of shared semantic dimen-
sions. Mathematically, the ?word ? document?
matrix D is expressed as a product of three ma-
trices:
D = V ?UT
by performing singular value decomposition
(SVD). V would correspond roughly to a ?word
? latent semantic dimension? matrix, UT is the
transposed of a ?document ? latent semantic
dimension? matrix, and ? is a diagonal matrix
whose values are indicative of the ?strength? of the
semantic dimensions. By reducing the size of ?,
for example by selecting the dimensions with the
top K values, we can obtain an approximation of
the original matrix D ? DK = VK?KUTK , where
we restrict the latent semantic dimensions taken
into account to the K chosen ones. Figure 3 shows
schematically the process.
We perform this decomposition and dimension
reduction step on the ?word ? document? ma-
trix built from the training data only, and using
K=400. Both the training and test data are then
reduction
SVDanddimension
dimensionlatent semantic dimensionlatent semantic
d
i
m
e
n
s
i
o
n
l
a
t
e
n
t
 
s
e
m
a
n
t
i
c
K x K
d
i
m
e
n
s
i
o
n
l
a
t
e
n
t
 
s
e
m
a
n
t
i
c
w
o
r
d
s
V x D K x D
documents documents
V x K
w
o
r
d
s
x x
Figure 3: Schematic view of LSA
re-represented through the new word vectors from
matrix VK . Because the LSA space was built only
from the training data, only the shared words and
shared etymological ancestors are used to produce
representations of the test data. The categorization
is done again with SVM. The results of this exper-
iment are shown in Figure 4, together with an LSA
baseline ? using the raw data and relying on shared
words and names as overlap.
4 Discussion
The experiments whose results we present here
were produced using unfiltered data ? all words in
the datasets, all etymological ancestors up to the
desired depth, no filtering based on frequency of
occurrence. Feature filtering is commonly done in
machine learning when the data has many features,
and in text categorization when using the bag-of-
words representation in particular. We chose not to
perform this step for two main reasons: (i) filter-
ing is sensitive to the chosen threshold; (ii) LSA
thrives on word co-occurrences, which would be
drastically reduced by word removal. The point
that etymology information is a useful addition to
the task of cross-language text categorization can
be made without finding the optimal filtering set-
up.
The baseline experiments show that despite
the relatively large word overlap (approx. 14000
terms), cross-language text categorization gives
low results. Adding a first batch of etymological
information ? approximately 4000 shared immedi-
ate ancestors ? leads to an increase of 18 points in
terms of F1-score on the BoW experimental set-up
for English training/Italian testing, and 21 points
for Italian training/English testing. Further addi-
tions of etymological ancestors at depths 2 and
3 results in an increase of 21 points in terms of
F1-score for English training/Italian testing, and
27 points for Italian training/English testing. The
higher increase in performance on this experimen-
tal configuration for Italian training/English test-
ing is explained by the higher term overlap be-
656
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
F
1
?
s
c
o
r
e
Italian training, English testing
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
F
1
?
s
c
o
r
e
English training, Italian testing
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
A
c
c
u
r
a
c
y
Italian training, English testing
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
A
c
c
u
r
a
c
y
English training, Italian testing
0.42
0.830.79
0.65
0.43
BoW_etym BoW_etym LSA_etym
LSA_etymBoW_etym
LSA_etym
LSA_etymBoW_etym
depth=1 depth=2 depth=3
0.54
0.84 0.87 0.82
0.89
0.74
0.69
0.80
0.64
BoW_baseline LSA_baseline
0.720.71
Figure 4: CLTC results with etymological features
tween the training and test data, as evidenced by
the statistics in Table 3.
The next processing step induced a represen-
tation of the shared words that encodes deeper
level dependencies between words and documents
based on word co-occurrences in documents. The
LSA space built on the training data leads to a
vector representation of the shared words, includ-
ing the shared etymological ancestors, that cap-
tures more than the obvious word-document co-
occurrences. Using this representation leads to a
further increase of 15 points in F1-score for En-
glish training/Italian testing set-up over the BoW
representation, and 14 points over the baseline
LSA-based categorization. The increase for the
Italian training/English testing is 5 points over the
BoW representation, but 20 points over the base-
line LSA. We saw that the high performance BoW
on Italian training/English testing is due to the
high term overlap. The clue to why the increase
when using LSA is lower than for English train-
ing/Italian testing is in the way LSA operates ? it
relies heavily on word co-occurrences in finding
the latent semantic dimensions of documents and
words. We expect then that in the Italian training
collection, words are ?less shared? among docu-
ments, which means a lower average document
frequency. Figure 5 shows the changes in aver-
age document frequency for the two training col-
lections, starting with the raw data (depth 0), and
with additional etymological features.
 50
 60
 70
 80
 90
 100
 110
 120
 130
 140
 0  1  2  3  4
A
v
e
r
a
g
e
 
D
F
Etymology depth
Average document frequency for words in the training data
ENIT
Figure 5: Document frequency changes with the
addition of etymological features
The shape of the document frequency curves
mirror the LSA results ? the largest increase is the
effect of adding the set of direct etymological an-
cestors, and additions of further, more distant, an-
cestors lead to smaller improvements.
657
We have performed the experiments described
above on two releases of the Etymological dictio-
nary. The results described in the paper were ob-
tained on the latest release (February 2013). The
difference in results on the two dictionary versions
was significant: a 4 and 5 points increase respec-
tively in micro-averaged F1-score in the bag-of-
words setting for English training/Italian testing
and Italian training/English testing, and a 2 and
6 points increase in the LSA setting. This indi-
cates that more etymological information is better,
and the dynamic nature of Wikipedia and the Wik-
tionary could lead to an ever increasing and better
etymological resource for NLP applications.
5 Conclusion
The motivation for this work was to test the hy-
pothesis that information about word etymology is
useful for computational approaches to language,
in particular for text classification. Cross-language
text classification can be used to build compara-
ble corpora in different languages, using a single
language starting point, preferably one with more
resources, that can thus spill over to other lan-
guages. The experiments presented have shown
clearly that etymological ancestors can be used
to provide the necessary bridge between the lan-
guages we considered ? English and Italian. Mod-
els produced on English data when using etymo-
logical information perform with high accuracy
(89%) and high F1-score (80) on Italian test data,
with an increase of almost 40 points over a simple
bag-of-words model, which, for crossing language
boundaries, relies exclusively on shared names
and words. Training on Italian data and testing on
English data performed almost as well (87% accu-
racy, 75 F1-score). We plan to expand our experi-
ments to more languages with shared etymologies,
and investigate what characteristics of languages
and data indicate that etymological information is
beneficial for the task at hand.
We also plan to explore further uses for this lan-
guage bridge, at a finer semantic level. Monolin-
gual and cross-lingual textual entailment in par-
ticular would be interesting applications, because
they require finding shared meaning on two text
fragments. Word etymologies would allow recog-
nizing words with shared ancestors, and thus with
shared meaning, both within and across languages.
Acknowledgements
We thank the reviewers for the helpful comments.
This work was financially supported by the EC-
funded project EXCITEMENT ? EXploring Cus-
tomer Interactions through Textual EntailMENT
FP7 ICT-287923. Carlo Strapparava was partially
supported by the PerTe project (Trento RISE).
References
Harold Borko and Myrna Bernick. 1962. Auto-
matic Document Classification. System Develop-
ment Corporation, Santa Monica, CA.
Gerard de Melo and Gerhard Weikum. 2010. Towards
universal multilingual knowledge bases. In Prin-
ciples, Construction, and Applications of Multilin-
gual Wordnets. Proceedings of the 5th Global Word-
Net Conference (GWC 2010), pages 149?156, New
Delhi, India.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the American Socienty for Information Science,
41(6):391?407.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent semantic
indexing. In AAAI Symposium on CrossLanguage
Text and Speech Retrieval.
Alex Chengyu Fang, Wanyin Li, and Nancy Ide. 2009.
Latin etymologies as features on BNC text cate-
gorization. In 23rd Pacific Asia Conference on
Language, Information and Computation (PACLIC
2009), pages 662?669.
Blaz Fortuna and John Shawe-Taylor. 2005. The use of
machine translation tools for cross-lingual text min-
ing. In Learning with multiple views ? Workshop
at the 22nd International Conference on Machine
Learning (ICML 2005).
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts.
Alfio Gliozzo and Carlo Strapparava. 2006. Ex-
ploiting comparable corpora and bilingual dictionar-
ies for cross-language text categorization. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics (COLING-ACL 2006), pages 553?560, Sydney,
Australia.
658
Yuhong Guo and Min Xiao. 2012. Cross language
text classification via subspace co-regularized multi-
view learning. In Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML
2012), Edinburgh, Scotland, UK.
Henry Hitchings. 2009. The Secret Life of Words: How
English Became English. John Murray Publishers.
Yue Liu, Lin Dai, Weitao Zhou, and Heyan Huang.
2012. Active learning for cross language text cat-
egorization. In Proceedings of the 16th Pacific-Asia
conference on Advances in Knowledge Discovery
and Data Mining (PAKDD 2012), pages 195?206,
Kuala Lumpur, Malaysia.
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural corre-
spondence learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1118?1127, Uppsala,
Sweden.
Leonardo Rigutini, Marco Maggini, and Bing Liu.
2005. An EM based training algorithm for cross-
language text categorization. In Proceedings of the
International Conference on Web Intelligence (WI
2005), pages 200?206, Compiegne, France.
Fabrizio Sebastiani. 2005. Text categorization. In
Alessandro Zanasi, editor, Text Mining and its Ap-
plications, pages 109?129. WIT Press, Southamp-
ton, UK.
Lei Shi, Rada Mihalcea, and Minhgjun Tian. 2010.
Cross language text classification by model trans-
lation and semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
1057?1067, Uppsala, Sweden.
Chang Wan, Rong Pan, and Jifei Li. 2011. Bi-
weighting domain adaptation for cross-language text
classification. In Proceedings of the 22nd Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI 2011), pages 1535?1540, Barcelona, Catalonia,
Spain.
Ke Wu, Xiaolin Wang, and Bao-Liang Lu. 2008.
Cross language text categorization using a bilingual
lexicon. In Third International Joint Conference
on Natural Language Processing (IJCNLP 2008),
pages 165?172, Hyderabad, India.
659
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1446?1455,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
BRAINSUP: Brainstorming Support for Creative Sentence Generation
Go?zde O?zbal
FBK-irst
Trento, Italy
gozbalde@gmail.com
Daniele Pighin
Google Inc.
Zu?rich, Switzerland
daniele.pighin@gmail.com
Carlo Strapparava
FBK-irst
Trento, Italy
strappa@fbk.eu
Abstract
We present BRAINSUP, an extensible
framework for the generation of creative
sentences in which users are able to
force several words to appear in the sen-
tences and to control the generation pro-
cess across several semantic dimensions,
namely emotions, colors, domain related-
ness and phonetic properties. We evalu-
ate its performance on a creative sentence
generation task, showing its capability of
generating well-formed, catchy and effec-
tive sentences that have all the good qual-
ities of slogans produced by human copy-
writers.
1 Introduction
A variety of real-world scenarios involve talented
and knowledgable people in a time-consuming
process to write creative, original sentences gen-
erated according to well-defined requisites. For
instance, to advertise a new product it could be
desirable to have its name appearing in a punchy
sentence together with some keywords relevant for
marketing, e.g. ?fresh?, or ?thirst? for the adver-
tisement of a drink. Besides, it could be interesting
to characterize the sentence with respect to a spe-
cific color, like ?blue? to convey the idea of fresh-
ness, or to a color more related to the brand of the
company, e.g. ?red? for a new Ferrari. Moreover,
making the slogan evoke ?joy? or ?satisfaction?
could make the advertisement even more catchy
for customers. On the other hand, there are many
examples of provocative slogans in which copy-
writers try to impress their readers by suscitating
strong negative feelings, as in the case of anti-
smoke campaigns (e.g., ?there are cooler ways to
die than smoking? or ?cancer cures smoking?), or
the famous beer motto ?Guinness is not good for
you?. As another scenario, creative sentence gen-
eration is also a useful teaching device. For ex-
ample, the keyword or linkword method used for
second language learning links the translation of
a foreign (target) word to one or more keywords
in the native language which are phonologically
or lexically similar to the target word (Sagarra and
Alba, 2006). To illustrate, for teaching the Ital-
ian word ?tenda?, which means ?curtain? in En-
glish, the learners are asked to imagine ?rubbing
a tender part of their leg with a curtain?. These
words should co-occur in the same sentence, but
constructing such sentences by hand can be a dif-
ficult and very time-consuming process. O?zbal
and Strapparava (2011), who attempted to auto-
mate the process, conclude that the inability to re-
trieve from the web a good sentence for all cases
is a major bottleneck.
Although state of the art computational mod-
els of creativity often produce remarkable results,
e.g., Manurung et al (2008), Greene et al (2010),
Guerini et al (2011), Colton et al (2012) just to
name a few, to our best knowledge there is no at-
tempt to develop an unified framework for the gen-
eration of creative sentences in which users can
control all the variables involved in the creative
process to achieve the desired effect.
In this paper, we advocate the use of syntactic
information to generate creative utterances by de-
scribing a methodology that accounts for lexical
and phonetic constraints and multiple semantic di-
mensions at the same time. We present BRAIN-
SUP, an extensible framework for creative sen-
tence generation in which users can control all the
parameters of the creative process, thus generat-
ing sentences that can be used for practical ap-
plications. First, users can define a set of key-
words which must appear in the final sentence.
Second, they can slant the output towards a spe-
1446
Domain Keywords BRAINSUP output examples
coffee waking,
cup
Between waking and doing there
is a wondrous cup.
coke drink, ex-
haustion
The physical exhaustion wants
the dark drink.
health day, juice,
sunshine
With juice and cereal the normal
day becomes a summer sunshine.
beauty kiss,lips
Passionate kiss, perfect lips. ?
Lips and eyes want the kiss.
mascara drama,lash
Lash your drama to the stage. ?
A mighty drama, a biting lash.
pickle crunch, bite Crunch your bite to the top. ?
Crunch of a savage byte. ? A
large byte may crunch a little at-
tention.
soap
skin,
love,
touch
A touch of love is worth a fortune
of skin. ? The touch of froth is
the skin of love. ? A skin of water
is worth a touch of love.
Table 1: A selection of sentences automatically
generated by BRAINSUP for specific domains.
cific emotion, color or domain. At the same time,
they can require a sentence to include desired pho-
netic properties, such as rhymes, alliteration or
plosives. The combination of these features al-
lows for the generation of potentially catchy and
memorable sentences by establishing connections
between linguistic, emotional (LaBar and Cabeza,
2006), echoic and visual (Borman et al, 2005)
memory, as exemplified by the system outputs
showcased in Table 1. Other creative dimensions
can easily be plugged in, due to the inherently
modular structure of the system.
BRAINSUP supports the creative process by
greedily exploring a huge solution space to pro-
duce completely novel utterances responding to
user requisites. It exploits syntactic constraints to
dramatically cut the size of the search space, thus
making it possible to focus on the creative aspects
of sentence generation.
2 Related work
Research in creative language generation has
bloomed in recent years. In this section, we pro-
vide a necessarily succint overview of a selection
of the studies that most heavily inspired and influ-
enced the development of BRAINSUP.
Humor generators are a notable class of sys-
tems exploring new venues in computational cre-
ativity (Binsted and Ritchie, 1997; McKay, 2002;
Manurung et al, 2008). Valitutti et al (2009)
present an interactive system which generates hu-
morous puns obtained through variation of famil-
iar expressions with word substitution. The varia-
tion takes place considering the phonetic distance
and semantic constraints such as semantic similar-
ity, semantic domain opposition and affective po-
larity difference. Possibly closer to slogan genera-
tion, Guerini et al (2011) slant existing textual ex-
pressions to obtain more positively or negatively
valenced versions using WordNet (Miller, 1995)
semantic relations and SentiWordNet (Esuli and
Sebastiani, 2006) annotations. Stock and Strap-
parava (2006) generate acronyms based on lexical
substitution via semantic field opposition, rhyme,
rythm and semantic relations. The model is lim-
ited to the generation of noun phrases.
Poetry generation systems face similar chal-
lenges to BRAINSUP as they struggle to combine
semantic, lexical and phonetic features in a unified
framework. Greene et al (2010) describe a model
for poetry generation in which users can control
meter and rhyme scheme. Generation is modeled
as a cascade of weighted Finite State Transduc-
ers that only accept strings conforming to the de-
sired rhyming scheme. Toivanen et al (2012) at-
tempt to generate novel poems by replacing words
in existing poetry with morphologically compat-
ible words that are semantically related to a tar-
get domain. Content control and the inclusion of
phonetic features are left as future work and syn-
tactic information is not taken into account. The
Electronic Text Composition project1 is a corpus
based approach to poetry generation which recur-
sively combines automatically generated linguistic
constituents into grammatical sentences. Colton et
al. (2012) propose another data-driven approach to
poetry generation based on simile transformation.
The mood and theme of the poems are influenced
by daily news. Constraints about phonetic proper-
ties of the selected words or their frequencies can
be enforced during retrieval. Unlike these exam-
ples, BRAINSUP makes heavy use of syntactic in-
formation to enforce well-formed sentences and to
constraint the search for a solution, and provides
an extensible framework in which various forms
of linguistic creativity can easily be incorporated.
Several slogan generators are available on the
web2, but their capabilities are very limited as they
can only replace single words or word sequences
within existing slogan. This often results in syn-
tactically incorrect outputs. Furthermore, they do
not allow for other forms of user control.
1http://slought.org/content/11199
2E.g.: http://www.procato.com/slogan+
generator, http://www.sloganizer.net/en/,
http://www.sloganmania.com/index.htm.
1447
3 Architecture of BRAINSUP
To effectively support the creative process with
useful suggestions, we must be able to generate
sentences conforming to the user needs. First of
all, users can select the target words that need to
appear in the sentence. In the context of second
language learning, these might be the words that a
learner must associate in order to expand her vo-
cabulary. For slogan generation, the target words
could be the key features of a product, or target-
defining keywords that copywriters want to explic-
itly mention. On top of that, a user can character-
ize the generated sentences according to several
dimensions, namely: 1) a specific semantic do-
main, e.g.: ?sports? or ?blankets?; 2) a specific
emotion, e.g., ?joy?, ?anger? or just ?negative?; 3)
a specific color, e.g., ?red? or ?blue?; 4) a com-
bination of phonetic properties of the words that
will appear in the sentence, i.e., rhymes, allitera-
tions and plosives. More formally, the user input
is a tuple: U = ?t,d, c, e, p,w? , where t is the
set of target words, d is a set of words defining the
target domain, c and p are, respectively, the color
and the emotion towards which the user wants to
slant the sentence, p represents the desired pho-
netic features, and w is a set of weights that control
the influence of each dimension on the generative
process, as detailed in Section 3.3. For target and
domain words, users can explicitly select one or
more POSes to be considered, e.g., ?drink/verb?
or ?drink/verb,noun?.
The sentence generation process is based on
morpho-syntactic patterns which we automati-
cally discover from a corpus of dependency parsed
sentences P . These patterns represent very gen-
eral skeletons of well-formed sentences that we
employ to generate creative sentences by only
focusing on the lexical aspects of the process.
Candidate fillers for each empty position (slot)
in the patterns are chosen according to the lexi-
cal and syntactic constraints enforced by the de-
pendency relations in the patterns. These con-
straints are learned from relation-head-modifier
co-occurrence counts estimated from a depen-
dency treebank L. A beam search in the space of
all possible lexicalizations of a syntactic pattern
promotes the words with the highest likelihood of
satisfying the user specification.
Algorithm 1 provides a high-level description of
the creative sentence generation process. Here, ?
is a set of meta-parameters that affect search com-
plexity and running time of the algorithm, such
as the minimum/maximum number of solutions to
Algorithm 1 SentenceGeneration(U,?,P,L): U is the
user specification, ? is a set of meta-parameters; P and L are
two dependency treebanks.
O ? ?
for all p ? CompatiblePatterns?(U,P) dowhile NotEnoughSolutions?(O) do
O ? O ? FillInPattern?(U, p,L)
return SelectBestSolutions?(O)
DT NNS VBD DT JJ NN IN DT NN
The * * a * * in the *
det nsubj
dobj
det
amod
prep
pobj
det
Figure 1: Example of a syntactic pattern. A ?*?
represents an empty slot to be filled with a filler.
be generated, the maximum number of patterns to
consider, or the maximum size of the generated
sentences. CompatiblePatterns(?) finds the most
frequent syntactic patterns in P that are compat-
ible with the user specification, as explained in
Section 3.1; FillInPattern(?) carries out the beam
search, and returns the best solutions generated for
each pattern p given U . The algorithm terminates
when at least a minimum number of solutions have
been generated, or when all the compatible pat-
terns have been exhausted. Finally, only the best
among the generated solutions are shown to the
user. More details about the search in the solution
space are provided in Section 3.2.
3.1 Pattern selection
We generate creative sentences starting from
morpho-syntactic patterns which have been au-
tomatically learned from a large corpus P . The
choice of the corpus from which the patterns
are extracted constitutes the first element of the
creative sentence generation process, as differ-
ent choices will generate sentences with different
styles. For example, a corpus of slogans or punch-
lines can result in short, catchy and memorable
sentences, whereas a corpus of simplified English
would be a better choice to learn a second lan-
guage or to address low reading level audiences.
A pattern is the syntactic skeleton of a class
of sentences observed in P . Within a pattern, a
second element of creativity involves the selec-
tion of original combinations of words (fillers) that
do not violate the grammaticality of the sentence.
The patterns that we employ are automatic de-
pendency trees from which all content-words have
been removed, as exemplified in Figure 1. After
selecting the target corpus, we parse all the sen-
tences with the Stanford Parser (Klein and Man-
1448
ning, 2003) and produce the patterns by stripping
away all content words from the parses. Then,
for each pattern we count how many times it has
been observed in the corpus. Additionally, we
keep track of what kind of empty slots, i.e., empty
positions, are available in each pattern. For ex-
ample, the pattern in Figure 1 can accommodate
up to two singular nouns (NN), one plural noun
(NNS), one adjective (JJ) and one verb in the past
tense (VBD). This information is needed to se-
lect the patterns which are compatible with the
target words t in the user specification U . For
example, this pattern is not compatible with t =
[heading/VBG, edge/NN] as the pattern does not
have an empty slot for a gerundive verb, while it
satisfies t = [heading/NN, edge/NN] as it can
accommodate the two singular nouns. While re-
trieving patterns, we also need to enforce that a
pattern be not completely filled just by adding the
target words t, as under these conditions there
would be no room to achieve any kind of creative
effect. Therefore, we also require that the pat-
terns retrieved by CompatiblePatterns(?) have
more empty slots than the size of t. The mini-
mum and maximum number of excess slots in the
pattern are two other meta-parameters controlled
by ?. CompatiblePatterns(?) returns compati-
ble patterns ordered by their frequency, i.e. when
generating solutions the first patterns that are ex-
plored are the most frequently observed ones. In
this way, we achieve the following two objectives:
1) we compensate for the unavoidable errors intro-
duced by the automatic parser, as frequently ob-
served parses are less likely to be the result of
an erroneous interpretation of a sentence; and 2)
we generate sentences that are most likely to be
catchy and memorable, being based on syntactic
constructs that are used more frequently. To avoid
always selecting the same patterns for the same
kinds of inputs, we add a small random compo-
nent (also controlled by ?) to the pattern sorting
algorithm, thus allowing for sentences to be gen-
erated also from non-top ranked patterns.
3.2 Searching the solution space
With the compatible patterns selected, we can ini-
tiate a beam search in the space of all possible
lexicalizations of the patterns, i.e., the space of
all sentences that can be generated by respect-
ing the syntactic constraints encoded by each pat-
tern. The process starts with a syntactic pattern
p containing only stop words, syntactic relations
and morphologic constraints (i.e., part-of-speech
DT NNS VBD DT JJ NN IN DT NN
The fires X a * smoke in the *
det nsubj
dobj
det
amod
prep
pobj
det
Figure 2: A partially lexicalized sentence with a
highlighted empty slot marked with X. The rele-
vant dependencies to fill in the slot are shown in
boldface.
tags) for the empty slots. The search advances to-
wards a complete solution by selecting an empty
slot to fill and trying to place candidate fillers in
the selected position. Each partially lexicalized
solution is scored by a battery of scoring func-
tions that compete to generate creative sentences
respecting the user specificationU , as explained in
Section 3.3. The most promising solutions are ex-
tended by filling another slot, until completely lex-
icalized sentences, i.e., sentences without empty
slots, are generated.
To limit the number of words that can occupy
a given position in a sentence, we define a set of
operators that return a list of candidate fillers for
a slot solely based on syntactic clues. To achieve
that, we analyze a large corpus of parsed sentences
L3 and store counts of observed head-relation-
modifier (?h, r,m?) dependency relations. Let
?r(h) be an operator that, when applied to a head
word h in a relation r, returns the set of words in
L which have been observed as modifiers for h in
r with a specific POS. To simplify the notation,
we assume that the relation r also carries along
the POS of the head and modifier slots. As an
example, with respect to the tree depicted in Fig-
ure 2, ?amod(smoke) would return all the words
with POS equal to ?JJ? that have been observed as
adjective modifiers for the singular noun ?smoke?.
We will refer to ?r(?) as the dependency operator
for r. For every ?r(?), we also define an inverse
dependency operator ??1r (?), which returns the list
of the possible heads in r when applied to a mod-
ifier word m. For instance, with respect to Fig-
ure 2, ??1nsubj(fires) would return the set of verbs inthe past tense of which ?fires? as a plural noun can
be a subject.
While filling in a given slot X , the dependency
operators can be combined to obtain a list of words
which are likely to occupy that position given the
syntactic constraints induced by the structure of
the pattern. Let W = ?i{wi} be the set of words
which are directly connected to the empty slot by
3Distinct from the corpus used for pattern selection, P .
1449
a dependency relation. Each word wi implies a
constraint that candidate fillers for X must satisfy.
If wi is the head of X , then a direct operator is
used to retrieve a list of fillers that satisfy the ith
constraint. Conversely, if wi is a modifier of X ,
an inverse operator is employed.
As an example, let us consider the partially
completed sentence shown in Figure 2 having
an empty slot marked with X . Here, the word
?smoke? is a modifier for X , to which it is con-
nected by a dobj relation. Therefore, we can ex-
ploit ??1dobj(smoke) to obtain a ranked list of wordsthat can occupy X according to this constraint.
Similarly, the ??1nsubj(fires) operator can be used toretrieve a list of verbs in the past tense that ac-
cept ?fires? as nsubj modifier. Finally ??1prep(in)
can further restrict our options to verbs that ac-
cepts complements introduced by the preposition
?in?. For example, the words ?generated?, ?pro-
duced?, ?caused? or ?formed? would be good can-
didates to fill in the slot considering all the pre-
vious constraints. More formally, we can de-
fine the set of candidate fillers for a slot X , CX ,
as: CX = ??1rhX,X(hX) ? (
?
wi|wi?MX ?rwi,X(wi)),where rwi,X is the type of relation between wi and
X , MX is the set of modifiers of X and hX is the
syntactic head of X .4
Concerning the order in which slots are filled,
we start from those that have the highest num-
ber of dependencies (both head or modifiers) that
have been already instantiated in the sentence, i.e.,
we start from the slots that are connected to the
highest number of non-empty slots. In doing so
we maximize the constraints that we can rely on
when inserting a new word, and eventually gener-
ate more reliable outputs.
3.3 Filler selection and solution scoring
We have devised a set of feature functions that ac-
count for different aspects of the creative sentence
generation process. By changing the weight w of
the feature functions in U , users can control the
extent to which each creativity component will af-
fect the sentence generation process, and tune the
output of the system to better match their needs.
As explained in the remainder of this section, fea-
ture functions are responsible for ranking the can-
didate slot fillers to be used during sentence gen-
eration and for selecting the best solutions to be
4An empty slot does not generate constraints for X . In
addition, there might be cases in which it is not possible to
find a filler that satisfies all the constraints at the same time.
In such cases, all the fillers that satisfy the maximum number
of constraints are considered.
Algorithm 2 RankCandidates(U, f , c1, c2, s,X): c1
and c2 are two candidate fillers for the slot X in the sentence
s = [s0, . . . sn]; f is the set of feature functions; U is the user
specification.
sc1 ? s, sc2 ? s, sc1 [X]? c1, sc2 [X]? c2
for all f ? SortFeatureFunctions?(U, f) do
if f(sc1 , U) > f(sc2 , U) then return c1  c2
else if f(sc1 , U) < f(sc2 , U) then
return c1 ? c2
return c1 ? c2
shown to the users.
Algorithm 2 details the process of ranking can-
didate fillers. To compare two candidates c1 and c2
for the slot X in the sentence s, we first generate
two sentences sc1 and sc2 in which the empty slot
X is occupied by c1 and c2, respectively. Then, we
sort the feature functions based on their weights
in descending order, and in turn we apply them
to score the two sentences. As soon as we find
a scorer for which one sentence is better than the
other, we can take a decision about the ranking of
the fillers. This approach makes it possible to es-
tablish a strict order of precedence among feature
functions and to select fillers that have a highest
chance of maximizing the user satisfaction.
Concerning the scoring of partial solutions and
complete sentences, we adopt a simple linear com-
bination of scoring functions. Let s be a (partial)
sentence, f = [f0, . . . , fk] be the vector of scor-
ing functions and w = [w0, . . . , wk] the associ-
ated vector of weights in U . The overall score of s
is calculated as score(s, U) = ?ki=0wifi(s, U) .Solutions that do not contain all the required target
words are discarded and not shown to the user.
Currently, the model employs the following 12
feature functions:
Chromatic and emotional connota-
tion. The chromatic connotation of a
sentence s = [s0, . . . , sn] is computed as
f(s, U) =
?
si(sim(si, c) ?
?
cj 6=c sim(si, cj)),where c is the user selected target color and
sim(si, cj) is the degree of association between
the word si and the color cj as calculated by
Mohammad (2011). All the words in the sentence
which have an association with the target color
c give a positive contribution, while those that
are associated with a color ci 6= c contribute
negatively. Emotional connotation works exactly
in the same way, but in this case word-emotion
associations are taken from (Mohammad and
Turney, 2010).
Domain relatedness. This feature function uses
an LSA (Deerwester et al, 1990) vector space
1450
model to measure the similarity between the words
in the sentence and the target domain d speci-
fied by the user. It is calculated as: f(s, U) =?
di v(di)?
?
si v(si)
??di v(di)???
?
si v(si)?
where v(?) returns the rep-
resentation of a word in the vector space.
Semantic cohesion. This feature behaves ex-
actly like domain relatedness, with the only dif-
ference that it measures the similarity between the
words in the sentence and the target words t.
Target-words scorer. This feature function
simply counts what fraction of the target
words t is present in a partial solution:
f(s, U) = (
?
si|si?t 1)/|t|. The target word scorertakes care of enforcing the presence of the target
words in the sentences. Letting beam search find
the best placement for the target words comes at
no extra cost and results in a simple and elegant
model.
Phonetic features (plosives, alliteration and
rhyme). All the phonetic features are based on
the phonetic representation of English words of
the Carnegie Mellon University pronouncing dic-
tionary (Lenzo, 1998). The plosives feature is cal-
culated as the ratio between the number of plo-
sive sounds in a sentence and the overall num-
ber of phonemes. For the alliteration scorer, we
store the phonetic representation of each word in
s in a trie (i.e., prefix tree), and count how many
times each node ni of the trie (corresponding to a
phoneme) is traversed. Let ci be the value of the
counts for ni. The alliteration score is then cal-
culated as f(s, U) = (?i|ci>1 ci)/
?
i ci. Moresimply put, we count how many of the phonetic
prefixes of the words in the sentence are repeated,
and then we normalize this value by the total num-
ber of phonemes in s. The rhyme feature works
exactly in the same way, with the only difference
that we invert the phonetic representation of each
word before adding it to the TRIE. Thus, we give
higher scores to sentences in which several words
share the same phonetic ending.
Variety scorer. This feature function promotes
sentences that contain as many different words as
possible. It is calculated as the number of distinct
words in the sentence over the size of the sentence.
Unusual-words scorer. To increase the ability
of the model to generate sentences containing non-
trivial word associations, we may want to prefer
solutions in which relatively uncommon words are
employed. Inversely, we may want to lower lex-
ical complexity to generate sentences more ap-
propriate for certain education or reading levels.
We define ci as the number of times each word
si ? s is observed in a corpus V . Accord-
ingly, the value of this feature is calculated as:
f(s, U) = (1/|s|)(?si 1/ci).
N-gram likelihood. This is simply the likeli-
hood of a sentence estimated by an n-gram lan-
guage model, to enforce the generation of well-
formed word sequences. When a solution is not
complete, in the computation we include only the
sequences of contiguous words (i.e., not inter-
rupted by empty slots) having length greater than
or equal to the order of the n-gram model.
Dependency likelihood. This feature is re-
lated to the dependency operators introduced
in Section 3.2 and it enforces sentences in
which dependency chains are well formed. We
estimate the probability of a modifier word
m and its head h to be in the relation r
as pr(h,m) = cr(h,m)/(?hi
?
mi cr(hi,mi)),where cr(?) is the number of times that m
depends on h in the dependency treebank
L and hi,mi are all the head/modifier pairs
observed in L. The dependency-likelihood
of a sentence s can then be calculated as
f(s, U) = exp(
?
?h,m,r??r(s) log pr(h,m)), r(s)being the set of dependency relations in s.
4 Evaluation
We evaluated our model on a creative sentence
generation task. The objective of the evaluation
is twofold: we wanted to demonstrate 1) the effec-
tiveness of our approach for creative sentence gen-
eration, in general, and 2) the potential of BRAIN-
SUP to support the brainstorming process behind
slogan generation. To this end, the annotation tem-
plate included one question asking the annotators
to rate the quality of the generated sentences as
slogans.
Five experienced annotators were asked to rate
432 creative sentences according to the follow-
ing criteria, namely: 1) Catchiness: is the sen-
tence attractive, catchy or memorable? [Yes/No]
2) Humor: is the sentence witty or humorous?
[Yes/No]; 3) Relatedness: is the sentence seman-
tically related to the target domain? [Yes/No]; 4)
Correctness: is the sentence grammatically cor-
rect? [Ungrammatical/Slightly disfluent/Fluent];
5) Success: could the sentence be a good slogan
for the target domain? [As it is/With minor edit-
ing/No]. In these last two cases, the annotators
1451
were instructed to select the middle option only
in cases where the gap with a correct/successful
sentence could be filled just by performing minor
editing. The annotation form had no default val-
ues, and the annotators did not know how the eval-
uated sentences were generated, or whether they
were the outcome of one or more systems.
We started by collecting slogans from an on-
line repository of slogans5. Then, we randomly
selected a subset of these slogans and for each of
them we generated an input specification U for the
system. We used the commercial domain of the
advertised product as the target domain d. Two
or three content words appearing in each slogan
were randomly selected as the target words t. We
did so to simulate the brainstorming phase behind
the slogan generation process, where copywriters
start with a set of relevant keywords to come up
with a catchy slogan. In all cases, we set the tar-
get emotion to ?positive? as we could not estab-
lish a generally valid criteria to associate a spe-
cific emotion to a product. Concerning chromatic
slanting, for target domains having a strong chro-
matic correlation we allowed the system to slant
the generated sentences accordingly. In the other
cases, a random color association was selected. In
this manner, we produced 10 tuples ?t,d, c, e, p?.
Then, from each tuple we produced 5 complete
user specifications by enabling or disabling differ-
ent feature function combinations6. The four com-
binations of features are: base: Target-word scorer
+ N-gram likelihood + Dependency likelihood +
Variety scorer + Unusual-words scorer + Seman-
tic cohesion; base+D: all the scorers in base +
Domain relatedness; base+D+C: all the scorers in
base+D + Chromatic connotation; base+D+E: all
the scorers in base+D + Emotional connotation;
base+D+P: all the scorers in base+D + Phonetic
features. For each of the resulting 50 input config-
urations, we generated up to 10 creative sentences.
As the system could not generate exactly 10 solu-
tions in all the cases, we ended up with a set of
432 items to annotate. The weights of the feature
functions were set heuristically, due to the lack
of an annotated dataset suitable to learn an opti-
5http://www.tvacres.com/advertising_
slogans.htm
6An alternative strategy to keep the annotation effort un-
der control would have been to generate fewer sentences from
a larger number of inputs. We adopted the former setting
since we regarded it as more similar to a brainstorming ses-
sion, where the system proposes different alternatives to in-
spire human operators. Forcing BRAINSUP to only output
one or two sentences would have limited its ability to explore
and suggest potentially valuable outputs.
MC Cat. Hum. Corr. Rel. Succ. RND2 RND3
2 - - 16.67 - 22.22 - 37.043 47.45 39.58 43.52 13.66 44.21 62.50 49.384 33.10 37.73 32.18 21.99 22.22 31.25 12.355 19.44 22.69 07.64 64.35 11.34 06.25 01.23
Table 2: Majority classes (%) for the five dimen-
sions of the annotation.
mal weight configuration. We started by assign-
ing the highest weight to the Target Word scorer
(i.e., 1.0), followed by the Variety and Unusual
Word scorers (0.99), the Phonetic Features, Chro-
matic/Emotional Connotation and Semantic Co-
hesion scorers (0.98) and finally the Domain, N -
gram and Dependency Likelihood scorers (0.97).
These settings allow us to enforce an order of
precedence among the scorers during slot-filling,
while giving them virtually equal relevance for so-
lution ranking.
As discussed in Section 3 we use two differ-
ent treebanks to learn the syntactic patterns (P)
and the dependency operators (L). For these ex-
periments, patterns were learned from a corpus
of 16,000 proverbs (Mihalcea and Strapparava,
2006), which offers a good selection of short sen-
tences with a good potential to be used for slo-
gan generation. This choice seemed to be a good
compromise as, to our best knowledge, there is
no published slogan dataset with an adequate size.
Besides, using existing slogans might have legal
implications that we might not be aware of. De-
pendency operators were learned by dependency
parsing the British National Corpus7. To reduce
the amount of noise introduced by the automatic
parses, we only considered sentences having less
than 20 words. Furthermore, we only considered
sentences in which all the content words are listed
in WordNet (Miller, 1995) with the observed part
of speech.8 The LSA space used for the semantic
feature functions was also learned on BNC data,
but in this case no filtering was applied.
4.1 Results
To measure the agreement among the annota-
tors, similarly to Mohammad (2011) and Ozbal
and Strapparava (2012) we calculated the majority
class for each dimension of the annotation task. A
7http://www.natcorp.ox.ac.uk/
8Since the CMU pronouncing dictionary used by the pho-
netic scorers is based on the American pronunciation of
words, we actually pre-processed the whole BNC by replac-
ing all British-English words with their American-English
counterparts. To this end, we used the mapping available at
http://wordlist.sourceforge.net/.
1452
Cat. Rel. Hum. Succ. Corr.
Yes 67.59 93.98 12.73 32.41 64.35
Partly - - - 23.15 31.71
No 32.41 06.02 87.27 44.44 03.94
Table 3: Majority decisions (%) for each annota-
tion dimension.
majority class greater than or equal to 3 means that
the absolute majority of the 5 annotators agreed
on the same decision9. Table 2 shows the ob-
served agreement for each dimension. The column
labeled RND2 (RND3) shows the random agree-
ment for a given number of annotators and a binary
(ternary) decision. For example, all five annotators
(MC=5) agreed on the annotation of the catchiness
of the slogans in 19.44% of the cases. The random
chance of agreement for 5 annotators on the binary
decision problem is 6.25%. The figures for MC ?
4 are generally high, confirming a good agreement
among the annotators. The agreement on the relat-
edness of the slogans is especially high, with all 5
annotators taking the same decision in almost two
cases out of three, i.e., 64.35%.
Table 3 lists the distribution of answers for each
dimension in the cases where a decision can be
taken by majority vote. The generated slogans
are found to be catchy in more than 2/3 of the
cases, (i.e., 67.59%), completely successful in 1/3
of the cases (32.41%) and completely correct in
2/3 of the cases (64.35%). These figures demon-
strate that BRAINSUP is very effective in gener-
ating grammatical utterances that have all the ap-
pealing properties of a successful slogan. As for
humor, the sentences are found to have this prop-
erty in only 12.73% of cases. Even though the
figure is not very high, we should also consider
that BRAINSUP is not explicitly trying to gener-
ate amusing utterances. Concerning success, we
should point out that in 23.15% of the cases the
annotators have found that the generated slogans
have the potential to be turned into successful ones
only with minor editing. This is a very important
piece of result, as it corroborates our claim that
BRAINSUP can indeed be a valuable tool for copy-
writing, even when it does not manage to output a
perfectly good sentence. Similar conclusions can
be drawn concerning the correctness of the output,
as in almost one third of the cases the slogans are
9For the binary decisions (i.e., catchiness, relatedness and
humor), at least 3 annotators out of 5 must necessarily agree
on the same option.
only affected by minor disfluencies.
The relatedness figure is especially high, as in
almost 94% of the cases the majority of annota-
tors found the slogans to be pertinent to the tar-
get domain. This result is not surprising, as all
the slogans are generated by considering keywords
that already exist in real slogans for the same do-
main. Anyhow, this is exactly the kind of setting in
which we expect BRAINSUP to be employed, i.e.,
to support creative sentence generation starting
from a good set of relevant keywords. Nonethe-
less, it is very encouraging to observe that the gen-
eration process does not deteriorate the positive
impact of the input keywords.
We would also like to mention that in 63 cases
(14.58%) the majority of the annotators have la-
beled the slogans favorably across all 5 dimen-
sions. The examples listed in Table 1 are selected
from this set. It is interesting to observe how
the word associations established by BRAINSUP
can result in pertinent yet unintentional rhetori-
cal devices such as metaphors (?a summer sun-
shine?), puns (?lash your drama?) and personifica-
tions (?lips and eyes want?). Some examples show
the effect of the phonetic features, e.g. plosives in
?passionate kiss, perfect lips?, alliteration in ?the
dark drink? and rhyming in ?lips and eyes want
the kiss?. In some cases, the output of BRAINSUP
seems to be governed by mysterious philosophical
reasoning, as in the delicate examples generated
for ?soap?.
For comparison, Table 4 lists a selection of
the examples that have been labeled as unsuc-
cessful by the majority of raters. In some cases,
BRAINSUP is improperly selecting attributes that
highlight undesirable properties in the target do-
main, e.g., ?A pleasant tasting, a heady wine?. To
avoid similar errors, it would be necessary to rea-
son about the valence of an attribute for a spe-
cific domain. In other cases, the N -gram and the
Dependency Likelihood features may introduce
phrases which are very cohesive but unrelated to
the rest of the sentence, e.g., ?Unscrupulous doc-
tors smoke armored units?. Many of these errors
could be solved by increasing the weight of the
Semantic Cohesion and Domain Relatedness scor-
ers. In other cases, such as ?A sixth calorie may
taste an own good? or ?A same sunshine is fewer
than a juice of day?, more sophisticated reason-
ing about syntactic and semantic relations in the
output might be necessary in order to enforce the
generation of sound and grammatical sentences.
We could not find a significant correlation be-
1453
Domain Keywords BRAINSUP output examples
pleasure wine, tast-
ing
A pleasant tasting, a heady wine.
? A fruity tasting may drink a
sparkling wine.
healthy day, juice,
sunshine
Drink juice of your sunshine, and
your weight will choose day of
you. ? A same sunshine is fewer
than a juice of day.
cigarette doctors,
smoke
Unscrupulous doctors smoke ar-
mored units. ? Doctors smoke no
arrow.
mascara drama,
lash
The such drama is the lash.
soap skin, love,
touch
The touch of skin is the love of
cacophony. ? You love an own
skin for a first touch.
coke calorie,
taste, good
A sixth calorie may taste an own
good.
coffee waking,
cup
You cannot cup hands without
waking some fats.
Table 4: Unsuccessful BRAINSUP outputs.
tween the input variables (e.g., presence or ab-
sence of phonetic features or chromatic slanting)
and the outcome of the annotation, i.e. the sys-
tem by and large produces correct, catchy, related
and (at least potentially) successful outputs regard-
less of the specific input configurations. In this re-
spect, it should be noted that we did not carry out
any kind of optimization of the feature weights,
which might be needed to obtain more heavily
characterized sentences. Furthermore, to better
appreciate the contribution of the individual fea-
tures, comparative experiments in which the users
evaluate the system before and after triggering a
feature function might be necessary. Concern-
ing the correlation among output dimensions, we
only observed relatively high Spearman correla-
tion between correctness and relatedness (0.65),
and catchiness and success (0.68).
5 Conclusion
We have presented BRAINSUP, a novel system
for creative sentence generation that allows users
to control many aspects of the creativity process,
from the presence of specific target words in the
output, to the selection of a target domain, and
to the injection of phonetic and semantic proper-
ties in the generated sentences. BRAINSUP makes
heavy use of dependency parsed data and statistics
collected from dependency treebanks to ensure the
grammaticality of the generated sentences, and to
trim the search space while seeking the sentences
that maximize the user satisfaction.
The system has been designed as a support-
ing tool for a variety of real-world applications,
from advertisement to entertainment and educa-
tion, where at the very least it can be a valu-
able support for time-consuming and knowledge-
intensive sentence generation needs. To demon-
strate this point, we carried out an evaluation on a
creative sentence generation benchmark showing
that BRAINSUP can effectively produce catchy,
memorable and successful sentences that have the
potential to inspire the work of copywriters.
To our best knowledge, this is the first system-
atic attempt to build an extensible framework that
allows for multi-dimensional creativity while at
the same time relying on syntactic constraints to
enforce grammaticality. In this regard, our ap-
proach is dual with respect to previous work based
on lexical substitution, which suffers from limited
expressivity and creativity latitude. In addition, by
acquiring the lexicon and the sentence structure
from two distinct corpora, we can guarantee that
the sentences that we generate have never been
observed. We believe that our contribution con-
stitutes a valid starting point for other researchers
to deal with unexplored dimensions of creativity.
As future work, we plan to use machine learn-
ing techniques to estimate optimal weights for the
feature functions in different use cases. We would
also like to consider syntactic clues while reason-
ing about semantic properties of the sentence, e.g.,
color and emotion associations, instead on relying
solely on lexical semantics. Concerning the exten-
sion of the capabilities of BRAINSUP, we want to
include common-sense knowledge and reasoning
to profit from more sophisticated semantic rela-
tions and to inject humor on demand. Further tun-
ing of BRAINSUP to build a dedicated system for
slogan generation is also part of our future plans.
After these improvements, we would like to con-
duct a more focused evaluation on slogan genera-
tion involving human copywriters and domain ex-
perts in an interactive setting.
We would like to conclude this paper with a pearl
of BRAINSUP?s wisdom:
It is wiser to believe in science
than in everlasting love.
Acknowledgments
Go?zde O?zbal and Carlo Strapparava were partially
supported by the PerTe project (Trento RISE).
1454
References
Kim Binsted and Graeme Ritchie. 1997. Computa-
tional rules for generating punning riddles. Humor -
International Journal of Humor Research, 10(1):25?
76, January.
Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Pic-net: Pictorial representations for illustrated se-
mantic networks. In Proceedings of the AAAI Spring
Symposium on Knowledge Collection from Volun-
teer Contributors.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full-FACE Poetry Generation. In Proceedings of
the 3rd International Conference on Computational
Creativity, pages 95?102.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
Of The American Society for Information Science,
41(6):391?407.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC?06), pages 417?422.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry
with applications to generation and translation. In
EMNLP, pages 524?533.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2011. Slanting existing text with valentino. In Pro-
ceedings of the 16th international conference on In-
telligent user interfaces, IUI ?11, pages 439?440,
New York, NY, USA. ACM.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kevin S. LaBar and Roberto Cabeza. 2006. Cognitive
neuroscience of emotional memory. Nature reviews.
Neuroscience, 7(1):54?64, January.
Kevin Lenzo. 1998. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Ruli Manurung, Graeme Ritchie, Helen Pain, An-
nalu Waller, Dave O?Mara, and Rolf Black. 2008.
The Construction of a Pun Generator for Language
Skills Development. Applied Artificial Intelligence,
22(9):841?869, October.
J McKay. 2002. Generation of idiom-based witticisms
to aid second language learning. In Twente Work-
shop on Language Technology 20, pages 70?74.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126?142, May.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?41.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 26?
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011. Even the abstract have color:
Consensus in word-colour associations. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 368?373, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Go?zde O?zbal and Carlo Strapparava. 2011. Autom-
atized Memory Techniques for Vocabulary Acquisi-
tion in a Second Language. In Alexander Verbraeck,
Markus Helfert, Jose? Cordeiro, and Boris Shishkov,
editors, CSEDU, pages 79?87. SciTePress.
Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 703?711, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228?243.
Oliviero Stock and Carlo Strapparava. 2006. Laughing
with hahacronym, a computational humor system.
In proceedings of the 21st national conference on
Artificial intelligence - Volume 2, pages 1675?1678.
AAAI Press.
J. M. Toivanen, H. Toivonen, A. Valitutti, and O. Gross.
2012. Corpus-based Generation of Content and
Form in Poetry. In International Conference on
Computational Creativity, pages 175?179.
A. Valitutti, C. Strapparava, , and O. Stock. 2009.
Graphlaugh: a tool for the interactive generation of
humorous puns. In Proceedings of ACII-2009, Third
Conference on Affective Computing and Intelligent
Interaction, Demo track.
1455
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352?357,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automation and Evaluation of the Keyword Method
for Second Language Learning
G
?
ozde
?
Ozbal
Trento RISE
Trento, Italy
gozbalde@gmail.com
Daniele Pighin
Google
Z?urich, Switzerland
biondo@google.com
Carlo Strapparava
FBK-irst
Trento, Italy
strappa@fbk.eu
Abstract
In this paper, we combine existing
NLP techniques with minimal supervi-
sion to build memory tips according to
the keyword method, a well established
mnemonic device for second language
learning. We present what we believe to be
the first extrinsic evaluation of a creative
sentence generator on a vocabulary learn-
ing task. The results demonstrate that NLP
techniques can effectively support the de-
velopment of resources for second lan-
guage learning.
1 Introduction
The keyword method is a mnemonic device (Co-
hen, 1987; Thompson, 1987) that is especially
suitable for vocabulary acquisition in second lan-
guage learning (Mizumoto and Kansai, 2009;
Hummel, 2010; Shen, 2010; Tavakoli and Gerami,
2013). In this method, a target word in a foreign
language L2 can be learned by a native speaker of
another language L1 in two main steps: 1) one or
more L1 words, possibly referring to a concrete
entity, are chosen based on orthographic or pho-
netic similarity with the target word; 2) an L1 sen-
tence is constructed in which an association be-
tween the translation of the target word and the
keyword(s) is established, so that the learner, when
seeing or hearing the word, immediately recalls
the keyword(s). To illustrate, for teaching the Ital-
ian word cuore which means heart in English, the
learner might be asked to imagine ?a lonely heart
with a hard core?.
The keyword method has already been proven
to be a valuable teaching device. However, the
preparation of the memorization tips for each new
word is an activity that requires considerable time,
linguistic competence and creativity. To the best
of our knowledge, there is only one study which
attempts to automate the mechanism of the key-
word method. In (
?
Ozbal and Strapparava, 2011),
we proposed to automate the keyword method by
retrieving sentences from the Web. However, we
did not provide any evaluation to demonstrate the
effectiveness of our approach in a real life sce-
nario. In addition, we observed that retrieval poses
severe limitations in terms of recall and sentence
quality, and it might incur copyright violations.
In this paper, we overcome these limitations by
introducing a semi-automatic system implement-
ing the keyword method that builds upon the key-
word selection mechanism of
?
Ozbal and Strappar-
ava (2011) and combines it with a state-of-the-art
creative sentence generation framework (
?
Ozbal et
al., 2013). We set up an experiment to simulate
the situation in which a teacher needs to prepare
material for a vocabulary teaching resource. Ac-
cording to our scenario, the teacher relies on au-
tomatic techniques to generate relatively few, high
quality mnemonics in English to teach Italian vo-
cabulary. She only applies a very light supervi-
sion in the last step of the process, in which the
most suitable among the generated sentences are
selected before being presented to the learners. In
this stage, the teacher may want to consider factors
which are not yet in reach of automatic linguistic
processors, such as the evocativeness or the mem-
orability of a sentence. We show that the automat-
ically generated sentences help learners to estab-
lish memorable connections which augment their
ability to assimilate new vocabulary. To the best of
our knowledge, this work is the first documented
extrinsic evaluation of a creative sentence genera-
tor on a real-world application.
2 Related work
The effectiveness of the keyword method (KM)
is a well-established fact (Sar?c?oban and Bas??bek,
2012). Sommer and Gruneberg (2002) found that
using KM to teach French made learning easier
and faster than conventional methods. Sagarra
and Alba (2006) compared the effectiveness of
352
three learning methods including the semantic
mapping, rote memorization (i.e., memorization
by pure repetition, with no mnemonic aid) and
keyword on beginner learners of a second lan-
guage. Their results show that using KM leads
to better learning of second language vocabulary
for beginners. Similar results have been reported
by Sar?c?oban and Bas??bek (2012) and Tavakoli
and Gerami (2013). Besides all the experimental
results demonstrating the effectiveness of KM, it
is worthwhile to mention about the computational
efforts to automate the mechanism. In (
?
Ozbal and
Strapparava, 2011) we proposed an automatic vo-
cabulary teaching system which combines NLP
and IR techniques to automatically generate mem-
ory tips for vocabulary acquisition. The system
exploits orthographic and phonetic similarity met-
rics to find the best L2 keywords for each target L1
word. Sentences containing the keywords and the
translation of the target word are retrieved from
the Web, but we did not carry out an evaluation
of the quality or the coverage of the retrieved sen-
tences. In
?
Ozbal et al (2013) we proposed an ex-
tensible framework for the generation of creative
sentences in which users are able to force sev-
eral words to appear in the sentences. While we
had discussed the potentiality of creative sentence
generation as a useful teaching device, we had not
validated our claim experimentally yet. As a previ-
ous attempt at using NLP for education, Manurung
et al (2008) employ a riddle generator to create
a language playground for children with complex
communication needs.
3 Memory tip generation
Preparing memory tips based on KM includes two
main ingredients: one or more keywords which are
orthographically or phonetically similar to the L2
word to be learned; and a sentence in which the
keywords and the translation of the target L2 word
are combined in a meaningful way. In this section,
we detail the process that we employed to generate
such memory tips semi-automatically.
3.1 Target word selection and keyword
generation
We started by compiling a collection of Ital-
ian nouns consisting of three syllables from var-
ious resources for vocabulary teaching includ-
ing http://didattica.org/italiano.
htm and http://ielanguages.com, and
produced a list of 185 target L2 words. To gen-
erate the L1 keywords for each target word, we
adopted a similar strategy to
?
Ozbal and Strappa-
rava (2011). For each L2 target word t, the key-
word selection module generates a list of possi-
ble keyword pairs, K. A keyword pair k ? K
can either consist of two non-empty strings, i.e.,
k = [w
0
, w
1
], or of one non-empty and one empty
string, i.e., w
1
= . Each keyword pair has the
property that the concatenation of its elements is
either orthographically or phonetically similar to
the target word t. Orthographic and phonetic sim-
ilarity are evaluated by means of the Levenshtein
distance (Levenshtein, 1966). For orthographic
similarity, the distance is calculated over the char-
acters in the words, while for phonetic similarity
it is calculated over the phonetic representations
of t and w
0
+ w
1
. We use the CMU pronuncia-
tion dictionary
1
to retrieve the phonetic represen-
tation of English words. For Italian words, instead,
their phonetic representation is obtained from an
unpublished phonetic lexicon developed at FBK-
irst.
3.2 Keyword filtering and ranking
Unlike in (
?
Ozbal and Strapparava, 2011), where
we did not enforce any constraints for selecting
the keywords, in this case we applied a more so-
phisticated filtering and ranking strategy. We re-
quire at least one keyword in each pair to be a
content word; then, we require that at least one
keyword has length ? 3; finally, we discard pairs
containing at least one proper noun. We allowed
the keyword generation module to consider all the
entries in the CMU dictionary, and rank the key-
word pairs based on the following criteria in de-
creasing order of precedence: 1) Keywords with
a smaller orthographic/phonetic distance are pre-
ferred; 2) Keywords consisting of a single word
are preferred over two words (e.g., for the target
word lavagna, which means blackboard, lasagna
takes precedence over love and onion); 3) Key-
words that do not contain stop words are preferred
(e.g., for the target word pettine, which means
comb, the keyword pair pet and inn is ranked
higher than pet and in, since in is a stop word); 4)
Keyword pairs obtained with orthographic similar-
ity are preferred over those obtained with phonetic
similarity, as learners might be unfamiliar with the
phonetic rules of the target language. For example,
for the target word forbice, which means scissors,
1
http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
353
Group Target Sentence
A1 campagna a company runs the country
A1 isola an island of remote isolated communities
A1 fabbrica a fabric worker in a factory
A1 bagnino lifeguards carry no bag
A1 inverno the inferno started, winter left
A1 cielo the sky has no ceiling
A1 marrone blood and marrow in a brown water
A1 cuore the lonely heart has hard core
A1 coperta a piece of copper in the corner of a blanket
A1 locanda an inn oak door with lock and key
A2 piazza a square building serves a free pizza
A2 calzino big bloke with sock in the casino
A2 scatola a cardboard box sat in a scuttle of a house
A2 ragazzo boys also have rag dolls
A2 angolo a corner kick came at an angle
A2 cestino a teen movie uses basket to play the chess
A2 carbone the coal is the form of carbon
A2 cassetto a blank cassette tape is in a drawer
A2 farfalla the butterflies are far in the fall
A2 tovaglia a damp cloth towel
B1 duomo the old cathedral has a dome
B1 aceto a vinegar sauce contains the acid
B1 nuvola the sophisticated novel depicts the cloud
B1 chiesa the Catholic church has Swiss cheese
B1 bacino the explosion in the back broke the pelvis
B1 maiale a pork meat comes in the mail
B1 minestra Chinese ministries have soup
B1 estate this estate is for summer
B1 bozzolo a buzz comes wrapped in the cocoon
B1 arnese harness a technology to develop a tool
B2 asino an Asian elephant is riding a donkey
B2 miele do not make honey to walk a mile
B2 polmone crowded pullmans stop the lungs
B2 fagiolo a topical facial bean cream
B2 fiore a fire in a flower market
B2 compressa the clay tablet is in the compressed form
B2 cavallo horse running fast in cavalry
B2 fiume the muddy river has smoke and fumes
B2 pittore a famous painter has precious pictures
B2 manico manic people have broken necks
Table 1: Sentences used in the vocabulary acqui-
sition experiment.
the keyword pair for and bid is preferred to for and
beach.
We selected up to three of the highest ranked
keyword pairs for each target word, obtaining 407
keyword combinations for the initial 185 Italian
words, which we used as the input for the sentence
generator.
3.3 Sentence generation
In this step, our goal was to generate, for each Ital-
ian word, sentences containing its L1 translation
and the set of orthographically (or phonetically)
similar keywords that we previously selected. For
each keyword combination, starting from the top-
ranked ones, we generated up to 10 sentences by
allowing any known part-of-speech for the key-
words. The sentences were produced by the state
of the art sentence generator of
?
Ozbal et al (2013).
The system relies on two corpora of automatic
parses as a repository of sentence templates and
lexical statistics. As for the former, we combined
two resources: a corpus of 16,000 proverbs (Mi-
halcea and Strapparava, 2006) and a collection of
5,000 image captions
2
collected by Rashtchian et
al. (2010). We chose these two collections since
they offer a combination of catchy or simple sen-
tences that we expect to be especially suitable
for second language learning. As for the sec-
ond corpus, we used LDC?s English GigaWord 5th
Edition
3
. Of the 12 feature functions described
in (
?
Ozbal et al, 2013), we only implemented the
following scorers: Variety (to prevent duplicate
words from appearing in the sentences); Seman-
tic Cohesion (to enforce the generation of sentence
as lexically related to the target words as possi-
ble); Alliteration, Rhyme and Plosive (to intro-
duce hooks to echoic memory in the output); De-
pendency Operator andN -gram (to enforce output
grammaticality).
We observed that the sentence generation mod-
ule was not able to generate a sentence for 24%
of the input configurations. For comparison, when
we attempted to retrieve sentences from the Web
as suggested in
?
Ozbal and Strapparava (2011), we
could collect an output for less than 10% of the in-
put configurations. Besides, many of the retrieved
sentences were exceedingly long and complex to
be used in a second language learning experiment.
3.4 Sentence selection
For each L1 keyword pair obtained for each L2
target word, we allowed the system to output up to
10 sentences. We manually assessed the quality of
the generated sentences in terms of meaningful-
ness, evocativeness and grammaticality to select
the most appropriate sentences to be used for the
task. In addition, for keyword pairs not containing
the empty string, we prioritized the sentences in
which the keywords were closer to each other. For
example, let us assume that we have the keywords
call and in for the target word collina. Among
the sentences ?The girl received a call in the bath-
room? and ?Call the blond girl in case you need?,
the first one is preferred, since the keywords are
closer to each other. Furthermore, we gave pri-
ority to the sentences that included the keywords
2
http://vision.cs.uiuc.edu/
pascal-sentences/
3
http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2011T07
354
in the right order. To illustrate, for the same key-
words and the target words, we would prefer the
sentence ?I called him in the morning yesterday?
over ?You talk a lot in a call?.
Accordingly, for each target word in random or-
der, we sequentially scanned the outputs generated
for each keyword pair. As soon as a sentence of
adequate quality was found, we added it to our
evaluation data and moved on to the next keyword.
We continued this process until we selected a sen-
tence for 40 distinct target words, which we set
as the target size of the experiment. We had to
inspect the outputs generated for 48 target words
before we were able to select 40 good examples,
meaning that for 17% of the target words the sen-
tence generator could not produce a sentence of
acceptable quality.
4 Experiment setup
For our experiment, we drew inspiration from
Sagarra and Alba (2006). We compared the re-
tention error rate of learners who tried to memo-
rize new words with or without the aid of the auto-
matically generated sentences. Through academic
channels, we recruited 20 native English speakers
with no prior knowledge of Italian.
4
After obtaining the sentences as explained in
Section 3, we shuffled and then divided the whole
set including 40 target words together with their
translation, the generated keywords and sentences
into 2 batches (A, B) and further divided each
batch into 2 groups consisting of 10 elements (A1,
A2, B1 and B2). The set of sentences assigned
to each group is listed in Table 1: Column ?Tar-
get? reports the Italian target word being taught;
Column ?Sentence? shows the automatically gen-
erated sentence, where the translation of the tar-
get word is shown in bold and the keyword(s) in
italic. For the experiments, we randomly assigned
each subject to one of the batches (A or B). Then,
each subject was asked to memorize all the word
pairs in a batch, but they would see the memory
tips only for one of the two groups, which was
again randomly assigned. This approach resulted
in 4 different memorization exercises, namely 1)
A1 with tips and A2 without, 2) A2 with tips and
A1 without, 3) B1 with tips and B2 without, 4) B2
with tips and B1 without.
4
We preferred to select the experiment subjects in person
as opposed to crowdsourcing the evaluation to be able to ver-
ify the proficiency of the subjects in the two languages and to
ensure the reliability of the outcome of the evaluation.
Error rate (%) Reduction
Group Rote KW ?
e
%
e
A1 4.08 3.39 0.69 16.95
A2 12.07 10.42 1.65 13.69
B1 12.77 10.00 2.77 21.67
B2 22.50 12.50 10.00 44.44
Macro-average 12.85 9.08 3.78 29.39
Micro-average 11.27 8.25 3.02 26.76
Table 2: Per-group and overall retention error rate
when using rote or keyword-aided (KW) memo-
rization.
When memorizing the translations without the
aid of memory tips, the subjects were instructed
to focus only on the Italian word and its English
translation and to repeat them over and over in
their mind. Conversely, when relying on the au-
tomatic memory tips the subjects were shown the
word, its translation and the generated sentence in-
cluding the keywords. In this case, the subjects
were instructed to read the sentence over and over
trying to visualize it.
After going through each set of slides, we dis-
tracted the subjects with a short video in order to
reset their short term memory. After that, their re-
tention was tested. For each Italian word in the ex-
ercise, they were asked to select the English trans-
lation among 5 alternatives, including the correct
translation and 4 other words randomly selected
from the same group. In this way, the subjects
would always have to choose among the words
that they encountered during the exercise.
5
We
also added an extra option ?I already knew this
word? that the subjects were instructed to select
in case they already knew the Italian word prior to
taking part in the experiment.
5 Experiment results
Table 2 summarizes the outcome of the experi-
ment. The contribution of the automatically gen-
erated sentences to the learning task is assessed
in terms of error rate-reduction, which we mea-
sure both within each group (rows 1-4) and on the
whole evaluation set (rows 5-6). Due to the pres-
ence of the ?I already knew this word? option in
the learning-assessment questionnaire, the number
of the actual answers provided by each subject can
be slightly different, hence the difference between
macro- and micro-average.
5
Otherwise, they could easily filter out the wrong answers
just because they were not exposed to them recently.
355
The error rate for each memorization technique
t (where t = R for ?Rote memorization? and
t = K for ?keyword-aided memorization?) is cal-
culated as: e
t
=
i
t
c
t
+i
t
, where c
t
and i
t
are the
number of correct and incorrect answers provided
by the subjects, respectively. The absolute error
rate reduction ?e is calculated as the absolute dif-
ference in error rate between rote and keyword-
aided memorization, i.e.: ?
e
= e
R
? e
K
. Finally,
the relative error rate reduction %
e
is calculated as
the the ratio between the absolute error rate reduc-
tion ?e and the error rate of rote memorization e
R
,
i.e.,: %
e
=
?
e
e
R
=
e
R
?e
K
e
R
.
The overall results (rows 5 and 6 in Table 2)
show that vocabulary learning noticeably im-
proves when supported by the generated sen-
tences, with error rates dropping by almost 30%
in terms of macro-average (almost 27% for micro-
average). The breakdown of the error rate across
the 4 groups shows a clear pattern. The results
clearly indicate that one group (A1) by chance
contained easier words to memorize as shown by
the low error rate (between 3% and 4%) obtained
with both methods. Similarly, groups A2 and B1
are of average difficulty, whereas group B2 ap-
pears to be the most difficult, with an error rate
higher than 22% when using only rote memoriza-
tion. Interestingly, there is a strong correlation
(Pearson?s r = 0.85) between the difficulty of
the words in each group (measured as the error
rate on rote memorization) and the positive contri-
bution of the generated sentences to the learning
process. In fact, we can see how the relative er-
ror rate reduction %
e
increases from?17% (group
A1) to almost 45% (group B2). Based on the re-
sults obtained by Sagarra and Alba (2006), who
showed that the keyword method results in bet-
ter long-term word retention than rote memoriza-
tion, we would expect the error rate reduction to be
even higher in a delayed post-test. All in all, these
findings clearly support the claim that a state-of-
the-art sentence generator can be successfully em-
ployed to support keyword-based second language
learning. After completing their exercise, the sub-
jects were asked to provide feedback about their
experience as learners. We set up a 4-items Lik-
ert scale (Likert, 1932) where each item consisted
of a statement and a 5-point scale of values rang-
ing from (1) [I strongly disagree] to (5) [I strongly
agree]. The distribution of the answers to the ques-
tions is shown in Table 3. 60% of the subjects ac-
knowledged that the memory tips helped them in
Rating (%)
Question 1 2 3 4 5
Sentences helped 5 20 15 35 25
Sentences are grammatical - 25 30 35 10
Sentences are catchy - 25 10 50 15
Sentences are witty - 25 25 50 -
Table 3: Evaluation of the generated sentences on
a 5-point Likert scale.
the memorization process; 45% found that the sen-
tences were overall correct; 65% confirmed that
the sentences were catchy and easy to remember;
and 50% found the sentences to be overall witty
although the sentence generator does not include a
mechanism to generate humor. Finally, it is worth
mentioning that none of the subjects noticed that
the sentences were machine generated, which we
regard as a very positive assessment of the qual-
ity of the sentence generation framework. From
their comments, it emerges that the subjects ac-
tually believed that they were just comparing two
memorization techniques.
6 Conclusion and Future Work
In this paper, we have presented a semi-automatic
system for the automation of the keyword method
and used it to teach 40 Italian words to 20 En-
glish native speakers. We let the system select
appropriate keywords and generate sentences au-
tomatically. For each Italian word, we selected the
most suitable among the 10 highest ranked sug-
gestions and used it for the evaluation. The sig-
nificant reduction in retention error rate (between
17% and 45% on different word groups) for the
words learned with the aid of the automatically
generated sentences shows that they are a viable
low-effort alternative to human-constructed exam-
ples for vocabulary teaching.
As future work, it would be interesting to in-
volve learners in an interactive evaluation to un-
derstand the extent to which learners can bene-
fit from ad-hoc personalization. Furthermore, it
should be possible to use frameworks similar to
the one that we presented to automate other teach-
ing devices based on sentences conforming to spe-
cific requirements (Dehn, 2011), such as verbal
chaining and acrostic.
Acknowledgements
This work was partially supported by the PerTe
project (Trento RISE).
356
References
Andrew D. Cohen. 1987. The use of verbal and
imagery mnemonics in second-language vocabulary
learning. Studies in Second Language Acquisition,
9:43?61, 2.
M.J. Dehn. 2011. Working Memory and Academic
Learning: Assessment and Intervention. Wiley.
K. M. Hummel. 2010. Translation and short-term L2
vocabulary retention: Hindrance or help? Language
Teaching Research, 14(1):61?74.
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707?710.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of Psychology, 22(140):1?55.
Ruli Manurung, Graeme Ritchie, Helen Pain, Annalu
Waller, Dave O?Mara, and Rolf Black. 2008. The
Construction of a Pun Generator for Language Skill
Development. Appl. Artif. Intell., 22(9):841?869,
October.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126?142, May.
A. Mizumoto and O. T. Kansai. 2009. Examining
the effectiveness of explicit instruction of vocabu-
lary learning strategies with Japanese EFL university
students. Language Teaching Research 13, 4.
G?ozde
?
Ozbal and Carlo Strapparava. 2011. MEANS:
Moving Effective Assonances for Novice Students.
In Proceedings of the 16th International Confer-
ence on Intelligent User Interfaces (IUI 2011), pages
449?450, New York, NY, USA. ACM.
G?ozde
?
Ozbal, Daniele Pighin, and Carlo Strapparava.
2013. BRAINSUP: Brainstorming Support for Cre-
ative Sentence Generation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1446?1455,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon?s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228?243.
A. Sar?c?oban and N. Bas??bek. 2012. Mnemonics tech-
nique versus context method in teaching vocabulary
at upper-intermediate level. Journal of Education
and Science, 37(164):251?266.
Helen H. Shen. 2010. Imagery and verbal coding ap-
proaches in Chinese vocabulary instruction. Lan-
guage Teaching Research, 14(4):485?499.
Steffen Sommer and Michael Gruneberg. 2002. The
use of linkword language computer courses in a
classroom situation: a case study at rugby school.
?Language Learning Journal, 26(1):48?53.
M. Tavakoli and E. Gerami. 2013. The effect of key-
word and pictorial methods on EFL learners? vocab-
ulary learning and retention. PORTA LINGUARUM,
19:299?316.
G. Thompson. 1987. Using bilingual dictionar-
ies. ELT Journal, 41(4):282?286. cited By (since
1996)6.
357
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 28?32,
Beijing, August 2010
The Color of Emotions in Texts
Carlo Strapparava and Gozde Ozbal
FBK-irst
strappa@fbk.eu, gozbalde@gmail.com
Abstract
Color affects every aspect of our lives.
There has been a considerable interest
in the psycholinguistic research area ad-
dressing the impact of color on emotions.
In the experiments conducted by these
studies, subjects have usually been asked
to indicate their emotional responses to
different colors. On the other side, sensing
emotions in text by using NLP techniques
has recently become a popular topic in
computational linguistics. In this paper,
we introduce a semantic similarity mecha-
nism acquired from a large corpus of texts
in order to check the similarity of col-
ors and emotions. Then we investigate
the correlation of our results with the out-
comes of some psycholinguistic experi-
ments. The conclusions are quite inter-
esting. The correlation varies among dif-
ferent colors and globally we achieve very
good results.
1 Introduction
In our daily speech, we frequently make use of
colors in order to increase our expressiveness by
invoking different emotions. For instance, we usu-
ally stress the redness of someone?s face for the
implication of his/her anger or excitement, or we
use phrases including the color black to refer to a
depressed mood. On the other hand, the color pink
is mostly used with positive connotations such as
?to see everything in pink light?, where the mean-
ing is related to optimism and happiness.
Actually, the parts of the nervous system which
are responsible for emotional arousal are affected
as soon as a color is perceived. Thus, the term
color emotion has lately been used to represent
the emotions arousing in human beings when they
percept a color (Xin et al, 2004).
The correlation of color and emotion has been
the focus of a lot of psycholinguistic studies so
far. In the experiments conducted by these studies,
subjects have usually been asked to indicate their
emotional responses to different colors so that
some general results stating which color arouses
what kind of emotion could be obtained.
Sensing emotion, or in other words, affective
sensing in text by using Natural Language Pro-
cessing (NLP) techniques is recently a very pop-
ular topic in computational linguistics. There ex-
ist several studies focusing on the automatic iden-
tification of emotions in text with the help of
both knowledge-based and corpus-based methods.
Thus it is conceivable to explore whether state-of-
the-art corpus analysis techniques can give sup-
port to psycholinguistic experiments.
Considering that psycholinguistic experiments
are very costly since a lot of resources are required
for both the setup and evaluation phases, employ-
ing a corpus-based approach for affective sensing
could be much more efficient for all analysis to be
held in the future, if this technique was proven to
give reasonable results.
In this paper, we employ a semantic similarity
mechanism automatically obtained in an unsuper-
vised way from a large corpus of texts in order
to check the similarity of color and emotion via
computational analysis methods. We adopt the
psycholinguistic experiments as references, with
which we compare our results to find out if there
is a correlation between the two approaches.
28
The paper is organized as follows. In Section
2, we introduce some related work focusing on
the association of color and emotion only from
a psycholinguistic point of view, since this topic
has not been addressed by computational analysis
techniques so far. In Section 3, we describe the
methodology for implementing a similarity be-
tween colors and emotions, in particular how to
represent an emotion in a latent semantic space.
We present the evaluation of our approach and
make a comparison with the results of psycholin-
guistic experiments in Section 4. Lastly, we report
the conclusions and possible future work in Sec-
tion 5.
2 Background
As we mentioned previously, there has been a con-
siderable interest in the psycholinguistic research
area addressing the impact of color on emotions.
(Zentner, 2001) mainly addressed the question
of whether young children could show reliable
color preferences. This study also tried to make a
comparison with the results obtained with adults
and older children. Subjects? color preferences
were obtained by asking them to choose the one
that they prefer among an array of colored card-
board rectangles. As an alternative way to repre-
sent musical information for providing feedback
on emotion expression in music, (Bresin, 2005)
suggested to use a graphical non-verbal represen-
tation of expressivity in music performance by ex-
ploiting color as an index of emotion. And for the
purpose of determining which colors were most
suitable for an emotional expression, some ex-
periments were conducted, where subjects rated
how well several colors and their nuances corre-
sponded to music performances expressing differ-
ent emotions. (Kaya, 2004) tried to investigate
and discuss the associations between color and
emotion by conducting experiments where college
students were asked to indicate their emotional re-
sponses to principal, intermediate and achromatic
colors, and the reasons for their choices.
There exist also some research investigating
whether the color perception is related to the re-
gion of the subject. For instance, (Gao et al,
2007) analyzed and compared the color emotions
of people from seven regions in a psychophysical
experiment, with an attempt to clarify the influ-
ences of culture and color perception attributes on
color emotions. This study suggested that it might
be possible to compose a color emotion space by
using a restricted number of factors. As for (So-
riano and Valenzuela, 2009), this study tried to
find out why there was often a relationship be-
tween color and emotion words in different lan-
guages. In order to achieve this, a new experi-
mental methodology called the Implicit Associa-
tion Test (IAT) was used to explore the implicit
connotative structure of the Peninsular Spanish
color terms in terms of Osgood?s universal se-
mantic dimensions explained in (Adams and Os-
good, 1973). The research conducted by (Xin et
al., 2004) tried to compare the color emotional re-
sponses that were obtained by conducting visual
experiments in different regions by using a set of
color samples. A quantitative approach was used
in this study in an attempt to compare the color
emotions among these regions. (Madden et al,
2000) focused on the possible power of color for
creating and sustaining brand and corporate im-
ages in international marketing. This study tried
to explore the favorite colors of consumers from
different countries, the meanings they associated
with colors, and their color preferences for a logo.
The study that we will use for evaluating our re-
sults is a work which focused on the topic ?emo-
tional responses to color used in advertisement?
(Alt, 2008). During the experiments, this study
conducted a survey where the subjects were re-
quired to view an advertisement with a dominant
color hue, and then select a specific emotional re-
sponse and a positive/negative orientation related
with this color. More than 150 subjects partici-
pated in this study, roughly equally partitioned in
gender. There are two main reasons why we pre-
ferred to use this study for our evaluation proce-
dure. Firstly, the presentation and organization of
the results provide a good reference for our own
experiments. In addition, it focusses on adver-
tisement, which is one of the applicative fields we
want to address in future work.
3 Methodology
Sensing emotions from text is an appealing task
of natural language processing (Pang and Lee,
29
2008; Strapparava and Mihalcea, 2007): the au-
tomatic recognition of affective states is becom-
ing a fundamental issue in several domains such
as human-computer interaction or sentiment anal-
ysis for opinion mining. Indeed, a large amount
of textual material has become available form the
Web (e.g. blogs, forums, social networks), rais-
ing the attractiveness of empirical methods analy-
sis on this field.
For representing the emotions, we exploit the
methodology described in (Strapparava and Mi-
halcea, 2008). The idea underlying the method is
the distinction between direct and indirect affec-
tive words.
For direct affective words (i.e. words that di-
rectly denote emotions), authors refer to the
WORDNET AFFECT (Strapparava and Valitutti,
2004) lexicon, a freely available extension of the
WORDNET database which employs some basic
emotion labels (e.g. anger, disgust, fear, joy, sad-
ness) to annotate WORDNET synsets.
For indirect affective words, a crucial aspect
is building a mechanism to represent an emotion
starting from affective lexical concepts and to in-
troduce a semantic similarity among generic terms
(and hence also words denoting colors) and these
emotion representations.
Latent Semantic Analysis is used to acquire,
in an unsupervised setting, a vector space from
the British National Corpus1. In LSA, term co-
occurrences in a corpus are captured by means of
a dimensionality reduction operated by a singu-
lar value decomposition on the term-by-document
matrix representing the corpus. LSA has the
advantage of allowing homogeneous representa-
tion and comparison of words, word sets (e.g.
synsets), text fragments or entire documents. For
representing word sets and texts by means of a
LSA vector, it is possible to use a variation of
the pseudo-document methodology described in
(Berry, 1992). This variation takes into account
also a tf-idf weighting schema. In practice, each
document can be represented in the LSA space by
summing up the normalized LSA vectors of all the
1BNC is a very large (over 100 million words) cor-
pus of modern English, both spoken and written (see
http://www.hcu.ox.ac.uk/bnc/). Other more spe-
cific corpora could also be considered, to obtain a more do-
main oriented similarity.
terms contained in it. Therefore a set of words
(and even all the words labeled with a particular
emotion) can be represented in the LSA space,
performing the pseudo-document technique on
them.
As stated in (Strapparava and Mihalcea, 2008),
each emotion can be represented in various ways
in the LSA space. The particular one that we are
employing is the ?LSA Emotion Synset? setting,
which has proved to give the best results in terms
of fine-grained emotion sensing. In this setting,
the synsets of direct emotion words, taken form
WORDNET AFFECT, are considered.
For our purposes, we compare the similarities
among the representations of colors and emotions
in the latent similarity space.
4 Experiments
For the experiments in this paper, we built an
LSA vector space on the full BNC corpus us-
ing 400 dimensions. To compare our approach
with the psycholinguistic experiments reported in
(Alt, 2008), we represent the following emotions:
anger, aversion/disgust, fear, happiness/joy, and
sadness. And we consider the colors Blue, Red,
Green, Orange, Purple, Yellow. Table 1 reports
the rankings of emotions according to colors from
(Alt, 2008).
Color Ranking of Emotions
Anger Aversion/
Disgust
Fear Joy Sadness
Blue 5 2 4 1 3
Red 1 4 2 3 5
Green 5 2 3 1 4
Orange 4 2 3 1 5
Purple 5 2 4 1 3
Yellow 5 2 4 1 3
Table 1: Emotions ranked by colors from psy-
cholinguistic experiments
In Table 2 we report our results of ranking emo-
tions with respect to colors using the similarity
mechanism described in the previous section. To
evaluate our results with respect to the psycholin-
guistic reference, we use Spearman correlation
coefficient. The resulting correlation between two
approaches for each color is reported in Table 3.
We can observe that the global correlation is
rather good (0.75). In particular, it is very high
30
Color Ranking Emotions using Similarity with Color
Anger Aversion/
Disgust
Fear Joy Sadness
Blue 4 2 3 1 5
Red 4 3 2 1 5
Green 4 2 3 1 5
Orange 4 2 3 1 5
Purple 5 2 3 1 4
Yellow 4 2 3 1 5
Table 2: Emotions ranked by similarity with col-
ors
Color Correlation
Blue 0.7
Red 0.3
Green 0.9
Orange 1.0
Purple 0.9
Yellow 0.7
Total 0.75
Table 3: Correlation
for the colors Orange, Green and Purple, which
implies that the use of language for these colors is
quite in accordance with psycholinguistic results.
The results are good for Blue and Yellow as well,
while the correlation is not so high for Red. This
could suggest that Red is a quite ambiguous color
with respect to emotions.
5 Conclusions
There are emotional and symbolic associations
with different colors. This is also revealed in our
daily use of language, as we frequently make ref-
erences to colors for increasing our expressiveness
by invoking different emotions. While most of
the research conducted so far with the aim of an-
alyzing the relationship between color and emo-
tion was based on psycholinguistic experiments,
the goal of this study was exploring this associ-
ation by employing a corpus-based approach for
affective sensing.
In order to show that our approach was provid-
ing reasonable results, we adopted one of the ex-
isting psycholinguistic experiments as a reference.
Following that adoption, we made a comparison
between the results of this research and our own
technique. Since we have observed that these two
results were highly correlated as we expected, we
would like to explore further this direction. Cer-
tainly different cultures can play a role for variant
emotional responses (Adams and Osgood, 1973).
Thus, as a next step, we are planning to investi-
gate how the perception of color by human be-
ings varies in different languages by again con-
ducting a computational analysis with NLP tech-
niques. Employing this approach could be very
useful and efficient for the design of applications
related to the fields of multimedia, automatic ad-
vertisement production, marketing and education
(e.g. e-learning environments)
In addition, based on our exploration about the
color perception of emotions from a corpus-based
point of view, we suggest that ?visual? informa-
tion regarding objects and events could be ex-
tracted from large amounts of text, using the same
kind of techniques proposed in the present paper.
This information can be easily exploited for cre-
ation of dictionaries or used in dynamic visualiza-
tion of text such as kinetic typography (Strappar-
ava et al, 2007). As a concrete example, our ap-
proach can be extended to discover the association
of colors not only with emotions, but also with in-
direct affective words in various languages. We
believe that the discovery of this kind of relation-
ship would allow us to automatically build col-
orful dictionaries, which could substantially help
users with both interpretation and memorization
processes.
References
Adams, F. M. and C. E. Osgood. 1973. A cross-
cultural study of the affective meanings of colour.
Journal of cross-cultural psychology, 4:135?156.
Alt, M. 2008. Emotional responses to color associated
with an advertisement. Master?s thesis, Graduate
College of Bowling Green State University, Bowl-
ing Green, Ohio.
Berry, M. 1992. Large-scale sparse singular value
computations. International Journal of Supercom-
puter Applications, 6(1):13?49.
Bresin, R. 2005. What is the color of that music
performance? In Proceedings of the International
Computer Music Conference (ICMA 2005), pages
367?370.
Gao, X.P., J.H. Xin, T. Sato, A. Hansuebsai, M. Scalzo,
K. Kajiwara, S. Guan, J. Valldeperas, M. Lis Jose,
31
and M. Billger. 2007. Analysis of cross-cultural
color emotion. Color research and application,
32(223?229).
Kaya, N. 2004. Relationship between color and emo-
tion: a study of college students. College Student
Journal, pages 396?405.
Madden, T. J., K. Hewett, and S. Roth Martin. 2000.
Managing images in different cultures: A cross-
national study of color meanings and preferences.
Journal of International Marketing, 8(4):90?107.
Ortony, A., G. L. Clore, and M. A. Foss. 1987.
The psychological foundations of the affective lexi-
con. Journal of Personality and Social Psychology,
53:751?766.
Pang, B. and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Soriano, C. and J. Valenzuela. 2009. Emotion and
colour across languages: implicit associations in
spanish colour terms. Social Science Information,
48:421?445, September.
Strapparava, C. and R. Mihalcea. 2007. SemEval-
2007 task 14: Affective Text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval 2007), pages 70?74, Prague, June.
Strapparava, C. and R. Mihalcea. 2008. Learning to
identify emotions in text. In SAC ?08: Proceedings
of the 2008 ACM symposium on Applied computing,
pages 1556?1560, New York, NY, USA. ACM.
Strapparava, C. and A. Valitutti. 2004. WordNet-
Affect: an affective extension of WordNet. In Pro-
ceedings of LREC, volume 4, pages 1083?1086.
Strapparava, C., A. Valitutti, and O. Stock. 2007.
Dances with words. In Proc. of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07), Hyderabad, India, January.
Xin, J.H., K.M. Cheng, G. Taylor, T. Sato, and A. Han-
suebsai. 2004. A cross-regional comparison of
colour emotions. part I. quantitative analysis. Color
Research and Application, 29:451?457.
Zentner, M. R. 2001. Preferences for colors and color-
emotion combinations in early childhood. Develop-
mental Science, 4(4):389?398.
32
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 114?125,
Dublin, Ireland, August 23, 2014.
A Computational Approach to Generate a Sensorial Lexicon
Serra Sinem Tekiro?lu
FBK-Irst
Via Sommarive 18
Povo, I-38100 Trento
tekiroglu@fbk.eu
G?zde ?zbal
Trento RISE
Via Sommarive 18
Povo, I-38100 Trento
gozbalde@gmail.com
Carlo Strapparava
FBK-Irst
Via Sommarive 18
Povo, I-38100 Trento
strappa@fbk.eu
Abstract
While humans are capable of building connections between words and sensorial modalities by
using commonsense knowledge, it is not straightforward for machines to interpret sensorial in-
formation. To this end, a lexicon associating words with human senses, namely sight, hearing,
taste, smell and touch, would be crucial. Nonetheless, to the best of our knowledge, there is no
systematic attempt in the literature to build such a resource. In this paper, we propose a compu-
tational method based on bootstrapping and corpus statistics to automatically associate English
words with senses. To evaluate the quality of the resulting lexicon, we create a gold standard via
crowdsourcing and show that a simple classifier relying on the lexicon outperforms two base-
lines on a sensory classification task, both at word and sentence level. The results confirm the
soundness of the proposed approach for the construction of the lexicon and the usefulness of the
resource for computational applications.
1 Introduction
The connection between our senses and the way we perceive the world has been an important philosophi-
cal topic for centuries. According to a classification that dates back to Aristotle (Johansen, 1997), senses
can be categorized as sight, hearing, taste, smell and touch. With the help of perception, we can process
the data coming from our sensory receptors and become aware of our environment. While interpreting
sensory data, we unconsciously use our existing knowledge, experience and understanding of the world
to create a private experience (Bernstein, 2010).
Language has a significant role as our main communication device to convert our private experiences
to shared representations of the environment that we perceive (Majid and Levinson, 2011). As a basic
example, giving a name to a color, such as red, provides a tool to describe a visual feature of an object.
In addition to the words which describe the direct sensorial features of objects, languages include many
other lexical items that are connected to sense modalities in various semantic roles. For instance, while
some words can be used to describe a perception activity (e.g., to smell, to gaze, to listen), others can
simply be physical phenomenons that can be perceived by sensory receptors (e.g., flower, fire, sugar).
Common usage of language can be very dense in terms of sensorial words. As an example, the sentence
?I tasted a delicious soup.? contains three sensorial words: to taste as a perception activity, delicious as
a perceived sensorial feature and soup as a physical phenomenon. While we, as humans, have the ability
to connect words with senses intuitively by using our commonsense knowledge, it is not straightforward
for machines to interpret sensorial information.
From a computational point of view, a sensorial lexicon could be useful for many scenarios. Rodriguez-
Esteban and Rzhetsky (2008) report that using words related to human senses in a piece of text could
clarify the meaning of an abstract concept by facilitating a more concrete imagination. Based on this
result, an existing text could be automatically modified with sensory words for various purposes such
as attracting attention or biasing the audience towards a specific concept. In addition, sensory words
can be utilized to affect private psychology by inducing a positive or negative sentiment (Majid and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/
114
Levinson, 2011). As an example, de Araujo et al. (2005) show that the pleasantness level of the same
odor can be altered by labeling it as body odor or cheddar cheese. As another motivation, the readability
and understandability of text could also be enhanced by using sensory words (Rodriguez-Esteban and
Rzhetsky, 2008).
Yet another area which would benefit from such a resource is advertisement especially by using synaes-
thesia1, as it reinforces creative thinking and it is commonly exploited as an imagination boosting tool in
advertisement slogans (Pricken, 2008). As an example, we can consider the slogans ?Taste the rainbow?
where the sense of sight is combined with the sense of taste or ?Hear the big picture? where sight and
hearing are merged.
There are various studies both in computational linguistics and cognitive science that build resources
associating words with several cognitive features such as abstractness-concreteness (Coltheart, 1981;
Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors
(?zbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our
knowledge, there is no attempt in the literature to build a resource that associates words with senses.
In this paper, we propose a computational method to automatically generate a sensorial lexicon2 that
associates words in English with senses. Our method consists of two main steps. First, we generate the
initial seed words for each sense category with the help of a bootstrapping approach. Then, we exploit a
corpus based probabilistic technique to create the final lexicon. We evaluate this resource with the help
of a gold standard that we obtain by using the crowdsourcing service provided by CrowdFlower3.
The sensorial lexicon embodies 22,684 English lemmas together with their part-of-speech (POS) in-
formation that have been linked to one or more of the five senses. Each entry in this lexicon consists of a
lemma-POS pair and a score for each sense that indicates the degree of association. For instance, the verb
stink has the highest score for smell as expected while the scores for the other four senses are very low.
The noun tree, which is a concrete object and might be perceived by multiple senses, has high scores for
sight, touch and smell.
The rest of the paper is organized as follows. We first review previous work relevant to this task in
Section 2. Then in Section 3, we describe the proposed approach in detail. In Section 4, we explain the
annotation process that we conducted and the evaluation strategy that we adopted. Finally, in Section 4,
we draw our conclusions and outline possible future directions.
2 Related Work
Since to the best of our knowledge there is no attempt in the literature to automatically associate words
with human senses, in this section we will summarize the most relevant studies that focused on linking
words with various other cognitive features.
There are several studies dealing with word-emotion associations. WordNet Affect Lexicon (Strap-
parava and Valitutti, 2004) maps WordNet (Fellbaum, 1998) synsets to various cognitive features (e.g.,
emotion, mood, behaviour). This resource is created by using a small set of synsets as seeds and expand-
ing them with the help of semantic and lexical relations among these synsets. Yang et al. (2007) propose
a collocation model with emoticons instead of seed words while creating an emotion lexicon from a cor-
pus. Perrie et al. (2013) build a word-emotion association lexicon by using subsets of a human-annotated
lexicon as seed sets. The authors use frequencies, counts, or unique seed words extracted from an n-
gram corpus to create lexicons in different sizes. They propose that larger lexicons with less accurate
generation method perform better than the smaller human annotated lexicons. While a major drawback
of manually generated lexicons is that they require a great deal of human labor, crowdsourcing services
provide an easier procedure for manual annotations. Mohammad and Turney (2010) generate an emotion
lexicon by using the crowdsourcing service provided by Amazon Mechanical Turk4 and it covers 14,200
term-emotion associations.
1American Heritage Dictionary (http://ahdictionary.com/) defines synaesthesia in linguistics as the description of one
kind of sense impression by using words that normally describe another.
2The sensorial lexicon is publicly available, upon request to the authors.
3http://www.crowdflower.com/
4http://www.mturk.com/mturk
115
Regarding the sentiment orientations and subjectivity levels of words, Sentiwordnet (Esuli and Sebas-
tiani, 2006) is constructed as an extension toWordNet and it provides sentiments in synset level. Positive,
negative and neutral values are assigned to synsets by using ternary classifiers and synset glosses. An-
other study that has been inspirational for the design of our approach is Banea et al. (2008). The authors
generate a subjectivity lexicon starting with a set of seed words and then using a similarity measure among
the seeds and the candidate words.
Concerning the association between colors and words, Mohammad (2011) builds a color-word asso-
ciation lexicon by organizing a crowdsourcing task on Amazon Mechanical Turk. Instead, ?zbal et al.
(2011) aim to automate this process and propose three computational methods based on image analysis,
language models and latent semantic analysis (LSA) (Landauer and Dumais, 1997). The authors com-
pare these methods against a gold standard obtained by the crowdsourcing service of AmazonMechanical
Turk. The best performance is obtained by using image features while LSA performs slightly better than
the baseline.
Finally, there have been efforts in the literature about the association of words with their abstractness-
concreteness and imageability levels. MRC Psycholinguistic Database (Coltheart, 1981) includes
abstractness-concreteness and imageability ratings of a small set of words determined according to psy-
cholinguistic experiments. Turney et al. (2011) propose to use LSA similarities of words with a set of
seed words to automatically calculate the abstractness and concreteness degrees of words.
3 Automatically Associating Senses with Words
We adopt a two phased computational approach to construct a large sensorial lexicon. First, we employ a
bootstrapping strategy to generate a sufficient number of sensory seed words from a small set of manually
selected seed words. In the second phase, we perform a corpus based probabilistic method to estimate
the association scores to build a larger lexicon.
3.1 Selecting Seed Words
The first phase of the lexicon construction process aims to collect sensorial seed words, which are directly
related to senses (e.g., sound, tasty and sightedness). To achieve that, we utilized a lexical database called
FrameNet (Baker et al., 1998), which is built upon semantic frames of concepts in English and lexical
units (i.e., words) that evoke these frames. The basic idea behind this resource is that meanings of words
can be understood on the basis of a semantic frame. A semantic frame consists of semantic roles called
frame elements, which are manually annotated in more than 170,000 sentences. We have considered
FrameNet to be especially suitable for the collection of sensorial seed words since it includes semantic
roles and syntactic features of sensational and perceptional concepts.
In order to determine the seed lemma-POS pairs in FrameNet, we first manually determined 31
frames that we found to be highly connected to senses such as Hear, Color, Temperature and Percep-
tion_experience. Then, we conducted an annotation task and asked 3 annotators to determine which
senses the lemma-POS pairs evoking the collected frames are associated with. At the end of this task, we
collected all the pairs (i.e., 277) with 100% agreement to constitute our initial seed set. This set contains
277 lemma-POS pairs associated with a specific sense such as the verb click with hearing, the noun glitter
with sight and aromatic with smell.
3.2 Seed Expansion via Bootstrapping
In this step, we aim to extend the seed list that we obtained from FrameNet with the help of a bootstrapping
approach. To achieve that, we adopt a similar approach to Dias et al. (2014), who propose a repetitive
semantic expansion model to automatically build temporal associations of synsets in WordNet. Figure 1
provides an overview of the bootstrapping process. At each iteration, we first expand the seed list by
using semantic relations provided by WordNet. We then evaluate the accuracy of the new seed list for
sense classification by means of cross-validation against WordNet glosses. For each sense, we continue
iterating until the cross-validation accuracy becomes stable or starts to decrease. The following sections
explain the whole process in detail.
116
WordNet
MapNet
SVM Cross-validation
Synset SetExpansion
SenseSeedSynsetsFrameNet
Break-PointDetection
Figure 1: Bootstrapping procedure to expand the seed list.
Extending the Seed List with WordNet
While the initial sensory seed list obtained fromFrameNet contains only 277 lemma-POS pairs, we extend
this list by utilizing the semantic relations provided by WordNet. To achieve that, we first map each
lemma-POS pair in the seed list to WordNet synsets with the help of MapNet (Tonelli and Pighin, 2009),
which is a resource providing directmapping betweenWordNet synsets and FrameNet lexical units. Then,
we add to the list the synsets that are in WordNet relations direct antonymy, similarity, derived-from,
derivationally-related, pertains-to, attribute and also-see with the already existing seeds. For instance,
we add the synset containing the verb laugh for the synset of the verb crywith the relation direct antonymy,
or the synset containing the adjective chilly for the synset of the adjective cold with the relation similarity.
We prefer to use these relations as they might allow us to preserve the semantic information as much as
possible during the extension process. It is worth mentioning that these relations were also found to be
appropriate for preserving the affective connotation by Valitutti et al. (2004). Additionally, we use the
relations hyponym and hyponym-instance to enrich the seed set with semantically more specific synsets.
For instance, for the noun seed smell, we expand the list with the hyponyms of its synset such as the
nouns bouquet, fragrance, fragrancy, redolence and sweetness.
Cross-validation for Sensorial Model
After obtaining new synsets with the help of WordNet relations in each bootstrapping cycle, we build a
five-class sense classifier over the seed synsets defined by their glosses provided in WordNet. Similarly
to Dias et al. (2014), we assume that the sense information of sensorial synsets is preserved in their
definitions. Accordingly, we employ a support vector machine (SVM) (Boser et al., 1992; Vapnik, 1998)
model with second degree polynomial kernel by representing the gloss of each synset as a vector of
lemmas weighted by their counts. For each synset, its gloss is lemmatized by using Stanford Core NLP5
and cleaned from the stop words. After each iteration cycle, we perform a 10-fold cross-validation in the
updated seed list to detect the accuracy of the new sensorial model. For each sense class, we continue
iterating and thereby expanding the seed list until the classifier accuracy steadily drops.
Table 1 lists the precision (P), recall (R) and F1 values obtained for each sense after each iteration until
the bootstrapping mechanism stops. While the iteration number is provided in the first column, the values
under the last column group present the micro-average of the resulting multi-class classifier. The change
in the performance values of each class in each iteration reveal that the number of iterations required to
obtain the seed lists varies for each sense. For instance, the F1 value of touch continues to increase until
the fourth cycle whereas hearing records a sharp decrease after the first iteration.
After the bootstrapping process, we create the final lexicon by repeating the expansion for each class
until the optimal number of iterations is reached. The last row of Table 1, labeled as Final, demonstrates
the accuracy of the classifier trained and tested on the final lexicon, i.e., using the seeds selected after
iteration 2 for Sight, iteration 1 for Hearing, iteration 3 for Taste and Smell and iteration 4 for Touch.
5http://nlp.stanford.edu/software/corenlp.shtml
117
Sight Hearing Taste Smell Touch Micro-average
It# P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
1 .873 .506 .640 .893 .607 .723 .716 .983 .828 .900 .273 .419 .759 .320 .451 .780 .754 .729
2 .666 .890 .762 .829 .414 .552 .869 .929 .898 .746 .473 .579 .714 .439 .543 .791 .787 .772
3 .643 .878 .742 .863 .390 .538 .891 .909 .900 .667 .525 .588 .720 .482 .578 .796 .786 .776
4 .641 .869 .738 .832 .400 .540 .866 .888 .877 .704 .500 .585 .736 .477 .579 .784 .774 .765
5 .640 .869 .737 .832 .400 .540 .866 .888 .877 .704 .500 .585 .738 .474 .578 .784 .774 .764
Final .805 .827 .816 .840 .408 .549 .814 .942 .873 .685 .534 .600 .760 .582 .659 .800 .802 .790
Table 1: Bootstrapping cycles with validation results.
According to F1 measurements of each iteration, while hearing and taste have a lower value for the final
model, sight, smell and touch have higher results. It should also be noted that the micro-average of the F1
values of the final model shows an increase when compared to the third iteration which has the highest
avarage F1 value among the iterations. At the end of this step we have a seed synset list consisting of
2572 synsets yielding the highest performance when used to learn a sensorial model.
3.3 Sensorial Lexicon Construction Using Corpus Statistics
After generating the seed lists consisting of synsets for each sense category with the help of a set of
WordNet relations and a bootstrapping process, we use corpus statistics to create our final sensorial lex-
icon. More specifically, we exploit a probabilistic approach based on the co-occurence of the seeds and
the candidate lexical entries. Since working on the synset level would raise the data sparsity problem in
synset tagged corpora such as SemCor (Miller et al., 1993) and we need a corpus that provides sufficient
statistical information, we migrate from synset level to lexical level. Accordingly, we treat each POS
role of the same lemmas as a distinct seed and extract 4287 lemma-POS pairs from 2572 synsets. In this
section, we explain the steps to construct our final sensorial lexicon in detail.
Corpus and Candidate Words
As a corpus, we use a subset of English GigaWord 5th Edition released by Linguistic Data Consortium
(LDC)6. This resource is a collection of almost 10million English newswire documents collected in recent
years, whose content sums up to nearly 5 billion words. The richly annotated GigaWord data comprises
automatic parses obtained with the Stanford parser (Klein and Manning, 2003) so that we easily have
access to the lemma and POS information of each word in the resource. For the scope of this study, we
work on a randomly chosen subset that contains 79800 sentences and we define a co-occurrence event as
the co-existence of a candidate word and a seed word within a window of 9 words (the candidate word,
4 words to its left and 4 words to its right). In this manner, we analyze the cooccurrence of each unique
lemma-POS pair in the corpus with the sense seeds. We eliminate the candidates which have less than 5
cooccurences with the sense categories.
Normalized Pointwise Mutual Information
For the cooccurrence analysis of the candidate words and seeds, we use pointwise mutual information
(PMI), which is simply a measure of association between the probability of the co-occurence of two
events and their individual probabilities when they are assumed to be independent (Church and Hanks,
1990) and it is calculated as:
PMI(x, y) = log
[
p(x, y)
p(x)p(y)
]
(1)
To calculate the PMI value of a candidate word and a specific sense, we consider p(x) as the probability
of the candidate word to occur in the corpus. Therefore, p(x) is calculated as p(x) = c(x)/N , where c(x)
is the total count of the occurences of the candidate word x in the corpus and N is the total cooccurrence
count of all words in the corpus. Similarly, we calculate p(y) as the total occurrence count of all the
6http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07
118
majority class 3 4 5 6 7 8 9 10
word 0 0.98 3.84 9.96 11.63 16.66 34.41 12.42
sentence 0.58 2.35 7.07 10.91 13.27 15.63 21.23 16.51
Table 2: Percentage of words and sentences in each majority class.
seeds for the sense considered (y). p(y) can thus be formulated as c(y)/N . p(x,y) is the probability of the
cooccurence of a candidate word x with a sense event y.
A major shortcoming of PMI is its sensitivity for low frequency data (Bouma, 2009). As one possible
solution, the author introduces Normalized Pointwise Mutual Information (NPMI), which normalizes the
PMI values to the range (-1, +1) with the following formula:
NPMI(x, y) =
PMI(x, y)
? log p(x, y) (2)
We calculated NPMI values for each candidate word and five sense events in the corpus. The sensorial
lexicon covers 22,684 lemma-POS pairs and a score for each sense class that denotes their association
degrees.
4 Evaluation
To evaluate the performance of the sensorial classification and the quality of the lexicon, we first created
a gold standard with the help of a crowdsourcing task. Then, we compared the decisions coming from the
lexicon against the gold standard. In this section, we explain the annotation process that we conducted
and the evaluation technique that we adopted in detail. We also provide a brief discussion about the
obtained results.
4.1 Crowdsourcing to Build a Gold Standard
The evaluation phase of the sensorial lexicon requires a gold standard data to be able to conduct a mean-
ingful assessment. Since to our best knowledge there is no resource with sensory associations of words
or sentences, we designed our own annotation task using the crowdsourcing service CrowdFlower. For
the annotation task, we first compiled a collection of sentences to be annotated. Then, we designed two
questions that the annotators were expected to answer for a given sentence. While the first question is
related to the sense association of a whole sentence, the second asks the annotators the sense associations
of the words in the same sentence to collect a fine-grained gold standard.
We collected a dataset of 340 sentences consisting of 300 advertisement slogans from 11 advertisement
categories (e.g., fashion, food, electronics) and 40 story sentences from a story corpus. We collected the
slogans from various online resources such as http://slogans.wikia.com/wiki and http://www.
adslogans.co.uk/. The story corpus is generated as part of a dissertation research (Alm, 2008) and it
provides stories as a collection of sentences.
In both resources, we first determined the candidate sentences which had at least five tokens and con-
tained at least one adjective, verb or noun. In addition, we replaced the brand names in the advertisement
slogans with X to prevent any bias. For instance, the name of a well-known restaurant in a slogan might
cause a bias towards taste. Finally, the slogans used in the annotation task were chosen randomly among
the candidate sentences by considering a balanced number of slogans from each category. Similarly, 40
story sentences were selected randomly among the candidate story sentences. To give a more concrete
idea, for our dataset we obtained an advertisement slogan such as ?X's Sugar Frosted Flakes They're
Great!? or a story sentence such as ?The ground is frozen, and besides the snow has covered everything.?
In the crowdsourcing task we designed, the annotators were required to answer 2 questions for a given
sentence. In the first question, they were asked to detect the human senses conveyed or directly described
by a given sentence. To exemplify these cases, we provided two examples such as ?I saw the cat? that
directly mentions the action of seeing and ?The sun was shining on the blue water.? that conveys the sense
of sight by using visual descriptions or elements like ?blue? or ?shine? which are notable for their visual
119
Category Si He Ta Sm To
personal care 49.36 10.75 0.00 13.29 26.58
travel 58.18 0.00 29.09 0.00 12.72
fashion 43.47 0.00 0.00 26.08 30.43
beauty 84.56 0.00 0.00 0.00 15.43
computing 32.25 59.13 0.00 0.00 8.60
food 0.00 5.46 94.53 0.00 0.00
beverages 22.68 0.00 59.79 0.00 17.52
communications 25.00 67.50 0.00 0.00 0.075
electronics 45.94 54.05 0.00 0.00 0.00
education 28.57 42.85 0.00 0.00 28.57
transport 61.81 38.18 0.00 0.00 0.00
story 58.37 20.81 0.00 7.23 13.57
Table 3: The categories of the annotated data and their sense association percentages.
properties. The annotators were able to select more than one sense for each sentence and together with
the five senses we provided another option as None which should be selected when an annotator could
not associate a sentence with any sense. The second question was devoted do determining word-sense
associations. Here, the annotators were expected to associate the words in each sentence with at least
one sense. Again, annotators could choose None for every word that they could not confidently associate
with a sense.
The reliability of the annotators was evaluated on the basis of 20 control sentences which were highly
associated with a specific sense and which included at least one sensorial word. For instance, for the con-
trol sentence ?The skin you love to touch?, we only considered as reliable the annotators who associated
the sentence with touch and the word touch with the sense touch7. Similarly, for the slogan ?The most
colourful name in cosmetics.?, an annotator was expected to associate the sentence with at least the sense
sight and the word colorful to at least the sense sight. The raters who scored at least 70% accuracy on
average on the control questions for the two tasks were considered to be reliable. Each unit was annotated
by at least 10 reliable raters.
Similarly to Mohammad (2011) and ?zbal et al. (2011), we calculated the majority class of each anno-
tated item to measure the agreement among the annotators. Table 2 demonstrates the observed agreement
at both word and sentence level. Since 10 annotators participated in the task, the annotations with a ma-
jority class greater than 5 can be considered as reliable (?zbal et al., 2011). Indeed, for 85.10% of the
word annotations the absolute majority agreed on the same decision, while 77.58% of the annotations in
the sentence level have majority class greater than 5. The high agreement observed among the annotators
in both cases confirms the quality of the resulting gold standard data.
In Table 3, we present the results of the annotation task by providing the association percentage of each
category with each sense, namely sight (Si), hear (He), taste (Ta), smell (Sm) and touch (To). As demon-
strated in the table, while the sense of sight can be observed in almost every advertisement category and
in story, smell and taste are very rare. We observe that the story sentences invoke all sensorial modali-
ties except taste, although the percentage of sentences annotated with smell is relatively low. Similarly,
personal care category has an association with four of the senses while the other categories have either
very low or no association with some of the sense classes. Indeed, the perceived sensorial effects in the
sentences vary according to the category such that the slogans in the travel category are highly associated
with sight whereas the communication category is highly associated with hearing. While the connection
of the food and beverages categories with taste is very high as expected, they have no association with the
sense of smell. This kind of analysis could be useful for copywriters to decide which sensory modalities
to invoke while creating a slogan for a specific product category.
7If the annotators gave additional answers to the expected ones, we considered their answers as correct.
120
4.2 Evaluation Measures
Based on the annotation results of our crowdsourcing task, we propose an evaluation technique consider-
ing that a lemma-POS or a sentencemight be associated withmore than one sensorymodalities. Similar to
the evaluation framework defined by ?zbal et al. (2011), we adapt the evaluation measures of SemEval-
2007 English Lexical Substitution Task (McCarthy and Navigli, 2007), where a system generates one or
more possible substitutions for a target word in a sentence preserving its meaning.
For a given lemma-POS or a sentence, which we will name as item in the rest of the section, we allow
our system to provide as many sensorial associations as it determines using a specific lexicon. While
evaluating a sense-item association of a method, a best and an oot score are calculated by considering
the number of the annotators who associate that sense with the given item, the number of the annotators
who associate any sense with the given item and the number of the senses the system gives as an answer
for that item. More specifically, best scoring provides a credit for the best answer for a given item by
dividing it to the number of the answers of the system. oot scoring, on the other hand, considers only a
certain number of system answers for a given item and does not divide the credit to the total number of
the answers. Unlike the lexical substitution task, a limited set of labels (i.e., 5 sense labels and none) are
allowed for the sensorial annotation of sentences or lemma-POS pairs. For this reason, we reformulate
out-of-ten (oot) scoring used by McCarthy and Navigli (2007) as out-of-two.
In Equation 3, best score for a given item i from the set of items I, which consists of the items annotated
with a specific sense by a majority of 5 annotators, is formulated whereH
i
is the multiset of gold standard
sense associations for item i and S
i
is the set of sense associations provided by the system. oot scoring,
as formulated in Equation 4, accepts up to 2 sense associations s from the answers of system S
i
for a
given item i and the credit is not divided by the number of the answers of the system.
best (i) =
?
s?S
i
freq (s ? H
i
)
|H
i
| ? |S
i
|
(3)
oot (i) =
?
s?S
i
freq (s ? H
i
)
|H
i
|
(4)
As formulated in Equation 5, to calculate the precision of an item-sense association task with a specific
method, the sum of the scores (i.e., best or oot) for each item is divided by the number of items A, for
which the method can provide an answer. In recall, the denominator is the number of the items in the
gold standard for which an answer is given by the annotators.
P =
?
i?A
score
i
|A|
R =
?
i?I
score
i
|I|
(5)
4.3 Evaluation Method
For the evaluation, we compare the accuracy of a simple classifier based on the sensorial lexicon against
two baselines on a sense classification task, both at word and sentence level. To achieve that, we use
the gold standard that we obtain from the crowdsourcing task and the evaluation measures best and oot.
The lexicon-based classifier simply assigns to each word in a sentence the sense values found in the
lexicon. The first baseline simply assigns a random float value, which is in the range of (-1,1), to each
sense association of each lemma-POS pair in the sensorial lexicon. The second baseline instead builds
the associations by using a Latent Semantic Analysis space generated from the British National Corpus8
(BNC), which is a very large (over 100 million words) corpus of modern English. More specifically,
this baseline calculates the LSA similarities between each candidate lemma-POS pair and sense class
by taking the cosine similarity between the vector of the target lemma-POS pair and the average of the
vectors of the related sensory word (i.e., see, hear, touch, taste, and smell) for each possible POS tag.
For instance, to get the association score of a lemma-POS pair with the sense sight, we first average the
vectors of see (noun) and see (verb) before calculating its cosine similarity with the target lemma-POS
pair.
8http://www.hcu.ox.ac.uk/bnc/
121
For the first experiment, i.e., word-sense association, we automatically associate the lemma-POS pairs
obtained from the annotated dataset with senses by using i) the sensorial lexicon, ii) the random baseline,
iii) the LSA baseline. To achieve that, we lemmatize and POS tag each sentence in the dataset by using
Stanford CoreNLP. In the end, for eachmethod and target word, we obtain a list of senses sorted according
to their sensorial association values in decreasing order. It is worth noting that we only consider the non-
negative sensorial associations for the sensorial lexicon and the random baseline, and the associations
above the value of 0.4 which we empirically set as the threshold for the LSA baseline. For instance,
the sensorial lexicon associates the noun wine with [smell, taste, sight]. In this experiment, best scoring
considers the associated senses as the best answer, smell, taste, sight according to the previous example,
and calculates a score with respect to the best answer in the gold standard and the number of the senses
in this answer. Instead, oot scoring takes the first two answers, smell and taste according to the previous
example, and assigns the score accordingly.
To determine the senses associated with a sentence for the second experiment, we use a method similar
to the one proposed by Turney (2002). For each sense, we simply calculate the average score of the
lemma-POS pairs in a sentence. We set a threshold value of 0 to decide whether a sentence is associated
with a given sense. In this manner, we obtain a sorted list of average sensory scores for each sentence
according to the three methods. For instance, the classifier based on the sensorial lexicon associates the
sentence Smash it to pieces, love it to bits. with [touch, taste]. For the best score, only touch would be
considered, whereas oot would consider both touch and taste.
4.4 Evaluation Results
In Table 4, we list the F1 values that we obtained with the classifier using the sensorial lexicon and the
two baselines (Random and LSA) according to both best and oot measures. In addition, we provide
the performance of the sensorial lexicon in two preliminary steps, before bootstrapping (BB) and after
bootstrapping (AB) to observe the incremental progress of the lexicon construction method. As can be
observed from the table, the best performance for both experiments is achieved by the sensorial lexicon
when compared against the baselines.
While in the first experiment the lexicon generated after the bootstrapping step (AB) provides a very
similar performance to the final lexicon according to the bestmeasure, it can only build sense associations
for 69 lemmas out of 153 appearing in the gold standard. Instead, the final lexicon attempts to resolve
129 lemma-sense associations and results in a better recall value. Additionally, AB yields a very high
precision as expected, since it is created by a controlled semantical expansion from manually annotated
sensorial words. The LSA baseline slightly improves the random baseline according to both best and oot
measures and it also outperforms BB for oot. BB lexicon includes only 573 lemmas which are collected
from 277 synsets and we can not obtain 2 sense association scores for oot in this lexicon since each lemma
is associated with only one sense with a value of 1.
Concerning the sentence classification experiment, the classifier using the sensorial lexicon yields the
highest performance in bothmeasures. The very high F1 value obtained with the oot scoring indicates that
the right answer for a sentence is included in the first two decisions in many cases. The low performance
of the LSA baseline might be arising due to its tendency to link the sentences with the sense of touch (i.e.,
215 sentences out of 320 gold standard data). It would be interesting to see the impact of using another
corpus to build the LSA space and constituting the sense vectors differently.
After the manual analysis of the sensorial lexicon and gold standard data, we observe that the sensorial
classification task could be nontrivial. For instance, a story sentence ?He went to sleep again and snored
until the windows shook.? has been most frequently annotated as hearing. While the sensorial lexicon
classifier associates this sentence with touch as the best answer, it can provide the correct association
hearing as the second best answer. To find out the best sensorial association for a sentence, a classification
method which exploits various aspects of sensorial elements in a sentence, such as the number of sensorial
words or their dependencies, could be a better approach than using only the average sensorial values.
Based on our observations in the error cases, the advertisement slogan ?100% pure squeezed sunshine?
is associated with touch as the best answer by both the sensorial lexicon and LSA baseline while it is most
frequently annotated as sight in the gold standard. This slogan is an example usage of synaesthesia and
122
Lemma Sentence
Model best oot best oot
Random 21.10 37.59 21.10 37.59
LSA 26.35 37.60 31.01 37.63
Lexicon-BB 45.22 45.22 49.60 51.12
Lexicon-AB 55.85 55.85 59.89 63.21
Sensorial Lexicon 55.86 80.13 69.76 80.73
Table 4: Evaluation results.
metaphors in advertising language. To clarify, a product from the category of beverages, which might be
assumed to have a taste association, is described by a metaphorical substitution of a taste-related noun,
most probably the name of a fruit, with a sight-related noun; sunshine. This metaphorical substitution,
then used as the object of a touch-related verb, to squeeze, produces a synaesthetic expression with touch
and sight.
5 Conclusion
In this paper we have presented a computational method to build a lexicon that associates words with
senses by employing a two-step strategy. First, we collected seed words by using a bootstrapping ap-
proach based on a set of WordNet relations. Then, we performed a corpus based statistical analysis to
produce the final lexicon. The resulting sensorial lexicon consists of 22,684 lemma-POS pairs and their
association degrees with five sensory modalities. To our best knowledge, this is the first systematic at-
tempt to build a sensorial lexicon and we believe that our contribution constitutes a valid starting point
for the community to consider sensorial information conveyed by text as a feature for various tasks and
applications. The results that we obtain by comparing our lexicon against the gold standard are promis-
ing even though not conclusive. The results confirm the soundness of the proposed approach for the
construction of the lexicon and the usefulness of the resource for text classification and possibly other
computational applications.
As future work, we would like to explore the effect of using different kinds of WordNet relations dur-
ing the bootstrapping phase. It would also be interesting to experiment with relations provided by other
resources such as ConceptNet (Liu and Singh, 2004), which is a semantic network containing common
sense, cultural and scientific knowledge. We would also like to use the sensorial lexicon for various
applicative scenarios such as slanting existing text towards a specific sense with text modification. We
believe that our resource could be extremely useful for automatic content personalization according to
user profiles. As an example, one can imagine a system that automatically replaces hearing based ex-
pressions with sight based ones in pieces of texts for a hearing-impaired person. Finally, we plan to
investigate the impact of using sensory information for metaphor detection and interpretation based on
our observations during the evaluation.
Acknowledgments
We would like to thank Daniele Pighin for his insightful comments and valuable suggestions.
This work was partially supported by the PerTe project (Trento RISE).
References
Ebba Cecilia Ovesdotter Alm. 2008. Affect in Text and Speech. Ph.D. thesis, University of Illinois at Urbana-
Champaign.
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. pages 86--90.
Association for Computational Linguistics.
123
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008. A bootstrapping method for building subjectivity lexi-
cons for languages with scarce resources. In LREC.
D. Bernstein. 2010. Essentials of Psychology. PSY 113 General Psychology Series. Cengage Learning.
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A Training Algorithm for Optimal Margin Clas-
sifiers. In Proceedings of the 5th Annual Workshop on Computational learning theory.
Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. Proceedings of GSCL,
pages 31--40.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Comput. Linguist., 16(1):22--29, March.
Max Coltheart. 1981. The mrc psycholinguistic database. The Quarterly Journal of Experimental Psychology,
33(4):497--505.
Ivan E de Araujo, Edmund T Rolls, Maria In?s Velazco, Christian Margot, and Isabelle Cayeux. 2005. Cognitive
modulation of olfactory processing. Neuron, 46(4):671--679.
Ga?l Harry Dias, Mohammed Hasanuzzaman, St?phane Ferrari, and Yann Mathet. 2014. Tempowordnet for
sentence time tagging. In Proceedings of the Companion Publication of the 23rd International Conference
on World Wide Web Companion, WWW Companion '14, pages 833--838, Republic and Canton of Geneva,
Switzerland. International World Wide Web Conferences Steering Committee.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion
mining. In Proceedings of LREC, volume 6, pages 417--422.
Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press, Cambridge, MA ;
London.
Thomas Kjeller Johansen. 1997. Aristotle on the Sense-organs. Cambridge University Press.
Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In IN PROCEEDINGS OF THE
41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, pages 423--430.
Thomas K Landauer and Susan T Dumais. 1997. A solution to plato's problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.
H. Liu and P. Singh. 2004. Conceptnet - a practical commonsense reasoning tool-kit. BT Technology Journal,
22(4):211--226, October.
Asifa Majid and Stephen C Levinson. 2011. The senses in language and culture. The Senses and Society, 6(1):5--
18.
Diana McCarthy and Roberto Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In Proceed-
ings of the 4th International Workshop on Semantic Evaluations, pages 48--53. Association for Computational
Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the workshop on Human Language Technology, HLT '93, pages 303--308, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Saif M Mohammad and Peter D Turney. 2010. Emotions evoked by common words and phrases: Using mechan-
ical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in Text, pages 26--34. Association for Computational Lin-
guistics.
Saif Mohammad. 2011. Colourful language: Measuring word-colour associations. In Proceedings of the 2nd
Workshop on Cognitive Modeling and Computational Linguistics, pages 97--106. Association for Computational
Linguistics.
G?zde ?zbal, Carlo Strapparava, Rada Mihalcea, and Daniele Pighin. 2011. A comparison of unsupervised meth-
ods to associate colors with words. In Affective Computing and Intelligent Interaction, pages 42--51. Springer.
Jessica Perrie, Aminul Islam, Evangelos Milios, and Vlado Keselj. 2013. Using google n-grams to expand word-
emotion association lexicon. In Computational Linguistics and Intelligent Text Processing, pages 137--148.
Springer.
124
Mario Pricken. 2008. Creative Advertising Ideas and Techniques from the World's Best Campaigns. Thames &
Hudson, 2nd edition.
R. Rodriguez-Esteban and A. Rzhetsky. 2008. Six senses in the literature. The bleak sensory landscape of biomed-
ical texts. EMBO reports, 9(3):212--215, March.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings of
LREC, volume 4, pages 1083--1086.
Sara Tonelli and Daniele Pighin. 2009. New features for framenet - wordnet mapping. In Proceedings of the
Thirteenth Conference on Computational Natural Language Learning (CoNLL'09), Boulder, CO, USA.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings of the 2011 Conference on the Empirical Methods in
Natural Language Processing, pages 680--690.
Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 417-
-424. Association for Computational Linguistics.
Alessandro Valitutti, Carlo Strapparava, and Oliviero Stock. 2004. Developing affective lexical resources. Psych-
Nology Journal, 2(1):61--83.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi Chen. 2007. Building emotion lexicon from weblog corpora.
In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages
133--136. Association for Computational Linguistics.
125
