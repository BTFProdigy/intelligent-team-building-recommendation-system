Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1423?1434, Dublin, Ireland, August 23-29 2014.
Million-scale Derivation of Semantic Relations
from a Manually Constructed Predicate Taxonomy
Motoki Sano
?
Kentaro Torisawa
?
Julien Kloetzer
?
Chikara Hashimoto
?
Istv
?
an Varga
?
Jong-Hoon Oh
?
? ? ? ? ?
National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
?
NEC Knowledge Discovery Research Laboratories, Kanagawa, 211-8666, Japan
{
?
msano,
?
torisawa,
?
julien,
?
ch,
?
rovellia}@nict.go.jp,
?
vistvan@az.jp.nec.com
Abstract
We manually created a semantic taxonomy called Phased Predicate Template Taxonomy (PPTT)
that covers 12,023 predicate templates (i.e., predicates with one argument slot like ?rescue X?)
and derived from it various semantic relations between these templates on a million-instance
scale (70%-80% precision level). The derived relations include entailment (e.g., rescue X?X is
alive), happens-before (e.g., buy X?drink X), and a novel relation type anomalous obstruction
(e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTT?s design
and the use of statistical methods.
1 Introduction
Databases of various semantic relations between natural language expressions are indispensable knowl-
edge for many NLP applications. For instance, entailment relations are crucial in information extraction
and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014).
Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are im-
portant for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire
those relations with a broad coverage. Although many sophisticated machine learning techniques have
been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky,
2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al.,
2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness
in the input data. In this work we take a completely different approach: we manually construct a seman-
tic lexicon called Phased Predicate Template Taxonomy (PPTT), and derive various types of semantic
relations on a large-scale by using it. Our target language is Japanese, but examples are given in English
for simplicity throughout this paper.
PPTT is a taxonomy of predicate templates (predicates with one argument slot like rescue X, ?Tem-
plate? hereafter) that classifies templates according to phases of story concerning an entity denoted by
X. In the story, or the ?life? of the entity X, X can be anticipated, created, then execute its function and
finally it may collapse and become deficient. Anticipation, creation, execution, collapse, deficiency of X
can be seen as such phases of story concerning X, and PPTT classifies templates into 41 semantic classes
each of which corresponds to a phase. In other words, PPTT provides a way to describe the stories of var-
ious entities that constitute this world, and we believe that PPTT (partly) reflects how we understand the
world and its entities. Accordingly, PPTT can also provide a way to derive various semantic knowledge
about this world such as the happens-before relation between events involving an entity, e.g., since the
creation phase usually occurs before the execution phase, invent X (creation phase) is likely to happen-
before use X (execution phase). In addition, entailment relations can be derived: since the creation phase
of an object X must have occurred if X is in its execution phase, it implies that use X is likely to entail
invent X.
In addition, there are ups and downs in stories; some entities suffer setbacks in their stories. PPTT de-
scribes such ?ups and downs? by means of a recently proposed semantic polarity, excitation (Hashimoto
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1423
et al., 2012). Excitation classifies templates into excitatory, inhibitory, and neutral; an excitatory tem-
plate like install X and buy X indicates that the main function, effect, purpose or role of the entity referred
to by the X of the template is activated, enhanced, or prepared,
1
while an inhibitory template like unin-
stall X and X is canceled roughly indicates that it is deactivated or suppressed. Neutral templates are
neither excitatory nor inhibitory (e.g., consider X). Roughly speaking, an excitatory template expresses
the events that contribute to turn on the function of X, while an inhibitory template expresses the events
that contribute to turn off or not to turn on the function of X. Then, in PPTT, excitatory and inhibitory
respectively correspond to ?ups? and ?downs? in the story of X. The phases in PPTT are marked accord-
ing to these ups and downs. Accordingly, PPTT can derive many antonymous contradiction pairs like
install X?uninstall X, as Hashimoto et al. did, though we omit the detail for space limitation. Moreover,
PPTT can derive a huge volume of anomalous obstruction, a contradiction-like novel semantic relation
that we propose in this paper, like X is canceled;(cannot) buy X and X is sold out;(cannot) buy X,
which indicate that if X is canceled or sold out, you cannot buy X. Anomalous obstruction should be
used for Why-type QA (Oh et al., 2013), as well as a novel system that warns a user who wants to buy
a commercial product that the product is started to be sold out or canceled in various e-commerce sites
without any application-specific coding.
As suggested, a story has a temporal order between its phases, which we call the canonical temporal
order. In addition, some phases in a story would enable or necessitate another phase in the same story to
occur. In PPTT, these relations are embodied in various temporal-semantic links between phases. Note
that each link between two phases does not guarantee that every possible pair of templates taken from
the two phases has such semantic relations; it just indicates that there exists such tendencies. Despite the
absence of the guarantee, PPTT?s links enable a million-scale derivation of semantic relations with the
help of distributional similarity. In existing resources such as WordNet (Fellbaum, 1998), the links are
assumed to be 100% correct, but it would be hard to have such absolutely correct links in a million-scale.
Hence, we believe that our approximate links are more useful for a large-scale relation derivation.
Note that the goal of our PPTT project is to derive a wide range of semantic relations on a large scale,
rather than to complete a comprehensive template taxonomy. As such, PPTT lacks some templates as
described in later sections. Nevertheless, we believe that our design brings much more good than harm,
since we could generate various semantic relations on a million scale thanks to PPTT. Our experimental
results show that we can derive 4.4 million happens-before relation instances with 79.5% precision,
0.5 million entailment relation instances with 70.0% precision, and one million anomalous obstruction
relation instances with 73.5% precision. Constructing the PPTT taxonomy requires a manual labor cost,
which amounted to three man-months in our case; however, we believe that this cost is lower than the
cost for developing highly-precise automatic acquisition methods for all of happens-before, entailment,
contradiction, and anomalous obstruction relations.
We plan to release PPTT and the derived relation instances after the manual annotation of the derived
instances to the NLP community.
2 Related Works
PPTT might resemble other semantic lexicons created in the long history of NLP (Levin, 1993; Kipper
et al., 2006; Fellbaum, 1998; Bond et al., 2009; Fillmore, 1976; Baker et al., 1998; Halliday, 1985;
Pustejovsky et al., 2003; Puscasu and Mititelu, 2008; Bejar et al., 1991; Jurgens et al., 2012). PPTT
is different in that it primarily aims at deriving various types of semantic relations on a large scale ex-
ploiting the notion of the phase of story, rather than being a comprehensive taxonomy like those existing
semantic lexicons. As a result, PPTT can derive more varieties of semantic relations between templates
than any one of those existing lexicons. From WordNet (Fellbaum, 1998; Bond et al., 2009), we can de-
rive entailment and contradiction relations using synsets and synset-links that represent relations such as
?troponym?, ?antonym? and ?entailment?. However, happens-before and anomalous obstruction relations
1
The above definition is slightly different from the original one in Hashimoto et al. (2012). We inserted the verb ?prepared?
into the original definition. This clarifies that various preparation processes for X, such as buy X, can be regarded as excitatory
templates. We also assume that such templates as X exists and have X, which mean little more than just existence, are regarded
as excitatory templates in PPTT based on the assumption that existence can be regarded as preparation for the function of X.
1424
cannot be derived from it, since there is no information on temporal ordering except that on causality.
From VerbNet (Levin, 1993; Kipper et al., 2006), the hyponymy/synonymy type of entailment relations
may be derived using templates in the same verb classes constructed based on shared syntactic behavior,
possibly with the help of statistical methods. However, the other types of relations that can be derived
from PPTT cannot be derived from VerbNet, since there is no link representing relationships between the
verb classes. FrameNet (Fillmore, 1976; Baker et al., 1998) was used to derive hyponymy/synonymy
types of entailment (Coyne and Rambow, 2009; Aharon et al., 2010) using information such as a Frame-
to-frame relation ?Inheritance? (is-a relation). In addition, happens-before relations can be derived using
?Precedes? (Later-Earlier relations). However, since it does not contain semantic constraints like en-
ablement and necessity that PPTT contains, it is not trivial to derive presupposition type of entailment
or anomalous obstruction instances from it. TimeML (Pustejovsky et al., 2003; Puscasu and Mititelu,
2008) contains various temporal information and can be used to derive context-dependent happens-before
relations such as the relation between ?leaves? and ?will not hear? in the sentence ?If Graham leaves to-
day, he will not hear Sabine? through TLINK (Pustejovsky et al., 2003) annotated manually; thus, it is
difficult to derive context-independent relations from it, while they can be derived from PPTT. Besides,
since it covers only temporal information, it is difficult to derive other types of relations from it. From
Bejar et al.?s semantic relation taxonomy of lexical pairs (Bejar et al., 1991; Jurgens et al., 2012),
using semantic relation categories such as ?act: act attribute? (e.g., creep:slow), lexical entailment rela-
tions were extracted (Turney and Mohammad, 2014). However, it is not trivial to derive happens-before
or anomalous obstruction relations from it since it does not contain information on temporal sequences
between verbs.
Furthermore, our work differs from automatic methods for extracting temporal or causal relations
(Szpektor et al., 2007; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Talukdar et al.,
2012; Hashimoto et al., 2012; Hashimoto et al., 2014) in that our method does not require that target
pairs co-occur in a document, unlike the previous methods. Hence, our method is likely to be immune
to data sparseness. We could actually derive a wide range of relation instances that were rarely written
in documents because they were too commonsensical (e.g., X is constructed happens-before sew (some-
thing) at X). Needless to say, such commonsensical knowledge is often needed to develop intelligent
systems.
3 PPTT Design
In PPTT, templates are organized hierarchically into three levels. In each level, there are classes that
correspond to phases of stories, which we call Level-0 (L0), Level-1 (L1), and Level-2 (L2) classes.
Each template belongs to only one class at each level. In the following, we describe each level.
3.1 L0-Classes and L0-Links
First we divided the entire story concerning an entity X into five phases: non-existence, existence, func-
tioning, non-existence to existence transition and existence to non-existence transition. Then we created
the five L0-classes listed below, each of which corresponds to one of these five phases.
Non-existence Class The class of templates that do not entail the existence of X, e.g., plan
X.
2
Existence Class The class of templates that entail X?s existence but does not imply the execu-
tion of its main function or the achievement of its objectives, e.g., buy X, X exists.
Functioning Class The class of templates that imply the execution of X?s main function or
the achievement of its objectives, e.g., use X, eat X.
Non-existence to Existence Transition Class (NET Class) The class of templates that ex-
press the transition from a situation in which X does not exist to a situation in which it
exists, e.g., manufacture X.
2
One might think the definition of the Non-existence Class should be ?the templates that DO entail X?s NON-EXISTENCE?.
We did not use such a definition because it would overlook many templates that are consistent with X?s NON-EXISTENCE but
DO NOT entail X?s NON-EXISTENCE, like order X.
1425
Existence to Non-existence Transition Class (ENT Class) The class of templates that ex-
press the transition from a situation in which X exists to a situation in which it does not
exist, e.g., dismantle X.
!"#$%&'()%#*%+,-.((+!"#"$%&'()%*%
/&'()%#*%+,-.((+!"#"$%+,-%*% 01#*2"#'#3+,-.((+!"#"$%,.!%*%
/&'()%#*%+)"+!"#$%&'()%#*%++45.#('2"#+6/!47+,-.((++!"#"$%/0.1()2'!%*%
!"#$%&'()%#*%+)"+/&'()%#*%++45.#('2"#+6!/47+,-.((+!"#"$%1(),3(42,5!%*%
Figure 1: L0-links among L0-classes.
As mentioned in the introduction, we assume
a canonical temporal order among L0-classes.
For instance, templates in the NET class (e.g.,
manufacture X) should refer to events that usu-
ally happen before those events referred to by
templates in the Existence class (e.g., buy X),
Functioning class (e.g., use X) and ENT class
(e.g., dismantle X). We enumerated such tem-
poral restrictions, each of which is represented
by a link in Figure 1, which we call L0-links
and used them for deriving relations. Note that
we did not set any L0-link between the Exis-
tence class and the Functioning class because
the events described by them may happen in various orders or have temporal overlap. For example, X
exists should have temporal overlap with use X.
Of course, such metaphysical notions as the canonical temporal order and the phases must have many
complications and exceptions. First, many templates that have the neutral excitation polarity (Hashimoto
et al., 2012) did not seem to follow the canonical temporal order among L0-classes. For instance, since
the neutral template think about X does not entail the existence of X, it belongs to the Non-existence
class but one can consider X while X exists or while it is functioning or even after it is collapsed and
violate canonical temporal ordering. For this reason, we excluded neutral templates from PPTT and will
deal with them in a different framework as a future work. In addition, although we did not assume a
temporal order between the Existence class and the Functioning class, some templates in these classes
have a happens-before relation as special cases (e.g., buy X in the Existence class happens before eat
X in the Functioning class). The proposed L0-links also cause problems. For instance, order X (Non-
Existence class) may not always happen before create X (NET class) even though the L0-links indicate
a happens-before relation between their classes. We dealt as far as possible with such cases in level 2
with L2-classes, which are finer than L0-classes. Nonetheless, we stress that the overall plausibility of
the canonical temporal order among L0-classes was experimentally confirmed through the derivation of
happens-before relations only using L0-links. Note that the design of the L0-classes was inspired by the
Generative Lexicon (Pustejovsky, 1998) and Aristotle?s Entelecheia (Aristotle, 1987).
3.2 L1-Classes
Excitation
L0-class Excitatory Inhibitory
POTENTIAL class FORECLOSING class
Non-existence class e.g., plan X e.g., prevent X
ENABLING class INCOMMODE class
Existence class e.g., buy X e.g., weaken X
ACTUALIZING class DISORDERING class
Functioning class e.g., X functions e.g., X loses
GENERATING class
NET class e.g., X is born N/A
CORRUPTING class
ENT class N/A e.g., destroy X
Table 1: L1-classes.
Next, we divided some L0-classes into
L1-classes using the excitation polar-
ity (Hashimoto et al., 2012) to intro-
duce ?ups and downs? to PPTT, which
enables to capture semantic inconsis-
tencies between templates (e.g., in-
stall X?uninstall X) and negative in-
teraction between the events referred
to by the templates in PPTT (e.g., X
is canceled;(cannot) hold X). Excita-
tion was originally proposed for recog-
nizing contradictions and causal rela-
tions between templates and then was
successfully applied to other deep se-
mantic processing (Oh et al., 2013; Varga et al., 2013; Kloetzer et al., 2013; Hashimoto et al., 2014).
1426
As shown in Table 1, we divided each of three L0-classes (Non-existence class, Existence class and
Functioning class) into two L1-classes, each of which corresponds to excitatory and inhibitory. Since
the transition to an existence situation can be interpreted as an enhancement of an entity?s function, we
assumed that all the templates in the NET classes are excitatory because they express a transition of
entity X from a non-existence situation to an existence situation. Similarly, we assume all the templates
from the ENT class are inhibitory. Also, L1-classes do not have specific links between them beside the
L0-links from their parent classes.
3.3 L2-Classes and L2-Links
Finally, we divided L1-classes into 41 L2-classes. Specifically, we first roughly grouped together seman-
tically similar templates from the same L1-class and identified the common semantic properties among
them. Note that in the rough grouping, we classified templates so that the resulting groups fit into fine-
grained phases in the story concerning X.
After this initial grouping, we classified all the templates into the L2-classes that are listed in Table
3 alongside the classification criteria and the number of templates in each class. As the classification
criteria, we used the identified common semantic properties among members of each class. Note that
some L2-classes can be regarded as a subset of another L2-class. For instance, the PROHIBIT L2-class
can be seen as a subset of the PREVENTION L2-class. When a template meets the classification criteria
of both a subset class and its superset class, we classified it into the subset class.
We also made links called L2-links between the L2-classes. The motivation behind this is to capture
finer temporal-semantic constraints that could not be specified at Level-0 and Level-1 as well as to
capture the temporal-semantic constraints inside a single L0 or L1-class. For example, the temporal
order between buy X and eat X is encoded in a L2-link between the ACQUISITION and EXECUTION L2-
classes, while there is no L0-link between the Existence L0-class (class of buy X) and the Functioning
L0-class (class of eat X). This exemplifies that the L2- and L0-links complement each other.
Each L2-link has one of the six types of temporal-semantic links that are summarized in Table 2 with
the number of links of each type. The link types were designed to capture how the events referred to by
the templates in a class affect the occurrence or non-occurrence of the events referred to by the templates
in a class in the past, present, or future. C
1
and C
2
being two L2-classes, C
1
?s effect on the occurrence or
non-occurrence of C
2
is represented by Positive (
+
) and Negative (
?
) links, respectively, while C
1
?s effect
on the past, present, or future phase of X expressed by C
2
is represented by Past, Present, and Future
links, respectively. For instance, the Past
+
link from the ABANDONMENT class to the ACQUISITION
class indicates that a template from the ACQUISITION class (e.g., obtain X) must occur before a template
from the ABANDONMENT class (e.g., get rid of X), and the Future
?
link from the PROHIBIT class to
the EXECUTION class indicates that templates from the PROHIBIT class (e.g., ban X) disable templates
from the EXECUTION class (e.g., utilize X). Notice that L2-links represent such semantic constraints as
enablement and necessity in addition to temporal order, and they are useful for deriving various kinds of
semantic relations including entailment and anomalous obstruction, as shown in a later section. The first
author of this paper hand-labeled the links between every combination of L2-class pairs by considering
the name of the classes and a few example templates in each.
Positive Negative
Past If C
1
occurred, C
2
must have occurred.
e.g.,FORGETTING
Past
+
? RECOGNITION; X is forgotten
Past
+
? X is
recognized (55 links)
If C
1
occurred, C
2
COULD NOT have occurred.
e.g.,CREATION
Past
?
? PREVENTION; X is generated
Past
?
? X
is prevented (438 links)
Present While C
1
is taking place, C
2
must be taking place.
e.g.,INITIATION
Present
+
? BEING; X is started
Present
+
? X
exists (73 links)
While C
1
is taking place, C
2
CANNOT take place.
e.g.,ENHANCEMENT
Present
?
? DEGRADATION; X is enhanced
Present
?
? X is deteriorated (496 links)
Future C
1
enables C
2
to occur. e.g.,PREPARATION
Future
+
? EXECUTION;
X is customized
Future
+
? X is executed (90 links)
C
1
DISABLEs C
2
to occur. e.g.,
DEFICIENCY
Future
?
? PROVISION; X does not exist
Future
?
?
X is provided (210 links)
Table 2: Types and numbers of L2-links in PPTT. Link direction is C
1
? C
2
.
1427
Non-existence L0-class: Potential L1-class (578) / Foreclosing L1-class (178)
DESIRE entails that X is desired but unlike PLANNING or DEMAND, it does not entail that X is planned or requested, e.g.,
desire X, want X (48).
PLANNING entails that X is planned but does not entail that X is requested. Unlike DEMAND, it does not assume that a person
other than the Planner will carry out X, e.g., plan X, conspire X (72).
DEMAND entails that X is requested. Unlike PLANNING, it assumes that a person other than the Demander will carry out X,
e.g., order X (252).
APPROVAL entails that X is approved or permitted and that there was a plan or a demand before approving, e.g., permit X, accept
X (80).
FEAR entails that X is expected and that X is a source of anxiety or fear, e.g., fear X, worry about X (13).
ANTICIPATION entails that X is expected but unlike FEAR, does not entail that X is a source of anxiety or fear, e.g., forecast X, predict
X (24).
SEARCH entails that X is searched for but unlike DESIRE or DEMAND, does not entail that X is desired or requested, e.g.,
search for X (89).
PREVENTION entails that X is prevented. Unlike CANCELATION, it does not entail that there was a plan or a demand before
preventing, e.g., preclude X (54).
CANCELATION entails that X is canceled and that there was a plan or demand before canceling, e.g., cancel X, give up X (34).
PROHIBIT entails that X is prohibited. X?s right or ability to be generated or used is taken away. e.g., ban X, forbid X (39).
POSTPONE entails that X is postponed, e.g., postpone X, defer X (15).
DEFICIENCY entails that X does not exist but does not entail that it is prevented, canceled, prohibited, or postponed, as in other
L2-classes of Foreclosing L1-class. e.g., lack X, X is absent (36).
NET L0-class: Generating L1-class (596)
SYMBOLIZATION entails that X transits from non-existence to existence as a kind of (semiotic) representation, e.g., write X, compose
(music) X (13).
CREATION entails that X transits from non-existence to existence. Unlike SYMBOLIZATION, X is not limited to a semiotic
representation, and unlike TRANSFORMATION, it focuses less on transformation from another entity. generate X,
cause X (509).
TRANSFORMATION entails that X transits from non-existence to existence as a result of transformation. Unlike CREATION, it focuses on
the transformation from another entity, e.g., turn into X (74).
ENT L0-class: Corrupting L1-class (622)
COLLAPSE entails that X transits from existence to non-existence by dying, being eliminated, or being destroyed. Unlike CON-
VERSION , it focuses less on transformation, e.g., destroy X, kill X (588).
CONVERSION entails that X transits from existence to non-existence by transforming X into an another entity, e.g., turned from X,
changed from X (34).
Existence L0-class: Enabling L1-class (3,536) / Incommode L1-class (1,355)
RECOGNITION entails that X is recognized or sensed, e.g., find X, feel X (308).
SELECTION entails that X is selected, e.g., appoint X, choose X (139).
ENCOUNTER entails that X emerges as a result of transportation, e.g., send X, X arrives (407).
ACQUISITION entails that X is obtained and possessed, e.g., buy X, catch X (482).
PROVISION entails that X is handed to be possessed, e.g., sell X, render X (422).
ENHANCEMENT entails that X is extended, improved, or supported, e.g., increase X, help X (880).
PREPARATION entails that X is arranged, connected, or qualified in preparation to execute its function, e.g., cook X, install X (822).
BEING entails that X is existing or living but does not entail that X is recognized, selected, encountered, acquired, enhanced,
or prepared, as in other L2-classes of the Enabling L1-class, e.g., X exists, X lives (76).
UNRECOGNIZING entails that X is not recognized or sensed but unlike FORGETTING, does not entail that X was previously recognized,
e.g., overlook X (8).
FORGETTING entails that X is forgotten and that X was once recognized, e.g., forget X, lose memory of X (8).
UNSELECTING entails that X is not selected, e.g., alternate X, reject X (46).
SEPARATION entails that X is left or separated as a result of transportation, e.g., X leaves, send X away (114).
ABANDONMENT entails that X is not possessed as a result of being thrown away, e.g., throw X away, renounce X (58).
DEPRIVATION entails that X was taken away without the permission of a possessor, e.g., steal X, take X away (102).
DEGRADATION entails that X is reduced, deteriorated, or interrupted, e.g., X is weakened, attack X (908).
UNPREPARED entails that X is unprepared, disconnected, or unqualified, e.g., X is uninstalled, X is disconnected (111).
Functioning L0-class: Actualizing L1-class (4,460) / Disordering L1-class (698)
EXECUTION entails that the function of X is executed but unlike WORKING, does not entail that X successfully satisfies its function,
e.g., ignite X (966).
WORKING entails that the function of X is carried out and that X successfully satisfies its function, e.g., X functions, cleaned by
X (3,106).
INITIATION entails that X is started or continued, e.g., start X, open X (185).
SUCCESS entails that X accomplished its goal and the result of the execution of its function is evaluated positively, e.g., accom-
plish X, X wins (203).
SUSPENSION entails that the function of X is suspended but unlike FINISHING, does not entail that its function is terminated, e.g.,
suspend X (133).
DYSFUNCTION entails that the function of X is executed but X is performing poorly, e.g., X is sluggish, bored by X (196).
FINISHING entails that X is terminated, e.g., end X, finish X. (110).
FAILURE entails that X fails to accomplish its goal and the result of the execution of its function is evaluated negatively, e.g., X
is defeated (259).
Table 3: PPTT classes. The number in parentheses indicates the number of templates in PPTT.
1428
Note that the existence of an L2-link does not guarantee that the semantic properties specified by it
hold for all the possible template pairs taken from the class pair it connects. The cost of hand-labelling
the links with such guarantees is prohibitively high because we would have to check all of the template
combinations. We empirically evaluated the validity of the links in our experiments below although this
is not a direct evaluation since the relations we derived are different from the ones given to the links.
4 Construction of PPTT and Relation Derivation
Using the automatic acquisition method proposed by Hashimoto et al. (2012), we collected 10,825 can-
didates of excitatory/inhibitory templates from a 600-million-page web corpus (hereafter, WCorpus).
Hashimoto et al.?s method constructs a network of templates based on their co-occurrence in sentences
with a small number of seed templates of which excitation polarity are assigned manually, and infers the
polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et
al., 2005). Then, we added the 20,000 most frequent templates in the corpus that could not be extracted
automatically for a total of 30,825 templates.
Three human annotators (not the authors) judged the polarity of the templates, and we included the
excitatory and the inhibitory templates but excluded the neutral templates in PPTT due to the reason
discussed in Section 3.1. We also excluded templates whose variable X is the subject of a transitive verb.
This is because the subject position is often occupied by living things, and since the functions/objectives
of such subjects seem difficult to identify, it is often difficult to judge whether such templates should be
classified into the Functioning class or another. After applying these two restrictions, the first author
classified the remaining 12,023 templates in PPTT.
In this work, we derived happens-before, entailment and anomalous obstruction relations among tem-
plates from PPTT. The target data is the set of all the template pairs such that a noun exists with which
both templates of the pair co-occur at least 100 times in WCorpus. We denote this set of the template
pairs by TP100, and all the relation derivations pick up template pairs as relation instances from it. This
is because in our preliminary experiments, we found that the relation instance candidates taken from
outside of TP100 had much lower precision. The relation derivation itself is quite simple and consists
of the following two steps.
Step 1 Select L0-links or types of L2-links that are expected to represent a target semantic
relation (e.g., Present
+
links are expected to represent entailment, since they represent
the relations between classes where ?While C
1
is taking place, C
2
must be taking place?.)
and extract all the class pairs connected by the selected links (e.g., INITIATION L2-class
Present
+
? BEING L2-class). Enumerate all the template pairs from the intersection between
TP100 and the extracted class pairs (e.g., X is started
Present
+
? X exists).
Step 2 If necessary, rank the relation instance candidates that are extracted in Step1 by distri-
butional similarity scores between the templates that compose the candidates, computed
with WCorpus.
5 Experiments
This section reports our experiments on semantic relation derivation. Derived relation instances were
marked by three human annotators (not the authors) who voted to break ties. Unless stated otherwise,
we asked them to mark a template pair as negative if they found any noun that can be placed in both
templates? argument slots and makes the template pair a negative sample for the target relation, and
positive otherwise.
5.1 Happens-Before Relation
Following Regneri et al. (2010), we assumed template
1
(T
1
) has a happens-before relationwith template
2
(T
2
) iff one event expressed by T
1
normally happens before another expressed by T
2
, provided that both
events occur. Below are our four methods to derive happens-before relation instances, each of which
uses different links. Note that we did not use distributional similarity in this experiment.
1429
H1 uses the 55 pairs of L2-classes connected by L2-link Past
+
, meaning that a template in a
class must occur before another.
H2 uses the 90 pairs of L2-classes connected by L2-link Future
+
, i.e., a template in a class
often enables another to occur.
H3 uses the 474 pairs of L2-classes connected by one of the seven L0-links in Figure 1, i.e.,
the canonical temporal order links.
All is the union of H1-H3 results.
We prepared two baselines; HB-Ptn is a pattern-based method based on Chklovski and Pantel (2004a).
It extracts template pairs in TP100 that were connected in WCorpus by one of manually collected 73
conjunctives expressing temporal order, such as after and before, and which either shared the same
argument or the second template was filled by the pronouns it, this, or that. Random is a random
sampling from TP100.
Three annotators annotated 200 random samples from each method?s output. Fleiss? kappa was .56
(moderate agreement). The results of their majority vote are summarized in Table 4. The recall was
estimated against the number of positive samples in TP100 based on the precision of Random. The
precision of all of our four methods is reasonably high for such a difficult task, and the number of
relations derived by All reached about 4.4 million. The recall of All exceeds 65%, which we believe is
quite high. HB-Ptn suffered from low recall, probably due to the data sparseness in WCorpus. Table 5
shows examples of the derived happens-before relations alongside L2-classes of the templates, the L2-
links between the classes and the original Japanese templates. The acquired relations included many
unexpected but correct happens-before relations, like compose (a piece of music) X?relax by X.
Actually, it is difficult to fairly compare our work and previous works on temporal relation acqui-
sition, due to differences in language, the data used, and the methodologies. Nonetheless, our result
with 79.5% precision is at least five times larger than the English data released by Chambers et al.
(cs.stanford.edu/people/nc/schemas), which contains around 870,000 ?before? relation candidates and
happens-before database in the VerbOcean (Chklovski and Pantel, 2004a) that covers 4,205 relations.
Considering our method is completely different from theirs, we believe that our contribution is valuable.
Setting/Method Precision (%) # of Pairs Recall (%)
H1 83.5 1,113,280 18.0
H2 70.5 1,524,557 20.8
H3 67.0 3,837,116 49.7
All 79.5 4,387,781 67.5
HB-Ptn 53.0 32,288 0.3
Random 18.0 28,717,454 100.0
Table 4: Happens-before derivation performance.
boil X?eat X
PREPARATION Class
Future
+
? EXECUTION Class
X wo niru? X wo taberu
compose (a piece of music) X?relax by X
SYMBOLIZATION Class
Past
+
? WORKING Class
X wo sakkyoku-suru? X de rirakkusu-suru
Table 5: Examples of happens-before relation.
5.2 Entailment Relation
Below are our proposed methods to derive entailment relations.
Present+.DIFF extracts the 32 class pairs that are composed of DIFFERENT L2-classes and
are connected by the Present
+
links, meaning that a template in a class must occur simul-
taneously with another template in another class, and ranks all the possible template pairs
taken from each class pair using Hashimoto et al.?s (2009) conditional probability based
similarity measure for entailment recognition.
Present+.SAME extracts the 41 class pairs that are composed of the SAME L2-classes and
are connected with the Present
+
links, and ranks all the template pairs from each class
pair using Hashimoto et al.?s similarity.
Past+ extracts the 55 pairs of L2-classes that are connected with the Past
+
links, meaning that
a template in a class must occur before another, and ranks all the template pairs from each
class pair using Hashimoto et al.?s similarity.
1430
Baseline-HAS is our baseline which is our implementation of Hashimoto et al. (2009) for entailment
recognition; it ranks all the template pairs in TP100 by Hashimoto et al.?s score. Our methods can be
seen as the restrictions of the output of the baseline method using the extracted PPTT?s class pairs.
0e+00 2e+04 4e+04 6e+04 8e+04 1e+05
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Number of template pairs (sorted by score)
Pr
ec
isi
on
Present+.DIFFPresent+.SAMEPast+Baseline-HAS
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Pr
ec
isi
on
Figure 2: Entailment derivation performance.
Three annotators hand-labeled 500
random samples from the top 100,000
template pairs for each method. The
kappa was .59 (moderate agreement),
and the results of their majority vote
are presented in Figure 2. Table 6
shows examples of Proposed methods?
outputs. The restriction of the class
pairs in our method contributed to much
higher precision than using the state-of-
the-art method alone.
Since the precision of Past+ is quite
high for the top 100,000 pairs, we an-
notated an additional 500 random sam-
ples from the top 500,000 pairs. Accord-
ing to this annotation, the top 408,610
pairs had 70% precision, implying that
after merging all the top pairs extracted
by Present+.DIFF, Present+.SAME and Past+ whose precisions exceeded 70%, we had 0.49 million
entailment pairs with 70% precision. With Baseline-HAS, we derived only 24,000 with the same preci-
sion. Also, the JapaneseWordNet (v.1.1) covers only 2.4% of the pairs in the manually annotated positive
samples from our proposed methods through the ?synsets? or any ?synlinks?. We analyzed 200 samples
from the positive samples not covered by WordNet and found that 49.5% are the hyponymy type (e.g.,
boil X?heat X), 39.0% are the backward presupposition type (e.g., complete X?start X), and 11.5% are
the synonymy type (e.g., X passes away?X dies). This seems to imply that our methods are better at
deriving all types of entailment, while WordNet might be effective for only the synonymy type. In addi-
tion, by analyzing all the positive samples, we confirmed that the different types of entailment pairs were
derived with different L2-links; 88.1% of the positive samples from Present+.DIFF and Present+.SAME
require that two events referred to by the two templates occur with temporal overlap (e.g., equip X?X
exists, i.e. X is equipped while X exists), while 96.7% of those from Past+ were the backward presuppo-
sition type, in which an event entails another event that happened before it. This shows that the L2-links
were useful for deriving various fine-grained types of entailment.
get X?X exists (X wo nyuushu-suru ? X ga sonzai-suru ) ACQUISITION Class
Present
+
? BEING Class
evolve into X?change into X (X ni shinka-suru ? X ni kawaru ) TRANSFORMATION Class
Present
+
? TRANSFORMATION Class
close (a shop) X?make X (X wo heiten-suru ? X wo tsukuru ) FINISHING Class
Past
+
? CREATION Class
Table 6: Examples of entailment.
5.3 Anomalous Obstruction Relation
We assumed that template
1
(T
1
) like X is sold out has an anomalous obstruction relation with template
2
(T
2
) like buy X (denoted as X is sold out;(cannot) buy X) iff: (A) the event expressed by T
1
prevents
the event expressed by T
2
from occurring; (B) T
1
expresses an event that should not happen if everything
about the variable X goes as expected; and (C) T
2
expresses another event in which the function of X is
executed, enhanced, or prepared. We derived anomalous obstructions, by generating all of the possible
template pairs from the 88 L2-class pairs connected by Future
?
L2-links. These indicate that the events
expressed by the templates in the first class of a pair disable the events expressed by the templates in the
second class. Also, to confirm that the templates of the first class in a pair express an unexpected event,
1431
we required the disabler class to have the inhibitory polarity and the disabled class to be excitatory.
Otherwise, we would obtain such pairs as INITIATION;PLANNING (e.g., start X;schedule X), which
indeed express the prevention relation (Barker and Szpakowicz, 1995), i.e., ?scheduling X would not
occur after starting X,? which is different from anomalous obstruction.
Three annotators annotated 200 random samples for each method, and the results of their majority
vote are summarized in Table 7, where Random refers to a random baseline using TP100. The recall
was estimated using the number of positive samples provided by Random. The kappa was .60 (moderate
agreement). 73.5% precision, 26.4% recall against the positive samples in TP100, and more than one
million outputs of our proposed method are reasonably high/large results for this difficult task. Table 8
shows examples of Proposed?s outputs. ?(cannot)? was attached to disabled templates for readability.
Setting/Method Precision # of Pairs Recall
Proposed 73.5 1,081,405 26.4
Random 10.5 28,717,454 100.0
Table 7: Performance of anomalous obstruc-
tion derivation.
prohibit X;(cannot) exhibit X PROHIBIT Class
Future
?
? EXECUTION Class
X wo kinshi-suru;X wo kookai-suru
break X;(cannot) utilize X COLLAPSE CLASS
Future
?
? EXECUTION CLASS
X wo kowasu;X wo riyo-suru
Table 8: Examples of anomalous obstruction.
6 Conclusion
In this work, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a net-
work of semantically coherent classes of templates and derived semantic relations including entailment
from it in a million-instance scale. Future work will extend PPTT to cover non-excitatory/non-inhibitory
templates and generate richer structural knowledge similar to full-fledged scripts (Schank and Abelson,
1977) and narrative schemas (Chambers and Jurafsky, 2011).
Acknowledgements
We would like to thank three anonymous reviewers for many useful comments and advices on the
manuscript of this paper.
References
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010. Generating entailment rules from framenet. In Pro-
ceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages 241?246, Stroudsburg, PA, USA.
ACL.
Aristotle. 1987. De Anima (Translated by Hugh Lawson-Tancred). Penguin Classics, London.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of
the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference
on Computational Linguistics - Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA, USA. ACL.
Ken Barker and Stan Szpakowicz. 1995. Interactive semantic analysis of clause level relationships. In Proceedings
of PACLING ?95, Brisbane.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cognitive and Psychometric Analysis of Analogical Problem
Solving. Springer-Verlag, New York.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure
optimization. Comput. Linguist., 38(1):73?111, March.
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka Uchimoto, Takayuki Kuribayashi, and Kyoko Kanzaki.
2009. Enhancing the japanese wordnet. In Proceedings of the 7th Workshop on Asian Language Resources,
ALR7, pages 1?8, Stroudsburg, PA, USA. ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of
ACL-08: HLT, pages 789?797, Columbus, Ohio, June. ACL.
1432
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ?09, pages 602?610,
Stroudsburg, PA, USA. ACL.
Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 976?986, Portland, Oregon, USA, June. ACL.
Timothy Chklovski and Patrick Pantel. 2004a. Path analysis for refining verb relations. In In Proceedings of KDD
Workshop on Link Analysis and Group Detection (LinkKDD-04), Seattle, WA.
Timothy Chklovski and Patrick Pantel. 2004b. Verbocean: Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?04, pages 33?40, Barcelona, Spain, July. ACL.
Robert Coyne and Owen Rambow. 2009. Lexpar: A freely available english paraphrase lexicon automatically
extracted from framenet. In Proceedings of the Third IEEE International Conference on Seman- tic Computing.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational,
evaluation and approaches. Natural Language Engineering, 15(4):i?xvii.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.
Charles J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of
Sciences: Conference on the Origin and Development of Language and Speech, 280(1):20?32.
Michael A.K. Halliday. 1985. An Introduction to Functional Grammar. Arnold, 1st edition.
Chikara Hashimoto, Kentaro Torisawa, KowKuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama. 2009.
Large-scale verb entailment acquisition from the Web. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1172?1181, Singapore, August. ACL.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excitatory
or inhibitory: a new semantic orientation extracts contradiction and causality from the web. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 619?630, Stroudsburg, PA, USA. ACL.
Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano, Istv?an Varga, Jong-Hoon Oh, and Yutaka
Kidawara? 2014. Toward future scenario generation: Extracting event causality exploiting semantic relation,
context, and association features. In Proceedings of the 52nd Annual Meeting of the Association for Computa-
tional Linguistics, Baltimore, USA, June. Association for Computational Linguistics.
David A. Jurgens, Peter D. Turney, Saif M. Mohammad, and Keith J. Holyoak. 2012. Semeval-2012 task 2: Mea-
suring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computa-
tional Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Semantic Evaluation, SemEval ?12, pages 356?364, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending verbnet with novel verb
classes. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC
2006), pages 731?738, Genoa, Italy, June.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, Motoki Sano, and Kiy-
onori Ohtake. 2013. Two-stage method for large-scale acquisition of contradiction pattern pairs using entail-
ment. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages
693?703, Seattle, Washington, USA, October. ACL.
Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago and London.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013.
Why-question answering using intra- and inter-sentential causal relations. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1733?1743, Sofia,
Bulgaria, August. ACL.
1433
Georgiana Puscasu and Verginica Barbu Mititelu. 2008. Annotation of wordnet verbs with timeml event classes.
In Bente Maegaard Joseph Mariani Jan Odijk Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of the Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
James Pustejovsky, Jos Castao, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer, and Graham Katz.
2003. Timeml: Robust specification of event and temporal expressions in text. In in Fifth International Work-
shop on Computational Semantics (IWCS-5.
James Pustejovsky. 1998. The Generative Lexicon. MIT Press, Cambridge.
Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experi-
ments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
979?988, Uppsala, Sweden, July. ACL.
Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding: an Inquiry into Human
Knowledge Structures. L. Erlbaum, Hillsdale, NJ.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,
Prague, Czech Republic, June. ACL.
Hiroya Takamura, Takashi Inui, andManabu Okumura. 2005. Extracting semantic orientations of words using spin
model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),
pages 133?140, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell. 2012. Acquiring temporal constraints between relations.
In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM
?12, pages 992?1001, New York, NY, USA. ACM.
P. D. Turney and S. M. Mohammad. 2014. Experiments with three approaches to recognizing lexical entailment.
Natural Language Engineering, FirstView:1?40, 5.
Istv?an Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon
Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1619?1629, Sofia, Bulgaria, August. ACL.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Language Learning, pages 194?204, Jeju Island, Korea,
July. ACL.
1434
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 693?703,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Two-stage Method for Large-scale Acquisition of
Contradiction Pattern Pairs using Entailment
Julien Kloetzer? Stijn De Saeger? Kentaro Torisawa? Chikara Hashimoto?
Jong-Hoon Oh? Motoki Sano? Kiyonori Ohtake??
Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Kyoto, Japan
{?julien, ? stijn, ? torisawa, ? ch, ?rovellia, ?msano, ??kiyonori.ohtake}@nict.go.jp
Abstract
In this paper we propose a two-stage method
to acquire contradiction relations between
typed lexico-syntactic patterns such as Xdrug
prevents Ydisease and Ydisease caused by
Xdrug . In the first stage, we train an SVM
classifier to detect contradiction pattern pairs
in a large web archive by exploiting the exci-
tation polarity (Hashimoto et al, 2012) of the
patterns. In the second stage, we enlarge the
first stage classifier?s training data with new
contradiction pairs obtained by combining the
output of the first stage?s classifier and that of
an entailment classifier. We acquired this way
750,000 typed Japanese contradiction pattern
pairs with an estimated precision of 80%. We
plan to release this resource to the NLP com-
munity.
1 Introduction
The ability to detect contradictory information in
text has many practical applications. Among those,
Murakami et al (2009) pointed out that a contra-
diction recognition system can detect conflicts and
anomalies in large bodies of texts and flag them to
help users identify unreliable information. For ex-
ample, many Japanese web pages claim that agari-
cus prevents cancer, where agaricus is a species of
mushroom found in a variety of commercial prod-
ucts. Although this has been accepted by many
Japanese people, by Googling keywords ?agaricus?,
?promotes? and ?cancer?, we can find pages claim-
ing that ?agaricus promotes cancer?, some of which
point to a study authorized by the Japanese Min-
istry of Health, Labour and Welfare1 reporting that
1 http://www.mhlw.go.jp/topics/bukyoku/iyaku/syoku-
anzen/qa/060213-1.html
a commercial product containing agaricus promoted
cancer. Obviously, the existence of these pages casts
serious doubt on the ability of agaricus to prevent
cancer and encourages readers to dig more about this
subject.
The above example suggests that recognizing
contradictory information can guide users to a true
fact. Likewise, we believe that contradiction recog-
nition is also useful when dealing with non-factual
information that occupy most of our daily lives. For
instance, there is a big controversy recently whether
Japan should join an economic partnership agree-
ment called the Trans Pacific Partnership (TPP), and
quite serious but contradictory claims are plentiful in
the mass media and on the web, e.g., TPP will wipe
out Japan?s agricultural businesses and TPP will
strengthen Japan?s agricultural businesses. Neither
of these are facts; they are predictions that can only
be realized or disputed after the underlying decision-
making is done: joining or refusing the TPP.
Furthermore, after reading documents including
contradictory predictions, one should notice that
each of them is supported by a convincing the-
ory that has no obvious defect, e.g., ?Exports of
Japan?s agricultural products will increase thanks to
the TPP? or ?A large amount of low-price agricul-
tural products will be imported to Japan due to the
TPP?. Even if one of these predictions may just hap-
pen to be true because of unexpected reasons such as
minor fluctuations in the Japanese yen, we must sur-
vey such theories that support contradictory predic-
tions, conduct balanced decision-making, and pre-
pare counter measures for the expected problems af-
ter examining multiple viewpoints. Contradiction
recognition should be useful to select documents to
be surveyed.
693
Figure 1: Method workflow
We have developed a method for recog-
nizing pairs of contradictory binary patterns
such as ??X promotes Y?, ?X prevents Y?? and
??X will wipe out Y?, ?X will strengthen Y??. To
solve the problem described above, we can easily
develop a system that can find contradictory text
fragments from the web like ?agaricus promotes
cancer? and ?agaricus prevents cancer? from the
discovered contradictory pattern pairs.
Our method is a two-stage procedure with three
supervised classifiers (Fig. 1). In the first stage,
we build a classifier BASE to recognize contradic-
tions between binary patterns, and a classifier ENT
to recognize entailment. In the second stage, we
combine the contradiction pairs recognized by BASE
and the entailment pairs recognized by ENT to ex-
pand BASE?s training data and train a new contra-
diction classifier, EXP. This expansion using en-
tailment is one key idea of this work: we acquired
750,000 contradiction pairs with 80% precision us-
ing the expanded training data, more than doubling
the 285,000 pairs acquired at the same precision
level without expansion. We also demonstrate that
this result is not trivial by showing that our method
outperforms an alternative one based on Integer Lin-
ear Programming inspired by the successful entail-
ment recognition method of Berant et al (2011).
As another technical contribution of this work, we
exploit the recently proposed semantic polarity of
excitation (Hashimoto et al, 2012) to recognize con-
tradictions between binary patterns. Hashimoto et
al. (2012) previously showed that excitation polari-
ties are useful to recognize contradictions between
phrases that consist of a noun and a predicate, such
as ?promote cancer? and ?prevent cancer?. While
it is trivial to extend this framework to contradic-
tions between unary patterns such as ?promote X?
and ?prevent X? by replacing the common nouns
in each pair with a variable, the information rep-
resented in unary patterns is often vague, and it is
unlikely that a contradiction between unary patterns
directly leads to the discovery of unreliable infor-
mation to be flagged or to a meaningful survey of
complex problems. As exemplified by the agaricus
and TPP examples, contradictions between binary
patterns that include two variables such as ?X pro-
motes Y? or ?X will wipe out Y? are more useful
than those between unary patterns. We also show
that it is not trivial to recognize contradictions be-
tween binary patterns using contradictions between
unary patterns.
Most works dealing with contradiction recogni-
tion up till now (Harabagiu et al, 2006; Bobrow
et al, 2007; Kawahara et al, 2008; Kawahara et
al., 2010; Ohki et al, 2011) focus on recognizing
contradictions between full sentences or documents,
not text fragments that match our relatively short
patterns (survey in Section 5). We expect that the
contradictory pattern pairs we acquired can be used
as building blocks in such full-fledged contradiction
recognition for full sentences or documents, simi-
larly to antonym pairs in Harabagiu et al (2006).
Also, we should emphasize that our method
focuses on the most challenging part of contra-
diction recognition according to the classification
of De Marneffe et al (2008). Since we discard
patterns with negations, an evident source of contra-
dictions like ??X causes Y?, ?X does not cause Y??,
most of our output are non-trivial contradic-
tions related to high-level semantic phenomena,
e.g., contradiction pairs related to antonyms
like ??X? Y?????, ?X? Y??????
(??X increases Y?, ?X decreases Y??), lexical contra-
dictions like ??X? Y????, ?Y? X?????
(??X wins against Y?, ?Y wins against X??), or
contradictions due to common-sense knowledge
like ??X? Y???????, ?X? Y??????
(??X reassures Y?, ?X betrays Y??). We believe
acquiring such contradictions in a large scale is a
valuable contribution.
The following is the outline of this paper. Sec-
tion 2 details our target and our proposed method.
Evaluation results are discussed in Section 3. Sec-
694
Figure 2: Detailed data flow
tion 4 details our features set, and Section 5 related
work. Section 6 provides a conclusion.
2 Proposed method
As showed in Figure 1, our method consists of
three supervised classifiers. Classifiers BASE and
EXP recognize contradiction relations between bi-
nary patterns, and ENT recognizes entailment rela-
tions between binary patterns. The contradiction
pairs recognized by BASE and the entailment pairs
recognized by ENT are combined to generate new
contradiction pairs, part of which are then added to
BASE training data to train the EXP classifier. Our
final output is the set of all binary pattern pairs re-
garded as contradictions by EXP. Since the depen-
dencies between these three classifiers, their distinct
sets of training data, and the two data sets to be clas-
sified (we describe those in the two sections below)
is a bit complex, we show a complete description of
the whole process in Figure 2.
The key idea is in the scheme that expands the
training data. Logically speaking, patterns p and r
are contradictory if there exists a pattern q such that
p entails q and q contradicts r. For example, since
?X causes Y? entails ?X promotes Y? and ?X pro-
motes Y? contradicts ?X prevents Y?, then ?X causes
Y? contradicts ?X prevents Y?. Hence, by combin-
ing entailment and contradiction pairs, we can ob-
tain more contradiction pairs.
Following this property of contradiction relations,
we collect a set of pattern pairs {?p, r?} for which
there exists a pattern q such that ENT recognizes that
p entails q and BASE recognizes that q contradicts r.
Then we rank these pairs based on a novel scoring
function called Contradiction Derivation Precision
(CDP) and expand BASE training data by adding to
it the top-ranked pairs according to CDP in order to
train EXP. This ranking scheme selects highly accu-
rate contradiction pairs and prevents errors caused
by BASE and ENT from being propagated to EXP.
In the following, after defining the patterns for
which we acquire contradiction relations, we de-
scribe BASE, EXP, ENT, and our expansion scheme.
2.1 Patterns
In this work, a binary pattern is a word sequence
on the path of dependency relations connecting two
nouns in a syntactic dependency tree, like ?X causes
Y?, and we say a noun pair co-occurs with a pattern
if the two nouns are connected by this pattern in the
dependency tree of a sentence in the corpus.
We focus on typed binary patterns, which place
semantic class restrictions on the noun pairs they
co-occur with, e.g., ?Yorganization is in Xlocation?.
Subscripts organization and location indicate the se-
mantic classes of the X and Y slots. Since typed
patterns can distinguish between multiple senses
of ambiguous patterns, they greatly reduce errors
due to pattern ambiguity (De Saeger et al, 2009;
Schoenmackers et al, 2010; Berant et al, 2011).
We automatically induced semantic classes from our
corpus using the EM-based noun clustering algo-
695
rithm presented in Kazama and Torisawa (2008),
and clustered one million nouns into 500 rela-
tively clean semantic classes, including for example
classes of diseases and of chemical substances.
The binary patterns and their co-occurring noun
pairs were extracted from our corpus of 600 mil-
lion Japanese web pages dependency parsed with
KNP (Kurohashi and Nagao, 1994). We restricted
our patterns to the most frequent 3.9 million pat-
terns of the form ?X-[case particle] Y-[case parti-
cle] predicate? such as ?X-ga Y-ni aru? (?X is in Y?)
which do not contain any negation, number, symbol
or punctuation character. Based on our observation
that patterns in meaningful contradiction and entail-
ment pairs tend to share many co-occurring noun
pairs, we used as input to our classifiers the set Pall
of 792 million pattern pairs for which both patterns
share three co-occurring noun pairs.
2.2 BASE: First stage Classifier for
Contradiction
Below, we detail BASE: its training data and input
data to be classified, and some experimental results.
Our first stage classifier for contradictions, BASE,
is an SVM that uses commonsensical surface and
lexical resources based features, such as n-grams ex-
tracted from patterns, which will be detailed in Sec-
tion 4. An important point to be stressed here is
that we restricted the pattern pairs to be classified
by BASE by exploiting their excitation polarity, a
semantic orientation proposed by Hashimoto et al
(2012). Excitation characterizes unary patterns as
excitatory, inhibitory, or neutral. Excitatory unary
patterns, such as ?cause X? or ?increase X?, entail
that the function, effect, purpose, or role of their ar-
gument?s referent is activated or enhanced, and in-
hibitory unary patterns, such as ?prevent X? or ?X
disappears?, entail that the function, effect, purpose,
or role of their argument?s referent is deactivated or
suppressed. Neutral unary patterns like ?close to X?
are neither excitatory nor inhibitory.
We exploited excitation to restrict the input of
BASE. Based on the result of Hashimoto et al
(2012) showing that two unary patterns with op-
posite polarity have a higher chance to be a con-
tradiction, we extracted from set Pall the set Popp
of binary pattern pairs that contain unary patterns
with opposite excitation polarities as sub-patterns.
??Y cause X?, ?Y prevent X?? is an example of such
a pair since the unary sub-patterns ?cause X? and
?prevent X? are respectively excitatory and in-
hibitory. We used here 6,470 excitation unary pat-
terns hand-labeled as either excitatory (4,882 pat-
terns) or inhibitory (1,588 patterns). Set Popp con-
tains 8 million pattern pairs with roughly 38% true
contradiction pairs, and is the input to BASE. We
will show in experiments at the end of this section
that this restriction is necessary to obtain good per-
formance for BASE. We also tried to add the excita-
tion polarities in BASE?s feature set and classify Pall,
but the performance was worse.
Training Data Another key feature of BASE is
that it is distantly supervised. We did not use
training samples that are directly manually anno-
tated. Instead we automatically generated training
data from a smaller set of (non-)contradiction unary
pattern pairs. We first prepared a set of roughly
800 unary pattern pairs hand-labeled by three human
annotators as contradictions (238 pairs) and non-
contradictions (558 pairs) using majority vote. The
inter-annotator agreement was 0.78 (Fleiss?kappa).
Inspired by Hashimoto et al (2012), we selected
these unary pattern pairs among pairs with high dis-
tributional similarity, with and without restricting
them to having opposite excitation polarity, such as
to get a fair distribution of contradictions and non-
contradictions.
We then extracted from set Pall all 256,000 pat-
tern pairs containing a contradictory unary pattern
pair, and all 5.2 million pattern pairs containing a
non-contradictory unary pattern pair, which we re-
spectively used as positive and negative training data
(estimated 79% and 73% accuracy from 200 hand-
labeled samples). Table 1 shows some examples.
The optimal composition of training data for
BASE was determined according to preliminary ex-
periments using our development set (1,000 manu-
ally labelled samples. See Section 3.1). We trained
20 different classifiers using from 6,250 to 50,000
positive samples (4 sets) and from 12,500 to 200,000
negative samples (5 sets), doubling the amounts in
each step, for a total of 20 configurations. We could
not try a larger training data due to long training time
but we do not expect it to be a problem because the
worst performance was observed with large train-
696
Table 1: Examples of training samples for BASE obtained from unary pattern pairs
Binary pattern pair (the unary pattern pair that extracted it is underlined) Unary pattern pair label
Y ? X ??? (X is bad in Y too) - Y ?? X ??? (X is good even in Y) contradiction
Y ? X ???? (Y too heads toward X) - Y ? X ??? (Y too comes out of X) contradiction
X ?Y ? ??? (add Y to X) - X ?Y ? ??? (insert X into Y) non-contradiction
Y ? X ??? (Y too comes to X) - Y ?? X ??? (go to X with Y) non-contradiction
Figure 3: Effect of the restriction using excitation
ing data (25,000 positives and 200,000 negatives;
the difference from the optimal setting was 2.3% in
average precision). The optimal training data set,
Trainbase, consists of 12,500 positives and 100,000
negatives samples as described above and is the one
we use in our experiments below and in Section 3.
Since BASE input for classification data is Popp
we also tried sampling Trainbase from Popp. We
obtained 56.27% average precision for our classi-
fier BASE, and 52.99% when restricting the source
of training data to pairs in Popp. We believe that the
difference lies in the size of the sets from which we
sampled our training data: while there are 5.46 mil-
lion binary pattern pairs in Pall with a hand-labeled
unary pattern pair in Pall, there are only 237,000
pairs in Popp. We believe this much smaller sam-
ple source lead to a lower performance because it
included much less variations of the patterns.
To train BASE and other classifiers mentioned in
this paper, we used the SVM tool TinySVM2 with
a polynomial kernel of degree 2, the setting which
showed the best performance during our preliminary
experiments.
Effect of Excitation Polarities We also empiri-
cally examined the effect of the restriction on the
patterns using excitation polarities. We used our test
set (2,000 manually annotated samples described in
2 http://chasen.org/?taku/software/TinySVM/
Section 3.1) and 250 manually annotated samples
(majority vote from 3 annotators) from top ranked
pairs of Pall to draw precision curves for BASE over
the top 2 million binary pairs from both Popp and
Pall. In each case we assumed that pairs were dis-
tributed uniformly (i.e., with a constant interval) in
the ranked list of pairs of Popp and Pall, and com-
puted precision accordingly. Since the pairs sets
are reasonably large and were sampled randomly we
thought this was a reasonable hypothesis. The pre-
cision over Popp is higher than that over Pall with
a large margin, suggesting that the restriction using
excitation polarities is beneficial.
2.3 ENT: First stage Classifier for Entailment
ENT is an SVM classifier for entailment trained us-
ing 27,500 hand-annotated binary pattern pairs (set
Trainent, 45% of positive entailment pairs) created
for some previous work (Kloetzer et al, 2013). It es-
sentially uses the same feature set as that for BASE
with the addition of several distributional similar-
ity measures (see Section 4 below for more details).
This classifier is given all pairs of Pall as input and
scores each of them. For this study, we considered
the 44.5 million pattern pairs with a positive SVM
score as entailment pairs. Manual annotation of 200
random samples revealed that the precision of these
pairs was 63% and that the top 7.1 million pairs had
80% precision (result interpolated from the top 16%
of the annotated samples).
2.4 Second stage: Training Data Expansion
and Classifier EXP
Below, we show how we combine BASE?s top output
(hereafter C) and ENT?s top output (hereafter E) in
the second stage of our method to expand Trainbase
and train a new classifier, EXP.
The training data expansion process is based on
the following logical constraint: if a pattern p entails
a pattern q and pattern q contradicts a third pattern r,
then p must contradict r. For example, because ?X
697
Table 2: Examples of triplets ?p, q,r? where p entails q, q contradicts r, and hence p contradicts r
Pattern p Pattern q Pattern r X/Y examples SV M Score(p, r) CDP (p, r)
Y ?? X ???? Y ?? X ????? Y ? X ???? ??/? 0.3 0.98X disappears from Y X vanishes from Y Y is full of X anger/eye
Y ? X ????? Y ? X ???? Y ?? X ???? ??/?? -0.3 0.61stop X in Y finish X in Y start X in Y April/activity
X ? Y ??? X ? Y ??? X ? Y ??? ???/?? 0.07 0.45X shows Y X have Y X loses Y team/confidence
Algorithm 1 Training data expansion: C is the top 5%
output of BASE, E is the top output of ENT (score > 0)
1: procedure EXPAND(C, E)
2: Compute the set of expanded pairs C? = {?p, r? |
?q : ?p, q?? E,?q, r?? C}.
3: Rank the pairs in C? using CDP.
4: Add the N top-ranked pairs in C? \ C as new positive
samples to Trainbase.
5: Remove incoherent negative training samples using
negative cleaning.
6: end procedure
causes Y? (pattern p) entails ?X promotes Y? (pattern
q) and the latter contradicts ?X prevents Y? (pattern
r), we conclude that ?X causes Y? (p) contradicts
?X prevents Y? (r). We call the former contradic-
tion ?q, r? a source contradiction pair, and the later
pair ?p, r? an expanded contradiction pair. Based on
this idea, we combine C and E to aggressively ex-
pand Trainbase. This process is described in Al-
gorithm 1, and Table 2 shows examples of triples
?p, q,r? obtained in our experiments.
Expanding pairs fromC andE compounds the er-
rors made by BASE and ENT, hence it is crucial to
select a highly precise subset of the expanded pairs.
Taking the top pairs according to their SVM score
would achieve this, but since BASE already handles
correctly such pairs, they should not help much as
new training data. We therefore propose a new scor-
ing function for selecting highly precise expanded
pairs: Contradiction Derivation Precision (CDP ).
CDP was designed according to the following
assumption: a source contradiction pair that derives
correct expanded pairs with a high precision should
be reliable. Probably, all the expanded pairs derived
from such a reliable source pair will be correct and
should be included in the new training data .
In our formulation of CDP , correctness of an ex-
panded pair is judged according to the pair?s SVM
score using BASE. In other words, we regard an
expanded pair that has an SVM score above some
threshold ? as a true contradiction. A source contra-
diction pair that derives true contradiction pairs with
a high precision is regarded as a reliable source con-
tradiction pair. CDP , which is defined over a ex-
panded pairs, is the maximum precision among that
of the source contradiction pairs that derive a given
expanded pair.
We first define CDPsub(q, r) over a source con-
tradiction pair ?q, r? as the ratio of expanded pairs
obtained from ?q, r? whose SVM score is above
threshold ?. This ratio corresponds to the precision
of the expanded pairs derived from the source con-
tradiction pair ?q, r?.
CDPsub(q, r) = |{?p, r? ? Ex(q, r) | Sc(p, r) > ?}|
|Ex(q, r)|
HereEx(q, r) is the set of expanded pairs derived
from a source pair ?q, r?, and Sc is the SVM score
given by BASE. In our experiments, we set ? = 0.46
such that pattern pairs for which BASE gives a score
over ? corresponds to the top 5% of BASE?s output.
CDP (p, r) over an expanded pair is defined as fol-
lows, where Source(p, r) is the set of source con-
tradiction pairs that were derived into the expanded
pair ?p, r?.
CDP (p, r) = max?q,r??Source(p,r)CDPsub(q, r)
We then expand the top 5% contradictions of
BASE?s output (set C) and pattern pairs scored pos-
itively by ENT (set E), rank all expanded pairs not
already in C according to CDP, and add the top N
pairs with the highest CDP values as positives to
Trainbase to train EXP. The value of N shall be
determined empirically in later experiments using
a development set. Note that, since CDP (p, r) is
independent of ?p, r??s SVM score, even pairs that
were assigned a negative score by BASE can become
highly ranked by CDP (second triplet in Table 2)
698
and be added to train EXP, hence we expect EXP to
learn something new from these pairs.
Finally, after the addition of expanded pairs, we
remove incoherent training samples. We propose to
remove from the negative training samples of EXP
any pattern pair that may conflict with the newly
added positives; we call this step negative cleaning.
Intuitively, since the content word pairs in a pattern
pair should present some of the strongest evidence
for determining the patterns (non-)contradiction sta-
tus, we remove any negative sample that shares a
content word pair with one of the added expanded
pairs. The final training data for EXP, set Trainexp,
consists of the following: (1) positive samples from
Trainbase, (2) (positive) expanded pairs, and (3)
negative training samples from Trainbase, cleaned
using negative cleaning. We confirmed in our exper-
iments that negative cleaning was necessary to train
a strong EXP classifier (details omitted for reason of
space).
After training EXP with Trainexp, we classify
Popp with EXP to produce the final output of the
whole method. Note that while this expansion pro-
cess can be re-iterated with EXP?s output, our exper-
iments failed to show any improvement with subse-
quent iterations.
3 Evaluation
This section presents our experimental results. We
describe first how we constructed test and develop-
ment data, and then report comparison results be-
tween our method and others including BASE and an
Integer Linear Programming-based (ILP) method.
3.1 Development and Test Data
We asked three human annotators to label 3,000 bi-
nary pattern pairs randomly sampled from Popp as
contradiction or non-contradiction to be used as de-
velopment (1,000 pairs) and test (2,000 pairs) sets.
We considered a pattern pair as a true contradic-
tion relation if at least two out of the three annota-
tors marked it as positive. The inter-rater agreement
score (Fleiss Kappa) was 0.523, indicating moderate
agreement (Landis and Koch, 1977). As a definition
of contradiction, we used the notion of incompati-
bility (i.e., two statements are extremely unlikely to
be simultaneously true) proposed by De Marneffe et
Figure 4: Precision of all the compared methods
al. (2008). We then say binary patterns such as ?X
causes Y? and ?X prevents Y? are contradictory if
the above definition holds for any noun pair that can
instantiate the patterns? variables in the provided se-
mantic class pair.
Because our semantic classes are obtained by au-
tomatic clustering and have no meaningful labels,
we followed Szpektor et al (2007) and provided the
annotators with three random noun pairs that co-
occur with the patterns as a proxy for the class pair.
The annotators marked a given pattern pair as posi-
tive if the contradiction relation between the patterns
held for all three noun pairs presented.
3.2 Experimental Results
Here we show how our proposed method outper-
forms baseline methods. We compare the following
four methods:
? PROPOSED: our proposed method. N , the
number of newly added positive training sam-
ples during the training data expansion pro-
cess, was set to 6,000 according to preliminary
experiments using the development set. We
tried 50 different values of N from 1,000 up to
50,000, adding 1,000 each time, and chose the
N value giving the highest average precision
against our development set (1,000 samples).
? BASE: our first stage classifier.
? PROP-SCORE: same as PROPOSED except for
the use of BASE?s SVM score instead of CDP .
N was set to 30,000 in the same way we set N
for PROPOSED.
? HAS: an adaptation of the contradiction ex-
traction method presented in Hashimoto et al
699
(2012). For a binary pattern pair we first
extracted its unary pattern pair with opposite
polarity (or one at random in case there are
two) and scored it based on our implementa-
tion of Hashimoto et al (2012); the score is
based on the distributional similarity between
unary patterns and an excitation score obtained
using a minimally supervised method based on
the spin model. We then scored the binary pat-
tern pair by the score of this unary pattern pair.
We ranked the pattern pairs of our test set (2,000
random pairs from set Popp) based on the score pro-
duced by each method. For each tested method we
assumed that pairs in the test set were distributed
uniformly like explained in Section 2.2. The pre-
cision curves we obtained are shown in Figure 4.
PROPOSED clearly outperformed BASE and ac-
quired around 750,000 contradiction pattern pairs
with an estimated precision of 80%, out of which
some examples are shown in Table 3. These pairs
cover 26,941 content word pairs and reduce to
272,164 untyped pairs, showing that PROPOSED
does not just acquire a handful of contradictions in
many different class pairs. Also, when matching
these pairs against an antonyms database (extracted
from the dictionary of the morphical analyzer JU-
MAN) we found that only 100,886 of these pattern
pairs contain an antonym pair, which means that
most of the extracted pairs? contradictions are due
to more complex phenomena than simple antonymy.
With the same precision, BASE and PROP-SCORE
acquired only 285,000 pairs (covering 11,794 con-
tent word pairs) and 636,000 pairs respectively. This
implies that our two-stage method can more than
double the number of highly precise contradiction
pairs we acquire as well as increasing their vari-
ety, and that ranking expanded pairs using our scor-
ing function CDP is better than with SVM score,
though even PROP-SCORE performs better than
BASE in our setting. Finally, the poor performance
of HAS suggests that extending the Hashimoto et
al.?s framework to recognition of binary patterns is
not a trivial task.
As to why adding only 6,000 top pairs ranked
by CDP performs better than adding 30,000 pairs
ranked by SVM score, the pattern pairs added in
PROP-SCORE had high SVM scores given by BASE
and as such are already handled nicely by BASE.
Table 3: Examples of pairs acquired by PROPOSED: con-
tradiction (label +) and non-contradiction (label -)
Lab. Pattern pairs (with rank) X/Y example
Y ? X ???? - Y ?? X ????? ??/??
+ X finished Y - X started from Y sale/yesterday
Rank 228,039
X ? Y ??? - Y ? X ??? ??/????
+ X wins against Y - Y wins against X Japan/Vietnam
Rank: 258,068
X ? Y ??? - X ?? Y ??? ?/??
- X lose Y - Have Y in X people/interest
Rank 474,143
Y ? X ???? - Y ?? X ??? ??/??
+ Lose X in Y - Have X in Y too confidence/
Rank 522,534 oneself
Y ? X ????? - X ? Y ???? 9 ?/??
- Y falls down to X - raise Y to X 9th/ranking
Rank 538,901
X ? Y ????? - X ?? Y ??? ?/????
+ Y exists in X - Keep Y out X inside/virus
Rank 620,430
X ?? Y ??? - X ? Y ???? ?/?
- Remove Y off X - X answer with Y I (or me)/eyes
Rank 652,530
Y ? X ?????? - X ? Y ??? ?/??
+ Kick out Y from X - Y remains in X body/fatigue
Rank 697,177
Y ? X ?????? - Y ? X ????? ?/??
+ X reassures Y - X betrays Y I/her
Rank: 749,916
Hence, we think the effect of adding a new sam-
ple from PROP-SCORE is smaller than that in PRO-
POSED, because in PROPOSED we add to the train-
ing data pattern pairs with both high and low (possi-
bly negative) SVM scores.
Finally, while the quality of the entailment pairs
plays a very important role in the assumption that
was the base of CDP , these results show that even
a simple rule such as ?Use entailment pairs with
SVM score over 0 to expand contradictions before
ranking them with CDP ? is sufficient to make the
method work. Though it may be possible to design
a more complex CDP formula which takes entail-
ment score into account, we did not explore this di-
rection in this work.
Comparison with an ILP-based method Finally,
we would like to compare our method with an ILP-
based method. The interaction between contradic-
tion and entailment that forms the basis for our ex-
pansion method has a natural interpretation as an op-
timization problem. We thus compared our method
to the following ILP formulation of this interaction
inspired by Berant et al (2011), using our test set:
700
Figure 5: Comparison between PROPOSED, BASE and
BASE+ILP on a restricted test set (1,306 samples)
(1) G = argmax
?
p6=q
(e(p, q)??)?Epq +(c(p, q)??)?Cpq
(2) s.t. ?p,q,r Epq + Cqr ? Cpr ? 1
(3) ?p,q Epq + Cpq ? 1
(4) ?p,q Epq ? {0, 1} (5) ?p,q Cpq ? {0, 1}
The objective in Equation (1) is a sum over the
weights of every pair of patterns ?p, q?, where Epq
indicates whether a pair ?p, q? is an entailment pair
(Equation (4)), andCpq indicates whether it is a con-
tradiction pair (Equation (5)). e(p, q) and c(p, q) are
the score given respectively by ENT and BASE, and
? is a prior defining the weight of a pair as neither
entailment nor contradiction that shall be set before
any experimentation. Equation (2) states the tran-
sitivity relation which is the basis of our expansion
method. Finally, Equation (3) states that a given pat-
tern pair cannot be a contradiction pair and an entail-
ment pair at the same time. Since our patterns are
class-dependent, we solved separate ILP instances
for each semantic class pair.
We drew a precision curve for each of BASE,
PROPOSED and BASE+ILP. To draw the curve for
BASE+ILP, we incrementally raised the sample?s
non-contradiction non-entailment prior ? (more de-
tails in Berant et al (2011)). Because of the com-
putational difficulty of ILP (NP-complete) and the
size of our data, the computation for the ILP-based
method ran out of memory on a 72GB machine for
116 class pairs out of the 1,031 that our test set cov-
ers. For this reason, we only used the 1,306 samples
of the test set covered by the remaining 915 class
pairs. We also measured the performance of BASE
and PROPOSED on the same restricted test set.
Figure 5 shows that under these conditions the
ILP-based method performance resembles BASE
and is worse than PROPOSED on all data points.
PROPOSED performs slightly worse in this setting
compared to when classifying the whole of Popp,
but this only means that its performance is good for
the 116 class pairs we ignored in this experiment.
While this comparison is only made in a restricted
setting, our expansion method still outperforms ILP
and is clearly more scalable. The ILP results could
be improved by adding more constraints (contradic-
tion is symmetric, entailment is transitive), but this
would also make the problem even more intractable
in terms of computational costs.
4 Features
In this section we present the features used in our
classifiers, which are mainly categorized into three:
surface features (i.e., those reflecting the patterns?
content itself), features based on external lexical re-
sources, and distributional similarity based features;
all features are listed in Table 4. ENT uses all the
features while BASE and EXP use all except for the
distributional similarity based ones. The optimality
of the feature sets was confirmed through ablation
tests using the development set (results omitted for
the sake of space).
Since patterns with a contradiction or entailment
relation are often superficially similar, for instance,
in case structure or inflection, we use a number of
surface features based on string similarity measures,
extending the feature sets used by Malakasiotis and
Androutsopoulos (2007) for entailment recognition.
They include bag-of-words features such as n-grams
and similarity scores concerning the bag-of-words
such as their Euclidian distance.
To complement the surface features with knowl-
edge about the content words, we used lexi-
cal databases including such as antonymy, syn-
onymy, entailment, or allography. The presence
of such word pairs is usually a good indicator of
(non-)contradiction or (non-)entailment at the pat-
tern level. More specifically, for any word pair
?wp,wq? taken from a pattern pair ?p, q? we mark
the presence of ?wp,wq? in each of the lexical re-
sources as a binary feature. We used the Japanese
lexical resources distributed by the ALAGIN Fo-
rum3: the verb entailment database (117,000 verb
3 http://www.alagin.jp/
701
Table 4: Features summary, computed over a pair of patterns ?p, q?
su
rfa
ce Similarity measures: common elements ratios, Dice coefficient, Jaccard and discounted Jaccard scores, Cosine, Euclidian, Manhattan, Levenshteinand Jaro distances; computed over: the patterns? 1-, 2- and 3-grams sets of: characters, morphemes, their stems & POS; content words and stems
binary feature for each of the patterns? subtrees, 1- and 2-grams ; patterns? lengths and length ratios
le
x.
r. entries in databases of verb entailments and non-entailments, synonyms, antonyms, allographs ; checked over: pairs of content words,
pairs of content word stems, same for the reverse pattern pair ?q, p?
di
s.s
. Distributional similarity measures: Common elements ratios, Jaccard and discounted Jaccard scores, sets and sets intersection cardinality,
DIRT (Lin and Pantel, 2001), Weeds (Weeds and Weir, 2003) and Hashimoto (Hashimoto et al, 2009) scores; computed over: patterns?
co-occurring noun pairs, POS tags of those, nouns co-occurring in each variable slot, nouns co-occurring with each unary sub-patterns
ot
he
r binary feature for each semantic class pair and individual semantic classes
patterns frequency rank in the given semantic class pair
pairs; Alagin ID A-2), the databases of synonyms,
antonyms and meronyms (respectively 111,000,
5000 and 2500 pairs; Alagin ID A-9), and the al-
lographic word database (2.7 million pairs; Alagin
ID A-7). We also used the information concerning
allographic words in the dictionary of the morpho-
logical analyzer JUMAN4.
Distributional similarity values between patterns
are based on the idea that patterns that appear in
similar contexts tend to have similar meanings and
as such are useful to recognize entailment (Lin and
Pantel, 2001). We computed as features several dis-
tributional similarity measures on the sets of each
pattern?s co-occurring noun pairs and their POS
tags, of nouns co-occurring in each variable slot, and
with each of the pattern?s unary sub-patterns.
We also added a few more uncategorizable fea-
tures. See Table 4 for more details.
5 Related Work
A number of previous work dealt with the recogni-
tion of contradictions between sentences. Harabagiu
et al (2006) proposed a contradiction detection
method that focuses on negation, antonymy and
some discourse information. Kawahara et al (2010)
also used negations and antonyms to extract con-
trastive/contradictory statements from the web to
present users with a bird ?s-eye view of statements
about a given topic. Bobrow et al (2007) showed
a method using logical forms with relatively precise
results. Ohki et al (2011) proposed a method to rec-
ognize confinment, a novel semantic relation related
to both entailment and contradiction. While we do
not deal ourselves directly with sentences, we expect
that the binary pattern pairs we acquire can play a
role similar to that of basic linguistic resources such
4 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
as antonyms and negations in these works. Closer
to our work, Ritter et al (2008) presented a method
for detecting contradictions between functional re-
lations like ?X was born in Y?, but these constitute
only a part of the semantic relations expressed by the
binary patterns we deal with in this paper.
Other works analyzed contradictions from lin-
guistic/semantic viewpoints. Voorhees (2008) ana-
lyzed the contradiction recognition-task of the RTE3
contest. Magnini and Cabrio (2010) examined rela-
tions between contradictions and textual entailment
samples. De Marneffe et al (2008) presented a
typology of contradictions, and showed that con-
tradictions can arise from a multitude of phenom-
ena. They showed contradictions based on lexical or
world knowledge are challenging and require a high-
level understanding of language and/or the world.
As stated in the introduction, these are the types of
contradictions our method focuses on.
6 Conclusion
This paper showed how to acquire a large number of
contradiction pairs between lexico-syntactic binary
patterns by exploiting (1) the interaction between
contradiction and entailment, and (2) excitation po-
larities. In the end, we could acquire 750,000 typed
contradiction pattern pairs with an estimated 80%
precision. The resulting contradiction pairs cov-
ered ones deeply related to world knowledge such
as the pair ??X reassures Y?, ?X betrays Y??. We ex-
pect our work to lead to a high level analysis of
textual information, such as flagging unreliable in-
formation or identifying important documents to be
surveyed for understanding complex social prob-
lems. We plan to release the data we acquired to
the NLP community through the ALAGIN Forum5.
5 http://www.alagin.jp/
702
References
J. Berant, I. Dagan, and J. Goldberger. 2011. Global
learning of typed entailment rules. In Proceedings of
ACL 2011, pages 610?619.
D. G. Bobrow, C. Condoravdi, R. Crouch, V. De Paiva,
L. Karttunen, T. H. King, R. Nairn, L. Price, and
A. Zaenen. 2007. Precision-focused textual inference.
In Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, page 16?21.
M.-C. De Marneffe, A. N. Rafferty, and C. D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL 2008, page 1039?1047.
S. De Saeger, K. Torisawa, J. Kazama, K. Kuroda, and
M. Murata. 2009. Large scale relation acquisition us-
ing class dependent patterns. In Proceedings of ICDM
2009, page 764?769.
S.M. Harabagiu, A. Hickl, and V.F. Lacatusu. 2006.
Negation, contrast and contradiction in text process-
ing. In Proceedings of AAAI 2006, pages 755?762.
C. Hashimoto, K. Torisawa, K. Kuroda, S. De Saeger,
M. Murata, and J. Kazama. 2009. Large-scale verb
entailment acquisition from the web. In Proceedings
of EMNLP 2009, volume 3, page 1172?1181.
C. Hashimoto, K. Torisawa, S. De Saeger, J.-H. Oh, and
J. Kazama. 2012. Excitatory or inhibitory: A new se-
mantic orientation extracts contradiction and causality
from the web. In Proceedings of EMNLP 2012.
D. Kawahara, S. Kurohashi, and K. Inui. 2008. Grasp-
ing major statements and their contradictions toward
information credibility analysis of web contents. In
Proceedings of WI-IAT 2008, volume 1, page 393?
397.
D. Kawahara, K. Inui, and S. Kurohashi. 2010. Iden-
tifying contradictory and contrastive relations between
statements to outline web information on a given topic.
In Proceedings of COLING 2010, page 534?542.
J. Kazama and K. Torisawa. 2008. Inducing gazetteers
for named entity recognition by large-scale clustering
of dependency relations. Proceedings of ACL 2008,
page 407?415.
J. Kloetzer, S. De Saeger, K. Torisawa, M. Sano,
C. Hashimoto, and J. Gotoh. 2013. Large-scale acqui-
sition of entailment pattern pairs. In Information Pro-
cessing Society of Japan (IPSJ) Kansai-Branch Con-
vention.
S. Kurohashi and M. Nagao. 1994. KN parser: Japanese
dependency/case structure analyzer. In Proceedings
of the Workshop on Sharable Natural Language Re-
sources, page 48?55.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
page 159?174.
D. Lin and P. Pantel. 2001. Dirt - discovery of inference
rules from text. In Proceedings of the ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 323?328.
B. Magnini and E. Cabrio. 2010. Contradiction-focused
qualitative evaluation of textual entailment. In Pro-
ceedings of the Workshop on Negation and Speculation
in Natural Language Processing, page 86?94.
P. Malakasiotis and I. Androutsopoulos. 2007. Learning
textual entailment using SVMs and string similarity
measures. In Proceedings of the ACL- PASCAL Work-
shop on Textual Entailment and Paraphrasing, page 42
?47.
K. Murakami, E. Nichols, S. Matsuyoshi, A. Sumida,
S. Masuda, K. Inui, and Y. Matumoto. 2009. State-
ment map: assisting information crediblity analysis
by visualizing arguments. In Proceedings of the 3rd
workshop on Information credibility on the web, page
43?50. ACM.
M. Ohki, S. Matsuyoshi, J. Mizuno, K. Inui, E. Nichols,
K. Murakami, S. Masuda, and Y. Matsumoto. 2011.
Recognizing confinement in web texts. In the Pro-
ceedings of the Ninth International Conference on
Computational Semantics, page 215?224.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction?no, it?s not: a case study
using functional relations. In Proceedings of EMNLP
2008, pages 11?20.
S. Schoenmackers, O. Etzioni, D. S Weld, and J. Davis.
2010. Learning first-order horn clauses from web text.
In Proceedings of EMNLP 2010, page 1088?1098.
I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-
based evaluation of entailment rule acquisition. In
Proceedings of ACL 2007, volume 45, page 456?463.
E. M. Voorhees. 2008. Contradictions and justifications:
Extensions to the textual entailment task. In Proceed-
ings of ACL 2008, page 63?71.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP
2003, page 81?88. Association for Computational
Linguistics.
703
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1619?1629,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Aid is Out There:
Looking for Help from Tweets during a Large Scale Disaster
Istva?n Varga? Motoki Sano? Kentaro Torisawa? Chikara Hashimoto?
Kiyonori Ohtake? Takao Kawai? Jong-Hoon Oh? Stijn De Saeger?
?Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Japan
{istvan, msano, torisawa, ch, kiyonori.ohtake, rovellia, stijn}@nict.go.jp
?Knowledge Discovery Research Laboratories, NEC Corporation, Japan
t-kawai@bx.jp.nec.com
Abstract
The 2011 Great East Japan Earthquake
caused a wide range of problems, and as
countermeasures, many aid activities were
carried out. Many of these problems and
aid activities were reported via Twitter.
However, most problem reports and corre-
sponding aid messages were not success-
fully exchanged between victims and lo-
cal governments or humanitarian organi-
zations, overwhelmed by the vast amount
of information. As a result, victims could
not receive necessary aid and humanitar-
ian organizations wasted resources on re-
dundant efforts. In this paper, we propose
a method for discovering matches between
problem reports and aid messages. Our
system contributes to problem-solving in
a large scale disaster situation by facilitat-
ing communication between victims and
humanitarian organizations.
1 Introduction
The 2011 Great East Japan Earthquake in March
11, 2011 killed 15,883 people and destroyed over
260,000 households (National Police Agency of
Japan, 2013). Accustomed way of living suddenly
became unmanageable and people found them-
selves in extreme conditions for months.
Just after the disaster, many people used Twitter
for posting problem reports and aid messages as
it functioned while most communication channels
suffered disruptions (Winn, 2011; Acar and Mu-
raki, 2011; Sano et al, 2012). Examples of such
problem reports and aid messages, translated from
Japanese tweets, are given below (P1, A1).
P1 My friend said infant formula is sold out. If
somebody knows shops in Sendai-city where
they still have it in stock, please let us know.
A1 At Jusco supermarket in Sendai, you can still
buy water and infant formula.
If A1 would have been forwarded to the sender
of P1, it could have helped since it would help
the ?friend? to obtain infant formula. But in re-
ality, the majority of such reports/messages, es-
pecially unforeseen ones went unnoticed amongst
the mass of information (Ohtake et al, 2013). In
addition, there were cases where many humani-
tarian organizations responded to the same prob-
lems and wasted precious resources. For instance,
many volunteers responded to problems which
were heavily reported by public media, leading
to oversupply (Saijo, 2012). Such waste of re-
sources could have been avoided if the organiza-
tions would have successfully shared the aid mes-
sages for the same problems.
Such observations motivated this work. We de-
veloped methods for recognizing problem reports
and aid messages in tweets and finding proper
matches between them. By browsing the discov-
ered matches, victims can be assisted to over-
come their problems, and humanitarian organiza-
tions can avoid redundant relief efforts. We define
problem reports, aid messages and their successful
matches as follows.
Problem report: A tweet that informs about the
possibility or emergence of a problem that re-
quires a treatment or countermeasure.
Aid message: A tweet that (1) informs about sit-
uations or actions that can be a remedy or so-
lution for a problem, or (2) informs that the
problem is solved or is about to be solved.
Problem-aid tweet match: A tweet pair is a
problem-aid tweet match (1) if the aid mes-
sage informs how to overcome the problem,
(2) if the aid message informs about the set-
1619
tlement of the problem, or (3) if the aid mes-
sage provides information which contributes
to the settlement of the problem.
In this work we excluded direct requests, such as
?Send us food!?, from problem reports. This is be-
cause it is relatively easy to recognize such direct
requests by checking mood types (i.e., imperative)
and their behavior is quite different from prob-
lem reports like ?People in Sendai are starving?.
Problem reports in this work do not directly state
which actions are required, only implying the ne-
cessity of a countermeasure through claiming the
existence of problems.
An underlying assumption of our method is that
we can find a noun-predicate dependency relation
that works as an indicator of problems and aids in
problem reports and aid messages, which we refer
to as problem nucleus and aid nucleus.1 An exam-
ple of problem nucleus is ?infant formula is sold
out? in P1, and that of aid nucleus is ?(can) buy
infant formula? in A1. Many problem-aid tweet
matches can be recognized through problem and
aid nuclei pairs.
We also assume that if the problem and aid nu-
clei match, they share the same noun. Then, the
semantics of predicates in the nuclei is the main
factor that decides whether the nuclei constitute
a match. We introduce a semantic classification
of predicates according to the framework of ex-
citation polarities proposed in Hashimoto et al
(2012). Our hypothesis is that excitation polarities
along with trouble expressions can characterize
problem reports, aid messages and their matches.
We developed a supervised method encoding such
information into its features.
An evident alternative to this approach is to use
sentiment analysis (Mandel et al, 2012; Tsagkali-
dou et al, 2011) assuming that problem reports
should include something ?bad? while aid mes-
sages describe something ?good?. However, we
will show that this does not work well in our exper-
iments. We think this is due to mismatch between
the concepts of problem/aid and sentiment polar-
ity. Note that previous work on ?demand? recogni-
tion also found similar tendencies (Kanayama and
Nasukawa, 2008).
Another issue in this task is, of course, the
context surrounding problem/aid nuclei. The fol-
1We found that out of 500 random tweets only 4.5% of
problem reports and 9.1% of aid messages did not contain
any problem report/aid message nuclei.
lowing (imaginary) tweets exemplify the problems
caused by contexts.
FP1 I do not believe infant formula is sold out
in Sendai.
FA1 At Jusco supermarket in Iwaki, you can still
buy infant formula.
The problem nuclei of FP1 and P1 are the same
but FP1 is not a problem report because of the ex-
pression ?I do not believe?. The aid nuclei of FA1
and A1 are the same but FA1 does not constitute
a proper match with P1 because FA1 and P1 re-
fer to different cities, ?Iwaki? and ?Sendai?. In
this work, the problems concerning the modality
and other semantic modifications to problem/aid
nuclei by context are dealt with by the introduc-
tion of features representing the text surrounding
the nuclei in machine learning. As for the loca-
tion problem, we apply a location recognizer to all
tweets and restrict the matching candidates to the
tweet pairs referring to the same location.
2 Approach
!"#$%"&"'()*&+*,*&-.(.+*,/+/0$0,/0,)-+$*#.(,'+
!"#$%&'("&!#")("&*#+,-.&"(
12001.+ 12001+/0$0,/0,)-+#0&*3",+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+
/-0('&11/+&("&*#+,-.&"(
*(/+!0..*'0++*(/+,5)&05.+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+ *(/+!0..*'0+*(/+,5)&05.+
!"#$%&'2/-0()3&&)('/)*4(
$#"4&0!+*,/+*(/+,"5,+*#0+1%0+.*!06+.*!0+'0"'#*$%()*&+&")*3",+
&")*3",+#0)"',(70#+
!"#$%&'2/-0('/)*4("&*#+,-.&"(
Figure 1: Problem-aid matching system overview.
We developed machine learning based systems
to recognize problem reports, aid messages and
problem-aid tweet matches. Figure 1 illustrates
the whole system. First, location names in tweets
are identified by matching tweets against our loca-
tion dictionary, described in Section 3. Then, each
tweet is paired with each dependency relation in
the tweet, which is a candidate of problem/aid nu-
clei and given to the problem report and aid mes-
sage recognizers. A tweet-nucleus-candidate pair
judged as problem report is combined with another
tweet-nucleus-candidate pair recognized as an aid
message if the two nuclei share the same noun and
the tweets share the same location name, and given
to the problem-aid match recognizer.
1620
In the following, problem and aid nuclei are
denoted by a noun-template pair. A template is
composed of a predicate and its argument posi-
tion. For instance, ?water supply stopped? in P2
is a problem nucleus, ?water supply recovered? in
A2 is an aid nucleus and they are denoted by the
noun-template pairs ?water supply, X stopped? and
?water supply, X recovered?.
P2 In Sendai city, water supply stopped.
A2 In Sendai city, water supply recovered.
Roughly speaking, we regard the tasks of prob-
lem report recognition and aid message recogni-
tion as the tasks of finding proper problem/aid
nuclei in tweets and our method performs these
tasks based on the semantic properties of nouns
and templates in problem/aid nucleus candidates
and their surrounding contexts.
The basic intuition behind this approach can
be explained using excitation polarity proposed in
Hashimoto et al (2012). Excitation polarity differ-
entiates templates into ?excitatory? or ?inhibitory?
with regard to the main function or effect of en-
tities referred to by their argument noun. While
excitatory templates (e.g., cause X, buy X, suf-
fer from X) entail that the main function or ef-
fect is activated or enhanced, inhibitory templates
(e.g., ruin X, prevent X, X runs out) entail that
the main function or effect is deactivated or sup-
pressed. The templates that do not fit into the
above categorization are classified as ?neutral?.
We observed that problem reports in general
included either of (A) a dependency relation be-
tween a noun referring to some trouble and an
excitatory template or (B) a dependency rela-
tion between a noun not referring to any trouble
and an inhibitory template. Examples of (A) in-
clude ?carbon monoxide poisoning, suffer from
X?, ?false rumor, spread X?. They refer to events
that activate troubles. On the other hand, (B) is
exemplified by ?school, X is collapsed?, ?battery,
X runs out?, which imply that some non-trouble
objects such as resources, appliances and facilities
are dysfunctional. We assume that if we can find
such dependency relations in tweets, the tweets are
likely to be problem reports.
Contrary, a tweet is more likely to be an aid
message when it includes either (C) a dependency
relation between a noun referring to some trouble
and an inhibitory template or (D) a dependency re-
lation between a noun not referring to any trou-
trouble non-trouble
excitatory (A) problem nucleus (D) aid nucleus
inhibitory (C) aid nucleus (B) problem nucleus
Table 1: Problem/aid-excitation matrix.
ble and an excitatory template. Examples of (C)
are ?flu, X was eradicated (in some shelter)? and
?debris, remove X?. They represent the dysfunc-
tion of troubles and can mean the solution or the
settlement of troubles. On the other hand, exam-
ples of (D) include ?school, X re-build? and ?baby
formula, buy X?. They entail that some resources
function properly or become available. These for-
mulations are summarized in Table 1.
As an interesting consequence of such a view
on problem/aid nucleus, we can say the following
regarding problem-aid tweet matchings: when a
problem nucleus and an aid nucleus are an ade-
quate match, the excitation polarities of their tem-
plates are opposite. Consider the following tweets.
P3 Some people were going back to Iwaki, but the
water system has not come back yet. It?s ter-
rible that bath is unusable.
A3 We open the bath for the public, located on
the 2F of Iwaki Kuhon temple. If you?re stay-
ing at a relief shelter and would like to take a
bath, you can use it.
?Bath is unusable? in P3 is a problem nucleus
while ?open the bath? in A3 is an aid nucleus.
Since the problem reported in P3 can be solved
with A3, they are a successful match. The in-
hibitory template ?X is unusable? indicates that
the function of ?bath?, a non-trouble expression,
is suppressed. The excitatory template ?open X?
indicates that the function of ?bath? is activated.
The same holds when we consider the noun re-
ferring to troubles like ?flu?. The polarity of the
template in a problem nucleus should be excita-
tory like ?flu is raging? while that of an aid nucleus
should be inhibitory like ?flu, X was eradicated?.
These examples keep the constraint that the prob-
lem and aid nucleus should have opposite polari-
ties when they constitute a match.
Note that the formulations of problem report,
aid message and their matches or the excitation
matrix (Table 1) were not presented to our anno-
tators and our test/training data may contain data
that contradict with the formulations. These for-
mulations constitute the hypothesis to be validated
in this work.
1621
An important point to be stressed here is that
there are problem-aid tweet matches that do not
fit into our formulations. For instance, we as-
sume that the problem nucleus and aid nucleus in
a proper match share the same noun. However,
tweet pairs such as ?There are many injured people
in Sendai city? and ?We are sending ambulances
to Sendai? can constitute a proper match, but there
is no proper problem-aid nuclei pair that share the
same noun in these tweets. (We can find the de-
pendency relations sharing ?Sendai? but they do
not express anything about the contents of prob-
lem and aid.) The point is that the tweet pairs can
be judged because people know ambulances can
be a countermeasure to injured people as world
knowledge. Introducing such world knowledge is
beyond the scope of this current study.
Also, we exclude direct requests from problem
reports. As mentioned in the introduction, identi-
fying direct requests is relatively easy, hence we
excluded them from our target.
3 Problem Report and Aid Message
Recognizers
We recognize problem reports and aid messages in
given tweets using a supervised classifier, SVMs
with linear kernel, which worked best in our pre-
liminary experiments. The feature set given to
the SVMs are summarized in the top part of Ta-
ble 2. Note that we used a common feature
set for both the problem report recognizer and
aid message recognizer and that it is categorized
into several types: features concerning trouble
expressions (TR), excitation polarity (EX), their
combination (TREX1) and word sentiment polar-
ity (WSP), features expressing morphological and
syntactic structures of nuclei and their context sur-
rounding problem/aid nuclei (MSA), features con-
cerning semantic word classes (SWC) appearing
in nuclei and their context, request phrases, such
as ?Please help us?, appearing in tweets (REQ),
and geographical locations in tweets recognized
by our location recognizer (GL). MSA is used to
express the modality of nuclei and other contex-
tual information surrounding nuclei. REQ was in-
troduced based on our observation that if there are
some requests in tweets, problem nuclei tend to
appear as justification for the requests.
We also attempted to represent nucleus template
IDs, noun IDs and their combinations directly in
our feature set to capture typical templates fre-
TR Whether the nucleus noun is a trouble/non-trouble expression.
EX1 The excitation polarity and the value of the excitation score of the
nucleus template.
TREX1 All possible combinations of trouble/non-trouble of TR and exci-
tation polarities of EX1.
WSP1 Whether the nucleus noun is positive/negative/not in theWord Sen-
timent Polarity (WSP) dictionary.
WSP2 Whether the nucleus template is positive/negative/not in the WSP
dictionary.
WSP3 Whether the nucleus template is followed by a positive/negative
word within the tweet.
MSA1 Morpheme n-grams, syntactic dependency n-grams in the tweet
and morpheme n-grams before and after the nucleus template.
(1 ? n ? 3)
MSA2 Character n-grams of the nucleus template to capture conjugation
and modality variations. (1 ? n ? 3)
MSA3 Morpheme and part-of-speech n-grams within the bunsetsu con-
taining the nucleus template to capture conjugation and modality
variations. (1 ? n ? 3) (A bunsetsu is a syntactic constituent
composed of a content word and several function words, the small-
est unit of syntactic analysis in Japanese.)
MSA4 The part-of-speech of the nucleus template?s head to capture
modality variations outside the nucleus template?s bunsetsu.
MSA5 The number of bunsetsu between the nucleus noun and the nucleus
template. We found that a long distance between the noun and the
template suggests parsing errors.
MSA6 Re-occurrence of the nucleus noun?s postpositional particle be-
tween the nucleus noun and the nucleus template. We found
that the re-occurrence of the same postpositional particle within
a clause suggests parsing errors.
SWC1 The semantic class n-grams in the tweet.
SWC2 The semantic class(es) of the nucleus noun.
REQ Presence of a request phrase in the tweet, identified from within
426 manually collected request phrases.
GL Geographical locations in the tweet identified using our location
recognizer. Existence/non-existence of locations in tweets are also
encoded.
EX2 Whether the problem and aid nucleus templates have the same or
opposite excitation polarities.
EX3 Product of the values of the excitation scores for the problem and
the aid nucleus template.
TREX2 All possible combinations of trouble/non-trouble of TR, excitation
polarity EX1 of the problem nucleus template and excitation po-
larity EX1 of the aid nucleus template.
SIM1 Common semantic word classes of the problem report and aid mes-
sage.
SIM2 Whether there are common nouns modifying the common nucleus
noun or not in the problem report and aid message.
SIM3 Whether the words in the same word class modify the common
nucleus noun or not in the problem report and aid message.
SIM4 The semantic similarity score between the problem nucleus tem-
plate and the aid nucleus template.
CTP Whether the problem nucleus template and the aid nucleus tem-
plate are in contradiction relation dictionary or not.
SSR1 Problem report recognizer?s SVM score of problem nucleus tem-
plate.
SSR2 Problem report recognizer?s SVM score of aid nucleus template.
SSR3 Aid message recognizer?s SVM score of the problem nucleus tem-
plate.
SSR4 Aid message recognizer?s SVM score of the aid nucleus template.
Table 2: Features used with the problem re-
port recognizer and the aid message recognizer
(above); additional features used in training the
problem-aid match recognizer (below).
quently appearing in problem and aid nuclei, but
since there was no improvement we omit them.
The other feature types need some non-trivial
dictionaries. In the following, we explain how we
created the dictionaries for each feature type along
with the motivation behind their introduction.
Trouble Expressions (TR) As mentioned previ-
ously, trouble expressions work as good evidence
for recognizing problem reports and aid messages.
The TR feature indicates whether the noun in the
problem/aid nucleus candidate is a trouble ex-
1622
pression or not. For this purpose, we created
a list of trouble expressions following the semi-
supervised procedure presented in De Saeger et al
(2008). After manual validation of the list, we ob-
tained 20,249 expressions referring to some trou-
bles, such as ?tsunami? and ?flu?. The value of the
TR feature is determined by checking whether the
nucleus noun is contained in the list.
Excitation Polarities (EX) The excitation po-
larities are also important in recognizing problem
reports and aid messages as mentioned before. For
constructing the dictionary for excitation polarities
of templates, we applied the bootstrapping proce-
dure in Hashimoto et al (2012) to 600 millionWeb
pages. Hashimoto?s method provides the value of
the excitation score in [?1, 1] for each template
indicating the polarities and their strength. Posi-
tive value indicates excitatory, negative value in-
hibitory and small absolute value neutral. After
manual checking of the results by the majority
vote of three human annotators (other than the au-
thors), we limited the templates to the ones that
have score values consistent with the majority vote
of the annotators, obtaining a dictionary consisting
of 7,848 excitatory, 836 inhibitory and 7,230 neu-
tral templates. The Fleiss? (1971) kappa-score was
0.48 (moderate agreement). We used the excita-
tion score values as feature values. Excitation has
already been used in many works, such as causal-
ity and contradiction extraction (Hashimoto et al,
2012) or Why-QA (Oh et al, 2013).
Word Sentiment Polarity (WSP) As we sug-
gested before, full-fledged sentiment analysis to
recognize the expressions, including clauses and
phrases, that refer to something good or bad was
not effective in our task. However, the sentiment
polarity, assigned to single words turned out to
be effective. To identify the sentiment polarity
of words, we employed the word sentiment polar-
ity dictionary used with a sentiment analysis tool
for Japanese, the Opinion Extraction Tool soft-
ware2, which is an implementation of Nakagawa
et al (2010). The dictionary includes 9,030 posi-
tive and 27,951 negative words. Note that we used
the Opinion Extraction Tool in the experiments to
check the effectiveness of the full-fledged senti-
ment analysis in this task.
Semantic Word Class (SWC) We assume that
nouns in the same semantic class behave simi-
2Provided at the ALAGIN Forum (http://www.alagin.jp/).
larly in crisis situations. For example, if ?infec-
tion? appears in a problem report, the tweets in-
cluding ?pulmonary embolism? are also likely to
be problem reports. Semantic word class features
are used to capture such tendencies. We applied
an EM-style word clustering algorithm in Kazama
and Torisawa (2008) to 600 millionWeb pages and
clustered 1 million nouns into 500 classes. This
algorithm has been used in many works, such as
relation extraction (De Saeger et al, 2011) and
Why-QA (Oh et al, 2012), and can generate vari-
ous kinds of semantically clean word classes, such
as foods, disease names, and natural disasters. We
used the word classes in tweets as features.3
Geographical Locations (GL) Our location
recognizer matches tweets against our loca-
tion dictionary. Location names and their
existence/non-existence in tweets constitute evi-
dence, thus we encoded such information into our
features. The location dictionary was created from
the Japan Post code data4 and Wikipedia, contain-
ing 2.7 million location names including cities,
schools and other facilities (Kazama et al, 2013).
4 Problem-Aid Match Recognizer
After problem report and aid message recogni-
tion, the positive outputs of the respective classi-
fiers are used as input in this step. The problem-
aid match recognizer classifies an aid message-
nucleus pair together with the problem report-
nucleus pair employing SVMs with linear ker-
nel, which performed best in this task again. The
problem-aid match recognizer uses all the features
used in the problem report recognizer and the aid
message recognizer along with additional features
regarding: excitation polarity (EX) and trouble
expressions (TR), distributional similarity (SIM),
contradiction (CTP) and SVM-scores of the prob-
lem report and aid message recognizers (SSR).
Here also we attempted to capture typical or fre-
quent matches of nuclei using template and noun
IDs and their combinations, but we did not observe
any improvement so we omit them from the fea-
ture set. The bottom part of Table 2 summarizes
the additional feature set, some of which are de-
scribed below in more detail.
3There is a slight complication here. For each noun n, EM
clustering estimates a probability distribution P (n|c?) for n
and semantic class c?. From this distribution we obtained
discrete semantic word classes by assigning each noun n to
semantic class c = argmaxc? p(c?|n).
4http://www.post.japanpost.jp/zipcode/download.html
1623
As for TR and EX, our intuition is that if a prob-
lem nucleus and an aid nucleus are an adequate
match, their excitation polarities are opposite, as
described in Section 2. We encode whether the ex-
citation polarities of nuclei templates are the same
or not in our features. Also, the excitation polar-
ities of problem and aid nuclei and TR are com-
bined (TREX1, TREX2) so that the classifier can
know whether the nuclei follow the constraint for
adequate matches described in Section 2.
As for SIM, if an aid message matches a prob-
lem report, besides the common nucleus noun, it is
reasonable to assume that certain contexts are se-
mantically similar. We capture this characteristic
in three ways. SIM1 looks for common semantic
word classes in the problem report and aid mes-
sage. SIM2 and SIM3 target the modifiers of the
common nucleus noun if they exist.
We also observed that if an aid message matches
a problem report, the problem nucleus template
and aid nucleus template are often distributionally
similar. A typical example is ?X is sold out? and
?buy X?. SIM4 captures this tendency. As the dis-
tributional similarity between templates, we used
a Bayesian distributional similarity measure pro-
posed by Kazama et al (2010).5
CTP indicates whether the problem and aid nu-
clei are in contradiction relation or not. This fea-
ture was implemented based on the observation
that when problem and aid nuclei are in contradic-
tion relation, they are often proper matches (e.g.,
?blackout, ?X starts?? and ?blackout, ?X ends??).
CTP indicates whether nucleus pairs are in the
one million contradiction phrase pairs6 automat-
ically obtained by applying a method proposed by
Hashimoto et al (2012) to 600 million Web pages.
5 Experiments
We evaluated our problem report recognizer and
problem-aid match recognizer. For the sake of
space, we give only the performance figures of the
aid message recognizer at the end of Section 5.1.
We collected tweets posted during and after
the 2011 Great East Japan Earthquake, between
March 10 and April 4, 2011. After applying
keyword-based filtering with a list of over 300
5The original similarity was defined over noun pairs and it
was estimated from dependency relations. Obtaining similar-
ity between template pairs, not noun pairs, is straightforward
given the same dependency relations. We used 600 million
Web pages for this similarity estimation.
6The precision of the pairs was reported as around 70%.
disaster related keywords, we obtained 55 million
tweets. After dependency parsing7, we used them
in our evaluation.
5.1 Problem Report Recognition
Firstly, we evaluated our problem report recog-
nizer. Particularly, we assessed the effect of ex-
citation polarities and trouble expressions in two
settings. The first is against a naturally distributed
gold standard data. The second targets problem
reports with problem nuclei unseen in the training
data.
In both experiments we observed that the per-
formance drops when excitation polarities and
trouble expressions are removed from the feature
set. The performance drop was larger in the sec-
ond experiment which suggests that the excitation
polarities and trouble expressions are more effec-
tive against unseen problem reports.
Training and test data for problem report recog-
nition consist of tweet-nucleus candidate pairs
randomly sampled from our 55 million tweet data.
The training data (R) and test data (T ) consist of
13,000 and 1,000 pairs, respectively, manually la-
beled by three annotators (other than the authors)
as problem or other. Final judgment was made by
majority vote. The Fleiss? kappa score for train-
ing and test data for annotation judgement is 0.74
(substantial agreement).
Our problem report recognizer and its variants
are listed in Table 3. Table 4 shows the evalua-
tion results. The proposed method achieved about
44% recall and nearly 80% precision, outperform-
ing all other systems in terms of precision, F-score
and average precision8. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.05).9 Note that F-measure dropped
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here EX and TR denote all excitation po-
larity and trouble expression related features, respectively,
including their combinations (TREX1).
PROPOSED+OET: The proposed method incorporating the
classification results of problem nucleus candidates by the
Opinion Extraction Tool as additional binary features.
RULE-BASED: The method that regards only nuclei satisfying
the constraint in Table 1 as problem nuclei.
Table 3: Evaluated problem report recognizers.
7http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
8We calculate average precision using the formula: aP =?n
k=1
(Prec(k)?rel(k))
n , where Prec(k) is the precision atcut-off k and rel(k) is an indicator function equaling 1 if
the item at rank k is relevant, zero otherwise.
9Throughout this paper we performed two-tailed test of
1624
Recognition system R (%) P (%) F (%) aP (%)
PROPOSED 44.26 79.41 56.83 71.82
PROPOSED-TR&EX 45.08 74.83 56.26 69.67
PROPOSED-EX 44.67 74.66 55.89 69.90
PROPOSED-TR 43.85 74.31 55.15 69.44
PROPOSED-MSA 28.69 70.71 40.81 57.74
PROPOSED-SWC 43.42 75.97 55.25 70.61
PROPOSED-WSP 43.14 77.83 55.50 70.45
PROPOSED-REQ 42.64 76.16 55.50 54.67
PROPOSED-GL 44.14 78.34 55.50 56.46
PROPOSED+OET 44.24 79.41 56.82 71.81
RULE-BASED 30.32 67.96 41.93 n/a
Table 4: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem report rec-
ognizers.
whenever each type of feature was removed, im-
plying that each type of feature is effective in this
task. Especially note the performance drop if we
remove excitation polarities (EX), trouble expres-
sion (TR) and both excitation and trouble expres-
sion features (TR&EX), confirming that they are
crucial in recognizing problem reports with high
accuracy. Also note that the performance of PRO-
POSED+OET was actually slightly worse than that
of the proposed method. This suggests that full-
fledged sentiment analysis is not effective at least
in this setting. The rule-based method achieved
relatively high precision despite of the low recall,
demonstrating the importance of problem and aid
nuclei formulations described in Section 1.
The second experiment assessed the efficiency
of our problem report recognizer against unseen
problem nuclei under the condition that every tem-
plate in nuclei has excitation polarity. We sampled
the training and test data so that the problem nu-
cleus nouns and templates in the training and test
data are disjoint. First we created a subset of the
test data by selecting the samples which had nu-
clei with excitation templates. We call this sub-
set T ?. Next, we removed samples from training
data R if either of their problem nouns or tem-
plates appeared in the nuclei of T ?. The result-
ing new training data (called R?) and test data (T ?)
consist of 6,484 and 407 tweet-nucleus candidate
pairs, respectively. We trained our problem report
recognizer using R? and tested its performance us-
ing T ?. Figure 2 shows the precision-recall curves
obtained by changing the threshold on the SVM
scores. The effectiveness of excitation polarities
and trouble expressions was more evident in this
setting. The PROPOSED?s performance was ac-
tually better in this setting (almost 50% recall at
population proportion (Ott and Longnecker, 2010) using
SVM-threshold=0.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pre
cisi
on
Recall
PROPOSED-TRPROPOSED-EX PROPOSED-TR&EXPROPOSED
Figure 2: Precision-recall curves of problem re-
port recognizers against unseen problem nuclei.
more that 80% precision), than the previous set-
ting, showing that excitation templates and trouble
expressions are crucial in achieving high perfor-
mance especially for unseen problem nuclei. The
same was confirmed when we removed excitation
polarity and trouble expression related features,
with performance dropping by 7.43 points in terms
of average precision. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.01). This implies, assuming that we
have a wide-coverage dictionary of templates with
excitation polarities, that excitation polarities are
important in dealing with unexpected problems in
disaster situations.
We also evaluated the aid-message recognizer,
using tweet-nucleus pairs in R and T as train-
ing and test data and the annotation scheme was
also the same. The average Fleiss? kappa score
was 0.55 (moderate agreement). Our recognizer
achieved 53.82% recall and 65.67% precision and
showed similar tendencies with the problem report
recognizer, with the excitation polarities and trou-
ble expressions contributing to higher accuracy.
We can conclude that excitation polarities and
trouble expressions are important in identifying
problem reports and aid messages during disaster
situations.
5.2 Problem-Aid Matching
Next, we evaluated the performance of the
problem-aid match recognizer. We applied our
problem report recognizer and aid message recog-
nizer to all 55 million tweets and combined the
tweet-nucleus pairs judged as problem reports and
aid messages, respectively, to create the training
and test data.
The training data consists of two parts (M1 and
M2). M1 includes many variations of the aid
messages for each problem report, while M2 en-
1625
sures diversity in nouns and templates in problem
nuclei. For M1, we randomly picked up problem
reports from the output of the problem report rec-
ognizer and to each we attached up to 30 randomly
picked, distinct aid messages that have the same
nucleus noun. Building M2 follows the construc-
tion method of M1, except that: (1) we used up
to 30 distinct problem nuclei for each noun; (2)
for each problem report we attached only one ran-
domly picked aid message.
In creating the test data T2, we followed the
construction method used for M2 to assess the
performance of our proposal with a large variety
of problems. M1, M2 and T2 consist of 3,000,
6,000 and 1,000 samples, respectively. The an-
notation was done by majority vote of three hu-
man annotators (other than the authors), the aver-
age Fleiss? kappa-score for training and test data
was 0.63 (substantial agreement).
We trained the problem-aid match recognizers
of Table 5 with M1 and M2. The evaluation
results performed on T2 are shown in Table 6.
We can observe that, among the nuclei related
features, the trouble expression (TR) and excita-
tion polarity (EX) features and their combination
(TR&EX) contribute most to the performance, al-
though the contribution of nuclei related features
is less in comparison to the problem report and aid
message recognition. The improvement in preci-
sion when using TR&EX is marginally significant
(p = 0.056). Instead, morphological and syntactic
analysis (MSA) and semantic word class (SWC)
features greatly improved performance.
As the final experiments, we evaluated top-
ranking matches of our problem-aid match recog-
nizer, where the recognizer classified all the possi-
ble combinations of tweet-nuclei pairs taken from
55 million tweets. In addition, we assessed the ef-
fectiveness of excitation polarities and trouble ex-
pressions by comparing all positive matches pro-
duced by our full problem-aid match recognizer
(PROPOSED) and those produced by the problem-
aid match recognizer (PROPOSED-TR&EX) that
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here also EX and TR denote all excitation
polarity and trouble expression related features, respec-
tively, including their combinations (TREX1 and TREX2).
RULE-BASED: The method that judges only problem-aid nuclei
combinations with opposite excitation polarities as proper
matches.
Table 5: Evaluated problem-aid match recogniz-
ers.
Matching system R (%) P (%) F (%) aP (%)
PROPOSED 30.67 70.42 42.92 55.16
PROPOSED-TR&EX 28.83 67.14 40.33 53.99
PROPOSED-EX 31.29 67.11 42.68 54.19
PROPOSED-TR 30.56 69.33 42.42 54.85
PROPOSED-MSA 13.50 53.66 21.57 44.52
PROPOSED-SWC 26.99 67.69 38.59 52.23
PROPOSED-WSP 30.61 69.51 42.50 54.81
PROPOSED-CTP 30.06 70.00 42.05 54.94
PROPOSED-SIM 29.95 70.11 41.97 54.98
PROPOSED-REQ 30.58 70.25 42.61 54.67
PROPOSED-GL 30.61 70.31 42.65 55.02
PROPOSED-SSR 30.67 69.44 42.72 54.91
RULE-BASED 15.33 17.36 16.28 n/a
Table 6: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem-aid match
recognizers.
did not use excitation polarities and trouble ex-
pressions in its feature set. Note that PROPOSED-
TR&EX was fed by the problem report and aid
message recognizers that didn?t use excitation po-
larities and trouble expressions. For both systems?
training data we used R for the problem report
and aid message recognizers; M1 and M2 for the
problem-aid matching recognizers.
PROPOSED and PROPOSED-TR&EX output 15.2
million and 13.4 million positive matches, cover-
ing 1,691 and 1,442 nucleus nouns, respectively.
Table 7 shows match samples identified with PRO-
POSED. We observed that the output of each sys-
tem was dominated by just a handful of frequent
nucleus nouns, such as ?water? or ?gasoline?. We
preferred to assess the performance of our system
against a large variation of problem-aid nuclei,
thus we restricted the number of matches to 10
for each noun10. After this restriction the number
of matches found by PROPOSED and PROPOSED-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000  9000
Pre
cisi
on
Rank
PROPOSED (unseen)PROPOSED-TR&EX (unseen)PROPOSED (all)PROPOSED-TR&EX (all)
Figure 3: Problem-aid match recognition perfor-
mance for ?all? and ?unseen? problem reports.
10Note that this setting is a pessimistic estimation of our
system?s overall performance, since according to our obser-
vations problem reports with very frequent nucleus nouns had
proper matches with a higher accuracy than problem reports
with less frequent nucleus nouns.
1626
Problem report: ???????????????????
??????????????????????????
??????????????????????????
(Starting from the 17th, the Iwaki Joban Hospital, the Iwaki
Urology Clinique, the Takebayashi Sadakichi Memorial Clin-
ique and the Izumi Central Clinique have all suspended dial-
ysis sessions. Patients are advised to urgently make contact.)
Aid message: ???????????????????
??????????????????????????
(Restart of dialysis sessions: short dialysis sessions are
available at the Iwaki Urology Clinique between 9 AM and
4 PM.)
Problem report: ??????????????????
?????????????????????????
?????????????????????????
?????
(Please spread this message. According to my father in
Sendai, there are more and more people whose phones ran
out of battery. We need phone chargers!)
Aid message: ???????????????????
???????????
([Please spread] At the City Hall of Wakabayashi-ku, Sendai,
you can recharge your phone battery.)
Table 7: Examples from the output of the proposed
method in the ?all? setting. Problem report and aid
message nuclei are boldfaced in the English trans-
lations.
TR&EX was 8,484 and 7,363, respectively.
The performance of PROPOSED and
PROPOSED-TR&EX were assessed in two
settings: ?all? and ?unseen?. For ?all?, we selected
400 problem-aid matches from the outputs of the
respective systems after applying the 10-match
restriction. For ?unseen?, first we removed the
samples from the systems? outputs if either the
nucleus noun or template pair appear in the nuclei
of the problem-aid match recognizers? training
data. Next we applied the same sampling process
as with ?all?. Three annotators (other than the
authors) manually labeled the sample sets, final
judgment being made by majority vote. The
Fleiss? kappa score for all test data was 0.73
(substantial agreement).
Figure 3 shows the systems? precision curves,
drawn from the samples whose X-axis positions
represent the ranks according to SVM scores. In
both scenarios we can confirm that excitation po-
larity and trouble expression related features con-
tribute to this task. In the ?all? setting in terms
of average precision calculated over the top 7,200
matches, PROPOSED?s 62.36% is 10.48 points
higher than that of PROPOSED-TR&EX. For un-
seen problem/aid nuclei PROPOSED method?s av-
erage precision of 58.57% calculated at the top
3,800 matches is 5.47 points higher than that of
PROPOSED-TR&EX at the same data point. The
improvement in precision when using TR&EX is
statistically significant in both settings (p < 0.01).
6 Related Work
Twitter has been observed as a platform for situ-
ational awareness during various crisis situations
(Starbird et al, 2010; Vieweg et al, 2010), as sen-
sors for an earthquake reporting system (Sakaki et
al., 2010; Okazaki and Matsuo, 2010) or to de-
tect epidemics (Aramaki et al, 2011). Besides
Twitter, blogs or forums have also been the tar-
get of community response analysis (Qu et al,
2009; Torrey et al, 2007). Similar to our work
are the ones of Neubig et al (2011) and Ishino et
al. (2012), who tackle specific problems that occur
during disasters (i.e., safety information and trans-
portation information, respectively); and Munro
(2011), who extracted ?actionable messages? (re-
quests and aids, indiscriminately), matching being
performed manually. Our work differs from (Neu-
big et al, 2011) and (Ishino et al, 2012) in that we
do not restrict the range of problem reports, and as
opposed to (Munro, 2011), matching is automatic.
Systems such as that of Seki (2011)11 or Munro
(2013)12 are successful examples of crisis crowd-
sourcing, but these require extensive human inter-
vention to coordinate useful information.
Another category of related work relevant to our
task is troubleshooting. Baldwin et al (2007) and
Raghavan et al (2010) use discussion forums to
solve technical problems using supervised learn-
ing methods, but these approaches presume that
the solution of a specific problem is within the
same thread. In our work we do not employ struc-
tural characteristics of tweets as restrictions (e.g.,
a problem report and its aid message need to be in
the same tweet chain).
7 Conclusions
In this paper, we proposed a method to dis-
cover matches between problem reports and aid
messages from tweets in large-scale disasters.
Through a series of experiments, we demonstrated
that the performance of the problem-aid match-
ing can be improved with the usage of seman-
tic orientation of excitation polarities, proposed in
(Hashimoto et al, 2012), and trouble expressions.
We are planning to deploy our system and re-
lease model files of the classifiers to assist relief
efforts in future crisis scenarios.
11http://www.sinsai.info/
12http://www.mission4636.org/
1627
References
Adam Acar and Yuya Muraki. 2011. Twitter for cri-
sis communication: Lessons learned from Japan?s
tsunami disaster. International Journal of Web
Based Communities, 7(3):392?402.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using Twitter. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1568?1576.
Timothy Baldwin, David Martinez, and Richard B.
Penman. 2007. Automatic thread classification for
Linux user forum information access. In Proceed-
ings of the 12th Australasian Document Computing
Symposium (ADCS 2007), pages 72?79.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi
Kazama. 2008. Looking for trouble. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING 2008), pages 185?
192.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 5:378?382.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama.
2012. Excitatory or inhibitory: A new semantic
orientation extracts contradiction and causality from
the web. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2012), pages 619?630.
Aya Ishino, Shuhei Odawara, Hidetsugu Nanba, and
Toshiyuki Takezawa. 2012. Extracting transporta-
tion information and traffic problems from tweets
during a disaster: Where do you evacuate to? In
Proceedings of the Second International Conference
on Advances in Information Mining and Manage-
ment (IMMM 2012), pages 91?96.
Hiroshi Kanayama and Tetsuya Nasukawa. 2008. Tex-
tual demand analysis: Detection of users? wants and
needs from opinions. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING 2008), pages 409?416.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
Bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 247?256.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
Jun Goto, and Istva?n Varga. 2013. Saigaiji jouhou
e no shitsumon outo shisutemu no tekiyou no koko-
romi. (An attempt for applying question-answering
system on disaster related information). In Pro-
ceeding of the Nineteenth Annual Meeting of The
Association for Natural Language Processing. (in
Japanese).
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language Analysis in Social Me-
dia (LASM 2012), pages 27?36.
Robert Munro. 2011. Subword and spatiotempo-
ral models for identifying actionable information in
Haitian Kreyol. In Proceedings of the Fifteenth
Conference on Computational Natural Language
Learning (CoNLL-2011), pages 68?77.
Robert Munro. 2013. Crowdsourcing and the
crisis-affected community. Information Retrieval,
16(2):210?266.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL HLT
2010), pages 786?794.
National Police Agency of Japan. 2013. Damage sit-
uation and public countermeasures associated with
2011 Tohoku district ? off the Pacific Ocean Earth-
quake. http://www.npa.go.jp/archive/
keibi/biki/higaijokyo_e.pdf. (accessed
on 30 April, 2013).
Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety infor-
mation mining? what can NLP do in a disaster?.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 965?973.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2012), pages 368?378.
1628
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013).
Kiyonori Ohtake, Kentaro Torisawa, Jun Goto, and
Stijn De Saeger. 2013. Saigaiji ni okeru hi-
saisha to kyuuen kyuujosha kan no souhoko komyu-
nikeeshon. (Bi-directional communication between
victims and rescures during a crisis). In Proceeding
of the Nineteenth Annual Meeting of The Association
for Natural Language Processing. (in Japanese).
Makoto Okazaki and Yutaka Matsuo. 2010. Seman-
tic Twitter: Analyzing tweets for real-time event
notification. In Proceedings of the 2008/2009 in-
ternational conference on Social software: Re-
cent trends and developments in social software
(BlogTalk 2008), pages 63?74.
R. Lyman Ott and Michael T. Longnecker, 2010. An
Introduction to Statistical Methods and Data Analy-
sis, chapter 10.2. Brooks Cole, 6th edition.
Yan Qu, Philip Fei Wu, and Xiaoqing Wang. 2009.
Online community response to major disaster: A
study of Tianya forum in the 2008 Sichuan Earth-
quake. In 42st Hawaii International International
Conference on Systems Science (HICSS-42), pages
1?11.
Preethi Raghavan, Rose Catherine, Shajith Ikbal,
Nanda Kambhatla, and Debapriyo Majumdar. 2010.
Extracting problem and resolution information from
online discussion forums. In Proceedings of the
16th International Conference on Management of
Data (COMAD 2010).
Takeo Saijo. 2012. Hito-o tasukeru sungoi shikumi. (A
stunning system that saves people). Diamond Inc.
(in Japanese).
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web (WWW 2010), pages 851?860.
Motoki Sano, Istva?n Varga, Jun?ichi Kazama, and Ken-
taro Torisawa. 2012. Requests in tweets dur-
ing a crisis: A systemic functional analysis of
tweets on the Great East Japan Earthquake and
the Fukushima Daiichi nuclear disaster. In Pa-
pers from the 39th International Systemic Func-
tional Congress (ISFC39), pages 135?140.
Haruyuki Seki. 2011. Higashi-nihon daishinsai fukkou
shien platform sinsai.info no naritachi to kongo no
kadai. (The organizational structure of sinsai.info
restoration support platform for the 2011 Great East
Japan Earthquake and future challenges). Journal of
digital practices, 2(4):237?241. (in Japanese).
Kate Starbird, Leysia Palen, Amanda L. Hughes, and
Sarah Vieweg. 2010. Chatter on the red: What
hazards threat reveals about the social life of mi-
croblogged information. In Proceedings of The
2010 ACM Conference on Computer Supported Co-
operative Work (CSCW 2010), pages 241?250.
Cristen Torrey, Moira Burke, Matthew L. Lee,
Anind K. Dey, Susan R. Fussell, and Sara B. Kiesler.
2007. Connected giving: Ordinary people coordi-
nating disaster relief on the Internet. In Proceedings
of the 40th Annual Hawaii International Conference
on System Sciences (HICSS-40), pages 179?188.
Katerina Tsagkalidou, Vassiliki Koutsonikola, Athena
Vakali, and Konstantinos Kafetsios. 2011. Emo-
tional aware clustering on micro-blogging sources.
In Proceedings of the 4th international conference
on Affective computing and intelligent interaction
(ACII 2011), pages 387?396.
Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two nat-
ural hazards events: What Twitter may contribute
to situational awareness. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems (CHI 2010), pages 1079?1088.
Patrick Winn. 2011. Japan tsunami disaster: As Japan
scrambles, Twitter reigns. GlobalPost, 18 March.
1629
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733?1743,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Why-Question Answering
using Intra- and Inter-Sentential Causal Relations
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ? Motoki Sano?
Stijn De Saeger? Kiyonori Ohtake?
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{?rovellia,? torisawa,? ch,? msano,?stijn,?kiyonori.ohtake}@nict.go.jp
Abstract
In this paper, we explore the utility of
intra- and inter-sentential causal relations
between terms or clauses as evidence for
answering why-questions. To the best of
our knowledge, this is the first work that
uses both intra- and inter-sentential causal
relations for why-QA. We also propose
a method for assessing the appropriate-
ness of causal relations as answers to a
given question using the semantic orienta-
tion of excitation proposed by Hashimoto
et al (2012). By applying these ideas
to Japanese why-QA, we improved preci-
sion by 4.4% against all the questions in
our test set over the current state-of-the-
art system for Japanese why-QA. In addi-
tion, unlike the state-of-the-art system, our
system could achieve very high precision
(83.2%) for 25% of all the questions in the
test set by restricting its output to the con-
fident answers only.
1 Introduction
?Why-question answering? (why-QA) is a task to
retrieve answers from a given text archive for a
why-question, such as ?Why are tsunamis gen-
erated?? The answers are usually text fragments
consisting of one or more sentences. Although
much research exists on this task (Girju, 2003;
Higashinaka and Isozaki, 2008; Verberne et al,
2008; Verberne et al, 2011; Oh et al, 2012), its
performance remains much lower than that of the
state-of-the-art factoid QA systems, such as IBM?s
Watson (Ferrucci et al, 2010).
In this work, we propose a quite straightfor-
ward but novel approach for such difficult why-
QA task. Consider the sentence A1 in Table 1,
which represents the causal relation between the
cause, ?the ocean?s water mass ..., waves are gen-
A1 [Tsunamis that can cause large coastal inundation
are generated]effect because [the ocean?s water
mass is displaced and, much like throwing a stone
into a pond, waves are generated.]cause
A2 [Earthquake causes seismic waves which set up
the water in motion with a large force.]cause
This causes [a tsunami.]effect
A3 [Tsunamis]effect are caused by [the sudden dis-
placement of huge volumes of water.]cause
A4 [Tsunamis weaken as they pass through
forests]effect because [the hydraulic resistance of
the trees diminish their energy.]cause
A5 [Automakers in Japan suspended production for an
array of vehicles]effect because [the magnitude 9
earthquake and tsunami hit their country on Friday,
March 11, 2011.]cause
Table 1: Examples of intra/inter-sentential causal
relations. Cause and effect parts of each causal re-
lation, marked with [..]cause and [..]effect, are con-
nected by the underlined cue phrases for causality,
such as because, this causes, and are caused by.
erated,? and its effect, ?Tsunamis ... are gener-
ated.? This is a good answer to the question, ?Why
are tsunamis generated??, since the effect part is
more or less equivalent to the (propositional) con-
tent of the question. Our method finds text frag-
ments that include such causal relations with an
effect part that resembles a given question and pro-
vides them as answers.
Since this idea looks quite intuitive, many peo-
ple would probably consider it as a solution to
why-QA. However, to our surprise, we could not
find any previous work on why-QA that took this
approach. Some methods utilized the causal re-
lations between terms as evidence for finding an-
swers (i.e., matching a cause term with an answer
text and its effect term with a question) (Girju,
2003; Higashinaka and Isozaki, 2008). Other ap-
proaches utilized such clue terms for causality as
?because? as evidence for finding answers (Mu-
rata et al, 2007). However, these algorithms did
not check whether an answer candidate, i.e., a text
fragment that may be provided as an answer, ex-
plicitly contains a complex causal relation sen-
1733
tence with the effect part that resembles a ques-
tion. For example, A5 in Table 1 is an incorrect an-
swer to ?Why are tsunamis generated??, but these
previous approaches would probably choose it as a
proper answer due to ?because? and ?earthquake?
(i.e., a cause of tsunamis). At least in our exper-
imental setting, our approach outperformed these
simpler causality-based QA systems.
Perhaps this approach was previously deemed
infeasible due to two non-trivial technical chal-
lenges. The first challenge is to accurately iden-
tify a wide range of causal relations like those in
Table 1 in answer candidates. To meet this chal-
lenge, we developed a sequence labeling method
that identifies not only intra-sentential causal re-
lations, i.e., the causal relations between two
terms/phrases/clauses expressed in a single sen-
tence (e.g., A1 in Table 1), but also the inter-
sentential causal relations, which are the causal
relations between two terms/phrases/clauses ex-
pressed in two adjacent sentences (e.g., A2) in a
given text fragment.
The second challenge is assessing the appropri-
ateness of each identified causal relation as an an-
swer to a given question. This is important since
the causal relations identified in the answer candi-
dates may have nothing to do with a given ques-
tion. In this case, we have to reject these causal
relations because they are inappropriate as an an-
swer to the question. When a single answer candi-
date contains many causal relations, we also have
to select the appropriate ones. Consider the causal
relations in A1?A4. Those in A1?A3 are appro-
priate answers to ?Why are tsunamis generated??,
but not the one in A4. To assess the appropri-
ateness, the system must recognize textual entail-
ment, i.e., ?tsunamis (are) generated? in the ques-
tion is entailed by all ?tsunamis are generated? in
A1, ?cause a tsunami? in A2 and ?tsunamis are
caused? in A3 but not by ?tsunamis weaken? in
A4. This quite difficult task is currently being
studied by many researchers in the RTE field (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
al., 2010; Shima et al, 2011; Bentivogli et al,
2011). To meet this challenge, we developed a
relatively simple method that can be seen as a
lightweight approximation for this difficult RTE
task, using excitation polarities (Hashimoto et al,
2012).
Through our experiments on Japanese why-QA,
we show that a combination of the above methods
can improve why-QA accuracy. In addition, our
proposed method can be successfully combined
with other approaches to why-QA and can con-
tribute to higher accuracy. As a final result, we im-
proved the precision by 4.4% against all the ques-
tions in our test set over the current state-of-the-art
system of Japanese why-QA (Oh et al, 2012). The
difference in the performance became much larger
when we only compared the highly confident an-
swers of each system. When we made our sys-
tem provide only its confident answers according
to their confidence score given by our system, the
precision of these confident answers was 83.2%
for 25% of all the questions in our test set. In the
same setting, the precision of the state-of-the-art
system (Oh et al, 2012) was only 62.4%.
2 Related Work
Although there were many previous works on the
acquisition of intra- and inter-sentential causal re-
lations from texts (Khoo et al, 2000; Girju, 2003;
Inui and Okumura, 2005; Chang and Choi, 2006;
Torisawa, 2006; Blanco et al, 2008; De Saeger et
al., 2009; De Saeger et al, 2011; Riaz and Girju,
2010; Do et al, 2011; Radinsky et al, 2012), their
application to why-QA was limited to causal re-
lations between terms (Girju, 2003; Higashinaka
and Isozaki, 2008).
As previous attempts to improve why-QA per-
formance, such semantic knowledge as Word-
Net synsets (Verberne et al, 2011), semantic
word classes (Oh et al, 2012), sentiment analy-
sis (Oh et al, 2012), and causal relations between
terms (Girju, 2003; Higashinaka and Isozaki,
2008) has been used. These previous studies took
basically bag-of-words approaches and used the
semantic knowledge to identify certain seman-
tic associations using terms and n-grams. On
the other hand, our method explicitly identifies
intra- and inter-sentential causal relations between
terms/phrases/clauses that have complex struc-
tures and uses the identified relations to answer
a why-question. In other words, our method
considers more complex linguistic structures than
those used in the previous studies. Note that our
method can complement the previous approaches.
Through our experiments, we showed that it is
possible to achieve a higher precision by combin-
ing our proposed method with bag-of-words ap-
proaches considering semantic word classes and
sentiment analysis in our previous work (Oh et al,
1734
Document	 ?retrieval	 ?from	 ?Japanese	 ?web	 ?texts	 ?
Answer	 ?candidate	 ?extrac?on	
Answer	 ?candidate	 ?extrac?on	 ?	 ?from	 ?the	 ?retrieved	 ?documents	 ?
Answer	 ?re-??ranker	 ?
Answer	 ?re-??ranking	
top-n answer	 ?candidates	 ?by	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?causal	 ?rela?on	 ?recogni?on	 ?
Causal	 ?rela?on	 ?recogni?on	 ?model	 ?top-n answer	 ?candidates	 ?
training	 ?
training	 ?
Why-??ques?on	 ?
recogni?on	 ?	 ?
recogni?on	 ?	 ?
Figure 1: System architecture
2012).
3 System Architecture
We first describe the system architecture of
our QA system before describing our proposed
method. It is composed of two components: an-
swer candidate extraction and answer re-ranking
(Fig. 1). This architecture is basically the same as
that used in our previous work (Oh et al, 2012).
We extended our previous work by introducing
causal relations recognized from answer candi-
dates to the answer re-ranking. The features used
in our previous work are very different from those
in this work, and we found that combining both
improves accuracy.
Answer candidate extraction: In our previous
work, we implemented the method of Murata et
al. (2007) for our answer candidate extractor. We
retrieved documents from Japanese web texts us-
ing Boolean AND and OR queries generated from
the content words in why-questions. Then we ex-
tracted passages of five sentences from these re-
trieved documents and ranked them with the rank-
ing function proposed by Murata et al (2007).
This method ranks a passage higher when it con-
tains more query terms that are closer to each other
in the passage. We used a set of clue terms, includ-
ing the Japanese counterparts of cause and reason,
as query terms for the ranking. The top ranked
passages are regarded as answer candidates in the
answer re-ranking. See Murata et al (2007) for
more details.
Answer re-ranking: Re-ranking the answer
candidates is done by a supervised classifier
(SVMs) (Vapnik, 1995). In our previous work, we
employed three types of features for training the
re-ranker: morphosyntactic features (n-grams of
morphemes and syntactic dependency chains), se-
mantic word class features (semantic word classes
obtained by automatic word clustering (Kazama
and Torisawa, 2008)) and sentiment polarity fea-
tures (word and phrase polarities). Here, we used
semantic word classes and sentiment polarities for
identifying such semantic associations between a
why-question and its answer as ?if a disease?s
name appears in a question, then answers that in-
clude nutrient names are more likely to be correct?
by semantic word classes and ?if something un-
desirable happens, the reason is often also some-
thing undesirable? by sentiment polarities. In this
work, we propose causal relation features gener-
ated from intra- and inter-sentential causal rela-
tions in answer candidates and use them along
with the features proposed in our previous work
for training our re-ranker.
4 Causal Relations for Why-QA
We describe causal relation recognition in Sec-
tion 4.1 and describe the features (of our re-ranker)
generated from causal relations in Section 4.2.
4.1 Causal Relation Recognition
We restrict causal relations to those expressed by
such cue phrases for causality as (the Japanese
counterparts of) because and as a result like in
the previous work (Khoo et al, 2000; Inui and
Okumura, 2005) and recognize them in the fol-
lowing two steps: extracting causal relation candi-
dates and recognizing causal relations from these
candidates.
4.1.1 Extracting Causal Relation Candidates
We identify cue phrases for causality in answer
candidates using the regular expressions in Ta-
ble 2. Then, for each identified cue phrase, we
extract three sentences as a causal relation candi-
date, where one contains the cue phrase and the
other two are the previous and next sentences in
the answer candidates. When there is more than
one cue phrase in an answer candidate, we use
all of them for extracting the causal relation can-
didates, assuming that each of the cue phrases is
linked to different causal relations. We call a cue
phrase used for extracting a causal relation candi-
date a c-marker (causality marker) of the candi-
date to distinguish it from the other cue phrases in
the same causal relation candidate.
1735
Regular expressions Examples
(D|?)? ?? P? ?? (for),??? (for),????
(as a result),???? (for)
?? ?? (since or because of)
?? (??|?) ???? (from the fact that),??
? (by the fact that)
(??|??) C ??? (because),??? (It is be-
cause)
D? RCT (P|C)+ ??? (the reason is), ???
(is the cause),?????? (from
this reason)
Table 2: Regular expressions for identifying cue
phrases for causality. D, P and C represent
demonstratives (e.g., ?? (this) and ?? (that)),
postpositions (including case markers such as ?
(nominative), ? (genitive)), and copula (e.g., ?
? (is) and ??? (is)) in Japanese, respectively.
RCT, which represents Japanese terms meaning
reason, cause, or thanks to, is defined as fol-
lows: RCT = {?? (reason), ?? (cause), ?
? (cause), ??? (cause), ??? (thanks to),
?? (thanks to),?? (reason) }.
4.1.2 Recognizing Causal Relations
Next, we recognize the spans of the cause and ef-
fect parts of a causal relation linked to a c-marker.
We regard this task as a sequence labeling problem
and use Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001) as a machine learning frame-
work. In our task, CRFs take three sentences
of a causal relation candidate as input and gen-
erate their cause-effect annotations with a set of
possible cause-effect IOB labels, including Begin-
Cause (B-C), Inside-Cause (I-C), Begin-Effect (B-
E), Inside-Effect (I-E), and Outside (O). Fig 2
shows an example of such sequence labeling. Al-
though this example is about sequential labeling
shown on English sentences for ease of explana-
tion, it was actually done on Japanese sentences.
We used the three types of feature sets in Table 3
for training the CRFs, where j is in the range of
i? 4 ? j ? i+4 for current position i in a causal
relation candidate.
Type Features
Morphological feature mj , mj+1j , posj , posj+1j
Syntactic feature sj , sj+1j , bj , bj+1j
C-marker feature (mj , cm), (mj+1j , cm)
(sj , cm), (sj+1j , cm)
Table 3: Features for training CRFs, where
xj+1j = xjxj+1
Morphological features: mj and posj in Ta-
ble 3 represent the jth morpheme and the POS tag.
S1:	 ?Earthquake	 ?causes	 ?seismic	 ?waves	 ?which	 ?set	 ?up	 ?the	 ?water	 ?in	 ?mo?on	 ?with	 ?a	 ?large	 ?force.	 ?EOS	 ?S2:	 ?This	 ?causes	 ?a	 ?tsunami.	 ?EOS	 ?S3:	 ?EOA	 ?
S1	 ? Earthquake	 ? causes	 ? ?	 ? with	 ? a	 ? large	 ? force	 ? .	 ? EOS	 ?IOB	 ? B-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? O	 ?S2	 ? This	 ? causes	 ? a	 ? tsunami	 ? .	 ? EOS	 ?IOB	 ? O	 ? O	 ? B-??E	 ? I-??E	 ? I-??E	 ? O	 ?S3	 ? EOA	 ?IOB	 ? O	 ?
CRFs	 ?
A	 ?causal	 ?rela?on	 ?candidate	 ?from	 ?A2	 ?
Figure 2: Recognizing causal relations by se-
quence labeling: Underlined text This causes rep-
resents a c-marker, and EOS and EOA represent
end-of-sentence and end-of-answer candidates.
??	 ? ??	 ? ???	 ? ???	 ? ???????	 ? ???	 ? ??	 ? ????????	 ?
water	 ? ice	 ? if	 ?(it)	 ?becomes	 ? its	 ?volume	 ? because	 ?(it)	 ?	 ?increases	 ? an	 ?iceberg	 ? water	 ? float	 ?on	 ?(water)	 ?
Subtree	 ?informa?on	 ?used	 ?for	 ?syntac?c	 ?features	 ?	 ?
subtree	 ? subtree	 ? child	 ? child	 ? c-??marker	 ? subtree-??of-??parent	 ?
subtree-??of-??parent	 ?
parent	 ?
???????	 ?
???	 ?
[??????????????]cause???[?????????????]effect	 ?(Because	 ?[the	 ?volume	 ?of	 ?the	 ?water	 ?increases	 ?if	 ?it	 ?becomes	 ?ice]cause,	 ?[an	 ?iceberg	 ?floats	 ?on	 ?water]effect.)	 ?
????????	 ?
root	 ?
c-??marker	 ?node	 ?
Figure 3: Example of syntactic information related
to a c-marker used for syntactic features
We use JUMAN1, a Japanese morphological ana-
lyzer, for generating our morphological features.
Syntactic features: The span of the causal rela-
tions in a given causal relation candidate strongly
depends on the c-marker in the candidate. Es-
pecially for intra-sentential causal relations, their
cause and effect parts often appear in the subtrees
of the c-marker?s node or those of the c-marker?s
parent node in a syntactic dependency tree struc-
ture. Fig. 3 shows an example that follows this ob-
servation, where the c-marker node is represented
in a hexagon and the other nodes are in a rectan-
gle. Note that each node in Fig. 3 is a word phrase
(called a bunsetsu), which is the smallest unit of
syntactic analysis in Japanese. A bunsetsu is a
syntactic constituent composed of a content word
and several function words such as postpositions
and case markers. Syntactic dependency is repre-
sented by an arrow in Fig. 3. For example, there
is syntactic dependency from word phrase ??
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
1736
(water) to??? (if (it) becomes), i.e.,?? dep???
???. We encode this subtree information into
sj , which is the syntactic information of a word
phrase to which the jth morpheme belongs. sj
only has one of six values: 1) the c-marker?s node
(c-marker), 2) the c-marker?s child node (child),
3) the c-marker?s parent node (parent), 4) in the c-
marker?s subtree but not the c-marker?s child node
(subtree), 5) in the subtree of the c-marker?s par-
ent node but not the c-marker?s node (subtree-of-
parent) and 6) the others (others). bj is the word
phrase information of the jth morpheme (mj) that
represents whether mj is in the beginning or in-
side a word phrase. For generating our syntactic
features, we use KNP2, a Japanese syntactic de-
pendency parser.
C-marker features: As our c-marker features,
we use a pair composed of c-marker cm and one
of the following: mj , mj+1j , sj , or sj+1j .
4.2 Causal Relation Features
We use terms, partial trees (in a syntactic depen-
dency tree structure), and the semantic orienta-
tion of excitation (Hashimoto et al, 2012) to as-
sess the appropriateness of each causal relation ob-
tained by our causal relation recognizer as an an-
swer to a given question. Finding answers with
term matching and partial tree matching has been
used in the literature of question answering (Girju,
2003; Narayanan and Harabagiu, 2004; Moschitti
et al, 2007; Higashinaka and Isozaki, 2008; Ver-
berne et al, 2008; Surdeanu et al, 2011; Verberne
et al, 2011; Oh et al, 2012), while that with the
excitation polarity is proposed in this work.
We use three types of features. Each fea-
ture type expresses the causal relations in an an-
swer candidate that are determined to be appro-
priate as answers to a given question by term
matching (tf1?tf4), partial tree matching (pf1?
pf4) and excitation polarity matching (ef1?ef4).
We call these causal relations used for generating
our causal relation features candidates of an ap-
propriate causal relation in this section. Note that
if one answer candidate has more than one candi-
date of an appropriate causal relation found by one
matching method, we generated features for each
appropriate candidate and merged all of them for
the answer candidate.
2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
Type Description
tf1 word n-grams of causal relations
tf2 word class version of tf1
tf3 indicator for the existence of candidates of an
appropriate causal relation identified by term
matching in an answer candidate
tf4 number of matched terms in candidates of an ap-
propriate causal relation
pf1 syntactic dependency n-grams (n dependency
chain) of causal relations
pf2 word class version of pf1
pf3 indicator for the existence of candidates of an ap-
propriate causal relation identified by partial tree
matching in an answer candidate
pf4 number of matched partial trees in candidates of
an appropriate causal relation
ef1 types of noun-polarity pairs shared by causal re-
lations and the question
ef2 ef1 coupled with each noun?s word class
ef3 indicator for the existence of candidates of an ap-
propriate causal relation identified by excitation
polarity matching in an answer candidate
ef4 number of noun-polarity pairs shared by the
question and the candidates of an appropriate
causal relation
Table 4: Causal relation features: n in n-grams
is n = {2, 3} and n-grams in an effect part are
distinguished from those in a cause part.
4.2.1 Term Matching
Our term matching method judges that a causal re-
lation is a candidate of an appropriate causal rela-
tion if its effect part contains at least one content
word (nouns, verbs, and adjectives) in the ques-
tion. For example, all the causal relations of A1?
A4 in Table 1 are candidates of an appropriate
causal relation to the question, ?Why is a tsunami
generated??, by term matching with question term
tsunami.
tf1?tf4 are generated from candidates of an ap-
propriate causal relation identified by term match-
ing. The n-grams of tf1 and tf2 are restricted
to those containing at least one content word in
a question. We distinguish this matched word
from the other words by replacing it with QW, a
special symbol representing a word in the ques-
tion. For example, word 3-gram ?this/cause/QW?
is extracted from This causes tsunamis in A2 for
?Why is a tsunami generated?? Further, we cre-
ate a word class version of word n-grams by con-
verting the words in these word n-grams into their
corresponding word class using the semantic word
classes (500 classes for 5.5 million nouns) from
our previous work (Oh et al, 2012). These word
classes were created by applying the automatic
word clustering method of Kazama and Torisawa
(2008) to 600 million Japanese web pages. For
example, the word class version of word 3-gram
1737
?this/cause/QW? is ?this/cause/QW,WCtsunami?,
where WCtsunami represents the word class of
a tsunami. tf3 is a binary feature that indi-
cates the existence of candidates of an appropri-
ate causal relation identified by term matching in
an answer candidate. tf4 represents the degree
of the relevance of the candidates of an appro-
priate causal relation measured by the number of
matched terms: one, two, and more than two.
4.2.2 Partial Tree Matching
Our partial tree matching method judges a causal
relation as a candidate of an appropriate causal re-
lation if its effect part contains at least one par-
tial tree in a question, where the partial tree covers
more than one content word. For example, only
the causal relation A1 among A1?A4 is a can-
didate of an appropriate causal relation for ques-
tion ?Why are tsunamis generated?? by partial
tree matching because only its effect part contains
partial tree ?tsunamis dep??? (are) generated? of the
question.
pf1?pf4 are generated from candidates of an
appropriate causal relation identified by the par-
tial tree matching. The syntactic dependency n-
grams in pf1 and pf2 are restricted to those that
contain at least one content word in a question. We
distinguish this matched content word from the
other content words in the n-gram by converting
it to QW, which represents a content word in the
question. For example, syntactic dependency 2-
gram ?QW dep??? cause? and its word class version
?QW,WCtsunami dep??? cause? are extracted from
Tsunamis that can cause in A1. pf3 is a binary
feature that indicates whether an answer candidate
contains candidates of an appropriate causal rela-
tion identified by partial tree matching. pf4 rep-
resents the degree of the relevance of the candi-
date of an appropriate causal relation measured by
the number of matched partial trees: one, two, and
more than two.
4.2.3 Excitation Polarity Matching
Hashimoto et al (2012) proposed a semantic ori-
entation called excitation polarities. It classifies
predicates with their argument position (called
templates) into excitatory, inhibitory and neu-
tral. In the following, we denote a template
as ?[argument position,predicate].? According to
Hashimoto?s definition, excitatory templates im-
ply that the function, effect, purpose, or the role of
an entity filling an argument position in the tem-
plates is activated/enhanced. On the contrary, in-
hibitory templates imply that the effect, purpose
or the role of an entity is deactivated/suppressed.
Neutral templates are those that neither activate
nor suppress the function of an argument.
We assume that the meanings of a text can
be roughly captured by checking whether each
noun in the text is activated or suppressed in the
sense of the excitation polarity framework, where
the activation and suppression of each entity (or
noun) can be detected by looking at the excita-
tion polarities of the templates that are filled by
the entity. For instance, effect part ?tsunamis
that can cause large coastal inundation are gen-
erated? of A1 roughly means that ?tsunamis? are
activated and ?inundation? is (or can be) acti-
vated. This activation/suppression configuration
of the nouns is consistent with sentence ?tsunamis
are caused? in which ?tsunamis? are activated.
This consistency suggests that A1 is a good an-
swer to question ?Why are tsunamis caused??, al-
though the ?tsunamis? are modified by different
predicates; ?cause? and ?generate.? On the other
hand, effect part ?tsunamis weaken as they pass
through forests? of A4 implies that ?tsunamis?
are suppressed. This suggests that A4 is not
a good answer to ?Why are tsunamis caused??
Note that the consistency checking between ac-
tivation/suppression configurations of nouns3 in
texts can be seen as a rough but lightweight ap-
proximation of the recognition of textual entail-
ments or paraphrases.
Following the definition of excitation polarity
in Hashimoto et al (2012), we manually classi-
fied templates4 to each polarity type and obtained
8,464 excitatory templates, such as [?, ???]
([subject, increase]) and [?, ????] ([sub-
ject, improve]), 2,262 inhibitory templates, such
as [?, ??] ([object, prevent]) and [?, ??]
([subject, die]), and 7,230 neutral templates such
as [?, ???] ([object, consider]). With these
templates, we obtain activation/suppression con-
figurations (including neutral) for the nouns in the
causal relations in the answer candidates and ques-
3 Because the activation/suppression configurations of
nouns come from an excitation polarity of templates, ?[argu-
ment position,predicate],? the semantics of verbs in the tem-
plates are implicitly considered in this consistency checking.
4 Varga et al (2013) has used the same templates as ours,
except they restricted their excitation/inhibitory templates to
those whose polarity is consistent with that given by the au-
tomatic acquisition method of Hashimoto et al (2012).
1738
tions.
Next, we assume that a causal relation is ap-
propriate as an answer to a question if the effect
part of the causal relation and the question share
at least one common noun with the same polarity.
More detailed information concerning the config-
urations of all the nouns in all the candidates of an
appropriate causal relation (including their cause
parts) and the question are encoded into our fea-
ture set ef1?ef4 in Table 4 and the final judgment
is done by our re-ranker.
For generating ef1 and ef2, we classified all the
nouns coupled with activation/suppression/neutral
polarities in a causal relation into three types:
SAME (the question contains the same noun with
the same polarity), DiffPOL (the question con-
tains the same noun with different polarity), and
OTHER (the others). ef1 indicates whether each
type of noun-polarity pair exists in a causal rela-
tion. Note that the types for the effect and cause
parts are represented in distinct features. ef2 is the
same as ef1 except that the types are augmented
with the word classes of the corresponding nouns.
In other words, ef2 indicates whether each type
of noun-polarity pair exists in the causal relation
for each word class. ef3 indicates the existence of
candidates of an appropriate causal relation iden-
tified by this matching scheme, and ef4 repre-
sents the number of noun-polarity pairs shared by
the question and the candidates of an appropriate
causal relations (one, two, and more than two).
5 Experiments
We experimented with causal relation recognition
and why-QA with our causal relation features.
5.1 Data Set for Why-Question Answering
For our experiments, we used the same why-QA
data set as the one used in our previous work (Oh
et al, 2012). This why-QA data set is composed
of 850 Japanese why-questions and their top-20
answer candidates obtained by answer candidate
extraction from 600 million Japanese web pages.
Three annotators checked the top-20 answer can-
didates of these 850 questions and the final judg-
ment was made by their majority vote. Their inter-
rater agreement by Fleiss? kappa reported in Oh et
al. (2012) was substantial (? = 0.634). Among the
850 questions, 250 why-questions were extracted
from the Japanese version of Yahoo! Answers,
and another 250 were created by annotators. In
our previous work, we evaluated the system with
these 500 questions and their answer candidates as
training and test data in 10-fold cross-validation.
The other 350 why-questions were manually built
from passages describing the causes or reasons of
events/phenomena. These questions and their an-
swer candidates were used as additional training
data for testing subsamples in each fold during the
10-fold cross-validation. In our why-QA experi-
ments, we evaluated our why-QA system with the
same settings.
5.2 Data Set for Causal Relation Recognition
We built a data set composed of manually anno-
tated causal relations for evaluating our causal re-
lation recognition. As source data for this data set,
we used the same 10-fold data that we used for
evaluating our why-QA (500 questions and their
answer candidates). We extracted the causal re-
lation candidates from the answer candidates in
each fold, and then our annotator (not an author)
manually marked the span of the cause and effect
parts of a causal relation for each causal relation
candidate, keeping in mind that the causal rela-
tion must be expressed in terms of a c-marker in
a given causal relation candidate. Finally, we had
a data set made of 16,051 causal relation candi-
dates, 8,117 of which had a true causal relation;
the number of intra- and inter-sentential causal re-
lations were 7,120 and 997, respectively.
Note that this data set can be partitioned into ten
folds by using the 10-fold partition of its source
data. We performed 10-fold cross validation to
evaluate our causal relation recognition with this
10-fold data.
5.3 Causal Relation Recognition
We used CRF++5 for training our causal relation
recognizer. In our evaluation, we judged a sys-
tem?s output as correct if both spans of the cause
and effect parts overlapped those in the gold stan-
dard. Evaluation was done by precision, recall,
and F1.
Precision Recall F1
BASELINE 41.9 61.0 49.7
INTRA-SENT 84.5 75.4 79.7
INTER-SENT 80.2 52.6 63.6
ALL 83.8 71.1 77.0
Table 5: Results of causal relation recognition (%)
Table 5 shows the result. BASELINE represents
5 http://code.google.com/p/crfpp/
1739
the result for our baseline system that recognizes
a causal relation by simply taking the two phrases
adjacent to a c-marker (i.e., before and after) as
cause and effect parts of the causal relation. We
assumed that the system had an oracle for judging
correctly whether each phrase is a cause part or an
effect part. In other words, we judged that a causal
relation recognized by BASELINE is correct if both
cause and effect parts in the gold standard are adja-
cent to a c-marker. INTRA-SENT and INTER-SENT
represent the results for intra- and inter-sentential
causal relations and ALL represents the result for
the both causal relations by our method. From
these results, we confirmed that our method rec-
ognized both intra- and inter-sentential causal rela-
tions with over 80% precision, and it significantly
outperformed our baseline system in both preci-
sion and recall rates.
Precision Recall F1
ALL-?MORPH? 80.8 66.4 72.9
ALL-?SYNTACTIC? 82.9 67.0 74.1
ALL-?C-MARKER? 76.3 51.4 61.4
ALL 83.8 71.1 77.0
Table 6: Ablation test results for causal relation
recognition (%)
We also investigated the contribution of the
three types of features used in our causal rela-
tion recognition to the performance. We evalu-
ated the performance when we removed one of
the three types of features (ALL-?MORPH?, ALL-
?SYNTACTIC? and ALL-?C-MARKER?) and com-
pared the results in these settings with the one
when all the feature sets were used (ALL). Ta-
ble 6 shows the result. We confirmed that all the
feature sets improved the performance, and we got
the best performance when using all of them. We
used the causal relations obtained from the 10-fold
cross validation for our why-QA experiments.
5.4 Why-Question Answering
We performed why-QA experiments to confirm
the effectiveness of intra- and inter-sentential
causal relations in a why-QA task. In
this experiment, we compared five systems:
four baseline systems (MURATA, OURCF, OH
and OH+PREVCF) and our proposed method
(PROPOSED).
MURATA corresponds to our answer candidate
extraction.
OURCF uses a re-ranker trained with only our
causal relation features.
OH, which represents our previous work (Oh et
al., 2012), has a re-ranker trained with mor-
phosyntactic, semantic word class, and senti-
ment polarity features.
OH+PREVCF is a system with a re-ranker
trained with the features used in OH and with
the causal relation feature proposed in Hi-
gashinaka and Isozaki (2008). The causal re-
lation feature includes an indicator that deter-
mines whether the causal relations between
two terms appear in a question-answer pair;
cause in an answer and its effect in a question.
We acquired the causal relation instances (be-
tween terms) from 600 million Japanese web
pages using the method of De Saeger et al
(2009) and exploited the top-100,000 causal
relation instances in this system.
PROPOSED has a re-ranker trained with our
causal relation features as well as the three
types of features proposed in Oh et al (2012).
Comparison between OH and PROPOSED re-
veals the contribution of our causal relation
features to why-QA.
We used TinySVM6 with a linear kernel
for training the re-rankers in OURCF, OH,
OH+PREVCF and PROPOSED. Evaluation was
done by P@1 (Precision of the top-answer) and
Mean Average Precision (MAP); they are the same
measures used in Oh et al (2012). P@1 measures
how many questions have a correct top-answer
candidate. MAP measures the overall quality of
the top-20 answer candidates. As mentioned in
Section 5.1, we used 10-fold cross-validation with
the same setting as the one used in Oh et al (2012)
for our experiments.
P@1 MAP
MURATA 22.2 27.0
OURCF 27.8 31.4
OH 37.4 39.1
OH+PREVCF 37.4 38.9
PROPOSED 41.8 41.0
Table 7: Why-QA results (%)
Table 7 shows the evaluation results. Our pro-
posed method outperformed the other four sys-
tems and improved P@1 by 4.4% over OH, which
is the-state-of-the-art system for Japanese why-
6 http://chasen.org/?taku/software/TinySVM/
1740
QA. OURCF showed the performance improve-
ment over MURATA. Although this suggests the
effectiveness of our causal relation features, the
overall performance of OURCF was lower than
that of OH. OH+PREVCF outperformed neither
OH nor PROPOSED. This suggests that our ap-
proach is more effective than previous causality-
based approaches (Girju, 2003; Higashinaka and
Isozaki, 2008), at least in our setting.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 10  20  30  40  50  60  70  80  90  100
P
re
ci
si
on
 (%
)
% of questions
PROPOSED
OH
OurCF
Figure 4: Effect of causal relation features on the
top-answers
We also compared confident answers of
OURCF, OH, and PROPOSED by making each sys-
tem provide only the k confident top-answers (for
k questions) selected by their SVM scores given
by each system?s re-ranker. This reduces the num-
ber of questions that can be answered by a system,
but the top-answers become more reliable as k de-
creases. Fig. 4 shows this result, where the x axis
represents the percentage of questions (against all
the questions in our test set) whose top-answers
are given by each system, and the y axis repre-
sents the precision of the top-answers at a certain
point on the x axis. When both systems provided
top-answers for 25% of all the questions in our test
set, our method achieved 83.2% precision, which
is much higher than OH?s (62.4%). This exper-
iment confirmed that our causal relation features
were also effective in improving the quality of the
highly confident answers.
However, the high precision by our method was
bound to confident answers for a small number
of questions, and the difference in the precision
between OH and PROPOSED in Fig. 4 became
smaller as we considered more answers with lower
confidence. We think that one of the reasons is the
relatively small coverage of the excitation polarity
lexicon, a core resource in our excitation polarity
matching. We are planning to enlarge the lexicon
to deal with this problem.
Next, we investigated the contribution of the
intra- and inter-sentential causal relations to the
performance of our method. We used only one
of the two types of causal relations for generating
causal relation features (INTRA-SENT and INTER-
SENT) for training our re-ranker and compared the
results in these settings with the one when both
were used (ALL (PROPOSED)). Table 8 shows
the result. Both intra- and inter-sentential causal
relations contributed to the performance improve-
ment.
P@1 MAP
INTER-SENT 39.0 39.7
INTRA-SENT 40.4 40.5
ALL (PROPOSED) 41.8 41.0
Table 8: Results with/without intra- and inter-
sentential causal relations (%)
We also investigated the contributions of the
three types of causal relation features by ablation
tests (Table 9). When we do not use the fea-
tures by excitation polarity matching (ALL-{ef1?
ef4}), the performance is the worst. This implies
that the contribution of excitation polarity match-
ing exceeds the other two.
P@1 MAP
ALL-{tf1?tf4} 40.8 40.7
ALL-{pf1?pf4} 41.0 40.9
ALL-{ef1?ef4} 39.6 40.5
ALL (PROPOSED) 41.8 41.0
Table 9: Ablation test results for why-QA (%)
6 Conclusion
In this paper, we explored the utility of intra- and
inter-sentential causal relations for ranking answer
candidates to why-questions. We also proposed a
method for assessing the appropriateness of causal
relations as answers to a given question using the
semantic orientation of excitation. Through ex-
periments, we confirmed that these ideas are ef-
fective for improving why-QA, and our proposed
method achieved 41.8% P@1, which is 4.4% im-
provement over the current state-of-the-art system
of Japanese why-QA. We also showed that our
system achieved 83.2% precision for its confident
answers, when it only provided its confident an-
swers for 25% of all the questions in our test set.
1741
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search (JAIR), 38(1):135?187.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. In Pro-
ceedings of TAC.
E. Blanco, N. Castell, and Dan I. Moldovan. 2008.
Causal relation extraction. In Proceedings of
LREC?08.
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, 42(3):662?678.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 16(1):1?17.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of ICDM ?09, pages 764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong Hoon Oh, Istv?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of EMNLP ?11,
pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of EMNLP ?11, pages 294?303.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Roxana Girju. 2003. Automatic detection of causal
relations for question answering. In Proceedings of
the ACL 2003 workshop on Multilingual summariza-
tion and question answering, pages 76?83.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orientation
extracts contradiction and causality from the web. In
Proceedings of EMNLP-CoNLL ?12.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of IJCNLP ?08, pages
418?425.
Takashi Inui and Manabu Okumura. 2005. Investigat-
ing the characteristics of causal relations in Japanese
text. In In Annual Meeting of the Association
for Computational Linguistics (ACL) Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medical
database using graphical patterns. In Proceedings of
ACL ?00, pages 336?343.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML ?01, pages
282?289.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL ?07,
pages 776?783.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A sys-
tem for answering non-factoid Japanese questions
by using passage retrieval weighted based on type
of answer. In Proceedings of NTCIR-6.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of COLING ?04, pages 693?701.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of EMNLP-CoNLL ?12, pages 368?378.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proceedings of WWW
?12, pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In ICSC
?10, pages 361?368.
Hideki Shima, Hiroshi Kanayama, Cheng wei Lee,
Chuan jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of NTCIR-9 RITE: Recognizing Inference in TExt.
In Proceedings of NTCIR-9.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
1742
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL ?06, pages 57?64.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Istvan Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of ACL ?13.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using syntactic infor-
mation for improving why-question answering. In
Proceedings of COLING ?08, pages 953?960.
Suzan Verberne, Lou Boves, and Wessel Kraaij. 2011.
Bringing why-qa to web search. In Proceedings of
ECIR ?11, pages 491?496.
1743
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 987?997,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Toward Future Scenario Generation: Extracting Event Causality
Exploiting Semantic Relation, Context, and Association Features
Chikara Hashimoto? Kentaro Torisawa? Julien Kloetzer? Motoki Sano?
Istva?n Varga? Jong-Hoon Oh? Yutaka Kidawara??
? ? ? ? ? ??National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
?NEC Knowledge Discovery Research Laboratories, Nara, 630-0101, Japan
{
? ch, ? torisawa, ? julien, ? msano, ? rovellia, ??kidawara}@nict.go.jp
Abstract
We propose a supervised method of
extracting event causalities like conduct
slash-and-burn agriculture?exacerbate
desertification from the web using se-
mantic relation (between nouns), context,
and association features. Experiments
show that our method outperforms base-
lines that are based on state-of-the-art
methods. We also propose methods of
generating future scenarios like conduct
slash-and-burn agriculture?exacerbate
desertification?increase Asian dust (from
China)?asthma gets worse. Experi-
ments show that we can generate 50,000
scenarios with 68% precision. We also
generated a scenario deforestation con-
tinues?global warming worsens?sea
temperatures rise?vibrio parahaemolyti-
cus fouls (water), which is written in no
document in our input web corpus crawled
in 2007. But the vibrio risk due to global
warming was observed in Baker-Austin
et al (2013). Thus, we ?predicted? the
future event sequence in a sense.
1 Introduction
The world can be seen as a network of causal-
ity where people, organizations, and other kinds
of entities causally depend on each other. This
network is so huge and complex that it is almost
impossible for humans to exhaustively predict the
consequences of a given event. Indeed, after the
Great East Japan Earthquake in 2011, few ex-
pected that it would lead to an enormous trade
deficit in Japan due to a sharp increase in en-
ergy imports. For effective decision making that
carefully considers any form of future risks and
chances, we need a system that helps humans do
scenario planning (Schwartz, 1991), which is a
decision-making scheme that examines possible
future events and assesses their potential chances
and risks. Our ultimate goal is to develop a system
that supports scenario planning through generat-
ing possible future events using big data, which
would contain what Donald Rumsfeld called ?un-
known unknowns?1 (Torisawa et al, 2010).
To this end, we propose a supervised method
of extracting such event causality as conduct
slash-and-burn agriculture?exacerbate desertifi-
cation and use its output to generate future sce-
narios (scenarios), which are chains of causal-
ity that have been or might be observed in
this world like conduct slash-and-burn agricul-
ture?exacerbate desertification?increase Asian
dust (from China)?asthma gets worse. Note that,
in this paper, A?B denotes that A causes B, which
means that ?if A happens, the probability of B in-
creases.? Our notion of causality should be inter-
preted probabilistically rather than logically.
Our method extracts event causality based on
three assumptions that are embodied as features
of our classifier. First, we assume that two nouns
(e.g. slash-and-burn agriculture and desertifica-
tion) that take some specific binary semantic rela-
tions (e.g. A CAUSES B) tend to constitute event
causality if combined with two predicates (e.g.
conduct and exacerbate). Note that semantic re-
lations are not restricted to those directly relevant
to causality like A CAUSES B but can be those that
might seem irrelevant to causality like A IS AN
INGREDIENT FOR B (e.g. plutonium and atomic
bomb as in plutonium is stolen?atomic bomb is
made). Our underlying intuition is the observation
that event causality tends to hold between two en-
tities linked by semantic relations which roughly
entail that one entity strongly affects the other.
Such semantic relations can be expressed by (oth-
erwise unintuitive) patterns like A IS AN INGRE-
DIENT FOR B. As such, semantic relations like the
MATERIAL relation can also be useful. (See Sec-
1http://youtu.be/GiPe1OiKQuk
987
tion 3.2.1 for a more intuitive explanation.)
Our second assumption is that there are gram-
matical contexts in which event causality is more
likely to appear. We implement what we con-
sider likely contexts for event causality as con-
text features. For example, a likely context of
event causality (underlined) would be: CO2 levels
rose, so climatic anomalies were observed, while
an unlikely context would be: It remains uncertain
whether if the recession is bottomed the declining
birth rate is halted. Useful context information in-
cludes the mood of the sentences (e.g., the uncer-
tainty mood expressed by uncertain above), which
is represented by lexical features (Section 3.2.2).
The last assumption embodied in our associa-
tion features is that each word of the cause phrase
must have a strong association (i.e., PMI, for ex-
ample) with that of the effect phrase as slash-and-
burn agriculture and desertification in the above
example, as in Do et al (2011).
Our method exploits these features on top of our
base features such as nouns and predicates. Exper-
iments using 600 million web pages (Akamine et
al., 2010) show that our method outperforms base-
lines based on state-of-the-art methods (Do et al,
2011; Hashimoto et al, 2012) by more than 19%
of average precision.
We require that event causality be self-
contained, i.e., intelligible as causality without the
sentences from which it was extracted. For ex-
ample, omit toothbrushing?get a cavity is self-
contained, but omit toothbrushing?get a girl-
friend is not since this is not intelligible without a
context: He omitted toothbrushing every day and
got a girlfriend who was a dental assistant of den-
tal clinic he went to for his cavity. This is im-
portant since future scenarios, which are gener-
ated by chaining event causality as described be-
low, must be self-contained, unlike Hashimoto et
al. (2012). To make event causality self-contained,
we wrote guidelines for manually annotating train-
ing/development/test data. Annotators regarded
as event causality only phrase pairs that were
interpretable as event causality without contexts
(i.e., self-contained). From the training data, our
method seemed to successfully learn what self-
contained event causality is.
Our scenario generation method generates sce-
narios by chaining extracted event causality; gen-
erating A?B?C from A?B and B?C. The chal-
lenge is that many acceptable scenarios are over-
looked if we require the joint part of the chain (B
above) to be an exact match. To increase the num-
ber of acceptable scenarios, our method identifies
compatibility w.r.t causality between two phrases
by a recently proposed semantic polarity, exci-
tation (Hashimoto et al, 2012), which properly
relaxes the chaining condition (Section 3.1 de-
scribes it). For example, our method can iden-
tify the compatibility between sea temperatures
are high and sea temperatures rise to chain global
warming worsens?sea temperatures are high
and sea temperatures rise?vibrio parahaemolyti-
cus2 fouls (water). Accordingly, we generated
a scenario deforestation continues?global warm-
ing worsens?sea temperatures rise?vibrio para-
haemolyticus fouls (water), which is written in
no document in our input web corpus that was
crawled in 2007, but the vibrio risk due to global
warming has actually been observed in the Baltic
sea and reported in Baker-Austin et al (2013). In
a sense, we ?predicted? the event sequence re-
ported in 2013 by documents written in 2007. Our
experiments also show that we generated 50,000
scenarios with 68% precision, which include con-
duct terrorist operations?terrorist bombing oc-
curs?cause fatalities and injuries?cause eco-
nomic losses and the above ?slash-and-burn agri-
culture? scenario (Section 5.2). Neither is written
in any document in our input corpus.
In this paper, our target language is Japanese.
However, we believe that our ideas and methods
are applicable to many languages. Examples are
translated into English for ease of explanation.
Supplementary notes of this paper are available
at http://khn.nict.go.jp/analysis/
member/ch/acl2014-sup.pdf.
2 Related Work
For event causality extraction, clues used by
previous methods can roughly be categorized
as lexico-syntactic patterns (Abe et al, 2008;
Radinsky et al, 2012), words in context (Oh et
al., 2013), associations among words (Torisawa,
2006; Riaz and Girju, 2010; Do et al, 2011), and
predicate semantics (Hashimoto et al, 2012). Be-
sides features similar to those described above, we
propose semantic relation features3 that include
those that are not obviously related to causality.
We show that such thorough exploitation of new
and existing features leads to high performance.
2A bacterium in the sea causing food-poisoning.
3Radinsky et al (2012) and Tanaka et al (2012) used se-
mantic relations to generalize acquired causality instances.
988
Other clues include shared arguments (Torisawa,
2006; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009), which we ignore since we tar-
get event causality about two distinct entities.
To the best of our knowledge, future scenario
generation is a new task, although previous works
have addressed similar tasks (Radinsky et al,
2012; Radinsky and Horvitz, 2013). Neither in-
volves chaining and restricts themselves to only
one event causality step. Besides, the events they
predict must be those for which similar events
have previously been observed, and their method
only applies to news domain.
Some of the scenarios we generated are written
on no page in our input web corpus. Similarly,
Tsuchida et al (2011) generated semantic knowl-
edge like causality that is written in no sentence.
However, their method cannot combine more than
two pieces of knowledge unlike ours, and their tar-
get knowledge consists of nouns, but ours consists
of verb phrases, which are more informative.
Tanaka et al (2013)?s web information analy-
sis system provides a what-happens-if QA service,
which is based on our scenario generation method.
3 Event Causality Extraction Method
This section describes our event causality extrac-
tion method. Section 3.1 describes how to extract
event causality candidates, and Section 3.2 details
our features. Section 3.3 shows how to rank event
causality candidates.
3.1 Event Causality Candidate Extraction
We extract the event causality between two events
represented by two phrases from single sentences
that are dependency parsed.4 We obtained sen-
tences from 600 million web pages. Each phrase
in the event causality must consist of a predicate
with an argument position (template, hereafter)
like conduct X and a noun like slash-and-burn
agriculture that completes X. We also require the
predicate of the cause phrase to syntactically de-
pend on the effect phrase in the sentence from
which the event causality was extracted; we guar-
antee this by verifying the dependencies of the
original sentence. In Japanese, since the tempo-
ral order between events is usually determined by
precedence in a sentence, we require the cause
phrase to precede the effect phrase. For context
4We used a Japanese dependency parser called J.DepP
(Yoshinaga and Kitsuregawa, 2009), available at http://
www.tkl.iis.u-tokyo.ac.jp/?ynaga/jdepp/.
feature extraction, the event causality candidates
are accompanied by the original sentences from
which they were extracted.
Excitation We only keep the event causality
candidates each phrase of which consists of exci-
tation templates, which have been shown to be ef-
fective for causality extraction (Hashimoto et al,
2012) and other semantic NLP tasks (Oh et al,
2013; Varga et al, 2013; Kloetzer et al, 2013a).
Excitation is a semantic property of templates that
classifies them into excitatory, inhibitory, and neu-
tral. Excitatory templates such as cause X entail
that the function, effect, purpose or role of their ar-
gument?s referent is activated, enhanced, or man-
ifested, while inhibitory templates such as lower
X entail that it is deactivated or suppressed. Neu-
tral ones like proportional to X belong to neither
of them. We collectively call both excitatory and
inhibitory templates excitation templates. We ac-
quired 43,697 excitation templates by Hashimoto
et al?s method and the manual annotation of exci-
tation template candidates.5 We applied the exci-
tation filter to all 272,025,401 event causality can-
didates from the web and 132,528,706 remained.
After applying additional filters (see Section A
in the supplementary notes) including those based
on a stop-word list and a causal connective list
to remove unlikely event causality candidates that
are not removed by the above filter, we finally ac-
quired 2,451,254 event causality candidates.
3.2 Features for Event Causality Classifier
3.2.1 Semantic Relation Features
We hypothesize that two nouns with some particu-
lar semantic relations are more likely to constitute
event causality. Below we describe the semantic
relations that we believe are likely to constitute
event causality.
CAUSATION is the causal relation between two
entities and is expressed by binary patterns like
A CAUSES B. Deforestation and global warming
might complete the A and B slots. We manually
collected 748 binary patterns for this relation. (See
Section B in the supplementary notes for examples
of our binary patterns.)
MATERIAL is the relation between a material
and a product made of it (e.g. plutonium and
5Hashimoto et al?s method constructs a network of tem-
plates based on their co-occurrence in web sentences with a
small number of polarity-assigned seed templates and infers
the polarity of all the templates in the network by a constraint
solver based on the spin model (Takamura et al, 2005).
989
atomic bomb) and can be expressed by A IS MADE
OF B. Its relation to event causality might seem
unclear, but a material can be seen as a ?cause?
of a product. Indeed materials can participate
in event causality with the help of such template
pairs as A is stolen?B is made as in plutonium is
stolen?atomic bomb is made. We manually col-
lected 187 binary patterns for this relation.
NECESSITY?s patterns include A IS NECES-
SARY FOR B, which can be filled with verbal apti-
tude and ability to think. Noun pairs with this rela-
tion can constitute event causality when combined
with template pairs like improve A?cultivate B.
We collected 257 patterns for this relation.
USE is the relation between means (or instru-
ments) and the purpose for using them. A IS USED
FOR B is a pattern of the relation, which can be
filled with e-mailer and exchanges of e-mail mes-
sages. Note that means can be seen as ?causing?
or ?realizing? the purpose of using the means in
this relation, and actually event causality can be
obtained by incorporating noun pairs of this rela-
tion into template pairs like activate A?conduct
B. 2,178 patterns were collected for this relation.
PREVENTION is the relation expressed by pat-
terns like A PREVENTS B, which can be filled with
toothbrushing and periodontal disease. This rela-
tion is, so to speak, ?negative CAUSATION? since
the entity denoted by the noun completing the A
slot makes the entity denoted by the B noun NOT
realized. Such noun pairs mean event causality
by substituting them into template pairs like omit
A?get B. The number of patterns is 490.
The experiments in Section 5.1.1 show that not
only CAUSATION and PREVENTION (?negative
CAUSATION?) but the other relations are also ef-
fective for event causality extraction.
In addition, we invented the EXCITATION rela-
tion that is expressed by binary patterns made of
excitatory and inhibitory templates (Section 3.1).
For instance, we make binary patterns A RISES B
and A LOWERS B from excitatory template rise X
and inhibitory template lower X respectively. The
EXCITATION relation roughly means that A acti-
vates B (excitatory) or suppresses it (inhibitory).
We simply add an additional argument position to
each template in the 43,697 excitation templates to
make binary patterns. We restricted the argument
positions (represented by Japanese postpositions)
of the A slot to either ha (topic marker), ga (nomi-
native), or de (instrumental) and those of the B slot
to either ha, ga, de, wo (accusative), or ni (dative),
SR1: Binary pattern of our semantic relations that co-
occurs with two nouns of an event causality candi-
date in our web corpus.
SR2: Semantic relation types (e.g CAUSATION and EN-
TAILMENT) of the binary pattern of SR1. EXCITA-
TION is divided into six sub types based on the ex-
citation polarity of the binary patterns, the argument
positions, and the existence of causative markers. A
CAUSATION pattern, B BY A, constitutes an indepen-
dent relation called the BY relation.
Table 1: Semantic relation features.
and obtained 55,881 patterns.
Moreover, for broader coverage, we acquired
binary patterns that entail or are entailed by one
of the patterns of the above six semantic relations.
Those patterns were acquired from our web cor-
pus by Kloetzer et al (2013b)?s method, which ac-
quired 185 million entailment pairs with 80% pre-
cision from our web corpus and was used for con-
tradiction acquisition (Kloetzer et al, 2013a). We
acquired 335,837 patterns by this method. They
are class-dependent patterns, which have seman-
tic class restrictions on arguments. The semantic
classes were obtained from our web corpus based
on Kazama and Torisawa (2008). See De Saeger
et al (2009), De Saeger et al (2011) and Kloet-
zer et al (2013a) for more on our patterns. They
collectively constitute the ENTAILMENT relation.
Table 1 shows our semantic relation features. To
use them, we first make a database that records
which noun pairs co-occur with each binary pat-
tern. Then we check a noun pair (the nouns of the
cause and effect phrases) for each event causality
candidate, and give the candidate all the patterns
in the database that co-occur with the noun pair.
3.2.2 Context Features
We believe that contexts exist where event causal-
ity candidates are more likely to appear, as de-
scribed in Section 1. We developed features that
capture the characteristics of likely contexts for
Japanese event causality (See Section C in the sup-
plementary notes). In a nutshell, they represent a
connective (C1 and C2 in Section C), the distance
between the elements of event causality candidate
(C3 and C4), words in context (C5 to C8), the ex-
istence of adnominal modifier (9 to C10), and the
existence of additional arguments of cause and ef-
fect predicates (C13 to C20), among others.
3.2.3 Association Features
These features measure the association strength
between slash-and-burn agriculture and deser-
990
AC1: The CEA value, the sum of AC2, AC3, and AC4.
AC2: Do et al?s S
pp
. This is the association measure
between predicates, which is the product of AC5,
AC6 and AC7 below. They are calculated from the
132,528,706 event causality candidates in Section
3.1. We omit Do et al?s Dist, which is a constant
since we set our window size to one.
AC3: Do et al?s S
pa
. This is the association measure be-
tween arguments and predicates, which is the sum
of AC8 and AC9. They are calculated from the
132,528,706 event causality candidates.
AC4: Do et al?s S
aa
, which is PMI between arguments.
We obtained it in the same way as Filter 5 in the sup-
plementary notes.
AC5: PMI between predicates.
AC6 / AC7: Do et al?s max / IDF .
AC8: PMI between a cause noun and an effect predicate.
AC9: PMI between a cause predicate and an effect noun.
Table 2: CEA-based association features.
tification in conduct slash-and-burn agricul-
ture?exacerbate desertification for instance and
consist of CEA-, Wikipedia-, definition-, and web-
based features. CEA-based features are based
on the Cause Effect Association (CEA) measure
of Do et al (2011). It consists of association
measures like PMI between arguments (nouns),
between arguments and predicates, and between
predicates (Table 2). Do et al used it (along
with discourse relations) to extract event causality.
Wikipedia-based features are the co-occurrence
counts and the PMI values between cause and ef-
fect nouns calculated using Wikipedia (as of 2013-
Sep-19). We also checked whether an Wikipedia
article whose title is a cause (effect) noun con-
tains its effect (cause) noun, as detailed in Section
D.1 in the supplementary notes. Definition-based
features, as detailed in Section D.2 in the sup-
plementary notes, resemble the Wikipedia-based
features except that the information source is the
definition sentences automatically acquired from
our 600 million web pages using the method of
Hashimoto et al (2011). Web-based features
provide association measures between nouns us-
ing various window sizes in the 600 million web
pages. See Section D.3 for detail. Web-based as-
sociation measures were obtained from the same
database as AC4 in Table 2.
3.2.4 Base Features
Base features represent the basic properties of
event causality like nouns, templates, and their ex-
citation polarities (See Section E in the supple-
mentary notes). For B3 and B4, 500 semantic
classes were obtained from our web corpus using
the method of Kazama and Torisawa (2008).
3.3 Event Causality Scoring
Using the above features, a classifier6 classifies
each event causality candidate into causality and
non-causality. An event causality candidate is
given a causality score CScore, which is the SVM
score (distance from the hyperplane) that is nor-
malized to [0, 1] by the sigmoid function 1
1+e
?x
.
Each event causality candidate may be given mul-
tiple original sentences, since a phrase pair can ap-
pear in multiple sentences, in which case it is given
more than one SVM score. For such candidates,
we give the largest score and keep only one origi-
nal sentence that corresponds to the largest score.7
Original sentences are also used for scenario gen-
eration, as described below.
4 Future Scenario Generation Method
Our future scenario generation method creates
scenarios by chaining event causalities. A naive
approach chains two phrase pairs by exact match-
ing. However, this approach would overlook many
acceptable scenarios as discussed in Section 1. For
example, global warming worsens?sea tempera-
tures are high and sea temperatures rise?vibrio
parahaemolyticus fouls (water) can be chained to
constitute an acceptable scenario, but the joint part
is not the same string. Note that the two phrases
are not simply paraphrases; temperatures may be
rising but remain cold, or they may be decreasing
even though they remain high.
What characterizes two phrases that can be the
joint part of acceptable scenarios? Although we
have no definite answer yet, we name it the causal-
compatibility of two phrases and provide its pre-
liminary characterization based on the excitation
polarity. Remember that excitatory templates like
cause X entail that X?s function or effect is acti-
vated, but inhibitory templates like lower X entail
that it is suppressed (Section 3.1). Two phrases
are causally-compatible if they mention the same
entity (typically described by a noun) that is pred-
icated by the templates of the same excitation po-
larity. Indeed, both X rise and X are high are ex-
citatory and hence sea temperatures are high and
sea temperatures rise are causally-compatible.8
6We used SVMlight with the polynominal kernel (d = 2),
available at http://svmlight.joachims.org.
7Future work will exploit other original sentences, as sug-
gested by an anonymous reviewer.
8Using other knowledge like verb entailment (Hashimoto
et al, 2009) can be helpful too, which is further future work.
991
Scenarios (scs) generated by chaining causally-
compatible phrase pairs are scored by Score(sc),
which embodies our assumption that an acceptable
scenario consists of plausible event causality pairs:
Score(sc) =
?
cs?CAUS(sc)
CScore(cs)
where CAUS(sc) is a set of event causality
pairs that constitutes sc and cs is a member of
CAUS(sc). CScore(cs), which is cs?s score,
was described in Section 3.3.
Our method optionally applies the following
two filters to scenarios for better precision: An
original sentence filter removes a scenario if two
event causality pairs that are chained in it are ex-
tracted from original sentences between which no
word overlap exists other than words constituting
causality pairs. In this case, the two event causal-
ity pairs tend to be about different topics and con-
stitute an incoherent scenario. A common argu-
ment filter removes a scenario if a joint part con-
sists of two templates that share no argument in
our ?argument, template? database, which is com-
piled from the syntactic dependency data between
arguments and templates extracted from our web
corpus. Such a scenario tends to be incoherent too.
5 Experiments
5.1 Event Causality Extraction
Next we describe our experiments on event causal-
ity extraction and show (a) that most of our fea-
tures are effective and (b) that our method outper-
forms the baselines based on state-of-the-art meth-
ods (Do et al, 2011; Hashimoto et al, 2012). Our
method achieved 70% precision at 13% recall; we
can extract about 69,700 event causality pairs with
70% precision, as described below.
For the test data, we randomly sampled 23,650
examples of ?event causality candidate, origi-
nal sentence? among which 3,645 were positive
from 2,451,254 event causality candidates ex-
tracted from our web corpus (Section 3.1). For
the development data, we identically collected
11,711 examples among which 1,898 were posi-
tive. These datasets were annotated by three anno-
tators (not the authors), who annotated the event
causality candidates without looking at the origi-
nal sentences. The final label was determined by
majority vote. The training data were created
by the annotators through our preliminary experi-
ments and consists of 112,110 among which 9,657
Method Ave. prec. (%)
Proposed 46.27
w/o Context features 45.68
w/o Association features 45.66
w/o Semantic relation features 44.44
Base features only 41.29
Table 3: Ablation tests.
Semantic relations Ave. prec. (%)
All semantic relations (Proposed) 46.27
CAUSATION 45.86
CAUSATION and PREVENTION 45.78
None (w/o Semantic relation features) 44.44
Table 4: Ablation tests on semantic relations.
were positive. The Kappa (Fleiss, 1971) of their
judgments was 0.67 (substantial agreement (Lan-
dis and Koch, 1977)). These three datasets have
no overlap in terms of phrase pairs. About nine
man-months were required to prepare the data.
Our evaluation is based on average precision;9
we believe that it is important to rank the plausible
event causality candidates higher.
5.1.1 Ablation Tests
We evaluated the features of our method by ab-
lation tests. Table 3 shows the results of remov-
ing the semantic relation, the context, and the as-
sociation features from our method. All the fea-
ture types are effective and contribute to the per-
formance gain that was about 5% higher than the
Base features only. Proposed achieved 70% pre-
cision at 13% recall. We then estimated that, with
the precision rate, we can extract 69,700 event
causality pairs from the 2,451,254 event causality
candidates, among which the estimated number of
positive examples is 377,794.
Next we examined whether the semantic rela-
tions that do not seem directly relevant to causality
like MATERIAL are effective. Table 4 shows that
the performance degraded (46.27 ? 45.86) when
we only used the CAUSATION binary patterns and
their entailing and entailed patterns compared to
Proposed. Even when adding the PREVENTION
(?negative CAUSATION?) patterns and their entail-
ing and entailed patterns, the performance was still
slightly worse than Proposed. The performance
was even worse when using no semantic relation
(?None? in Table 4). Consequently we conclude
that not only semantic relations directly relevant
9It is obtained by computing the precision for each point
in the ranked list where we find a positive sample and aver-
aging all the precision figures (Manning and Schu?tze, 1999).
992
Method Ave. prec. (%)
w/o Wikipedia-based features 46.52
Proposed 46.27
w/o definition-based features 46.21
w/o Web-based features 46.15
w/o CEA-based features 45.80
Table 5: Ablation tests on association features.
Method Ave. prec. (%)
Proposed 46.27
Proposed-CEA 45.80
CEA
sup
21.77
CEA
uns
16.57
Table 6: Average precision of our proposed meth-
ods and baselines using CEA.
to causality like CAUSATION but also those that
seem to lack direct relevance to causality like MA-
TERIAL are somewhat effective.
Finally, Table 5 shows the performance drop
by removing the Wikipedia-, definition-, web-,
and CEA-based features. The CEA-based fea-
tures were the most effective, while the Wikipedia-
based ones slightly degraded the performance.
5.1.2 Comparison to Baseline Methods
We compared our method and two baselines based
on Do et al (2011): CEA
uns
is an unsupervised
method that uses CEA to rank event causality can-
didates, and CEA
sup
is a supervised method us-
ing SVM and the CEA features, whose ranking is
based on the SVM scores. The baselines are not
complete implementations of Do et al?s method
which uses discourse relations identified based on
Lin et al (2010) and exploits them with CEA
within an ILP framework. Nonetheless, we believe
that this comparison is informative since CEA can
be seen as the main component; they achieved a
F1 of 41.7% for extracting causal event relations,
but with only CEA they still achieved 38.6%.
Table 6 shows the average precision of the com-
pared methods. Proposed is our proposed method.
Proposed-CEA is Proposed without the CEA-
features and shows their contribution. Proposed
is the best and the CEA features slightly contribute
to the performance, as Proposed-CEA indicates.
We observed that CEA
sup
and CEA
uns
performed
poorly and tended to favor event causality candi-
dates whose phrase pairs were highly relevant to
each other but described the contrasts of events
rather than event causality (e.g. build a slow mus-
cle and build a fast muscle) probably because their
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
re
ci
si
on
Recall
	
	



Figure 1: Precision-recall curves of proposed
methods and baselines using CEA.
Method Ave. prec. (%)
Proposed 49.64
Cs
uns
30.38
Cs
sup
27.49
Table 7: Average precision of our proposed
method and baselines using Cs.
main components are PMI values. Figure 1 shows
their precision-recall curves.
Next we compared our method with the base-
lines based on Hashimoto et al (2012). They de-
veloped an automatic excitation template acqui-
sition method that assigns each template an ex-
citation value in range [?1, 1] that is positive if
the template is excitatory and negative if it is in-
hibitory. They ranked event causality candidates
by Cs(p
1
, p
2
) = |s
1
| ? |s
2
|, where p
1
and p
2
are
the two phrases of event causality candidates, and
|s
1
| and |s
2
| are the absolute excitation values of
p
1
?s and p
2
?s templates. The baselines are as fol-
lows: Cs
uns
is an unsupervised method that uses
Cs for ranking, and Cs
sup
is a supervised method
using SVM with Cs as the only feature that uses
SVM scores for ranking. Note that some event
causality candidates were not given excitation val-
ues for their templates, since some templates were
acquired by manual annotation without Hashimoto
et al?s method. To favor the baselines for fairness,
the event causality candidates of the development
and test data were restricted to those with excita-
tion values. Since Cs
sup
performed slightly better
when using all of the training data in our prelimi-
nary experiments, we used all of it.
Table 7 shows the average precision of the com-
pared methods. Proposed is our method. Its av-
erage precision is different from that in Table 6
due to the difference in test data described above.
Cs
uns
and Cs
sup
did not perform well. Many
993
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
re
ci
si
on
Recall

	

	
Figure 2: Precision-recall curves of proposed
methods and baselines using Cs.
phrase pairs described two events that often hap-
pen in parallel but are not event causality (e.g. re-
duce the intake of energy and increase the energy
consumption) in the highly ranked event causality
candidates of Cs
uns
and Cs
sup
. Figure 2 shows
their precision-recall curves.
Hashimoto et al (2012) extracted 500,000 event
causalities with about 70% precision. However, as
described in Section 1, our event causality crite-
ria are different; since they regarded phrase pairs
that were not self-contained as event causality
(their annotators checked the original sentences of
phrase pairs to see if they were event causality),
their judgments tended to be more lenient than
ours, which explains the performance difference.
In preliminary experiments, since our proposed
method?s performance degraded when Cs was in-
corporated, we did not use it in our method.
5.2 Future Scenario Generation
To show that our future scenario generation meth-
ods can generate many acceptable scenarios with
reasonable precision, we experimentally com-
pared four methods: Proposed, our scenario
generation method without the two filters, Pro-
posed+Orig, our method with the original sen-
tence filter, Proposed+Orig+Comm, our method
with the original sentence and common argument
filters, and Exact, a method that chains event
causality by exact matching.
Beginning events As the beginning event of a
scenario, we extracted nouns that describe social
problems (social problem nouns, e.g. deforesta-
tion) from Wikipedia to focus our evaluation on
the ability to generate scenarios about them, which
is a realistic use-case of scenario generation. We
extracted 557 social problem nouns and used the
cause phrases of the event causality candidates that
Two-step Three-step
Exact 1,000 (44.10) 1,000 (23.50)
Proposed 2,000 (32.25) 2,000 (12.55)
Proposed+Orig 995 (36.28) 602 (17.28)
Proposed+Orig+Comm 708 (38.70) 339 (17.99)
Table 8: Number of scenario samples and their
precision (%) in parentheses.
consisted of one of the social problem nouns as the
scenario?s beginning event.
Event causality We applied our event causality
extraction method to 2,451,254 candidates (Sec-
tion 3.1) and culled the top 1,200,000 phrase pairs
from them (See Section F in the supplementary
notes for examples). Some phrase pairs have the
same noun pairs and the same template polar-
ity pairs (e.g. omit toothbrushing?get a cavity
and neglect toothbrushing?have a cavity, where
omit X and neglect X are inhibitory and get X and
have X are excitatory). We removed such phrase
pairs except those with the highest CScore, and
960,561 phrase pairs remained, from which we
generated two- or three-step scenarios that con-
sisted of two or three phrase pairs.
Evaluation samples The numbers of two- and
three-step scenarios generated by Proposed were
217,836 and 5,288,352, while those of Exact were
22,910 and 72,746. We sampled 2,000 from Pro-
posed?s two- and three-step scenarios and 1,000
from those of Exact. We applied the filters to the
sampled scenarios of Proposed, and the results
were regarded as the sample scenarios of Pro-
posed+Orig and Proposed+Orig+Comm. Table
8 shows the number and precision of the samples.
Note that, for the diversity of the sampled scenar-
ios, our sampling proceeded as follows: (i) Ran-
domly sample a beginning event phrase from the
generated scenarios. (ii) Randomly sample an ef-
fect phrase for the beginning event phrase from the
scenarios. (iii) Regarding the effect phrase as a
cause phrase, randomly sample an effect phrase
for it, and repeat (iii) up to the specified number
of steps (2 or 3). The samples were annotated by
three annotators (not the authors), who were in-
structed to regard a sample as acceptable if each
event causality that constitutes it is plausible and
the sample as a whole constitutes a single coherent
story. Final judgment was made by majority vote.
Fleiss? kappa of their judgments was 0.53 (moder-
ate agreement), which is lower than the kappa for
the causality judgment. This is probably because
994
Two-step Three-step
Exact 2,085 1,237
Proposed 5,773 0
Proposed+Orig 4,107 0
Proposed+Orig+Comm 3,293 21,153
Table 9: Estimated number of acceptable scenar-
ios with a 70% precision rate.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10000 20000 30000 40000 50000 60000 70000
P
re
ci
si
on
Estimated number of acceptable scenarios

	

	

	

Figure 3: Precision-scenario curves (2-step).
scenario judgment requires careful consideration
about various possible futures for which individ-
ual annotators tend to draw different conclusions.
Result 1 Table 9 shows the estimated number
of acceptable scenarios generated with 70% pre-
cision. The estimated number is calculated as the
product of the recall at 70% precision and the
number of acceptable scenarios in all the gener-
ated scenarios, which is estimated by the anno-
tated samples. Figures 3 and 4 show the precision-
scenario curves for the two- and three-step sce-
narios, which illustrate how many acceptable sce-
narios can be generated with what precision. The
curve is drawn in the same way as the precision-
recall curve except that the X-axis indicates the
estimated number of acceptable scenarios. At
70% precision, all of the proposed methods out-
performed Exact in the two-step setting, and Pro-
posed+Orig+Comm outperformed Exact in the
three-step setting.
Result 2 To evaluate the top-ranked scenarios
of Proposed+Orig+Comm in the three-step set-
ting with more samples, the annotators labeled 500
samples from the top 50,000 of its output. 341
(68.20%) were acceptable, and the estimated num-
ber of acceptable scenarios at a precision rate of
70% and 80% are 26,700 and 5,200 (See Section H
in the supplementary notes). The ?terrorist oper-
ations? scenario and the ?slash-and-burn agricul-
ture? scenario in Section 1 were ranked 16,386th
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  100000 200000 300000 400000 500000 600000 700000
P
re
ci
si
on
Estimated number of acceptable scenarios

	

	

	

Figure 4: Precision-scenario curves (3-step).
and 21,968th. Next we examined how many of
the top 50,000 scenarios were acceptable and non-
trivial, i.e., found in no page in our input web cor-
pus, using the 341 acceptable samples. A scenario
was regarded as non-trivial if its nouns co-occur in
no page of the corpus. 22 among the 341 samples
were non-trivial. Accordingly, we estimate that
we can generate 2,200 (50,000?22
500
) acceptable and
non-trivial scenarios from the top 50,000. (See
Section G in the supplementary notes for exam-
ples of the generated scenarios.)
Discussion Scenario deforestation contin-
ues?global warming worsens?sea temperatures
rise?vibrio parahaemolyticus fouls (water)
was generated by Proposed+Orig+Comm. It
is written in no page in our input web corpus,
which was crawled in 2007.10 But we did find
a paper Baker-Austin et al (2013) that observed
the emerging vibrio risk in the Baltic sea due to
global warming. In a sense, we ?predicted? an
event observed in 2013 from documents written
in 2007, although the scenario was ranked as low
as 240,738th.
6 Conclusion
We proposed a supervised method for event
causality extraction that exploits semantic rela-
tion, context, and association features. We also
proposed methods for our new task, future sce-
nario generation. The methods chain event causal-
ity by causal-compatibility. We generated non-
trivial scenarios with reasonable precision, and
?predicted? future events from web documents.
Increasing their rank is future work.
10The corpus has pages where global warming, sea tem-
peratures, and vibrio parahaemolyticus happen to co-occur.
But they are either diaries where the three words appear sep-
arately in different topics or lists of arbitrary words.
995
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling
the relation-oriented and argument-oriented ap-
proaches. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING
2008), pages 1?8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu,
Takuya Kawada, Kentaro Inui, Sadao Kurohashi,
and Yutaka Kidawara. 2010. Organizing informa-
tion on the web to support user judgments on in-
formation credibility. In Proceedings of 2010 4th
International Universal Communication Symposium
Proceedings (IUCS 2010), pages 122?129.
Craig Baker-Austin, Joaquin A. Trinanes, Nick G. H.
Taylor, Rachel Hartnell, Anja Siitonen, and Jaime
Martinez-Urtaza. 2013. Emerging vibrio risk at
high latitudes in response to ocean warming. Nature
Climate Change, 3:73?77.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation of Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 789?
797.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL and the 4th IJCNLP of the AFNLP
(ACL-IJCNLP 2009), pages 602?610.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of the IEEE International
Conference on Data Mining (ICDM 2009), pages
764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), pages 294?303.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Masaki Murata, and Jun?ichi Kazama. 2009. Large-
scale verb entailment acquisition from the web. In
Proceedings of EMNLP 2009: Conference on Em-
pirical Methods in Natural Language Processing,
pages 1172?1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jun?ichi Kazama, and Sadao Kuro-
hashi. 2011. Extracting paraphrases from definition
sentences on the web. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1087?1097.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orienta-
tion extracts contradiction and causality from the
web. In Proceedings of EMNLP-CoNLL 2012: Con-
ference on Empirical Methods in Natural Language
Processing and Natural Language Learning, pages
619?630.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa,
Chikara Hashimoto, Jong-Hoon Oh, and Kiyonori
Ohtake. 2013a. Two-stage method for large-scale
acquisition of contradiction pattern pairs using en-
tailment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2013), pages 693?703.
Julien Kloetzer, Kentaro Torisawa, Stijn De Saeger,
Motoki Sano, Chikara Hashimoto, and Jun Gotoh.
2013b. Large-scale acquisition of entailment pattern
pairs. In Information Processing Society of Japan
(IPSJ) Kansai-Branch Convention 2013.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
pdtb-styled end-to-end discourse parser. Technical
report, School of Computing, National University of
Singapore.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 1733?
1743.
996
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of Sixth
ACM International Conference on Web Search and
Data Mining (WSDM 2013), pages 255?264.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for news
events prediction. In Proceedings of International
World Wide Web Conference 2012 (WWW 2012),
pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In 2010
IEEE Fourth International Conference on Semantic
Computing, pages 361?368.
Peter Schwartz. 1991. The Art of the Long View. Dou-
bleday.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pages 133?140.
Shohei Tanaka, Naoaki Okazaki, and Mitsuru Ishizuka.
2012. Acquiring and generalizing causal inference
rules from deverbal noun constructions. In Proceed-
ings of 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1209?
1218.
Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake,
Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii,
and Kentaro Torisawa. 2013. WISDOM2013: A
large-scale web information analysis system. In
Companion Volume of the Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013) (Demo Track), pages
45?48.
Kentaro Torisawa, Stijn de Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kak-
izawa, Masaki Murata, Kow Kuroda, and Ichiro Ya-
mada. 2010. Organizing the web?s information ex-
plosion to discover unknown unknowns. New Gen-
eration Computing (Special Issue on Information
Explosion), 28(3):217?236.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De
Saeger, Jong Hoon Oh, Jun?ichi Kazama, Chikara
Hashimoto, and Hayato Ohwada. 2011. Toward
finding semantic relations not written in a single sen-
tence: An inference method using auto-discovered
rules. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP 2011), pages 902?910.
Istva?n Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 1619?1629.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009.
Polynomial to linear: Efficient classification with
conjunctive features. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 542?1551.
997
