Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017?1026,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Weighted Alignment Matrices for Statistical Machine Translation
Yang Liu , Tian Xia , Xinyan Xiao and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,xiatian,xiaoxinyan,liuqun}@ict.ac.cn
Abstract
Current statistical machine translation sys-
tems usually extract rules from bilingual
corpora annotated with 1-best alignments.
They are prone to learn noisy rules due
to alignment mistakes. We propose a new
structure called weighted alignment matrix
to encode all possible alignments for a par-
allel text compactly. The key idea is to as-
sign a probability to each word pair to in-
dicate how well they are aligned. We de-
sign new algorithms for extracting phrase
pairs from weighted alignment matrices
and estimating their probabilities. Our ex-
periments on multiple language pairs show
that using weighted matrices achieves con-
sistent improvements over using n-best
lists in significant less extraction time.
1 Introduction
Statistical machine translation (SMT) relies heav-
ily on annotated bilingual corpora. Word align-
ment, which indicates the correspondence be-
tween the words in a parallel text, is one of the
most important annotations in SMT. Word-aligned
corpora have been found to be an excellent source
for translation-related knowledge, not only for
phrase-based models (Och and Ney, 2004; Koehn
et al, 2003), but also for syntax-based models
(e.g., (Chiang, 2007; Galley et al, 2006; Shen
et al, 2008; Liu et al, 2006)). Och and Ney
(2003) indicate that the quality of machine transla-
tion output depends directly on the quality of ini-
tial word alignment.
Modern alignment methods can be divided into
two major categories: generative methods and dis-
criminative methods. Generative methods (Brown
et al, 1993; Vogel and Ney, 1996) treat word
alignment as a hidden process and maximize the
likelihood of bilingual training corpus using the
expectation maximization (EM) algorithm. In
contrast, discriminative methods (e.g., (Moore et
al., 2006; Taskar et al, 2005; Liu et al, 2005;
Blunsom and Cohn, 2006)) have the freedom to
define arbitrary feature functions that describe var-
ious characteristics of an alignment. They usu-
ally optimize feature weights on manually-aligned
data. While discriminative methods show supe-
rior alignment accuracy in benchmarks, genera-
tive methods are still widely used to produce word
alignments for large sentence-aligned corpora.
However, neither generative nor discriminative
alignment methods are reliable enough to yield
high quality alignments for SMT, especially for
distantly-related language pairs such as Chinese-
English and Arabic-English. The F-measures for
Chinese-English and Arabic-English are usually
around 80% (Liu et al, 2005) and 70% (Fraser
and Marcu, 2007), respectively. As most current
SMT systems only use 1-best alignments for ex-
tracting rules, alignment errors might impair trans-
lation quality.
Recently, several studies have shown that offer-
ing more alternatives of annotations to SMT sys-
tems will result in significant improvements, such
as replacing 1-best trees with packed forests (Mi
et al, 2008) and replacing 1-best word segmenta-
tions with word lattices (Dyer et al, 2008). Sim-
ilarly, Venugopal et al (2008) use n-best align-
ments instead of 1-best alignments for translation
rule extraction. While they achieve significant im-
provements on the IWSLT data, extracting rules
from n-best alignments might be computationally
expensive.
In this paper, we propose a new structure named
weighted alignment matrix to represent the align-
ment distribution for a sentence pair compactly. In
a weighted matrix, each element that corresponds
to a word pair is assigned a probability to measure
the confidence of aligning the two words. There-
fore, a weighted matrix is capable of using a lin-
1017
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
Figure 1: An example of word alignment between
a pair of Chinese and English sentences.
ear space to encode the probabilities of exponen-
tially many alignments. We develop a new algo-
rithm for extracting phrase pairs from weighted
matrices and show how to estimate their relative
frequencies and lexical weights. Experimental re-
sults show that using weighted matrices achieves
consistent improvements in translation quality and
significant reduction in extraction time over using
n-best lists.
2 Background
Figure 1 shows an example of word alignment be-
tween a pair of Chinese and English sentences.
The Chinese and English words are listed horizon-
tally and vertically, respectively. The dark points
indicate the correspondence between the words in
two languages. For example, the first Chinese
word ?zhongguo? is aligned to the fourth English
word ?China?.
Formally, given a source sentence f = fJ
1
=
f
1
, . . . , f
j
, . . . , f
J
and a target sentence e = eI
1
=
e
1
, . . . , e
i
, . . . , e
I
, we define a link l = (j, i) to
exist if f
j
and e
i
are translation (or part of trans-
lation) of one another. Then, an alignment a is a
subset of the Cartesian product of word positions:
a ? {(j, i) : j = 1, . . . , J ; i = 1, . . . , I} (1)
Usually, SMT systems only use the 1-best align-
ments for extracting translation rules. For exam-
ple, given a source phrase ?f and a target phrase
e?, the phrase pair ( ?f , e?) is said to be consistent
(Och and Ney, 2004) with the alignment if and
only if: (1) there must be at least one word in-
side one phrase aligned to a word inside the other
phrase and (2) no words inside one phrase can be
aligned to a word outside the other phrase.
After all phrase pairs are extracted from the
training corpus, their translation probabilities can
be estimated as relative frequencies (Och and Ney,
2004):
?(e?|
?
f) =
count(
?
f, e?)
?
e?
?
count(
?
f , e?
?
)
(2)
where count( ?f , e?) indicates how often the phrase
pair ( ?f, e?) occurs in the training corpus.
Besides relative frequencies, lexical weights
(Koehn et al, 2003) are widely used to estimate
how well the words in ?f translate the words in
e?. To do this, one needs first to estimate a lexi-
cal translation probability distribution w(e|f) by
relative frequency from the same word alignments
in the training corpus:
w(e|f) =
count(f, e)
?
e
?
count(f, e
?
)
(3)
Note that a special source NULL token is added
to each source sentence and aligned to each un-
aligned target word.
As the alignment a? between a phrase pair ( ?f, e?)
is retained during extraction, the lexical weight
can be calculated as
p
w
(e?|
?
f, a?) =
|e?|
?
i=1
1
|{j|(j, i) ? a?}|
?
w(e
i
|f
j
) (4)
If there are multiple alignments a? for a phrase
pair ( ?f , e?), Koehn et al (2003) choose the one
with the highest lexical weight:
p
w
(e?|
?
f) = max
a?
{
p
w
(e?|
?
f, a?)
}
(5)
Simple and effective, relative frequencies and
lexical weights have become the standard features
in modern discriminative SMT systems.
3 Weighted Alignment Matrix
We believe that offering more candidate align-
ments to extracting translation rules might help
improve translation quality. Instead of using n-
best lists (Venugopal et al, 2008), we propose a
new structure called weighted alignment matrix.
We use an example to illustrate our idea. Fig-
ure 2(a) and Figure 2(b) show two alignments of
a Chinese-English sentence pair. We observe that
some links (e.g., (1,4) corresponding to the word
1018
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
1.0
0.6
0.40.4
1.0
1.0
0.4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(a) (b) (c)
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c)
the resulting weighted alignment matrix that takes the two alignments as samples, of which the initial
probabilities are 0.6 and 0.4, respectively.
pair (?zhongguo?, ?China?)) occur in both align-
ments, some links (e.g., (2,3) corresponding to the
word pair (?de?,?of?)) occur only in one align-
ment, and some links (e.g., (1,1) corresponding
to the word pair (?zhongguo?, ?the?)) do not oc-
cur. Intuitively, we can estimate how well two
words are aligned by calculating its relative fre-
quency, which is the probability sum of align-
ments in which the link occurs divided by the
probability sum of all possible alignments. Sup-
pose that the probabilities of the two alignments in
Figures 2(a) and 2(b) are 0.6 and 0.4, respectively.
We can estimate the relative frequencies for every
word pair and obtain a weighted matrix shown in
Figure 2(c). Therefore, each word pair is associ-
ated with a probability to indicate how well they
are aligned. For example, in Figure 2(c), we say
that the word pair (?zhongguo?, ?China?) is def-
initely aligned, (?zhongguo?, ?the?) is definitely
unaligned, and (?de?, ?of?) has a 60% chance to
get algned.
Formally, a weighted alignment matrix m is a
J ? I matrix, in which each element stores a link
probability p
m
(j, i) to indicate how well f
j
and
e
i
are aligned. Currently, we estimate link proba-
bilities from an n-best list by calculating relative
frequencies:
p
m
(j, i) =
?
a?N
p(a)? ?(a, j, i)
?
a?N
p(a)
(6)
=
?
a?N
p(a)? ?(a, j, i) (7)
where
?(a, j, i) =
{
1 (j, i) ? a
0 otherwise (8)
Note that N is an n-best list, p(a) is the probabil-
ity of an alignment a in the n-best list, ?(a, j, i)
indicates whether a link (j, i) occurs in the align-
ment a or not. We assign 0 to any unseen
alignment. As p(a) is usually normalized (i.e.,
?
a?N
p(a) ? 1), we remove the denominator in
Eq. (6).
Accordingly, the probability that the two words
f
j
and e
i
are not aligned is
p?
m
(j, i) = 1.0? p
m
(j, i) (9)
For example, as shown in Figure 2(c), the prob-
ability for the two words ?de? and ?of? being
aligned is 0.6 and the probability that they are not
aligned is 0.4.
Intuitively, the probability of an alignment a is
the product of link probabilities. If a link (j, i)
occurs in a, we use p
m
(j, i); otherwise we use
p?
m
(j, i). Formally, given a weighted alignment
matrix m, the probability of an alignment a can
be calculated as
p
m
(a) =
J
?
j=1
I
?
i=1
(p
m
(j, i) ? ?(a, j, i) +
p?
m
(j, i) ? (1? ?(a, j, i))) (10)
It proves that the sum of all alignment proba-
bilities is always 1:
?
a?A
p
m
(a) ? 1, where A
1019
1: procedure PHRASEEXTRACT(fJ
1
, e
I
1
, m, l)
2: R ? ?
3: for j
1
? 1 . . . J do
4: j
2
? j
1
5: while j
2
< J ? j
2
? j
1
< l do
6: T ? {i|?j : j
1
? j ? j
2
? p
m
(j, i) > 0}
7: i
l
? MIN(T )
8: i
u
? MAX(T )
9: for n? 1 . . . l do
10: for i
1
? i
l
? n + 1 . . . i
u
do
11: i
2
? i
1
+ n? 1
12: R ? R? {(f j2
j
1
, e
i
2
i
1
)}
13: end for
14: end for
15: j
2
? j
2
+ 1
16: end while
17: end for
18: returnR
19: end procedure
Figure 3: Algorithm for extracting phrase pairs
from a sentence pair ?fJ
1
, e
I
1
? annotated with a
weighted alignment matrix m.
is the set of all possible alignments. Therefore, a
weighted alignment matrix is capable of encoding
the probabilities of 2J?I alignments using only a
J ? I space.
Note that p
m
(a) is not necessarily equal to p(a)
because the encoding of a weighted alignment ma-
trix changes the alignment probability distribu-
tion. For example, while the initial probability of
the alignment in Figure 2(a) (i.e., p(a)) is 0.6, the
probability of the same alignment encoded in the
matrix shown in Figure 2(c) (i.e., p
m
(a)) becomes
0.1296 according to Eq. (10). It should be em-
phasized that a weighted matrix encodes all pos-
sible alignments rather than the input n-best list,
although the link probabilities are estimated from
the n-best list.
4 Phrase Pair Extraction
In this section, we describe how to extract phrase
pairs from the training corpus annotated with
weighted alignment matrices (Section 4.1) and
how to estimate their relative frequencies (Section
4.2) and lexical weights (Section 4.3).
4.1 Extraction Algorithm
Och and Ney (2004) describe a ?phrase-extract?
algorithm for extracting phrase pairs from a sen-
tence pair annotated with a 1-best alignment.
Given a source phrase, they first identify the target
phrase that is consistent with the alignment. Then,
they expand the boundaries of the target phrase if
the boundary words are unaligned.
Unfortunately, this algorithm cannot be directly
used to manipulate a weighted alignment matrix,
which is a compact representation of all pos-
sible alignments. The major difference is that
the ?tight? phrase that has both boundary words
aligned is not necessarily the smallest candidate
in a weighted matrix. For example, in Figure
2(a), the ?tight? target phrase corresponding to
the source phrase ?zhongguo de? is ?of China?.
According to Och?s algorithm, the target phrase
?China? breaks the alignment consistency and
therefore is not valid candidate. However, this is
not true for using the weighted matrix shown in
Figure 2(c). The target phrase ?China? is treated
as a ?potential? candidate 1, although it might be
assigned only a small fractional count (see Table
1).
Therefore, we enumerate all potential phrase
pairs and calculate their fractional counts for
eliminating less promising candidates. Figure 3
shows the algorithm for extracting phrases from
a weighted matrix. The input of the algorithm
is a source sentence fJ
1
, a target sentence eI
1
, a
weighted alignment matrix m, and a phrase length
limit l (line 1). After initializing R that stores col-
lected phrase pairs (line 2), we identify the cor-
responding target phrases for all possible source
phrases (lines 3-5). Given a source phrase f j2
j
1
, we
find the lower and upper bounds of target positions
(i.e., i
l
and i
u
) that have positive link probabili-
ties (lines 6-8). For example, the lower bound is
3 and the upper bound is 5 for the source phrase
?zhongguo de? in Figure 2(c). Finally, we enu-
merate all target phrases that allow for unaligned
boundary words with varying phrase lengths (lines
9-14). Note that we need to ensure that 1 ? i
1
? I
and 1 ? i
2
? I in lines 10-11, which are omitted
for simplicity.
4.2 Calculating Relative Frequencies
To estimate the relative frequency of a phrase pair,
we need to estimate how often it occurs in the
training corpus. Given an n-best list, the fractional
count of a phrase pair is the probability sum of
the alignments with which the phrase pair is con-
sistent. Obviously, it is unrealistic for a weighted
alignment matrix to enumerate all possible align-
ments explicitly to calculate fractional counts. In-
stead, we resort to link probabilities to calculate
1By potential, we mean that the fractional count of a
phrase pair is positive. Section 4.2 describes how to calcu-
late fractional counts.
1020
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
1.0
0.6
0.40.4
1.0
1.0
0.4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 4: An example of calculating fractional
count. Given the phrase pair (?zhongguo de?, ?of
China?), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
counts efficiently. Equivalent to explicit enumera-
tion, we interpret the fractional count of a phrase
pair as the probability that it satisfies the two align-
ment consistency conditions (see Section 2).
Given a phrase pair, we divide the elements of
a weighted alignment matrix into three categories:
(1) inside elements that fall inside the phrase pair,
(2) outside elements that fall outside the phrase
pair while fall in the same row or the same col-
umn, and (3) irrelevant elements that fall outside
the phrase pair while fall in neither the same row
nor the same column. Figure 4 shows an exam-
ple. Given the phrase pair (?zhongguo de?, ?of
China?), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
To what extent a phrase pair satisfies the align-
ment consistency is measured by calculating in-
side and outside probabilities. Although there are
the same terms in the parsing literature, they have
different meanings here. The inside probability in-
dicates the chance that there is at least one word
inside one phrase aligned to a word inside the
other phrase. The outside probability indicates the
chance that no words inside one phrase are aligned
to a word outside the other phrase.
Given a phrase pair (f j2
j
1
, e
i
2
i
1
), we denote the in-
side area as in(j
1
, j
2
, i
1
, i
2
) and the outside area
as out(j
1
, j
2
, i
1
, i
2
). Therefore, the inside proba-
bility of a phrase pair is calculated as
?(j
1
, j
2
, i
1
, i
2
) = 1?
?
(j,i)?in(j
1
,j
2
,i
1
,i
2
)
p?
m
(j, i) (11)
target phrase ? ? count
of China 1.0 0.36 0.36
of China ?s 1.0 0.36 0.36
China ?s 1.0 0.24 0.24
China 1.0 0.24 0.24
?s economy 0.4 0 0
Table 1: Some candidate target phrases of the
source phrase ?zhongguo de? in Figure 4, where ?
is inside probability, ? is outside probability, and
count is fractional count.
For example, the inside probability for (?zhong-
guo de?, ?of China?) in Figure 4 is 1.0, which
means that there always exists at least one aligned
word pair inside.
Accordingly, the outside probability of a phrase
pair is calculated as
?(j
1
, j
2
, i
1
, i
2
) =
?
(j,i)?out(j
1
,j
2
,i
1
,i
2
)
p?
m
(j, i) (12)
For example, the outside probability for
(?zhongguo de?, ?of China?) in Figure 4 is 0.36,
which means the probability that there are no
aligned word pairs outside is 0.36.
Finally, we use the product of inside and outside
probabilities as the fractional count of a phrase
pair:
count(f
j
2
j
1
, e
i
2
i
1
) = ?(j
1
, j
2
, i
1
, i
2
)?
?(j
1
, j
2
, i
1
, i
2
) (13)
Table 1 lists some candidate target phrases of
the source phrase ?zhongguo de? in Figure 4. We
also give their inside probabilities, outside proba-
bilities, and fractional counts.
After collecting the fractional counts from the
training corpus, we then use Eq. (2) to calculate
relative frequencies in two translation directions.
Often, our approach extracts a large amount of
phrase pairs from training corpus as we soften
the alignment consistency constraint. To main-
tain a reasonable phrase table size, we discard any
phrase pair that has a fractional count lower than
a threshold t. During extraction, we first obtain
a list of candidate target phrases for each source
phrase, as shown in Table 1. Then, we prune the
list according to the threshold t. For example, we
only retain the top two candidates in Table 1 if
t = 0.3. Note that we perform the pruning locally.
Although it is more reasonable to prune a phrase
table after accumulating all fractional counts from
1021
training corpus, such global pruning strategy usu-
ally leads to very large disk and memory require-
ments.
4.3 Calculating Lexical Weights
Recall that we need to obtain two translation prob-
ability tables w(e|f) and w(f |e) before calculat-
ing lexical weights (see Section 2). Following
Koehn et al (2003), we estimate the two distribu-
tions by relative frequencies from the training cor-
pus annotated with weighted alignment matrices.
In other words, we still use Eq. (3) but the way of
calculating fractional counts is different now.
Given a source word f
j
, a target word e
i
, and
a weighted alignment matrix, the fractional count
count(f
j
, e
i
) is p
m
(j, i). For NULL words, the
fractional counts can be calculated as
count(f
j
, e
0
) =
I
?
i=1
p?
m
(j, i) (14)
count(f
0
, e
i
) =
J
?
j=1
p?
m
(j, i) (15)
For example, in Figure 4, count(de, of) is 0.6,
count(de,NULL) is 0.24, and count(NULL,of) is
0.24.
Then, we adapt Eq. (4) to calculate lexical
weight:
p
w
(e?|
?
f ,m) =
|e?|
?
i=1
(
(
1
{j|p
m
(j, i) > 0}
?
?
?j:p
m
(j,i)>0
p(e
i
|f
j
)? p
m
(j, i)
)
+
p(e
i
|f
0
)?
|
?
f |
?
j=1
p?
m
(j, i)
)
(16)
For example, for the target word ?of? in Figure
4, the sum of aligned and unaligned probabilities
is
1
2
? (p(of|de)? 0.6 + p(of|fazhan)? 0.4) +
p(of|NULL)? 0.24
Note that we take link probabilities into account
and calculate the probability that a target word
translates a source NULL token explicitly.
5 Experiments
5.1 Data Preparation
We evaluated our approach on Chinese-to-English
translation. We used the FBIS corpus (6.9M
+ 8.9M words) as the training data. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT evaluation test
set as our development set, and used the NIST
2005 test set as our test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
To obtain weighted alignment matrices, we fol-
lowed Venugopal et al (2008) to produce n-
best lists via GIZA++. We first ran GIZA++
to produce 50-best lists in two translation direc-
tions. Then, we used the refinement technique
?grow-diag-final-and? (Koehn et al, 2003) to all
50 ? 50 bidirectional alignment pairs. Suppose
that p
s2t
and p
t2s
are the probabilities of an align-
ment pair assigned by GIZA++, respectively. We
used p
s2t
? p
t2s
as the probability of the result-
ing symmetric alignment. As different alignment
pairs might produce the same symmetric align-
ments, we followed Venugopal et al (2008) to
remove duplicate alignments and retain only the
alignment with the highest probability. Therefore,
there were 550 candidate alignments on average
for each sentence pair in the training data. We
obtained n-best lists by selecting the top n align-
ments from the 550-best lists. The probability of
each alignment in the n-best list was re-estimated
by re-normalization (Venugopal et al, 2008). Fi-
nally, these n-best alignments served as samples
for constructing weighted alignment matrices.
After extracting phrase pairs from n-best lists
and weighted alignment matrices, we ran Moses
(Koehn et al, 2007) to translate the development
and test sets. We used the simple distance-based
reordering model to remove the dependency of
lexicalization on word alignments for Moses.
5.2 Effect of Pruning Threshold
Our first experiment investigated the effect of
pruning threshold on translation quality (BLEU
scores on the test set) and the phrase table size (fil-
tered for the test set), as shown in Figure 5. To
save time, we extracted phrase pairs just from the
first 10K sentence pairs of the FBIS corpus. We
used 12 different thresholds: 0.0001, 0.001, 0.01,
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. Obvi-
ously, the lower the threshold is, the more phrase
pairs are extracted. When t = 0.0001, the number
of phrase pairs used on the test set was 460,284
1022
0.195
0.196
0.197
0.198
0.199
0.200
0.201
0.202
0.203
0.204
0.205
0.206
0.207
0.208
 150  200  250  300  350  400  450  500
BL
EU
 sc
or
e
phrase table size (103)
t=10-4
t=10-3
t=10-2
t=0.9...0.1
Figure 5: Effect of pruning threshold on transla-
tion quality and phrase table size.
and the BLEU score was 20.55. Generally, both
the number of phrase pairs and the BLEU score
went down with the increase of t. However, this
trend did not hold within the range [0.1, 0.9]. To
achieve a good tradeoff between translation qual-
ity and phrase table size, we set t = 0.01 for the
following experiments.
5.3 N -best lists Vs. Weighted Matrices
Figure 6 shows the BLEU scores and aver-
age extraction time using n-best alignments and
weighted matrices, respectively. We used the en-
tire training data for phrase extraction. When us-
ing 1-best alignments, Moses achieved a BLEU
score of 0.2826 and the average extraction time
was 4.19 milliseconds per sentence pair (see point
n = 1). The BLEU scores rose with the in-
crease of n for using n-best alignments. How-
ever, the score went down slightly when n = 50.
This suggests that including more noisy align-
ments might be harmful. These improvements
over 1-best alignments are not statistically signif-
icant. This finding failed to echo the promising
results reported by Venogopal et al (2008). We
think that there are two possible reasons. First,
they evaluated their approach on the IWSLT data
while we used the NIST data. It might be easier
to obtain significant improvements on the IWSLT
data in which the sentences are shorter. Sec-
ond, they used the hierarchical phrase-based sys-
tem while we used the phrase-based system, which
might be less sensitive to word alignments because
the alignments inside the phrase pairs hardly have
an effect.
When using weighted alignment matrices, we
0.280
0.281
0.282
0.283
0.284
0.285
0.286
0.287
0.288
0.289
0.290
0.291
0.292
0.293
 0  10  20  30  40  50  60  70  80  90
BL
EU
 sc
or
e
average extracting time (milliseconds/sentence pair)
n=1
n=5
n=10
n=50
n=5
n=10
n=50
n-best
m(n)
Figure 6: Comparison of n-best alignments and
weighted alignment matrices. We use m(n) to de-
note the matrices that take n-best lists as samples.
obtained higher BLEU scores than using n-best
lists with much less extraction time. We achieved
a BLEU score of 0.2901 when using the weighted
matrices estimated from 10-best lists. The abso-
lute improvement of 0.75 over using 1-best align-
ments (from 0.2826 to 0.2901) is statistically sig-
nificant at p < 0.05 by using sign-test (Collins
et al, 2005). Although the improvements over n-
best lists are not always statistically significant,
weighted alignment matrices maintain consistent
superiority in both translation quality and extrac-
tion speed.
5.4 Comparison of Parameter Estimation
In theory, the set of phrase pairs extracted from n-
best alignments is the subset of the set extracted
from the corresponding weighted matrices. In
practice, however, this is not true because we use
the pruning threshold t to maintain a reasonable
table size. Even so, the phrase tables produced by
n-best lists and weighted matrices still share many
phrase pairs.
Table 2 gives some statistics. We use m(10)
to represent the weighted matrices estimated from
10-best lists. ?all? denotes the full phrase table,
?shared? denotes the intersection of two tables,
and ?non-shared? denotes the complement. Note
that the probabilities of ?shared? phrase pairs are
different for the two approaches. We obtained
6.13M and 6.34M phrase pairs for the test set by
using 10-best lists and the corresponding matrices,
respectively. There were 4.58M phrase pairs in-
cluded by both tables. Note that the relative fre-
quencies and lexical weights for the same phrase
1023
shared non-shared all
method phrases BLEU phrases BLEU phrases BLEU
10-best 4.58M 28.35 1.55M 12.32 6.13M 28.47
m(10) 4.58M 28.90 1.76M 13.21 6.34M 29.01
Table 2: Comparison of phrase tables learned from n-best lists and weighted matrices. We use m(10)
to represent the weighted matrices estimated from 10-best lists. ?all? denotes the full phrase table,
?shared? denotes the intersection of two tables, and ?non-shared? denotes the complement. Note that the
probabilities of ?shared? phrase pairs are different for the two approaches.
0.200
0.210
0.220
0.230
0.240
0.250
0.260
0.270
0.280
0.290
 0  50  100  150  200  250
BL
EU
 sc
or
e
training corpus size (103)
1-best
10-best
m(10)
Figure 7: Comparison of n-best alignments and
weighted alignment matrices with varying training
corpus sizes.
pairs might be different in two tables. We found
that using matrices outperformed using n-best lists
even with the same phrase pairs. This suggests that
our methods for parameter estimation make better
use of noisy data. Another interesting finding was
that using the shared phrase pairs achieved almost
the same results with using full phrase tables.
5.5 Effect of Training Corpus Size
To investigate the effect of training corpus size on
our approach, we extracted phrase pairs from n-
best lists and weighted matrices trained on five
training corpora with varying sizes: 10K, 50K,
100K, 150K, and 239K sentence pairs. As shown
in Figure 7, our approach outperformed both 1-
best and n-best lists consistently. More impor-
tantly, the gains seem increase when more training
data are used.
5.6 Results on Other Language Pairs
To further examine the efficacy of the proposed ap-
proach, we scaled our experiments to large data
with multiple language pairs. We used the Eu-
roparl training corpus from the WMT07 shared
S?E F?E G?E
Sentences 1.26M 1.29M 1.26M
Foreign words 33.16M 33.18M 29.58M
English words 31.81M 32.62M 31.93M
Table 3: Statistics of the Europarl training data.
?S? denotes Spanish, ?E? denotes English, ?F? de-
notes French, ?G? denotes German.
1-best 10-best m(10)
S?E 30.90 30.97 31.03
E?S 31.16 31.25 31.34
F?E 30.69 30.76 30.82
E?F 26.42 26.65 26.54
G?E 24.46 24.58 24.66
E?G 18.03 18.30 18.20
Table 4: BLEU scores (case-insensitive) on the
Europarl data. ?S? denotes Spanish, ?E? denotes
English, ?F? denotes French, ?G? denotes Ger-
man.
task. 2 Table 3 shows the statistics of the train-
ing data. There are four languages (Spanish,
French, German, and English) and six transla-
tion directions (Foreign-to-English and English-
to-Foreign). We used the ?dev2006? data in the
?dev? directory as the development set and the
?test2006? data in the ?devtest? directory as the
test set. Both the development and test sets contain
2,000 sentences with single reference translations.
We tokenized and lowercased all the training,
development, and test data. We trained a 4-gram
language model using SRI Language Modeling
Toolkit on the target side of the training corpus for
each task. We ran GIZA++ on the entire train-
ing data to obtain n-best alignments and weighted
matrices. To save time, we just used the first 100K
sentences of each aligned training corpus to ex-
tract phrase pairs.
2http://www.statmt.org/wmt07/shared-task.html
1024
Table 4 lists the case-insensitive BLEU scores
of 1-best, 10-best, and m(10) on the Europarl
data. Using weighted packed matrices continued
to show advantage over using 1-best alignments on
multiple language pairs. However, these improve-
ments were very small and not significant. We at-
tribute this to the fact that GIZA++ usually pro-
duces high quality 1-best alignments for closely-
related European language pairs, especially when
trained on millions of sentences.
6 Related Work
Recent studies has shown that SMT systems
can benefit from making the annotation pipeline
wider: using packed forests instead of 1-best trees
(Mi et al, 2008), word lattices instead of 1-best
segmentations (Dyer et al, 2008), and n-best
alignments instead of 1-best alignments (Venu-
gopal et al, 2008). We propose a compact repre-
sentation of multiple word alignments that enables
SMT systems to make a better use of noisy align-
ments.
Matusov et al (2004) propose ?cost matrices?
for producing symmetric alignments. Kumar et al
(2007) describe how to use ?posterior probabil-
ity matrices? to improve alignment accuracy via
a bridge language. Although not using the term
?weighted matrices? directly, they both assign a
probability to each word pair.
We follow Och and Ney (2004) to develop
a new phrase extraction algorithm for weighted
alignment matrices. The methods for calculating
relative frequencies (Och and Ney, 2004) and lex-
ical weights (Koehn et al, 2003) are also adapted
for the weighted matrix case.
Many researchers (e.g., (Venugopal et al, 2003;
Deng et al, 2008)) observe that softening the
alignment consistency constraint help improve
translation quality. For example, Deng et al
(2008) define a feature named ?within phrase pair
consistency ratio? to measure the degree of consis-
tency. As each link is associated with a probability
in a weighted matrix, we use these probabilities to
evaluate the validity of a phrase pair.
We estimate the link probabilities by calculating
relative frequencies over n-best lists. Niehues and
Vogel (2008) propose a discriminative approach to
modeling the alignment matrix directly. The dif-
ference is that they assign a boolean value instead
of a probability to each word pair.
7 Conclusion and Future Work
We have presented a new structure called weighted
alignment matrix that encodes the alignment dis-
tribution for a sentence pair. Accordingly, we de-
velop new methods for extracting phrase pairs and
estimating their probabilities. Our experiments
show that the proposed approach achieves better
translation quality over using n-best lists in less
extraction time. An interesting finding is that our
approach performs better than the baseline even
they use the same phrase pairs.
Although our approach consistently outper-
forms using 1-best alignments for varying lan-
guage pairs, the improvements are comparatively
small. One possible reason is that taking n-best
lists as samples sometimes might change align-
ment probability distributions inappropriately. A
more principled solution is to directly model the
weighted alignment matrices, either in a genera-
tive or a discriminative way. We believe that better
estimation of alignment distributions will result in
more significant improvements.
Another interesting direction is applying our ap-
proach to extracting translation rules with hierar-
chical structures such as hierarchical phrases (Chi-
ang, 2007) and tree-to-string rules (Galley et al,
2006; Liu et al, 2006). We expect that these
syntax-based systems could benefit more from our
approach.
Acknowledgement
The authors were supported by Microsoft Re-
search Asia Natural Language Processing Theme
Program grant (2009-2010), High-Technology
R&D Program (863) Project No. 2006AA010108,
and National Natural Science Foundation of China
Contract 60736014. Part of this work was done
while Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Stephan Vogel, Alon Lavie, Fran-
cisco Guzman, Nguyen Bach, Andreas Zollmann,
Vamshi Ambati, and Kevin Gimpel for their help-
ful feedback.
References
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of COLING/ACL 2006, pages 65?72,
Sydney, Australia, July.
1025
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540, Ann Arbor, USA, June.
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008.
Phrase table training for precision and recall: What
makes a good phrase and a good phrase pair?
In Proceedings of ACL/HLT 2008, pages 81?88,
Columbus, Ohio, USA, June.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of ACL/HLT 2008, pages
1012?1020, Columbus, Ohio, June.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing word alignment quality for statistical machine
translation. Computational Linguistics, Squibs and
Discussions, 33(3):293?303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL 2006, pages 961?968,
Sydney, Australia, July.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT/NAACL 2003, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL 2007 (poster), pages
77?80, Prague, Czech Republic, June.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of EMNLP 2007,
pages 42?50, Prague, Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of ACL 2005, pages 459?466, Ann Arbor, Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL 2006,
pages 609?616, Sydney, Australia, July.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING
2004, pages 219?225, Geneva, Switzerland, August.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL/HLT 2008,
pages 192?199, Columbus, Ohio, June.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In Proceedings of COLING/ACL 2006,
pages 513?520, Sydney, Australia, July.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of WMT-3, pages 18?25, Columbus,
Ohio, USA, June.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL/HLT 2008, pages 577?585,
Columbus, Ohio, June.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proceedings of ICSLP
2002, pages 901?904, Denver, Colorado, Septem-
ber.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73?80, Vancouver, British Columbia, Canada,
October.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL 2003,
pages 319?326, Sapporo, Japan, July.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-
best alignments and parses in mt training. In Pro-
ceedings of AMTA 2008, pages 192?201, Waikiki,
Hawaii, October.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836?841, Copen-
hagen, Danmark, August.
1026
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200?1208,
Beijing, August 2010
Joint Tokenization and Translation
Xinyan Xiao ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
As tokenization is usually ambiguous for
many natural languages such as Chinese
and Korean, tokenization errors might po-
tentially introduce translation mistakes for
translation systems that rely on 1-best to-
kenizations. While using lattices to of-
fer more alternatives to translation sys-
tems have elegantly alleviated this prob-
lem, we take a further step to tokenize
and translate jointly. Taking a sequence
of atomic units that can be combined to
form words in different ways as input, our
joint decoder produces a tokenization on
the source side and a translation on the
target side simultaneously. By integrat-
ing tokenization and translation features
in a discriminative framework, our joint
decoder outperforms the baseline trans-
lation systems using 1-best tokenizations
and lattices significantly on both Chinese-
English and Korean-Chinese tasks. In-
terestingly, as a tokenizer, our joint de-
coder achieves significant improvements
over monolingual Chinese tokenizers.
1 Introduction
Tokenization plays an important role in statistical
machine translation (SMT) because tokenizing a
source-language sentence is always the first step
in SMT systems. Based on the type of input, Mi
and Huang (2008) distinguish between two cat-
egories of SMT systems : string-based systems
(Koehn et al, 2003; Chiang, 2007; Galley et al,
source
target
tokenize+translate
string tokenization
translation
source
target
string
tokenize
tokenization
translate
translation
(a)
(b)
Figure 1: (a) Separate tokenization and translation and (b)
joint tokenization and translation.
2006; Shen et al, 2008) that take a string as input
and tree-based systems (Liu et al, 2006; Mi et al,
2008) that take a tree as input. Note that a tree-
based system still needs to first tokenize the input
sentence and then obtain a parse tree or forest of
the sentence. As shown in Figure 1(a), we refer to
this pipeline as separate tokenization and transla-
tion because they are divided into single steps.
As tokenization for many languages is usually
ambiguous, SMT systems that separate tokeniza-
tion and translation suffer from a major drawback:
tokenization errors potentially introduce transla-
tion mistakes. As some languages such as Chi-
nese have no spaces in their writing systems, how
to segment sentences into appropriate words has
a direct impact on translation performance (Xu et
al., 2005; Chang et al, 2008; Zhang et al, 2008).
In addition, although agglutinative languages such
as Korean incorporate spaces between ?words?,
which consist of multiple morphemes, the gran-
ularity is too coarse and makes the training data
1200
considerably sparse. Studies reveal that seg-
menting ?words? into morphemes effectively im-
proves translating morphologically rich languages
(Oflazer, 2008). More importantly, a tokenization
close to a gold standard does not necessarily leads
to better translation quality (Chang et al, 2008;
Zhang et al, 2008). Therefore, it is necessary
to offer more tokenizations to SMT systems to
alleviate the tokenization error propagation prob-
lem. Recently, many researchers have shown that
replacing 1-best tokenizations with lattices im-
proves translation performance significantly (Xu
et al, 2005; Dyer et al, 2008; Dyer, 2009).
We take a next step towards the direction of
offering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), our approach tokenizes
and translates jointly to find a tokenization and
a translation for a source-language string simul-
taneously. We integrate translation and tokeniza-
tion models into a discriminative framework (Och
and Ney, 2002), within which tokenization and
translation models interact with each other. Ex-
periments show that joint tokenization and trans-
lation outperforms its separate counterparts (1-
best tokenizations and lattices) significantly on
the NIST 2004 and 2005 Chinese-English test
sets. Our joint decoder also reports positive results
on Korean-Chinese translation. As a tokenizer,
our joint decoder achieves significantly better to-
kenization accuracy than three monolingual Chi-
nese tokenizers.
2 Separate Tokenization and Translation
Tokenization is to split a string of characters into
meaningful elements, which are often referred to
as words. Typically, machine translation sepa-
rates tokenization from decoding as a preprocess-
ing step. An input string is first preprocessed by a
tokenizer, and then is translated based on the tok-
enized result. Take the SCFG-based model (Chi-
ang, 2007) as an example. Given the character
sequence of Figure 2(a), a tokenizer first splits it
into the word sequence as shown in Figure 2(b),
then the decoder translates the word sequence us-
ing the rules in Table 1.
This approach makes the translation process
simple and efficient. However, it may not be
? ? ? ? ? ? 0? 1 2 3 4 5 6 7
Figure 2: Chinese tokenization: (a) character sequence; (b)
and (c) tokenization instances; (d) lattice created from (b)
and (c). We insert ?-? between characters in a word just for
clarity.
r1 tao-fei-ke ?Taufik
r2 duo fen ? gain a point
r3 x1 you-wang x2 ? x1 will have the chance to x2
Table 1: An SCFG derivation given the tokenization of Fig-
ure 2(b).
optimal for machine translation. Firstly, optimal
granularity is unclear for machine translation. We
might face severe data sparseness problem by us-
ing large granularity, while losing much useful in-
formation with small one. Consider the example
in Figure 2. It is reasonable to split duo fen into
two words as duo and fen, since they have one-
to-one alignments to the target side. Nevertheless,
while you and wang also have one-to-one align-
ments, it is risky to segment them into two words.
Because the decoder is prone to translate wang as
a verb look without the context you. Secondly,
there may be tokenization errors. In Figure2(c),
tao fei ke is recognized as a Chinese person name
with the second name tao and the first name fei-ke,
but the whole string tao fei ke should be a name of
the Indonesian badminton player.
Therefore, it is necessary to offer more tok-
enizations to SMT systems to alleviate the tok-
enization error propagation problem. Recently,
many researchers have shown that replacing 1-
best tokenizations with lattices improves transla-
tion performance significantly. In this approach, a
lattice compactly encodes many tokenizations and
is fixed before decoding.
1201
0 1 2 3 4 5 6 7
1 2
3
Figure 3: A derivation of the joint model for the tokenization
in Figure 2(b) and the translation in Figure 2 by using the
rules in Table 1. N means tokenization while  represents
translation.
3 Joint Tokenization and Translation
3.1 Model
We take a next step towards the direction of of-
fering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), the decoder takes an un-
tokenized string as input, and then tokenizes the
source side string while building the correspond-
ing translation of the target side. Since the tradi-
tional rules like those in Table 1 natively include
tokenization information, we can directly apply
them for simultaneous construction of tokeniza-
tion and translation by the source side and target
side of rules respectively. In Figure 3, our joint
model takes the character sequence in Figure 2(a)
as input, and synchronously conducts both trans-
lation and tokenization using the rules in Table 1.
As our model conducts tokenization during de-
coding, we can integrate tokenization models as
features together with translation features under
the discriminative framework. We expect tok-
enization and translation could collaborate with
each other. Tokenization offers translation with
good tokenized results, while translation helps to-
kenization to eliminate ambiguity. Formally, the
probability of a derivation D is represented as
P (D) ?
?
i
?i(D)?i (1)
where ?i are features defined on derivations in-
cluding translation and tokenization, and ?i are
feature weights. We totally use 16 features:
? 8 traditional translation features (Chiang,
2007): 4 rule scores (direct and reverse trans-
lation scores; direct and reverse lexical trans-
lation scores); language model of the target
side; 3 penalties for word count, extracted
rule and glue rule.
? 8 tokenization features: maximum entropy
model, language model and word count of
the source side (Section 3.2). To handle
the Out Of Vocabulary (OOV) problem (Sec-
tion 3.3), we also introduce 5 OOV features:
OOV character count and 4 OOV discount
features.
Since our model is still a string-based model, the
CKY algorithm and cube pruning are still applica-
ble for our model to find the derivation with max
score.
3.2 Adding Tokenization Features
Maximum Entropy model (ME). We first intro-
duce ME model feature for tokenization by cast-
ing it as a labeling problem (Xue and Shen, 2003;
Ng and Low, 2004). We label a character with the
following 4 types:
? b: the begin of a word
? m: the middle of a word
? e: the end of a word
? s: a single-character word
Taking the tokenization you-wang of the string
you wang for example, we first create a label se-
quence b e for the tokenization you-wang and then
calculate the probability of tokenization by
P (you-wang | you wang)
= P (b e | you wang)
= P (b | you, you wang)
? P (e | wang, you wang)
Given a tokenization wL1 with L words for a
character sequence cn1 , we firstly create labels ln1
for every characters and then calculate the proba-
bility by
P (wL1 |cn1 ) = P (ln1 |cn1 ) =
n?
i=1
P (li|ci, cn1 ) (2)
1202
Under the ME framework, the probability of as-
signing the character c with the label l is repre-
sented as:
P (l|c, cn1 ) =
exp[?i ?ihi(l, c, cn1 )]?
l? exp[
?
i ?ihi(l?, c, cn1 )]
(3)
where hi is feature function, ?i is the feature
weight of hi. We use the feature templates the
same as Jiang et al, (2008) to extract features for
ME model. Since we directly construct tokeniza-
tion when decoding, it is straight to calculate the
ME model score of a tokenization according to
formula (2) and (3).
Language Model (LM). We also use the n-
gram language model to calculate the probability
of a tokenization wL1 :
P (wL1 ) =
L?
i=1
P (wi|wi?1i?n+1) (4)
For instance, we compute the probability of the
tokenization shown in Figure 2(b) under a 3-gram
model by
P (tao-fei-ke)
?P (you-wang | tao-fei-ke)
?P (duo | tao-fei-ke, you-wang)
?P (fen | you-wang, duo)
Word Count (WC). This feature counts the
number of words in a tokenization. Language
model is prone to assign higher probabilities to
short sentences in a biased way. This feature can
compensate this bias by encouraging long sen-
tences. Furthermore, using this feature, we can
optimize the granularity of tokenization for trans-
lation. If larger granularity is preferable for trans-
lation, then we can use this feature to punish the
tokenization containing more words.
3.3 Considering All Tokenizations
Obviously, we can construct the potential tok-
enizations and translations by only using the ex-
tracted rules, in line with traditional translation
decoding. However, it may limits the potential to-
kenization space. Consider a string you wang. If
you-wang is not reachable by the extracted rules,
the tokenization you-wang will never be consid-
ered under this way. However, the decoder may
still create a derivation by splitting the string as
small as possible with tokenization you wang and
translating you with a and wang with look, which
may hurt the translation performance. This case
happens frequently for named entity especially.
Overall, it is necessary to assure that the de-
coder can derive all potential tokenizations (Sec-
tion 4.1.3).
To assure that, when a span is not tokenized into
a single word by the extracted rules, we will add
an operation, which is considering the entire span
as an OOV. That is, we tokenize the entire span
into a single word with a translation that is the
copy of source side. We can define the set of all
potential tokenizations ?(cn1 ) for the character se-
quence cn1 in a recursive way by
?(cn1 ) =
n?1?
i
{?(ci1)
?
{w(cni+1)}} (5)
here w(cni+1) means a word contains characters
cni+1 and
?
means the times of two sets. Ac-
cording to this recursive definition, it is easy to
prove that all tokenizations is reachable by using
the glue rule (S ? SX,SX) and the added op-
eration. Here, glue rule is used to concatenate the
translation and tokenization of the two variables S
and X, which acts the role of the operator ? in
equation (5).
Consequently, this introduces a large number
of OOVs. In order to control the generation of
OOVs, we introduce the following OOV features:
OOV Character Count (OCC). This feature
counts the number of characters covered by OOV.
We can control the number of OOV characters by
this feature. It counts 3 when tao-fei-ke is an OOV,
since tao-fei-ke has 3 characters.
OOV Discount (OD). The chances to be OOVs
vary for words with different counts of characters.
We can directly attack this problem by adding
features ODi that reward or punish OOV words
which contains with i characters, or ODi,j for
OOVs contains with i to j characters. 4 OD fea-
tures are used in this paper: 1, 2, 3 and 4+. For
example, OD3 counts 1 when the word tao-fei-ke
is an OOV.
1203
Method Train #Rule Test TFs MT04 MT05 Speed
Separate
ICT 151M ICT ? 34.82 33.06 2.48
SF 148M SF ? 35.29 33.22 2.55
ME 141M ME ? 33.71 30.91 2.34
All 219M Lattice ? 35.79 33.95 3.83? 35.85 33.76 6.79
Joint
ICT 151M
Character
?
36.92 34.69 17.66
SF 148M 37.02 34.56 17.37
ME 141M 36.78 34.17 17.23
All 219M 37.25** 34.88** 17.52
Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train
and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8
tokenization features is used (?) or not (?). ICT, SF and ME are segmenter names for preprocessing. All means combined
corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al, (2008). ** means
significantly (Koehn, 2004) better than Lattice (p < 0.01).
4 Experiments
In this section, we try to answer the following
questions:
1. Does the joint method outperform conven-
tional methods that separate tokenization
from decoding. (Section 4.1)
2. How about the tokenization performance of
the joint decoder? (Section 4.2)
4.1 Translation Evaluation
We use the SCFG model (Chiang, 2007) for our
experiments. We firstly work on the Chinese-
English translation task. The bilingual training
data contains 1.5M sentence pairs coming from
LDC data.1 The monolingual data for training
English language model includes Xinhua portion
of the GIGAWORD corpus, which contains 238M
English words. We use the NIST evaluation sets
of 2002 (MT02) as our development data set, and
sets of 2004(MT04) and 2005(MT05) as test sets.
We use the corpus derived from the People?s Daily
(Renmin Ribao) in Feb. to Jun. 1998 containing
6M words for training LM and ME tokenization
models.
Translation Part. We used GIZA++ (Och and
Ney, 2003) to perform word alignment in both di-
rections, and grow-diag-final-and (Koehn et al,
2003) to generate symmetric word alignment. We
extracted the SCFG rules as describing in Chiang
(2007). The language model were trained by the
1including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and
LDC2005T06
SRILM toolkit (Stolcke, 2002).2 Case insensitive
NIST BLEU (Papineni et al, 2002) was used to
measure translation performance.
Tokenization Part. We used the toolkit imple-
mented by Zhang (2004) to train the ME model.
Three Chinese word segmenters were used for
comparing: ICTCLAS (ICT) developed by insti-
tute of Computing Technology Chinese Academy
of Sciences (Zhang et al, 2003); SF developed at
Stanford University (Huihsin et al, 2005) and ME
which exploits the ME model described in section
(3.2).
4.1.1 Joint Vs. Separate
We compared our joint tokenization and trans-
lation with the conventional separate methods.
The input of separate tokenization and translation
can either be a single segmentation or a lattice.
The lattice combines the 1-best segmentations of
segmenters. Same as Dyer et al, (2008), we also
extracted rules from a combined bilingual corpus
which contains three copies from different seg-
menters. We refer to this version of rules as All.
Table 2 shows the result.3 Using all rule ta-
ble, our joint method significantly outperforms the
best single system SF by +1.96 and +1.66 points
on MT04 and MT05 respectively, and also out-
performs the lattice-based system by +1.46 and
+0.93 points. However, the 8 tokenization fea-
tures have small impact on the lattice system,
probably because the tokenization space limited
2The calculation of LM probabilities for OOVs is done
by the SRILM without special treatment by ourself.
3The weights are retrained for different test conditions, so
do the experiments in other sections.
1204
ME LM WC OCC OD MT05
? ? ? ? ? 24.97? ? ? ? ? 25.30
? ? ? ? ? 24.70
? ? ? ? ? 24.84
? ? ? ? ? 25.51
? ? ? ? ? 25.34
? ? ? ? ? 25.74? ? ? ? ?
26.37
Table 3: Effect of tokenization features on Chinese-English
translation task. ?
?
? denotes using a tokenization feature
while ??? denotes that it is inactive.
by lattice has been created from good tokeniza-
tion. Not surprisingly, our decoding method is
about 2.6 times slower than lattice method with
tokenization features, since the joint decoder takes
character sequences as input, which is about 1.7
times longer than the corresponding word se-
quences tokenized by segmenters. (Section 4.1.4).
The number of extracted rules with different
segment methods are quite close, while the All
version contains about 45% more rules than the
single systems. With the same rule table, our joint
method improves the performance over separate
method up to +3.03 and +3.26 points (ME). In-
terestingly, comparing with the separate method,
the tokenization of training data has smaller effect
on joint method. The BLEU scores of MT04 and
MT05 fluctuate about 0.5 and 0.7 points when ap-
plying the joint method, while the difference of
separate method is up to 2 and 3 points respec-
tively. It shows that the joint method is more ro-
bust to segmentation performance.
4.1.2 Effect of Tokenization Model
We also investigated the effect of tokenization
features on translation. In order to reduce the time
for tuning weights and decoding, we extracted
rules from the FBIS part of the bilingual corpus,
and trained a 4-gram English language model on
the English side of FBIS.
Table 3 shows the result. Only using the 8 trans-
lation features, our system achieves a BLEU score
of 24.97. By activating all tokenization features,
the joint decoder obtains an absolute improve-
ment by 1.4 BLEU points. When only adding
one single tokenization feature, the LM and WC
fail to show improvement, which may result from
their bias to short or long tokenizations. How-
Method BLEU #Word Grau #OOV
ICT 33.06 30,602 1.65 644
SF 33.22 30,119 1.68 882
ME 30.91 29,717 1.70 1,614
Lattice 33.95 30,315 1.66 494
JointICT 34.69 29,723 1.70 996
JointSF 34.56 29,839 1.69 972
JointME 34.17 29,771 1.70 1,062
JointAll 34.88 29,644 1.70 883
Table 4: Granularity (Grau, counts of character per word)
and counts of OOV words of different methods on MT05.
The subscript of joint means the type of rule table.
ever, these two features have complementary ad-
vantages and collaborate well when using them to-
gether (line 8). The OCC and OD features also
contribute improvements which reflects the fact
that handling the generation of OOV is important
for the joint model.
4.1.3 Considering All Tokenizations?
In order to explain the necessary of considering
all potential tokenizations, we compare the perfor-
mances of whether to tokenize a span as a single
word or not as illustrated in section 3.3. When
only tokenizing by the extracted rules, we obtain
34.37 BLEU on MT05, which is about 0.5 points
lower than considering all tokenizations shown in
Table 2. This indicates that spuriously limitation
of the tokenization space may degenerate transla-
tion performance.
4.1.4 Results Analysis
To better understand why the joint method can
improve the translation quality, this section shows
some details of the results on the MT05 data set.
Table 4 shows the granularity and OOV word
counts of different configurations. The lattice
method reduces the OOV words quite a lot which
is 23% and 70% comparing with ICT and ME. In
contrast, the joint method gain an absolute im-
provement even thought the OOV count do not
decrease. It seems the lattice method prefers to
translate more characters (since smaller granular-
ity and less OOVs), while our method is inclined
to maintain integrity of words (since larger granu-
larity and more OOVs). This also explains the dif-
ficulty of deciding optimal tokenization for trans-
lation before decoding.
There are some named entities or idioms that
1205
Method Type F1 Time
Monolingual
ICT 97.47 0.010
SF 97.48 0.007
ME 95.53 0.008
Joint
ICT 97.68 9.382
SF 97.68 10.454
ME 97.60 10.451
All 97.70 9.248
Table 5: Comparison of segmentation performance in terms
of F1 score and speed (second per sentence). Type column
means the segmenter for monolingual method, while repre-
sents the rule tables used by joint method.
are split into smaller granularity by the seg-
menters. For example:???? which is an English
name ?Stone? or ??-g -u? which means
?teenage?. Although the separate method is possi-
ble to translate them using smaller granularity, the
translation results are in fact wrong. In contrast,
the joint method tokenizes them as entire OOV
words, however, it may result a better translation
for the whole sentence.
We also count the overlap of the segments
used by the JointAll system towards the single
segmentation systems. The tokenization result
of JointAll contains 29, 644 words, and shares
28, 159 , 27, 772 and 27, 407 words with ICT ,
SF and ME respectively. And 46 unique words
appear only in the joint method, where most of
them are named entity.
4.2 Chinese Word Segmentation Evaluation
We also test the tokenization performance of our
model on Chinese word segmentation task. We
randomly selected 3k sentences from the corpus
of People?s Daily in Jan. 1998. 1k sentences
were used for tuning weights, while the other 2k
sentences were for testing. We use MERT (Och,
2003) to tune the weights by minimizing the error
measured by F1 score.
As shown in Table 5, with all features activated,
our joint decoder achieves an F1 score of 97.70
which reduces the tokenization error comparing
with the best single segmenter ICT by 8.7%. Sim-
ilar to the translation performance evaluation, our
joint decoder outperforms the best segmenter with
any version of rule tables.
Feature F1
TFs 97.37
TFs + RS 97.65
TFs + LM 97.67
TFs + RS + LM 97.62
All 97.70
Table 6: Effect of the target side information on Chinese
word segmentation. TFs stands for the 8 tokenization fea-
tures. All represents all the 16 features.
4.2.1 Effect of Target Side Information
We compared the effect of the 4 Rule Scores
(RS), target side Language Model (LM) on tok-
enization. Table 6 shows the effect on Chinese
word segmentation. When only use tokenization
features, our joint decoder achieves an F1 score
of 97.37. Only integrating language model or rule
scores, the joint decoder achieves an absolute im-
provement of 0.3 point in F1 score, which reduces
the error rate by 11.4%. However, when combin-
ing them together, the F1 score deduces slightly,
which may result from the weight tuning. Us-
ing all feature, the performance comes to 97.70.
Overall, our experiment shows that the target side
information can improve the source side tokeniza-
tion under a supervised way, and outperform state-
of-the-art systems.
4.2.2 Best Tokenization = Best Translation?
Previous works (Zhang et al, 2008; Chang et
al., 2008) have shown that preprocessing the in-
put string for decoder by better segmenters do
not always improve the translation quality, we re-
verify this by testing whether the joint decoder
produces good tokenization and good translation
at the same time. To answer the question, we
used the feature weights optimized by maximiz-
ing BLEU for tokenization and used the weights
optimized by maximizing F1 for translation. We
test BLEU on MT05 and F1 score on the test data
used in segmentation evaluation experiments. By
tuning weights regarding to BLEU (the configura-
tion for JointAll in table 2), our decoder achieves
a BLEU score of 34.88 and an F1 score of 92.49.
Similarly, maximizing F1 (the configuration for
the last line in table 6) leads to a much lower
BLEU of 27.43, although the F1 is up to 97.70.
This suggests that better tokenization may not al-
ways lead to better translations and vice versa
1206
Rule #Rule Method Test Time
Morph 46M Separate 21.61 4.12Refined 55M 21.21 4.63
All 74M Joint 21.93* 5.10
Table 7: Comparison of Separate and Joint method in terms
of BLEU score and decoding speed (second per sentence) on
Korean-Chinese translation task.
even by the joint decoding. This also indicates the
hard of artificially defining the best tokenization
for translation.
4.3 Korean-Chinese Translation
We also test our model on a quite different task:
Korean-Chinese. Korean is an agglutinative lan-
guage, which comes from different language fam-
ily comparing with Chinese.
We used a newswire corpus containing 256k
sentence pairs as training data. The development
and test data set contain 1K sentence each with
one single reference. We used the target side of
training set for language model training. The Ko-
rean part of these data were tokenized into mor-
pheme sequence as atomic unit for our experi-
ments.
We compared three methods. First is directly
use morpheme sequence (Morph). The second
one is refined data (Refined), where we use selec-
tive morphological segmentation (Oflazer, 2008)
for combining morpheme together on the training
data. Since the selective method needs alignment
information which is unavailable in the decod-
ing, the test data is still of morpheme sequence.
These two methods still used traditional decoding
method. The third one extracting rules from com-
bined (All) data of methods 1 and 2, and using
joint decoder to exploit the different granularity
of rules.
Table 7 shows the result. Since there is no gold
standard data for tokenization, we do not use ME
and LM tokenization features here. However, our
joint method can still significantly (p < 0.05) im-
prove the performance by about +0.3 points. This
also reflects the importance of optimizing granu-
larity for morphological complex languages.
5 Related Work
Methods have been proposed to optimize tok-
enization for word alignment. For example, word
alignment can be simplified by packing (Ma et al,
2007) several consecutive words together. Word
alignment and tokenization can also be optimized
by maximizing the likelihood of bilingual corpus
(Chung and Gildea, 2009; Xu et al, 2008). In fact,
these work are orthogonal to our joint method,
since they focus on training step while we are con-
cerned of decoding. We believe we can further
the performance by combining these two kinds of
work.
Our work also has connections to multilingual
tokenization (Snyder and Barzilay, 2008). While
they have verified that tokenization can be im-
proved by multilingual learning, our work shows
that we can also improve tokenization by collabo-
rating with translation task in a supervised way.
More recently, Liu and Liu (2010) also shows
the effect of joint method. They integrate parsing
and translation into a single step and improve the
performance of translation significantly.
6 Conclusion
We have presented a novel method for joint tok-
enization and translation which directly combines
the tokenization model into the decoding phase.
Allowing tokenization and translation to collab-
orate with each other, tokenization can be opti-
mized for translation, while translation also makes
contribution to tokenization performance under a
supervised way. We believe that our approach can
be applied to other string-based model such as
phrase-based model (Koehn et al, 2003), string-
to-tree model (Galley et al, 2006) and string-to-
dependency model (Shen et al, 2008).
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang, Zhiyang Wang and Zongcheng Ji for
their helpful feedback.
1207
References
Chang, Pi-Chuan, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In the
Third Workshop on SMT.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chung, Tagyoung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP 2009.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proc. ACL 2008.
Dyer, Chris. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc.
NAACL 2009.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL 2006.
Huihsin, Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In
Fourth SIGHAN Workshop.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proc. ACL 2008.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Liu, Yang and Qun Liu. 2010. Joint parsing and trans-
lation. In Proc. Coling 2010.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL 2006.
Ma, Yanjun, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proc. ACL 2007.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proc. EMNLP
2004.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Proc. ACL
2003.
Oflazer, Kemal. 2008. Statistical machine translation
into a morphologically complex language. In Proc.
CICL 2008.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
ACL 2002.
Shen, Libin, Xu Jinxi, and Weischedel Ralph. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL 2008.
Snyder, Benjamin and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proc. ACL 2008.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit.
Xu, Jia, Evgeny Matusov, Richard Zens, and Her-
mann Ney. 2005. Integrated chinese word segmen-
tation in statistical machine translation. In Proc.
IWSLT2005.
Xu, Jia, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
chinese word segmentation for statistical machine
translation. In Proc. Coling 2008.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In SIGHAN Work-
shop.
Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. Hhmm-based chinese lexical an-
alyzer ictclas. In the Second SIGHAN Workshop.
Zhang, Ruiqiang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In the Third
Workshop on SMT.
Zhang, Le. 2004. Maximum entropy modeling toolkit
for python and c++.
1208
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880?888,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Fast Generation of Translation Forest
for Large-Scale SMT Discriminative Training
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn
Abstract
Although discriminative training guarantees to
improve statistical machine translation by in-
corporating a large amount of overlapping fea-
tures, it is hard to scale up to large data due to
decoding complexity. We propose a new al-
gorithm to generate translation forest of train-
ing data in linear time with the help of word
alignment. Our algorithm also alleviates the
oracle selection problem by ensuring that a
forest always contains derivations that exactly
yield the reference translation. With millions
of features trained on 519K sentences in 0.03
second per sentence, our system achieves sig-
nificant improvement by 0.84 BLEU over the
baseline system on the NIST Chinese-English
test sets.
1 Introduction
Discriminative model (Och and Ney, 2002) can
easily incorporate non-independent and overlapping
features, and has been dominating the research field
of statistical machine translation (SMT) in the last
decade. Recent work have shown that SMT benefits
a lot from exploiting large amount of features (Liang
et al, 2006; Tillmann and Zhang, 2006; Watanabe
et al, 2007; Blunsom et al, 2008; Chiang et al,
2009). However, the training of the large number
of features was always restricted in fairly small data
sets. Some systems limit the number of training ex-
amples, while others use short sentences to maintain
efficiency.
Overfitting problem often comes when training
many features on a small data (Watanabe et al,
2007; Chiang et al, 2009). Obviously, using much
more data can alleviate such problem. Furthermore,
large data also enables us to globally train millions
of sparse lexical features which offer accurate clues
for SMT. Despite these advantages, to the best of
our knowledge, no previous discriminative training
paradigms scale up to use a large amount of training
data. The main obstacle comes from the complexity
of packed forests or n-best lists generation which
requires to search through all possible translations
of each training example, which is computationally
prohibitive in practice for SMT.
To make normalization efficient, contrastive esti-
mation (Smith and Eisner, 2005; Poon et al, 2009)
introduce neighborhood for unsupervised log-linear
model, and has presented positive results in various
tasks. Motivated by these work, we use a translation
forest (Section 3) which contains both ?reference?
derivations that potentially yield the reference trans-
lation and also neighboring ?non-reference? deriva-
tions that fail to produce the reference translation.1
However, the complexity of generating this transla-
tion forest is up to O(n6), because we still need bi-
parsing to create the reference derivations.
Consequently, we propose a method to fast gener-
ate a subset of the forest. The key idea (Section 4)
is to initialize a reference derivation tree with maxi-
mum score by the help of word alignment, and then
traverse the tree to generate the subset forest in lin-
ear time. Besides the efficiency improvement, such
a forest allows us to train the model without resort-
1Exactly, there are no reference derivations, since derivation
is a latent variable in SMT. We call them reference derivation
just for convenience.
880
0,4
0,1
2,4
3,4
1 30 4
21
3
5
4
6
2
hyper-
edge rule
e1 r1 X ? ?X1 bei X2, X1 was X2?
e2 r2 X ? ?qiangshou bei X1,
the gunman was X1?
e3 r3 X ? ?jingfang X1, X1 by the police?
e4 r4 X ? ?jingfang X1, police X1 ?
e5 r5 X ? ?qiangshou, the gunman?
e6 r6 X ? ?jibi, shot dead?
Figure 1: A translation forest which is the running example throughout this paper. The reference translation is ?the
gunman was killed by the police?. (1) Solid hyperedges denote a ?reference? derivation tree t1 which exactly yields
the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to
swap the order ofX3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally,
this is done by deleting a node X0,1.
ing to constructing the oracle reference (Liang et al,
2006; Watanabe et al, 2007; Chiang et al, 2009),
which is non-trivial for SMT and needs to be deter-
mined experimentally. Given such forests, we glob-
ally learn a log-linear model using stochastic gradi-
ent descend (Section 5). Overall, both the generation
of forests and the training algorithm are scalable, en-
abling us to train millions of features on large-scale
data.
To show the effect of our framework, we globally
train millions of word level context features moti-
vated by word sense disambiguation (Chan et al,
2007) together with the features used in traditional
SMT system (Section 6). Training on 519K sentence
pairs in 0.03 seconds per sentence, we achieve sig-
nificantly improvement over the traditional pipeline
by 0.84 BLEU.
2 Synchronous Context Free Grammar
We work on synchronous context free grammar
(SCFG) (Chiang, 2007) based translation. The el-
ementary structures in an SCFG are rewrite rules of
the form:
X ? ??, ??
where ? and ? are strings of terminals and nonter-
minals. We call ? and ? as the source side and the
target side of rule respectively. Here a rule means a
phrase translation (Koehn et al, 2003) or a transla-
tion pair that contains nonterminals.
We call a sequence of translation steps as a
derivation. In context of SCFG, a derivation is a se-
quence of SCFG rules {ri}. Translation forest (Mi
et al, 2008; Li and Eisner, 2009) is a compact repre-
sentation of all the derivations for a given sentence
under an SCFG (see Figure 1). A tree t in the forest
corresponds to a derivation. In our paper, tree means
the same as derivation.
More formally, a forest is a pair ?V,E?, where V
is the set of nodes, E is the set of hyperedge. For
a given source sentence f = fn1 , Each node v ? V
is in the form Xi,j , which denotes the recognition
of nonterminal X spanning the substring from the i
through j (that is fi+1...fj). Each hyperedge e ? E
connects a set of antecedent to a single consequent
node and corresponds to an SCFG rule r(e).
3 Our Translation Forest
We use a translation forest that contains both ?ref-
erence? derivations that potentially yield the refer-
ence translation and also some neighboring ?non-
reference? derivations that fail to produce the ref-
erence translation. Therefore, our forest only repre-
sents some of the derivations for a sentence given an
SCFG rule table. The motivation of using such a for-
est is efficiency. However, since this space contains
both ?good? and ?bad? translations, it still provides
evidences for discriminative training.
First see the example in Figure 1. The derivation
tree t1 represented by solid hyperedges is a reference
derivation. We can construct a non-reference deriva-
tion by making small change to t1. By replacing the
e3 of t1 with e4, we obtain a non-reference deriva-
881
tion tree t2. Considering the rules in each derivation,
the difference between t1 and t2 lies in r3 and r4. Al-
though r3 has a same source side with r4, it produces
a different translation. While r3 provides a swap-
ping translation, r4 generates a monotone transla-
tion. Thus, the derivation t2 fails to move the sub-
ject ?police? to the behind of verb ?shot dead?, re-
sulting a wrong translation ?the gunman was police
shot dead?. Given such derivations, we hope that
the discriminative model is capable to explain why
should use a reordering rule in this context.
Generally, our forest contains all the reference
derivationsRT for a sentence given a rule table, and
some neighboring non-reference derivations NT ,
which can be defined fromRT .
More formally, we call two hyperedges e1 and e2
are competing hyperedges, if their corresponding
rules r(e1) = ??1, ?1? and r(e2) = ??2, ?2? :
?1 = ?2 ? ?1 ?= ?2 (1)
This means they give different translations for a
same source side. We use C(e) to represent the set
of competing hyperedges of e.
Two derivations t1 = ?V 1, E1? and t2 =
?V 2, E2? are competing derivations if there exists
e1 ? E1 and e2 ? E2: 2
V 1 = V 2 ? E1 ? e1 = E2 ? e2
? e2 ? C(e1) (2)
In other words, derivations t1 and t2 only differ in
e1 and e2, and these two hyperedges are competing
hyperedges. We useC(t) to represent the set of com-
peting derivations of tree t, and C(t,e) to represent
the set of competing derivations of t if the competi-
tion occurs in hyperedge e in t.
Given a rule table, the set of reference derivations
RT for a sentence is determined. Then, the set of
non-reference derivations NT can be defined from
RT :
?t?RT C(t) (3)
Overall, our forest is the compact representation of
RT and NT .
2The definition of derivation tree is similar to forest, except
that the tree contains exactly one tree while forest contains ex-
ponentially trees. In tree, the hyperedge degrades to edge.
Algorithm 1 Forest Generation
1: procedure GENERATE(t)
2: list? t
3: for v ? t in post order do
4: e? incoming edge of v
5: append C(t, e) to list;
6: for u ? child(v) from left to right do
7: tn? OPERATE(t, u)
8: if tn ?= t then
9: append tn to list
10: for e? ? tn ? e? /? t do
11: append C(tn,e?) to list
12: if SCORE(t) < SCORE(tn) then
13: t? tn
14: return t,list
4 Fast Generation
It is still slow to calculate the entire forest defined
in Section 3, therefore we use a greedy decoding for
fast generating a subset of the forest. Starting form
a reference derivation, we try to slightly change the
derivation into a new reference derivation. During
this process, we collect the competing derivations
of reference derivations. We describe the details of
local operators for changing a derivation in section
4.1, and then introduce the creation of initial refer-
ence derivation with max score in Section 4.2.
For example, given derivation t1, we delete the
node X0,1 and the related hyperedge e1 and e5. Fix-
ing the other nodes and edges, we try to add a new
edge e2 to create a new reference translation. In this
case, if rule r2 really exists in our rule table, we get
a new reference derivation t3. After constructing t3,
we first collect the new tree and C(t3, e2). Then, we
will move to t3, if the score of t3 is higher than t2.
Notably, if r2 does not exist in the rule table, we fail
to create a new reference derivation. In such case,
we keep the origin derivation unchanged.
Algorithm 1 shows the process of generation.3
The input is a reference derivation t, and the out-
put is a new derivation and the generated derivations.
3For simplicity, we list all the trees, and do not compress
them into a forest in practice. It is straight to extent the algo-
rithm to get a compact forest for those generated derivations.
Actually, instead of storing the derivations, we call the generate
function twice to calculate gradient of log-linear model.
882
0,4
0,1 2,4
0,4
2,4
0,4
2,40,2
Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also
need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a
new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c).
The list used for storing forest is initialized with the
input tree (line 2). We visit the nodes in t in post-
order (line 3). For each node v, we first append the
competing derivations C(t,e) to list, where e is in-
coming edge of v (lines 4-5). Then, we apply oper-
ators on the child nodes of v from left to right (lines
6-13). The operators returns a reference derivation
tn (line 7). If it is new (line 8), we collect both the tn
(line 9), and also the competing derivationsC(tn, e?)
of the new derivation on those edges e? which only
occur in the new derivation (lines 10-11). Finally, if
the new derivation has a larger score, we will replace
the origin derivation with new one (lines 12-13).
Although there is a two-level loop for visiting
nodes (line 3 and 6), each node is visited only one
time in the inner loops. Thus, the complexity is
linear with the number of nodes #node. Consid-
ering that the number of source word (also leaf node
here) is less than the total number of nodes and is
more than ?(#node+1)/2?, the time complexity of
the process is also linear with the number of source
word.
4.1 Lexicalize and Generalize
The function OPERATE in Algorithm 1 uses two op-
erators to change a node: lexicalize and generalize.
Figure 2 shows the effects of the two operators. The
lexicalize operator works on nonterminal nodes. It
moves away a nonterminal node and attaches the
children of current node to its parent. In Figure 2(b),
the node X0,1 is deleted, requiring a more lexical-
ized rule to be applied to the parent node X0,4 (one
more terminal in the source side). We constrain the
lexicalize operator to apply on pre-terminal nodes
whose children are all terminal nodes. In contrast,
the generalize operator works on terminal nodes and
inserts a nonterminal node between current node and
its parent node. This operator generalizes over the
continuous terminal sibling nodes left to the current
node (including the current node). Generalizing the
node bei in Figure 2(b) results Figure 2(c). A new
node X0,2 is inserted as the parent of node qiang-
shou and node bei.
Notably, there are two steps when apply an oper-
ator. Suppose we want to lexicalize the node X0,1
in t1 of Figure 1, we first delete the node X0,1 and
related edge e1 and e5, then we try to add the new
edge e2. Since rule table is fixed, the second step
is a process of decoding. Therefore, sometimes we
may fail to create a new reference derivation (like
r2 may not exist in the rule table). In such case, we
keep the origin derivation unchanged.
The changes made by the two operators are local.
Considering the change of rules, the lexicalize oper-
ator deletes two rules and adds one new rule, while
the generalize operator deletes one rule and adds two
new rules. Such local changes provide us with a way
to incrementally calculate the scores of new deriva-
tions. We use this method motivated by Gibbs Sam-
pler (Blunsom et al, 2009) which has been used for
efficiently learning rules. The different lies in that
we use the operator for decoding where the rule ta-
ble is fixing.
4.2 Initialize a Reference Derivation
The generation starts from an initial reference
derivation with max score. This requires bi-parsing
(Dyer, 2010) over the source sentence f and the ref-
erence translation e. In practice, we may face three
problems.
First is efficiency problem. Exhaustive search
over the space under SCFG requires O(|f |3|e|3).
883
To parse quickly, we only visit the tight consistent
(Zhang et al, 2008) bi-spans with the help of word
alignment a. Only visiting tight consistent spans
greatly speeds up bi-parsing. Besides efficiency,
adoption of this constraint receives support from the
fact that heuristic SCFG rule extraction only extracts
tight consistent initial phrases (Chiang, 2007).
Second is degenerate problem. If we only use
the features as traditional SCFG systems, the bi-
parsing may end with a derivation consists of some
giant rules or rules with rare source/target sides,
which is called degenerate solution (DeNero et al,
2006). That is because the translation rules with rare
source/target sides always receive a very high trans-
lation probability. We add a prior score log(#rule)
for each rule, where #rule is the number of occur-
rence of a rule, to reward frequent reusable rules and
derivations with more rules.
Finally, we may fail to create reference deriva-
tions due to the limitation in rule extraction. We
create minimum trees for (f , e,a) using shift-reduce
(Zhang et al, 2008). Some minimum rules in the
trees may be illegal according to the definition of
Chiang (2007). We also add these rules to the rule
table, so as to make sure every sentence is reachable
given the rule table. A source sentence is reachable
given a rule table if reference derivations exists. We
refer these rules as added rules. However, this may
introduce rules with more than two variables and in-
crease the complexity of bi-parsing. To tackle this
problem, we initialize the chart with minimum par-
allel tree from the Zhang et al (2008) algorithm,
ensuring that the bi-parsing has at least one path to
create a reference derivation. Then we only need to
consider the traditional rules during bi-parsing.
5 Training
We use the forest to train a log-linear model with a
latent variable as describe in Blunsom et al(2008).
The probability p(e|f) is the sum over all possible
derivations:
p(e|f) =
?
t??(e,f)
p(t, e|f) (4)
where ?(e, f) is the set of all possible derivations
that translate f into e and t is one such derivation.4
4Although the derivation is typically represent as d, we de-
notes it by t since our paper use tree to represent derivation.
Algorithm 2 Training
1: procedure TRAIN(S)
2: Training Data S = {fn, en,an}Nn=1
3: Derivations T = {}Nn=1
4: for n = 1 to N do
5: tn ? INITIAL(fn, en,an)
6: i? 0
7: for m = 0 to M do
8: for n = 0 to N do
9: ? ? LEARNRATE(i)
10: (?L(wi, tn), tn)?GENERATE(tn)
11: wi ? wi + ? ??L(wi, tn)
12: i? i + 1
13: return
?MN
i=1 wi
MN
This model defines the conditional probability of
a derivation t and the corresponding translation e
given a source sentence f as:
p(t, e|f) = exp
?
i ?ihi(t, e, f)
Z(f) (5)
where the partition function is
Z(f) =
?
e
?
t??(e,f)
exp
?
i
?ihi(t, e, f) (6)
The partition function is approximated by our for-
est, which is labeled as Z?(f), and the derivations
that produce reference translation is approximated
by reference derivations in Z?(f).
We estimate the parameters in log-linear model
using maximum a posteriori (MAP) estimator. It
maximizes the likelihood of the bilingual corpus
S = {fn, en}Nn=1, penalized using a gaussian prior
(L2 norm) with the probability density function
p0(?i) ? exp(??2i /2?2). We set ?2 to 1.0 in our
experiments. This results in the following gradient:
?L
??i
= Ep(t|e,f)[hi]? Ep(e|f)[hi]?
?i
?2 (7)
We use an online learning algorithm to train the
parameters. We implement stochastic gradient de-
scent (SGD) recommended by Bottou.5 The dy-
namic learning rate we use is N(i+i0) , where N is the
5http://leon.bottou.org/projects/sgd
884
number of training example, i is the training itera-
tion, and i0 is a constant number used to get a initial
learning rate, which is determined by calibration.
Algorithm 2 shows the entire process. We first
create an initial reference derivation for every train-
ing examples using bi-parsing (lines 4-5), and then
online learn the parameters using SGD (lines 6-12).
We use the GENERATE function to calculate the gra-
dient. In practice, instead of storing all the deriva-
tions in a list, we traverse the tree twice. The first
time is calculating the partition function, and the
second time calculates the gradient normalized by
partition function. During training, we also change
the derivations (line 10). When training is finished
after M epochs, the algorithm returns an averaged
weight vector (Collins, 2002) to avoid overfitting
(line 13). We use a development set to select total
epoch m, which is set as M = 5 in our experiments.
6 Experiments
Our method is able to train a large number of fea-
tures on large data. We use a set of word context
features motivated by word sense disambiguation
(Chan et al, 2007) to test scalability. A word level
context feature is a triple (f, e, f+1), which counts
the number of time that f is aligned to e and f+1 oc-
curs to the right of f . Triple (f, e, f?1) is similar ex-
cept that f?1 locates to the left of f . We retain word
alignment information in the extracted rules to ex-
ploit such features. To demonstrate the importance
of scaling up the size of training data and the effect
of our method, we compare three types of training
configurations which differ in the size of features
and data.
MERT. We use MERT (Och, 2003) to training 8
features on a small data. The 8 features is the same
as Chiang (2007) including 4 rule scores (direct and
reverse translation scores; direct and reverse lexi-
cal translation scores); 1 target side language model
score; 3 penalties for word counts, extracted rules
and glue rule. Actually, traditional pipeline often
uses such configuration.
Perceptron. We also learn thousands of context
word features together with the 8 traditional features
on a small data using perceptron. Following (Chiang
et al, 2009), we only use 100 most frequent words
for word context feature. This setting use CKY de-
TRAIN RTRAIN DEV TEST
#Sent. 519,359 186,810 878 3,789
#Word 8.6M 1.3M 23K 105K
Avg. Len. 16.5 7.3 26.4 28.0
Lon. Len. 99 95 77 116
Table 1: Corpus statistics of Chinese side, where Sent.,
Avg., Lon., and Len. are short for sentence, longest,
average, and length respectively. RTRAIN denotes the
reachable (given rule table without added rules) subset of
TRAIN data.
coder to generate n-best lists for training. The com-
plexity of CKY decoding limits the training data into
a small size. We fix the 8 traditional feature weights
as MERT to get a comparable results as MERT.
Our Method. Finally, we use our method to train
millions of features on large data. The use of large
data promises us to use full vocabulary of training
data for the context word features, which results mil-
lions of fully lexicalized context features. During
decoding, when a context feature does not exit, we
simply ignore it. The weights of 8 traditional fea-
tures are fixed the same as MERT also. We fix these
weights because the translation feature weights fluc-
tuate intensely during online learning. The main rea-
son may come from the degeneration solution men-
tioned in Section 4.2, where rare rules with very high
translation probability are selected as the reference
derivations. Another reason could be the fact that
translation features are dense intensify the fluctua-
tion. We leave learning without fixing the 8 feature
weights to future work.
6.1 Data
We focus on the Chinese-to-English translation task
in this paper. The bilingual corpus we use con-
tains 519, 359 sentence pairs, with an average length
of 16.5 in source side and 20.3 in target side,
where 186, 810 sentence pairs (36%) are reach-
able (without added rules in Section 4.2). The
monolingual data includes the Xinhua portion of
the GIGAWORD corpus, which contains 238M En-
glish words. We use the NIST evaluation sets of
2002 (MT02) as our development set, and sets of
MT03/MT04/MT05 as test sets. Table 2 shows the
statistics of all bilingual corpus.
We use GIZA++ (Och and Ney, 2003) to perform
885
System #DATA #FEAT MT03 MT04 MT05 ALL
MERT 878 8 33.03 35.12 32.32 33.85
Perceptron 878 2.4K 32.89 34.88 32.55 33.76
Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*519K 13.9M 34.19* 35.72* 33.09* 34.69*
Improvement over MERT +1.16 +0.60 +0.77 +0.84
Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast
generation method with different data (only reachable or full data). #Data is the size of data for training the feature
weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01).
word alignment in both directions, and grow-diag-
final-and (Koehn et al, 2003) to generate symmet-
ric word alignment. We extract SCFG rules as de-
scribed in Chiang (2007) and also added rules (Sec-
tion 4.2). Our algorithm runs on the entire training
data, which requires to load all the rules into the
memory. To fit within memory, we cut off those
composed rules which only happen once in the train-
ing data. Here a composed rule is a rule that can be
produced by any other extracted rules. A 4-grams
language model is trained by the SRILM toolkit
(Stolcke, 2002). Case-insensitive NIST BLEU4 (Pa-
pineni et al, 2002) is used to measure translation
performance.
The training data comes from a subset of the
LDC data including LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06. Since the rule ta-
ble of the entire data is too large to be loaded to
the memory (even drop one-count rules), we remove
many sentence pairs to create a much smaller data
yet having a comparable performance with the entire
data. The intuition lies in that if most of the source
words of a sentence need to be translated by the
added rules, then the word alignment may be highly
crossed and the sentence may be useless. We cre-
ate minimum rules from a sentence pair, and count
the number of source words in those minimum rules
that are added rules. For example, suppose the result
minimum rules of a sentence contain r3 which is an
added rule, then we count 1 time for the sentence. If
the number of such source word is more than 10%
of the total number, we will drop the sentence pair.
We compare the performances of MERT setting
on three bilingual data: the entire data that contains
42.3M Chinese and 48.2M English words; 519K
data that contains 8.6M Chinese and 10.6M En-
glish words; FBIS (LDC2003E14) parts that con-
tains 6.9M Chinese and 9.1M English words. They
produce 33.11/32.32/30.47 BLEU tested on MT05
respectively. The performance of 519K data is com-
parable with that of entire data, and much higher
than that of FBIS data.
6.2 Result
Table 3 shows the performance of the three different
training configurations. The training of MERT and
perceptron run on MT02. For our method, we com-
pare two different training sets: one is trained on
all 519K sentence pairs, the other only uses 186K
reachable sentences.
Although the perceptron system exploits 2.4K
features, it fails to produce stable improvements
over MERT. The reason may come from overfitting,
since the training data for perceptron contains only
878 sentences. However, when use our method to
learn the word context feature on the 519K data,
we significantly improve the performance by 0.84
points on the entire test sets (ALL). The improve-
ments range from 0.60 to 1.16 points on MT03-
05. Because we use the full vocabulary, the num-
ber of features increased into 13.9 millions, which is
impractical to be trained on the small development
set. These results confirm the necessity of exploiting
more features and learning the parameters on large
data. Meanwhile, such results also demonstrate that
we can benefits from the forest generated by our fast
method instead of traditional CKY algorithm.
Not surprisingly, the improvements are smaller
when only use 186K reachable sentences. Some-
times we even fail to gain significant improvement.
This verifies our motivation to guarantee all sentence
886
 0
 30
 60
 90
 120
 150
 180
 0  10  20  30  40  50  60  70  80  90
Tr
ain
ing
 Ti
me
(M
illis
eco
nds
)
Sentence Length
Figure 3: Plot of training times (including forest genera-
tion and SGD training) versus sentence length. We ran-
domly select 1000 sentence from the 519K data for plot-
ting.
are reachable, so as to use all training data.
6.3 Speed
How about the speed of our framework? Our method
learns in 32 mlliseconds/sentence. Figure 3 shows
training times (including forest generation and SGD
training) versus sentence length. The plot confirms
that our training algorithm scales linearly. If we
use n-best lists which generated by CKY decoder
as MERT, it takes about 3105 milliseconds/sentence
for producing 100-best lists. Our method accelerates
the speed about 97 times (even though we search
twice to calculate the gradient). This shows the effi-
ciency of our method.
The procedure of training includes two steps. (1)
Bi-parsing to initialize a reference derivation with
max score. (2) Training procedure which generates
a set of derivations to calculate the gradient and up-
date parameters. Step (1) only runs once. The av-
erage time of processing a sentence for each step
is about 9.5 milliseconds and 30.2 milliseconds re-
spectively.
For simplicity we do not compress the generated
derivations into forests, therefore the size of result-
ing derivations is fairly small, which is about 265.8
for each sentence on average, where 6.1 of them are
reference derivations. Furthermore, we use lexical-
ize operator more often than generalize operator (the
ration between them is 1.5 to 1). Lexicalize operator
is used more frequently mainly dues to that the ref-
erence derivations are initialized with reusable (thus
small) rules.
7 Related Work
Minimum error rate training (Och, 2003) is perhaps
the most popular discriminative training for SMT.
However, it fails to scale to large number of features.
Researchers have propose many learning algorithms
to train many features: perceptron (Shen et al, 2004;
Liang et al, 2006), minimum risk (Smith and Eisner,
2006; Li et al, 2009), MIRA (Watanabe et al, 2007;
Chiang et al, 2009), gradient descent (Blunsom et
al., 2008; Blunsom and Osborne, 2008). The com-
plexity of n-best lists or packed forests generation
hamper these algorithms to scale to a large amount
of data.
For efficiency, we only use neighboring deriva-
tions for training. Such motivation is same as con-
trastive estimation (Smith and Eisner, 2005; Poon et
al., 2009). The difference lies in that the previous
work actually care about their latent variables (pos
tags, segmentation, dependency trees, etc), while
we are only interested in their marginal distribution.
Furthermore, we focus on how to fast generate trans-
lation forest for training.
The local operators lexicalize/generalize are use
for greedy decoding. The idea is related to ?peg-
ging? algorithm (Brown et al, 1993) and greedy de-
coding (Germann et al, 2001). Such types of local
operators are also used in Gibbs sampler for syn-
chronous grammar induction (Blunsom et al, 2009;
Cohn and Blunsom, 2009).
8 Conclusion and Future Work
We have presented a fast generation algorithm for
translation forest which contains both reference
derivations and neighboring non-reference deriva-
tions for large-scale SMT discriminative training.
We have achieved significantly improvement of 0.84
BLEU by incorporate 13.9M feature trained on 519K
data in 0.03 second per sentence.
In this paper, we define the forest based on com-
peting derivations which only differ in one rule.
There may be better classes of forest that can pro-
duce a better performance. It?s interesting to modify
the definition of forest, and use more local operators
to increase the size of forest. Furthermore, since the
generation of forests is quite general, it?s straight to
887
apply our forest on other learning algorithms. Fi-
nally, we hope to exploit more features such as re-
ordering features and syntactic features so as to fur-
ther improve the performance.
Acknowledgement
We would like to thank Yifan He, Xianhua Li, Daqi
Zheng, and the anonymous reviewers for their in-
sightful comments. The authors were supported by
National Natural Science Foundation of China Con-
tracts 60736014, 60873167, and 60903138.
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proc. of EMNLP
2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. of ACL 2009.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathemat-
ics of statistical machine translation. Computational
Linguistics, 19:263?311.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33?40.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. of EMNLP 2009.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP 2002.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on SMT.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In Proc. of NAACL
2010.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc. of
ACL 2001.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL 2009.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of ACL 2006.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL 2002.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proc. of NAACL 2004.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL 2005.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING/ACL 2006.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. of ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proc. of ACL 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proc. of Coling 2008.
888
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255?264,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Max-Margin Synchronous Grammar Induction for Machine Translation
Xinyan Xiao and Deyi Xiong?
School of Computer Science and Technology
Soochow University
Suzhou 215006, China
xyxiao.cn@gmail.com, dyxiong@suda.edu.cn
Abstract
Traditional synchronous grammar induction
estimates parameters by maximizing likeli-
hood, which only has a loose relation to trans-
lation quality. Alternatively, we propose a
max-margin estimation approach to discrim-
inatively inducing synchronous grammars for
machine translation, which directly optimizes
translation quality measured by BLEU. In
the max-margin estimation of parameters, we
only need to calculate Viterbi translations.
This further facilitates the incorporation of
various non-local features that are defined on
the target side. We test the effectiveness of our
max-margin estimation framework on a com-
petitive hierarchical phrase-based system. Ex-
periments show that our max-margin method
significantly outperforms the traditional two-
step pipeline for synchronous rule extraction
by 1.3 BLEU points and is also better than pre-
vious max-likelihood estimation method.
1 Introduction
Synchronous grammar induction, which refers to
the process of learning translation rules from bilin-
gual corpus, still remains an open problem in sta-
tistical machine translation (SMT). Although state-
of-the-art SMT systems model the translation pro-
cess based on synchronous grammars (including
bilingual phrases), most of them still learn trans-
lation rules via a pipeline with word-based heuris-
tics (Koehn et al, 2003). This pipeline first builds
word alignments using heuristic combination strate-
gies, then heuristically extracts rules that are consis-
tent with word alignments. Such heuristic pipeline
?Corresponding author
is not elegant theoretically. It brings an undesirable
gap that separates modeling and learning in an SMT
system.
Therefore, researchers have proposed alternative
approaches to learning synchronous grammars di-
rectly from sentence pairs without word alignments,
via generative models (Marcu and Wong, 2002;
Cherry and Lin, 2007; Zhang et al, 2008; DeNero
et al, 2008; Blunsom et al, 2009; Cohn and Blun-
som, 2009; Neubig et al, 2011; Levenberg et al,
2012) or discriminative models (Xiao et al, 2012).
Theoretically, these approaches describe how sen-
tence pairs are generated by applying sequences of
synchronous rules in an elegant way. However, they
learn synchronous grammars by maximizing likeli-
hood,1 which only has a loose relation to transla-
tion quality (He and Deng, 2012). Moreover, gen-
erative models are normally hard to be extended to
incorporate useful features, and the discriminative
synchronous grammar induction model proposed by
Xiao et al (2012) only incorporates local features
defined on parse trees of the source language. Non-
local features, which encode information from parse
trees of the target language, have never been ex-
ploited before due to the computational complexity
of normalization in max-likelihood estimation.
Consequently, we would like to learn syn-
chronous grammars in a discriminative way that can
directly maximize the end-to-end translation quality
measured by BLEU (Papineni et al, 2002), and is
also able to incorporate non-local features from tar-
get parse trees.
We thus propose a max-margin estimation method
1More precisely, the discriminative model by Xiao et al
(2012) maximizes conditional likelihood.
255
to discriminatively induce synchronous grammar di-
rectly from sentence pairs without word alignments.
We try to maximize the margin between a reference
translation and a candidate translation with transla-
tion errors that are measured by BLEU. The more
serious the translation errors, the larger the margin.
In this way, our max-margin method is able to learn
synchronous grammars according to their translation
performance. We further incorporate various non-
local features defined on target parse trees. We ef-
ficiently calculate the non-local feature values of a
translation over its exponential derivation space us-
ing the inside-outside algorithm. Because our max-
margin estimation optimizes feature weights only by
the feature values of Viterbi and reference transla-
tions, we are able to efficiently perform optimization
even with non-local features.
We apply the proposed max-margin estimation
method to learn synchronous grammars for a hi-
erarchical phrase-based translation system (Chiang,
2007) which typically produces state-of-the-art per-
formance. With non-local features defined on tar-
get parse trees, our max-margin method significantly
outperforms the baseline that uses synchronous
rules learned from the traditional pipeline by 1.3
BLEU points on large-scale Chinese-English bilin-
gual training data.
The remainder of this paper is organized as fol-
lows. Section 2 presents the discriminative syn-
chronous grammar induction model with the non-
local features. In Section 3, we elaborate our max-
margin estimation method which is able to directly
optimize BLEU, and discuss how we induce gram-
mar rules. Local and non-local features are de-
scribed in Section 4. Finally, in Section 5, we verify
the effectiveness of our method through experiments
by comparing it against both the traditional pipeline
and max-likelihood estimation method.
2 Discriminative Model with Non-local
Features
Let S denotes the set of all strings in a source lan-
guage. Given a source sentence s ? S , T (s) denotes
all candidate translations in the target language that
can be generated by a synchronous grammar G. A
translation t ? T (s) is generated by a sequence of
translation steps (r1, ..., rn), where we apply a syn-
?? ? ?? ?? ??10 2 3 4 5
 [1,3]
 [1,5]
 [0,5]
[1,6]
[0,6]
[4,6]
10 2 3 4 5 6
bushi yu shalong juxing huitan
r1: ? yu shalong? with Sharon ?
r2: ? X juxing huitan? held a talk X ?
r3: ? bushi X ? Bush X ?
Figure 1: A derivation of a sentence pair represented by
a synchronous tree. The above and below part are the
parses in the source language side and the target language
side respectively. Left subscript of a node X denotes the
source span, while right subscript denotes the target span.
A dashed line denotes an alignment from a source span
to a target span. The annotation for a dashed line cor-
responds to the rewriting rule used in the corresponding
step of the derivation.
chronous rule r ? G in one step. We refer to such
a sequence of translation steps as a derivation (See
Figure 1) and denote it as d ? D(s), where D(s)
represents the derivation space of a source sentence.
Given an input source sentence s, we output a pair
?t,d? in SMT. Thus, we study the triple ?s, t,d? in
SMT.
In our discriminative model, we calculate the
value of a triple ?s, t,d? according to the following
scoring function:
f(s, t,d) = ?T?(s, t,d) (1)
where ? ? ? is a feature weight vector, and ? is the
feature function.
There are exponential outputs in SMT. Therefore
it is necessary to factorize the feature function in or-
der to perform efficient calculation over the SMT
output space using dynamic programming. We de-
compose the feature function of a triple ?s, t,d? into
256
? ??
 [1,5]
?? ??
[1,6]
Figure 2: Example features for the derivation in Figure 1.
Shaded nodes denote information encoded in the feature.
a sum of values of each synchronous rule in the
derivation d.
?(s, t,d) =
?
r?d
?(r, s)
? ?? ?
local
+
?
r?d
?(r, s, t)
? ?? ?
non-local
(2)
Our feature functions include both local and non-
local features. A feature is a local feature if and
only if it can be factored among the translation steps
in a derivation. In other words, the value of a lo-
cal feature for ?s, t,d? can be calculated as a sum of
local scores in each translation step, and the calcula-
tion of each local score only requires to look at the
rule used in corresponding step and the input sen-
tence. Otherwise, the feature is a non-local feature.
Our discriminative model allows to incorporate non-
local features that are defined on target translations.
For example, a rule feature in Figure 2(a), which
indicates the application of a specific rule in a
derivation, is a local feature. A source span bound-
ary feature in Figure 2(b) that is defined on the
source parse tree is also a local feature. However,
a target span boundary feature in Figure 2(c), which
assesses the target parse structure, is a non-local fea-
ture. According to Figure 1, the span is parsed in
step r2, but it also depends on the translation bound-
ary word ?held? generated in previous step r1. We
will describe the details of both local and non-local
features that we use in Section 4.
Non-local features enable us to model the target
parse structure in a derivation. However, it is com-
putationally expensive to calculate the expected val-
ues of non-local features over D(s), as non-local
features require to record states of target boundary
s, S, S s is a sentence in a source language;
S means source training sentences;
S denotes all the possible sentences;
t, T, T symbols for the target language that
similar to s, S, S;
d, D derivation and derivation space;
D(s) space of derivations for
a source sentence;
D(s, t) space of derivations for
a source sentence with its translation;
H(s) hypergraph that represents D(s);
H(s, t) hypergraph that represents D(s, t);
Table 1: Notations in this paper. We give an abstract of
related notations for clarity.
words and result in an extremely large number of
states during dynamic programming. Fortunately,
when integrating out derivations over the derivation
space D(s, t) of a source sentence and its transla-
tion, we can efficiently calculate the non-local fea-
tures. Because all derivations in D(s, t) share the
same translation, there is no need to maintain states
for target boundary words. We will discuss this com-
putational problem in details in Section 3.3. In the
proposed max-margin estimation described in next
section, we only need to integrate out derivation
for a Viterbi translation and a reference translation
when updating feature weights. Therefore, the de-
fined non-local features allow us to not only explore
useful knowledge on the target parse trees, but also
compute them efficiently over D(s, t) during max-
margin estimation.
3 Max-Margin Estimation
In this section, we describe how we use a parallel
training corpus {S,T} = {(s(i), t(i))}Ni=1 to esti-
mate feature weights ?, which contain parameters of
the induced synchronous grammars and the defined
non-local features.
We choose the parameters that maximize the
translation quality measured by BLEU using the
max-margin estimation (Taskar et al, 2004). Mar-
gin refers to the difference of the model score be-
tween a reference translation t(i) and a candidate
translation t. We hope that the worse the transla-
tion quality of t, the larger the margin between t
and t(i). In this way, we penalize larger translation
257
errors more severely than smaller ones. This intu-
ition is expressed by the following equation.
min 1
2
???2 (3)
s.t. f(s(i), t(i))? f(s(i), t) ? cost(t(i), t)
?t ? T (s(i))
Here, f(s, t) is the feature function of a translation,
and cost function cost(t(i), t) measures the trans-
lation errors of a candidate translation t comparing
with a reference translation t(i). We define the cost
function via the widely-used translation evaluation
metric BLEU. We use the smoothed sentence level
BLEU-4 (Lin and Och, 2004) here:
cost(t(i), t) = 1? BLEU-4(t(i), t) (4)
In Section 3.1, we will discuss how we use the
scoring function f(s, t,d) to calculate f(s, t). Then
in Section 3.2, we recast the equation (3) as an un-
constrained empirical loss minimization problem,
and describe the learning algorithm for optimizing
? and inducing G. Finally, we give the details of
inference for the learning algorithm in Section 3.3.
3.1 Integrate Out Derivation by Averaging
Although we only model the triple ?s, t,d? in the
equation (1), it?s necessary to calculate the scoring
function f(s, t) of a translation by integrating out
the variable of derivation as derivation is not ob-
served in the training data.
We use an averaging computation over all possi-
ble derivations of a translation D(s, t). We call this
an average derivation based estimation:
f(s, t) = 1
|D(s, t)|
?
d?D(s,t)
f(s, t,d) (5)
The ?average derivation? can be considered as the
geometric central point in the space D(s, t).
Another possible way to deal with the latent
derivation is max-derivation, which uses the max-
operator over D(s, t). The max derivation method
sets f(s, t) as maxd?D(s,t) f(s, t,d). It is often
adopted in traditional SMT systems. Nevertheless,
we instead use average-derivation for two reasons.2
2Imagine that H(s, t) in the Algorithm 1 is replaced by a
maximum derivation inH(s, t).
First, as a translation has an exponential number of
derivations, finding the max derivation of a refer-
ence translation for learning is nontrivial (Chiang et
al., 2009). Second, the max derivation estimation
will result in a low rule coverage, as rules in a max
derivation only covers a small fraction of rules in
the D(s, t). Because rule coverage is important in
synchronous grammar induction, we would like to
explore the entire derivation space using the average
operator.
3.2 Learning Algorithm
We reformulate the equation (3) as an unconstrained
empirical loss minimization problem as follows:
min ?
2
???2 + 1
N
N
?
n=1
L(s(i), t(i), ?) (6)
Where ? denotes the regularization strength for
L2-norm. The loss function of a sentence pair
L(s(i), t(i), ?) is a convex hinge loss function de-
noted by:
max{0,?f(s(i), t(i)) (7)
+ max
t?T (s(i))
(
f(s(i), t) + cost(t(i), t)
)
}
According to the second max-operator in the
hinge loss function, the optimization towards BLEU
is expressed by cost-augmented inference. Cost-
augmented inference finds a translation that has a
maximum model score augmented with cost.
t? = max
t?T (s(i))
(
f(s(i), t) + cost(t(i), t)
)
(8)
We applied the Pegasos algorithm for the op-
timization of equation (6) (Shalev-Shwartz et al,
2007). This is an online algorithm, which alternates
between stochastic gradient descent steps and pro-
jection steps. When the loss function is non-zero, it
updates weights according to the sub-gradient of the
hinge loss function. Using the average scoring func-
tion in the equation (5), the sub-gradient of hinge
loss function for a sentence pair is the difference of
average feature values between a Viterbi translation
258
Algorithm 1 UPDATE(s, t, ?,G) ? One step in online algorithm. s, t are short for s(i), t(i) here
1: H(s, t)? BIPARSE(s, t, ?) ? Build hypergraph of reference translation
2: G?G +H(s, t) ? Discover rules fromH(s, t)
3: t?, d?? argmax?t?,d???D(s) f(s, t?,d?) + cost(t, t?) ? Find Viterbi translation
4: H(s, t?)? BIPARSE(s, t?, ?) ? Build hypergraph of Viterbi translation
5: if f(s, t) < f(s, t?) + cost(t, t?) then
6: ? ? (1? ??)? + ? ? ?L?? (H(s, t),H(s, t?)) ? Update ? by gradient
?L
?? and learning rate ?
7: ? ? min {1, 1/
?
?
??? } ? ? ? Projection by scaling
8: return G, ?
and a reference translation.
?L
??
= 1
|D(s(i), t(i))|
?
d?D(s(i),t(i))
?(s(i), t(i),d)
? 1
|D(s(i), t?)|
?
d?D(s(i),t?)
?(s(i), t?,d) (9)
Algorithm 1 shows the procedure of one step in
the online optimization algorithm. The procedure
discovers rules and updates weights in an online
fashion. In the procedure, we first biparse the sen-
tence pair to construct a synchronous hypergraph of
a reference translation (line 1). In the biparsing al-
gorithm, synchronous rules for constructing hyper-
edges are not required to be in G, but can be any
rules that follow the form defined in Chiang (2007).
Thus, the biparsing algorithm can discover new rules
that are not in G. Then we collect the translation
rules discovered in the hypergraph of the reference
translation (line 2), which are rules indicated by hy-
peredges in the hypergraph. We then calculate the
Viterbi translation according to the scoring function
and cost function (see Section 3.3) (line 3), and build
the synchronous hypergraph for the Viterbi transla-
tion (line 4). Finally, we update weights according to
the Pegasos algorithm (line 5). The sub-gradient is
calculated based on the hypergraph of Viterbi trans-
lation and reference translation.
In practice, in order to process the data in a paral-
lel manner, we use a larger step size of 1000 for the
learning algorithm. In each step of our online opti-
mization algorithm, we first biparse 1000 reference
sentence pairs in parallel. Then, we collect grammar
rules from the generated reference hypergraphs. Af-
ter that, we compute the gradients of 1000 sentence
pairs in parallel, by calculating feature weights over
reference hypergraphs and Viterbi hypergraphs. Fi-
nally, we update the feature weights using the sum
of these gradients.
3.3 Inference
There are two parts that need to be calculated in
the learning algorithm: finding a cost-augmented
Viterbi translation according to the scoring func-
tion and cost function (Equation 8), and constructing
synchronous hypergraphs for the Viterbi and refer-
ence translation so as to discover rules and calculate
average feature values in Equation (9). Following
the traditional decoding procedure, we resort to the
cube-pruning based algorithm for approximation.
To find the Viterbi translation, we run the tra-
ditional translation decoding algorithm (Chiang,
2007) to get the best derivation. Then we use
the translation yielded by the best derivation as the
Viterbi translation. In order to obtain the BLEU
score in the cost function, we need to calculate the
ngram precision. It is calculated in a way similar to
the calculation of the ngram language model. The
computation of BLEU-4 requires to record 3 bound-
ary words in both the left and right side during dy-
namic programming. Therefore, even when we use
a language model whose order is less than 4, we still
expands the states to record 3 boundary words so as
to calculate the cost measured by BLEU.
We build synchronous hypergraphs using the
cube-pruning based biparsing algorithm (Xiao et al,
2012). Algorithm 2 shows the procedure. Using
a chart, the biparsing algorithm constructs k-best
alignments for every source word (lines 1-5) and k-
best hyperedges for every source span (lines 6-13)
from the bottom up. Thus, a synchronous hyper-
graph is generated during the construction of the
chart. More specifically, for a source span, it first
creates cubes L for all source parses ? that are in-
259
Algorithm 2 BIPARSE(s, t, ?) ? (Xiao et al, 2012)
 Create k-best alignments for each source word
1: for i? 1, .., |s| do
2: for j ? 1, .., |t| do
3: Lj ? {?, tj} ? si aligns to tj or not
4: L? ?L1, ..., L|t|?
5: chart[s, i]? KBEST(L,?,?)
 Create k-best hyperedges for each source span
6: H? ?
7: for h? 1, .., |s| do ? h is the size of span
8: for all i, j s.t. j ? i = h do
9: L? ?
10: for ? inferable from chart do
11: L? L + ?chart[?1], ..., chart[?|?|]?
12: chart[X, i, j]? KBEST(L,?,?)
13: H?H + chart[X, i, j] ? save hyperedges
14: returnH
ferable from the chart (lines 9-11). Here ?i is a par-
tial source parse that covers either a single source
word or a span of source words. Then it uses the
cube pruning algorithm to keep the top k derivations
among all partial derivations that share the same
source span [i, j] (line 12). Notably, this biparsing
algorithm does not require specific translation rules
as input. Instead, it is able to discover new syn-
chronous grammar rules when constructing a syn-
chronous hypergraph: extracting each hyperedge in
the hypergraph as a synchronous rule.
Based on the biparsing algorithm, we are able to
construct the reference hypergraph H(s(i), t(i)) and
Viterbi hypergraph H(s(i), t?). By the reference hy-
pergraph, we collect new synchronous translation
rules and record them in the grammar G. We also
calculate the average feature values of hypergraphs
using the inside-outside algorithm (Li et al, 2009),
so as to compute the gradients.
4 Features
One advantage of the discriminative method is that
it enables us to incorporate arbitrary features. As
shown in Section 2, our model incorporates both lo-
cal and non-local features.
4.1 Local Features
Rule features We associate each rule with an indi-
cator feature. Each indicator feature counts the num-
ber of times that a rule appears in a derivation. In
this way, we are able to learn a weight for every rule
according to the entire structure of sentence.
Word association features Lexicalized features
are widely used in traditional SMT systems. Here
we adopt two lexical weights called noisy-or fea-
tures (Zens and Ney, 2004). The noisy-or feature
is estimated by word translation probabilities output
by GIZA++. We set the initial weight of these two
lexical scores with equivalent positive values. The
lexical weights enable our system to score and rank
the hyperedges at the beginning. Although word
alignment features are used, we do not constrain the
derivation space of a sentence pair by prefixed word
alignment, and do not require any heuristic align-
ment combination strategy.
Length feature We integrate the length of target
translation that is used in traditional SMT system as
our feature.
Source span boundary features We use this kind
of feature to assess the source parse tree in a deriva-
tion. Previous work (Xiong et al, 2010) has shown
the importance of phrase boundary features for
translation. Actually, this kind of feature is a good
cue for deciding the boundary where a rule is to be
learnt. Following Taskar et al (2004), for a bispan
[i, j, k, l] in a derivation, we define the feature tem-
plates that indicates the boundaries of a span by its
beginning and end words: {B : si+1;E : sj ;BE :
si+1, sj}.
Source span orientation features Orientation
features are only used for those spans that are swap-
ping. In Figure 1, the translation of source span [1, 3]
is swapping with that of span [4, 5] by r2, thus ori-
entation feature for span [1, 3] is activated. We also
define three feature templates for a swapping span
similar to the boundary features: {B : si+1;E :
sj ;BE : si+1, sj}. In practice, we add a prefix to
the orientation features so as to distinguish these fea-
tures from the boundary features.
4.2 Non-local Features
Target span boundary features We also want to
assess the target tree structure in a derivation. We
define these features in a way similar to source span
boundary features. For a bispan [i, j, k, l] in a deriva-
tion, we define the feature templates that indicates
260
System Grammar Size MT03 MT04 MT05 Avg.
Moses 302.5M 34.26 36.56 32.69 34.50
Baseline 77.8M 33.83 35.81 33.23 34.29
Max-margin 59.4M 34.62 37.14 34.00 35.25+Sparse feature 35.48 37.31 34.07 35.62
Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses
denotes the implementation of hierarchical phrased-model in Moses (Koehn et al, 2007). +Sparsefeature means
that those sparse features used in the grammar induction are also used during decoding. The improvement of max-
margin over Baseline is statistically significant (p < 0.01).
target span boundary as: {B : tk+1;E : tl;BE :
tk+1, tl}.
Target span orientation features Similar target
orientation features are used for a swapping span
[i, j, k, l] with feature templates {B : tk+1;E :
tl;BE : tk+1, tl}.
Relative position features Following Blunsom
and Cohn (Blunsom and Cohn, 2006), we integrate
features indicating the closeness to the alignment
matrix diagonal. For an aligned word pair with
source position i and target position j, the value of
this feature is | i|s| ?
j
|t| |. As this feature depends
on the length of the target sentence, it is a non-local
feature.
Language model We also incorporate an ngram
language model which is an important component
in SMT. For efficiency, we use a 3-gram language
model trained on the target side of our training data
during the induction of synchronous grammars.
5 Experiment
In this section, we present our experiments on the
NIST Chinese-to-English translation tasks. We first
compare our max-margin based method with the tra-
ditional pipeline on a large bitext which contains
1.1 million sentences. We then present a detailed
comparison on a smaller dataset, in order to analyze
the effectiveness of max-margin estimation compar-
ing with the max likelihood estimation (Xiao et al,
2012), and also the effectiveness of the non-local
features that are defined on the target side.
5.1 Setup
The baseline system is the hierarchical phrase based
system (Chiang, 2007). We used a bilingual corpus
that contains 1.1M sentences (44.6 million words)
of up to length 40 from the LDC data.3 Our 5-gram
language model was trained by SRILM toolkit (Stol-
cke, 2002). The monolingual training data includes
the Xinhua section of the English Gigaword corpus
and the English side of the entire LDC data (432 mil-
lion words).
We used the NIST 2002 (MT02) as our develop-
ment set, and the NIST 2003-2005 (MT03-05) as the
test set. Case-insensitive NIST BLEU-4 (Papineni
et al, 2002) is used to measure translation perfor-
mance, and also the cost function in the max-margin
estimation. Statistical significance in BLEU differ-
ences was tested by paired bootstrap re-sampling
(Koehn, 2004). We used minimum error rate train-
ing (MERT) (Och, 2003) to optimize feature weights
for the traditional log-linear model.
We used the same decoder as the baseline system
in all estimation methods. Without special explana-
tion, we used the same features as those in the tra-
ditional pipeline: forward and backward translation
probabilities, forward and backward lexical weights,
count of extracted rules, count of glue rules, length
of translation, and language model. For the lexical
weights we used the noisy-or in all configurations
including the baseline system. For the discrimina-
tive grammar induction, rule translation probabili-
ties were calculated using the expectations of rules
in the synchronous hypergraphs of sentence pairs.
As our max-margin synchronous grammar induc-
tion is trained on the entire bitext, it is necessary to
load all the rules into the memory during training.
To control the size of rule table, we used Viterbi-
3Including LDC2002E18, LDC2003E07, LDC2003E14,
LDC2004T07, LDC2005T06 and Hansards portion of
LDC2004T08.
261
System Feature Function MT03 MT04 MT05 Avg.
Baseline ? 31.76 33.08 31.06 31.96
Max-likelihood local 32.84 34.54 31.61 33.00
Max-margin local 32.97 34.92 31.99 33.29local,non-local 33.27 34.83 32.32 33.47
Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus. For max-margin method, we
present two results according to the usages of non-local features. The max-margin with non-local features significantly
outperforms the Baseline (p < 0.01) and also the max-likelihood estimation (p < 0.05).
pruning (Huang, 2008) when collecting rules as
shown in line 2 of optimization procedure in Section
3.2. Furthermore, we aggressively discarded those
large rules (The number of source symbols or the
number of target symbols are more than two) that
occur only in one sentence. Whenever the learning
algorithm processes 50K sentences, we performed
this discarding operation for large rules.
5.2 Result on Large Dataset
Table 2 shows the translation results. Our method
induces 59.4 million synchronous rules, which are
76.3% of the grammar size of baseline. Note that
Moses allows the boundary words of a phrase to be
unaligned, while our baseline constraints the initial
phrase to be tightly consistent with word alignment.
Therefore, Moses extract a much larger rule table
than that of our baseline.
With fewer translation rules, our method obtains
an average improvement of +0.96 BLEU points on
the three test sets over the Baseline. As the differ-
ence between the baseline and our max-margin syn-
chronous grammar induction model only lies in the
grammar, this result clearly denotes that our learnt
grammar does outperform the grammar extracted by
the traditional two-step pipeline.
We also incorporate the sparse features during de-
coding in a way similar to Xiao et al (2012) and
Dyer et al (2011). In order to optimize these sparse
features with the dense features by MERT, we group
features of the same type into one coarse ?summary
feature?, and get three such features including: rule,
phrase-boundary and phrase orientation features. In
this way, we rescale the weights of the three ?sum-
mary features? with the 8 dense features by MERT.
We achieve a further improvement of +0.37 BLEU
points. Therefore, our training algorithm is able to
learn the useful information encoded by the sparse
features for translation.
5.3 Comparison of Estimation Objective and
Non-Local Feature
Wewant to investigate whether the max-margin esti-
mation is able to outperform the max-likelihood es-
timation method (Xiao et al, 2012). Therefore we
carried out experiments to compare them directly.
As the max-margin method is able to use non-local
features, we compare two settings of features for the
max-margin method. One uses only local features,
the other uses both local and non-local features. Be-
cause the training procedure need to run on the entire
corpus, which is time consuming, we therefore use
a smaller corpus containing 50K sentences from the
entire bitext for comparison.
Table 3 shows the results. When using only local
features, the max-margin method consistently out-
performs the max-likelihood method in all three test
sets. This clearly shows the advantage of learning
grammars by optimizing BLEU over likelihood.
When incorporating the non-local features into
the max-margin method, we achieve further im-
provement against the max-margin method with-
out non-local features. With non-local features,
our max-margin estimation method outperforms the
baseline by 1.5 BLEU points, and is better than
the max-likelihood estimation by 0.5 BLEU points.
Based on these results, we believe that non-local fea-
tures, which encode information from target parse
structures, are helpful for grammar induction. This
further confirms the advance of the max-margin es-
timation, as it provides us a convenient way to use
non-local features.
262
6 Related Work
As the synchronous grammar is the key compo-
nent in SMT systems, researchers have proposed
various methods to improve the quality of gram-
mars. In addition to the generative and discrimina-
tive models introduced in Section 1, researchers also
have made efforts on word alignment and grammar
weight rescoring.
The first line is to modify word alignment by ex-
ploring information of syntactic structures (May and
Knight, 2007; DeNero and Klein, 2010; Pauls et
al., 2010; Burkett et al, 2010; Riesa et al, 2011).
Such syntactic information is combined with word
alignment via a discriminative framework. These
methods prefer word alignments that are consistent
with syntactic structure alignments. However, la-
beled word alignment data are required in order to
learn the discriminative model.
Yet another line is to rescore the weights of trans-
lation rules. This line of work tries to improve the
relative frequency estimation used in the traditional
pipeline. They rescore the weights or probabilities
of extracted rules. The rescoring is done by using
the similar latent log-linear model as ours (Blun-
som et al, 2008; Ka?a?ria?inen, 2009; He and Deng,
2012), or incorporating various features using la-
beled word aligned bilingual data (Huang and Xi-
ang, 2010). However, in rescoring, translation rules
are still extracted by the heuristic two-step pipeline.
Therefore these previous work still suffers from the
inelegance problem of the traditional pipeline.
Our work also relates to the discriminative train-
ing (Och, 2003; Watanabe et al, 2007; Chiang et al,
2009; Xiao et al, 2011; Gimpel and Smith, 2012)
that has been widely used in SMT systems. Notably,
these discriminative training methods are not used to
learn grammar. Instead, they assume that grammar
are extracted by the traditional two-step pipeline.
7 Conclusion
In this paper we have presented a max-margin esti-
mation for discriminative synchronous grammar in-
duction. By associating the margin with the transla-
tion quality, we directly learn translation rules that
optimize the translation performance measured by
BLEU. Max-margin estimation also provides us a
convenient way to incorporate non-local features.
Experiment results validate the effectiveness of opti-
mizing parameters by BLEU, and the importance of
incorporating non-local features defined on the tar-
get language. These results confirm the advantage of
our max-margin estimation framework as it can both
optimize BLEU and incorporate non-local features.
Feature engineering is very important for discrim-
inative models. Researchers have proposed various
types of features for machine translation, which are
often estimated from word alignments. We would
like to investigate whether further improvement can
be achieved by incorporating such features, espe-
cially the context model (Shen et al, 2009) in the
future. Because our proposed model is quite general,
we are also interested in applying this method to
induce linguistically motivated synchronous gram-
mars for syntax-based SMT.
Acknowledgments
The first author was partially supported by 863
State Key Project (No. 2011AA01A207) and
National Key Technology R&D Program (No.
2012BAH39B03). We are grateful to the anony-
mous reviewers for their insightful comments. We
also thank Yi Lin for her invaluable feedback.
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Prob. ACL 2006, July.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. ACL 2009.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Proc. NAACL 2010.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proc. SSST 2007, NAACL-HLT Workshop on Syntax
and Structure in Statistical Translation, April.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
263
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. EMNLP 2009.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proc. ACL 2010.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP 2008.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The cmu-ark german-english
translation system. In Proc. WMT 2011.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proc. ACL 2012.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for smt. In Proc. Coling 2010.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.
Matti Ka?a?ria?inen. 2009. Sinuhe ? statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proc. EMNLP
2009.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL 2007 (demonstration session).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A bayesian model for learning scfgs with discontigu-
ous rules. In Proc. EMNLP 2012. Association for
Computational Linguistics, July.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL 2009.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Pro. Coling 2004.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP 2002.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc.
EMNLP 2007.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL 2011.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proc. NAACL
2010.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In Proc.
EMNLP 2011.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proc. ICML 2007.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. EMNLP 2004.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In Proc. EMNLP
2011.
Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and
Shouxun Lin. 2012. Unsupervised discriminative in-
duction of synchronous grammar for machine transla-
tion. In Proc. Coling 2012.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proc. NAACL2010.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Prob.
NAACL 2004.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. ACL 2008.
264
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750?758,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Topic Similarity Model
for Hierarchical Phrase-based Translation
Xinyan Xiao? Deyi Xiong? Min Zhang?? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Human Language Technology
Institute of Computing Technology Institute for Infocomm Research
Chinese Academy of Sciences
{xiaoxinyan, liuqun, sxlin}@ict.ac.cn {dyxiong, mzhang?}@i2r.a-star.edu.sg
Abstract
Previous work using topic model for statis-
tical machine translation (SMT) explore top-
ic information at the word level. Howev-
er, SMT has been advanced from word-based
paradigm to phrase/rule-based paradigm. We
therefore propose a topic similarity model to
exploit topic information at the synchronous
rule level for hierarchical phrase-based trans-
lation. We associate each synchronous rule
with a topic distribution, and select desirable
rules according to the similarity of their top-
ic distributions with given documents. We
show that our model significantly improves
the translation performance over the baseline
on NIST Chinese-to-English translation ex-
periments. Our model also achieves a better
performance and a faster speed than previous
approaches that work at the word level.
1 Introduction
Topic model (Hofmann, 1999; Blei et al, 2003) is
a popular technique for discovering the underlying
topic structure of documents. To exploit topic infor-
mation for statistical machine translation (SMT), re-
searchers have proposed various topic-specific lexi-
con translation models (Zhao and Xing, 2006; Zhao
and Xing, 2007; Tam et al, 2007) to improve trans-
lation quality.
Topic-specific lexicon translation models focus
on word-level translations. Such models first esti-
mate word translation probabilities conditioned on
topics, and then adapt lexical weights of phrases
?Corresponding author
by these probabilities. However, the state-of-the-
art SMT systems translate sentences by using se-
quences of synchronous rules or phrases, instead of
translating word by word. Since a synchronous rule
is rarely factorized into individual words, we believe
that it is more reasonable to incorporate the topic
model directly at the rule level rather than the word
level.
Consequently, we propose a topic similari-
ty model for hierarchical phrase-based translation
(Chiang, 2007), where each synchronous rule is as-
sociated with a topic distribution. In particular,
? Given a document to be translated, we cal-
culate the topic similarity between a rule and
the document based on their topic distributions.
We augment the hierarchical phrase-based sys-
tem by integrating the proposed topic similarity
model as a new feature (Section 3.1).
? As we will discuss in Section 3.2, the similarity
between a generic rule and a given source docu-
ment computed by our topic similarity model is
often very low. We don?t want to penalize these
generic rules. Therefore we further propose a
topic sensitivity model which rewards generic
rules so as to complement the topic similarity
model.
? We estimate the topic distribution for a rule
based on both the source and target side topic
models (Section 4.1). In order to calculate sim-
ilarities between target-side topic distributions
of rules and source-side topic distributions of
given documents during decoding, we project
750
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(a) ?? ?? ? opera-
tional capability
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(b) ??X1 ? grandsX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(c) ??X1 ? giveX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(d) X1 ?? ?? X2 ?
held talksX1 X2
Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution,
where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares
the same source Chinese string, but they have different topic distributions due to the different English translations.
the target-side topic distributions of rules into
the space of source-side topic model by one-to-
many projection (Section 4.2).
Experiments on Chinese-English translation tasks
(Section 6) show that, our method outperforms the
baseline hierarchial phrase-based system by +0.9
BLEU points. This result is also +0.5 points high-
er and 3 times faster than the previous topic-specific
lexicon translation method. We further show that
both the source-side and target-side topic distribu-
tions improve translation quality and their improve-
ments are complementary to each other.
2 Background: Topic Model
A topic model is used for discovering the topics
that occur in a collection of documents. Both La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
and Probabilistic Latent Semantic Analysis (PLSA)
(Hofmann, 1999) are types of topic models. LDA
is the most common topic model currently in use,
therefore we exploit it for mining topics in this pa-
per. Here, we first give a brief description of LDA.
LDA views each document as a mixture pro-
portion of various topics, and generates each word
by multinomial distribution conditioned on a topic.
More specifically, as a generative process, LDA first
samples a document-topic distribution for each doc-
ument. Then, for each word in the document, it sam-
ples a topic index from the document-topic distribu-
tion and samples the word conditioned on the topic
index according the topic-word distribution.
Generally speaking, LDA contains two types of
parameters. The first one relates to the document-
topic distribution, which records the topic distribu-
tion of each document. The second one is used for
topic-word distribution, which represents each topic
as a distribution over words. Based on these param-
eters (and some hyper-parameters), LDA can infer a
topic assignment for each word in the documents. In
the following sections, we will use these parameters
and the topic assignments of words to estimate the
parameters in our method.
3 Topic Similarity Model
Sentences should be translated in consistence with
their topics (Zhao and Xing, 2006; Zhao and Xing,
2007; Tam et al, 2007). In the hierarchical phrase
based system, a synchronous rule may be related to
some topics and unrelated to others. In terms of
probability, a rule often has an uneven probability
distribution over topics. The probability over a topic
is high if the rule is highly related to the topic, other-
wise the probability will be low. Therefore, we use
topic distribution to describe the relatedness of rules
to topics.
Figure 1 shows four synchronous rules (Chiang,
2007) with topic distributions, some of which con-
tain nonterminals. We can see that, although the
source part of rule (b) and (c) are identical, their top-
ic distributions are quite different. Rule (b) contains
a highest probability on the topic about ?China-U.S.
relationship?, which means rule (b) is much more
related to this topic. In contrast, rule (c) contains
an even distribution over various topics. Thus, giv-
en a document about ?China-U.S. relationship?, we
hope to encourage the system to apply rule (b) but
penalize the application of rule (c). We achieve this
by calculating similarity between the topic distribu-
tions of a rule and a document to be translated.
More formally, we associate each rule with a rule-
topic distribution P (z|r), where r is a rule, and z is
a topic. Suppose there are K topics, this distribution
751
can be represented by a K-dimension vector. The
k-th component P (z = k|r) means the probability
of topic k given the rule r. The estimation of such
distribution will be described in Section 4.
Analogously, we represent the topic information
of a document d to be translated by a document-
topic distribution P (z|d), which is also a K-
dimension vector. The k-th dimension P (z = k|d)
means the probability of topic k given document d.
Different from rule-topic distribution, the document-
topic distribution can be directly inferred by an off-
the-shelf LDA tool.
Consequently, based on these two distribution-
s, we select a rule for a document to be translat-
ed according to their topic similarity (Section 3.1),
which measures the relatedness of the rule to the
document. In order to encourage the application
of generic rules which are often penalized by our
similarity model, we also propose a topic sensitivity
model (Section 3.2).
3.1 Topic Similarity
By comparing the similarity of their topic distribu-
tions, we are able to decide whether a rule is suitable
for a given source document. The topic similarity
computes the distance of two topic distributions. We
calculate the topic similarity by Hellinger function:
Similarity(P (z|d), P (z|r))
=
K
?
k=1
(
?
P (z = k|d) ?
?
P (z = k|r)
)2
(1)
Hellinger function is used to calculate distribution
distance and is popular in topic model (Blei and Laf-
ferty, 2007).1 By topic similarity, we aim to encour-
age or penalize the application of a rule for a giv-
en document according to their topic distributions,
which then helps the SMT system make better trans-
lation decisions.
3.2 Topic Sensitivity
Domain adaptation (Wu et al, 2008; Bertoldi and
Federico, 2009) often distinguishes general-domain
data from in-domain data. Similarly, we divide the
rules into topic-insensitive rules and topic-sensitive
1We also try other distance functions, including Euclidean
distance, Kullback-Leibler divergence and cosine function.
They produce similar results in our preliminary experiments.
rules according to their topic distributions. Let?s
revisit Figure 1. We can easily find that the topic
distribution of rule (c) distribute evenly. This in-
dicates that it is insensitive to topics, and can be
applied in any topics. We call such a rule a topic-
insensitive rule. In contrast, the distributions of the
rest rules peak on a few topics. Such rules are called
topic-sensitive rules. Generally speaking, a topic-
insensitive rule has a fairly flat distribution, while a
topic-sensitive rule has a sharp distribution.
A document typically focuses on a few topics, and
has a sharp topic distribution. In contrast, the distri-
bution of topic-insensitive rule is fairly flat. Hence,
a topic-insensitive rule is always less similar to doc-
uments and is punished by the similarity function.
However, topic-insensitive rules may be more
preferable than topic-sensitive rules if neither of
them are similar to given documents. For a doc-
ument about the ?military? topic, the rule (b) and
(c) in Figure 1 are both dissimilar to the document,
because rule (b) relates to the ?China-U.S. relation-
ship? topic and rule (c) is topic-insensitive. Never-
theless, since rule (c) occurs more frequently across
various topics, it may be better to apply rule (c).
To address such issue of the topic similarity mod-
el, we further introduce a topic sensitivity model to
describe the topic sensitivity of a rule using entropy
as a metric:
Sensitivity(P (z|r))
= ?
K
?
k=1
P (z = k|r) ? log (P (z = k|r)) (2)
According to the Eq. (2), a topic-insensitive rule has
a large entropy, while a topic-sensitive rule has a s-
maller entropy. By incorporating the topic sensitivi-
ty model with the topic similarity model, we enable
our SMT system to balance the selection of these t-
wo types of rules. Given rules with approximately
equal values of Eq. (1), we prefer topic-insensitive
rules.
4 Estimation
Unlike document-topic distribution that can be di-
rectly learned by LDA tools, we need to estimate the
rule-topic distribution according to our requirement.
In this paper, we try to exploit the topic information
752
of both source and target language. To achieve this
goal, we use both source-side and target-side mono-
lingual topic models, and learn the correspondence
between the two topic models from word-aligned
bilingual corpus.
Specifically, we use two types of rule-topic dis-
tributions: one is source-side rule-topic distribution
and the other is target-side rule-topic distribution.
These two rule-topic distributions are estimated by
corresponding topic models in the same way (Sec-
tion 4.1). Notably, only source language documents
are available during decoding. In order to compute
the similarity between the target-side topic distribu-
tion of a rule and the source-side topic distribution
of a given document?we need to project the target-
side topic distribution of a synchronous rule into the
space of the source-side topic model (Section 4.2).
A more principle way is to learn a bilingual topic
model from bilingual corpus (Mimno et al, 2009).
However, we may face difficulty during decoding,
where only source language documents are avail-
able. It requires a marginalization to infer the mono-
lingual topic distribution using the bilingual topic
model. The high complexity of marginalization pro-
hibits such a summation in practice. Previous work
on bilingual topic model avoid this problem by some
monolingual assumptions. Zhao and Xing (2007)
assume that the topic model is generated in a mono-
lingual manner, while Tam et al, (2007) construct
their bilingual topic model by enforcing a one-to-
one correspondence between two monolingual topic
models. We also estimate our rule-topic distribution
by two monolingual topic models, but use a differ-
ent way to project target-side topics onto source-side
topics.
4.1 Monolingual Topic Distribution Estimation
We estimate rule-topic distribution from word-
aligned bilingual training corpus with documen-
t boundaries explicitly given. The source and tar-
get side distributions are estimated in the same way.
For simplicity, we only describe the estimation of
source-side distribution in this section.
The process of rule-topic distribution estimation
is analogous to the traditional estimation of rule
translation probability (Chiang, 2007). In addition
to the word-aligned corpus, the input for estimation
also contains the source-side topic-document distri-
bution of every documents inferred by LDA tool.
We first extract synchronous rules from training
data in a traditional way. When a rule r is extracted
from a document d with topic distribution P (z|d),
we collect an instance (r, P (z|d), c), where c is the
fraction count of an instance as described in Chiang,
(2007). After extraction, we get a set of instances
I = {(r, P (z|d), c)} with different document-topic
distributions for each rule. Using these instances,
we calculate the topic probability P (z = k|r) as
follows:
P (z = k|r) =
?
I?I c? P (z = k|d)
?K
k?=1
?
I?I c? P (z = k?|d)
(3)
By using both source-side and target-side
document-topic distribution, we obtain two rule-
topic distributions for each rule in total.
4.2 Target-side Topic Distribution Projection
As described in the previous section, we also esti-
mate the target-side rule-topic distribution. How-
ever, only source document-topic distributions are
available during decoding. In order to calculate
the similarity between the target-side rule-topic dis-
tribution of a rule and the source-side document-
topic distribution of a source document, we need to
project target-side topics into the source-side topic
space. The projection contains two steps:
? In the first step, we learn the topic-to-topic cor-
respondence probability p(zf |ze) from target-
side topic ze to source-side topic zf .
? In the second step, we project the target-side
topic distribution of a rule into source-side top-
ic space using the correspondence probability.
In the first step, we estimate the correspondence
probability by the co-occurrence of the source-side
and the target-side topic assignment of the word-
aligned corpus. The topic assignments are output
by LDA tool. Thus, we denotes each sentence pair
by (zf , ze,a), where zf and ze are the topic as-
signments of source-side and target-side sentences
respectively, and a is a set of links {(i, j)}. A
link (i, j) means a source-side position i aligns to
a target-side position j. Thus, the co-occurrence of
a source-side topic with index kf and a target-side
753
e-topic f-topic 1 f-topic 2 f-topic 3
enterprises ??(agricultural) ??(enterprise) ??(develop)
rural ??(rural) ??(market) ??(economic)
state ??(peasant) ??(state) ??(technology )
agricultural ??(reform) ??(company) ??(China)
market ??(finance) ??(finance) ??(technique)
reform ??(social) ??(bank) ??(industry)
production ??(safety) ??(investment) ??(structure)
peasants ??(adjust) ??(manage) ??(innovation)
owned ??(policy) ??(reform) ??(accelerate)
enterprise ??(income) ??(operation) ??(reform)
p(zf |ze) 0.38 0.28 0.16
Table 1: Example of topic-to-topic correspondence. The
last line shows the correspondence probability. Each col-
umnmeans a topic represented by its top-10 topical word-
s. The first column is a target-side topic, while the rest
three columns are source-side topics.
topic ke is calculated by:
?
(zf ,ze,a)
?
(i,j)?a
?(zfi , kf ) ? ?(zej , ke) (4)
where ?(x, y) is the Kronecker function, which is 1
if x = y and 0 otherwise. We then compute the
probability of P (z = kf |z = ke) by normalizing
the co-occurrence count. Overall, after the first step,
we obtain an correspondence matrix MKe?Kf from
target-side topic to source-side topic, where the item
Mi,j represents the probability P (zf = i|ze = j).
In the second step, given the correspondence ma-
trix MKe?Kf , we project the target-side rule-topic
distribution P (ze|r) to the source-side topic space
by multiplication as follows:
T (P (ze|r)) = P (ze|r) ?MKe?Kf (5)
In this way, we get a second distribution for a rule
in the source-side topic space, which we called pro-
jected target-side topic distribution T (P (ze|r)).
Obviously, our projection method allows one
target-side topic to align to multiple source-side top-
ics. This is different from the one-to-one correspon-
dence used by Tam et al, (2007). From the training
result of the correspondence matrix MKe?Kf , we
find that the topic correspondence between source
and target language is not necessarily one-to-one.
Typically, the probability P (z = kf |z = ke) of a
target-side topic mainly distributes on two or three
source-side topics. Table 1 shows an example of
a target-side topic with its three mainly aligned
source-side topics.
5 Decoding
We incorporate our topic similarity model as a
new feature into a traditional hiero system (Chi-
ang, 2007) under discriminative framework (Och
and Ney, 2002). Considering there are a source-
side rule-topic distribution and a projected target-
side rule-topic distribution, we add four features in
total:
? Similarity (P (zf |d), P (zf |r))
? Similarity(P (zf |d), T (P (ze|r)))
? Sensitivity(P (zf |r))
? Sensitivity(T (P (ze|r))
To calculate the total score of a derivation on each
feature listed above during decoding, we sum up the
correspondent feature score of each applied rule.2
The source-side and projected target-side rule-
topic distribution are calculated before decoding.
During decoding, we first infer the topic distribution
P (zf |d) for a given document on source language.
When applying a rule, it is straightforward to calcu-
late these topic features. Obviously, the computa-
tional cost of these features is rather small.
In the topic-specific lexicon translation model,
given a source document, it first calculates the topic-
specific translation probability by normalizing the
entire lexicon translation table, and then adapts the
lexical weights of rules correspondingly. This makes
the decoding slower. Therefore, comparing with the
previous topic-specific lexicon translation method,
our method provides a more efficient way for incor-
porating topic model into SMT.
6 Experiments
We try to answer the following questions by experi-
ments:
1. Is our topic similarity model able to improve
translation quality in terms of BLEU? Further-
more, are source-side and target-side rule-topic
distributions complementary to each other?
2Since glue rule and rules of unknown words are not extract-
ed from training data, here, we just ignore the calculation of the
four features for them.
754
System MT06 MT08 Avg Speed
Baseline 30.20 21.93 26.07 12.6
TopicLex 30.65 22.29 26.47 3.3
SimSrc 30.41 22.69 26.55 11.5
SimTgt 30.51 22.39 26.45 11.7
SimSrc+SimTgt 30.73 22.69 26.71 11.2
Sim+Sen 30.95 22.92 26.94 10.2
Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the
traditional hierarchical system (?Baseline?) and the topic-specific lexicon translation method (?TopicLex?). ?SimSrc?
and ?SimTgt? denote similarity by source-side and target-side rule-distribution respectively, while ?Sim+Sen? acti-
vates the two similarity and two sensitivity features. ?Avg? is the average BLEU score on the two test sets. Scores
marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01).
2. Is it helpful to introduce the topic sensitivi-
ty model to distinguish topic-insensitive and
topic-sensitive rules?
3. Is it necessary to project topics by one-to-many
correspondence instead of one-to-one corre-
spondence?
4. What is the effect of our method on various
types of rules, such as phrase rules and rules
with non-terminals?
6.1 Data
We present our experiments on the NIST Chinese-
English translation tasks. The bilingual training da-
ta contains 239K sentence pairs with 6.9M Chinese
words and 9.14M English words, which comes from
the FBIS portion of LDC data. There are 10,947
documents in the FBIS corpus. The monolingual da-
ta for training English language model includes the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the NIST
evaluation set of 2005 (MT05) as our development
set, and sets of MT06/MT08 as test sets. The num-
bers of documents in MT05, MT06, MT08 are 100,
79, and 109 respectively.
We obtained symmetric word alignments of train-
ing data by first running GIZA++ (Och and Ney,
2003) in both directions and then applying re-
finement rule ?grow-diag-final-and? (Koehn et al,
2003). The SCFG rules are extracted from this
word-aligned training data. A 4-gram language
model was trained on the monolingual data by the
SRILM toolkit (Stolcke, 2002). Case-insensitive
NIST BLEU (Papineni et al, 2002) was used to mea-
sure translation performance. We used minimum er-
ror rate training (Och, 2003) for optimizing the fea-
ture weights.
For the topic model, we used the open source L-
DA tool GibbsLDA++ for estimation and inference.3
GibssLDA++ is an implementation of LDA using
gibbs sampling for parameter estimation and infer-
ence. The source-side and target-side topic models
are estimated from the Chinese part and English part
of FBIS corpus respectively. We set the number of
topic K = 30 for both source-side and target-side,
and use the default setting of the tool for training and
inference.4 During decoding, we first infer the top-
ic distribution of given documents before translation
according to the topic model trained on Chinese part
of FBIS corpus.
6.2 Effect of Topic Similarity Model
We compare our method with two baselines. In addi-
tion to the traditional hiero system, we also compare
with the topic-specific lexicon translation method in
Zhao and Xing (2007). The lexicon translation prob-
ability is adapted by:
p(f |e,DF ) ? p(e|f,DF )P (f |DF ) (6)
=
?
k
p(e|f, z = k)p(f |z = k)p(z = k|DF ) (7)
However, we simplify the estimation of p(e|f, z =
k) by directly using the word alignment corpus with
3http://gibbslda.sourceforge.net/
4We determine K by testing {15, 30, 50, 100, 200} in our
preliminary experiments. We find that K = 30 produces a s-
lightly better performance than other values.
755
Type Count Src% Tgt%
Phrase-rule 3.9M 83.4 84.4
Monotone-rule 19.2M 85.3 86.1
Reordering-rule 5.7M 85.9 86.8
All-rule 28.8M 85.1 86.0
Table 3: Percentage of topic-sensitive rules of various
types of rule according to source-side (?Src?) and target-
side (?Tgt?) topic distributions. Phrase rules are fully
lexicalized, while monotone and reordering rules contain
nonterminals (Section 6.5).
topic assignment that is inferred by the GibbsL-
DA++. Despite the simplification of estimation, the
improvement of our implementation is comparable
with the improvement in Zhao et al,(2007). Given a
new document, we need to adapt the lexical transla-
tion weights of the rules based on topic model. The
adapted lexicon translation model is added as a new
feature under the discriminative framework.
Table 2 shows the result of our method compar-
ing with the traditional system and the topic-lexicon
specific translation method described as above. By
using all the features (last line in the table), we im-
prove the translation performance over the baseline
system by 0.87 BLEU point on average. Our method
also outperforms the topic-lexicon specific transla-
tion method by 0.47 points. This verifies that topic
similarity model can improve the translation quality
significantly.
In order to gain insights into why our model is
helpful, we further investigate how many rules are
topic-sensitive. As described in Section 3.2, we use
entropy to measure the topic sensitivity. If the en-
tropy of a rule is smaller than a certain threshold,
then the rule is topic sensitive. Since documents of-
ten focus on some topics, we use the average entropy
of document-topic distribution of all training docu-
ments as the threshold. We compare both source-
side and target-side distribution shown in Table 3.
We find that more than 80 percents of the rules are
topic-sensitive, thus provides us a large space to im-
prove the translation by exploiting topics.
We also compare these methods in terms of the
decoding speed (words/second). The baseline trans-
lates 12.6 words per second, while the topic-specific
lexicon translation method only translates 3.3 word-
s in one second. The overhead of the topic-specific
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
One-to-One 30.27 22.12 26.20
One-to-Many 30.51 22.39 26.45
Table 4: Effects of one-to-one and one-to-many topic pro-
jection.
lexicon translation method mainly comes from the
adaptation of lexical weights. It takes 72.8% of
the time to do the adaptation, despite only lexical
weights of the used rules are adapted. In contrast,
our method has a speed of 10.2 words per second for
each sentence on average, which is three times faster
than the topic-specific lexicon translation method.
Meanwhile, we try to separate the effects of
source-side topic distribution from the target-side
topic distribution. From lines 4-6 of Table 2. We
clearly find that the two rule-topic distributions im-
prove the performance by 0.48 and 0.38 BLEU
points over the baseline respectively. It seems that
the source-side topic model is more helpful. Fur-
thermore, when combine these two distributions, the
improvement is increased to 0.64 points. This indi-
cates that the effects of source-side and target-side
distributions are complementary.
6.3 Effect of Topic Sensitivity Model
As described in Section 3.2, because the similari-
ty features always punish topic-insensitive rules, we
introduce topic sensitivity features as a complemen-
t. In the last line of Table 2, we obtain a fur-
ther improvement of 0.23 points, when incorporat-
ing topic sensitivity features with topic similarity
features. This suggests that it is necessary to dis-
tinguish topic-insensitive and topic-sensitive rules.
6.4 One-to-One Vs. One-to-Many Topic
Projection
In Section 4.2, we find that source-side topic and
target-side topics may not exactly match, hence we
use one-to-many topic correspondence. Yet anoth-
er method is to enforce one-to-one topic projection
(Tam et al, 2007). We achieve one-to-one projection
by aligning a target topic to the source topic with the
largest correspondence probability as calculated in
Section 4.2.
Table 4 compares the effects of these two method-
756
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
Phrase-rule 30.53 22.29 26.41
Monotone-rule 30.72 22.62 26.67
Reordering-rule 30.31 22.40 26.36
All-rule 30.95 22.92 26.94
Table 5: Effect of our topic model on three types of rules.
Phrase rules are fully lexicalized, while monotone and
reordering rules contain nonterminals.
s. We find that the enforced one-to-one topic method
obtains a slight improvement over the baseline sys-
tem, while one-to-many projection achieves a larger
improvement. This confirms our observation of the
non-one-to-one mapping between source-side and
target-side topics.
6.5 Effect on Various Types of Rules
To get a more detailed analysis of the result, we
further compare the effect of our method on differ-
ent types of rules. We divide the rules into three
types: phrase rules, which only contain terminal-
s and are the same as the phrase pairs in phrase-
based system; monotone rules, which contain non-
terminals and produce monotone translations; re-
ordering rules, which also contain non-terminals but
change the order of translations. We define the
monotone and reordering rules according to Chiang
et al, (2008).
Table 5 show the results. We can see that our
method achieves improvements on all the three type-
s of rules. Our topic similarity method on mono-
tone rule achieves the most improvement which is
0.6 BLEU points, while the improvement on reorder-
ing rules is the smallest among the three types. This
shows that topic information also helps the selec-
tions of rules with non-terminals.
7 Related Work
In addition to the topic-specific lexicon transla-
tion method mentioned in the previous sections,
researchers also explore topic model for machine
translation in other ways.
Foster and Kunh (2007) describe a mixture-model
approach for SMT adaptation. They first split a
training corpus into different domains. Then, they
train separate models on each domain. Finally, they
combine a specific domain translation model with a
general domain translation model depending on var-
ious text distances. One way to calculate the dis-
tance is using topic model.
Gong et al (2010) introduce topic model for fil-
tering topic-mismatched phrase pairs. They first as-
sign a specific topic for the document to be translat-
ed. Similarly, each phrase pair is also assigned with
one specific topic. A phrase pair will be discarded if
its topic mismatches the document topic.
Researchers also introduce topic model for cross-
lingual language model adaptation (Tam et al, 2007;
Ruiz and Federico, 2011). They use bilingual topic
model to project latent topic distribution across lan-
guages. Based on the bilingual topic model, they ap-
ply the source-side topic weights into the target-side
topic model, and adapt the n-gram language model
of target side.
Our topic similarity model uses the document top-
ic information. From this point, our work is related
to context-dependent translation (Carpuat and Wu,
2007; He et al, 2008; Shen et al, 2009). Previous
work typically use neighboring words and sentence
level information, while our work extents the con-
text into the document level.
8 Conclusion and Future Work
We have presented a topic similarity model which
incorporates the rule-topic distributions on both the
source and target side into traditional hierarchical
phrase-based system. Our experimental results show
that our model achieves a better performance with
faster decoding speed than previous work on topic-
specific lexicon translation. This verifies the advan-
tage of exploiting topic model at the rule level over
the word level. Further improvement is achieved by
distinguishing topic-sensitive and topic-insensitive
rules using the topic sensitivity model.
In the future, we are interesting to find ways to
exploit topic model on bilingual data without docu-
ment boundaries, thus to enlarge the size of training
data. Furthermore, our training corpus mainly focus
on news, it is also interesting to apply our method on
corpus with more diverse topics. Finally, we hope to
apply our method to other translation models, espe-
cially syntax-based models.
757
Acknowledgement
The authors were supported by High-Technology
R&D Program (863) Project No 2011AA01A207
and 2012BAH39B03. This work was done dur-
ing Xinyan Xiao?s internship at I2R. We would like
to thank Yun Huang, Zhengxian Gong, Wenliang
Chen, Jun lang, Xiangyu Duan, Jun Sun, Jinsong
Su and the anonymous reviewers for their insightful
comments.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proc of WMT 2009.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. AAS, 1(1):17?35.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. In Proceedings of the MT Sum-
mit XI.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010.
Statistical machine translation based on lda. In Proc.
IUCS 2010, page 286?290, Oct.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proc. EMNLP 2008.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of UAI 1999, pages 289?296.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proc. of EMNLP 2009.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Nick Ruiz and Marcello Federico. 2011. Topic adapta-
tion for lecture translation through bilingual latent se-
mantic models. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, July.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. ICSLP 2002.
Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proc. Coling 2008.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Proc.
ACL 2006.
Bin Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation. In
Proc. NIPS 2007.
758
