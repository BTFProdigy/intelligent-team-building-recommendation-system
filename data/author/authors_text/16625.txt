NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20?28,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Deep Neural Network Language Models
Ebru Ar?soy, Tara N. Sainath, Brian Kingsbury, Bhuvana Ramabhadran
IBM T.J. Watson Research Center
Yorktown Heights, NY, 10598, USA
{earisoy, tsainath, bedk, bhuvana}@us.ibm.com
Abstract
In recent years, neural network language mod-
els (NNLMs) have shown success in both
peplexity and word error rate (WER) com-
pared to conventional n-gram language mod-
els. Most NNLMs are trained with one hid-
den layer. Deep neural networks (DNNs) with
more hidden layers have been shown to cap-
ture higher-level discriminative information
about input features, and thus produce better
networks. Motivated by the success of DNNs
in acoustic modeling, we explore deep neural
network language models (DNN LMs) in this
paper. Results on a Wall Street Journal (WSJ)
task demonstrate that DNN LMs offer im-
provements over a single hidden layer NNLM.
Furthermore, our preliminary results are com-
petitive with a model M language model, con-
sidered to be one of the current state-of-the-art
techniques for language modeling.
1 Introduction
Statistical language models are used in many natural
language technologies, including automatic speech
recognition (ASR), machine translation, handwrit-
ing recognition, and spelling correction, as a crucial
component for improving system performance. A
statistical language model represents a probability
distribution over all possible word strings in a lan-
guage. In state-of-the-art ASR systems, n-grams are
the conventional language modeling approach due
to their simplicity and good modeling performance.
One of the problems in n-gram language modeling
is data sparseness. Even with large training cor-
pora, extremely small or zero probabilities can be
assigned to many valid word sequences. Therefore,
smoothing techniques (Chen and Goodman, 1999)
are applied to n-grams to reallocate probability mass
from observed n-grams to unobserved n-grams, pro-
ducing better estimates for unseen data.
Even with smoothing, the discrete nature of n-
gram language models make generalization a chal-
lenge. What is lacking is a notion of word sim-
ilarity, because words are treated as discrete enti-
ties. In contrast, the neural network language model
(NNLM) (Bengio et al, 2003; Schwenk, 2007) em-
beds words in a continuous space in which proba-
bility estimation is performed using single hidden
layer neural networks (feed-forward or recurrent).
The expectation is that, with proper training of the
word embedding, words that are semantically or gra-
matically related will be mapped to similar loca-
tions in the continuous space. Because the prob-
ability estimates are smooth functions of the con-
tinuous word representations, a small change in the
features results in a small change in the probabil-
ity estimation. Therefore, the NNLM can achieve
better generalization for unseen n-grams. Feed-
forward NNLMs (Bengio et al, 2003; Schwenk
and Gauvain, 2005; Schwenk, 2007) and recur-
rent NNLMs (Mikolov et al, 2010; Mikolov et al,
2011b) have been shown to yield both perplexity and
word error rate (WER) improvements compared to
conventional n-gram language models. An alternate
method of embedding words in a continuous space
is through tied mixture language models (Sarikaya
et al, 2009), where n-grams frequencies are mod-
eled similar to acoustic features.
To date, NNLMs have been trained with one hid-
20
den layer. A deep neural network (DNN) with mul-
tiple hidden layers can learn more higher-level, ab-
stract representations of the input. For example,
when using neural networks to process a raw pixel
representation of an image, lower layers might de-
tect different edges, middle layers detect more com-
plex but local shapes, and higher layers identify ab-
stract categories associated with sub-objects and ob-
jects which are parts of the image (Bengio, 2007).
Recently, with the improvement of computational
resources (i.e. GPUs, mutli-core CPUs) and better
training strategies (Hinton et al, 2006), DNNs have
demonstrated improved performance compared to
shallower networks across a variety of pattern recog-
nition tasks in machine learning (Bengio, 2007;
Dahl et al, 2010).
In the acoustic modeling community, DNNs
have proven to be competitive with the well-
established Gaussian mixture model (GMM) acous-
tic model. (Mohamed et al, 2009; Seide et al, 2011;
Sainath et al, 2012). The depth of the network (the
number of layers of nonlinearities that are composed
to make the model) and the modeling a large number
of context-dependent states (Seide et al, 2011) are
crucial ingredients in making neural networks com-
petitive with GMMs.
The success of DNNs in acoustic modeling leads
us to explore DNNs for language modeling. In this
paper we follow the feed-forward NNLM architec-
ture given in (Bengio et al, 2003) and make the neu-
ral network deeper by adding additional hidden lay-
ers. We call such models deep neural network lan-
guage models (DNN LMs). Our preliminary experi-
ments suggest that deeper architectures have the po-
tential to improve over single hidden layer NNLMs.
This paper is organized as follows: The next sec-
tion explains the architecture of the feed-forward
NNLM. Section 3 explains the details of the baseline
acoustic and language models and the set-up used
for training DNN LMs. Our preliminary results are
given in Section 4. Section 5 summarizes the related
work to our paper. Finally, Section 6 concludes the
paper.
2 Neural Network Language Models
This section describes a general framework for feed-
forward NNLMs. We will follow the notations given
























  
 
	

	

	

	
