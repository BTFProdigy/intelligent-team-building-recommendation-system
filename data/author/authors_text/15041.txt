Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1114?1124, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
On Amortizing Inference Cost for Structured Prediction
Vivek Srikumar? and Gourab Kundu? and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801
{vsrikum2, kundu2, danr}@illinois.edu
Abstract
This paper deals with the problem of predict-
ing structures in the context of NLP. Typically,
in structured prediction, an inference proce-
dure is applied to each example independently
of the others. In this paper, we seek to op-
timize the time complexity of inference over
entire datasets, rather than individual exam-
ples. By considering the general inference
representation provided by integer linear pro-
grams, we propose three exact inference the-
orems which allow us to re-use earlier solu-
tions for certain instances, thereby completely
avoiding possibly expensive calls to the infer-
ence procedure. We also identify several ap-
proximation schemes which can provide fur-
ther speedup. We instantiate these ideas to the
structured prediction task of semantic role la-
beling and show that we can achieve a speedup
of over 2.5 using our approach while retain-
ing the guarantees of exactness and a further
speedup of over 3 using approximations that
do not degrade performance.
1 Introduction
Typically, in structured prediction applications, ev-
ery example is treated independently and an infer-
ence algorithm is applied to each one of them. For
example, consider a dependency parser that uses the
maximum spanning tree algorithm (McDonald et al
2005) or its integer linear program variants (Riedel
and Clarke, 2006; Martins et al 2009) to make pre-
dictions. Given a trained model, the parser addresses
* These authors contributed equally to this work.
each sentence separately and runs the inference al-
gorithm to predict the parse tree. Thus, the time
complexity of inference over the test set is linear in
the size of the corpus.
In this paper, we ask the following question: For
a given task, since the inference procedure predicts
structures from the same family of structures (depen-
dency trees, semantic role structures, etc.), can the
fact that we are running inference for a large num-
ber of examples help us improve the time complexity
of inference? In the dependency parsing example,
this question translates to asking whether, having
parsed many sentences, we can decrease the parsing
time for the next sentence.
Since any combinatorial optimization problem
can be phrased as an integer linear program (ILP),
we frame inference problems as ILPs for the purpose
of analysis. By analyzing the objective functions
of integer linear programs, we identify conditions
when two ILPs have the same solution. This allows
us to reuse solutions of previously solved problems
and theoretically guarantee the optimality of the so-
lution. Furthermore, in some cases, even when the
conditions are not satisfied, we can reuse previous
solutions with high probability of being correct.
Given the extensive use of integer linear programs
for structured prediction in Natural Language Pro-
cessing over the last few years, these ideas can be ap-
plied broadly to NLP problems. We instantiate our
improved inference approaches in the structured pre-
diction task of semantic role labeling, where we use
an existing implementation and a previous trained
model that is based on the approach of (Punyakanok
et al 2008). We merely modify the inference pro-
1114
cess to show that we can realize the theoretical gains
by making fewer calls to the underlying ILP solver.
Algorithm Speedup
Theorem 1 2.44
Theorem 2 2.18
Theorem 3 2.50
Table 1: The speedup for semantic role labeling cor-
responding to the three theorems described in this
paper. These theorems guarantee the optimality of
the solution, thus ensuring that the speedup is not
accompanied by any loss in performance.
Table 1 presents a preview of our results, which
are discussed in Section 4. All three approaches in
this table improve running time, while guaranteeing
optimum solutions. Allowing small violations to the
conditions of the theorems provide an even higher
improvement in speedup (over 3), without loss of
performance.
The primary contributions of this paper are:
1. We pose the problem of optimizing inference
costs over entire datasets rather than individ-
ual examples. Our approach is agnostic to the
underlying models and allows us to use pre-
trained scoring functions.
2. We identify equivalence classes of ILP prob-
lems and use this notion to prove exact con-
ditions under which no inference is required.
These conditions lead to algorithms that can
speed up inference problem without losing the
exactness guarantees. We also use these con-
ditions to develop approximate inference algo-
rithms that can provide a further speedup.
3. We apply our approach to the structured pre-
diction task of semantic role labeling. By not
having to perform inference on some of the in-
stances, those that are equivalent to previously
seen instances, we show significant speed up in
terms of the number of times inference needs to
be performed. These gains are also realized in
terms of wall-clock times.
The rest of this paper is organized as follows: In
section 2, we formulate the problem of amortized
inference and provide motivation for why amortized
gains can be possible. This leads to the theoretical
discussion in section 3, where we present the meta-
algorithm for amortized inference along with sev-
eral exact and approximate inference schemes. We
instantiate these schemes for the task of semantic
role labeling (Section 4). Section 5 discusses related
work and future research directions.
2 Motivation
Many NLP tasks can be phrased as structured pre-
diction problems, where the goal is to jointly assign
values to many inference variables while account-
ing for possible dependencies among them. This de-
cision task is a combinatorial optimization problem
and can be solved using a dynamic programming ap-
proach if the structure permits. In general, the infer-
ence problem can be formulated and solved as inte-
ger linear programs (ILPs).
Following (Roth and Yih, 2004) Integer linear
programs have been used broadly in NLP. For exam-
ple, (Riedel and Clarke, 2006) and (Martins et al
2009) addressed the problem of dependency pars-
ing and (Punyakanok et al 2005; Punyakanok et
al., 2008) dealt with semantic role labeling with this
technique.
In this section, we will use the ILP formulation
of dependency parsing to introduce notation. The
standard approach to framing dependency parsing as
an integer linear program was introduced by (Riedel
and Clarke, 2006), who converted the MST parser
of (McDonald et al 2005) to use ILP for inference.
The key idea is to build a complete graph consist-
ing of tokens of the sentence where each edge is
weighted by a learned scoring function. The goal
of inference is to select the maximum spanning tree
of this weighted graph.
2.1 Problem Formulation
In this work, we consider the general inference prob-
lem of solving a 0-1 integer linear program. To per-
form inference, we assume that we have a model that
assigns scores to the ILP decision variables. Thus,
our work is applicable not only in cases where in-
ference is done after a separate learning phase, as in
(Roth and Yih, 2004; Clarke and Lapata, 2006; Roth
and Yih, 2007) and others, but also when inference
is done during the training phase, for algorithms like
1115
the structured perceptron of (Collins, 2002), struc-
tured SVM (Tsochantaridis et al 2005) or the con-
straints driven learning approach of (Chang et al
2007).
Since structured prediction assigns values to a
collection of inter-related binary decisions, we de-
note the ith binary decision by yi ? {0, 1} and the
entire structure as y, the vector composed of all the
binary decisions. In our running example, each edge
in the weighted graph generates a single decision
variable (for unlabeled dependency parsing). For
each yi, let ci ? < denote the weight associated with
it. We denote the entire collection of weights by the
vector c, forming the objective for this ILP.
Not all assignments to these variables are valid.
Without loss of generality, these constraints can be
expressed using linear inequalities over the infer-
ence variables, which we write as MTy ? b for
a real valued matrix M and a vector b. In depen-
dency parsing, for example, these constraints ensure
that the final output is a spanning tree.
Now, the overall goal of inference is to find the
highest scoring structure. Thus, we can frame infer-
ence as an optimization problem p with n inference
variables as follows:
arg max
y?{0,1}n
cTy (1)
subject to MTy ? b. (2)
For brevity, we denote the space of feasible solutions
that satisfy the constraints for the ILP problem p as
Kp = {y ? {0, 1}n|MTy ? b}. Thus, the goal of
inference is to find
arg max
y?Kp
cTy.
We refer to Kp as the feasible set for the inference
problem p and yp as its solution.
In the worst case, integer linear programs are
known to be NP-hard. Hence, solving large prob-
lems, (that is, problems with a large number of con-
straints and/or variables) can be infeasible.
For structured prediction problems seen in NLP,
we typically solve many instances of inference prob-
lems. In this paper, we investigate whether an infer-
ence algorithm can use previous predictions to speed
up inference time, thus giving us an amortized gain
in inference time over the lifetime of the program.
We refer to inference algorithms that have this capa-
bility as amortized inference algorithms.
In our running example, each sentence corre-
sponds to a separate ILP. Over the lifetime of the
dependency parser, we create one inference instance
(that is, one ILP) per sentence and solve it. An amor-
tized inference algorithm becomes faster at parsing
as it parses more and more sentences.
2.2 Why can inference costs be amortized over
datasets?
In the rest of this section, we will argue that the time
cost of inference can be amortized because of the
nature of inference in NLP tasks. Our argument is
based on two observations, which are summarized in
Figure (1): (1) Though the space of possible struc-
tures may be large, only a very small fraction of
these occur. (2) The distribution of observed struc-
tures is heavily skewed towards a small number of
them.
x?s p?s y?s
ILP
formulation
Inference
Examples
ILPs
Solutions
Figure 1: For a structured prediction task, the infer-
ence problem p for an example x needs to be for-
mulated before solving it to get the structure y. In
structured prediction problems seen in NLP, while
an exponential number of structures is possible for a
given instance, in practice, only a small fraction of
these ever occur. This figure illustrates the empirical
observation that there are fewer inference problems
p?s than the number of examples and the number of
observed structures y?s is even lesser.
As an illustration, consider the problem of part-
of-speech tagging. With the standard Penn Treebank
tag set, each token can be assigned one of 45 labels.
Thus, for a sentence of size n, we could have 45n
structures out of which the inference process needs
to choose one. However, a majority of these struc-
tures never occur. For example, we cannot have a
1116
 0
 100000
 200000
 300000
 400000
 500000
 0  10  20  30  40  50Number of tokens
Part-of-speech statistics, using tagged Gigaword text
Number of examples of size Number of unique POS tag sequences
(a) Part-of-speech tagging
 0
 100000
 200000
 300000
 400000
 500000
 0  10  20  30  40  50Size of sentence
Unlabeled dependency parsing statistics, using tagged Gigaword text
Number of examples of sizeNumber of unique dependency trees
(b) Unlabeled dependency parsing
 0
 20000
 40000
 60000
 80000
 100000
 120000
 140000
 160000
 0  1  2  3  4  5  6  7  8Size of the input (number of argument candidates)
SRL statistics , using tagged Gigaword text
Number of examples of sizeNumber of unique SRL structures
(c) Semantic role labeling
Figure 2: Number of inference instances for different input sizes (red solid lines) and the number of unique
structures for each size (blue dotted lines). The x-axis indicates the size of the input (number of tokens
for part of speech and dependency, and number of argument candidates for SRL.) Note that the number of
instances is not the number of unique examples of a given length, but the number of times an inference
procedure is called for an input of a given size.
 0
 2
 4
 6
 8
 10
 12
 0  5000  10000  15000  20000Solution Id
Log frequency of solutions for sentences with 5 tokens
(a) Sentence length = 5
-1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  50000  100000  150000  200000  250000Solution Id
Log frequency of solutions for sentences with 10 tokens
(b) Sentence length = 10
-1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  50000  100000  150000  200000  250000  300000  350000Solution Id
Log frequency of solutions for sentences with 15 tokens
(c) Sentence length = 15
Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences
with five, ten and fifteen tokens. The x-axes list different unique part-of-speech tag sequences for the entire
sentence. These plots show that for sentences of a given length, most structures (solutions) that are possible
never occur, or occur very infrequently; only a few of the possible structures (solutions) actually occur
frequently.
sentence where all the tokens are determiners.
Furthermore, many sentences of the same size
share the same part-of-speech tag sequence. To
quantify the redundancy of structures, we part-of-
speech tagged the English Gigaword corpus (Graff
and Cieri, 2003). Figure (2a) shows the number
of sentences in the corpus for different sentence
lengths. In addition, it also shows the number of
unique part-of-speech tag sequences (over the en-
tire sentence) for each size. We see that the number
of structures is much fewer than the number of in-
stances for any sentence size. Note that 45n quickly
outgrows the number of sentences as n increases.
The figures (2b) and (2c) show similar statistics for
unlabeled dependency parsing and semantic role la-
beling. In the former case, the size of the instance is
the number of tokens in a sentence, while in the lat-
ter, the size is the number of argument candidates
that need to be labeled for a given predicate. In
both cases, we see that the number of empirically
observed structures is far fewer than the number of
instances to be labeled.
Thus, for any given input size, the number of in-
stances of that size (over the lifetime of the program)
far exceeds the number of observed structures for
that size. Moreover, the number of observed struc-
tures is significantly smaller than the number of the-
oretically possible structures. Thus, we have a small
number of structures that form optimum structures
for many inference instances of the same size.
Our second observation deals with the distribu-
tion of structures for a given input size. Figure (3)
1117
shows the log frequencies of part-of-speech tagging
sequences for sentences of lengths five, ten and fif-
teen. In all cases, we see that a few structures are
most frequent. We observed similar distributions of
structures for all input sizes for dependency parsing
and semantic role labeling as well.
Since the number of structures for a given exam-
ple size is small, many examples x?s, and hence
many inference problems p?s, are associated with
the same structure y. These observations suggest the
possibility of getting an amortized gain in inference
time by characterizing the set of inference problems
that produce the same structure. Then, for a new in-
ference problem, if we can identify that it belongs to
a known set, that is, will yield a solution that we have
already seen, we do not have to run inference at all.
The second observation also suggests that this char-
acterization of sets of problems that have the same
solution can be done in a data-driven way because
characterizing a small number of structures can give
us high coverage.
3 Amortizing inference costs
In this section, we present different schemes for
amortized inference leading up to an inference meta-
algorithm. The meta-algorithm is both agnostic to
the underlying inference algorithm that is used by
the problem and maintains the exactness properties
of the underlying inference scheme. That is, if we
have an exact/approximate inference algorithm with
a certain guarantees, the meta-algorithm will have
the same guarantees, but with a speedup.
3.1 Notation
For an integer linear program p with np variables,
we denote its objective coefficients by cp and its fea-
sible set byKp. We denote its solution as as yp. We
represent vectors by boldfaced symbols and their ith
component using subscripts.
We consider many instantiations of the inference
problem and use superscripts to denote each indi-
vidual instance. Thus, we have a large collection of
inference instances P = {p1,p2, ? ? ? } along with
their respective solutions {y1p,y
2
p, ? ? ? }.
Definition 1 (Equivalence classes of ILPs). Two in-
teger linear programs are said to be in the same
equivalence class if they have the same number of
inference variables and the same feasible set.
We square brackets to denote equivalence classes.
If [P ] is an equivalence class of ILPs, we use the
notation K[P ] to denote its feasible set and n[P ] to
denote the number of variables. Also, for a program
p, we use the notation p ? [P ] to indicate that it
belongs to the equivalence class [P ].
3.2 Exact theorems
Our goal is to characterize the set of objective func-
tions which will have the same solution for a given
equivalence class of problems.
Suppose we have solved an ILP p to get a solution
yp. For every inference variable that is active in the
solution (i.e., whose value is 1), increasing the corre-
sponding objective value will not change the optimal
assignment to the variables. Similarly, for all other
variables (whose value in the solution is 0), decreas-
ing the objective value will not change the optimal
solution. This intuition gives us our first theorem for
checking whether two ILPs have the same solution
by looking at the difference between their objective
coefficients.
Theorem 1. Let p denote an inference problem
posed as an integer linear program belonging to an
equivalence class [P ]. Let q ? [P ] be another infer-
ence instance in the same equivalence class. Define
?c = cq ? cp to be the difference of the objective
coefficients of the ILPs. Then, yp is the solution of
the problem q if for each i ? {1, ? ? ? , np}, we have
(2yp,i ? 1)?ci ? 0 (3)
The condition in the theorem, that is, inequal-
ity (3), requires that the objective coefficients corre-
sponding to values yp,i that are set to 1 in p increase,
and those that correspond to values of yp,i set to 0,
decrease. Under these conditions, if yp is the max-
imizer of the original objective, then it maximizes
the new objective too.
Theorem 1 identifies perturbations of an ILP?s ob-
jective coefficients that will not change the optimal
assignment. Next, we will characterize the sets of
objective values that will have the same solution us-
ing a criterion that is independent of the actual so-
lution. Suppose we have two ILPs p and q in an
equivalence class [P ] whose objective values are cp
and cq respectively. Suppose y? is the solution to
1118
both these programs. That is, for every y ? K[P ],
we have cTpy ? c
T
py
? and cTqy ? c
T
qy
?. Multiply-
ing these inequalities by any two positive real num-
bers x1 and x2 and adding them shows us that y?
is also the solution for the ILP in [P ] which has the
objective coefficients x1cp + x2cq. Extending this
to an arbitrary number of inference problems gives
us our next theorem.
Theorem 2. Let P denote a collection
{p1,p2, ? ? ? ,pm} of m inference problems in
the same equivalence class [P ] and suppose that
all the problems have the same solution, yp. Let
q ? [P ] be a new inference program whose optimal
solution is y. Then y = yp if there is some x ? <m
such that x ? 0 and
cq =
?
j
xjcjp. (4)
From the geometric perspective, the pre-condition
of this theorem implies that if the new coefficients
lie in the cone formed by the coefficients of the pro-
grams that have the same solution, then the new pro-
gram shares the solution.
Theorems 1 and 2 suggest two different ap-
proaches for identifying whether a new ILP can
use the solution of previously solved inference in-
stances. These theorems can be combined to get a
single criterion that uses the objective coefficients of
previously solved inference problems and their com-
mon solution to determine whether a new inference
problem will have the same solution. Given a collec-
tion of solved ILPs that have the same solution, from
theorem 2, we know that an ILP with the objective
coefficients c =
?
j xjc
j
p will share the solution.
Considering an ILP whose objective vector is c and
applying theorem 1 to it gives us the next theorem.
Theorem 3. Let P denote a collection
{p1,p2, ? ? ? ,pm} of m inference problems
belonging to the same equivalence class [P ].
Furthermore, suppose all the programs have the
same solution yp. Let q ? [P ] be a new inference
program in the equivalence class. For any x ? <m,
define ?c(x) = cq ?
?
j xjc
j
p. The assignment
yp is the optimal solution of the problem q if there
is some x ? <m such that x ? 0 and for each
i ? {1, np}, we have
(2yp,i ? 1)?ci ? 0 (5)
Theorem Condition
Theorem 1 ?i ? {1, ? ? ? , np},
(2yp,i ? 1)?ci ? 0; ?i.
Theorem 2 ? x ? <m, such that
x ? 0 and cq =
?
j xjc
j
p
Theorem 3 ? x ? <m, such that
x ? 0 and (2yp,i ? 1)?ci ? 0; ?i.
Table 2: Conditions for checking whether yp is the
solution for an inference problem q ? [P ] according
to theorems 1, 2 and 3. Please refer to the statements
of the theorems for details about the notation.
3.3 Implementation
Theorems 1, 2 and 3 each specify a condition that
checks whether a pre-existing solution is the opti-
mal assignment for a new inference problem. These
conditions are summarized in Table 2. In all cases,
if the condition matches, the theorems guarantee that
the two solutions will be the same. That is, applying
the theorems will not change the performance of the
underlying inference procedure. Only the number of
inference calls will be decreased.
In our implementation of the conditions, we used
a database1 to cache ILPs and implemented the
retrieval of equivalence classes and solutions as
queries to the database. To implement theorem 1,
we iterate over all ILPs in the equivalence class and
check if the condition is satisfied for one of them.
The conditions of theorems 2 and 3 check whether a
collection of linear (in)equalities has a feasible solu-
tion using a linear program solver.
We optimize the wall-clock time of theorems 2
and 3 by making two observations. First, we do not
need to solve linear programs for all possible ob-
served structures. Given an objective vector, we only
need consider the highest scoring structures within
an equivalence class. (All other structures cannot
be the solution to the ILP.) Second, since theorem
2 checks whether an ILP lies within a cone, we can
optimize the cache for theorem 2 by only storing the
ILPs that form on the boundary of the cone. A sim-
ilar optimization can be performed for theorem 3 as
well. Our implementation uses the following weaker
version of this optimization: while caching ILPs, we
1We used the H2 database engine, which can be downloaded
from http://h2database.com, for all caching.
1119
do not add an instance to the cache if it already satis-
fies the theorem. This optimization reduces the size
of the linear programs used to check feasibility.
3.4 Approximation schemes
So far, in the above three theorems, we retain the
guarantees (in terms of exactness and performance)
of the underlying inference procedure. Now, we will
look at schemes for approximate inference. Unlike
the three theorems listed above, with the following
amortized inference schemes, we are not guaranteed
an optimal solution.
3.4.1 Most frequent solution
The first scheme for approximation uses the ob-
servation that the most frequent solution occurs an
overwhelmingly large number of times, compared to
the others. (See the discussion in section 2.2 and fig-
ures 3a, 3b and 3c for part-of-speech tagging.) Un-
der this approximation scheme, given an ILP prob-
lem, we simply pick the most frequent solution for
that equivalence class as the solution, provided this
solution has been seen a sufficient number of times.
If the support available in the cache is insufficient,
we call the underlying inference procedure.
3.4.2 Top-K approximation
The previous scheme for approximate amortized
inference is agnostic to the objective coefficients of
integer linear program to be solved and uses only
its equivalence class to find a candidate structure.
The top-K approach extends this by scoring the K
most frequent solutions using the objective coeffi-
cients and selecting the highest scoring one as the
solution to the ILP problem. As with the previous
scheme, we only consider solutions that have suffi-
cient support.
3.4.3 Approximations to theorems 1 and 3
The next approximate inference schemes relaxes
the conditions in theorems 1 and 3 by allowing the
inequalities to be violated by . That is, the inequal-
ity (3) from Theorem 1 now becomes
(2yp,i ? 1)?ci +  ? 0. (6)
The inequality (5) from Theorem 3 is similarly re-
laxed as follows:
(2yp,i ? 1)?ci +  ? 0 (7)
3.5 Amortized inference algorithm
Each exact and approximate inference approach de-
scribed above specifies a condition to check whether
an inference procedure should be called for a
new problem. This gives us the following meta-
algorithm for amortized inference, parameterized by
the actual scheme used: If the given input instance p
satisfies the condition specified by the scheme, then
use the cached solution. Otherwise, call the infer-
ence procedure and cache the solution for future use.
4 Experiments
In this section, we apply the theory from Section 3 to
the structure prediction problem of semantic role la-
beling. Since the inference schemes presented above
are independent of the learning aspects, we use an
off-the-shelf implementation and merely modify the
inference as discussed in Section 3.5.
The goal of the experiments is to show that us-
ing an amortized inference algorithm, we can make
fewer calls to the underlying inference procedure.
For the exact inference algorithms, doing so will not
change the performance as compared to the under-
lying system. For the approximations, we can make
a trade-off between the inference time and perfor-
mance.
4.1 Experimental setup
Our goal is to simulate a long-running NLP process
that can use a cache of already solved problems to
improve inference time. Given a new input problem,
our theorems require us to find all elements in the
equivalence class of that problem along with their
solutions. Intuitively, we expect a higher probability
of finding members of an arbitrary equivalence class
if the size of the cache is large. Hence, we processed
sentences from the Gigaword corpus and cached the
inference problems for our task.
The wall-clock time is strongly dependent on such
specific implementation of the components, which
are independent of the main contributions of this
work. Also, in most interesting applications, the
computation time for each step will be typically
dominated by the number of inference steps, espe-
cially with efficient implementations of caching and
retrieval. Hence, the number of calls to the underly-
ing procedure is the appropriate complexity param-
1120
eter. Let NBase be the number of times we would
need to call the underlying inference procedure had
we not used an amortized algorithm. (This is the
same as the number of inference problems.) Let NA
be the number of times the underlying inference pro-
cedure is actually called using an amortized algo-
rithm A. We define the speedup of A as
Speedup(A) =
NBase
NA
. (8)
We also report the clock speedup of our implemen-
tation for all algorithms, which is the ratio of the
wall-clock time taken by the baseline algorithm to
that of the amortized algorithm. For measuring time,
we only measure the time for inference as the other
aspects (feature extraction, scoring, etc.) are not
changed.
4.2 Semantic Role Labeling
The goal of Semantic Role Labeling (SRL) (Palmer
et al 2010) is to identify and assign semantic roles
to arguments of verb predicates in a sentence. For
example, consider the the sentence John gave the
ball to Mary. The verb give takes three arguments,
John, the ball and to Mary, which are labeled A0,
A1 and A2 respectively.
We used the system of (Punyakanok et al 2008)
as our base SRL system. It consists of two classi-
fiers trained on the Propbank corpus. The first one,
called the argument identifier, filters argument can-
didates which are generated using a syntactic parse-
based heuristic. The second model scores each can-
didate that has not been filtered for all possible argu-
ment labels. The scores for all candidates of a pred-
icate are combined via inference. As in the system
of (Punyakanok et al 2008), the softmax function
is applied to the raw classifier scores to ensure that
they are in the same numeric range.
Inference mandates that certain structural and
linguistic constraints hold over the full predicate-
argument structure for a verb. (Punyakanok et al
2008) modeled inference via an integer linear pro-
gram instance, where each assignment of labels
to candidates corresponds to one decision variable.
Given a set of argument candidates, the feasible set
of decisions is dependent of the number of argument
candidates and the verb predicate. Thus, in terms
of the notation used in this paper, the equivalence
classes are defined by the pair (predicate, number of
argument candidates).
We ran the semantic role labeler on 225,000 verb
predicates from the Gigaword corpus and cached
the equivalence classes, objective coefficients and
solutions generated by the SRL system. We re-
port speedup for the various amortized inference
schemes on the standard Penn Treebank test set. On
this data, the unaltered baseline system, processes
5127 integer linear programs and achieves an F1 of
75.85%.
Table 3 shows the speedup and performance for
the various inference schemes. The most frequent
and top-K systems are both naive solutions that take
advantage of the cache of stored problems. In spite
of their simplicity, they attain F1 scores of 62%
and 70.06% because few structures occur most fre-
quently, as described in section 2.2. We see that all
the exact theorems attain a speedup higher than two
without losing performance. (The variation in F1 be-
tween them is because of the existence of different
equivalent solutions in terms of the objective value.)
This shows us that we can achieve an amortized gain
in inference. Note that a speedup of 2.5 indicates
that the solver is called only for 40% of the exam-
ples. The approximate versions of theorems 1 and 3
(with  = 0.3 in both cases, which was not tuned)
attain an even higher gain in speedup over the base-
line than the base versions of the theorems. Interest-
ingly, the SRL performance in both cases does not
decline much even though the conditions of the the-
orems may be violated.
5 Related work and Future directions
In recent years, we have seen several approaches to
speeding up inference using ideas like using the cut-
ting plane approach (Riedel, 2009), dual decompo-
sition and Lagrangian relaxation (Rush et al 2010;
Chang and Collins, 2011). The key difference be-
tween these and the work in this paper is that all
these approaches solve one instance at a time. Since
we can use any inference procedure as a underlying
system, the speedup reported in this paper is appli-
cable to all these algorithms.
Decomposed amortized inference In this paper,
we have taken advantage of redundancy of struc-
tures that can lead to the re-use of solutions. In the
1121
Type Algorithm # instances # solver Speedup Clock F1
calls speedup
Exact Baseline 5127 5217 1.0 1.0 75.85
Exact Theorem 1 5127 2134 2.44 1.54 75.90
Exact Theorem 2 5127 2390 2.18 1.14 75.79
Exact Theorem 3 5127 2089 2.50 1.36 75.77
Approx. Most frequent (Support = 50) 5127 2812 1.86 1.57 62.00
Approx. Top-10 solutions (Support = 50) 5127 2812 1.86 1.58 70.06
Approx. Theorem 1 (approx,  = 0.3) 5127 1634 3.19 1.81 75.76
Approx. Theorem 3 (approx,  = 0.3) 5127 1607 3.25 1.50 75.46
Table 3: Speedup and performance for various inference methods for the task of Semantic Role Labeling.
All the exact inference algorithms get a speedup higher than two. The speedup of the approximate version
of the theorems is even higher without loss of performance. The clock speedup is defined as the ratio of the
inference times of the baseline and the given algorithm. All numbers are averaged over ten trials.
part of speech example, we showed redundancy of
structures at the sentence level (Figure 2a). How-
ever, for part-of-speech tagging, the decisions are
rarely, if at all, dependent on a very large context.
One direction of future work is to take advantage of
the fact that the inference problem can be split into
smaller sub-problems. To support this hypothesis,
we counted the number of occurrences of ngrams
of tokens (including overlapping and repeated men-
tions) for n <= 10 and compared this to the number
of unique part-of-speech ngrams of this length. Fig-
ure 4 shows these two counts. Following the argu-
ment in Section 2.2, this promises a large amortized
gain in inference time. We believe that such decom-
position can also be applied to other, more complex
structured prediction tasks.
The value of approximate inference From the
experiments, we see that the first two approximate
inference schemes (most frequent solution and the
top-K scheme) can speed up inference with the
only computational cost being the check for pre-
conditions of the exact theorems. Effectively, these
algorithms have parameters (i.e., the support param-
eter) that allow us to choose between the inference
time and performance. Figure 5 shows the perfor-
mance of the most frequent and top-K baselines for
different values of the support parameter, which in-
dicates how often a structure must occur for it to be
considered. We see that for lower values of support,
we can get a very high speedup but pay with poorer
performance.
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 7e+06
 0  2  4  6  8  10
Number of tokens
Part-of-speech ngram statistics, using tagged Gigaword text
Number of instances for size Number of unique structures for given length
Figure 4: The red line shows the number of ngrams
of tokens (including overlapping and repeated oc-
currences) in the Gigaword corpus and the blue line
shows the number of unique POS tag sequences.
However, the prediction of the approximate al-
gorithms can be used to warm-start any solver that
can accept an external initialization. Warm-starting
a solver can give a way to get the exact solution and
yet take advantage of the frequency of structures that
have been observed.
Lifted inference The idea of amortizing inference
time over the dataset is conceptually related to the
idea of lifted inference (de Salvo Braz et al 2005).
We abstract many instances into equivalence classes
and deal with the inference problem with respect to
the equivalence classes in the same way as done in
lifted inference algorithms.
1122
 1
 1.5
 2
 2.5
 3
 3.5
 0  200  400  600  800  1000 50
 55
 60
 65
 70
 75
 80
Support
Performance of the most frequent and top-K schemes for different values of support
SpeedupPerformance of most frequent solution (F1)Performance of top-K solution (F1)
Figure 5: Most frequent solutions and top-K:
Speedup and SRL performance (F1) for different
values of the support parameter, using the most-
frequent solutions (dashed blue line) and the top-
K scheme (thick gray line). Support indicates how
many times a structure should be seen for it to be
considered. Note that the speedup values for both
schemes are identical (red line).
6 Conclusion
In this paper, we addressed structured prediction in
the context of NLP and proposed an approach to im-
prove inference costs over an entire dataset, rather
than individual instances. By treating inference
problems as instances of integer linear programs, we
proposed three exact theorems which identify exam-
ples for which the inference procedure need not be
called at all and previous solutions can be re-used
with the guarantee of optimality. In addition, we
also proposed several approximate algorithms. We
applied our algorithms, which are agnostic to the
actual tasks, to the problem semantic role labeling,
showing significant decrease in the number of infer-
ence calls without any loss in performance. While
the approach suggested in this paper is evaluated in
semantic role labeling, it is generally applicable to
any NLP task that deals with structured prediction.
Acknowledgements
The authors wish to thank Sariel Har-Peled and the members
of the Cognitive Computation Group at the University of Illi-
nois for insightful discussions and the anonymous reviewers for
their valuable feedback. This research is sponsored by the Army
Research Laboratory (ARL) under agreement W911NF-09-2-
0053. The authors also gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labo-
ratory (AFRL) prime contract no. FA8750-09-C-0181. This
work is also supported by the Intelligence Advanced Research
Projects Activity (IARPA) Foresight and Understanding from
Scientific Exposition (FUSE) Program via Department of In-
terior National Business Center contract number D11PC2015.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of ARL, DARPA, AFRL, IARPA,
or the US government.
References
Y-W. Chang and M. Collins. 2011. Exact decoding
of phrase-based translation models through lagrangian
relaxation. EMNLP.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
R. de Salvo Braz, E. Amir, and D. Roth. 2005. Lifted
first-order probabilistic inference. In IJCAI.
D Graff and C. Cieri. 2003. English gigaword.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In EMNLP, pages 523?530, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic Role
Labeling, volume 3. Morgan & Claypool Publishers.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
IJCAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
S. Riedel. 2009. Cutting Plane MAP Inference for
Markov Logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, CoNLL.
1123
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research.
1124
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 905?913,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Margin-based Decomposed Amortized Inference
Gourab Kundu? and Vivek Srikumar? and Dan Roth
University of Illinois, Urbana-Champaign
Urbana, IL. 61801
{kundu2, vsrikum2, danr}@illinois.edu
Abstract
Given that structured output prediction is
typically performed over entire datasets,
one natural question is whether it is pos-
sible to re-use computation from earlier
inference instances to speed up inference
for future instances. Amortized inference
has been proposed as a way to accomplish
this. In this paper, first, we introduce a new
amortized inference algorithm called the
Margin-based Amortized Inference, which
uses the notion of structured margin to
identify inference problems for which pre-
vious solutions are provably optimal. Sec-
ond, we introduce decomposed amortized
inference, which is designed to address
very large inference problems, where ear-
lier amortization methods become less ef-
fective. This approach works by decom-
posing the output structure and applying
amortization piece-wise, thus increasing
the chance that we can re-use previous so-
lutions for parts of the output structure.
These parts are then combined to a global
coherent solution using Lagrangian relax-
ation. In our experiments, using the NLP
tasks of semantic role labeling and entity-
relation extraction, we demonstrate that
with the margin-based algorithm, we need
to call the inference engine only for a third
of the test examples. Further, we show that
the decomposed variant of margin-based
amortized inference achieves a greater re-
duction in the number of inference calls.
1 Introduction
A wide variety of NLP problems can be natu-
rally cast as structured prediction problems. For
* These authors contributed equally to this work.
some structures like sequences or parse trees, spe-
cialized and tractable dynamic programming algo-
rithms have proven to be very effective. However,
as the structures under consideration become in-
creasingly complex, the computational problem of
predicting structures can become very expensive,
and in the worst case, intractable.
In this paper, we focus on an inference tech-
nique called amortized inference (Srikumar et al,
2012), where previous solutions to inference prob-
lems are used to speed up new instances. The
main observation that leads to amortized inference
is that, very often, for different examples of the
same size, the structures that maximize the score
are identical. If we can efficiently identify that two
inference problems have the same solution, then
we can re-use previously computed structures for
newer examples, thus giving us a speedup.
This paper has two contributions. First, we de-
scribe a novel algorithm for amortized inference
called margin-based amortization. This algorithm
is on an examination of the structured margin of
a prediction. For a new inference problem, if this
margin is larger than the sum of the decrease in the
score of the previous prediction and any increase
in the score of the second best one, then the previ-
ous solution will be the highest scoring one for the
new problem. We formalize this intuition to derive
an algorithm that finds provably optimal solutions
and show that this approach is a generalization of
previously identified schemes (based on Theorem
1 of (Srikumar et al, 2012)).
Second, we argue that the idea of amortization
is best exploited at the level of parts of the struc-
tures rather than the entire structure because we
expect a much higher redundancy in the parts.
We introduce the notion of decomposed amor-
tized inference, whereby we can attain a significant
improvement in speedup by considering repeated
sub-structures across the dataset and applying any
amortized inference algorithm for the parts.
905
We evaluate the two schemes and their combi-
nation on two NLP tasks where the output is en-
coded as a structure: PropBank semantic role la-
beling (Punyakanok et al, 2008) and the problem
of recognizing entities and relations in text (Roth
and Yih, 2007; Kate and Mooney, 2010). In these
problems, the inference problem has been framed
as an integer linear program (ILP). We compare
our methods with previous amortized inference
methods and show that margin-based amortization
combined with decomposition significantly out-
performs existing methods.
2 Problem Definition and Notation
Structured output prediction encompasses a wide
variety of NLP problems like part-of-speech tag-
ging, parsing and machine translation. The lan-
guage of 0-1 integer linear programs (ILP) pro-
vides a convenient analytical tool for representing
structured prediction problems. The general set-
ting consists of binary inference variables each of
which is associated with a score. The goal of in-
ference is to find the highest scoring global assign-
ment of the variables from a feasible set of assign-
ments, which is defined by linear inequalities.
While efficient inference algorithms exist for
special families of structures (like linear chains
and trees), in the general case, inference can be
computationally intractable. One approach to deal
with the computational complexity of inference
is to use an off-the-shelf ILP solver for solv-
ing the inference problem. This approach has
seen increasing use in the NLP community over
the last several years (for example, (Roth and
Yih, 2004; Clarke and Lapata, 2006; Riedel and
Clarke, 2006) and many others). Other approaches
for solving inference include the use of cutting
plane inference (Riedel, 2009), dual decomposi-
tion (Koo et al, 2010; Rush et al, 2010) and
the related method of Lagrangian relaxation (Rush
and Collins, 2011; Chang and Collins, 2011).
(Srikumar et al, 2012) introduced the notion of
an amortized inference algorithm, defined as an
inference algorithm that can use previous predic-
tions to speed up inference time, thereby giving an
amortized gain in inference time over the lifetime
of the program.
The motivation for amortized inference comes
from the observation that though the number of
possible structures could be large, in practice, only
a small number of these are ever seen in real
data. Furthermore, among the observed structures,
a small subset typically occurs much more fre-
quently than the others. Figure 1 illustrates this
observation in the context of part-of-speech tag-
ging. If we can efficiently characterize and iden-
tify inference instances that have the same solu-
tion, we can take advantage of previously per-
formed computation without paying the high com-
putational cost of inference.
Figure 1: Comparison of number of instances and the num-
ber of unique observed part-of-speech structures in the Gi-
gaword corpus. Note that the number of observed structures
(blue solid line) is much lower than the number of sentences
(red dotted line) for all sentence lengths, with the difference
being very pronounced for shorter sentences. Embedded in
the graph are three histograms that show the distribution of
observed structures for sentences of length 15, 20 and 30. In
all cases, we see that a small number of tag sequences are
much more frequent than the others.
We denote inference problems by the bold-
faced letters p and q. For a problem p, the goal
of inference is to jointly assign values to the parts
of the structure, which are represented by a col-
lection of inference variables y ? {0, 1}n. For all
vectors, subscripts represent their ith component.
Each yi is associated with a real valued cp,i ? <
which is the score for the variable yi being as-
signed the value 1. We denote the vector com-
prising of all the cp,i as cp. The search space
for assignments is restricted via constraints, which
can be written as a collection of linear inequalities,
MTy ? b. For a problem p, we denote this fea-
sible set of structures by Kp.
The inference problem is that of finding the fea-
sible assignment to the structure which maximizes
the dot product cTy. Thus, the prediction problem
can be written as
arg max
y?Kp
cTy. (1)
906
We denote the solution of this maximization prob-
lem as yp.
Let the set P = {p1,p2, ? ? ? } denote previously
solved inference problems, along with their re-
spective solutions {y1p,y2p, ? ? ? }. An equivalence
class of integer linear programs, denoted by [P ],
consists of ILPs which have the same number of
inference variables and the same feasible set. Let
K[P ] denote the feasible set of an equivalence class
[P ]. For a program p, the notation p ? [P ] indi-
cates that it belongs to the equivalence class [P ].
(Srikumar et al, 2012) introduced a set of amor-
tized inference schemes, each of which provides a
condition for a new ILP to have the same solu-
tion as a previously seen problem. We will briefly
review one exact inference scheme introduced in
that work. Suppose q belongs to the same equiv-
alence class of ILPs as p. Then the solution to q
will be the same as that of p if the following con-
dition holds for all inference variables:
(2yp,i ? 1)(cq,i ? cp,i) ? 0. (2)
This condition, referred to as Theorem 1 in that
work, is the baseline for our experiments.
In general, for any amortization scheme
A, we can define two primitive operators
TESTCONDITIONA and SOLUTIONA. Given
a collection of previously solved prob-
lems P and a new inference problem q,
TESTCONDITIONA(P,q) checks if the solu-
tion of the new problem is the same as that
of some previously solved one and if so,
SOLUTIONA(P,q) returns the solution.
3 Margin-based Amortization
In this section, we will introduce a new method
for amortizing inference costs over time. The key
observation that leads to this theorem stems from
the structured margin ? for an inference problem
p ? [P ], which is defined as follows:
? = min
y?K[P ],y 6=yp
cTp(yp ? y). (3)
That is, for all feasible y, we have cTpyp ? cTpy+
?. The margin ? is the upper limit on the change in
objective that is allowed for the constraint setK[P ]
for which the solution will not change.
For a new inference problem q ? [P ], we define
? as the maximum change in objective value that
can be effected by an assignment that is not the
A B = yp
cp
cq?
?
decrease in
value of yp
inc
rea
sin
go
bje
cti
ve
cpTyp
Two assignments
Figure 2: An illustration of the margin-based amortization
scheme showing the very simple case with only two compet-
ing assignments A and B. Suppose B is the solution yp for
the inference problem p with coefficients cp, denoted by the
red hyperplane, and A is the second-best assignment. For a
new coefficient vector cq, if the margin ? is greater than the
sum of the decrease in the objective value of yp and the max-
imum increase in the objective of another solution (?), then
the solution to the new inference problem will still be yp. The
margin-based amortization theorem captures this intuition.
solution. That is,
? = max
y?K[P ],y 6=yp
(cq ? cp)T y (4)
Before stating the theorem, we will provide an in-
tuitive explanation for it. Moving from cp to cq,
consider the sum of the decrease in the value of
the objective for the solution yp and ?, the maxi-
mum change in objective value for an assignment
that is not the solution. If this sum is less than the
margin ?, then no other solution will have an ob-
jective value higher than yp. Figure 2 illustrates
this using a simple example where there are only
two competing solutions.
This intuition is captured by our main theorem
which provides a condition for problems p and q
to have the same solution yp.
Theorem 1 (Margin-based Amortization). Let p
denote an inference problem posed as an inte-
ger linear program belonging to an equivalence
class [P ] with optimal solution yp. Let p have
a structured margin ?, i.e., for any y, we have
cTpyp ? cTpy + ?. Let q ? [P ] be another infer-
ence instance in the same equivalence class and
let ? be defined as in Equation 4. Then, yp is the
solution of the problem q if the following holds:
?(cq ? cp)Typ + ? ? ? (5)
907
Proof. For some feasible y, we have
cTqyp ? cTqy ? cTqyp ? cTpy ??
? cTqyp ? cTpyp + ? ??
? 0
The first inequality comes from the definition of ?
in (4) and the second one follows from the defini-
tion of ?. The condition of the theorem in (5) gives
us the final step. For any feasible y, the objective
score assigned to yp is greater than the score as-
signed to y according to problem q. That is, yp is
the solution to the new problem.
The margin-based amortization theorem pro-
vides a general, new amortized inference algo-
rithm. Given a new inference problem, we check
whether the inequality (5) holds for any previously
seen problems in the same equivalence class. If so,
we return the cached solution. If no such problem
exists, then we make a call to an ILP solver.
Even though the theorem provides a condition
for two integer linear programs to have the same
solution, checking the validity of the condition re-
quires the computation of ?, which in itself is an-
other integer linear program. To get around this,
we observe that if any constraints in Equation 4
are relaxed, the value of the resulting maximum
can only increase. Even with the increased ?, if
the condition of the theorem holds, then the rest
of the proof follows and hence the new problem
will have the same solution. In other words, we
can solve relaxed, tractable variants of the maxi-
mization in Equation 4 and still retain the guaran-
tees provided by the theorem. The tradeoff is that,
by doing so, the condition of the theorem will ap-
ply to fewer examples than theoretically possible.
In our experiments, we will define the relaxation
for each problem individually and even with the
relaxations, the inference algorithm based on the
margin-based amortization theorem outperforms
all previous amortized inference algorithms.
The condition in inequality (5) is, in fact, a strict
generalization of the condition for Theorem 1 in
(Srikumar et al, 2012), stated in (2). If the latter
condition holds, then we can show that ? ? 0 and
(cq ? cp)Typ ? 0. Since ? is, by definition, non-
negative, the margin-based condition is satisfied.
4 Decomposed Amortized Inference
One limitation in previously considered ap-
proaches for amortized inference stems from the
expectation that the same full assignment maxi-
mizes the objective score for different inference
problems, or equivalently, that the entire structure
is repeated multiple times. Even with this assump-
tion, we observe a speedup in prediction.
However, intuitively, even if entire structures
are not repeated, we expect parts of the assign-
ment to be the same across different instances. In
this section, we address the following question:
Can we take advantage of the redundancy in com-
ponents of structures to extend amortization tech-
niques to cases where the full structured output is
not repeated? By doing so, we can store partial
computation for future inference problems.
For example, consider the task of part of speech
tagging. While the likelihood of two long sen-
tences having the same part of speech tag sequence
is not high, it is much more likely that shorter sec-
tions of the sentences will share the same tag se-
quence. We see from Figure 1 that the number of
possible structures for shorter sentences is much
smaller than the number of sentences. This im-
plies that many shorter sentences share the same
structure, thus improving the performance of an
amortized inference scheme for such inputs. The
goal of decomposed amortized inference is to ex-
tend this improvement to larger problems by in-
creasing the size of equivalence classes.
To decompose an inference problem, we use the
approach of Lagrangian Relaxation (Lemare?chal,
2001) that has been used successfully for various
NLP tasks (Chang and Collins, 2011; Rush and
Collins, 2011). We will briefly review the under-
lying idea1. The goal is to solve an integer linear
program q, which is defined as
q : max
MTy?b
cTqy
We partition the constraints into two sets, say C1
denoting M1Ty ? b1 and C2, denoting con-
straints M2Ty ? b2. The assumption is that in
the absence the constraints C2, the inference prob-
lem becomes computationally easier to solve. In
other words, we can assume the existence of a sub-
routine that can efficiently compute the solution of
the relaxed problem q?:
q? : max
M1Ty?b1
cTqy
1For simplicity, we only write inequality constraints in
the paper. However, all the results here are easily extensible
to equality constraints by removing the non-negativity con-
straints from the corresponding dual variables.
908
We define Lagrange multipliers ? ? 0, with one
?i for each constraint in C2. For problem q, we
can define the Lagrangian as
L(y,?) = cTqy ? ?T
(
M2Ty ? b2
)
Here, the domain of y is specified by the constraint
set C1. The dual objective is
L(?) = max
M1Ty?b1
cTqy ? ?T
(
M2Ty ? b2
)
= max
M1Ty?b1
(
cq ? ?TM2
)T y + ?Tb2.
Note that the maximization in the definition of the
dual objective has the same functional form as q?
and any approach to solve q? can be used here to
find the dual objective L(?). The dual of the prob-
lem q, given by min??0 L(?), can be solved us-
ing subgradient descent over the dual variables.
Relaxing the constraints C2 to define the prob-
lem q? has several effects. First, it can make the re-
sulting inference problem q? easier to solve. More
importantly, removing constraints can also lead to
the merging of multiple equivalence classes, lead-
ing to fewer, more populous equivalence classes.
Finally, removing constraints can decompose the
inference problem q? into smaller independent
sub-problems {q1,q2, ? ? ? } such that no constraint
that is inC1 has active variables from two different
sets in the partition.
For the sub-problem qi comprising of variables
yi, let the corresponding objective coefficients be
cqi and the corresponding sub-matrix of M2 be
Mi2. Now, we can define the dual-augmented sub-
problem as
max
Mi1
Ty?bi1
(
cqi ? ?TMi2
)T
yi (6)
Solving all such sub-problems will give us a com-
plete assignment for all the output variables.
We can now define the decomposed amortized
inference algorithm (Algorithm 1) that performs
sub-gradient descent over the dual variables. The
input to the algorithm is a collection of previ-
ously solved problems with their solutions, a new
inference problem q and an amortized inference
scheme A (such as the margin-based amortization
scheme). In addition, for the task at hand, we first
need to identify the set of constraints C2 that can
be introduced via the Lagrangian.
First, we check if the solution can be obtained
without decomposition (lines 1?2). Otherwise,
Algorithm 1 Decomposed Amortized Inference
Input: A collection of previously solved infer-
ence problems P , a new problem q, an amor-
tized inference algorithm A.
Output: The solution to problem q
1: if TESTCONDITION(A, q, P ) then
2: return SOLUTION(A, q, P )
3: else
4: Initialize ?i ? 0 for each constraint in C2.
5: for t = 1 ? ? ?T do
6: Partition the problem q into sub-
problems q1,q2, ? ? ? such that no con-
straint in C1 has active variables from
two partitions.
7: for partition qi do
8: yi ? Solve the maximization prob-
lem for qi (Eq. 6) using the amortized
scheme A.
9: end for
10: Let y? [y1;y2; ? ? ? ]
11: if M2y ? b2 and (b2 ?M2y)i?i = 0
then
12: return y
13: else
14: ??
[
?? ?t
(
b2 ?M2Ty
)]
+
15: end if
16: end for
17: return solution of q using a standard infer-
ence algorithm
18: end if
we initialize the dual variables ? and try to ob-
tain the solution iteratively. At the tth itera-
tion, we partition the problem q into sub-problems
{q1,q2, ? ? ? } as described earlier (line 6). Each
partition defines a smaller inference problem with
its own objective coefficients and constraints. We
can apply the amortization scheme A to each sub-
problem to obtain a complete solution for the re-
laxed problem (lines 7?10). If this solution satis-
fies the constraints C2 and complementary slack-
ness conditions, then the solution is provably the
maximum of the problem q. Otherwise, we take a
subgradient step to update the value of ? using a
step-size ?t, subject to the constraint that all dual
variables must be non-negative (line 14). If we do
not converge to a solution in T iterations, we call
the underlying solver on the full problem.
In line 8 of the algorithm, we make multiple
calls to the underlying amortized inference pro-
cedure to solve each sub-problem. If the sub-
909
problem cannot be solved using the procedure,
then we can either solve the sub-problem using a
different approach (effectively giving us the stan-
dard Lagrangian relaxation algorithm for infer-
ence), or we can treat the full instance as a cache
miss and make a call to an ILP solver. In our ex-
periments, we choose the latter strategy.
5 Experiments and Results
Our experiments show two results: 1. The margin-
based scheme outperforms the amortized infer-
ence approaches from (Srikumar et al, 2012).
2. Decomposed amortized inference gives further
gains in terms of re-using previous solutions.
5.1 Tasks
We report the performance of inference on two
NLP tasks: semantic role labeling and the task of
extracting entities and relations from text. In both
cases, we used an existing formulation for struc-
tured inference and only modified the inference
calls. We will briefly describe the problems and
the implementation and point the reader to the lit-
erature for further details.
Semantic Role Labeling (SRL) Our first task is
that of identifying arguments of verbs in a sen-
tence and annotating them with semantic roles
(Gildea and Jurafsky, 2002; Palmer et al, 2010)
. For example, in the sentence Mrs. Haag plays
Eltiani., the verb plays takes two arguments: Mrs.
Haag, the actor, labeled as A0 and Eltiani, the
role, labeled as A1. It has been shown in prior
work (Punyakanok et al, 2008; Toutanova et al,
2008) that making a globally coherent prediction
boosts performance of SRL.
In this work, we used the SRL system of (Pun-
yakanok et al, 2008), where one inference prob-
lem is generated for each verb and each infer-
ence variables encodes the decision that a given
constituent in the sentence takes a specific role.
The scores for the inference variables are obtained
from a classifier trained on the PropBank cor-
pus. Constraints encode structural and linguistic
knowledge about the problem. For details about
the formulations of the inference problem, please
see (Punyakanok et al, 2008).
Recall from Section 3 that we need to define a
relaxed version of the inference problem to effi-
ciently compute ? for the margin-based approach.
For a problem instance with coefficients cq and
cached coefficients cp, we take the sum of the
highest n values of cq ? cp as our ?, where n is
the number of argument candidates to be labeled.
To identify constraints that can be relaxed for
the decomposed algorithm, we observe that most
constraints are not predicate specific and apply for
all predicates. The only constraint that is predi-
cate specific requires that each predicate can only
accept roles from a list of roles that is defined for
that predicate. By relaxing this constraint in the
decomposed algorithm, we effectively merge all
the equivalence classes for all predicates with a
specific number of argument candidates.
Entity-Relation extraction Our second task is
that of identifying the types of entities in a sen-
tence and the relations among them, which has
been studied by (Roth and Yih, 2007; Kate and
Mooney, 2010) and others. For the sentence
Oswald killed Kennedy, the words Oswald and
Kennedy will be labeled by the type PERSON, and
the KILL relation exists between them.
We followed the experimental setup as de-
scribed in (Roth and Yih, 2007). We defined one
inference problem for each sentence. For every
entity (which is identified by a constituent in the
sentence), an inference variable is introduced for
each entity type. For each pair of constituents, an
inference variable is introduced for each relation
type. Clearly, the assignment of types to entities
and relations are not independent. For example, an
entity of type ORGANIZATION cannot participate
in a relation of type BORN-IN because this rela-
tion label can connect entities of type PERSON and
LOCATION only. Incorporating these natural con-
straints during inference were shown to improve
performance significantly in (Roth and Yih, 2007).
We trained independent classifiers for entities and
relations and framed the inference problem as in
(Roth and Yih, 2007). For further details, we refer
the reader to that paper.
To compute the value of ? for the margin-based
algorithm, for a new instance with coefficients cq
and cached coefficients cp, we define ? to be the
sum of all non-negative values of cq ? cp.
For the decomposed inference algorithm, if the
number of entities is less than 5, no decomposi-
tion is performed. Otherwise, the entities are par-
titioned into two sets: set A includes the first four
entities and set B includes the rest of the entities.
We relaxed the relation constraints that go across
these two sets of entities to obtain two independent
inference problems.
910
5.2 Experimental Setup
We follow the experimental setup of (Srikumar et
al., 2012) and simulate a long-running NLP pro-
cess by caching problems and solutions from the
Gigaword corpus. We used a database engine to
cache ILP and their solutions along with identi-
fiers for the equivalence class and the value of ?.
For the margin-based algorithm and the Theo-
rem 1 from (Srikumar et al, 2012), for a new in-
ference problem p ? [P ], we retrieve all infer-
ence problems from the database that belong to
the same equivalence class [P ] as the test prob-
lem p and find the cached assignment y that has
the highest score according to the coefficients of
p. We only consider cached ILPs whose solution
is y for checking the conditions of the theorem.
This optimization ensures that we only process a
small number of cached coefficient vectors.
In a second efficiency optimization, we pruned
the database to remove redundant inference prob-
lems. A problem is redundant if solution to that
problem can be inferred from the other problems
stored in the database that have the same solution
and belong to the same equivalence class. How-
ever, this pruning can be computationally expen-
sive if the number of problems with the same so-
lution and the same equivalence class is very large.
In that case, we first sampled a 5000 problems ran-
domly and selected the non-redundant problems
from this set to keep in the database.
5.3 Results
We compare our approach to a state-of-the-art ILP
solver2 and also to Theorem 1 from (Srikumar
et al, 2012). We choose this baseline because
it is shown to give the highest improvement in
wall-clock time and also in terms of the num-
ber of cache hits. However, we note that the re-
sults presented in our work outperform all the pre-
vious amortization algorithms, including the ap-
proximate inference methods.
We report two performance metrics ? the per-
centage decrease in the number of ILP calls, and
the percentage decrease in the wall-clock infer-
ence time. These are comparable to the speedup
and clock speedup defined in (Srikumar et al,
2012). For measuring time, since other aspects
of prediction (like feature extraction) are the same
across all settings, we only measure the time taken
for inference and ignore other aspects. For both
2We used the Gurobi optimizer for our experiments.
tasks, we report the runtime performance on sec-
tion 23 of the Penn Treebank. Note that our amor-
tization schemes guarantee optimal solution. Con-
sequently, using amortization, task accuracy re-
mains the same as using the original solver.
Table 1 shows the percentage reduction in the
number of calls to the ILP solver. Note that for
both the SRL and entity-relation problems, the
margin-based approach, even without using de-
composition (the columns labeled Original), out-
performs the previous work. Applying the de-
composed inference algorithm improves both the
baseline and the margin-based approach. Overall,
however, the fewest number of calls to the solver is
made when combining the decomposed inference
algorithm with the margin-based scheme. For the
semantic role labeling task, we need to call the
solver only for one in six examples while for the
entity-relations task, only one in four examples re-
quire a solver call.
Table 2 shows the corresponding reduction in
the wall-clock time for the various settings. We
see that once again, the margin based approach
outperforms the baseline. While the decomposed
inference algorithm improves running time for
SRL, it leads to a slight increase for the entity-
relation problem. Since this increase occurs in
spite of a reduction in the number of solver calls,
we believe that this aspect can be further improved
with an efficient implementation of the decom-
posed inference algorithm.
6 Discussion
Lagrangian Relaxation in the literature In the
literature, in applications of the Lagrangian relax-
ation technique (such as (Rush and Collins, 2011;
Chang and Collins, 2011; Reichart and Barzilay,
2012) and others), the relaxed problems are solved
using specialized algorithms. However, in both the
relaxations considered in this paper, even the re-
laxed problems cannot be solved without an ILP
solver, and yet we can see improvements from de-
composition in Table 1.
To study the impact of amortization on running
time, we modified our decomposition based infer-
ence algorithm to solve each sub-problem using
the ILP solver instead of amortization. In these ex-
periments, we ran Lagrangian relaxation for until
convergence or at most T iterations. After T itera-
tions, we call the ILP solver and solve the original
problem. We set T to 100 in one set of exper-
911
% ILP Solver calls required
Method Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 ? 100 ?
(Srikumar et al, 2012) 41 24.4 59.5 57.0
Margin-based 32.7 16.6 28.2 25.4
Table 1: Reduction in number of inference calls
% time required compared to ILP Solver
Method Semantic Role Labeling Entity-Relation Extraction
Original + Decomp. Original + Decomp.
ILP Solver 100 ? 100 ?
(Srikumar et al, 2012) 54.8 40.0 81 86
Margin-based 45.9 38.1 58.1 61.3
Table 2: Reduction in inference time
iments (call it Lag1) and T to 1 (call it Lag2).
In SRL, compared to solving the original problem
with ILP Solver, both Lag1 and Lag2 are roughly
2 times slower. For entity relation task, compared
to ILP Solver, Lag1 is 186 times slower and Lag2
is 1.91 times slower. Since we used the same im-
plementation of the decomposition in all experi-
ments, this shows that the decomposed inference
algorithm crucially benefits from the underlying
amortization scheme.
Decomposed amortized inference The decom-
posed amortized inference algorithm helps im-
prove amortized inference in two ways. First,
since the number of structures is a function of its
size, considering smaller sub-structures will allow
us to cache inference problems that cover a larger
subset of the space of possible sub-structures. We
observed this effect in the problem of extracting
entities and relations in text. Second, removing a
constraint need not always partition the structure
into a set of smaller structures. Instead, by re-
moving the constraint, examples that might have
otherwise been in different equivalence classes be-
come part of a combined, larger equivalence class.
Increasing the size of the equivalence classes in-
creases the probability of a cache-hit. In our ex-
periments, we observed this effect in the SRL task.
7 Conclusion
Amortized inference takes advantage of the reg-
ularities in structured output to re-use previous
computation and improve running time over the
lifetime of a structured output predictor. In this pa-
per, we have described two approaches for amor-
tizing inference costs over datasets. The first,
called the margin-based amortized inference, is a
new, provably exact inference algorithm that uses
the notion of a structured margin to identify previ-
ously solved problems whose solutions can be re-
used. The second, called decomposed amortized
inference, is a meta-algorithm over any amortized
inference that takes advantage of previously com-
puted sub-structures to provide further reductions
in the number of inference calls. We show via ex-
periments that these methods individually give a
reduction in the number of calls made to an infer-
ence engine for semantic role labeling and entity-
relation extraction. Furthermore, these approaches
complement each other and, together give an addi-
tional significant improvement.
Acknowledgments
The authors thank the members of the Cognitive Computa-
tion Group at the University of Illinois for insightful discus-
sions and the anonymous reviewers for valuable feedback.
This research is sponsored by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053. The authors
also gratefully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. This material also
is based on research sponsored by DARPA under agreement
number FA8750-13-2-0008. This work has also been sup-
ported by the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior National Business
Center contract number D11PC20155. The U.S. Govern-
ment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright an-
notation thereon. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the view of ARL,
DARPA, AFRL, IARPA, DoI/NBC or the US government.
912
References
Y-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. EMNLP.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
R. Kate and R. Mooney. 2010. Joint entity and relation
extraction using card-pyramid parsing. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 203?212. Asso-
ciation for Computational Linguistics.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In EMNLP.
C. Lemare?chal. 2001. Lagrangian Relaxation. In
Computational Combinatorial Optimization, pages
112?156.
M. Palmer, D. Gildea, and N. Xue. 2010. Semantic
Role Labeling, volume 3. Morgan & Claypool Pub-
lishers.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics.
R. Reichart and R. Barzilay. 2012. Multi event extrac-
tion guided by global constraints. In NAACL, pages
70?79.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In EMNLP.
S. Riedel. 2009. Cutting plane MAP inference for
Markov logic. Machine Learning.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Hwee Tou Ng and Ellen Riloff, editors,
CoNLL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
A.M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through Lagrangian re-
laxation. In ACL, pages 72?82, Portland, Oregon,
USA, June.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
V. Srikumar, G. Kundu, and D. Roth. 2012. On amor-
tizing inference cost for structured prediction. In
EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008.
A global joint model for semantic role labeling.
Computational Linguistics, 34:161?191.
913
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 229?237,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Adapting Text instead of the Model: An Open Domain Approach
Gourab Kundu, Dan Roth
University of Illinois at Urbana Champaign
Urbana, IL 61801
{kundu2,danr}@illinois.edu
Abstract
Natural language systems trained on labeled
data from one domain do not perform well
on other domains. Most adaptation algorithms
proposed in the literature train a new model for
the new domain using unlabeled data. How-
ever, it is time consuming to retrain big mod-
els or pipeline systems. Moreover, the domain
of a new target sentence may not be known,
and one may not have significant amount of
unlabeled data for every new domain.
To pursue the goal of an Open Domain NLP
(train once, test anywhere), we propose ADUT
(ADaptation Using label-preserving Transfor-
mation), an approach that avoids the need for
retraining and does not require knowledge of
the new domain, or any data from it. Our ap-
proach applies simple label-preserving trans-
formations to the target text so that the trans-
formed text is more similar to the training do-
main; it then applies the existing model on
the transformed sentences and combines the
predictions to produce the desired prediction
on the target text. We instantiate ADUT for
the case of Semantic Role Labeling (SRL)
and show that it compares favorably with ap-
proaches that retrain their model on the target
domain. Specifically, this ?on the fly? adapta-
tion approach yields 13% error reduction for
a single parse system when adapting from the
news wire text to fiction.
1 Introduction
In several NLP tasks, systems trained on annotated
data from one domain perform well when tested
on the same domain but adapt poorly to other do-
mains. For example, all systems of CoNLL 2005
shared task (Carreras and M?rquez, 2005) on Se-
mantic Role Labeling showed a performance degra-
dation of almost 10% or more when tested on a dif-
ferent domain.
Most works in domain adaptation have focused
on learning a common representation across train-
ing and test domains (Blitzer et al, 2006; Daum?III,
2007; Huang and Yates, 2009). Using this represen-
tation, they retrain the model for every new domain.
But these are not Open Domain Systems since the
model needs to be retrained for every new domain.
This is very difficult for pipeline systems like SRL
where syntactic parser, shallow parser, POS tagger
and then SRL need to be retrained. Moreover, these
methods need to have a lot of unlabeled data that is
taken from the same domain, in order to learn mean-
ingful feature correspondences across training and
test domain. These approaches cannot work when
they do not have a lot of unlabeled data from the test
domain or when the test domain in itself is very di-
verse, e.g., the web.
The contribution of this paper is a new frame-
work for adaptation. We propose ADUT (ADap-
tation Using label-preserving Transformation) as a
framework in which a previously learned model can
be used on an out-of-domain example without re-
training and without looking at any labeled or unla-
beled data for the domain of the new example. The
framework transforms the test sentence to generate
sentences that have, in principle, identical labeling
but that are more like instances from the training do-
main. Consequently, it is expected that the exist-
229
ing model will make better predictions on them. All
these predictions are then combined to choose the
most probable and consistent prediction for the test
sentence.
ADUT is a general technique which can be ap-
plied to any natural language task. In this paper, we
demonstrate its usefulness on the task of semantic
role labeling (Carreras and M?rquez, 2005). Start-
ing with a system that was trained on the news text
and does not perform well on fiction, we show that
ADUT provides significant improvement on fiction,
and is competitive with the performance of algo-
rithms that were re-trained on the test domain.
The paper is organized as follows. Section 2 dis-
cusses two motivating examples. Section 3 gives a
formal definition of our adaptation framework. Sec-
tion 4 describes the transformation operators that we
applied for this task. Section 5 presents our joint in-
ference approach. Section 6 describes our semantic
role labeling system and our experimental results are
in Section 7. Section 8 describes the related works
for domain adaptation. Finally in Section 9 we con-
clude the paper with a discussion.
2 Motivating Examples
One of the key reasons for performance degradation
of an NLP tool is unseen features such as words in
the new domain that were not seen in the training
domain. But if an unknown word is replaced by a
known word without changing the labeling of the
sentence, tools perform better. For example, in the
task of syntactic parsing, the unknown word checkup
causes the Charniak parser to make a wrong co-
ordination decision on the sentence
He was discharged from the hospital af-
ter a two-day checkup and he and his par-
ents had what Mr. Mckinley described as
a ?celebration lunch? at the cafeteria on
the campus.
If we replace the word checkup with its hyper-
nym examination which appears in training data, the
parse gets corrected. Figure 1 shows both original
and corrected parse trees.
For the task of semantic role labeling, systems do
not perform well on the predicates that are infre-
quent in training domain. But if an infrequent predi-
cate is replaced with a frequent predicate from train-
ing domain such that both predicates have similar
semantic argument structure, the system performs
better. Consider the following sentence
Scotty gazed out at ugly gray slums.
The semantic role for the phrase at ugly gray slums
with respect to predicate gaze is A1. But the pred-
icate gaze appears only once in training data and
our model predicts at ugly gray slums as AM-LOC
instead of A1. But if gaze is replaced with look
which occurs 328 times in training data and has sim-
ilar argument structure (in the same VerbNet class as
gaze), the system makes the correct prediction.
3 Problem Formulation
Let the in-domain distribution be Di and out-of-
domain distribution be Do. We have a model f
trained over a set of labeled examples drawn from
Di. If Di and Do are very dissimilar, f will not per-
form well on examples drawn from Do. The prob-
lem is to get good performance from f on Do with-
out retraining f .
We define a Transformation g to be a function that
maps an example e into a set of examples E. So g :
X ? 2X where X is the entire space of examples.
In this paper, we only consider the Label-preserving
Transformations which satisfy the property that all
transformed examples in E have the same label as
input example e, i.e., ?x x ? Sk ? g(x) ? Sk
where Sk is the set of examples with label k . Let
G be a set of label-preserving transformation func-
tions. G = {g1, g2, . . ., gp}.
At evaluation time, for test example d, we will
apply G to get a set of examples T1. Let T2 = {d? ?
T1 : Di(d?) > Di(d)}. So all examples in T2 have
same label as d but have a higher probability than
d to be drawn from the in-domain distribution. So
f should perform better on examples in T2 than on
d. For each d? ? T2, f will produce scores for the
output labels. The scores will be combined subject
to constraints to produce the final output.
4 Transformation Functions
After applying a transformation function to get a
new sentence from an input sentence, we remem-
ber the mapping of segments across the original
230
a. S1
S
NP
He
VP
was VP
discharged PP
from the hospital
PP
after NP
NP
a two-day
checkup
SBAR
NP
and he and his parents
VP
had . . . campus
.
b. S1
S
S
NP
He
VP
was VP
discharged PP
from the hospital
PP
after NP
a two-day
examination
and S
NP
he and his parents
VP
had . . . campus
.
Figure 1: a. Original Parse tree b. Corrected Parse tree after replacement of unknown word checkup by examination
and transformed sentence. Thus, after annotating
the transformed sentence with SRL, we can transfer
the roles to the original sentence through this map-
ping. Transformation functions can be divided into
two categories. The first category is Transforma-
tions From List which uses external resources like
WordNet, VerbNet and Word Clusters. The second
is Learned Transformations that uses transformation
rules that have been learned from training data.
4.1 Transformation From List
I. Replacement of Predicate:
As noted in (Huang and Yates, 2010), 6.1% of the
predicates in the Brown test set do not appear in WSJ
training set and 11.8% appear at most twice. Since
the semantic roles of a sentence depend on the pred-
icate, these infrequent predicates hurt SRL perfor-
mance on new domains. Note that since all predi-
cates in PropBank are verbs, we will use the words
predicate and verb interchangeably.
We count the frequency of each predicate and its
accuracy in terms of F1 score over the training data.
If the frequency or the F1 score of the predicate in
the test sentence is below a threshold, we perturb
that predicate. We take all the verbs in the same class
of VerbNet1 as the original verb (in case the verb is
present in multiple classes, we take all the classes).
In case the verb is not present in VerbNet, we take
its synonyms from WordNet. If there is no synonym
in WordNet, we take the hypernyms.
From this collection of new verbs, we select verbs
that have a high accuracy and a high frequency in
1
http://verbs.colorado.edu/ mpalmer/projects/verbnet.html
training. We replace the original verb with each of
these new verbs and generate one new sentence for
each new verb; the sentence is retained if the parse
score for the new sentence is higher than the parse
score for the original sentence.2 VerbNet has de-
fined a set of verb-independent thematic roles and
grouped the verbs according to their usage in frames
with identical thematic roles. But PropBank anno-
tation was with respect to each verb. So the same
thematic role is labeled as different roles for dif-
ferent verbs in PropBank. For example, both warn
and advise belong to the same VerbNet class (37.9)
and take thematic roles of Recipient (person being
warned or advised) and Topic (topic of warning or
advising). But Recipient was marked as A2 for warn
and A1 for advise and Topic was marked as A1 for
warn and A2 for advise in PropBank annotation.
Semlink3 provides a mapping from the thematic role
to PropBank role for each verb. After the SRL anno-
tates the new sentence with PropBank roles for the
new verb, we map the PropBank roles of the new
verb to their corresponding thematic roles and then
map the thematic roles to the corresponding Prop-
Bank roles for the original verb.
II. Replacement and Removal of Quoted Strings:
Quoted sentences can vary a lot from one domain
to another. For example, in WSJ, quoted sentences
are like formal statements but in Brown, these are
like informal conversations. We generate the trans-
formations in the following ways:
1) We use the content of the quoted string as one
2
Parse score is the parse probability returned by Charniak or Stanford parser.
3
http://verbs.colorado.edu/semlink/
231
sentence. 2) We replace each quoted string in turn
with a simple sentence (This is good) to generate a
new sentence. 3) If a sentence has a quoted string in
the beginning, we move that quoted string after the
first NP and VP that immediately follow the quoted
string. For example, from the input sentence, ?We
just sit quiet?, he said. we generate the sentences 1)
We just sit quiet 2) ?This is good?, he said. 3) He
said, ?We just sit quiet?.
III. Replacement of Unseen Words:
A major difficulty for domain adaptation is that
some words in the new domain do not appear in the
training domain. In the Brown test set, 5% of total
words were never seen in the WSJ training set.
Given an unseen word which is not a verb, we
replace it with WordNet synonyms and hypernyms
that were seen in the training data. We used the
clusters obtained in (Liang, 2005) from running the
Brown algorithm (Brown et al, 1992) on Reuters
1996 dataset. But since this cluster was generated
automatically, it is noisy. So we chose replacements
from the Brown clusters selectively. We only replace
those words for which the POS tagger and the syn-
tactic parser predicted different tags. For each such
word, we find its cluster and select the set of words
from the cluster. We delete from this set al words
that do not take at least one part-of-speech tag that
the original word can take (from WordNet). For each
candidate synonym or hypernym or cluster member,
we get a new sentence. Finally we only keep those
sentences that have higher parse scores than the orig-
inal sentence.
IV. Sentence Split based on Stop Symbols:
We split each sentence based on stop symbols like
; and . . Each of the splitted sentences becomes one
transformation of the original sentence.
V. Sentence Simplification:
We have a set of heuristics for simplifying the
constituents of the parse tree; for example, replac-
ing an NP with its first and last word, removal of
PRN phrases etc. We apply these heuristics and gen-
erate simpler sentences until no more simplification
is possible. Examples of our heuristics are given in
Table 1.
Note that we can use composition of multiple
transformation functions as one function. A compo-
sition p1  p2(s) = ?a?p1(s)p2(a). We apply III,
IIII, IVI and VI.
Node Input Example Simplified Example Operation
NP He and she ran. He ran. replace
NP The big man ran. The man ran. replace
ADVP He ran fast. He ran. delete
PP He ran in the field. He ran. delete
PRN He ? though sick ? ran. He ran. delete
VP He walked and ran. He ran. delete
TO I want him to run. I want that he can ran. rewrite
Table 1: Examples of Simplifications (Predicate is run)
4.2 Learned Transformations
The learned model is inaccurate over verbs and roles
that are infrequent in the training data. The purpose
of the learned transformation is to transfer such a
phrase in the test sentence in place of a phrase of a
simpler sentence; this is done such that there exists
a mapping from the role of the phrase in the new
sentence to the role of the phrase in the original sen-
tence.
Phrase Representation: A phrase tuple is a 3-
tuple (t, i, h) where, t is the phrase type, i is the in-
dex, and h is the headword of the phrase. We denote
by PR the Phrase Representation of a sentence ? an
ordered list of phrase tuples. A phrase tuple corre-
sponds to a node in the tree. We only consider phrase
tuples that correspond to nodes that are (1) a sibling
of the predicate node or (2) a sibling of an ancestor
of the predicate node. Phrase tuples inPR are sorted
based on their position in the sentence. The index i
of the phrase tuple containing the predicate is taken
to be zero with the indices of the phrase tuples on
the left (right) sequentially decreasing (increasing).
Transformation Rule: We denote by Label(n, s)
the semantic role of nth phrase in the PR of the
sentence s. Let Replace(ns, nt, ss, st) be a new
sentence that results from inserting the phrase ns in
sentence ss instead of phrase nt in sentence st. We
will refer to st as target sentence and to nt as the
target phrase. Let sp be a sequence of phrase tuples
named as source pattern. If Label(ns, ss) = r1 and
Label(nt, Replace(ns, nt, ss, st)) = r2, then denote
f(r2) = r1. In this case we call the 6-tuple (st, nt,
p, sp, ns, f ) a transformation rule. We call f the
232
label correspondence function.
Example: Consider the sentence st = ?But it did
not sing." and the rule ? : (st, nt, p, sp, ns, f). Let:
nt = ?3, p = entitle,
sp = [?2, NP, ?][?1, AUX, ?][0, V, entitle][1, ?, to]
ns = ?2, f = {<A0, A2>} ? {<Ai,Ai>|i 6= 0}.
The PR of ?.st is {[?4, CC, But] [?3, NP, it]
[?2, AUX, did] [?1, RB, not] [0, VB, sing] [1, ., .]}.
Consider the input sentence ss: Mr. X was entitled
to a discount . with PR of {[?2, NP, X] [?1, AUX,
was] [0, V, entitle] [1, PP, to][2, ., .]}. Since ?.sp is
a subsequence of the PR of ss, ? will apply to the
predicate entitle of ss. The transformed sentence is:
str = Replace(?.ns, ?.nt, ss, ?.st) = But Mr. X
did not sing. with PR of {[?4, CC, But] [?3, NP,
X] [?2, AUX, did] [?1, RB, not] [0, VB, sing] [1,
., .]}. If the SRL system assigns the semantic role
of A0 to the phrase Mr. X of str, the semantic role
of Mr. X in ss can be recovered through ?.f since
?.f(A0) = A2 = Label(?2, ss).
While checking if ?.sp is a subsequence of the
PR of the input sentence, ? in each tuple of ?.sp
has to be considered a trivial match. So ? will
match the sentence He is entitled to a reward. with
PR = {[?2, NP, He] [?1, AUX, is] [0, V, entitle]
[1, PP, to][2, ., .]} but will not match the sentence
The conference was entitled a big success. with
PR = {[?2, NP, conference] [?1, AUX, was] [0,
V, entitle] [1, S, success][2, ., .]} (mismatch position
is bolded). The index of a phrase tuple cannot be ?,
only the head word or type can be ? and the rules
with more ? strings in the source pattern are more
general since they can match more sentences.
Algorithm 1 GenerateRules
1: Input: predicate v, semantic role r, Training sentences D, SRL
Model M
2: Output: set of rules R
3: R? GetInitialRules(v, r,D,M)
4: repeat
5: J ? ExpandRules(R)
6: K ? R ? J
7: sort K based on accuracy, support, size of source pattern
8: select some rules R ? K based on database coverage
9: until all rules in R have been expanded before
10: return R
The algorithm for finding rules for a semantic role
r of a predicate v is given in Algorithm 1. It is a
specific to general beam search procedure that starts
with a set of initial rules (Line 3, detail in Algorithm
2) and finds new rules from these rules (Line 5, de-
tail in Algorithm 3). In Line 7, the rules are sorted
by decreasing order of accuracy, support and number
of ? strings in the source pattern. In Line 8, a set of
rules are selected to cover all occurrences of the se-
mantic role r with the predicate v a specific number
of times. This process continues until no new rules
are found. Note that these rules need to be learned
only once and can be used for every new domain.
Algorithm 2 GetInitialRules
1: Input: predicate v, semantic role r, Training sentences D, SRL-
Model M
2: Output: Set of initial rules I
3: I ? ?
4: T ? {s ? D : length(s) <= e}
5: S ? {s ? D : s has role r for predicate v}
6: M ? Set of all semantic roles
7: for each phrase p1 in s1 ? S with gold label r for predicate v do
8: for each phrase p2 in s2 ? T labeled as a core argument do
9: if s1 6= s2 and p1 and p2 have same phrase types then
10: ? ? empty rule
11: ?.st ? s2, ?.p? v
12: ?.nt ? index of p2 in PR of s2
13: ?.ns ? index of p1 in PR of s1
14: ?.sp ? phrase tuples for phrases from p1 to v and two
phrases after v in PR of s1
15: L? ?
16: for each sentence s3 ?D with predicate v do
17: if ?.sp is a subsequence of PR of s3 then
18: x? replace(?.ns, ?.nt, s3, ?.st)
19: annotate x with SRL using M
20: r1 ? the gold standard semantic role of the
phrase with index ?.ns in PR of s3
21: r2 ? Label(?.nt, x)
22: if r2 /? L then
23: insert(r2, r1) in ?.f
24: L = L ? {r2}
25: end if
26: end if
27: end for
28: for each role j ?M ? L do
29: insert(j, j) in ?.f
30: end for
31: I ? I? {?}
32: end if
33: end for
34: end for
35: return I
The algorithm for generating initial rules for the
semantic role r of predicate v is given in Algorithm
2. Shorter sentences are preferred to be target sen-
tences(Line 4). A rule ? is created for every (p1,p2)
pair where p1, p2 are phrases, p1 has the semantic
role r in some sentence s1, p2 is labeled as a core
argument(A0 ? A5) in some sentence in T and the
phrase types of p1 and p2 in their respective parse
trees are same(Lines 7 ? 9). Every sentence s3 in
233
training corpus with predicate ?.p is a potential can-
didate for applying ? (Line 16) if ?.sp is a subse-
quence ofPR of s3(Line 17). After applying ? to s3,
a transformed sentence x is created(Line 18). Lines
20 ? 26 find the semantic role r2 of the transferred
phrase from SRL annotation of x using model M
and create a mapping from r2 to the gold standard
role r1 of the phrase in s3. L maintains the set of se-
mantic roles for which mappings have been created.
In lines 28 ? 30, all unmapped roles are mapped to
themselves.
The algorithm for creating new rules from a set
of existing rules is given in Algorithm 3. Lines 4 ?
13 generate all immediate more general neighbors of
the current rule by nullifying the headword or phrase
type element in any of the phrase tuples in its source
pattern.
Algorithm 3 ExpandRules
1: Input: a set of rules R
2: Output: a set of expanded rules E
3: E ? ?
4: for each phrase tuple c in the source pattern of r ? R do
5: if c is not the tuple for predicate then
6: create a new rule r? with all components of r
7: mark the head word of c in the source pattern of r? to ?
8: add r? to E
9: create a new rule r?? with all components of r
10: mark the phrase type of c in the source pattern of r?? to ?
11: add r?? to E
12: end if
13: end for
14: return E
5 Combination by Joint Inference
The transformation functions transform an input
sentence into a set of sentences T . From each trans-
formed sentence ti, we get a set of argument can-
didates Si. Let S =
?|T |
i=1 Si be the set of all ar-
guments. Argument classifier assigns scores for
each argument over the output labels(roles) in S
that is then converted into a probability distribu-
tion over the possible labels using the softmax func-
tion (Bishop, 1995). Note that multiple arguments
with the same span can be generated from multiple
transformed sentences.
First, we take all arguments from S with distinct
span and put them in S?. For each argument arg in
S?, we calculate scores over possible labels as the
sum over the probability distribution (over output la-
bels) of all arguments in S that have the same span
as arg divided by the number of sentences in T that
contained arg. This results in a set of arguments with
distinct spans and for each argument, a set of scores
over possible labels. Following the joint inference
procedure in (Punyakanok et al, 2008), we want to
select a label for each argument such that the total
score is maximized subject to some constraints. Let
us index the set S? as S?1:M where M = |S?|. Also
assume that each argument can take a label from a
set P . The set of arguments in S?1:M can take a set
of labels c1:M ? P 1:M . Given some constraints, the
resulting solution space is limited to a feasible set F;
the inference task is: c1:M = arg maxc1:M?F (P 1:M )
?M
i=1 score(S
?i = ci).
The constraints used are: 1) No overlapping or
embedding argument. 2) No duplicate argument for
core arguments A0-A5 and AA. 3) For C-arg, there
has to be an arg argument.
6 Experimental Setup
In this section, we discuss our experimental setup
for the semantic role labeling system. Similar to the
CoNLL 2005 shared tasks, we train our system using
sections 02-21 of the Wall Street Journal portion of
Penn TreeBank labeled with PropBank. We test our
system on an annotated Brown corpus consisting of
three sections (ck01 - ck03).
Since we need to annotate new sentences with
syntactic parse, POS tags and shallow parses, we do
not use annotations in the CoNLL distribution; in-
stead, we re-annotate the data using publicly avail-
able part of speech tagger and shallow parser1, Char-
niak 2005 parser (Charniak and Johnson, 2005) and
Stanford parser (Klein and Manning, 2003).
Our baseline SRL model is an implementation of
(Punyakanok et al, 2008) which was the top per-
forming system in CoNLL 2005 shared task. Due to
space constraints, we omit the details of the system
and refer readers to (Punyakanok et al, 2008).
7 Results
Results for ADUT using only the top parse of Char-
niak and Stanford are shown in Table 2. The Base-
line model using top Charniak parse (BaseLine-
Charniak) and top Stanford parse (BaseLine-
Stanford) score respectively 76.4 and 73.3 on the
1
http://cogcomp.cs.illinois.edu/page/software
234
WSJ test set. Since we are interested in adaptation,
we report and compare results for Brown test set
only. On this set, both ADUT-Charniak and ADUT-
Stanford significantly outperform their respective
baselines. We compare with the state-of-the-art sys-
tem of (Surdeanu et al, 2007). In (Surdeanu et
al., 2007), the authors use three models: Model
1 and 2 do sequential tagging of chunks obtained
from shallow parse and full parse. Model 3 assumes
each predicate argument maps to one syntactic con-
stituent and classifies it individually. So Model 3
matches our baseline model. ADUT-Charniak out-
performs the best individual model (Model 2) of
(Surdeanu et al, 2007) by 1.6% and Model 3 by
3.9%. We also tested another system that used clus-
ter features and word embedding features computed
following (Collobert and Weston, 2008). But we
did not see any performance improvement on Brown
over baseline.
System P R F1
BaseLine-Charniak 69.6 61.8 65.5
ADUT-Charniak 72.75 66.1 69.3
BaseLine-Stanford 70.8 56.5 62.9
ADUT-Stanford 72.5 60.0 65.7
(Surdeanu et al, 2007)(Model 2) 71.8 64.0 67.7
(Surdeanu et al, 2007)(Model 3) 72.4 59.7 65.4
Table 2: Comparing single parse system on Brown.
All state-of-the-art systems for SRL are a com-
bination of multiple systems. So we combined
ADUT-Stanford, ADUT-Charniak and another sys-
tem ADUT-Charniak-2 based on 2nd best Charniak
parse using joint inference. In Table 3, We com-
pare with (Punyakanok et al, 2008) which was the
top performing system in CoNLL 2005 shared task.
We also compare with the multi parse system of
(Toutanova et al, 2008) which uses a global joint
model using multiple parse trees. In (Surdeanu et al,
2007), the authors experimented with several com-
bination strategies. Their first combination strategy
was similar to ours where they directly combined the
outputs of different systems using constraints (de-
noted as Cons in Table 3). But their best result on
Brown set was obtained by treating the combina-
tion of multiple systems as a meta-learning problem.
They trained a new model to score candidate argu-
ments produced by individual systems before com-
bining them through constraints (denoted as LBI in
Table 3). We also compare with (Huang and Yates,
2010) where the authors retrained a SRL model us-
ing HMM features learned over unlabeled data of
WSJ and Brown.
System P R F1 Retrain
(Punyakanok et al, 2008) 73.4 62.9 67.8 ?
(Toutanova et al, 2008) NR NR 68.8 ?
(Surdeanu et al, 2007) (Cons) 78.2 62.1 69.2 ?
(Surdeanu et al, 2007) (LBI) 81.8 61.3 70.1 ?
ADUT-combined 74.3 67.0 70.5 ?
(Huang and Yates, 2010) 77.0 70.9 73.8 X
Table 3: Comparison of the multi parse system on Brown.
Table 3 shows that ADUT-Combined performs
better than (Surdeanu et al, 2007) (Cons) when in-
dividual systems have been combined similarly. We
believe that the techniques in (Surdeanu et al, 2007)
of using multiple models of different kinds (two
based on sequential tagging of chunks to capture ar-
guments whose boundaries do not match a syntac-
tic constituent) and training an additional model to
combine the outputs of individual systems are or-
thogonal to the performance improvement that we
have and applying these methods will further in-
crease the performance of our final system which is
a research direction we want to pursue in future.
We did an ablation study to determine which
transformations help and by how much. Table 4
presents results when only one transformation is ac-
tive at a time. We see that each transformation im-
proves over the baseline.
The effect of the transformation of Replacement
of Predicate on infrequent verbs is shown in Table
5. This transformation improves F1 as much as 6%
on infrequent verbs.
The running time for ADUT-Charniak on Brown
set is 8 hours compared to SRL training time of 20
hours. Average number of transformed sentences
generated by ADUT-Charniak for every sentence
from Brown is 36. The times are calculated based
on a machine with 2x 6-Core Xeon X5650 Proces-
sor with 48G memory.
235
Transformation P R F1
Baseline 69.6 61.8 65.5
Replacement of Unknown Words 70.6 62.1 66.1
Replacement of Predicate 71.2 62.8 66.8
Replacement of Quotes 71.0 63.4 67.0
Simplification 70.3 62.9 66.4
RuleTransformation 70.9 62.2 66.2
Sentence Split 70.8 62.1 66.2
Together 72.75 66.1 69.3
Table 4: Ablation Study for ADUT-Charniak
Frequency Baseline Replacement of Predicate
0 64.2 67.8
less than 3 59.7 65.1
less than 7 58.9 64.8
all predicates 65.5 66.78
Table 5: Performance on Infrequent Verbs for the Trans-
formation of Replacement of Predicate
8 Related Work
Traditional adaptation techniques like (Daum?III,
2007; Chelba and Acero, 2004; Finkel and Man-
ning, 2009; Jiang and Zhai, 2007; Blitzer et al,
2006; Huang and Yates, 2009; Ando and Zhang,
2005; Ming-wei Chang and Roth, 2010) need to re-
train the model for every new domain. In (Umansky-
Pesin et al, 2010), there was no retraining; instead,
a POS tag was predicted for every unknown word
in the new domain by considering contexts of that
word collected by web search queries. We differ
from them in that our transformations are label-
preserving; moreover, our transformations aim at
making the target text resemble the training text.
We also present an algorithm to learn transformation
rules from training data. Our application domain,
SRL, is also more complex and structured than POS
tagging.
In (McClosky et al, 2010), the task of multiple
source parser adaptation was introduced. The au-
thors trained parsing models on corpora from dif-
ferent domains and given a new text, used a linear
combination of trained models. Their approach re-
quires annotated data from multiple domains as well
as unlabeled data for the new domain, which is not
needed in our framework. In (Huang and Yates,
2010), the authors trained a HMM over the Brown
test set and the WSJ unlabeled data. They derived
features from Viterbi optimal states of single words
and spans of words and retrained their models us-
ing these features. In (Vickrey and Koller, 2008),
a large number of hand-written rules were used to
simplify the parse trees and reduce syntactic vari-
ation to overcome feature sparsity. We have sev-
eral types of transformations, and use less than 10
simplification heuristics, based on replacing larger
phrases with smaller phrases and deleting unneces-
sary parse tree nodes. There are also some methods
for unsupervised semantic role labeling (Swier and
Stevenson, 2004), (Abend et al, 2009) that easily
adapt across domains but their performances are not
comparable to supervised systems.
9 Conclusion
We presented a framework for adaptating natural
language text so that models can be used across do-
mains without modification. Our framework sup-
ports adapting to new domains without any data or
knowledge of the target domain. We showed that our
approach significantly improves SRL performance
over the state-of-the-art single parse based system
on Brown set. In the future, we would like to extend
this approach to other NLP problems and study how
combining multiple systems can further improve its
performance and robustness.
Acknowledgements This research is sponsored
by the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053 and by the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, conclusions or rec-
ommendations are those of the authors and do not
necessarily reflect the view of the ARL, the DARPA,
AFRL, or the US government.
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling . In Proceedings of the ACL.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple labeled
236
and unlabeled data . Journal of Machine Learning Re-
search.
Christopher Bishop. 1995. Neural Networks for Pattern
recognition, chapter 6.4: Modelling conditional distri-
butions. Oxford University Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. D. Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational
Linguistics, 18(4):467?479.
Xavier Carreras and Llu?s M?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling .
In Proceedings of CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL.
Ciprian Chelba and Alex Acero. 2004. Little data
can help a lot. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of ICML.
Hal Daum?III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the the Annual Meeting of the
Association of Computational Linguistics (ACL).
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical bayesian domain adaptation . In Proceed-
ings of NAACL.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling . In Proceedings of ACL.
Fei Huang and Alexander Yates. 2010. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Proceedings of NIPS.
Percy Liang. 2005. Semi-supervised learning for natural
language. Masters thesis, Massachusetts Institute of
Technology.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Proceedings of NAACL.
Michael Connor Ming-wei Chang and Dan Roth. 2010.
The necessity of combining adaptation methods. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Mas-
sachusetts, USA.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Mihai Surdeanu, Llu?s M?rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Robert S. Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34:161?191.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for pos tagging of unknown words . In Proceedings of
Coling.
David Vickrey and Daphne Koller. 2008. Sentence sim-
plification for semantic role labeling. In Proceedings
of the ACL-HLT.
237
