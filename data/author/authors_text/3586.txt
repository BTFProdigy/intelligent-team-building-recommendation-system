Example-based Machine Translation Based on Syntactic Transfer
with Statistical Models
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Eiichiro Sumita
ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, ?Keihanna Science City?
Kyoto, 619-0288, Japan
{kenji.imamura,hideo.okuma,taro.watanabe,eiichiro.sumita}@atr.jp
Abstract
This paper presents example-based machine
translation (MT) based on syntactic trans-
fer, which selects the best translation by us-
ing models of statistical machine translation.
Example-based MT sometimes generates in-
valid translations because it selects similar ex-
amples to the input sentence based only on
source language similarity. The method pro-
posed in this paper selects the best transla-
tion by using a language model and a trans-
lation model in the same manner as statisti-
cal MT, and it can improve MT quality over
that of ?pure? example-based MT. A feature
of this method is that the statistical models
are applied after word re-ordering is achieved
by syntactic transfer. This implies that MT
quality is maintained even when we only ap-
ply a lexicon model as the translation model.
In addition, translation speed is improved by
bottom-up generation, which utilizes the tree
structure that is output from the syntactic
transfer.
1 Introduction
In response to the ongoing expansion of bilingual
corpora, many machine translation (MT) meth-
ods have been proposed that automatically ac-
quire their knowledge or models from the cor-
pora. Recently, two major approaches to such ma-
chine translation have emerged: example-based
machine translation and statistical machine trans-
lation.
Example-based MT (Nagao, 1984) regards a
bilingual corpus as a database and retrieves exam-
ples that are similar to an input sentence. Then,
a translation is generated by modifying the tar-
get part of the examples while referring to trans-
lation dictionaries. Most example-based MT sys-
tems employ phrases or sentences as the unit for
examples, so they can translate while consider-
ing case relations or idiomatic expressions. How-
ever, when some examples conflict during re-
E =
J =
A =
NULL0 show1 me2 the3 one4 in5 the6 window7
uindo1 no2 shinamono3 o4 mise5 telidasai6
7 0 4 0 1 1( )
Figure 1: Example of Word Alignment between
English and Japanese (Watanabe and Sumita,
2003)
trieval, example-based MT selects the best exam-
ple scored by the similarity between the input and
the source part of the example. This implies that
example-based MT does not check whether the
translation of the given input sentence is correct
or not.
On the other hand, statistical MT employing
IBM models (Brown et al, 1993) translates an in-
put sentence by the combination of word transfer
and word re-ordering. Therefore, when it is ap-
plied to a language pair in which the word order is
quite different (e.g., English and Japanese, Figure
1), it becomes difficult to find a globally optimal
solution due to the enormous search space (Watan-
abe and Sumita, 2003).
Statistical MT could generate high-quality
translations if it succeeded in finding a globally
optimal solution. Therefore, the models employed
by statistical MT are superior indicators of the
quality of machine translation. Using this feature,
Akiba et al (2002) achieved selection of the best
translation among those output by multiple MT
engines.
This paper presents an example-based MT
method based on syntactic transfer, which selects
the best translation by using models of statisti-
cal MT. This method is roughly structured using
two modules (Figure 2). One is an example-based
syntactic transfer module. This module constructs
Input Sentence Output Sentence
Example-based
Syntactic Transfer
Thesaurus
Preprocessing Postprocessing
Statistical
Generation
Translation
Dictionary
Transfer
Rules
Translation
Model
Language
Model
Figure 2: Structure of Proposed Method
tree structures of the target language by parsing
and mapping the input sentence while referring to
transfer rules. The other is a statistical generation
module, which selects the best word sequence of
the target language in the same manner as statis-
tical MT. Therefore, this method is sequentially
combined example-based and statistical MT.
The proposed method has the following advan-
tages.
? From the viewpoint of example-based MT, the
quality of machine translation improves by se-
lecting the best translation not only from the
similarity judgment between the input sen-
tence and the source part of the examples but
also from the scoring of translation correctness
represented by the word transfer and word con-
nection.
? From the viewpoint of statistical MT, an ap-
propriate translation can be obtained even if
we use simple models because a global search
is applied after word re-ordering by syntac-
tic transfer. In addition, the search space
becomes smaller because the example-based
transfer generates syntactically correct candi-
dates for the most appropriate translation.
The rest of this paper is organized as follows:
Section 2 describes the example-based syntactic
transfer, Section 3 describes the statistical gen-
eration, Section 4 evaluates an experimental sys-
tem that uses this method, and Section 5 compares
other hybrid methods of example-based and statis-
tical MT.
2 Example-based Syntactic Transfer
The example-based syntactic transfer used in this
paper is a revised version of the Hierarchical
Phrase Alignment-based Translator (HPAT, re-
fer to (Imamura, 2002)). This section gives an
overview with an example of Japanese-to-English
machine translation.
2.1 Transfer Rules
Transfer rules are automatically acquired from
bilingual corpora by using hierarchical phrase
alignment (HPA; (Imamura, 2001)). HPA parses
bilingual sentences and acquires corresponding
syntactic nodes of the source and target sentences.
The transfer rules are created from their node cor-
respondences. Figure 3 shows an example of the
transfer rules. Variables, such as X and Y in Fig-
ure 3, denote non-terminal symbols that corre-
spond between source and target grammar. The
set of transfer rules is regarded as synchronized
context-free grammar.
The difference between this approach and con-
ventional synchronized context-free grammar is
that source examples are added to each transfer
rule. The source example is an instance (i.e., a
headword) of the variables that appeared in the
training corpora. For example, the source exam-
ple of Rule 1 in Figure 3 is obtained from a phrase
pair of the Japanese verb phrase ?furaito (flight)
wo yoyaku-suru (reserve)? and the English verb
phrase ?make a reservation for the flight.?
2.2 Syntactic Transfer Process
When an input sentence is given, the target tree
structure is constructed in the following three
steps.
1. The input sentence is parsed by using the
source grammar of the transfer rules.
2. The nodes in the source tree are mapped to the
target nodes by using transfer rules.
3. If non-terminal symbols remain in the leaves of
the target tree, candidates of translated words
are inserted by referring to the translation dic-
tionary.
An example of the syntactic transfer process is
shown in Figure 4 for the input sentence ?basu
wa 11 ji ni de masu (The bus will leave at 11
o?clock).? There are two points worthy of notice in
this figure. First, nodes in which the word order is
inverted are generated after transfer (cf. VP node
represented by a bold frame). Word re-ordering
is achieved by syntactic transfer. Second, words
No. Source Grammar Target Grammar Source Example
1 VP ? X
PP
Y
VP
? VP ? Y
VP
X
PP
((furaito (flight), yoyaku-suru (reserve)) ..)
2 VP ? Y
VP
X
ADVP
((soko (there), yuku (go)) ..)
3 VP ? Y
BEVP
X
NP
((hashi (bridge), aru (be)) ..)
4 S ? X
NP
wa Y
VP
masu ? S ? X
NP
Y
VP
((kare (he), enso-suru (play)) ..)
5 S ? X
NP
will Y
VP
((basu (bus), tomaru (stop)) ..)
Figure 3: Example of Transfer Rules
bus
bath
go
leave
start
11
NP -> a X3
NP -> the X3
NP -> X3
VP -> Y2 X2
VP -> X5 PP -> at X4
ADVP -> X4
NP -> X6 o?clock
NP -> X6
basu
(bus)
11
deru
(leave)
NP
X3
NP
X6 ji
                 (o?clock)
PP
X4 ni
VP
X5
VP
X2 Y2
S
X1 wa Y1 masu
X1
Y1
Y2 X2
Japanese English
S -> X1 will Y1
Figure 4: Example of Syntactic Transfer Process
(Bold frames are syntactic nodes mentioned in text)
that do not correspond between the source and tar-
get sentences (e.g., the determiner ?a? or ?the?)
are automatically inserted or eliminated by the tar-
get grammar (cf. NP node represented by a bold
frame). Namely, transfer rules work in a manner
similar to the functions of distortion, fertility, and
NULL in IBM models.
2.3 Usage of Source Examples
Example-based transfer utilizes the source exam-
ples for disambiguation of mapping and parsing.
Specifically, the semantic distance (Sumita and
Iida, 1991) is calculated between the source exam-
ples and the headwords of the input sentence, and
the transfer rules that contain the nearest exam-
ple are used to construct the target tree structure.
The semantic distance between words is defined
as the distance from the leaf node to the most spe-
cific common abstraction (MSCA) in a thesaurus
(Ohno and Hamanishi, 1984).
For example, if the input phrase ?ie (home) ni
kaeru (return)? is given, Rules 1 to 3 in Figure 3
are used for the syntactic transfer, and three target
nodes are generated without any disambiguation.
However, when we compare the source examples
with the headword of the variables X (ie) and Y
(kaeru), only Rule 2 is used for the transfer be-
cause the semantic distance of the example (soko
(there), yuku (go)) is the nearest. In the current
implementation, all rules that contain examples of
the same distance are used.
Consequently, example-based transfer achieves
translation while considering case relations or id-
iomatic expressions based on the semantic dis-
tance from the source examples.
3 Statistical Generation
3.1 Translation Model and Language Model
Statistical generation searches for the most ap-
propriate sequence of target words from the tar-
get tree output from the example-based syntactic
transfer. The most appropriate sequence is deter-
mined from the product of the translation model
and the language model in the same manner as sta-
tistical MT. In other words, when F and E denote
the channel target and channel source sequence,
respectively, the output word sequence E? that sat-
isfies the following equation is searched for.
E? = argmax
E
P (E|F )
= argmax
E
P (E)P (F |E). (1)
We only utilize the lexicon model as the trans-
lation model in this paper, similar to the models
proposed by Vogel et al (2003). Namely, when f
and e denote the channel target and channel source
word, respectively, the translation probability is
computed by the following equation.
P (F |E) =
?
j
?
i
t(f
j
|e
i
). (2)
The IBM models include other models, such
as fertility, NULL, and distortion models. As we
described in Section 2.2, the quality of machine
translation is maintained using only the lexicon
model because syntactical correctness is already
preserved by example-based transfer.
For the language model, we utilize a standard
word n-gram model.
3.2 Bottom-up Generation
We can construct word graphs by serializing the
target tree structure, which allows us to select the
best word sequence from the graphs. However,
the tree structure already shares nodes transferred
from the same input sub-sequence. The cost of
calculating probabilities is equivalent if we cal-
culate the probabilities while serializing the tree
structure. We call this method bottom-up genera-
tion in this paper.
Figure 5 shows a partial example of bottom-
up generation when the target tree in Figure 4
is given. For each node, word sub-sequences
and their probabilities (language and translation)
are obtained from child nodes. Then, the new
probabilities of the word sequence combination
are calculated, and the n-best sequences are se-
lected. These n-best sequences and their prob-
abilities are reused to calculate the probabilities
of parent nodes. When the translation probabil-
ity is calculated, the source word sub-sequence is
obtained by tracing transfer mapping, and the ap-
plied translation model is restricted to the source
sub-sequence. In other words, the translation
probability is locally calculated between the cor-
responding phrases.
Set Name Item English Japanese
Training # of Sentences 152,170
# of Words 886,708 1,007,484
Test # of Sentences 510
# of Words 2,973 3,340
Table 1: Corpus Size
When the generation reaches the top node, the
language probability is re-calculated with marks
for start-of-sentence and end-of-sentence, and the
n-best list is re-sorted. As a result, the translation
?The bus will leave at 11 o?clock? is obtained from
the tree of Figure 4.
Bottom-up generation calculates the probabili-
ties of shared nodes only once, so it effectively
uses tree information.
4 Evaluation
In order to evaluate the effect when models of sta-
tistical MT are integrated into example-based MT,
we compared various methods that changed the
statistical generation module.
4.1 Experimental Setting
Bilingual Corpus The corpus used in the fol-
lowing experiments is the Basic Travel Expression
Corpus (Takezawa et al, 2002; Kikui et al, 2003).
This is a collection of Japanese sentences and their
English translations based on expressions that are
usually found in phrasebooks for foreign tourists.
We divided it into subsets for training and testing
as shown in Table 1.
Transfer Rules Transfer rules were acquired
from the training set using hierarchical phrase
alignment, and low-frequency rules that appeared
less than twice were removed. The number of
rules was 24,310.
Translation Model and Language Model We
used a lexicon model of IBM Model 4 learned by
GIZA++ (Och and Ney, 2003) and word bigram
and trigram models learned by CMU-Cambridge
Statistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997).
Compared Methods We compared the follow-
ing four methods.
? Baseline (Example-based Transfer only)
The best translation that had the same seman-
tic distance was randomly selected from the
the bus TM: -0.07LM: -1.94
bus TM: -0.07LM: -0.0
XNP n-best
n-best n-best
will
YVP
leave at 11 o?clock TM: -2.72LM: -4.58
start at 11 o?clock TM: -3.62LM: -4.17
leaves at 11 o?clock TM: -2.72LM: -3.11
YVP
leave TM: -1.88LM: -0.0
start TM: -2.78LM: -0.0
leaves TM: -1.88LM: -0.0
XPP
at 11 o?clock TM: -0.84LM: -2.79
at 11 TM: -4.91LM: -2.26
a bus TM: -0.07LM: -2.11
S
bus will start at 11 o?clock
the bus will leave at 11 o?clock
bus will leave at 11 o?clock TM: -7.13LM: -14.30
TM: -8.03
LM: -13.84
TM: -7.13
LM: -13.54
<s> </s>
Figure 5: Example of Bottom-up Generation
(TM and LM denote log probabilities of the translation and language models, respectively)
tree that was output from the example-based
transfer module. The translation words were
selected in advance as those having the highest
frequency in the training corpus. This is the
baseline for translating a sentence when using
only the example-based transfer.
? Bottom-up
The bottom-up generation selects the best
translation from the outputs of the example-
based transfer. We used a 100-best criterion
in this experiment.
? All Search
For all combinations that can be generated
from the outputs of the example-based trans-
fer, we calculated the translation and language
probabilities and selected the best translation.
Namely, a globally optimal solution was se-
lected when the search space was restricted by
the example-based transfer.
? LM Only
In the same way as All Search, the best trans-
lation was searched for, but only the language
model was used for calculating probabilities.
The purpose of this experiment is to measure
the influence of the translation model.
Evaluation Metrics From the test set, 510 sen-
tences were evaluated by the following automatic
and subjective evaluation metrics. The number
of reference translations for automatic evaluation
was 16 per sentence.
BLEU: Automatic evaluation by BLEU score
(Papineni et al, 2002).
NIST: Automatic evaluation by NIST score
(Doddington, 2002).
mWER: The mean rate by calculating the word
error rates between the MT results and all ref-
erence translations, where the lowest rate is se-
lected.
Subjective Evaluation: Subjective evaluation
by an English native speaker into the four ranks
of A: Perfect, B: Fair, C: Acceptable, and D:
Nonsense.
Automatic Evaluation Subjective Evaluation Translation Speed
Method BLEU NIST mWER A A+B A+B+C Mean Worst
(sec./sent.) (sec.)
Baseline 0.410 9.06 0.423 51.6% 64.3% 70.4% 0.180 10.82
Bottom-up 0.491 9.99 0.366 62.2% 72.5% 80.4% 0.211 5.03
All Search 0.498 10.04 0.353 62.9% 73.1% 80.8% 1.23 171.31
LM Only 0.491 9.11 0.385 57.6% 66.9% 72.0% 1.624 220.69
Table 2: MT Quality and Translation Speed vs. Generation Methods
4.2 Results
Table 2 shows the results of the MT quality and
translation speed among each method.
First, comparing the baseline with the statisti-
cal generations (Bottom-up and All Search), the
MT quality of statistical generation improved in
all evaluation metrics. Accordingly, the models of
statistical MT are effective for improving the MT
quality of example-based MT.
Next, comparing Bottom-up with All Search,
the MT quality of bottom-up generation was
slightly low. Bottom-up generation locally applies
the translation model to a partial tree. In other
words, the probability is calculated without word
alignment linked to the outside of the tree. This re-
sult indicates that the results of bottom-up genera-
tion are not equal to the global optimal solution.
Comparing LM Only with the statistical gener-
ations, the MT quality of ranks A+B+C by subjec-
tive evaluation significantly decreased. This is be-
cause the n-gram language model used here does
not consider output length, and shorter translations
are preferred. Although the language model was
effective to some degree, it could not evaluate the
equivalence of the translation and the input sen-
tence. Therefore, we concluded that the transla-
tion model is necessary for improving MT quality.
Finally, focusing on translation speed, the worst
time for Bottom-up generation was dramatically
faster than that for All Search. Bottom-up gen-
eration effectively uses shared nodes of the target
tree, so it can improve translation speed. There-
fore, bottom-up generation is suitable for tasks
that require real-time processing, such as spoken
dialogue translation.
5 Discussion
We incorporated example-based MT in models
of statistical MT. However, some methods to ob-
tain initial solutions of statistical MT by example-
based MT have already been proposed. For
example, Marcu (2001) proposed a method in
which initial translations are constructed by com-
bining bilingual phrases from translation mem-
ory, which is followed by modifying the transla-
tions by greedy decoding (Germann et al, 2001).
Watanabe and Sumita (2003) proposed a decoding
algorithm in which translations that are similar to
the input sentence are retrieved from bilingual cor-
pora and then modified by greedy decoding.
The difference between our method and these
methods involves whether modification is applied.
Our approach simply selects the best translation
from candidates that are output from example-
based MT. Even though example-based MT can
output appropriate translations to some degree,
our method assumes that the candidates contain
a globally optimal solution. This means that
the upper bound of MT quality is limited by the
example-based transfer, so we have to improve
this stage in order to further improve MT quality.
For instance, example-based MT can be improved
by applying an optimization algorithm that uses
an automatic evaluation of MT quality (Imamura
et al, 2003).
6 Conclusions
This paper demonstrated that example-based MT
can be improved by incorporating it in models of
statistical MT. The example-based MT used in this
paper is based on syntactic transfer, so word re-
ordering is achieved in the transfer module. Us-
ing this feature, the best translation was selected
by using only a lexicon model and an n-gram lan-
guage model. In addition, bottom-up generation
achieved faster translation speed by using the tree
structure of the target sentence.
Acknowledgements
The authors would like to thank Kadokawa Pub-
lishers, who permitted us to use the hierarchy of
Ruigo-shin-jiten.
The research reported here is supported in part
by a contract with the Telecommunications Ad-
vancement Organization of Japan entitled, ?A
study of speech dialogue translation technology
based on a large corpus.?
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro
Sumita. 2002. Using language and transla-
tion models to select the best among outputs
from multiple MT systems. In Proceedings of
COLING-2002, pages 8?14.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the CMU-
Cambridge toolkit. In Proceedings of Eu-
roSpeech 97, pages 2707?2710.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram
co-occurrence statistics. In Proceedings of the
HLT Conference, San Diego, California.
Ulrich Germann, Michael Jahr, Kevin Knight,
Daniel Marcu, and Kenji Yamada. 2001. Fast
decoding and optimal decoding for machine
translation. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 228?235.
Kenji Imamura, Eiichiro Sumita, and Yuji Mat-
sumoto. 2003. Feedback cleaning of machine
translation rules using automatic evaluation. In
Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL 2003), pages 447?454.
Kenji Imamura. 2001. Hierarchical phrase align-
ment harmonized with parsing. In Proceed-
ings of the 6th Natural Language Processing
Pacific Rim Symposium (NLPRS 2001), pages
377?384.
Kenji Imamura. 2002. Application of transla-
tion knowledge acquired by hierarchical phrase
alignment for pattern-based MT. In Proceed-
ings of the 9th Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-2002), pages 74?84.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech translation.
In Proceedings of EuroSpeech 2003, pages
381?384.
Daniel Marcu. 2001. Towards a unified approach
to memory- and statistical-based machine trans-
lation. In Proceedings of 39th Annual Meeting
of the Association for Computational Linguis-
tics, pages 386?393.
Makoto Nagao. 1984. A framework of mechani-
cal translation between Japanese and English by
analogy principle. In Artificial and Human In-
telligence, pages 173?180, Amsterdam: North-
Holland.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Susumu Ohno and Masato Hamanishi. 1984.
Ruigo-Shin-Jiten. Kadokawa, Tokyo. in
Japanese.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 311?318.
Eiichiro Sumita and Hitoshi Iida. 1991. Experi-
ments and prospects of example-based machine
translation. In Proceedings of the 29th ACL,
pages 185?192.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki
Sugaya, Hirofumi Yamamoto, and Seiichi Ya-
mamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel con-
versations in the real world. In Proceedings
of the Third International Conference on Lan-
guage Resources and Evaluation (LREC 2002),
pages 147?152.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia
Tribble, Ashish Venugopal, Bing Zhao, and
Alex Waibel. 2003. The CMU statistical ma-
chine translation system. In Proceedings of the
9th Machine Translation Summit (MT Summit
IX), pages 402?409.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical machine
translation. In Proceedings of Machine Trans-
lation Summit IX, pages 410?417.
155
156
157
158
159
160
161
162
171
172
173
174
Automatic Expansion of Equivalent Sentence Set
Based on Syntactic Substitution
Kenji Imamura, Yasuhiro Akiba and Eiichiro Sumita
ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, ?Keihanna Science City?, Kyoto, 619-0288, Japan
{kenji.imamura,yasuhiro.akiba,eiichiro.sumita}@atr.co.jp
Abstract
In this paper, we propose an automatic quan-
titative expansion method for a sentence set
that contains sentences of the same meaning
(called an equivalent sentence set). This task
is regarded as paraphrasing. The features of
our method are: 1) The paraphrasing rules are
dynamically acquired by Hierarchical Phrase
Alignment from the equivalent sentence set,
and 2) A large equivalent sentence set is gen-
erated by substituting source syntactic struc-
tures. Our experiments show that 561 sen-
tences on average are correctly generated from
8.48 equivalent sentences.
1 Introduction
Sentences can be represented by various expressions even
though they have the same meaning. Paraphrasing that
transfer from sentence to sentence (Barzilay and McK-
eown, 2001) is a technique that generates such various
expressions.
In this paper, we propose an automatic quantitative
expansion method for a sentence set that contains sen-
tences of the same meaning (called an equivalent sen-
tence set), as a paraphrasing technique. Our method is
roughly structured from the following two phases.
1. Extract phrasal correspondences that have the same
meaning (called equivalent phrases) from the source
equivalent sentence set (acquisition phase).
2. Based on the parse tree of the sentence selected
from the source set, generate target sentences by re-
cursively substituting the equivalent phrases for the
source phrases (generation phase).
Paraphrasing is regarded as machine translation into
the same language. In this paper, we apply syntactic sub-
stitution for generating sentences, which corresponds to
transfer-based machine translation. In addition, Hierar-
chical Phrase Alignment (HPA) (Imamura, 2001), which
is an automatic acquisition method for machine transla-
tion rules, is applied to acquire the paraphrasing rules.
Namely, two equivalent sentences are regarded as bilin-
gual sentences, and simplified machine translation is car-
ried out.
Paraphrasing by our method has the following charac-
teristics.
 Not only lexical paraphrasing but also phrasal para-
phrasing can be generated because our method is
based on structural substitution.
 Equivalent phrases extracted by HPA are not only
semantically but also grammatically equivalent.
Thus, our method rarely generates ungrammatical
sentences by substitution.
Expansion of the equivalent sentence set can be ap-
plied to automatic evaluation of machine translation qual-
ity (Papineni et al, 2002; Akiba et al, 2001), for exam-
ple. These methods evaluate the quality of the transla-
tion by measuring the similarity between machine trans-
lation results and translations done by humans (called ref-
erences). However, the accuracy increases when multi-
ple references are applied because one source sentence
can be translated into multiple target expressions. Our
method generates multiple sentences that are suitable for
this purpose.
2 Acquisition of Paraphrasing Rules:
Hierarchical Phrase Alignment
Hierarchical Phrase Alignment is based on the assump-
tion that an ?equivalent phrase pair has the same informa-
tion and the same grammatical role.? We decompose this
assumption into the following two conditions for compu-
tation.
 The words in the phrase pair correspond, with no
deficiency and no excess.
 The phrases are of the same syntactic category.
Therefore, HPA is a task to extract phrase pairs that
satisfy the above two conditions. The procedure of HPA
is summarized as follows. 1
1. Tag and parse two equivalent sentences.
2. Extract corresponding words (called word links) be-
tween the sentences. In this paper, we regard identi-
cal words and words that belong to the same group
in a thesaurus as word links.
3. Check all combinations of syntactic nodes between
the sentences. If the node pair satisfies the above
two conditions, then output the pair as an equivalent
phrase. Namely, if no words in the phrase link to the
outside of the other phrase, and the nodes have the
same category, the phrase pair is regarded as equiv-
alent.
Figure 1 shows an example of equivalent phrase extrac-
tion from source equivalent sentences. The upper sen-
tence is interrogative, the lower sentence is imperative,
and they have the same meaning. For example, focusing
on the upper phrase ?get me,? this phrase is VP and con-
tains two word links. However, no nodes contain only the
links ?get?, and ?me? in the lower sentence. On the other
hand, focusing on the upper phrase ?get me a taxi,? it con-
tains four word links that correspond to the lower phrase
?get a taxi for me?, and they have the same syntactic cat-
egory. Therefore, the node pair VP(4) is regarded as an
equivalent phrase.
By iterating the above process, HPA consequently ex-
tracts eight nodes as equivalents from the source sen-
tences shown in Figure 1. Excluding the identical
phrases, the following three phrases are acquired as
equivalent phrases.
 ?get me a taxi? and ?get a taxi for me?
 ?10 in the morning? and ?10 a.m.?
 ?at 10 in the morning? and ?at 10 a.m.?
HPA can extract phrasal correspondences from source
equivalent sentences even if their sentence structures
are significantly different. In addition, because node
pairs have to be in the same syntactic category, un-
paraphrasable correspondences, such as ?morning? and
?a.m.,? are ignored even though they have word links.
3 Expansion of Equivalent Sentence Set
The equivalent phrases extracted by HPA are substi-
tutable with one another because they are semantically
and grammatically equivalent. Therefore, they are re-
garded as bi-directional paraphrasing rules. When we
paraphrase from any N sentences, target equivalent sen-
tences are generated by the following procedure, where
1The original method of HPA has two additional features.
1) Ambiguity of parsing is resolved by comparing parse trees of
input sentences. 2) It employs partial parsing to analyze irregu-
lar sentences. Details are described in (Imamura, 2001).
Would you get me a taxi at 10 in the morning?
Please get a taxi for me at 10 a.m.
NP
NMP
NP(5)
VMP(6)
NP(1)NP(2)VP(3)
VP
VP(4)
VP(7)
S(8)
NP(5)
VMP(6)
NP(2)NP(1)
VMP
VP(3)
VP
VP(4)
VP(7)
ADVP
VP
S(8)
Figure 1: Example of Equivalent Phrase Extraction from
English Equivalent Sentences (The lines between the sen-
tences denote word links, the trees denote parsing re-
sults, and the numbers on the nodes denote corresponding
equivalent phrases.)
the range from Step 1 to Step 3 corresponds to the acqui-
sition phase, and Steps 4 and 5 correspond to the genera-
tion phase.
1. First, select one sentence from the source equivalent
sentence set.
2. Process HPA with the remaining (N ? 1) sentences,
and extract equivalent phrases.
3. Repeat Steps 1 and 2 for all combinations of the
source sentences. All phrases that construct the
source set and their paraphrasing rules are acquired.
4. Next, select one tree created by HPA from the source
equivalent sentence set, and trace the tree top-down.
If a node registered in the paraphrasing rules is
found, substitute the equivalent phrase for the node.
Substitution is recursively done until it reaches a
leaf.
5. Repeat Step 4 with all sentences in the source set.
For example, when the source equivalent sentence set
contains only the two sentences shown in Figure 1, the
following six sentences are generated. Our method gen-
erates all sentences constructed from the phrases of N
sentences.
Would you get a taxi for me at 10 a.m.?
Would you get a taxi for me at 10 in the morning?
Would you get me a taxi at 10 a.m.?
Please get me a taxi at 10 in the morning
Please get me a taxi at 10 a.m.
Please get a taxi for me at 10 in the morning.
110
100
1000
10000
0 20 40 60 80 100 120 140 160 180
N
um
be
r o
f G
en
er
at
ed
 S
en
te
nc
es
Number of Equivalent Phrases
Figure 2: Relationship between Number of Equivalent
Phrases and Number of Generated Sentences
Japanese Correctness
OK NG Total
Translation OK 892 (61%) 382 (26%) 1274 (86%)
Effectiveness NG 87 ( 6%) 112 ( 8%) 199 (14%)
Total 979 (66%) 494 (34%) 1473 (100%)
Table 1: Quality of Generated Sentences
4 Experiments
Expansion experiments of Japanese equivalent sentences
were carried out. We used 339 source equivalent sentence
sets selected from ATR corpus (Furuse et al, 1994). The
sets were created by having ten Japanese native speakers
translate English sentences into Japanese. The number of
different sentences was 8.48 sentences per English sen-
tence on average.
Number of Generated Sentences Figure 2 is a graph
plotting the number of equivalent phrases and the number
of of generated sentences. Each point denotes a source
equivalent sentence set. Consequently 60.2 equivalent
phrases on average were acquired, and 920 sentences on
average were generated from a source set.
Quality of Generated Sentences We randomly se-
lected five sentences per set from above generated sen-
tences and showed them to a Japanese native speaker to-
gether with the English sentences. One-by-one, he/she
judged whether the sentences were good or not from the
viewpoints of Japanese correctness (grammatically and
pragmatically correct or not) and translation effectiveness
(understandable or not). The results are shown in Table
1.
Consequently, approximately 61 percent of the gener-
ated sentences were judged good from the viewpoints of
both Japanese correctness and translation effectiveness.
In other words, 561 sentences on average were correctly
generated, and the source equivalent sentence sets were
expanded about 66 times. About 39% of the generated
sentences contain errors. However, we believe that our
method is effective when we make a large equivalent sen-
tence set because eliminating error sentences is easier
than creating a large set manually.
Our error analysis found that major errors are caused
by inconsistency in the modality. Our method does not
consider pragmatical correctness although it generates
syntactically correct sentences.
5 Conclusion
We proposed an expansion method for an equivalent sen-
tence set based on syntactic substitution. Our method dy-
namically acquires paraphrasing rules by using HPA, and
generates many sentences by applying the rules to a parse
tree recursively. One application of our method is the au-
tomatic evaluation of machine translation quality. We are
planning to integrate this method into a form of automatic
evaluation.
Acknowledgment
The research reported here is supported in part by a con-
tract with the Telecommunications Advancement Orga-
nization of Japan entitled, ?A study of speech dialogue
translation technology based on a large corpus.?
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of
Machine Translation Summit VIII, pages 15?20.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics, pages 50?57.
Osamu Furuse, Y. Sobashima, Toshiyuki Takezawa, and
N. Uratani. 1994. Bilingual corpus for speech transla-
tion. In Proceedings of the AAAI?94 Workshop ?Inte-
gration of Natural Language and Speech Processing?,
pages 84?91.
Kenji Imamura. 2001. Hierarchical phrase alignment
harmonized with parsing. In Proceedings of the 6th
Natural Language Processing Pacific Rim Symposium
(NLPRS 2001), pages 377?384.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318.
Feedback Cleaning of Machine Translation Rules
Using Automatic Evaluation
Kenji Imamura, Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
Seika-cho, Soraku-gun, Kyoto, Japan
{kenji.imamura,eiichiro.sumita}@atr.co.jp
Yuji Matsumoto
Nara Institute of
Science and Technology
Ikoma-shi, Nara, Japan
matsu@is.aist-nara.ac.jp
Abstract
When rules of transfer-based machine
translation (MT) are automatically ac-
quired from bilingual corpora, incor-
rect/redundant rules are generated due to
acquisition errors or translation variety in
the corpora. As a new countermeasure
to this problem, we propose a feedback
cleaning method using automatic evalua-
tion of MT quality, which removes incor-
rect/redundant rules as a way to increase
the evaluation score. BLEU is utilized
for the automatic evaluation. The hill-
climbing algorithm, which involves fea-
tures of this task, is applied to searching
for the optimal combination of rules. Our
experiments show that the MT quality im-
proves by 10% in test sentences according
to a subjective evaluation. This is consid-
erable improvement over previous meth-
ods.
1 Introduction
Along with the efforts made in accumulating bilin-
gual corpora for many language pairs, quite a few
machine translation (MT) systems that automati-
cally acquire their knowledge from corpora have
been proposed. However, knowledge for transfer-
based MT acquired from corpora contains many in-
correct/redundant rules due to acquisition errors or
translation variety in the corpora. Such rules con-
flict with other existing rules and cause implausible
MT results or increase ambiguity. If incorrect rules
could be avoided, MT quality would necessarily im-
prove.
There are two approaches to overcoming incor-
rect/redundant rules:
? Selecting appropriate rules in a disambiguation
process during the translation (on-line process-
ing, (Meyers et al, 2000)).
? Cleaning incorrect/redundant rules after
automatic acquisition (off-line processing,
(Menezes and Richardson, 2001; Imamura,
2002)).
We employ the second approach in this paper.
The cutoff by frequency (Menezes and Richardson,
2001) and the hypothesis test (Imamura, 2002) have
been applied to clean the rules. The cutoff by fre-
quency can slightly improve MT quality, but the im-
provement is still insufficient from the viewpoint of
the large number of redundant rules. The hypothesis
test requires very large corpora in order to obtain a
sufficient number of rules that are statistically confi-
dent.
Another current topic of machine translation is
automatic evaluation of MT quality (Papineni et al,
2002; Yasuda et al, 2001; Akiba et al, 2001). These
methods aim to replace subjective evaluation in or-
der to speed up the development cycle of MT sys-
tems. However, they can be utilized not only as de-
velopers? aids but also for automatic tuning of MT
systems (Su et al, 1992).
We propose feedback cleaning that utilizes
an automatic evaluation for removing incor-
rect/redundant translation rules as a tuning method
Training
Corpus
Automatic
Acquisition
Translation
Rules
Evaluation
Corpus
MT
Engine
Automatic
Evaluation
MT Results
Rule 
Selection/Deletion
Feedback Cleaning
Figure 1: Structure of Feedback Cleaning
(Figure 1). Our method evaluates the contribution
of each rule to the MT results and removes inap-
propriate rules as a way to increase the evaluation
scores. Since the automatic evaluation correlates
with a subjective evaluation, MT quality will im-
prove after cleaning.
Our method only evaluates MT results and does
not consider various conditions of the MT engine,
such as parameters, interference in dictionaries, dis-
ambiguation methods, and so on. Even if an MT
engine avoids incorrect/redundant rules by on-line
processing, errors inevitably remain. Our method
cleans the rules in advance by only focusing on the
remaining errors. Thus, our method complements
on-line processing and adapts translation rules to the
given conditions of the MT engine.
2 MT System and Problems of Automatic
Acquisition
2.1 MT Engine
We use the Hierarchical Phrase Alignment-based
Translator (HPAT) (Imamura, 2002) as a transfer-
basedMT system. The most important knowledge in
HPAT is transfer rules, which define the correspon-
dences between source and target language expres-
sions. An example of English-to-Japanese transfer
rules is shown in Figure 2. The transfer rules are
regarded as a synchronized context-free grammar.
When the system translates an input sentence, the
sentence is first parsed by using source patterns of
the transfer rules. Next, a tree structure of the tar-
get language is generated by mapping the source
patterns to the corresponding target patterns. When
non-terminal symbols remain in the target tree, tar-
get words are inserted by referring to a translation
dictionary.
Ambiguities, which occur during parsing or map-
ping, are resolved by selecting the rules that mini-
mize the semantic distance between the input words
and source examples (real examples in the training
corpus) of the transfer rules (Furuse and Iida, 1994).
For instance, when the input phrase ?leave at 11
a.m.? is translated into Japanese, Rule 2 in Figure
2 is selected because the semantic distance from the
source example (arrive, p.m.) is the shortest to the
head words of the input phrase (leave, a.m.).
2.2 Problems of Automatic Acquisition
HPAT automatically acquires its transfer rules from
parallel corpora by using Hierarchical Phrase Align-
ment (Imamura, 2001). However, the rule set con-
tains many incorrect/redundant rules. The reasons
for this problem are roughly classified as follows.
? Errors in automatic rule acquisition
? Translation variety in corpora
? The acquisition process cannot generalize
the rules because bilingual sentences de-
pend on the context or the situation.
? Corpora contain multiple (paraphrasable)
translations of the same source expres-
sion.
In the experiment of Imamura (2002), about
92,000 transfer rules were acquired from about
120,000 bilingual sentences 1. Most of these rules
are low-frequency. They reported that MT quality
slightly improved, even though the low-frequency
rules were removed to a level of about 1/9 the pre-
vious number. However, since some of them, such
as idiomatic rules, are necessary for translation, MT
quality cannot be dramatically improved by only re-
moving low-frequency rules.
3 Automatic Evaluation of MT Quality
We utilize BLEU (Papineni et al, 2002) for the au-
tomatic evaluation of MT quality in this paper.
BLEU measures the similarity between MT re-
sults and translation results made by humans (called
1In this paper, the number of rules denotes the number of
unique pairs of source patterns and target patterns.
Rule No. Syn. Cat. Source Pattern Target Pattern Source Example
1 VP X
VP
at Y
NP
? Y? de X? ((present, conference) ...)
2 VP X
VP
at Y
NP
? Y? ni X? ((stay, hotel), (arrive, p.m) ...)
3 VP X
VP
at Y
NP
? Y? wo X? ((look, it) ...)
4 NP X
NP
at Y
NP
? Y? no X? ((man, front desk) ...)
Figure 2: Example of HPAT Transfer Rules
references). This similarity is measured by N-gram
precision scores. Several kinds of N-grams can be
used in BLEU. We use from 1-gram to 4-gram in
this paper, where a 1-gram precision score indicates
the adequacy of word translation and longer N-gram
(e.g., 4-gram) precision scores indicate fluency of
sentence translation. The BLEU score is calculated
from the product of N-gram precision scores, so this
measure combines adequacy and fluency.
Note that a sizeable set of MT results is necessary
in order to calculate an accurate BLEU score. Al-
though it is possible to calculate the BLEU score of a
single MT result, it contains errors from the subjec-
tive evaluation. BLEU cancels out individual errors
by summing the similarities of MT results. There-
fore, we need all of the MT results from the evalua-
tion corpus in order to calculate an accurate BLEU
score.
One feature of BLEU is its use of multiple ref-
erences for a single source sentence. However, one
reference per sentence is used in this paper because
an already existing bilingual corpus is applied to the
cleaning.
4 Feedback Cleaning
In this section, we introduce the proposed method,
called feedback cleaning. This method is carried out
by selecting or removing translation rules to increase
the BLEU score of the evaluation corpus (Figure 1).
Thus, this task is regarded as a combinatorial op-
timization problem of translation rules. The hill-
climbing algorithm, which involves the features of
this task, is applied to the optimization. The fol-
lowing sections describe the reasons for using this
method and its procedure. The hill-climbing al-
gorithm often falls into locally optimal solutions.
However, we believe that a locally optimal solution
is more effective in improving MT quality than the
previous methods.
4.1 Costs of Combinatorial Optimization
Most combinatorial optimization methods iterate
changes in the combination and the evaluation. In
the machine translation task, the evaluation process
requires the longest time. For example, in order to
calculate the BLEU score of a combination (solu-
tion), we have to translate C times, where C denotes
the size of the evaluation corpus. Furthermore, in
order to find the nearest neighbor solution, we have
to calculate all BLEU scores of the neighborhood.
If the number of rules is R and the neighborhood
is regarded as consisting of combinations made by
changing only one rule, we have to translate C ? R
times to find the nearest neighbor solution. Assume
that C = 10, 000 and R = 100, 000, the number
of sentence translations (sentences to be translated)
becomes one billion. It is infeasible to search for
the optimal solution without reducing the number of
sentence translations.
A feature of this task is that removing rules is eas-
ier than adding rules. The rules used for translating
a sentence can be identified during the translation.
Conversely, the source sentence set S[r], where a
rule r is used for the translation, is determined once
the evaluation corpus is translated. When r is re-
moved, only the MT results of S[r] will change,
so we do not need to re-translate other sentences.
Assuming that five rules on average are applied to
translate a sentence, the number of sentence trans-
lations becomes 5 ? C + C = 60, 000 for testing
all rules. On the contrary, to add a rule, the entire
corpus must be re-translated because it is unknown
which MT results will change by adding a rule.
4.2 Cleaning Procedure
Based on the above discussion, we utilize the hill-
climbing algorithm, in which the initial solution
contains all rules (called the base rule set) and the
search for a combination is done by only removing
static: C
eval
, an evaluation corpus
R
base
, a rule set acquired from the entire training corpus (the base rule set)
R, a current rule set, a subset of the base rule set
S[r], a source sentence set where the rule r is used for the translation
Doc
iter
, an MT result set of the evaluation corpus translated with the current rule set
procedure CLEAN-RULESET ()
R ? R
base
repeat
R
iter
? R
R
remove
? ?
score
iter
? SET-TRANSLATION()
for each r in R
iter
do
if S[r] = ? then
R ? R
iter
? {r}
translate all sentences in S[r], and obtain the MT results T [r]
Doc[r] ? the MT result set that T [r] is replaced from Doc
iter
the rule contribution contrib[r] ? score
iter
? BLEU-SCORE(Doc[r])
if contrib[r] < 0 then add r to R
remove
end
R ? R
iter
?R
remove
until R
remove
= ?
function SET-TRANSLATION () returns a BLEU score of the evaluation corpus translated with R
Doc
iter
? ?
for each r in R
base
do S[r] ? ? end
for each s in C
eval
do
translate s and obtain the MT result t
obtain the rule set R[s] that is used for translating s
for each r in R[s] do add s to S[r] end
add t to Doc
iter
end
return BLEU-SCORE(Doc
iter
)
Figure 3: Feedback Cleaning Algorithm
rules. The algorithm is shown in Figure 3. This al-
gorithm can be summarized as follows.
? Translate the evaluation corpus first and then
obtain the rules used for the translation and the
BLEU score before removing rules.
? For each rule one-by-one, calculate the BLEU
score after removing the rule and obtain the dif-
ference between this score and the score before
the rule was removed. This difference is called
the rule contribution.
? If the rule contribution is negative (i.e., the
BLUE score increases after removing the rule),
remove the rule.
In order to achieve faster convergence, this algo-
rithm removes all rules whose rule contribution is
negative in one iteration. This assumes that the re-
moved rules are independent from one another.
5 N-fold Cross-cleaning
In general, most evaluation corpora are smaller than
training corpora. Therefore, omissions of cleaning
Training
Corpus
Training
Evaluation
Training
Evaluation
Training
Evaluation
Training
Base
Rule Set
Rule
Subset 1
Rule
Subset 2
Rule
Subset 3
Feedback
Cleaning
Feedback
Cleaning
Feedback
Cleaning
Rule
Deletion Rule Contributions
Cleaned
Rule Set
Divide
Figure 4: Structure of Cross-cleaning
(In the case of three-fold cross-cleaning)
will remain because not all rules can be tested by the
evaluation corpus. In order to avoid this problem, we
propose an advanced method called cross-cleaning
(Figure 4), which is similar to cross-validation.
The procedure of cross-cleaning is as follows.
1. First, create the base rule set from the entire
training corpus.
2. Next, divide the training corpus into N pieces
uniformly.
3. Leave one piece for the evaluation, acquire
rules from the rest (N ? 1) of the pieces, and
repeat them N times. Thus, we obtain N pairs
of rule set and evaluation sub-corpus. Each rule
set is a subset of the base rule set.
4. Apply the feedback cleaning algorithm to each
of the N pairs and record the rule contributions
even if the rules are removed. The purpose of
this step is to obtain the rule contributions.
5. For each rule in the base rule set, sum up the
rule contributions obtained from the rule sub-
sets. If the sum is negative, remove the rule
from the base rule set.
The major difference of this method from cross-
validation is Step 5. In the case of cross-cleaning,
Set Name Feature English Japanese
Training # of Sentences 149,882
Corpus # of Words 868,087 984,197
Evaluation # of Sentences 10,145
Corpus # of Words 59,533 67,554
Test # of Sentences 10,150
Corpus # of Words 59,232 67,193
Table 1: Corpus Size
the rule subsets cannot be directly merged because
some rules have already been removed in Step 4.
Therefore, we only obtain the rule contributions
from the rule subsets and sum them up. The summed
contribution is an approximate value of the rule
contribution to the entire training corpus. Cross-
cleaning removes the rules from the base rule set
based on this approximate contribution.
Cross-cleaning uses all sentences in the training
corpus, so it is nearly equivalent to applying a large
evaluation corpus to feedback cleaning, even though
it does not require specific evaluation corpora.
6 Evaluation
In this section, the effects of feedback cleaning are
evaluated by using English-to-Japanese translation.
6.1 Experimental Settings
Bilingual Corpora The corpus used in the fol-
lowing experiments is the Basic Travel Expression
Corpus (Takezawa et al, 2002). This is a collec-
tion of Japanese sentences and their English trans-
lations based on expressions that are usually found
in phrasebooks for foreign tourists. We divided it
into sub-corpora for training, evaluation, and test as
shown in Table 1. The number of rules acquired
from the training corpus (the base rule set size) was
105,588.
Evaluation Methods of MT Quality We used the
following two methods to evaluate MT quality.
1. Test Corpus BLEU Score
The BLUE score was calculated with the test
corpus. The number of references was one for
each sentence, in the same way used for the
feedback cleaning.
0.22
0.24
0.26
0.28
0.3
0.32
0 1 2 3 4 5 6 7 8 9
80k
90k
100k
110k
120k
BL
EU
 S
co
re
N
um
be
r o
f R
ul
es
Number of Iterations
Test Corpus BLEU Score
Evaluation Corpus BLEU Score
Number of Rules
Figure 5: Relationship between Number of Itera-
tions and BLEU Scores/Number of Rules
2. Subjective Quality
A total of 510 sentences from the test corpus
were evaluated by paired comparison. Specif-
ically, the source sentences were translated us-
ing the base rule set, and the same sources were
translated using the rules after the cleaning.
One-by-one, a Japanese native speaker judged
which MT result was better or that they were
of the same quality. Subjective quality is repre-
sented by the following equation, where I de-
notes the number of improved sentences and D
denotes the number of degraded sentences.
Subj. Quality = I ?D
# of test sentences
(1)
6.2 Feedback Cleaning Using Evaluation
Corpus
In order to observe the characteristics of feedback
cleaning, cleaning of the base rule set was carried
out by using the evaluation corpus. The results are
shown in Figure 5. This graph shows changes in
the test corpus BLEU score, the evaluation corpus
BLEU score, and the number of rules along with the
number of iterations.
Consequently, the removed rules converged at
nine iterations, and 6,220 rules were removed. The
evaluation corpus BLEU score was improved by in-
creasing the number of iterations, demonstrating that
the combinatorial optimization by the hill-climbing
algorithm worked effectively. The test corpus BLEU
score reached a peak score of 0.245 at the second
iteration and slightly decreased after the third itera-
tion due to overfitting. However, the final score was
0.244, which is almost the same as the peak score.
The test corpus BLEU score was lower than
the evaluation corpus BLEU score because the
rules used in the test corpus were not exhaustively
checked by the evaluation corpus. If the evaluation
corpus size could be expanded, the test corpus score
would improve.
About 37,000 sentences were translated on aver-
age in each iteration. This means that the time for
an iteration is estimated at about ten hours if trans-
lation speed is one second per sentence. This is a
short enough time for us because our method does
not require real-time processing. 2
6.3 MT Quality vs. Cleaning Methods
Next, in order to compare the proposed methods
with the previous methods, the MT quality achieved
by each of the following five methods was measured.
1. Baseline
The MT results using the base rule set.
2. Cutoff by Frequency
Low-frequency rules that appeared in the train-
ing corpus less often than twice were removed
from the base rule set. This threshold was
experimentally determined by the test corpus
BLEU score.
3. ?2 Test
The ?2 test was performed in the same manner
as in Imamura (2002)?s experiment. We intro-
duced rules with more than 95 percent confi-
dence (?2 ? 3.841).
4. Simple Feedback Cleaning
Feedback cleaning was carried out using the
evaluation corpus in Table 1.
5. Cross-cleaning
N-fold cross-cleaning was carried out. We ap-
plied five-fold cross-cleaning in this experi-
ment.
The results are shown in Table 2. This table shows
that the test corpus BLEU score and the subjective
2In this experiment, it took about 80 hours until convergence
using a Pentium 4 2-GHz computer.
Previous Methods Proposed Methods
Baseline Cutoff by Freq. ?2 Test Simple FC Cross-cleaning
# of Rules 105,588 26,053 1,499 99,368 82,462
Test Corpus BLEU Score 0.232 0.234 0.157 0.244 0.277
Subjective Quality +1.77% -6.67% +6.67% +10.0%
# of Improved Sentences 83 115 83 100
# of Same Quality 353 246 378 361
(Same Results) (257) (114) (266) (234)
# of Degraded Sentences 74 149 49 49
Table 2: MT Quality vs. Cleaning Methods
quality of the proposed methods (simple feedback
cleaning and cross-cleaning) are considerably im-
proved over those of the previous methods.
Focusing on the subjective quality of the proposed
methods, some MT results were degraded from the
baseline due to the removal of rules. However, the
subjective quality levels were relatively improved
because our methods aim to increase the portion of
correct MT results.
Focusing on the number of the rules, the rule
set of the simple feedback cleaning is clearly a lo-
cally optimal solution, since the number of rules
is more than that of cross-cleaning, although the
BLEU score is lower. In comparing the number of
rules in cross-cleaning with that in the cutoff by fre-
quency, the former is three times higher than the lat-
ter. We assume that the solution of cross-cleaning
is also the locally optimal solution. If we could find
the globally optimal solution, the MT quality would
certainly improve further.
7 Discussion
7.1 Other Automatic Evaluation Methods
The idea of feedback cleaning is independent of
BLEU. Some automatic evaluation methods of MT
quality other than BLEU have been proposed. For
example, Su et al (1992), Yasuda et al (2001), and
Akiba et al (2001) measure similarity between MT
results and the references by DP matching (edit dis-
tances) and then output the evaluation scores. These
automatic evaluation methods that output scores are
applicable to feedback cleaning.
The characteristics common to these methods, in-
cluding BLEU, is that the similarity to references
are measured for each sentence, and the evaluation
score of an MT system is calculated by aggregating
the similarities. Therefore, MT results of the eval-
uation corpus are necessary to evaluate the system,
and reducing the number of sentence translations is
an important technique for all of these methods.
The effects of feedback cleaning depend on the
characteristics of objective measures. DP-based
measures and BLEU have different characteristics
(Yasuda et al, 2003). The exploration of several
measures for feedback cleaning remains an interest-
ing future work.
7.2 Domain Adaptation
When applying corpus-based machine translation to
a different domain, bilingual corpora of the new do-
main are necessary. However, the sizes of the new
corpora are generally smaller than that of the orig-
inal corpus because the collection of bilingual sen-
tences requires a high cost.
The feedback cleaning proposed in this paper can
be interpreted as adapting the translation rules so
that the MT results become similar to the evaluation
corpus. Therefore, if we regard the bilingual corpus
of the new domain as the evaluation corpus and carry
out feedback cleaning, the rule set will be adapted to
the new domain. In other words, our method can be
applied to adaptation of an MT system by using a
smaller corpus of the new domain.
8 Conclusions
In this paper, we proposed a feedback cleaning
method that utilizes automatic evaluation to remove
incorrect/redundant translation rules. BLEU was
utilized for the automatic evaluation of MT qual-
ity, and the hill-climbing algorithm was applied to
searching for the combinatorial optimization. Uti-
lizing features of this task, incorrect/redundant rules
were removed from the initial solution, which con-
tains all rules acquired from the training corpus. In
addition, we proposed N-fold cross-cleaning to re-
duce the influence of the evaluation corpus size. Our
experiments show that the MT quality was improved
by 10% in paired comparison and by 0.045 in the
BLEU score. This is considerable improvement over
the previous methods.
Acknowledgment
The research reported here is supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled, ?A study of
speech dialogue translation technology based on a
large corpus.?
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of
Machine Translation Summit VIII, pages 15?20.
Osamu Furuse and Hitoshi Iida. 1994. Constituent
boundary parsing for example-based machine transla-
tion. In Proceedings of COLING-94, pages 105?111.
Kenji Imamura. 2001. Hierarchical phrase alignment
harmonized with parsing. In Proceedings of the 6th
Natural Language Processing Pacific Rim Symposium
(NLPRS 2001), pages 377?384.
Kenji Imamura. 2002. Application of translation knowl-
edge acquired by hierarchical phrase alignment for
pattern-based MT. In Proceedings of the 9th Confer-
ence on Theoretical and Methodological Issues in Ma-
chine Translation (TMI-2002), pages 74?84.
Arul Menezes and Stephen D. Richardson. 2001. A
best first alignment algorithm for automatic extrac-
tion of transfer mappings from bilingual corpora. In
Proceedings of the ?Workshop on Example-Based Ma-
chine Translation? in MT Summit VIII, pages 35?42.
Adam Meyers, Michiko Kosaka, and Ralph Grishman.
2000. Chart-based translation rule application in ma-
chine translation. In Proceedings of COLING-2000,
pages 537?543.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311?318.
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 1992.
A new quantitative quality measure for machine trans-
lation systems. In Proceedings of COLING-92, pages
433?439.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002),
pages 147?152.
Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Sei-
ichi Yamamoto, and Masuzo Yanagida. 2001. An au-
tomatic evaluation method of translation quality using
translation answer candidates queried from a parallel
corpus. In Proceedings of Machine Translation Sum-
mit VIII, pages 373?378.
Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Sei-
ichi Yamamoto, and Masuzo Yanagida. 2003. Appli-
cations of automatic evaluation methods to measuring
a capability of speech translation system. In Proceed-
ings of the 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), pages 371?378.
Taking Account of the User's View in 3D Multimodal Instruction 
Dialogue 
Yuk iko  I. Nakano and Ken j i  hnamura  and Hisash i  Ohara  
1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan 
{yukiko, i lnamura, ohara}@ntl;nly.isl.ntt.co.jp 
Abst ract  
While recent advancements in virtual reality technology 
have created a rich communication interface linking hu- 
mans and computers, there has beefl little work on build- 
ing dialogue systems for 3D virtual worlds. This paper 
proposes a method for altering the instruction dialogue 
to match the user's view in a virtual enviromnent. -\~re 
illustrate the method with the system MID-aD, which in- 
teractively instructs the user on dismantling some parts 
of a car. First, in order to change the content of ~he 
instruction dialogue to match the user's view, we extend 
the refinement-driven plmming algorithm by using the 
user's view as a l)lan constraint. Second, to manage the 
dialogue smoothly, the systeln keeps track of the user's 
viewpoint as part of the dialogue skate and uses this 
information for coping with interruptive sul)dialogues. 
These mechanisms enable MID-3D to set instruction di- 
alogues in an incremental way; it takes account of the 
user's view even when it changes frequently. 
1 I n t roduct ion  
In a aD virtual enviromnent, we can freely walk 
through the virtual space and view three di- 
mensional objects from various angles. A inul- 
tilnodal dialogue system for such a virtual en- 
vironment should ainl to realize conversations 
which are performed in the real world. It would 
also be very useflll for education, where it is 
necessary to learn in near real-life situations. 
One of the most significant characteristics of
3D virtual environments is that the user can se- 
lect her/his own view from whidi to observe the 
virtual world. Thus, the nmltimodal instruc- 
tion dialogue system should be able to set the 
course of the dialogue by considering the user's 
current view. However, previous works on nml- 
tilnodal presentation generation and instruc- 
tion dialogue generation (Wahlster et al, 1993; 
Moore, 1995; Cawsey, 1992) do not achieve this 
goal because they were not designed to hail- 
(lie dialogues pertbrmed in 3D virtual environ- 
ments .  
This paper proposes a method that ensures 
that the course of the dialogue matches the 
user's view in the virtual environment. More 
specificall> we focus on (1) how to select the 
contents of the dialogue since it is essential 
that the instruction dialogue system form a se- 
quence of dialogue contents that is coherent 
and comprehensible, and (2) how to control 
mixed-initiative instruction dialogues nloothly, 
especially how to manage interruptive subdia- 
logues. These two problelns basically determine 
the course of the dialogue. 
First, in order to decide the appropriate con- 
tent, we propose a content selection mechanism 
based on plan-based multilnodal presentation 
generation (Andrd and Rist, 1993; Wahlster et 
al., 1993). We extend this algorithm by using 
the user's view as a constraint in expanding the 
plan. In addition, by employing tilt incremen- 
tal planning algorithm, the syst;em can adjust 
the content o match the user's view during on- 
going conversations. 
Second, ill order to nlanage interruptive sub- 
dialogues, we propose a dialogue management 
mechanism that takes account of the user's 
view. This mechanism maintains the user's 
viewpoint as a dialogue state in addition to in- 
tentional and linguistic context (Rich and Sid- 
her, 1998). It maintains the dialogue state as a 
focus stack of discourse segments and updates 
it at each turn. Tlms, it Call track the view- 
point information in an on-going dialogue. By 
using this viewpoint inibrlnation in restarting 
the dialogue after an interruptive subdialogue, 
the dialogue Inai~agement medmnism returns 
the user's viewpoint o that of the interrupted 
segment. 
These two mechanisms work as a core dia- 
logue engine in MID-3D (Multimodal Instruc- 
tion Dialogue system for 3D virtual environ- 
ments). They make it possible to set the in- 
struction dialogue in an increnlental ww while 
572 
Figure 1: Right angle 
Figure 2: l,efl; angle 
considering the user's view. They also (mal)h'~ 
MID-a1) to (:re~te coherent and mixe, d-initiative 
(liah)gues in virtual enviromuents. 
This paper is organized as lbllows. In Sec- 
ti(m 2, we define the 1)rol)h;ms spc(:ifi(: 1;o 313 
multimoda\] (tiah)gne genera.tion. Section 3 de- 
scribes rclat;ed works. \ ]n S('x:l;ion 4, we pro- 
pose the MID-a1) architecture. Sections 5 ;rod 
6 des(:ril)e the contenl; plmming meclm.nism a.nd 
the dialogue manngement meclm.nism, a.nd show 
they dynami(:ally decide coherent insl;rn(:t;ions, 
and control mixed-initial;ire diah)guc.s consider- 
ing the user's view. V~/e also show a smnt)le di- 
:dogue in Section 7. 
2 Prob lems 
In a virtual emdromnent, the user can freely 
move a.round the world and select her/his own 
view. r\['he systelll C&llllOt; predict where the user 
will stand and what; s/he observes in the vir- 
tual environment. This section describes two 
types of 1)roblems in generating instru(:tion dia- 
logues ibr such virtual enviromnents. They arc 
caused l)y mismatches b(~,twe(;ll tile llSel'~S vi0,w- 
l)oint ;m(1 the sta.te of th(; dialogue. 
First, the syStelll shouM check whether the 
user's view matches the tbcns of the next ex- 
change when the systen~ tries to ('hange COllllllll- 
ni('ative goals. \]if a mismatch occurs, the system 
shouhl choose the instru(:tion (li~dogue content 
according to the user's view. Figure 1 a,n(1 2 m:e 
examl)les of observing a car's front suspension 
from (liff(',r(mt, points of view. In Figm'(', 1, the 
right; side of the steering system can 1)e seen, 
while Figure 2 shows the left side. If the system 
is not aware of the user's view, I;he system may 
talk about the left; tie rod end even though the 
user's view remains the right side (Figure 1). 
In such n (:ase, the system shouM chang(: its d(> 
scril)tion or ask the user to change her/his view 
to |;11('. left; side. view (Figure 2) and r('.(-Olmnen(:e 
its instruction hi)out this part. Therefore, the 
system should be al)le to change the content 
of the dialogue according 1;o the user's view. 
In order to ac(:omplish this, the system shoul(1 
lmve ;1. content selection nlechan.ism whi(:h in- 
crementally (let:ides i;h('~ content while ('he(:king 
the llSef~s (;llrrellt vi(!w. 
Second, t;here could 1)e a case in which 1;21(; 
user chang(~s 1;he, topi(: as well as the vie\vl)oillt 
as interrupl;ing the. system's instru('t;ion, i n such 
a case, the (tia.h)gue~ system shouhl kee l) track of 
the user's viewpoint as ~ 1)art of the dialogue 
state nnd return to that viewpoint when resmn- 
ing the (lia.logu(? after the interrupl;ing sul)(li- 
alogue. Sul)l)ose that while the sys|;em is (',x- 
l)lnining tlm right; t)i(; rod end, th('. user initially 
looks a,t the right side, (l"igure 1) hut then shifts 
her/his view to the left (Figure 2) and asks 
about the \]eft knu(-kle arm. After finishing a 
sub(lialogue about this arm, the syst(;nl tries 
to return to the dialogue al)out the interrupted 
topic. At this time, if the sysl;em resumed the 
dialogue using the current view (Figure 2), the 
view and the instruction would \])e(;olne mis- 
matched. When resmning the interrupted i- 
alogue, it would be less (:onfllsing to the user 
if the system retm:ned to the user's prior view- 
l)oint rather than selecting n new o11o. '\].'he user 
may be (:onfilsed if the dialogue is resulned but 
the observed state looks different. 
\,Ve address the ~fl)ove problems. In order to 
(:ope wit;h the first; problem, we present a con- 
tent selection mechanism that incrementally ex- 
pands the content plan of a multimodal dialogue 
while checking the user's view. To solve the 
second 1)roblem, we present a. dialogue nmnage- 
merit me(:\]mnism l;hat keel)s t:ra(-k of the user's 
viewpoint as a part of the diah)gue context and 
573 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  -: 
~; '~ Operation l,uttons 
Figure 3: The system architecture 
uses this intbrmation in resuming the dialogue 
after interruptive subdialogues. 
3 Re la ted  work  
There are many multimodal systems, such as 
nmltimedia presentation systems and animated 
agents (Mwbury, 1993; Lester et al, 1997; 
Bares and Lester, 1997; Stone and Lester, 1996; 
Towns et al, 1998)~ all of which use 3D graph- 
ics and 3D animations. In some of them (May- 
bury, 1993; Wahlster et al, 1993; Towns et 
al., 1998), planning is used in generating mul- 
timodal presentations including graphics and 
animations. They are similar to MID-aD in 
that they use planning mechanisms in content 
planning. However, in presentation systems, 
unlike dialogue systems, the user just watches 
the presentation without changing her/his view. 
Therefore, these studies are not concerned with 
dlanging the content of the discourse to match 
the user's view. 
In some studies of dialogue management 
(Rich and Sidner, 1998; Stent et M., 1999), 
the state of the dialogue is represented using 
Grosz and Sidner's framework (Grosz and Sid- 
ner, 1986). We also adopt this theory in our di- 
alogue management mechanism. However, they 
do not keep track of the user's viewpoint infor- 
mation as a part of the dialogue state because 
they were not concerned with dialogue manage- 
ment in virtual environments. 
Studies on pedagogical agents have goals 
closer to ours. In (Rickel and .\]ohnson, 1999), 
a pedagogical agent demonstrates the sequen- 
tial operation of complex machiuery and an- 
swers some follow up questions fl'on~ the stu- 
dent. Lester et al (1999) proposes a life- 
like pedagogical agent that supports problem- 
solving activities. Although these studies are 
concerned with building interactive learning en- 
vironments using natural anguage, they do not 
discuss how to decide the course of on-going in- 
struction dialogues in an incremental nd coher- 
ent way. 
4 Overview of the System 
Arch i tec ture  
This section describes the architecture of MID- 
3D. This system instructs users how to disman- 
tle the steering system of a cal'. Tile system 
steps through the procedure and the user can 
interrupt he system's instructions at any time. 
Figme 3 shows the architecture and a snapshot 
of the system. The 3D virtual environment is
viewed through an application window. A 3D 
model of a part of the car is provided and a frog- 
574 
like character is used as the pedagogical agent 
(Johnson et al, 2000). The user herself/himself 
Call also al)l)ear in the virtual enviromn(mt as 
an avatar. The buttons to the right of the 3D 
scre(m are operation 1)uttons tbr changillg the 
viewpoint. By using these buttons, the user can 
freely change her/his viewt)oint at any time. 
This system consists of five main modules: 
hll)Ut Analyzer, Domain Plan Reasoner, Con- 
tent Planner (CP), Sentence Planner, Dialogue 
Manager (DM), and Virtual Environment Con- 
troller. 
First of all, the user's inputs are interpreted 
through the Input Analyzer. It receives trings 
of characters from the voice recognizer and 
the user's inputs ti'om the Virtual Environment 
Controller. It interl)rets these inputs, trans- 
forms them into a semantic reprcsentation~ and 
sends them to the DM. 
The DM, working as a dialogue management 
mechanism, keeI)s track of the dialogue (:ontext 
including the user:s view and decides the, next 
goal (or a(:tion) of the system. Ut)on receiv- 
ing an intmt from the user through the Input 
Analyzer, the DM sends it to the l)omaill Plan 
Reasoner (DPR) to get discourse goals for re- 
st)onding to the inlmt. For example, if th(: user 
requests ome instruction, the DI'I{ decides the 
sequence of steps that realizes the l)rocedure 1)y 
refi~rring to domain knowh~dge. Th(: 1354 (;hen 
adds (;he discourse goals to the goal agenda. 
If the user does not sulmlit a ~lew (;ot)ie , the 
DM (:ontilmes to expand the, instruction plan 
1)y sending a goal in the goal agenda to (:lie CP. 
Details of the I)M are given in Section 6. 
After the goal is sent to the CP, it decides the 
apl)ropriate contents of instruction dialogue by 
eml)loying a refinement-driven hierar(:hi(:al lin- 
ear 1)lamfing technique. When it; receives a goal 
fl'om the DM, it exl)ands the goal and returns 
its sul)goal to the DM. 13y ret)eating this pro- 
cess, the dialogue contents are, gradually spec- 
ified. Theretbre, the CP provides the scenario 
tbr the instruction 1)ased on the control 1)rovided 
by the DM. Details of the CP are provided in 
Section 5. 
The Sentence Plalmer generates urface, lin- 
guisti(: expressions coordinated with action 
(Kato et al, 1996). The linguistic exl)ressions 
arc. output through a voice synthesizer. Actions 
;/re realized through the Virtual Enviromnent 
Controller as 3D animation. 
For the Virtual Environment Controller, we 
use HyCLASS (Kawanol)e et al, 1998), which 
<Operator 1> 
(:tleader 
:Iiftbcl 
:Constraints 
:Main-Acts 
:Subskliary-Acts 
<Operator 2> 
(:lleader 
:Effect 
:Conslraints 
:Main-Acts 
:Subsidiary-Acts 
(Inshuct-act N l l ?act MM) 
(BMB S 11 (Goal II (Done 11 ?act))) 
((KB (Obj ?act ?object)) 
(Visible-p (Visible ?ol~iect t))) 
((Look S II) 
(Request S I I (Try It (action ?act)) NO-SYNC MM)) 
((Describe- act S II ?act MM) 
(Reset S (actioll ?act)))) 
(Instruct-act S 11 ?act MM) 
(BMB S 11 (Goal I1 (Done 11 '?act))) 
((KB (Obj ?act ?object)) 
(Visiblc-p (Visible ?object oil))) 
((Look S ll) 
(Make-recognize S 11 (Object ?object) MM) 
(Rcqucst S 11 (Try I1 (action ?act)) NO-SYNC M M)) 
((l)escribc-act S 11 ?act MM) 
(Reset S (action '?act)))) 
Figure 4: Exanlt)les of Content Plan Operators 
is a 3D simulation-1)ased nvironment tbr edu- 
(:ational activities. Several APls are provided 
tbr controlling HyCLASS. By using these in- 
terfaces, the CP and the DM can discern the 
liser~s view and issue an action command in ()l'- 
der to challge the virtual (;nvironnmllt. \?h(m 
HyCLASS receives an action command, it in- 
terprets the command and renders the 31) ani- 
mation corresponding to the action in real time. 
5 Se lec t ing  the  Conten(;  o f  
I ns t ruct ion  D ia logue  
Ill this section, we introduce the CP and show 
how the instruction dialogue is (leeided in all 
in(:renl(:ntal way to ma, tch the user's view. 
5.1 Content  P lanner  
In MID-3D, the CP is (:ailed by the DM. Wheal 
a goal is put to the CP fl'(nn the DM, it; selects a 
plan operator fi)r achieving the goal, applies the 
ol)erator to lind new subgoals, and returns them 
to l;he \])M. The sul)goals are then added to the 
goal agenda maintained by the DM. Theretbre, 
the CP provides the seenm:io tbr the instruc- 
tion dialogue to the DM and enables MID-3D 
to output coherent instructions. Moreover, the 
Content Planer emt)loys depth-first search with 
a retinement-drivell hierarchical linear plmming 
algorithm as in (Cmvsey, 1992). The advantage 
of this method is that the t)lan is de, veloped in- 
crenmntally, and can be changed while the con- 
versation is in progress. Thus, by aI)plying this 
algorithm to 3D dialogues, it be(-omes lmssible 
to set instruction dialogue strategies that are 
contingent on the user's view. 
575 
5.2 Considering the User's View in 
Content  Se lect ion 
In order to decide the dialogue content accord- 
ing to tile user's view, we extend the descrip- 
tion of the content plan operator (Andrd and 
Rist, 1993) by using the user's view as a con- 
straint in plan operator selection. We also mod- 
ify the constraint checking flmctions of |;lie pre- 
vious planning algorithm such that HyCLASS 
is queried about the state of the virtual envi- 
ronment. 
Figure 4 shows examples of content plan op- 
erators. Each operator consists of the name 
of the operator (Header), the etfcct resulting 
from plan execution (Effect), the constraints for 
executing the plan (Constraints), the essential 
subgoals (Main-acts), and the optional subgoals 
(Subsidiary-acts). As shown in {Operator 1.) 
in Figure 4, we use the constraint (gisible-p 
(Visible ?object t)) to check whether the 
object is visible fl'om tile user's viewpoint. 
Actually, the CP asks HyCLASS to examine 
whether the object is in the student's field of 
view. 
If an object is bound to the ?ob jec t  vari- 
able by rel~rring to the knowledge base, and 
the object is visible to the user, (Operator 1) 
is selected. As a result, two Main-Acts (look- 
ing at the, user and requesting to try to do 
the action) and two Subsidiary-Acts (showing 
how to do the action, then resetting the state) 
are set as subgoals and returned to the DM. 
In contrast, if l;he object is not visible to the 
user, {Operator 2} is selected. In this case, a 
goal for making the user i(tenti(y the object is 
added to the Main-Acts; (Hake-recognize S 
H (Object ?object) MM). 
As shown al)ove, the user's view is considered 
in deciding the instruction strategy. In addition 
to the above example, the distance between the 
target object and the user as well as three di- 
mensional overlapping of objects, can also be 
considered as constraims related to the user's 
view. 
Although the user's view is also considered in 
selecting locative expressions of objects in the 
Sentence Planner in MID-3D, we do not discuss 
this issue here becanse surface generation is not 
the tbcus of this paper. 
6 Manag ing  I n ter rupt ive  
Subd ia logue  
The DM controls the other components ofMID- 
3D based on a discourse model that represents 
the state of tile dialogue. This section describes 
the DM and shows how the user's view is used 
in managing the instruction dialogue. 
6.1 Maintaining the  D iscourse  Mode l  
The DM maintains a discourse model for track- 
ing the state of the dialogue. The discourse 
model consists of the discourse goal agenda 
(agenda), focus stack, and dialogue history. The 
agenda is a list of goals that should be achieved 
through a dialogue between the user and the 
system. If all the goals in the agenda re accom- 
plished, the instruction (tialogue finishes suc- 
cessflflly. The focus stack is a sta& of discourse 
segment frames (DSF). Each DSF is a frmne 
structure that stores the tbllowing inlbrmation 
as slot vMues: 
utterance content (UC): A list of utter- 
ance contents constructing a discourse segment. 
Physical actions are also regarded as uttcra.nce 
contents (D;rguson and Allen, 1998). 
discourse purpose (1)19: The purt)ose of a dis- 
course segment. 
- 9oal state (GS): A state (or states) whi('h 
shouhl 1)e accomplished to achieve the discourse 
lmrpose of the segment. 
In addition to these, we add the user's view- 
point slot to the DSF description in order to 
track the user's viewl)oint information: 
user's vic.'wpoint (UV): Current user's view- 
point, which is represented as the position and 
orientation of the camera. The position consists 
of x-, y-, and z-coordinates. The orientation 
consists of x-, y-, and z-angles of the ('amera. 
The basic algorithm of the DM is to repeat 
(a) th(; peribnning actions step and (1)) updat- 
ing the discourse model, until there is no un- 
satisfied goal in the agenda (~IYaum, 1994). In 
1)ertbrming actions step, the DM decides what 
to do next ill the current dialogue state, an(1 
then pertbnns the action. When continuing the 
system explanation, the DM posts the first goal 
in the agenda to the CP. If the user's response 
is needed in the current state, the 1)M waits tbr 
the nser's input. 
The other step in the DM algorith.m is to up- 
date the discourse model according to the state 
that results from the actions pertbrmed by the 
user as well as the actions peribrmed by the sys- 
tem. Although we do not detail this step here, 
the tbllowing operations could be executed e- 
pending on the case. if the current discourse 
purpose is accomplished, the top level DSF is 
popped and added to the dialogue history, q_/he 
576 
l I)SFI21 
DSFI2 
DSFI Jf J J 
UV: ((18, -20, -263) (0, 0.3 I, 0)) 
UC: ((IJseJ~act (Ask where heal_r)) 
I)P: (Response-to-user-act 
(Uscr-act (ask where bootr))) 
GS: ((Know 11 (About (l'lace_of boot_r)))...) 
UV: ((-38, -22, -259) (0, -0.33, 0)) 
UC: ((System-act (lnl'(~rm S 11 (Show S (Action 
rcmovc-tiemd end.I)) NO-SYNC I'R)) 
DI': (I)cscribe-acl S l I rcmove-licrod end I)) 
GS: ((Know 1I (llove-lo-do 11 
(action remove-tiered eml I)))...) 
Figure 5: Example of the state of a dialogue 
system then assunms that the user understands 
the instruction and adds the assumption to the 
user model. If a new discourse 1)urpose is in- 
troduced from the CP, the I)M creates a new 
DSF by setting the header of the selected plan 
operator in the discourse lmrpose slot mM the 
effi~ct of the operator in the goal state slot. The 
DSF is then trashed to the tbcus stack. If the 
current discourse purpose is contimmd, the DM 
updates the information of the top level DSF. 
6.2 Cons ider ing  the  User 's  V iew in 
Coping wi th  Interrupt ive 
Subd ia logues  
The main ditlbxence of the Dialogue Manager of 
our system from the i)revious one is to maintain 
the user's viewpoint information and use this in 
managing the dialogue. When the DM updates 
the information of the current DSt i', it observes 
the user's viewi~oint at  that petal; and renews 
the UV slot and it also adds the sema.nl;ic rep- 
resentation of utterance (or action) in the UC 
slot. As a result, it becomes possible to update 
the user's viewpoint information at each turn, 
and to track the user's viewl)oint in an on-going 
dialogue. 
By using this mechanism, the DM can cope 
with interruptive subdialognes. In resmning 
from a subdialogue, the user may become con- 
timed if the dialogue is resumed but the ob- 
served state differs from what the user relllelll- 
hers. In order to match the view to the re- 
sumed dialogtm, the I)M refers the UV slot of 
the top DSF and puts the users view ha& to 
that point. This ensures that the user experi- 
ences a smooth transition back to the previous 
topic. Figure 5 shows an example of the state of 
a dialogue. DSF12 represents a discourse seg- 
ment that describes how to remove the left tie 
rod end. DSF121 represents the user-initiated 
interrul)tive subdialogue about where the right 
\[14\]System: Theleft knuckle arm is removed like this. 
(with the anilnation showing the left knuckle 
arm coming off) 
\[ 15\]User: (After moving the viewpoint to Figure I and 
clicking the right knuckle ann) 
What is this? 
\[16\]System: This is the right knuckle arm. 
\[I 7\]Uscr: OK. 
\[18\]Systeln: Now, let's continue the explanation. 
(with changing the view to the one in utterance \[ 14\]) 
\[19\]System: Theleft knuckle arm is removed like this. 
(with the animation showing the left knuckle arm 
coming off) 
\[20\]User: Yes. 
\[21\]System: Now, try removing the left knuckle arm. 
\[22\]User: (After clicking the left knuckle arm) Remove it. 
\[23\]System: Youhave succcssfitlly completed thc operation. 
\[24\]System: Thenext step is to remove the right ie rod cad. 
\[25\]System: Let's move to the right ie rod end. 
1126\] User: Ycs. 
\[27\]System: (Moving the view to the right) 
The right ie rod end is in fiont of you. 
\[28\] User: Yes, 1 see. 
Figure 6: Example of a dialogue with MII)-3D 
boot is. hmnediately before starting DSF\]21, 
the user's viewpoint in l)SF12 is ((-38, -22, 
-259) (0, -0.33, 0)). After completing the 
subdialogue \])y answering the user's question, 
DSF121 is l)opped and the system resmnes 
DSF12. At this time, the \])M gets the view- 
point value of the top DSF (DSF12), alld (;Oltl- 
mands ItyCLASS to change the viewpoint to 
that view, which is in this case ((-as, -22, -2,59) 
(0, -0.a3, 0)) ' The systeln then restarts the 
interrupted ialogue. 
7 Exmnple  
In order to illustrate the behavior of MID-3D, 
an example is shown in Figure 6. This is a part 
of an instruction dialogue on how to dismantle 
the steering system of a car. The current topic 
is removing the left knuckle arm. In utterance 
\[14\], the system describes how to remove this 
part in conjunction with an animation created 
by HyCLASS. 
In \[15\], the user interrupted the system's in- 
struction and asked "What is this?" by clicking 
the right knuckle arm. At this point, the user's 
speech input was interpreted in the Input An- 
~In the current system, it; is not 1)ossible to move 
the camera to an arbitrary point because of the limi- 
tations of the virtual environment controller employed. 
Accordingly, this func|;ion is al)proximated by selecting 
the nearest of several predetined viewpoints. 
577 
alyzer and a user initiative subdialogue started 
by t)ushing another DSF onto the focus stack. 
In order to answer the question, the DM asked 
the Domain Plan Reasoner how to answer the 
user's question. As a result, a discourse goal was 
returned to the DM and added to the agenda. 
The DM then sent the goal (Describe-name S 
H (object  knuckle_arm_r)) to the CP. This 
goal generated utterance \[16\]. 
In system utterance \[18\], in order to resume 
the dialogue, a recta-comment, "Now let's con- 
tinue the explanation", was generated and the 
viewpoint returned to the previous one in \[14\] 
as noted in the DSF. After returning to the pre- 
vious view, the interrupted goal was re-planned. 
As a result, utterance \[19\] was generated. 
After completing this operation in \[23\], 
the next step, removing the right tie rod 
end, is started. At this time, if the 
user is viewing the left side (Figure 2) and 
the system has the goal ( Ins t ruct -ac t  S 
H remove-tierod_end_r MR), (Operator 2} in 
Figure 4 is applied because the target object, 
right tie rod end, is not visible fi'om the user's 
viewpoint. Thus a goal of making the user view 
the right tie rod end is added as a subgoal and 
utterances \[24\] and \[25\] are generated. 
8 Discuss ion  
This paper proposed a inethod tbr altering in- 
struction dialogues to match the user's view in 
a virtual enviromnent. We described the Con- 
tent Planner which can incrementally decide co- 
herent instruction dialogue content to match 
changes in the user's view. We also presented 
the Dialogue Manager, which can keep track 
of the user's viewpoint in an on-going dialogue 
and use this intbrmation i resuming from inter- 
ruptive subdialogues. These mechanisms allow 
to detect mismatches between the user's view- 
point and the topic at any point in the dialogue, 
and then to choose the instruction content and 
user's viewpoint appropriately. MID-3D, an ex- 
perimental system that uses these mechanisms, 
shows that the method we proposed is effective 
in realizing instruction dialogues that suit the 
user's view in virtual enviromnents. 
Re ferences  
Elisabeth Andr6 and Thmnas Rist. 1993. The design of 
il lustrated ocuments as a planning task. In Mark T. 
Maybury, editor, Intelligent Multimedia Interfaces, 
pages 94-116. AAAI Press / The MIT Press. 
Will iam H. Bares and James C. Lester. 1997. Real- 
time generation of customized 3D animated explana- 
tions for knowledge-based learning environments. In 
AAAI97, pages 347-354. 
Alison Cawsey. 1992. Explanation and Interaction: The 
Computer Generation of Expalanatory Dialogues. The. 
MIT Press. 
George Ferguson and James F. Allen. 1998. TRIPS: 
An integrated intelligent problem-solving assistant. 
In AAAI98, pages 567-572. 
Barbara J. Orosz and Candace L. Sidner. 1986. Atten- 
tion, intentions, and the structure of discourse. Com- 
putational Linguistics, 12(3):175-204. 
W. Lewis Johnson, Jeff W. Rickel, and James C. Lester. 
2000. Animated pedagogical agents: Face-to-face in- 
teraction in interactive learning environments. Inter- 
national Journal of Artificial InteUigencc in Educa- 
tion. 
Tsuneaki Kato, Ynkiko I. Nakano, Hideharu Nakajima, 
and Takaaki Hasegawa. 1996. Interactive mnltimodal 
explanations and their temporal coordination. In 
ECAI-96, pages 261-265. John Willey and Sons Lim- 
ited. 
Akihisa Kawanobe, Susumn Kakuta, Hirofumi Touhei, 
and Katsumi Hosoya. 1998. Preliminary report 
on HyCLASS anthoring tool. In ED-MEDIA/ED- 
TELECOM. 
James C. Lester, Jennifer L. Voerlnan, Stuart O. Towns, 
and Charles B. Callaway. 1997. Cosmo: A lih;-like 
animated pedagogical gent witl, deictie believability. 
In IJCAI-97 Workshop, Animated Interface Agent. 
Jmnes C. Lester, Brian A. Stone, and Gray D. Stelling. 
1999. Lifelike pedagogical gents for mixed-initiative 
problem solving in constructivist learning environ- 
ments. User Modeling and User-Adapted Interaction, 
9(1-2):1-44. 
Mark T. Maybury. 1993. Planning multimedia explana- 
tion using communicative acts. In Mark T. Maylmry, 
editor, Intelligent Multimedia Interfaces, pages 59 -74. 
AAAI Press / The MIT Press. 
Johamm D. Moore. 1995. Participating in Explanatory 
Dialogues: Interpreting and I~esponding to Questions 
in Context. MIT Press. 
Chm'les Rich and Candace L. Sidner. 1998. COLLA- 
GEN: A collaboration manager for software interfhce 
agents. User Modeling and User-Adapted Interaction, 
8:315-350. 
Jeff W. Rickel and W. Lewis Johnson. 1999. Animated 
agents for procedual training in virtual reality: Per- 
ception, cognition and motor control. Applied Artifi- 
cial Intellifence, 13:343-392. 
Amanda Stent, John Dowding, Jean Mark Gawron, Eliz- 
abeth Owen Brat, and Robert Moore. 1999. The 
CommandTalk spoken dialogue systeln. In AC'Lgg, 
pages 183-190. 
Brian A. Stone and James C. Lester. 1996. Dynami- 
cally sequencing an animated pedagogical agent. In 
AAAI96, pages 424-431. 
Stuart G. Towns, Charles B. Callaway, and 3anles C. 
Lester. 1998. Generating coordinated natural lan- 
guage and 3D animations for complex spatial expla- 
nations. In AAAI98, pages 112-119. 
David R. Traum. 1994. A Computational Theory of 
Grounding in Natural Language Conversation. Ph.D. 
thesis, University of Rochester. 
Wolfgang \Vahlster, Elisabcth Andr6, Wolfgang Fin- 
kler, Hans-Jiirgen Profitlieh, and Thomas Rist. 1993. 
Plan-based integration of natural anguage and graph- 
ics generation. Artificial Intelligence, 63:387-427. 
578 
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 168?176,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
 Tag Confidence Measure for Semi-Automatically Updating  
Named Entity Recognition 
Kuniko Saito and Kenji Imamura 
NTT Cyber Space Laboratories, NTT Corporation 
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan 
{saito.kuniko, imamura.kenji}@lab.ntt.co.jp 
 
 
Abstract 
We present two techniques to reduce ma-
chine learning cost, i.e., cost of manually 
annotating unlabeled data, for adapting 
existing CRF-based named entity recog-
nition (NER) systems to new texts or 
domains. We introduce the tag posterior 
probability as the tag confidence measure 
of an individual NE tag determined by 
the base model. Dubious tags are auto-
matically detected as recognition errors, 
and regarded as targets of manual correc-
tion. Compared to entire sentence poste-
rior probability, tag posterior probability 
has the advantage of minimizing system 
cost by focusing on those parts of the 
sentence that require manual correction. 
Using the tag confidence measure, the 
first technique, known as active learning, 
asks the editor to assign correct NE tags 
only to those parts that the base model 
could not assign tags confidently. Active 
learning reduces the learning cost by 
66%, compared to the conventional 
method. As the second technique, we 
propose bootstrapping NER, which semi-
automatically corrects dubious tags and 
updates its model.  
1 Introduction 
Machine learning, especially supervised learning, 
has achieved great success in many natural lan-
guage tasks, such as part-of-speech (POS) tag-
ging, named entity recognition (NER), and pars-
ing. This approach automatically encodes lin-
guistic knowledge as statistical parameters 
(models) from large annotated corpora. In the 
NER task, which is the focus of this paper, se-
quential tagging1 based on statistical models is 
                                                          
1Tags are assigned to each input unit (e.g., word) one by one. 
similarly used; studies include Conditional Ran-
dom Fields (CRFs; Lafferty et al, 2001, Suzuki 
et al, 2006). However, the manual costs incurred 
in creating annotated corpora are extremely high.  
On the other hand, Consumer Generated Me-
dia (CGM) such as blog texts has attracted a lot 
of attention recently as an informative resource 
for information retrieval and information extrac-
tion tasks. CGM has two distinctive features; 
enormous quantities of new texts are generated 
day after day, and new vocabularies and topics 
come and go rapidly. The most effective ap-
proach to keep up with new linguistic phenom-
ena is creating new annotated corpora for model 
re-training at short intervals. However, it is diffi-
cult to build new corpora expeditiously because 
of the high manual costs imposed by traditional 
schemes.  
   To reduce the manual labor and costs, vari-
ous learning methods, such as active learning 
(Shen et al, 2004, Laws and Sch?tze, 2008), 
semi-supervised learning (Suzuki and Isozaki, 
2008) and bootstrapping (Etzioni, 2005) have 
been proposed. Active learning automatically 
selects effective texts to be annotated from huge 
raw-text corpora. The correct answers are then 
manually annotated, and the model is re-trained. 
In active learning, one major issue is data selec-
tion, namely, determining which sample data is 
most effective. The data units used in conven-
tional methods are sentences. 
   Automatically creating annotated corpora 
would dramatically decrease the manual costs. In 
fact, there always are some recognition errors in 
any automatically annotated corpus and the edi-
tor has to correct errors one by one. Since sen-
tences are used as data units, the editor has to pay 
attention to all tags in the selected sentence be-
cause it is not obvious where the recognition er-
ror is. However, it is a waste of manual effort to 
168
annotate all tags because most tags must be la-
beled correctly by the base model2. 
In this paper, we propose a confidence meas-
ure based on tag posterior probability for the 
NER task. Our method does not use the confi-
dence of a sentence, but instead computes the 
confidence of the tag assigned to each word. The 
tag confidence measure allows the sentence to 
which the base model might assign an incorrect 
tag to be selected automatically. Active learning 
becomes more efficient because we correct only 
those tags that have low confidence (cf. Sec. 4). 
We can realize the same effect as active 
learning if we can automatically correct the se-
lected data based upon our tag confidence meas-
ure. Our proposal "Semi-Automatically Updating 
NER" automatically corrects erroneous data by 
using a seed NE list generated from other infor-
mation sources. Semi-Automatically Updating 
NER easily keeps up with new words because it 
enables us to update the model simply by provid-
ing a new NE list (cf. Sec. 5). 
2 Named Entity Recognition Task 
The NER task is to recognize entity names such 
as organizations and people. In this paper, we use 
17 NE tags based on the IOB2 scheme (Sang and 
De Meulder, 1999) combined with eight 
Japanese NE types defined in the IREX 
workshop (IREX 1999) as shown in Table 1. 
For example, ??? (Tokyo)/? (City)/?
(in)? is labeled like this: 
???/B-<LOC> ?/I-<LOC> ?/O?. 
This task is regarded as the sequential tagging 
problem, i.e., assigning NE tag sequences 
nttT L1= to word sequences nwwW L1= .  
Recently, discriminative models such as 
Conditional Random Fields (CRFs) have been 
successfully applied to this task (Lafferty et al, 
2001). In this paper, we use linear-chain CRFs 
based on the Minimum Classification Error 
framework (Suzuki et al, 2006). The posterior 
probability of a tag sequence is calculated as 
follows: 
))},,(
),((exp{
)(
1
)|(
1
1
iibb
b
iiaa
a
n
i
ttf
wtf
WZ
WTP
?
=
??+
??= ?
?
?
 (1) 
where iw  and it are the i-th word and its 
corresponding NE tag, respectively. ),( iia wtf  
                                                          
2 A base model is the initial model trained with the initial 
annotated corpora. 
and ),( 1 iib ttf ? is a feature function 3 . a? and 
b? is a parameter to be estimated from the 
training data. Z(W) is a normalization factor 
over all candidate paths expressed as follows: 
))}.,(
),((exp{)(
1
1
iibb
b
iiaa
a
n
iT
ttf
wtfWZ
?
=
??+
??= ??
?
?
(2) 
The best tag sequence that maximizes Formula 
(1) is located using the Viterbi algorithm. 
 
Table 1.  NE Types and Tags. 
NE Types NE Tags 
PERSON B-<PSN> I-<PSN> 
LOCATION B-<LOC> I-<LOC> 
ORGANIZATION B-<ORG> I-<ORG> 
ARTIFACT B-<ART> I-<ART> 
DATE B-<DAT> I-<DAT> 
TIME B-<TIM> I-<TIM> 
MONEY B-<MNY> I-<MNY> 
PERCENT B-<PCT> I-<PCT> 
outside an NE O 
 
3 Error Detection with Tag Confidence 
Measure 
3.1 Tag Posterior Probability 
It is quite natural to consider sentence posterior 
probability as a confidence measure of the esti-
mated tag sequences. We focus on tag posterior 
probability, and regard it as the confidence 
measure of the decoded tag itself. Our method 
tries to detect the recognition error of each tag by 
referring to the tag confidence measure. 
Figure 1 overviews the calculation of tag 
confidence measure. The confidence score of tag 
ji,t , which is a candidate tag for word iw , is 
calculated as follows: 
   ,W)|T,P(t=W)|P(t T
ji,ji, ?             (3) 
where ?T ji, W)|T,P(t is the summation of all NE 
tag sequences that pass through ji,t . This prob-
ability is generally called the marginal probabil-
ity. k,=j L1,  represents the number of NE 
tags shown in Table 1( i.e., k=17 in this paper). 
The tag confidence score of ji,t  can be cal-
culated efficiently using forward and backward 
                                                          
3 We used n-grams (n=1, 2, 3) of surface forms and parts-
of-speech within a five word window and 2-gram combina-
tions of NE tags as the feature set. 
169
Figure 1. Overview of the tag confidence measure calculation. 
The W ord  Sequence
The Tag C andidates
1w 12 ?iww K iw ni ww K1+
<s> 1,1t 1,11,2 ?itt K 1,it 1,1,1 ni tt K+ </s>
jt ,1 jnji tt ,,1 K+
M M M M M M
jij tt ,1,2 ?K
MM
kt ,1 kit , knki tt ,,1 K+kik tt ,1,2 ?K
jit ,
ji ,? ji ,?
algorithms as follows (Manning and Sch?tze, 
1999): 
,??
Z(W)
=W)|P(t ji,ji,ji, ?1            (4) 
where 
)}},,(
),(exp{{
1
,1,
iibb
b
iiaa
a
k
kiji
ttf
wtf
?
?
??+
???=?
?
???
   (5) 
)}},,(
),(exp{{
1
11,1,
+
+++
??+
???=?
iibb
b
iiaa
a
k
kiji
ttf
wtf
?
???
 (6)  
1,0, =? j                            (7) 
1.1, =? j+n                           (8) 
In this manner, the confidence scores of all 
tags of each word in a given sentence are calcu-
lated. The rejecter then refers to the highest tag 
confidence score in judging whether the decoded 
NE tag is correct or incorrect. 
3.2 Rejecter 
The rejecter tries to detect dubious tags in the 
NER result derived by the method described in 
Section 2. For each word, the rejecter refers to 
the decoded tag td, which maximizes Formula (1), 
and the most confident tag t1, in terms of the pos-
terior probability as defined in Formula (4). The 
judgment procedure is as follows: 
 
[1] If td is NOT identical to t1, then td is deter-
mined to be dubious, and so is rejected as an 
incorrect tag.4  
[2] Else, if the confidence score of t1, called cs1, 
is below the predefined threshold, td is de-
termined to be dubious, and so is rejected as 
an incorrect tag. 
[3] Otherwise, td is accepted as a correct tag. 
 
                                                          
4 The decoded tag td rarely disagrees with the most confi-
dent tag t1 due to a characteristic of the CRFs. 
Increasing the threshold also increases the 
number of rejected tags and manual annotation 
cost. In practice, the threshold should be empiri-
cally set to achieve the lowest judgment error 
rate using development data. There are two types 
of judgment errors: false acceptance and false 
rejection. False rejection is to reject a correct tag, 
and false acceptance is to accept an incorrect tag 
in error. The judgment error rate is taken as the 
ratio of these two types of errors in all instances. 
4 Active Learning 
Tag-wise recognition error detection is also help-
ful for data selection in active learning. If a sen-
tence contains several rejected tags, it contains 
some new information which the base model 
does not have. In other words, this sentence is 
worth learning. Our approach, then, is to base 
data selection (sentence selection) on the pres-
ence of rejected tags. However, it is not neces-
sary to check and correct all tags in each selected 
sentence. We only have to check and correct the 
rejected tags to acquire the annotated sentences. 
Figure 2 shows our active learning scheme. 
Model 
Re-training
Selected Data
Base Data
(Labeled)
Additional Data
(Unlabeled)
Base
Model
Recognition Error Detector
Morphological
Analyzer
NER Decoder
Calculation of the
Tag Confidence Measure
Data Selection
Manually 
Corrected Data
Updated
Model
Model Learning
Correct the Rejected 
Tags by Hand
Rejecter
Figure 2. Active Learning Scheme 
170
Figure 3. Learning Curves. 
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0 0.2 0.4 0.6 0.8 1
Word Check Rate
F-
m
ea
su
re
Tag Base(proposed)
Sentence Base
First, the NER decoder assigns an NE tag to each 
word5 of the additional data using the base model 
trained with the base data. The recognition error 
detector then determines whether each tag can be 
confidently accepted as described in Section 3. In 
this step, the confidence score is calculated using 
the same base model used for NER decoding. 
Next, the sentences with at least one rejected tag 
are selected. Only the rejected tags are manually 
checked and corrected. Finally, the model is re-
trained and updated with the merged data con-
sisting of the manually corrected data and the 
base data. 
4.1 Experiments 
We evaluated the efficiency of our active learn-
ing method from the perspective of learning cost.  
A blog corpus consisting of 45,694 sentences in 
blog articles on the WWW was prepared for the 
experiments. This corpus was divided into four 
segments as shown in Table 2. All sentences 
were manually annotated including additional 
data. For additional data, these tags were initially 
hidden and used only for simulating manual cor-
rection as shown below. Development data was 
used for optimizing the threshold by measuring 
the rejecter?s judgment error rate as described in 
Subsection 3.2. 
  
Table 2. Data Used for Active Learning. 
Base Data 11,553 sentences, 162,227 words 
Development Data 1,000 sentences,  19,710 words 
Additional Data 32,163 sentences, 584,077 words 
Test Data 978 sentences,  17,762 words 
 
We estimated the learning cost from the rate 
of hand-labeled tags. The Word Check Rate 
(WCR) represents the ratio of the number of the 
words in the additional data that need to be 
manually checked and annotated, to the total 
number of words in the additional data, and is 
expressed as follows: 
WCR= Checked Tags / Total Words. 
 
The system obtained various sizes of selected 
data as the rejecter changed its threshold from 
0.1 to 1.0 for data selection. Only the rejected 
tags in the selected data were replaced with the 
tags originally assigned by hand (i.e., correct 
tags). This procedure simulates manual correc-
tion. The manually corrected data was merged 
with the base data to update the base model. 
                                                          
5 The morphological analyzer segments an input sen-
tence into a word sequence and assigns parts-of-
speech to each word. 
We compared our method with data selection 
based on the sentence confidence measure.  
Posterior probabilities of sentences were used as 
the confidence measure, and low-confidence 
scoring sentences were selected. In contrast to 
our active learning method, all tags in the se-
lected sentences were replaced with the correct 
tags in this case.  
We evaluated the effectiveness of the up-
dated models against the test data by F-measure 
as follows: 
.
2
precision+recall
precisionrecall
=F
??
            (9) 
4.2 Results and Discussions 
4.2.1 Learning Curves and Accuracies 
Figure 3 shows learning curves of two active 
learning methods; one is based on our tag confi-
dence measure (Tag Based selection), and the 
other is based on the sentence confidence meas-
ure (Sentence Based selection). In order to reach 
the F-measure of approximately 0.76, Sentence 
Based selection requires approximately 60% of 
the entire data set to be checked by hand. In con-
trast, Tag Based selection requires only 20% or 
thereabouts. In other words, our Tag Based selec-
tion technique basically matches the performance 
of Sentence Based selection with only 1/3 of the 
learning cost. 
4.2.2 Types of Tag Replacement 
We further investigated the effects of tag-based 
judgment from the results of an experiment on 
our Tag Based selection. We categorized tag re-
placements of the rejected tags into the following 
four types: 
 
? No Change: the rejected tag is replaced 
with the same tag. 
? O-to-BI: the rejected tag is an O-tag. It is 
replaced with a B-tag or an I-tag. 
171
? BI-to-O: the rejected tag is a B-tag or an I-
tag.  It is replaced with an O-tag. 
? BI-to-BI: the rejected tag is a B-tag or an I-
tag. It is replaced with another B-tag or I-tag. 
 
Table 3 shows the distribution of these four 
categories in the selected data for the threshold 
of 0.5. This threshold achieves the lowest judg-
ment error rate given the development set. 
The rate of No Change replacement type is 
the highest. This means that the rejecter rejected 
too many tags, which actually did not need to be 
checked by hand. Although this result does not 
have a negative influence on the accuracy of the 
updated model, it is not preferable from the 
learning cost perspective. Further consideration 
should be given in order to improve the rejecter's 
judgment. 
O-to-BI type accounts for the 2nd highest per-
centage of all replacements: it is almost one third 
of all changes. Excluding No Change type (i.e., 
among O-to-BI, BI-to-O and BI-to-BI types), O-
to-BI type makes up nearly 60% of these three 
replacement types. This result shows that there 
were many new NEs not recognized by the base 
model in the selected data. 
 
Table 3. The Distribution of Replacement Types. 
Replacement Type Frequency %
No Change 13,253 43.6
O-to-BI 10,042 33.0
BI-to-O 2,419 8.0
BI-to-BI 4,688 15.4
Total 30,402 100.0
 
5 Bootstrapping for NER 
As mentioned in Section 4, we have to correct an 
O-tag to a B-tag or an I-tag in many cases, al-
most 60% of all actual corrections. This situation 
arises from a characteristic of the NER task. In 
the NER task, most NE tags in the entire corpus 
are O-tags. In fact, we found that 91 % of all tags 
were O-tags in the additional data discussed in 
Section 4. Thus, when a new NE appears in a 
sentence, this new NE is often mistakenly given 
an O-tag by the base model. 
The fact that only O-tags are dominant im-
plies that we have a chance to find a correct B-
tag or I-tag when we look up the 2nd candidate. 
This is because one of these top two candidates 
is inevitably a B-tag or an I-tag. Thus, it is valu-
able to consider what the NEXT preferable tag is 
when the most preferable tag is rejected. 
We examined in detail the accuracy of the tag 
candidates when the threshold is 0.5 as summa-
rized in Table 4. When the top tag (i.e., the tag 
with the highest tag confidence score) is accepted, 
its accuracy is 94 %, obviously high. On the 
other hand, the top tag?s accuracy is only 43 % 
when it is rejected. However, focusing both on 
the top tag and on the 2nd tag provides an oppor-
tunity to correct the rejected tag in this case. If 
we consider these top two tags together when the 
1st tag is rejected, the possibility of finding the 
correct tag is 72 %, relatively high. This suggests 
that the system is capable of correcting the re-
jected tag automatically by using the top two tag 
candidates. On this background, automatic cor-
rection is attempted for re-training the model 
through the use of a bootstrapping scheme. 
 
Table 4. Accuracy of the Tags. 
Rejecter?s Judgment of the Top Tag 
ACCEPT REJECT 
Top Tag Top Tag 2nd Tag 
94 % 43 % 29 % 
 
Figure 4 shows an example of the top two tag 
candidates and their tag confidence scores when 
the top tag?s confidence score is lower than the 
threshold (=0.5). We call this lattice the ?tag 
graph? in this paper. The system failed to recog-
nize the movie title ?3?????? (?Sancho-
me no Yuuhi?, which means ?Sunset on Third 
Street?) as ARTIFACT only with the top tag 
candidates. However, it may find a correct tag 
sequence using the top two tag candidates 
(shaded cells in Figure 4). Once the system iden-
tifies the correct tag sequence automatically in 
the tag graph, the sequence is used as a manually 
annotated sequence. We introduce this new tech-
nique, Semi-Automatically Updating NER. 
 
Figure 4. The Top Two Tag Candidates with 
Tag Confidence Measures. 
Top Tag 2nd Tag  
Tag score Tag scor
e 
??(Today) B-<DAT> 0.95   
?(?) O 0.98   
3(Third) O 0.47 B-<ART> 0.36
??(Street) O 0.38 I-<ART> 0.36
?(on) O 0.49 I-<ART> 0.38
??(Sunset) I-<ART> 0.39 O 0.34
?(?) O 0.99   
?(is) O 0.99   
??(broadcast) O 0.99   
172
5.1 Semi-Automatically Updating NER 
Figure 5. Semi-Automatically Updating 
NER Scheme. 
Selected Data 
with Tag Graphs 
Model  
Re-training 
Base Data 
(Labeled) 
Additional Data 
(Unlabeled)
Base 
Model 
Recognition Error Detector 
Morphological 
Analyzer 
NER Decoder 
Calculation of the 
Tag Confidence Measure 
Data Selection 
Automatically 
Corrected Data
Updated
Model
Model Learning 
Rejecter 
Automatic Correction 
Seed NE List
By extracting the correct tag sequence in each 
tag graph as shown in Figure 4, it is possible to 
obtain automatically corrected data, which also 
serve as new training data. Based on this idea, 
we propose Semi-Automatically Updating NER, 
which is hereafter simply referred to as Updating 
NER. 
Figure 5 overviews Updating NER. The re-
jecter produces the sentences with tag graphs 
based on the tag confidence measure. In this new 
procedure, however, the rejecter?s role differs 
from that described in Section 4 as follows: 
 
[1] When the highest confidence score cs1 equals 
or exceeds the threshold, the rejecter accepts 
only the top candidate tag t1, otherwise it 
goes to Step 2. 
[2] When cs1 is less than the threshold, the re-
jecter accepts not only the top tag t1 but also 
the 2nd tag t2. 
 
Sentences that contain the 2nd candidates are 
selected in data selection for subsequent process-
ing. The correct tag sequence in each tag graph is 
identified in automatic correction as follows:  
 
[1] Select the tag sequence that has the longest6 
and consistent NE from the tag graph. 
[2] If the longest NE also exists in a seed NE list, 
which will be described below, the system 
extracts the entire sentence with its tag se-
quence as corrected data. 
  
In Step 1, the system selects one preferable 
tag sequence based on the longest NE match. In 
the tag graph shown in Figure 4, there are 16 
possible sequences because four words ?3?, ??
?(Street)?, ? (on)? and ?? ??(Sunset)? each 
have two tag candidates; O or B for ?3?, O or I 
for ? (Street)? and ? (on)?, and I or O for ??? ?
??(Sunset)?. For example, ?B I I I?, ?B I I O?, 
?B I O O?, ?O O O I?, ?O O O O? and the rest. 
Because the sequence ?B I I I? constructs the 
longest NE, the system selects the tag sequence 
that contains the ARTIFACT ?3????? .? 
Other sequences that contain partial NEs such as 
?3?, ?3 ?, ?3?? ????, which are all ARTI-
FACTs, are ignored.  
In Step 2, the system judges whether the tag 
sequence selected in Step 1 is indeed correct. 
                                                          
6 By longest, we mean the longest tag sequence that does 
not include any O-tags. 
However, the system requires some hints to 
judge the correctness, so we need to prepare a 
seed NE list, which contains surface forms and 
NE types. This list can be created by manually 
annotation of possible NEs or automatic genera-
tion from other sources such as dictionaries. 
When the same NE exists both in the selected tag 
sequence and the seed NE list, the system re-
gards the selected tag sequence as reliable and 
extracts it as automatically corrected data. Fi-
nally, the model is updated by merging the 
automatically corrected data with the base data. 
Bootstrapping means that data selection and 
correction of the selected data are completely 
automatic; we still have to prepare the seed NE 
list somehow. Thus the learning cost is quite low 
because we only need to provide an NE list as a 
seed. Updating NER is capable of modifying the 
model to keep up with the emergence of new 
named entities. Therefore, it is effective to ana-
lyze the large amount of texts that emerge every-
day, such as blogs on the WWW. 
5.2 Experiments 
We tested our Updating NER with a large 
amount of blog texts from the WWW. One 
week?s worth of blog texts was crawled on the 
WWW to generate the additional data. Table 5 
shows the statistics of the data used in our ex-
periments. The test data contained only the blog 
texts generated in December 2006, and the base 
data is about a half year older than the test data.  
Therefore, it is difficult for the base model to 
recognize new NEs in the test data. One week?s 
173
worth of December 2006 blog texts were pre-
pared for bootstrapping. The overlap between the 
test data and the additional data was removed in 
advance. We set the rejecter?s threshold at 0.5 
and selected the data with tag graphs from the 
additional data. 
Japanese Wikipedia entries were used as the 
seed NE list. The titles of Wikipedia articles 
were regarded as surface forms. NE types were 
estimated from the category sections of each arti-
cle, based on heuristic rules prepared in advance. 
We collected 104,296 entries as a seed NE list. 
Using this seed list, Updating NER extracted 
the seed NE and its context from the selected 
data automatically. If the system found a match, 
it extracted the sentence with its tag sequence 
from the selected data. The automatically cor-
rected data was then merged with the base data in 
order to re-train the base model. 
For comparison, we evaluated the effect of the 
seed NE list itself. If there is a sequence of words 
that can be found in the seed list, then that se-
quence is always recognized as a NE. Note that 
the other words are simply decoded using the 
base model. We call this method ?user diction-
ary?. Here, we use recall and precision to evalu-
ate the accuracy of the model. 
 
Table 5. Data Description for Updating NER. 
Base Data 
(blog in Sep. 04-Jun. 06) 
43,716 sentences 
746,304 words 
Additional Data 
(one week?s blog in Dec. 06) 
240,474 sentences 
3,677,077 words 
Selected Data from the 
Additional Data 
113,761 sentences 
2,466,464 words 
Test Data 
(blog in Dec.06) 
1,609 sentences 
21,813 words 
 
5.3 Results 
Table 6 shows the details of accuracy results re-
garding the following four NE types: PERSON, 
LOCATION, ORGANIZATION, and ARTI-
FACT, which are referred to hereafter as PSN, 
LOC, ORG and ART, respectively. Although we 
added Wikipedia as a user dictionary to the base 
model, it only slightly improved the recall. In 
fact, it has no positive and sometimes a negative 
effect on precision (e.g., ART decreased from 
0.666 to 0.619). This indicates that adding an NE 
list as a dictionary is not enough to improve the 
accuracy of a NER system. This is because the 
NER system cannot discriminate an NE from 
surrounding unrelated words. It simply extracts 
matched sequences of words, so it overestimates 
the number of NEs. 
On the contrary, our Updating NER improved 
both recall and precision (e.g., the recall and the 
precision in ART improved from 0.320 to 0.364 
and from 0.666 to 0.694, respectively.). This 
means that not only the NE list but also the con-
texts are actually needed to retrain the model. 
Our Updating NER scheme has the advantage of 
finding the reliable context of a seed NE list 
automatically. Although some manual effort is 
needed to provide a seed NE list, its associated 
cost is lower than the cost of annotating the en-
tire training data. Thus, we regard Updating NER 
as a promising solution for reducing learning 
cost in practical NER systems. 
  As shown in Table 6, neither user dictionary 
method nor Updating NER improves the accu-
racy in ORG. We assume that this is caused by 
the distribution of NE types in the seed NE list. 
In the seed list selected from the Wikipedia en-
tries, PSN-type is dominant (74%). ORG-type is 
scant at only 11%, so the system did not have 
enough chances to retrain the ORG-type. Rather, 
it might be the case that the system had a ten-
dency to recognize ORG-type as PSN-type be-
cause peoples' names are often used as organiza-
tion names. Further investigation is needed to 
clarify the impact of the distribution and the 
quality of the seed NE list. 
 
Table 6. Details of Accuracy. 
  PSN LOC ORG ART
rec. 0.640 0.737 0.688 0.320Base Model 
prec. 0.699 0.811 0.652 0.666
rec. 0.686 0.729 0.688 0.354+Wikipedia  
(user dic.) prec. 0.716 0.815 0.654 0.619
rec. 0.649 0.747 0.678 0.364+Wikipedia 
(UpdatingNER) prec. 0.728 0.822 0.632 0.694
 
5.4 Discussions 
Compared to conventional machine learning 
techniques, the most distinctive feature of Updat-
ing NER is that the system can focus on the top 
two candidates when the confidence score of the 
top candidate is low. This feature actually has a 
great advantage in the NER task, because the 
system is capable of determining what the next 
preferable tag is when a new NE appears which 
is assigned an O-tag by the base model. 
   Updating NER, however, has one weak point. 
That is, the following two strict conditions are 
required to correct the selected data automati-
cally. First, the correct tag sequence must appear 
in tag graphs (i.e., as one of the top two tag can-
didates). Second, the NE must also appear in the 
seed NE list. These conditions decrease the 
174
chance of extracting sentences with correct tag 
sequences from the selected data. 
To overcome this weakness, one practical 
approach is to use Updating NER in combination 
with active learning. In the case of active learn-
ing, we do not need the correct tags in the top 
two candidates. The editor can assign correct 
tags without considering the order of candidates. 
In short, active learning has broad coverage in 
terms of learning, while Updating NER does not. 
Therefore, active learning is suitable for improv-
ing the performance level of the entire base 
model. Updating NER has the advantage of stay-
ing current with new named entities which 
emerge every day on the WWW. In practical use, 
for example, it will be better to update the model 
every week with Updating NER to keep up with 
new named entities, and occasionally perform 
active learning (every six months or so) to en-
hance the entire model. In the future, we plan to 
evaluate the efficiency of our two learning meth-
ods in practical applications, such as domain ad-
aptation and acquisition of hot trend NE words 
from blog texts on the WWW. 
6 Related Works 
To date, there have been many related works on 
active learning not only for the NER task (Shen 
et al, 2004, Laws and Sch?tze, 2008) but also 
for other tasks, such as POS tagging (Engelson 
and Dagan, 1996), text classification (Lewis and 
Catlett, 1994), parsing (Hwa, 2000), and confu-
sion set disambiguation (Banko and Brill, 2001). 
Active learning aims at effective data selection 
based on criterion measures, such as the confi-
dence measure. Most previous works focus on 
the Sentence-Based criterion evaluation and data 
selection. Our proposal differs from those previ-
ous works in that we focus on the Tag-Based 
strategy, which judges whether each tag should 
be accepted or rejected. This approach maxi-
mizes the effectiveness of manual annotation by 
leaving the accepted tags in without any manual 
correction. As a result, our Tag-based approach 
reduces the manual annotation cost by 66 %, 
compared to the Sentence-Base method. 
   Semi-supervised learning has become an ac-
tive area in machine learning; it utilizes not only 
annotated corpora but also huge amounts of plain 
text for model training. Several studies adapted  
semi-supervised learning to suit NLP tasks, such 
as word sense disambiguation (Yarowsky, 1995), 
text classification (Fujino et al, 2008), and 
chunking and NER (Suzuki and Isozaki, 2008). 
Suzuki and Isozaki (2008) suggest that a GIGA-
word size plain text corpus may further improve 
the performance of the state-of-the-art NLP sys-
tem. In this paper, however, we aim at model 
adaptation to the CGM domain to keep up with 
the new linguistic phenomena that are emerging 
every day. Because it is difficult to obtain GIGA-
word size plain text sets that reflect such new 
linguistic phenomena, it is not practical to di-
rectly apply this approach to our task. 
   Bootstrapping is similar to semi-supervised 
learning in that it also allows the use of plain text 
(Etzioni 2005, Pantel and Pennacchioti 2006). In 
this learning method, it is possible to extract new 
instances automatically from plain text with 
small seed data prepared manually. Our Updating 
NER is similar to bootstrapping in that it extracts 
new annotated corpora automatically from plain 
text data starting with a seed NE list. However, 
the goal of conventional bootstrapping is to de-
velop a new dictionary or thesaurus by extracting 
new instances. On the contrary, our goal is to 
acquire a new NE and its surrounding context in 
a sentence, not to build a NE dictionary (i.e., cor-
rect tag sequence). It is the tag sequence and not 
a single NE that is needed for model training. 
Updating NER is a novel approach in the point 
of applying bootstrapping to the framework of 
supervised learning. This approach is quite effec-
tive in that it has the advantage of reducing learn-
ing cost compared with active learning because 
only a seed NE list is needed.   
7 Conclusions 
To reduce machine learning cost, we introduced 
two techniques that are based on a tag confidence 
measure determined from tag posterior probabil-
ity. Dubious tags are automatically detected as 
recognition errors using the tag confidence 
measure. This approach maximizes the effective-
ness of manual annotation by leaving the confi-
dent tags in without any manual correction. 
   We first applied this technique to active 
learning by correcting error tags manually. We 
found that it matches the performance of the 
learning method based on the sentence confi-
dence measure with only 1/3 of the learning cost.   
   Next, we proposed Semi-Automatic Updat-
ing NER which has a bootstrap learning scheme, 
by expanding the scope from the top tag candi-
date to include the 2nd candidate. With this new 
scheme, it is possible to collect auto-labeled data 
from a large data source, such as blog texts on 
the WWW, by simply providing a seed NE list.  
175
References 
M. Banko and E. Brill. 2001. Scaling to Very Very 
Large Corpora for Natural Language Disambigua-
tion. In Proc. of ACL-2001, pages 26-33. 
S. A. Engelson and I. Dagan. 1999. Committee-Based 
Sample Selection for Probabilistic Classifiers. 
Journal of Artificial Intelligence Research, 
vol.11(1999), pages 335-360. 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. S. Weld, and A. Yates. 
2005. Unsupervised Named-Entity Extraction from 
the Web: An Experimental Study. Artificial Intelli-
gence, 165(1), pages 91-134. 
A. Fujino, N. Ueda, and K. Saito. 2008. Semisuper-
vised Learning for a Hybrid Generative 
/Discriminative Classifier Based on the Maximum 
Entropy Principle. IEEE Transactions on Pattern 
Analysis and Machine Intelligence (TPAMI), 30(3), 
pages 424-437. 
R. Hwa. 2000. Sample Selection for Statistical 
Grammer Induction. In Proc. of EMNLP/VLC-2000, 
pages 45-52. 
IREX Committee (ed.), 1999. In Proc. of the IREX 
workshop.  http://nlp.cs.nyu.edu/irex/ 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic Models for 
Segmenting and Labeling Sequence Data. In Proc. 
of ICML-2001. pages 282-289.  
F. Laws and H. Sch?tze. 2008. Stopping Criteria for 
Active Learning of Named Entity Recognition. In 
Proc. of COLING-2008, pages 465-472. 
D. Lewis and J. Gatlett. 1994. Heterogeneous uncer-
tainty sampling for supervised learning. In Proc. of 
ICML-1994, pages 148-156. 
C. D. Manning and H. Sch?tze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lev-
eraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proc. of COLING-
ACL-2006, pages 113-120. 
E. F. T. K. Sang and F. De Meulder. 1999. Represent-
ing text chunks. In Proc. of EACL-1999, pages 
173-179. 
D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 
2004. Multi-Criteria-based Active Learning for 
Named Entity Recognition. In Proc. of ACL-2004, 
pages 589-596. 
J. Suzuki and H. Izozaki. 2008. Semi-Supervised Se-
quential Labeling and Segmentation using Giga-
word Scale Unlabeled Data. In Proc. of ACL-2008, 
pages 665-673. 
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Train-
ing Conditional Random Fields with Multivariate 
Evaluation Measures. In Proc. of COLING-ACL-
2006. pages 617-624. 
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc. 
of ACL-1995, pages 189-196. 
X. Zhu. 2007. Semi-Supervised Learning, ICML-
2007 Tutorial. 
176
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225?228,
Prague, June 2007. c?2007 Association for Computational Linguistics
Japanese Dependency Parsing Using Sequential Labeling
for Semi-spoken Language
Kenji Imamura and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
{imamura.kenji, kikui.genichiro}@lab.ntt.co.jp
Norihito Yasuda
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
n-yasuda@cslab.kecl.ntt.co.jp
Abstract
The amount of documents directly published
by end users is increasing along with the
growth of Web 2.0. Such documents of-
ten contain spoken-style expressions, which
are difficult to analyze using conventional
parsers. This paper presents dependency
parsing whose goal is to analyze Japanese
semi-spoken expressions. One characteris-
tic of our method is that it can parse self-
dependent (independent) segments using se-
quential labeling.
1 Introduction
Dependency parsing is a way of structurally ana-
lyzing a sentence from the viewpoint of modifica-
tion. In Japanese, relationships of modification be-
tween phrasal units called bunsetsu segments are an-
alyzed. A number of studies have focused on parsing
of Japanese as well as of other languages. Popular
parsers are CaboCha (Kudo and Matsumoto, 2002)
and KNP (Kurohashi and Nagao, 1994), which were
developed to analyze formal written language ex-
pressions such as that in newspaper articles.
Generally, the syntactic structure of a sentence
is represented as a tree, and parsing is carried out
by maximizing the likelihood of the tree (Charniak,
2000; Uchimoto et al, 1999). Units that do not
modify any other units, such as fillers, are difficult
to place in the tree structure. Conventional parsers
have forced such independent units to modify other
units.
Documents published by end users (e.g., blogs)
are increasing on the Internet alng with the growth
of Web 2.0. Such documents do not use controlled
written language and contain fillers and emoticons.
This implies that analyzing such documents is diffi-
cult for conventional parsers.
This paper presents a new method of Japanese
dependency parsing that utilizes sequential labeling
based on conditional random fields (CRFs) in or-
der to analyze semi-spoken language. Concretely,
sequential labeling assigns each segment a depen-
dency label that indicates its relative position of de-
pendency. If the label set includes self-dependency,
the fillers and emoticons would be analyzed as seg-
ments depending on themselves. Therefore, since it
is not necessary for the parsing result to be a tree,
our method is suitable for semi-spoken language.
2 Methods
Japanese dependency parsing for written language
is based on the following principles. Our method re-
laxes the first principle to allow self-dependent seg-
ments (c.f. Section 2.3).
1. Dependency moves from left to right.
2. Dependencies do not cross each other.
3. Each segment, except for the top of the parsed
tree, modifies at most one other segment.
2.1 Dependency Parsing Using Cascaded
Chunking (CaboCha)
Our method is based on the cascaded chunking
method (Kudo and Matsumoto, 2002) proposed as
the CaboCha parser 1. CaboCha is a sort of shift-
reduce parser and determines whether or not a seg-
ment depends on the next segment by using an
1http://www.chasen.org/?taku/software/cabocha/
225
SVM-based classifier. To analyze long-distance de-
pendencies, CaboCha shortens the sentence by re-
moving segments for which dependencies are al-
ready determined and which no other segments de-
pend on. CaboCha constructs a tree structure by re-
peating the above process.
2.2 Sequential Labeling
Sequential labeling is a process that assigns each
unit of an input sequence an appropriate label (or
tag). In natural language processing, it is applied
to, for example, English part-of-speech tagging and
named entity recognition. Hidden Markov models
or conditional random fields (Lafferty et al, 2001)
are used for labeling. In this paper, we use linear-
chain CRFs.
In sequential labeling, training data developers
can design labels with no restrictions.
2.3 Cascaded Chunking Using Sequential
Labeling
The method proposed in this paper is a generaliza-
tion of CaboCha. Our method considers not only
the next segment, but also the followingN segments
to determine dependencies. This area, including the
considered segment, is called the window, and N is
called the window size. The parser assigns each seg-
ment a dependency label that indicates where the
segment depends on the segments in the window.
The flow is summarized as follows:
1. Extract features from segments such as the
part-of-speech of the headword in a segment
(c.f. Section 3.1).
2. Carry out sequential labeling using the above
features.
3. Determine the actual dependency by interpret-
ing the labels.
4. Shorten the sentence by deleting segments for
which the dependency is already determined
and that other segments have never depended
on.
5. If only one segment remains, then finish the
process. If not, return to Step 1.
An example of dependency parsing for written
language is shown in Figure 1 (a).
In Steps 1 and 2, dependency labels are supplied
to each segment in a way similar to that used by
Label Description
? Segment depends on a segment outside of win-
dow.
0Q Self-dependency
1D Segment depends on next segment.
2D Segment depends on segment after next.
-1O Segment is top of parsed tree.
Table 1: Label List Used by Sequential Labeling
(Window Size: 2)
other sequential labeling methods. However, our
sequential labeling has the following characteristics
since this task is dependency parsing.
? The labels indicate relative positions of the de-
pendent segment from the current segment (Ta-
ble 1). Therefore, the number of labels changes
according to the window size. Long-distance de-
pendencies can be parsed by one labeling process
if we set a large window size. However, growth
of label variety causes data sparseness problems.
? One possible label is that of self-dependency
(noted as ?0Q? in this paper). This is assigned
to independent segments in a tree.
? Also possible are two special labels. Label ?-1O?
denotes a segment that is the top of the parsed
tree. Label ??? denotes a segment that depends
on a segment outside of the window. When the
window size is two, the segment depends on a
segment that is over two segments ahead.
? The label for the current segment is determined
based on all features in the window and on the
label of the previous segment.
In Step 4, segments, which no other segments de-
pend on, are removed in a way similar to that used
by CaboCha. The principle that dependencies do
not cross each other is applied in this step. For ex-
ample, if a segment depends on a segment after the
next, the next segment cannot be modified by other
segments. Therefore, it can be removed. Similarly,
since the ??? label indicates that the segment de-
pends on a segment after N segments, all interme-
diate segments can be removed if they do not have
??? labels.
The sentence is shortened by iteration of the
above steps. The parsing finishes when only one
segment remains in the sentence (this is the segment
226
(a) Written Language
--- 2D 1D 1D -1O
2D 1D -1O
Output
Input
Label
Label
kare wa
(he)
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
(He was moved by her warm heart.)
Seg. No. 1 2 3 4 5
kare wa
(he)
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
(b) Semi-spoken Language
Input Uuuum, kyo wa
(today)
...... choshi
(condition)
yokatta desu.
(be good)
0Q --- 0Q 1D -1O
1D -1O
(Uuuum, my condition .... was good today.)
Seg. No. 1 2 3 4 5
Label
Label
Uuuum, kyo wa
(today)
...... choshi
(condition)
yokatta desu.
(be good)Output
1st
Labeling
2nd
Labeling
Figure 1: Examples of Dependency Parsing (Window Size: 2)
Corpus Type # of Sentences # of Segments
Kyoto Training 24,283 234,685
Test 9,284 89,874
Blog Training 18,163 106,177
Test 8,950 53,228
Table 2: Corpus Size
at the top of the parsed tree). In the example in Fig-
ure 1 (a), the process finishes in two iterations.
In a sentence containing fillers, the self-
dependency labels are assigned by sequential label-
ing, as shown in Figure 1 (b), and are parsed as in-
dependent segments. Therefore, our method is suit-
able for parsing semi-spoken language that contains
independent segments.
3 Experiments
3.1 Experimental Settings
Corpora In our experiments, we used two cor-
pora. One is the Kyoto Text Corpus 4.0 2, which is
a collection of newspaper articles with segment and
dependency annotations. The other is a blog cor-
pus, which is a collection of blog articles taken as
semi-spoken language. The blog corpus is manually
annotated in a way similar to that used for the Kyoto
text corpus. The sizes of the corpora are shown in
Table 2.
Training We used CRF++ 3, a linear-chain CRF
training tool, with eleven features per segment. All
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html
3http://www.chasen.org/?taku/software/CRF++/
of these are static features (proper to each segment)
such as surface forms, parts-of-speech, inflections
of a content headword and a functional headword
in a segment. These are parts of a feature set that
many papers have referenced (Uchimoto et al, 1999;
Kudo and Matsumoto, 2002).
Evaluation Metrics Dependency accuracy and
sentence accuracy were used as evaluation metrics.
Sentence accuracy is the proportion of total sen-
tences in which all dependencies in the sentence
are accurately labeled. In Japanese, the last seg-
ment of most sentences is the top of the parsed trees,
and many papers exclude this last segment from the
accuracy calculation. We, in contrast, include the
last one because some of the last segments are self-
dependent.
3.2 Accuracy of Dependency Parsing
Dependency parsing was carried out by combining
training and test corpora. We used a window size
of three. We also used CaboCha as a reference for
the set of sentences trained only with the Kyoto cor-
pus because it is designed for written language. The
results are shown in Table 3.
CaboCha had better accuracies for the Kyoto test
corpus. One reason might be that our method man-
ually combined features and used parts of com-
binations, while CaboCha automatically finds the
best combinations by using second-order polyno-
mial kernels.
For the blog test corpus, the proposed method
using the Kyoto+Blog model had the best depen-
227
Test Corpus Method Training Corpus Dependency Accuracy Sentence Accuracy
(Model)
Kyoto Proposed Method Kyoto 89.87% (80766 / 89874) 48.12% (4467 / 9284)
(Written Language) (Window Size: 3) Kyoto + Blog 89.76% (80670 / 89874) 47.63% (4422 / 9284)
CaboCha Kyoto 92.03% (82714 / 89874) 55.36% (5140 / 9284)
Blog Proposed Method Kyoto 77.19% (41083 / 53226) 41.41% (3706 / 8950)
(Semi-spoken Language) (Window Size: 3) Kyoto + Blog 84.59% (45022 / 53226) 52.72% (4718 / 8950)
CaboCha Kyoto 77.44% (41220 / 53226) 43.45% (3889 / 8950)
Table 3: Dependency and Sentence Accuracies among Methods/Corpora
 88
 88.5
 89
 89.5
 90
 90.5
 91
 1  2  3  4  5
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
D
ep
en
de
nc
y 
Ac
cu
ra
cy
 (%
)
# 
of
 F
ea
tu
re
s
Window Size
Dependency Accuracy
# of Features
Figure 2: Dependency Accuracy and Number of
Features According to Window Size (The Kyoto
Text Corpus was used for training and testing.)
dency accuracy result at 84.59%. This result was
influenced not only by the training corpus that con-
tains the blog corpus but also by the effect of self-
dependent segments. The blog test corpus contains
3,089 self-dependent segments, and 2,326 of them
(75.30%) were accurately parsed. This represents
a dependency accuracy improvement of over 60%
compared with the Kyoto model.
Our method is effective in parsing blogs be-
cause fillers and emoticons can be parsed as self-
dependent segments.
3.3 Accuracy According to Window Size
Another characteristic of our method is that all de-
pendencies, including long-distance ones, can be
parsed by one labeling process if the window cov-
ers the entire sentence. To analyze this characteris-
tic, we evaluated dependency accuracies in various
window sizes. The results are shown in Figure 2.
The number of features used for labeling in-
creases exponentially as window size increases.
However, dependency accuracy was saturated after a
window size of two, and the best accuracy was when
the window size was four. This phenomenon implies
a data sparseness problem.
4 Conclusion
We presented a new dependency parsing method us-
ing sequential labeling for the semi-spoken language
that frequently appears in Web documents. Sequen-
tial labeling can supply segments with flexible la-
bels, so our method can parse independent words
as self-dependent segments. This characteristic af-
fects robust parsing when sentences contain fillers
and emoticons.
The other characteristics of our method are us-
ing CRFs and that long dependencies are parsed in
one labeling process. SVM-based parsers that have
the same characteristics can be constructed if we in-
troduce multi-class classifiers. Further comparisons
with SVM-based parsers are future work.
References
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL-2000, pages 132?139.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc. of
CoNLL-2002, Taipei.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 282?289.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese dependency structure analysis based
on maximum entropy models. In Proc. of EACL?99,
pages 196?203, Bergen, Norway.
228
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85?88,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Discriminative Approach to Predicate-Argument Structure Analysis
with Zero-Anaphora Resolution
Kenji Imamura, Kuniko Saito, and Tomoko Izumi
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka, Kanagawa, 239-0847, Japan
{imamura.kenji,saito.kuniko,izumi.tomoko}@lab.ntt.co.jp
Abstract
This paper presents a predicate-argument
structure analysis that simultaneously con-
ducts zero-anaphora resolution. By adding
noun phrases as candidate arguments that
are not only in the sentence of the target
predicate but also outside of the sentence,
our analyzer identifies arguments regard-
less of whether they appear in the sen-
tence or not. Because we adopt discrimi-
native models based on maximum entropy
for argument identification, we can easily
add new features. We add language model
scores as well as contextual features. We
also use contextual information to restrict
candidate arguments.
1 Introduction
Predicate-argument structure analysis is a type of
semantic role labeling, which is an important mod-
ule to extract event information such as ?who did
what to whom? from a sentence. There are many
arguments called zero pronouns that do not appear
in the surface of a sentence in Japanese. In this
case, predicate-argument structures cannot be con-
structed if we only rely on the syntactic informa-
tion of a single sentence. Similar phenomena also
happen in English noun predicates, in which ar-
guments of noun predicates sometimes do not ex-
ist in the sentence due to things such as ellipses
(Jiang and Ng, 2006). To correctly extract the
structures from such sentences, it is necessary to
resolve what zero pronouns refer to by using other
information such as context.
Although predicate-argument structure analysis
and zero-anaphora resolution are closely related,
it was not until recently that these two tasks were
lumped together. Due to the developments of
large annotated corpora with predicate-argument
and coreference relations (e.g.,(Iida et al, 2007))
and with case frames, several works using statisti-
cal models have been proposed to solve these two
tasks simultaneously (Sasano et al, 2008; Taira et
al., 2008).
In this paper, we present a predicate-argument
structure analysis that simultaneously resolves the
anaphora of zero pronouns in Japanese, based on
supervised learning. The analyzer obtains candi-
date arguments not only from the sentence of the
target predicate but also from the previous sen-
tences. It then identifies the most likely argu-
ments based on discriminative models. To iden-
tify arguments that appear in the sentence and are
represented by zero pronouns without distinction,
the analyzer introduces the following features and
techniques: the language model features of noun
phrases, contextual features, and restrictions of
candidate arguments.
2 Predicate-Argument Structure
Analyzer
2.1 Procedure and Models
The procedure of our predicate-argument structure
analyzer is as follows. The input to the analyzer is
an article (multiple sentences) because our target
is to identify arguments spread across sentences.
1. First, each sentence is individually analyzed
and segmented into base phrases by a morpho-
logical analyzer and a base phrase chunker. In
Japanese, a base phrase is usually constructed
by one or more content words (such as base
noun phrases) and function words (such as case
particles). In addition, dependency relations
among base phrases are parsed by a depen-
dency parser. In this paper, base phrases and
dependency relations are acquired from an an-
notated corpus (i.e., correct parses).
2. Next, predicates are extracted from the base
phrases. In general, a predicate is determined
85
Name Note
Baseline
Features
Predicate Form and POS of the predi-
cate
Noun Form and POS of the head-
word of the candidate phrase
Particle Form and POS of the particle
of the candidate phrase
Path Dependency relation between
the predicate and the candi-
date phrase
Passive Passive auxiliary verbs that
the predicate contains
PhPosit Relative phrase position be-
tween the predicate and the
candidate phrase
SentPosit Relative sentence position be-
tween the predicate and the
candidate phrase
Additional
Features
(c.f.,
Sec. 2.2
and 2.3)
LangModel Language model scores
Used Flag whether the candidate
phrase was used as arguments
of previous predicates
SRLOrder Order in Salient Referent List
Table 1: Features Used in this Paper
based on parts of speech such as verbs and ad-
jectives. In this paper, the predicates are also
provided from an annotated corpus.
3. Concurrently, noun phrases and their head-
words are extracted as candidate arguments
from base phrases. If an argument of a predi-
cate is a zero pronoun, it is likely that the argu-
ment itself has appeared in previous sentences.
Therefore, the analyzer collects not only all
phrases in the sentence but also some phrases
in the previous sentences. We also add the spe-
cial noun phrase NULL, which denotes that the
argument of the predicate is not required or did
not appear in the article (i.e., exophoric).
4. Next, features needed for an argument iden-
tifier are extracted from each pair of a predi-
cate and a candidate argument. Features used
in this paper are shown in Table 1. Base-
line features are roughly those of the predi-
cate, the noun phrase, and their relations (on
the phrasal/sentential sequence and the depen-
dency tree). For binary features, we use all
combinations of these features listed above.
5. Finally, the argument identifier selects the best
phrases for nominative, accusative, and dative
cases from the candidate arguments (Figure 1).
In this paper, we use maximum entropy models
normalized for each predicate to each case. That
is, the identifier directly selects the best phrase that
NULL Phrase 1 Phrase 2 Phrase 3 Phrase 4 ...
Candidate Arguments
Phrase 1 Phrase 3 NULL
Candidate Arguments
in Sentence of Predicate
Candidate Arguments
before Sentences of Predicate
zero-anaphoric(inter-sentential)
exophoric
or no argument
Select
Best
Phrase
Dat.
Model
Select
Best
Phrase
Acc.
Model
Select
Best
Phrase
Nom.
Model
Figure 1: Summary of Argument Identification
satisfies the following equations from the candi-
date arguments:
n? = argmax
n
j
?N
P (d(n
j
) = 1|X
j
;M
c
) (1)
P (d(n
j
) = 1|X
j
;M
c
) =
1
Z
c
(X)
exp
?
k
{?
c
k
f
k
(d(n
j
) = 1, X
j
)}(2)
Z
c
(X) =
?
n
j
?N
exp
?
k
{?
c
k
f
k
(d(n
j
) = 1, X
j
)} (3)
X
j
= ?n
j
, v, A? (4)
where n, c, and v denote a noun phrase of an argu-
ment, the case, and the target predicate, respec-
tively, N denotes a set of candidate arguments,
d(n) is a function that returns 1 iff the phrase n
becomes the argument, and M
c
denotes the model
of the case c. In addition, f
k
(d(n
j
) = 1, X
j
) is a
feature function, ?
c
k
denotes a weight parameter
of the feature function, and A denotes an article in
which all sentences are parsed.
As shown, our analyzer can assign the best noun
phrases to arguments regardless of whether they
appear in the sentence or not by collecting candi-
dates spread across multiple sentences. Further-
more, because the identifier is regarded as a selec-
tor based on the discriminative models, our ana-
lyzer has two properties: 1) New features can be
easily added. 2) The precision can be improved by
restricting the candidate arguments appropriately.
When we analyze predicate-argument struc-
tures and zero-anaphora resolution, syntactic in-
formation sometimes does not help because refer-
ents of zero pronouns do not appear in the sen-
tence of the predicate. To overcome this problem,
86
we introduce additional information, i.e., language
model scores and contextual information.
2.2 Language Models
Even if syntactic information does not help to
identify arguments, we can expect that a certain
noun phrase might be the correct argument of the
predicate when we put it in place of the zero
pronoun and the sentence becomes meaningful.
Therefore, we add language model scores as fea-
tures of the identifier. Because the appearance or-
der of argument phrases is not strongly constricted
in Japanese, we construct generation models that
reflect dependency relations among a predicate, its
case and a noun phrase. That is, we regard gen-
eration probabilities P (n|c, v) acquired from the
dependency tree as the scores of language models.
The language models are built from large plain
texts by using a dependency parser. First, predi-
cates and the base phrases that directly depend on
the predicates are aquired from parsed sentences.
Next, case particles and headwords are extracted
from the base phrases. Finally, generation prob-
abilities are computed using maximum likelihood
estimation. Good-Turing discounting and backoff
smoothing are also applied. Here, it is necessary
to assign generation probabilities to NULLs. Re-
garding the training corpus that will be described
in Section 3, the NULL rates of the nominative,
accusative, and dative cases were 16.7%, 59.9%,
and 81.6%, respectively. We assign these rates to
the backoff term P (NULL|c).
Using the language models, generation proba-
bilities of the noun phrases are computed for ev-
ery case of the predicate, and features that main-
tain the logarithms of language model scores are
added (?LangModel? features in Table 1). Thus,
the values of these feature functions are real.
2.3 Usage of Context
Centering theory claims that noun phrases that
have been used once tend to be used again within
the same context. We adopt this claim and add two
different kinds of features. One is the feature that
indicates whether a candidate has been used as an
argument of predicates in the preceding sentences
(?Used? features). However, the Used features are
affected by the accuracy of the previous analyses.
Thus, we also adopt the Salience Reference List
(Nariyama, 2002), which only uses explicit sur-
face case markers or a topic marker, and added
Training Development Test
# of Articles 1,751 480 695
# of Sentences 24,225 4,833 9,272
# of Predicates 67,145 13,594 25,500
# of Arguments
Nom. 56,132 11,969 21,931
Acc. 26,899 5,566 10,329
Dat. 12,332 3,147 5,944
Table 2: Corpus Statistics
their priority order to the List as another feature
(?SRLOrder? feature).
Another way to adopt contextual information
is to restrict the candidate arguments. When we
analyzed the training corpus from the viewpoint
of zero pronouns, it was found that 102.2 noun
phrases on average were required as candidate ar-
guments if we did not stipulate any restrictions.
When the candidate arguments we had restricted
to those that had been used as arguments of the
predicate appeared in a previous one sentence
(namely, noun phrases appeared in more than one
sentence before have a chance to remain), then the
number of candidate arguments significantly de-
creased to an average of 3.2 but they covered the
62.5% of the referents of zero pronouns.
By using these characteristics, our analyzer re-
stricts the candidate arguments to those that are of
the same sentence, and those that were used as the
arguments of another predicate in a previous sen-
tence.
3 Experiments
3.1 Experimental Settings
Corpora: We used the NAIST Text Corpus ver-
sion 1.4b (Iida et al, 2007) and the Kyoto Text
Corpus 4.0 as the annotated corpora. We could
obtain dependency and predicate-argument struc-
tures because these corpora were annotated to al-
most the same newspaper articles. We divided
them into training, development, and test sets as
shown in Table 2.
Argument Identification Models: Maximum
entropy models were trained using the training set.
In these experiments, we used the Gaussian prior,
and the variance was tuned using the development
set. Candidate argument restrictions were applied
during both training and decoding.
Language Models: Language models were
trained from twelve years of newspaper articles
(Mainichi Shinbun newspaper 1991-2002, about
87
# of
Case Type Args. Prec. Rec. F
Nom. Dep. 14,287 85.2% 88.8% 87.0%
Zero-Intra 4,581 58.8% 43.4% 50.0%
Zero-Inter 3,063 47.5% 7.6% 13.1%
Total 21,931 79.4% 68.0% 73.2%
Acc. Dep. 9,316 95.6% 92.2% 93.9%
Zero-Intra 742 53.7% 21.6% 30.8%
Zero-Inter 271 25.0% 0.4% 0.7%
Total 10,329 94.3% 84.7% 89.2%
Dat. Dep. 5,409 91.1% 72.6% 80.8%
Zero-Intra 396 0.0% 0.0% 0.0%
Zero-Inter 139 0.0% 0.0% 0.0%
Total 5,944 91.1% 66.1% 76.6%
Table 3: Results on the Test Set
5.5M sentences) using the method described in
Section 2.2. However, we eliminated articles that
overlap the NAIST Corpus.
Evaluation: We evaluated the precision and re-
call rates, and F scores, all of which were com-
puted by comparing system output and the correct
answer of each argument. We also evaluated the
rate at which all arguments of a predicate were
completely identified as predicate-argument accu-
racy.
3.2 Results
The results are shown in Table 3. This table
shows accuracies of the argument identification
according to each case and each dependency re-
lation between predicates and arguments. The
predicate-argument accuracy on the test set was
59.4% (15,140/25,500).
First, focusing on the F scores of the Dep. rela-
tions, which denote a predicate and an argument in
the same sentence and directly depend upon each
other, scores of over 80% were obtained for all
cases. Compared with Taira et al (2008), they
were higher in the nominative and accusative cases
but were lower in the dative case. Overall, we ob-
tained F scores between 73.2% and 89.2%.
Next, focusing on the intra-sentential (Zero-
Intra) and inter-sentential (Zero-Intra) zero-
anaphora, the analyzer identified arguments at
some level from the viewpoint of precision. How-
ever, the recall rates and F scores were very
low. The Zero-Inter recall rate for the nominative
case, in which zero pronouns are centered, was
only 7.6%. This is because our method preferred
NULL phrases over unreliable phrases appearing
before the predicate sentence. In fact, the analyzer
output only 488 arguments, although the answer
was 3,063. To control the NULL preference is a
future work for our analyzer.
4 Discussions and Conclusions
We proposed a predicate-argument structure anal-
ysis that simultaneously conducts zero-anaphora
resolution. By adding noun phrases as candidate
arguments that are not only in the sentence of
the target predicate but also outside of the sen-
tence, our analyzer identified arguments regard-
less of whether they appear in the sentence or
not. Because we adopted discriminative models
for argument identification, we can easily add new
features. By using this property, we added lan-
guage model scores as well as contextual features.
We also used contextual information to restrict
candidate arguments. As a result, we achieved
predicate-argument accuracy of 59.4%, and accu-
racies of argument identification were F-scores of
73.2%?89.2%.
Verifying argument structures by language
models evokes selectional preference of case
frames. Sasano et al (2008) has proposed statis-
tical models using case frames built from 1.6 B
sentences. Because the amount of the resources
used in our study is quite different, we cannot di-
rectly compare the methods and results. However,
because our analyzer has scalability that can freely
add new features, for our future work, we hope to
adopt the case frames as new features and compare
their effect.
References
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop in ACL-2007, pages 132?139.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138?145.
Shigeko Nariyama. 2002. Grammar for ellipsis res-
olution in Japanese. In Proceedings of TMI-2002,
pages 135?145.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for Japanese zero anaphora resolution. In Proceed-
ings of COLING-2008, pages 769?776.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008. A Japanese predicate argument structure anal-
ysis using decision lists. In Proceedings of EMNLP-
2008, pages 523?532.
88
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 806?815, Dublin, Ireland, August 23-29 2014.
Predicate-Argument Structure Analysis with Zero-Anaphora Resolution
for Dialogue Systems
Kenji Imamura, Ryuichiro Higashinaka, and Tomoko Izumi
NTT Media Intelligence Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan
{imamura.kenji,higashinaka.ryuchiro,izumi.tomoko}@lab.ntt.co.jp
Abstract
This paper presents predicate-argument structure analysis (PASA) for dialogue systems in
Japanese. Conventional PASA and semantic role labeling have been applied to newspaper arti-
cles. Because pronominalization and ellipses frequently appear in dialogues, we base our PASA
on a strategy that simultaneously resolves zero-anaphora and adapt it to dialogues. By incor-
porating parameter adaptation and automatically acquiring knowledge from large text corpora,
we achieve a PASA specialized to dialogues that has higher accuracy than that for newspaper
articles.
1 Introduction
Semantic role labeling (SRL) and predicate-argument structure analysis (PASA) are important analysis
techniques for acquiring ?who did what to whom? from sentences
1
. These analyses have been applied to
written texts because most annotated corpora comprise newspaper articles (Carreras and M`arquez, 2004;
Carreras and M`arquez, 2005; Matsubayashi et al., 2014).
Recently, systems for speech dialogue between humans and computers (e.g., Siri of Apple Inc. and
Shabette Concier of NTT DoCoMo) have become familiar with the popularization of smart phones. A
man-machine dialogue system has to interpret human utterances to associate them with system utter-
ances. The predicate-argument structure could be an effective data structure for dialogue management.
However, it is unclear whether we can apply the SRL/PASA for newspaper articles to dialogues because
there are many differences between them, such as the number of speakers, written or spoken language,
and context processing. For example, the following dialogue naturally includes pronouns, and thus
anaphora resolution is necessary for semantic role labeling.
A: [I]
ARG0
want [an iPad Air]
ARG1
.
B: [When]
ARGM
will [you]
ARG0
buy [it(=an iPad Air)]
ARG1
?
Similar phenomena exist in Japanese dialogues. However, most pronouns are omitted (called zero-
pronouns), and zero-anaphora resolution is necessary for Japanese PASA.
A: [iPad Air]
NOM
-ga hoshii-na.
iPad Air NOM. want
?? want an iPad Air.?
B: itsu ?
NOM
?
ACC
kau-no?
when buy?
?When will ? buy ???
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Recent SRL systems assign labels of predicates and their arguments as semantic roles. Consequently, SRL and PASA are
very similar tasks. We use the term predicate-argument structure analysis in this paper because most Japanese analyzers use
this term.
806
This paper presents predicate-argument structure analysis with zero-anaphora resolution for Japanese
chat dialogues. Here, we regard the task of constructing PASA for dialogues as a kind of domain adap-
tation from newspaper articles to dialogues. M`arquez et al. (2008) and Pradhan et al. (2008) indicated
that the tuning of parameter distribution and reducing the out-of-vocabulary are important for the do-
main adaptation of SRL. We also focus on parameter distribution and out-of-vocabulary to construct a
PASA adapted to dialogues. To the best of our knowledge, this is the first paper to describe a PASA for
dialogues that include many zero-pronouns.
The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese.
Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper
articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it
was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper.
2 Related Work
2.1 Semantic Role Labeling in English
The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of
annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and
M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank
(Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source
texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez
et al. (2008) provides a review of SRL.
OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news,
broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the
Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected
to be applied to dialogue analysis.
A few SRL studies have focused on not only verbal predicates (e.g., ?decide?) but also nominal predi-
cates (e.g., ?decision?) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because
the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase ?the
decision? is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of
nominal predicates.
2.2 Predicate-Argument Structure Analyses in Japanese
Japanese material includes the NAIST Text Corpus (Iida et al., 2007)
2
, which is an annotated corpus
of predicate-argument structures and coreference information for newspaper articles. Argument noun
phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and
the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as
zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments.
Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al.,
2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve
the zero-anaphora caused by zero-pronouns.
Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether
the methods for newspapers can be applied to dialogue conversations.
3 Characteristics of Chat Dialogues
We first collected chat dialogues of two speakers and annotated them with the predicate-argument struc-
ture. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent
in speech dialogues, were rare. The theme was one of 20 topics, such as meals, travel, hobbies, and
TV/radio programs. Annotation of the predicate-argument structure complied with the NAIST Text Cor-
pus. Figure 1 shows a chat dialogue example and its predicate-argument structure annotation.
2
http://cl.naist.jp/nldata/corpus/. We use version 1.5 with our own preprocessing in this paper. NAIST is
an acronym of ?Nara Institute of Science and Technology.?
807
A: natsu-wa (exo2)
NOM
(exog)
DAT
dekake-tari-shimashi-ta-ka?
?Did (you)
NOM
go (anywhere)
DAT
in this summer??
B: 8-gatsu-wa Ito-no [hanabi-taikai]
DAT
-ni (exo1)
NOM
yuki-mashi-ta.
?(I)
NOM
went to
[
the fireworks
?1
]
DAT
at Ito in August.?
A:
[
hanabi
?2
]
ACC
,
[
watashi
?3
]
NOM
-mo mi-takatta-desu.
?
[
Fireworks
?2
]
ACC
,
[
I
?3
]
NOM
also wanted to see (it).?
A: demo, kotoshi-wa (exo1)
NOM
isogashiku-te (exo1)
NOM
(
*
2)
ACC
mi-ni (
*
2)
DAT
ike-masen-deshita.
?But (I)
NOM
couldn?t go (?2)
DAT
to see (it=*2)
ACC
this year because (I)
NOM
was busy.?
Figure 1: Chat Dialogue Example and Its Predicate-Argument Structure Annotation
Lower lines denote glosses of the upper lines. The bold words denote predicates, the square brack-
ets [] denote intra-sentential arguments, and the round brackets () denote inter-sentential or exophoric
arguments.
# of Articles # of Sentences # of Words # of Predicates
Corpus Set /Dialogues /Utterances (per Sentence) (per Sentence)
NAIST Text Corpus Training 1,751 24,283 664,898 (27.4) 68,602 (2.83)
Development 480 4,833 136,585 (28.3) 13,852 (2.87)
Test 696 9,284 255,624 (27.5) 26,309 (2.83)
Chat Dialog Corpus Training 184 6,960 61,872 (8.9) 7,470 (1.07)
Test 101 4,056 38,099 (9.4) 5,333 (1.31)
Table 1: Sizes of Corpora
Zero- Zero- Exophora
Case Corpus # of Arguments Dep Intra Inter exo1 exo2 exog
Nominative NAIST 68,598 54.5% 17.3% 11.4% 2.0% 0.0% 14.7%
Dialogue 7,467 31.8% 7.4% 12.6% 23.9% 5.6% 18.8%
Accusative NAIST 27,986 89.2% 6.9% 3.4% 0.0% 0.0% 0.4%
Dialogue 1,901 46.6% 12.8% 27.5% 0.8% 0.1% 12.2%
Datative NAIST 6,893 84.7% 10.2% 4.3% 0.0% 0.0% 0.8%
Dialogue 2,089 37.6% 7.8% 15.0% 2.5% 1.1% 36.1%
Table 2: Distribution of Arguments in Training Corpora
Table 1 shows the statistics of the NAIST Text Corpus and the Chat Dialogue Corpus we created
3
.
The size of the Dialogue Corpus is about 10% of the NAIST Corpus. The NAIST Corpus is divided into
three parts: training, development, and test. The Dialogue Corpus is divided into training and test.
Table 2 shows distributions of arguments in the training sets of the NAIST/Dialogue corpora. We clas-
sified the arguments into the following six categories because each argument presents different difficulties
for analysis by its position and syntactic relation. The first two categories (Dep and Zero-Intra) are
the ones that in which the predicate and the argument occupy the same sentence.
? Dep: The argument directly depends on the predicate and vice versa on the parse tree.
? Zero-Intra: Intra-sentential zero-pronoun. The predicate and the argument are in the same
sentence, but there is no direct dependency.
? Zero-Inter: Inter-sentential zero-pronoun. The predicate and the argument are in different
sentences.
? exo1/exo2/exog: These are exophoric and denote zero-pronouns of the first person, second per-
son, and the others (general), respectively.
By Table 2, we can see that the ratios of Dep in all cases decreased in the Dialogue Corpus. In the other
categories, the tendencies between the nominative case and the accusative/dative cases were different. In
the nominative case, the Zero-Intra also decreased in the Dialogue Corpus, and the declines were
3
We regard a dialogue and an utterance as an article and a sentence, respectively.
808
exo1 exo2 exogNULL Phrase 1 Phrase 2 Phrase 3 Phrase 4 ?
Special Noun Phrases Candidate Argumentsin Past  Sentences Candidate Argumentsin Current Sentence
Candidate Arguments
SelectorNominativeModel SelectorAccusativeModel SelectorDativeModel
exo1exophoric(first person) zero-anaphoric(inter-sentential)
Phrase 2 NULLno argument
Figure 2: Structure of Argument Identification and Classification
assigned to exo1 and exo2. Namely, the arguments in a sentence were reduced, and zero-pronouns
increased compared with the newspaper articles. Note that many antecedents were the first or second
person. On the other hand, in the accusative and dative cases, the declines of the Dep were assigned to
the Zero-Inter or the exog in the Dialogue Corpus. Namely, anaphora resolution across multiple
sentences is important to dialogue analysis. In contrast, most arguments and the predicate appear in the
same sentence in the accusative/dative cases of newspapers.
4 Basic Strategy for Predicate-Argument Structure Analysis and Zero-Anaphora
Resolution
4.1 Architecture
We use Imamura et al. (2009)?s method developed for newspaper articles as the base PASA in this paper.
It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and
exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following
steps for each sentence (utterance).
1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords
are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus
are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency
parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the
part-of-speech tags and the parse trees.
2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora
for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified
using part-of-speech patterns that include verbs, adjectives, and copular verbs.
3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate
(called the current sentence) and the past sentences. Concretely, the following base phrases are
regarded as candidates.
? All noun phrases in the current sentence are extracted as intra-sentential candidates regardless
of syntactic relations.
? From the past sentences, noun phrases are contextually extracted as inter-sentential candidates.
Details are described in Section 4.4.
? Exophoric labels (exo1, exo2, and exog) and the NULL (the argument is not required) are
added as special noun phrases.
809
4. The features are generated from the predicate phrase, candidate arguments, and their relations. The
best candidate for each case is independently selected (Figure 2).
4.2 Models
The models for the selector are based on maximum entropy classification. The selector identifies the best
noun phrase n? that satisfies the following equations from the candidate argument set N.
n? = argmax
n
j
?N
P (d(n
j
) = 1|X
j
;M
c
) (1)
P (d(n
j
) = 1|X
j
;M
c
) =
1
Z
c
(X)
exp
?
k
{?
ck
f
k
(d(n
j
) = 1, X
j
)} (2)
Z
c
(X) =
?
n
j
?N
exp
?
k
{?
ck
f
k
(d(n
j
) = 1, X
j
)} (3)
X
j
= ?n
j
, v, A? (4)
where n denotes a candidate argument, N denotes a set of candidate arguments of predicate v, d(n) is
a function that returns 1 iff candidate n becomes the argument, and M
c
denotes the model of case c. In
addition, f
k
(d(n
j
) = 1, X
j
) is a feature function, ?
ck
denotes a weight parameter of the feature function,
and A denotes the article from which all sentences are parsed.
Training phase optimizes the weight parameters in order to maximize the difference in posterior prob-
abilities among the correct noun phrase and the other candidates. Specifically, the model of case M
c
is
learnt by minimizing the following loss function `
c
.
`
c
= ?
?
i
logP (d(n
i
) = 1|X
i
;M
c
) +
1
2C
?
k
||?
ck
||
2
(5)
where n
i
denotes the correct noun phrase of the i-th predicate in the training set, X
i
denotes the i-th
tuple of the correct noun phrase, the predicate, and the article ?n
i
, v
i
, A
i
?. Since the posterior probability
is normalized for each set of candidate arguments of a predicate by Equation (3), the probability of
the correct noun phrase approaches closer to 1.0, and the probabilities of the other candidates approach
closer to 0.0 in Equation (5).
4.3 Features
Similar to other studies (e.g., (Gildea and Jurafsky, 2002)), we use three types of features: 1) predicate
features, 2) noun phrase (NP) features, and 3) the relationship between predicates and noun phrases
(Table 3). We also introduce combined features of the ?Noun? with all other binary features because the
features aim to select the best noun phrase.
The special features in this paper are the dependency language models (three types) and the obligatory
case information (?Frame? feature), which are automatically acquired from large text corpora. We discuss
them in Section 5.2.
4.4 Context Processing
Contexts of dialogues and newspaper articles are different. We should employ context processing spe-
cialized for the dialogues. However, contexts, including system and user utterances, should be managed
collectively by the dialogue manager from the viewpoint of dialogue systems. Thus, this study uses the
same context processing for the newspaper articles and dialogues. Note that the method in this paper
controls the context by selecting the inter-sentential candidates. We can easily alter context management
by providing candidate arguments from an external manager.
Context processing in this paper is as follows.
? From the current sentence, trace back to the past, and find a sentence that contains the other pred-
icate (we call this the prior sentence). This process aims to ignore utterances that do not contain
predicates.
810
Type Name Value Remark
Predicate Pred Binary Lemma of the predicate.
PType Binary Type of predicate. One of ?verb?, ?adjective?, and ?copular verb?.
Voice Binary Declarative or not. If not, the passive/causative auxiliary verb is assigned.
Suffix Binary Sequence of the functional words of the main clause. This feature aims to reflect
the speech act of the utterance.
Frame Binary Obligatory case information. The case requires argument (1) or not (0).
Noun Phrase Noun Binary Headword of the NP
Particle Binary Case particle of the base phrase. If the NP is a special noun phrase, this is NULL.
NType Binary If the substance of the NP is in the article, this is ?NP?; otherwise the same value
of the ?Noun? feature.
Surround Binary POS tags of the surrounding words of the NP. The window size is ?2.
Relation
between
Predicate and
NP
PhPosit Binary Distance between the predicate and the NP. If they are in different sentences, or
the NP is an exophora, this is NULL.
Syn Binary Dependency path between the predicate and the NP. If they are in different sen-
tences, or the NP is an exophora, this is NULL.
Speaker Binary Whether the speakers of the predicate and the NP are the same (SAME) or not
(OTHER).
Dependency
Language
Models
log P (n|c, v) Real Generation probability of NP n given predicate v and case c.
log P (v|c, n) Real Generation probability of predicate v given NP n and case c.
log P (c|n) Real Generation probability of case c given NP n.
Table 3: List of Features
? All noun phrases that lie between the prior to the current sentence are added to the candidate argu-
ments. In addition, noun phrases that are used as arguments of any predicates are also added (called
argument recycling (Imamura et al., 2009)). Argument recycling covers wide contexts because it
can employ distant noun phrases if the past predicates have inter-sentential arguments.
5 Adaptation to Chat Dialogues
The method described in the previous section is common to dialogues and newspaper articles. This
section describes the adaptation made to target dialogues.
5.1 Adaptation of Model Parameters
In order to tune the difference in the argument distribution, model parameters of the selectors are adapted
to the dialogue domain. We use the feature augmentation method (Daum?e, 2007) as the domain adap-
tation technique; it has the same effect as regarding the source domain to be prior knowledge, and the
parameters are optimized to the target domain. Concretely, the models of the selectors are learnt and
applied as follows.
1. First, the feature space is segmented into three parts: common, source, and target.
2. The NAIST Corpus and the Dialogue Corpus are regarded as the source and the target domains,
respectively. The features from the NAIST Corpus are deployed to the common and the source
spaces, and those from the Dialogue Corpus are deployed to the common and the target spaces.
3. The parameters are estimated in the usual way on the above feature space. The weights of the
common features are emphasized if the features are consistent between the source and target. With
regard to domain-dependent features, the weights in the respective space, source or target, are em-
phasized.
4. When the argument is identified, the selectors use only the features in the common and target spaces.
The parameters in the spaces are optimized to the target domain, plus we can utilize the features
that appear only in the source domain data.
5.2 Weak Knowledge Acquisition from Very Large Resources
In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the
training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large
811
text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide
information about unknown words with some confidence but they do contain some errors. We use them
as the features of the models, and parameters are optimized by the discriminative learning of the selectors.
5.2.1 Obligatory Case Information (Frame Feature)
Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises
subsets of the case frames that only clarify whether the cases of each predicate are necessary or not.
The OCI dictionary is automatically constructed from large text corpora as follows. The process
assumes that 1) most of the cases match the case markers if the noun phrase directly depends on the
predicate, and 2) if the case is obligatory, the occurrence rate on a specific predicate is higher than the
average rate of all predicates.
1. Similar to PASA in this paper (c.f., Section 4.1), predicates and base phrases are identified by
tagging and parsing raw texts.
2. Noun phrases that directly depend on the predicate and accompany a case marker are extracted. We
sum up the frequency of the predicate and cases.
3. Highly frequent predicates are selected according to the final dictionary size. Obligation of the cases
is determined so as to satisfy the following two conditions.
? Co-occurrence of the predicate and the case ?v, c? are higher than the significance level (p ?
0.001; LLR ? 10.83) by the log-likelihood-ratio test.
? The case of the predicate appears at least 10% more frequently than the average of all predi-
cates.
We constructed two OCI dictionaries. The Blog dictionary contains about 480k predicates from one
year of blogs (about 2.3G sentences,). The News dictionary contains about 200k predicates from 12
years of newspaper articles (about 7.7M sentences). The coverage of predicates in the training set of the
Dialogue Corpus was 98.5% by the Blog dictionary and 96.4% by the News dictionary.
5.2.2 Dependency Language Models
Dependency language models (LMs) represent semantic/pragmatic collocations among predicate v, case
c, and noun phrase n. The generation probabilities of v, c, and n are computed by n-gram models. More
concretely, the following real values are computed. The purpose of the biases (probabilities involved
<unk>) is to correct the values to be positive.
? logP (n|c, v) ? logP (<unk>|c, v)
? logP (v|c, n) ? logP (v|c,<unk>)
? logP (c|n) ? logP (c|<unk>)
Each dependency LM is constructed from the tuples of ?v, c, n? extracted in Section 5.2.1 using the
SRILM (Stolcke et al., 2011). Note that since the obligatory case information corresponds to the gener-
ation probability of the case (P (c|v)), we exclude it from the dependency LMs.
Similar to the OCI dictionaries, we constructed two sets of dependency language models from the Blog
and the News sentences. The coverage of triples ?v, c, n? appeared in the training set of the Dialogue
Corpus was 76.4% by the Blog LMs and 38.3% by the News LMs. The Blog LMs cover the Dialogue
Corpus more comprehensively than the News LMs.
6 Experiments
We evaluate the accuracies of the proposed PASA on the Dialogue Corpus (Table 1) from the perspectives
of parameter adaptation and the effect of the automatically acquired knowledge. The evaluation metric
is F-measure of each case (includes exophora identification).
812
a) Adaptation b) NAIST? c) Dialogue? d) Adaptation e) Adaptation
# of OCI:Blog OCI:Blog OCI:Blog OCI:News? OCI:Blog
Case Type Args. LMs:Blog LMs:Blog LMs:Blog LMs:Blog LMs:News?
Nominative Dep 1,811 83.3%?? 77.6% 82.7% 83.0% 82.7%
Zero-Intra 511 37.4% 43.7%? 36.6% 36.5% 38.1%
Zero-Inter 767 8.6% ? 9.1% 9.0% 8.3% 4.5%
exo1 1,193 70.2%? 13.5% 69.9% 70.1% 70.3%
exo2 281 46.8%?? 0.0% 43.1% 47.2% 46.8%
exog 767 46.8%? 32.5% 27.9% 47.2% 47.7%?
Total 5,330 61.5%? 44.4% 61.1% 61.4% 61.4%
Accusative Dep 614 84.2%?? ? 78.6% 81.5% 84.2% 82.4%
Zero-Intra 149 42.9%? ?? 27.1% 45.0% 38.9% 34.3%
Zero-Inter 399 30.4%? ? 0.5% 30.9% 29.4% 24.3%
exo1 19 0.0% 0.0% 0.0% 9.5% 10.0%
exo2 7 0.0% 0.0% 0.0% 0.0% 0.0%
exog 98 25.6%? 0.0% 27.9% 25.2% 25.6%
Total 1,286 59.0%? ? 51.6% 58.9% 58.4% 56.0%
Dative Dep 566 80.5%?? 54.0% 79.0% 80.1% 80.7%
Zero-Intra 70 20.7%? ? 0.0% 20.0% 20.7% 11.8%
Zero-Inter 169 14.6%? 0.0% 14.8% 14.4% 13.4%
exo1 32 0.0% 0.0% 0.0% 0.0% 0.0%
exo2 4 0.0% 0.0% 0.0% 0.0% 0.0%
exog 265 45.4%?? 0.0% 43.1% 44.0% 44.9%
Total 1,106 58.6%?? 32.2% 57.2% 58.2% 58.4%
Table 4: F-measures among Methods/OCI dictionary/Dependency LMs on Dialogue Test Set
The bold values denote the highest F-measures among all methods. The marks ?, ?, ?, ? denote sig-
nificantly better methods by comparing a) with b), c), d), and e), respectively. We used the bootstrap
resampling method (1,000 iterations) as the significance test, in which the significance level was 0.05.
6.1 Experiment 1: Effect of Parameter Adaptation
We compared three methods in order to evaluate parameter adaptation: a) The feature augmentation is
applied to the training (Adaptation). b) Only the NAIST Corpus is used for training (NAIST Training).
c) Only the Dialogue Corpus is used (Dialogue Training). The NAIST Training corresponds to a conven-
tional PASA for newspaper articles. The results on the Dialogue test set are shown in the 4th, 5th, and
6th columns in Table 4.
First, comparing methods a) Adaptation and b) NAIST training, Adaptation was better than the NAIST
training for most types (The ? mark denotes ?significantly better?). In particular, the total F-measures
of all cases were significantly better than NAIST training. Focusing on the types of arguments, the most
characteristic results were exophoras of the first/second persons (exo1 and exo2) of the nominative
case. These two types dominate of the nominative case (about 28%), and exo1 (70.2%) and exo2
(46.8%) became analyzable. Other types such as the Zero-Inter and the exog of the accusative and
dative cases, which could not be analyzed by NAIST training, became analyzable.
Comparing methods a) Adaptation and c) Dialogue training (c.f., ?), the F-measures of Dialogue
training approached those of Adaptation even though the size of the Dialogue Corpus was small. Only
the F-measure of the dative case of Adaptation was significantly better than Dialogue training in total.
This does not imply that the corpus size is sufficient. Rather, we suppose that the Adaptation strategy
could not adequately utilize the advantages of the NAIST Corpus. Adding more dialogue data would
further improve the accuracies on the Dialogue test set.
6.2 Experiment 2: Differences among Automatically Acquired Knowledge
The columns a), d), and e) in Table 4 show the results for the proposed method (Adaptation). Note that
the combination of the OCI dictionary and the dependency language models were changed to a) ?Blog,
Blog?, d) ?News, Blog?, and e) ?Blog, News?.
When the OCI dictionary was changed from a) Blog to d) News (c.f., ?), there were no significant
differences in almost all types except for the Zero-Intra of the accusative case. We suppose that this
813
is because the coverage of the Blog and News dictionaries were almost the same, and obligatory cases of
predicates are general information regardless of the domain.
On the contrary, when the dependency LMs were changed from a) Blog to e) News (c.f., ?), the F-
measures of some types significantly dropped, especially the Zero-Intra and Zero-Inter types,
which are strongly influenced by semantic relation. For example, the Zero-Inter type of the ac-
cusative case was changed from 30.4% to 24.3%, and the F-measure consequently decreased by 3.0
points in total in the accusative case. Zero-anaphora resolution cannot rely on syntax, and the dependency
LMs that measure semantic collocation become relatively important. The Blog LMs yielded greater cov-
erage than the News LMs in this experiment. We can conclude that high-coverage LMs are better for
improving the zero-anaphora resolution.
7 Conclusion
This paper presented predicate-argument structure analysis with zero-anaphora resolution for dialogues.
We regarded this task as a kind of domain adaptation from newspaper articles, which are conventionally
studied, to dialogues. The model parameters were adapted to the dialogues by using a domain adapta-
tion technique. In order to address the out-of-vocabulary issue, the obligatory case information and the
dependency language models were constructed from large text corpora and applied to the selectors.
As a result, arguments that could not be analyzed by PASA for newspaper articles (e.g., zero-pronouns
of the first and second persons in the nominative case) became analyzable by adding only a small number
of dialogues. The parameter adaptation achieved some improvement. Moreover, we confirmed that high-
coverage dependency LMs contribute to improving zero-anaphora resolution and the overall accuracy.
Although we focused on parameter distribution and out-of-vocabulary in this paper, there are the other
differences between dialogues and newspaper articles. For example, we did not discuss the exchange
of turns, which is a special phenomenon of dialogues. To consider further phenomena is our future
work. We are also evaluating the effectiveness of our PASA by incorporating it into a dialogue system
(Higashinaka et al., 2014).
References
Xavier Carreras and Llu??s M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling.
In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational
Natural Language Learning (CoNLL-2004), pages 89?97, Boston, Massachusetts, USA, May.
Xavier Carreras and Llu??s M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling.
In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages
152?164, Ann Arbor, Michigan, June.
Hal Daum?e, III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?263, Prague, Czech Republic, June.
Matthew Gerber and Joyce Y. Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates.
Computational Linguistics, 38(4):755?798.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Ryuichiro Higashinaka, Kenji Imamura, Toyomi Meguro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki
Sugiyama, Toru Hirano, Toshiro Makino, and Yoshihiro Matsuo. 2014. Towards an open domain conversa-
tional system fully based on natural language processing. In Proceedings of the 25th International Conference
on Computational Linguistics (COLING 2014), Dublin, Ireland, August.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, pages 57?60, New York City, USA, June.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with
predicate-argument and coreference relations. In Proceedings of the Linguistic Annotation Workshop, pages
132?139, Prague, Czech Republic, June.
814
Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicate-argument structure
analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,
pages 85?88, Singapore, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role labeling of NomBank: A maximum entropy approach.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 138?145,
Sydney, Australia, July.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust Japanese case
analysis. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002),
pages 425?431, Taipei, Taiwan, August.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Learning-based argument structure analy-
sis of event-nouns in Japanese. In Proceedings of the Conference of the Pacific Association for Computational
Linguistics (PACLING), pages 208?215, Melbourne, Australia, September.
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL
2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference
Workshops), pages 63?69, Taipei, Taiwan.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese
morphological analysis. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 230?237,
Barcelona, Spain, July.
Egoitz Laparra and German Rigau. 2013. ImpAr: A deterministic algorithm for implicit semantic role labelling.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1180?1189, Sofia, Bulgaria, August.
Llu??s M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling:
An introduction to the special issue. Computational Linguistics, 34(2):145?159.
Yuichiro Matsubayashi, Ryu Iida, Ryohei Sasano, Hikaru Yokono, Suguru Matsuyoshi, Atsushi Fujita, Yusuke
Miyao, and Kentaro Inui. 2014. Issues on annotation guidelines for Japanese predicate-argument structures.
Journal of Natural Language Processing, 21(2):333?377, April. in Japanese.
Martha Palmer, Daniel Gildia, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
Sameer S. Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Computa-
tional Linguistics, 34(2):289?310.
Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue, editors. 2012. Joint Conference on EMNLP and
CoNLL: Proceeding of the Shared Task: Modeling Multilingual Unrestricted Coreference in Onto Notes, Jeju,
Korea, July.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for
Japanese zero anaphora resolution. In Proceedings of the 22nd International Conference on Computational
Linguistics (Coling 2008), pages 769?776, Manchester, UK, August.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi, and Manabu Okumura. 2013. Automatic knowledge ac-
quisition for case alternation between the passive and active voices in Japanese. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1213?1223, Seattle, Washington,
USA, October.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2011), Waikoloa,
Hawaii, December.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata. 2008. A Japanese predicate argument structure analysis using
decision lists. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,
pages 523?532, Honolulu, Hawaii, October.
Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Matsumoto. 2011. Jointly extracting Japanese predicate-
argument relation with markov logic. In Proceedings of 5th International Joint Conference on Natural Lan-
guage Processing, pages 1125?1133, Chiang Mai, Thailand, November.
815
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 928?939, Dublin, Ireland, August 23-29 2014.
Towards an open-domain conversational system fully based on natural
language processing
Ryuichiro Higashinaka
1
, Kenji Imamura
1
, Toyomi Meguro
2
, Chiaki Miyazaki
1
Nozomi Kobayashi
1
, Hiroaki Sugiyama
2
, Toru Hirano
1
Toshiro Makino
1
, Yoshihiro Matsuo
1
1
NTT Media Intelligence Laboratories
2
NTT Communication Science Laboratories
{higashinaka.ryuichiro, imamura.kenji, meguro.toyomi, miyazaki.chiaki,
kobayashi.nozomi, sugiyama.hiroaki, hirano.tohru,
makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
This paper proposes an architecture for an open-domain conversational system and evaluates an
implemented system. The proposed architecture is fully composed of modules based on natu-
ral language processing techniques. Experimental results using human subjects show that our
architecture achieves significantly better naturalness than a retrieval-based baseline and that its
naturalness is close to that of a rule-based system using 149K hand-crafted rules.
1 Introduction
Although task-oriented dialogue systems have been extensively researched over the decades (Walker
et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain
conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore
and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational
system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge
for a domain and create understanding and generation modules for that domain (Nakano et al., 2000).
However, for open-domain conversation, such preparation cannot be performed. Since it is difficult to
handle users? open-domain utterances, to create workable systems, conventional approaches have used
hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rule-
based approach is the high cost and the dependence on individual skills of developers, which hinders
systematic development. Another problem with the rule-based approach is its low coverage; that is, the
inability to handle unexpected utterances.
The recent increase of web data has propelled the development of approaches that use data retrieved
from the web for open-domain conversation (Shibata et al., 2009; Ritter et al., 2011). The merit of such
retrieval-based approaches is that, owing to the diversity of the web, systems can retrieve at least some
responses for user input, which solves the coverage problem. However, this comes at the cost of utterance
quality. Since the web, especially Twitter, is inherently noisy, it is, in many cases, difficult to sift out
appropriate sentences from retrieval results.
In this paper, we propose an architecture for an open-domain conversational system. The proposed
architecture is fully composed of modules based on natural language processing (NLP) techniques. Our
stance is not just to hand-craft or to search the web for utterances, but to create a system that can fully
understand and generate utterances. We want to show that it is possible to build an open-domain conver-
sational system by combining NLP modules, which will open the way to a systematic development and
improvement. We describe our open-domain conversational system based on our architecture and present
results of an evaluation of its performance by human subjects. We compare our system with rule-based
and retrieval-based systems, and show that our architecture is a promising direction. In this work, we
regard the term open-domain conversation to be interchangeable with non-task-oriented dialogue, casual
conversation (Eggins and Slade, 2005), chat, or social dialogue (Bickmore and Cassell, 2000). We use
the term to denote that user input is not restricted in any way as in open-domain question answering
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
928
	
			
	

	
		
	

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726?731,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Entity Set Expansion using Topic information
Kugatsu Sadamitsu, Kuniko Saito, Kenji Imamura and Genichiro Kikui?
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
{sadamitsu.kugatsu, saito.kuniko, imamura.kenji}@lab.ntt.co.jp
kikui@cse.oka-pu.ac.jp
Abstract
This paper proposes three modules based on
latent topics of documents for alleviating ?se-
mantic drift? in bootstrapping entity set ex-
pansion. These new modules are added to a
discriminative bootstrapping algorithm to re-
alize topic feature generation, negative exam-
ple selection and entity candidate pruning. In
this study, we model latent topics with LDA
(Latent Dirichlet Allocation) in an unsuper-
vised way. Experiments show that the accu-
racy of the extracted entities is improved by
6.7 to 28.2% depending on the domain.
1 Introduction
The task of this paper is entity set expansion in
which the lexicons are expanded from just a few
seed entities (Pantel et al, 2009). For example,
the user inputs a few words ?Apple?, ?Google? and
?IBM? , and the system outputs ?Microsoft?, ?Face-
book? and ?Intel?.
Many set expansion algorithms are based on boot-
strapping algorithms, which iteratively acquire new
entities. These algorithms suffer from the general
problem of ?semantic drift?. Semantic drift moves
the extraction criteria away from the initial criteria
demanded by the user and so reduces the accuracy
of extraction. Pantel and Pennacchiotti (2006) pro-
posed Espresso, a relation extraction method based
on the co-training bootstrapping algorithm with en-
tities and attributes. Espresso alleviates semantic-
drift by a sophisticated scoring system based on
? Presently with Okayama Prefectural University
pointwise mutual information (PMI). Thelen and
Riloff (2002), Ghahramani and Heller (2005) and
Sarmento et al (2007) also proposed original score
functions with the goal of reducing semantic-drift.
Our purpose is also to reduce semantic drift. For
achieving this goal, we use a discriminative method
instead of a scoring function and incorporate topic
information into it. Topic information means the
genre of each document as estimated by statisti-
cal topic models. In this paper, we effectively uti-
lize topic information in three modules: the first
generates the features of the discriminative mod-
els; the second selects negative examples; the third
prunes incorrect examples from candidate examples
for new entities. Our experiments show that the pro-
posal improves the accuracy of the extracted entities.
The remainder of this paper is organized as fol-
lows. In Section 2, we illustrate discriminative boot-
strapping algorithms and describe their problems.
Our proposal is described in Section 3 and experi-
mental results are shown in Section 4. Related works
are described in Section 5. Finally, Section 6 pro-
vides our conclusion and describes future works.
2 Problems of the previous Discriminative
Bootstrapping method
Some previous works introduced discriminative
methods based on the logistic sigmoid classifier,
which can utilize arbitrary features for the relation
extraction task instead of a scoring function such as
Espresso (Bellare et al, 2006; Mintz et al, 2009).
Bellare et al reported that the discriminative ap-
proach achieves better accuracy than Espresso when
the number of extracted pairs is increased because
726
multiple features are used to support the evidence.
However, three problems exist in their methods.
First, they use only local context features. The dis-
criminative approach is useful for using arbitrary
features, however, they did not identify which fea-
ture or features are effective for the methods. Al-
though the context features and attributes partly re-
duce entity word sense ambiguity, some ambiguous
entities remain. For example, consider the domain
broadcast program (PRG) and assume that PRG?s
attribute is advertisement. A false example is shown
here: ?Android ?s advertisement employs Japanese
popular actors. The attractive smartphone begins to
target new users who are ordinary people.? The en-
tity Android belongs to the cell-phone domain, not
PRG, but appears with positive attributes or contexts
because many cell-phones are introduced in adver-
tisements as same as broadcast program. By us-
ing topic, i.e. the genre of the document, we can
distinguish ?Android? from PRG and remove such
false examples even if the false entity appeared with
positive context strings or attributes. Second, they
did not solve the problem of negative example se-
lection. Because negative examples are necessary
for discriminative training, they used all remaining
examples, other than positive examples, as negative
examples. Although this is the simplest technique,
it is impossible to use all of the examples provided
by a large-scale corpus for discriminative training.
Third, their methods discriminate all candidates for
new entities. This principle increases the risk of gen-
erating many false-positive examples and is ineffi-
cient. We solve these three problems by using topic
information.
3 Set expansion using Topic information
3.1 Basic bootstrapping methods
In this section, we describe the basic method
adopted from Bellare (Bellare et al, 2006). Our
system?s configuration diagram is shown in Figure
1. In Figure 1, arrows with solid lines indicate the
basic process described in this section. The other
parts are described in the following sections. After
Ns positive seed entities are manually given, every
noun co-occurring with the seed entities is ranked
by PMI scores and then selected manually as Na
positive attributes. Ns and Na are predefined ba-
Figure 1: The structure of our system.
sic adjustment numbers. The entity-attribute pairs
are obtained by taking the cross product of seed en-
tity lists and attribute lists. The pairs are used as
queries for retrieving the positive documents, which
include positive pairs. The document set De,a in-
cluding same entity-attribute pair {e, a} is regarded
as one example Ee,a to alleviate over-fitting for con-
text features. These are called positive examples in
Figure 1. Once positive examples are constructed,
discriminative models can be trained by randomly
selecting negative examples.
Candidate entities are restricted to only the
Named Entities that lie in the close proximity to the
positive attributes. These candidates of documents,
including Named Entity and positive attribute pairs,
are regarded as one example the same as the train-
ing data. The discriminative models are used to cal-
culate the discriminative positive score, s(e, a), of
each candidate pair, {e, a}. Our system extracts Nn
types of new entities with high scores at each iter-
ation as defined by the summation of s(e, a) of all
positive attributes (AP );
?
a?AP s(e, a). Note that
we do not iteratively extract new attributes because
our purpose is entity set expansion.
3.2 Topic features and Topic models
In previous studies, context information is only used
as the features of discriminative models as we de-
scribed in Section 2. Our method utilizes not only
context features but also topic features. By utiliz-
ing topic information, our method can disambiguate
the entity word sense and alleviate semantic drift.
In order to derive the topic information, we utilize
statistical topic models, which represent the relation
727
between documents and words through hidden top-
ics. The topic models can calculate the posterior
probability p(z|d) of topic z in document d. For
example, the topic models give high probability to
topic z =?cell-phone? in the above example sen-
tences 1. This posterior probability is useful as a
global feature for discrimination. The topic feature
value ?t(z, e, a) is calculated as follows.
?t(z, e, a) =
?
d?De,a p(z|d)
?
z?
?
d?De,a p(z?|d)
.
In this paper, we use Latent Dirichlet Allocation
(LDA) as the topic models (Blei et al, 2003). LDA
represents the latent topics of the documents and the
co-occurrence between each topic.
In Figure 1, shaded part and the arrows with bro-
ken lines indicate our proposed method with its use
of topic information including the following sec-
tions.
3.3 Negative example selection
If we choose negative examples randomly, such ex-
amples are harmful for discrimination because some
examples include the same contexts or topics as the
positive examples. By contrast, negative examples
belonging to broad genres are needed to alleviate se-
mantic drift. We use topic information to efficiently
select such negative examples.
In our method, the negative examples are cho-
sen far from the positive examples according to the
measure of topic similarity. For calculating topic
similarity, we use a ranking score called ?positive
topic score?, PT (z), defined as follows, PT (z) =
?
d?DP p(z|d), where DP indicates the set of pos-
itive documents and p(z|d) is topic posterior prob-
ability for a given positive document. The bottom
50% of the topics sorted in decreasing order of pos-
itive topic score are used as the negative topics.
Our system picks up as many negative documents
as there are positive documents with each selected
negative topic being equally represented.
3.4 Candidate Pruning
Previous works discriminate all candidates for ex-
tracting new entities. Our basic system can constrain
1z is a random variable whose sample space is represented
as a discrete variable, not explicit words.
the candidate set by positive attributes, however, this
is not enough as described in Section 2. Our candi-
date pruning module, described below, uses the mea-
sure of topic similarity to remove obviously incor-
rect documents.
This pruning module is similar to negative exam-
ple selection described in the previous section. The
positive topic score, PT , is used as a candidate con-
straint. Taking all positive examples, we select the
positive topics, PZ, which including all topics z sat-
isfying the condition PT (z) > th. At least one
topic with the largest score is chosen as a positive
topic when PT (z) ? th about all topics. After se-
lecting this positive topic, the documents including
entity candidates are removed if the posterior prob-
ability satisfy p(z|d) ? th for all topics z. In this
paper, we set the threshold to th = 0.2. This con-
straint means that the topic of the document matches
that of the positive entities and can be regarded as a
hard constraint for topic features.
4 Experiments
4.1 Experimental Settings
We use 30M Japanese blog articles crawled in May
2008. The documents were tokenized by JTAG
(Fuchi and Takagi, 1998), chunked, and labeled with
IREX 8 Named Entity types by CRFs using Mini-
mum Classification Error rate (Suzuki et al, 2006),
and transformed into features. The context features
were defined using the template ?(head) entity (mid.)
attribute (tail)?. The words included in each part
were used as surface, part-of-speech and Named En-
tity label features added position information. Max-
imum word number of each part was set at 2 words.
The features have to appear in both the positive and
negative training data at least 5 times.
In the experiments, we used three domains, car
(?CAR?), broadcast program (?PRG?) and sports or-
ganization (?SPT?). The adjustment numbers for ba-
sic settings are Ns = 10, Na = 10, Nn = 100. Af-
ter running 10 iterations, we obtained 1000 entities
in total. SVM light (Joachims, 1999) with second
order polynomial kernel was used as the discrimina-
tive model. Parallel LDA, which is LDA with MPI
(Liu et al, 2011), was used for training 100 mix-
ture topic models and inference. Training corpus for
topic models consisted of the content gathered from
728
CAR PRG SPT
1. Baseline 0.249 0.717 0.781
2. Topic features + 1. 0.483 0.727 0.844
3. Negative selection + 2. 0.509 0.762 0.846
4. Candidate pruning + 3. 0.531 0.824 0.848
Table 1: The experimental results for the three domains.
Bold font indicates that the difference between accuracy
of the methods in the row and the previous row is signifi-
cant (P < 0.05 by binomial test) and italic font indicates
(P < 0.1).
14 days of blog articles. In the Markov-chain Monte
Carlo (MCMC) method, sampling was iterated 200
times for training with a burn-in taking 50 iterations.
These parameters were selected based on the results
of a preliminary experiment.
Four experimental settings were examined. First
is Baseline; it is described in Section 3.1. Second is
the first method with the addition of topic features.
Third is the second method with the addition of a
negative example selection module. Fourth is the
third method with the addition of a candidate prun-
ing module (equals the entire shaded part in Fig-
ure 1). Each extracted entity is labeled with cor-
rect or incorrect by two evaluators based on the re-
sults of a commercial search engine. The ? score for
agreement between evaluators was 0.895. Because
the third evaluator checked the two evaluations and
confirmed that the examples which were judged as
correct by either one of the evaluators were correct,
those examples were counted as correct.
4.2 Experimental Results
Table 1 shows the accuracy and significance for each
domain. Using topic features significantly improves
accuracy in the CAR and SPT domains. The nega-
tive example selection module improves accuracy in
the CAR and PRG domains. This means the method
could reduce the risk of selecting false-negative ex-
amples. Also, the candidate pruning method is ef-
fective for the CAR and PRG domains. The CAR
domain has lower accuracy than the others. This
is because similar entities such as motorcycles are
extracted; they have not only the same context but
also the same topic as the CAR domain. In the SPT
domain, the method with topic features offer signif-
icant improvements in accuracy and no further im-
provement was achieved by the other two modules.
To confirm whether our modules work properly,
we show some characteristic words belonging to
each topic that is similar and not similar to target do-
main in Table 2. Table 2 shows characteristic words
for one positive topic zh and two negative topics zl
and ze, defined as follow.
? zh (the second row) is the topic that maximizes
PT (z), which is used as a positive topic.
? zl (the fourth row) is the topic that minimizes
PT (z), which is used as a negative topic.
? ze (the fifth row) is a topic that, we consider, ef-
fectively eliminates ?drifted entities? extracted
by the baseline method. ze is eventually in-
cluded in the lower half of topic list sorted by
PT (z).
For a given topic, z, we chose topmost three words
in terms of topic-word score. The topic-word score
of a word, v, is defined as p(v|z)/p(v), where p(v)
is the unigram probability of v, which was estimated
by maximum likelihood estimation. For utilizing
candidate pruning, near topics including zh must be
similar to the domain. By contrast, for utilizing neg-
ative example selection, the lower half of topics, zl,
ze and other negative topics, must be far from the
domain. Our system succeeded in achieving this.
As shown in ?CAR? in Table 2, the nearest topic
includes ?shaken? (automobile inspection) and the
farthest topic includes ?naika? (internal medicine)
which satisfies our expectation. Furthermore, the ef-
fective negative topic is similar to the topic of drifted
entity sets (digital device). This indicates that our
method successfully eliminated drifted entities. We
can confirm that the other domains trend in the same
direction as ?CAR? domain.
5 Related Works
Some prior studies use every word in a docu-
ment/sentence as the features, such as the distribu-
tional approaches (Pantel et al, 2009). These meth-
ods are regarded as using global information, how-
ever, the space of word features are sparse, even if
the amount of data available is large. Our approach
can avoid this problem by using topic models which
729
domain CAR PRG SPT
words of the
nearest topic zh
(highest PT score)
shaken
(automobile inspection),
nosha (delivering a car),
daisha (loaner car)
Mari YAMADA,
Tohru KUSANO,
Reiko TOKITA
(Japanese stars)
toshu (pitcher),
senpatsu
(starting member),
shiai (game)
drifted entities
(using baseline)
iPod, mac
(digital device)
PS2, XBOX360
(video game)
B?z, CHAGE&ASKA
(music)
words of effective
negative topic ze
(Lower half of
PT score)
gaso (pixel),
kido (brightness),
mazabodo (mother board)
Lv. (level),
kariba (hunting area),
girumen (guild member)
sinpu (new release),
X JAPAN ,
Kazuyoshi Saito
(Japanese musicians)
words of
the farthest topic zl
(Lowest PT score)
naika (internal medicine),
hairan (ovulation),
shujii (attending doctor)
tsure (hook a fish),
choka (result of hooking),
choko (diary of hooking)
toritomento (treatment),
keana (pore),
hoshitsu (moisture retention)
Table 2: The characteristic words belonging to three topics, zh, zl and ze. zh is the nearest topic and zl is the farthest
topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating ?drifted entities? extracted
by the baseline system.
are clustering methods based on probabilistic mea-
sures. By contrast, Pas?ca and Durme (2008) pro-
posed clustering methods that are effective in terms
of extraction, even though their clustering target is
only the surrounding context. Ritter and Etzioni
(2010) proposed a generative approach to use ex-
tended LDA to model selectional preferences. Al-
though their approach is similar to ours, our ap-
proach is discriminative and so can treat arbitrary
features; it is applicable to bootstrapping methods.
The accurate selection of negative examples is a
major problem for positive and unlabeled learning
methods or general bootstrapping methods and some
previous works have attempted to reach a solution
(Liu et al, 2002; Li et al, 2010). However, their
methods are hard to apply to the Bootstrapping al-
gorithms because the positive seed set is too small
to accurately select negative examples. Our method
uses topic information to efficiently solve both the
problem of extracting global information and the
problem of selecting negative examples.
6 Conclusion
We proposed an approach to set expansion that uses
topic information in three modules and showed that
it can improve expansion accuracy. The remaining
problem is that the grain size of topic models is not
always the same as the target domain. To resolve
this problem, we will incorporate the active learning
or the distributional approaches. Also, comparisons
with the previous works are remaining work. From
another perspective, we are considering the use of
graph-based approaches (Komachi et al, 2008) in-
corporated with the topic information using PHITS
(Cohn and Chang, 2000), to further enhance entity
extraction accuracy.
References
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2006. Lightly-supervised at-
tribute extraction. In Proceedings of the Advances in
Neural Information Processing Systems Workshop on
Machine Learning for Web Search.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
David Cohn and Huau Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings of the 17th International Conference on Ma-
chine Learning, pages 167?174.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese
Morphological Analyzer using Word Co-occurrence-
JTAG. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, pages 409?413.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Proceedings of the Advances in Neu-
ral Information Processing Systems.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning. Software available at
http://svmlight.joachims.org/.
730
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1011?1020.
Xiao-Li Li, Bing Liu, and See-Kiong Ng. 2010. Neg-
ative Training Data can be Harmful to Text Classi-
fication. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 218?228.
Bing Liu, Wee S. Lee, Philip S. Yu, and Xiaoli Li. 2002.
Partially supervised classification of text documents.
In Proceedings of the 19th International Conference
on Machine Learning, pages 387?394.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. PLDA+: Parallel latent dirich-
let alocation with data placement and pipeline pro-
cessing. ACM Transactions on Intelligent Systems
and Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, pages 19?27.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113?120.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938?
947.
Alan Ritter and Oren Etzioni. 2010. A Latent Dirich-
let Allocation method for Selectional Preferences. In
Proceedings of the 48th ACL Conference, pages 424?
434.
Luis Sarmento, Valentin Jijkuon, Maarten de Rijke, and
Eugenio Oliveira. 2007. More like these: grow-
ing entity classes from seeds. In Proceedings of the
16th ACM Conference on Information and Knowledge
Management, pages 959?962.
Jun Suzuki, Erik McDermott, and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multivari-
ate Evaluation Measures. In Proceedings of the 21st
COLING and 44th ACL Conference, pages 217?224.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using ex-
traction pattern contexts. In Proceedings of the 2002
conference on Empirical methods in natural language
processing, pages 214?221.
731
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 388?392,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Grammar Error Correction
Using Pseudo-Error Sentences and Domain Adaptation
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi Nishikawa
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan
{
imamura.kenji, saito.kuniko
sadamitsu.kugatsu, nishikawa.hitoshi
}
@lab.ntt.co.jp
Abstract
This paper presents grammar error correction
for Japanese particles that uses discrimina-
tive sequence conversion, which corrects erro-
neous particles by substitution, insertion, and
deletion. The error correction task is hindered
by the difficulty of collecting large error cor-
pora. We tackle this problem by using pseudo-
error sentences generated automatically. Fur-
thermore, we apply domain adaptation, the
pseudo-error sentences are from the source
domain, and the real-error sentences are from
the target domain. Experiments show that sta-
ble improvement is achieved by using domain
adaptation.
1 Introduction
Case marks of a sentence are represented by postpo-
sitional particles in Japanese. Incorrect usage of the
particles causes serious communication errors be-
cause the cases become unclear. For example, in
the following sentence, it is unclear what must be
deleted.
mail o todoi tara sakujo onegai-shi-masu
mail ACC. arrive when delete please
?When ? has arrived an e-mail, please delete it.?
If the accusative particle o is replaced by a nomi-
native one ga, it becomes clear that the writer wants
to delete the e-mail (?When the e-mail has arrived,
please delete it.?). Such particle errors frequently
occur in sentences written by non-native Japanese
speakers.
This paper presents a method that can automat-
ically correct Japanese particle errors. This task
corresponds to preposition/article error correction in
English. For English error correction, many stud-
ies employ classifiers, which select the appropriate
prepositions/articles, by restricting the error types
to articles and frequent prepositions (Gamon, 2010;
Han et al, 2010; Rozovskaya and Roth, 2011).
On the contrary, Mizumoto et al (2011) proposed
translator-based error correction. This approach can
handle all error types by converting the learner?s
sentences into the correct ones. Although the target
of this paper is particle error, we employ a similar
approach based on sequence conversion (Imamura
et al, 2011) since this offers excellent scalability.
The conversion approach requires pairs of the
learner?s and the correct sentences. However, col-
lecting a sufficient number of pairs is expensive. To
avoid this problem, we use additional corpus con-
sisting of pseudo-error sentences automatically gen-
erated from correct sentences that mimic the real-
errors (Rozovskaya and Roth, 2010b). Furthermore,
we apply a domain adaptation technique that re-
gards the pseudo-errors and the real-errors as the
source and the target domain, respectively, so that
the pseudo-errors better match the real-errors.
2 Error Correction by Discriminative
Sequence Conversion
We start by describing discriminative sequence con-
version. Our error correction method converts the
learner?s word sequences into the correct sequences.
Our method is similar to phrase-based statistical ma-
chine translation (PBSMT), but there are three dif-
ferences; 1) it adopts the conditional random fields,
2) it allows insertion and deletion, and 3) binary and
real features are combined. Unlike the classification
388
Incorrect Particle Correct Particle Note
? no/POSS. INS
? o/ACC. INS
ga/NOM. o/ACC. SUB
o/ACC. ni/DAT. SUB
o/ACC. ga/NOM. SUB
wa/TOP. o/ACC. SUB
no/POSS. ? DEL
: :
Table 1: Example of Phrase Table (partial)
approach, the conversion approach can correct mul-
tiple errors of all types in a sentence.
2.1 Basic Procedure
We apply the morpheme conversion approach that
converts the results of a speech recognizer into word
sequences for language analyzer processing (Ima-
mura et al, 2011). It corrects particle errors in the
input sentences as follows.
? First, all modification candidates are obtained by
referring to a phrase table. This table, called the
confusion set (Rozovskaya and Roth, 2010a) in
the error correction task, stores pairs of incorrect
and correct particles (Table 1). The candidates are
packed into a lattice structure, called the phrase
lattice (Figure 1). To deal with unchanged words,
it also copies the input words and inserts them into
the phrase lattice.
? Next, the best phrase sequence in the phrase lat-
tice is identified based on the conditional random
fields (CRFs (Lafferty et al, 2001)). The Viterbi
algorithm is applied to the decoding because error
correction does not change the word order.
? While training, word alignment is carried out by
dynamic programming matching. From the align-
ment results, the phrase table is constructed by ac-
quiring particle errors, and the CRF models are
trained using the alignment results as supervised
data.
2.2 Insertion / Deletion
Since an insertion can be regarded as replacing an
empty word with an actual word, and deletion is the
replacement of an actual word with an empty one,
we treat these operations as substitution without dis-
tinction while learning/applying the CRF models.
mailnounInput Words oACC. todoiverb taraPART ?
Phrase Lattice mail o todoi tara
copy INS copySUB copy copy
<s>
Incorrect Particle
noun
noPOSS.
ACC.
gaNOM.
niDAT.
verb PART
oACC.
Figure 1: Example of Phrase Lattice
However, insertion is a high cost operation be-
cause it may occur at any location and can cause
lattice size to explode. To avoid this problem, we
permit insertion only immediately after nouns.
2.3 Features
In this paper, we use mapping features and link fea-
tures. The former measure the correspondence be-
tween input and output words (similar to the trans-
lation models of PBSMT). The latter measure the
fluency of the output word sequence (similar to lan-
guage models).
The mapping features are all binary. The focusing
phrase and its two surrounding words of the input
are regarded as the window. The mapping features
are defined as the pairs of the output phrase and 1-,
2-, and 3-grams in the window.
The link features are important for the error cor-
rection task because the system has to judge output
correctness. Fortunately, CRF, which is a kind of
discriminative model, can handle features that de-
pend on each other; we mix two types of features
as follows and optimize their weights in the CRF
framework.
? N -gram features: N -grams of the output words,
from 1 to 3, are used as binary features. These
are obtained from a training corpus (paired sen-
tences). Since the feature weights are optimized
considering the entire feature space, fine-tuning
can be achieved. The accuracy becomes almost
perfect on the training corpus.
? Language model probability: This is a logarith-
mic value (real value) of the n-gram probability
of the output word sequence. One feature weight
is assigned. The n-gram language model can be
389
constructed from a large sentence set because it
does not need the learner?s sentences.
Incorporating binary and real features yields a
rough approximation of generative models in semi-
supervised CRFs (Suzuki and Isozaki, 2008). It can
appropriately correct new sentences while maintain-
ing high accuracy on the training corpus.
3 Pseudo-error Sentences and Domain
Adaptation
The error corrector described in Section 2 requires
paired sentences. However, it is expensive to col-
lect them. We resolve this problem by using pseudo-
error sentences and domain adaptation.
3.1 Pseudo-Error Generation
Correct sentences, which are halves of the paired
sentences, can be easily acquired from corpora such
as newspaper articles. Pseudo-errors are generated
from them by the substitution, insertion, and dele-
tion functions according to the desired error pat-
terns.
We utilize the method of Rozovskaya and Roth
(2010b). Namely, when particles appear in the cor-
rect sentence, they are replaced by incorrect ones in
a probabilistic manner by applying the phrase table
(which stores the error patterns) in the opposite di-
rection. The error generation probabilities are rel-
ative frequencies on the training corpus. The mod-
els are learnt using both the training corpus and the
pseudo-error sentences.
3.2 Adaptation by Feature Augmentation
Although the error generation probabilities are com-
puted from the real-error corpus, the error distribu-
tion that results may be inappropriate. To better fit
the pseudo-errors to the real-errors, we apply a do-
main adaptation technique. Namely, we regard the
pseudo-error corpus as the source domain and the
real-error corpus as the target domain, and models
are learnt that fit the target domain.
In this paper, we use Daume (2007)?s feature aug-
mentation method for the domain adaptation, which
eliminates the need to change the learning algo-
rithm. This method regards the models for the
source domain as the prior distribution and learns
the models for the target domain.
Common Source TargetFeature Space
Ds Ds 0Source Data
Dt 0 DtTarget Data
Figure 2: Feature Augmentation
We briefly review feature augmentation. The fea-
ture space is segmented into three parts: common,
source, and target. The features extracted from the
source domain data are deployed to the common
and the source spaces, and those from the target do-
main data are deployed to the common and the target
spaces. Namely, the feature space is tripled (Figure
2).
The parameter estimation is carried out in the
usual way on the above feature space. Consequently,
the weights of the common features are emphasized
if the features are consistent between the source and
the target. With regard to domain dependent fea-
tures, the weights in the source or the target space
are emphasized.
Error correction uses only the features in the com-
mon and target spaces. The error distribution ap-
proaches that of the real-errors because the weights
of features are optimized to the target domain. In ad-
dition, it becomes robust against new sentences be-
cause the common features acquired from the source
domain can be used even when they do not appear in
the target domain.
4 Experiments
4.1 Experimental Settings
Real-error Corpus: We collected learner?s sen-
tences written by Chinese native speakers. The sen-
tences were created from English Linux manuals
and figures, and Japanese native speakers revised
them. From these sentences, only particle errors
were retained; the other errors were corrected. As
a result, we obtained 2,770 paired sentences. The
number of incorrect particles was 1,087 (8.0%) of
13,534. Note that most particles did not need to be
revised. The number of pair types of incorrect parti-
cles and their correct ones was 132.
Language Model: It was constructed from
Japanese Wikipedia articles about computers and
390
0.5
0.6
0.7
0.8
0.9
1
Precision Rate
TRGSRCALLAUG
0.3
0.4
0 0.05 0.1 0.15 0.2 0.25
Precision Rate
Recall Rate
Figure 3: Recall/Precision Curve (Error Generation Mag-
nification is 1.0)
Japanese Linux manuals, 527,151 sentences in total.
SRILM (Stolcke et al, 2011) was used to train a
trigram model.
Pseudo-error Corpus: The pseudo-errors were
generated using 10,000 sentences randomly selected
from the corpus for the language model. The mag-
nification of the error generation probabilities was
changed from 0.0 (i.e., no errors) to 2.0 (the relative
frequency in the real-error corpus was taken as 1.0).
Evaluation Metrics: Five-fold cross-validation
on the real-error corpus was used. We used two met-
rics: 1) Precision and recall rates of the error correc-
tion by the systems, and 2) Relative improvement,
the number of differences between improved and de-
graded particles in the output sentences (no changes
were ignored). This is a practical metric because it
denotes the number of particles that human rewriters
do not need to revise after the system correction.
4.2 Results
Figure 3 plots the precision/recall curves for the fol-
lowing four combinations of training corpora and
method.
? TRG: The models were trained using only the
real-error corpus (baseline).
? SRC: Trained using only the pseudo-error corpus.
? ALL: Trained using the real-error and pseudo-
error corpora by simply adding them.
? AUG:
The proposed method. The feature augmentation
was realized by regarding the pseudo-errors as the
-50
0
+50
+100
0.0 0.5 1.0 1.5 2.0 
Relative Improvement
-150
-100
Relative Improvement
Error Generation Probability(Magnification)
TRGSRCALLAUG
Figure 4: Relative Improvement among Error Generation
Probabilities
source domain and the real-errors as the target do-
main.
The SRC case, which uses only the pseudo-error
sentences, did not match the precision of TRG. The
ALL case matched the precision of TRG at high
recall rates. AUG, the proposed method, achieved
higher precision than TRG at high recall rates. At
the recall rate of 18%, the precision rate of AUGwas
55.4%; in contrast, that of TRG was 50.5%. Fea-
ture augmentation effectively leverages the pseudo-
errors for error correction.
Figure 4 shows the relative improvement of each
method according to the error generation probabil-
ities. In this experiment, ALL achieved higher im-
provement than TRG at error generation probabili-
ties ranging from 0.0 to 0.6. Although the improve-
ments were high, we have to control the error gen-
eration probability because the improvements in the
SRC case fell as the magnification was raised. On
the other hand, AUG achieved stable improvement
regardless of the error generation probability. We
can conclude that domain adaptation to the pseudo-
error sentences is the preferred approach.
5 Conclusions
This paper presented an error correction method of
Japanese particles that uses pseudo-error generation.
We applied domain adaptation in which the pseudo-
errors are regarded as the source domain and the
real-errors as the target domain. In our experiments,
domain adaptation achieved stable improvement in
system performance regardless of the error genera-
tion probability.
391
References
Hal Daume, III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 256?263, Prague, Czech Republic.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2010), pages
163?171, Los Angeles, California.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an ESL/EFL error correction sys-
tem. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta.
Kenji Imamura, Tomoko Izumi, Kugatsu Sadamitsu, Ku-
niko Saito, Satoshi Kobashikawa, and Hirokazu Masa-
taki. 2011. Morpheme conversion for connecting
speech recognizer and language analyzers in unseg-
mented languages. In Proceedings of Interspeech
2011, pages 1405?1408, Florence, Italy.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference
on Machine Learning (ICML-2001), pages 282?289,
Williamstown, Massachusetts.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining revision log of
language learning SNS for automated Japanese error
correction of second language learners. In Proceed-
ings of 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 147?155,
Chiang Mai, Thailand.
Alla Rozovskaya and Dan Roth. 2010a. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 961?970, Cambridge, Massachusetts.
Alla Rozovskaya and Dan Roth. 2010b. Training
paradigms for correcting errors in grammar and usage.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-HLT
2010), pages 154?162, Los Angeles, California.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for ESL correction tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Techologies (ACL-HLT 2011), pages 924?933,
Portland, Oregon.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU
2011), Waikoloa, Hawaii.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-08:
HLT), pages 665?673, Columbus, Ohio.
392
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 64?72,
Beijing, August 2010
Standardizing Complex Functional Expressions in Japanese  
Predicates: Applying Theoretically-Based Paraphrasing Rules  
 
Tomoko Izumi?    Kenji Imamura?    Genichiro Kikui?    Satoshi Sato? 
?NTT Cyber Space Laboratories, 
NTT Corporation 
{izumi.tomoko, imamura.kenji, 
kikui.genichiro}@lab.ntt.co.jp
?Graduate School of Engineering,  
Nagoya University 
ssato@nuee.nagoya-u.ac.jp 
 
 
 
Abstract 
In order to accomplish the deep semantic 
understanding of a language, it is essen-
tial to analyze the meaning of predicate 
phrases, a content word plus functional 
expressions. In agglutinating languages 
such as Japanese, however, sentential 
predicates are multi-morpheme expres-
sions and all the functional expressions 
including those unnecessary to the mean-
ing of the predicate are merged into one 
phrase. This triggers an increase in sur-
face forms, which is problematic for 
NLP systems. We solve this by introduc-
ing simplified surface forms of predi-
cates that retain only the crucial meaning 
of the functional expressions. We con-
struct paraphrasing rules based on syn-
tactic and semantic theories in linguistics. 
The results of experiments show that our 
system achieves the high accuracy of 
77% while reducing the differences in 
surface forms by 44%, which is quite 
close to the performance of manually 
simplified predicates. 
 
1 Introduction 
The growing need for text mining systems such 
as opinion mining and sentiment analysis re-
quires the deep semantic understanding of lan-
guages (Inui et al, 2008). In order to accomplish 
this, one needs to not only focus on the meaning 
of a single content word such as buy but also the 
meanings conveyed by function words or func-
tional expressions such as not and would like to. 
In other words, to extract and analyze a predi-
cate, it is critical to consider both the content 
word and the functional expressions (Nasukawa, 
2001). For example, the functional expressions 
would like to as in the predicate ?would like to 
buy? and can?t as in ?can?t install? are key ex-
pressions in detecting the customer?s needs and 
complaints, providing valuable information to 
marketing research applications, consumer opi-
nion analysis etc.  
Although these functional expressions are 
important, there have been very few studies that 
extensively deal with these functional expres-
sions for use in natural language processing 
(NLP) systems (e.g., Tanabe et al, 2001; Mat-
suyoshi and Sato, 2006, 2008). This is due to the 
fact that functional expressions are syntactically 
complicated and semantically abstract and so are 
poorly handled by NLP systems. 
In agglutinating languages such as Japanese, 
functional expressions appear in the form of 
suffixes or auxiliary verbs that follow the 
content word without any space. This sequence 
of a content word (c for short) plus several of 
functional expressions (f for short) forms a 
predicate in Japanese (COMP for completive 
aspect marker, NOM for nominalizer, COP for 
copular verb).   
(1) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
c -f1 -f2 -f3 -f4 -f5 
?(I) wanted to buy (it)? 
The meaning of ?want to? is expressed by -tai 
(f2) and the past tense is expressed by -ta (f3). 
64
The other functional expressions, -chai(f1), -n(f4), 
and -da(f5), only slightly alter the predicative 
meaning of ?wanted to buy,? as there is no direct 
English translation. Therefore, (1) expresses the 
same fact as (2). 
(2)  kai -takat -ta 
  buy -want -PAST 
?(I) wanted to buy (it).? 
As shown, in Japanese, once one extracts a 
predicate phrase, the number of differences in 
surface forms increases drastically regardless of 
their similarities in meaning. This is because 
sentential predicates are multi-word or multi-
morpheme expressions and there are two differ-
ent types of functional expressions, one which is 
crucial for the extraction of predicative meaning 
and the other, which is almost unnecessary for 
NLP applications. This increase in surface forms 
complicates NLP systems including text mining 
because they are unable to recognize that these 
seemingly different predicates actually express 
the same fact. 
In this study, we introduce paraphrasing rules 
to transform a predicate with complex functional 
expressions into a simple predicate. We use the 
term standardize to refer to this procedure. 
Based on syntactic and semantic theories in lin-
guistics, we construct a simple predicate struc-
ture and categorize functional expressions as 
either necessary or unnecessary. We then pa-
raphrase a predicate into one that only retains the 
crucial meaning of the functional expression by 
deleting unnecessary functional expressions 
while adding necessary ones. 
 The paper is organized as follows. In Section 
2, we provide related work on Japanese 
functional expressions in NLP systems as well as 
problems that need to be solved. Section 3 
introduces several linguistic theories and our 
standardizing rules that we constructed based on 
these theories. Section 4 describes the 
experiments conducted on our standardization 
system and the results. Section 5 discusses the 
results and concludes the paper. Throughout this 
paper, we use the term functional expressions to 
indicate not only a single function word but also 
compounds (e.g., would like to). 
 
2 Previous Studies and Problems  
Shudo et al (2004) construct abstract semantic 
rules for functional expressions and use them in 
order to find whether two different predicates 
mean the same. Matsuyoshi and Sato (2006, 
2008) construct an exhaustive dictionary of 
functional expressions, which are hierarchically 
organized, and use it to produce different func-
tional expressions that are semantically equiva-
lent to the original one.  
 Although these studies provide useful in-
sights and resources for NLP systems, if the in-
tention is to extract the meaning of a predicate, 
we find there are still problems that need to be 
solved. There are two problems that we focus on. 
 The first problem is that many functional ex-
pressions are unnecessary, i.e., they do not ac-
tually alter the meaning of a predicate.   
(3) yabure -teshimat -ta -no -dearu 
 rip -COMP -PAST -NOM -COP 
 c -f1 -f2 -f3 -f4  
 ?(something) ripped.? 
(3) can be simply paraphrased as (4) 
(4) yabure -ta 
 rip -PAST 
 c -f1  
In actual NLP applications such as text mining, 
it is essential that the system recognizes that (3) 
and (4) express the same event of something 
?ripped.? In order to achieve this, the system 
needs to recognize -teshimat, -no, and -dearu as 
unnecessary (f1, f3, f4 ??). Previous studies that 
focus on paraphrasing of one functional expres-
sion to another (f ? f?) cannot solve this prob-
lem. 
 The second problem is that we sometimes 
need to add certain functional expressions in 
order to retain the meaning of a predicate (? ?f). 
(5) (Hawai-ni) P1iki, P2nonbirishi -takat -ta 
 (Hawaii-to)  go relax -want -PAST 
   c1 c2 f1 f2 
?I wanted to go to Hawaii and relax.? 
(5) has a coordinate structure, and two verbal 
predicates, iki (P1) ?go? and nonbirishi-takat-ta 
(P2) ?wanted to relax?, are coordinated.  
 As the English translation indicates, the first 
predicate in fact means iki-takat-ta ?wanted to 
65
go,? which implies that the speaker was not able 
to go to Hawaii. If the first predicate was ex-
tracted and analyzed as iku, the base (present) 
form of ?go,? then this would result in a wrong 
extraction of predicate, indicating the erroneous 
fact of going to Hawaii in the future (Present 
tense in Japanese expresses a future event). In 
this case, we need to add the functional expres-
sions takat ?want? and ta, the past tense marker, 
to the first verbal predicate.  
As shown, there are two problems that need 
to be solved in order for a system to extract the 
actual meaning of a predicate. 
i. Several functional expressions are neces-
sary for sustaining the meaning of the event 
expressed by a predicate while others barely 
alter the meaning (f ??). 
ii. Several predicates in coordinate sentences 
lack necessary functional expressions at the 
surface level (? ?f) and this results in a 
wrong extraction of the predicate meaning. 
Based on syntactic and semantic theories in lin-
guistics, we construct paraphrasing rules and 
solve these problems by standardizing complex 
functional expressions. 
 
3 Construction of Paraphrasing Rules  
The overall flow of our standardizing system is 
depicted in Figure 1. The system works as fol-
lows. 
i. Given a parsed sentence as an input, it ex-
tracts a predicate(s) and assigns a semantic 
label to each functional expression based on 
Matsuyoshi and Sato (2006). 
ii. As for an intermediate predicate, necessary 
functional expressions are added if missing 
(? ?f). 
iii. From each predicate, delete unnecessary 
functional expressions that do not alter the 
meaning of the predicate (f ??). 
iv. Conjugate each element and generate a 
simplified predicate.  
There are two fundamental questions that we 
need to answer to accomplish this system. 
A) What are UNNECCESARY functional ex-
pressions (at least for NLP applications), 
i.e., those that do not alter the meaning of 
the event expressed by a predicate? 
B) How do we know which functional expres-
sions are missing and so should be added? 
We answer these questions by combining what is 
needed in NLP applications and what is dis-
cussed in linguistic theories. We first answer 
Question A. 
 
3.1 Categorization of Functional Expressions 
As discussed in Section 1 and in Inui et al 
(2008), what is crucial in the actual NLP appli-
cations is to be able to recognize whether two 
seemingly different predicates express the same 
fact.  
This perspective of factuality is similar to the 
truth-value approach of an event denoted by pre-
dicates as discussed in the field of formal seman-
tics (e.g., Chierchia and Mcconnel-Ginet, 2000; 
Portner, 2005). Although an extensive investiga-
tion of these theories is beyond the scope of this 
paper, one can see that expressions such as tense 
(aspect), negation as well as modality, are often 
discussed in relation to the meaning of an event 
(Partee et al, 1990; Portner, 2005). 
Tense (Aspect): Expresses the time in (at/for) 
which an event occurred. 
Negation: Reverses the truth-value of an event. 
Modality: Provides information such as possi-
bility, obligation, and the speaker?s eagerness 
with regard to an event and relate it to what is 
true in reality. 
The above three categories are indeed useful in 
explaining the examples discussed above. 
(6) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
 aspect modality tense(aspect) 
(7) kai -takat -ta 
 buy -want -PAST 
  modality tense(aspect) 
 ?wanted to buy? 
The predicate ?kat-chai-takat-ta-n-da? in (6) and 
?kai-takat-ta? in (7) express the same event be-
cause they share the same tense (past), negation 
(none), and modality (want). Although (6) has 
the completive aspect marker -chai while (7) 
does not, they still express the same fact. This is 
because the Japanese past tense marker -ta also 
has a function to express the completive aspect. 
The information expressed by -chai in (6) is re-
66
dundant and so unnecessary.  
On the other hand, the predicate ?iku? in (5) 
and ?iki-takat-ta,? which conveys the actual 
meaning of the predicate, express a different fact 
because they establish a different tense (present 
vs. past) and different modality (none vs. want).  
As shown, once we examine the abstract se-
mantic functions of functional expressions, we 
can see the factual information in a predicate is 
influenced by tense (aspect), negation, and mod-
ality. Therefore, the answer to Question A is that 
necessary functional expressions are those that 
belong to tense (aspect), negation, and modality. 
Furthermore, if there are several functional ex-
pressions that have the same semantic function, 
retaining one of them is sufficient. 
 
3.2 Adding Necessary Functional Expressions 
The next question that we need to answer is how 
we find which functional expressions are miss-
ing when standardizing an intermediate predicate 
in a coordinate structure (e.g., (5)). We solve this 
based on a detailed analysis of the syntactic 
structure of predicates. 
Coordinate structures are such that several 
equivalent phrases are coordinated by conjunc-
tions such as and, but, and or. If a predicate is 
coordinated with another predicate, these two 
predicates must share the same syntactic level. 
Therefore, the structure in (5) is indeed depicted 
as follows (What TP and ModP stand for will be 
discussed later). 
[TP[ModP[VP(Hawai-ni) iki][VPnonbirishi]takat]ta ] 
[TP[ModP[VP(Hawaii-to) go][VPrelax] want]PAST] 
This is the reason why the first predicate iki 
should be paraphrased as iki-takat-ta ?wanted to 
go.? It needs to be tagged with the modality ex-
pression tai and the past tense marker ta, which 
seems to attach only to the last predicate.  
 This procedure of adding necessary function-
al expressions to the intermediate predicate is 
not as simple as it seems, however.   
(8) nemutai-mitai-de kaeri -tagatte -tei -ta 
sleepy-seems-COP gohome-want-CONT-PAST 
?He seemed sleepy and wanted to go home.? 
In (8), the first predicate nemutai-mitai-de ?seem 
to be sleepy? should be paraphrased as nemutai-
mitai-dat-ta, ?seemed to be sleepy,? in which 
only the functional expression indicating past is 
required. The other functional expressions such 
as tagatte ?want,? and the aspect marker tei 
(CONTinuation) should not be added (nemutai-
mitai-de-tagat(want)-tei(CONT)-ta(PAST) is 
completely ungrammatical).  
Input  
A parsed Sentence 
Hontoo-wa Hawai-ni iki, nonbirishi takat ta n da kedo
Really-TOP Hawaii-to go relax want PAST NOM COP but 
?I wanted to go to Hawaii and relax if I could.? 
i. Predicate Extraction &
Labeling Semantic Classes 
to Functional Expressions 
ii. ADD necessary 
functional expressions 
(? ? f) 
iii. DELETE unnecessary 
functional expressions 
(f ? ?) 
iv. Conjugate and  
Generate simple predicates 
Output  
Simplified Predicates 
iki 
go 
c 
[[[VP] ?] ?] 
iki tai ta
go want PAST
c [??] [??]
iki takat ta
go want PAST 
nonbirishi takat ta  
relax want PAST  
iki-takat-ta
?wanted to go? 
nonbirishi-takat-ta 
?wanted to relax? 
Figure 1. The flow of Standardization. 
iki tai ta
go want PAST 
c [??] [??] 
nonbirishi takat ta n da kedo 
relax want PAST NOM COP but 
c [??] [??] [??] [??] [????]
nonbirishi takat ta n da kedo 
relax  want PAST NOM COP but 
c [??] [??] [??] [??] [????] 
[[[VP]             ModP]    TP] 
67
 Furthermore, the intermediate predicate in the 
following example does not allow any functional 
expressions to be added. 
(9) (imawa) yasui-ga (mukashiwa) takakat-ta 
(today) inexpensive-but (in old days) expensive-
PAST 
?(They) are inexpensive (today), (but) used to 
be very expensive (in the old days.)? 
In (9), the first predicate yasui ?inexpensive? 
should not be paraphrased as yasukat-ta ?was 
inexpensive? since this would result in the un-
grammatical predicate of ?*(they) were inexpen-
sive (today).?  
 In order to add necessary functional expres-
sions to an intermediate predicate, one needs to 
solve the following two problems. 
i. Find whether the target predicate indeed 
lacks necessary functional expressions.  
ii. If such a shortfall is detected, decide which 
functional expressions should be added to 
the predicate. 
We solve these problems by turning to the in-
completeness of the syntactic structure of a pre-
dicate. 
Studies such as Cinque (2006) and Rizzi 
(1999) propose detailed functional phrases such 
as TopP (Topic Phrase) in order to fully describe 
the syntactic structures of a language. We adopt 
this idea and construct a phrase structure of Jap-
anese predicates which borrows from the func-
tional phrases of TP, ModP, and FocP (Figure 2). 
 ModP stands for a modality phrase and this is 
where modality expressions can appear.1  FocP 
stands for a focus phrase. This is the phrase 
where the copula da appears. This phrase is 
needed because several modality expressions 
syntactically need the copula da in either the 
following or preceding position (Kato, 2007). 
The existence of FocP also indicates that the 
modality expressions within the phrase are com-
plete (no more modality phrase is attached). TP 
stands for a tense phrase and this is where the 
tense marker appears.  
 Note that this structure is constructed for the 
purpose of Standardization and other functional 
                                                 
1 The structure of Figure 2 is recursive. A modality expres-
sion can appear after a TP. Also, more than one ModP can 
appear although ModP and FocP are optional. 
projections such as NegP (negation phrase) will 
not be discussed although we assume there must 
be one. Based on the predicate structure in Fig-
ure 2, we solve the two problems as follows. 
 
The first problem: Detecting whether the target 
predicate lacks necessary functional expressions.  
? If the predicate has the past tense marker ta 
or if the coordinate conjunction following 
the predicate is for combining phrases with 
tense, then consider the predicate as com-
plete and do not add any functional expres-
sions. Otherwise, consider the predicate as 
incomplete and add the appropriate func-
tional expressions. 
The underlying principle of this rule is that if a 
predicate is tensed, then its syntactic structure is 
complete. As often described in syntactic theo-
ries (e.g., Adger, 2003), a sentence can be said to 
be a phrase with tense (i.e., TP). In other words, 
if a predicate is tensed, then it can stand alone as 
a sentence.  
 By adopting this idea, we judge the com-
pleteness of a predicate by the existence of tense. 
Because Japanese marks past tense by the past 
tense marker -ta, if a predicate has -ta, it is com-
plete and no functional expressions need be add-
ed.  
 However, Japanese does not hold an explicit 
present tense marker; the base form of a verb is 
also a present form. We solve this by looking at 
which conjunction follows the predicate. As dis-
cussed in Minami (1993), the finite state and the 
type of conjunction are related; some conjunc-
tions follow tensed phrases while others follow 
infinitival phrases. Following this, we categorize 
all the coordinate conjunctions based on whether 
they can combine with a tensed phrase. These 
conjunctions are listed as tensed in Table 1. If 
TP  
3 
(FocP) T:ta PAST [??] 
3 
(ModP)*   Foc:da COP [??] 
3 
VP   Mod: mitai ?seems? [??] 
4 
iku ?go? 
Figure 2. Structure of a predicate. 
68
the target phrase is followed by one of those 
conjunctions, then we do not add any functional 
expressions to them because they are complete. 
 
The second problem: Finding the appropriate 
functional expressions for incomplete interme-
diate predicates. 
As discussed, we assume that predicates are 
coordinated at one of the functional phrase levels 
in Figure 2. Functional expressions that need to 
be added are, therefore, those of the outer phras-
es of the target phrase.  
 For example, if the target phrase has da, the 
head of FocP, then it only needs the past tense 
marker to be added, which is located above the 
FocP (i.e., TP). This explains the paraphrasing 
pattern of (8). Therefore, by looking at which 
functional expressions held by the target predi-
cate, one can see that functional expressions to 
be added are those that belong to phrases above 
the target phrase. 
 As shown, the answer to Question B is that 
we only add functional expressions to incom-
plete predicates, which are judged based on the 
existence/absence of tense. The appropriate 
functional expressions to be added are those of 
outer phrases of the target phrase. 
 
3.3 Implementing the Standardization 
In this final subsection, we describe how we ac-
tually implement our theoretical observations in 
our standardization system.  
 
CATEGORIZE functional expressions 
First, we selected functional expressions that 
belong to our syntactic and semantic categories 
from those listed in Matsuyoshi and Sato (2006), 
a total of about 17,000 functional expressions 
with 95 different semantic labels. We use ab-
stract semantic labels, such as ?completion,? 
?guess,? and ?desire? for the categorization 
(Table 2).  
 We divided those that did not belong to our 
syntactic and semantic categories into Deletables 
and Undeletables. Deletables are those that do 
not alter the meaning of an event and are, there-
fore, unnecessary. Undeletables are those that 
are a part of content words, and so cannot be 
deleted (e.g., kurai [??] ?about? as in 1-man-
en-kurai-da ?is about one million yen?). Based 
on the categorization of semantic labels as well 
as surface forms of functional expressions, our 
system works as follows; 
 
ADD necessary functional expressions  
A-1: Examine whether the target predicate has 
the tense marker ta or it is followed by the 
conjunctions categorized as tensed. If not, 
then go to Step A-2. 
A-2: Based on the semantic label of the target 
predicate, decide which level of syntactic 
phrase the predicate projects. Add functional 
expressions from the last predicate that be-
longs to outer phrases. 
 
DELETE unnecessary functional expressions 
D-1: Delete all the functional expressions that 
are categorized as Deletables. 
D-2: Leave only one functional expression if 
there is more than one same semantic label. 
For those categorized as Negation, however, 
delete all if the number of negations is even. 
Otherwise, leave one. 
D-3: Delete those categorized as Focus if they 
do not follow or precede a functional expres-
sion categorized as Modality.  
 
GENERATE simple predicates 
Last, conjugate all the elements and generate 
simplified surface forms of predicates. 
 
4 Experiments and Evaluations 
4.1 Constructing Paraphrase Data 
We selected 2,000 sentences from newspaper 
and blog articles in which more than one predi-
cate were coordinated.2 We manually extracted 
predicates (c-f1-f2..fn). Half of them were those in 
which the last predicate had three or more func-
tional expressions (n ? 3). 
                                                 
2 We use Mainichi Newspapers from the year 2000. 
Table 1. Coordinate conjunctions. 
Not tensed Tensed 
gerundive 
form, te  
shi, dakedenaku, ueni, bakarika, 
hoka(ni)(wa), keredo, ga, nonitai-
shi(te),ippou(de),hanmen  
69
We then asked one annotator with a linguistic 
background to paraphrase each predicate into the 
simplest form possible while retaining the mean-
ing of the event.3 We asked another annotator, 
who also has a background in linguistics, to 
check whether the paraphrased predicates made 
by the first annotator followed our criterion, and 
if not, asked the first annotator to make at least 
one paraphrase. 424 out of 4,939 predicates 
(8.5%) were judged as not following the crite-
rion and were re-paraphrased. This means that 
the accuracy of 91.5% is the gold standard of our 
task. 
One of the authors manually assigned a cor-
rect semantic label to each functional expression. 
Procedure i in Figure 1 is, therefore, manually 
implemented in our current study. 
 
4.2 Experiments and Results 
Based on the standardization rules discussed in 
Section 3, our system automatically paraphrased 
functional expressions of test predicates into 
simple forms. We excluded instances that had 
segmentation errors and those that were judged 
as inappropriate as a predicate. 4  A total of 
1,501 intermediate predicates (287 for develop-
ment and 1,214 for test) and 1,958 last predi-
cates (391 for development and 1,567 for test) 
were transformed into simple predicates. 
 The accuracy was measured based on the ex-
act match in surface forms with the manually 
constructed paraphrases. For comparison, we 
                                                 
3 We asked to delete or add functional expressions from 
each predicate when paraphrasing. Only the surface forms 
(and not semantic labels) were used for annotation. 
4 In Japanese, a gerundive form of a verb is sometimes used 
as a postposition. The annotators excluded these examples 
as ?not-paraphrasable.? 
used the following baseline methods. 
? No Add/Delete: Do not add/delete any 
functional expression.  
? Simp Add: Simply add all functional ex-
pressions that the intermediate phrase does 
not have from the last predicate. 
Table 3 indicates the results. Our standardizing 
system achieved high accuracy of around 77% 
and 83 % in open (against the test set) and 
closed tests (against the development set) com-
pared to the baseline methods (No Add/Delete 
(open), 55%; Simp Add (open), 33%). 
We also measured the reduced rate of differ-
ences in surface forms. We counted the number 
of types of functional expressions in the last pre-
dicates (a sequence of f1-f2-f3 is counted as one) 
before and after the standardization. 
For comparison, we also counted the number 
of functional expressions of the manually pa-
raphrased predicates. Table 4 lists the results. As 
shown, our standardizing system succeeded in 
reducing surface differences in predicates from 
the original ones at the rate of 44.0%, which is 
quite close to the rate achieved by the human 
annotators (52.0%). 
 
5 Discussion and Conclusion 
Our standardization system succeeded in gene-
rating simple predicates in which only functional 
expressions crucial for the factual meaning of 
the predicate were retained.  
The predicates produced by our system 
showed fewer variations in their surface forms 
while around 77% of them exactly matched the 
simplified predicates produced by human anno-
tators, which is quite high compared to the base-
line systems.  
Table 2. Syntactic and semantic categorization of semantic labels. 
Syntactic  Semantic  Semantic Labels
T if the 
surface is ta 
Tense 
(Aspect) 
??(completion),??,??,??,??,??,??,??,???, ???, ??,?
?, ??, ?? 
 Negation ??(negation), ??,????,????,???,???,???, ???, ???
Mod Modality ??(guess),  ??(desire),??,??,??,??,??,??, ??, ??, ??, ??
??, ???, ???,???,??,???,???
Foc Focus ??(affirmation), ???,??
 Deletables ??(politeness),?-??,??,??,?-??,????,??, ??, ????, ?
?,??, ????,????,??,??,???
 Undele-
tables 
??(about), ??,??,???,???,??,??,??,??, ??, ????,?
?, ??, ??, ??, ??, ???, ???, ??, ???, ????, ????, ??, 
??, ???, ??,??,??,??,??,??,??,??,?? 
70
This was achieved because we constructed 
solid paraphrasing rules by applying linguistic 
theories in semantics and syntax. The quite low 
accuracy of the baseline method, especially 
SimpAdd, further supports our claim that im-
plementing linguistic theories in actual NLP ap-
plications can greatly improve system perfor-
mance. 
Unlike the study by Inui et al (2008), we did 
not include the meaning of a content word for 
deciding the factuality of the event nor did we 
include it in the paraphrasing rules. This lowers 
the accuracy. Several functional expressions, 
especially those expressing aspect, can be de-
leted or added depending on the meaning of the 
content word. This is because content words in-
herently hold aspectual information, and one 
needs to compare it to the aspectual information 
expressed by functional expressions. Because we 
need a really complicated system to compute the 
abstract semantic relations between a content 
word and functional expressions, we leave this 
problem for future research.  
Regardless of this, our standardizing system 
is useful for a lot of NLP applications let alne 
text mining. As mentioned in Inui et al (2008), 
bag-of-words-based feature extraction is insuffi-
cient for conducting statistically-based deep se-
mantic analysis, such as factual analysis. If stan-
dardized predicates were used instead of a single 
content word, we could expect an improvement 
in those statistically-based methods because each 
predicate holds important information about fact 
while differences in surface forms are quite li-
mited. 
In conclusion, we presented our system for 
standardizing complex functional expressions in 
Japanese predicates. Since our paraphrasing 
rules are based on linguistic theories, we suc-
ceeded in producing simple predicates that have 
only the functional expressions crucial to under-
standing of the meaning of an event. Our future 
research will investigate the relationship be-
tween the meaning of content words and those of 
functional expressions in order to achieve higher 
accuracy. We will also investigate the impact of 
our standardization system on NLP applications.  
 
References 
Adger, David (2003). Core Syntax: A minimalist ap-
proach. New York: Oxford University Press. 
Chierchia, Gennaro, & Sally McConnell-Ginet (2000). 
Meaning and grammar: An introduction to se-
mantics (2nd ed.). Cambridge, MA: The MIT 
press. 
Cinque, Guglielmo (2006). Restructuring and func-
tional heads: The cartography of syntactic struc-
tures, Vol. 4. New York: Oxford University Press. 
Haugh, Michael (2008). Utterance-final conjunctive 
particles and implicature in Japanese conversation. 
Pragmatics, 18 (3), 425-451. 
Inui, Kentaro, Shuya Abe, Kazuo Hara, Hiraku Mori-
ta, Chitose Sao, Megumi Eguchi, Asuka Sumida, 
Koji Murakami, & Suguru Matsuyoshi (2008). 
Experience mining: Building a large-scale data-
base of personal experiences and opinions from 
web documents. Proceedings of the 2008 
IEEE/WIC/ACM International Conference on 
Web Intelligence and Intelligent Agent Technolo-
gy, Vol. 1., 314-321. 
Kato, Shigehiro (2007). Nihongo-no jutsubu-kouzou 
to kyoukaisei [Predicate complex structure and 
morphological boundaries in Japanese]. The an-
nual report on cultural science, Vol. 122(6) (pp. 
97-155). Sapporo, Japan: Hokkaido University, 
Graduate School of Letters. 
Matsuyoshi, Suguru, & Satoshi Sato (2006). Compi-
lation of a dictionary of Japanese functional ex-
pressions with hierarchical organization. Proceed-
ings of the 21st International Conference on 
Computer Processing of Oriental Languages 
 Normalization No Add/Delete Simp Add 
Open (Intermediate) 77.7%(943/1214) 57.8%(702/1214) 32.8%(398/1214) 
Closed (Intermediate) 82.9%(238/287) 62.0%%(178/287) 35.2%(101/287) 
Open (Last) 76.2%(1194/1567) 51.9% (203/391) n.a 
Closed (Last) 83.4%(326/391) 48.1%(188/391) n.a. 
Table 3. Results of our normalization system. 
Original 943 types Reduced Rate
Normalization 530 types 44.0% 
Human Annotation 448 types 52.0% 
Table 4. Reduced rate of surface forms. 
71
(ICCPOL), Lecture Notes in Computer Science, 
Vol. 4285, 395-402.  
Matsuyoshi, Suguru, & Satoshi Sato (2008). Auto-
matic paraphrasing of Japanese functional expres-
sions using a hierarchically organized dictionary. 
Proceedings of the 3rd International Joint Confe-
rence on Natural Language Processing (IJCNLP), 
Vol. 1, 691-696. 
Minami, Fujio (1993). Gendai nihongobunpou-no 
rinkaku [Introduction to modern Japanese gram-
mar]. Tokyo: Taishuukan. 
Nasukawa, Tetsuya (2001). Kooru sentaa-niokeru 
tekisuto mainingu [Text mining application for 
call centers]. Journal of Japanese society for Ar-
tificial Intelligence, 16(2), 219-225. 
Partee, Barbara H., Alice ter Meulen, & Robert E. 
Wall (1990). Mathematical methods in Linguistics. 
Dordrecht, The Netherland: Kluwer. 
Portner, Paul H. (2005). What is meaning?: Funda-
mentals of formal semantics. Malden, MA: 
Blackwell. 
Rizzi, Luigi (1999). On the position ?Int(errogative)? 
in the left periphery of the clause. Ms., Universit? 
di Siena. 
Shudo, Kosho, Toshifumi Tanabe, Masahito Takaha-
shi, & Kenji Yoshimura (2004). MWEs as non-
propositional content indicators. Proceedings of 
second Association for Computational Linguistics 
(ACL) Workshops on Multiword Expressions: In-
tegrating Processing, 32-39. 
Tanabe, Toshifumi, Kenji Yoshimura & Kosho Shu-
do (2001). Modality expressions in Japanese and 
their automatic paraphrasing. Proceedings of the 
6th Natural Language Processing Pacific Rim 
Symposium (NLPRS), 507-512. 
Tsujimura, Natsuko. (2007). An Introduction to Jap-
anese Linguistics (2nd Ed.). Malden, MA: Black-
well. 
72
