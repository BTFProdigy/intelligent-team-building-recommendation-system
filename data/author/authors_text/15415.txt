Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 107?115,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Modeling the Use of Graffiti Style Features to Signal Social Relations 
within a Multi-Domain Learning Paradigm 
Mario Piergallini1, A. Seza Do?ru?z2, Phani Gadde1, David Adamson1, Carolyn P. Ros?1,3  
1Language Technologies 
Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
{mpiergal,pgadde, 
dadamson}@cs.cmu.edu 
2Tilburg University, TSH, 
5000 LE Tilburg, The 
Netherlands/ 
Language Technologies 
Institute, Carnegie Mellon 
University, 5000 Forbes 
Ave.,Pittsburgh PA 15213 
a.s.dogruoz@gmail.com 
3Human-Computer 
Interaction Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
cprose@cs.cmu.edu 
 
Abstract 
In this paper, we present a series of 
experiments in which we analyze the usage of 
graffiti style features for signaling personal 
gang identification in a large, online street 
gangs forum, with an accuracy as high as 83% 
at the gang alliance level and 72% for the 
specific gang.  We then build on that result in 
predicting how members of different gangs 
signal the relationship between their gangs 
within threads where they are interacting with 
one another, with a predictive accuracy as high 
as 66% at this thread composition prediction 
task.  Our work demonstrates how graffiti 
style features signal social identity both in 
terms of personal group affiliation and 
between group alliances and oppositions.  
When we predict thread composition by 
modeling identity and relationship 
simultaneously using a multi-domain learning 
framework paired with a rich feature 
representation, we achieve significantly higher 
predictive accuracy than state-of-the-art 
baselines using one or the other in isolation. 
1 Introduction 
Analysis of linguistic style in social media has 
grown in popularity over the past decade.  
Popular prediction problems within this space 
include gender classification (Argamon et al., 
2003), age classification (Argamon et al., 2007), 
political affiliation classification (Jiang & 
Argamon, 2008), and sentiment analysis (Wiebe 
et al., 2004).  From a sociolinguistic perspective, 
this work can be thought of as fitting within the 
area of machine learning approaches to the 
analysis of style (Biber & Conrad, 2009), 
perhaps as a counterpart to work by variationist 
sociolinguists in their effort to map out the space 
of language variation and its accompanying 
social interpretation (Labov, 2010; Eckert & 
Rickford, 2001).  One aspiration of work in 
social media analysis is to contribute to this 
literature, but that requires that our models are 
interpretable.  The contribution of this paper is an 
investigation into the ways in which stylistic 
features behave in the language of participants of 
a large online community for street gang 
members.  We present a series of experiments 
that reveal new challenges in modeling stylistic 
variation with machine learning approaches.  As 
we will argue, the challenge is achieving high 
predictive accuracy without sacrificing 
interpretability. 
 Gang language is a type of sociolect that has 
so far not been the focus of modeling in the area 
of social media analysis.  Nevertheless, we argue 
that the gangs forum we have selected as our 
data source provides a strategic source of data for 
exploring how social context influences stylistic 
language choices, in part because it is an area 
where the dual goals of predictive accuracy and 
interpretability are equally important. In 
particular, evidence that gang related crime may 
account for up to 80% of crime in the United 
States attests to the importance of understanding 
the social practices of this important segment of 
society (Johnsons, 2009).  Expert testimony 
attributing meaning to observed, allegedly gang-
related social practices is frequently used as 
evidence of malice in criminal investigations 
(Greenlee, 2010).  Frequently, it is police officers 
who are given the authority to serve as expert 
witnesses on this interpretation because of their 
routine interaction with gang members.  
107
Nevertheless, one must consider their lack of 
formal training in forensic linguistics (Coulthard 
& Johnson, 2007) and the extent to which the 
nature of their interaction with gang members 
may subject them to a variety of cognitive biases 
that may threaten the validity of their 
interpretation (Kahneman, 2011).   
 Gang-related social identities are known to be 
displayed through clothing, tattoos, and language 
practices including speech, writing, and gesture 
(Valentine, 1995), and even dance (Philips, 
2009).  Forensic linguists have claimed that these 
observed social practices have been over-
interpreted and inaccurately interpreted where 
they have been used as evidence in criminal trials 
and that they may have even resulted in 
sentences that are not justified by sufficient 
evidence (Greenlee, 2010).  Sociolinguistic 
analysis of language varieties associated with 
gangs and other counter-cultural groups attests to 
the challenges in reliable interpretation of such 
practices (Bullock, 1996; Lefkowitz, 1989).  If 
we as a community can understand better how 
stylistic features behave due to the choices 
speakers make in social contexts, we will be in a 
better position to achieve high predictive 
accuracy with models that are nevertheless 
interpretable.  And ultimately, our models may 
offer insights into usage patterns of these social 
practices that may then offer a more solid 
empirical foundation for interpretation and use of 
language as evidence in criminal trials. 
 In the remainder of the paper we describe our 
annotated corpus.  We then motivate the 
technical approach we have taken to modeling 
linguistic practices within the gangs forum.  
Next, we present a series of experiments 
evaluating our approach and conclude with a 
discussion of remaining challenges. 
2 The Gangs Forum Corpus 
The forum that provides data for our experiments 
is an online forum for members of street gangs. 
The site was founded in November, 2006. It was 
originally intended to be an educational resource 
compiling knowledge about the various gang 
organizations and the street gang lifestyle. Over 
time, it became a social outlet for gang members. 
There are still traces of this earlier focus in that 
there are links at the top of each page to websites 
dedicated to information about particular gangs. 
At the time of scraping its contents, it had over a 
million posts and over twelve thousand active 
users.   Our work focuses on analysis of stylistic 
choices that are influenced by social context, so 
it is important to consider some details about the 
social context of this forum.  Specifically, we 
discuss which gangs are present in the data and 
how the gangs are organized into alliances and 
rivalries.  Users are annotated with their gang 
identity at two levels of granularity, and threads 
are annotated with labels that indicate which 
gang dominates and how the participating gangs 
relate to one another.   
2.1 User-Level Annotations 
At the fine-grained level, we annotated users 
with the gang that they indicated being affiliated 
with,  including Bloods, Crips, Hoovers, 
Gangster Disciples, other Folk Nation, Latin 
Kings, Vice Lords, Black P. Stones, other People 
Nation, Trinitarios, Norte?os, and Sure?os.  
There was also an Other category for the smaller 
gangs.  For a coarser grained annotation of gang 
affiliation, we also noted the nation, otherwise 
known as gang alliance, each gang was 
associated with.   
For our experiments, a sociolinguist with 
significant domain expertise annotated the gang 
identity of 3384 users.  Information used in our 
annotation included the user?s screen name, their 
profile, which included a slot for gang affiliation, 
and the content of their posts.  We used regular 
expressions to find gang names or other 
identifiers occurring within the gang affiliation 
field and the screen names and annotated the 
users that matched.  If the value extracted for the 
two fields conflicted, we marked them as 
claiming multiple gangs.  For users whose 
affiliation could not be identified automatically, 
we manually checked their profile to see if their 
avatar (an image that accompanies their posts) or 
other fields there contained any explicit 
information.  Otherwise, we skimmed their posts 
for explicit statements of gang affiliation.   
Affiliation was unambiguously identified 
automatically for 56% of the 3384 users from 
their affiliation field.  Another 36% were 
identified automatically based on their screen 
name.  Manual inspection was only necessary in 
9% of the cases.  Users that remained ambiguous, 
were clearly fake or joke accounts, or who 
claimed multiple gangs were grouped together in 
an ?Other? category, which accounts for 6.2% of 
the total.  Thus, 94% of the users were classified 
into the 12 specific gangs mentioned above. 
108
At a coarse-grained level, users were also 
associated with a nation.  The nation category 
was inspired by the well-known gang alliances 
known as the People Nation and Folks Nation, 
which are city-wide alliances of gangs in 
Chicago. We labeled the Crips and Hoovers as a 
nation since they are closely allied gangs.  
Historically, the Hoovers began breaking away 
from the Crips and are rivals with certain subsets 
of Crips, but allies with the majority of other 
Crips gangs.  The complex inner structure of the 
Crips alliance will be discussed in Section 5 
where we interpret our quantitative results. 
There are a large number of gangs that 
comprise the People and Folks Nations. The 
major gangs within the People Nation are the 
Latin Kings, Vice Lords and Black P. Stones. 
The Folks Nation is dominated by the Gangster 
Disciples with other Folks Nation gangs being 
significantly smaller. The People Nation, Blood 
and Norte?os gangs are in a loose, national 
alliance against the opposing national alliance of 
the Folks Nation, Crips and Sure?os. Remaining 
gangs were annotated as other, such as the 
Trinitarios, that don't fit into this national 
alliance system nor even smaller alliances.   
2.2 Thread-Level Annotations 
In addition to person-level annotations of gang 
and nation, we also annotated 949 threads with 
dominant gang as well as thread composition, by 
which we mean whether the users who 
participated on the thread were only from allied 
gangs, included opposing gangs, or contained a 
mix of gangs that were neither opposing nor 
allied.  These 949 threads were ones where a 
majority of the users who posted were in the set 
of 3384 users annotated with a gang identity. 
For the dominant gang annotation at the 
gang level, we consider only participants on the 
thread for whom there was an annotated gang 
affiliation. If members of a single gang produced 
the majority of the posts in the thread, then that 
was annotated as the dominant gang of the thread. 
If no gang had a majority in the thread, it was 
instead labeled as Mixed. For dominant gang at 
the nation level, the same procedure was used, 
but instead of looking for which gang accounted 
for more of the members, we looked for which 
gang alliance accounted for the majority of users. 
For the thread composition annotation, we 
treated the Bloods, People Nation, and Norte?os 
as allied with each other as the ?Red set?.  We 
treated Crips, Hoovers, Folks Nation, and 
Sure?os as allies with each other as the ?Blue 
set?.  The Red and Blue sets oppose one another.  
The Latin Kings and Trinitarios also oppose one 
another.  Thread composition was labeled as 
Allied, Mixed or Opposing depending on the 
gangs that appeared in the thread. As with the 
dominant gang annotation, only annotated users 
were considered. If all of the posts were by users 
of the same gang or allied gangs, the thread was 
labeled as Allied.  If there were any posts from 
rival gangs, it was labeled as Opposing. 
Otherwise, it was labeled as Mixed. If the users 
were all labeled with Other as their gang it was 
also labeled as Mixed.  
3 Modeling Language Practices at the 
Feature Level 
In this section, we first describe the rich feature 
representation we developed for this work.  
Finally, we discuss the motivation for employing 
a multi-domain learning framework in our 
machine-learning experiments. 
3.1 Feature Space Design: Graffiti Style 
Features 
While computational work modeling gang-
related language practices is scant, we can learn 
lessons from computational work on other types 
of sociolects that may motivate a reasonable 
approach.  Gender prediction, for example, is a 
problem where there have been numerous 
publications in the past decade (Corney et al., 
2002; Argamon et al., 2003; Schler et al., 2005; 
Schler, 2006; Yan & Yan, 2006; Zhang et al., 
2009).  Because of the complex and subtle way 
gender influences language choices, it is a 
strategic example to motivate our work. 
 Gender-based language variation arises from 
multiple sources. Among these, it has been noted 
that within a single corpus comprised of samples 
of male and female language that the two 
genders do not speak or write about the same 
topics. This is problematic because word-based 
features such as unigrams and bigrams, which 
are very frequently used, are highly likely to pick 
up on differences in topic (Schler, 2006) and 
possibly perspective. Thus, in cases where 
linguistic style variation is specifically of 
interest, these features do not offer good 
generalizability (Gianfortoni et al., 2011). 
Similarly, in our work, members of different 
109
gangs are located in different areas associated 
with different concerns and levels of 
socioeconomic status.  Thus, in working to 
model the stylistic choices of gang forum 
members, it is important to consider how to 
avoid overfitting to content-level distinctions. 
 Typical kinds of features that have been used 
in gender prediction apart from unigram features 
include part-of-speech (POS) ngrams (Argamon 
et al., 2003), word-structure features that cluster 
words according to endings that indicate part of 
speech (Zhang et al., 2009), features that indicate 
the distribution of word lengths within a corpus 
(Corney et al., 2002), usage of punctuation, and 
features related to usage of jargon (Schler et al., 
2005). In Internet-based communication, 
additional features have been investigated such 
as usage of internet specific features including 
?internet speak? (e.g., lol, wtf, etc.), emoticons, 
and URLs (Yan & Yan, 2006).   
Transformation Origin or meaning 
b^, c^, h^, p^ ?Bloods up? Positive towards 
Bloods, Crips, Hoovers, 
Pirus, respectively 
b ? bk, c ? ck 
h ? hk, p ? pk 
Blood killer, Crip killer 
Hoover killer, Piru killer 
ck ? cc, kc Avoid use of ?ck? since it 
represents Crip killer 
o ? x, o ? ? Represents crosshairs, 
crossing out the ?0?s in a 
name like Rollin? 60s Crips 
b ? 6 Represents the six-pointed 
star. Symbol of Folk Nation 
and the affiliated Crips. 
e ? 3 Various. One is the trinity in 
Trinitario. 
s ? 5 Represents the five-pointed 
star. Symbol of People 
Nation and the affiliated 
Bloods. 
Table 1: Orthographical substitutions from gang 
graffiti symbolism 
 
 In order to place ourselves in the best position 
to build an interpretable model, our space of 
graffiti style features was designed based on a 
combination of qualitative observations of the 
gangs forum data and reading about gang 
communication using web accessible resources 
such as informational web pages linked to the 
forum and other resources related to gang 
communication (Adams & Winter, 1997; Garot, 
2007).  Specifically, in our corpus we observed 
gang members using what we refer to as graffiti 
style features to mark their identity.  Gang 
graffiti employs shorthand references to convey 
affiliation or threats (Adams & Winter, 
1997).  For example, the addition of a <k> after a 
letter representing a rival gang stands for ?killer.? 
So, writing <ck> would represent ?crip killer.? A 
summary of these substitutions can be seen in 
Table 1.  Unfortunately, only about 25% of the 
users among the 12,000 active users employ 
these features in their posts, which limits their 
ability to achieve a high accuracy, but 
nevertheless offers the opportunity to model a 
frequent social practice observed in the corpus.  
 The graffiti style features were extracted 
using a rule-based algorithm that compares 
words against a standard dictionary as well as 
using some phonotactic constraints on the 
position of certain letters.  The dictionary was 
constructed using all of the unique words found 
in the AQUAINT corpus (Graff, 2002).  If a 
word in a post did not match any word from the 
AQUAINT corpus, we tested it against each of 
the possible transformations in Table 1.  
Transformations were applied to words using 
finite state transducers.  If some combination 
transformations from that table applied to the 
observed word could produce some term from 
the AQUAINT corpus, then we counted that 
observed word as containing the features 
associated with the applied transformations. 
 The transformations were applied in the order 
of least likely to occur in normal text to the most 
likely. Since ?bk? only occurs in a handful of 
obscure words, for example, almost any 
occurrence of it can be assumed to be a 
substitution and the ?k? can safely be removed 
before the next step. By contrast, ?cc? and ?ck? 
occur in many common words so they must be 
saved for last to ensure that the final dictionary 
checks have any simultaneous substitutions 
already removed. 
 When computing values for the graffiti style 
features for a text, the value for each feature was 
computed as the number of words (tokens) that 
contained the feature divided by the total number 
of words (tokens) in the document.  We used a 
set of 13 of these features, chosen on the basis of 
how frequently they occurred and how strongly 
they distinguished gangs from one another (for 
example, substituting ?$? for ?s? was a 
transformation that was common across gangs in 
110
our qualitative analysis, and thus did not seem 
beneficial to include).  
Transformation Freq. False 
Positive 
rate 
False 
Negative 
rate 
b^, c^, h^, p^ 15103 0% 0% 
b ? bk 26923 1% 0% 
c ? ck 16144 25% 8% 
h ? hk 10053 1% 0% 
p ? pk 5669 3% 0% 
ck ? cc, kc 72086 2% 0% 
o ? x, o ? ? 13646 15% 5% 
b ? 6 2470 16% 0% 
e ? 3 8628 28% 1% 
s ? 5 13754 6% 0% 
Table 2: Evaluation of extraction of graffiti style 
features over the million post corpus 
 
 The feature-extraction approach was 
developed iteratively. After extracting the 
features over the corpus of 12,000 active users, 
we created lists of words where the features were 
detected, sorted by frequency. We then manually 
examined the words to determine where we 
observed errors occurring and then made some 
minor adjustments to the extractors.  Table 2 
displays a quantitative evaluation of the accuracy 
of the graffiti style feature extraction. 
 Performance of the style features was 
estimated for each style-feature rule.  For each 
rule, we compute a false positive and false 
negative rate.  For false positive rate, we begin 
by retrieving the list of words marked by the 
feature extraction rule containing the associated 
style marking. From the full set of words that 
matched a style feature rule, we selected the 200 
most frequently occurring word types.  We 
manually checked that complete set of word 
tokens and counted the number of misfires.  The 
false positive rate was then calculated for each 
feature by dividing the number of tokens that 
were misfires over the total number of tokens in 
the set. In all cases, we ensured that at least 55% 
of the total word tokens were covered, so 
additional words may have been examined.  
 In the case of false negatives, we started with 
the set of word types that did not match any word 
in the dictionary and also did not trigger the style 
feature rule.  Again we sorted word types in this 
list by frequency and selected the top 200 most 
frequent.  We then manually checked for missed 
instances where the associated style feature was 
used but not detected.  The false negative rate 
was then the total number of word tokens within 
this word type set divided by the total number of 
word tokens in the complete set of word types. 
 Another type of feature we used referenced 
the nicknames gangs used for themselves and 
other gangs, which we refer to as Names features.  
The intuition behind this is simple: someone who 
is a member of the Crips gang will talk about the 
Crips more often. The measure is simply how 
often a reference to a gang occurs per document. 
Some of these nicknames we included were 
gang-specific insults, with the idea that if 
someone uses insults for Crips often, they are 
likely not a Crip. The last type of reference is 
words that refer to gang alliances like the People 
Nation and Folks Nation. Members of those 
Chicago-based gangs frequently refer to their 
gang as the ?Almighty [gang name] Nation?. 
Gang Positive/Neutral 
Mentions 
Insults 
Crips crip, loc crab, ckrip, ck 
Bloods blood, damu, 
piru, ubn 
slob, bklood, 
pkiru, bk, pk 
Hoovers hoover, groover, 
crim, hgc, hcg 
snoover, 
hkoover, hk 
Gangster 
Disciples 
GD, GDN, 
Gangster 
Disciple 
gk, dk, nigka 
Folks 
Nations 
folk, folknation, 
almighty, nation 
 
People 
Nation 
people, 
peoplenation, 
almighty, nation 
 
Latin 
Kings 
alkqn, king, 
queen 
 
Black P. 
Stones 
stone, abpsn, 
moe, black p. 
 
Vice 
Lords 
vice, lord, vl, 
avln, foe, 4ch 
 
Table 3: Patterns used for gang name features.  For all 
gangs listed in the table, there are slang terms used as 
positive mentions of the gang.  For some gangs there 
are also typical insult names. 
 
We used regular expressions to capture 
occurrences of these words and variations on 
them such as the use of the orthographic 
substitutions mentioned previously, plurals, 
feminine forms, etc. Additionally, in the Blood 
and Hoover features, they sometimes use 
numbers to replace the ?o?s representing the 
street that their gang is located on. So the Bloods 
from 34th Street, say, might write ?Bl34d?. 
111
3.2 Computational Paradigm: Multi-
domain learning 
The key to training an interpretable model in our 
work is to pair a rich feature representation with 
a model that enables accounting for the structure 
of the social context explicitly.  Recent work in 
the area of multi-domain learning offers such an 
opportunity (Arnold, 2009; Daum? III, 2007; 
Finkel & Manning, 2009).  In our work, we treat 
the dominant gang of a thread as a domain for 
the purpose of detecting thread composition.  
This decision is based on the observation that 
while it is a common practice across gangs to 
express their attitudes towards allied and 
opposing gangs using stylistic features like the 
Graffiti style features, the particular features that 
serve the purpose of showing affiliation or 
opposition differ by gang.  Thus, it is not the 
features themselves that carry significance, but 
rather a combination of who is saying it and how 
it is being said. 
 As a paradigm for multi-domain learning, we 
use Daume?s Frustratingly Easy Domain 
Adaptation approach (Daum? III, 2007) as 
implemented in LightSIDE (Mayfield & Ros?, 
2013). In this work, Daum? III proposes a very 
simple ?easy adapt? approach, which was 
originally proposed in the context of adapting to 
a specific target domain, but easily generalizes to 
multi-domain learning. The key idea is to create 
domain-specific versions of the original input 
features depending on which domain a data point 
belongs to. The original features represent a 
domain-general feature space. This allows any 
standard learner to appropriately optimize the 
weights of domain-specific and domain-general 
features simultaneously.  In our work, this allows 
us to model how different gangs signal within-
group identification and across-group animosity 
or alliance using different features.  The resulting 
model will enable us to identify how gangs differ 
in their usage of style features to display social 
identity and social relations. 
 It has been noted in prior work that style is 
often expressed in a topic-specific or even 
domain-specific way (Gianfortoni et al., 2011).  
What exacerbates these problems in text 
processing approaches is that texts are typically 
represented with features that are at the wrong 
level of granularity for what is being 
modeled.  Specifically, for practical reasons, the 
most common types of features used in text 
classification tasks are still unigrams, bigrams, 
and part-of-speech bigrams, which are highly 
prone to over-fitting. When text is represented 
with features that operate at too fine-grained of a 
level, features that truly model the target style are 
not present within the model.  Thus, the trained 
models are not able to capture the style itself and 
instead capture features that correlate with that 
style within the data (Gianfortoni et al., 2011). 
 This is particularly problematic in cases 
where the data is not independent and identically 
distributed (IID), and especially where instances 
that belong to different subpopulations within the 
non-IID data have different class value 
distributions.  In those cases, the model will tend 
to give weight to features that indicate the 
subpopulation rather than features that model the 
style.   Because of this insight from prior work, 
we contrast our stylistic features with unigram 
features and our multi-domain approach with a 
single-domain approach wherever appropriate in 
our experiments presented in Section 4. 
4 Prediction Experiments 
In this section we present a series of prediction 
experiments using the annotations described in 
Section 2.  We begin by evaluating our ability to 
identify gang affiliation for individual users.  
Because we will use dominant gang as a domain 
feature in our multi-domain learning approach to 
detect thread composition, we also present an 
evaluation of our ability to automatically predict 
dominant gang for a thread.  Finally, we evaluate 
our ability to predict thread composition.  All of 
our experiments use L1 regularized Logistic 
regression. 
4.1 Predicting Gang Affiliation per User 
The first set of prediction experiments we ran 
was to identify gang affiliation.  For this 
experiment, the full set of posts contributed by a 
user was concatenated together and used as a 
document from which to extract text features.  
We conducted this experiment using a 10-fold 
cross-validation over the full set of users 
annotated for gang affiliation. Results contrasting 
alternative feature spaces at the gang level and 
nation level are displayed in Table 4.  We begin 
with a unigram feature space as the baseline.  We 
contrast this with the Graffiti style features 
described above in Section 3.1.  Because all of 
the Graffiti features are encoded in words as 
pairs of characters, we contrast the carefully 
extracted Graffiti style features with character 
112
bigrams.  Next we test the nickname features 
also described in Section 3.1.  Finally, we test 
combinations of these features.   
 Gang Nation 
Unigrams 70% 81% 
Character Bigrams 64% 76% 
Graffiti Features 44% 68% 
Name Features 63% 78% 
Name + Graffiti 67% 81% 
Unigrams + Name 70% 82% 
Unigrams + Character 
Bigrams 
71% 82% 
Unigrams + Graffiti 71% 82% 
Unigrams + Name  + 
Graffiti 
72% 83% 
Unigrams + Name  + 
Character Bigrams 
72% 79% 
Table 4: Results (percent accuracy) for gang 
affiliation prediction at the gang and nation level. 
  
     We note that the unigram space is a 
challenging feature space to beat, possibly 
because only about 25% of the users employ the 
style features we identified with any regularity.  
The character bigram space actually significantly 
outperforms the Graffiti features, in part because 
it captures aspects of both the Graffiti features, 
the name features, and also some other gang 
specific jargon.  When we combine the stylistic 
features with unigrams, we start to see an 
advantage over unigrams alone.  The best 
combination is Unigrams, Graffiti style features, 
and Name features, at 72% accuracy (.65 Kappa) 
at the gang level and 83% accuracy (.69 Kappa) 
at the nation level.  Overall the accuracy is 
reasonable and offers us the opportunity to 
expand our analysis of social practices on the 
gangs forum to a much larger sample in our 
future work than we present in this first foray. 
4.2 Predicting Dominant Gang per Thread 
In Section 4.3 we present our multi-domain 
learning approach to predicting thread 
composition.  In that work, we use dominant 
gang on a thread as a domain.  In those 
experiments, we contrast results with hand-
annotated dominant gang and automatically-
predicted dominant gang.  In order to compute an 
automatically-identified dominant gang for the 
949 threads used in that experiment, we build a 
model for gang affiliation prediction using data 
from the 2689 users who did not participate on 
any of those threads as training data so there is 
no overlap in users between train and test. 
     The feature space for that classifier included 
unigrams, character bigrams, and the gang name 
features since this feature space tied for best 
performing at the gang level in Section 4.1 and 
presents a slightly lighter weight solution than 
Unigrams, graffiti style features, and gang name 
features. We applied that trained classifier to the 
users who participated on the 949 threads.  From 
the automatically-predicted gang affiliations, we 
computed a dominant gang using the gang and 
nation level for each thread using the same rules 
that we applied to the annotated user identities 
for the annotated dominant gang labels described 
in Section 2.2.  We then evaluated our 
performance by comparing the automatically-
identified dominant gang with the more carefully 
annotated one.  Our automatically identified 
dominant gang labels were 73.3% accurate (.63 
Kappa) at the gang level and 76.6% accurate (.72 
Kappa) at the nation level. This experiment is 
mainly important as preparation for the 
experiment presented in Section 4.3. 
4.3 Predicting Thread Composition 
Our final and arguably most important prediction 
experiments were for prediction of thread 
composition.  This is where we begin to 
investigate how stylistic choices reflect the 
relationships between participants in a 
discussion.  We conducted this experiment twice, 
specifically, once with the annotated dominant 
gang labels (Table 5) and once with the 
automatically predicted ones (Table 6).  In both 
cases, we evaluate gang and nation as alternative 
domain variables.  In both sets of experiments, 
the multi-domain versions significantly 
outperform the baseline across a variety of 
feature spaces, and the stylistic features provide 
benefit above the unigram baseline.  In both 
tables the domain and nation variables are hand-
annotated. * indicates the results are significantly 
better than the no domain unigram baseline.  
Underline indicates best result per column.  And 
bold indicates overall best result.  
     The best performing models in both cases 
used a multi-domain model paired with a stylistic 
feature space rather than a unigram space.  Both 
models performed significantly better than any of 
the unigram models, even the multi-domain 
versions with annotated domains. Where gang 
was used as the domain variable and Graffiti 
style features were the features used for 
prediction, we found that the high weight 
features associated with Allied threads were 
113
either positive about gang identity for a variety 
of gangs other than their own (like B^ in a Crips 
dominated thread) or protective (like CC in a 
Bloods dominated thread).   
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 58%* 60%* 
Character 
Bigrams 
49% 55% 56% 
Graffiti 
Features 
53% 54% 61%* 
Name 
Features 
54% 63%* 66%* 
Name + 
Graffiti 
54% 61%* 65%* 
Unigrams 
+ Name 
52% 58%* 61%* 
Unigrams 
+ Graffiti 
53% 57% 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 61%* 65%* 
Table 5: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches, one 
with dominant gang as the domain variables, and the 
other with dominant nation as the domain variable. In 
this case, the domain variables are annotated. 
 
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 57% 57% 
Character 
Bigrams 
49% 53% 55% 
Graffiti 
Features 
53% 65%* 58%* 
Name 
Features 
54% 61%* 59%* 
Name + 
Graffiti 
54% 60%* 59%* 
Unigrams 
+ Name 
52% 56% 56% 
Unigrams 
+ Graffiti 
53% 58%* 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 60%* 59%* 
Table 6: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches with 
predicted domain variables, one with dominant gang 
as the domain variables, and the other with dominant 
nation as the domain variable.  
 
Crips-related features were the most frequent 
within this set, perhaps because of the complex 
social structure within the Crips alliance, as 
discussed above.  We saw neither features 
associated with negative attitudes of the gang 
towards others nor other gangs towards them in 
these Allied threads, but in opposing threads, we 
see both, for example, PK in Crips threads or BK 
in Bloods threads.  Where unigrams are used as 
the feature space, the high weight features are 
almost exclusively in the general space rather 
than the domain space, and are generally 
associated with attitude directly rather than gang 
identity.  For example, ?lol,? and ?wtf.? 
5 Conclusions  
We have presented a series of experiments in 
which we have analyzed the usage of stylistic 
features for signaling personal gang 
identification and between gang relations in a 
large, online street gangs forum.  This first foray 
into modeling the language practices of gang 
members is one step towards providing an 
empirical foundation for interpretation of these 
practices.  In embarking upon such an endeavor, 
however, we must use caution.  In machine-
learning approaches to modeling stylistic 
variation, a preference is often given to 
accounting for variance over interpretability, 
with the result that interpretability of models is 
sacrificed in order to achieve a higher prediction 
accuracy.  Simple feature encodings such as 
unigrams are frequently chosen in a (possibly 
misguided) attempt to avoid bias.  As we have 
discussed above, however, rather than cognizant 
introduction of bias informed by prior linguistic 
work, unknown bias is frequently introduced 
because of variables we have not accounted for 
and confounding factors we are not aware of, 
especially in social data that is rarely IID. Our 
results suggest that a strategic combination of 
rich feature encodings and structured modeling 
approach leads to high accuracy and 
interpretability.  In our future work, we will use 
our models to investigate language practices in 
the forum at large rather than the subset of users 
and threads used in this paper1. 
                                                          
1 An appendix with additional analysis and the 
specifics of the feature extraction rules can be found 
at http://www.cs.cmu.edu/~cprose/Graffiti.html. This 
work was funded in part by ARL 
000665610000034354.   
114
References  
Adams, K. & Winter, A. (1997). Gang graffiti as a 
discourse genre, Journal of Sociolinguistics 1/3. Pp 
337-360. 
Argamon, S., Koppel, M., Fine, J., & Shimoni, A. 
(2003). Gender, genre, and writing style in formal 
written texts, Text, 23(3), pp 321-346. 
Argamon, S., Koppel, M., Pennebaker, J., & Schler, J. 
(2007). Mining the blogosphere: age, gender, and 
the varieties of self-expression. First Monday 
12(9). 
Arnold, A. (2009). Exploiting Domain And Task 
Regularities For Robust Named Entity 
Recognition. PhD thesis, Carnegie Mellon 
University, 2009. 
Biber, D. & Conrad, S. (2009). Register, Genre, and 
Style, Cambridge University Press 
Bullock, B. (1996). Derivation and Linguistic Inquiry: 
Les Javnais, The French Review 70(2), pp 180-191. 
Corney, M., de Vel, O., Anderson, A., Mohay, G. 
(2002). Gender-preferential text mining of e-mail 
discourse, in the Proceedings of the 18th Annual 
Computer Security Applications Conference. 
Coulthard, M. & Johnson, A. (2007). An Introduction 
to Forensic Linguistics: Language as Evidence, 
Routledge 
Daum? III, H. (2007). Frustratingly Easy Domain 
Adaptation. In Proceedings of the 45th Annual 
Meeting of the Association of Computational 
Linguistics, pages 256-263. 
Eckert, P. & Rickford, J. (2001). Style and 
Sociolinguistic Variation, Cambridge: University 
of Cambridge Press. 
Finkel, J. & Manning, C. (2009). Hierarchical 
Bayesian Domain Adaptation. In Proceedings of 
Human Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. 
Garot, R. (2007). ?Where You From!?: Gang Identity 
as Performance, Journal of Contemporary 
Ethnography, 36, pp 50-84. 
Gianfortoni, P., Adamson, D. & Ros?, C. P. (2011).  
Modeling Stylistic Variation in Social Media with 
Stretchy Patterns, in Proceedings of First 
Workshop on Algorithms and Resources for 
Modeling of Dialects and Language Varieties, 
Edinburgh, Scottland, UK, pp 49-59. 
Graff, D. (2002).  The AQUAINT Corpus of English 
News Text, Linguistic Data Consortium, 
Philadelphia 
Greenlee, M. (2010).  Youth and Gangs, in M. 
Coulthard and A. Johnson (Eds.). The Routledge 
Handbook of Forensic Linguistics, Routledge. 
Jiang, M. & Argamon, S. (2008). Political leaning 
categorization by exploring subjectivities in 
political blogs. In Proceedings of the 4th 
International Conference on Data Mining, pages 
647-653. 
Johnsons, K. (2009).  FBI: Burgeoning gangs behind 
up to 80% of U.S. Crime, in USA Today, January 
29, 2009. 
Kahneman,  D. (2011).  Thinking Fast and Slow, 
Farrar, Straus, and Giroux 
Krippendorff, K. (2013). Content Analysis: An 
Introduction to Its Methodology (Chapter 13), 
SAGE Publications 
Labov, W. (2010). Principles of Linguistic Change: 
Internal Factors (Volume 1), Wiley-Blackwell. 
Lefkowitz, N. (1989).  Talking Backwards in French, 
The French Review 63(2), pp 312-322. 
Mayfield, E. & Ros?, C. P. (2013). LightSIDE: Open 
Source Machine Learning for Text Accessible to 
Non-Experts, in The Handbook of Automated 
Essay Grading, Routledge Academic Press.        
http://lightsidelabs.com/research/ 
Philips, S. (2009).  Crip Walk, Villian Dance, Pueblo 
Stroll: The Embodiment of Writing in African 
American Gang Dance, Anthropological Quarterly 
82(1), pp69-97. 
Schler, J., Koppel, M., Argamon, S., Pennebaker, J. 
(2005). Effects of Age and Gender on Blogging, 
Proceedings of AAAI Spring Symposium on 
Computational Approaches for Analyzing Weblogs. 
Schler, J. (2006). Effects of Age and Gender on 
Blogging. Artificial Intelligence, 86, 82-84. 
Wiebe, J., Bruce, R., Martin, M., Wilson, T., & Ball, 
M. (2004). Learning Subjective Language, 
Computational Linguistics, 30(3). 
Yan, X., & Yan, L. (2006). Gender classification of 
weblog authors. AAAI Spring Symposium Series 
Computational Approaches to Analyzing Weblogs 
(p. 228?230). 
Zhang, Y., Dang, Y., Chen, H. (2009). Gender 
Difference Analysis of Political Web Forums : An 
Experiment on International Islamic Women?s 
Forum, Proceedings of the 2009 IEEE international 
conference on Intelligence and security 
informatics, pp 61-64. 
 
115
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104?113,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Rare Social Phenomena in Conversation:
Empowerment Detection in Support Group Chatrooms
Elijah Mayfield, David Adamson, and Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA 15213
{emayfiel, dadamson, cprose}@cs.cmu.edu
Abstract
Automated annotation of social behavior
in conversation is necessary for large-scale
analysis of real-world conversational data.
Important behavioral categories, though,
are often sparse and often appear only
in specific subsections of a conversation.
This makes supervised machine learning
difficult, through a combination of noisy
features and unbalanced class distribu-
tions. We propose within-instance con-
tent selection, using cue features to selec-
tively suppress sections of text and bias-
ing the remaining representation towards
minority classes. We show the effective-
ness of this technique in automated anno-
tation of empowerment language in online
support group chatrooms. Our technique
is significantly more accurate than multi-
ple baselines, especially when prioritizing
high precision.
1 Introduction
Quantitative social science research has experi-
enced a recent expansion, out of controlled set-
tings and into natural environments. With this
influx of interest comes new methodology, and
the inevitable question arises of how to move
towards testable hypotheses, using these uncon-
trolled sources of data as scientific lenses into the
real world.
The study of conversational transcripts is a key
domain in this new frontier. There are certain
social and behavioral phenomena in conversation
that cannot be easily identified through question-
naire data, self-reported surveys, or easily ex-
tracted user metadata. Examples of these social
phenomena in conversation include overt displays
of power (Prabhakaran et al, 2012) or indicators
of rapport and relationship building (Wang et al,
2012). Manually annotating these social phenom-
ena cannot scale to large data, so researchers turn
to automated annotation of transcripts (Rose? et al,
2008). While machine learning is highly effec-
tive for annotation tasks with relatively balanced
labels, such as sentiment analysis (Pang and Lee,
2004), more complex social functions are often
rarer. This leads to unbalanced class label distri-
butions and a much more difficult machine learn-
ing task. Moreover, features indicative of rare so-
cial annotations tend to be drowned out in favor of
features biased towards the majority class. The net
effect is that classification algorithms tend to bias
towards the majority class, giving low accuracy for
rare class detection.
Automated annotation of social phenomena also
brings opportunities for real-world applications.
For example, real-time annotation of conversation
can power adaptive intervention in collaborative
learning settings (Rummel et al, 2008; Adamson
and Rose?, 2012). However, with the considerable
power of automation comes great responsibility. It
is critical to avoid intervening in the case of er-
roneous annotations, as providing unnecessary or
inappropriate support in such a setting has been
shown to be harmful to group performance and so-
cial cohesion (Dillenbourg, 2002; Stahl, 2012).
We propose adaptations to existing machine
learning algorithms which improve recognition of
rare annotations in conversational text data. Our
primary contribution comes in the form of within-
instance content selection. We develop a novel al-
gorithm based on textual cues, suppressing infor-
mation which is likely to be irrelevant to an in-
stance?s class label. This allows features which
predict minority classes to gain prominence, help-
ing to sidestep the frequency of common features
pointing to a majority class label.
Additionally, we propose modifications to ex-
isting algorithms. First, we identify a new appli-
cation of logistic model trees to text data. Next,
104
we define a modification of confidence-based en-
semble voting which encourages minority class la-
beling. Using these techniques, we demonstrate a
significant improvement in classifier performance
when recognizing the language of empowerment
in support group chatrooms, a critical application
area for researchers studying conversational inter-
actions in healthcare (Uden-Kraan et al, 2009).
The remainder of this paper is structured as fol-
lows. We introduce the domain of empowerment
in support contexts, along with previous studies on
the challenges that these annotations (and similar
others) bring to machine learning. We introduce
our new technique for improving the ability to au-
tomate this annotation, along with other optimiza-
tions to the machine learning workflow which are
tailored to this skewed class balance. We present
experimental results showing that our method is
effective, and provide a detailed analysis of the be-
havior of our model and the features it uses most.
We conclude with a discussion of particularly use-
ful applications of this work.
2 Background
We ground this paper?s discussion of machine
learning with a real problem, turning to the an-
notation of empowerment language in chat1. The
concept of empowerment, while a prolific area
of research, lacks a broad definition across pro-
fessionals, but broadly relates to ?the power to
act efficaciously to bring about desired results?
(Boehm and Staples, 2002) and ?experiencing per-
sonal growth as a result of developing skills and
abilities along with a more positive self-definition?
(Staples, 1990). Participants in online support
groups feel increased empowerment (Uden-Kraan
et al, 2009; Barak et al, 2008). Quantita-
tive studies have shown the effect of empower-
ment through statistical methods such as structural
equation modeling (Vauth et al, 2007), as have
qualitative methods such as deductive transcript
analysis (Owen et al, 2008) and interview studies
(Wahlin et al, 2006).
The transition between these styles of research
has been gradual. Pioneering work has demon-
strated the ability to distinguish empowerment lan-
guage in written texts, including prompted writ-
ing samples (Pennebaker and Seagal, 1999), nar-
1Definitions of empowerment are closely related to the
notion of self-efficacy (Bandura, 1997). For simplicity, we
use the former term exclusively in this paper.
Table 1: Empowerment label distribution in our
corpus.
Annotation Label # %
Self-Empowerment NA 1522 79.3
POS 202 10.5
NEG 196 10.2
Other-Empowerment NA 1560 81.3
POS 217 11.3
NEG 143 7.4
ratives in online forums (Hoybye et al, 2005), and
some preliminary analysis of synchronous discus-
sion (Ogura et al, 2008; Mayfield et al, 2012b).
These transitional works have used limited analy-
sis methodology; in the absence of sophisticated
natural language processing, their conclusions of-
ten rely on coarse measures, such as word counts
and proportions of annotations in a text.
Users, of course, do not express empowerment
in every thread in which they participate, which
leads to a challenge for machine learning. Threads
often focus on a single user?s experiences, in
which most participants in a chat are merely com-
mentators, if they participate at all, matching pre-
vious research on shifts in speaker salience over
time (Hassan et al, 2008). This leads to many
user threads which are annotated as not applicable
(N/A). We move to our proposed approach with
these skewed distributions in mind.
3 Data
Our data consists of a set of chatroom conversa-
tion transcripts from the Cancer Support Commu-
nity2. Each 90-minute conversation took place in
the context of a weekly meeting in a real-time chat,
with up to 6 participants in addition to a profes-
sional therapist facilitating the discussion. In to-
tal, 2,206 conversations were collected from 2007-
2011. This data offers potentially rich insight into
coping and social support; however, annotating
such a dataset by hand would be prohibitively ex-
pensive, even when it is already transcribed.
Twenty-one of these conversations have been
annotated, as originally described and analyzed
in (Mayfield et al, 2012b)3. This data was dis-
entangled into threads based on common themes
or topics, as in prior work (Elsner and Charniak,
2www.cancersupportcommunity.org
3All annotations were found to be adequately reliable be-
tween humans, with thread disentanglement f = 0.75 and
empowerment annotation ? > 0.7.
105
Figure 1: An example mapping from a single thread?s chat lines (left) to the per-user, per-thread instances
used for classification in this paper (right), with example annotations for self-empowerment indicated.
2010; Adams and Martel, 2010). A novel per-
user, per-thread annotation was then employed
for empowerment annotation, following a coding
manual based on definitions like those in Section
2. Each user was assigned a label of positive
or negative empowerment if they exhibited such
emotions, or was left blank if they did not do so
within the context of that thread. This annotation
was performed both for their self-empowerment
as well as their attitude towards others? situations
(other-empowerment). An example of this annota-
tion for self-empowerment is presented in Figure
1 and the distribution of labels is given in Table 1.
Most previous annotation tasks attempt to an-
notate on a per-utterance basis, such as dialogue
act tagging (Popescu-Belis, 2008), or on arbitrary
spans of text, such as in the MPQA subjectivity
corpus (Wiebe et al, 2005). However, for our task,
a per-user, per-thread annotation is more appropri-
ate, because empowerment is often indicated best
through narrative (Hoybye et al, 2005). Human
annotators are instructed to take this context into
account when annotating (Mayfield et al, 2012b).
It would therefore be nonsensical to annotate indi-
vidual lines as ?embodying? empowerment. Simi-
lar arguments have been made for sentiment, espe-
cially as the field moves towards aspect-oriented
sentiment (Breck et al, 2007). Assigning labels
based on thread boundaries allows for context to
be meaningfully taken into account, without cross-
ing topic boundaries.
However, this granularity comes with a price:
the distribution of class values in these instances
is highly skewed. In our data, the vast majority of
users? threads are marked as not applicable to em-
powerment. Perhaps more inconveniently, while
taking context into account is important for reli-
able annotation, it leads to extraneous information
in many cases. Many threads can have multiple
lines of contributions that are topically related to
an expression of empowerment (and thus belong
in the same thread), but which do not indicate any
empowerment themselves. This exacerbates the
likelihood of instances being classified as N/A.
We choose to take advantage of these attributes
of threads. We know from research in discourse
analysis that many sections of conversations are
formulaic and rote, like introductions and greet-
ings (Schegloff, 1968). We additionally know that
polarity often shifts in dialogue through the use
of discourse connectives such as conjunctions and
transitional phrases. These issues have been ad-
dressed in work in the language technologies com-
munity, most notably through the Penn Discourse
Treebank (Prasad et al, 2008); however, their ap-
plications to noisier synchronous conversation has
beenrare in computational linguistics.
With these linguistic insights in mind, we ex-
amine how we can make best use of them for
machine learning performance. While techniques
for predicting rare events (Weiss and Hirsh, 1998)
and compensating for class imbalance (Frank and
106
Bouckaert, 2006), these approaches generally fo-
cus on statistical properties of large class sets with-
out taking the nature of their datasets into account.
In the next section, we propose a new algorithm
which takes advantage specifically of the linguis-
tic phenomena in the conversation-based data that
we study for empowerment detection. As such,
our algorithm is highly suited to this data and task,
with the necessary tradeoff in uncertain generality
to new domains with unrelated data.
4 Cue Discovery for Content Selection
Our algorithm performs content selection by
learning a set of cue features. Each of these fea-
tures indicates some linguistic function within the
discourse which should downplay the importance
of features either before or after that discourse
marker. Our algorithm allows us to evaluate the
impact of rules against a baseline, and to itera-
tively judge each rule atop the changes made by
previous rules.
This algorithm fits into existing language tech-
nologies research which has attempted to partition
documents into sections which are more or less
relevant for classification. Many researchers have
attempted to make use of cue phrases (Hirschberg
and Litman, 1993), especially for segmentation
both in prose (Hearst, 1997) and conversation
(Galley et al, 2003). The approach of content se-
lection, meanwhile, has been explored for senti-
ment analysis (Pang and Lee, 2004), where indi-
vidual sentences may be less subjective and there-
fore less relevant to the sentiment classification
task. It is also similar conceptually to content
selection algorithms that have been used for text
summarization (Teufel and Moens, 2002) and text
generation (Sauper and Barzilay, 2009), both of
which rely on finding highly-relevant passages
within source texts.
Our work is distinct from these approaches.
While we have coarse-grained annotations of em-
powerment, there is no direct annotation of what
makes a good cue for content selection. With
our cues, we hope to take advantage of shallow
discourse structure in conversation, such as con-
trastive markers, making use of implicit structure
in the conversational domain.
4.1 Notation
Before describing extensions to the baseline lo-
gistic regression model, we define notation. Our
data is arranged hierarchically. We assume that
we have a collection of d training documents Tr =
{D1 . . . Dd}, each of which contains many train-
ing instances (in our task, an instance consists of
all lines of chat from one user in one thread). Our
total set of n instances I thus consists of instances
{I1, I2, . . . In}. Each document contains lines of
chat L and each instance Ii is comprised of some
subset of those lines, Li ? L.
Our feature space X = {x1, x2, . . . xm} con-
sists of m unigram features representing the ob-
served vocabulary used in our corpus. Each in-
stance is associated with a feature vector x? con-
taining values for each x ? X, and each feature
x that is present in the i-th instance maintains a
?memory? of the lines in which it appeared in that
instance, Lix, where Lix ? Li. Our potential out-
put labels consist of Y = {NA,NEG,POS},
though this generalizes to any nominal classifica-
tion task. Each instance I is associated with ex-
actly one y ? Y for self-empowerment and one
for other-empowerment; these two labels do not
interact and our tasks are treated as independent
in this paper4. We define classifiers as functions
f(x?? y ? Y); in practice, we use logistic regres-
sion via LibLINEAR (Fan et al, 2008).
We define a content selection rule as a pairing
r = ?c, t? between a cue feature c ? X and a se-
lection function t ? T . We created a list of possi-
ble selection functions, given a cue c, maximizing
for generality while being expressive. These are
illustrated in Figure 2 and described below:
? Ignore Local Future (A): Ignore all features
from the two lines after each occurrence of c.
? Ignore All Future (B): Ignore all features
occurring after the first occurrence of c.
? Ignore Local History (C): Ignore all features
in the two lines preceding each occurrence of
c.
? Ignore All History (D): Ignore all features
occurring only before the last occurrence of
c.
We define an ensemble member E = ?R, fR? -
the ordered list of learned content selection rules
R = [r1, r2, . . . ] and a classifier fC trained on in-
stances transformed by those rules. Our final out-
4Future work may examine the interaction of jointly an-
notating multiple sparse social phenomena.
107
Figure 2: Effects of content selection rules, based
on a cue feature (ovals) observed at lines m and n.
put of a trained model is a set of ensemble mem-
bers {E1, . . . , Ek}.
4.2 Algorithm
Our ensemble learning follows the paradigm
of cross-validated committees (Parmanto et al,
1996), where k ensemble members are trained by
subdividing our training data into k subfolds. For
each ensemble classifier, cue rulesR are generated
on k ? 1 subfolds (Trk) and evaluated on the re-
maining subfold (Tek). In practice, with 21 train-
ing documents, 7-fold cross-validation, and k = 3
ensemble members, each generation set consists
of 12 documents? instances, while each evaluation
set contains instances from 6 documents.
Our full algorithm is presented in Algorithm
1, and is broken into component parts for clar-
ity. Algorithm 2 begins by measuring the base-
line classifier?s ability to recognize minority-class
labels. After training on Trk, we measure the
average probability assigned to the correct label
of instances in Tek, but only for instances whose
correct labels are minority classes (remember, be-
cause both Trk and Tek are drawn from the over-
all Tr, we have access to true class labels). We
choose this subset of only minority instances, as
we are not interested in optimizing to the majority
class.
We next enumerate all rules that we wish to
judge. To keep this problem tractable, we ignore
features which do not occur in at least 5% of train-
ing instances. For the remaining features, we cre-
ate a candidate rule for each possible pairing of
features and selection functions. For each of these
candidates, we test its utility by selecting content
as if it were an actual rule, then building a new
classifier (trained on the generation set) using in-
stances that have been altered in that way. In the
evaluation set, we measure the difference in prob-
ability of minority class labels being assigned cor-
rectly between the baseline and this altered space.
This measure of an individual rule?s impact is de-
scribed in Algorithm 3.
Once we have evaluated every possible rule
once, we select the top-ranked rule and ap-
ply it to the feature set. We then iteratively
progress through our now-ranked list of candi-
dates, each time treating the newly filtered dataset
as our new baseline. We search only top can-
didates for efficiency, following the fixed-width
search methodology for feature selection in very
high-dimensionality feature spaces (Gu?tlein et al,
2009). Each ensemble classifier is finally retrained
on all training data, after applying the correspond-
ing content selection rules to that data.
5 Prediction
Our prediction algorithm begins with a stan-
dard implementation of cross-validated commit-
tees (Parmanto et al, 1996), whose results are
aggregated with a confidence voting method in-
tended to favor rare labels (Erp et al, 2002).
Cross-validated committees are an ensemble tech-
nique used to subsample training data to produce
multiple hypotheses for classification. Each clas-
sifier produced by our cue-based transformation
is trained on a subset of our training data. Each
makes predictions on all test set instances, pro-
ducing a distribution of confidence across possi-
ble labels. These values serve as inputs to a voting
method to produce a final label for each instance.
Compared to other ensemble methods, cross-
validated committees as described above are a
good fit for our task, because of its unique unit of
analysis. As thread-level analysis is the set of in-
dividual participants? turns in a conversation, we
risk overfitting if we sample from the same con-
versations for the training and testing sets. In con-
trast to standard bagging, hard sampling bound-
aries never train and test on instances drawn from
the same conversation.
To aggregate the votes from members of this en-
semble into a final prediction, we employ a variant
on Selfridge?s Pandemonium (Selfridge, 1958).
If a minority label is selected as the highest-
confidence value in any classifier in our ensem-
ble, it is selected. The majority label, by contrast,
is only selected if it is the most likely prediction
by all classifiers in our ensemble. Thus consen-
sus is required to elect the majority class, and the
strongest minority candidate is elected otherwise.
108
In : generation set Trk, evaluation set Tek
Out: ensemble committee {E1 . . . Ek}
for i = 1 to k do
Rfinal ? [ ];
Xfreq ? {x ? X | freq(x) ? Trk >
5%};
R? Xfreq ? T ;
R? ? R;
repeat
Pbase ? EvaluateClassifier(Trk,Tek);
EvaluateRules(Pbase,Trk,Tek, R?);
Trk,Tek ? ApplyRule(R?[0]);
R? R?R?[0];
?? score(R?[0]);
Rfinal ? Rfinal +R?[0];
R? ? R[0 . . . 50];
until ? < threshold;
Trfinal ? Trk ? Tek;
foreach r ? Rfinal do
Trfinal ? ApplyRule(Trfinal, r);
end
Train f(x?? y) on Trfinal;
end
Algorithm 1: LearnSelectionCues()
This approach is designed to bias the prediction
of our machine learning algorithms in favor of mi-
nority classes in a coherent manner. If there is a
plausible model that has been trained which rec-
ognizes the possibility of a rare label, it is used;
the prediction only reverts to the majority class
when no plausible minority label could be chosen.
As validation of this technique, we compare our
?minority pandemonium? approach against both
typical pandemonium and standard sum-rule con-
fidence voting (Erp et al, 2002).
5.1 Logistic Model Stumps
One characteristic of highly skewed data is that,
while minority labels may be expressed in a num-
ber of different surface forms, there are many ob-
vious cases in which they do not apply. These
cases can actually be harmful to classification of
borderline cases. Features that could be given high
weight in marginal cases may be undervalued in
?low-hanging fruit? easy cases. To remove those
obvious instances, a very simple screening heuris-
tic is often enough to eliminate frequent pheno-
types of instances where the rare annotation is
not present. Prior work has sometimes screened
training data through obvious heuristic rules, espe-
In : generation set Trk, evaluation set Tek
Out: minority class probability average Pbase
Train f(x?? y) on Trk;
Temink ? {Instance I ? Tek | yI 6= ?NA?}
;
Pbase ? 0 ;
foreach Instance I ? Temink do
Pbase ? Pbase + P (f(x?I) = yI)
end
Pbase = Pbase/size(Temink )Algorithm 2: EvaluateClassifier()
In : Trk, Tek, rules R, base probability Pbase
Out: R sorted on each rule?s improvement
score
foreach Rule r ? R do
Tr?k,Te?k ? ApplyRule(Trk,Tek, r);
Palter ? EvaluateClassifier(Tr?k,Te?k);
score(r)? Palter ? Pbase;
end
Sort R on score(r) from high to low;
Algorithm 3: EvaluateRules()
cially in speech recognition; for instance, training
speech recognition for words followed by a pause
separately from words followed by another word
(Franco et al, 2010), or training separate models
based on gender (Jiang et al, 1999).
We achieve this instance screening by learn-
ing logistic model tree stumps (Landwehr et al,
2005), which allow us to quickly partition data if
there is a particularly easy heuristic that can be
learned to eliminate a large number of majority-
class labels. One challenge of this approach is
our underlying unigram feature space - tree-based
algorithms are generally poor classifiers for the
high-dimensionality, low-information features in a
lexical feature space (Han et al, 2001). To com-
pensate, we employ a smaller, denser set of binary
features for tree stump screening: instance length
thresholds and LIWC category membership.
First, we define a set of features that split based
on the number of lines an instance contains, from
1 to 10 (only a tiny fraction of instances are more
than 10 lines long). For example, a feature split-
ting on instances with lines ? 2 would be true
for one- and two-line instances, and false for all
others. Second, we define a feature for each cate-
gory in the Linguistic Inquiry and Word Count dic-
tionary (Tausczik and Pennebaker, 2010) - these
broad classes of words allow for more balanced
109
Figure 3: Precision/recall curves for algorithms.
After 50% recall all models converge and there are
no significant differences in performance.
splits than would unigrams alone. Each category?s
feature is true if any word in that category was
used at least once in that instance.
We exhaustively sweep this feature space, and
report the most successful stump rules for each an-
notation task. In our other experiments, we report
results with and without the best rule for this pre-
processing step; we also measure its impact alone.
6 Experimental Results
All experiments were performed using LightSIDE
(Mayfield and Rose?, 2013). We use a binary uni-
gram feature space, and we perform 7-fold cross-
validation. Instances from the same chat transcript
never occur in both train and testing folds. Fur-
thermore, we assume that threads have been dis-
entangled already, and our experiments use gold
standard thread structure. While this is not a triv-
ial assumption, prior work has shown thread dis-
entanglement to be manageable (Mayfield et al,
2012a); we consider it an acceptable simplify-
ing assumption for our experiments. We compare
our methods against baselines including a majority
baseline, a baseline logistic regression classifier
with L2 regularized features, and two common en-
semble methods, AdaBoost (Freund and Schapire,
1996) and bagging (Breiman, 1996) with logistic
regression base classifiers5.
Table 2 presents the best-performing result
from each classification method. For self-
empowerment recognition, all methods that we
introduce are significant improvements in ?, the
5These methods usually use weak, unstable base classi-
fiers; however, in our experiments, those performed poorly.
Table 2: Performance for baselines, common en-
semble algorithms, and proposed methods. Statis-
tically significant improvements over baseline are
marked (p < .01, ?; p < .05, *; p < 0.1, +).
Self Other
Method % ? % ?
Majority 79.3 .000 81.3 .000
LR Baseline 81.0 .367 81.0 .270
LR + Boosting 78.1 .325 78.5 .275
LR + Bagging 81.2 .352 81.9 .265
LR + Committee 81.0 .367 81.0 .270
Learned Stumps 81.8* .385? 81.7 .293+
Content Selection 80.9 .389? 80.7 .282
Stumps+Selection 81.3 .406? 79.4 .254
Table 3: Performance of content-selection
wrapped learners, for minority voting and two
baseline voting methods.
Self Other
Method % ? % ?
Pandemonium 80.3 .283 81.4 .239
Averaged 80.6 .304 81.6 .251
Minority Voting 80.9? .389? 80.7 .282
measurement of agreement over chance, compared
to all baselines. While accuracy remains stable,
this is due to predictions shifting away from the
majority class and towards minority classes. Our
combined model using both logistic model tree
stumps and content selection is significantly better
than either alone (p < .01). To compare the mi-
nority pandemonium voting method against base-
lines of simple pandemonium and summed confi-
dence voting, Table 3 presents the results of con-
tent selection wrappers with each voting method.
Minority voting is more effective compared to
standard confidence voting, improving ? while
modestly reducing accuracy; this is typical of a
shift towards minority class predictions.
7 Discussion
These results show promise for our techniques,
which are able to distinguish features of rare la-
bels, previously awash in a sea of irrelevance. Fig-
ure 3 shows the impact of our rules as we tune
to different levels of recall, with a large boost in
precision when recall is not important; our model
converges with the baseline for high-recall, low-
precision tuning. This suggests that our method is
particularly suitable for tasks where confident la-
110
Table 4: Cue rules commonly selected by the algo-
rithm. Average improvement over the LR baseline
is also shown.
Self-Empowerment
Cue Transformation ?%
and,but Ignore Local Future +5.0
have Ignore All History +4.3
! Ignore All History +4.2
me,my Ignore All History +3.4
Other-Empowerment
Cue Transformation ?%
and,but Ignore Local Future +5.5
you Ignore Local History +5.2
?s Ignore Local History +4.1
that Ignore Local History +3.9
beling of a few instances is more important than
labeling as many instances as possible. This is
common when tasks have a high cost or carry high
risk (for instance, providing real-time conversa-
tional supports with an agent, where inappropriate
intervention could be disruptive). Other low-recall
applications include exploration large corpora for
exemplar instances, where the most confident pre-
dictions for a given label should be presented first
for analyst use. In the rest of this section, we
examine notable within-instance and per-instance
rules selected by our methods. These rules are
summarized in Tables 4 and 5.
For both self- and other-empowerment, we find
pronoun rules that match the task (first-person and
second-person pronouns for self-Empowerment
and other-Empowerment respectively). In both
tasks, we find cue rules that suppress the context
preceding personal pronouns. These, as well as
the possessive suffix ?s, echo the per-instance ef-
fect of the Self and You splits, anticipating that
what follows such a personal reference is likely to
bear an evaluation of empowerment. Exclamation
marks may indicate strong emotion - we find many
instances where what precedes a line with an ex-
clamation is more objective, and what follows in-
cludes an assessment. Conjunctions but and and
are selected as cue rules suppressing the two lines
that follow the occurrence - suggesting, as sus-
pected, that connective discourse markers play a
role in indicating empowerment (Fraser, 1999).
The best-performing stump splits for the Self-
Empowerment annotation are Line Length ? 1
and the LIWC word-categories Article, Swear, and
Table 5: Best decision rules for logistic model
stumps. Significant improvement (p < 0.05) in-
dicated with *.
Self-Empowerment
Split Rule ? ?? % ?%
Split ? 1 * 0.385 +.018 81.8 +0.8
LIWC-Article 0.379 +.012 81.6 +0.6
LIWC-Swear * 0.376 +.009 81.4 +0.4
LIWC-Self * 0.376 +.009 81.5 +0.5
Other-Empowerment
Split Rule ? ?? % ?%
LIWC-You 0.293 +.023 81.7 +0.7
LIWC-Eating * 0.283 +.013 81.6 +0.6
LIWC-Negate * 0.282 +.012 82.3 +1.3
LIWC-Present 0.281 +.011 81.6 +0.6
Self. The split on line length corresponds to the
observation that longer instances provide greater
opportunity for personal narrative self-assessment
to occur (95% of single-line instances are labeled
NA). The Article category may serve as a proxy for
content length - article-less instances in our corpus
include one-line social greetings and exchanges
of contact information. Swear words may be a
cue for awareness of self-empowerment - a recent
study of women coping with illness reported that
swearing in the presence of others, but not alone,
was related to potentially harmful outcomes (Rob-
bins et al, 2011). Among other- oriented split
rules, Eating stands out as non-obvious, although
medical literature has suggested a link between
dietary behavior and empowerment attitudes in a
study of women with cancer (Pinto et al, 2002).
8 Conclusion
We have demonstrated an algorithm for improv-
ing automated classification accuracy on highly
skewed tasks for conversational data. This algo-
rithm, particularly its focus on content selection, is
rooted in the structural format of our data, which
can generalize to many tasks involving conversa-
tional data. Our experiments show that this model
significantly improves machine learning perfor-
mance. Our algorithm is taking advantage of
structural facets of discourse markers, lending ba-
sic sociolinguistic validity to its behavior. Though
we have treated each of these rarely-occurring la-
bels as independent thus far, in practice we know
that this is not the case. Joint prediction of labels
through structured modeling is an obvious next
111
step for improving classification accuracy.
This is an important step towards large-scale
analysis of the impact of support groups on pa-
tients and caregivers. Our method can be used to
confidently highlight occurrences of rare labels in
large data sets. This has real-world implications
for professional intervention in social conversa-
tional domains, especially in scenarios where such
an intervention is likely to be associated with a
high cost or high risk. With the construction of
more accurate classifiers, we open the possibility
of automating annotation on large conversational
datasets, enabling new directions for researchers
with domain expertise.
Acknowledgments
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485.
References
Paige Adams and Craig Martel. 2010. Conversational
thread extraction and topic detection in text-based
chat. In Semantic Computing.
David Adamson and Carolyn Penstein Rose?. 2012.
Coordinating multi-dimensional support in collabo-
rative conversational agents. In Proceedings of In-
telligent Tutoring Systems.
Albert Bandura. 1997. Self-Efficacy: The Exercise of
Control.
Azy Barak, Meyran Boniel-Nissim, and John Suler.
2008. Fostering empowerment in online support
groups. Computers in Human Behavior.
A Boehm and L H Staples. 2002. The functions of the
social worker in empowering: The voices of con-
sumers and professionals. Social Work.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Pierre Dillenbourg. 2002. Over-scripting cscl: The
risks of blending collaborative learning with instruc-
tional design. Three worlds of CSCL. Can we sup-
port CSCL?
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Merijn Van Erp, Louis Vuurpijl, and Lambert
Schomaker. 2002. An overview and comparison of
voting methods for pattern recognition. In Frontiers
in Handwriting Recognition. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Horacio Franco, Harry Bratt, Romain Rossier,
Venkata Rao Gadde, Elizabeth Shriberg, Victor
Abrash, and Kristin Precoda. 2010. Eduspeak: A
speech recognition and pronunciation scoring toolkit
for computer-aided language learning applications.
Language Testing.
Eibe Frank and Remco R Bouckaert. 2006. Naive
bayes for text classification with unbalanced classes.
Knowledge Discovery in Databases.
Bruce Fraser. 1999. What are discourse markers?
Journal of pragmatics, 31(7):931?952.
Yoav Freund and Robert E Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL.
Martin Gu?tlein, Eibe Frank, Mark Hall, and Andreas
Karwath. 2009. Large-scale attribute selection us-
ing wrappers. In Proceedings of IEEE CIDM.
Eui-Hong Han, George Karypis, and Vipin Kumar.
2001. Text categorization using weight adjusted
k-nearest neighbor classification. Lecture Notes in
Computer Science: Advances in Knowledge Discov-
ery and Data Mining.
Ahmed Hassan, Anthony Fader, Michael H Crespin,
Kevin M Quinn, Burt L Monroe, Michael Colaresi,
and Dragomir R Radev. 2008. Tracking the dy-
namic evolution of participant salience in a discus-
sion. In Proceedings of Coling.
Marti A Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics.
Mette Terp Hoybye, Christoffer Johansen, and Tine
Tjornhoj-Thomsen. 2005. Online interaction ef-
fects of storytelling in an internet breast cancer sup-
port group. Psycho-oncology.
Hui Jiang, Keikichi Hirose, and Qiang Huo. 1999. Ro-
bust speech recognition based on a bayesian predic-
tion approach. In IEEE Transactions on Speech and
Audio Processing.
Niels Landwehr, Mark Hall, and Eibe Frank. 2005.
Logistic model trees. Machine Learning.
Elijah Mayfield and Carolyn Penstein Rose?. 2013.
Lightside: Open source machine learning for text.
In Handbook of Automated Essay Evaluation: Cur-
rent Applications and New Directions.
112
Elijah Mayfield, David Adamson, and Carolyn Pen-
stein Rose?. 2012a. Hierarchical conversation struc-
ture prediction in multi-party chat. In Proceedings
of SIGDIAL Meeting on Discourse and Dialogue.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and
Carolyn Penstein Rose?. 2012b. Discovering habits
of effective online support group chatrooms. In
ACM Conference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jason E. Owen, Erin O?Carroll Bantum, and Mitch
Golant. 2008. Benefits and challenges experienced
by professional facilitators of online support groups
for cancer survivors. In Psycho-Oncology.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the Association for Computational Linguistics.
Bambang Parmanto, Paul Munro, and Howard R
Doyle. 1996. Improving committee diagnosis with
resampling techniques. In Proceedings of NIPS.
James W Pennebaker and J D Seagal. 1999. Forming
a story: The health benefits of narrative. Journal of
Clinical Psychology.
Bernardine M Pinto, Nancy C Maruyama, Matthew M
Clark, Dean G Cruess, Elyse Park, and Mary
Roberts. 2002. Motivation to modify lifestyle risk
behaviors in women treated for breast cancer. In
Mayo Clinic Proceedings.
Andrei Popescu-Belis. 2008. Dimensionality of di-
alogue act tagsets: An empirical analysis of large
corpora. In Language Resources and Evaluation.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting overt display of power in
written dialogs. In Proceedings of NAACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Megan L Robbins, Elizabeth S Focella, Shelley Kasle,
Ana Mar??a Lo?pez, Karen L Weihs, and Matthias R
Mehl. 2011. Naturalistically observed swear-
ing, emotional support, and depressive symptoms
in women coping with illness. Health Psychology,
30:789.
Carolyn Penstein Rose?, Yi-Chia Wang, Yue Cui, Jaime
Arguello, Karsten Stegmann, Armin Weinberger,
and Frank Fischer. 2008. Analyzing collabo-
rative learning processes automatically: Exploit-
ing the advances of computational linguistics in
computer-supported collaborative learning. In Inter-
national Journal of Computer Supported Collabora-
tive Learning.
Nikol Rummel, Armin Weinberger, Christof Wecker,
Frank Fischer, Anne Meier, Eleni Voyiatzaki,
George Kahrimanis, Hans Spada, Nikolaos Avouris,
and Erin Walker. 2008. New challenges in cscl:
Towards adaptive script support. In Proceedings of
ICLS.
Christina Sauper and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of ACL.
Emanuel A Schegloff. 1968. Sequencing in conversa-
tional openings. American Anthropologist.
Oliver G Selfridge. 1958. Pandemonium: a
paradigm for learning. In Proceedings of Sympo-
sium on Mechanisation of Thought Processes, Na-
tional Physical Laboratory.
Gerry Stahl. 2012. Interaction analysis of a biology
chat. Productive multivocality.
Lee H Staples. 1990. Powerful ideas about empower-
ment. Administration in Social Work.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientic articles: Experiments with relevance and
rhetorical status. Computational Linguistics.
C F Van Uden-Kraan, C H C Drossaert, E Taal, E R
Seydel, and M A F J Van de Laar. 2009. Partici-
pation in online patient support groups endorses pa-
tients empowerment. Patient Education and Coun-
seling.
R Vauth, B Kleim, M Wirtz, and P W Corrigan. 2007.
Self-efficacy and empowerment as outcomes of self-
stigmatizing and coping in schizophrenia. Psychia-
try Research.
Ingrid Wahlin, Anna-Christina Ek, and Ewa Idvali.
2006. Patient empowerment in intensive carean in-
terview study. Intensive and Critical Care Nursing.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan Black, and Justine Cassell. 2012. ?love
ya, jerkface:? using sparse log-linear models to build
positive (and impolite) relationships with teens. In
Proceedings of SIGDIAL.
Gary M Weiss and Haym Hirsh. 1998. Learning to
predict rare events in event sequences. In Proceed-
ings of KDD.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation.
113
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 49?59,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
 
 
Modeling of Stylistic Variation in Social Media with Stretchy Patterns 
   Philip Gianfortoni David Adamson Carolyn P. Ros? Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University Language Technologies Institute Language Technologies Institute Language Technologies Institute Pittsburgh, PA Pittsburgh, PA Pittsburgh, PA pwg@cs.cmu.edu dadamson@cs.cmu.edu cprose@cs.cmu.edu      Abstract In this paper we describe a novel feature discovery technique that can be used to model stylistic variation in sociolects. While structural features offer much in terms of expressive power over simpler features used more frequently in machine learning approaches to modeling linguistic variation, they frequently come at an excessive cost in terms of feature space size expansion.  We propose a novel form of structural features referred to as ?stretchy patterns? that strike a balance between expressive power and compactness in order to enable modeling stylistic variation with reasonably small datasets.  As an example we focus on the problem of modeling variation related to gender in personal blogs.  Our evaluation demonstrates a significant improvement over standard baselines. 1 Introduction The contribution of this paper is a novel approach to feature induction seeking to model stylistic variation at a level that not only achieves high performance, but generalizes across domains better than alternative techniques. Building on an earlier template based approach for modeling sarcasm (Tsur et al, 2010), we investigate the use of what we have termed ?stretchy? features to model 
stylistic variation related to sociolects, which can be thought of as a form of dialect.  Specifically, we focus on the problem of gender based classification.  Gender classification and age classification have both received increased attention in the social media analysis community in recent years (Goswami et al, 2009; Barbieri, 2008; Cieri et al, 2004), most likely because large data sets annotated with these variables have recently become available.  Machine learning technology provides a lens with which to explore linguistic variation that complements earlier statistical techniques used by variationist sociolinguists in their work mapping out the space of dialect variation and its accompanying social interpretation (Labov, 2010a; Labov, 2010b; Eckert & Rickford, 2001).  These complementary approaches share a common foundation in numerical methods, however while descriptive statistics and inferential statistics mainly serve the purpose of describing non-random differences in distributions between communities, machine learning work in the area of social media analysis asks the more challenging question of whether the differences described are big enough to enable identification of community membership by means of those differences. In the remainder of the paper, we first introduce prior work in a variety of related areas that both demonstrates why generalizable models characterizing sociolects within social media contexts are challenging to create and motivates our novel approach.  Next we describe our 
49
  
technical approach for inducing ?stretchy patterns?.  We then present a series of experiments that demonstrate that our stretchy patterns provide advantages over alternative feature spaces in terms of avoiding overfitting to irrelevant content-based features as evidenced both in terms of achieving higher performance with smaller amounts of training data and in terms of generalizing better across subpopulations that share other demographic and individual difference variables.  2 Prior Work Analysis of social media has grown in popularity over the past decade.  Nevertheless, results on problems such as gender classification (Argamon et al, 2003), age classification (Argamon et al, 2007), political affiliation classification (Jiang & Argamon, 2008), and sentiment analysis (Wiebe et al, 2004) demonstrate how difficult stylistic classification tasks can be, and even more so when the generality is evaluated by testing models trained in one domain on examples from another domain. Prior work on feature engineering has attempted to address this generalization difficulty.  Here we motivate our ?stretchy pattern? approach to feature engineering for modeling sociolects, using gender analysis as a lens through which to understand the problem. 2.1 Variation Analysis and Gender Since the earliest work in the area of variationist sociolinguistics, gender has been a variable of interest, which explains interesting differences in communication style that have been the topic of discussion both in academic circles (Holmes & Meyerhoff, 2003) and in the popular press (Tannen, 2001). The immense significance that has been placed on these differences, whether they are viewed as essentially linked to inherent traits, learned cultural patterns, or socially situated identities that are constructed within interaction, warrants attention to gender based differences within the scope of dialect variation. While one may view gender differences in communication from multiple angles, including topic, stance, and style, we focus specifically on linguistic style in our work.  Numerous attempts to computationally model gender based language variation have been published in the past decade (Corney et al, 2002; 
Argamon et al, 2003; Schler et al, 2005; Schler, 2006; Yan & Yan, 2006; Zhang et al, 2009; Mukherjee & Liu, 2010). Gender based language variation arises from multiple sources. For example, within a single corpus comprised of samples of male and female language that the two genders do not speak or write about the same topics. This has been reported to be the case with blog corpora such as the one used in this paper. Even in cases where pains have been taken to control for the distribution of topics associated with each gender within a corpus (Argamon et al, 2003), it?s still not clear the extent to which that distribution is completely controlled. For example, if one is careful to have equal numbers of writing samples related to politics from males and females, it may still be the case that males and females are discussing different political issues or are addressing political issues from a different role based angle. While these differences are interesting, they do not fit within the purview of linguistic style variation.  Word based features such as unigrams and bigrams are highly likely to pick up on differences in topic (Schler, 2006) and possibly perspective. Thus, in cases where linguistic style variation is specifically of interest, these features are not likely to be included in the set of features used to model the variation even if their use leads to high performance within restricted domains. Typical kinds of features that are used instead include part-of-speech (POS) n-grams (Koppel, 2002; Argamon et al, 2003), word structure features that cluster words according to endings that indicate part of speech (Zhang et al, 2009), features that indicate the distribution of word lengths within a corpus (Corney et al, 2002), usage of punctuation, and features related to usage of jargon (Schler et al, 2005). In Internet-based communication, additional features have been investigated such as usage of internet specific features including ?internet speak? (e.g., lol, wtf, etc.), emoticons, and URLs (Yan & Yan, 2006). In addition to attention to feature space design issues, some work on computational modeling of gender based language variation has included the development of novel feature selection techniques, which have also had a significant impact on success (Mukherjee & Liu, 2010; Zhang, Dang, & Chen, 2009).  Of these features, the only ones that capture stylistic elements that extend beyond individual 
50
  
words at a time are the POS ngram features. The inclusion of these features has been motivated by their hypothesized generality, although in practice, the generality of gender prediction models has not been formally evaluated in the gender prediction literature. 2.2 Domain Adaptation in Social Media Recent work in the area of domain adaptation (Arnold et al, 2008; Daum? III, 2007; Finkel & Manning, 2009) raises awareness of the difficulties with generality of trained models and offers insight into the reasons for the difficulty with generalization. We consider these issues specifically in connection with the problem of modeling gender based variation. One problem, also noted by variationist sociolinguists, is that similar language variation is associated with different variables (McEnery, 2006).  For example, linguistic features associated with older age are also more associated with male communication style than female communication style for people of the same age (Argamon et al, 2007).  Another problem is that style is not exhibited by different words than those that serve the purpose of communicating content.  Thus, there is much about style that is expressed in a topic specific way.   What exacerbates these problems in text processing approaches is that texts are typically represented with features that are at the wrong level of granularity for what is being modeled.  Specifically, for practical reasons, the most common types of features used in text classification tasks are still unigrams, bigrams, and part-of-speech bigrams.  While relying heavily on these relatively simple features has computational advantages in terms of keeping the feature space size manageable, which aids in efficient model learning, in combination with the complicating factors just mentioned, these text classification approaches are highly prone to over-fitting.  Specifically, when text is represented with features that operate at too fine grained of a level, features that truly model the target style are not present within the model.  Thus, the trained models are not able to capture the style itself and instead capture features that merely correlate with that style within the data.  Thus, in cases where the data is not independent and identically distributed (IID), 
and where instances that belong to different subpopulations within the non-IID data have different class value distributions, the model will tend to give weight to features that indicate the subpopulation rather than features that model the style.  This may lead to models that perform well within datasets that contain the same distribution of subpopulations, but will not generalize to different subpopulations, or even datasets composed of different proportions of the same subpopulations. Models employing primarly unigrams and bigrams as features are particularly problematic in this respect.  2.3 Automatic Feature Engineering In recent years, a variety of manual and automatic feature engineering techniques have been developed in order to construct feature spaces that are adept at capturing interesting language variation without overfitting to content based variation, with the hope of leading to more generalizable models.  POS n-grams, which have frequently been utilized in genre analysis models (Argamon et al, 2003), are a strategic balance between informativity and simplicity. They are able to estimate syntactic structure and style without modeling it directly. In an attempt to capture syntactic structure more faithfully, there has been experimentation within the area of sentiment analysis on using syntactic dependency features (Joshi & Ros?, 2009; Arora, Joshi, & Ros?, 2009). However, results have been mixed. In practice, the added richness of the features comes at a tremendous cost in terms of dramatic increases in feature space size. What has been more successful in practice is templatizing the dependency features in order to capture the same amount of structure without creating features that are so specific.  Syntactic dependency based features are able to capture more structure than POS bigrams, however, they are still limited to representing relationships between pairs of words within a text. Thus, they still leave much to be desired in terms of representation power. Experimentation with graph mining from dependency parses has also been used for generating rich feature spaces (Arora et al, 2010). However, results with these features has also been disappointing. In practice, the rich features with real predictive power end up being 
51
  
difficult to find amidst myriads of useless features that simply add noise to the model. One direction that has proven successful at exceeding the representational power and performance of POS bigrams with only a very modest increase in feature space size has been a genetic programming based approach to learning to build a strategic set of rich features so that the benefits of rich features can be obtained without the expense in terms of feature space expansion. Successful experiments with this technique have been conducted in the area of sentiment analysis, with terminal symbols including unigrams in one case (Mayfield & Ros?, 2010) and graph features extracted from dependency parses in another (Arora et al, 2010). Nevertheless, improvements using these strategic sets of evolved features have been very small even where statistically significant, and thus it is difficult to justify adding so much machinery for such a small improvement.  Another direction is to construct template based features that combine some aspects of POS n-grams in that they are a flat representation, and the backoff version of dependency features, in that the symbols represent sets of words, which may be POS tags, learned word classes, distribution based word classes (such as high frequency words or low frequency words), or words. Such types of features have been used alone or in combination with sophisticated feature selection techniques or bootstrapping techniques, and have been applied to problems such as detection of sarcasm (Tsur et al, 2010), detection of causal connections between events (Girju, 2010), or machine translation (Gimpel et al, 2011). Our work is most similar to this class of approaches.  3  Technical Approach: Stretchy Patterns Other systems have managed to extract and employ patterns containing gaps with some success.  For example, Gimpel (2011) uses Gibbs sampling to collect patterns containing single-word gaps, and uses them among other features in a machine translation system.  Our patterns are more like the ones described in Tsur (2010), which were applied to the task of identifying sarcasm in sentences.  We predicted that a similar method would show promise in extracting broader stylistic features indicative of the author?s group-aligned dialect. We have chosen 
the classification of an author?s gender as the task to which we can apply our patterns. 3.1    Pattern-Based Features To extract their sarcasm-detecting patterns, Tsur (2010) first defined two sets of words: High Frequency Words (HFW) and Content Words (CW).  The HFW set contained all words that occurred more than 100 times per million, and the CW set contained all words in the corpus that occurred fewer than 1000 times per million.  Thus, a word could be contained in the HFW set, the CW set, or both. Such patterns must begin and end with words in the HFW set, and (as in our implementation) are constrained in the number of words drawn from each set. Additionally, as a preprocessing step, in their approach they made an attempt to replace phrases belonging to several categories of domain-specific phrases, such as product and manufacturer names with a label string, which was then added to the HFW set, indicating membership.  For example, given an input such as ?Garmin apparently does not care much about product quality or customer support?, a number of patterns would be produced, including ?[company] CW does not CW much?.  3.2   Stretchy Patterns Tsur?s patterns were applied as features to classify sentences as sarcastic (or not), within the domain of online product reviews. Here our implementation and application diverge from Tsur?s ? the blog corpus features large multi-sentence documents, and span a diverse set of topics and authors. We aim to use these patterns not to classify sentiment or subtlety, but to capture the style and structure employed by subsets of the author-population.  We define a document as an ordered list of tokens.  Each token is composed of a surface-form lexeme and any additional syntactic or semantic information about the word at this position (in our case this is simply the POS tag, but other layers such as Named Entity might be included). We refer to any of the available forms of a token as a type. A category is a set of word-types. Each type must belong to at least one category.  All categories have a corresponding label, by which they?ll be referred to within the patterns to come. Gap is a 
52
  
special category, containing all types that aren?t part of any other category. The types belonging to any defined category may also be explicitly added to the Gap category.   A stretchy pattern is defined as a sequence of categories, which must not begin or end with a Gap category.  We designate any number of adjacent Gap instances in a pattern by the string ?GAP+?1 and every other category instance by its label.  As a convention, the label of a singleton category is the name of the type contained in the category (thus "writes" would be the label of a category containing only surface form "writes" and "VBZ" would be the label of the a category containing only the POS tag "VBZ"). The overall number of Gap and non-Gap category instances comprising a pattern is restricted - following Tsur (2010), we allow no more than six tokens of either category. In the case of Gap instances, this restriction is placed on the number of underlying tokens, and not the collapsed GAP+ form.   A sequence of tokens in a document matches a pattern if there is some expansion where each token corresponds in order to the pattern?s categories. A given instance of GAP+ will match between zero and six tokens, provided the total number of Gap instances in the pattern do not exceed six2.  By way of example, two patterns follow, with two strings that match each. Tokens that match as Gaps are shown in parenthesis. [cc] (GAP+) [adj] [adj] ?and (some clients were) kinda popular...? ?from (our) own general election...?  for (GAP+) [third-pron] (GAP+) [end] [first-pron] ?ready for () them (to end) . I am...? ?for (murdering) his (prose) . i want??  Although the matched sequences vary in length and content, the stretchy patterns preserve information about the proximity and ordering of particular words and categories. They focus on the relationship between key (non-Gap) words, and allow a wide array of sequences to be matched  by 
                                                           1 This is actually an extractor parameter, but we collapse all adjacent gaps for all our experiments. 2 The restrictions on gaps are extractor parameters, but we picked zero to six gaps for our experiments. 
a single pattern in a way that traditional word-class n-grams would not. Our ?stretchy pattern? formalism strictly subsumes Tsur?s approach in terms of representational power.  In particular, we could generate the same patterns described in Tsur (2010) by creating a singleton surface form category for each word in Tsur?s HFW and then creating a category called [CW] that contains all of the words in the Tsur CW set, in addition to the domain-specific product/manufacturer categories Tsur employed.  Label Category Members adj JJ, JJR, JJS cc CC, IN md MD end <period>, <comma>, <question>, <exclamation> first-pron I, me, my, mine, im, I?m second-pron you, your, youre, you?re, yours, y?all third-pron he, him emotional feel, hurt, lonely, love time hour, hours, late, min, minute, minutes, months, schedule, seconds, time, years,  male_curse fucking, fuck, jesus, cunt, fucker female_curse god, bloody, pig, hell, bitch, pissed, assed, shit Table 1. Word Categories 3.3   Word Categories With the aim of capturing general usage patterns, and motivated by the results of corpus linguists and discourse analysts, a handful token categories were defined, after the fashion of the LIWC categories as discussed in Gill (2009). Tokens belonging to categories may be replaced with their category label as patterns are extracted from each document. As a token might belong to multiple categories, the same token sequence may generate, and therefore match multiple patterns.   Words from a list of 800 common prepositions, conjunctions, adjectives, and adverbs were included as singleton surface-form categories. Determiners in particular are absent from this list (and from the POS categories that follow), as their absence or presence in a noun phrase is one of the primary variations the stretchy gaps of our patterns were intended to smooth over.  A handful of POS categories were selected, reflecting previous research and predictions about gender differences in language usage. For example, to capture the ?hedging? discussed in Holmes (2003) as more common in female speech, the modal tag MD was included as a singleton 
53
  
category. A category comprising the coordinating conjunction and preposition tags (CC, IN) was included to highlight transitions in complicated or nested multi-part sentences.  Additionally, where previous results suggested variation within a category based on gender (e.g. swearing, as in McEnery (2006)), two categories were added, with the words most discriminative for each gender. However, even those words most favored by male authors might appear in contexts where males would never use them - it is our hope that by embedding these otherwise-distinguishing features within the structure afforded by gap patterns we can extract more meaningful patterns that more accurately and expressively capture the style of each gender. 3.4   Extraction and Filtering Patterns are extracted from the training set, using a sliding window over the token stream to generate all allowable combinations of category-gap sequences within the window. This generates an exponential number of patterns - we initially filter this huge set based on each pattern?s accuracy and coverage as a standalone classifier, discarding those with less than a minimum precision or number of instances within the training set. In the experiments that follow, these thresholds were set to a minimum of 60% per-feature precision, and at least 15 document-level hits. 4 Evaluation We have motivated the design of our stretchy patterns by the desire to balance expressive power and compactness. The evidence of our success should be demonstrated along two dimensions: first, that these compact features allow our models to achieve a higher performance when trained on small datasets and second, that models trained with our stretchy patterns generalize better between domains. Thus, in this section, we present two evaluations of our approach in comparison to three baseline approaches. 4.1   Dataset We chose to use the Blog Authorship Corpus for our evaluation, which has been used in earlier work related to gender classification (Schler 2006), 
and which is available for web download3. Each instance contains a series of personal blog entries from a single author. For each blog, we have metadata indicating the gender, age, occupation, and astrological sign of the author. From this corpus, for each experiment, we randomly selected a subset in which we have balanced the distribution of gender and occupation. In particular, we selected 10 of the most common occupations in the dataset, specifically Science, Law, Non-Profit, Internet, Engineering, Media, Arts, Education, Technology, and Student. We randomly select the same number of blogs from each of these occupations, and within occupation based sets, we maintain an even distribution of male and female authors. We treat the occupation variable as a proxy for topic since bloggers typically make reference to their work in their posts. We make use of this proxy for topic in our evaluation of domain generality below. 4.2   Baseline Approaches We can find in the literature a variety of approaches to modeling gender based linguistic variation, as outlined in our prior work discussion above. If our purpose was to demonstrate that our stretchy patterns beat the state-of-the-art at the predictive task of gender classification, it would be essential to implement one of these approaches as our baseline. However, our purpose here is instead to address two more specific research questions instead, and for that we argue that we can learn something from comparing with three more simplistic baselines, which differ only in terms of feature extraction. The three baseline models we tested included a unigram model, a unigram+bigram model, and a Part-of-Speech bigram model. For part-of-speech tagging we used the Stanford part-of-speech tagger4 (Toutanova et al, 2003).  Our three baseline feature spaces have been very commonly used in the language technologies community for a variety of social media analysis tasks, the most common of which in recent years has been sentiment analysis. While these feature spaces are simple, they have remained surprisingly strong baseline approaches when testing is done 
                                                           3  http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm 4  http://nlp.stanford.edu/software/tagger.shtml 
54
  
within domain, and with large enough training sets. However, these relatively weak, low level features are notorious for low performance when datasets are too small and for low generalizability when evaluated in a cross-domain setting. Because of this, we expect to see our baseline approaches perform well when both training and testing data match in terms of topic distribution and when we use our largest amount of training data. However, we expect performance to degrade as training data set size decreases as well as when we test in a cross-domain setting. We expect to see degradation also with our proposed stretchy patterns. However, we will consider our claims to have been supported if we see less degradation with our stretchy patterns than with the baseline approaches.  We did minimal preprocessing on the textual data prior to feature extraction for all approaches. Specifically, all numbers in the text were replaced with a <number> symbol. Punctuation was separated from words and treated as a separate symbol. All tokens were downcased so that we generalize across capitalization options. In all cases, we use a support vector machine approach to training the model, using the SMO implementation found in Weka (Witten & Frank, 2005), using a linear polynomial kernel and default settings. For each model, we first use a Chi-Squared filter for attribute selection over the training data, retaining only the top 3,000 features prior to training.  4.3  Study 1: Learning on Small Datasets The purpose of Study 1 was to test the claim that our stretchy patterns achieve higher performance when we train using a small amount of data. For this evaluation, we constructed a test set of 3,000 instances that we use consistently across training configurations. Specifically, we selected 300 blogs from each of the 10 occupations listed above such that 150 of them were from male authors and 150 from female authors. We constructed also a set of training sets of size 300, 800, 1500, 2000, and 3000 randomly selected blogs respectively, in which we maintain the same occupation and gender distribution as in the test set. To compensate for sampling eccentricities, two samples of each training size were extracted, and their results averaged for each experiment. In all cases, from each blog, we randomly selected one 
blog entry that was at least 100 words long. For each baseline approach as well as the stretchy feature approach, we build a model using each training set, which we then test using the common test set. Thus, for each approach, we can examine how performance increases as amount of training data increases, and we can compare this growth curve between approaches.  Training Set Size Unigram Unigram + Bigram POS Bigram Stretchy Patterns 300 49.9  (-.002) 49.85(-.002) 51.6   ( .032) 48.65(-.027) 800 51.65( .029) 50.15 (.003) 50.55 ( .014) 53.15 ( .072) 1500 48.6  (-.028) 49.98     (0) 48.63 (-.028) 53.95 ( .066) 2000 50.55( .011) 51.7   (.034) 51.82 ( .063) 53.98 ( .079) 3000 49.48(-.010) 50.8   (.016) 49.88 ( .0025) 59.05 ( .181) Table 2 Classification accuracy for varying data sizes (with kappa in parentheses)  The dramatic mediocrity of the baselines? performance highlights the difficulty of the selected data set, confirming the sense that most of what these n-gram models pick up is not truly gender-specific usage, but shadows of the distribution of topics (here, occupations) between the genders. At all sizes except the smallest (where no approach is significantly better than random), our approach outperforms the baselines. At size 800, this difference is marginal (p < .1), and at the larger sizes, it is a significant increase (p < .05). 4.4  Study 2: Evaluation of Domain Generality For our evaluation of domain generality, we randomly selected 200 blogs from each of the 10 most common occupations in the corpus, 100 of which were by male authors and 100 by female authors. As in the evaluation above, from each blog, we randomly selected one blog entry that was at least 100 words long. In order to test domain generality, we perform a leave-one-occupation-out cross validation experiment, which we refer to as a Cross Domain evaluation setting. In this setting, on each fold, we always test on blogs from an occupation that was not represented within the training data. Thus, indicators of gender that are specific to an occupation will not generalize from training to test.  Table 3 displays the results from the comparison of our stretchy feature approach with each of the baseline approaches. On average, stretchy patterns generalized better to new domains 
55
  
than the other approaches. The stretchy feature approach beat the baseline approaches in a statistically significant way (p < .05). Occupation Unigram Unigram + Bigram POS Bigram Stretchy Patterns Engineering 49.5    (-.01) 53      ( .06) 49    (-.02) 50.5    ( .01) Education 49       (-.02) 52      ( .04) 54.5  ( .09) 51       ( .02) Internet 55.5    ( .11) 47.5   (-.05) 55.5 ( .11) 56.5    ( .13) Law 51.5    ( .03) 46.5   (-.07) 46.5 (-.07) 50.5    ( .01) Non-Profit 50         ( 0 ) 54      ( .08) 49    (-.02) 51.      ( .02) Technology 50         ( 0 ) 53.5   ( .07) 50     ( 0 )  51.5    ( .03) Arts 48       (-.04) 46.5   (-.07) 51    ( .02) 55.4    ( .11) Media 53       ( .06) 50        ( 0 ) 45    (-.10) 51.5    ( .02) Science 52       ( .04) 48      (-.04) 40.5 (-.19) 59.5    ( .19) Student 51       ( .02) 46      (-.09) 55    ( .10) 62       ( .24) Average 50.95  (.002) 49.7 (-.007) 49.6  ( .01) 53.94  ( .08) Random CV 61.05    (.22) 59.65  (.19) 57.95 (.16) 62.8    ( .26) Table 3 Accuracy from leave-one-occupation-out cross-validation (with kappa in parentheses)  For random cross-validation, our approach performed marginally better than the unigram baseline, and again significantly exceeds the performance of the other two baselines. Note that for all approaches, there is a significant drop in performance from Random CV to the cross-domain setting, showing that all approaches, including ours, suffer from domain specificity to some extent.  However, while all of the baselines drop down to essentially random performance in the cross-domain setting, and stretchy patterns remain significantly higher than random, we show that our approach has more domain generality, although it still leaves room for improvement on that score. 5 Qualitative Analysis of Results Here we present a qualitative analysis of the sorts of patterns extracted by our method. Although we cannot draw broad conclusions from a qualitative investigation of such a small amount of data, we did observe some interesting trends.   As our features do not so much capture syntactic structure as the loose proximity and order of classes of words, we?ll say less about structure and more about what sort of words show up in each others? neighborhood. In particular, a huge proportion of the top-ranked patterns feature instances of the [end] and [first-pron] categories, 
suggesting that much of the gender distinction captured by our patterns is to be found around sentence boundaries and self-references. It?s believable and encouraging that ?the way I talk about myself? is an important element in distinguishing style between genders.  The Chi-squared ranking of the stretchy patterns gives us a hint as to the predictive effect of each as a feature. In the discussion and examples that follow, we?ll draw from the highest-ranked features, and refer to the weights? signs to label each pattern as ?male? or ?female?.  In these features the discourse analyst or dialectician can find fodder for their favorite framework, or support for popularly held views on gender and language. For example, we find that about twice as many of the patterns containing either [third-pron] or [second-pron] in the neighborhood of [first-pron] are weighted toward female, supporting earlier findings that women are more concerned with considering interpersonal relationships in their discourse than are men, as  in Kite (2002). For example,   [first-pron] (GAP+) [third-pron]  ?i (have time for) them?  Supporting the notion that distinctively female language is ?deviant,? and viewed as a divergence from a male baseline, as discussed in Eckert & McConnell-Ginet (1992), we note that more of the top-ranked patterns are weighted toward female. This might suggest that the ?female? style is less standard and therefore harder to detect. Additionally, we only find adjacent [end] markers, capturing repeated punctuation, in our female-weighted patterns. For instance,    [adj] (GAP+) [end] (GAP+) [end] [end]  ?new (songs) ! ( :-) see yas ) . .?  This divergence from the standard sentence form, while more common overall in informal electronic communications, does occur more frequently among female authors in the data. Further analysis of the data suggests that emoticons like :-) would have formed a useful category for our patterns, as they occur roughly twice as often in female posts, and often in the context of end-of-sentence punctuation.    We provide a rough numerical overview of the features extracted during the random cross-validation experiment. Samples of high-ranking 
56
  
stretchy patterns appear in Tables 4 and 5. Note that sequences may match more than one pattern, and that GAP+ expansions can have zero length.  [first-pron] (female) and i have time for... (female) a freshman , my brother is... (male) and i overcame my fear ...  [end] (GAP+) [first-pron] (female) no ! ! ! (i just guess) i... (female) all year . (. .) i am so... (male) the internet . () i ask only... [end] (GAP+) [end] and (female) positives . (gotta stay positive) . and hey... (female) at the park ..(. sitting at teh bench alone ..).  and walking down on my memory line... (male) sunflower . (she has a few photo galleries ..).   and i would like... like (GAP+) [first-pron] (female) well like (anywho . . . I got) my picture back? (female) it?s times like (these that I miss) my friends... (male) with something like (that in the air ,) i don't...	
 Table 4. Female Patterns.  [adj] (GAP+) [end] (GAP+) [first-pron] (male) her own digital (camera) . (what  enlightens) me is... (male) a few (photo galleries ..) . (and) i would... (female) money all (year) . (..) i am so much... [first-pron] (GAP+) [end] (male) again . i (ate so well today , too) . lots of ... (male) movie i ('d already seen once before) .  (female) a junior and i (have the top locker) . lol [end] (GAP+) [first-pron] (GAP+) [cc] (male) food ! () i ('m so hooked) on this delicious... (male) galleries .(.. and) i (would like) for you to... (female) alot better . () i (have a locker right) above... so (GAP+) [end] (male) was it ? so (cousins , stay posted) . remember... (male) experience you've gained so (far) . if... (female) , its been so (damn crappy out) . ok bye Table 5. Male Patterns.  Although our patterns capture much more than the unigram frequencies of categories, a glance at such among the extracted patterns will prove enlightening. Of the 3000 patterns considered, 1407 were weighted to some degree toward male, and 1593 toward female. Overall, female patterns include more of our chosen categories than their male counterparts. Many of these imbalances matched our initial predictions, in particular the greater number of female patterns with [first-pron] 
(772 vs. 497), [second-pron] (47 vs 27), [third-pron] (286 vs. 203), and [end] (851 vs. 618), [emotion] (36 vs. 20).   Contrary to our expectations, [md] appeared only slightly more frequently in female patterns (73 vs. 66), and [time] appeared in only a few male patterns (22 female vs. 7 male) - of these time-patterns, most of the matching segments included the word ?time? itself, instead of any other time-related words. No patterns containing the divided curse categories were among the top-ranked features.  6 Conclusions and Current Directions In this paper we described a novel template based feature creation approach that we refer to as stretchy patterns. We have evaluated our approach two ways, once to show that with this approach we are able to achieve higher performance than baseline approaches when small amounts of training data are used, and one in which we demonstrated that we are able to achieve better performance in a cross domain evaluation setting.  While the results of our experiments have shown promising results, we acknowledge that we have scratched the surface of the problem we are investigating. First, our comparison was limited to just a couple of strategically selected baselines. However, there have been many variations in the literature on gender classification specifically, and genre analysis more generally, that we could have included in our evaluations, and that would likely offer additional insights. For example, we have tested our approach against POS bigrams, but we have not utilized longer POS sequences, which have been used in the literature on gender classification with mixed results. In practice, longer POS sequences have only been more valuable than POS bigrams when sophisticated feature selection techniques have been used (Mukherje & Liu, 2010). Attention may also be directed to the selection or generation of word categories better suited to stretchy patterns. Alternative approaches to selecting or clustering these features should also be explored. 7 Acknowledgements This research was funded by ONR grant N000141110221 and NSF DRL-0835426. 
57
  
References  Argamon, S., Koppel, M., Fine, J., & Shimoni, A. (2003). Gender, genre, and writing style in formal written texts, Text, 23(3), pp 321-346. Argamon, S., Koppel, M., Pennebaker, J., & Schler, J. (2007). Mining the blogosphere: age, gender, and the varieties of self-expression. First Monday 12(9). Arnold, A. (2009). Exploiting Domain And Task Regularities For Robust Named Entity Recognition. PhD thesis, Carnegie Mellon University, 2009. Arora, S., Joshi, M., Ros?, C. P. (2009). Identifying Types of Claims in Online Customer Reviews, Proceedings of the North American Chapter of the Association for Computational Linguistics.  Arora, S., Mayfield, E., Ros?, C. P., & Nyberg, E. (2010). Sentiment Classification using Automatically Extracted Subgraph Features, Proceedings of the NAACL HLT Workshop on Emotion in Text. Barbieri, F. (2008). Patterns of age-based linguistic variation in American English. Journal of Sociolinguistics 12(1), pp 58-88.  Cieri, C., Miller, D., & Walker, K. (2004). The fisher corpus: a resource for the next generations of speech-to-text. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pp 69-71. Corney, M., de Vel, O., Anderson, A., Mohay, G. (2002). Gender-preferential text mining of e-mail discourse, in the Proceedings of the 18th Annual Computer Security Applications Conference. Daum? III, H. (2007). Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256-263. Eckert, P. & Rickford, J. (2001). Style and Sociolinguistic Variation, Cambridge: University of Cambridge Press. Eckert, P. & McConnell-Ginet, S. (1992). Think Practically and Look Locally: Language and Gender as Community- Based Practice. In  the Annual Review of Anthropology, Vol. 21, pages 461-490. Finkel, J. & Manning, C. (2009). Hierarchical Bayesian Domain Adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Gill, A., Nowson, S. & Oberlander, J. (2009). What Are They Blogging About? Personality, Topic and 
Motivation in Blogs. In Proceedings of the Third International ICWSM Conference. Gimpel, K., Smith, N. A. (2011). Unsupervised Feature Induction for Modeling Long-Distance Dependencies in Machine Translation, Forthcoming. Girju, R. (2010). Towards Social Causality: An Analysis of Interpersonal Relationships in Online Blogs and Forums, Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media. Goswami, S., Sarkar, S. & Rustagi, M. (2009). Stylometric analysis of bloggers? age and gender. In Proceedings of the Third International ICWSM Conference.  Holmes, J. & Meyerhoff, M. (2003). The Handbook of Language and Gender, Blackwell Publishing. Jiang, M. & Argamon, S. (2008). Political leaning categorization by exploring subjectivities in political blogs. In Proceedings of the 4th International Conference on Data Mining, pages 647-653. Joshi, M. & Ros?, C. P. (2009). Generalizing Dependency Features for Opinion Mining, Proceedings of the Association for Computational Linguistics. Kite, M. (2002) Gender Stereotypes, in the Encyclopedia of Women and Gender: Sex Similarities and DIfferences, Volume 1, Academic Press. Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.  Labov, W. (2010a). Principles of Linguistic Change: Internal Factors (Volume 1), Wiley-Blackwell. Labov, W. (2010b). Principles of Linguistic Change: Social Factors (Volume 2), Wiley-Blackwell. Mayfield, E. & Ros?, C. P. (2010). Using Feature Construction to Avoid Large Feature Spaces in Text Classification, in Proceedings of the Genetic and Evolutionary Computation Conference. McEnery, T. (2006). Swearing in English: Bad language, purity and power from 1586 to the present, Routledge. Mukherjee, A. & Liu, B. (2010). Improved Gender Classification of Blog Authors, Proceedings of EMNLP 2010. Schler, J., Koppel, M., Argamon, S., Pennebaker, J. (2005). Effects of Age and Gender on Blogging, Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. 
58
  
Schler, J. (2006). Effects of Age and Gender on Blogging. Artificial Intelligence, 86, 82-84. Tannen, D. (2001). You Just Don?t Understand: Women and Men in Conversation, First Quill. Tsur, O., Davidov, D., & Rappoport, A. (2010). ICWSM ? A Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product Reviews, Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media. Wiebe, J., Bruce, R., Martin, M., Wilson, T., & Ball, M. (2004). Learning Subjective Language, Computational Linguistics, 30(3). Witten, I. & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and Techniques, second edition, Elsevier, San Francisco. Yan, X., & Yan, L. (2006). Gender classification of weblog authors. AAAI Spring Symposium Series Computational Approaches to Analyzing Weblogs (p. 228?230). Zhang, Y., Dang, Y., Chen, H. (2009). Gender Difference Analysis of Political Web Forums : An Experiment on International Islamic Women?s Forum, Proceedings of the 2009 IEEE international conference on Intelligence and security informatics, pp 61-64.    
59
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 60?69,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Hierarchical Conversation Structure Prediction in Multi-Party Chat
Elijah Mayfield, David Adamson, and Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{emayfiel, dadamson, cprose}@cs.cmu.edu
Abstract
Conversational practices do not occur at a sin-
gle unit of analysis. To understand the inter-
play between social positioning, information
sharing, and rhetorical strategy in language,
various granularities are necessary. In this
work we present a machine learning model
for multi-party chat which predicts conversa-
tion structure across differing units of analy-
sis. First, we mark sentence-level behavior us-
ing an information sharing annotation scheme.
By taking advantage of Integer Linear Pro-
gramming and a sociolinguistic framework,
we enforce structural relationships between
sentence-level annotations and sequences of
interaction. Then, we show that clustering
these sequences can effectively disentangle
the threads of conversation. This model is
highly accurate, performing near human accu-
racy, and performs analysis on-line, opening
the door to real-time analysis of the discourse
of conversation.
1 Introduction
When defining a unit of analysis for studying lan-
guage, one size does not fit all. Part-of-speech tag-
ging is performed on individual words in sequences,
while parse trees represent language at the sentence
level. Individual tasks can be performed at the lex-
ical, sentence, or document level, or even to arbi-
trary length spans of text (Wiebe et al, 2005), while
rhetorical patterns are annotated in a tree-like struc-
ture across sentences or paragraphs.
In dialogue, the most common unit of analysis is
the utterance, usually through dialogue acts. Here,
too, the issue of granularity and specificity of tags
has been a persistent issue, along with the inte-
gration of larger discourse structure. Both theory-
driven and empirical work has argued for a col-
lapsing of annotations into fewer categories, based
on either marking the dominant function of a given
turn (Popescu-Belis, 2008) or identifying a single
construct of interest and annotating only as nec-
essary to distinguish that construct. We take the
latter approach in this work, predicting conversa-
tion structure particularly as it relates to informa-
tion sharing and authority in dialogue. We use sys-
temic functional linguistics? Negotiation annotation
scheme (Mayfield and Rose?, 2011) to identify utter-
ances as either giving or receiving information. This
annotation scheme is of particular interest because in
addition to sentence-level annotation, well-defined
sequences of interaction are incorporated into the
annotation process. This sequential structure has
been shown to be useful in secondary analysis of
annotated data (Mayfield et al, 2012a), as well as
providing structure which improves the accuracy of
automated annotations.
This research introduces a model to predict infor-
mation sharing tags and Negotiation sequence struc-
ture jointly with thread disentanglement. We show
that performance can be improved using integer lin-
ear programming to enforce constraints on sequence
structure. Structuring and annotation of conversa-
tion is available quickly and with comparatively lit-
tle effort compared to manual annotation. More-
over, all of our results in this paper were obtained
using data a real-world, chat-based internet commu-
nity, with a mix of long-time expert and first-time
60
novice users, showing that the model is robust to the
challenges of messy data in natural environments.
The remainder of this paper is structured as fol-
lows. First, we review relevant work in annota-
tion at the levels of utterance, sequence, and thread,
and applications of each. We then introduce the
domain of our data and the framework we use for
annotation of conversation structure. In Section 4
we define a supervised, on-line machine learning
model which performs this annotation and structur-
ing across granularities. In Section 5, we evaluate
this model and show that it approaches or matches
human reliability on all tasks. We conclude with dis-
cussion of the utility of this conversation structuring
algorithm for new analyses of conversation.
2 Related Work
Research on multi-party conversation structure is
widely varied, due to the multifunctional nature of
language. These structures have been used in di-
verse fields such as computer-supported collabora-
tive work (O?Neill and Martin, 2003), dialogue sys-
tems (Bohus and Horvitz, 2011), and research on
meetings (Renals et al, 2012). Much work in an-
notation has been inspired by speech act theory and
dialogue acts (Traum, 1994; Shriberg et al, 2004),
which operate primarily on the granularity of indi-
vidual utterances. A challenge of tagging is the issue
of specificity of tags, as previous work has shown
that most utterances have multiple functions (Bunt,
2011). General tagsets have attempted to capture
multi-functionality through independent dimensions
which produce potentially millions of possible an-
notations, though in practice the number of varia-
tions remains in the hundreds (Jurafsky et al, 1998).
Situated work has jointly modelled speech act and
domain-specific topics (Laws et al, 2012).
Additional structure inspired by linguistics, such
as adjacency pairs (Schegloff, 2007) or dialogue
games (Carlson, 1983), has been used to build dis-
course relations between turns. This additional
structure has been shown to improve performance
of automated analysis (Poesio and Mikheev, 1998).
Identification of this fine-grained structure of an in-
teraction has been studied in prior work, with appli-
cations in agreement detection (Galley et al, 2004),
addressee detection (op den Akker and Traum,
2009), and real-world applications, such as cus-
tomer service conversations (Kim et al, 2010).
Higher-order structure has also been explored in dia-
logue, from complex graph-like relations (Wolf and
Gibson, 2005) to simpler segmentation-based ap-
proaches (Malioutov and Barzilay, 2006). Utterance
level-tagging can take into account nearby structure,
e.g. forward-looking and backward-looking func-
tions in DAMSL (Core and Allen, 1997), while dia-
logue management systems in intelligent agents of-
ten have a plan unfolding over a whole dialogue
(Ferguson and Allen, 1998).
In recent years, threading and maintaining of mul-
tiple ?floors? has grown in popularity (Elsner and
Charniak, 2010), especially in text-based media.
This level of analysis is designed with the goal of
separating out sub-conversations which are indepen-
dently coherent. There is a common ground emerg-
ing in the thread detection literature on best prac-
tices for automated prediction. Early work viewed
the problem as a time series analysis task (Bingham
et al, 2003). Treating thread detection as a cluster-
ing problem, with lines representing instances, was
given great attention in Shen et al (2006). Subse-
quent researchers have treated the thread detection
task as based in discourse coherence, and have pur-
sued topic modelling (Adams, 2008) or entity refer-
ence grids (Elsner and Charniak, 2011) to define that
concept of coherence.
Other work integrates local discourse structure
with the topic-based threads of discourse. Ai et al
(2007) utilizes information state, a dialogue man-
agement component which loosely parallels thread
structure, to improve dialogue act tagging. In the
context of Twitter conversations, Ritter et al (2010)
suggests using dialogue act tags as a middle layer to-
wards conversation reconstruction. Low-level struc-
ture between utterances has also been used as a
foundation for modelling larger-level sociological
phenomena between speakers in a dialogue, for in-
stance, identifying leadership (Strzalkowski et al,
2011) and rapport between providers and patients
in support groups (Ogura et al, 2008). These
works have all pointed to the utility of incorporat-
ing sentence-level annotations, low-level interaction
structure, and overarching themes into a unified sys-
tem. To our knowledge, however, this work is the
first to present a single system for simultaneous an-
61
Negotiation/Threads Seq User Text
K2 1 C [M], fast question, did your son have a biopsy?
K2 1 C or does that happen when he comes home
K1 2 V i have 3 dogs.
K1 2 V man?s best friend
f 2 S :-D
o 2 C and women
K2 3 J what kind of dogs????
K1 4 C [D], I keep seeing that you are typing and then it stops
K2 5 C how are you doing this week
K1 3 V the puppies are a maltese/yorkie mix and the full grown is a pomara-
nian/yorkie.
K1 1 M No, he did not have a biopsy.
K1 1 M The surgeon examined him and said that by feel, he did not think the
lump was cancerous, and he should just wait until he got home.
f 1 C that has to be very hard
o 7 M A question, however? [J], you would probably know.
K2 7 M He was told that they could not just do a needle biopsy, that he would
have to remove the whole lump in order to tell if it was malignant.
o 8 D Yes.
K1 8 D I was waiting for [M] to answer.
K1 7 J That sounds odd to me
Table 1: An example excerpt with Negotiation labels, sequences, and threads structure (columns) annotated.
notation and structuring at all three levels.
3 Data and Annotation
Our data comes from the Cancer Support Commu-
nity, which provides chatrooms, forums, and other
resources for support groups for cancer patients.
Each conversation took place in the context of a
weekly meeting, with several patient participants as
well as a professional therapist facilitating the dis-
cussion. In total, our annotated corpus consists of
45 conversations. This data was sampled from three
group sizes - 15 conversations from small groups (2
patients, in addition to the trained facilitator), 15
from medium-sized groups (3-4 patients), and 15
from large groups (5 or more patients).
3.1 Annotation
Our data is annotated at the three levels of granu-
larity described previously in this paper: sentences,
sequences, and threads. In this section we define
those annotations in greater detail. Sentence-level
and sequence-level annotations were performed us-
ing the Negotiation framework from systemic func-
tional linguistics (Martin and Rose, 2003). Once
sequences were identified, those sequences were
grouped together into threads based on shared topic.
We annotate our data using an adaptation of the
Negotiation framework. This framework has been
proven reliable and reproducible in previous work
(Mayfield and Rose?, 2011). By assigning aggregate
scores over a conversation, the framework also gives
us a notion of Authoritativeness. This metric, de-
fined later in Section 5, allows us to test whether
automated codes faithfully reproduce human judg-
ments of information sharing behavior at a per-user
level. This metric has proven to be a statistically
significant indicator of outcome variables in direc-
tion giving (Mayfield et al, 2011) and collaborative
learning domains (Howley et al, 2011).
In particular, Negotiation labels define whether
each speaker is a source or recipient of information.
Our annotation scheme has four turn-level codes
and a rigidly defined information sharing structure,
rooted in sociolinguistic observation. We describe
62
each in detail below.
Sentences containing new information are marked
as K1, as the speaker is the ?primary knower,? the
source of information. These sentences can be gen-
eral facts and world knowledge, but can also con-
tain opinions, retelling of narrative, or other contex-
tualized information, so long as the writer acts as
the source of that information. Sentences requesting
information, on the other hand, are marked K2, or
?secondary knower,? when the writer is signalling
that they want information from other participants
in the chat. This can be direct question asking, but
can also include requests for elaboration or indirect
illocutionary acts (e.g. ?I?d like to hear more.?).
In addition to these primary moves, we also use a
social feedback code, f, for sentences consisting of
affective feedback or sentiment, but which do not
contain new information. These moves can include
emoticons, fixed expressions such as ?good luck,? or
purely social banter. All other moves, such as typo
correction or floor grabbing, are labelled o.
This annotation scheme is highly flexible and
adaptive to new domains, and is not specific to med-
ical topics or chatroom-based media. It also gives us
a well-defined structure of an interaction: each se-
quence consists of exactly one primary knower (K1)
move, which can consist of any number of primary
knower sentences from a single speaker. If a K2
move occurs in the sequence, it occurs before any
K1 moves. Feedback moves (f) may come at any
time so long as the speaker is responding to another
speaker in the same sequence. Sentences labeled
o are idiosyncratic and may appear anywhere in a
sequence. In section 4.3, we represent these con-
straints formally.
In addition to grouping sentences together into se-
quences structurally, we also group those sequences
into threads. These threads are based on annotator
judgement, but generally map to the idea that a sin-
gle thread should be on a single theme, e.g. ?han-
dling visiting relatives at holidays.? These threads
are both intrinsically interesting for identifying the
topics of a conversation, as well as being a useful
preprocessing step for any additional, topic-based
annotation that may be desired for later analysis.
We iteratively developed a coding manual for
these layers of annotation; to test reliability at each
iteration of instructions, two annotators each inde-
Figure 1: Structured output at each phase of the two-
pass machine learning model. In pass one, utterances are
grouped into sequences with organizational structure; the
second pass groups sequences based on shared themes.
pendently annotated one full conversation. Inter-
annotator reliability is high for sentence-level an-
notation (? = 0.75). Following Elsner and Char-
niak (2010), we use micro-averaged f-score to eval-
uate inter-rater agreement on higher-level structure.
We find that inter-annotator agreement is high for
both sequence-level structure (f = 0.82) and thread-
level structure (f = 0.80). A detailed description
of the annotation process is available in Mayfield et
al. (2012b). After establishing reliability, our entire
corpus was annotated by one human coder.
4 Conversation Structure Prediction
In previous work, the Negotiation framework has
been automatically coded with high accuracy (May-
field and Rose?, 2011). However, that work restricted
the domain to a task-based, two-person dialogue,
and structure was viewed as a segmentation, rather
than threading, formulation. At each turn, a se-
quence could continue or a new sequence could be-
gin.
Here, we extend this automated coding to larger
groups speaking in unstructured, social chat, and we
extend the structured element of this coding scheme
to structure by sequence and thread. To our knowl-
edge, this is also the first attempt to utilize functional
sequences of interaction as a preprocessing step for
thread disentanglement in chat. We now present a
comprehensive machine learning model which an-
notates a conversation by utterance, groups utter-
ances topics by local structure into sequences, and
assigns sequences to threads.
63
4.1 On-Line Instance Creation
This is a two-pass algorithm. The first pass la-
bels sentences and detects sequences, and the second
pass groups these sequences into threads. We follow
Shen et al (2006) in treating the sequence detection
problem as a single-pass clustering algorithm. Their
model is equivalent to the Previous Cluster model
described below, albeit with more complex features.
In that work a threshold was defined in order for a
new message to be added to an existing cluster. If
that threshold is not passed, a new cluster is formed.
Modelling the probability that a new cluster should
be formed is similar to a context-sensitive threshold,
and because we do not impose a hard threshold, we
can pass the set of probabilities for cluster assign-
ments to a structured prediction system.
4.2 Model Definitions
At its core, our model relies on three probabilistic
classifiers. One of these models is a classification
model, and the other two treat sequence and thread
structure as clusters. All models use the LightSIDE
(Mayfield and Rose?, 2010) with the LibLinear algo-
rithm (Fan et al, 2008) for machine learning..
Negotiation Classifier (Neg)
The Negotiation model takes a single sentence as
input. The output of this model is a distribution over
the four possible sentence-level labels described in
section 3.1. The set of features for this model con-
sists of unigrams, bigrams, and part-of-speech bi-
grams. Part-of-speech tagging was performed using
the Stanford tagger (Toutanova et al, 2003) within
LightSIDE.
Cluster Classifiers (PC, NC)
We use two models of cluster assignment prob-
ability. The Previous Cluster (PC) classifier
takes as input a previous set of sentences C =
{c1, c2, . . . , cn} and set of new sentences N =
{N1, N2, . . . , Nm}. To evaluate whether c? should
be added to this cluster, we train a binary proba-
bilistic classifier that predicts the probability that the
sentences inN belong to the same cluster as the sen-
tences already inC. In the first pass, each inputN to
the PC classifier is a set containing a single sentence,
and each C is the set of sentences in a previously-
identified sequence. In the second pass, each N is a
sequence as predicted by the first pass.
The PC model uses two features. The first is a
time-based feature, measuring the amount of time
that has elapsed between the last sentence in C and
the first sentence in N . The time feature is repre-
sented differently between sequence prediction and
thread prediction. Elsner and Charniak (2010) rec-
ommends using bucketed nominal values based on
the log time, to group together very recent and very
distant posts. We follow this for sequence predic-
tion. Due to the more complex structure of the se-
quence grouping task in the second pass, we use a
raw numeric time feature. The second feature is a
coherence metric, the cosine similarity between the
centroid of C and the centroid of N . We define the
centroid based on TF-IDF weighted unigram vec-
tors.
We impose a threshold after which previous clus-
ters are no longer considered as options for the
PC classifier. Because sequences are shorter than
threads, we set these thresholds separately, at 90 sec-
onds for sequences and 120 seconds for threads. Ap-
proximately 1% of correct assignments are impossi-
ble due to these thresholds.
The New Cluster (NC) classifier takes as input
a set of sentences n = {n1, n2, . . . , nm}, and pre-
dicts the probability that a given sentence is initiat-
ing a new sequence (or, in the second pass, whether
a given sequence is initiating a new thread). This
model contains only unigram features.
At each sentence s we consider the set of possible
previous cluster assignments C = {c1, c2, . . . , cn},
and define psc(s, c) to be the probability that s
will be assigned to cluster c. We define pnc(s) =
?sNC(s). The addition of a weight parameter to
the output of the NC classifier allows us to tune the
likelihood of transitioning to a new cluster. This pre-
diction structure is illustrated in Figure 2. In the
first pass, these cluster probabilities are used in con-
junction with the output of the Negotiation classifier
to form a structured output; in the second pass, the
maximum cluster probability is chosen.
4.3 Constraining Sequence Structure with ILP
In past work the Negotiation framework has bene-
fited from enforced constraints of linguistically sup-
ported rules on sequence structure (Mayfield and
64
Figure 2: The output of the cluster classifier in either pass
is a set of probabilities corresponding to possible clus-
ter assignments, including that of creating a new cluster.
In the second pass, the input is a set of sentences (a se-
quence) rather than a single sentence, and output assign-
ments are to threads rather than sequences.
Rose?, 2011). Constraints on the structure of anno-
tations are easily defined using Integer Linear Pro-
gramming. Recent work has used boolean logic
(Chang et al, 2008) to allow intuitive rules about
a domain to be enforced at classification time. ILP
inference was performed using Learning-Based Java
(Rizzolo and Roth, 2010).
First, we define the classification task. Opti-
mization is performed given the set of probabilities
N (s) as the distribution output of the Neg classifier
given sentence s as input, and the set of probabilities
C(s) = pnc(s) ? psc(s, c), ?c ? C. Instance classi-
fication requires maximizing the objective function:
arg max
n?N (s),c?C(s)
n+ c
We impose constraints on sequence prediction. If
the most likely output from this function assigns
a label that is incompatible with the assigned se-
quence, either the label is changed or a new se-
quence is assigned so that constraints are met. For
each constraint, we give the intuition from sec-
tion 3.1, followed by our formulation of that con-
straint. us is shorthand for the user who wrote
sentence s; ns is shorthand for a proposed Ne-
gotiation label of sentence s; while cs is a pro-
posed sequence assignment for s, c? is shorthand
for assignment to a new sequence, and Sc =
{(nc,1, uc,1), (nc,2, uc,2), . . . , (nc,k, uc,k)} is the set
of Negotiation labels n and users u associated with
sentences (sc,1 . . . sc,k) already in sequence c.
1. K2 moves, if any, occur before K1 moves.
((cs = c) ? (ns = K2))
? (@i ? Sc s.t. nc,i = K1)
2. f moves may occur at any time but must be re-
sponding to a different speaker in the same se-
quence.
((cs = c) ? (ns = f))
? (?i ? Sc s.t. uc,i 6= us)
3. Functionally, therefore, f moves may not initi-
ate a sequence).
(cs = c?) ? (ns 6= f)
4. Speakers do not respond to their own requests
for information (the speakers of K2 and K1
moves in the same sequence must be different).
((cs = c) ? (ns = K1))
? (?i ? Sc, ((nc,i = K2) ? (uc,i 6= us)))
5. Each sequence consists of at most one continu-
ous series of K1 moves from the same speaker.
(cs = c) ? ((?i ? Sc s.t. (nc,i = K1))
? ( (uc,i = us) ? (?j > i,
(uc,j = us) ? (nc,i = K1)) )
Human annotators treated these rules as hard con-
straints, as the classifier does. In circumstances
where these rules would be broken (for instance, due
to barge-in or trailing off), a new sequence begins.
5 Evaluation
5.1 Methods
To evaluate the performance of this model, we wish
to know how it replicates human annotation at each
granularity. For Negotiation labels, agreement is
measured by terms of absolute accuracy and kappa
agreement above chance. We also include a measure
of aggregate information sharing behavior per user.
This score, which we term Information Authorita-
tiveness (Auth), is defined per user as the percentage
65
of their contentful sentences (K1 or K2) which were
giving information (K1). To measure performance
on this measure, we measure the r2 coefficient be-
tween user authoritativeness scores calculated from
the predicted labels compared to actual labels. This
is equivalent to measuring the variance explained by
our model, where each data point represents a single
user?s predicted and actual authoritativeness scores
over the course of a whole conversation (n = 215).
Sequence and thread agreement is evaluated by
micro-averaged f-score (MAF), defined in prior
work for a gold sequence i with size ni, and a pro-
posed sequence j with size nj , based on precision
and recall metrics:
P = nijnj R =
nij
ni F (i, j) =
2?P?R
P+R
MAF across an entire conversation is then a
weighted sum of f-scores across all sequences1:
MAF =
?
i
ni
n maxj F (i, j)
We implemented multiple baselines to test
whether our methods improve upon simpler ap-
proaches. For sequence and thread prediction, we
implement the following baselines. Speaker Shift
predicts a new thread every time a new writer adds a
line to the chat. Turn Windows predicts a new se-
quence or thread after every n turns. Pause Length
predicts a new sequence or thread every time that a
gap of n seconds has occurred between lines of chat.
For both of the previous two baselines, we vary the
parameter n to optimize performance and provide
a challenging baseline. None of these models use
any features or constraints, and are based on heuris-
tics. To compare to our model, we present both an
Unconstrained model, which uses machine learn-
ing and does not impose sequence constraints from
Section 4.3, as well as our full Constrained model.
Evaluation is performed using 15-fold cross-
validation. In each fold, one small, one medium,
and one large conversation are held out as a test set,
and classifiers are trained on the remaining 42 con-
versations. Significance is evaluated using a paired
student?s t-test per conversation (n = 45).
Sentence-Level (Human ? = 0.75)
Model Accuracy ? Auth r2
Unconstrained .7736 .5870 .7498
Constrained .7777 .5961 .7355
Sequence-Level (Human MAF = 0.82)
Model Precision Recall MAF
Speaker Shift .7178 .5140 .5991
Turn Windows .7207 .6233 .6685
Pause Length .8479 .6582 .7411
Unconstrained .7909 .7068 .7465
Constrained .8557 .7116 .7770
Thread-Level (Human MAF = 0.80)
Model Precision Recall MAF
Turn Windows .5994 .7173 .6531
Pause Length .6145 .6316 .6229
Unconstrained .7132 .5781 .6386
Constrained .6805 .6024 .6391
Table 2: Tuned optimal annotation performances of base-
line heuristics compared to our machine learning model.
5.2 Results
Results of experimentation show that all models
are highly accurate in their respective tasks. With
sentence-level annotation approaching 0.6 ?, the
output of the model is reliable enough to allow
automatically annotated data to be included reli-
ably alongside human annotations. Performance for
sequence-based modelling is even stronger, with no
statistically significant difference in f-score between
the machine learning model and human agreement.
Table 2 reports our best results after tuning to
maximize performance of baseline models, our orig-
inal machine learning model, and the model with
ILP constraints enforced between Negotiation labels
and sequence. In all three cases, we see machine
performance approaching, but not matching, human
agreement. Incorporating ILP constraints improves
per-sentence Negotiation label classification by a
small but significant amount (p < .001).
Clustering performance is highly robust, as
demonstrated in Figure 3, which shows the effect of
changing window sizes and pause lengths and values
of ?s for machine learned models. Our thread disen-
tanglement performance matches our baselines, and
1This metric extends identically to a gold thread i and pro-
posed thread j.
66
Figure 3: Parameter sensitivity on sequence-level (top)
and thread-level (bottom) annotation models.
is in line with heuristic-based assignments from El-
sner and Charniak (2010). In sequence clustering,
we observe improvement across all metrics. The
Constrained model achieves a higher f-score than all
other models (p < 0.0001). We determine through
a two-tailed confidence interval that sequence clus-
tering performance is statistically indistinguishable
from human annotation (p < 0.05).
Error analysis suggests that the constraints are too
punishing on the most constrained labels, K2 and f.
The differences in performance between constrained
and unconstrained models is largely due to higher
recall for both K1 and o move prediction, while
recall for K2 and f moves lowered slightly. One
possibility for future work may include compensat-
ing for this by artificially inflating the likelihood of
highly-constrained Negotiation labels. Additionally,
we see that the most common mistakes involve dis-
tinguishing between K1 and f moves. While many
f moves are obviously non-content-bearing (?Wow,
what fun!?), others, especially those based in humor,
may look grammatical and contentful (?We?ve got to
stop meeting this way.?). Better detection of humor
and a more well-defined definition of what informa-
tion is being shared will improve this aspect of the
model. Overall, these errors do not limit the efficacy
of the model for enabling future analysis.
6 Conclusion and Future Work
This work has presented a unified machine learn-
ing model for annotating information sharing acts
on a sentence-by-sentence granularity; grouping se-
quences of sentences based on functional structure;
and then grouping those sequences into topic-based
threads. The model performs at a high accuracy,
approaching human agreement at the sentence and
thread level. Thread-level accuracy matched but did
not exceed simpler baselines, suggesting that this
model could benefit from a more elaborate repre-
sentation of coherence and topic. At the level of se-
quences, the model performs statistically the same
as human annotation.
The automatic annotation and structuring of di-
alogue that this model performs is a vital prepro-
cessing task to organize and structure conversational
data in numerous domains. Our model allows re-
searchers to abstract away from vocabulary-based
approaches, instead working with interaction-level
units of analysis. This is especially important in
the context of interdisciplinary research, where other
representations may be overly specialized towards
one task, and vocabulary may differ for spurious rea-
sons across populations and cultures.
Our evaluation was performed on a noisy, real-
world chatroom corpus, and still performed very ac-
curately. Coherent interfacing between granularities
of analysis is always a challenge. Segmentation,
tokenization, and overlapping or inconsistent struc-
tured output are nontrivial problems. By incorpo-
rating sentence-level annotation, discourse-level se-
quence structure, and topical thread disentanglement
into a single model, we have shown one way to re-
duce or eliminate this interfacing burden and allow
greater structural awareness in real-world systems.
Future work will improve this model?s accuracy fur-
ther, test its generality in new domains such as spo-
ken multi-party interactions, and evaluate its useful-
ness in imposing structure for secondary analysis.
67
Acknowledgments
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485, Of-
fice of Naval Research grant N000141110221, and
in part by the Pittsburgh Science of Learning Center,
which is funded by the National Science Foundation
grant SBE-0836012.
References
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007. Using information state to improve dia-
logue move identification in a spoken dialogue system.
In Proceedings of Interspeech.
Ella Bingham, Ata Kaban, and Mark Girolami. 2003.
Topic identification in dynamical text by complexity
pursuit. In Neural Processing Letters.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn tak-
ing in situated dialog. In Procedings of SIGDIAL.
Harry Bunt. 2011. Multifunctionality in dialogue. In
Computer Speech and Language.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. Massachussetts Institute of Tech-
nology.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of the Association for the Ad-
vancement of Artificial Intelligence.
Mark G Core and James F Allen. 1997. Coding dialogs
with the damsl annotation scheme. In AAAI Fall Sym-
posium on Communicative Action in Humans and Ma-
chines.
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Micha Elsner and Eugene Charniak. 2011. Disentan-
gling chat with local coherence models. In Proceed-
ings of the Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification.
George Ferguson and James Allen. 1998. Trips: An in-
tegrated intelligent problem-solving assistant. In Pro-
ceedings of AAAI.
Michael Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL.
Iris Howley, Elijah Mayfield, and Carolyn Penstein Rose?.
2011. Missing something? authority in collaborative
learning. In Proceedings of Computer Supported Col-
laborative Learning.
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Switchboard discourse language
modelling final report. Technical report.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
M Barton Laws, Mary Catherine Beach, Yoojin Lee,
William H. Rogers, Somnath Saha, P Todd Korthuis,
Victoria Sharp, and Ira B Wilson. 2012. Provider-
patient adherence dialogue in hiv care: Results of a
multisite study. AIDS Behavior.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL/COLING.
J.R. Martin and David Rose. 2003. Working with Dis-
course: Meaning Beyond the Clause. Continuum.
Elijah Mayfield and Carolyn Penstein Rose?. 2010. An
interactive tool for supporting error analysis for text
mining. In NAACL Demonstration Session.
Elijah Mayfield and Carolyn Penstein Rose?. 2011. Rec-
ognizing authority in dialogue with an integer linear
programming constrained model. In Proceedings of
Association for Computational Linguistics.
Elijah Mayfield, Michael Garbus, David Adamson, and
Carolyn Penstein Rose?. 2011. Data-driven interac-
tion patterns: Authority and information sharing in di-
alogue. In Proceedings of AAAI Fall Symposium on
Building Common Ground with Intelligent Agents.
Elijah Mayfield, David Adamson, Alexander I Rudnicky,
and Carolyn Penstein Rose?. 2012a. Computational
representations of discourse practices across popula-
tions in task-based dialogue. In Proceedings of the
International Conference on Intercultural Collabora-
tion.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and Car-
olyn Penstein Rose?. 2012b. Discovering habits of ef-
fective online support group chatrooms. In ACM Con-
ference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jacki O?Neill and David Martin. 2003. Text chat in ac-
tion. In Proceedings of the International Conference
on Supporting Group Work.
68
Rieks op den Akker and David Traum. 2009. A compari-
son of addressee detection methods for multiparty con-
versations. In Workshop on the Semantics and Prag-
matics of Dialogue.
Massimo Poesio and Andrei Mikheev. 1998. The pre-
dictive power of game structure in dialogue act recog-
nition: Experimental results using maximum entropy
estimation. In Proceedings of the International Con-
ference on Spoken Language Processing.
Andrei Popescu-Belis. 2008. Dimensionality of dialogue
act tagsets: An empirical analysis of large corpora. In
Language Resources and Evaluation.
Steve Renals, Herve? Bourlard, Jean Carletta, and Andrei
Popescu-Belis. 2012. Multimodal Signal Processing:
Human Interactions in Meetings.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of NAACL.
Nicholas Rizzolo and Dan Roth. 2010. Learning based
java for rapid development of nlp systems. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
E. Schegloff. 2007. Sequence organization in interac-
tion: A primer in conversation analysis. Cambridge
University Press.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In Proceedings of SIGIR.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The icsi meeting
recorder dialog act (mrda) corpus. In Proceedings of
SIGDIAL.
Tomek Strzalkowski, George Aaron Broadwell, Jennifer
Stromer-Galley, Samira Shaikh, Ting Liu, and Sarah
Taylor. 2011. Modeling socio-cultural phenomena in
online multi-party discourse. In AAAI Workshop on
Analyzing Microtext.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
David Traum. 1994. A computational theory of ground-
ing in natural language conversation. Ph.D. thesis.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics.
69
