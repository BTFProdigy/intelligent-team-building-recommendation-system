Construction of an Objective Hierarchy of Abstract Concepts           
via Directional Similarity  
Kyoko Kanzaki  Eiko Yamamoto Hitoshi Isahara 
Computational Linguistics Group, 
National Institute of Information and Communications 
Technology 
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan,  
{kanzaki, eiko, isahara}@nict.go.jp 
Qing Ma 
Faculty of Science  
and Technology 
Ryukoku University 
Seta, Otsu,520-2194, Japan 
qma@math.ryukoku.ac.jp
Abstract 
The method of organization of word mean-
ings is a crucial issue with lexical databases. 
Our purpose in this research is to extract word 
hierarchies from corpora automatically. Our 
initial task to this end is to determine adjec-
tive hyperonyms. In order to find adjective 
hyperonyms, we utilize abstract nouns. We 
constructed linguistic data by extracting se-
mantic relations between abstract nouns and 
adjectives from corpus data and classifying 
abstract nouns based on adjective similarity 
using a self-organizing semantic map, which 
is a neural network model (Kohonen 1995). 
In this paper we describe how to hierarchi-
cally organize abstract nouns (adjective hy-
peronyms) in a semantic map mainly using 
CSM. We compare three hierarchical organi-
zations of abstract nouns, according to CSM, 
frequency (Tf.CSM) and an alternative simi-
larity measure based on coefficient overlap, to 
estimate hyperonym relations between words. 
1. Introduction 
A lexical database is necessary for computers, 
and even humans, to fully understand a word's 
meaning because the lexicon is the origin of lan-
guage understanding and generation. Progress is 
being made in lexical database research, notably 
with hierarchical semantic lexical databases such 
as WordNet, which is used for NLP research 
worldwide.  
When compiling lexical databases, it is impor-
tant to consider what rules or phenomena should 
be described as lexical meanings and how these 
lexical meanings should be formalized and stored 
electronically. This is a common topic of discus-
sion in computational linguistics, especially in 
the domain of computational lexical semantics. 
The method of organization of word meanings 
is also a crucial issue with lexical databases. In 
current lexical databases and/or thesauri, abstract 
nouns indicating concepts are identified manually 
and words are classified in a top-down manner 
based on human intuition. This is a good way to 
make a lexical database for users with a specific 
purpose. However, word hierarchies based on 
human intuition tend to vary greatly depending 
on the lexicographer, and there is often dis-
agreement as to the make-up of the hierarchy. If 
we could find an objective method to organize 
word meanings based on real data, we would 
avoid this variability. 
Our purpose in this research is to extract word 
hierarchies from corpora automatically. Our ini-
tial task to this end is to determine adjective hy-
peronyms. In order to find adjective hyperonyms, 
we utilize abstract nouns. Past linguistic research 
has focused on classifying the semantic relation-
ship between abstract nouns and adjectives 
(Nemoto 1969, Takahashi 1975).  
We constructed linguistic data by extracting 
semantic relations between abstract nouns and 
adjectives from corpus data and classifying ab-
stract nouns based on adjective similarity using a 
self-organizing semantic map (SOM), which is a 
neural network model (Kohonen 1995). The rela-
tive proximity of words in the semantic map in-
dicates their relative similarity.  
In previous research, word meanings have 
been statistically modeled based on syntactic in-
formation derived from a corpus. Hindle (1990) 
used noun-verb syntactic relations, and Hatzivas-
siloglou and McKeown (1993) used coordinated 
adjective-adjective modifier pairs. These meth-
ods are useful for the organization of words deep 
within a hierarchy, but do not seem to provide a 
solution for the top levels of the hierarchy.  
To find an objective hierarchical word struc-
ture, we utilize the complementary similarity 
measure (CSM), which estimates a one-to-many 
relation, such as superordinate?subordinate rela-
tions (Hagita and Sawaki 1995, Yamamoto and 
Umemura 2002).  
In this paper we propose an automated method 
for constructing adjective hierarchies by connect-
ing strongly related abstract nouns in a top-down 
fashion within a semantic map, mainly using 
CSM. We compare three hierarchical organiza-
tions of abstract nouns, according to CSM, fre-
quency (Tf.CSM) and an alternative similarity 
measure based on coefficient overlap, to estimate 
hyperonym relations between words. 
2. Linguistic clues to extract adjective hy-
peronyms from corpora 
In order to automatically extract adjective hy-
peronyms we use syntactic and semantic relations 
between words.  
There is a good deal of linguistic research fo-
cused on the syntactic and semantic functions of 
abstract nouns, including Nemoto (1969), Taka-
hashi (1975), and Schmid (2000). Takahashi 
(1975) illustrated the sentential function of ab-
stract nouns with the following examples. 
a.  Yagi  wa  seishitsu  ga  otonashii. 
(goat) topic (nature) subject (gentle) 
        The nature of goats is gentle 
b.   Zou    wa   hana   ga     nagai. 
    (elephant) topic  (a nose) subject  (long) 
         The nose of an elephant is long 
He examined the differences in semantic func-
tion between ?seishitsu (nature)? in (a) and ?hana 
(nose)? in (b), and explained that ?seishitsu (na-
ture)? in (a) indicates an aspect of something, i.e., 
the goat, and ?hana (nose)? in (b) indicates part 
of something, i.e., the elephant. He recognized 
abstract nouns in (a) as a hyperonym of the at-
tribute that the predicative adjectives express. 
Nemoto (1969) identified expressions such as 
?iro ga akai (the color is red)? and ?hayasa ga 
hayai (the speed is fast)? as a kind of meaning 
repetition, or tautology.  
In this paper we define such abstract nouns 
that co-occur with adjectives as adjective hy-
peronyms. We semi-automatically extracted from 
corpora 365 abstract nouns used as this kind of 
head noun, according to the procedures described 
in Kanzaki et al (2000). We collected abstract 
nouns from two year's worth of articles from the 
Mainichi Shinbun newspaper, and extracted ad-
jectives co-occurring with abstract nouns in the 
manner of (a) above from 100 novels, 100 essays 
and 42 year's worth of newspaper articles, includ-
ing 11 year's worth of Mainichi Shinbun articles, 
10 year's worth of Nihon Keizai Shinbun (Japa-
nese economic newspaper) articles, 7 year's wor-
th of Sangyoukinyuuryuutsu Shinbun (an eco-
nomic newspaper) articles, and 14 year's worth of 
Yomiuri Shinbun articles. The total number of 
abstract noun types is 365, the number of adjec-
tive types is 10,525, and the total number of ad-
jective tokens is 35,173. The maximum number 
of co-occurring adjectives for a given abstract 
noun is 1,594. 
3. On the Self-Organizing Semantic Map  
3.1  Input data 
Abstract nouns are located in the semantic map 
based on the similarity of co-occurring adjectives 
after iteratively learning over input data. 
In this research, we focus on abstract nouns 
co-occurring with adjectives. In the semantic 
map, there are 365 abstract nouns co-occurring 
with adjectives. The similarities between the 365 
abstract nouns are determined according to the 
number of common co-occurring adjectives. We 
made a list such as the following. 
OMOI (feeling): ureshii (glad), kanashii (sad), 
shiawasena (happy), ? 
KIMOCHI (though): ureshii (glad), tanoshii (pleased), 
hokorashii (proud), ? 
KANTEN (viewpoint): igakutekina (medical), 
rekishitekina (historical), ... 
When two (or more) sets of adjectives with 
completely different characteristics co-occur with 
an abstract noun and the meanings of the abstract 
noun can be distinguished correspondingly, we 
treat them as two different abstract nouns. For 
example, the Japanese abstract noun ?men? is 
treated as two different abstract nouns with 
?men1? meaning ?one side (of the characteristics 
of someone or something)? and ?men2? meaning 
?surface?. The former co-occurs with ?gentle?, 
?kind? and so on. The latter co-occurs with 
?rough?, ?smooth? and so on. 
3.2  The Self-Organizing Semantic Map 
Ma (2000) classified co-occurring words using 
a self-organizing semantic map (SOM). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
We made a semantic map of the above-
mentioned 365 abstract nouns using SOM, based 
on the cosine measure. The distribution of the 
words in the map gives us a sense of the semantic 
distribution of the words. However, we could not 
precisely identify the relations between words in 
the map (Fig 1). In Fig. 1 lines on the maps indi-
cate close relations between word pairs. In the 
cosine-based semantic map, there is no clear cor-
respondence between word similarities and the 
distribution of abstract nouns in the map.    
To solve this problem we introduced the 
complementary similarity measure (CSM). This 
similarity measure estimates one-to-many 
relations, such as superordinate?subordinate 
relations (Hagita and Sawaki 1995, Yamamoto 
and Umemura 2002). We can find the 
hierarchical distribution of words in the semantic 
map according to the value of CSM (Fig 2). In 
the CSM-based SOM, lines are concentrated at 
the bottom right hand corner, that is, most ab-
stract nouns are located at the bottom right-hand 
corner.  
Next, we find hierarchical relations between 
whole abstract nouns, not between word pairs, on 
the map automatically. 
4. How to construct hierarchies of nominal 
adjective hyperonyms in the Semantic 
Map 
4.1 Similarity measures, CSM and Yates? 
correction 
A feature of CSM is its ability to estimate hi-
erarchical relations between words. This similar-
ity measure was developed for the recognition of 
degraded machine-printed text (Hagita and Sa-
waki, 1995). Yates? correction is often used in 
order to increase the accuracy of approximation. 
Hierarchical relations can be extracted accurately 
when the CSM value is high. Yates? correction 
can extract different relations from high CSM 
values. When the CSM value is low, the result is 
not reliable, in which case we use Yates? correc-
tion. 
According to Yamamoto and Umemura (2002), 
who adopted CSM to classify words, CSM is cal-
culated as follows. 
))(( dbca
bcadCSM
++
?
=  
Yates? correction is calculated as follows. 
))()()((
)2/|(| 2
dbcadcba
nbcadnYates
++++
??
=  
Here n is the sum of the number of co-
occurring adjectives; a indicates the number of 
times the two labels appear together; b indicates 
the number of times ?label 1? occurs but ?label 
2? does not; c is the number of times ?label 2? 
occurs but ?label 1? does not; and d is the num-
ber of times neither label occurs. In our research, 
each ?label? is an abstract noun, a indicates the 
number of adjectives co-occurring with both ab-
stract nouns, b and c indicate the number of ad-
jectives co-occurring with either abstract noun 
Figure 1. The Cosine-based SOM of word similarity Figure 2. The CSM-based SOM of word similarity
(?label 1? and ?label 2?, respectively), and d in-
dicates the number of adjectives co-occurring 
with neither abstract noun. We calculated hierar-
chical relations between word pairs using these 
similarity measures. 
4.2 Construction of a hierarchy of abstract 
nouns using CSM and Yates' correc-
tion 
The hierarchy construction process is as fol-
lows: 
1) Based on the results of CSM, ?koto (mat-
ter)? is the hyperonym of all abstract nouns. 
First, we connect super/sub-ordinate words 
with the highest CSM value while keeping the 
super-subordinate relation.  
2) When the normalized value of CSM is 
lower, the number of extracted word pairs be-
comes increasing overwhelmingly, and the reli-
ability of CSM diminishes. Word pairs with a 
normalized CSM value of less than 0.4 are lo-
cated far from the common hyperonym ?koto 
(matter)? on the semantic map. If we construct a 
hierarchy using CSM values only, a long hierar-
chy containing irrelevant words emerges. In this 
case, the word pairs calculated by Yates' correc-
tion are more accurate than those from CSM. We 
combine words using Yates? correction, when the 
value of CSM is less than 0.4. When we connect 
word pairs with a high Yates? value, we find a 
hyperonym of the super-ordinate noun of the pair 
and connect the pair to the hyperonym. If a word 
pair appears only in the Yates' correction data, 
that is, we cannot connect the pair with high 
Yates? value to the hyperonym with high CSM 
value, they are combined with ?koto (matter)?. 
3) Finally, if a short hierarchy is contained in a 
longer hierarchy, it is merged with the longer 
hierarchy and we insert ?koto (matter)? at the 
root of all hierarchies. 
4.3  Results 
The number of groups obtained was 161. At its 
deepest, the hierarchy was 15 words deep, and at 
its shallowest, it was 4 words deep. The 
following is a breakdown of the number of 
groups at different depths in the hierarchy.  
The greatest concentration of groups is at 
depth 7. There are 140 groups from depth 5 to 
depth 10, which is 87% of all groups. 
 
 
 
 
 
 
 
The word that has the strongest relation with 
?koto (matter)? is ?men1 (side1)?. The number of 
groups in which ?koto (matter)? and ?men1 
(side1)? are hyperonyms is 96 (59.6%). The larg-
est number of groups after that is a group in 
which ?koto (matter)?, ?men1 (side1)? and 
?imeeji (image)? are hyperonyms. The number of 
groups in this case is 59 groups, or 36.6% of the 
total. With respect to the value of CSM, the co-
occurring adjectives are similar to ?men1 (side1)? 
and ?imeeji (image)?.  
Other words that have a direct relation with 
?koto (matter)? are ?joutai (state)? and ?toki 
(when)?. They have the most number of groups 
after ?men1 (side1)? among all the children of 
?koto (matter)?. The number of groups subsumed 
by ?joutai (state)? group and ?toki (when)? are 21 
and 19, respectively. Other direct hyponyms of 
?koto (matter)? are: 
ki (feeling): 6 groups  
ippou (while or grow ?er and er): 3 groups  
me2 (eyes): 3 groups  
katachi1 (in the form of): 3 groups  
iikata (how to say): 2 groups  
yarikata (how to): 2 groups 
There is little hierarchical structure to these 
groups, as they co-occur with few adjectives. 
4.4 The Hierarchies of abstract concepts in 
the semantic map 
In the following semantic maps, where abstract 
nouns are distributed using SOM and CSM (see 
Section 3), hierarchies of abstract nouns are 
drawn with lines. The bottom right hand corner is 
?koto (matter)?, a starting point for the distribu-
tion of abstract nouns.  
Five main types of hierarchies are found from 
patterns of lines on the map, as follows: 
The first figure, Fig.3, is hierarchies of ?kanji 
(feeling), kimochi (feeling) ?? on the semantic 
map. The location of hierarchies of ?yousu (as-
pect), omomochi (look), kaotsuki (on one?s face), 
?? is similar to this type of the location. Hierar-
chies of ?sokumen (one side), imi (meaning), 
kanten (viewpoint),  kenchi (standpoint) ?? on 
Depth 4 5 6 7 8 9 
Groups 3 16 27 32 23 23 
Depth 10 11 12 13 14 15 
Groups 19 7 3 4 3 1 
Table 1: The depth of the hierarchy by CSM
  
 
 
 
 
 
the map are shown in Fig. 4. The lines of the hi-
erarchies go up from the bottom right hand cor-
ner to the upper left hand corner and then turn 
towards the upper right hand corner. The loca-
tion of hierarchies of ?nouryoku (ability), sainou 
(talent) ?? is similar to this one. 
The hyperonym of ?teido (degree)? is ?joutai 
(state)?. In Fig.5 these abstract nouns are located 
at the bottom of the map. The location of hierar-
chies of ?kurai (rather than)? and ?hou (compara-
tively)? are similar to this one. The hierarchies of 
?joutai (state), joukyou (situation), yousou (as-
pect), jousei (the state of affairs)? are shown in 
Fig.6. The lines are found at a higher location 
than the line of ?teido(degree)?. The lines of the 
hierarchies of ?joutai (state), ori (when), sakari 
(in the hight of), sanaka (while)? are similar to 
these lines. 
The lines of the hierarchies of ?seikaku (char-
acter)?, ?gaikan (appearance)?and ?utsukushisa 
(beauty)? are similar to each other. We show the 
hierarchies of ?seikaku (character)? in Fig.7. The-
se lines in Fig.7 are located from the right end to 
the upper left hand corner. From the following, 
we can find five main types of hierarchies. 
From the starting point ? koto (matter)?, 
-The hierarchies of ?men (side), inshou (impres-
sion), kanji (feeling), kibun (mood), kimochi 
(feeling)? 
-The hierarchies of ?men (side), sokumen (one- 
side), imi (meaning), kanten (viewpoint), kenchi 
(standpoint)? 
-The hierarchies of ?joutai (state), teido (degree)? 
-The hierarchies of ?joutai (state), jousei (situa-
tion)?  
-The hierarchies of ?men (side), inshou (impres-
sion), seikaku (character) or gaikan (appear-
ance) or utsukushisa (beauty)?.  
The lines in Fig.8 are not peculiar, and appear 
in an area of the hierarchies of ?seikaku (charac-
Fig.3: Hierarchies of  
?kimochi (feeling)? 
Fig.4:Hierarchies of 
?sokumen (one side)? 
Fig.5:Hierarchies of 
?teido (degree)? 
Fig8: Hierarchies of 
?kanshoku (feel)? 
Fig.6:  Hierarchies of  
?jousei (situation)? 
Fig.7:Hierarchies of 
?seikaku (character)? 
ter)? in Fig.7. As Fig.8 shows, the hierarchies of 
?men (side), inshou (impression), kanji (feeling), 
kanshoku (feel) or kansei (sensitivity)? are lo-
cated in the area of the hierarchies of ?seikaku 
(character)?, above the hierarchies of ?kimochi 
(feeling)? in Fig.3. 
5. Comparison of hierarchies of super-
ordinate nouns of adjectives. 
We compare the hierarchy mentioned above 
with ones obtained from two kinds of data. 
1) Hierarchies obtained by: 
CSM and Yate?s correction 
corpus occurrence data (no frequency). 
2) Hierarchies obtained by: 
Tf.CSM and Yate?s correction 
corpus frequency data. 
3) Hierarchies obtained by: 
Overlap coefficient and Yates' correction 
corpus occurrence data (no frequency). 
 
As both CSM and the Overlap coefficient are 
?measures of inclusion?, we compared CSM and 
Tf.CSM with the Overlap coefficient. 
The number of groups that were obtained by 
CSM, Tf.CSM and the Overlap coefficient are 
the following. 
Table 2. Total number of groups obtained from CSM, 
Tf.CSM and Ovlp (Overlap) 
 groups 
CSM 161 
Tf.CSM 158 
Ovlp 240 
The Depth of hierarchies obtained from CSM, 
Tf.CSM, and the Overlap coefficient are as fol-
lows: 
Table 3. The hierarchy depth for CSM, Tf.CSM,  
and the Overlap coefficient 
 
In the case of CSM, there are 32 groups at 
depth 7, which is the greatest number of groups. 
The greatest concentration of groups is at depth 5 
to 10. In the case of Tf.CSM, the greatest number 
of groups is 25 at depth 8. The greatest concen-
tration of groups is at depth 5 to 13. In the case of 
the overlap coefficient, the greatest number of 
groups is 61 at depth 5. The greatest concentra-
tion of groups is at depth 3 to 7. 
0
10
20
30
40
50
60
70
3 4 5 6 7 8 9 10 11 12 13 14 15
CSM
Tf.CSM
Ovlp
 
 
 
From this result, we can see that hierarchies 
generated by Tf.CSM are relatively deep, and 
those generated by the Overlap coefficient are 
relatively shallow.  
In the case of the Overlap coefficient, abstract 
nouns in lower layers are sometimes directly re-
lated to abstract nouns in the highest layers. On 
the other hand, in hierarchies generated by CSM 
and Tf.CSM, abstract nouns in the highest layers 
are related to those in the lowest layers via ab-
stract nouns in the middle layers. The following 
indicates the number of overlapping hierarchies 
for CSM, Tf.CSM and Overlap. 
Table 4. The number of overlapping hierarchies 
among CSM, Tf.CSM and Overlap 
CSM&Tf.CSM 37 
CSM&Ovlp 7 
Tf.CSM&Ovlp 2 
CSM&Tf.CSM&Ovlp 7 
The hierarchy generated by Tf.CSM is the 
deepest, and includes some hierarchies generated 
by CSM and the Overlap coefficient. The hierar-
chy generated by CSM is more similar to the one 
made by Tf.CSM than that for the Overlap coef-
ficient: the number of completely corresponding 
hierarchies for CSM and Tf.CSM is 37, that for 
CSM and the Overlap coefficient is 7, and that 
for Tf.CSM and the Overlap coefficient is 2. The 
total number of hierarchies that correspond com-
pletely between CSM, Tf.CSM and the Overlap 
coefficient is 7, and the number of hierarchies 
which are generated by two of the methods and 
included in the third is 57. 
depth 3 4 5 6 7 8 9
CSM 0 3 16 27 32 23 23
Tf.CSM 1 5 10 18 13 25 11
Ovlp 32 56 61 57 21 7 2
depth 10 11 12 13 14 15 
CSM 19 7 3 4 3 1 
Tf.CSM 24 13 14 14 7 2 
Ovlp 2 0 0 0 0 0 
Figure 9. Distribution of hierarchy depth for CSM, 
Tf.CSM, and Overlap coefficient 
We investigated these 64 hierarchies precisely, 
checking adjectives appearing at each depth as 
indicated by an abstract noun in this paper.  In 6 
of these hierarchies, the same adjectives were 
found at all levels of the hierarchy. In 14 of the 
remaining 58 hierarchies, the same adjectives 
were found in all but the deepest level.  These 
20 hierarchies are the most plausible in the strict 
sense of the word. Below, we give examples of 
these hierarchies. In the next stage of this re-
search, we intend to investigate the remaining 44 
hierarchies to determine the reason for the differ-
ence in adjective content. 
The common hyperonym: koto (matter) --- 
men1 (side) --- 
sokumen (one side) --- 
imi (meaning) --- 
kanten (viewpoint) --- 
me2 (eyes) --- 
mikata (view) --- 
hyouka (evaluation) --- 
ippou (while or grow -er and er) --- 
ikioi (force) --- 
sokudo (speed) --- 
jikoku (time) --- 
6. Conclusion 
We have suggested how to make a hierarchy 
of adjectives automatically by connecting 
strongly-related abstract nouns in a top-down 
fashion. We generated a word hierarchy from 
corpus data by using a combination of two 
methods: a self-organizing semantic map and a 
directional similarity measure. As our directional 
similarity measure, we utilized the complement-
ary similarity measure (CSM). Then we com-
pared the hierarchy generated by CSM with that 
generated by Tf.CSM and the Overlap coefficient. 
In the case of Tf.CSM, the hierarchy is deeper 
than the others because there are more abstract 
nouns in the middle layer. In the case of the 
Overlap coefficient, the hierarchy is shallow, but 
there are more hyponyms in the lower layer than 
with the other two methods. As a result, the 
hierarchies generated by CSM have more com-
mon hierarchical relations than those generated 
by the other two methods. In future work, we will 
analyze common hierarchies made by the three 
methods in detail and examine differences among 
them in order to generate an abstract conceptual 
hierarchy of adjectives. We will then compare 
our hierarchy with thesauri compiled manually. 
After we have completed the experiment on Jap-
anese adjectives, we are keen to investigate dif-
ferences and similarities in adjective hypero-
nyms between Japanese and other languages such 
as English by means of our method. 
Acknowledgement 
We would like to thank Dr. Masaki Murata of 
NICT for allowing us to use his drawing tool. 
References  
Nemoto, K. 1969. The combination of the noun with 
?ga-Case? and the adjective, Language research2 
for the computer, National Language Research In-
stitute: 63-73/ 
Takahashi, T. 1975. A various phase related to the 
part-whole relation investigated in the sentence, 
Studies in the Japanese language 103, The society 
of Japanese Linguistics: 1-16. 
Kohonen, T. 1995. Self-Organizing Maps, Springer. 
Hindle, D. 1990. Noun Classification From Predicate-
Argument Structures, In the Proceedings of the 28th 
Annual Meeting of the Association for Computa-
tional Linguistics: 268-275 
Hatzivassiloglou,V. and McKeown,R.K. 1993. To-
wards the Automatic Identification of Adjectival 
Scales: Clustering Adjectives According to Mean-
ing, In the Proceedings of the 31st Annual Meeting 
of the Association for Computational Linguistics: 
172-182.  
Hagita, N. and Sawaki, M. 1995. Robust Recognition 
of Degraded Machine-Printed Characters using 
Complimentary Similarity Measure and Error-
Correction Learning?In the Proceedings of the 
SPIE ?The International Society for Optical Engi-
neering, 2442: 236-244. 
Yamamoto, E. and Umemura, K. 2002. A Similarity 
Measure for estimation of One?to-Many Relation-
ship in Corpus, Journal of Natural Language Proc-
essing: 45-75.  
Hans-Jorg Shmid. 2000. English Abstract Nouns as 
Conceptual Shells, Mouton de Gruyter. 
Kanzaki, K., Ma., Q. and Isahara, H. (2000), Similari-
ties and Differences among Semantic Behaviors of 
Japanese Adnominal Constituents, In the Proceed-
ings of the Syntactic and Semantic Complexity in 
Natural Language Processing Systems, ANLP and 
NAACL. 
Ma, Q., Kanzaki, K., Murata, M., Uchimoto, K. and 
Isahara, H. 2000. Self-Organization Semantic Maps 
of Japanese Noun in Terms of Adnominal Constitu-
ents, In Proceedings of IJCNN?2000, Como, Italy, 
vol.6.: 91-96. 
Non-Factoid Japanese Question Answering through Passage Retrieval
that Is Weighted Based on Types of Answers
Masaki Murata and Sachiyo Tsukawaki
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{murata,tsuka}@nict.go.jp
Qing Ma
Ryukoku University
Otsu, Shiga, 520-2194, Japan
qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru
Kyoto University
Yoshida-Nihonmatsu-Cho, Sakyo
Kyoto, 606-8501 Japan
kanamaru@hi.h.kyoto-u.ac.jp
Hitoshi Isahara
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We constructed a system for answering non-
factoid Japanese questions. We used var-
ious methods of passage retrieval for the
system. We extracted paragraphs based on
terms from an input question and output
them as the preferred answers. We classified
the non-factoid questions into six categories.
We used a particular method for each cate-
gory. For example, we increased the scores
of paragraphs including the word ?reason?
for questions including the word ?why.? We
participated at NTCIR-6 QAC-4, where our
system obtained the most correct answers
out of all the eight participating teams. The
rate of accuracy was 0.77, which indicates
that our methods were effective.
1 Introduction
A question-answering system is an application de-
signed to produce the correct answer to a question
given as input. For example, when ?What is the
capital of Japan?? is given as input, a question-
answering system may retrieve text containing sen-
tences like ?Tokyo is Japan?s capital and the coun-
try?s largest and most important city?, and ?Tokyo
is also one of Japan?s 47 prefectures?, from Web-
sites, newspaper articles, or encyclopedias. The sys-
tem then outputs ?Tokyo? as the correct answer.
We believe question-answering systems will become
a more convenient alternative to other systems de-
signed for information retrieval and a basic compo-
nent of future artificial intelligence systems. Numer-
ous researchers have recently been attracted to this
important topic. These researchers have produced
many interesting studies on question-answering sys-
tems (Kupiec, 1993; Ittycheriah et al, 2001; Clarke
et al, 2001; Dumis et al, 2002; Magnini et al, 2002;
Moldovan et al, 2003). Evaluation conferences and
contests on question-answering systems have also
been held. In particular, the U.S.A. has held the Text
REtrieval Conferences (TREC) (TREC-10 commit-
tee, 2001), and Japan has hosted the Question-
Answering Challenges (QAC) (National Institute of
Informatics, 2002) at NTCIR (NII Test Collection
for IR Systems ) 3. These conferences and contests
have aimed at improving question-answering sys-
tems. The researchers who participate in these create
question-answering systems that they then use to an-
swer the same questions, and each system?s perfor-
mance is then evaluated to yield possible improve-
ments.
We addressed non-factoid question answering in
NTCIR-6 QAC-4. For example, when the question
was ?Why are people opposed to the Private Infor-
mation Protection Law?? the system retrieved sen-
tences based on terms appearing in the question and
output an answer using the retrieved sentences. Nu-
merous studies have addressed issues that are in-
volved in the answering of non-factoid questions
(Berger et al, 2000; Blair-Goldensohn et al, 2003;
727
Xu et al, 2003; Soricut and Brill, 2004; Han et al,
2005; Morooka and Fukumoto, 2006; Maehara et
al., 2006; Asada, 2006).
We constructed a system for answering non-
factoid Japanese questions for QAC-4. We used
methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions
including the word ?why.? We performed exper-
iments using the NTCIR-6 QAC-4 data collection
and tested the effectiveness of our methods.
2 Categories of Non-Factoid Questions
We used six categories of non-factoid questions in
this study. We constructed the categories by con-
sulting the dry run data in QAC-4.
1. Definition-oriented questions (Questions that
require a definition to be given in response.)
e.g., K-1 to wa nandesuka? (What is K-1?)
2. Reason-oriented questions (Questions that re-
quire a reason to be given in response.)
e.g., kojin jouhou hokogou ni hantai shiteiru
hito wa doushite hantai shiteiru no desuka?
(Why are people opposed to the Private Infor-
mation Protection Law?)
3. Method-oriented questions (Questions that re-
quire an explanation of a method to be given in
response.)
e.g., sekai isan wa donoyouni shite kimeru no
desuka?? (How is a World Heritage Site deter-
mined?)
4. Degree-oriented questions (Questions that re-
quire an explanation of the degree of something
to be given in response.)
5. Change-oriented questions (Questions that re-
quire a description of things that change to be
given in response.)
e.g., shounen hou wa dou kawari mashitaka?
(How was the juvenile law changed?)
6. Detail-oriented questions (Questions that re-
quire a description of the particulars or details
surrounding a sequence of events to be given in
response.)
e.g., donoyouna keii de ryuukyuu oukoku wa ni-
hon no ichibu ni natta no desuka? (How did
Ryukyu come to belong to Japan?)
3 Question-answering Systems in this
Study
The system has three basic components:
1. Prediction of type of answer
The system predicts the answer to be a partic-
ular type of expression based on whether the
input question is indicated by an interrogative
pronoun, an adjective, or an adverb. For exam-
ple, if the input question is ?Why are people
opposed to the Private Information Protection
Law??, the word ?why? suggests that the an-
swer will be an expression that describes a rea-
son.
2. Document retrieval
The system extracts terms from the input ques-
tion and retrieves documents by using these
terms. Documents that are likely to contain
the correct answer are thus gathered during the
retrieval process. For example, for the input
question ?Why are people opposed to the Pri-
vate Information Protection Law??, the system
extracts ?people,? ?opposed,? ?Private,? ?Infor-
mation,? ?Protection,? and ?Law? as terms and
retrieves the appropriate documents based on
these.
3. Answer detection
The system separates the retrieved documents
into paragraphs and retrieves those that contain
terms from the input question and a clue ex-
pression (e.g., ?to wa? (copula sentence) for the
definition sentence). The system outputs the re-
trieved paragraphs as the preferred answer.
3.1 Prediction of type of answer
We used the following rules for predicting the type
of answer. We constructed the rules by consulting
the dry run data in QAC-4.
728
1. Definition-oriented questions Questions in-
cluding expressions such as ?to wa nani,?
?donna,? ?douiu,? ?douitta,? ?nanimono,?
?donoyouna mono,? ?donna mono,? and ?douiu
koto? (which all mean ?what is?) are rec-
ognized by the system as being definition-
oriented questions.
2. Reason-oriented questions Questions including
expressions such as ?naze? (why), ?naniyue?
(why), ?doushite? (why), ?nani ga riyuu de?
(what is the reason), and ?donna riyuu de?
(what reason), are recognized by the system as
being reason-oriented questions.
3. Method-oriented questions Questions includ-
ing expressions such as ?dou,? ?dousureba,?
?douyatte,? ?dono youni shite,? ?ikani shite,?
?ikani,? and ?donnna houhou de? (which all
mean ?how?) are recognized by the system as
being method-oriented questions.
4. Degree-oriented questions Questions including
expressions such as ?dorekurai? (how much),
?dorekurai no? (to what extent), and ?dono
teido? (to what extent), are recognized by the
system as being degree-oriented questions.
5. Change-oriented questions Questions includ-
ing expressions such as ?naniga chigau? (What
is different), ?donoyuni kawaru? (How is ...
changed), and ?dokoga kotonaru? (What is dif-
ferent), are recognized by the system as being
change-oriented questions.
6. Detail-oriented questions Questions including
expressions such as ?dono you na keii,? ?dono
you na ikisatsu,? and ?dono you na nariyuki?
(which all mean ?how was?) are recognized by
the system as being detail-oriented questions.
3.2 Document retrieval
Our system extracts terms from a question by using
the morphological analyzer, ChaSen (Matsumoto et
al., 1999). The analyzer first eliminates preposi-
tions, articles, and similar parts of speech. It then
retrieves documents by using the extracted terms.
The documents are retrieved as follows:
We first retrieve the top k
dr1
documents with the
highest scores calculated using the equation
Score(d)
=
?
term t
?
?
?
tf(d, t)
tf(d, t) + kt
length(d) + k
+
? + k
+
? log
N
df(t)
?
?
?
,
(1)
where d is a document, t is a term extracted from
a question, and tf(d, t) is the frequency of t oc-
curring in d. Here, df(t) is the number of docu-
ments in which t appears, N is the total number
of documents, length(d) is the length of d, and ?
is the average length of all documents. Constants
k
t
and k
+
are defined based on experimental re-
sults. We based this equation on Robertson?s equa-
tion (Robertson and Walker, 1994; Robertson et al,
1994). This approach is very effective, and we have
used it extensively for information retrieval (Murata
et al, 2000; Murata et al, 2001; Murata et al, 2002).
The question-answering system uses a large number
for k
t
.
We extracted the top 300 documents and used
them in the next procedure.
3.3 Answer detection
In detecting answers, our system first generates can-
didate expressions for them from the extracted docu-
ments. We use two methods for extracting candidate
expressions. Method 1 uses a paragraph as a candi-
date expression. Method 2 uses a paragraph, two
continuous paragraphs, or three continuous para-
graphs as candidate expressions.
We award each candidate expression the follow-
ing score.
Score(d)
= ?mint1?T log
?
t2?T3
(2dist(t1, t2)df(t2)
N
)
+ 0.00000001 ? length(d)
= maxt1?T
?
t2?T3
log
N
2dist(t1, t2) ? df(t2)
+ 0.00000001 ? length(d)
(2)
729
T3 = {t|t ? T, 2dist(t1, t)df(t)
N
? 1}, (3)
where d is a candidate expression, T is the set of
terms in the question, dist(t1, t2) is the distance
between t1 and t2 (defined as the number of char-
acters between them with dist(t1, t2) = 0.5 when
t1 = t2), and length(d) is the number of charac-
ters in a candidate expression. The numerical term,
0.00000001 ? length(d), is used for increasing the
scores of long paragraphs.
For reason-oriented questions, our system uses
some reason terms such as ?riyuu? (reason),
?gen?in? (cause), and ?nazenara? (because) as terms
for Eq. 2 in addition to terms from the input ques-
tion. This is because we would like to increase the
score of a document that includes reason terms for
reason-oriented questions.
For method-oriented questions, our system uses
some method terms such as ?houhou? (method),
?tejun? (procedure), and ?kotoniyori? (by doing) as
terms for second document retrieval (re-ranking) in
addition to terms from the input question.
For detail-oriented questions, our system uses
some method terms such as ?keii? (a detail, or a se-
quence of events), ?haikei? (background), and ?rek-
ishi? (history) as terms for second document re-
trieval (re-ranking) in addition to terms from the in-
put question.
For degree-oriented questions, when candidate
paragraphs include numerical expressions, the score
(Score(d)) is multiplied by 1.1.
For definition-oriented questions, the system first
extracts focus expressions. When the question in-
cludes expressions such as ?X-wa?, ?X-towa?, ?X-
toiunowa?, and ?X-tte?, X is extracted as a fo-
cus expression. The system multiplies the score,
(Score(d)), of the candidate paragraph having ?X-
wa?, ?X-towa or something by 1.1. When the can-
didate expression includes focus expressions having
modifiers (including modifier clauses and modifier
phrases), the modifiers are used as candidate expres-
sions, and the scores of the candidate expressions are
multiplied by 1.1.
Below is an example of a candidate expression
that is a modifier clause in a sentence.
Table 1: Results
Method Correct A B C D
Method 1 57 18 42 10 89
Method 2 77 5 67 19 90
(There were a total of 100 questions.)
Question sentence: sekai isan jouyaku to
wa dono youna jouyaku desu ka?
(What is the Convention concerning the
Protection of the World Cultural and Nat-
ural Heritage?)
Sentence including answers:
1972 nen no dai 17 kai yunesuko soukai de
saitaku sareta sekai isan jouyaku ....
(Convention concerning the Pro-
tection of the World Cultural
and Natural Heritage, which
was adopted in 1972 in the 17th gen-
eral assembly meeting of the UN Educational,
Scientific and Cultural Organization.)
Finally, our system extracts candidate expressions
having high scores, (Score(d)s), as the preferred
output. Our system extracts candidate expressions
having scores that are no less than the highest score
multiplied by 0.9 as the preferred output.
We constructed the methods for answer detection
by consulting the dry run data in QAC-4.
4 Experiments
The experimental results are listed in Table 1. One
hundred non-factoid questions were used in the ex-
periment. The questions, which were generated by
the QAC-4 organizers, were natural and not gener-
ated by using target documents. The QAC-4 orga-
nizers checked four or fewer outputs for each ques-
tion. Methods 1 and 2 were used to determine what
we used as answer candidate expressions (Method 1
uses one paragraph as a candidate answer. Method
2 uses one paragraph, two paragraphs, or three para-
graphs as candidate answers.).
?A,? ?B,? ?C,? and ?D? are the evaluation criteria.
?A? indicates output that describes the same content
as that in the answer. Even if there is a supplemen-
tary expression in the output, which does not change
730
the content, the output is judged to be ?A.? ?B? in-
dicates output that contains some content similar to
that in the answer but contains different overall con-
tent. ?C? indicates output that contains part of the
same content as that in the answer. ?D? indicates
output does not contain any of the same content as
that in the answer. The numbers for ?A,? ?B,? ?C,?
and ?D? in Table 1 indicate the number of questions
where an output belongs to ?A,? ?B,? ?C,? and ?D?.
?Correct? indicates the number of questions where
an output belongs to ?A,? ?B,? or ?C?. The evalu-
ation criteria ?Correct? was also used officially at
NTCIR-6 QAC-4.
We found the following.
? Method 1 obtained higher scores in evaluation
A than Method 2. This indicates that Method 1
can extract a completely relevant answer more
accurately than Method 2.
? Method 2 obtained higher scores in evaluation
?Correct? than Method 1. The rate of accuracy
for Method 2 was 0.77 according to evaluation
?Correct?. This indicates that Method 2 can ex-
tract more partly relevant answers than Method
1. When we want to extract completely relevant
answers, we should use Method 1. When we
want to extract more answers, including partly
relevant answers, we should use Method 2.
? Method 2 was the most accurate (0.77) of those
used by all eight participating teams. We could
detect paragraphs as answers including input
terms and the key terms related to answer types
based the methods discussed in Section 3.3.
Our system obtained the best results because
our method of detecting answers was the most
effective.
Below is an example of the output of Method 1,
which was judged to be ?A.?
Question sentence:
jusei ran shindan wa douiu baai ni okon-
awareru noka?
(When is amniocentesis performed on a
pregnant woman?)
System output:
omoi idenbyou no kodono ga umareru no
wo fusegu.
(To prevent the birth of children with seri-
ous genetic disorders )
Examples of answers given by organizers:
omoi idenbyou
(A serious genetic disorder)
omoi idenbyou no kodomo ga umareru
kanousei ga takai baai
(To prevent the birth of children with seri-
ous genetic disorders.)
5 Conclusion
We constructed a system for answering non-factoid
Japanese questions. An example of a non-factoid
question is ?Why are people opposed to the Pri-
vate Information Protection Law?? We used vari-
ous methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions in-
cluding the word ?why.? We participated at NTCIR-
6 QAC-4, where our system obtained the most cor-
rect answers out of all the eight participating teams.
The rate of accuracy was 0.77, which indicates that
our methods were effective.
We would like to apply our method and system to
Web data in the future. We would like to construct a
sophisticated system that can answer many kinds of
complicated queries such as non-factoid questions
based on a large amount of Web data.
Acknowledgements
We are grateful to all the organizers of NTCIR-6
who gave us the chance to participate in their con-
test to evaluate and improve our question-answering
system. We greatly appreciate the kindness of all
those who helped us.
731
References
Yoshiaki Asada. 2006. Processing of definition type
questions in a question answering system. Master?s
thesis, Yokohama National University. (in Japanese).
AdamBerger, Rich Caruana, David Cohn, Dayne Freitag,
and Vibhu Mittal. 2000. Bridging the lexical chasm:
Statistical approaches to answer-finding. In Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR-2000), pages 192?199.
Sasha Blair-Goldensohn, Kathleen R. McKeown, and
Andrew Hazen Schlaikjer. 2003. A hybrid approach
for qa track definitional questions. In Proceedings
of the 12th Text Retrieval Conference (TREC-2003),
pages 185?192.
Charles L. A. Clarke, Gordon V. Cormack, and
Thomas R. Lynam. 2001. Exploiting redundancy
in question answering. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Susan Dumis, Michele Banko, Eric Brill, Jimmy Lin, and
Andrew Ng. 2002. Web question answering: Is more
always better? In Proceedings of the 25th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, and
Hae-Chang Rim. 2005. Phrase-based definitional
question answering using definition terminology. In
Lecture Notes in Computer Science 3689, pages 246?
259.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. IBM?s Statistical Ques-
tion Answering System. In TREC-9 Proceedings.
Julian Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line ency-
clopedia. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Hideyuki Maehara, Jun?ichi Fukumoto, and Noriko
Kando. 2006. A BE-based automated evaluation
for question-answering system. IEICE-WGNLC2005-
109, pages 19?24. (in Japanese).
Bernardo Magnini, Matto Negri, Roberto Prevete, and
Hristo Tanev. 2002. Is it the right answer? Exploiting
web redundancy for answer validation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual 2nd edition.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-
hai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems,
21(2):133?154.
Kokoro Morooka and Jun?ichi Fukumoto. 2006. Answer
extraction method for why-type question answering
system. IEICE-WGNLC2005-107, pages 7?12. (in
Japanese).
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara. 2000.
Japanese probabilistic information retrieval using lo-
cation and category information. The Fifth Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 81?88.
Masaki Murata, Masao Utiyama, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2001. CRL at NTCIR2.
Proceedings of the Second NTCIR Workshop Meeting
on Evaluation of Chinese & Japanese Text Retrieval
and Text Summarization, pages 5?21?5?31.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002.
High performance information retrieval using many
characteristics and many techniques. Proceedings of
the Third NTCIR Workshop (CLIR).
National Institute of Informatics. 2002. Proceedings of
the Third NTCIR Workshop (QAC).
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-Poisson model for
probabilistic weighted retrieval. In Proceedings of the
Seventeenth Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: Beyond the factoid. In In Proceedings
of the Human Language Technology and Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-2004), pages
57?64.
TREC-10 committee. 2001. The tenth text retrieval con-
ference. http://trec.nist.gov/pubs/trec10/t10 proceed-
ings.html.
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC 2003 QA at BBN: answering definitional ques-
tions. In Proceedings of the 12th Text Retrieval Con-
ference (TREC-2003), pages 98?106.
732
 
	 ffBuilding an Annotated Japanese-Chinese Parallel Corpus  
? A Part of NICT Multilingual Corpora  
Yujie Zhang and  Kiyotaka Uchimoto and Qing Ma and Hitoshi Isahara 
 
National Institute of Information and Communications Technology 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 
(yujie, uchimoto,qma, isahara)@nict.go.jp
 
Abstract 
We are constricting a Japanese-Chinese 
parallel corpus, which is a part of the 
NICT Multilingual Corpora. The corpus is 
general domain, of large scale of about 
40,000 sentence pairs, long sentences, 
annotated with detailed information and 
high quality. To the best of our knowledge, 
this will be the first annotated Japanese-
Chinese parallel corpus in the world. We 
created the corpus by selecting Japanese 
sentences from Mainichi Newspaper and 
then manually translating them into 
Chinese. We then annotated the corpus 
with morphological and syntactic 
structures and alignments at word and 
phrase levels. This paper describes the 
specification in human translation and the 
scheme of detailed information annotation, 
and the tools we developed in the corpus 
construction. The experience we obtained 
and points we paid special attentions are 
also introduced for share with other 
researches in corpora construction.     
1 Introduction 
A parallel corpus is a collection of articles, 
paragraphs, or sentences in two different languages. 
Since a parallel corpus contains translation 
correspondences between the source text and its 
translations at different level of constituents, it is a 
critical resource for extracting translation 
knowledge in machine translation (MT). Although 
recently some versions of machine translation 
software have become available in the market, 
translation quality is still a significant problem. 
Therefore, a detailed examination into human 
translation is still required. This will provide a basis 
for radically improving machine translation in the 
near future. In addition, in MT system development, 
the example-based method and the statistics-based 
method are widely researched and applied. So, 
parallel corpora are required by the translation 
studies and practical system development.   
The raw text of a parallel corpus contains 
implicit knowledge. If we annotate some 
information, we can get explicit knowledge from 
the corpus. The more information that is annotated 
on a parallel corpus, the more knowledge we can 
get from the corpus. The parallel corpora of 
European languages are usually raw texts without 
annotation on syntactic structure since their 
syntactic structures are similar and MT does not 
require such annotation information. However, 
when language pairs are different in syntactic 
structures, such as the pair of English and Japanese 
and the pair of Japanese and Chinese, 
transformation between syntactic structures is 
difficult. A parallel corpus annotated with syntactic 
structures would thus be helpful to MT.  Besides 
MT, an annotated parallel corpus can be applied to 
cross-lingual information retrieval, language 
teaching, machine-aided translation, bilingual 
lexicography, and word-sense disambiguation.  
Parallel corpora between European languages 
are well developed and are available through the 
Linguistic Data Consortium (LDC). However, 
parallel corpora between European languages and 
Asian languages are less developed, and parallel 
corpora between two Asian languages are even less 
developed.  
The National Institute of Information and 
Communications Technology therefore started a 
project to build multilingual parallel corpora in 
2002 (Uchimoto et al, 2004). The project focuses 
on Asian language pairs and annotation of detailed 
information, including syntactic structure and 
alignment at word and phrase levels. We call the 
corpus the NICT Multilingual Corpora. The corpus 
will be open to the public in the near future. 
2 Overview of the NICT Multilingual 
Corpora 
At present, a Japanese-English parallel corpus and a 
Japanese-Chinese parallel corpus are under 
construction following systematic specifications. 
The parallel texts in each corpus consist of the 
original text in the source language and its 
translations in the target language. The original data 
is from newspaper articles or journals, such as 
85
Mainichi Newspaper in Japanese. The original 
articles were translated by skilled translators. In 
human translation, the articles of one domain were 
all assigned to the same translators to maintain 
consistent terminology in the target language. 
Different translators then revised the translated 
articles. Each article was translated one sentence to 
one sentence, so the obtained parallel corpora are 
already sentence aligned.  
  The details of the current version of the NICT 
Multilingual Corpora are listed in Table 1. 
Corpora Total Original Translation 
Japanese 
(19,669 
sentences, 
Mainichi 
Newspaper) 
English 
Translation 
Japanese-
English 
Parallel 
Corpus 
37,987 
sentence 
pairs; 
(English 
900,000 
words) English 
(18,318 
Sentences, 
Wall Street 
Journal) 
Japanese 
Translation 
Japanese-
Chinese 
Parallel 
Corpus 
38,383 
sentence 
pairs; 
(Chinese 
1,410,892 
Characters, 
926,838 
words) 
Japanese 
(38,383 
sentences, 
Mainichi 
Newspaper) 
Chinese 
Translation 
Table 1 Details of current version of NICT Multilingual 
Corpora 
 
   The following is an example of English and 
Chinese translations of a Japanese sentence from 
Mainichi Newspaper. 
[Ex. 1] 
J: ????????????????????????
?????? 
E: They were all about nineteen years old and had 
no strength left even to answer questions. 
C: ?????????????????????
?????????????  
 
In addition to the human translation, another big 
task is annotating the information. We finish the 
task by two steps: automatic annotation and human 
revision. In automatic annotation, we applied 
existing analysis techniques and tag sets. In human 
revision, we developed assisting tools that have 
powerful functions to help annotators in revision. 
The annotation task for each language included 
morphological and syntactic structure annotation.  
The annotation task for each language pair included 
alignments at word and phrase level.  
The NICT Multilingual Corpora constructed in 
this way have the following characteristics. 
(1) Since the original data is from newspaper and 
journals, the domain of each corpus is therefore rich.  
(2) Each corpus consists of original sentences and 
their translations, so they are already sentence 
aligned. In translation of each sentence, the context 
of the article is also considered. Thus, the context of 
each original article is also well maintained in its 
translation, which can be exploited in the future. 
(3) The corpora are annotated at high quality with 
morphological and syntactic structures and 
word/phrase alignment.  
 In the following section, we will describe the 
details in the construction of the Japanese-Chinese 
parallel corpus. 
3 Human Translation from Japanese to 
Chinese   
About 40,000 Japanese sentences from issues of 
Mainichi Newspaper were translated by skilled 
translators. The translation guidelines were as 
follows. 
(1) One Japanese sentence is translated into one 
Chinese sentence. 
(2) Among several translation candidates, the one 
that is close to the original sentence in syntactic 
structure is preferred. The aim is to avoid 
translating a sentence too freely, i.e., 
paraphrasing. 
(3) To obtain intelligible Chinese translations, 
information of the proceeding sentences in the 
same article should be added. Especially, a 
subject should be supplemented because a 
subject is usually required in Chinese, while in 
Japanese subjects are often omitted . 
(4)  To obtain natural Chinese translations, 
supplement, deletion, replacement, and 
paraphrase  should be made when necessary. 
When a translation is very long, word order can 
be changed or commons can be inserted. These 
are the restrictions on (2), i.e., the naturalness 
of the Chinese translations is the priority.  
 
  One problem in translation is how to translate 
proper nouns in the newspaper articles. We pay 
special attentions to them in the following way.  
(1) Proper nouns  
When proper nouns did not exist in Japanese-
Chinese dictionaries, new translations were created 
and then confirmed using the Chinese web. For 
kanji in proper nouns, if there was a Chinese 
character having the same orthography as the kanji, 
the Chinese character was used in the Chinese 
translation; if there was a traditional Chinese 
character having the same orthography as the kanji, 
the simplified character of the traditional Chinese 
character was used in the translation; otherwise, a 
Chinese character whose orthography is similar to 
that of the kanji was used in the translation.  
(2) Special things in Japan 
86
 Explanations were added if necessary. For example, 
?????, translated from ????? (grand sumo 
tournament), is well known in China, while ????, 
translated from ???? (spring labor offensive), is 
not known in China. In this case, an explanation 
???????? was added behind the unfamiliar 
term. We attempt to introduce new words about 
Japanese culture into Chinese through the 
construction of the corpus.    
 
   Producing high-quality Chinese translations is 
crucial to this parallel corpus. We controlled the 
quality by the following treatments.  
(1) The first revision of a translated article was 
conducted by a different translator after the first 
translation. The reviewers checked whether the 
meanings of the Chinese translations corresponded 
accurately to the meanings of the original sentences 
and modified the Chinese translations if necessary. 
(2) The second revision was conducted by Chinese 
natives without referring to the original sentences. 
The reviewers checked whether the Chinese 
translations were natural and passed the unnatural 
translations back to translators for modification. 
(3) The third revision was conducted by a Chinese 
native in the annotation process of Chinese 
morphological information. The words that did not 
exist in the dictionary of contemporary Chinese 
were checked to determine whether they were new 
words. If not, the words were designated as 
informal or not written language and were replaced 
with suitable words. The word sequences that 
missed the Chinese language model?s part-of-
speech chain were also adjusted.        
 
 Until now, 38,383 Japanese sentences have 
been translated to Chinese, and of those, 22,000 
Chinese translations have been revised three times, 
and we are still working on the remaining 18,000 
Chinese translations.  
4 Morphological Information Annotation 
Annotation consists of automatic analyses and 
manual revision. 
4.1 Annotation on Japanese Sentences  
Japanese morphological and syntactic analyses 
follow the definitions of part-of-speech categories 
and syntactic labels of the Corpus of Spontaneous 
Japanese (Maekawa, 2000).  
A morphological analyzer developed in that 
project was applied for automatic annotation on the 
Japanese sentences and then the automatically 
tagged sentences were revised manually. An 
annotated senetence is illustrated in Figure 1, which 
is the Japanese sentence in Ex. 1 in Section 2.  
 
 
 
 
 
 
# S-ID:950104141-008 
* 0 2D 
???? ???? * ?? * * * 
* 1 2D 
?? ?????? * ?? ?? * * 
? ?? * ??? ???????? * * 
?? ??? * ??? ???????? * * 
? ? * ?? ???? * * 
* 2 6D 
?? ???? * ?? ???? * * 
? ? ? ??? * ??? ???????? 
? ? * ?? ?? * * 
* 3 4D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 4 5D 
??? ???? ??? ?? * ???? ??? 
* 5 6D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 6 -1D 
??? ???? ?? ?? * ???? ???? 
? ? ?? ??? ?????? ???? ??? 
?? ?? ?? ??? ???? ???? ??? 
? ? * ?? ?? * * 
EOJ 
 
Figure 1. An annotated Japanese sentence 
 
The data of one sentence begins from the line ?# S-
ID... ? and ends with the mark ?EOJ?. The line 
headed by ?*? indicates the beginning of a phrase 
and the following lines are morphemes in that 
phrase. For example, the line ?* 0 2D? indicates the 
phrase whose number is 0. The following line ???
??  ? ? ? ?  * ? ?  * * *? indicates the 
morpheme in the phrase. There are seven fields in 
each morpheme line, token form, phonetic alphabet, 
dictionary form, part-of-speech, sub-part-of-speech, 
verbal category and conjugation form. In the line ?* 
0 2D?, the numeral 2 in ?2D? indicates that the 
phrase 0 ?????? modifies the phrase 2 ???
? ? ?. The syntactic structure analysis adopts 
dependency-structure analysis in which modifier-
modified relations between phrases are determined. 
The dependency-structure of the example in Figure 
1 is demonstrated in Figure 2. 
 
???? ?????? ???? ??? ??? ??? ???????
 
       Figure 2  Example of syntactic structure 
 
87
4.2 Annotation on Chinese Sentences 
For Chinese morphological analysis, we used the 
analyser developed by Peking University, where the 
research on definition of Chinese words and the 
criteria of word segmentation has been conducted 
for over ten years. The achievements include a 
grammatical knowledge base of contemporary 
Chinese, an automatic morphological analyser, and 
an annotated People?s Daily Corpus. Since the 
definition and tagset are widely used in Chinese 
language processing, we also took the criteria as the 
basis of our guidelines.  
A morphological analyzer developed by Peking 
University (Zhou and Yu, 1994) was applied for 
automatic annotation of the Chinese sentences and 
then the automatically tagged sentences were 
revised by humans. An annotated sentence is 
illustrated in Figure 3, which is the Chinese 
sentence in Ex. 1 in Section 2. 
 
S-ID: 950104141-008 
??/r  ??/j  ??/n  ?/d  ?/v   ??/m  ?/q 
??/m  ?/u  ???/n   ?/w  ??/r  ??/d  
?/p  ??/v  ??/n  ?/u  ??/n  ?/d   
??/v  ?/w 
Figure 3  An annotated Chinese sentence  
 
4.3 Tool for Manual Revision 
We developed a tool to assist annotators in revision. 
The tool has both Japanese and Chinese versions. 
Here, we introduce the Chinese version. The input 
of the tool is the automatically segmented and part-
of-speech tagged sentences and the output is revised 
data. The basic functions include separating a 
sequence of characters into two words, combining 
two segmented words into one word, and selecting 
a part-of-speech for a segmented word from a list of 
parts-of-speech. In addition, the tool has the 
following functions. 
(1) Retrieves a word in the grammatical knowledge 
base of contemporary Chinese of Peking University 
(Yu et al, 1997).  
This is convenient when annotators want to 
confirm whether a segmented word is authorized by 
the grammatical knowledge base, and when they 
want to know the parts-of-speech of a word defined 
by the grammatical knowledge base.  
(2) Retrieves a word in other annotated corpora or 
the sentences that have been revised.   
This is convenient when annotators want to see 
how the same word has been annotated before.  
(3) Retrieves a word in the current file.  
It collects all the sentences in the current file 
that contain the same word and then sorts their 
context on the left and right of the word. By 
referring to the sorted contexts, annotators can 
select words with the same syntactic roles and 
change all of the parts-of-speech to a certain one all 
in one operation. This is convenient when 
annotators want to process the same word in 
different sentences, aiming for consistency in 
annotation.     
(4) Adds new words to the grammatical knowledge 
base dynamically.  
The updated grammatical knowledge base can 
be used by the morphological analyser in the next 
analysis. 
(5) Indexes to sentences by an index file.  
The automatically discovered erroneous 
annotations can be stored in one index file, pointing 
to the sentences that are to be revised.  
 
The interface of the tool is shown in Figure 4 
and Figure 5. 
 
Figure 4 Interface of the manual revision tool (Retrieves 
a word in the grammatical knowledge base of 
contemporary Chinese) 
 
Figure 5    Interface of the manual revision tool 
(Retrieves a word in the current file) 
  
 
In Figure 4, the small window in the lower left 
displays the retrieved result of the word ? ??? in 
the grammatical knowledge base; the lower right 
window displays the retrieved result of the same 
word in the annotated People?s Daily Corpus. 
88
In Figure 5, the small window in the lower left is used to 
define retrieval conditions in the current file. In this 
example, the orthography of ???? is defined. The 
lower right window displays the sentences containing the 
word  ???? retrieved from the current file. The left and 
right contexts of one word are shown with the retrieved 
word. The contents of any column can be sorted by 
clicking the top line of the column. 
5 Annotation of word alignment  
Since automatic word alignment techniques cannot 
reach as high a level as the morphological analyses, 
we adopt a practical method of using multiple 
aligners. One aligner is a lexical knowledge-based 
approach, which was implemented by us based on 
the work of Ker (Ker and Chang, 1997). Another 
aligner is the well-known GIZA++ toolkit, which is 
a statistics-based approach. For GIZA++, two 
directions were adopted: the Chinese sentences 
were used as source sentences and the Japanese 
sentences as target sentences, and vice versa.  
The results produced by the lexical knowledge-
based aligner, C? J of GIZA++, and J?C of 
GIZA++ were selected in a majority decision. If an 
alignment result was produced by two or three 
aligners at the same time, the result was accepted. 
Otherwise, was abandoned.  In this way, we aimed 
to utilize the results of each aligner and maintain 
high precision at the same time. Table 2 showed the 
evaluation results of the multi-aligner on 1,127 test 
sentence pairs, which were manually annotated with 
gold standards, totally 17,332 alignments.  
 
 Precision 
(%) 
Recall 
(%) 
F-measure
Multi-aligner 79.3 62.7 70
Table 2 Evaluation results of the multi-aligner 
  
The multi-aligner produced satisfactory results. 
This performance is evidence that the multi-aligner 
is feasible for use in assisting word alignment 
annotation.  
For manual revision, we also developed an 
assisting tool, which consist of a graphical interface 
and internal data management. Annotators can 
correct the output of the automatic aligner and add 
alignments that it has not identified. In addition to 
assisting with word alignment, the tool also 
supports annotation on phrase alignment. Since 
Japanese sentences have been annotated with phrase 
structures, annotators can select each phrase on the 
Japanese side and then align them with words on 
the Chinese side. For idioms in Japanese sentences, 
two or more phrases can be selected. 
The input and output file of the manual 
annotation is in XML format. The data of one 
sentence pair consists of the Chinese sentence 
annotated with morphological information, the 
Japanese sentence annotated with morphological 
and syntactic structure information, word alignment, 
and phrase alignment.  
The alignment annotation at word and phrase is 
ongoing, the former focusing on lexical translations 
and the latter focusing on pattern translations. After 
a certain amount of data is annotated, we plan to 
exploit the annotated data to improve the 
performance of automatic word alignment. We will 
also investigate a method to automatically identify 
phrase alignments from the annotated word 
alignment and a method to automatically discover 
the syntactic structures on the Chinese side from the 
annotated phrase alignments.       
6 Conclusion  
We have described the construction of a Japanese-
Chinese parallel corpus, a part of the NICT 
Multilingual Corpus. The corpus consists of about 
40,000 pairs of Japanese sentences and their 
Chinese translations. The Japanese sentences are 
annotated with morphological and syntactic 
structures and the Chinese sentences are annotated 
with morphological information. In addition, word 
and phrase alignments are annotated. A high quality 
of annotation was obtained through manual 
revisions, which were greatly assisted by the 
revision tools we developed in the project. To the 
best of our knowledge, this will be the first 
annotated Japanese-Chinese parallel corpus in the 
world.  
In the future, we will finish the annotation on the 
remaining data and add syntactic structures to the 
Chinese sentences.  
  
References  
Dice, L.R. 1945. Measures of the amount of 
ecologic association between species. Journal of 
Ecology (26), pages 297?302. 
Ker, S.J., Chang, J.S. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, Vol. 23, Num. 2, pages 313?343. 
Liu Q. 2004.  Research into some aspects of 
Chinese-English machine translation. Doctoral 
Dissertation.  
Maekawa, K., Koiso, H., Furui, F., Isahara, H. 2000. 
Spontaneous Speech Corpus of Japanese. 
Proceedings of LREC2000, pages 947?952. 
LDC. 1992.  Linguistic data Consortium. 
http://www.ldc.upenn.edu/. 
Uchimoto, K. and Zhang,Y., Sudo, K., Murata, M., and 
Sekine, S.,  Isahara,  H. Multilingual Aligned Parallel 
89
Treebank Corpus Reflecting Contextual Information 
and Its Applications. Proceedings of the MLR2004: 
PostCOLING Workshop on Multilingual Linguistic 
Resources, pages 63-70. 
Yamada, K., Knight, K. 2001.A syntax-based Statistical 
Translation Model. In Proceedings of the ACL , pages 
523-530.  
Yu, Shiwen. 1997. Grammatical Knowledge Base of 
Contemporary Chinese. Tsinghua Publishing 
Company. 
Zhang, Y., Ma, Q., Isahara, H. 2005. Automatic 
Construction of Japanese-Chinese Translation 
Dictionary Using English as Intermediary. Journal of 
Natural Language Processing, Vol. 12, No. 2, pages 
63-85. 
Zhou, Q., Yu, S. 1994.  Blending Segmentation with 
Tagging in Chinese Language Corpus 
Processing.  In Proc. of COLING-94, pages 
1274?1278. 
 
 
90
Hybrid Neuro and Rule-Based Part of Speech Taggers 
Qing Ma, Masaki  Murata,  Kiyotaka Uch imoto ,  H i tosh i  Isahara 
Communic~t ion s l{esea.rch Labora.tory 
Ministry of Posts a.nd Telecommm~ications 
588-2, lwa,oka,, Nishi-ku, Kobe 6511-2/192, 3a, pa,n 
{qma, murata., uchimoto,  isa.hara}{))crl.go.jp 
Abstract  
A hybrid system R)r tagging part of speech is 
descril)ed that consists of a neuro tagger and 
a rule-based correcter. The neuro tagger is 
an initia.1--state a.nnotator tha.t uses difl'ertnt 
h_,,ngths of contexts based on longe, st context l)ri- 
ority. Its inputs a.re weighted 1)y information 
gains tha.t are obtained by information ma.xi- 
mization. The rule-1)ased correcter is construct- 
ed by a. sol; of trm~sfc)rma.tion rules to xna.ke Ul) 
for the shortcomings o\[' the nou17o tagger. Corn- 
puter experiments show that ahnost 20% of the 
errors ma.de by the neuro tagger a.re correct- 
ed by the, st trans\[orma.tion rules, so tha.t the 
hybrid system ca.n reach a.n a,tcura.cy of 95.5% 
counting only the ambiguous words and 99.1% 
counting all words when a. small Thai corpus 
with 22,311 a mbig;uous words is used t))v tra.in- 
ing. This a(;cu racy is far higher than that using 
an IIMM and is also higher tha.n that using a. 
rule-1)ased model. 
1 Introduct ion 
Many pa.rt of speech (POS) tatters  proposed 
so far (e.g., Brill, 1994; Meria.ldo, 1994; l)aele- 
marls, el. al., 1996; and Schmid, 1994) ha.re 
achieved a. high accura.ey partly because a. very 
large amount of dal,~ was used to 1;rain them 
(e.g., on the order of 1,000,000 words for \]'hl- 
glish). For ma.ny other la.nguages (e.g., Thai, 
which we treat in this paper)~ however, it is not 
as easy to cremate \]a.rge corpora from which lm:ge 
amounts of tr~fining data can be extra.cted. It is 
therefore desirable to construct a practic;d tag- 
ger tha.t needs as little training d a.t;a~ as possible. 
A multi-neuro tagger (Ma a.nd ls~hara, 11998) 
and its slimmed-down version called the ela.s- 
tic neuro tagger (Ma, el; al., 1999), which have 
high genera.lizing ability and therefore are good 
at dealing with the problems of data sp~u:se- 
hess, were proposed to satist~y this requh:ement. 
These taggers perform POS tagging using difl'er- 
ent lengths of corltexts I)~.~sed on longest context 
prk)rity, and each element of tile input is weight- 
ed with information gains (Quinla.n, 1993) for 
retlecting that tile elements of the input h~ve 
different rtlevances in t~Gging. They ha.d a tag- 
ging accuracy of 94.4% (counting only the am- 
biguous words in part of speech) in computer ex- 
periments when a. small 'l'ha.i corpus with 22,311 
am biguous words was u se(l for tr~fi n ing. This ~(:- 
curacy is bu" higher thml t\]lat; USillg tile hidden 
Marker model (IIMM), the main approach to 
\])art o\[ speech tagging, ~nd is ~dso higher t,\]lan 
tha.t using a. rule-based mode\]. 
Neuro taggers, however, htwe several crucial 
shortcomings. First, even in the case where the 
POS of a word is uniquely determined by the 
word on its left, for example, a neural net will 
also try to perlbrnl tagging based on tile com- 
plete context. As a result, even for" when the 
word on tile left; is the same, the tagging result~ 
s will be difl'erent if the complete contexts are 
different, rl'ha, t is~ the neuro tagger carl hard- 
ly acquire the rules with single inputs. Fur- 
thermore, although lexica.l in\[brma.tion is very 
ilnport~ult in t~gging, it is difficult for: neural 
nets to use it becmme doing so would make the 
network enorlnous. That is, the neuro tagger 
ca.nnot acquire (;lit rules with lexical informs> 
tion. Additionally, Imca.use of convergence and 
509 
over-training l)roblems, it is impossible and also 
not advisM)le to train neural nets to an a.ccura,- 
cy of 100%. The training should be stopped at 
an appropriate level of a.ceuracy. Consequently, 
neural nets may not acquire some usefnl rules. 
To make up for these shortcomings of the 
neuro tagger, we introduce in this pa.per a rule- 
based corrector as tile post-processor and con- 
struct a hyl)rid system. The rule-based cot- 
rector is constructed by a set of transforma- 
tion rules, which is acqnired by transforma?ion- 
based error-driven learning (Brill, 1.994:) from 
training corpus using a set of templates. The 
templates are designed to SUl)l)ly the rules that 
the neuro tagger can hardly acquire. Actual- 
ly, by examining the transformation rules ac- 
quired in the computer experiments, the 99.9% 
of them are exactly those that the neuro tagger 
can hardly acquire~ even when using a template 
set including those for generating the'rules that 
the neuro tagger can easily acquire. This rein- 
forces onr expectation that the rule-based ap- 
proach is a well-suited method to cope with the 
shortcomings of the neuro t~gger. Computer ex- 
periments hows thai; about  200/0 of errors made 
by the neuro tagger can be corrected by using 
these rules and that the hybrid system ca.n reach 
an accuracy of 95.5% counting only the aml)ign- 
ous words and 99.1% counting all words in l, he 
testing corpus, when tile same corpus described 
above is used for training. 
2 POS Tagg ing  Prob lems 
In this paper, suppose there is a lexicon V, 
where the POSs that can be served by each word 
are list.ed, and tiler(; is a set of POSs, l?. That is, 
unknown words that do not exist in the lexicon 
are not dealt with. The POS tagging problem 
is thus to find a string of POSs T = T172..-% 
(ri C F, i = 1 , - . . , s )  by following procedure 
~o when sentence W = wlw2...w.~ (wi C V, 
i = 1 , . - . , s )  is given. 
#:W ~ -+ rt, (1) 
where t is the index of the target word (the word 
to be tagged), and W t is a word sequence with 
length l + 1 + r (:entered on the target word: 
l i e  t : wt_  1 . . . .  i o  t ? . . Wt+r~ (2) 
where t - 1 > 1, t + r _< s. 'l'agging ca.n thus be 
regarded as a classification problem 1) 3, replacing 
the POS with (;lass and can therefore be handled 
by using neural nets. 
3 Hybr id  System 
Our hybrid system (Fig. J) consists of a neuro 
tagger, which is used as an initial-state an nota- 
i;or, ~nd a rule-based corrector, which corrects 
the outputs of the neuro tagger. When a word 
seque,~ce W t \[see l~q. (2)\] is given, the neuro 
tagger outl)ut a tagging result rN(Wt) for tile 
target word wt at first. The rule-based correc- 
tor then corrects the output of the neuro tagger 
as a fine tuner and gives tile final tagging result 
Ncuro Tagger Rule-Based 
Corrcctor 
Figure 1: Hybrid neuro and rule-based tagger. 
3.1 Neuro  tagger 
As shown in Fig. 2, the neuro tagger consists 
of a three-layer I)erceptron with elastic input. 
This section mainly descril)es the construction 
of inl)ut and output of the nenro tagger, and 
the elasticity by which it; becomes possible to 
use variable length of context for tagging. For 
details of the architecture of l)erceptron see e.g., 
Haykin, 1994 and for details of the features of 
the neuro tagger see Ma and isahara, 1998 and 
Ma, et aJ., 1999. 
lnl)ut I PT  is constructed fi'om word se- 
quence W t \[Eq. (2)\], which is centered on target 
word wt and has length l + 1 + r: 
I PT  = (iptt_ l , . . ' ,  i ph , . . . ,  iph+.,.), (3) 
provided that input length l+ J+r  has elasticity, 
as described a,t tile end of this section. When 
woM wis given in position x (x = t - l , . . . , t+r ) ,  
510 
OPT 
ip t t_  I ...... i l ) \[t_ I ipl t 
11"1" 
il)lt+l ...... i\]Ht+r 
Figure 2: Neuro tagger. 
element ipt ,  of input I PT  is a. weighted pattern, 
defined as 
ipt.,: = :/.,,. (e,,,,('-,, 'e,." ,q,,~), (4) 
where g;,; is the inIbrmation gain which can be 
obtained using information theory (for details 
see Ma and lsahara., 11998) and 7 is the number 
of tyl)es of POSs. l\[' w is a word that apl)ears 
in the tra.ining data, then ea.ch bit e,,,i can be 
obtained: 
= P,,ot,(/ I , , ,) ,  (s) 
where l>'rob(ril'w) is a prior l>robal)ility of r i 
tha.t (;he woM 'w can take,. It is estitnated from 
the t raining (la,ta: 
C _ (<,  "') 
c' ( ,4  ' 
where C'(r ( ,w)  is the number of lfimes both r: 
at++d w al)pea, r , a,nd C(w)  is the number oi' times 
w appears in the training data. 1\[ w is a word 
that does not at)pear ill (,he training data~ then 
each t)it c,,,,i is obtained: 
~ i\['r i is a. candidate 
(7) e.,,,," = (} otherwise, 
where 7,, is the number of P()Ss that the word 
'w Call ta.ke. Output OPT is defined as 
OFT= ,o+), (s) 
provi<led that the output OI)T is decoded as 
S r/ i fO i= l  &O/=0for j? i  
YN(Wt) Unknown otherwise, 
(.9) 
where rN(W,) is the ?a.gging result obtained by 
the neuro tagger. 
There is more inforlnation available for con- 
structing the input for words on the left, be- 
cause they have already been tagged. In the 
tagging phase, instead of using (4:)-(6), the in- 
put can be constructed simply as 
i / , ,_4 = . oPT( - i ) ,  (1()) 
where i = 1, . . . ,1,  and O I )T ( - i )  means the out- 
put of the tagger for the ith word before the 
target word. ltowever, in the training process, 
the out;put of the tagger is not alway.a correct 
a.nd cannot be ted back to the inputs directly. 
Instead, a weighted awerage of the actual output 
a.nd tlm desired output is used: 
iptt_i = 9t- i  ' (wol ,T " 0 PT  ( -  i) + WlOJ,:s " I) l iS) ,  
(1.1) 
where 1)l':,q' is the desired output, 
o: , : ,5 '  : (& , / )2 , . . . ,  
whose bits are defined as 
\] iI' r i is a desired answer 
I)i = 0 otherwise, (la) 
and WOl,'r and w/)l,\],q' are respecLh:ely de\[(ned as 
1'\]013J 
~,:o1 '~ . . . .  (.14) 
1JACT '  
a,nd 
'w>l,; ,s,  = :1 - wopT ,  (15) 
where \]@,uo and \]'JAC'T are  the objective and 
actual errors. Thus, at the beginning of train- 
ing, the weighting of the desired output is largo. 
It decreases to zero during training. 
Elastic inputs are used in the neuro tagger 
so that the length of COlltext is variable in tag- 
ging based on longest context priority. In (te~ 
tail, (l, r) is initially set as large as possible for 
tagging. If rN('Wi) = Unknown,  then (1, r) is 
reduced by some constant interval. This l)ro- 
cess is repeated until rN(W~) 7 k Unknown or 
(1, r) = (0,0). On the other hand, to nmke the 
same set of connection weights of the neu ro tag- 
ger with the largest (1,'r) ava.ilable a.s lnuch as 
511 
possible when using short inputs for tagging, in 
training phase the neuro tagger is regarded as a 
neural network that has gradually grown fi'om 
small one. The training is therefore performed 
step by step from small networks to large ones 
(for details see Ma, et al 1999). 
3.2 Rule-based eorreetor 
Even when the POS of a word can be deter- 
mined with certainty by only the word on the 
left, for example, tile neuro tagger still tries to 
tag based on the complete context. That is, 
in general, what tile neuro tagger can easily 
acquire by learning is the rules whose condi- 
tional parts are constructed by all inpttts ip tx  
(x = t - l , . . . , t  + r) that are .joined with all 
AND logical operator, i.e., ( ip t t - t  & "'" iptt  & 
? .. iptt+~, -+ OPT) .  In other words, it is (lit: 
ticult for tile neuro tagger to learn rules whose 
conditional parts are constructed by only a sin- 
gle input like ( ipt,.  --+ OPT)  ~). Also, although 
lexical information is very important in tagging, 
it is difficult for the neuro tagger to use it, be- 
cause doing so would make the network euof  
mous. Tha.t is, the neuro tagger cannot acquire 
rules whose conditional parts consist of lexical 
information like (w -4 OPT) ,  (w&r  -4  OPT) ,  
and (w~w2 --+ OPT) ,  where w, Wl, and w2 are 
words and 7- is tile POS. Furthermore, because 
of convergence and over-training 1)rol)lems, it is 
iml)ossible and also not advisable to train net> 
ral nets to all accuracy of 100%. The training 
should be stopped at an apl)rol)riate level of a.c- 
curacy. Thus, neural net may not acquire some 
useful rules. 
The transfbrmation rule-based corrector 
makes up for these crucial shortcomings. 
The rules are acquired Dora a training co l  
pus using a set of transformation templates 
by transformation-based rror-driven learning 
(Brill, 1994). Tile templates are constructed 
using only those that supply the rules that tile 
nenro tagger can hardly acquire, i.e., are those 
1)The neuro tagger can also learn this kind of rules 
because it can tag tile word using only ipt, (the input 
of tile target word), ill the case of reducing tile (I, r) to 
(0,0), as described in Sec. a.l. The rules with single 
input described here, however, are a more general case, 
ill which the input call be ipt,~ (~: = t - 1, . . . ,  t + r). 
for acquiring the rules with single input, with 
lexical information, and with AND logica.1 in- 
put of POSs and lexical information. The set of 
templa,tes i  shown in Table 112). 
According to the learning procedure shown 
in Table 2, an ordered list of transformation 
rules are acquired by applying the template set 
to a training corpus, which had ah'eady been 
tagged by the neuro tagger. After tile trans- 
formation rules are acquired, a corl)us is tagged 
as tbllows. It is first tagged by the neuro tag- 
ger. The tagged corpus is then corrected by 
using the ordered list of transformation rules. 
The correction is a repetitive process applying 
the rules ill order to the corptlS, which is then 
updated, until all rules have been applied. 
4 Exper imenta l  Results  
Data :  For our computer experiments, we used 
tile same Thai corpus used by Ma et al (1999). 
Its 10,d52 sentences were randomly divided in- 
to two sets: one with 8,322 sentences for trail> 
ing and the other with 2,1.30 sentences for test- 
int. The training set contained 12d,331 word- 
s, of which 22,311 were ambiguous; the testing 
set contained a4,5~14 words, of which 6,717 were 
ambiguous. For training tile n euro tagger, only 
the ambiguous wor(ls in the. training set were 
used. For training the HMM, all tilt words in 
the training set were used. In both cases, all the 
words in tile training set were used to estimate 
Prob( r i lw) ,  tim probability of "c i that wor(I w 
can be (for details on the HMM, see Ma, et al, 
1999). In the corpus, 4:7 types of POSs are de- 
fined (Charoenporn et al, 1997); i.e., 7 = 47. 
Neuro tagger: The neuro tagger was con- 
structed by a three-layer perceptron whose 
input-middle-outI)ut layers had p z, 2 7 units, 
respectively, where p = 7 ? (1 + I + r). The 
(l + 1 + r) had tile following elasticity. In train- 
ing, tile (I, r) was increased step by step as (71,1) 
-+ (u,2) (a,2) (a,a) a,d gra,dual 
training fl'om a small to a large network was 
pertbrmed. Ill tagging, on the other hand, the 
2)To see whether this set is suitable, a immloer of ad- 
ditional experiments were conducted using various sets 
of templates. The details are described in Sec. 4. 
512 
r ~ I a,1)lc 1: Set o\[' templa.tes for tra.ns\[orln;~tion rules 
Change t;ag v a to t;ag v ? when: 
(single inlm|;) 
( input ('onsists of a POS) 
1. left (right) word is tagged v. 
2. second left (right) word is tagged r. 
3. third left (right) word is ta.gged r. 
(inI)n|; consist;s of a word) 
4. ta.rget word is ~. 
5. left (right) word is w. 
6. second left, (right) word is w. 
(AND logical inpu? ot" words) 
7. l, arget word is 'UO 1 ~tlld left (right) word is wu. 
8. left; (right) word is u,1 and second lcfl, (,'ight) word is w2. 
9. left, word is w~ a.nd right, word is 'wu. 
(AND logical in.lint; of POS and words) 
10. ta.rget word is uq and left (right) word is llaggod r. 
:11. left (righl;) word is .w~ and left. (right) word is tagged r. 
12. ta.rget word is w~, left (right) word is ,w.,, and left (right) word is tagged r. 
Ta,1)le 2: l)roetdure for learning transi'orma,tion rules 
1. Apply neuro taggtr to training corpus, which is then updated. 
2. Compare tagged results with desired ones and find errors.  
3. Ma.teh templates l'or all errors and obtain set of tra.nsformation rules. 
d. Select rule in corpus with the maximum value of' (cn l , _qood-h .  cnl,_bad), where 
cnZ_qood: number that transforms incorrect ags to correct elliS: 
c'nl._bad: number that transforms correct tags to incorrect ones, 
h: weight to control the strict, hess of generating 1;he rule. 
5. Apply selecttd rule to training corpus, which is then updated. 
6. Append selected rule to ordered list o1" trausl'orma.tion rules. 
7. Ih'4)eal; steps :2 through (j until no such rule can I)e selected, i.e., c 'n , t _good-  
h,. cnl,_bad < O. 
(l, 'r) was inversely reduce(l ste l) by step as (3,3) 
-+ (3,2) (2,2) (2,:1) O,:l) (:l,o) 
(0,0) a.s needed, provided tJlat the number of 
units in the middle layer was kept a.t the ma.xi- 
I l l  l l l l l  vahle. 
Ru le -based  cor re t to r :  The parameter h in 
the tw~Juat;ion function (cnl ,_9ood - h, . c'M._bad) 
used in 1;he learning procedure (Table 2) is a 
weight to control the strictness of generating a. 
rule. IF It is large, the weight of cnt_bad is la.rge 
and the possibility of generating incorrect rules 
is reduced. By regarding the neuro tagger as ~d- 
ready having high accuracy and using tile rule-- 
based correcter as a fine tuner, weight h. was set 
to a. large vahm, 100. Applying |;lit templates 
Co the training corptm, which had already been 
tagged 1) 5, the neuro ta.gger, we obta.ined a.n or- 
dered list; of 520 transfbrmation rules. '.l'~d)le 3 
shows the first 15 transfbrmation rules. 
Results :  Table 4 shows the resull;s of I)()S tag- 
ging for the testing data.. In addition to the 
accuracy o\[" the neuro tagger and hybrid sys- 
tem, the ta.ble also shows tile accuracy of a, bast- 
line model, the IIMM, and a rule-based model 
\['or comparison. The baseline model is one that 
performs tagging without using the contextual 
inlorma.tion; instead, it performs ta.gging using 
only f'requency informa.tion: the proba.bility of 
P()S that; tach word can be. The rule-based 
model, to be exact, is also a hybrid system con- 
513 
'l'a.1)le 3: First 15 transfbrmation rules 
No. F rom To  Cond i t i on  
1 PREL 
2 PREL 
3 Unknown 
4 XVHI4 
5 VATT 
6 Unknown 
7 NCI4N 
8 VATT 
O PREL 
i0 VST~ 
ii VfiTT 
12 NCMN 
13 NCHN 
14 Unknown 
15 NCNN 
RPRE le f t  word is punctuation and r ight  tuord is 5~gu 
RPRE le f t  yard is ~ 
RDVN le f t  ~ord Ls tagged XVfiE 
XVBH le f t  word is II~D 
flDVN le f t  word is  ~ 
VRTT le f t  word is  tagged PREL 
RPRE le f t  word is ua 
VSTfi l e f t  word is ~q~ 
RPRE r ight  word is ~gu and second r ight  word is a~q;J 
RDVN target word is ~t~4 
ADVN target  word is  ~4~ 
RPRE target word is n14 and le f t  word is eentluu 
RPRE le f t  word is ~tt and le f t  ward is tagged NCHN 
fiDVN th i rd  le f t  word is tagged WCT 
CNIT taPget ~ord is nn~ 
where PREL: Relat ive Pronoun, RPRE: Preposit ion,  fiDVN: fidverb with normal form . . . .  
Table d: Results of POS ta,gging for testing data* 
model baseline IIMM rule-based lleuro hybrid 
accuracy 0.836 0.891 0.935 0.944 0.955 
*Accurac9 was determiued only for am lfiguous words. 
sisting of an initial-state annotator and a set of 
transformation rules. As the initial-state anno- 
b~tor, however, the baseline model is used in- 
stea.d of' the neuro tagger. And, its rule set. has 
1,177 transformation rules acquired h'om a more 
general teml)late set, which is described at the 
end of this section. The reason for using a gener- 
al template set is that the sol; of tra.nsibrma.tion 
rules in the rule-based model should be the main 
annotator, not a fine post-processing tuner. For 
the same reason, the parameter to control the 
strictness of generating a rule, h, was set to  a 
small value, \], so that a larger number of rules 
were generated. 
As shown in the table, the accuracy of the 
nenro tagger was far higher than that of the 
HMM and higher than that of the rule-based 
model. The accuracy of the rule-based mod- 
el, on the other hand, was also far higher than 
that of the IIMM, ~lthough it was inferior to 
that of the neuro tagger. The accuracy of the 
hybrid system was 1.1% higher than that of the 
neuro tagger. Actually, the rule-based corrector 
corrected 88.4% and 19.7% of the errors made 
by the neuro tagger for the training and testing 
data, respectively. 
Because the template set shown in Table 1 
was designed only to make up for the short- 
comings of the neuro tagger, tile set is smal- 
l compared to that used by Brill (1994). To 
see whether this set is la.rge enough for our sys- 
tem, we perlbrmed two additional experiments 
in which (\]) a sol; constructed 193' adding the 
templates with OR logical input of words to the 
original set and (2) a, set constructed 1)5' fnrther 
adding the templates with AND and OR logi- 
cal inputs of POSs to the set of case (1) were 
used. The set used in case (2) inclnded the set 
used by Brill (\]994) and all the nets nsed in our 
experiments. It was also used for acquiring the 
transformation rules in the rule-based model. 
The experimental results show that compared 
to the original case, the accuracy in case (1) 
was improved very little and the accuracy in 
case (2) was also improved only 0.03%. These 
results show that the original set is nearly la.rge 
enough for our system. 
To see whether tile set is snitable tbr our 
system, we performed ~tn additional experimen- 
t using the original set in which the templa.tes 
with OR logical inputs were used instead of the 
templates with AND logical inputs. The accu- 
racy dropped by 0.1%. Therefore, tile templates 
with AND logical inputs are more suitable than 
514 
those with O11 logical inputs. 
We also performed an experiment using a 
template set without lexical intbrmation. In this 
case, l;he accuracy dropl)ed by 0.9%, indicating 
that lexical informatioll is important in tagging. 
To determine the effect o1' using a. large h, 
for generating rules, we per\['ormed an experi- 
ment with h = 1. In this case, the accuracy 
dropped by only 0.045%, an insignifica.nt differ- 
ence compared to the case of h, = 100. 
By examining the acquired rules that were 
obtained by al)plying the most COml)lete tem- 
plate set, i.e., the set used in case (2) described 
above, we found that 99.9% of them were those 
that can be obtained by a.pl)lying the original 
set of templates, rl'ha.t is, the acquired rules 
were almost those that are dif\[icult \['or the neu- 
re tagger to acquire. '.l'his rein forced our expec- 
tat;ion that the rule-based al)l)roach is a well- 
suited method to cope with the shortcoming of 
the neuro tagger. 
Finally, il, should 1)e noted that ill the liter- 
atures, tile tagging a.ccuracy is usua.lly delined 
by counting a.ll tile words regardless of whether 
they are a.nlbiguous or not. If we used this dell- 
nil:ion, t\]le accura.cy of our hybrid system would 
be 99.1%. 
5 Conc lus ion  
To collstruct a 1)tactical tagger that needs as 
little training data. a.s possible, neuro taggers, 
which have high generalizing al)ility and there- 
fore a.re good at dealing with the problems ofda~ 
ta. sl)a,rseness, have been proposed so fa.r. Neu- 
re tatters,  however, have crucial shortcomings: 
they ca.nnot utilize lexical information; they 
have trouble learning rules with single inputs; 
and they cannot learn training data to an ac~ 
curacy of 100%. To make up for these short- 
comings, we introduced a rule-based correcter, 
which is constructed by a. set of trans\[brma.tion 
rules obtained by error-driven learning, for post 
1)recessing and constructed a hybrid tagging 
system, l{y examining the transtbrma.tion rules 
acquired in the computer experiments, we found 
that 1;he 99.9% of them were those that; the neu- 
re tagger can hardly acquire, even when using a. 
template set including t;hose for generating the 
rules that the neuro tagger can easily acquire. 
This reinlbrced our expecta.tion that the rule- 
based approach is a well-suited method to cope 
with the shortcoming of the neuro tagger. Com- 
puter experiments showed that 19.7% of the er- 
rors made by the neuro tagger were corrected 
by the tra.nslbrmation rules, so the hybrid sys- 
tem rea.ched an accuracy of 95.5% counting only 
the ambiguous words and 99.\]% counting all the 
words in the testing data, when a small corpus 
with only 22,311 ambiguous words was used tbr 
train int. ~l'h is ind icates thai; ou r tagging ,qystem 
can nearly reach a pra.ctica.l level in terms of tag- 
ging accuracy even when a small Thai corpus is 
used tbr tra.ining. This kind of tagging system 
can be used to constructs multilingua.1 corpora 
that include languages in which large corpora 
have not yet been constructed. 
References  
l~rill, E.: Transfornmtion-based rror-driven lca.rn- 
ing and natural language processing: ~ case s- 
tudy ill 1)art-of-sl)eech tagging, Computational 
Li~g'uistics, Vol. 21, No. 4, pp. 543-565, 199~1. 
Cha.roenporll, T., Sornlertlanlva.nich, V., ~md Isa- 
hara, 11.: Building a la.rge Thai text corpus 
parl; of speech tagged corpus: OI{CIlll), Pro< 
Natural Language Processi~fl Pacific lNm ,5'gn~- 
po.du'm \[997, Phuket, Thailand, pp. 509-5\]2, 
1997. 
I)aelemans, W., Z~wrel, a., Berck, P., and C,i/lis, S.: 
MI3'I': A m<mlory-based pm-t of speech tagger- 
genera.tot, P'roc. /tl.h Workshop on Very Large 
Co,'po~zl, Copenhagen, l)em na.rk, pp. 1-1+1, 99(5. 
l\]aykin, S.: Neural Nchvorlcs, Macmillan College 
Publishing Coral)any, Inc., 199/t. 
Ma, Q. and lsahm'a., H.: A multi-neuro tagger us- 
ing variable lengths of contexts, Prec. COLING- 
ACL'g8, Montreal, pp. 802-806, 1998. 
Ma, Q., Uchimoto, K., Mura.ta, M., and 1sahara H.: 
F, lastic neural networks tbr part of speech tag: 
ging, Prec. IJCNN'99, Washington, \])C., pp. 
2991-2996, 1999. 
Meriaklo, B.: Tagging English text with a proba- 
bilistic model, Computational Linguistics, Vo\]. 
20, No. 2, pp. 1.55-171, 19(.)4. 
Quinla.n, 3.: G'~.5: Programs Jot Machine Learning, 
San Mateo, CA: Morgan Kaufinann, 1993. 
Schmid, 1t.: l'art-of-speech tagging with neural net- 
works, Prec. COLING'94, Kyoto, Japan, pp. 
172-176, 1994. 
515 
Bunsetsu  Ident i f i ca t ion  Us ing  Category -Exc lus ive  Ru les  
Masaki Murata Kiyotaka Uchimoto Qing Ma Hitoshi Isahara 
C()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications 
588-2, \]waoka, Nishi-ku, Kobe, 651-2d92, Japan 
? - j  ( ' . tel:-k81- 78-969-2 \]81 tax: +81- 78-369-2189 http://www-karc.crl, go.j p/ips/murata 
{ murata,u(:himoto,qma,isahara}(@crl.go.ji) 
Abstract 
This pal>or describes two new bunsetsu identificatkm 
methods using supervised learning. Sin(:e ,Jat)anese 
syntactic analysis ix usnally done after bunsetsu 
identification, lmnsetsu identiiieation is iml)orl;ant 
for analyzing Japanese sentences. In experiments 
comparing the four previously available machine- 
learning methods (decision tree, maximmn-entropy 
method, example-based apI)roaeh and deeiskm list,) 
an(l two new methods llSing categot'y-exclusive rul s~ 
the new method using l;he category-exclusive rules 
with the highest similarity t)erformetl best. 
1 Introduction 
This paper is about machine learning methods for 
identifying bwnsr'ts'~zs, which correspond to English 
phrasal units such as noun phrases and t)rel)ositional 
phrases. Since .Japanes(.' syntactic analysis ix usu- 
ally done after lmnselisu identitication (Uchimot;o el; 
a\].. 1999), i(lentitlying lmnsetsu is important l'or an- 
alyzing ,J;~p~tnese ltt(}tl(:es. The  conventional stud- 
ies on lmnsetsu  identitieation ~ have used hand-made 
rules (Kameda, \]995; Kurohashi, 3998), })ill; bun- 
sel;su identification is not an easy task. Conventional 
studies used many hand-nmde rules develot)ed at the 
cost of many man-hours. Kurohashi, tbr examl)le, 
made 146 rules for lmnsetsu identification (Kuro- 
hashi, 1998). 
Itl }l.tl a . t te t l lp t  to reduce the mnnber of man- 
hours, we used machine-learning methods for bun- 
setsu identitication. Because it; was not clear which 
machine-learning method would 1)e the one most al)- 
propriate for bunsctsu identification, so we tried a 
variety of them. In this paper we rei)orl; ext)er- 
inlel lts comparing tbur inachine-learning me, thods 
(decision tree, maximmn entropy, example-based, 
and decision list; methods) and our new methods us- 
ing category-exclusive rules. 
l lhmsetsu ideni,illcation is a ln'oblem similar to ohm,king 
(lLamshaw and Marcus, 1995; Sang and \h;ellsl;ra, 1999) in 
other l;mguages. 
2 Bunsetsu identification problem 
We conducted experiments (m the following super- 
vised learning methods tbr idel~tiflying }mnsetsu: 
? \])eeision {;l'ee method 
? Max i lnun l  ent ropy  method  
? Examt)le-based method (use of sinfilarity) 
? Decision list (use of probability and frequency) 
? Method 1 (use of exclusive rules) 
? Method 2 (use of exclusive rules with the high- 
est similarity). 
In general, t)misetsu identification is (tone afl;er 
morl)hological and l)efore syntactic analysis. Mor- 
1)hological analysis correst)onds to part-of-st)ee(:h 
tagging ill English. Japanese syntactic structures are 
usually ref)resented by the. relations between lmn- 
setsus, which correspond to l)hrasal units such as a 
noml l)hrase or a t)repositional 1)hrase in \]r, nglish. 
St), 1)unsetsu identification is imi)ortant in .lnpanese 
sentence mmlysis. 
In this paper, we identit\[y a bunsetsu by using 
intbrmation Dora a morl~hological nalysis. Bun- 
setsu identitication is treated as the task of deciding 
whether to insert a "\[" mark to indicate the partition 
between two hunsetsus as in Figure 1. There, fore, 
bunsetsu identilical;ion is done by judging whether a
partition mark should be inserted between two adja- 
cent nlorphemes or not. (We. do not use l;he inserted 
partition mark in the tbllowing analysis ill this paper 
for the sake of simplicity.) 
Our lmnsetsu identification method uses i;t1(} lilOr- 
phok)gk:al intbrmation of the two preceding and two 
succeeding morphemes ofan analyzed space bel;ween 
two adjacent morphemes. We use the following mor- 
phological information: 
(i) Major part-of  speech (POS) category, 2 
(ii) Minor P()S category or intlection tYl)e, 
(iii) Semantic information (the first three-digit nun> 
bet of a category nmnlmr as used ill "BGIt" 
(NLI{,I, 1964:)), 
2Part-of-spec.ch ~ttegories follow those of 3 \[/MAN (Kuro- 
hashi and N~tgao, 1998). 
565 
bohu .qa 
(I) nominative-case particle 
(I identify bunsetsu.) 
\[ bunsetsu wo 
(bunsetsu) objective-case particle 
I matomeagcru 
(identify) 
Figure 1: Example of identified bunsetsus 
Major POS 
Minor POS 
Semantics 
Word 
bun wo ~ugiru 
(sentence) (obj) (divide) 
((I) divide sentences) 
Noun Particle Verb 
Normal Noun Case-Particle Normal Form 
x None 217 
x wo ku.qiru 
Symbol 
Punctuation 
X 
X 
Figure 2: hfformation used in bunsetsu identification 
(iv) Word (lexical iifformation). 
For simplicity we do not use the "Semmltic infor- 
matioif' and "Word" in either of the two outside 
morphemes. 
Figure 2 shows the information used to judge 
whether or not to insert a partition mark in the space 
between two adjacent morphemes, "wo (obj)" and 
"kugiru (divide)," in the sentence "bun wo kugiru. 
((I) divide sentences)." 
3 Bunsetsu  ident i f i ca t ion  process  fo r  
each  mach ine- learn ing  method 
a.1 Deeision-tree method 
In this work we used the program C4.5 (Quinlan, 
1.995) for the decision-tree l arning method. The 
four types of information, (i) major POS, (ii) mi- 
nor POS, (iii) semmltic information, and (iv) word, 
mentioned in the previous section were also used 
as features with the decision-tree l arning method. 
As shown in Figure 3, the number of features is 12 
(2 + 4 + 4 + 2) because we do not use (iii) semantic 
information and (iv) word information from the two 
outside morphemes. 
In Figure 2, for example, the value of the feature 
'the major POS of the far left morpheme' is 'Noun.' 
a.2 Maximum-entropy method 
The maximum-entropy method is useful with sparse 
data conditions and has been used by many re- 
searchers (Berger et al, 1996; Ratnaparkhi, 1996; 
Ratnaparkhi, 1997; Borthwick el; al., 1998; Uchi- 
moto et al, 1999). In our maximuln-entropy exper- 
iment we used Ristad's system (Ristad, 1998). The 
analysis is performed by calculating the probability 
of inserting or not inserting a partition mark, from 
the output of the system. Whichever probability is 
higher is selected as the desired answer. 
In the maximum-entropy method, we use the same 
four types of morI)hological information, (i) major 
POS, (ii) minor POS, (iii) semantic information, and 
(iv) word, as in the decision-tree method. However, 
it, does not consider a combination of features. Un- 
like the decision-tree method, as a result, we had to 
combine features mmmally. 
First we considered a combination of the bits of 
each morphological information. Because there were 
four types of information, the total number of com- 
binations was 2 ~-  1. Since this number is large 
and intractable, we considered that (i) major POS, 
(ii) minor POS, (iii) semantic information, aim (iv) 
word information gradually becolne inore specific in 
this order, and we coml)ined the four types of infor- 
mation in the following way: 
Information A: (i) major POS 
Intbrmation B: (i) major POS and (ii) minor POS 
hfformat, ion C: (i) major POS, (ii) minor POS and 
(iii) semantic information 
Information D: (i) major POS, (ii) minor POS, 
(iii) semantic informa~aion a d (iv) word 
(~) 
We used only Information A and B for the two out- 
side morphemes because we (lid not use semantic 
and word information in the same way it is used in 
the decision-tree inethod. 
Next, we considered the combinations ofeach type 
of information. As shown in Figure 4, the number 
of combinations was 64 (2 x 4 x 4 x 2). 
For data sparseness, in addition to the above com- 
binations, we considered the cases in which frst, one 
of the two outside morphemes was not used, sec- 
ondly, neither of the two outside ones were used, m~d 
thirdly, only one of the two middle ones is used. The 
nmnber of features used in the maximum-entropy 
method is 152, which is obtained as follows: a
3When we extr~,cted features  f rom all of  the  ar t ic les  on  
566 
Far M't mort)henm Left morl)heme 
( Ma.ior POS '} 
~'Major POS'\[ ,J Minor POS 
I, Minor POgJ +/S,mmntic Information + 
( \?ord , 
2 4 
Right morl)hemc Far right mort)heine 
Minor POS f Ma.ior POS ~ 
Semantic Infi)rmation ' + \[ Minor POS j 
Word 
4 2 
Figure 3: Features used in the decision-tree method 
Far left morl)heme 
Information 
hffbrnmtion A } 
2 
Left morpheme Right morl)hcme 
( In format ion!} (Infbrmation A
J hfformation J Intbrmation B
/hfformation & ~ hfformation C
I, Infbrmation 1, hffbrmation D
4 4 
Far right morpheme 
J" Information 
& \[hlformation B A } 
2 
Figure 4: lJ~,atm'es used in the maximunt-entrol)y met.hod. 
No. of t>atures= 2 x 4 x 4 x 2 
+2 x 4 x 4 
+ 4 x 4 x 2 
+ 4 x 4 
+ 4 
+ 4 
= 152 
In Figure 2, (;lie feature that uses Infornultion 
B in the far left morl)heme, Infbrnmtion D in the 
left mort)heine, Information C in the right mor- 
pheme, and Information A in the fa.r right mop 
l/heme is "Noun: Nornml Noun; Particle: Case- 
Particle: none: wo; Verl): Nornml Form: 217; Sym- 
bol". In tim maximmn-entrol)y method we used for 
each space 152 ligatures uch as this ()tie. 
3.3  Example -based  method (use of  
s imi lar i ty)  
An example-based method was t)rollosed t) 3, Nagao 
(Nagao, 1984) in an attempt to solve I)roblenls in 
machine translation. To resolve a. l)rol)h'm, it; uses 
the most similar (;xami)le. In the i)resent work, the 
examt)le-1)ased method imt)artially used the same 
four types of information (see Eli. (1)) as used in 
the maxinmm-entrotly method, 
To use tills method, we must define the similarity 
of an ini)ut to an example. We use the 152 1)atterns 
fl'om the maximum-entropy method to establish the 
level of similarity. We define the similarity S be- 
tween all input and an exmnl)le according to which 
one of these 152 levels is the lnatching level, as fol- 
lows. (The equation reflects the importance of the 
two middle morphemes.) 
January 1, 1995 of a Kyoto University corpus (l;hc mnnber of 
spaces between mrl)henms was 25,81d) by using this method, 
the nunfl)e,r of types of features was 1,534,701. 
S = s(m_t) x s(m-H) x 10,000 
+ s(m_2) x s(.q.~) (2) 
Here m- l ,  m4-\], m-2, and m+2 refer respectively to 
the left;, rigid;, far M't, ;rod far righl; mortflmnms, and 
s(x) is the mort)hological similarity of a ll lOl't)hell le 
x, which is defined as follows: 
s(x) =1 (when no information of x is matched) 
2 (when Information A of x is matdmd) 
3 (when hfl~)rmal:ion B of x is mate, heal) 
4 (when Information C of x is mat;cited) 
5 (when Information D of x is matched) 
(a) 
Figure 5 shows an exmnple of the levels of sim- 
ilarity. When a pattern matches Information A of 
all four lnort)henies , uch as "Noun; Particle; Verb; 
Symbol", its similarity is 40,004 (2 x 2 x 10,000 + 
2 x 2). When a pattern matches a pattern, such as 
" ; Particle: Case-Particle: none: wo; ; ", its 
similarity is 50,001 (5 x 1 x 10,000 + 1 x 1). 
The exmnl)le-1)ased method extracts the exam- 
ple with the highest level of similm'ity and checks 
whether or not that exami)le is marked. A partition 
marl{ is inserted in tile input data only when the ex~ 
amt)le iv marked. When multit)le exalnl)les have the 
same highest level of similarity, the selection of tile 
best example is ambiguous, hi this case, we count 
tile number of nlarked and mlinarked sl)aces in all 
of the examples and choose the larger. 
a .4  Decis ion- l ist  method (use of p robab i l i ty  
and f l ' equeney)  
T i le  decision-list method was proposed by Rivest 
(Rivest, 1987), in which tile rules are not expressed 
as a tree structure like in the decision-tree method, 
567 
No information 
hffornmtion A
Information B
Information C
Information D
bun wo kugiru 
(senWnce) (obj) (divide) 
S(X) ?~\]'- 2 7D,--1 ?/Zq-1 '11~'+2 
1 . . . .  
2 Noun Particle Verb Symbol 
3 Normal Noun Case-Particle Normal Form Punctuation 
4 x None 217 x 
5 x wo kugiru x 
Figure 5: Exmnple of levels of similarity 
but are expanded by combining all the features, and 
are stored in a one-dimensional list. A priority or- 
der is defined in a certain way and all of the rules 
are arranged in this order. The decision-list method 
searches for rules Dora tile top of the list and an- 
alyzes a particular problem by using only the first 
applicable rule. 
In this study we used ill the decision-list method 
the same 152 types of patterns that were used in/;lie 
ma.ximuln-entropy method. 
To determine the priority order of the rules, we re- 
ferred to Yarowsky's method (Yarowsky, 1994) and 
Nishiokwama's method (Nishiokaymna et al, 1998) 
and used the probability a.nd frequency of each rule 
as measures of this priority order. When nnlltiple 
rifles had the same probability, the rules were ar- 
ranged in order of their frequency. 
Suppose, for example, that Pattern A "Noun: 
Normal Noun; Particle: Case-Particle: none: wo; 
Verb: Normal Form: 217; Symhol: Punctuatioif' 
occurs 13 times in a learlfing set and that tell of 
the occurrences include the inserted partition Inal:k. 
Suppose also thai; Pattern B "Noun; Particle; Verb; 
Symbol" occurs 12a times in a learning set and that 
90 of the occurrences include the mark. 
This exmnple is recognized by the following rules: 
Pattern A ~ Partition 76.9% (10/ 13), Freq. 23 
Pattern B => Partition 73.2% (90/123), Freq. 123 
Many similar rules were made and were then listed 
in order of their probabilities and, for any one prob- 
ability, in order of their frequencies. This list was 
searched from tile top ml(:l the answer was obtained 
by using the first, ai)plicable rule. 
3.5 Method  I (use of eategory-exe lus ive  
rules) 
So far, we have described the four existing machine 
learning methods. In the next two sections we de- 
scribe our methods. 
It is reasoimble to consider tile 152 patterns used 
in three of the previous methods. Now, let us sup- 
pose that the 152 patterns fl'om the learning set yield 
the statistics of Figure 6. 
"Partition" means that the rule determines that a 
partition mark should be inserted in the input data 
and "non-t)arl:ition" ineans that tile rule determines 
that a partition mark should not be inserted. 
Suppose that when we solve a hypothetical prob- 
lem Patterns A to G are apt)licable. If we use the 
decision-list inethod, only Rule A is used, which is 
applied first, and this determines that a partition 
mark should not be inserted. For Rules B, C, and 
D, although the fl'equency of each rule is lower thml 
that of Rule A, tile suln of their frequencies of the 
rules is higher, so we think that it is better to use 
Rules B, C, ml(t D than Rule A. Method 1 follows 
this idea, but we do not simply sum up tile frequen- 
cies. Instead, we count the munber of exalnples used 
ill Rules B~ C, and D and judge the category having 
tile largest number of exmnplcs that satisfy the pat- 
tern with the highest probability to be the desired 
ai1swer. 
For exmnple, suppose that in the above examt)le 
the number of examples atis(ying Rules B, C, and 
D is 65. (Because some exmnples overlq) in multi- 
pie rules, the total nunfl)er of exalnples is actually 
smaller than the total number of tile frequencies of 
the three rules.) In this case, among the examples 
used by the rules having 100% probability, tile nmn- 
ber of examples of partition is 65, m~d the number 
of examt)les of non-t)artitioi~ s 34. So, we deternline 
that tile desired answer is to partition. 
A rule having 100% probability is called a 
category-exclusive rule because all the data satist~y- 
ing it belong to one category, which is either parti- 
tion or noi>partition. Because for any given space 
the number of rules used call be as large as 152, 
category-exclusive rules are applie(t often ~. Method 
1 uses all of these category-exclusive rules, so we call 
it tile method using category-exclusive rules. 
Solving problems by using rules whose prol)abili- 
l;ies are nol; 100% may result ill the wrong solutions. 
Almost all of the traditional machine learning meth- 
ods solve problelns by usiug rules whose i)robabilities 
4'l'he ratio of the spaces analyzed by using category- 
exclusive rules is 99.30% (16864/16983) in Experiinent 1 of 
Section d. This indicates that ahnost all of the spaces are 
analyzed by category-exclusive rules. 
568 
\]{,uIe A: 
l{,uh.' B: 
Rule C: 
Rule D: 
Rule E: 
Rule F: 
Rule G: 
l 'attern A 
Pattern \] 
Pattern C 
Patl;crn \]) 
Patt;ern 1'3 
Pal;l;erll F 
Pat tern  G 
-?- I)rol)ability of non-i)al|;ition 
=> probability of partition 
=> i)rolml)ility of partition 
=> probal)ility of 1)artition 
:~ i)robability of partition 
:~ probability of parl:ition 
=> probability of non-partition 
m0% (a4/34)  
100% (,33/a3) 
\]oo% (25/2s)  
J()0(X, ( 1!)/ 19) 
8J.3% (1?)?}/123) 
76.9% ( 10/ ~a) 
57.4% (31{)/540) 
Figm'e 6: ;Ill (',Xaml)h; of rules used in Method :l 
1,?equency 34 
Frequency 33 
Frequency 25 
Frequency 19 
Frequcncy 123 
Fl'o, qucncy \] 3 
lq'equc, ncy 540 
are not J(}0%. By using such methods, we cannot 
hol)e to improve, a,:curacy. If we want to improve ac- 
(;llra(;y~ we nlllst use catt;gory-excltl,qive l'lllCS. The l 'e  
are some eases, however, tbr which, even if we take 
this at)l)r()ach, eategory-exchlsive rules aye rarely al> 
plied. In such cases, we lilllSl; add new feai;ures t() 
I;he mlalysis to create a situation in which many 
c i~tegory -exeh ls ive  ru les  Call \])(; appli(~d. 
I\]owever, il; is not suflieient to use  t ;~/tt~ory- 
exclusive rules. There arc, many nmaningless rules 
which \]la,1)l)ell to  1)e c~tl;egor3~-ex(;lll,qive o l l ly  ill ~t 
learning set. We lllllSt consider how to (~Iimim/te 
such meaningh;ss rule,q. 
3.6  Method  2 (r ising category -exc lus ive  
ru les  w i th  the  h ighest  sint i lar it ;y) 
Method 2 combines the, exami)h>based method and 
Method 1. That  is, it; combines the. method using 
similarity m~d the method usint'~ category-exchlsive 
rules in order to eliminate the meaningless (:art;gory- 
exclusive rules ment;ion(;(l i l lhe 1)revious t;el;ion. 
Mel;ho(1 2 also uses 152 patl(!rus for i(tentillving 
\])llllS(',|;,qll. q~h(!s(, ~ t)~li;l,(!l'll~q ~/l'(? llF,(RI ;IS rules i~ the 
,q;lllle ~,\;;ty }is ill Met\ ] lo( \ ]  \]. l)esir('d HIISWtH',q ill'(; (h;t(; l-  
mined by using the rule. having the high(>t probabil- 
ity. When mull;iple rules have the same 1)rolmbility, 
M(;thod 2 uses the wdue of the similarity described 
in the section of the examl)h>based m(;thod and }lll- 
alyzes the 1)robk:m with the rule having the highest 
simihu'ity. When multiple rules have th(; stone prob- 
nbilil;y and similm'ity, the method takes the exam- 
pies used by the rules having the highest probabil ity 
and the higlmst si,nilarity, and chooses the (:ategory 
with the larger llllllI))CF Of exami)les as t:hc desired 
answer~ in the same way as in Method 1. 
Itowever, when (:ategory-c.xchlsive rules having 
l l lt iro tha l l  Olte fre(l l lel lCy exist, the a})ove t ) roccdl l r ( ;  
is performed after el iminating all of the category- 
exclusive rules having one frequency, in el;her words, 
category-exclusive rules having more than one fl'e- 
quency are giwm a higher priority than category- 
exclusive rules having only one. flTo.qll(;nc.y })lit hav- 
ing ~ high(w similarity. This is 1)c(:ause eategory- 
(;xclusivc rules having only one fl'equen(:y are not so 
reliabh',. 
4 Experiments and discussion 
In our experiments we used a Kyoto  University text 
eorlms (Knrohashi and Nagao, 1997), which is a 
tagged corpus made Ul) of articles fi'om the Mainichi 
newspaper. All exl)eriments reported in this paper 
we.re performed using art, ielcs dated fi'om ,\]mmary 
\] to 5, 1995. We obtained the correct infi)rnmtion 
()n morphoh)gy and }mnse.t;su identiticathm from the 
tagged corpus. 
The following experiments were conducted to de- 
termine which supervised \]earnillg~ lnethod achieves 
the high<'.st a(:Cllra(:y l~tl;e. 
? Exlmriment \] 
\[,(;arninl,; set: ,Janllary 1, 1.995 
~J?t;Sl; st?t: ,\]atlll~/l'y 3, 1.995 
? \]';xt)eriment 2 
Learning set: 3ammry 4, 1995 
Test set: .\]a.nuary 5, 1995 
ltecm>e we used F, xlmriment \] in maki,lg Method 
I and Method 2, \]i;Xl)erinieut 71 is a ch)sc'd data..~et 
for Mel:l~od \] and Method 2. So, we l)crformed Ex- 
lmriment 2. 
The ,'(;suits arc. listed in '12fl)lt;,q I to d. \Ve used 
KNP2.0b4 (Kurohashi, 11997) mM KNP2.0t/6 (Kuro- 
hashi: 1998), which are bmlsetsu identitication and 
syntael;i(" analysis systems using tmmy hand-made 
rules in addition 1;o the six methods des(:ribed in 
Section 3. Be('mtse KNP is not based on a machine 
learning inethod but :many hand-made rules, in the 
KNP results "Learning selY and '~'.Test et" in the ta- 
llies have nt) meanings. In the eXll(wiment of KNP, 
we also uses morphological information in a corpus. 
~\].~hc ';F': ill l;\]le tables indicates the F-measure~ which 
is the. harmonic mean of a recall and a precision. A 
recall is l;he fl'action of correctly identilied partit ions 
out of all the partitions. A t)reeision is the ffae- 
th)n of correctly identitied partit ions out of all the 
SlmCeS which were judged to have a partit ion mark  
inserted. 
Tables I to -/I show the. following results: 
? In the test set I;he dc.cision-tree method was 
a little better thmt the maximmn-entropy 
569 
Table 1: Results of learning set of Exper iment  1 
Method D ~ 
Decision %-ee 99.58% 
Maximum Entropy 99.20% 
Example-Based 99.98% 
Decision List 99.98% 
Method 1 99.98% 
Method 2 99.98% 
KNP 2.0114 99.23% 
KNP 2.0116 99.73% 
Recall Precision 
99.66% 99.51% 
99.35% 99.06% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
99.78% 98.69% 
99.77% 99.69% 
The number of spaces between two )nor flmmes is 
25,814. The number of lmrtitions is 9,523. 
Table 3: Results of learning set of Exi)er iment 2 
Method 
Decision Tree 
Maximum Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.01)4 
KNP 2.066 
99.07% I 
99.99% I 
99.99% I 
99.99% I 
99.99% I 
98.94% I 
99.47% I 
RecaU Precision 
99.71% 99.69% 
99.23% 98.92% 
100,00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
99.50% 98.39% 
99.47% 99.48% 
The mlmber of spaces between two mor )heines is 
27,665. The number of partitions is 10 143. 
Table 2: Results of test set of Exper iment  1 
Method 
~ ion  Tree 
Maximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.066 
- F Recall 
98.87% 98.67% 99.08% 
98.90% 98.75% 99.06% 
99.02% 198.69% 99.36% 
98.95% i 98.43% ! 99.48% 
98.98% 198.54%! 99.43% 
99.16% I 98.88% ' 99.45% 
99.13% 99.72% ! 98.54% 
99.66% ~_99.68% ! 99.64% 
:)heroes is The lmmber of spaces between two mor 
Precision 
16,983. The mmiber of partitions is 6,166. 
T~ble 4: Results of test set of Exper iment  2 
Method 
Decision Tree 
M aximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.01)6 
P RTcad I Precision 
98.50% 98.51% I 98.49%- 
98.57% 98.55%1 
i 98.82% 98.71%1 
198.75% 98.27%1 
i 98.79% 98.54% I 
i 98.9\[1% 98.65% I 
199.(/7% 99.43%1 
L 99.51% 99.40% ~_ 
98.59% 
98.93% 
99.23% 
99.43% 
99.15% 
98.71% 
99.61% 
The nmnber of" spaces between two morphemes i
32,3o4. The number of partitions is 11,756. 
method.  Al though the maximuln-entropy 
method has a weak point  in that  it, does not 
learn the combinat ions of features, we could 
overcome this weakness by malting almost all of 
the combilmtions of features to produce a higher 
accuracy rate. 
? Tile decision-list n lethod was bet ter  t itan the 
maximum-entropy method in this experinmnt.  
? Tile example-based nlethod obtained the high- 
est accuracy rate among the four exist ing meth- 
ods. 
? Altt lough Method 1, which uses tim category- 
exclusive rule, was worse than the exmnple- 
based method,  it was better  than ti le decision- 
list method.  One reason for this was that  
ti le decision-l ist metl lod chooses rules rmldomly 
when mult iple rules have identical probabi l i t ies 
mid fl'equeneies. 
? Method 2, which uses the category-exchlsive 
rule with the highest similarity, achieved the 
highest accuracy rate among ti le supervised 
learning methods.  
? Tim example-based method,  tim decision-l ist 
inethod, Method 1 and Method 2 obta ined ac- 
curacy rates of about  100% for the leanfing set. 
This indicates that  these methods m:e especial ly 
strong for learning sets. 
? Tile two methods using similarity example-  
based method mid Method 2) were always bet-  
ter than the other methods, indicat ing that  the 
use of s imilar ity is eflective if we can define it 
approl)r iately. 
? We carried out experinmnts by using KNP,  a 
system that  uses ninny ha.nd-made rules. The 
F-measure of KNP was highest in the test set. 
? We used two versions of KNP, KNP 2.0b4 and 
KNP 2.0b6. The lat ter  was mud l  better  t lmn 
tlm former, iudicat ing tha.t the improvements 
made by hand are  effective. But, the mainte-  
nance of rules by hand has a l imit, so the im- 
provements made by hand are not always effec- 
tive. 
Tlle above exper iments indicate that  Method 2 is 
best among the machine learning methods '5. 
In Table 5 we show some cases which were par- 
t i t ioned incorrectly with KNP but correctly with 
51n these experiments, the. differences were very small. 
But, we think that the differences are significant to some ex- 
tent because we performed Experiment 1 and Experiment 2, 
the data we used are a large corplls containing about a few 
ten thousand morphemes and tagged objectively in advance, 
and the difference of about 0.1% is large in the precisions of 
99%. 
570 
Table 5: Cases when KNP was incorrect and Method 
2 wan correct  
ko tsukotsu \[ N H1,,'Tr 9aman.sh i 
(steadily) (lm prurient wit;h) 
L_{"" 1)e patient with ... steadily) 
lyoyuu wo \] motte \]~xrT;~l~ shirizoke 
i(enough Stl'engi;h) obj (have) (1)eat off) 
(... beat off ... having enough sl;rength) 
~aisha wo I gurupu-wake \ [ ~  
,.o.,v,,,,y obj (~r,,,,pi,,~) (do) 
(... 11o grouping companies) 
Method 2. A partition with "NEED" indicates that 
KNP missed inserting the i)artition mark, and a par- 
tition with "WRONG" indicates that KNP inserted 
the partitiol~ mark incorrectly. In the test set of Ex- 
periment 1, the F-measure of KNP2.0b6 was 99.66%. 
The F-measur(. ~ increases to 99.83%, ml(ler the as- 
sumption that when KNP2.0t)6 or Method 2 in cor- 
rect, the answer is correct. Although the accuracy 
rate for KNP2.0b6 was high, there were some cases 
in which KNP t)artitioned incorrectly and Method 
2 partitioned correctly, A combination of Method 
2 with KNP2.0b6 may be able to iml)rove the F- 
lile~lsllrO. 
The only 1)revious research resolving Imnnetsu 
identification by machine learning methods, in the 
work by Zhang (Zhang and Ozeki, 1998). The 
decision-tree, ine, thod was used in this work. But 
this work used only a small mmther of intor- 
l l l;ttion for t)llllsetsll identification" and (lid not 
achieve ll igh accuracy rat;es. (The  recall  rate 
was 97 .6%(=2502/ (2502+62) ) ,  the 1)recision rate 
was 92 .4%(=2502/ (2502+205) ) ,  and F -measure  was 
94.2%.) 
5 Conc lus ion  
To solve tile t)roblem of aecm'ate btmsetsu iden- 
tification, we carried out ext)eriments comt)aring 
tbur existing machine-learning methods (decision- 
tree method, maxilnum-entrol)y method, examI)le- 
based method and decision-list method). We ob- 
tained the following order of acem'acy in bunsetsu 
identification. 
Example-Based > Decision List > 
Maximum Entropy > Decision Tree 
We also described a new method which uses 
category-exclusive rules with the highest similarity. 
This method performed better than the other learn- 
ing methods in ore" exi)eriments. 
(>l'his work used oifly the POS information of the two roof 
phemes of an analyzed space. 
References  
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. \])ella Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Linguistics, 
22(l):ag-rl. 
Andrew Borthwicl% John Sterling, Eugene Agichtein, aim 
Ralph Grishlnan. 1998. Exploiting Diverse Knowledge 
Sources via Maximum I';ntropy in Named Entity ll.ecoglfi- 
tion. In Proceedings of the Sixth Workshop on Very LaT~le 
Corpora, pages 152 -160. 
Masayuki Kameda. 1995. Simple Jalmnese analysis tool q jp.  
The Association for Natural Lan, guage Processing, the Isl 
National Convention, pages 349-352. (ill .)~ti)~tlleSe ). 
Sadao Kurohashi and Makot<) Nagao. 1997. Kyoto University 
text corpus 1)ro./ect. pages 115-118. (in .lapanese). 
Sadao lgurohashi and MM~oto Nagao, 1998. Japanese Mof  
phological Analysis System JUMAN version 3.5. \])ei)art- 
mcnt of Informatics, Kyoto University. (in Japanese). 
Sadao Kurohashi, 1997. Japanese Dependency/Case Struc- 
ture Anahdzer KNI ) version 2.Obj. Department of lnfof  
matics, Kyoto Ulfiversity. (in Japmmse). 
Sadao Kurohashi, 1!)!18. Japanese Dependency/Uase Struc- 
ture Analyzer KNI' version 2.0b6. Del)artment of Infor- 
m~tics, l(yoto University. (in .lapmmse.). 
Makoto Nagao. 1984. A I,'ralneworl( of a Mechanical Transh> 
ti(m between Jai)anese alld English 1)y Analogy lh'incit)le. 
Artificial a\]l(l Iluman hitelligence~ pages 173 q80. 
Shigeyuld Nishiokayama, Takehito Utsuro, and Yu.ii Mat- 
sumoto. 119!18. Extracting preference of dependency be- 
twee/t ,Japanese subordinate clauses from corlms. IE ICE-  
WGNL098- ll, pages 31-38. (in .lapnese). 
NI,I{,\]. 1964. (National Language Resemvh Institute). 
Word List b?l Semantic Principles. Syuei SyuI)pan. (in 
Japanese). 
.\]. 1{.. Quinl;m. 1995. Pro qrams for machine learning. 
Lmme A. I{;mlshaw ;uld Mitchell 1'. Marcus. 1(.)(.15. Text 
clmnking using transformationq)ased l arning. \]11 Proceed- 
ira.IS of th, e Th, ird Workshop on Very La*#e Corpora, l)ages 
82 (.)4. 
Adwait l{.atnat)arklfi. 1996. A Maximum l,;nl;ropy Model tin" 
l'art-OlCSt)eech Tagging. 1)rocecdin9 s of P)mpirieal Method 
for Natural Language l'roeessings, pages 133 1,12. 
Adwait ll,atnaparkhi, 19!17. A l,inear Observed Time Statis- 
ticaI l'm'ser Based ou Maximum \]~ntropy Models. ht t~l'o - 
ceedings of Empirical Method for Natural l, an.quage l'ro- 
cessings. 
Eric Sven Ristad. 1998. Maximum Elltropy Modeling 
Toolkit, Release 1.6 beta. http://www,mnemonic.com/ 
software/metal. 
Ronald L. Rivest. 1987. lmarning l)ecision lasts. Machine 
Learning~ 2:229 -246. 
Erik P. Tjong Kim Sang and .lorn \reenstra. 1999. ll.el)re- 
senting text chunks, in EA CL'99. 
Kiyotaka Uehimoto, Satoshi Sekiim, and IIitoshi Isalmra. 
1999.. Japanese dependency structure analysis based on 
maximmn entrol)y models. In Proceedings of the Ninth 
CoTtference of the 15"ulwpeau Chapter of the Association 
for Computational Linguistics (P)A CL), pages 196-203. 
\])avid Yarowsky. 1!)94. Decision lists fo," lexieal ambiguity 
resolution: Application to accent restoration in Spanish 
and l,Yench. In 22th Alt, Tt~tal Meeting of the Assoeitation 
of the Computational Linguistics, pages 88-95. 
Yujie Zlmng and Kazuhiko Ozeki. 1998. The applica- 
tion of classitieation trees to bunsetsu segmentation of 
Jat)anese sentences. Journal of Natural Language Process- 
ing, 5(4):17-33. 
571 
Word Order  Acqu is i t ion  f rom Corpora  
Kiyotaka Uchimoto I, Masaki Murata t, Qing Ma*, 
Satoshi Sekine*, and Hitoshi Isahara t 
tCommunications Research Laboratory 
Ministry of Posts and Telecommunications 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto ,murata, qma, isahara\] @crl. go. jp 
*New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekYne~cs, nyu. edu 
Abstract 
In this paper we describe a method of acquiring word 
order fl'om corpora. Word order is defined as the or- 
der of modifiers, or the order of phrasal milts called 
'bunsetsu' which depend on the stone modifiee. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to de- 
ciding the word order mid which word order tends to 
be selected when several kinds of information con- 
flict. The contribution rate of each piece of informa- 
tion in deciding word order is eiIiciently learned by a 
model within a maximum entropy framework. The 
performance of this traiimd model can be ewfluated 
by checking how many instances of word order st- 
letted by the model agree with those in the original 
text. In this paper, we show t, hat even a raw cor- 
pits that has not been tagged can be used to train 
the model, if it is first analyzed by a parser. This 
is possible because the word order of the text in the 
corpus is correct. 
1 Introduction 
Although it is said tha~ word order is free in 
Japanese, linguistic research shows that there art 
certain word order tendencies - -  adverbs of time, for 
example, tend to t)recede subjects, mM bunsetsus in 
a sentence that are modified by a long modifier tend 
to precede other bunsetsus in the sentence. Knowl- 
edge of these word order tendencies would be useful 
in analyzing and generating sentences. 
Ii1 this paper we define word order as the order of 
nrodifiers, or the order of bunsetsns wlfich depend on 
the same modifiee. There arc several elements which 
contribute to deciding the word order, and they are 
summarized by Saeki (Saeki, 1.998) as basic condi- 
tions that govern word order. When interpreting 
these conditions according to our definition, we era: 
summarize them ,~ tbllows. 
Component la l  eondit lons 
? A bunsetsu having a deep dependency tends 
to precede a bunsetsu having a shallow depen- 
dency. 
When there is a long distance between amodifier 
and its modifiee, the modifier is defined as a bun- 
setsu having a deep dependency. For example, 
the usual word order of modifiers in Japanese 
is tlm following: a bunsetsu which contains an 
interjection, a bunsetsu which contains an ad- 
verb of time, a bunsetsu which contains a sub- 
ject, and a bunsetsu which contains an object. 
Here, the bunsetsu containing an adverb of time 
is defined as a bunsetsu having deeper depen- 
dency than the one containing a subject. We 
call the concept representing the distance be- 
tween a modifier and its modifiee the depth of 
dependency. 
A bunsetsu having wide dependency tends to 
precede a bunsetsu having narrow dependency. 
A bunsetsu having wide dependency is defined 
as a bunsetsu which does not rigidly restrict its 
modifiee. For example, the bunsetsu "~btqlo_c 
(to Tokyo)" often depends on a bunsetsu whicll 
contains a verb of motion such as "ihu (go)" 
while the bunsetsu "watashi_.qa (I)" can depend 
on a bunsetsu which contains any kind of verb. 
Here, the bunsetsu "watashi_ga (I)" is defined as 
a bunsetsu having wider dependency than 1;11o 
tmnsetsu ':Tok~./o_c (to Tokyo)." We call the 
concept of how rigidly a modifier restricts its 
modifiee the width of dependency. 
Syntact i c  condit ions 
? A bunsetsu modified by a long inodifier ton(Is to 
precede a bunsetsu modified by a short lnodifier. 
A long modifier is a long clause, or a clause that 
contains many bunsetsus. 
? A bunsel, su containing a reference pronoun tends 
to precede other bunsetsus in the sentence. 
? A bunsetsu containing a repetition word tends 
to precede other bunsetsus in the sentence. 
A repetition word is a word referring to a word 
in a preceding sentence. For example, Taro 
mid Hanako in the following text are repetition 
words. "Taro and Hanako love each other. Taro 
is a civil servant and Hanako is a doctor." 
? A bunsetsu containing the case marker "wa" 
tends to precede other bunsetsus in the sentence. 
A mnnber of studies have tried to discover the rela- 
tionship between these conditions and word order in 
871 
Japanese. Tokunaga and Tanalca proposed a model 
for estimating JaI)anese word order based on a dic- 
tionary. They focused on the width of dependency 
(Tokunaga and Tanal~a, 1991). Under their model, 
however, word order is restricted to the order of case 
elements of verbs, and it is pointed out that the 
model can deal with only the obligatory case and 
it cmmot deal with contextual information (Saeki, 
1998). An N-gram model fbr detecting word order 
has also been proposed by Maruyama (Maruyama, 
1994), but under this model word order is defined as 
the order of morpheines in a sentence. The problem 
setting of Maruyama's study thus differed fl'om ours, 
and the conditions listed above were not taken into 
account in that study. As for estimating word or- 
der in English, a statistical model has been proposed 
by Shaw and Hatzivassiloglou (Shaw and Hatzivas- 
siloglou, 1999). Under their model, however, word 
order is restricted to the order of premodifiers or 
modifiers depending on nouns, and the model does 
not simultaneously take into account many elements 
that contribute to determining word order. It would 
be difficult to apply the model to estimating word 
order in Japanese when considering the many condi- 
tions as listed above. 
In this paper, we propose a method for acquiring 
from corpora the relationship between the conditions 
itemized above and word order in Japanese. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to decid- 
ing the word order and which word order tends to be 
selected when several kinds of information conflict. 
The contribution rate of each piece of information in 
deciding word order is efficiently learned by a model 
within a maximum entrot)y (M.E.) framework. The 
performance of the trained model can be evaluated 
according to how many instances of word order se- 
lected by the model agree with those in the original 
text. Because the word order of the text in the corpus 
is correct, the model can be trained using a raw co> 
pus instead of a tagged corpus, if it is first analyzed 
by a parser. In this paper, we show experimental re- 
sults demonstrating that this is indeed possible even 
when the parser is only 90% accurate. 
This work is a part of the corpus based text gen- 
eration. A whole sentence can be generated in the 
natural order by using the trained model, given de- 
pendencies between bunsetsus. It could be helpful 
for several applications uch as refinement support 
and text generation in machine translation. 
2 Word  Order  Acqu is i t ion  and 
Es t imat ion  
2.1 Word  Order  Mode l  
This section describes a model which estimates the 
likelihood of the appropriate word order. We call 
this model a word order model, and we implemented 
it within an M.E. framework. 
Given tokenization of a test corpus, the problem 
of word order estimation in Japanese can be reduced 
to the problem of assigning one of two tags to each 
relationship between two modifiers. A relationship 
could be tagged with "1" to indicate that the order 
of the two modifiers is appropriate, or with "0" to in- 
dicate that it is not. Ordering all modifiers so as to 
assign the tag "1" to all relationshit)s indicates that 
all modifiers art  in the appropriate word order. The 
two tags form the space of "futures" in the M.E. 
formulation of our estimation problem of word or- 
der between two modifiers. The M.E. model, as well 
as other similar models allows the computation of 
P(flh) for any f in the space of possible futures, F, 
and for every h in the space of possible histories, H. 
A "history" in maximum entropy is all of the condi- 
tioning data that enable us to make a decision in the 
space of futures. In the estimation problem of word 
order, we could reformulate this in terms of finding 
the probability of f associated with the relationship 
at index t in the test cortms as: 
P(flht) = P( f l  hfformation derivable 
from the test corpus 
related to relationship t) 
The computation of P(flh) in any M.E. models is 
dependent on a set of "features" which should be 
hdpful in making a prediction about the flmlre. Like 
most current M.E. models in computational linguis- 
tics, our model is restricted to features which are 
binary functions of the history and future. For in- 
stance, one of our features is 
1. : if has(h,x) = true, 
x = "Mdfl'l - Head-  
g(h,f) = POS(Major) : verb" (1) 
&f= l  
0 : otherwise. 
Here "has(h,x)" is a binary flmction which returns 
true if the history h has feature z. We focus on the 
attributes of a bunsetsu itself and on the features 
occurring between bunsetsus. 
Given a set of features and some training data, 
the maximum entropy estimation process produces a
model ill which every feature .qi has associated with it 
a parameter ai. This allows us to compute the con- 
ditional probability as follows (Berger et al, 1996): 
ag~ (h .f) 
P( / Ih ) -  1L ' (2) 
Z (h) 
ct i . (3) 
Y i 
The maximum entropy estimation technique guaran- 
tees that for every feature gi, the expected value of 
gi according to the M.E. model will equal the empir- 
ical expectation of gi in the training corpus. In other 
words: 
P(h,/). Mh, f) 
h,f 
= (4) 
h / 
Here /5 is an empirical probability and l~ Ie  is the 
872 
Table l: Example of estimating the probabilities of word orders. 
? ) I"I~{H (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (Taro) / bLo ( l ) I t~yed. ) "  /I~,,'~<~,~ x llail.-)'nxt, x 1.7:-.x,~j~m ::: 0.6 x 0.8 x 0.3 0.144 ) \["NI!Iii:k (Taro) )  I~1!11 (yesterday) / ?:LX ~ (tennis) / bt:? (played.)" IP:mi~ ~H x I~*H,)-: x,,< x t?r,l~.?::x,: := 0.4 x 0.8 ? 0.7 0.224 
\[")kl'!l~l:~ (Taro) / -Y : :x '~ (tennis) / 'a~H (yesterday) / b?:o (plowed.)" I/)~:,,~ll x tg.=.x'~.,~H x P,k~a*~L-)::?~ =: 0.4 x 0.2 x 0 .7  0.05(i 
1"9:-':;? ~ (tennis) / II~{H (yesl.erd;ty) / >kfl\[~l~: ( l'&l'O) / \[,~:.? (played.)" \[13*lll,2<I,u:l. x l: ':.xt, jall x l!).:.x.~,.j<r~m :: 0.6 x 0.2 x 0.3 0.036 
\["5:=x ~ (tennis) / )kf~lll:t (Taro) / I~I~Ft (yesterday) / blz0 (played.)" ~ ? H  x l~;.:.xr, ~,~ZI? x l?y:.x~l~,~ =: 0.4 x 0.2 x 0.3 0.024 
prol)ability assigned by the M.E. model. 
We detine a word order model as a model which 
learns the at)l)ropriate order of each pair of nlodifiers 
which depend on the same modifiee. 'l'his model is 
derived from Eq. (2) as follows? Assmne that there 
are two bunsetsus 231 and 23~ which depend on the 
buusetsu B and that  It is the information derivable 
from the test corpus. \]?lie probability that "B\] B2" is 
the at)propriate order is given by the following equa- 
tion: 
\]iik: :1 ffi( l  ,b) (~'i ,i 
where .qi(1 < i < k) is a fl,atm'e and "1" indicates 
that the order is at)propriate. The terms cq,i and 
(~0,i are estimated fl'oln a eorl)us which is nlorpho- 
logically and syntactically analyzed. When there are 
three or more b\]msetsus that det)end on tit('. S}tlne 
? moditiee, the probability is estimated as follows: I or 
~'t bunsetsus 231, 232, . . . ,  23n which depend on the 
bmtsetsu B and for the information h derivaMe from 
the test corpus, the prot)ability t;hat "23\] 23~ . . .  23," 
is the at)propriate order, or P( l lh) ,  is represented ass 
the probability that every two bunsetsus "Bi ~i-Fj 
(1 _< i < n - 1,1 < j < 'n - i)" are the appropri~ 
ate. order, or  P({14~i, i_ l . j  = l \ [ l<  i <. n - -  1, l :< j < 
n - - i} lh ) ,  ilere "I4Li+j -- l" represents that ".l~i 
23i-Fj" is the appropriate order. Let us assume that 
every 14Q,~:+j is independent each other. Then 1)(1 Ih,) 
is derived as follows: 
2 ' ( l ib )  = \]1\]. i , . -  \ ] ,  
1 < .i _< , , , -  i}lh) 
n- - |  n - i  
i -- I  j --1 
= l l  1I j), 
i= l  j= l  
where \[Si,i+ j is the information derivable when fb- 
cusing on the bunsetsu 13 m~d its modifiers 13i and 
Bi+j. 
For example, in the sentence "I~ U (kinou, yester- 
day) / :kflll ~ (Taro_wa, Taro) / -P ~ x ~ (tcnnis_wo, 
tennis) / b t:o (sita., l)layed.)," where a "/" repre- 
sents a bunsetsu boundary, there are three bunset- 
sus that depend on the verb "b  ~: (sita)." We train 
a word order inodel under the assmnl)tion that the 
orders of three t)airs of modifiers -"I~l U" and "~ 
f$1.~," "Net\] " and "?  7-:7, ~ ," and ":kl~l*l~" and "5: 
m :7, ~"  .... are al)ttropriate. We use various ldnds of 
intormation in and around the target bunsetsus as 
features. For example, the information or the feature 
that a noun of time i)recedes a t)rot)er noun is deriv- 
able fl'om the order "IP} H (yesterday) / Y;fll~ I~ (Taro) 
/ b 1=o (pl~\yed.)," and the feature that a case fol- 
lowed by a case marker "w?' precedes a case followed 
by a caqe marker "wo" is derivable from the order ":~ 
fll~ It. ( Taro_wa, Taro) / ? ~ 7. ?k (tennis_wo, tennis) / 
b 2C_o (sita., t)layed.)." 
2.2 Word Order  Es t imat ion  
This section describes the algorithm of estimating 
the word order by using a trained word order model. 
The word order estimation is defined as deciding 
the order of ntoditiers or bunsetsus which depend 
on the same modifiee. The input of this task con- 
sists of modifiers and informat, ion necessary to know 
whether or not features are found. The output is 
the order of the inodifiers. We assume that lexical 
selection in each bunsetsu is already done and all 
del)endencies in a sentence are found. The informa- 
tion necessary to know whether or not features are 
found is morphological, syntactic, semmltic, and ('on- 
textual information, and the locations of bunsetsu 
bonndaries. The features used in our ext)eriments 
are described in Section 3. 
Word order is estimated in the following steps. 
Procedures 
1. All possible orders of modifiers are found. 
2. For each, the probability that it is apt)ropriate 
is estimated by a word order model, or Eq. (6). 
3. The order with the highest probability of 1)eing 
approl)riate is selected. 
l)br example, given the sentence "1~ U (kinou, 
yesterday) /:kfilIl~ (Taro_wa, Taro) /? : :x  ~ (tcn- 
nis_wo, temfis) / b t:o (sita., played.)," tim modi- 
tiers of a verb "b  ?:_ (played)" are three tmnsetsus, 
"l~ U (yesterday)," :k~/i ~ (Taro)," "?  = x ~ (ten- 
nis)." Their apt)ropriate order is estimated in the 
following steps. 
1. The probabilities that the orders of the three 
pairs of modifiers "N- LI " and ":is: BII l~ ," "I~ 
U" and "?~:7 ,~,"  and "~fllIl?" and "?  
c .x  ~" are appropriate are estimated. As- 
sume, for example, ~-H ,;k~l~ta, PrI~ It ,? :-~. ~, and 
P;kfzlIla,~ ca  ~ are respectively 0.6, 0.8, and 0.7. 
2. As shown in Table 1, probabilities are estimated 
for all six possible orders. The order "I~ U / :k 
fill IS / -7- ~- y. ~ / b \]Co ," which has the highest 
probability, is selected as the most apt)ropriate 
order. 
2.3 Per fo rmance  Eva luat ion  
The pcrformancc of a word order model can be eval- 
uated in the following way. First, extract from a 
test corpus bunsetsus having two or more modifiers. 
Then, using those 1)unsetsus and their modifiers as 
873 
Data 
~Jtlnsetsll lJtlnsetstl nHIYiber Of babel 
nlllllber modifier 
0 1 P 
1 5 
2 3 
3 4 
4 5 P 
5 
Table 2: Example of modifiers extracted fl'om a corpus. 
Modifiers (Bunsetsu number) 
Strings in a bunsetsu 
>kflli ~ ( Taro_to, Taro and) 
~Y'{a ( lIanako_to, llanako) 
-Y- = x q~ (tennis_no, tennis) 
~lc .  (sial_hi, tournament) 
lll'C, (dete,, participate,) 
~@ b t=, (yusyo_sita., won.) 
Moditiers whose modiliee is the bunsetsu 
in the left column. 
~,~ a (0) 
?=x0~ (2) 
~a  (0) ~-r-~a 0) '~ :  (a) 
~e  (o) ~?-~* (1) If'~ (4) 
input, estimate the orders of the modifiers as de- 
scribed in Section 2.2. The percentage of the modi- 
flees whose modifiers' word order agrees with that in 
the original text then gives what we call the agree- 
ment rate. It is a measure of how close the word 
order estimated by the model is to the actual word 
order in the training corpus. 
We use the following two measurements to calcu- 
late the agreement rate. 
Pa i r  of  modi f ie rs  The first measurement is the 
percentage of the pairs of modifiers whose word 
order agrees with that in the test corpus. For 
exmnple, given the sentence in a test corpus "N 
kl (kinou, yesterday) / ~t I la  (Taro_wa, Taro) 
/ -7- = 2` ~2 (tennis_wo, tennis) / t. ~:o (sita., 
played.)," if the word order estimated by the 
model is "~ H (yesterday) / -7" -- 2. ~ (tennis) / 
~1~ ~:~ (Taro) / b too (played.)," then the or- 
ders of the pairs of modifiers in the original sen- 
tence are "N H / ;k~l~  ," "15 H / -7- =- :7, ~ ," and 
"~lit:~ / ~--2` ~ ," and those in the estimated 
word order are "~H / -~---2`~," "~H / 
1~1~ la~ ," and "Y- = 2` ~ / %:t~ll lak ." The agreement 
rate is 67% (2/3) because two of the three orders 
are the same as those in the original sentence. 
Complete  agreement  The second measurement is 
the percentage of the modifiees whose modifiers' 
word order agrees with that in the test corpus. 
3 Exper iments  and  D iscuss ion  
In our experiment, we used the Kyoto University text 
corpus (Version 2) (Kurohashi mid Nagao, 1997), a 
tagged corpus of the Mainichi newspaper. For train- 
ing, we used 17,562 sentences from newspaper arti- 
cles appearing in 1995, from January 1st to Jmmary 
8th and from Jmmary 10th to June 9th. For testing, 
we used 2,394 sentences fl'om articles appearing on 
January 9th and from June 10th to June 30th. 
3.1 Def in i t ion of  Word  Order  in  a Corpus  
In the Kyoto University corpus, each bunsetsu has 
only one modifiee. When a bunsetsu Bm depends on 
a bunsetsu Bd and there is a bunsetsu /3p that de- 
pends on and is coordinate with \])d, Bp has not only 
the information that its modifiee is \]~d but also a la- 
bel indicating a coordination or the information that 
it is coordinate with B d. This information indirectly 
shows that the bunsetsu Bm can depend on both \]3p 
and Bd. In this case, we consider Bm a modifier of 
both Bv and B d. 
Under this condition, modifiers of a bunsetsu B 
are identified in the following steps. 
1. Bunsetsus that depend on a bunsetsu B are clas- 
sifted as modifiers of B. 
2. When B has a label indicating a coordination, 
bunsetsus that are to tile left of 13 and depend on 
the same modifiee as B are classified as modifiers 
of B. 
3. Bunsetsus that depend on a modifier of B and 
have a label indicating a coordination are clas- 
sifted as modifiers of B. The third step is re- 
peated. 
When the above procedure is completed, all bunset- 
sus that coordinate with each other are identified as 
modifiers which depend oi1 the same nmdifiee. For 
example, from the data listed on the left side of To- 
ble 2, the modifiers listed in the right-hand column 
are identified for each bunsetsu. "Nt~I; ~ (Taro_to, 
Taro and)," "?~g-~ IS (Hanako_to, Hanako)," "ql "(, 
(dete,, participate,)" are all identified as modifiers 
which depend on the same modifiee "~ b 7=? 
(yusyo_sita., won.)." 
3.2 Exper imenta l  Resu l ts  
The features used in our experiment are listed in Ta- 
bles 3 and 4. Each feature consists of a type and 
a value. The features consist basically of some at- 
tributes of the bunsetsu itself, and syntactic and con- 
textual information. We call the features listed in 
Tables 3 'basic features.' We selected them man- 
ually so that they reflect the basic conditions gov- 
erning word order that were sunmmrized by Saeki 
(Saeki, 1998). The features in Table 4 are combina- 
tions of basic features ('combined features') and were 
also selected manually. They are represented by the 
nmne of the target bunsetsu plus the feature type of 
the basic features. The total number of features was 
about 190,000, and 51,590 of them were observed in 
the training cortms three or more times. These were 
the ones we used in our experiment. 
The following terms are used in these tables: 
Mdf r l ,  Mdf r2 ,  Mdfe:  The word order model de- 
scribed in Section 2.1 estimates the probability 
that modifiers are in the appropriate order as 
the product of the probabilities of all pairs of 
modifiers. When estimating the probability tbr 
each pair of modifiers, the model assmnes that 
the two modifiers are in the appropriate order. 
Here we call the left modifier Mdfrl, the right 
modifier Mdfr2, and their modifiee Mdfe. 
Head:  the rightmost word in a bunsetsu other than 
those whose major pro't-of-speech I category is 
1Part-of-speech categories follow those of JUMAN (Kuro- 
hashi and Nagao, 1998). 
874 
Table 3: Basic features. 
Bas ic  features  
Feature values (Number of type) Feature type tegors\] Target 
1)tmsetsus 
1 Mdfrl, Mdfr2, 
Mdfe 
2 Mdfrl, Mdfr2, 
Mdfe 
3 Mdfrl, Mdfr2, 
Mdfe 
4 Mdfrl, Mdh'2, 
Mdfe 
m m  
\[\]ead-POS(Major) 
\[tead-POS(Minor) 
\[lead-hff(Major) 
\[Iead-Inf(Minor) 
\[Iead-SemFeat(110) 
\[1ead-SemFeat(111 ) 
\[Iead-SemFeat(433) 
5 Mdfrl, Mdff2, rype(String) 
Mdfe rype(Major) 
type(Minor) 
6 Mdfrl, Mdfr2, lOSIll l(String) 
Mdfe lOSIIIl(Minor) 
lOSII12(String) 
lOSUI2(Minor) 
7 Mdfrl, Mdfr2, Period 
Mdfe 
8 Mdfrl, Mdfr2 Numl)erOfMdfrs 
Mdfe NumberOfMdfrs 
9 Mdfrl, Mdfr2, 
Mdfe 
10 Mdfl'l, Mdfr2 
Coordination 
(Total : 90) 
Mdfrl-MdfrType-ll )to-Mdfr2-Typc 
Mdfl'2-MdfrTypeql )to-Mdfl-I -q'ype 
Mdfi'l -Md frType-lDto- Md fr2-MdfrType 
11 Mdfrl, Mdfr2, Rel)etition-llead-l,ex 
Mdfe Repetition-Md fr-lleadq,ex 
12 Mdfrl, Mdfr2 ReferencePronoun 
i~efereneePronou  (String) 
',.%066) 
~u (verb), s~u (adjective), ~,'~ (noun) . . . .  (11) 
~'~:~ (common oun), m'~ (quantifier) . . . .  (24) 
~1~ (vowel verb) . . . .  (30) 
'.'~ (stem), t~*~ (fandamental form) . . . .  (60) 
rrue (1) 
true (1) 
true (1) 
:~, :a ,  <-u<, t:u, ~, ~=, t . . . .  (7:3) 
uJ:~ (post-positional particle), . . .  (43) 
?,~JJ'~ (e~use marker), ~*$ (imperative form) . . .  (102) 
'~',5, ~<', a~, ,., l,~. . . . .  (63) 
)ill\], ~$m (ease marker), . . .  (5) 
~, ~, *, ~,*,, ... ((~3) 
~,~;~1 (ease marker) . . . .  (41 
\[nil\], \[exist\] (2) 
A(0), B(1), C(2), 1)(3 or more) (4) 
A(2), B(:3), C(4 or more) (3) 
P(Coordim~te), A(Apposition), I)(otherwise) (3) 
\]'rue, False (2) 
rrue, False (2) 
true, False (2) 
86.65% 73.87% 
\[--0.79%) (--1.54%) 
87.07% 75.03% 
',--O.37%) (--0.38%) 
87.39% 75.20% 
',-0.05%) (-0.m%) 
87.21% 75.20% 
\[--0.23%) (--0.21%) 
84.78% 70.03% 
(-2.66%) (-5.38%) 
87.32% 75.14% 
(-0.12% (--0.27%) 
87.39% 7 ~  
(-0.05%) (+0.13%) 
87.14% 74.86% 
(-0.30%) (-0.55%) 
87.40% 70.30?./o 
(-0.04%) (-0.0~%) 
8~.2~% 73.61% 
(--1.18% (--1.80%) 
87.34% 75.09% 
(--0.10%) (--0.'32%) 
\[nil\],\[exist\] (2) 87.31% 75.id% 
\[nil\], \[exist\] (2) (--0.13%) (--0.27%) 
\[nil\], \[exist\] (2) 87.27% 75.12% 
:~, :a,  :~ .~,~=.~, ,  ~. . . . .  (42) (--0.17%) (--0.29%) 
'%} @ (special marks)," "112 N (1)ost-posi~ioual 
particles)," or "}~N~? (suffixes)." 
Head-Lex :  the fllndalnental forth (unintlected 
forln) of the head word. Only words with a fre- 
quency of tlve or more are used. 
Head- In f :  the inflection type of a head. 
SemFeat :  We use the upper third layers of bunrui 
.qoihyou (NLl/I(National Language Research In- 
stitute), 19641 as semantic features. Bunrui goi- 
hyou is a Japanese thesaurus that has a tree 
structure and consists of seven layers. The tree 
has words in its leaves, and each word has a fig- 
ure indicating its category number. For exam- 
ple, the figure in parenthesis of a feature "Head- 
SemFeat( l l0)" in Table 3 shows the upper three 
digits of the category number of the head word 
or the ancestor node of the head word in the 
third layer in the tree. 
Type:  the rightmost word other than those whose 
major part-of-speech category is "~@ (special 
marks)." If the major category of the word 
is neither "NJN (post-positional particles)" nor 
"}~/~'~ (suffixes)," and the word is inflectable, 2 
then the type is represented by the inflection 
type. 
JOSHI1 ,  JOSHI2 : JOSHI1  is the rightmost post- 
positional particle in the bunsetsu. And if there 
are two or more post-positional particles in the 
bunsetsu, JOSHI2 is the second-rightmost post- 
positiolml particle. 
NmnberOfMdf rs :  number of modifiers. 
2The inflection types follow those of J UMAN. 
Mdfr l -Mdf rType ,  Mdf r2 -Mdf rType:  Types of 
tile modifiers of Mdfi'l and Mdfr2. 
X- IDto -Y :  X is identical to Y. 
Repet i t ion -Head-Lex :  a ret)etition word allpear- 
ing ill a preceding senteuce. 
Re ferencePronour l :  a reference pronoun appear- 
ing in the target bunsetsu or ill its modifiers. 
Categories 1 to 6 ill Table 3 reI)resent attributes 
in a bunsetsu, categories 7 to 10 represent syntac- 
tic information, and categories 11 and 12 represent 
contextual information. 
The results of our experiment are listed in Table 5. 
The first line shows tlle agreement rate when we esti- 
mated word order for 5,278 bunsetsus that have two 
or more modifiers and were extracted from 2,394 sen- 
tences al)pearing on Jmmary 9th and from June 10th 
to June 301tl. \Ve used bunsetsu boundary informa- 
tion and syntactic and contextual information which 
were derivable froln the test corpus and related to 
the input bunsetsus. As syntactic ilffOrlnation we 
used dependency inforlnation, coordinate structure, 
and information on whether the target bunsetsu is at 
the eM of a sentence. As contextual information we 
used the preceding sentence. The values in the row 
labeled Baseline1 in Table 5 are the agreement rates 
obtained when every order of all pairs of modifiers 
was selected randolnly. And values in the B&seline2 
row are the agreement rates obtained when we used 
the following equation instead of Eq. (5): 
freq(w12) 
PMu'(llh) = freq(w12) + frcq(w21)" (7) 
875 
Table 4: Combined features. 
Accuracy without 
the feature 
Pair of Complete 
modifiers :tgreement 
87.23% 74.65% 
(-0.21%) (-0.76%) 
Combined  features  
- -  Twin tbatures 
(Mdfr 1-Type, Mdfr2-Type) ,  
(Mdfr 1-Type,  Mdfe- I Iead-Lex) ,  
(Mdfr  1-Type,  Md fe- t tead-POS) ,  
(Mdfr  1-Type,  Mdfr  1-Coordlnrtt ion),  
(Mdfr  1-Type, Mdf r2 -Mdf rType- IDto -Md fr1-2"ypc), 
(MdfrE-Type,  Mdfe-Head-Lex) ,  
(Mdfrg-Type,  Mdfe-Head-POS) ,  
(Mdfr2-Type,  Mdfr2-Ooord inat ion) ,  
(Mdfr2-Type,  Md fr 1-MdfrType- lDto-Mdfr2-Type) ,  
Mdfr  1-Head-Lex,  Mdfe-Per iod) ,  
Mdfr  1 -nead-POS,  Mdfe-Per lod) ,  
Mdfr  1-1tead-POS, Mdfr  1-Repet l t lon-Head- I ,ex) ,  
Mdfr2- I tead-Lex,  Mdfe-Pet lod) ,  
Mdfr2-Ite,~d-POS, Mdfe-Per lod) ,  
Mdf r2 - I Iead-POS,  Mdfr2- I tepet l t lon- I lead-Lex)  
' t~iplet  tca tures  87.22% i 74.86% 
Mdfr l -Wype,  Mdfr2-Type,  Mdfe- l lead-Lex) ,  (--0.220./0) (--0.55%) 
Mdfr l -Type ,  Mdf r2 -Type,  Mdfe-Head-POS) ,  
Mdf r l -Type ,  Mdf r l -Coord inat lon ,  Mdfe-Type) ,  
MdfrE-Type,  Mdfr2-Coord lnat lon ,  Mdfe-Type) ,  
Mdf r l - JOSHI1 ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-Lex),  
Mdf r l -aOSHl l ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-POS) ,  
Mdf r2 - JOSHI1 ,  Mdfr2- JOSI I I2 ,  Mdfe-Head-Lex),  
Mdf r2-aOSH\[1 ,  Mdfr2-JOSl~12, Mdfo- l lead-POS)  
All of  above  combined  features  85.79% 77.67% 
~--1.65%) (--3.74%) 
Table 5: Results of agreement rates. 
Agreement  ra te  
Pair of modifiers Coml)lete agreement 
Our method 87.44%(72,367/14,137) 75.41% (3,980/5,278) 
Baseline1 48.96% (6,921/14,137) 35.10% (7,747/5,278) 
Baseline2 49.20% (6,956/14,137) 53.84% (1,786/5,278) 
IIere we assume that B1 and \]32 are modifiers, their 
modifiee is B, the word types of B1 and \]32 are re- 
spectively Wl and we. The values frcq(wr2) and 
frcq(w.27 ) then respectively represent the fl'equencies 
with which w7 and w,2 appeared in the order "WT, we, 
mid w" and "w2, WT, and w" in Malnichi newspaper 
articles fl'om 1991 to 1997. a Equation (7) means 
that given the sentence "~t~lt I:t (Taro_wa) / ~- -- ~, 
(tennis_wo) / b ~-:o (sita.)," one of two possibili- 
ties, "1$ (wa) / ~ (wo) / t, ~:o (sita.)" and "#c (wo) 
/ tS (wa) / b ~:o (sita.)," which has the higher fre- 
quency, is selected. 
3.3 Features  and Agreement  Rate  
This section describes how much each feature set con- 
tributes to improving the agreement rate. 
The values listed in the rightmost columns in Ta- 
bles 3 and 4 shows the performance of the word or- 
der estimation without each feature set. The values 
in parentheses are the percentage of improvement or
degradation to the formal experiment. In the exper- 
iments, when a basic feature was deleted, the com- 
bined features that included the basic feature were 
also deleted. The most useful feature is the type of 
3When wl and w2 were the same word, we used the head 
words in Bt  and 132 as Wl and w2. When one offreq(wt2) and 
freq(w21) was zero and the other was five or more, we used 
the f lequencies when they appeared in the order "Wl ws" and 
"w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl). 
When both freq(wl.2) and freq(w27) were zero, we instead 
used random figures between 0 and t. 
bunsetsu, which basically signifies the case marker or 
inflection type. This result is close to our expecta- 
tions. 
We selected features that, according to linguistic 
studies, as mudl  as possible reflect the basic condi- 
tions governing word order. The rightmost column 
in Tables 3 and 4 shows the extent o which each con- 
dition contributes to improving the agreement rate. 
However, each category of features might be rougher 
than that which is linguistically interesting. For ex- 
ample, all case markers uch as "wa" and "wo" were 
classified into the same category, and were deleted 
together in the experiment when single categories 
were removed. An experiment that considers each 
of these markers eparately would help us verify the 
importance of these markers separately. If we find 
new features in future linguistic research on word or- 
der, the experiments lacking each feature separately 
would help us verify their importance in the same 
manner .  
3.4 Tra in ing  Corpus  and Agreement  Rate  
The agreement rates for the training corpus and the 
test corpus are shown in Figure 1 as a function of 
the amount of training data (ntunber of sentences). 
The agreement rates in the "pair of modifiers" and 
'?!19~ . . . . .  %:I:: 'i, ~ . . . . . . . . .  ~':z~-,~',:: : : i 
= 
?I 90 90 
!' 
S5 ~5 i ~ - 
E 
75 ~ 75 
7O 7O 
65 . . . . . . . .  (,5 0 . . . .  
0 2000 400{) 6000 80O0 Io(mO 120{}0 14O00 16000 18000 2000 4000 6,300 ~000 IODO0 12(;00 14000 16O00 la00{1 
\] 11o NtllObor o~ Snnloncos \]o 111o Traif l in9 Data l lm Number ol Sentences in l lm T f 4dlli11U \[)ala 
Figure 1: Relationship between tile amount of training 
data and the agreement rate. 
"Complete agreement" measurements were respec- 
tiw~ly 82.54% and 68.40%. These values were ob- 
tained with very small training sets (250 sentences). 
These rates m'e considerably higher than those of 
the baselines, indicating that word order in Japanese 
can be acquired fl'om newspaper articles even with a 
small training set. 
With 17,562 training sentences, the agreemenl, 
rate in the "Complete agreement" measurement was 
75.41%. We randomly selected and analyzed 100 
modifiees from 1,298 modifiees whose modifiers' word 
order did not agree with those in the original text. 
We found that 48 of them were in a natural order 
and 52 of them were in an unnatural order. The 
former result shows that the word order was rela- 
tively fl'ee and several orders were acceptable. The 
latter result shows that the word order acquisition 
was not sufficient. To complete the acquisition we 
need more training corpora and features which take 
into account different information than that m Ta- 
bles 3 mid 4. We found many idiomatic expres- 
876 
sions in the uimatural word order results, such as "~ 
ffl\[~il~:5~ (houchi-kokka_ga,  country under the rule 
of law) / \ [ l} l~  (kiitc, to listen) /~#t~ (alcireru, 
to disgust), ~rj ~ b ?= a ~ (souan-s~,ta-no_ga, orlgl- 
,ration) / ~ *o ~- *o co (somosomo-no, at all) / ~t~  U 
(hg~'ima~'4 the beginning)," and ""~ l~ (g#-~4 taste) / 
~'~B (seikon, one's heart and soul) /g~ 6 (homcru, 
to trot somethil,g into soinething)." We think that 
the apt)ropriate word order for these idiomatic ex- 
pressions could be acquired if we had more training 
data. We also found several coordinate structures in 
the Ulnlatural word order results, suggesting that we 
should survey linguistic studies on coordinate struc- 
tures and try to find efllcient features for acquiring 
word order from coordinate structures. 
We (lid not use the results of semantic and con- 
textual analyses as input because corpora with se- 
mantic and contextuM tags were not available. If 
such corpora were available, we could more et\[iciently 
use features dealing with seinantic features, reference 
pronouns, and repetition words. We plan to make 
corpora with semantic and contextual tags and use 
these tags as input. 
3.5 Acqu is i t ion  f rom a Raw Corpus  
In this section, we show that a raw cortms instead of 
a tagged corpus can be used to train the lnodel, if it 
is first analyzed by a parser. We used the lnorl)holog- 
ical analyzer JUMAN and a tmrser KNP (Kurohashi, 
11198) which is based on a det)endency grainlnar, 
it, order to extract iuforumtion from a raw corpus 
for detecting whether or not each feature is found. 
'l?tm accuracy of JUMAN for detecting inorphologi- 
cal boundaries and part-of-speech tags is about 98%, 
and the parsecs dependency accuracy is about 90%. 
These results were obtained from analyzing Mainichi 
newspaper articles. 
We used 217,562 sentences for training. When 
these sel~t, ences were all extracted from a raw corlms , 
the agreement rate was 87.64% for "pair of modifiers" 
and was 75.77% for "Colnplete agreement." When 
the 217,562 training sentences were sentences fl'oln 
the tagged cortms (17,562 sentences) used in our for- 
real exl)eriment aInl froln a raw cortms, the agree- 
" e S :~ ment rate for "pair of lno(hfi.r, was 87.66% and 
for "Complete agreement" was 75.88%. These rates 
were about 0.5% higher than those obtained when we 
used only sentences from a tagged corlms. Thus, we 
can acquire word order by adding inforlnation froln 
a rmv corpus even if we do not have a large tagged 
corpus. The results also indicate that the parser ac- 
curacy is not so significant for word order acquisition 
and that an accuracy of about 90% is sufficient. 
4 Conc lus ion 
This paper described a method of acquiring word or- 
der froln corpora. We defined word order as the order 
of lnodifiers which depend on tile same lnodifiee. The 
lnethod uses a model which estimates the likelihood 
of the apt)ropriate word order. The lnodel automat- 
ically discovers what the tendency of the word order 
in Japanese is by nsing various ldnds of information 
in and arouud the target bunsetsus plus syntactic 
and contextual inforlnation. The contribution rate 
of each piece of inforination in deciding word order 
is efficiently learned by a model implemented within 
an ),,I.E. framework. Comparing results of experi- 
ments controlling for each piece of information, we 
found that the type of inforinatiou having the great~ 
est influence was the case marker or inflection type in 
a bunsetsu. Analyzing the relationship between the 
amount of training data and the agreement rate, we 
fimnd that word order could be acquired even with 
a small set of training data. We also folmd that a 
raw cortms as well as a tagged cortms can be used to 
train the model, if it is first, analyzed by a parser. The 
agreement rate was 75.41% for the Kyoto University 
corpus. We analyzed the lnodifiees whose modifiers' 
word order did not agree with that in the original 
text, and folmd that 48% of theln were in a natural 
order. This shows that, in umny cases, word order 
in Japanese is relatively free and several orders are 
acceptable. 
The text we used were lmwspaper articles, which 
tend to have a standard word order, but we think 
that word orders tend to differ between ditferent 
styles of writing. We would therefore like to carry 
out experiments with other types of texts, such as 
novels, having styles different froln that of newspa- 
pers. 
it has been (lift\]cult o evaluate tile reslflts of text 
generation objectively becmlse there have been no 
good stmldards for ewlllmtion. By using the stan- 
(lard we describe in this paper, however, we can evN- 
uate results objectively, at least for word order esti- 
mation in text, generation. 
We expect hat our lnodel can be used for several 
applications as well as linguistic veritication, such as 
text; refinement silt)port and text generation in nla- 
chine translation. 
References  
Adam L. Better, Stephen A. I)ella Pietra, and Vincent J. Della 
Pietra. 199(L A Maximum t'\]ntropy Approach to N~ttural ,~tn- 
gmtge Processing. Computational Linguistics, 22(11:39-71. 
Sadao Kurohashi and Makol.o Nagao. 1997. Kyoto University 
Text Corl)uS Project. In Proceedings of The Third Annual 
Mectin9 of The Association for Natural Language Process- 
ing, pages 115-118. (in Japanese). 
Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morpho- 
logical Analysis System JUMAN Version 3.6. l)epartment of 
Informatics, Kyoto University. 
Sadao Kurohashi, 11198. Japanese Dependency/Case Structure 
Analyzer KNP Version 2.0b6. Department of Inforln~tties, Ky- 
ore University. 
IIiroshl Maruyama. 1994. Experhnents on V~rord-Order Recovery 
Using N-Cram Models. In YTte \]~9th Annual (;onvention IPS 
Japan. (in Japanese). 
NLRl(National Language Research Institute). 1964. Word List 
by Semantic Pri~ciples. Syuei Syuppan. (in Japmlese). 
Tetsuo Saekl. 1998. Yousetsu nihongo no 9ojun (Survey: Word 
Order in Japanese). Kuroshio Syupl)an. (in Japanese). 
James Shaw and Vasileios Ilatzivassiloglou. 1999. Ordering 
Among l'remodifiers. In Proceedings of the 37th Annual Meet- 
ing of the Association for Computational Linguistics (,4 CL), 
pages 135-143. 
Takenobu Tokunaga and IIozumi Tanaka. 1991. On Estimat- 
ing Japanese Word Order B~used on Valency Information. 
Keiryo Kokugogaku (Mathematical Linguistics), 18(21:53-(;5. 
(in Japanese). 
877 
 
	ff 
		ffSimilarities and Differences among Semantic Behaviors of 
Japanese Adnominal Constituents 
Kyoko Kanzaki and Qing Ma and Hitoshi Isahara 
Communications Research Laboratory 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku, Kobe, 651-2492, Japan 
{kanzakilqma\[isahara} @crl.go.jp 
Abst rac t  
This paper treats the classification of the se- 
mantic functions performed by adnominal con- 
stituents in Japanese, where many parts of 
speech act as adnominal constituents. In order 
to establish a formal treatment of the semantic 
roles, the similarities and differences among ad- 
nominal constituents, i.e. adjectives and "noun 
+ NO (in English "of + noun")" structures, 
which have a broad range of semantic func- 
tions, are discussed. This paper also proposes 
an objective method of classifying these con- 
structs using a large amount of linguistic data. 
The feasibility of this was verified with a self- 
organizing semantic map based on a neural net- 
work model. 
1 In t roduct ion  
Pustejovsky (Pustejovsky, 1995) proposed the 
theory of a generative l xicon as a framework by 
which meanings of words are expressed in one 
unified representation. This kind ofgenerativity 
would be very useful for NLP, especially if it is 
applicable to the complex semantic structures 
represented by various modification relations. 
In our previous research on adjectives (Isahara 
and Kanzaki, 1999) we used Pustejovsky's the- 
ory to classify adjectives in Japanese. In this pa- 
per we take the first steps in a similar classifica- 
tion of the Japanese "noun + NO" construction. 
Bouillon (Bouillon, 1996) applied this theory 
to the adnominal constituent of mental states. 
Saint-Dizier (Saint-Dizier, 1998) discussed ad- 
jectives in French. 
Isahara and Kanzaki (Isahara and Kanzaki, 
1999) treated a much wider range of phenom- 
ena of adnominal constituents. They classified 
the semantic roles of adnominal constituents 
in .Japanese. where many parts of speech act 
as adnominal constituents, and discussed a for- 
mal treatment of their semantic roles. In their 
research, adnominal constituents, mainly ad- 
jectives which function as adverbials, are dis- 
cussed. The present paper describes the sim- 
ilarities and differences among adnominal con- 
stituents, i.e. adjectives and "noun + NO t (in 
English "of + noun")" structures which have 
a broad range of semantic functions. This pa- 
per proposes an objective method for classifying 
these structures using a large amount of linguis- 
tic data. The feasibility of this was verified with 
a self-organizing semantic map based on a neu- 
ral network model. 
In section 2, we explain the semantic func- 
tions performed by "noun + NO." In section 
3, we discuss how we can semi-automatically 
obtain and classify examples of adjectives and 
"noun + NO" structures which have similar se- 
mantic functions. In section 4, we introduce a
self-organizing semantic map to verify the result 
of this classification. In section 5, we discuss 
similarities and differences between adjectives 
and "noun + NO" structures. 
2 The  D ivers i ty  o f  Semant ic  
Re la t ions  between "noun -t- NO"  
and  the i r  Head Nouns  
Among Japanese adnominal constituents, "
noun + NO" represents a wider range of seman- 
tic relations than other adnominal constituents. 
Therefore, "noun + NO" does not always be- 
have like the other adnominal constituents. In 
previous work, some researchers have analyzed 
semantic relations between the noun in the 
"noun + NO" structure and its head noun (Shi- 
mazu et al, 1986). Here, we show several ex- 
amples that demonstrate he diversity of the se- 
l "NO" is a Japanese postpositiona| which can repre- 
sent a wide range of semantic relations. It is similar to 
"of" in English. 
59 
mantic relation between "noun + NO" struc- 
tures and their head nouns shown in their re- 
search. 
DENWA NO SECCHI 
DENSHA NO TUUKIN 
ASHITA NO DEITO 
BILU NO MAE 
KODOMO NO NAMAE 
BAKUHATSU NO GEN'IN 
KAISHI NO JIKOKU 
HEYA NO BANGOU 
KANOJO NO NOUTO 
BENGOSHI NO SMITH SAN 
installation of 
the telephone 
commuting by 
train 
a date for 
tomorrow 
in front of 
the building 
the name of 
the child 
the cause of 
the explosion 
the starting time 
the number of 
the room 
her note 
Mr. Smith, 
the lawyer 
These semantic relations between "noun + 
NO" structures and their head nouns are dif- 
ferent than those between other adnominal con- 
stituents, e.g. adjectives and their head nouns. 
However, some "noun + NO" behavior is sim- 
ilar to the behavior of adjectives and nominal 
adjectivals. In these cases "noun + NO" seems 
not to differ semantically from adjectives and 
nominal adjectivals. Let us consider the English 
examples: 
financial world / world of finance ("ZAIKAI") 
industrial center / center of industry 
("SANGYOU NO CHUUSHIN") 
In this case "noun + NO" need not be dis- 
tinguished from an adjective with respect o se- 
mantic behavior. However, in the following ex- 
amples it is necessary to distinguish them from 
one another. 
global center / center of tile globe 
("SEKAI NO CHUUSHIN 
/ CHIKYUU NO CHUUSHIN") 
We do not have a discrimination criteria that 
automatically recognizes whether a "noun + 
NO" structure is similar in its semantic behav- 
ior to that of adjectives or not. We have at- 
tempted to gather, semi-automatically, nolms in 
the "n(mn + NO" structure which behave like 
adjectives. 
3 The  Exp lorat ion  o f  the  S imi la r i t ies  
o f  Semant ic  Funct ions  o f  "noun + 
NO"  St ructures  and  Ad jec t ives .  
(The  Method  for th is  Research)  
3.1 The Basic Concept  
There is one case in which the meanings of ad- 
nominal constituents are semantically similar 
to the features of the referents of their head 
nouns, e.g. adnominal constituents represent 
the concrete contents of their head nouns. Let 
us consider the Japanese phrase "KANASHII 
KIMOCHI (sad feeling)" and "YOROKOBI NO 
KIMOCHI (feeling of delight)" as examples. 
KANASHII KIMOCHI 
adjective noun 
(sad) (feeling) 
sad feeling 
YOROKOBI NO KIMOCHI 
noun postp, noun 
(delight) (of) (feeling) 
feeling of delight 
NB: The English gloss of the "noun + NO" 
examples hould be read from right to left. 
One meaning of "KIMOCHI (feeling)" repre- 
sents the semantic element <mental state>. In 
the above examples, the adjective, "KANASHII 
(sad)", and "noun + NO", "YOROKOBI NO 
(of delight)", represent he concrete contents 
of their head noun "KIMOCHI (feeling)", i.e. 
they also represent the mental state: "feeling". 
Therefore, even though they belong to different 
parts of speech (adjective/noun), they must be 
classified in the same semantic category since 
both carry the same meaning. Neither the ad- 
jective, "KANASHII (sad)", nor the "noun + 
NO", "YOROKOBI NO (of delight)", can ap- 
pear in predicative position without changing 
their meaning. 
However, if adnominal constituents do not 
share the same semantic oncept as their head 
noun, they cannot represent he contents of 
head nouns. The examples below demonstrate 
this. 
KANASHII KIMOCHI 
adje(:tive noun 
(sad) (feeling) 
JOHN NO KIMOCHI 
noun postp, noun 
(John) (of) (feeling) 
John's feeling 
60 
In the above examples, the noun in "noun + 
NO", "JOHN", does not include the concept, 
<mental state>, so it cannot represent the con- 
tent of "KIMOCHI (feeling)." The adjective, 
"KANASHII (sad)", and the noun in the "noun 
+ NO", "JOHN" do not embody the same con- 
cept and have a different semantic relation with 
their head noun. We cannot find the seman- 
tic similarities between "KANASHII (sad)" and 
"JOHN" that we could between "YOROKOBI 
NO (of delight)" and "KANASHII (sad)." We 
focus on the phenomena where adnominal con- 
stituents represent the concrete contents of their 
head nouns. This makes it possible to identify 
adjectives and "noun + NO" structures which 
are similar in semantic behavior to the referents 
of their head nouns. These expressions are ex- 
tracted semi-automatically from large corpora. 
3.2 How to Ext ract  the Necessary 
In format ion  
When we collect words which have some sim- 
ilarities, it is difficult to select the semantic 
axis for classification by making use of only 
the co-occurring words. In collecting similar 
words, some previous research took not only co- 
occurring words but also the context of these 
words into account (Grefenstette, 1994). One 
of the important points of our analysis is the 
introduction of the distinct semantic elements 
that both "noun + NO" structures and adjecti- 
vals (adjectives and nominals) have in common 
with their head nouns. We wanted to ascertain 
the similarities between "noun + NO" and other 
adnominal constituents based on these common 
semantic elements. For this reason, we used 
the semantic relations, in which adnominal con- 
stituents represent the concrete content of their 
head nouns, as a key to classification. We au- 
tomatically 2 extracted these relations from one 
year of newspaper articles from Mainichi Shim- 
bun (1994), 100 novels from Shincho publishers 
and 100 books covering a variety of topics. We 
used the following procedure to extract he nec- 
essary information. 
Step 1) Extract from the corpora, all nouns 
which are preceded by the Japanese xpression 
"TOIU" which is something like "that" or "of." 
"TOIU + noun (noun that/of ...)" is a typical 
,Japanese xpression which introduces ome in- 
2Only Step 3) is done manually. 
formation about the referent of the noun, such 
as apposition. Therefore, nouns found in this 
pattern may have their content elucidated by 
means of their modifiers. 
Step 2) Extract from the corpora, all "noun 
+ NO" structures, adjectives and nominal ad- 
jectivals which modify the nouns extracted in 
step 1. 
NB, the relationships between adnominal 
constituents and their modified nouns extracted 
here include not only representations of the con- 
tents of the noun, but also other various rela- 
tions. 
Step 3) Extract "noun + NO" structures, ad- 
jectives and nominal adjectivals which represent 
the contents of the referents of the modified 
nouns. Step 3 is done manually. 
Step 4) In order to find the distribution of 
their semantic ategories and analyze the se- 
mantic similarities between "noun + NO" and 
other adnominal constituents in each semantic 
category, we clustered the modified nouns auto- 
matically. This clustering was based on sets of 
similar adnominal constituents which represent 
the content of the referent of the modified noun. 
4 The  Semant ic  Map of  the  
Mod i f ied  Nouns  Const ructed  by  
the Self-Organizing System of the 
Neural Network Model 
We can gather similar modified nouns when we 
classify the modified nouns according to the 
similarities of the adnominal constituents, be- 
cause in our data both adnominal constituents 
and their modified nouns have the same se- 
mantic elements in common that we mentioned 
above. 
We attempted toconstruct the Semantic Map 
of the modified nouns gathered by the above- 
mentioned method by using the self-organizing 
system of the neural network model (Ma et al, 
2000). We suppose that both modified nouns 
and adnominal constituents have common se- 
rnantic elements when adnominal constituents 
represent he concrete content of their head 
nouns. If this is true, nouns with similar mean- 
ings are located near each other oil the semantic 
map, self-organized by the similarities of seman- 
tic elements among the adnominal constituents. 
The result of our experiment verified this sup- 
position (Figure I). The nouns with a similar 
61 
meaning are located near each other on the map 
and we could divide the distribution of the mod- 
ified nouns into seven categories (Figure 2). 
Each group, i.e. the "mental state" 
group, "state/ situation" group, "characteris- 
tics" group, "range/ area" group, "viewpoint/ 
standpoint" group, "aspect" group, and "oth- 
ers," represents a meaning held in common by 
nouns in the group. Mental state can be fur- 
ther divided into the state of emotion, mood 
and intention. As we analyze the adnominal 
constituents in each category of modified nouns, 
we can find the possibility of the co-occurrence 
of an adnominal constituent with a head noun. 
Table 1 shows examples of adjectives and nouns 
in "noun + NO" structures in each group. 
UT~A 
? 
Table 1: List of adjectives and "noun + NO" 
Structures 
<menta l  state: emot ion> 
Adj: KANASHII (sad), URESHII 
(pleasurable) 
noun+no: KANASHIMI (sadness), 
YOROKOBI (delight) 
<state /s i tuat ion> 
Adj: ISOGASHII (busy), 
MUTITUJONA (disorderly) 
noun+no: KURAYAMI (darkness), 
MUISHIKI (unconscious) 
<aspect> 
Adj: YUUMOUNA (brave), 
HIGEKITEKINA (tragic) 
noun+no: KONTON (chaos), TAIHAI 
(decadence) 
<character i s t i c> 
Adj: NONKINA (carefree), 
KISAKUNA (open-hearted) 
noun+no: I J IPPARI (stubbornness), 
GOUMANNA (arrogance) 
<range/area> 
Adj: JOUSHIKITEKINA (comnmnsense), 
KOUTEKINA (official) 
noun+no: GAKUMON (studies), GYOUMU 
(duty) 
<v iewpo int / s tandpo in t> 
Adj: KYOUIKUTEKINA (educational), 
SHOUGYOUTEKINA (economic) 
noun+no: KYOUIKU (education), EISEI 
(hygiene) 
Figure 1: Semantic Map 1 
Figure 2: Semantic Map 2 
In the mental state, state/situation, aspect 
and characteristics groups~ adjectives appear 
more frequently than "noun + NO" construc- 
tions. These are simple adjectives. Ill the 
range/area nd viewpoint/standpoint groups, 
62 
"noun + NO" structures appear more fre- 
quently than simple adjectives. Nominal adjec- 
tivals derived from nouns plus the suffix "TEKI- 
na" appear often with these noun groups. Most 
nouns in the groups "mental state: emotion", 
"state/situation" and "characteristics", contain 
abstract nouns which represent emotions, situa- 
tions or characteristics. There are few concrete 
nouns. However, in the groups "range/area" 
and "viewpoint/standpoint', here are many 
concrete nouns which represent natural phe- 
nomena, organizations or professional domains 
and few abstract nouns. We can find differences 
among "noun + NO" structures, that is, there 
are adjectives which behave like nouns semanti- 
cally and there are nouns which behave seman- 
tically like adjectives. 
5 The  semant ic  behav ior  o f  the  
"noun -t- NO"  s t ruc ture  wh ich  is 
s imi la r  to  that  o f  ad jec t ives  
5.1 Types  of nouns in the "noun -'t- 
NO"  s t ruc ture  
As we mentioned in section 3, we extracted the 
"noun + NO" structures which have the same 
semantic element, along with similar adjectives, 
from large corpora. For example, 
KIKEN_NA JOUTAI 
(dangerous) (situation) 
dangerous ituation 
In this case "dangerous" represents the state 
concretely. 
MIKETTEI NO JOUTAI 
(indecision) (of) (situation) 
a situation of indecision 
In this case, the "MIKETTEI NO (of in- 
decision)" also represents the state concretely. 
Here, both "KIKENN_NA (dangerous)" and 
"MIKETTEI NO (of indecision)" have tile same 
semantic element "state" in common. We find 
that a "situation" can be represented by both 
an adjective and the "noun + NO" structure. 
When "MIKETTEI NO (of indecision)" co- 
occurs with modified nouns other than "situa- 
tion", it mostly represents the semantic notion, 
e.g. "MIKETTEI NO MONDAI (a problem of 
indecision)", and so on. That is,"MIKETTEI 
NO (of indecision)," represents the situation of 
a problem. So we see that "MIKETTEI NO (of 
indecision)" is in itself like an adjective. 
On the other hand, "KUMORI NO (cloudi- 
ness)" behaves ometimes like an adjective and 
sometimes not. 
KUMORI NO JOUTAI 
(cloudiness) (of) (state) 
a state of cloudiness 
The semantic behavior of "KUMORI NO 
(of cloudiness)" is like the behavior of adjec- 
tives in that the cloudiness represents the state 
as "KIKEN_NA (dangerous)," however, "KU- 
MORI NO (of cloudiness)" does not always rep- 
resent the state of the referent of the modified 
noun though "MIKETTEI NO (of indecision)" 
always represents that. "KUMORI (cloudi- 
ness)" is a natural phenomenon which can be 
pointed to concretely. For example, 
KUMORI NO NISSU 
(cloudiness) (of) (amount) 
WA 4 GATU NI SITEWA IJOU DA. 
The amount of cloudiness is unusual for April. 
In this example, "KUMORI NO (of cloudi- 
ness)" modifies "NISSU (the amount)," and 
does not represent a state but the possessor of 
the amount. 
As the examples of "MIKETTEI NO (of 
indecision)" and "KUMORI NO (of cloudi- 
ness)" show, there are nouns which have the 
same properties as adjectives intrinsically (e.g. 
"MIKETTEI (indecision)"), and other nouns 
which intrinsically have different properties 
from adjectives (e.g. "KUMORI (cloudiness)"). 
So, it is important o consider the properties of 
the noun in "noun + NO" when we analyze the 
"noun + NO" which behaves emantically like 
an adjective. Such an analysis enables us to find 
the situation in which they act like adjectives. 
We classified nouns in "noun + NO" structures 
into three types based on what the nouns refer 
to. Nouns from the last category, 3), are similar 
to adjectives emantically. As adjectives do not 
represent concrete objects or verb-like notions, 
nouns from these categories only occasionally 
resemble adjectives. 
63 
Noun Categories: 
1) nouns which refer to concrete objects. (like 
rain, book, science, and so on) 
2) nominalizations (like decision, work, and so 
on) 
3) nouns which belong to neither 1) nor 2), 
e.g. abstract nouns and so on. 
As our corpora contain mainly newspaper ar- 
ticles, many compound nouns appear. Since the 
last word in a compound noun determines the 
properties of the whole word, we focus on the 
last word in classifying them. 
Table 2 contains examples of the noun cate- 
gories. "KOUGYOU TOSHI (industry city)" is 
an example of a compound noun where the last 
word "TOSHI (city)" determines the properties. 
Table 2: Some "noun + NO" constructions with 
"impression" 
1) nouns which refer to concrete objects 
KOUGYOU TOSHI, HINOKI 
(industry city) (cypress) 
2) nominalizations 
SOKUBAKU, KOUTEN 
(restriction) (improvement) 
3) nouns which belong to neither 1) nor 2) 
MUTONTYAKU, JAKUSHOU 
(carelessness) (weakness) 
In the following section, we analyze the sim- 
ilarities and differences of the semantic behav- 
ior of "noun + NO" structures and adjectives. 
Firstly, we describe the case in which the se- 
mantic behavior of "noun + NO" is similar to 
that of adjectives and then we mention the case 
in which the semantic behavior of "noun + NO" 
is different from that of adjectives. Secondly, we 
analyze several types of nouns in "noun + NO" 
which behave like adjectives, ewm though nouns 
in "noun + NO" are not intrinsically similar to 
adjectiw; types. 
5.2 The di f ferences of semant ic  
behav ior  between nouns  in "noun 
-b NO"  and adject ives 
For example, "KANASHII (sad)", "URESHII 
(pleasurable)", "ZANNEN_NA (regrettable)", 
"KANASHIMI NO (of sadness)", "YOROKOBI 
NO (of delight)" and so on, modify nouns 
such as "OMOI (thought)", "KANJI (emo- 
tion)" and so on. Using a set of adnomi- 
nal constituents, such as "KANASHII (sad)", 
"URESHII (pleasurable)", "ZANNEN..NA (re- 
grettable)", as keys for classification, we can 
classify the modified nouns, "OMOI (thought)", 
"KANJI (feeling)" and so on, into the same 
group. Then we can find a semantic relation- 
ship between these adnominal constituents and 
their head nouns, in this case, <emotion>. In 
the following, we describe the similar and dif- 
fering semantic behaviors of "noun ? NO" and 
other adjectives in the same semantic ategory. 
As we described in the previous ection, we ex- 
tract sets of "noun + NO" structures and ad- 
jectives from data which was sorted semanti- 
cally. Words in each set represent he seman- 
tic substance of the similar nouns which they 
modify. Therefore, their semantic categories 
are similar. Examples of modified nouns of a 
similar semantic category and their modifiers 
which have a semantic ategory similar to that 
of the nouns are listed in Table 3. Included are 
some "noun ? NO" examples which though co- 
occurring with <mental state> nouns are not 
classified as such themselves. There are many 
adjectives and nominal adjectivals which can 
modify nouns in Table 3, such as "AWARENA 
(poor)", "IJIRASHII (moving)" and "HOKO- 
RASHII (triumphant)." Some "noun ? NO" 
structures are semantically similar to these ad- 
jectives ince they represent the contents of the 
emotion, e.g. "FUKAI NO KAN (sensation of 
displeasure)" and "YOROKOBI NO KIMOCHI 
(feeling of delight)." Most nouns in these "noun 
+ NO" structures in Table 3 are classified into 
"mental activity by humans" by the "Word List 
Classified by Semantic Principles3. '' "Noun + 
NO" structures, which have this kind of seman- 
tic; category, are similar to adjectives and nom- 
inal adjectivals, as both represent he content 
of the human mind. We call this semantic at- 
'~This list was compiled by The Natural Language Re- 
search Institute, Tokyo. 
64 
Table 3: The modified nouns and adjectives, 
nominal adjectivals, and "noun + NO" 
collected in the semantic ategory, 
<mental state> 
Modi f ied nouns 
KANJI (feeling), KAN (sensation), 
OMOI (thought), KI (intention), 
NEN (inclination), KIMOCHI (mind), 
KIBUN (mood), KANJO (emotion), 
JO (passion) 
Adject ives  and nominal  adject ivals 
AWARE_NA (poor), IJIRASHII (moving), 
HOKORASHII (triumphant), 
KINODOKU_NA (unfortunate), 
SHIAWASE_NA (happy), 
ZANNEN_NA (disappointing), 
URESHII (pleasurable), ...and so on. 
"Nouns"  in the "noun + NO"  s t ruc ture  
a) mental  act iv i ty  
KANASHIMI (sadness), FUKAI (displeasure), 
SHITASHIMI (familiarity), 
ZOUO (abhorrence), GAMAN (endurance), 
KOUKAI (regret), YOROKOBI (joy), 
MANZOKU (satisfaction), 
RAKUTAN (disappointment), 
IGAI (unexpected), ...and so on. 
b) nominal izat ions 
HOSHIN (self-defense), 
CHIKUZAI (moneymaking), 
INTAI (retirement), HIHAN (criticism), 
HIYAKU (rapid progress), ...and so on 
egory created by these adnominal constituents 
and their modified nouns "Feeling." 
On the other hand, some adnominal rela- 
tionships concerning a mental state can only 
be represented by "noun + NO" structures, 
such as "HOSHIN NO KIMOCHI (desire of de- 
fending one's own interest)," "CHIKUZAI NO 
NEN (thought of moneymaking)" and "INTAI 
NO KIMOCHI (idea of retirement)." Event 
nouns are mainly used in these "noun + NO" 
structures. Adnominal modifying relations of 
"nominalization + NO + mental state_noun" 
structures represent an intentional mental state. 
This kind of intentional mental state cannot be 
expressed by adjectives. We call this semantic 
category "Intentional mental state." 
We discussed two types of semantic represen- 
tations above, i.e. Feeling and Intentional men- 
tal state. Feeling can be represented by adjec- 
tives and "noun + NO" structures. However, 
Intentional mental state can be represented 
only by "noun + NO" structures. From the 
standpoint of the characteristics of the modified 
nouns (they represent human mental states), 
these two mental activities (Feeling and Inten- 
tional mental state) are similar, even though 
there are .differences in whether the activity is 
intentional or not. However, from the stand- 
point of the selection of an adnominal relation- 
ship in the surface structure, whether the activ- 
ity has active intention or not will be the decid- 
ing factor for the selection between adjectives 
and "noun + NO" structures. 
5.3 The case where  the semant ic  
behav ior  of "noun + NO"  
structures is similar to that of 
adjectives 
Here we focus on nouns whose properties are 
unlike those of adjectives, i.e. the nouns which 
refer to concrete objects, verbal notions and so 
on.  
(1) In the case where "noun + NO" represents 
characteristics, there is some overlap be- 
tween the semantic behavior of adjectives 
and "noun + NO" structures. 
I) The case where the noun in "noun + NO" 
is a compound noun 
Let us compare "noun + NO" with adjective 
usage. 
MUKUCHI_NA INSHOU 
(reticent) (impression) 
GA TUYOI JOHN-SAN WA "" 
Jotm who makes a reticent impression "-" 
KOUGYOUTOSHI NO INSHOU 
(industry city) (of) (impression) 
GA TUYOI KAWASAKISHI WA... 
65 
KAWASAKI city which gives a strong im- 
pression of an industrial city 4 
b) Modified nouns which represent instances 
of the concrete nouns in compound nouns 
In the previous two examples, the differences 
between "noun + NO" and adjectives depend 
only on whether the nouns they modify rep- 
resent a person or a city where both head 
nouns have characteristics in common. How- 
ever, "KOUGYOUTOSHI (industry city)" does 
not always have the same semantic relation to 
the modified noun, as seen in the following ex- 
ample: 
KOUGYOUTOSHI NO YUKYUTI 
(industry city) (of) (vacant land) 
NI TYAKUMOKU. 
They noticed the vacant land 
in the industrial city. 
In this example, the semantic relation be- 
tween "KOUGYOUTOSHI NO (of industry 
city)" and "YUKYUTI (the vacant land)" indi- 
cate the relation of possession so that it is not a 
semantic relation that adjectives can represent. 
When the modified nouns are abstract nouns 
that represent the property ("INSHOU (impres- 
sion)" or "SEIKAKU (characteristics)" etc.), or 
instances of the concrete nouns in compound 
nouns ("KAWASAKI SHI (KAWASAKI city)"), 
the semantic function of compound nouns in 
"noun + NO" constructions represent the char- 
acteristics of the referent of the modified nouns 
as adjectives do. 
a) Modified nouns which are abstract nouns 
that represent a property. 
KOUGYOUTOSHI NO IMEJI 
(industry city) (of) (image) 
GA OOKII. 
The image of an industrial city is strong. 
KOUKYUUHIN NO INSHOU 
(high quality item) (of) (impression) 
GA TUYOI SHANERU 
(with) CHANNEL the impression of 
a high-quality item is strong. 
4Note that some words which are nouns in Japanese 
(e.g. industry, high quality)must be translated as adjec- 
tiw~ in English (e.g. industrial, high-quality) 
<city-SUZUKA-SHI> 
KOUGYOUTOSHI NO SUZUKA SHI 
(industry city) (of) (SUZUKA city) 
SUZUKA city which is an industrial city 
<item-diamonds> 
KOUKYUUHIN NO DAIYA 
(high quality item) (of) (diamond) 
Diamonds are a high-quality item 
<company-IBM> 
YURYOUGAISHA NO 
(excellent company) (of) 
IBM is an excellent company 
IBM 
When the modified noun is an instance 
of the last word of the modifying com- 
pound noun, the semantic function of the 
whole compound noun is similar to that 
of adjectives because, in this type of com- 
pound, we focus on the adjectival semantic 
element. For example, "KOUGYOU (indus- 
try)" in "KOUGYOUTOSHI (industry city)", 
"KOUKYUU (high-quality)" in "KOUKYU- 
UHIN (high quality item)", and "YUURYOU 
(excellent)" in "YUURYOUGAISHA (excellent 
company)" are adjectival. 
II) the nouns that refer to the concrete object 
in "noun + NO" 
Originally the nouns that refer to a concrete 
object or event do not have the same meaning as 
adjectives, however, they have similar semantic 
behavior to that of adjectives in the following 
case. 
KARE WA OTONASHII KIHUU 
(mild) (disposition) 
NO MOTINUSHI DA. 
He has a mild disposition. 
The "mild" represents he characteristic (dis- 
position). In the following examples the "noun 
+ NO" also indicate the characteristics of some- 
thing. 
66 
KODOMOTACHI WA ... MASSUGU 
NOBIRU 
HINOKI 
(HINOKI-tree) 
These children 
NO INSHOU 
(of) (impression) 
GA ARIMASHITA. 
give the impression of a 
HINOKI-tree which grows straight. 
KAGAKUGAISHA TOIU KOTODE IPPAN 
NO HITO NIWA 
KANKYOUOSEN NO INSHOU 
(environment pollution) (of) (impression) 
impression of environmental pollution 
GA TUYOKATTA. 
Ordinary people have a strong impression of 
environmental pollution from the chemical 
company. 
The impression the children make is of a 
"HINOKI (HINOKI-tree)" and the impression 
the chemical company makes is of "KANKY- 
OUOSEN (environmental pollution)". These 
"noun + NO'structures represent he charac- 
teristics of children and a company in same 
manner that the adjective "mild" indicates his 
characteristic. In these examples, nouns in 
"noun + NO" represent objects and events and 
so on, i.e. "HINOKI-tree" and "environmental 
pollution" these nouns ordinarily do not behave 
like adjectives. That is, the adjective "mild" 
can represent a characteristic directly, however, 
these nouns in "noun + NO" cannot represent 
the characteristics of something directly. We 
cannot say "that children are HINOKI-tree" 
and "the company is the environmental pollu- 
tion" while we can say "He is mild." That is, in 
this case, "noun + NO" cannot appear in the 
predicative position with this meaning. When 
we show the characteristics of something by us- 
ing nouns that refer to concrete objects and 
events, we need to specify the modified nouns 
which indicate the characteristics like "impres- 
sion, .... disposition" and so on. 
(2) "Noun + NO" can represent quantification. 
Some adjectives (:an also represent quantifi- 
cation. 
NIHON NO.HASHIMOTO SHUSHOU NO 
TEIAN WA AIKAWARAZU 
67 
TYUUSHOUTEKI_NA IKI 
(abstract) (level) 
NI TODOMATTA. 
The suggestion of the Japanese prime minis- 
ter, Hashimoto, was still in an abstract state. 
HUSAINO HIRITU GA KAKEI NI TOTTE 
KIKEN_NA IKI NI TASSHITEIRU. 
(dangerous) (level) 
The rate of debt has reached a dangerous 
level for the household budget. 
The suggestion of the Japanese prime min- 
ister is at an "abstract" level on the "concrete- 
abstract" scale and the rate of debt is at a "dan- 
gerous" level on the "safety-dangerous" scale. 
The level of concreteness and safety is repre- 
sented by adjectives. On the other hand, the 
nouns that refer to concrete objects and verbal 
notions also represent a level by inference from 
the context. We can infer the scale from the 
contextual situation. For example, 
KOUNIN KOUHO WA 
UWASA NO DANKAI 
(rumor) (of) (stage) 
the stage of rumor 
DA GA BORUGA SHI 
Though it is completely at the stage of ru- 
mor, the candidate for the succession is Mr. 
Borgar ... 
SHUSHOU GAWA WA "" 
(the prime minister and his staff) 
ENZETU NO IKI 
(speech) (of) (level) 
WO KOERARENAKATTA. 
Though the prime minister and his staff said 
"we will specify the guidelines of the govern- 
ment proposal during the election", after all 
it was still at the level of speech. 
GIJUTUTEKINIWA 
KANSEI NO IKI 
(completeness) (of) (level) 
NI TASSHITEITA. 
It reached a level of completeness, technically. 
In the above case, we do not have a seman- 
tic element of actual "talk" in the "rumor" 
or "speech" meaning nor a semantic element 
"event" in the "completeness" meaning, but we 
have the level of "rumor" on the "truth-rumor" 
scale, the level of "speech" on the "statement- 
speech" scale and the level of "completeness" on 
the "incompleteness-completeness" scale. The 
nouns that refer to concrete objects and verbal 
actions are similar to adjectives when they rep- 
resent a level in context. 
6 Conc lus ion  
In this paper, we discussed the similarities 
and differences among adnominal constituents, 
i.e. adjectives and "noun + NO" structures 
which have a broad range of semantic functions. 
Nouns and adjectives differ in part of speech, 
but they sometimes have similarities when used 
adnominally. In such a case, we need not dis- 
tinguish them from each other semantically. We 
investigated explicit criteria to detect similari- 
ties and differences between nouns and adjec- 
tives in adnominal usage. This research was ver- 
ified by using large corpora and a self-organizing 
mapping system based on the neural network 
model. In future work, we will attempt o sys- 
tematically classify words used adnominally ac- 
cording to the semantic behavior of adnominal 
constituents following the theoretical insights of 
Pustejovsky. 
Acknowledgment 
We would like to thank Catherine Macleod of 
New York University and Kiyotaka Uchimoto 
of the Communications Research Laboratory for 
their invaluable help in writing this paper. 
References 
P. Bouillon. 1996. Mental State Adjectives: the 
Perspective of Generative Lexicon. In Proc. 
of COLING96. 
G. Grefenstette. 1994. Corpus-Derived First, 
Second and Third-Order Word Affinities. In ' 
Proc. off the EURALEX '9~. 
H. Isahara and K. Kanzaki. 1999. Lexical Se- 
mantics to Disambiguate Polysemous Phe- 
nomena of Japanese Adnominal Constituents. 
In Proc. of A CL99. 
Q. Ma, K. Kanzaki, M. Murata, K. Uchi- 
moto, and H. Isahara. 2000. Construction 
of a Japanese Semantic Map using Self- 
Organizing Neural Network Model. In 6th 
Annual Meeting of the Association for Nat- 
ural Language Processing, Japan. (will ap- 
pear). 
J. Pustejovsky. 1995. The Generative Lexicon. 
The MIT Press. 
P. Saint-Dizier. 1998. A Generative Lex- 
icon Perspective for Adjectival Modifica- 
tion. In Proc. of the Conference volume2 
in 36th Annual Meeting of the Associa- 
tion for Computational Linguistics and 17th 
International Conference on Computational 
Linguistics(COLING-A CL '98). 
A. Shimazu, S. Naito, and H. Nomura. 1986. 
Analysis of semantic relations between ouns 
connected by a Japanese particle "no". 
Keiryo Kokugogaku (Mathematical Linguis- 
tics), 15(7). (in Japanese). 
68 
 
	ff 	

Trend Survey on Japanese Natural Language Processing Studies
over the Last Decade
Masaki Murata?, Koji Ichii?, Qing Ma?,?, Tamotsu Shirado?,
Toshiyuki Kanamaru?,?, and Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{murata,qma,shirado,kanamaru,isahara}@nict.go.jp
?Port and Airport Research Institute
Nagase 3-1-1, Yokosuka, Kanagawa 239-0826, Japan, ichii@pari.go.jp
?Ryukoku University, Otsu 520-2194, Japan, qma@math.ryukoku.ac.jp
?Kyoto University, Yoshida-Nihonmatsu, Sakyo, Kyoto 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Abstract
Using natural language processing, we
carried out a trend survey on Japanese
natural language processing studies that
have been done over the last ten years.
We determined the changes in the num-
ber of papers published for each re-
search organization and on each re-
search area as well as the relationship
between research organizations and re-
search areas. This paper is useful
for both recognizing trends in Japanese
NLP and constructing a method of sup-
porting trend surveys using NLP.
1 Introduction
We conducted a trend survey on Japanese nat-
ural language processing studies that have been
done over the last ten years. We used biblio-
graphic information from journal papers and an-
nual conference papers of the Association for
Natural Language Processing, Japan (The Asso-
ciation for Natural Language Processing, 1995-
2004; The Association for Natural Language Pro-
cessing, 1994-2003). Just ten years have passed
since the association was established. Therefore,
we can use the bibliographic information from the
past ten years. In this study, we investigated what
kinds of studies have been presented in journal
papers and annual conference papers on the Asso-
ciation for Natural Language Processing, Japan.
We first digitized documents listed in the bibli-
ographic information and then extracted various
pieces of useful information for the trend survey.
Figure 1: Change in the number of papers
We also examined the changes in the number of
papers put up by each Japanese research orga-
nization and the changes in the number of pa-
pers written on specific research areas. More-
over, we examined the relationship between each
Japanese research organization and each research
area. This study is useful for trend surveys of
studies performed by members of in the Associa-
tion for Natural Language Processing, Japan.
2 Trend survey on NLP research studies
We show the changes in the number of journal
papers and conference papers in Figure 1. Jour-
nal papers are reviewed, but conference papers are
not reviewed in the association. In comparing the
journal papers and conference papers, we found
that the number of conference papers was much
larger than that of journal papers. We also found
that although both types of papers decreased in
number at some point, they both demonstrate an
upward trend.
Conference papers have a temporal peak in the
fourth year and a temporal drop in the sixth year,
250
Figure 2: Change in the number of journal papers
by each research organization (The two numbers in
the parentheses indicate the total number of papers
and the average value of published years.)
while journal papers have a peak in the sixth year
and a drop in the eighth year. The temporal peak
and drop of the journal papers occurred just two
years after the peak and drop of the conference
papers. We presume this is because journal papers
need more time for reviewing and publishing, and
because journal papers are presented later than
conference papers for studies performed at the
same time.
3 Trend survey on research
organizations
Next, we investigated the change in the number
of papers put out by each research organization.
The results are represented in contour in Figures
2 and 3. The height in contour (the depth of a
black color) indicates the number of papers. We
calculated the average (we call it average value)
of the average, the mode, and the median of the
published years by using the data of the number
of papers performed by each research organiza-
tion. In the figures, each research organization is
listed in ascending order of the average value. We
added the total number of papers and the average
value to each research organization in the figures.
Therefore, research organizations that had many
papers in the earlier years are displayed higher
on the list, while research organizations that had
Figure 3: Change in the number of conference pa-
pers by each research organization
many papers in the later years are displayed lower.
Here, we displayed only research organizations
that had many total papers. If a research orga-
nization?s name was changed during the ten-year
period, we used the name that had the most usage
on published papers for displaying it.1
From these figures, we can see that ATR and
CRL (NICT) put out many journal papers, and
NTT, ATR, Tokyo Institute of Technology, CRL,
and the University of Tokyo put out many confer-
ence papers. We also found that while NTT and
ATR had many papers in the earlier years, CRL
and the Univ. of Tokyo had many papers in the
later years. We can expect that because CRL and
the Univ. of Tokyo demonstrate an upward ten-
dency, their quantity of papers will continue to in-
crease in the future. Using these figures, we can
see very easily in which reference year each re-
search organization put out many papers.
4 Trend survey on research areas
Next, we investigated the change in the number
of papers in each research area. The results are in
Figures 4, 5, and 6. (Because the volume of data
for conference papers was large, it was divided
into two figures.). For journal papers, the height
1When we counted the frequency of a research organiza-
tion whose name was changed, we used all the names of it
including old and new names.
251
Figure 4: Change in the number of journal papers
in each research area
in contour indicates the number of papers. For
conference papers, the height in contour indicates
the base two logarithm of the number of papers
added by one. Using the same method as that de-
scribed above, we calculated the average of the
average, mode, and median of the years papers
were published using the data of the number of
papers in each research area. In the figures, each
research area is displayed in ascending order of
the average value. We added the total number of
papers and the average value to each research area
in the figures. Here, we divided the title of each
paper into words using ChaSen software (Mat-
sumoto et al, 1999), and we treaded each word as
a research area. A paper with a particular word in
Figure 5: Change in the number of conference pa-
pers in each research area (part I)
its title was categorized in the research area indi-
cated by the word. Wemanually eliminated words
that were not indicative of a research area, for ex-
ample, ?teki? (of) and ?kenkyu? (study).
From these figures, it is clear that the research
areas of ?Japanese? and ?analysis? were studied
in an especially large number of papers. We
also found that for journal papers, because the
research areas of ?verb?, ?noun?, ?disambigua-
tion?, ?probability?, ?corpus?, and ?polysemic?
were displayed higher on the list, these areas were
studied thoroughly in the earlier years. Likewise,
we found that the research areas of ?morphol-
ogy?, ?dependency?, ?dialogue?, and ?speech?
were studied thoroughly in the sixth year and the
252
Figure 6: Change in the number of conference pa-
pers in each research area (part II)
research areas of ?summarization?, ?retrieval?,
?translation? and so on were studied well in the
later years. Special journal issues on ?summariza-
tion? were published in the sixth and ninth years,
so the research area of ?summarization? was rep-
resented in many papers in those years. We can
expect that because the research area of ?transla-
tion? demonstrates an upward tendency, the num-
ber of papers on this topic will continue to in-
crease in the future.
In terms of conference papers, we found that
the research areas of ?bilingual?, ?morphology?,
?probability?, ?dictionary?, ?statistics?, and so on
were studied well in the earlier years. In the lower
part of the figures, such research areas as ?re-
Figure 7: Change in the number of conference pa-
pers at each research organization in the research
area of ?translation?
Figure 8: Change in the number of conference pa-
pers in each research area in the research area of
?translation?
trieval?, ?summarization?, ?question? and ?para-
phrase? are found. Thus, we can see that these
research areas were studied thoroughly in recent
years. We can see very easily in which reference
years each research area was studied using these
figures.
5 Trend survey using part of data
Although we have focused on using all the data
in the trend survey so far, we can narrow down
the survey by looking only at a certain part of
the data. For example, when we want to exam-
253
Figure 9: Relationship between research organizations and research areas in journal papers (The name
of each research organization is given a ??? symbol.)
ine a trend survey on translation in more detail,
all we have to do is to extract papers on transla-
tion and use them for a trend survey. We carried
out a trend survey on machine translation in this
manner. We first extracted papers whose titles in-
cluded the word ?translation? and then performed
the same investigations as in Sections 3 and 4.
The results are in Figures 7 and 8. The height in
contour (the depth of a color) indicates the num-
ber of papers. From Figure 7, we can see that
NTT had many papers in the earlier years, and
ATR had many papers in later years. From Figure
7, we can also see that studies on translation of-
ten dealt with specific topics such as ?semantics?,
?knowledge? and ?dictionary? in earlier years and
?support?, ?example?, and ?retrieval? in more re-
cent years.
6 Relationship between research
organizations and research topics
Finally, we investigated the various research ar-
eas that research organizations studied more fre-
quently during the ten-year period. Here, we
show only the results for journal papers. We used
the same method as in the previous sections for
extracting research organizations and research ar-
eas from the data. We counted the cooccurrent
frequency of each research organization and each
research area. We then constructed a cross table
in this manner and then performed the dual scal-
ing method (Weller and Romney, 1990; Ueda et
al., 2003). The result is depicted in Figure 9. The
dual scaling method displays the relationship be-
tween research organizations and research areas.
In Figure 9, ?translation? appears in the lower
left quadrant, ?learning? appears in the lower
right quadrant, ?statistics? and ?retrieval? appear
in the upper right quadrant, and ?noun? and ?sen-
tence? appear in the upper left quadrant. In the
vicinity around these words, the research areas
and organizations relating to them appear. For ex-
ample, in the upper right quadrant, Hitachi and
University of Tokushima appear near ?statistics?
and ?retrieval?, which were frequent study topics
for them. Similarly, ?summarization? appears in
the near upper right area of the source origin and
is surrounded by JAIST, Toyohashi University of
Technology, and Tokyo Institute of Technology.,
indicating it was a frequent topic of study at those
institutions. We can easily see which research
topics were primarily studied by each organiza-
tion using this figure.
Also in Figure 9, research areas on numeri-
cals such as ?probability? and ?learning? appear
254
on the right side. Therefore, we can interpret the
figure as depicting quantitative research topics on
the right side and qualitative research topics on
the left side. Research areas using complicated
processing such as ?learning? and ?translation?
appear in the lower area and research areas deal-
ing with theory such as ?probability?, ?grammar?,
?sentence?, and ?noun? appear in the upper area.
Therefore, we can interpret the figure as depict-
ing theoretical research topics in the upper area
and research topics using complicated processing
in the lower area.
7 Conclusion
In this paper, we described a trend survey carried
out on Japanese natural language processing stud-
ies done over the last ten years. We were able to
investigate trend surveys on research areas very
easily by treating divided words in titles by a mor-
phological analyzer as the indications of research
areas. We displayed the changes in the number of
papers put out by each research organization and
written on specific research topics. We also dis-
played the relationship between research organi-
zations and research areas using the dual scaling
method. The simple methods we used that are de-
scribed here made it possible to show many useful
results.
This paper has the following two significant ef-
fects:
 This paper explained a trend survey on
Japanese natural language processing. By
reading it, we can understand the trends in
research on Japanese natural language pro-
cessing. For example, we can find out
which research areas were studied more of-
ten and we can see which research organiza-
tions were involved in studying natural lan-
guage processing. We can also see which re-
search organization studied a particular re-
search area most often over the ten-year pe-
riod.
 We used natural language processing to
carry out the trend survey described here.
For example, we automatically detected the
indication of a research area from words
used in titles by using a morphological ana-
lyzer. In addition, we displayed words that
were extracted by the morphological ana-
lyzer in several ways to display the results
of the trend survey effectively. The methods
used in this paper would be useful in other
trend surveys.
In short, this paper is useful for recognizing trends
in Japanese NLP and for constructing methods of
supporting trend surveys using NLP.
In the future, we would like to perform an in-
ternational trend survey on natural language pro-
cessing using international conference and jour-
nal papers such as IJCNLP, ACL, and the Journal
of Computational Linguistics. We would also like
to do trend surveys on other topics such as AI, bi-
ology, politics, and sociology.
The kinds of investigations we did can easily be
altered to do many other kinds of investigations
as well. For example, we can use the dual scal-
ing method by investigating the relationship be-
tween the reference years and the research organi-
zations/areas. We can also use the representation
in contour for the relationship between research
organizations and research areas. Although we
showed the data in ascending order of the aver-
age value of the published years, we could show
the data in different order, for example, the or-
der of the total number of papers or the order of
the location, i.e., showing similar research orga-
nizations/areas that are located near each other by
clustering research organizations/areas using their
cooccurrent words. We would like to continue
to study these kinds of support methods for trend
surveys in the future.
References
The Association for Natural Language Processing. 1994-
2003. Journal of Natural Language Processing.
The Association for Natural Language Processing. 1995-
2004. Proceedings of the Annual Meeting of The Associ-
ation for Natural Language Processing.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, and Masayuki Asahara.
1999. Japanese morphological analysis system ChaSen
version 2.0 manual 2nd edition.
Taichiro Ueda, Masao Karita, and Kazue Honda. 2003. Jis-
sen Workshop Excel Tettei Katsuyou Tahenryou Kaiseki.
Shuuwa System. (in Japanese).
Susan C. Weller and A. Kimball Romney. 1990. Metric
Scaling : Correspondence Analysis (Quantitative Appli-
cations in the Social Sciences). SAGE Publications.
255
Information Retrieval Capable of Visualization and High Precision
Qing Ma1,2 and Kousuke Enomoto1
1Ryukoku University / 2NICT, Japan
qma@math.ryukoku.ac.jp
Masaki Murata and Hitoshi Isahara
NICT, Japan
{murata,isahara}@nict.go.jp
Abstract
We present a neural-network based self-
organizing approach that enables vi-
sualization of the information retrieval
while at the same time improving its
precision. In computer experiments,
two-dimensional documentary maps in
which queries and documents were
mapped in topological order accord-
ing to their similarities were created.
The ranking of the results retrieved us-
ing the maps was better than that of
the results obtained using a conven-
tional TFIDF method. Furthermore, the
precision of the proposed method was
much higher than that of the conven-
tional TFIDF method when the process
was focused on retrieving highly rel-
evant documents, suggesting that the
proposed method might be especially
suited to information retrieval tasks in
which precision is more critical than re-
call.
1 Introduction
Information retrieval (IR) has been studied since
an earlier stage [e.g., (Menzel, 1966)] and sev-
eral kinds of basic retrieval models have been pro-
posed (Salton and Buckley, 1988) and a number
of improved IR systems based on these models
have been developed by adopting various NLP
techniques [e.g., (Evans and Zhai, 1996; Mitra
et al, 1997; Mandara, et al, 1998; Murata, et
al., 2000)]. However, an epoch-making technique
that surpasses the TFIDF weighted vector space
model, the main approach to IR at present, has not
yet been invented and IR is still relatively impre-
cise. There are also challenges presenting a large
number of retrieval results to users in a visual and
intelligible form.
Our aim is to develop a high-precision, visual
IR system that consists of two phases. The first
phase is carried out using conventional IR tech-
niques in which a large number of related docu-
ments are gathered from newspapers or websites
in response to a query. In the second phase the
visualization of the retrieval results and picking
are performed. The visualization process clas-
sifies the query and retrieval results and places
them on a two-dimensional map in topological
order according to the similarity between them.
To improve the precision of the retrieval process,
the picking process involves further selection of a
small number of highly relevant documents based
on the classification results produced by the visu-
alization process.
This paper presents a new approach by using
the self-organizing map (SOM) proposed by Ko-
honen (Kohonen, 1997) for this second IR phase1.
To enable the second phase to be slotted into a
practical IR system as described above, visual-
1There have been a number of studies of SOM on data
mining and visualization [e.g., (Kohonen, et al, 2000)] since
the WEBSOM was developed in 1996. To our knowledge,
however, these works mainly focused on confirming the ca-
pabilities of SOM in the self-organization and/or in the vi-
sualization. In this study, we slot the SOM-based processing
into a practical IR system that enables visualization of the
IR while at the same time improving its precision. The an-
other feature of our study differing from others is that we
performed comparative studies with TFIDF-based IR meth-
ods, the major approach to IR in NLP field.
138
ization and picking should be carried out for a
single query and set of related documents. In
this paper, however, for the purpose of evaluating
the proposed system, correct answer data, consist-
ing of multiple queries and related documents as
used in the 1999 IR contest, IREX (Murata, et
al., 2000), was used. The procedure of the sec-
ond IR-phase in this paper is therefore as follows.
Given a set of queries and related documents, a
documentary map is first automatically created
through self-organization. This map provides vis-
ible and continuous retrieval results in which all
queries and documents are placed in topological
order according to their similarity2. The docu-
mentary map provides users with an easy method
of finding documents related to their queries and
also enables them to see the relationships between
documents with regard to the same query, or even
the relationships between documents across dif-
ferent queries. In addition, the documents related
to a query can be ranked by simply calculating
the Euclidean distances between the points of the
queries and the points of the documents in the
map and then choosing the N closest documents
in ranked order as the retrieval results for each
query. If a small N is set, then the retrieval results
are limited to the most highly relevant documents,
thus improving the retrieval precision.
Computer experiments showed that meaning-
ful two-dimensional documentary maps could be
created; The ranking of the results retrieved us-
ing the map was better than that of the results ob-
tained using a conventional TFIDF method. Fur-
thermore, the precision of the proposed method
was much higher than that of the conventional
TFIDF method when the retrieval process focused
on retrieving the most highly relevant documents,
which indicates that the proposed method might
be particularly useful for picking the best docu-
ments, thus greatly improving the IR precision.
2 Self-organizing documentary maps
and ranking related documents
A SOM can be visualized as a two-dimensional
array of nodes on which a high-dimensional in-
2For a specific query, other queries and documents in the
map are considered to be irrelevant (i.e., documents unre-
lated to the query). This map is therefore equivalent to a
map consisting of one query and related and unrelated docu-
ments, which will be adopted in the practical IR system that
we aim to develop.
put vector can be mapped in an orderly manner
through a learning process. After the learning, a
meaningful nonlinear coordinate system for dif-
ferent input features is created over the network.
This learning process is competitive and unsuper-
vised and is called a self-organizing process.
Self-organizing documentary maps are ones in
which given queries and all related documents
in the collection are mapped in order of similar-
ity, i.e., queries and documents with similar con-
tent are mapped to (or best-matched by) nodes
that are topographically close to one another, and
those with dissimilar content are mapped to nodes
that are topographically far apart. Ranking is the
procedure of ranking documents related to each
query from the map by calculating the Euclidean
distances between the points of the queries and
the points of the documents in the map and choos-
ing the N closest documents as the retrieval result.
2.1 Data
The queries are those used in a dry run of the
1999 IREX contest and the documents relating to
the queries are original Japanese newspaper arti-
cles used in the contest as the correct answers. In
this study, only nouns (including Japanese verbal
nouns) were selected for use.
2.2 Data coding
Suppose we have a set of queries:
Q = {Q i (i = 1, ? ? ? , q)}, (1)
where q is the total number of queries, and a set
of documents:
A = {Ai j (i = 1, ? ? ? , q, j = 1, ? ? ? , ai)},
(2)
where ai is the total number of documents related
to Q i. For simplicity, where there is no need to
distinguish between queries and documents, we
use the same term ?documents? and the same no-
tation Di to represent either a query Q i or a doc-
ument Ai j. That is, we define a new set
D = {Di (i = 1, ? ? ? , d)} = Q
?
A (3)
which includes all queries and documents. Here,
d is the total number of queries and documents,
i.e.,
d = q +
q?
i=1
ai. (4)
139
Each document, Di, can then be defined by the
set of nouns it contains as
Di = {noun(i)1 , w(i)1 , ? ? ? , noun(i)ni , w(i)ni }, (5)
where noun(i)k (k = 1, ? ? ? , ni) are all different
nouns in the document Di and w(i)k is a weight
representing the importance of noun(i)k (k =
1, ? ? ? , ni) in document Di. The weights are com-
puted by their tf or tfidf values. That is,
w(i)j = tf(i)j or tf(i)j idfj . (6)
In the case of using tf, the weights are normalized
such that
w(i)1 + ? ? ?+ w(i)ni = 1. (7)
Also, when using the Japanese thesaurus, Bun-
rui Goi Hyou (The National Institute for Japanese
Language, 1964) (BGH for short), synonymous
nouns in the queries are added to the sets of
nouns from the queries shown in Eq. (5) and their
weights are set to be the same as those of the orig-
inal nouns.
Suppose we have a correlative matrix whose el-
ement dij is some metric of correlation, or a sim-
ilarity distance, between the documents Di and
Dj ; i.e., the smaller the dij , the more similar the
two documents. We can then code document Di
with the elements in the i-th row of the correlative
matrix as
V (Di) = [di1, di2, ? ? ? , did]T . (8)
The V (Di) ? <d is the input to the SOM. There-
fore, the method to compute the similarity dis-
tance dij is the key to creating the maps. Note
that the individual dij of vector V (Di) only re-
flects the relationships between a pair of docu-
ments when they are considered independently.
To establish the relationships between the doc-
ument Di and all other documents, representa-
tions such as vector V (Di) are required. Even
if we have these high-dimensional vectors for
all the documents, it is still difficult to estab-
lish their global relationships. We therefore need
to use an SOM to reveal the relationships be-
tween these high-dimensional vectors and repre-
sent them two-dimensionally. In other words, the
role of the SOM is merely to self-organize vec-
tors; the quality of the maps created depends on
the vectors provided.
In computing the similarity distance dij be-
tween documents, we take two factors into ac-
count: (1) the larger the number of common
nouns in two documents, the more similar the two
documents should be (i.e., the shorter the simi-
larity distance); (2) the distance between any two
queries should be based on their application to the
IR processing; i.e., by considering the procedure
used to rank the documents relating to each query
from the map. For this reason, the document-
similarity distance between queries should be set
to the largest value. To satisfy these two factors,
dij is calculated as follows:
dij =
?
??????
??????
1 if both Di and Dj
are queries
1? |Cij ||Di|+|Dj |?|Cij | not the case mentioned
above and i 6= j
0, if i=j
(9)
where |Di| and |Dj | are values (the numbers of
elements) of sets of documents Di and Dj de-
fined by Eq. (5) and |Cij | is the value of the in-
tersection Cij of the two sets Di and Dj . |Cij |
is therefore some metric of document similarity
(the inverse of the similarity distance dij) between
documents Di and Dj which is normalized by
|Di|+|Dj |?|Cij |. Before describing the methods
for computing them, we first rewrite the definition
of documents given by Eq. (5) for Di and Dj as
follows.
Di = {(c1, w(i)c1 , ? ? ? , cl, w(i)cl ),
(n(i)1 , w(i)1 , ? ? ? , n(i)mi , w(i)mi)}, (10)
and
Dj = {(c1, w(j)c1 , ? ? ? , cl, w(j)cl ),
(n(j)1 , w(j)1 , ? ? ? , n(j)mj , w(j)mj )}, (11)
where ck (k = 1, ? ? ? , l) are the common nouns of
documents Di and Dj and n(i)k (k = 1, ? ? ? ,mi)
and n(j)k (k = 1, ? ? ? ,mj) are nouns of documents
Di and Dj which differ from each other. By com-
paring Eq. (5) and Eqs. (10) and (11), we know
140
that l+mi +mj = ni + nj . Thus, |Di| (or |Dj |)
of Eq. (9) can be calculated as follows.
|Di| =
l?
k=1
w(i)ck +
mi?
k=1
w(i)k . (12)
For calculating |Cij |, on the other hand, since the
weights (of either common or different nouns)
generally differ between two documents, we de-
vised four methods which are expressed as fol-
lows.
Method A:
|Cij | =
l?
k=1
max(w(i)ck , w(j)ck ). (13)
Method B:
|Cij | =
l?
k=1
w(i)ck + w(j)ck
2 . (14)
Method C:
|Cij | =
?
?????
?????
?l
k=1 max(w(i)ck , w(j)ck ) if one is a query
and the other
is a document
?l
k=1
w(i)ck+w
(j)
ck
2 . if both are
documents
(15)
Method D:
|Cij | =
?
?????
?????
?l
k=1 max(w(i)ck , w(j)ck ) if one is a query
and the other
is a document?l
k=1 min(w(i)ck , w(j)ck ). if both are
documents
(16)
Note that we need not consider the case where
both are queries for calculating |Cij | because this
has been considered independently as shown by
Eq. (9).
3 Experimental Results
3.1 Data
Six queries Q i (i = 1, ? ? ? , q, q = 6) and 433
documents Ai j (i = 1, ? ? ? , q, q = 6, j =
1, ? ? ? , ai and
?q
i=1 ai = 433) used in the dry run
Table 1: Distribution of documents used in the
experiments
a1 a2 a3 a4 a5 a6
?6
i=1 ai
80 89 42 108 49 65 433
of the 1999 IREX contest were used for our ex-
periments. The distribution of these documents,
i.e., the number ai (i = 1, ? ? ? , q, q = 6) of docu-
ments related to each query, is shown in Table 1.
It should be noted that since the proposed IR
approach will be slotted into a practical IR sys-
tem in the second phase in which a small number
(say below 1,000, or even below 500) of the re-
lated documents should have been collected, this
experimental scale is definitely a practical one.
3.2 SOM
We used a SOM of a 40?40 two-dimensional ar-
ray. Since the total number d of queries and doc-
uments to be mapped was 439, i.e., d = q +?6
i=1 ai = 439, the number of dimensions of in-
put n was 439. In the ordering phase, the number
of learning steps T was set at 10,000, the initial
value of the learning rate ?(0) at 0.1, and the ini-
tial radius of the neighborhood ?(0) at 30. In the
fine adjustment phase, T was set at 15,000, ?(0)
at 0.01, and ?(0) at 5. The initial reference vec-
tors mi(0) consisted of random values between 0
and 1.0.
3.3 Results
We first performed a preliminary experiment and
analysis to determine which of the four methods
was the optimal one for calculating |Cij | shown
in Eqs. (13)-(16). Table 2 shows the IR precision,
i.e., the precision of the ranking results obtained
from the self-organized documentary maps cre-
ated using the four methods. The IR precision was
calculated by follows.
P = 1q
q?
i=1
#related to Q i in the retrieved ai documents
ai ,
(17)
where q is the total number of queries, # means
number, and ai is the total number of documents
related to Q i as shown in Table 1.
In the case of using tf values as weights of
nouns, method B obviously did not work. Al-
141
Table 2: IR precision for the four methods for cal-
culating |Cij |
Weight Method
A
Method
B
Method
C
Method
D
tf 0.33 0.20 0.41 0.45
tfidf 0.85 0.76 0.91 0.78
though the similarity between queries was manda-
torily set to the largest value, all six queries were
mapped in almost the same position, thus produc-
ing the poorest result. We consider the reason for
this was as follows. In general, the number of
words in a query is much smaller than the num-
ber of words in the documents, and the number
of queries is much smaller than the number of
documents collected. As described in section 2,
each query was defined by a vector consisting of
all similarities between the query and five other
queries and all documents in the collection. We
think that using the average weights of words ap-
pearing in the queries and documents to calculate
the similarities between queries and documents,
as in method B, tends to produce similar vectors
for the queries. All of these query vectors are then
mapped to almost the same position. With coding
method A, because the larger of the two weights
of a query and a document is used, the same prob-
lem could also arise in practice. There were no es-
sential differences between coding methods C and
D, which were almost equally precise. Neither of
these methods have the shortcomings described
above for methods A and B. However, when tfidf
values were used as the weights of the nouns, even
methods A and B worked quite well. Therefore, if
we use tfidf values as the weights of the nouns, we
may use either of the four methods. Based on this
analysis and the preliminary experimental result
that method C and D had highest precisions in the
cases of using tf and tfidf values as weights of the
nouns, respectively, we used methods C and D for
calculating |Cij | in all the remaining experiments.
Table 3 shows the IR precision obtained using
various methods. From this table we can see that
the proposed method in the case of SOM (w=tfidf,
C), i.e., using method C for calculating |Cij |, us-
ing tfidf values as the weights of nouns, and not
using the Japanese thesaurus (BGH), in the case
of SOM (w=tfidf, D), i.e., using method D, us-
ing tfidf values, and not using the BGH, and in
Table 3: IR precision obtained using various
methods
TFIDF TFIDF
(BGH)
SOM
(w=tf,
D)
SOM
(w=
tfidf,
C)
SOM
(w=
tfidf,
C,
BGH)
SOM
(w=
tfidf,
D)
SOM
(w=
tfidf,
D,
BGH)
0.67 0.75 0.45 0.91 0.77 0.78 0.73
Table 4: IR precision for top N related documents
N TFIDF TFIDF
(BGH)
SOM
(w=tf,
D)
SOM
(w=
tfidf,
C)
SOM
(w=
tfidf,
C,
BGH)
SOM
(w=
tfidf,
D)
SOM
(w=
tfidf,
D,
BGH)
10 0.83 0.88 0.75 1.0 0.97 1.0 0.97
20 0.79 0.86 0.68 0.99 0.95 0.98 0.97
30 0.73 0.84 0.62 0.99 0.94 0.97 0.91
40 0.71 0.82 0.58 0.98 0.90 0.97 0.87
the case of SOM (w=tfidf, C, BGH), i.e., using
method C, using tfidf values, and using the BGH
produced the highest, second highest, and third
highest precision, respectively, of all the methods
including the conventional TFIDF method. When
the BGH was used, however, the IR precision of
the proposed method dropped inversely, whereas
that of the conventional TFIDF improved. The
lower precision of the proposed method when us-
ing BGH might be due to the calculation of the
denominator of Eq. (9); this will be investigated
in future study.
Table 4 shows the IR precision obtained using
various methods when the retrieval process is fo-
cused on the top N related documents. From this
table we can see that the IR precision of the pro-
posed method, no matter whether the BGH was
used or not, or whether method C or D was used
for calculating |Cij |, was much higher than that
of the conventional TFIDF method when the pro-
cess was focused on retrieving the most relevant
documents. This result demonstrated that the pro-
posed method might be especially useful for pick-
ing highly relevant documents, thus greatly im-
proving the precision of IR.
Figure 1 shows the left-top area of a self-
organized documentary map obtained using the
proposed method in the case of SOM (w=tfidf,
D)3. From this map, we can see that query Q 4
3Note that the map obtained using the proposed method
in the case of SOM (w=tfidf, C), which had the highest IR
precision, was better than this.
142
Figure 1: Left-top area of self-organized docu-
mentary map
and its related documents A4 ? (where * denotes
an Arabic numeral), Q 2 and its related docu-
ments A2 ? were mapped in positions near each
other. Similar results were obtained for the other
queries which were not mapped in the area of the
figure. This map provides visible and continu-
ous retrieval results in which all queries and docu-
ments are placed in topological order according to
their similarities. The map provides an easy way
of finding documents related to queries and also
shows the relationships between documents with
regard to the same query and even the relation-
ships between documents across different queries.
Finally, it should be noted that each map that
consists of 400 to 500 documents was obtained in
10 minutes by using a personal computer with a
3GHZ CPU of Pentium 4.
4 Conclusion
This paper described a neural-network based self-
organizing approach that enables information re-
trieval to be visualized while improving its preci-
sion. This approach has a practical use by slot-
ting it into a practical IR system as the second-
phase processor. Computer experiments of practi-
cal scale showed that two-dimensional documen-
tary maps in which queries and documents are
mapped in topological order according to their
similarities can be created and that the ranking
of the results retrieved using the created maps
is better than that produced using a conventional
TFIDF method. Furthermore, the precision of the
proposed method was much higher than that of
the conventional TFIDF method when the pro-
cess was focused on retrieving the most relevant
documents, suggesting that the proposed method
might be especially suited to information retrieval
tasks in which precision is more important than
recall.
In future work, we first plan to re-confirm the
effectiveness of using the BGH and to further im-
prove the IR accuracy of the proposed method.
We will then begin developing a practical IR sys-
tem capable of visualization and high precision
using a two-phase IR procedure. In the first phase,
a large number of related documents are gath-
ered from newspapers or websites in response to
a query presented using conventional IR; the sec-
ond phase involves visualization of the retrieval
results and picking the most relevant results.
References
H. Menzel. 1966. Information needs and uses in science
and technology. Annual Review of Information Science
and Technology, 1, pp. 41-69.
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing & Management, 24(5), pp. 513-523.
D. A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. ACL?96, pp.
17-24.
M. Mitra, C. Buckley, A. Singhal, and C. Cardie, C.
1997. An analysis of statistical and syntactic phrases.
RIAO?97, pp. 200-214.
R. Mandara, T. Tokunana, and H. Tanaka 1998. The use
of WordNet in information retrieval. COLING-ACL?98
Workshop: Usage of WordNet in Natural Language
Processing Systems, pp. 31-37.
M. Murata, Q. Ma, K. Uchimoto, H. Ozaku, M. Uchiyama,
and H. Hitoshi 2000. Japanese probabilistic informa-
tion retrieval using location and category information.
IRAL?2000.
T. Kohonen 1997. Self-organizing maps. Springer, 2nd
Edition.
T. Kohonen, S. Kaski, K. Lagus, J. Salojarrvi, J. Honkela,
V. Paatero, and A. Saarela. 2000. Self Organization of
a Massive Document Collection. IEEE Trans. Neural
Networks, 11, 3, pp. 574-585.
The National Institute for Japanese Language. 1964. Bunrui
Goi Hyou (Japanese Thesaurus). Dainippon-tosho.
143
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 1?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Development of an automatic trend exploration system
using the MuST data collection
Masaki Murata1
murata@nict.go.jp
Qing Ma3,1
3qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru1,4
1kanamaru@nict.go.jp
Hitoshi Isahara1
isahara@nict.go.jp
1National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
3Ryukoku University
Otsu, Shiga, 520-2194, Japan
Koji Ichii2
ichiikoji@hiroshima-u.ac.jp
Tamotsu Shirado1
shirado@nict.go.jp
Sachiyo Tsukawaki1
tsuka@nict.go.jp
2Hiroshima University
1-4-1 Kagamiyama, Higashi-hiroshima,
Hiroshima 739-8527, Japan
4Kyoto University
Yoshida-nihonmatsu-cho, Sakyo-ku,
Kyoto, 606-8501, Japan
Abstract
The automatic extraction of trend informa-
tion from text documents such as news-
paper articles would be useful for explor-
ing and examining trends. To enable this,
we used data sets provided by a workshop
on multimodal summarization for trend in-
formation (the MuST Workshop) to con-
struct an automatic trend exploration sys-
tem. This system first extracts units, tem-
porals, and item expressions from news-
paper articles, then it extracts sets of ex-
pressions as trend information, and finally
it arranges the sets and displays them in
graphs. For example, when documents
concerning the politics are given, the sys-
tem extracts ?%? and ?Cabinet approval
rating? as a unit and an item expression in-
cluding temporal expressions. It next ex-
tracts values related to ?%?. Finally, it
makes a graph where temporal expressions
are used for the horizontal axis and the
value of percentage is shown on the ver-
tical axis. This graph indicates the trend
of Cabinet approval rating and is useful
for investigating Cabinet approval rating.
Graphs are obviously easy to recognize
and useful for understanding information
described in documents. In experiments,
when we judged the extraction of a correct
graph as the top output to be correct, the
system accuracy was 0.2500 in evaluation
A and 0.3334 in evaluation B. (In evalua-
tion A, a graph where 75% or more of the
points were correct was judged to be cor-
rect; in evaluation B, a graph where 50%
or more of the points were correct was
judged to be correct.) When we judged
the extraction of a correct graph in the top
five outputs to be correct, accuracy rose to
0.4167 in evaluation A and 0.6250 in eval-
uation B. Our system is convenient and ef-
fective because it can output a graph that
includes trend information at these levels
of accuracy when given only a set of doc-
uments as input.
1 Introduction
We have studied ways to automatically extract
trend information from text documents, such as
newspaper articles, because such a capability will
be useful for exploring and examining trends. In
this work, we used data sets provided by a work-
shop on multimodal summarization for trend in-
formation (the MuST Workshop) to construct an
automatic trend exploration system. This system
firsts extract units, temporals, and item expres-
sions from newspaper articles, then it extract sets
of expressions as trend information, and finally it
arranges the sets and displays them in graphs. For
example, when documents concerning the politics
1
are given, the system extracts ?%? and ?Cabinet
approval rating? as a unit and an item expression
including temporal expressions. It next extracts
values related to ?%?. Finally, it makes a graph
where temporal expressions are used for the hor-
izontal axis and the value of percentage is shown
on the vertical axis. This graph indicates the trend
of Cabinet approval rating and is useful for inves-
tigating Cabinet approval rating. Graphs are obvi-
ously easy to recognize and useful for understand-
ing information described in documents.
2 The MuST Workshop
Kato et al organized the workshop on multimodal
summarization for trend information (the MuST
Workshop) (Kato et al, 2005). In this work-
shop, participants were given data sets consisting
of newspaper documents (editions of the Mainichi
newspaper from 1998 and 1999 (Japanese docu-
ments)) that included trend information for vari-
ous domains. In the data, tags for important ex-
pressions (e.g. temporals, numerical expressions,
and item expressions) were tagged manually.1 The
20 topics of the data sets (e.g., the 1998 home-run
race to break the all-time Major League record,
the approval rating for the Japanese Cabinet, and
news on typhoons) were provided. Trend infor-
mation was defined as information regarding the
change in a value for a certain item. A change in
the number of home runs hit by a certain player or
a change in the approval rating for the Cabinet are
examples of trend information. In the workshop,
participants could freely use the data sets for any
study they chose to do.
3 System
3.1 Structure of the system
Our automatic trend exploration system consists
of the following components.
1. Component to extract important expressions
First, documents related to a certain topic are
given to the system, which then extracts im-
portant expressions that will be used to ex-
tract and merge trend information. The sys-
tem extracts item units, temporal units, and
item expressions as important expressions.
1We do not use manually provided tags for important ex-
pressions because our system automatically extracts impor-
tant expressions.
Here, important expressions are defined as
expressions that play important roles in a
given document set. Item expressions are de-
fined as expressions that are strongly related
to the content of a given document set.
1a. Component to extract important item
units
The system extracts item units that will
be used to extract and merge trend infor-
mation.
For example, when documents concern-
ing the home-run race are given, ?hon?
or ?gou? (the Japanese item units for the
number of home runs) such as in ?54
hon? (54th home run) are extracted.
1b. Component to extract important tempo-
ral units
The system extracts temporal units that
will also be used to extract and merge
trend information.
For example, the system extracts tempo-
ral units such as ?nichi? (day), ?gatsu?
(month), and ?nen? (year). In Japanese,
temporal units are used to express dates,
such as in ?2006 nen, 3 gatsu, 27 nichi?
for March 27th, 2006.
1c. Component to extract important item
expressions
The system extracts item expressions
that will also be used to extract and
merge trend information.
For example, the system extracts expres-
sions that are objects for trend explo-
ration, such as ?McGwire? and ?Sosa?
as item expressions in the case of docu-
ments concerning the home-run race.
2. Component to extract trend information sets
The system identifies the locations in sen-
tences where a temporal unit, an item unit,
and an item expression that was extracted by
the component to extract important expres-
sions appear in similar sentences and extracts
sets of important expressions described by
the sentences as a trend information set. The
system also extracts numerical values appear-
ing with item units or temporal units, and
uses the connection of the numerical values
and the item units or temporal units as nu-
merical expressions or temporal expressions.
2
For example, in the case of documents con-
cerning the home-run race, the system ex-
tracts a set consisting of ?item expression:
McGwire?, ?temporal expression: 11 day?
(the 11th), and ?numerical expression: 47
gou? (47th home run) as a trend information
set.
3. Component to extract and display important
trend information sets
The system gathers the extracted trend infor-
mation sets and displays them as graphs or by
highlighting text displays.
For example, for documents concerning
the home-run race, the system displays as
graphs the extracted trend information sets
for ?McGwire? . In these graphs, temporal
expressions are used for the horizontal axis
and the number of home runs is shown on the
vertical axis.
3.2 Component to extract important
expressions
The system extracts important expressions that
will be used to extract trend information sets. Im-
portant expressions belong to one of the following
categories.
? item units
? temporal units
? item expressions
We use ChaSen (Matsumoto et al, 1999), a
Japanese morphological analyzer, to extract ex-
pressions. Specifically, we use the parts of
speeches in the ChaSen outputs to extract the ex-
pressions.
The system extracts item units, temporal units,
and item expressions by using manually con-
structed rules using the parts of speeches. The
system extracts a sequence of nouns adjacent to
numerical values as item units. It then extracts
expressions from among the item units which in-
clude an expression regarding time or date (e.g.,
?year?, ?month?, ?day?, ?hour?, or ?second?) as
temporal units. The system extracts a sequence of
nouns as item expressions.
The system next extracts important item units,
temporal units, and item expressions that play im-
portant roles in the target documents.
The following three methods can be used to ex-
tract important expressions. The system uses one
of them. The system judges that an expression
producing a high value from the following equa-
tions is an important expression.
? Equation for the TF numerical term in Okapi
(Robertson et al, 1994)
Score =
?
i?Docs
TF
i
TF
i
+
l
i
?
(1)
? Use of total word frequency
Score =
?
i?Docs
TF
i
(2)
? Use of total frequency of documents where a
word appears
Score =
?
i?Docs
1 (3)
In these equations, i is the ID (identification
number) of a document, Docs is a set of document
IDs, TF
i
is the occurrence number of an expres-
sion in document i, l is the length of document i,
and ? is the average length of documents inDocs.
To extract item expressions, we also applied a
method that uses the product of the occurrence
number of an expression in document i and the
length of the expression as TF
i
, so that we could
extract longer expressions.
3.3 Component to extract trend information
sets
The system identifies the locations in sentences
where a temporal unit, an item unit, and an item
expression extracted by the component to extract
important expressions appears in similar sentences
and extracts sets of important expressions de-
scribed by the sentences as a trend information
set. When more than one trend information set
appears in a document, the system extracts the one
that appears first. This is because important and
new things are often described in the beginning of
a document in the case of newspaper articles.
3.4 Component to extract and display
important trend information sets
The system gathers the extracted trend informa-
tion sets and displays them in graphs or as high-
lighted text. In the graphs, temporal expressions
3
are used for the horizontal axis and numerical ex-
pressions are used for the vertical axis. The system
also displays sentences used to extract trend infor-
mation sets and highlights important expressions
in the sentences.
The system extracts multiple item units, tempo-
ral units, and item expressions (through the com-
ponent to extract important expressions) and uses
these to make all possible combinations of the
three kinds of expression. The system extracts
trend information sets for each combination and
calculates the value of one of the following equa-
tions for each combination. The system judges
that the combination producing a higher value rep-
resents more useful trend information. The fol-
lowing four equations can be used for this purpose,
and the system uses one of them.
? Method 1 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? S
1
? S
2
? S
3
(4)
? Method 2 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? (S
1
? S
2
? S
3
)
1
3 (5)
? Method 3 ? Use the frequency of trend in-
formation sets
M = Freq (6)
? Method 4 ? Use the scores of important ex-
pressions
M = S
1
? S
2
? S
3
(7)
In these equations, Freq is the number of trend
information sets extracted as described in Section
3.3, and S1, S2, and S3 are the values of Score as
calculated by the corresponding equation in Sec-
tion 3.2.
The system extracts the top five item units, the
top five item expressions, and the top three tem-
poral units through the component to extract im-
portant expressions and forms all possible combi-
nations of these (75 combinations). The system
then calculates the value of the above equations for
these 75 combinations and judges that a combina-
tion having a larger value represents more useful
trend information.
4 Experiments and Discussion
We describe some examples of the output of our
system in Sections 4.1, 4.2, and 4.3, and the re-
sults from our system evaluation in Section 4.4.
We made experiments using Japanese newspaper
articles.
4.1 Extracting important expressions
To extract important expressions we applied the
equation for the TF numerical term in Okapi and
the method using the product of the occurrence
number for an expression and the length of the
expression as TF
i
for item expressions. We did
experiments using the three document sets for ty-
phoons, the Major Leagues, and political trends.
The results are shown in Table 1.
We found that appropriate important expres-
sions were extracted for each domain. For ex-
ample, in the data set for typhoons, ?typhoon?
was extracted as an important item expression and
an item unit ?gou? (No.), indicating the ID num-
ber of each typhoon, was extracted as an im-
portant item unit. In the data set for the Major
Leagues, the MuST data included documents de-
scribing the home-run race between Mark McG-
wire and Sammy Sosa in 1998. ?McGwire? and
?Sosa? were properly extracted among the higher
ranks. ?gou? (No.) and ?hon? (home run(s)), im-
portant item units for the home-run race, were
properly extracted. In the data set for political
trends, ?naikaku shiji ritsu? (cabinet approval rat-
ing) was properly extracted as an item expression
and ?%? was extracted as an item unit.
4.2 Graphs representing trend information
We next tested how well our system graphed the
trend information obtained from the MuST data
sets. We used the same three document sets as in
the previous section. As important expressions in
the experiments, we used the item unit, the tempo-
ral unit, and the item expression with the highest
scores (the top ranked ones) which were extracted
by the component to extract important expressions
using the method described in the previous sec-
tion. The system made the graphs using the com-
ponent to extract trend information sets and the
component to extract and display important trend
information sets. The graphs thus produced are
shown in Figs. 1, 2, and 3. (We used Excel to draw
these graphs.) Here, we made a temporal axis for
each temporal expression. However, we can also
4
Table 1: Examples of extracting important expressions
Typhoon
item units temporal units item expressions
gou nichi taihuu
(No.) (day) (typhoon)
me-toru ji gogo
(meter(s)) (o?clock) (afternoon)
nin jigoro higai
(people) (around x o?clock) (damage)
kiro fun shashin setsumei
(kilometer(s)) (minute(s)) (photo caption)
miri jisoku chuushin
(millimeter(s)) (per hour) (center)
Major League
item units temporal units item expressions
gou nichi Maguwaia
(No.) (day) (McGwire)
hon nen honruida
(home run(s)) (year) (home run)
kai gatsu Ka-jinarusu
(inning(s)) (month) (Cardinals)
honruida nen buri Ma-ku Maguwaia ichiruishu
(home run(s)) (after x year(s) interval) (Mark McGwire, the first baseman)
shiai fun So-sa
(game(s)) (minute(s)) (Sosa)
Political Trend
item units temporal units item expressions
% gatsu naikaku shiji ritsu
(%) (month) (cabinet approval rating)
pointo gen nichi Obuchi naikaku
(decrease of x point(s)) (day) (Obuchi Cabinet)
pointo zou nen Obuchi shushou
(increase of x point(s)) (year) (Prime Minister Obuchi)
dai kagetu shijiritsu
(generation) (month(s)) (approval rating)
pointo bun no kitai
(point(s)) (divided) (expectation)
5
Figure 1: Trend graph for the typhoon data set
Figure 2: Trend graph for the Major Leagues data
set
display a graph where regular temporal intervals
are used in the temporal axis.
For the typhoon data set, gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as the top ranked item unit, temporal unit, and
item expression. The system extracted trend in-
formation sets using these, and then made a graph
where the temporal expression (day) was used for
the horizontal axis and the ID numbers of the ty-
phoons were shown on the vertical axis. The
MuST data included data for September and Octo-
ber of 1998 and 1999. Figure 1 is useful for seeing
when each typhoon hit Japan during the typhoon
season each year. Comparing the 1998 data with
that of 1999 reveals that the number of typhoons
increased in 1999.
For the Major Leagues data set, gou (No.), nichi
(day), and Maguwaia (McGwire) were extracted
with the top rank. The system used these to make
a graph where the temporal expression (day) was
used for the horizontal axis and the cumulative
number of home runs hit by McGwire was shown
on the vertical axis (Fig. 2). The MuST data
included data beginning in August, 1998. The
graph shows some points where the cumulative
number of home runs decreased (e.g., September
Figure 3: Trend graph for the political trends data
set
4th), which was obviously incorrect. This was be-
cause our system wrongly extracted the number of
home runs hit by Sosa when this was given close
to McGwire?s total.
In the political trends data set, %, gatsu
(month), and naikaku shiji ritsu (cabinet approval
rating) were extracted with the top rankings. The
system used these to make a graph where the
temporal expression (month) was used for the
horizontal axis and the Cabinet approval rating
(Japanese Cabinet) was shown as a percentage on
the vertical axis. The MuST data covered 1998
and 1999. Figure 2 shows the cabinet approval
rating of the Obuchi Cabinet. We found that the
overall approval rating trend was upwards. Again,
there were some errors in the extracted trend infor-
mation sets. For example, although June was han-
dled correctly, the system wrongly extracted May
as a temporal expression from the sentence ?in
comparison to the previous investigation in May?.
4.3 Sentence extraction and highlighting
display
We then tested the sentence extraction and high-
lighting display with respect to trend information
using the MuST data set; in this case, we used
the typhoon data set. As important expressions,
we used the item unit, the temporal unit, and the
item expression extracted with the highest scores
(the top ranked ones) by the component to extract
important expressions using the method described
in the previous section. Gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as an item unit, a temporal unit, and an item ex-
pression. The system extracted sentences includ-
ing the three expressions and highlighted these ex-
pressions in the sentences. The results are shown
in Figure 4. The first trend information sets to ap-
6
Sept. 16, 1998 No. 5
Large-scale and medium-strength Typhoon No. 5 made landfall near Omaezaki in Shizuoka Pre-
fecture before dawn on the 16th, and then moved to the northeast involving the Koshin, Kantou,
and Touhoku areas in the storm.
Sept. 21, 1998 No. 8
Small-scale Typhoon No. 8 made landfall near Tanabe City in Wakayama Prefecture around 4:00
p.m. on the 21st, and weakened while tracking to the northward across Kinki district.
Sept. 22, 1998 No. 7
Typhoon No. 7 made landfall near Wakayama City in the afternoon on the 22nd, and will hit the
Kinki district.
Sept. 21, 1998 No. 8
The two-day consecutive landfall of Typhoon No. 8 on the 21st and Typhoon No. 7 on the 22nd
caused nine deaths and many injuries in a total of six prefectures including Nara, Fukui, Shiga,
and so on.
Oct. 17, 1998 No. 10
Medium-scale and medium-strength Typhoon No. 10 made landfall on Makurazaki City in
Kagoshima Prefecture around 4:30 p.m. on the 17th, and then moved across the West Japan area
after making another landfall near Sukumo City in Kochi Prefecture in the evening.
Aug. 20, 1999 No. 11
The Meteorological Office announced on the 20th that Typhoon No. 11 developed 120 kilometers
off the south-southwest coast of Midway.
Sept. 14, 1999 No. 16
Typhoon No. 16, which developed off the south coast in Miyazaki Prefecture, made landfall near
Kushima City in the prefecture around 5:00 p.m. on the 14th.
Sept. 15, 1999 No. 16
Small-scale and weak Typhoon No. 16 became extratropical in Nagano Prefecture and moved out
to sea off Ibaraki Prefecture on the 15th.
Sept. 24, 1999 No. 18
Medium-scale and strong Typhoon No. 18 made landfall in the north of Kumamoto Prefecture
around 6:00 a.m. on the 24th, and after moving to Suo-Nada made another landfall at Ube City
in Yamaguchi Prefecture before 9:00 p.m., tracked through the Chugoku district, and then moved
into the Japan Sea after 10:00 p.m.
Sept. 25, 1999 No. 18
Typhoon No. 18, which caused significant damage in the Kyushu and Chugoku districts, weakened
and made another landfall before moving into the Sea of Okhotsk around 10:00 a.m. on the 25th.
Figure 4: Sentence extraction and highlighting display for the typhoon data set
7
pear are underlined twice and the other sets are
underlined once. (In the actual system, color is
used to make this distinction.) The extracted tem-
poral expressions and numerical expressions are
presented in the upper part of the extracted sen-
tence. The graphs shown in the previous section
were made by using these temporal expressions
and numerical expressions.
The extracted sentences plainly described the
state of affairs regarding the typhoons and were
important sentences. For the research being done
on summarization techniques, this can be consid-
ered a useful means of extracting important sen-
tences. The extracted sentences typically describe
the places affected by each typhoon and whether
there was any damage. They contain important
descriptions about each typhoon. This confirmed
that a simple method of extracting sentences con-
taining an item unit, a temporal unit, and an item
expression can be used to extract important sen-
tences.
The fourth sentence in the figure includes infor-
mation on both typhoon no.7 and typhoon no.8.
We can see that there is a trend information set
other than the extracted trend information set (un-
derlined twice) from the expressions that are un-
derlined once. Since the system sometimes ex-
tracts incorrect trend information sets, the high-
lighting is useful for identifying such sets.
4.4 Evaluation
We used a closed data set and an open data set
to evaluate our system. The closed data set was
the data set provided by the MuST workshop or-
ganizer and contained 20 domain document sets.
The data sets were separated for each domain.
We made the open data set based on the MuST
data set using newspaper articles (editions of the
Mainichi newspaper from 2000 and 2001). We
made 24 document sets using information retrieval
by term query. We used documents retrieved by
term query as the document set of the domain for
each query term.
We used the closed data set to adjust our system
and used the open data set to calculate the evalua-
tion scores of our system for evaluation.
We judged whether a document set included the
information needed to make trend graphs by con-
sulting the top 30 combinations of three kinds of
important expression having the 30 highest values
as in the method of Section 3.4. There were 19
documents including such information in the open
data. We used these 19 documents for the follow-
ing evaluation.
In the evaluation, we examined how accurately
trend graphs could be output when using the top
ranked expressions. The results are shown in Table
2. The best scores are described using bold fonts
for each evaluation score.
We used five evaluation scores. MRR is the av-
erage of the score where 1/r is given as the score
when the rank of the first correct output is r (Mu-
rata et al, 2005b). TP1 is the average of the pre-
cision in the first output. TP5 is the average of
the precision where the system includes a correct
output in the first five outputs. RP is the average
of the r-precision and AP is the average of the av-
erage precision. (Here, the average means that the
evaluation score is calculated for each domain data
set and the summation of these scores divided by
the number of the domain data sets is the average.)
R-precision is the precision of the r outputs where
r is the number of correct answers. Average pre-
cision is the average of the precision when each
correct answer is output (Murata et al, 2000). The
r-precision indicates the precision where the recall
and the precision have the same value. The preci-
sion is the ratio of correct answers in the system
output. The recall is the ratio of correct answers
in the system output to the total number of correct
answers.
Methods 1 to 4 in Table 2 are the methods used
to extract useful trend information described in
Section 3.4. Use of the expression length means
the product of the occurrence number for an ex-
pression and the length of the expression was used
to calculate the score for an important item ex-
pression. No use of the expression length means
this product was not used and only the occurrence
number was used.
To calculate the r-precision and average preci-
sion, we needed correct answer sets. We made the
correct answer sets by manually examining the top
30 outputs for the 24 (= 4? 6) methods (the com-
binations of methods 1 to 4 and the use of Equa-
tions 1 to 3 with or without the expression length)
and defining the useful trend information among
them as the correct answer sets.
In evaluation A, a graph where 75% or more of
the points were correct was judged to be correct.
In evaluation B, a graph where 50% or more of the
points were correct was judged to be correct.
8
Table 2: Experimental results for the open data
Evaluation A Evaluation B
MRR TP1 TP5 RP AP MRR TP1 TP5 RP AP
Use of Equation 1 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1360 0.1162 0.5522 0.4211 0.7368 0.1968 0.1565
Method 2 0.3847 0.3158 0.4211 0.1360 0.1150 0.5343 0.4211 0.6316 0.1880 0.1559
Method 3 0.3557 0.2632 0.4211 0.1360 0.1131 0.5053 0.3684 0.6316 0.1805 0.1541
Method 4 0.3189 0.2632 0.4211 0.1125 0.0973 0.4492 0.3158 0.6316 0.1645 0.1247
Use of Equation 2 and the expression length
Method 1 0.3904 0.3158 0.4737 0.1422 0.1154 0.5746 0.4211 0.7368 0.2127 0.1674
Method 2 0.3877 0.3158 0.4737 0.1422 0.1196 0.5544 0.4211 0.7368 0.2127 0.1723
Method 3 0.3895 0.3158 0.5263 0.1422 0.1202 0.5491 0.4211 0.7895 0.2127 0.1705
Method 4 0.2216 0.1053 0.3684 0.0846 0.0738 0.3765 0.2105 0.5789 0.1328 0.1043
Use of Equation 3 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
Use of Equation 1 and no use of the expression length
Method 1 0.3789 0.3158 0.4737 0.1294 0.1152 0.5456 0.4211 0.7368 0.2002 0.1627
Method 2 0.3750 0.3158 0.4211 0.1294 0.1137 0.5215 0.4211 0.6842 0.2002 0.1621
Method 3 0.3333 0.2632 0.4211 0.1119 0.1072 0.4798 0.3684 0.6842 0.1763 0.1552
Method 4 0.2588 0.1053 0.4737 0.1269 0.0872 0.3882 0.1579 0.6842 0.1833 0.1189
Use of Equation 2 and no use of the expression length
Method 1 0.3277 0.2105 0.4737 0.1134 0.0952 0.4900 0.2632 0.7895 0.1779 0.1410
Method 2 0.3662 0.2632 0.4737 0.1187 0.1104 0.5417 0.3684 0.7368 0.1831 0.1594
Method 3 0.3504 0.2632 0.4737 0.1187 0.1116 0.5167 0.3684 0.7368 0.1884 0.1647
Method 4 0.1877 0.0526 0.3684 0.0775 0.0510 0.3131 0.1053 0.5263 0.1300 0.0879
Use of Equation 3 and no use of the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
9
From the experimental results, we found that
the method using the total frequency for a word
(Equation 2) and the length of an expression was
best for calculating the scores of important expres-
sions.
Using the length of an expression was impor-
tant. (The way of using the length of an expres-
sion was described in the last part of Section 3.2.)
For example, when ?Cabinet approval rating? ap-
pears in documents, a method without expression
lengths extracts ?rating?. When the system ex-
tracts trend information sets using ?rating?, it ex-
tracts wrong information related to types of ?rat-
ing? other than ?Cabinet approval rating?. This
hinders the extraction of coherent trend informa-
tion. Thus, it is beneficial to use the length of an
expression when extracting important item expres-
sions.
We also found that method 1 (using both the fre-
quency of the trend information sets and the scores
of important expressions) was generally the best.
When we judged the extraction of a correct
graph as the top output in the experiments to be
correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
In terms of the evaluation scores for the 24 original
data sets (these evaluation scores were multiplied
by 19/24), when we judged the extraction of a cor-
rect graph as the top output in the experiments to
be correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
Our system is convenient and effective because it
can output a graph that includes trend information
at these levels of accuracy when given only a set
of documents as input.
As shown in Table 2, the best values for RP
(which indicates the precision where the recall and
the precision have the same value) and AP were
0.2127 and 0.1705, respectively, in evaluation B.
This RP value indicates that our system could
extract about one out of five graphs among the cor-
rect answers when the recall and the precision had
the same value.
5 Related studies
Fujihata et al (Fujihata et al, 2001) developed a
system to extract numerical expressions and their
related item expressions by using syntactic infor-
mation and patterns. However, they did not deal
with the extraction of important expressions or
gather trend information sets. In addition, they did
not make a graph from the extracted expressions.
Nanba et al (Nanba et al, 2005) took an
approach of judging whether the sentence rela-
tionship indicates transition (trend information)
or renovation (revision of information) and used
the judgment results to extract trend information.
They also constructed a system to extract nu-
merical information from input numerical units
and make a graph that includes trend information.
However, they did not consider ways to extract
item numerical units and item expressions auto-
matically.
In contrast to these systems, our system auto-
matically extracts item numerical units and item
expressions that each play an important role in a
given document set. When a document set for
a certain domain is given, our system automati-
cally extracts item numerical units and item ex-
pressions, then extracts numerical expressions re-
lated to these, and finally makes a graph based
on the extracted numerical expressions. When a
document set is given, the system automatically
makes a graph that includes trend information.
Our system also uses an original method of pro-
ducing more than one graphs and selecting an ap-
propriate graph among them using Methods 1 to 4,
which Fujihata et al and Namba et al did not use.
6 Conclusion
We have studied the automatic extraction of trend
information from text documents such as newspa-
per articles. Such extraction will be useful for ex-
ploring and examining trends. We used data sets
provided by a workshop on multimodal summa-
rization for trend information (the MuST Work-
shop) to construct our automatic trend exploration
system. This system first extracts units, tempo-
rals, and item expressions from newspaper arti-
cles, then it extracts sets of expressions as trend
information, and finally it arranges the sets and
displays them in graphs.
In our experiments, when we judged the extrac-
tion of a correct graph as the top output to be cor-
rect, the system accuracy was 0.2500 in evaluation
10
A and 0.3334 in evaluation B. (In evaluation A, a
graph where 75% or more of the points were cor-
rect was judged to be correct; in evaluation B, a
graph where 50% or more of the points were cor-
rect was judged to be correct.) When we judged
the extraction of a correct graph in the top five out-
puts to be correct, we obtained accuracy of 0.4167
in evaluation A and 0.6250 in evaluation B. Our
system is convenient and effective because it can
output a graph that includes trend information at
these levels of accuracy when only a set of docu-
ments is provided as input.
In the future, we plan to continue this line of
study and improve our system. We also hope to
apply the method of using term frequency in doc-
uments to extract trend information as reported by
Murata et al (Murata et al, 2005a).
References
Katsuyuki Fujihata, Masahiro Shiga, and Tatsunori
Mori. 2001. Extracting of numerical expressions
by constraints and default rules of dependency struc-
ture. Information Processing Society of Japan,
WGNL 145.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. MuST: A workshop on multimodal
summarization for trend information. Proceedings
of the Fifth NTCIR WorkshopMeeting on Evaluation
of Information Access Technologies: Information
Retrieval, Question Answering and Cross-Lingual
Information Access.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis
system ChaSen version 2.0 manual 2nd edition.
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000. Japanese probabilistic information retrieval
using location and category information. The Fifth
International Workshop on Information Retrieval
with Asian Languages, pages 81?88.
Masaki Murata, Koji Ichii, Qing Ma, Tamotsu Shirado,
Toshiyuki Kanamaru, and Hitoshi Isahara. 2005a.
Trend survey on Japanese natural language process-
ing studies over the last decade. In The Second In-
ternational Joint Conference on Natural Language
Processing, Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial
Abstracts.
Masaki Murata, Masao Utiyama, and Hitoshi Isahara.
2005b. Use of multiple documents as evidence with
decreased adding in a Japanese question-answering
system. Journal of Natural Language Processing,
12(2).
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
Information Processing Society of Japan, WGNL
168, pages 67?74.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
11
